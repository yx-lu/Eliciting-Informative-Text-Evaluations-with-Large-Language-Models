uid,paper_uid,paper_title,decision,comment,title
EE4Ml5hZtI,ryxyCeHtPB,"Pay Attention to Features, Transfer Learn Faster CNNs",Accept (Poster),"This paper presents an attention-based approach to transfer faster CNNs, which tackles the problem of jointly transferring source knowledge and pruning target CNNs.

Reviewers are unanimously positive on the paper, in terms of a well-written paper with a reasonable approach that yields strong empirical performance under the resource constraint.

AC feels that the paper studies an important problem of making transfer learning faster for CNNs, however, the proposed model is a relatively straightforward combination of fine-tuning and filter-pruning, each having very extensive prior works. Also, AC has very critical comments for improving this paper:

- The Attentive Feature Distillation (AFD) module is very similar to DELTA (Li et al. ICLR 2019) and L2T (Jang et al. ICML 2019), significantly weakening the novelty. The empirical evaluation should consider DELTA as baselines, e.g. AFS+DELTA.

I accept this paper, assuming that all comments will be well addressed in the revision.",Paper Decision
aC4uAM2KZ,BJlA6eBtvH,Differentiable Hebbian Consolidation for Continual Learning,Reject,"The reviewers agreed that this paper tackles an important problem, continual learning, with a method that is well motivated and interesting. The rebuttal was very helpful in terms of relating to other work. However, the empirical evaluation, while good, could be improved. In particular, it is not clear based on the evaluation to what extent more interesting continual learning problems can be tackled. We encourage the authors to continue pursuing this work.",Paper Decision
2aqzQ-03qO,SkeATxrKwH,"Generative Hierarchical Models for Parts, Objects, and Scenes",Reject,"The authors proposes a generative model with a hierarchy of latent variables corresponding to a scene, objects, and object parts. 

The submission initially received low scores with 2 rejects and 1 weak reject.  After the rebuttal, the paper was revised and improved, with significant portions of the paper completely rewritten (the description of the model was rewritten and a new experiment comparing the proposed model to SPAIR was added).  While the reviewers acknowledged the improvement in the paper and accordingly adjusted their score upward, the paper is still not sufficiently strong enough to be accepted (it currently has 3 weak rejects). 

The reviewer expressed the following concerns:
1. The experiments uses only a toy dataset that does not convincingly demonstrate the generalizability of the method to more realistic/varied scenarios. In particular, the reviewers voiced concern that the dataset is tailored to the proposed method

2. Lack of comparisons with baseline methods such as AIR/SPAIR and other work on hierarchical generative models such as SPIRAL.
In the revision, the author added an experiment comparing to SPAIR, so this is partially addressed.  As a whole, the paper is still weak in experimental rigor.  The authors argue that as their main contribution is the design and successful learning of a probabilistic scene graph representation, there is no need for ablation studies or to compare against baselines because their method ""can bring better compositionality, interpretability, transferability, and generalization"".  This argument is unconvincing as in a scientific endeavor, the validity of such claims needs to be shown via empirical comparisons with prior work and ablation studies.

3. Limited novelty
The method is a fairly straightforward extension of SPAIR with another hierarchy layer. This would not be a concern if the experimental aspects of the work was stronger. 

The AC agrees with the issues pointed by the reviewers.  In addition, the initial presentation of the paper was very poor.  While the paper has been improved, the changes are substantial (with the description of the method and intro almost entirely rewritten). Regardless, despite the improvements in writing, the paper is still not strong enough to be accepted.  I would recommend the authors improve the evaluation and resubmit. ",Paper Decision
o7Wu2AUY2i,S1x6TlBtwB,Mixture Distributions for Scalable Bayesian Inference,Reject,"This paper proposes to use mixture distributions to improve uncertainty estimates in BNNs. Ensemble methods are interpreted as a Bayesian mixture posterior approximation. To reduce the computation, a modification to BBB is provided based on a concrete mixture distribution.

Both R1 and R3 have given useful feedback. It is clear that interpretation of ensemble as a Bayesian posterior is well known, and some of them also have theoretical issues. The experiment to clearly comparing proposed mixture posterior to more commonly used mixture distribution is also necessary. 

Due to these reasons, I recommend to reject this paper. I encourage the authors to use reviewers feedback to improve the paper.",Paper Decision
_fHNjvlDFt,Skl6peHFwS,Best feature performance in codeswitched hate speech texts,Reject,"This paper focuses on hate speech detection and compares several classification methods including Naive Bayes, SVM, KNN, CNN, and many others. The most valuable contribution of this work is a dataset of ~400,000 tweets from 2017 Kenyan general election, although it is unclear whether the authors plan to release the dataset in the future.

The paper is difficult to follow, uses an incorrect ICLR format, and is full of typos.

All three reviewers agree that while this paper deals with an important topic in social media analysis, it is not ready for publication in its current state. The authors did not provide a rebuttal to reviewers' concerns.

I recommend rejecting this paper for ICLR.",Paper Decision
4awCnNnpYO,S1e2agrFvS,Geom-GCN: Geometric Graph Convolutional Networks,Accept (Spotlight),This paper is consistently supported by all three reviewers and thus an accept is recommended.,Paper Decision
CiCsKPUSHX,SyxhaxBKPS,Smart Ternary Quantization,Reject,"This paper studies mixed-precision quantization in deep networks where each layer can be either binarized or ternarized. The proposed regularization method is simple and straightforward. However, many details and equations are not stated clearly. Experiments are performed on small-scale image classification data sets. It will also be more convincing to try larger networks or data sets. More importantly, many recent methods that can train mixed-precision networks are not cited nor compared. Figures 3 and 4 are difficult to interpret, and sensitivity on the new hyper-parameters should be studied. The use of ""best validation accuracy"" as performance metric may not be fair. Finally, writing can be improved. Overall, the proposed idea might have merit, but does not seem to have been developed enough.",Paper Decision
uZDExoLfq,Syxi6grFwH,HIPPOCAMPAL NEURONAL REPRESENTATIONS IN CONTINUAL LEARNING,Reject,"This paper analyzes neural recording data taken from rodents performing a continual learning task using demixed principal component analysis, and aims to find representations for behaviorally relevant variables. They compare these features with those of a deep RL agent.

I am a big fan of papers like this that try to bridge between neuroscience and machine learning. It seems to have a great motivation and there are some interesting results presented. However the reviewers pointed out many issues that lead me to believe this work is not quite ready for publication. In particular, not considering space when analyzing hippocampal rodent data, as R2 points out, seems to be a major oversight. In addition, the sample size is incredibly small (5 rats, only 1 of which was used for the continual learning simulation). This seems to me like more of an exploratory, pilot study than a full experiment that is ready for publication, and therefore I am unfortunately recommending reject.

Reviewer comments were very thorough and on point. Sounds like the authors are already working on the next version of the paper with these points in mind, so I look forward to it.
",Paper Decision
paDqAd9O-j,BklsagBYPS,A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS,Reject,"This paper proposes to measure the distance of the generator manifold to the training data. The proposed approach bears significant similarity to past studies that also sought to analyze the behavior of generative models that define a low-dimensional manifold (e.g. Webster 2019, and in particular, Xiang 2017). I recommend that the authors perform a broader literature search to better contextualize the claims and experiments put forth in the paper.

The proposed method also suffers from some limitations that are not made clear in the paper. First, the measure depends only on the support of the generator, but not the density. For models that have support everywhere (exact likelihood models tend to have this property by construction), the measure is no longer meaningful. Even for VAEs, the measure is only easily applicable if the decoder is non-autoregressive so that the procedure can be applied only to the mean decoding. 

In this current state, I do not recommend the paper for submission.

Xiang (2017). On the Effects of Batch and Weight Normalization in Generative Adversarial Networks
Webster (2019). Detecting Overfitting of Deep Generative Networks via Latent Recovery
",Paper Decision
jt9gk0Eq0,BkeoaeHKDS,Gradients as Features for Deep Representation Learning,Accept (Poster),"The paper makes a reasonable contribution to extracting useful features from a pre-trained neural network.  The approach is conceptually simple and sufficient evidence is provided of its effectiveness.  In addition to the connection to tangent kernels there also appears to be a relationship to holographic feature representations of deep networks.  The authors did do a reasonable job of providing additional ablation studies, but the paper would be improved if a clearer study were added to investigate applying the technique to different layers.  All of the reviewer comments appear worthwhile, but AnonReviewer2 in particular provides important guidance for improving the paper.",Paper Decision
w-C7OPuhim,rJgqalBKvH,Deceptive Opponent Modeling with Proactive Network Interdiction for Stochastic Goal Recognition Control,Reject,This paper has been withdrawn by the authors.,Paper Decision
bgMqqD6tkI,Hyg96gBKPS,Monotonic Multihead Attention,Accept (Poster),"This paper extends previous models for monotonic attention to the multi-head attention used in Transformers, yielding ""Monotonic Multi-head Attention."" The proposed method achieves better latency-quality tradeoffs in simultaneous MT tasks in two language pairs.

The proposed method is a relatively straightforward extension of the previous Hard and Infinite Lookback monotonic attention models. However, all reviewers seem to agree that this paper is a meaningful contribution to the task of simultaneously MT, and the revised version of the paper (along with the authors' comments) addressed most of the raised concerns.

Therefore, I propose acceptance of this paper.",Paper Decision
fPjrfR2Q1_,HyeYTgrFPB,Massively Multilingual Sparse Word Representations,Accept (Poster),"This paper describes a new method for creating word embeddings that can operate on corpora from more than one language.  The algorithm is simple, but rivals more complex approaches.  

The reviewers were happy with this paper.  They were also impressed that the authors ran the requested multi-lingual BERT experiments, even though they did not show positive results. One reviewer did think that non-contextual word embeddings were of less interest to the NLP community, but thought your arguments for the computational efficiency were convincing.",Paper Decision
aR5hnzfg8o,HJeYalBKvr,Attention over Phrases,Reject,"This paper incorporates phrases within the transformer architecture.

The underlying idea is interesting, but the reviewers have raised serious concerns with both clarity and the trustworthiness of the experimental evaluation, and thus I cannot recommend acceptance at this time.",Paper Decision
YVEehT7h70,Skxd6gSYDS,Query-efficient Meta Attack to Deep Neural Networks,Accept (Poster),"This paper proposes a meta attack approach based on meta learning approaches to learn generalizable prior from the previously observed attack patterns. The proposed approach is able to attack targeted models with much fewer queries. After author response, all reviewers are very positive about the paper. Thus I recommend accept. ",Paper Decision
k5JHM7N2e,HJxdTxHYvB,BREAKING  CERTIFIED  DEFENSES:  SEMANTIC  ADVERSARIAL  EXAMPLES  WITH  SPOOFED  ROBUSTNESS  CERTIFICATES,Accept (Poster),"This work presents a ""shadow attack"" that fools certifiably robust networks by producing imperceptible adversarial examples by search outside of the certified radius. The reviewers are generally positive on the novelty and contribution of the work. ",Paper Decision
o30pC3H2Uu,SJgdpxHFvH,Meta-Learning Initializations for Image Segmentation,Reject,"The reviewers reached a consensus that the paper was not ready to be accepted in its current form. The main concerns were in regard to clarity, relatively limited novelty, and a relatively unsatisfying experimental evaluation. Although some of the clarity concerns were addressed during the response period, the other issues still remained, and the reviewers generally agreed that the paper should be rejected.",Paper Decision
LTAafnEG6,rkewaxrtvr,Privacy-preserving Representation Learning by Disentanglement,Reject,"The paper leverages variational auto-encoders (VAEs) and disentanglement to generate data representations that hide sensitive attributes. The reviewers have identified several issues with the paper, including its false claims or statements about differential privacy, unclear privacy guarantee, and lack of related work discussion. The authors have not directly addressed these issues.",Paper Decision
0lFT19rXkk,rJg8TeSFDH,An Exponential Learning Rate Schedule for Deep Learning,Accept (Spotlight),"After the revision, the reviewers agree on acceptance of this paper.    Let's do it.",Paper Decision
4lVMROrsN7,rJe8pxSFwr,End-to-end learning of energy-based representations for irregularly-sampled signals and images,Reject,"This work looks at ways to fill in incomplete data, through two different energy terms.
Reviewers find the work interesting, however it is very poorly written and nowhere near ready for publication. This comes on top of poorly stated motivation and insufficient comparison to prior work.
Authors have chosen not to answer the reviewers' comments.
We recommend rejection.",Paper Decision
i1zKlyyBEw,B1xSperKvH,Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation,Accept (Poster),"After the rebuttal, all reviewers rated this paper as a weak accept. 
The reviewer leaning towards rejection was satisfied with the author response and ended up raising their rating to a weak accept.  The AC recommends acceptance.",Paper Decision
K1gHRlmgml,S1erpeBFPB,How to 0wn the NAS in Your Spare Time,Accept (Poster),"This paper proposes using Flush+Reload to infer the deep network architecture of another program, when the two programs are running on the same machine (as in cloud computing or similar). 

There is some disagreement about this paper; the approach is thoughtful and well executed, but one reviewer had concerns about its applicability and realism. Upon reading the author's rebuttal I believe these to be largely addressed, or at least as realistically as one can in a single paper. Therefore I recommend acceptance.",Paper Decision
yjaXmBk1rN,S1lBTerYwH,Generalized Zero-shot ICD Coding,Reject,"This paper proposes a method to do zero-shot ICD coding, which involves assigning natural language labels (ICD codes) to input text. This is an important practical problem in healthcare, and it is not straightforward to solve, because many ICD codes have none or very few training examples due to the long distribution tail. The authors adapt a GAN-based technique previously used in vision to solve this problem. All of the reviewers agree that the paper is well written and well executed, and that the results are good. However, the reviewers have expressed concerns about the novelty of the GAN adaptation step, and left this paper very much borderline based on the scores it received. Due to the capacity restrictions I therefore have to recommend rejection, however I hope that the  authors resubmit elsewhere. ",Paper Decision
QfJjGq7LS,ryx4TlHKDS,EXACT ANALYSIS OF CURVATURE CORRECTED LEARNING DYNAMICS IN DEEP LINEAR NETWORKS,Reject,"This paper aims to study the effect of curvature correction techniques on training dynamics. The focus is on understanding how natural gradient based methods affect training dynamics of deep linear networks. The main conclusion of the analysis is that it does not fundamentally affect the path of convergence but rather accelerates convergence. They also show that layer correction techniques alone do not suffice. In the discussion the reviewers raised concerns about extrapolating too much based on linear networks and also lack of a cohesive literature review. One reviewer also mentioned that there is not enough technical detail. These issues were partially addressed in the response. I think the topic of the paper is interesting and timely. However, I concur with Reviewer #2 that there are still lots of missing detail and the connection with the nonlinear case is not clear (however the latter is not strictly necessary in my opinion if the rest of the paper is better written). As a result I think the paper in its current form is not ready for publication. ",Paper Decision
iV_Cewp2j-,HJgzpgrYDr,Learning to Reason: Distilling Hierarchy via Self-Supervision and Reinforcement Learning,Reject,"The authors present a self-supervised framework for learning a hierarchical policy in reinforcement learning tasks that combines a high-level planner over learned latent goals with a shared low-level goal-completing control policy.  The reviewers had significant concerns about both problem positioning (w.r.t. existing work) and writing clarity, as well as the fact that all comparative experiments were ablations, rather than comparisons to prior work.  While the reviewers agreed that the authors reasonably resolved issues of clarity, there was not agreement that concerns about positioning w.r.t. prior work and experimental comparisons were sufficiently resolved.  Thus, I recommend to reject this paper at this time.",Paper Decision
kQujRaJ0z,HyebplHYwB,The Shape of Data: Intrinsic Distance for Data Distributions,Accept (Poster),"This paper introduces a way to measure dataset similarities. Reviewers all agree that this method is novel and interesting. A few questions initially raised by reviewers regarding models with and without likelihood, geometric exposition, and guarantees around GW, are promptly answered by authors, which raised the score to all weak accept. 
",Paper Decision
Ni5eKNNvjY,B1xbTlBKwB,Measuring Numerical Common Sense: Is A Word Embedding Approach Effective?,Reject,"The authors tackle an interesting and important problem, developing numerical common-sense. They use a crowdsourcing service to collect a dataset and use regression from word embeddings to numerical common sense.

Reviewers were concerned with the size and quality of the dataset, the quality of the prediction methods used, and the analysis of the experimental results.

Given the many concerns, I recommend rejecting the paper, but I encourage the authors to revise the paper to address the concerns and resubmit to another venue.",Paper Decision
0emF5aKs-X,Bkel6ertwS,Learning DNA folding patterns with Recurrent Neural Networks ,Reject,"The authors consider the problem of predicting DNA folding patterns.                                                               
They use a range of simple, linear models and find that a bi-LSTM architecture                                                     
yielded best performance.                                                                                                          
                                                                                                                                   
This paper is below acceptance.                                                                                                    
Reviewers pointed out strong similarity to previously published work.                                                              
Furthermore the manuscript lacked in clarity, leaving uncertain eg details about                                                   
experimental details. ",Paper Decision
SPoGClV2DZ,BkexaxBKPB,Generative Adversarial Nets for Multiple Text Corpora,Reject,"The general consensus amongst the reviewers is that this paper is not quite ready for publication. The reviewers raised several issues with your paper, which I hope will help you as you work towards finding a home for this work.",Paper Decision
rEmwTBSm2J,rkgg6xBYDH,Understanding Generalization in Recurrent Neural Networks,Accept (Poster),"This paper presents a generalization bound for RNNs based on matrix-1 norm and Fisher-Rao norm. As the initial bound relies on non-signularity of input covariance, which may not always hold in practice, the authors present additional analysis by noise injection to ensure covariance is positive definite. Through the resulted bound, the paper discusses how weight decay and gradient clipping in the training can help generalization. There were some concerns raised by reviewers, including  rigorous report of the experiment results,  claims on generalization in IMDB experiment,  claims of no explicit dependence on the size of networks, and the relationship of small eigenvalues in input covariance to high frequency features. The authors responded to these and also revised their draft to address most of these concerns (in particular, authors added a new section in the appendix that includes additional experimental results). Reviewers were mainly satisfied with the responses and the revision, and they all recommend accept.
",Paper Decision
XrNsBg-gYo,HygkpxStvr,Weakly-Supervised Trajectory Segmentation for Learning Reusable Skills,Reject,"The authors present a multiple instance learning-based approach that uses weak supervison (of which skills appear in any given trajectory)  to automatically segment a set of skills from demonstrations.  The reviewers had significant concerns about the significance and performance of the method, as well as the metrics used for analysis.  Most notably, neither the original paper nor the rebuttal provided a sufficient justification or fix for the lack of analysis beyond accuracy scores (as opposed to confusion matrices, precision/recall, etc), which leaves the contribution and claims of the paper unclear.  Thus, I recommend rejection at this time.",Paper Decision
g-YhRcFdh,Bke02gHYwB,Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution,Reject,"The paper presents an approach to learning interpretable word embeddings. The reviewers put this in the lower half of the submissions. One reason seems to be the size of the training corpora used in the experiments, as well as the limited number of experiments; another that the claim of interpretability seems over-stated. There's also a lack of comparison to related work. I also think it would be interesting to move beyond the standard benchmarks - and either use word embeddings downstream or learn word embeddings for multiple languages [you should do this, regardless] and use Procrustes analysis or the like to learn a mapping: A good embedding algorithm should induce more linearly alignable embedding spaces. 

NB: While the authors cite other work by these authors, [0] seems relevant, too. Other related work: [1-4]. 

[0] https://www.aclweb.org/anthology/Q15-1016.pdf
[1] https://www.aclweb.org/anthology/Q16-1020.pdf
[2] https://www.aclweb.org/anthology/W19-4329.pdf
[3] https://www.aclweb.org/anthology/D17-1198/
[4] https://www.aclweb.org/anthology/D15-1183.pdf",Paper Decision
ju25JPc_-a,S1xRnxSYwS,Goten: GPU-Outsourcing Trusted Execution of Neural Network Training and Prediction,Reject,"This paper proposes a framework for privacy-preserving training of neural networks within a Trusted Execution Environment (TEE) such as Intel SGX. The reviewers found that this is a valuable research directions, but found that there were significant flaws in the experimental setup that need to be addressed. In particular, the paper does not run all the experiments in the same setup, which leads to the use of scaling factor in some cases. The reviewers found that this made it difficult to make sense of the results. The writing of this paper should be streamlined, along with the experiments before resubmission.",Paper Decision
dWBjn-o9m,r1x63grFvH,Limitations for Learning from Point Clouds,Reject,"The present paper establishes uniform approximation theorems (UATs) for PointNet and DeepSets that do not fix the cardinality of the input set. 

Two nonexperts read the paper and came away not understanding what this exercise has taught us and why the weakening of the hypotheses was important. The authors made no attempt to argue these points in their rebuttals and so I went looking at the paper to find the answer in their revisions, but did not find it after scanning through the paper. I think a paper like this needs to explain what is gained and what obstructions earlier approaches met, and why the current techniques side step those. One of the reviewers felt that the fixed cardinality assumption was mild. I'm really not sure why the authors didn't attack this idea. Maybe it is mild in some technical sense?

What I read of the paper seemed excellent in term of style and clarity. I think the paper simply needs to make a better case that it is not merely an exercise in topology. I think the result here is publishable on its own grounds, but for the paper to effectively communicate those findings, the authors should have revised it to address these issues. They chose not to and so I recommend ICLR take a pass. Once the reviewers revised the framing and scope/impact, provided it doesn't sound trivial, I think it'll be ready for publication.

",Paper Decision
PugTQsmZ1p,BJlahxHYDS,Conservative Uncertainty Estimation By Fitting  Prior Networks,Accept (Poster),"The paper provides theoretical justification for a previously proposed method for uncertainty estimation based on sampling from a prior distribution (Osband et al., Burda et al.).

The reviewers initially raised concerns about significance, clarity and experimental evaluation, but the author rebuttal addressed most of these concerns.

In the end, all the reviewers agreed that the paper deserves to be accepted.",Paper Decision
hYLvNQJ6XW,SJgn3lBtwH,Re-Examining Linear Embeddings for High-dimensional Bayesian Optimization,Reject,"This paper explores the practice of using lower-dimensional embeddings to perform Bayesian optimization on high dimensional problems.  The authors identify several issues with performing such an optimization on a lower-dimensional projection and propose solutions leading to better empirical performance of the optimization routine.  Overall the reviewers found the work well written and enjoyable.  However, the reviewers were concerned primarily about the connection to existing literature (R2) and the empirical analysis (R1, R3).  The authors claim that their method outperforms state-of-the-art on a range of problems but the reviewers did not feel there was sufficient empirical evidence to back up this claim. 
 Unfortunately, as such the paper is not quite ready for publication.  The authors claim to have significantly expanded the experiments in the response period, however, which will likely make it much stronger for a future submission.
",Paper Decision
GBLwLAMRgN,Syx33erYwH,ASYNCHRONOUS MULTI-AGENT GENERATIVE ADVERSARIAL IMITATION LEARNING,Reject,"This paper extends multi-agent imitation learning to extensive-form games. There is a long discussion between reviewer #3 and the authors on the difference between Markov Games (MGs) and Extensive-Form Games (EFGs). The core of the discussion is on whether methods developed under the MG formalism (where agents take actions simultaneously) naturally can be applied to the EFG problem setting (where agents can take actions asynchronously). Despite the long discussion, the authors and reviewer did not come to an agreement on this point. Given that it is a crucial point for determining the significance of the contribution, my decision is to decline the paper. I suggest that the authors add a detailed discussion on why MG methods cannot be applied to EFGs in the way suggested by reviewer #3 in the next version of this work and then resubmit.",Paper Decision
-Ug2fCPXLB,Hkxi2gHYvH,Predictive Coding for Boosting Deep Reinforcement Learning with Sparse Rewards,Reject,"The paper proposes to use the representation learned via CPC to do reward shaping via clustering the embedding and providing a reward based on the distance from the goal.

The reviewers point out some conceptual issues with the paper, the key one being that the method is contingent on a random policy being able to reach the goal, which is not true for difficult environments that the paper claims to be motivated by. One reviewer noted limited experiment runs and lack of comparisons with other reward shaping methods.

I recommend rejection, but hope the authors find the feedback helpful and submit a future version elsewhere.",Paper Decision
a0sO1Ldrx1,rklj3gBYvH,NORML: Nodal Optimization for Recurrent Meta-Learning,Reject,"The paper proposes a LSTM-based meta-learning approach that learns how to update each neuron in another model for best few-shot learning performance.

The reviewers agreed that this is a worthwhile problem and the approach has merits, but that it is hard to judge the significance of the work, given limited or unclear novelty compared to the work of Ravi & Larochelle (2017) and a lack of fair baseline comparisons.

I recommend rejecting the paper for now, but encourage the authors to take the reviewers' feedback into account and submit to another venue.",Paper Decision
Br72Ua5za1,r1gc3lBFPH,Keyword Spotter Model for Crop Pest and Disease Monitoring from Community Radio Data,Reject,"Main summary:  Design an effective and economical model which spots keywords about pests and disease from community radio data in Luganda and English.

Discussions:
all reviewers vote on rejecting the paper, due to lack of generalizability, training and evaluation discussion need work
Recommendation: Reject",Paper Decision
fhaP5EbL15,SJx9ngStPH,NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search,Accept (Poster),"The authors present a new benchmark for architecture search. Reviews were somewhat mixed, but also with mixed confidence scores. I recommend acceptance as poster - and encourage the authors to also cite https://openreview.net/forum?id=HJxyZkBKDr",Paper Decision
msEYgWjTQ,Hyg53gSYPB,Defense against Adversarial Examples by Encoder-Assisted Search in the Latent Coding Space,Reject,"The paper proposes a defense for adversarial attacks based on autoencoders that tries to find the closest point to the natural image in the output span of the decoder and ""purify"" the adversarial example. There were concerns about the work being too incremental over DefenseGAN and about empirical evaluation of the defense. It is crucial to test the defense methods against best available attacks to establish the effectiveness. Authors should also discuss and consider evaluating their method against the attack proposed in https://arxiv.org/pdf/1712.09196.pdf that claims to greatly reduce the defense accuracy of DefenseGAN. ",Paper Decision
a7t7RRFwj-,BkxthxHYvr,Conditional generation of molecules from disentangled representations,Reject,"The paper aims to generate molecules with desired properties using a variant of supervised variational auto-encoders. Disentanglement is encouraged among the style factors.  The reviewers point out that the idea is nice, but authors avoid quantitative comparison with SotA Graph-based generative models.  Especially, the JT-VAE is acknowledged as a strong baseline widely in the community and is a VAE-based model, it is important to do these comparisons. ",Paper Decision
Mv3l-P3-C,Bklu2grKwB,Learning RNNs with Commutative State Transitions,Reject,"This paper examines learning problems where the network outputs are intended to be invariant to permutations of the network inputs.  Some past approaches for this problem setting have enforced permutation-invariance by construction.  This paper takes a different approach, using a recurrent neural network that passes over the data. The paper proves the network will be permutation invariant when the internal state transition function is associative and commutative.  The paper then focuses on the commutative property by describing a regularization objective that pushes the recurrent network towards becoming commutative.  Experimental results with this regularizer show potentially better performance than DeepSet, another architecture that is designed for permutation invariance.

The subsequent discussion of the paper raised several concerns with the current version of the paper. The theoretical contributions for full permutation-invariance follow quickly from the prior DeepSet results.  The paper's focus on commutative regularization in the absence of associative regularization is not compelling if the objective is really for permutation invariance.  The experimental results were limited in scope.  These results lacked error bars and an examination of the relevance of associativity. The reviewers also identified several related lines of work which could provide additional context for the results that were missing from the paper.

This paper is not ready for publication due to the multiple concerns raised by the reviewers.  The paper would become stronger by addressing these concerns, particularly the associativity of the transition function, empirical results, and related work. ",Paper Decision
ZbmJAKoT_f,BkePneStwH,XD: Cross-lingual Knowledge Distillation for Polyglot Sentence Embeddings,Reject,"This paper proposes a method for transferring an NLP model trained on one language a new language, without using labeled data in the new language. 

Reviewers were split on their recommendations, but the reviews collectively raised a number of concerns which, together, make me uncomfortable accepting the paper. Reviewers were not convinced by the value of the experimental setting described in the paper—at least in the experiments conducted here, the claim that the model is distinctively effective depend on ruling out a large class of models arbitrarily. it would likely be valuable to find a concrete task/dataset/language combination that more closely aligns with the motivations for this work, and to evaluate whether the proposed method is genuinely the most effective practical option in that setting. Further, the reviewers raise a number of points involving baseline implementations, language families, and other issues, that collectively make me doubt that the paper is fully sound in its current form.",Paper Decision
Cz0iRDKaaz,HkxDheHFDr,LAVAE: Disentangling Location and Appearance,Reject,"This paper presents a VAE approach where the model learns representation while disentangling the location and appearance information. The reviewers found issues with the experimental evaluation of the paper, and have given many useful feedback. None of the reviewers were willing to change their score during the discussion period. with the current score, the paper does not make the cut for ICLR, and I recommend to reject this paper. ",Paper Decision
aN2RvgJtU9,Hygv3xrtDr,Sparse Skill Coding: Learning Behavioral Hierarchies with Sparse Codes,Reject,"The paper proposes an interesting idea of identifying repeated action sequences, or behavioral motifs, in the context of hierarchical reinforcement learning, using sparsity/compression.  While this is a fresh and useful idea, it appears that the paper requires more work, both in terms of presentation/clarity and in terms of stronger empirical results.
",Paper Decision
MX1NRIMyEa,HkgU3xBtDS,REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH,Reject,"This paper is a clear reject. The paper is very poorly written and contains zero citations. Also, the reviewers have a hard time understanding what the paper is about.",Paper Decision
Tn0JpKszF,SkgS2lBFPS,A Bilingual Generative Transformer for Semantic Sentence Embedding,Reject,"This paper presents a model for building sentence embeddings using a generative transformer model that encoders separately semantic aspects (that are common across languages)  and language-specific aspects. The authors evaluate their embeddings in a non-parametric way (i.e., on STS tasks by measuring cosine similarity) and find their method to outperform other sentence embeddings methods. The main concern that both reviewers (and myself) have about this work relates to its evaluation part. While the authors present a set of very interesting difficult evaluation and probing splits aiming at quantifying the linguistic behaviour of their model, it is unsatisfying the fact that the authors do not evaluate their model extensively in standard classification embedding benchmarks (e.g., as in GLUE). The authors comment: “[their model in producing embeddings] it isn’t as strong when using classification for final predictions. This indicates that the embeddings learned by our approach may be most useful when no downstream training is possible”. If this is true, why is it the case and isn’t it quite restrictive? I think this work is interesting with a nice analysis but the current empirical results are borderline  (yes, the model is better on STS, but this is quite limited of an idea compared to using these embeddings as features in a classification tasks). As such, I do not recommend this paper for acceptance but I do hope that authors will keep improving their method and will make it work in more general problems involving classification tasks.",Paper Decision
bqtUAvmNo8,ryxB2lBtvH,Learning to Coordinate Manipulation Skills via Skill Behavior Diversification,Accept (Poster),"This paper deals with multi-agent hierarchical reinforcement learning. A discrete set of pre-specified low-level skills are modulated by a conditioning vector and trained in a fashion reminiscent of Diversity Is All You Need, and then combined via a meta-policy which coordinates multiple agents in pursuit of a goal. The idea is that fine control over primitive skills is beneficial for achieving coordinated high-level behaviour.

The paper improved considerably in its completeness and in the addition of baselines, notably DIAYN without discrete, mutually exclusive skills. Reviewers agreed that the problem is interesting and the method, despite involving a degree of hand-crafting, showed promise for informing future directions. 

On the basis that this work addresses an interesting problem setting with a compelling set of experiments, I recommend acceptance.",Paper Decision
QGVQ7oSjL,SklEhlHtPr,DeepPCM: Predicting Protein-Ligand Binding using Unsupervised Learned Representations,Reject,"This paper uses unsupervised learning to create useful representations to improve the performance of models in predicting protein-ligand binding. After reviewers had time to consider each other's comments, there was consensus that the current work is too lacking in novelty on the modeling side to warrant publication in ICLR. Additionally, current experiments are lacking comparisons with important baselines. The work in its current form may be better suited for a domain journal. ",Paper Decision
fmCYK3XXCA,S1lVhxSYPH,Ternary MobileNets via Per-Layer Hybrid Filter Banks,Reject,"The paper presents a quantization method that generates per-layer hybrid filter banks consisting of full-precision and ternary weight filters for MobileNets. The paper is well-written. However, it is incremental. Moreover, empirical results are not convincing enough. Experiments are only performed on ImageNet. Comparison on more datasets and more model architectures should be performed.",Paper Decision
hODE9LtIBl,BJg73xHtvr,Constant Curvature Graph Convolutional Networks,Reject,"This paper proposes using non-Euclidean spaces for GCNs, leveraging the gyrovector space formalism. The model allows products of constant curvature, both positive and negative, generalizing hyperbolic embeddings. 

Reviewers got mixed impressions on this paper. Whereas some found its methodology compelling and its empirical evaluation satisfactory, it was generally perceived that this paper will greatly benefit from another round of reviewing. In particular, the authors should improve readability of the main text and provide a more thorough discussion on related recent (and concurrent) work. ",Paper Decision
DCJH7cc14u,HyxQ3gSKvr,Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding,Reject,"This paper proposes to use a mixture of Gaussians to variationally encode high-dimensional data through a latent space. The latent codes are constrained using the variational information bottleneck machinery. 

While the paper is well-motivated and relatively well-written, it contains minimal novel ideas. The consensus in reviews and lack of rebuttal make it clear that this paper should be significantly augmented with novel material before being published to ICLR.
",Paper Decision
_mmUJ7DqbB,Skx73lBFDS,Combining graph and sequence information to learn protein representations,Reject,"The paper presents a linear classifier based on a concatenation of two types of features for protein function prediction. The two features are constructed using methods from previous papers, based on peptide sequence and protein-protein interactions. 

All the reviewers agree that the problem is an important one, but the paper as it is presented does not provide any methodological advance, and weak empirical evidence of better protein function prediction. Therefore the paper would require a major revision before being suitable for ICLR.
",Paper Decision
Ykoc8KR8VD,HylznxrYDr,FINBERT:  FINANCIAL SENTIMENT ANALYSIS   WITH PRE-TRAINED LANGUAGE MODELS,Reject,"This paper presents FinBERT, a BERT-based model that is further trained on a financial corpus and evaluated on Financial PhraseBank and Financial QA. The authors show that FinBERT slightly outperforms baseline methods on both tasks.

The reviewers agree that the novelty is limited and this seems to be an application of BERT to financial dataset. There are many cases when it is okay to not present something entirely novel in terms of model as long as a paper still provides new insights on other things. Unfortunately, the new experiments in this paper are also not convincing. The improvements are very minor on small evaluation datasets, which makes the main contributions of the paper not enough for a venue such as ICLR.

The authors did not respond to any of the reviewers' concerns. I recommend rejecting this paper.",Paper Decision
GQzgiTL8Y,rylb3eBtwr,Robust Subspace Recovery Layer for Unsupervised Anomaly Detection,Accept (Poster),"Three reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.",Paper Decision
SJAwYKJaDC,HJx-3grYDB,Learning Nearly Decomposable Value Functions Via Communication Minimization,Accept (Poster),"The paper extends recent value function factorization methods for the case where limited agent communication is allowed. The work is interesting and well motivated. The reviewers brought up a number of mostly minor issues, such as unclear terms and missing implementation details. As far as I can see, the reviewers have addressed these issues successfully in their updated version. Hence, my recommendation is accept.",Paper Decision
42owlipPkB,H1x-3xSKDr,Batch Normalization is a Cause of Adversarial Vulnerability,Reject,"This article studies the effects of BN on robustness. The article presents a series of experiments on various datasets with noise, PGD adversarial attacks, and various corruption benchmarks, that show a drop in robustness when using BN. It is suggested that a main cause of vulnerability is the tiling angle of the decision boundary, which is illustrated in a toy example. 
The reviewers found the contribution interesting and that the effect will impact many DNNs. However, they the did not find the arguments for the tiling explanation convincing enough, and suggested more theory and experimental illustration of this explanation would be important. In the rebuttal the authors maintain that the main contribution is to link BN and adversarial vulnerability and consider their explanation reasonable. In the initial discussion the reviewers also mentioned that the experiments were not convincing enough and that the phenomenon could be an effect of gradient masking, and that more experiments with other attack strategies would be important to clarify this. In response, the revision included various experiments, including some with various initial learning schedules. The revision clarified some of these issues. However, the reviewers still found that the reason behind the effect requires more explanations. In summary, this article makes an important observation that is already generating a vivid discussion and will likely have an impact, but the reviewers were not convinced by the explanations provided for these observations. 
",Paper Decision
B2RWA3OW45,HkgxheBFDS,Undersensitivity in Neural Reading Comprehension,Reject,"The paper investigates the sensitivity of a QA model to perturbations in the input, by replacing content words, such as named entities and nouns, in questions to make the question not answerable by the document. Experimental analysis demonstrates while the original QA performance is not hurt, the models become significantly less vulnerable to such attacks. Reviewers all agree that the paper includes a thorough analysis, at the same time they all suggested extensions to the paper, such as comparison to earlier work, experimental results, which the authors made in the revision. However, reviewers also question the novelty of the approach, given data augmentation methods. Hence, I suggest rejecting the paper.",Paper Decision
U8NA3EqwE6,rJxe3xSYDS,Extreme Classification via Adversarial Softmax Approximation,Accept (Poster),"The paper proposes a fast training method for extreme classification problems where number of classes is very large. The method improves the negative sampling (method which uses uniform distribution to sample the negatives) by using an adversarial auxiliary model to sample negatives in a non-uniform manner. This has logarithmic computational cost and minimizes the variance in the gradients. There were some concerns about missing empirical comparisons with methods that use sampled-softmax approach for extreme classification. While these comparisons will certainly add further value to the paper, the improvement over widely used method of negative sampling and a formal analysis of improvement from hard negatives is a valuable contribution in itself that will be of interest to the community. Authors should include the experiments on small datasets to quantify the approximation gap due to negative sampling compared to full softmax, as promised.",Paper Decision
_0EKo7_1Cu,S1ly2grtvB,IS THE LABEL TRUSTFUL: TRAINING BETTER DEEP LEARNING MODEL VIA UNCERTAINTY MINING NET,Reject,"The paper presents an interesting idea but all reviewers pointed out problems with the writing (eg clarity of the motivation) and with the motivation of the experiments and link to the contest. The rebuttal helped, but it is clear that the paper requires more work before being acceptable to ICLR.",Paper Decision
0mdh_C8Rn,rkg1ngrFPr,Information Geometry of Orthogonal Initializations and Training,Accept (Poster),"I've gone over this paper carefully and think it's above the bar for ICLR.

The paper proves a relationship between the eigenvalues of the Fisher information matrix and the singular values of the network Jacobian. The main step is bounding the eigenvalues of the full Fisher matrix in terms of the eigenvalues and singular values of individual blocks using Gersgorin disks. The analysis seems correct and (to the best of my knowledge) novel, and relationships between the Jacobian and FIM are interesting insofar as they give different ways of looking at linearized approximations. The Gersgorin disk analysis seems like it may give loose bounds, but the analysis still matches up well with the experiments.

The paper is not quite as strong when it comes to relating the anslysis to optimization. The maximum eigenvalue of the FIM by itself doesn't tell us much about the difficulty of optimization. E.g., if the top FIM eigenvalue is increased, but the distance the weights need to travel is proportionately decreased (as seems plausible when the Jacobian scale is changed), then one could make just as fast progress with a smaller learning rate. So in this light, it's not too surprising that the analysis fails to capture the optimization dynamics once the learning rates are tuned. But despite this limitation, the contribution still seems worthwhile.

The writing can still be improved.

The claim about stability of the linearization explaining the training dynamics appears fairly speculative, and not closely related to the analysis and experiments. I recommend removing it, or at least removing it from the abstract.
",Paper Decision
5ic6a2vjtv,Hkg0olStDr,Multi-Step Decentralized Domain Adaptation,Reject,"This paper proposes a solution to the decentralized privacy preserving domain adaptation problem. In other words, how to adapt to a target domain without explicit data access to other existing domains. In this scenario the authors propose MDDA which consists of both a collaborator selection algorithm based on minimal Wasserstein distance as well as a technique for adapting through sharing discriminator gradients across domains. 

The reviewers has split scores for this work with two recommending weak accept and two recommending weak reject. However, both reviewers who recommended weak accept explicitly mentioned that their recommendation was borderline (an option not available for ICLR 2020). The main issues raised by the reviewers was lack of algorithmic novelty and lack of comparison to prior privacy preserving work. The authors agreed that their goal was not to introduce a new domain adaptation algorithm, but rather to propose a generic solution to extend existing algorithms to the case of privacy preserving and decentralized DA.  The authors also provided extensive revisions in response to the reviewers comments. Though the reviewers were convinced on some points (like privacy preserving arguments), there still remained key outstanding issues that were significant enough to cause the reviewers not to update their recommendations. 

Therefore, this paper is not recommended for acceptance in its current form. We encourage the authors to build off the revisions completed during the rebuttal phase and any outstanding comments from the reviewers. ",Paper Decision
CrvAxAeJz,Hyx0slrFvH,Mixed Precision DNNs: All you need is a good parametrization,Accept (Poster),"The reviewers uniformly vote to accept this paper. Please take comments into account when revising for the camera ready. I was also very impressed by the authors' responsiveness to reviewer comments, putting in additional work after submission.",Paper Decision
cQL0CNpLeP,SJxpsxrYPS,PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS,Accept (Spotlight),"This paper proposes a novel way to learn hierarchical disentangled latent representations by building on the previously published Variational Ladder AutoEncoder (VLAE) work. The proposed extension involves learning disentangled representations in a progressive manner, from the most abstract to the more detailed. While at first the reviewers expressed some concerns about the paper, in terms of its main focus (whether it was the disentanglement or the hierarchical aspect of the learnt representation), connections to past work, and experimental results, these concerns were fully alleviated during the discussion period. All of the reviewers now agree that this is a valuable contribution to the field and should be accepted to ICLR. Hence, I am happy to recommend this paper for acceptance as an oral.",Paper Decision
FHDVIbJfIr,r1g6ogrtDr,Co-Attentive Equivariant Neural Networks: Focusing Equivariance On Transformations Co-Occurring in Data,Accept (Poster),"The paper proposes an attention mechanism for equivariant neural networks towards the goal of attending to co-occurring features. It instantiates the approach with rotation and reflection transformations, and reports results on rotated MNIST and CIFAR-10. All reviewers have found the idea of using self-attention on top of equivariant feature maps technically novel and sound. There were some concerns about readability which the authors should try to address in the final version. ",Paper Decision
seWRbvb0vy,r1lnigSFDr,Improving the Gating Mechanism of Recurrent Neural Networks,Reject,"This submission proposes a new gating mechanism to improve gradient information propagation during back-propagation when training recurrent neural networks.

Strengths:
-The problem is interesting and important.
-The proposed method is novel.

Weaknesses:
-The justification and motivation of the UGI mechanism was not clear and/or convincing.
-The experimental validation is sometimes hard to interpret and the proposed improvements of the gating mechanism are not well-reflected in the quantitative results.
-The submission was hard to read and some images were initially illegible.

The authors improved several of the weaknesses but not to the desired level.

AC agrees with the majority recommendation to reject.",Paper Decision
M6zhdcZw4d,BklhsgSFvB,Learning to Transfer via Modelling Multi-level Task Dependency,Reject,"In this work, the authors address a multi-task learning setting and propose to enhance the estimation of task dependency with an attention mechanism capturing sample-dependant measure of task relatedness. All reviewers and AC agree that the current manuscript lacks clarity and convincing empirical evaluations that clearly show the benefits of the proposed approach w.r.t. state-of-the-art methods. Specifically, the reviewers raised several important concerns that were viewed by AC as critical issues:
(1) the empirical evaluations need to be significantly strengthened to show the benefits of the proposed methods over SOTA -- see R2’s request to empirically compare with the related recent work [Taskonomy, 2018] and R4’s request to compare with the work [End-to-end multi-task learning with attention, 2018]. R4 also suggested to include an ablation study to assess the benefits of the attention mechanism. Pleased to report that the authors addressed the ablation study in their rebuttal and confirmed that the proposed attention mechanism plays an important role in the performance of the proposed method. 
(2) All reviewers see an issue with the presentation clarity of the conceptual and technical contributions  -- see R4’s and R2’s detailed comments and questions regarding technical contributions; see R3’s and R4’s comments that the distinction between the general task dependency and the data-driven dependency is either not significant or is not clearly articulated; finding better examples to illustrate the difference (instead of reiterating the current ones) would strengthen the clarity and conceptual contributions.  
A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs more clarifications, empirical studies and polish to achieve the desired goal.

",Paper Decision
8l3nr14lO,rJx2slSKDS,Latent Variables on Spheres for Sampling and Inference,Reject,"This paper proposes to improve VAE/GAN by performing variational inference with a constraint that the latent variables lie on a sphere. The reviewers find some technical issues with the paper (R3's comment regarding theorem 3). They also found that the method is not motivated well, and the paper is not convincing. Based on this feedback, I recommend to reject the paper.",Paper Decision
-Dd2xEEF2I,ryloogSKDS,Deep Orientation Uncertainty Learning based on a Bingham Loss,Accept (Poster),"This paper considers the problem of reasoning about uncertain poses of objects in images. The reviewers agree that this is an interesting direction, and that the paper has interesting technical merit. ",Paper Decision
1T8Z0uBOH,B1xoserKPH,Analyzing Privacy Loss in Updates of Natural Language Models,Reject,"This paper report empirical implications of privacy ‘leaks’ in language models. Reviewers generally agree that the results look promising and interesting, but the paper isn’t fully developed yet. A few pointed out that framing the paper better to better indicate broader implications of the observed symptoms would greatly improve the paper. Another pointed out better placing this work in the context of other related work. Overall, this paper could use another cycle of polishing/enhancing the results. 
",Paper Decision
GVpbncQP92,HygPjlrYvB,Learning from Positive and Unlabeled Data  with Adversarial Training,Reject,"Thanks for your feedback to the reviewers, which helped us a lot to better understand your paper.
Through the discussion, the overall evaluation of this paper was significantly improved.
However, given the very high competition at ICLR2020, this submission is still below the bar unfortunately.
We hope that the discussion with the reviewers will help you improve your paper for potential future publication.",Paper Decision
7Ptr9lukkj,rygUoeHKvB,Deep exploration by novelty-pursuit with maximum state entropy,Reject,"There is insufficient support to recommend accepting this paper.  The reviewers unanimously recommended rejection, and did not change their recommendation after the author response period.  The technical depth of the paper was criticized, as was the experimental evaluation.  The review comments should help the authors strenghen this work.",Paper Decision
Mz8vg_BjQn,SJxUjlBtwB,Reconstructing continuous distributions of 3D protein structure from cryo-EM images,Accept (Spotlight),"The paper introduces a generative approach to reconstruct 3D images for cryo-electron microscopy (cryo-EM).

All reviewers really liked the paper, appreciate the challenging problem tackled and the proposed solution.

Acceptance is therefore recommended. ",Paper Decision
78WcDK50Bi,S1eSoeSYwr,Deep Evidential Uncertainty,Reject,"This paper presents a method for providing uncertainty for deep learning regressors through assigning a notion of evidence to the predictions.  This is done by putting priors on the parameters of the Gaussian outputs of the model and estimating these via an empirical Bayes-like optimization.  The reviewers in general found the methodology sensible although incremental in light of Sensoy et al. and Malinin & Gales but found the experiments thorough.  A comment on the paper pointed out that the approach was very similar to something presented in the thesis of Malinin (it seems unfair to expect the authors to have been aware of this, but the thesis should be cited and not just the paper which is a different contribution).  In discussion, one reviewer raised their score from weak reject to weak accept but the highest scoring reviewer explicitly was not willing to champion the paper and raise their score to accept.  Thus the recommendation here is to reject.  Taking the reviewer feedback into account, incorporating the proposed changes and adding more careful treatment of related work would make this a much stronger submission to a future conference.",Paper Decision
e6Mdhsu7aF,H1gBsgBYwH,Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint,Accept (Spotlight),This paper focuses on studying the double descent phenomenon in a one layer neural network training in an asymptotic regime where various dimensions go to infinity together with fixed ratios. The authors provide precise asymptotic characterization of the risk and use it to study various phenomena. In particular they characterize the role of various scales of the initialization and their effects. The reviewers all agree that this is an interesting paper with nice contributions. I concur with this assessment.  I think this is a solid paper with very precise and concise theory. I recommend acceptance.,Paper Decision
U4wu2YQF4P,r1lEjlHKPH,Better Knowledge Retention through Metric Learning,Reject,"Catastrophic forgetting in neural networks is a real problem, and this paper suggests a mechanism for avoiding this using a k-nearest neighbor mechanism in the final layer. The reason is that the layers below the last layer should not change significantly when very different data is introduced. 

While the idea is interesting none of the reviewers is entirely convinced about the execution and empirical tests, which had partially inconclusive. The reviewers had a number of questions, which were only partially satisfactorily answered. While some of the reviewers had less familiarity with the specific research topic, the seemingly most knowledgeable reviewer does not think the paper is ready for publication.

On balance, I think the paper cannot be accepted in its current state. The idea is interesting, but needs more work.",Paper Decision
15XxZlBJ_,BJe4oxHYPB,Winning the Lottery with Continuous Sparsification,Reject,"This paper proposes a new algorithm called Continuous Sparsification (CS) to search for winning tickets (in the context of the Lottery Ticket Hypothesis from Frankle & Carbin (2019)), as an alternative to the Iterative Magnitude Pruning (IMP) algorithm proposed therein. CS continuously removes parameters from a network during training, and learns the sub-network's structure with gradient-based methods instead of relying on pruning strategies. The papers shows empirically that CS finds lottery tickets that outperforms the ones learned by ITS with up to 5 times faster search, when measured in number of training epochs.

While this paper presents a novel contribution of pruning and of finding winning lottery tickets and is very well written, there are some concerns raised by the reviewers regarding the current evaluation. The paper presents no concrete data on the comparative costs of performing CS and IMP even though the core claim is that CS is more efficient. The paper does not disclose enough detail to compute these costs, and it seems like CS is more expensive than IMP for standard workflows. Moreover, the current presentation of the data through ""pareto curves"" is misleadingly favorable to CS. The reviewers suggest including more experiments on ImageNet and  a more thorough evaluation as a pruning technique beyond the lottery ticket hypothesis. We recommend the authors to address the detailed reviewers' comments in an eventual ressubmission.
",Paper Decision
V5p4pQNXef,rylmoxrFDH,Critical initialisation in continuous approximations of binary neural networks,Accept (Poster),"The authors study neural networks with binary weights or activations, and the so-called ""differentiable surrogates"" used to train them.
They present an analysis that unifies previously proposed surrogates and they study critical initialization of weights to facilitate trainability.

The reviewers agree that the main topic of the paper is important (in particular initialization heuristics of neural networks), however they found the presentation of the content lacking in clarity as well as in clearly emphasizing the main contributions. 
The authors imporved the readability of the manuscript in the rebuttal.

This paper seems to be at acceptance threshold and 2 of 3 reviewers indicated low confidence.
Not being familiar with this line of work, I recommend acceptance following the average review score.",Paper Decision
KXxJ4hX_lo,H1lMogrKDH,LEARNING DIFFICULT PERCEPTUAL TASKS WITH HODGKIN-HUXLEY NETWORKS,Reject,"The paper studies non-spiking Hudgkin-Huxley models and shows that under few simplifying assumptions the model can be trained using conventional backpropagation to yield accuracies almost comparable to state-of-the-art neural networks. Overall, the reviewers found the paper well-written, and the idea somewhat interesting, but criticized the experimental evaluation and potential low impact and interest to the community.  While the method itself is sound, the overall assessment of the paper is somewhat below what's expected from papers accepted to ICLR, and I’m thus recommending rejection.",Paper Decision
LO6qrJtyCZ,SkxMjxHYPS,Filter redistribution templates for iteration-lessconvolutional model reduction,Reject,"This paper examines how different distributions of the layer-wise number of CNN filters, as partitioned into a set of fixed templates, impacts the performance of various baseline deep architectures.  Testing is conducting from the viewpoint of balancing accuracy with various resource metrics such as number of parameters, memory footprint, etc.

In the end, reviewer scores were partitioned as two accepts and two rejects.  However, the actual comments indicate that both nominal accept reviewers expressed borderline opinions regarding this work (e.g., one preferred a score of 4 or 5 if available, while the other explicitly stated that the paper was borderline acceptance-worthy).  Consequently in aggregate there was no strong support for acceptance and non-dismissable sentiment towards rejection.

For example, consistent with reviewer comments, a primary concern with this paper is that the novelty and technical contribution is rather limited, and hence, to warrant acceptance the empirical component should be especially compelling.  However, all the experiments are limited to cifar10/cifar100 data, with the exception of a couple extra tests on tiny ImageNet added after the rebuttal.  But these latter experiments are not so convincing since the base architecture has the best accuracy on VGG, and only on a single MobileNet test do we actually see clear-cut improvement.  Moreover, these new results appear to be based on just a single trial per data set (this important detail is unclear), and judging from Figure 2 of the revision, MobileNet results on cifar data can have very high variance blurring the distinction between methods.  It is therefore hard to draw firm conclusions at this point, and these two additional tiny ImageNet tests notwithstanding, we don't really know how to differentiate phenomena that are intrinsic to cifar data from other potentially relevant factors.

Overall then, my view is that far more testing with different data types is warranted to strengthen the conclusions of this paper and compensate for the modest technical contribution.  Note also that training with all of these different filter templates is likely no less computationally expensive than some state-of-the-art pruning or related compression methods, and therefore it would be worth comparing head-to-head with such approaches.  This is especially true given that in many scenarios, test-time computational resources are more critical than marginal differences in training time, etc.",Paper Decision
AbMzpa7823,HkxZigSYwS,Universal Safeguarded Learned Convex Optimization with Guaranteed Convergence,Reject,"This paper gave a general L2O convergence theory called Learned Safeguarded KM (LSKM).  The reviewers found flaws both in theory and in experiments.  While all the reviewers have read the authors' rebuttal and gave detailed replies, they all agree to reject this paper.  I agree also.",Paper Decision
j4U28uFNXr,Bye-sxHFwB,A Gradient-Based Approach to Neural Networks Structure Learning,Reject,"This paper proposes a neural network architecture that represents each neuron with input and output embeddings. Experiments on CIFAR show that the proposed method outperforms baseline models with a fully connected layer.

I like the main idea of the paper. However, I agree with R1 and R2 that experiments presented in the paper are not enough to convince readers of the benefit of the proposed method. In particular, I would like to see a more comprehensive set of results across a suite of datasets. It would be even better, although not necessary, if the authors apply this method on top of different base architectures in multiple domains. At the very least, the authors should run an experiment to compare the proposed approach with a feed forward network on a simple/toy classification dataset. I understand that these experiments require a lot of computational resources. The authors do not need to reach SotA, but they do need to provide more empirical evidence that the method is useful in practice.

I also would like to see more discussions with regards to the computational cost of the proposed method. How much slower/faster is training/inference compared to a fully connected network?

The writing of the paper can also be improved. There are many a few typos throughout the paper, even in the abstract. 

I recommend rejecting this paper for ICLR, but would encourage the authors to polish it and run a few more suggested experiments to strengthen the paper.",Paper Decision
tdYDQN9mJ,ByeWogStDS,Sub-policy Adaptation for Hierarchical Reinforcement Learning,Accept (Poster),"This paper considers hierarchical reinforcement learning, and specifically the case where the learning and use of lower-level skills should not be decoupled. To this end the paper proposes Hierarchical Proximal Policy Optimization (HiPPO) to jointly learn the different layers of the hierarchy. This is compared against other hierarchical RL schemes on several Mujoco domains.

The reviewers raised three main issues with this paper. The first concerns an excluded baseline, which was included in the rebuttal. The other issues involve the motivation for the paper (in that there exist other methods that try and learn different levels of hierarchy together) and justification for some design choices. These were addressed to some extent in the rebuttal, but I believe this to still be an interesting contribution to the literature, and should be accepted.
",Paper Decision
aO_H8El6XE,rkeeoeHYvr,AdvCodec: Towards A Unified Framework for Adversarial Text Generation,Reject,"This paper proposes a method for generating text examples that are adversarial against a known text model, based on modifying the internal representations of a tree-structured autoencoder.

I side with the two more confident reviewers, and argue that this paper doesn't offer sufficient evidence that this method is useful in the proposed setting. I'm particularly swayed by R1, who raises some fairly basic concerns about the value of adversarial example work of this kind, where the generated examples look unnatural in most cases, and where label preservation is not guaranteed. I'm also concerned by the fact, which came up repeatedly in the reviews, that the authors claimed that using a tree-structured decoder encourages the model to generate grammatical sentences—I see no reason why this should be the case in the setting described here, and the paper doesn't seem to offer evidence to back this up.",Paper Decision
Pckrzrm8G,ByxloeHFPS,PROVABLY BENEFITS OF DEEP HIERARCHICAL RL,Reject,"This paper pursues an ambitious goal to provide a theoretical analysis HRL in terms of regret bounds. However, the exposition of the ideas has severe clarity issues and the assumptions about HMDPs used are overly simplistic to have an impact in RL research.
Finally, there is agreement between the reviewers and AC that the novelty of the proposed ideas is a weak factor and that the paper needs substantial revision.",Paper Decision
5944Z8zbo4,ByxJjlHKwr,Learning Latent State Spaces for Planning through Reward Prediction,Reject,"The authors propose a model-based RL algorithm, consisting of learning a                                                           
deterministic multi-step reward prediction model and a vanilla CEM-based MPC                                                       
actor.                                                                                                                             
In contrast to prior work, the model does not attempt to learn from observations                                                   
nor is a value function learned.                                                                                                   
The approach is tested on task from the mujoco control suit.                                                                       
                                                                                                                                   
The paper is below acceptance threshold.                                                                                           
It is a variation on previous work form Hafner et al.                                                                              
Furthermore, I think the approach is fundamentally limited: All the learning                                                       
derives from the immediate, dense reward signal, whereas the main challenges in RL                                                 
are found in sparse reward settings that require planning over long horizons, where value                                          
functions or similar methods to assign credit over long time windows are                                                           
absolutely essential.",Paper Decision
6Xu15W5vMR,BkxA5lBFvH,Hope For The Best But Prepare For The Worst: Cautious Adaptation In RL Agents,Reject,"The work this paper presents is interesting, but it is not quite ready yet for publication at ICLR. Specifically, the motivation of particular choices could be better, such as summing over quantiles, as indicated by Reviewer 1. The inherent trade-off between safety and speed of adaptation and how this relates to the proposed method could also use a clearer exposition.",Paper Decision
6hMYK3CMlf,rJl0ceBtDH,Semi-Supervised Boosting via Self Labelling,Reject,"The paper presents a new semi-supervised boosting approach. 

As reviewers pointed out and AC acknowledge, the paper is not ready to publish in various aspects: (a) limited novelty/contribution, (b) reproducibility issue and (c) arguable assumptions.

Hence, I recommend rejection.",Paper Decision
arrQRX9dsx,BygacxrFwS,Fractional Graph Convolutional Networks (FGCN) for Semi-Supervised Learning,Reject,"This paper proposes a fractional graph convolutional networks for semi-supervised learning, using a classification function repurposed from previous work, as well as parallelization and weighted combinations of pooling function. This leads to good results on several tasks.
Reviewers had concerns about the part played by each piece, the lack of comparison to recent related work, and asked for better explanation of the rationale of the method and more experimental details. Authors provided explanations and details, and a more thorough set of comparison to other work, showing better performance in some but not all cases.
However, concerns that the proposed innovations are too incremental remain.
Therefore, we cannot recommend acceptance.",Paper Decision
KO_Iv7zfaH,B1lTqgSFDH,Antifragile and Robust Heteroscedastic Bayesian Optimisation,Reject,"The reviewers initially gave scores of 1,1,3 citing primarily weak empirical results and a lack of theoretical justification.  The experiments are presented on synthetic examples, which is a great start but the reviewers found that this doesn't give strong enough evidence that the methods developed in the paper would work well in practice.  The authors did not submit an author response to the reviewers and as such the scores did not change during discussion.  This paper would be significantly strengthened with the addition of experiments on actual problems e.g. related to drug discovery which is the motivation in the paper.",Paper Decision
CUeWaYfwR,rkx35lHKwB,Generalizing Reinforcement Learning to Unseen Actions,Reject,"This paper proposes a method for reinforcement learning with unseen actions.  More precisely, the problem setting considers a partitioned action space.  The actions available during training (known actions) are a subset of all the actions available during evaluation (known and unknown actions).  The method can choose unknown actions during evaluation through an embedding space over the actions, which defines a distance between actions. The action embedding is trained by a hierarchical variational autoencoder. The proposed method and algorithmic variants are applied to several domains in the experiments section.

The reviewers discussed both strengths and weaknesses of the paper.  The strengths described by the reviewers include the use of the hierarchical VAE and the explanatory videos.  The primary weakness is the absence of sufficient detail when describing the solution.  The solution description is not sufficiently clear to understand the details of the regularization metrics.  The details of regularization are essential when some actions are never seen in training.  The reviewers also mentioned that the experiment analysis would benefit from more care.

This paper is not ready for publication, as the solution methods and experiments are not presented with sufficient detail.",Paper Decision
yM4Y_BPtYj,HkxnclHKDr,Provable Representation Learning for Imitation Learning via Bi-level Optimization,Reject,"This paper proposes a methodology for learning a representation given multiple demonstrations, by optimizing the representation as well as the learned policy parameters. The paper includes some theoretical results showing that this is a sensible thing to do, and an empirical evaluation.

Post-discussion, the reviewers (and me!) agreed that this is an interesting approach that has a lot of promise. But there was still concern about he empirical evaluation and the writing. Hence I am recommending rejection.",Paper Decision
9zYwbCAcfz,HkxjqxBYDB,Episodic Reinforcement Learning with Associative Memory,Accept (Poster),"The submission tackles the problem of data efficiency in RL by building a graph on top of the replay memory and propagate values based on this representation of states and transitions. The method is evaluated on Atari games and is shown to outperform other episodic RL methods.

The reviews were mixed initially but have been brought up by the revisions to the paper and the authors' rebuttal. In particular, there was a concern about theoretical support and the authors added a proof of convergence. They have also added additional experiments and explanations. Given the positive reviews and discussion, the recommendation is to accept this paper.",Paper Decision
sNKUHwtiSw,r1xo9grKPr,Flexible and Efficient Long-Range Planning Through Curious Exploration,Reject,"The authors consider planning problems with sparse rewards.                                                                        
They propose an algorithm that performs planning based on an auxiliary reward                                                      
given by a curiosity score.                                                                                                        
They test they approach on a range of tasks in simulated robotics environments                                                     
and compare to model-free baselines.                                                                                               
                                                                                                                                   
The reviewers mainly criticize the lack of competitive baselines; it comes as now                                                  
surprise that the baselines presented in the paper do not perform well, as they                                                    
make use of strictly less information of the problem.                                                                              
The authors were very active in the rebuttal period, however eventually did not                                                    
fully manage to address the points raised by the reviewers.                                                                        
                                                                                                                                   
Although the paper proposes an interesting approach, I think this paper is below                                                   
acceptance threshold.                                                                                                              
The experimental results lack baselines,                                                                                           
Furthermore, critical details of the algorithm are missing / hard to find.",Paper Decision
n2UryrKFWZ,BJxiqxSYPB,Learning to Prove Theorems by Learning to Generate Theorems,Reject,"This paper proposes to augment training data for theorem provers by learning a deep neural generator that generates data to train a prover, resulting in an improvement over the Holophrasm baseline prover. The results were restricted to one particular mathematical formalism -- MetaMath, a limitation raised one by reviewer. 

All reviewers agree that it's an interesting method for addressing an important problem. However there were some concerns about the strength of the experimental results from R4 and R1. R4 in particular wanted to see results on more datasets, an assessment with which I agree. Although the authors argued vigorously against using other datasets, I am not convinced. For instance, they claim that other datasets do not afford the opportunity to generate new theorems, or the human proofs provided cannot be understood by an automatic prover. In their words, 

""The idea of theorem generation can be applied to other systems beyond Metamath, but realizing it on another system is highly nontrivial. It can even involve new research challenges. In particular, due to large differences in logic foundations, grammar, inference rules, and benchmarking environments, the generation process, which is a key component of our approach, would be almost completely different for a new system. And the entire pipeline essentially needs to be re-designed and re-coded from scratch for a new formal system, which can require an unreasonable amount of engineering."" 

It sounds like they've essentially tailored their approach for this one dataset, which limits the generality of their approach, a limitation that was not discussed in the paper. 

There is also only one baseline considered, which renders their experimental findings rather weak. For these reasons, I think this work is not quite ready for publication at ICLR 2020, although future versions with stronger baselines and experiments could be quite impactful.




",Paper Decision
pWo2QQ3uy-,BJe55gBtvH,Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem,Accept (Spotlight),"The article is concerned with depth width tradeoffs in the representation of functions with neural networks. The article presents connections between expressivity of neural networks and dynamical systems, and obtains lower bounds on the width to represent periodic functions as a function of the depth. These are relevant advances and new perspectives for the theoretical study of neural networks. The reviewers were very positive about this article. The authors' responses also addressed comments from the initial reviews. ",Paper Decision
Kb089OOeb,HyeKcgHFvS,Gradient-based training of Gaussian Mixture Models in High-Dimensional Spaces,Reject,"The paper presents an SGD-based learning of a Gaussian mixture model, designed to match a data streaming setting.

The reviews state that the paper contains some quite good points, such as
* the simplicity and scalability of the method, and its robustness w.r.t. the initialization of the approach;
* the SOM-like approach used to avoid degenerated solutions;

Among the weaknesses are
* an insufficient discussion wrt the state of the art, e.g. for online EM;
* the description of the approach seems yet not mature (e.g., the constraint enforcement boils down to considering that the $\pi_k$ are obtained using softmax; the discussion about the diagonal covariance matrix vs the use of local principal directions is not crystal clear);
* the fact that experiments need be strengthened.

I thus encourage the authors to rewrite and polish the paper, simplifying the description of the approach and better positioning it w.r.t. the state of the art (in particular, mentioning the data streaming motivation from the start). Also, more evidence, and a more thorough analysis thereof, must be provided to back up the approach and understand its limitations.",Paper Decision
hTPnoV1iuk,S1gtclSFvr,Neural Phrase-to-Phrase Machine Translation,Reject,"This paper describes how they extend a previous phrase-based neural machine translation model to incorporate external dictionaries. The reviewers mention the small scale of the experiments, and the lack of clarity in the writing, and missing discussion on computational complexity. Even though the method seems to have the potential to impact the field, the paper is currently not strong enough for publication. The authors have not engaged in the discussion at all. ",Paper Decision
ZkfLnoKAp,H1MOqeHYvB,At Your Fingertips: Automatic Piano Fingering Detection,Reject,The paper shows an automatic piano fingering algorithm. The idea is good. But the reviewers find that the novelty is limited and it is an incremental work. All the reivewers agree to reject.,Paper Decision
SK9ARl-vUF,S1e_9xrFvS,Energy-based models for atomic-resolution protein conformations,Accept (Spotlight),"The paper proposes a data-driven approach to learning atomic-resolution energy functions. Experiment results show that the proposed energy function is similar to the state-of-art method (Rosetta) based on physical principles and engineered features. 

The paper addresses an interesting and challenging problem. The results are very promising. It is a good showcase of how ML can be applied to solve an important application problem. 

For the final version, we suggest that the authors can tune down some claims in the paper to fairly reflect the contribution of the work. ",Paper Decision
aVDw9i8E6I,BkluqlSFDS,Federated Learning with Matched Averaging,Accept (Talk),"The authors presented a Federate Learning algorithm which constructs the global model layer-wise by matching and averaging hidden representations. They empirically demonstrate their method outperforms existing federated learning algorithms

This paper has received largely positive reviews. Unfortunately one reviewer wrote a very short review but was generally appreciative of the work. Fortunately, R1 wrote a detailed review with very specific questions and suggestions. The authors have addresses most of the concerns of the reviewers and I have no hesitation in recommending that this paper should be accepted. I request the authors to incorporate all suggestions made by the reviewers. ",Paper Decision
QwmqusbVHJ,BylD9eSYPS,Clustered Reinforcement Learning,Reject,"The paper discusses a simple but apparently effective clustering technique to improve exploration. There are no theoretical results, hence the reader relies fully on the experiments to evaluate the method. Unfortunately, an in-dept analysis of the results is missing making it hard to properly evaluate the strength and weaknesses. Furthermore, the authors have not provided any rebuttal to the reviewers' concerns.",Paper Decision
yZXKfKU2Bg,BJxI5gHKDr,Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning,Accept (Poster),"The paper points out pitfalls of existing metrics for in-domain uncertainty quantification, and also studies different strategies for ensembling techniques.

The authors also satisfactorily addressed the reviewers' questions during the rebuttal phase. In the end, all the reviewers agreed that this is a valuable contribution and paper deserves to be accepted. 

Nice work!",Paper Decision
_ff90p8S7,rJxHcgStwr,Handwritten Amharic Character Recognition System Using Convolutional Neural Networks,Reject,"The submission proposes to use CNN for Amharic Character Recognition.   The authors used a straight forward application of CNNs to go from images of Amharic characters to the corresponding character.  There was no innovation on the CNN side. The main contribution of the work is the Amharic handwriting dataset and the experiments that were performed.

The reviewers indicated the following concerns:
1. There was no innovation to the method (a straight forward CNN is used) and is likely not of interest to the ICLR community
2. The dataset was divided into train/val split and does not contain a held-out test set.  Thus it was impossible to determine the generalization of the model.
3. The paper is poorly written with the initial version having major formatting issues and missing references. The revised version has fixed some of the formatting issues.  The paper still need to having more paragraph breaks to help with the readability of the paper (for instance, the introduction is still one big long paragraph).  The terminology and writing can also be improved.  For instance, in section 2.3, the authors write that ""500 dataset for each character were collected"".  It would be clearer to say that ""500 images for each character were collected"".

The submission received low reviews overall (3 rejects), which was unchanged after the rebuttal.  Due to the general consensus, there was limited discussion.  There were also major formatting issues with the initial submission.  The revised version was improved to have proper inclusion of Amharic characters in the text, missing figures, and references.  However, even after the revision, the paper still had the above issues with methodology (as noted by R4) and is likely of low interest for the ICLR community.  

The Amharic handwriting data and experiments using a CNN can be of interest to the different community and I would recommend the authors work on improving their paper based on reviewer comments and submit to different venue (such as a workshop focused on character recognition for different languages).
",Paper Decision
K4WTQ9-MPr,r1xH5xHYwH,Effects of Linguistic Labels on Learned Visual Representations in Convolutional Neural Networks: Labels matter!,Reject,"This paper explores training CNNs with labels of differing granularity, and finds that the types of information learned by the method depends intimately on the structure of the labels provided.

Thought the reviewers found value in the paper, they felt there were some issues with clarity, and didn't think the analyses were as thorough as they could be. I thank the authors for making changes to their paper in light of the reviews, and hope that they feel their paper is stronger because of the review process.",Paper Decision
AzZUG1xaI,B1eB5xSFvr,DiffTaichi: Differentiable Programming for Physical Simulation,Accept (Poster),"The paper provides a language for optimizing through physical simulations. The reviewers had a number of concerns related to paper organization and insufficient comparisons to related work (jax). During the discussion phase, the authors significantly updated their paper and ran additional experiments, leading to a much stronger paper.",Paper Decision
ucM05O_9nW,HJg4qxSKPB,Implicit Rugosity Regularization via Data Augmentation,Reject,"This paper aims to study the effect of data augmentation of generalization performance. The authors put forth a measure of rugosity or ""roughness"" based on the tangent Hessian of the function reminiscent of a classic result by Donoho et. al. The authors show that this measure changes in tandem with how much data augmentation helps. The reviewers and I concur that the rugosity measure is interesting. However, as the reviewer mention the main draw back of this paper is that this measure of rugosity when made explicit does not improve generalization. This is the main draw back of the paper. I agree with the authors that this measure is interesting in itself. However, I think in its current form the paper is not ready for prime time and recommend rejection. That said, I believe this paper has a lot of potential and recommend the authors to rewrite and carry out more careful experiments for a future submission.",Paper Decision
KT8aePozXB,Syx79eBKwr,A Mutual Information Maximization Perspective of Language Representation Learning,Accept (Spotlight),"This paper explores several embedding models (Skip-gram, BERT, XLNet) and describes a framework for comparing, and in the end, unifying them.  The framework is such that it actually suggests new ways of creating embeddings, and draws connections to methodology from computer vision.

One of the reviewers had several questions about the derivations in your paper and was worried about the paper's clarity.  But all of the reviewers appreciated the contributions of the paper, which joins multiple seemingly disparite models under into one theoretical framework.

The reviewers were positive about the paper, and in particular were happy to see the active response of authors to their questions and willingness to update the paper with their suggested improvements.",Paper Decision
QBNt8I0zAs,B1g79grKPr,Goal-Conditioned Video Prediction,Reject,"The paper addresses a video generation setting where both initial and goal state are provided as a basis for long-term prediction. The authors propose two types of models, sequential and hierarchical, and obtain interesting insights into the performance of these two models. Reviewers raised concerns about evaluation metrics, empirical comparisons, and the relationship of the proposed model to prior work.

While many of the initial concerns have been addressed by the authors, reviewers remain concerned about two issues in particular. First, the proposed model is similar to previous approaches with sequential latent variable models, and it is unclear how such existing models would compare if applied in this setting. Second, there are remaining concerns on whether the model may learn degenerate solutions. I quote from the discussion here, as I am not sure this will be visible to authors [about Figure 12]: ""now the two examples with two samples they show have the same door in the middle frame which makes me doubt the method learn[s] anything meaningful in terms of the agent walking through the door but just go to the middle of the screen every time.""",Paper Decision
BL0tboSR-F,HyeG9lHYwH,Compression without Quantization,Reject,"The paper proposes a method for lossy image compression. Based on the encoder-decoder framework, it replaces the discrete codes by continuous ones, so that the learning can be performed in an end-to-end way. The idea is interesting, but the motivation is based on a quantization ""problem"" that the authors show no evidence the competing method is actually suffering from. It is thus unclear how much does quantization in existing methods impact performance, and how much will fixing this benefit the overall system. Also, the authors may add some discussions on whether the proposed sampling of z_{c^\star} is indeed also a form of quantization.

Experimental results are not convincing. The proposed method is only compared with one method. While it works only slightly worse at low bit-rate region, the gap becomes larger in higher bit rate regions. Another major concern is that the encoding time is significantly longer. Ablation study is also needed. Finally, the writing can be improved.",Paper Decision
m49Ij4iVU,HyleclHKvS,A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed,Reject,"Two reviewers as well as the AC are confused by the paper—perhaps because the readability of it should be improved?  It is clear that the page limitation of conferences are problematic, with 7 pages of appendix (not part of the review) the authors may consider another venue to publish.  In its current form, the usefulness for the ICLR community seems limited.",Paper Decision
MKn75xxkem,BJg15lrKvS,Towards Understanding the Spectral Bias of Deep Learning,Reject,"The authors propose to understand spectral bias during training of neural networks from the perspective of the NTK. While reviewers appreciated aspects of the work, the general consensus was that the current version is not ready for publication; some concerns stem from whether the the NTK model and finite neural networks are sufficiently similar that we should be able to gain real practical insights into the behaviour of finite models. This is partly an empirical question, and stronger experiments are required to have a better sense of the answer. Nonetheless, the authors are encouraged to persist with this work, taking into account reviewer comments in future revisions.

",Paper Decision
5CmnutmWy,rJxycxHKDS,Domain Adaptive Multibranch Networks,Accept (Poster),"Although some criticism remains for experiments, I suggest to accept this paper.",Paper Decision
5WBMTct6Ho,r1eyceSYPr,Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models,Accept (Spotlight),"Main content:

Blind review #1 summarizes it well:

The paper proposes an algorithmic improvement that significantly simplifies training of energy-based models, such as the Restricted Boltzmann Machine. The key issue in training such models is computing the gradient of the log partition function, which can be framed as computing the expected value of f(x) = dE(x; theta) / d theta over the model distribution p(x). The canonical algorithm for this problem is Contrastive Divergence which approximates x ~ p(x) with k steps of Gibbs sampling, resulting in biased gradients. In this paper, the authors apply the recently introduced unbiased MCMC framework of Jacob et al. to completely remove the bias. The key idea is to (1) rewrite the expectation as a limit of a telescopic sum: E f(x_0) + \sum_t E f(x_t) - E f(x_{t-1}); (2) run two coupled MCMC chains, one for the “positive” part of the telescopic sum and one for the “negative” part until they converge. After convergence, all remaining terms of the sum are zero and we can stop iterating. However, the number of time steps until convergence is now random.

Other contributions of the paper are:
1. Proof that Bernoulli RBMs and other models satisfying certain conditions have finite expected number of steps and finite variance of the unbiased gradient estimator.
2. A shared random variables method for the coupled Gibbs chains that should result in faster convergence of the chains.
3. Verification of the proposed method on two synthetic datasets and a subset of MNIST, demonstrating more stable training compared to contrastive divergence and persistent contrastive divergence.

--

Discussion:

The main objection in reviews was to have meaningful empirical validation of the strong theoretical aspect of the paper, which the authors did during the rebuttal period to the satisfaction of reviewers.

--

Recommendation and justification:

As review #1 said, ""I am very excited about this paper and strongly support its acceptance, since the proposed method should revitalize research in energy-based models.""",Paper Decision
IWMijNANQw,HJlRFlHFPS,Unsupervised Distillation of Syntactic Information from Contextualized Word Representations,Reject,"This paper aims to disentangle semantics and syntax inside of popular contextualized word embedding models. They use the model to generate sentences which are structurally similar but semantically different. 

This paper generated a lot of discussion. The reviewers do like the method for generating structurally similar sentences, and the triplet loss.  They felt the evaluation methods were clever.  However, one reviewer raised several issues.  First, they thought the idea of syntax had not been well defined. They also thought the evaluation did not support the claims.  The reviewer also argued very hard for the need to compare performance to SOTA models.  The authors argued that beating SOTA is not the goal of their work, rather it is to understand what SOTA models are doing.  The reviewers also argue that nearest neighbors is not a good method for evaluating the syntactic information in the representations.  

I hope all of the comments of the reviewers will help improve the paper as it is revised for a future submission.",Paper Decision
IxJJVr5g9W,H1eRYxHYPB,Optimal Unsupervised Domain Translation,Reject,"The paper examines the problem of unsupervised domain translation. It poses the problem in a rigorous way for the first time and examines the shortcomings of existing CycleGAN-based methods. Then the authors propose to consider the problem through the lens of Optimal Transport theory and formulate a practical algorithm.

The reviewers agree that the paper addresses an important problem, brings clarity to existing methods, and proposes an interesting approach / algorithm, and is well-written. However, there was a shared concern about whether the new approach just moves the complexity elsewhere (into the design of the cost function). The authors claim to have addressed in the rebuttal by adding an extra experiment, but the reviewers remained unconvinced.

Based on the reviewer discussion, I recommend rejection at this time, but look forward to seeing the revised paper at a future venue.",Paper Decision
z0WprZhpD,rkg6FgrtPB,Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity,Reject,"Unfortunately the paper is confusingly written, and there is only agreement by all reviewers on the rejection of the paper.  Indeed, if all reviewers and the area chair do not interpret the paper well, the authors' best response would be to rewrite the papers rather than disagree with all reviewers.

In the area chair's opinion, the current form the paper does not merit publication.  The authors are advised to address the reviewers' concerns, rework the paper, and submit to a conference again.",Paper Decision
sTAzO-WkbD,BkghKgStPH,Continual Learning using the SHDL Framework with Skewed Replay Distributions,Reject,"The paper adapts a previously proposed modular deep network architecture (SHDL) for supervised learning in a continual learning setting.  One problem in this setting is catastrophic forgetting.  The proposed solution replays a small fraction of the data from old tasks to avoid forgetting, on top of a modular architecture that facilitates fast transfer when new tasks are added.  The method is developed for image inputs and evaluated experimentally on CIFAR-100.

The reviews were in agreement that this paper is not ready for publication.  All the reviews had concerns about the lack of explanation of the proposed solution and the experimental methods.  The reviewers were concerned about the choice of metrics not being comparable or justified: Reviewer4 wanted an apples-to-apples comparison, Reviewer1 suggested the paper follow the evaluation paradigm used in earlier papers, and Reviewer2 described the absence of an explained baseline value.  Two reviewers (Reviewer4 and Reviewer2) described the lack of details on the parameters, architecture, and training regime used for the experiments.  The paper did not not justify which aspects of the modular system contributed to the observed performance (Reviewer4 and Reviewer1).   Several additional concerns were also raised. 

The authors did not respond to any of the concerns raised by the reviewers. 
",Paper Decision
Ilh9vDgaeg,SJxstlHFPH,Differentiable Reasoning over a Virtual Knowledge Base,Accept (Talk),"This paper proposes a novel architecture for question-answering, which is trained in an end-to-end fashion.

The reviewers were unanimous in their vote to accept. Authors are encouraged to revise addressing reviewer comments.",Paper Decision
ULKoHfjLfb,S1xitgHtvS,Making Sense of Reinforcement Learning and Probabilistic Inference,Accept (Spotlight),"The paper explores in more detail the ""RL as inference"" viewpoint and highlights some issues with this approach, as well as ways to address these issues. The new version of the paper has effectively addressed some of the reviewers' initial concerns, resulting in an overall well-written paper with interesting insights.",Paper Decision
zHl_Nylh-T,BJlqYlrtPB,Negative Sampling in Variational Autoencoders,Reject,"This paper proposes to improve VAEs' modeling of out-of-distribution examples, by pushing the latent representations of negative examples away from the prior.  The general idea seems interesting, at least to some of the reviewers and to me.  However, the paper seems premature, even after revision, as it leaves unclear some of the justification and analysis of the approach, especially in the fully unsupervised case.  I think that with some more work it could be a very compelling contribution to a future conference.",Paper Decision
wVRpqamn_,HygqFlBtPS,Improved Training of Certifiably Robust Models,Reject,"The authors develop regularization schemes that aim to promote tightness of convex relaxations used to provides certificates of robustness to adversarial examples in neural networks.

While the paper make some interesting contributions, the reviewers had several concerns on the paper:
1) The aim of the authors' work and the distinction with closely related prior work is not clear from the presentation. In particular, the relationship to the ReLU stability regularizer (Xiao et al ICLR 2019) and the FastLin/CROWN-IBP work (https://arxiv.org/abs/1906.06316) is not very well presented in the theoretical sections or the experiments.

2) The theoretical results (proposition 1) requires very strong conditions to apply, which are unlikely to be satisfied for real networks. This calls into question the effectiveness of the framework developed by the authors.

While the paper has some interesting ideas, it seems unfit for publication in its present form. ",Paper Decision
u1XPecWHf,HJgKYlSKvr,Unsupervised Generative 3D Shape Learning from Natural Images,Reject,"The paper proposes a GAN approach for unsupervised learning of 3d object shapes from natural images. The key idea is a two-stage generative process where the 3d shape is first generated and then rendered to pixel-level images. While the experimental results are promising, the experimental results are mostly focused on faces (that are well aligned and share roughly similar 3d structures across the dataset). Results on other categories are preliminary and limited, so it's unclear how well the proposed method will work for more general domains. In addition, comparison to the existing baselines (e.g., HoloGAN; Pix2Scene; Rezende et al., 2016) is missing. Overall, further improvements are needed to be acceptable for ICLR. 


Extra note: Missing citation to a relevant work
Wang and Gupta, Generative Image Modeling using Style and Structure Adversarial Networks
https://arxiv.org/abs/1603.05631",Paper Decision
3aIN3Jc5OE,S1eYKlrYvr,Diagnosing the Environment Bias in Vision-and-Language Navigation,Reject,"The submission is a detailed and extensive examination of overfitting in vision-and-language navigation domains. The authors evaluate several methods across multiple environments, using different splits of the environment data into training, validation-seen, and validation-unseen. The authors also present an approach using semantic features which is shown to have little or no gap between training and validation performance. 

The reviewers had mixed reviews and there was substantial discussion about the merits of the paper. However, a significant issue was observed and confirmed with the authors, relating to tuning the semantic features and agent model on the unseen validation data. This is an important flaw, since the other methods were not tuned in this way, and there was no 'test' performance given in the paper. For this reason, the recommendation is to reject the paper. The authors are encouraged to fairly compare all models and resubmit their paper at another venue.",Paper Decision
36OPBUhY3i,SkluFgrFwH,Learning Mahalanobis Metric Spaces via Geometric Approximation Algorithms,Reject,"The paper proposes a method to handle Mahalanobis metric learning thorough linear programming.

All reviewers were unclear on what novelty of the approach is compared to existing work.

I recommend rejection at this time, but encourage the authors to incorporate reviewers' feedback (in particular placing the work in better context and clarifying the motivations) and resubmitting elsewhere.

",Paper Decision
GdXSkumVmv,rJgPFgHFwr,Laconic Image Classification: Human vs. Machine Performance,Reject,"The paper proposes and studies a task where the goal is to classify an image that has been intentionally degraded to reduce information content. 
All the reviewers found the comparison of human and machine performance interesting and valuable. However the reviewers expressed concerns and noted the following weaknesses: the presented results are not convincing to support our understanding of the differences between human and machine perception (R1), using entropy to quantify the distortion is not well motivated and has been addressed before (R1), lack of empirical evidence (R2). 
AC suggests, in its current state the manuscript is not ready for a publication. We hope the detailed reviews are useful for improving and revising the paper. ",Paper Decision
1wEHYgtPNG,SyevYxHtDB,Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks,Accept (Poster),"The paper proposed an optimization-based defense against model stealing attacks.  A criticism of the paper is that the method is computationally expensive, and was not demonstrated on more complex problems like ImageNet.  While this criticism is valid, other reviewers seem less concerned by this because the SOTA in this area is currently focused on smaller problems.  After considering the rebuttal, there is enough reviewer support for this paper to be accepted.",Paper Decision
YEVqGDsLM0,SJxDKerKDS,Reinforcement Learning with Structured Hierarchical Grammar Representations of Actions,Reject,"The topic of macro-actions/hierarchical RL is an important one and the perspective this paper takes on this topic by drawing parallels with action grammars is intriguing. However, some more work is needed to properly evaluate the significance. In particular, a better evaluation of the strengths and weaknesses of the method would improve this paper a lot.",Paper Decision
VqyyNWdNL,r1lIKlSYvH,The Usual Suspects? Reassessing Blame for VAE Posterior Collapse,Reject,"This manuscript investigates the posterior collapse in variational autoencoders and seeks to provide some explanations from the phenomenon. The primary contribution is to propose some previously understudied explanations for the posterior collapse that results from the optimization landscape of the log-likelihood portion of the ELBO.

The reviewers and AC agree that the problem studied is timely and interesting, and closely related to a variety of recent work investigating the landscape properties of variational autoencoders and other generative models. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the technical difficulty and importance of the results. In reviews and discussion, the reviewers noted issues with clarity of the presentation and sufficient justification of the results. There were also concerns about novelty. In the opinion of the AC, the manuscript in its current state is borderline, and should ideally be improved in terms of clarity of the discussion, and some more investigation of the insights that result from the analysis.",Paper Decision
3me0QzD4aS,SyxIterYwS,Dynamical System Embedding for Efficient Intrinsically Motivated Artificial Agents,Reject,"The paper proposes a novel method for embedding sequences of states and actions into a latent representation that enables efficient estimation of empowerment for an RL system. They use empowerment as intrinsic reward for safe exploration. While the reviewers agree that this paper has promise, they also agree that it is not quite ready for publication in its current state. In particular, the paper is lacking a theoretical justification for the proposed approach, the definition of empowerment used by the authors raised questions, and the manuscript would benefit from more clear and detailed description of the method. For these reasons I recommend rejection.",Paper Decision
7Cc6dSGla,SJxrKgStDH,SCALOR: Generative World Models with Scalable Object Representations,Accept (Poster),"After the author response and paper revision, the reviewers all came to appreciate this paper and unanimously recommended it be accepted.  The paper makes a nice contribution to generative modelling of object-oriented representations with large numbers of objects.  The authors adequately addressed the main reviewer concerns with their detailed rebuttal and revision.",Paper Decision
g6oIocKslj,Hye4KeSYDr,Evaluations and Methods for Explanation through Robustness Analysis,Reject,"The paper proposes an approach for finding an explainable subset of features by choosing features that simultaneously are: most important for the prediction task, and robust against adversarial perturbation. The paper provides quantitative and qualitative evidence that the proposed method works.

The paper had two reviews (both borderline), and the while the authors responded enthusiastically, the reviewers did not further engage during the discussion period.

The paper has a promising idea, but the presentation and execution in its current form have been found to be not convincing by the reviewers. Unfortunately, the submission as it stands is not yet suitable for ICLR.",Paper Decision
ljBHX9sphT,B1gNKxrYPB,Attributed Graph Learning with 2-D Graph Convolution,Reject,"The paper studies the problem of graph learning with attributes, and propose a 2-D graph convolution that models the node relation graph and the attribute graph jointly. The paper proposes and efficient algorithm and models intra-class variation. Empirical performance on 20-NG, L-Cora, and Wiki show the promise of the approach.

The authors responded to the reviews by updating the paper, but the reviewers unfortunately did not further engage during the discussion period. Therefore it is unclear whether their concerns have been adequately addressed.

Overall, there have been many strong submissions on graph neural networks at ICLR this year, and this submission as is currently stands does not quite make the threshold of acceptance.",Paper Decision
kdmSXxHAIK,HkgXteBYPB,Stochastic Neural Physics Predictor,Reject,"The paper presents a timely method for intuitive physics simulations that expand on the HTRN model, and tested in several physicals systems with rigid and deformable objects as well as other results later in the review. 

Reviewer 3 was positive about the paper, and suggested improving the exposition to make it more self-contained. Reviewer 1 raised questions about the complexity of tasks and a concerns of limited advancement provided by the paper. Reviewer 2, had a similar concerns about limited clarity as to how the changes contribute to the results, and missing baselines. The authors provided detailed responses in all cases, providing some additional results with various other videos. After discussion and reviewing the additional results, the role of the stochastic elements of the model and its contributions to performance remained and the reviewers chose not to adjust their ratings.

The paper is interesting, timely and addresses important questions, but questions remain. We hope the review has provided useful information for their ongoing research. ",Paper Decision
wmjX6QIqw2,HklQYxBKwS,"Neural tangent kernels, transportation mappings, and universal approximation",Accept (Poster),"The paper considers representational aspects of neural tangent kernels (NTKs). More precisely, recent literature on overparametrized neural networks has identified NTKs as a way to characterize the behavior of gradient descent on wide neural networks as fitting these types of kernels. This paper focuses on the representational aspect: namely that functions of appropriate ""complexity"" can be written as an NTK with parameters close to initialization (comparably close to what results on gradient descent get).  

The reviewers agree this content is of general interest to the community and with the proposed revisions there is general agreement that the paper has merits to recommend acceptance.",Paper Decision
jUclfT2jm,BJgMFxrYPB,Learning to Move with Affordance Maps,Accept (Poster),"This paper presents a framework for navigation that leverages learning spatial affordance maps (ie what parts of a scene are navigable) via a self-supervision approach in order to deal with environments with dynamics and hazards. They evaluate on procedurally generated VizDoom levels and find improvements over frontier and RL baseline agents.

Reviewers all agreed on the quality of the paper and strength of the results. Authors were highly responsive to constructive criticism and the engagement/discussion appears to have improved the paper overall. After seeing the rebuttal and revisions, I believe this paper will be a useful contribution to the field and I’m happy to recommend accept.
",Paper Decision
5IzGXvslrr,rkxMKerYwr,Towards Interpreting Deep Neural Networks via Understanding Layer Behaviors,Reject,"This paper studies the transfer of representations learned by deep neural networks across various datasets and tasks when the network is pre-trained on some dataset and subsequently fine-tuned on the target dataset. On the theoretical side the authors analyse two-layer fully connected networks. In an extensive empirical evaluation the authors argue that an appropriately pre-trained networks enable better loss landscapes (improved Lipschitzness). Understanding the transferability of representations is an important problem and the reviewers appreciated some aspects of the extensive empirical evaluation and the initial theoretical investigation. However, we feel that the manuscript needs a major revision and that there is not enough empirical evidence to support the stated conclusions. As a result, I will recommend rejecting this paper in the current form. Nevertheless, as the problem is extremely important I encourage the authors to improve the clarity and provide more convincing arguments towards the stated conclusions by addressing the issues raised during the discussion phase.",Paper Decision
35viKf4BF,S1eZYeHFDS,Deep Learning For Symbolic Mathematics,Accept (Spotlight),"The paper presents a deep learning approach for tasks such as symbolic integration and solving differential equations. 

The reviewers were positive and the paper has had extensive discussion, which we hope has been positive for the authors. 

We look forward to seeing the engagement with this work at the conference.",Paper Decision
s5DRxvMycx,HyxWteSFwS,Deep Interaction Processes for Time-Evolving Graphs,Reject,"All reviewers rated this paper as a weak reject.
The author response was just not enough to sway any of the reviewers to revise their assessment.
The AC recommends rejection.",Paper Decision
2MTnG4OKuk,rJleKgrKwS,Differentiable learning of numerical rules in knowledge graphs,Accept (Poster),"This paper presents a number of improvements on existing approaches to neural logic programming. The reviews are generally positive: two weak accepts, one weak reject. Reviewer 2 seems wholly in favour of acceptance at the end of discussion, and did not clarify why they were sticking to their score of weak accept. The main reason Reviewer 
 1 sticks to 6 rather than 8 is that the work extends existing work rather than offering a ""fundamental contribution"", but otherwise is very positive. I personally feel that
a) most work extends existing work
b) there is room in our conferences for such well executed extensions (standing on the shoulders of giants etc).

Reviewer 3 is somewhat unconvinced by the nature of the evaluation. While I understand their reservations, they state that they would not be offended by the paper being accepted in spite of their reservations.

Overall, I find that the review group leans more in favour of acceptance, and an happy to recommend acceptance for the paper as it makes progress in an interesting area at the intersection of differentiable programming and logic-based programming.",Paper Decision
-fTaMmnhJU,S1lxKlSKPH,Consistency Regularization for Generative Adversarial Networks,Accept (Poster),"The paper proposes a simple and effective way to stabilize training by adding consistency term to discriminator. Given the stochastic augmentation procedure $T(x)$ the loss is just a penalty on $D$. The main unsolved question why it help to make discriminator ""smoother"" in the consistency case for a standard GAN (since typically, no constraints are enforced). Nevertheless, at the moment this a working heuristics that gives new SOTA, and that is the main strength. The reviewer all agree to accept, and so do I.",Paper Decision
25AEPAUMhU,SkxxtgHKPS,On Generalization Error Bounds of Noisy Gradient Methods for Non-Convex Learning,Accept (Poster),"The authors provide bounds on the expected generalization error for noisy gradient methods (such as SGLD). They do so using the information theoretic framework initiated by Russo and Zou, where the expected generalization error is controlled by the mutual information between the weights and the training data. The work builds on the approach pioneered by Pensia, Jog, and Loh, who proposed to bound the mutual information for noisy gradient methods in a step wise fashion.

The main innovation of this work is that they do not implicitly condition on the minibatch sequence when bounding the mutual information. Instead, this uncertainty manifests as a mixture of gaussians. Essentially they avoid the looseness implied by an application of Jensen's inequality that they have shown was unnecessary.

I think this is an interesting contribution and worth publishing. It contributes to a rapidly progressing literature on generalization bounds for SGLD that are becoming increasingly tight.

I have one strong request that I will make of the authors, and I'll be quite disappointed if it is not executed faithfully.

1. The stepsize constraint and its violation in the experimental work is currently buried in the appendix. This fact must be brought into the main paper and made transparent to readers, otherwise it will pervert empirical comparisons and mask progress.

2. In fact, I would like the authors to re-run their experiments in a way that guarantees that the bounds are applicable. One approach is outline by the authors: the Lipschitz constant can be replaced by a max_i bound on the running squared gradient norms, and then gradient clipping can be used to guarantee that the step-size constraint is met.  The authors might compare step sizes, allowing them to use less severe gradient clipping. The point of this exercise is to verify that the learning dynamics don't change when the bound conditions are met. If they change, it may upset the empirical phenomena they are trying to study. If this change does upset the empirical findings, then the authors should present both, and clearly explain that the bound is not strictly speaking known to be valid in one of the cases. It will be a good open problem.




",Paper Decision
lpJWcFOdJr,SylkYeHtwr,SUMO: Unbiased Estimation of Log Marginal Probability for Latent Variable Models,Accept (Spotlight),"The paper proposes a new way to train latent variable models. The standard way of training using the ELBO produces biased estimates for many quantities of interest. The authors introduce an unbiased estimate for the log marginal probability and its derivative to address this. The new estimator is based on the importance weighted autoencoder, correcting the remaining bias using russian roulette sampling. The model is empirically shown to give better test set likelihood, and can be used in tasks where unbiased estimates are needed. 

All reviewers are positive about the paper. Support for the main claims is provided through empirical and theoretical results. The reviewers had some minor comments, especially about the theory, which the authors have addressed with additional clarification, which was appreciated by the reviewers. 

The paper was deemed to be well organized. There were some unclarities about variance issues and bias from gradient clipping, which have been addressed by the authors in additional explanation as well as an additional plot. 

The approach is novel and addresses a very relevant problem for the ICLR community: optimizing latent variable models, especially in situations where unbiased estimates are required. The method results in marginally better optimization compared to IWAE with much smaller average number of samples. The method was deemed by the reviewers to open up new possibilities such as entropy minimization. ",Paper Decision
beqc2bOBH,rkg0_eHtDr,Benefits of Overparameterization in Single-Layer Latent Variable Generative Models,Reject,"This paper studies over-parameterization for unsupervised learning. The paper does a series of empirical studies on this topic. Among other things the authors observe that larger models can increase the number latent variables recovered when fitting larger variational inference models. The reviewers raised some concern about the simplicity of the models studied and also lack of some theoretical justification. One reviewer also suggests that more experiments and ablation studies on more general models will further help clarify the role over-parameterized model for latent generative models. I agree with the reviewers that this paper is ""compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods"". I disagree with the reviewers that theoretical study is required as I think a good empirical paper with clear conjectures is as important. I do agree with the reviewers however that for empirical paper I think the empirical studies would have to be a bit more thorough with more clear conjectures. In summary, I think the paper is nice and raises a lot of interesting questions but can be improved with more through studies and conjectures. I would have liked to have the paper accepted but based on the reviewer scores and other papers in my batch I can not recommend acceptance at this time. I strongly recommend the authors to revise and resubmit. I really think this is a nice paper and has a lot of potential and can have impact with appropriate revision.",Paper Decision
ugWQ9NTxBn,SkxaueHFPB,Implicit competitive regularization in GANs,Reject,"The paper proposes to study ""implicit competitive regularization"", a phenomenon borne of taking a more nuanced game theoretic perspective on GAN training, wherein the two competing networks are ""model[ed] ... as agents acting with limited information and in awareness of their opponent"". The meaning of this is developed through a series of examples using simpler games and didactic experiments on actual GANs. An adversary-aware variant employing a Taylor approximation to the loss. 

Reviewer assessment amounted to 3 relatively light reviews, two of which reported little background in the area, and one more in-depth review, which happened to also be the most critical. R1, R2, R3 all felt the contribution was interesting and valuable. R1 felt the contribution of the paper may be on the light side given the original competitive gradient descent paper, on which this manuscript leans heavily, included GAN training (the authors disagreed); they also felt the paper would be stronger with additional datasets in the empirical evaluation (this was not addressed). R2 felt the work suffered for lack of evidence of consistency via repeated experiments, which the authors explained was due to the resource-intensity of the experiments. 

R5 raised that Inception scores for both the method and being noticeably worse than those reported in the literature, a concern that was resolved in an update and seemed to center on the software implementation of the metric. R5 had several technical concerns, but was generally unhappy with the presentation and finishedness of the manuscript, in particular the degree to which details are deferred to the CGD paper. (The authors maintain that CGD is but one instantiation of a more general framework, but given that the empirical section of the paper relies on this instantiation I would concur that it is under-treated.)

Minor updates were made to the paper, but R5 remains unconvinced (other reviewers did not revisit their reviews at all). In particular: experiments seem promising but not final (repeatability is a concern), the single paragraph ""intuitive explanation"" and cartoon offered in Figure 3 were viewed as insufficiently rigorous. A great deal of the paper is spent on simple cases, but not much is said about ICR specifically in those cases. 

This appears to have the makings of an important contribution, but I concur with R5 that it is not quite ready for mass consumption. As is, the narrative is locally consistent but quite difficult to follow section after section. It should also be noted that ICLR as a venue has a community that is not as steeped in the game theory literature as the authors clearly are, and the assumed technical background is quite substantial here. For a game theory novice, it is difficult to tell which turns of phrase refer to concepts from game theory and which may be more informally introduced herein. I believe the paper requires redrafting for greater clarity with a more rigorous theoretical and/or empirical characterization of ICR, perhaps involving small scale experiments which clearly demonstrates the effect. I also believe the authors have done themselves a disservice by not availing themselves of 10 pages rather than 8.

I recommend rejection at this time, but hope that the authors view this feedback as valuable and continue to improve their manuscript, as I (and the reviewers) believe this line of work has the potential to be quite impactful.",Paper Decision
h1l5jl__g,HJgpugrKPS,Scale-Equivariant Steerable Networks,Accept (Poster),"This work presents a theory for building scale-equivariant CNNs with steerable filters. The proposed method is compared with some of the related techniques . SOTA is achieved on MNIST-scale dataset and gains on STL-10 is demonstrated. The reviewers had some concern related to the method, clarity, and comparison with related works. The authors have successfully addressed most of these concerns. Overall, the reviewers are positive about this work and appreciate the generality of the presented theory and its good empirical performance. All the reviewers recommend accept.",Paper Decision
KWM6uzvTUb,B1e3OlStPB,DeepSphere: a graph-based spherical CNN,Accept (Spotlight),"This paper proposes a novel methodology for applying convolutional networks to spherical data through a graph-based discretization.   The reviewers all found the methodology sensible and the experiments convincing.  A common concern of the reviewers was the amount of novelty in the approach, as in it involves the combination of established methods, but ultimately they found that the empirical performance compared to baselines outweighed this.",Paper Decision
BTg6x3DRJE,rke3OxSKwr,Improved Training Techniques for Online Neural Machine Translation,Reject,"The paper proposes a method of training latency-limited (wait-k) decoders for online machine translation. The authors investigate the impact of the value of k, and of recalculating the transformer's decoder hidden states when a new source token arrives. They significantly improve over state-of-the-art results for German-English translation on the WMT15 dataset, however there is limited novelty wrt previous approaches. The authors responded in depth to reviews and updated the paper with improvements, for which there was no reviewer response. The paper presents interesting results but IMO the approach is not novel enough to justify acceptance at ICLR. 
",Paper Decision
63MySLlyvP,BJes_xStwS,GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE,Reject,This paper proposes a scalable approach for graph learning from data. The reviewers think the approach appears heuristic and it is not clear the algorithm is optimizing the proposed sparse graph recovery objective. ,Paper Decision
U4ysJBqtAS,H1ls_eSKPH,Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates,Reject,The reviewers have provided thorough reviews of your work. I encourage you to read them carefully should you decide to resubmit it to a later conference.,Paper Decision
JQDOodtjrq,HygcdeBFvr,Score and Lyrics-Free Singing Voice Generation,Reject,"Main content:

Blind review #1 summarizes it well:

his paper claims to be the first to tackle unconditional singing voice generation. It is noted that previous singing voice generation approaches leverage explicit pitch information (either of an accompaniment via a score or for the voice itself), and/or specified lyrics the voice should sing. The authors first create their own dataset of singing voice data with accompaniments, then use a GAN to generate singing voice waveforms in three different settings:
1) Free singer - only noise as input, completely unconditional singing sampling
2) Accompanied singer - Providing the accompaniment *waveform* (not symbolic data like a score - the model needs to learn how to transcribe to use this information) as a condition for the singing voice
3) Solo singer - The same setting as 1 but the model first generates an accompaniment then, from that, generates singing voice

--

Discussion:

The reviews generally point out that while a lot of new work has been done, this paper bites off too much at once: it tackles many different open problems, in a generative art domain where evaluation is subjective.

--

Recommendation and justification:

This paper is a weak reject, not because it is uninteresting or bad work, but because the ambitious scope is really too large for a single conference paper. In a more specialized conference like ISMIR, it would still have a good chance. The authors should break it down into conference sized chunks, and address more of the reviewer comments in each chunk.",Paper Decision
QEh3-eRxB,Byeq_xHtwS,Neural Video Encoding,Reject,The paper has several clarity and novelty issues.,Paper Decision
PgFDXBA3cl,H1lK_lBtvS,Classification-Based Anomaly Detection for General Data,Accept (Poster),"The paper presents a method that unifies classification-based approaches for outlier detection and (one-class) anomaly detection. The paper also extends the applicability to non-image data.

In the end, all the reviewers agreed that the paper makes a valuable contribution and I'm happy to recommend acceptance.",Paper Decision
YIcgBqa2U,SJeuueSYDH,Distributed Training Across the World,Reject,"The paper introduces a distributed algorithm for training deep nets in clusters with high-latency (i.e. very remote) nodes. While the motivation and clarity are the strengths of the paper, the reviewers have some concerns regarding novelty and insufficient theoretical analysis. ",Paper Decision
6MynmiUkxQ,Sye_OgHFwH,Unrestricted Adversarial Examples via Semantic Manipulation,Accept (Poster),"In this paper, the authors present adversarial attacks by semantic manipulations, i.e., manipulating specific detectors that result in imperceptible changes in the picture, such as changing texture and color, but without affecting their naturalness. Moreover, these tasks are done on two large scale datasets (ImageNet and MSCOCO) and two visual tasks (classification and captioning). Finally, they also test their adversarial examples against a couple of defense mechanisms and how their transferability. Overall, all reviewers agreed this is an interesting work and well executed, complete with experiments and analyses. I agree with the reviewers in the assessment. I think this is an interesting study that moves us beyond restricted pixel perturbations and overall would be interesting to see what other detectors could be used to generate these type of semantic manipulations. I recommend acceptance of this paper.
",Paper Decision
HNF76UX_5a,HJxDugSFDB,Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model,Reject,"An actor-critic method is introduced that explicitly aims to learn a good representation using a stochastic latent variable model. There is disagreement among the reviewers regarding the significance of this paper. Two of the three reviewers argue that several strong claims made in the paper that are not properly backed up by evidence. In particular, it is not sufficiently clear to what degree the shown performance improvement is due to the stochastic nature of the model used, one of the key points of the paper. I recommend that the authors provide more empirical evidence to back up their claims and then resubmit.",Paper Decision
fUcThWQGyg,BJlPOlBKDB,Closed loop deep Bayesian inversion:  Uncertainty driven acquisition for fast MRI,Reject,"The author responses and notes to the AC are acknowledged.  A fourth review was requested because this seemed like a tricky paper to review, given both the technical contribution and the application area.  Overall, the reviewers were all in agreement in terms of score that the paper was just below borderline for acceptance.  They found that the methodology seemed sensible and the application potentially impactful.  However, a common thread was that the paper was hard to follow for non-experts on MRI and the reviewers weren't entirely convinced by the experiments (asking for additional experiments and comparison to Zhang et al.).  The authors comment on the challenge of implementing Zhang is acknowledged and it's unfortunate that cluster issues prevented additional experimental results.  While ICLR certainly accepts application papers and particularly ones with interesting technical contribution in machine learning, given that the reviewers  struggled to follow the paper through the application specific language it does seem like this isn't the right venue for the paper as written.  Thus the recommendation is to reject.  Perhaps a more application specific venue would be a better fit for this work.  Otherwise, making the paper more accessible to the ML audience and providing experiments to justify the methodology beyond the application would make the paper much stronger.",Paper Decision
7zzTp7G9Lr,BJg8_xHtPr,OBJECT-ORIENTED REPRESENTATION OF 3D SCENES,Reject,"The author proposes a object-oriented probabilistic generative model of 3D scenes.  The model is based on the GQN with the key innovation being that there is a separate 3D representation per object (vs a single one for the entire scene).  A scene-volume map is used to prevent two objects from occupying the same space. The authors show that using this model, it's possible to learn the scene representation in an unsupervised manner (without the 3D ground truth). 

The submission has received relatively low scores with one weak accept and 3 weak rejects.  All reviewers found the initial submission to be unclear and poorly written (with 1 reject and 3 weak rejects initially).  The initial submission also failed to acknowledge prior work on object based representations in the 3D vision community.  Based on the reviewer feedback, the authors greatly improved the paper by reworking the notation and the description of the model, and included a discussion of related work from 3D vision.  Overall, the exposition of the paper was substantially improved.  Some of the reviewers recognize the improvement, and lifted their scores.  

However, the work still have some issues:
1. The experimental section is still weak
The reviewers (especially those from an computer vision background) questioned the lack of baseline comparisons and ablation studies, which the authors (in their rebuttal) felt to be unnecessary. It is this AC's opinion that comparisons against alternatives and ablations is critical for scientific rigor, and high quality work aims not to just propose new models, but also to demonstrate via experimental analysis how the model compares to previous models, and what parts of the model is necessary, coming up with new metrics, baselines, and evaluation when needed.

It is the AC's opinion that the authors should attempt to compare against other methods/baselines when appropriate.  For instance, perhaps it would make sense to compare the proposed model against IODINE and MONet.  Upon closer examination of the experimental results, the AC also finds that the description of the object detection quality to be not very precise.  Is the evaluation in 2D or 3D?  The filtering of predictions that are too far away from any ground truth also seems unscientific.  

2. The objects and arrangements considered in this paper is very simplistic.  

3. The writing is still poor and need improvement.
The paper needs an editing pass as the paper was substantially rewritten.  There are still grammar/typos, and unresolved references to Table ?? (page 8,9).


After considering the author responses and the reviewer feedback, the AC believe this work shows great promise but still need improvement.  The authors have tackled a challenging and exciting problem, and have provided a very interesting model.  The work can be strengthened by improving the experiments, analysis, and the writing.  The AC recommend the authors further iterate on the paper and resubmit.  As the revised paper was significantly different from the initial submission, an additional review cycle will also help ensure that the revised paper is properly fully evaluated.  The current reviewers are to be commended for taking the time and effort to look over the revision. ",Paper Decision
Up_neGBg1Y,HJl8_eHYvS,Discriminative Particle Filter Reinforcement Learning for Complex Partial observations,Accept (Poster),"The authors introduce an RL algorithm / architecture for partially observable                                                      
environments.                                                                                                                      
At the heart of it is a filtering algorithm based on a differentiable version of                                                   
sequential Monte Carlo inference.                                                                                                  
The inferred particles are fed into a policy head and the whole architecture is                                                    
trained by RL.                                                                                                                     
The proposed methods was evaluated on multiple environments and ablations                                                          
establish that all moving parts are necessary for the observed performance.                                                        
                                                                                                                                   
All reviewers agree that this is an interesting contribution for addressing the                                                    
important problem of acting in POMDPs.                                                                                             
                                                                                                                                   
I think this paper is well above acceptance threshold. However, I have a few points that I                                         
would quibble with:                                                                                                                
1) I don't see how the proposed trampling is fully differentiable; as far as I                                                     
understand it, no credit is assigned to the discrete decision which particle to                                                    
reuse. Adding a uniform component to the resampling distribution does not                                                          
make it fully differentiable, see eg [Filtering Variational Objectives. Maddison                                                   
et al]. I think the authors might use a form of straight-through gradient approximation.                                           
2) Just stating that unsupervised losses might incentivise the filter to learn                                                     
the wrong things, and just going back to plain RL loss is not in itself a novel                                                    
contribution; in extremely sparse reward settings, this will not be                                                                
satisfactory.           ",Paper Decision
-K7FkhWk5,rkl8dlHYvB,Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories,Accept (Poster),"This paper presents and evaluates a technique for unsupervised object part discovery in 3d -- i.e. grouping points of a point cloud into coherent parts for an object that has not been seen before. The paper received 3 reviews from experts working in this area. R1 recommended Weak Accept, and identified some specific technical questions for the authors to address in the response (which the authors provided and R1 seemed satisfied). R2 recommends Weak Reject, and indicates an overall positive view of the paper but felt the experimental results were somewhat weak and posed several specific questions to the reviewers. The authors' response convincingly addressed these questions. R3 recommends Accept, but suggests some additional qualitative examples and ablation studies. The author response again addresses these. Overall, the reviews indicate that this is a good paper with some specific questions and concerns that can be addressed; the AC thus recommends a (Weak) Accept based on the reviews and author responses.",Paper Decision
EnY6q3beyN,rylrdxHFDr,State Alignment-based Imitation Learning,Accept (Poster),"This paper seeks to adapt behavioural cloning to the case where demonstrator and learner have different dynamics (e.g. human demonstrator), by designing a state-based objective. The reviewers agreed the paper makes an important and interesting contribution, but were somewhat divided about whether the experiments were sufficiently impactful. They furthermore had additional concerns regarding the clarity of the paper and presentation of the method. Through discussion, it seems that these were sufficiently addressed that the consensus has moved towards agreeing that the paper sufficiently proves the concept to warrant publication (with one reviewer dissenting).

I recommend acceptance, with the view that the authors should put a substantial amount of work into improving the presentation of the paper based on the feedback that has emerged from the discussion before the camera ready is submitted (if accepted).",Paper Decision
qurX__LD4,r1gBOxSFwr,Reweighted Proximal Pruning for Large-Scale Language Representation,Reject,"This paper proposes a novel pruning method for use with transformer text encoding models like BERT, and show that it can dramatically reduce the number of non-zero weights in a trained model while only slightly harming performance.

This is one of the hardest cases in my pile. The topic is obviously timely and worthwhile. None of the reviewers was able to give a high-confidence assessment, but the reviews were all ultimately leaning positive. However, the reviewers didn't reach a clear consensus on the main strengths of the paper, even after some private discussion, and they raised many concerns. These concerns, taken together, make me doubt that the current paper represents a substantial, sound contribution to the model compression literature in NLP.

I'm voting to reject, on the basis of:

- Recurring concerns about missing strong baselines, which make it less clear that the new method is an ideal choice.
- Relatively weak motivations for the proposed method (pruning a pre-trained model before fine-tuning) in the proposed application domain (mobile devices).
- Recurring concerns about thin analysis.",Paper Decision
9-OmkaOVen,H1gNOeHKPS,Neural Arithmetic Units,Accept (Spotlight),"This paper extends work on NALUs, providing a pair of units which, in tandem, outperform NALUs. The reviewers were broadly in favour of the paper given the presentation and results. The one dissenting reviewer appears to not have had time to reconsider their score despite the main points of clarification being addressed in the revision. I am happy to err on the side of optimism here and assume they would be satisfied with the changes that came as an outcome of the discussion, and recommend acceptance.",Paper Decision
t3ThYgE2pw,rJe4_xSFDB,Lipschitz constant estimation of Neural Networks via sparse polynomial optimization,Accept (Poster),"This paper improves upper bound estimates on Lipschitz constants for neural networks by converting the problem into a polynomial optimization problem.  The proposed method also exploits sparse connections in the network to decompose the original large optimization problem into smaller ones that are more computationally tractable. The bounds achieved by the method improve upon those found from a quadratic program formulation.  The method is tested on networks with random weights and networks trained on MNIST and provides better estimates than the baselines.

The reviews and the author discussion covered several topics.  The reviewers found the paper to be well written.  The reviewers liked that tighter bounds on the Lipschitz constants can be found in a computationally efficient manner.  They also liked that the method was applied to a real-world dataset, though they noted that the sizes of the networks analyzed here are smaller than the ones in common use.  The reviewers pointed out several ways that the paper could be improved.  The authors adopted these suggestions including additional comparisons, computation time plots, error bars, and relevant references to related work.  The reviewers found the discussion and revised paper addressed most of their concerns.

This paper improves on existing methods for analyzing neural network architectures and it should be accepted.",Paper Decision
0UJ0v0KhtO,SJx4Ogrtvr,Random Bias Initialization Improving Binary Neural Network Training,Reject,"The article studies the behaviour of binary and full precision ReLU networks towards explaining differences in performance and suggests a random bias initialisation strategy. The reviewers agree that, while closing the gap between binary networks and full precision networks is an interesting problem, the article cannot be accepted in its current form. They point out that more extensive theoretical analysis and experiments would be important, as well as improving the writing. The authors did not provide a rebuttal nor a revision. ",Paper Decision
F8FXr3itAs,B1xmOgrFPS,Meta-RCNN: Meta Learning for Few-Shot Object Detection,Reject,"This paper develops a meta-learning approach for few-shot object detection. This paper is borderline and the reviewers are split. The problem is important, albeit somewhat specific to computer vision applications. The main concerns were that it was lacking a head-to-head comparison to RepMet and that it was missing important details (e.g. the image resolution was not clarified, nor was the paper updated to include the details). The authors suggested that the RepMet code was not available, but I was able to find the official code for RepMet via a simple Google search:
https://github.com/jshtok/RepMet
Reviewers also brought up concerns about an ICCV 2019 paper, though this should be considered as concurrent work, as it was not publicly available at the time of submission.
Overall, I think the paper is borderline. Given that many meta-learning papers compare on rather synthetic benchmarks, the study of a more realistic problem setting is refreshing. That said, it's unclear if the insights from this paper would transfer to other machine learning problem settings of interest to the ICLR community.
With all of this in mind, the paper is slightly below the bar for acceptance at ICLR.",Paper Decision
DiprJvmxlN,SJeQdeBtwB,Adversarially learned anomaly detection for time series data,Reject,"The paper proposes a cycle-consistent GAN architecture with measuring the reconstruction error of time series for anomaly detection.

The paper aims to address an important problem, but the current version is not ready for publication. We suggest the authors consider the following aspects for improving the paper:
1. The novelty of the proposed model: motivate the design choices and compare them with state-of-art methods
2. Evaluation: formalize the target anomalies and identify datasets/examples where the proposed model can significantly outperform existing solutions. 

",Paper Decision
GwTubrGbT,rkgfdeBYvH,Effect of Activation Functions on the Training of Overparametrized Neural Nets,Accept (Poster),"The article studies the role of the activation function in learning of 2 layer overparaemtrized networks, presenting results on the minimum eigenvalues of the Gram matrix that appears in this type of analysis and which controls the rate of convergence. The article makes numerous observations contributing to the development of principles for the design of activation functions and a better understanding of an active area of investigation as is convergence in overparametrized nets. The reviewers were generally positive about this article. ",Paper Decision
VHxYD5BXGe,H1xzdlStvB,Multi-Precision Policy Enforced Training (MuPPET) : A precision-switching strategy for quantised fixed-point training of CNNs,Reject,"The submission presents an approach to speed up network training time by using lower precision representations and computation to begin with and then dynamically increasing the precision from 8 to 32 bits over the course of training. The results show that the same accuracy can be obtained while achieving a moderate speed up. 

The reviewers were agreed that the paper did not offer a signficant advantage or novelty, and that the method was somewhat ad hoc and unclear. Unfortunately, the authors' rebuttal did not clarify all of these points, and the recommendation after discussion is for rejection. ",Paper Decision
_UYc9KyXZ,S1eZOeBKDS,Deep Spike Decoder (DSD),Reject,"The paper presents a model for learning spiking representations. The basic model is a a deep autoencoder trained end-to-end with a biophysical generative model and results are presented on EMG and sEMG data, with the aim to motivate further research in self-supervised learning.

The reviewers raised several points about the paper. Reviewer 1 raised concerns about lack of context on surrounding work, clarity of the model itself and motivating the loss. Reviewer 2 pointed out strengths of the paper in its simplicity and the importance of this problem, but also raised concerns about the papers clarity, again motivations on the loss function and sensibility of design choices. The authors responded to the feedback from reviewer 1, but overall the reviewer did not think their scores should be changed.

The paper in its current form is not yet ready for acceptance, and we hope there has been useful feedback from the reviewing process for their future research.",Paper Decision
uMu1stIs41,r1eWdlBFwS,Isolating Latent Structure with Cross-population Variational Autoencoders,Reject,"The paper proposes a hierarchical Bayesian model over multiple data sets that                                                      
has both data set specific as well as shared parameters.                                                                           
The data set specific parameters are further encouraged to only capture aspects                                                    
that vary across data sets by an addition mutual information contribution to the                                                   
training loss.                                                                                                                     
The proposed method is compared to standard VAEs on multiple data sets.                                                            
                                                                                                                                   
The reviewers agree that the main approach of the paper is sensible. However,                                                      
concerns were raised about general novelty, about the theoretical justification                                                    
for the proposed loss function and about the lack of non-trivial baselines.                                                         
The authors' rebuttal did not manage to full address these points.                                                                 
                                                                                                                                   
Based on the reviews and my own reading, I think this paper is slightly                                                            
below acceptance threshold.",Paper Decision
II4lbZQObA,BJxbOlSKPr,Learning Compact Embedding Layers via Differentiable Product Quantization,Reject,"The presented paper gives a differentiable product quantization framework to compress embedding and support the claim by experiments (the supporting materials are as large as the paper itself). Reviewers agreed that the idea is simple is interesting, and also nice and positive discussion appeared. However, the main limiting factor is the small novelty over Chen 2018b, and I agree with that. Also, the comparison with low rank is rather formal: of course it would be of full rank , as the authors claim in the answer, but looking at singular values is needed to make this claim. Also, one can use low-rank tensor factorization to compress embeddings, and this can be compared. 
To summarize, I think the contribution is not enough to be accepted.",Paper Decision
DFXmJ0Bk3S,HkxedlrFwB,Accelerating First-Order Optimization Algorithms,Reject,"All reviewers recommend rejection, and the authors have not provided a response.
",Paper Decision
G_AN7-BgEc,BylldxBYwH,Physics-Aware Flow Data Completion Using Neural Inpainting,Reject,"The authors present a physics-aware models for inpainting fluid data. In particular, the authors extend the vanilla U-net architecture and add losses that explicitly  bias the network towards physically meaningful solutions. 

While the reviewers found the work to be interesting, they raised a few questions/objections which are summarised below:

1) Novelty: The reviewers largely found the idea to be novel. I agree that this is indeed novel and a step in the right direction.
2) Experiments: The main objection was to the experimental methodology. In particular, since most of the experiments were on simulated data the reviewers expected simulations where the test conditions were a bit more different than the training conditions. It is not very clear whether the training and test conditions were different and it would have been useful if the authors had clarified this in the rebuttal. The reviewers have also suggested a more thorough ablation study.
3) Organisation: The authors could have used the space more effectively by providing additional details and ablation studies.

Unfortunately, the authors did not engage with the reviewers and respond to their queries. I understand that this could have been because of the poor ratings which would have made the authors believe that a discussion wouldn't help. The reviewers have asked very relevant Qs and made some interesting suggestions about the experimental setup. I strongly recommend the authors to consider these during subsequent submissions. 

Based on the reviewer comments and lack of response from the authors, I recommend that the paper cannot be accepted. ",Paper Decision
BSKqR4y2a-,BkeyOxrYwH,Imagine That! Leveraging Emergent Affordances for Tool Synthesis in Reaching Tasks,Reject,"This paper investigates the task of learning to synthesize tools for specific tasks (in this case, a simulated reaching task). The paper was reviewed by 3 experts and received Reject, Weak Reject, and Weak Reject opinions. The reviews are very encouraging of the topic and general approach taken by the paper -- e.g. R3 commenting on the ""coolness"" of the problem and R1 calling it an ""important problem from a cognitive perspective"" -- but also identify a number of concerns about baselines, novelty of proposed techniques, underwhelming performance on the task, whether experiments support the conclusions, and some missing or unclear technical details. Overall, the feeling of the reviewers is that they're ""not sure what I am supposed to get out of the paper"" (R3). The authors posted responses that addressed some of these issues, in particular clarifying their terminology and contribution, and clearing up some of the technical details. However, in post-rebuttal discussions, the reviewers still have concerns with the claims of the papers. In light of these reviews, we are not able to recommend acceptance at this time, but I agree with reviewers that this is a ""cool"" task and that authors should revise and submit to another venue.",Paper Decision
UuAtqjBE4,BJxkOlSYDH,Provable Filter Pruning for Efficient Neural Networks,Accept (Poster),"This paper presents a sampling-based approach for generating compact CNNs by pruning redundant filters. One advantage of the proposed method is a bound for the final pruning error.

One of the major concerns during review is the experiment design. The original paper lacks the results on real work dataset like ImageNet. Furthermore, the presentation is a little misleading. The authors addressed most of these problems in the revision.

Model compression and purring is a very important field for real world application, hence I choose to accept the paper.
",Paper Decision
Kg-j6npq7E,HJeRveHKDH,ADAPTIVE GENERATION OF PROGRAMMING PUZZLES,Reject,"The authors introducing programming puzzles as a way to help AI systems learn about reasoning. The authors then propose a GAN-like generation algorithm to generate diverse and difficult puzzles.

This is a very novel problem and the authors have made an interesting submission. However, at least 2 reviewers have raised severe concerns about the work. In particular, the relation to existing work as pointed by R2 was not very clear. Further, the paper was also lacking a strong empirical evaluation of the proposed ideas.  The authors did agree with most of the comments of the reviewers and made changes wherever possible. However, some changes have been pushed to future work or are not feasible right now. 

Based on the above observations, I recommended that the paper cannot be accepted now. The paper has a lot of potential and I would strongly encourage a revised submission addressing the questions/suggestions made by the reviewers.",Paper Decision
fbgK9NG51,ryeRwlSYPH,Learning transitional skills with intrinsic motivation,Reject,"The submission has two issues, identified by the reviewers; (1) the description of the proposed method was found to be confusing at times and could be improved, and (2) the proposed transitional skills were not well motivated/justified as a solution to the problem the authors propose to solve.",Paper Decision
3xiESHSF3l,HyeAPeBFwS,Quantifying uncertainty with GAN-based priors,Reject,"This paper suggests a Bayesian approach to make inference about latent variables for image inference tasks. While the idea in the paper seems elegant and simple, reviewers pointed out a few concerns, including lack of comparisons, missing references, and requested for more extensive validations. While a few comments might have been misunderstandings (eg lack of quantification - seems to be resolved by author’s comments), other comments are not (eg equation (8) needs further justification even if the final results don’t use it). We encourage authors to carefully review comments and edit the manuscript (perhaps some appendix items should be in the main to reduce confusion) for resubmitting to future conferences. ",Paper Decision
OwGtF3nhon,rkxawlHKDr,End to End Trainable Active Contours via Differentiable Rendering,Accept (Poster),"The submission presents a differentiable take on classic active contour methods, which used to be popular in computer vision. The method is sensible and the results are strong. After the revision, all reviewers recommend accepting the paper.",Paper Decision
1YYTsovr96,Bye6weHFvB,Plan2Vec: Unsupervised Representation Learning by Latent Plans,Reject,"The paper proposes a representation learning objective that makes it 
amenable to planning, 

The initial submission contained clear holes, such as missing related work and only containing very simplistic baselines. The authors have substantially updated the paper based on this feedback, resulting in a clear improvement.

Nevertheless, while the new version is a good step in the right direction, there is some additional work needed to fully address the reviewers' complaints. For example, the improved baselines are only evaluated in the most simple domain, while the more complex domains still only contain simplistic baselines that are destined to fail. There are also some unaddressed questions regarding the correctness of Eq. 4. Finally, the substantial rewrites have given the paper a less-than-polished feel.

In short, while the work is interesting, it still needs a few iterations before it's ready for publication.",Paper Decision
2bws3MYb5P,rklnDgHtDS,Compositional Language Continual Learning,Accept (Poster),"The paper addresses the task of continual learning in NLP for seq2seq style tasks. The key idea of the proposed method is to enable the network to represent syntactic and semantic knowledge separately, which allows the neural network to leverage compositionality for knowledge transfer and also solves the problem of catastrophic forgetting. The paper has been improved substantially after the reviewers' comments and also obtains good results on benchmark tasks. The only concern is that the evaluation is on artificial datasets. In future, the authors should try to include more evaluation on real datasets (however, this is also limited by availability of such datasets). As of now, I'm recommending an Acceptance.",Paper Decision
-STvBNexM,H1livgrFvr,Out-of-Distribution Image Detection Using the Normalized Compression Distance,Reject,"This paper proposes an out-of-distribution detection (OOD) method without assuming OOD in validation.

As reviewers mentioned, I think the idea is interesting and the proposed method has potential. However, I think the paper can be much improved and is not ready to publish due to the followings given reviewers' comments:

(a) The prior work also has some experiments without OOD in validation, i.e., use adversarial examples (AE) instead in validation. Hence, the main motivation of this paper becomes weak unless the authors justify enough why AE is dangerous to use in validation. 

(b) The performance of their replication of the prior method is far lower than reported. I understand that sometimes it is not easy to reproduce the prior results. In this case, one can put the numbers in the original paper. Or, one can provide detailed analysis why the prior method should fail in some cases.

(c) The authors follow exactly same experimental settings in the prior works. But, the reported score of the prior method is already very high in the settings, and the gain can be marginal. Namely, the considered settings are more or less ""easy problems"". Hence, additional harder interesting OOD settings, e.g., motivated by autonomous driving, would strength the paper.

Hence, I recommend rejection.",Paper Decision
YWTOjO-nze,SJxjPxSYDH,Discriminative Variational Autoencoder for Continual Learning with Generative Replay,Reject,"The paper presents a method for continual learning with a variant of VAE. The proposed approach is reasonable but technical contribution is quite incremental. The experimental results are limited to comparisons among methods with generative replay, and experimental results on more complex datasets (e.g., CIFAR 100, CUB, ImageNet) are missing. Overall, the contribution of the work in the current form seems insufficient for acceptance at ICLR.",Paper Decision
zt4p3lJx-,HkliveStvH,Connectivity-constrained interactive annotations for panoptic segmentation,Reject,The paper proposes two methods for interactive panoptic segmentation (a combination of semantic and instance segmentation) that leverages scribbles as supervision during inference. Reviewers had concerns about the novelty of the paper as it applies existing algorithms for this task and limited empirical comparison with other methods. Reviewers also suggested that ICLR may not be a good fit for the paper and I encourage the authors to consider submitting to a vision oriented conference. ,Paper Decision
eA8ubw6DE9,B1lqDertwr,Regularization Matters in Policy Optimization,Reject,"This paper proposes an analysis of regularization for policy optimization. While the multiple effects of regularization are well known in the statistics and optimization community, it is less the case in the RL community. This makes the novelty of the paper difficult to judge as it depends on the familiarity of RL researchers with the two aforementioned communities.

Besides the novelty aspect, which is debatable, reviewers had doubts on the significance of the results, and in particular on the metrics chosen (based on the rank). While defining a ""best"" algorithm is notoriously difficult, and could be considered outside of the scope of this paper, the fact is that the conclusions reached are still sensitive to that difficulty.

I thus regret to reject this paper as I feel not much more work is necessary to provide a compelling story. I encourage the authors to extend their choice of metrics to be more convincing in their conclusions.",Paper Decision
qdjEOPz41T,HkgFDgSYPH,Adaptive Online Planning for Continual Lifelong Learning,Reject,"A new setting for lifelong learning is analyzed and a new method, AOP, is introduced, which combines a model-free with a model-based  approach to deal with this setting.

While the idea is interesting, the main claims are insufficiently demonstrated. A theoretical justification is missing, and the experiments alone are not rigorous enough to draw strong conclusions. The three environments are rather simplistic and there are concerns about the statistical significance, for at least some of the experiments.",Paper Decision
ZiH5Xy3UTq,B1lKDlHtwS,Measuring causal influence with back-to-back regression: the linear case,Reject,"The authors introduce a method for disentangling effects of correlated predictors in the context of high dimensional outcomes. While the paper contains interesting ideas and has been substantially improved from its original form, the paper still does not meet the quality bar of ICLR due to its limitations in terms of limited applicability and experiments. The paper will benefit from a revision and resubmission to another venue.",Paper Decision
sGgjWrMaSl,ByxODxHYwB,Multi-source Multi-view Transfer Learning in Neural Topic Modeling with Pretrained Topic and Word Embeddings,Reject,"This paper presents a transfer learning framework in neural topic modeling. Authors claim and reviewers agree that this view of transfer learning in the realm of topic modeling is novel.

However, after much deliberation and discussion among the reviewers, we conclude that this paper does not contribute sufficient novelty in terms of the method. Also, reviewers find the experiments and results not sufficiently convincing.

I sincerely thank the authors for submitting to ICLR and hope to see a revised paper in a future venue.",Paper Decision
E3CJgBqpS4,Bke_DertPB,Adversarial Lipschitz Regularization,Accept (Poster),"This paper introduces an adversarial approach to enforcing a Lipschitz constraint on neural networks. The idea is intuitively appealing, and the paper is clear and well written. It's not clear from the experiments if this method outperforms competing approaches, but it is at least comparable, which means this is at the very least another useful tool in the toolbox. There was a lot of back-and-forth with the reviewers, mostly over the experiments and some other minor points. The reviewers feel like their concerns have all been addressed, and now agree on acceptance.
",Paper Decision
OyHf0auxT,rJePwgSYwB,SGD Learns One-Layer Networks in WGANs,Reject,"This article studies convergence of WGAN training using SGD and generators of the form $\phi(Ax)$, with results on convergence with polynomial time and sample complexity under the assumption that the target distribution can be expressed by this type of generator. This expands previous work that considered linear generators. An important point of discussion was the choice of the discriminator as a linear or quadratic function. The authors' responses clarified some of the initial criticism, and the scores improved slightly. Following the discussion, the reviewers agreed that the problem being studied is a difficult one and that the paper makes some important contributions. However, they still found that the considered settings are very restrictive, maintaining that quadratic discriminators would work only for the very simple type of generators and targets under consideration. Although the article makes important advances towards understanding convergence of WGAN training with nonlinear models, the relevance of the contribution could be greatly enhanced by addressing / discussing the plausibility or implications of the analysis in a practical setting, in the best case scenario addressing a more practical type of neural networks. ",Paper Decision
vuw3-o8vjE,r1gIwgSYwr,Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Leanring Beyond Global Prior,Reject,"This paper proposes PAC-Bayes bounds for meta-learning. The reviewers who are most knowledgeable about the subject and who read the paper most closely brought up several concerns regarding novelty (especially a description of how the proposed bounds relate to those in prior works (Pentina el al. (2014), Galanti et al. (2016) and Amit and Meir (2018))) and regarding clarity. The reviewers found theoretical analysis and proofs hard to follow. For these reasons, the paper isn't ready for publication at this time. See the reviewer's comments for details.",Paper Decision
_O9Ea5Bee,SJxSDxrKDr,Adversarial Training and Provable Defenses: Bridging the Gap,Accept (Talk),"The reviewers develop a novel technique for training neural networks that are provably robust to adversarial attacks, by combining provable defenses using convex relaxations with latent adversarial attacks that lie in the gap between the convex relaxation and the true realizable set of activations at a layer of the network. The authors show that the resulting procedure is computationally efficient and able to train neural networks to attain SOTA provable robustness to adversarial attacks.

The paper is well written and clearly explains an interesting idea, backed by thorough experiments. The reviewers were in consensus on acceptance and relatively minor concerns were clearly addressed in the rebuttal phase.

Hence, I strongly recommend acceptance.",Paper Decision
UF9ZlhlBAl,SyeHPgHFDr,Finding Deep Local Optima Using Network Pruning,Reject,"This paper provides empirical evidence on synthetic examples with a focus on understanding the relationship between the number of “good” local minima and number of irrelevant features. The reviewers find the problem discussed to be important. One of the reviewers has pointed out that the paper does not present deep insights and is more suitable for workshops. The authors did not provide a rebuttal, and it appears that the reviewers opinion has not changed.

The current score is clearly not sufficient to accept this paper in its current form. Due to this reason, I recommend to reject this paper. 
",Paper Decision
H8JxNMBdVe,S1ervgHFwS,Adversarial Training Generalizes Data-dependent Spectral Norm Regularization,Reject,"This paper shows an theoretical equivalence between the L2 PGD adversarial training and operator norm regularization. It gives an interesting observation and support it from both theoretical arguments and practical experiments. There has been a significant discussion between the reviewers and authors. Although the authors made efforts in rebuttal, it still leaves many places to improve and clarify, especially in improving the mathematical rigor of the  proof and experiments using state-of-the-art networks. 

",Paper Decision
Wk6jd2UkAV,H1lVvgHKDr,Knowledge Transfer via Student-Teacher Collaboration,Reject,"This paper has been assessed by three reviewers scoring it as follows: 6, 3, 8. The submission however attracted some criticism post-rebuttal from the reviewers e.g., why concatenating teacher to student is better than the use l2 loss or how the choice of transf. layers has been made (ad-hoc). Similarly, other major criticism includes lack of proper referencing to parts of work that have been in fact developed earlier in preceding papers. On balance, this paper falls short of the expectations of ICLR 2020, thus it cannot be accepted at this time. The authors are encouraged to work through major comments and resolve them for a future submission.",Paper Decision
oUCpuH5QEc,H1lNPxHKDH,A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case,Accept (Poster),"The article studies the set of functions expressed by a  network with bounded parameters in the limit of large width, relating the required norm to the norm of a transform of the target function, and extending previous work that addressed the univariate case. The article contains a number of observations and consequences. The reviewers were quite positive about this article. ",Paper Decision
A-Rv6ur7lh,rkxmPgrKwB,Weight-space symmetry in neural network loss landscapes revisited,Reject,"After communicating with each reviewer about the rebuttal, there seems to be a consensus that the paper contains a number of interesting ideas, but the motivation for the paper and the relationship to the literature needs to be expanded.  The reviewers have not changed their scores, and so there is not currently enough support to accept this paper.",Paper Decision
649iNz_YB,rJx7wlSYvB,Differentiable Bayesian Neural Network Inference for Data Streams,Reject,"The main contribution is a Bayesian neural net algorithm which saves computation at test time using a vector quantization approximation. The reviewers are on the fence about the paper. I find the exposition somewhat hard to follow. In terms of evaluation, they demonstrate similar performance to various BNN architectures which require Monte Carlo sampling. But there have been lots of BNN algorithms that don't require sampling (e.g. PBP, Bayesian dark knowledge, MacKay's delta approximation), so it seems important to compare to these. I think there may be promising ideas here, but the paper needs a bit more work before it is to be published at a venue such as ICLR.
",Paper Decision
JTjzaDPu5k,ByeMPlHKPH,Lite Transformer with Long-Short Range Attention,Accept (Poster),"This paper presents an efficient architecture of Transformer to facilitate implementations on mobile settings. The core idea is to decompose the self-attention layers to focus on local and global information separately. In the experiments on machine translation, it is shown to outperform baseline Transformer as well as the Evolved Transformer obtained by a costly architecture search. 
While all reviewers admitted the practical impact of the results in terms of engineering, the main concerns in the initial paper were the clarification of the mobile settings and scientific contributions. Through the discussion, reviewers are fairly satisfied with the authors’ response and are now all positive to the acceptance. Although we are still curious how it works on other tasks (as the title says “mobile applications”), I think the paper provides enough insights valuable to the community, so I’d like to recommend acceptance. 
",Paper Decision
dgedvlIOm,r1gfweBFPB,Learning by shaking: Computing policy gradients by physical forward-propagation,Reject,"While the reviewers generally appreciated the idea behind the method in the paper, there was considerable concern about the experimental evaluation, which did not provide a convincing demonstration that the method works in interesting and relevant problem settings, and did not compare adequately to alternative approach. As such, I believe this paper is not quite ready for publication in its current form.",Paper Decision
0RDHYAzwB4,HylfPgHYvr,Occlusion  resistant  learning  of  intuitive physics from videos,Reject,"The paper studies the problem of modeling inter-object dynamics with occlusions. It provides proof-of-concept demonstrations on toy 3d scenes that occlusions can be handled by structured representations using object-level segmentation masks and depth information. However, the technical novelty is not high and the requirement of such structured information seems impractical real-world applications which thus limits the significance of the proposed method.",Paper Decision
VAeW3Ppohl,B1eZweHFwr,Statistical Verification of General Perturbations by Gaussian Smoothing,Reject,"This paper proposes a smoothing-based certification against various forms of transformations, such as  rotations, translations. The reviewers have concerns on the novelty of the work and several technical issues. The authors have made efforts to address some of issues, but the work may still significantly benefit from a throughout improvement in both presentation and technical contribution.",Paper Decision
oJ4lESJm-a,SyegvgHtwr,Localised Generative Flows,Reject,"This paper proposes to overcome some fundamental limitations of normalizing flows by introducing auxiliary continuous latent variables. While the problem this paper is trying to address is mathematically legitimate, there is no strong evidence that this is a relevant problem in practice. Moreover, the proposed solution is not entirely novel, converting the flow in a latent-variable model. Overall, I believe this paper will be of minor relevance to the ICLR community.",Paper Decision
RISiI_bTQ6,HJxkvlBtwH,Certifying Neural Network Audio Classifiers,Reject,"The paper developed log abstract transformer, square abstract transformer and sigmoid-tanh abstract transformer to certifiy robustness of neural network models for audio. The work is interesting but the scope is limited. It presented a neural network certification methods for one particular type of audio classifiers that use MFCC as input features and LSTM as the neural network layers. This thus may have limited interest to the general readers. 

The paper targets to present an end-to-end solution to audio classifiers. Investigation on one particular type of audio classifier is far from sufficient. As the reviewers pointed out, there're large literature of work using raw waveform inputs systems. Also there're many state-of-the-art systems are HMM/DNN and attnetion based encoder-decoder models. In terms of neural network models, resent based models, transformer models etc are also important. A more thorough investigation/comparison would greatly enlarge the scope of this paper. ",Paper Decision
fTzxnkK3iD,SkeJPertPS,Collaborative Training of Balanced Random Forests for Open Set Domain Adaptation,Reject,"This paper proposes new target objectives for training random forests for better cross-domain generalizability. 

As reviewers mentioned, I think the idea of using random forests for domain adaptation is novel and interesting, while the proposed method has potential especially in the noisy settings. However, I think the paper can be much improved and is not ready to publish due to the following reviewers' comments:

- This paper is not well-written and has too many unclear parts in the experiments and method section. The results are not guaranteed to be reproducible given the content of the paper. Also, the organization of the paper could be improved.

- The open-set domain adaptation setting requires more elaboration. More carefully designed experiments should be presented. 

- It remains unclear how the feature extractors can be trained or fine-tuned in the DNN + tree architecture. Applying trees to high-dimensional features sacrifices the interpretability of the tree models, hampering the practical value of the approach.

Hence, I recommend rejection.",Paper Decision
Yg3ZN-5cc,HkgR8erKwB,PAC-Bayesian Neural Network Bounds,Reject,"This paper proposes PAC_Bayesian bounds for negative log-likelihood loss function. A few reviewers raised concerns around 1) distinguish their contributions better from prior work (eg Alquier). 2) confounders in their experiments. Both reviewers agreed that the paper, as it is written, does not provide sufficient evidence of significance. In addition, experiments shown in the paper varies two things - # parameters (therefore expressiveness and potential generalizability) and depth at each setting. As pointed out, this isn’t right - in order to capture the effect, one has to control for all confounders carefully. Another concerned raised were around Theorem 2 - that it contains data-distribution on the right hand side, which isn’t all that useful to calculate generalization bounds (we don’t have access to the distribution). We highly encourage authors to take another cycle of edits to better distinguish their work from others before future submissions. 
",Paper Decision
65qJOnY3zm,SyeRIgBYDB,Semi-Implicit Back Propagation,Reject,"The reviewers equivocally reject the paper, which is mostly experimental and the results of which are limited.  The authors do not react to the reviewers' comments.",Paper Decision
YsrnjBMa3,ByxaUgrFvH,Mutual Information Gradient Estimation for  Representation Learning,Accept (Poster),"This paper proposes the Mutual Information Gradient Estimator (MIGE) for estimating the gradient of the mutual information (MI), instead of calculating it directly. To build a tractable approximation to the gradient of MI, the authors make use of Stein's estimator followed by a random projection. The authors empirically evaluate the performance on representation learning tasks and show benefits over prior MI estimation methods.
The reviewers agree that the problem is important and challenging, and that the proposed approach is novel and principled. While there were some concerns about the empirical evaluation, most of the issues were addressed during the discussion phase. I will hence recommend acceptance of this paper. We ask the authors to update the manuscript as discussed.",Paper Decision
jfZlT8lP5L,HygTUxHKwH,Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning,Reject,"This paper proposes a method to reduce the instability issues of off-policy deep reinforcement learning.  The proposed solution constructs a simple MDP from the experience in the agent's replay memory.  This graph is used to compute a lower bound for the values from the original problem. Incorporating this bound can make the learning system less prone to soft divergence.

The reviewers appreciated the motivation of the paper and the direction of this research.  However, the reviewers were not convinced that the formulation was sufficiently complete.  There were concerns that the method makes additional assumptions about the data distribution (the presence of state aggregation and the absence of repeated states in continuous spaces).  Reviewers found related work was missing.  The reviewers also found multiple aspects of the presentation unclear even after the author response.  

This paper is not ready for publication as the generality of the proposed method was not sufficiently clear to the reviewers after the author response.",Paper Decision
O0qn9S8g0n,Bkl2UlrFwr,Iterative Deep Graph Learning for Graph Neural Networks,Reject,"The submission proposes a method for learning a graph structure and node embeddings through an iterative process. Smoothness and sparsity are both optimized in this approach. The iterative method has a stopping mechanism based on distance from a ground truth. 

The concerns of the reviewers were about scalability and novelty. Since other methods have used the same costs for optimization, as well as other aspects of this approach, there is little contribution other than the iterative process. The improvement over LDS, the most similar approach, is relatively minor. 

Although the paper is promising, more work is required to establish the contributions of the method. Recommendation is for rejection.
",Paper Decision
nnqOsjlyKL,BJxnIxSKDr,Mint: Matrix-Interleaving for Multi-Task Learning,Reject,"Reviewers put this paper in the lower half and question the theoretical motivation and the experimental design. On the other hand, this seems like an alternative general framework for solving large-scale multi-task learning problems. In the future, I would encourage the authors to evaluate on multi-task benchmarks such as SuperGLUE, decaNLP and C4. Note: It seems there's more similarities with Ruder et al. (2019) [0] than the paper suggests. 

[0] https://arxiv.org/abs/1705.08142",Paper Decision
oNJhPl8LlN,Byl28eBtwH,Learning Cluster Structured Sparsity by Reweighting,Reject,The paper is proposed a rejection based on majority reviews.,Paper Decision
mRjBB3rwc3,B1liIlBKvS,Selfish Emergent Communication,Reject,"There has been a long discussion on the paper, especially between the authors and the 2nd reviewer. While the authors' comments and paper modifications have improved the paper, the overall opinion on this paper is that it is below par in its current form. The main issue is that the significance of the results is insufficiently clear.  While the sender-receiver game introduced is interesting, a more thorough investigation would improve the paper a lot (for example, by looking if theoretical statements can be made).",Paper Decision
-5p4ADkiR,BkljIlHtvS,Decoupling Adaptation from Modeling with Meta-Optimizers for Meta Learning,Reject,"This paper presents a number of experiments involving the Model-Agnostic Meta-Learning (MAML) framework, both for the purpose of understanding its behavior and motivating specific enhancements.  With respect to the former, the paper argues that deeper networks allow earlier layers to learn generic modeling features that can be adapted via later layers in a task-specific way.  The paper then suggests that this implicit decomposition can be explicitly formulated via the use of meta-optimizers for handling adaptations, allowing for simpler networks that may not require generic modeling-specific layers.

At the end of the rebuttal and discussion phases, two reviewers chose rejection while one preferred acceptance.  In this regard, as AC I did not find clear evidence that warranted overriding the reviewer majority, and consistent with some of the evaluations, I believe that there are several points whereby this paper could be improved.

More specifically, my feeling is that some of the conclusions of this paper would either already be expected by members of the community, or else would require further empirical support to draw more firm conclusions.  For example, the fact that earlier layers encode more generic features that are not adapted for each task is not at all surprising (such low-level features are natural to be shared).  Moreover, when the linear model from Section 3.2 is replaced by a deep linear network, clearly the model capacity is not changed, but the effective number of parameters which determine the gradient update will be significantly expanded in a seemingly non-trivial way.  This is then likely to be of some benefit.

Consequently, one could naturally view the extra parameters as forming an implicit meta-optimizer, and it is not so remarkable that other trainable meta-optimizers might work well.  Indeed cited references such as (Park & Oliva, 2019) have already applied explicit meta-optimizers to MAML and few-shot learning tasks.  And based on Table 2, the proposed factorized meta-optimizer does not appear to show any clear advantage over the meta-curvature method from (Park & Oliva, 2019).  Overall, either by using deeper networks or an explicit trainable meta-optimizer, there are going to be more adaptable parameters to exploit and so the expectation is that there will be room for improvement.  Even so, I am not against the message of this paper.  Rather it is just that for an empirically-based submission with close ties to existing work, the bar is generally a bit higher in terms of the quality and scope of the experiments.

As a final (lesser) point, the paper argues that meta-optimizers allow for the decomposition of modeling and adaptation as mentioned above; however, I did not see exactly where this claim was precisely corroborated empirically.  For example, one useful test could be to recreate Figure 2 but with the meta-optimizer in place and a shallower network architecture. The expectation then might be that general features are no longer necessary.",Paper Decision
xnDRHE1izH,Bkg5LgrYwS,"Imitation Learning of Robot Policies using Language, Vision and Motion",Reject,"The present paper addresses the problem of imitation learning in multi-modal settings, combining vision, language and motion. The proposed approach learns an abstract task representation, and the goal is to use this as a basis for generalization. This paper was subject to considerable discussion, and the authors clarified several issues that reviewers raised during the rebuttal phase. Overall, the empirical study presented in the paper remains limited, for example in terms of ablations (which components of the proposed model have what effect on performance) and placement in the context of prior work. As a result, the depth of insights is not yet sufficient for publication.",Paper Decision
f-7L8Z9zZ,HkxcUxrFPS,Improving Visual Relation Detection using Depth Maps,Reject,"The paper proposes to improve visual relation prediction by using depth maps.  Since existing RGB images do not contain depth informations, the authors use a monocular depth estimation method to predict depth maps.  The authors show that using depths maps, they are able to improve prediction of relations between ground truth object bounding boxes and labels.  

The paper got relatively low scores (with 3 initial weak rejects).  After the revision and suggested improvements, one of the reviewers updated their score so the paper now has 2 weak rejects and 1 weak accept. 

The paper had the following weaknesses:
1. The paper has limited technical novelty as it combines off the shelf components.  The components also used different backbones (ResNet at some places, VGGNet at others) that were directly from prior work.  Was there any attempt to have an unified architecture? As the main novelty of the work is not in the model aspect, the paper needs to have stronger experiments and analysis.
2. More analysis on the quality of the depth estimation is needed.  Ideally, the work should provide some insight into whether some of the errors is due to having bad depth estimation?  The depth estimation method used is from 2016, there are newer depth estimation methods now.  Would having better depth estimation give improved results?  Experiments that illustrates that method works well with predicted bounding boxes instead of ground truth bounding boxes will also strengthen the paper.  
3. There was the question of whether the related Yang et al. 2018 workshop paper should be included as basis for comparison.  In the AC's opinion, Yang et al. 2018 is not concurrent work and should be treated as prior work.  However, it is not clear whether it is feasible to compare against that work.  The authors should attempt to do so and if infeasible, clearly articulate why that is the case.
4. As pointed out by R3, once there is a depth map available, it is also possible to compare against 3D methods (such as those that operate on point clouds)

Overall the paper had a nice insight by proposing the simple but effective idea of using depth information to help with visual relation prediction.  Still the work is somewhat borderline in quality.  In the AC's opinion, the main contribution and insight of the paper is of limited interest to the ICLR community, and it would be more appreciated in a computer vision conference.  The authors are encouraged to improve the paper with stronger experiments and analysis, incorporate various suggestions from the reviewers, and resubmit to a vision conference.
",Paper Decision
p72TsB7FtJ,S1et8gBKwH,Semi-supervised Pose Estimation with Geometric Latent Representations,Reject,"This paper addresses the problem of rotation estimation in 2D images. The method attempted to reduce the labeling need by learning in a semi-supervised fashion. The approach learns a VAE where the latent code is be factored into the latent vector and the object rotation. 

All reviewers agreed that this paper is not ready for acceptance. The reviewers did express promise in the direction of this work. However, there were a few main concerns. First, the focus on 2D instead of 3D orientation. The general consensus was that 3D would be more pertinent use case and that extension of the proposed approach from 2D to 3D is likely non-trivial. The second issue is that minimal technical novelty. The reviewers argue that the proposed solution is a combination of existing techniques to a new problem area. 

Since the work does not have sufficient technical novelty to compare against other disentanglement works and is being applied to a less relevant experimental setting, the AC does not recommend acceptance. 
",Paper Decision
IBVseua3oI,HklFUlBKPB,Identifying Weights and Architectures of Unknown ReLU Networks,Reject,"This article studies the identifiability of architecture and weights of a ReLU network from the values of the computed functions, and presents an algorithm to do this. This is a very interesting problem with diverse implications. The reviewers raised concerns about the completeness of various parts of the proposed algorithm and the complexity analysis, some of which were addressed in the author's response. Another concern raised was that the experiments were limited to small networks, with a proof of concept on more realistic networks missing. The revision added experiments with MNIST. Other concerns (which in my opinion could be studied separately) include possible limitations of the approach to networks with no shared weights nor pooling. The reviewers agree that the article concerns an interesting topic that has not been studied in much detail yet. Still, the article would benefit from a more transparent presentation of the algorithm and theoretical analysis, as well as more extensive experiments. ",Paper Decision
FWG9OWa4rJ,S1lF8xHYwS,Unsupervised Domain Adaptation through Self-Supervision,Reject,"Thanks for your detailed replies to the reviewers, which helped us a lot to clarify several issues.
Although the paper discusses an interesting topic and contains potentially interesting idea, its novelty is limited.
Given the high competition of ICLR2020, this paper is still below the bar unfortunately.",Paper Decision
Fz0A5YUrRf,H1lOUeSFvB,Improving Gradient Estimation in Evolutionary Strategies With Past Descent Directions,Reject,"The authors propose a novel approach to using surrogate gradient information in ES. Unlike previous approaches, their method always finds a descent direction that is better than the surrogate gradient. This allows them to use previous gradient estimates as the surrogate gradient. They prove results for the linear case and under simplifying assumptions that it extends beyond the linear case. Finally, they evaluate on MNIST and RL tasks and show improvements over ES.

After the revisions, reviewers were concerned about: 
* The strong (and potentially unrealistic) assumptions for the theorems. They felt that these assumptions trivialized the theorems.
* Limited experiments demonstrating advantages in situations where other more effective methods could be used. The performance on the RL tasks shows small gains compared to a vanilla ES approach. Thus, the usefulness of the approach is not clearly demonstrated. 

I think that the paper has the potential to be a strong submission if the authors can extend their experiments to more complex problems and demonstrate gains. At this time however, I recommend rejection.",Paper Decision
MdDlIalKA,BJlPLlrFvH,Variable Complexity in the Univariate and Multivariate Structural Causal Model,Reject,"The author response and revisions to the manuscript motivated two reviewers to increase their scores to weak accept. While these revisions increased the quality of the work, the overall assessment is just shy of the threshold for inclusion.",Paper Decision
oV9WXWF198,rygwLgrYPB,Regularizing activations in neural networks via distribution matching with the Wasserstein metric,Accept (Poster),This paper presents an interesting and novel idea that is likely to be of interest to the community. The most negative reviewer did not acknowledge the author response. The AC recommends acceptance.,Paper Decision
SLyRt7Anr,SJeLIgBKPS,Gradient Descent Maximizes the Margin of Homogeneous Neural Networks,Accept (Talk),"This paper studies the implicit regularization of the gradient descent in homogeneous and shows that when the training loss falls below a threshold, then the smoothed. This study generalizes some of the earlier related works by relying on weaker assumptions. Experiments on MNIST and CIFAR-10 are provided to backup the theoretical findings of the paper. 
R2 had some concern about one of the assumptions in this work (A4). While authors admitted that (A4) may not hold for all neural networks and all datasets, they stressed that this assumptions is reasonable when the network is overparameterized and can perfectly fit the training data. Overall, all reviewers are very positive about this submission and find a valuable step toward understanding implicit regularization.
",Paper Decision
Hz286cLG6d,HJe88xBKPr,Mixed Precision Training With 8-bit Floating Point,Reject,"This paper propose a method to train DNNs using 8-bit floating point numbers, by using an enhanced loss scaling method and stochastic rounding method. However, the proposed method lacks novel and both the paper presentation and experiments need to be improved throughout. ",Paper Decision
f3lzF5O0Hh,SygBIxSFDS,An Empirical and Comparative Analysis of Data Valuation with Scalable Algorithms,Reject,"There is insufficient support to recommend accepting this paper.  The authors provided detailed responses to the reviewer comments, but the reviewers did not raise their evaluation of the significance and novelty of the contributions as a result.  The feedback provided should help the authors improve their paper.",Paper Decision
K2Ul_Iq2kQ,SygSLlStwS,Consistent Meta-Reinforcement Learning via Model Identification and Experience Relabeling,Reject,"The authors propose an algorithm for meta-rl which reduces the problem to one of model identification. The main idea is to meta-train a fast-adapting model of the environment and a shared policy, both conditioned on task-specific context variables. At meta-testing, only the model is adapted using environment data, while the policy simply requires simulated experience. Finally, the authors show experimentally that this procedure better generalizes to out-of-distribution tasks than similar methods.

The reviewers agree that the paper has a few significant shortcomings. It's unclear how hyper-parameters are selected in the experimental section; the algorithm does not allow for continual adaptation; all policy learning is done through data relabelled by the model. 

Overall, the problem the paper addresses is very important, but we do not deem the paper publishable in its current form.",Paper Decision
FzBfpIGEnK,S1gEIerYwH,Transferring Optimality Across Data Distributions via Homotopy Methods,Accept (Poster),"This paper presents a theoretically motivated method based on homotopy continuation for transfer learning and demonstrates encouraging results on FashionMNIST and CIFAR-10. The authors draw a connection between this approach and the widely used fine-tuning heuristic. Reviewers find principled approaches to transfer learning in deep neural networks an important direction, and find the contributions of this paper an encouraging step in that direction. Alongside with the reviewers, I think homotopy continuation is a great numerical tool with a lot of untapped potentials for ML applications, and I am happy to see an instantiation of this approach for transfer learning. Reviewers had some concerns about experimental evaluations (reporting test performance in addition to training), and the writing of the draft. The authors addressed these in the revised version by including test performance in the appendix and rewriting the first parts of the paper. Two out of three reviewers recommend accept. I also find the homotopy analysis interesting and alongside with majority of reviewers, recommend accept. However, please try to iterate at least once more over the writing; simply long sentences and make sure the writing and flow are, for the camera ready version.",Paper Decision
xIu5RwzH6s,SJxE8erKDH,Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings,Accept (Poster),"This paper addresses the problem of many-to-many cross-domain mapping tasks with a double variational auto-encoder architecture, making use of the normalizing flow-based priors.

Reviewers and AC unanimously agree that it is a well written paper with a solid approach to a complicated real problem supported by good experimental results. There are still some concerns with confusing notations, and with human study to further validate their approach, which should be addressed in a future version.

I recommend acceptance.",Paper Decision
HCVhUOGSE,SJem8lSFwB,Dynamic Model Pruning with Feedback,Accept (Poster),"The paper proposes a new, simple method for sparsifying deep neural networks.                                                      
It use as temporary, pruned model to improve pruning masks via SGD, and eventually                                                 
applying the SGD steps to the dense model.                                                                                         
The paper is well written and shows SOTA results compared to prior work.                                                           
                                                                                                                                   
The authors unanimously recommend to accept this work, based on simplicity of                                                      
the proposed method and experimental results.                                                                                      
                                                                                                                                   
I recommend to accept this paper, it seems to make a simple, yet effective                                                         
contribution to compressing large-scale models.                                                                                    
                             ",Paper Decision
34y-FdHzqx,H1lQIgrFDS,$\ell_1$ Adversarial Robustness Certificates: a Randomized Smoothing Approach,Reject,"After reading the author's response, all the reviewers agree that this paper is an incremental work. The presentation need to be polished before publish.",Paper Decision
vOTurDFmk,rJxGLlBtwH,On the interaction between supervision and self-play in emergent communication,Accept (Poster),"This paper investigates how two means of learning natural language - supervised learning from labeled data and reward-maximizing self-play - can be combined. The paper empirically investigates this question, showing in two grounded visual language games that supervision followed by self-play works better than the reverse. 

The reviewers found this paper interesting and well executed, though not especially novel. The last is a reasonable criticism but in this case I think a little beside the point. In any case, since all the reviewers are in agreement I recommend acceptance.",Paper Decision
DNRVVjHdjt,rklfIeSFwS,CNAS: Channel-Level Neural Architecture Search,Reject,"This paper proposes a channel pruning approach based one-shot neural architecture search (NAS). As agreed by all reviewers, it has limited novelty, and the method can be viewed as a straightforward combination of NAS and pruning. Experimental results are not convincing. The proposed method is not better than STOA on the accuracy or number of parameters. The setup is not fair, as the proposed method uses autoaugment while the other baselines do not. The authors should also compare with related methods such as Bayesnas, and other pruning techniques. Finally, the paper is poorly written, and many related works are missing.",Paper Decision
PEIapz3c80,SkgWIxSFvr,FLAT MANIFOLD VAES,Reject,"The paper proposes to regularize the decoder of the VAE to have a flat pull-back metric, with the goal of making Euclidean distances in the latent space correspond to geodesic distances. This, in turn, results in faster geodesic distance computation. I share the concern of R2 that this regularization towards a flat metric could result in ""biased"" geodesic distances in regions where data is scarce. I suggest the authors discuss in the next version of the paper if there are situations where this regularization might have drawbacks and if possible, conduct experiments (perhaps on toy data) to either rule out or highlight these points, particularly about scarce data regions. ",Paper Decision
zYdRZ5AMXn,ryxWIgBFPS,A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms,Accept (Poster),"This paper proposes to discover causal mechanisms through meta-learning, and suggests an approach for doing so. The reviewers raised concerns about the key hypothesis (that the right causal model implies higher expected online likelihood) not being sufficiently backed up through theory or through experiments on real data. The authors pointed to a recent paper that builds upon this work and tests on a more realistic problem setting. However, the newer paper measures not the online likelihood of adaptation, but just the training error during adaptation, suggesting that the approach in this paper may be worse. Despite the concerns, the reviewers generally agreed that the paper included novel and interesting ideas, and addressed a number of the reviewers' other concerns about the clarity, references, and experiments. Hence, it makes a worthwhile contribution to ICLR.",Paper Decision
lqmh0Ir-dV,ByglLlHFDS,Expected Information Maximization: Using the I-Projection for Mixture Density Estimation,Accept (Poster),"The paper proposes a new algorithm called Expected Information Maximization (EIM) for learning latent variable models while computing the I-projection solely based on samples. The reviewers had several questions, which the authors sufficiently answered. The reviewers agree that the paper should be accepted. The authors should carefully read the reviewer questions and comments and use them to improve their final manuscript. ",Paper Decision
DWWN_6LZOi,HJlyLgrFvB,All Simulations Are Not Equal: Simulation Reweighing for Imperfect Information Games,Reject,"A method is introduced to estimate the hidden state in imperfect information in multiplayer games, in particular Bridge. This is interesting, but the paper falls short in various ways. Several reviewers complained about the readability of the paper, and also about the quality and presentation of the interesting results. 

It seems that this paper represents an interesting idea, but is not yet ready for publication.",Paper Decision
Yvguv1SCw,HyxyIgHFvr,Truth or backpropaganda? An empirical investigation of deep learning theory,Accept (Spotlight),"The authors take a closer look at widely held beliefs about neural networks. Using a mix of analysis and experiment, they shed some light on the ways these assumptions break down. The paper contributes to our understanding of various phenomena and their connection to generalization, and should be a useful paper for theoreticians searching for predictive theories.",Paper Decision
xlZALUMl5D,BJxAHgSYDB,Learning to Rank Learning Curves,Reject,"Authors propose a new way of early stopping for neural architecture search. In contrast to making keep or kill decisions based on extrapolating the learning curves then making decisions between alternatives, this work learns a model on pairwise comparisons between learning curves directly. Reviewers were concerned with over-claiming of novelty since the original version of this paper overlooked significant hyperparameter tuning works. In a revision, additional experiments were performed using some of the suggested methods but reviewers remained skeptical that the empirical experiments provided enough justification that this work was ready for prime time.   ",Paper Decision
YGt9E1n9u,ByxCrerKvS,Set Functions for Time Series,Reject,"The paper investigates a new approach to classification of irregularly sampled and unaligned multi-modal time series via set function mapping. Experiment results on health care datasets are reported to demonstrate the effectiveness of the proposed approach. 

The idea of extending set functions to address missing value in time series is interesting and novel. The paper does a good job at motivating the methods and describing the proposed solution. The authors did a good job at addressing the concerns of the reviewers. 

During the discussion, some reviewers are still concerned about the empirical results, which do not match well with published results (even though the authors provided an explanation for it). In addition, the proposed method is only tested on the health care datasets, but the improvement is limited. Therefore it would be worthwhile investigating other time series datasets, and most important answering the important question in terms of what datasets/applications the proposed method works well. 

The paper is one step away for being a strong publication. We hope the reviews can help improve the paper for a strong publication in the future. ",Paper Decision
sjPYc09Tek,SylpBgrKPH,MissDeepCausal: causal inference from incomplete data using deep latent variable models,Reject,"This paper addresses the problem of causal inference from incomplete data. The main idea is to use a latent confounders through a VAE. A multiple imputation strategy is then used to account for missing values. Reviewers have mixed responses to this paper. Initially, the scores were 8,6,3. After discussion the reviewer who rated is 8 reduced their score to 6, but at the same time the score of 3 went up to 6. The reviewers agree that the problem tackled in the paper is difficult, and also acknowledge that the rebuttal of the paper was reasonable and honest. The authors added a simulation study which shows good results.

The main argument towards rejection is that the paper does not beat the state of the art. I do think that this is still ok if the paper brings useful insights for the community even though it does not beat the state fo the art. For now, with the current score, the paper does not make the cut. For this reason, I recommend to reject the paper, but I encourage the authors to resubmit this to another venue after improving the paper.",Paper Decision
Q0Ah0bdfQg,H1e3HlSFDr,Variational Constrained Reinforcement Learning with Application to Planning at Roundabout,Reject,"This paper proposes to add constraints to the RL problem within a variational method. The hope is to specify a safe vs non-safe states. The reviewers were not convinced that this paper makes the cut for ICLR. Moreover, there was no rebuttal from the authors, so it didn't give the reviewer a chance to reconsider their opinion. Based on the current ratings, I recommend to reject this paper. ",Paper Decision
kWmxSk0YTw,Byl3HxBFwH,Efficient Deep Representation Learning by Adaptive Latent Space Sampling,Reject,"VAE-based sample selection for training NNs.  A well-written experimental paper that is demonstrated through a number of experiments, all of which are minimal and from which generalization is not per se expected.  The absence of an underlying theory, and the absence of rigorous experimentation makes me request to extend either or, better, both.  ",Paper Decision
EBa9tvs6Aw,r1nSxrKPH,Learning Functionally Decomposed Hierarchies for Continuous Navigation Tasks,Reject,"The submission proposes a complex, hierarchical architecture for continuous control RL that combines Hindsight Experience Replay, vision-based planning with privileged information, and low-level control policy learning. The authors demonstrate that the approach can achieve transfer of the different control levels between different bodies in a single environment.

The reviewers were initially all negative, but 2 were persuaded towards weak acceptance by the improvements to the paper and the authors' rebuttal. The discussion focused on remaining limitations: the use of a single maze environment for evaluation, as well as whether the baselines were fair (HAC in particular). After reading the paper, I believe that these limitations are substantial. In particular, this is not a general approach and its relevance is severely limited unless the authors demonstrate that it will work as well in a more general control setting, which is in their future work already. 

Thus I recommend rejection at this time.",Paper Decision
Csq5l8wPh,rygjHxrYDB,Deep Audio Priors Emerge From Harmonic Convolutional Networks,Accept (Poster),"This paper introduces a new convolution-like operation, called a Harmonic Convolution (weighted combination of dilated convolutions with different dilation factors/anchors), which operates on the STFT of an audio signal. Experiments are carried on audio denoising tasks and sound separation and seems convincing, but could have been more convincing: (i) with different types of noises for the denoising task (ii) comparison with more methods for sound separation. Apart those two concerns, the authors seem to have addressed most of reviewers' complaints.
",Paper Decision
7OVzVpDj71,BJxsrgStvr,Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks,Accept (Spotlight),"This work studies small but critical subnetworks, called winning tickets, that have very similar performance to an entire network, even with much less training. They show how to identify these early in the training of the entire network, saving computation and time in identifying them and then overall for the prediction task as a whole. 

The reviewers agree this paper is well-presented and of general interest to the community. Therefore, we recommend that the paper be accepted.",Paper Decision
3vFfxhvrS,SygcSlHFvS,On Understanding Knowledge Graph Representation,Reject,"The paper proposes a set of conditions that enable a mapping from word embeddings to relation embeddings in knowledge graphs. Then, using recent results about pointwise mutual information word embeddings, the paper provides insights to the latent space of relations, enabling a categorization of relations of entities in a knowledge graph. Empirical experiments on recent knowledge graph models (TransE, DistMult, TuckER and MuRE) are interpreted in light of the predictions coming from the proposed set of conditions.

The authors responded to reviewer comments well, providing significant updates during the discussion period. Unfortunately, the reviewers did not engage further after their original reviews, and so it is hard to tell whether they agreed that the changes resolved all their questions.

Overall, the paper provides much needed analysis for understanding of the latent space of relations on knowledge graphs. Unfortunately, the original submission did not clearly present the ideas, and it is unclear whether the updated version addresses all the concerns. The paper in its current state is therefore not yet suitable for publication at ICLR.",Paper Decision
r5Iy5tlmjt,Hkg9HgBYwH,Encoding Musical Style with Transformer Autoencoders,Reject,"Main content:

Blind review #3 summarizes it well:

This paper presents a technique for encoding the high level “style” of pieces of symbolic music. The music is represented as a variant of the MIDI format. The main strategy is to condition a Music Transformer architecture on this global “style embedding”.  Additionally, the Music Transformer model is also conditioned on a combination of both “style” and “melody” embeddings to try and generate music “similar” to the conditioning melody but in the style of the performance embedding. 

--

Discussion:

The reviewers questioned the novelty. Blind review #2 wrote: ""Overall, I think the paper presents an interesting application and parts of it are well written, however I have concerns with the technical presentation in parts of the paper and some of the methodology. Firstly, I think the algorithmic novelty in the paper is fairly limited. The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et. al. 2019b). However, the limited algorithmic novelty is not the main concern. The authors also mention an internal dataset of music audio and transcriptions, which can be a major contribution to the music information retrieval (MIR) community. However it is not clear if this dataset will be publicly released or is only for internal experiments.""

However, after revision, the same reviewer has upgraded the review to a weak accept, as the authors wrote ""We emphasize that our goal is to provide users with more fine-grained control over the outputs generated by a seq2seq language model. Despite its simplicity, our method is able to learn a global representation of style for a Transformer, which to the best of our knowledge is a novel contribution for music generation. Additionally, we can synthesize an arbitrary melody into the style of another performance, and we demonstrate the effectiveness of our results both quantitatively (metrics) and qualitatively (interpolations, samples, and user listening studies).""

--

Recommendation and justification:

This paper is borderline for the reasons above, and due to the large number of strong papers, is not accepted at this time. As one comment, this work might actually be more suitable for a more specialized conference like ISMIR, as its novel contribution is more to music applications than to fundamental machine learning approaches.",Paper Decision
ewQScuPBCY,BkeYSlrYwH,Collaborative Inter-agent Knowledge Distillation for Reinforcement Learning,Reject,"The paper introduces an ensemble of RL agents that share knowledge amongst themselves. Because there are no theoretical results, the experiments have to carry the paper.  The reviewers had rather different views on the significance of these experiments and whether they are sufficient to convincingly validate the learning framework introduced. Overall, because of the high bar for ICLR acceptance, this paper falls just below the threshold. 
",Paper Decision
Sx0-BssqxN,HJeYSxHFDS,Gauge Equivariant Spherical CNNs,Reject,"The paper extends Gauge invariant CNNs to Gauge invariant spherical CNNs.  The authors significantly improved both theory and experiments during the rebuttal and the paper is well presented. However, the topic is somewhat niche, and the bar for ICLR this year was very high, so unfortunately this paper did not make it. We encourage the authors to resubmit the work including the new results obtained during the rebuttal period.",Paper Decision
gbRYb-R4IC,ryxOBgBFPH,Preventing Imitation Learning with Adversarial Policy Ensembles,Reject,"Although the reviewers appreciated the novelty of this work, they unanimously recommended rejection.  The current version of the paper exhibits weak presentation quality and lacks sufficient technical depth.  The experimental evaluation was not found to be sufficiently convincing by any of the reviewers.  The submitted comments should help the authors improve their paper.",Paper Decision
SlKuUi-3Q,S1evHerYPr,Improving Generalization in Meta Reinforcement Learning using Learned Objectives,Accept (Spotlight),"This paper proposes a meta-RL algorithm that learns an objective function whose gradients can be used to efficiently train a learner on entirely new tasks from those seen during meta-training. Building off-policy gradient-based meta-RL methods is challenging, and had not been previously demonstrated. Further, the demonstrated generalization capabilities are a substantial improvement in capabilities over prior meta-learning methods. There are a couple related works that are quite relevant (and somewhat similar in methodology) and overlooked -- see [1,2]. Further, we strongly encourage the authors to run the method on multiple meta-training environments and to report results with more seeds, as promised. The contributions are significant and should be seen by the ICLR community. Hence, I recommend an oral presentation.

[1] Yu et al. One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning
[2] Sung et al. Meta-critic networks",Paper Decision
yJ_4TI698N,rkevSgrtPr,A closer look at the approximation capabilities of neural networks,Accept (Poster),"This is a nice paper on the classical problem of universal approximation, but giving a direct proof with good approximation rates, and providing many refinements and ties to the literature.

If possible, I urge the authors to revise the paper further for camera ready; there are various technical oversights (e.g., 1/lambda should appear in the approximation rates in theorem 3.1), and the proof of theorem 3.1 is an uninterrupted 2.5 page block (splitting it into lemmas would make it cleaner, and also those lemmas could be useful to other authors).",Paper Decision
SUTjQkAw4y,HJl8SgHtwr,VIMPNN: A physics informed neural network for estimating potential energies of out-of-equilibrium systems,Reject,"The paper considers the problem of estimating the electronic structure's ground state energy of a given atomic system by means of supervised machine learning, as a fast alternative to conventional explicit methods (DFT). For this purpose, it modifies the neural message-passing architecture to account for further physical properties, and it extends the empirical validation to also include unstable molecules. 

Reviewers acknowledged the valuable experimental setup of this work and the significance of the results in the application domain, but were generally skeptical about the novelty of the machine learning model under study. Ultimately, and given that the main focus of this conference is on Machine Learning methodology, this AC believes this work could be more suitable in a more specialized venue in computational/quantum chemistry. ",Paper Decision
csuTbCCKZU,S1gLBgBtDH,SLM Lab: A Comprehensive Benchmark and Modular Software Framework for Reproducible Deep Reinforcement Learning,Reject,"A new software framework fo Deep RL is introduced. This is a useful work for the community, but it is not a research work. I agree with Reviewer4 that somehow it is not a right venue: other papers need to have technical contributions, SOTA, and here - it is difficult but it is another type of work - accurate technical implementation and commenting. I do not feel right to have as it a paper on ICLR. ",Paper Decision
5E4qNbTPl2,rJerHlrYwH,Data-Efficient Image Recognition with Contrastive Predictive Coding,Reject,"The paper tackles the key question of achieving high prediction performances with few labels. The proposed approach builds upon Contrastive Predictive Coding (van den Oord et al. 2018). The contribution lies in i) refining CPC along several axes including model capacity, directional predictions, patch-based augmentation; ii) showing that the refined representation learned by the called CPC.v2 supports an efficient classification in a few-label regime, and can be transferred to another dataset; iii) showing that the auxiliary losses involved in the CPC are not necessarily predictive of the eventual performance of the network.

This paper generated a hot discussion. Reviewers were not convinced that the paper contributions are sufficiently innovative to deserve being published at ICLR. Authors argued that novelty does not have to lie in equations, and that the new ideas and evidence presented are worth. 

The area chair thinks that the paper raises profound questions (e.g., what auxiliary losses are most conducive to learning a good representation; how to divide the computational efforts among the preliminary phase of representation learning and the later phase of classifier learning), but given the number of options and details involved, these results may support several interpretations besides the authors'. 

The authors might also want to leave the claim about the generality of the CPC++ principles (e.g., regarding audio) for further work - or to bring additional evidence backing up this claim. 

In conclusion, this paper contains brilliant ideas and I hope to see them published with a strengthened analysis of its components. ",Paper Decision
Qn5FLwxngp,BkgrBgSYDS,"Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps",Accept (Spotlight),The paper generalizes several existing results for structured linear transformations in the form of K-matrices. This is an excellent paper and all reviewers confirmed that.,Paper Decision
5CFK0e5CiS,BJx4rerFwB,wMAN: WEAKLY-SUPERVISED MOMENT ALIGNMENT NETWORK FOR TEXT-BASED VIDEO SEGMENT RETRIEVAL,Reject,"This paper proposes a method for aligning an input text with the frames in a video that correspond to what the text describes in a weakly supervised way. The main technical contribution of the paper is the use of co-attention at different abstraction levels.

Among the four reviewers, one reviewer advocates for the paper while the others find this paper to be a borderline reject paper. Reviewer3 who was initially positive about the paper, during the discussion period, expressed that he/she wants to downgrade his/her rating to weak reject after reading the other reviewers' comments and concerns. The main concern of the reviewers is that the contribution of the paper incremental, particularly since the idea of co-attention has been used in many different area in other context. The authors responded to this in the rebuttal that the proposed approach incorporate different components such as Positional Encodings and is different from prior work, and that they experimentally perform superior compared to other co-attention usages such as LCGN. Although the AC understands the authors response, the majority of the reviewers are still not fully convinced about the contribution and their opinion stay opposed to the paper.",Paper Decision
ZrhQoS-k2,B1l4SgHKDH,Residual Energy-Based Models for Text Generation,Accept (Poster),"This paper proposes a Residual Energy-based Model for text generation.

After rebuttal and discussion, the reviewers all converged on a vote to accept, citing novelty and interestingness of the approach.

Authors are encouraged to revise to address reviewer comments.",Paper Decision
_LwcM0laOv,BylQSxHFwr,AtomNAS: Fine-Grained End-to-End Neural Architecture Search,Accept (Poster),"Reviewer #1 noted that he wishes to change his review to weak accept post rebuttal, but did not change his score in the system.  Presuming his score is weak accept, then all reviewers are unanimous for acceptance.  I have reviewed the paper and find the results appear to be clear, but the magnitude of the improvement is modest.  I concur with the weak accept recommendation. ",Paper Decision
Y-wmMPBIB,S1gmrxHFvB,AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty,Accept (Poster),"This paper tackles the problem of learning under data shift, i.e. when the training and testing distributions are different. The authors propose an approach to improve robustness and uncertainty of image classifiers in this situation. The technique uses synthetic samples created by mixing multiple augmented images, in addition to a Jensen-Shannon Divergence consistency loss. Its evaluation is entirely based on experimental evidence.

The method is simple, easy to implement, and effective. Though this is a purely empirical paper, the experiments are extensive and convincing. 

In the end, the reviewers didn't show any objections against this paper. I therefore recommend acceptance.",Paper Decision
3tHB8yvUa0,BygMreSYPB,Learning Latent Dynamics for Partially-Observed Chaotic Systems,Reject,"This paper presents an ODE-based latent variable model, argues that extra unobserved dimensions are necessary in general, and that deterministic encodings are also insufficient in general.  Instead, they optimize the latent representation during training.  They include small-scale experiments showing that their framework beats alternatives.

In my mind, the argument about fixed mappings being inadequate is a fair one, but it misses the fact that the variational inference framework already has several ways to address this shortcoming:
1) The recognition network outputs a distribution over latent values, which in itself does not address this issue, but provides regularization benefits.
2) The recognition network is just a strategy for speeding up inference.  There's no reason you can't just do variational inference or MCMC for inference instead (which is similar to your approach), or do semi-amortized variational inference.

Basically, this paper could have been somewhat convincing as a general exploration of approximate inference strategies in the latent ODE model.  Instead, it provides a lot of philosophical arguments and a small amount of empirical evidence that a particular encoder is insufficient when doing MAP inference.  It also seems like a problem that hyperparameters were copied from Chen et al 2018, but are used in a MAP setting instead of a VAE setting.  Finally, it's not clear how hyperparameters such as the size of the latent dimensions were chosen.",Paper Decision
-r5JLbwJ5p,SkxzSgStPS,Exploration via Flow-Based Intrinsic Rewards,Reject,"This paper proposes a method for improving exploration by implementing intrinsic rewards based on optical flow prediction error. The approach was evaluated on several Atari games, Super Mario, and VizDoom.

There are several strengths to this work, including the fact that it comes with open source code, and several reviewers agree it’s an interesting approach. R1 thought it was well-written and quite easy to follow. I also commend the authors for being so responsive with comments and for adding the new experiments that were asked for.

The main issue that reviewers pointed out, and which I am also concerned about, is how these particular games were chosen. R3 points out that these 5 Atari games are not known for being hard exploration games. Authors did conduct further experiments on 6 Atari games suggested by the reviewer, but the results didn’t show significant improvement over baselines.

I appreciate the authors’ argument that every method has “its niche”, but the environments chosen must still be properly motivated. I would have preferred to see results on all Atari games, along with detailed and quantitative analysis into why FICM fails on specific tasks. For instance, they state in the rebuttal that “The selection criteria of our environments is determined by the relevance of motions of the foreground and background components (including the controllable agent and the uncontrollable objects) to the performance (i.e., obtainable scores) of the agent.” But it doesn’t seem like this was assessed in any quantitative way.  Without this understanding, it’d be difficult for an outsider to know which tasks are appropriate to use with this approach. I urge the authors to focus on expanding and quantifying the work they depict in Figure 8, which, although it begins to illuminate why FICM works for some games and not others, is still only a qualitative snapshot of 2 games. I still think this is a very interesting approach and look forward to future versions of this paper.",Paper Decision
wRD_LEFtNX,BJgZBxBYPB,Learning Underlying Physical Properties From Observations For Trajectory Prediction,Reject,"This paper aims to estimate the parameters of a projectile physical equation from a small number of trajectory observations in two computer games. The authors demonstrate that their method works, and that the learnt model generalises from one game to another. However, the reviewers had concerns about the simplicity of the tasks, the longer term value of the proposed method to the research community, and the writing of the paper. During the discussion period, the authors were able to address some of these questions, however many other points were left unanswered, and the authors did not modify the paper to reflect the reviewers’ feedback. Hence, in the current state this paper appears more suitable for a workshop rather than a conference, and I recommend rejection.",Paper Decision
s51GZ-g4ee,SJeWHlSYDB,SPREAD  DIVERGENCE,Reject,"This paper studies spread divergence between distributions, which may exist in settings where the divergence between said distributions does not. The reviewers feel this work does not have sufficient technical novelty to merit acceptance at this time.",Paper Decision
DQcenQv7mq,HyxgBerKwB,GraphQA: Protein Model Quality Assessment using Graph Convolutional Network,Reject,"This paper introduces an approach for estimating the quality of protein models. The proposed method consists in using graph convolutional networks (GCNs) to learn a representation of protein models and predict both a local and a global quality score. Experiments show that the proposed approach performs better than methods based on 1D and 3D CNNs.

Overall, this is a borderline paper. The improvement over state of the art for this specific application is noticeable. However, a major drawback is the lack of methodological novelty, the proposed solution being a direct application of GCNs. It does not bring new insights in representation learning. The contribution would therefore be of interest to a limited audience, in light of which I recommend to reject this paper.",Paper Decision
4ZXllCVZ4,rygeHgSFDH,Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN),Accept (Spotlight),"This paper builds on the recent theoretical work by Khemakhem et al. (2019) to propose a novel flow-based method for performing non-linear ICA. The paper is well written, includes theoretical justifications for the proposed approach and convincing experimental results. Many of the initial minor concerns raised by the reviewers were addressed during the discussion stage, and all of the reviewers agree that this paper is an important contribution to the field and hence should be accepted. Hence, I am happy to recommend the acceptance of this paper as an oral. ",Paper Decision
LhoQJAOpJm,rkxgHerKvH,DEEP GRAPH SPECTRAL EVOLUTION NETWORKS FOR GRAPH TOPOLOGICAL TRANSFORMATION,Reject,"The reviewers kept their scores after the author response period, pointing to continued concerns with methodology, needing increased exposition in parts, and not being able to verify theoretical results. As such, my recommendation is to improve the clarity around the methodological and theoretical contributions in a revision.",Paper Decision
e5hzbLrzJ,HkxJHlrFvr,Angular Visual Hardness,Reject,"This paper proposes a new measure for CNN and show its correlation to human visual hardness. The topic of this paper is interesting, and it sparked many interesting discussions among reviews. After reviewing each others’ comments, reviewers decided to recommend reject due to a few severe concerns that are yet to be address. In particular, reviewer 1 and 2 both raised concerns about potentially misleading and perhaps confusing statements around the correlation between HSF and accuracy. A concrete step was suggested by a reviewer - reporting correlation between accuracy and HSF. A few other points were raised around its conflict/agreement with prior work [RRSS19], or self-contradictory statements as pointed out by Reviewer 1 and 2 (see reviewer 2’s comment). We hope authors would use this helpful feedback to improve the paper for the future submission. 
",Paper Decision
jgMXv76K27,HJgySxSKvB,Deep Relational Factorization Machines,Reject,"This paper proposes to combine FMs and GNNs. All reviewers voted reject, as the paper lacks experiments (eg ablation studies) and novelty. Writing can be significant improved - some information is missing. Authors did not respond to reviewers questions and concerns. For this reason, I recommend reject. 
",Paper Decision
DBUcoBMeL5,HJeANgBYwr,Towards Scalable Imitation Learning for Multi-Agent Systems with Graph Neural Networks,Reject,"This paper proposes a graph neural network based approach for scaling up imitation learning (e.g., of swarm behaviors). Reviewers noted key limitations in the discussion of related work, size of the proposed contribution in terms of model novelty, and evaluation / comparison to strong baselines. Reviewers appreciated the author replies which resolved some concerns but agree that the paper is overall not ready for publication.",Paper Decision
fJW8r359dn,BkxREeHKPS,On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks,Reject,"This paper proposes to reduce the number of variational parameters for mean-field VI. A low-rank approximation is used for this purpose. Results on a few small problems are reported.

As R3 has pointed out, the main reason to reject this paper is the lack of comparison of uncertainty estimates. I also agree that, recent Adam-like optimizers do use preconditioning that can be interpreted as variances, so it is not clear why reducing this will give better results.

I agree with R2's comments about missing the ""point estimate"" baseline. Also the reason for rank 1,2,3 giving better accuracies is unclear and I think the reasons provided by the authors is speculative.

I do believe that reducing the parameterization is a reasonable idea and could be useful. But it is not clear if the proposal of this paper is the right one. Due to this reason, I recommend to reject this paper. However, I highly encourage the authors to improve their paper taking these points into account.",Paper Decision
9r9v4rlI3Z,r1laNeBYPB,Memory-Based Graph Networks,Accept (Poster),"Four reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.",Paper Decision
6asfdkLJ22,Hkx3ElHYwS,GQ-Net: Training Quantization-Friendly Deep Networks,Reject,"The paper propose a new quantization-friendly network training algorithm called GQ (or DQ) net. The paper is well-written, and the proposed idea is interesting. Empirical results are also good. However, the major performance improvement comes from the combination of different incremental improvements. Some of these additional steps do seem orthogonal to the proposed idea. Also, it is not clear how robust the method is to the various hyperparameters / schedules. For example, it seems that some of the suggested training options are conflicting each other. More in-depth discussions and analysis on the setting of the regularization parameter and schedule for the loss term blending parameters will be useful.",Paper Decision
_DaNnE7JU,B1x3EgHtwB,ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks,Reject,"The paper develops linear over-parameterization methods to improve training of small neural network models. This is compared to training from scratch and other knowledge distillation methods. 

Reviewer 1 found the paper to be clear with good analysis, and raised concerns on generality and extensiveness of experimental work. Reviewer 2 raised concerns about the correctness of the approach and laid out several other possibilities. The authors conducted several other experiments and responded to all the feedback from the reviewers, although there was no final consensus on the scores.

The review process has made this a better paper and it is of interest to the community. The paper demonstrates all the features of a good paper, but due to a large number of strong papers, was not accepted at this time.",Paper Decision
4RIFJ4R3C7,HkejNgBtPB,Variational Template Machine for Data-to-Text Generation,Accept (Poster),"The paper addresses the problem of generating descriptions from structured data. In particular a Variational Template Machine  which explicitly disentangles templates from semantic content. They empirically demonstrate that their model performs better than existing methods on different methods. 

This paper has received a strong acceptance from two reviewers. In particular, the reviewers have appreciated the novelty and empirical evaluation of the proposed approach. R3 has raised quite a few concerns but I feel they were adequately addressed by the reviewers. Hence, I recommend that the paper be accepted. ",Paper Decision
KbSY705Mm,HJloElBYvB,Phase Transitions for the Information Bottleneck in Representation Learning,Accept (Poster),"This submission presents a theoretical study of phase transitions in IB: adjusting the IB parameter leads to step-wise behaviour of the prediction.  Quoting R3: “The core result is given by theorem 1: the phase transition betas necessarily satisfy an equation, where the LHS is expressed in terms of an optimal perturbation of the encoding function X->Z.”
This paper received a borderline review and two votes for weak accept.  The main comment for the borderline review was about the rigor of a proof and the use of << symbols.  The authors have updated the proof using limits as requested, addressing this primary concern.  On the balance, the paper makes a strong contribution to understanding an important learning setting and a contribution to theoretical understanding of the behavior of information bottleneck predictors.  ",Paper Decision
mw-VeufuXB,BkgqExrYvS,PopSGD: Decentralized Stochastic Gradient Descent in the Population Model,Reject,"This manuscript studies scaling distributed stochastic gradient descent to a large number of nodes. Specifically, it proposes to use algorithms based on population analysis (relevant for large numbers of distributed nodes) to implement distributed training of deep neural networks. 

In reviews and discussions, the reviewers and AC note missing or inadequate comparisons to previous work on asynchronous SGD, and possible lack of novelty compared to previous work. The reviewers also mentioned the incomplete empirical comparison to closely related work. On the writing, reviewers mentioned that the conciseness of the manuscript could be improved.
",Paper Decision
aqAKOavYDJ,B1ecVlrtDr,Symmetric-APL Activations: Training Insights and Robustness to Adversarial Attacks,Reject,"This work presents a learnable activation function based on adaptive piecewise linear (APL) units. Specifically, it extends APL to the symmetric form. The authors argue that S-APL activations can lead networks that are more robust to adversarial attacks. They present an empirical evaluation to prove the latter claim. However, the significance of these empirical results were not clear due to non-standard threat models used in black-box setting and the weak attacks used in open-box setting. The authors revised the submission and addressed some of the concerns the reviewers had. This effort was greatly appreciated by the reviewers. However, the issues related to the significance of robustness results remained unclear even after the revision. In particular, as pointed by R4, some of the revisions seem to be incomplete (Table 4). Also, the concern R4 had initially raised about non-standard black-box attacks was not addressed. Finally, some experimental details are still missing. While the revision indeed a great step, the adversarial experiments more clear and use more standard setup be convincing.
",Paper Decision
IvA7JNBQZF,SJeFNlHtPS,Hidden incentives for self-induced distributional shift,Reject,"The paper shows how meta-learning contains hidden incentives for distributional shift and how a technique called context swapping can help deal with this. Overall, distributional shift is an important problem, but the contributions made by this paper to deal with this, such as the introduction of unit-tests and context-swapping, is not sufficiently clear. Therefore, my recommendation is a reject.",Paper Decision
fu6heF04gq,BygY4grYDr,The divergences minimized by non-saturating GAN training,Reject,"As the reviewers point out, the core contribution might be potentially important but the current execution of the paper makes it difficult to gauge this importance. In the light of this, this paper does not seem ready for appearance in a conference like ICLR.",Paper Decision
vwetc35iBu,HJluEeHKwH,The Differentiable Cross-Entropy Method,Reject,"This paper proposes a differentiable version of CEM, allowing CEM to be used as an operator within end-to-end training settings. The reviewers all like the idea -- it is simple and should be of interest to the community. Unfortunately, the reviewers also are in consensus that the experiments are not sufficiently convincing. We encourage the authors to expand the empirical analysis, based on the reviewer's specific comments, and resubmit the paper to a future venue.",Paper Decision
fflH5pKQq,S1xO4xHFvB,Atomic Compression Networks,Reject,"This paper proposed a very general idea called Atomic Compression Networks (ACNs) to construct neural networks. The idea looks simple and effective.  However, the reason why it works is not well explained.  The experiments are not sufficient enough to convince the reviewers.",Paper Decision
IIg582klus,SJgwNerKvB,Continual learning with hypernetworks,Accept (Spotlight),"This paper proposes to use hypernetwork to prevent catastrophic forgetting. Overall, the paper is well-written, well-motivated, and the idea is novel. Experimentally, the proposed approach achieves SOTA on various (well-chosen) standard CL benchmarks (notably P-MNIST for CL, Split MNIST) and also does reasonably well on Split CIFAR-10/100 benchmark. The authors are suggested to investigate alternative penalties in the rehearsal objective, and also add comparison with methods like HAT and PackNet.",Paper Decision
_Exo7Gxf9,BJxDNxSFDH,Few-Shot Regression via Learning Sparsifying Basis Functions,Reject,All reviewers agree that this paper is not ready for publication.,Paper Decision
1yX16aYe6A,H1eLVxrKwS,Removing input features via a generative model to explain their attributions to classifier's decisions,Reject,"Perturbation-based methods often produce artefacts that make the perturbed samples less realistic. This paper proposes to corrects this through use of an inpainter.  Authors claim that this results in more plausible perturbed samples and produces methods more robust to hyperparameter settings. 
Reviewers found the work intuitive and well-motivated, well-written, and the experiments comprehensive.
However they also had concerns about minimal novelty and unfair experimental comparisons, as well as inconclusive results. Authors response have not sufficiently addressed these concerns.
Therefore, we recommend rejection.",Paper Decision
_Nzgy7iAC7,rJg8NertPr,Top-down training for neural networks,Reject,"The paper proposes a top-down approach to train deep neural networks -- freezing top layers after supervised pre-training, then re-initializing and retraining the bottom layers. As mentioned by all the reviewers, the novelty is on the low side. The paper is purely experimental (no theory), and the experimental section is currently too weak. In particular:
- Experiments on different domains should be performed.
- Different models should be evaluated.
- Ablation experiments should be performed to understand better under which conditions the proposed approach works.
- For speech recognition, WER should be reported - even if it is without a LM - such that one can compare with existing work.
",Paper Decision
HYRA5-a9WD,r1erNxBtwr,Demystifying Graph Neural Network Via Graph Filter Assessment,Reject,"The paper investigates graph convolutional filters, and proposes an adaptation of the Fisher score to assess the quality of a convolutional filter. Formally, the defined Graph Filter Discriminant Score assesses how the filter improves the Fisher score attached to a pair of classes (considering the nodes in each class, and their embedding through the filter and the graph structure, as propositional samples), taking into account the class imbalance.

An analysis is conducted on synthetic graphs to assess how the hyper-parameters (order, normalization strategy) of the filter rule the GFD score depending on the graph and class features. As could have been expected there no single killer filter.

A finite set of filters, called base filters, being defined by varying the above hyper-parameters, the search space is that of a linear combination of the base filters in each layer. Three losses are considered: with and without graph filter discriminant score, and alternatively optimizing the cross-entropy loss and the GFD; this last option is the best one in the experiments.

As noted by the reviewers and other public comments, the idea of incorporating LDA ideas into GNN is nice and elegant. The reservations of the reviewers are mostly related to the experimental validation: of course getting the best score on each dataset is not expected; but the set of considered problems is too limited and their diversity is limited too (as demonstrated by the very nice Fig. 5).

The area chair thus encourages the authors to pursue this very promising line of research and hopes to see a revised version backed up with more experimental evidence. ",Paper Decision
KoJ5dScvHj,S1lBVgHYvr,Towards Certified Defense for Unrestricted Adversarial Attacks,Reject,"This paper proposes a certified defense under the more general threat model beyond additive perturbation. The proposed defense method is based on adding noise to the classifier's outputs to limit the attacker's knowledge about the parameters, which is similar to differential privacy mechanism. The authors proved the query complexity for any attacker to generate a successful adversarial attack. The main objection of this work is (1) the assumption of the attacker and the definition of the query complexity (to recover the optimal classifier rather than generating an adversarial example successfully) is uncommon, (2) the claim is misleading, and (3) the experimental evaluation is not sufficient (only two attacks are evaluated). The authors only provided a brief response to address the reviewers’ comments/questions without submitting a revision. Unfortunately none of the reviewer is in support of this paper even after author response.
",Paper Decision
i9Sjg7i5zz,SylVNerFvr,Permutation Equivariant Models for Compositional Generalization in Language,Accept (Poster),"This paper proposes an equivariant sequence-to-sequence model for dealing with compositionality of language. They show these models are better at SCAN tasks.

Reviewers expressed two major concerns:
1) Limited clarity of section 4 which makes the paper difficult to understand.
2) Whether this could generalize to more complex types of compositionality.

Authors responded by revising Section 4 and answering the question of generalization. While the reviewers are not 100% satisfied, they agree there is enough novel contribution in this paper. 

I thank the authors for submitting and look forward to seeing a clearer revision in the conference.",Paper Decision
JbQRtKvp0,BJg4NgBKvH,Training binary neural networks with real-to-binary convolutions,Accept (Poster),"This paper proposes methodology to train binary neural networks.

The reviewers and authors engaged in a constructive discussion. All the reviewers like the contributions of the paper.

Acceptance is therefore recommended.",Paper Decision
rpuGy8Snh9,r1e7NgrYvH,DO-AutoEncoder: Learning and Intervening Bivariate Causal Mechanisms in Images,Reject,"The idea of integrating causality into an auto-encoder is interesting and very timely. While the reviewers find this paper to contain some interesting ideas, the technical contributions and mathematical rigor, scope of the method, and the presentation of results would need to be significantly improved in order for this work to reach the quality bar of ICLR.",Paper Decision
4-Ry-UlJY,BJgQ4lSFPH,StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding,Accept (Poster),"This paper proposes a pair of complementary word- and sentence-level pretraining objectives for BERT-style models, and shows that they are empirically effective, especially when used with an already-pretrained RoBERTa model.

Work of this kind has been extremely impactful in NLP, and so I'm somewhat biased toward acceptance: If this isn't published, it seems likely that other groups will go to the trouble to replicate roughly these experiments. However, I think the paper is borderline. Reviewers were impressed by the results, but not convinced that the ablations and analyses were sufficient to motivate the proposed methods, suggesting that some variants of the proposed methods could likely be substantially better. In addition, I agree strongly with R3 that framing this work around 'language structure' is disingenuous, and actively misleads readers about the contribution to the paper.",Paper Decision
GOaCZmAxuZ,r1xQNlBYPS,Multichannel Generative Language Models,Reject,"This paper presents a multi-view generative model which is applied to multilingual text generation. Although all reviewers find the overall approach is important and some results are interesting, the main concern is about the novelty. At the technical level, the proposed method is the extension of the original two-view KERMIT to multiviews, which I have to say incremental. At a higher level, multi-lingual language generation itself is not a very novel idea, and the contribution of the proposed method should be better positioned comparing to related studies. (for example, Dong et al, ACL 2015 as suggested by R#3). Also, some reviewers pointed out the problems in presentation and unconvincing experimental setup. I support the reviewers’ opinions and would like to recommend rejection this time.
I recommend authors to take in the reviewers’ comments and polish the work for the next chance. ",Paper Decision
4kseDFG3Ub,B1xMEerYvB,Smooth markets: A basic mechanism for organizing gradient-based learners,Accept (Poster),"The paper discusses smooth market games and demonstrate the merit of the approach.    The reviewers agree on the quality of the paper, and the comments have been addressed well by the authors.
",Paper Decision
JdgFOOBdA7,B1xfElrKPr,Enhancing the Transformer with explicit relational encoding for math problem solving,Reject,"This paper proposes a change in the attention mechanism of Transformers yielding the so-called ""Tensor-Product Transformer"" (TP-Transformer). The main idea is to capture filler-role relationships by incorporating a Hadamard product of each value vector representation (after attention) with a relation vector, for every attention head at every layer. The resulting model achieves SOTA on the Mathematics Dataset. Attention maps are shown in the analysis to give insights into how TP-Transformer is capable of solving the Mathematics Dataset's challenging problems. 

While the modified attention mechanism is interesting and the analysis is insightful (and improved with the addition of an experiment in NMT after the rebuttal), the reviewers expressed some concerns in the discussion stage:

1. The comparison to baseline is not fair (not to mention the 8.24% claim in conclusion). The proposed approach adds 5 million parameters to a normal transformer (table 1, 5M is a lot!), but in terms of interpolation, it only improves 3% (extrapolation improves 0.5%) at 700k steps. The rebuttal claimed that it is fair as long as the hidden size is comparable, but I don't think that's a fair argument. I suspect that increasing the feedforward hidden size (d_ff) of a normal transformer to match parameters (and add #training steps to match #train steps) might change the conclusion.
2. The new experiment on WMT further convinces me that the theoretical motivation does not hold in practice. Even with the added few million more parameters, it only improved BLEU by 0.05 (we usually consider >0.5 as significant or non-random). This might be because the feedforward and non-linearity can disambiguate as well. 

I also found the name TP-Transformer a bit misleading, since what is proposed and tested here is the Hadamard product (i.e. only the diagonal part of the tensor product). 

I recommend resubmitting an improved version of this paper with  stronger empirical evidence of outperformance of regular Transformers with comparable number of parameters.",Paper Decision
yNvVbsNy0f,HkxZVlHYvH,Ergodic Inference: Accelerate Convergence by Optimisation,Reject,"This paper presents a way of adapting an HMC-based posterior inference algorithm. It's based on two approximations: replacing the entropy of the final state with the entropy of the initial state, and differentiating through the MH acceptance step. Experiments show it is able to sample from some toy distributions and achieves slightly higher log-likelihood on binarized MNIST than competing approaches.

The paper is well-written, and the experiments seem pretty reasonable.

I don't find the motivations for the aforementioned approximations very convincing. It's claimed that encouraging entropy of P_0 has a similar effect to encouraging entropy of P_T, but it seems easy to come up with situations where the algorithm could ""cheat"" by finding a high-entropy P_0 which leads straight downhill to an atypically high-density region. Similarly, there was some reviewer discussion about whether it's OK to differentiate through the indicator function; while we differentiate through nondifferentiable functions all the time, it makes no sense to differentiate through a discontinuous function. (This is a big part of why adaptive HMC is hard.)

This paper has some promising ideas, but overall the reviewers and I don't think this is quite ready.
",Paper Decision
ctV0cFEYcE,r1l-VeSKwS,SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing,Reject,"I had a little bit of difficulty with my recommendation here, but in the end I don't feel confident in recommending this paper for acceptance, with my concerns largely boiling down to the lack of clear description of the overall motivation.

Standard adversarial attacks are meant to be *imperceptible* changes that do not change the underlying semantics of the input to the human eye. In other words, the goal of the current work, generating ""semantically meaningful"" perturbations goes against the standard definition of adversarial attacks. This left me with two questions:

1. Under the definition of semantic adversarial attacks, what is to prevent someone from swapping out the current image with an entirely different image? From what I saw in the evaluation measures utilized in the paper, such a method would be judged as having performed a successful attack, and given no constraints there is nothing stopping this.

2. In what situation would such an attack method would be practically useful?

Even the reviewers who reviewed the paper favorably were not able to provide answers to these questions, and I was not able to resolve this from my reading of the paper as well. I do understand that there is a challenge on this by Google. In my opinion, even this contest is somewhat ill-defined, but it also features extensive human evaluation to evaluate the validity of the perturbations, which is not featured in the experimental evaluation here.

While I think this work is potentially interesting, it seems that there are too many open questions that are not resolved yet to recommend acceptance at this time, but I would encourage the authors to tighten up the argumentation/evaluation in this regard and revise the paper to be better accordingly!",Paper Decision
BJzN0S-PQ,SkglVlSFPS,Uncertainty - sensitive learning and planning with ensembles,Reject,"The authors study planning problems with sparse rewards.                                                                           
They propose a tree search algorithm together with an ensemble of value                                                            
functions to guide exploration in this setting.                                                                                    
The value predictions from the ensemble are combined in a risk sensitive way,                                                      
therefore biasing the search towards states with high uncertainty in value                                                         
prediction.                                                                                                                        
The approach is applied to several grid-world environments.                                                                        
                                                                                                                                   
The reviewers mostly criticized the presentation of the material, in particular                                                    
that the paper provided insufficient details on the proposed                                                                       
method. Furthermore, the comparison to model-free RL methods was deemed somewhat                                                   
lacking, as the proposed algorithm has access to the ground truth model.                                                           
The authors improved the manuscript in the rebuttal.                                                                               
                                                                                                                                   
Based on the reviews and my own reading I think that the paper in it's current                                                     
form is below acceptance threshold. However, with further improved presentation                                                    
and baselines for the experiments, this has potential to be an important contribution.",Paper Decision
LWP2v64SVX,ByexElSYDr,Fair Resource Allocation in Federated Learning,Accept (Poster),"This manuscript proposes and analyzes a federated learning procedure with more uniform performance across devices, motivated as resulting in a fairer performance distribution. The resulting algorithm is tunable in terms of the fairness-performance tradeoff and is evaluated on a variety of datasets.

The reviewers and AC agree that the problem studied is timely and interesting, as there is limited work on fairness in federated learning. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and clarity of the conceptual and empirical results. In reviews and discussion, the reviewers noted insufficient justification of the approach and results, particularly in terms of broad empirical evaluation, and sensitivity of the results to misestimation of various constants. In the opinion of the AC, while the paper can be much improved, it seems to be technically correct, and the results are of sufficiently broad interest to consider publication.",Paper Decision
dXOs7kkw2,SkxlElBYDS,Continual Learning via Principal Components Projection,Reject,"There is no author response for this paper. The paper addresses the issue of catastrophic forgetting in continual learning. The authors build upon the idea from [Zheng,2019], namely finding gradient updates in the space perpendicular to the input vectors of the previous tasks resulting in less forgetting, and propose an improvement, namely to use principal component analysis to enable learning new tasks without restricting their solution space as in [Zheng,2019]. 
While the reviewers acknowledge the importance to study continual learning, they raised several concerns that were viewed by the AC as critical issues: (1) convincing experimental evaluation -- an analysis that clearly shows how and when the proposed method can solve the issue that [Zheng,2019] faces with (task similarity/dissimilarity scenario) would substantially strengthen the evaluation and would allow to assess the scope and contributions of this work; also see R3’s detailed concerns and questions on empirical evaluation, R2’s suggestion to follow the standard protocols, and R1’s suggestion to use PackNet and HAT as baselines for comparison;  (2) lack of presentation clarity -- see R2’s concerns how to improve, and R1’s suggestions on how to better position the paper. 
A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs clarifications, more empirical studies and polish to achieve the desired goal.
",Paper Decision
DPbGSqtfG2,Skey4eBYPS,Convolutional Conditional Neural Processes,Accept (Talk),"This paper presents Convolutional Conditional Neural Process (ConvCNP), a new member of the neural process family that models translation equivariance. Current models must learn translation equivariance from the data, and the authors show that ConvCNP can learn this as part of the model, which is much more generalisable and efficient. They evaluate the ConvCNP on several benchmarks, including an astronomical time-series modelling experiment, a sim2real experiment, and several image completion experiments and show excellent results. The authors wrote extensive responses the the reviewers, uploading a revised version of the paper, and there was some further discussion. This is a strong paper worthy of inclusion in ICLR and could have a large impact on many fields in ML/AI. ",Paper Decision
vCvdqC0Uq3,rJxRmlStDB,Self-Induced Curriculum Learning in Neural Machine Translation,Reject,"This paper presents a method for curriculum learning based on extracting parallel sentences from comparable corpora (wikipedia), and continuously retraining the model based on these examples. Two reviewers pointed out that the initial version of the paper lacked references and baselines from methods of mining parallel sentences from comparable corpora such as Wikipedia. The authors have responded at length and included some of the requested baseline results. This changed one reviewer's score but has not tipped the balance strongly enough for considering this for publication. ",Paper Decision
5qUMV1MbX5,rJlTXxSFPr,A Quality-Diversity Controllable GAN for Text Generation,Reject,"This paper provides a method (loss function) for training GAN model for generation of discrete text token generation. The aim of this loss method to control the trade off between quality vs diversity while generating the text data.

The paper is generally well written, but the experimental section is not overly good: Interpretation of the results is missing; error bars are missing. ",Paper Decision
OBol8Q_JAR,ByeaXeBFvH,Hydra: Preserving Ensemble Diversity for Model Distillation,Reject,"This work introduces a simple and effective method for ensemble distillation. The method is a simple extension of earlier “prior networks”: it differs in which, instead of fitting a single network to mimic a distribution produced by the ensemble, this work suggests to use multi-head (one head per individual ensemble member) in order to better capture the ensemble diversity. This paper experimentally shows that multi-head architecture performs well on MNIST and CIFAR-10 (they added CIFAR-100 in the revised version) in terms of accuracy and uncertainty.

While the method is effective and the experiments on CIFAR-100 (a harder task) improved the paper, the reviewers (myself included) pointed out in the discussion phase that the limited novelty remains a major weakness. The proposed method seems like a trivial extension of the prior work, and does not provide much additional insight. To remedy this shortcoming, I suggest the authors provide extensive experimental supports including various datasets and ablation studies.  

Another concern mentioned in the discussion is the fact that these small improvements are in spite of the fact that the proposed method ends up using many more parameters than the baselines. Including and comparing different model sizes in a full fledged experimental evaluation would better convey the trade-offs of the proposed approach.
",Paper Decision
dpWrpAY3HV,H1l2mxHKvr,Few-Shot Few-Shot Learning and the role of Spatial Attention,Reject,"This paper tackles the interesting problem of meta-learning in problem spaces where training ""tasks"" are scarce.  Two criticisms that seems to shared across reviewers are that (i) it is debatable how ""novel"" the space of meta learning with ""few"" tasks is, especially since there aren't established standard for how many training tasks should be available, and (ii) the paper could use more comparisons with baseline methods and ablations to understand the contributions.  As an AC, I down-weight criticism (i) because I don't feel the paper has to be creating a new problem definition; it's acceptable to make advances within an existing space.  However, criticism (ii) seems to remain.  After conferring with reviewers it seems that the rebuttal was not strong enough to significantly alter the reviewer's opinions on this issue, and so the paper does not have enough support to justify acceptance.  The paper certainly addresses interesting issues, and I look forward to seeing a revised/improved version at another venue. ",Paper Decision
4mOAMP_p_,BJlnmgrFvS,BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning,Reject,"The authors propose a novel algorithm for batch RL with offline data. The method is simple and outperforms a recently proposed algorithm, BCQ, on Mujoco benchmark tasks.

The main points that have not been addressed after the author rebuttal are:
* Lack of rigor and incorrectness of theoretical statements. Furthermore, there is little analysis of the method beyond the performance results.
* Non-standard assumptions/choices in the algorithm without justification (e.g., concatenating episodes).
* Numerous sloppy statements / assumptions that are not justified.
* No comparison to BEAR, making it challenging to evaluate their state-of-the-art claims.
The reviewers also point out several limitations of the proposed method. Adding a brief discussion of these limitations would strengthen the paper.

The method is interesting and simple, so I believe that the paper has the potential to be a strong submission if the authors incorporate the reviewers suggestions in a future submission. However, at this time, the paper falls below the acceptance bar.",Paper Decision
H7U8NobZmb,Hygi7xStvS,Lossless Data Compression with Transformer,Reject,"The paper proposes to use transformers to do lossless data compression. The idea is simple and straightforward (with adding n-gram inputs). The initial submission considered one dataset, a new dataset was added in the rebuttal. Still, there is no runtime in the experiments (and Transformers can take a lot of time to train). Since this is more an experimental paper, this is crucial (and the improvements reports are very small and it is difficult to judge if there are significant).
Overall, there was a positive discussion between the authors and the reviewers. The reviewers commented that concerns have been addressed, but did not change the evaluation which is  unanimous reject.  ",Paper Decision
zwVZBPeDT,rkeiQlBFPB,Meta-Learning with Warped Gradient Descent,Accept (Talk),A strong paper reporting improved approaches to meta-learning.,Paper Decision
mFPVG-rTb7,Sye57xStvB,Never Give Up: Learning Directed Exploration Strategies,Accept (Poster),"This paper tackles hard-exploration RL problems. The idea is to learn separate exploration and exploitation strategies using the same network (representation). The exploration is driven by intrinsic rewards, which are generated using an episodic memory and a lifelong novelty modules. Several experiments (simple and Atari domains) show that the proposed approach compares favourably with the baselines.

The work is novel both in terms of the episodic curiosity metric and its integration with the life-long curiosity metric, and the results are convincing. All reviewers being positive about this paper, I therefore recommend acceptance.",Paper Decision
SMZKHVhdmq,H1eqQeHFDS,AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing     ,Accept (Poster),"This paper treats the task of point cloud learning as a dynamic advection problem in conjunction with a learned background velocity field.  The resulting system, which bridges geometric machine learning and physical simulation, achieves promising performance on various classification and segmentation problems.  Although the initial scores were mixed, all reviewers converged to acceptance after the rebuttal period.  For example, a better network architecture, along with an improved interpolation stencil and initialization, lead to better performance (now rivaling the state-of-the-art) as compared to the original submission.  This helps to mitigate an initial reviewer concern in terms of competitiveness with existing methods like PointCNN or SE-Net.  Likewise, interesting new experiments such as PIC vs. FLIP were included.",Paper Decision
BevIX-9o2Y,rylqmxBKvH,Unsupervised Spatiotemporal Data Inpainting,Reject,"This paper studies the problem of unsupervised inpainting occluded areas in spatiotemporal sequences and propose a GAN-based framework which is able to complete the occluded areas given the stochastic model of the occlusion process. The reviewers agree that the problem is interesting, the paper is well written, and that the proposed approach is reasonable. However, after the discussion phase the critical point raised by AnonReviewer1 remains: in principle, when applying different corruptions in each step, the model is able to see the entire video over the duration of the training. This coupled with the strong assumptions on the mask distribution makes it questionable whether the approach should be considered unsupervised. Given that the results of the supervised methods significantly outperform the unsupervised ones, this issue needs to be carefully addressed to provide a clear and convincing selling point. Hence, I will recommend rejection and encourage the authors to address the remaining issues (the answers in the rebuttal are a good starting point).",Paper Decision
UDcp0KviE8,r1xF7lSYDS,Transferable Recognition-Aware Image Processing,Reject,"This paper presents several models for recognition-aware image enhancement. The authors propose to enhance the image quality in the presence of image degradation (e.g., low-resolution, noise, compression artifacts) as well as to improve the recognition accuracy in a joint model. While acknowledging that the paper is addressing an interesting direction, the reviewers and AC note the following potential weaknesses: presentation clarity, limited technical contributions, insufficient empirical evidence. AC can confirm all the reviewers have read the rebuttal and have contributed to the discussion. All the reviewers and AC agree that the rebuttal was informative, and the authors have partially addressed some of the concerns (e.g. additional experiments). R2 has raised the score from reject to weak reject. However, at this stage AC suggest the manuscript is below the acceptance bar and needs a major revision before submitting for another round of reviews. We hope the reviews are useful for improving and revising the paper.",Paper Decision
ylPSjiQo4V,BklOXeBFDS,Transfer Active Learning For Graph Neural Networks,Reject,"Paper proposes a method for active learning on graphs. Reviewers found the presentation of the method confusing and somewhat lacking novelty in light of existing works (some of which were not compared to). After the rebuttal and revisions, reviewers minds were not changed from rejection. ",Paper Decision
ARsXkGTci,rkedXgrKDH,Trajectory growth through random deep ReLU networks,Reject,"This article studies the length of one-dimensional trajectories as they are mapped through the layers of a ReLU network, simplifying proof methods and generalising previous results on networks with random weights to cover different classes of weight distributions including sparse ones. It is observed that the behaviour is similar for different distributions, suggesting a type of universality. The reviewers found that the paper is well written and appreciated the clear description of the places where the proofs deviate from previous works. However, they found that the results, although adding interesting observations in the sparse setting, are qualitatively very close to previous works and possibly not substantial enough for publication in ICLR. The revision includes some experiments with trained networks and updates the title to better reflect the contribution. However, the reviewers did not find this convincing enough. The article would benefit from a deeper theory clarifying the observations that have been made so far, and more extensive experiments connecting to practice. ",Paper Decision
NligvqpEYK,SyxD7lrFPH,Frequency Pooling: Shift-Equivalent and Anti-Aliasing Down Sampling,Reject,"This submission has been assessed by three reviewers and scored 3/6/1. The reviewers also have not increased their scores after the rebuttal. Two reviewers pointed to poor experimental results that do not fully support what is claimed in contributions and conclusions. Theoretical support for the reconstruction criterion was considered weak. Finally, the paer is pointed to be a special case of (Zhang 2019). While the paper has some merits, all reviewers had a large number of unresolved criticism. Thus, this paper cannot be accepted by ICLR2020.
",Paper Decision
L6uhbfuLi,HklvmlrKPB,Improving Sequential Latent Variable Models with Autoregressive Flows,Reject,The paper scores low on novelty. The experiments and model analysis are not very strong.,Paper Decision
eEx-yC4jiT,rkgvXlrKwH,SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference,Accept (Talk),"The paper presents a framework for scalable Deep-RL on really large-scale architecture, which addresses several problems on multi-machine training of such systems with many actors and learners running.  Large-scale experiments and impovements over IMPALA are presented, leading to new SOTA results. The reviewers are very positive over this work, and I think this is an important contribution to the overall learning / RL community.",Paper Decision
cOn8m4Y4mI,Hye87grYDH,Sparse Transformer: Concentrated Attention Through Explicit Selection,Reject,"The paper proposes a variant of Sparse Transformer where only top K activations are kept in the softmax. The resulting transformer model is applied to NMT, image caption generation and language modeling, where it outperformed a vanilla Transformer.

While the proposed idea is simple, easy to implement, and it does not add additional computational or memory cost, the reviewers raised several concerns in the discussion phase, including: several baselines missing from the tables; incomplete experimental details; incorrect/misleading selection of best performing model in tables of results (e.g. In Table 1, the authors boldface their results on En-De (29.4) and De-En (35.6) but in fact, the best performance on these is achieved by competing models, respectively 29.7 and 35.7. The caption claims their model ""achieves the state-of-the-art performances in En-Vi and De-En"" but this is not true for De-En (albeit by 0.1). In Table 3, they boldface their result of 1.05 but the best result is 1.02; the text says their model beats the Transf-XL ""with an advantage"" (of 0.01) but do not point out that the advantage of Adaptive-span over their model is 3 times as large (0.03)).

This prevents me from recommending acceptance of this paper in its current form. I strongly encourage the authors to address these concerns in a future submission.",Paper Decision
TEIHULnYrd,SklSQgHFDS,Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration,Reject,"The paper presents a method for intrinsically motivated exploration using successor features by interleaving the exploration task with intrinsic rewards and extrinsic task original external rewards. In addition, the paper proposes ""successor feature control"" (distance between consecutive successor features) as an intrinsic reward. The proposed method is interesting and it can potentially address the limitation of existing exploration methods based on intrinsic motivation. In experimental results, the method is evaluated on navigation tasks using Vizdoom and DeepMind Lab, as well as continuous control tasks of Cartpole in the DeepMind control suite, with promising results. 

On the negative side, there are some domain-specific properties (e.g., moderate map size with relatively simple structures, different rooms having visually distinct patterns, bottleneck states generally leading to better rewards, etc.) that make the proposed method work well. In addition, off-policy learning of the successor features could be a potential technical issue. Finally, the proposed method is not evaluated against stronger baselines on harder exploration tasks (such as Atari Montezuma's revenge, etc.), thus the addition of such results would make the paper more convincing. In the current form, the paper seems to need more work to be acceptable for ICLR.",Paper Decision
Xyls5Y1Vra,BkxSmlBFvr,You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings,Accept (Poster),The authors analyze knowledge graph embedding models for multi-relational link predictions. Three reviewers like the work and recommend acceptance. The paper further received several positive comments from the public. This is solid work and should be accepted.,Paper Decision
UO8v0jRsvU,Bkf4XgrKvS,Unsupervised Learning of Graph Hierarchical Abstractions with Differentiable Coarsening and Optimal Transport,Reject,"This paper presents a differentiable coarsening approach for graph neural network. It provides the empirical demonstration that the proposed approach is competitive to existing pooling approaches. However, although the paper shows an interesting observation, there are remaining novelty as well as clarity concerns. In particular, the contribution of the proposed work over the graph kernels based on other forms of coarsening such as the early work of Shervashidze et al. as well as higher-order WL (pointed out by Reviewer1) remains unclear. We believe the paper currently lacks comparisons and discussions, and will benefit from additional rounds of future revisions.
",Paper Decision
ciz94Hh2f,r1gEXgBYDH,Defensive Tensorization: Randomized Tensor Parametrization for Robust Neural Networks,Reject,"Three reviewers have assessed this submission and were moderately positive about it . However, the reviewers have also raised a number of concerns. Initially, they complained about substandard experimentation which has been resolved to some degree after rebuttal (rev. believe more can be done in terms of unifying them, investigating backbones, attack methods, and experimental settings in light of recent papers).

A somewhat bigger criticism concerns the theoretical part: 
1. Rev. remained unclear why using tensor decomposition techniques is a sound approach for designing robust network.
2. AC and rev. also noted during discussions that using low rank constraints (and other mechanisms) and i.e. encouraging smoothness (one important mechanism among many in robustness to attacks) have been extensively investigated in the literature, yet, the proposed idea makes scarce if any theoretical connection to such important theoretical tools.

Some references (not exhaustive) that may help authors further study the above aspects are:
Certified Adversarial Robustness via Randomized Smoothing, Cohen et al.
Local Gradients Smoothing: Defense against localized adversarial attacks, Naseer et al.
Limitations of the Lipschitz constant as adefense against adversarial examples, Huster et al.
Learning Low-Rank Representations, Huster et al.

On balance, AC feels that despite the enthusiasm, this paper is not ready yet for the publication in ICLR as the key theory behind the proposed idea is missing. Thus, this submission falls marginally short of acceptance in ICLR 2020. However, the authors are encouraged to build up a compelling theory and resubmit to another venue (currently the paper feels like a solid workshop idea that needs to be investigated further).",Paper Decision
ccZjzpHCsU,BJl7mxBYvB,Robust Reinforcement Learning via Adversarial Training with  Langevin Dynamics,Reject,"The authors address the problem of robust reinforcement learning. They propose an adversarial perspective on robustness. Improving the robustness can now be seen as two agent playing a competitive game, which means that in many cases the first agent needs to play a mixed strategy. The authors propose an algorithm for optimizing such mixed strategies. 

Although the reviewers are convinced of the relevance of the work (as a first approach of Bayesian learning to reach mixed Nash equilibria, which is useful not only for robustness but for any problem that can be formulated as zero-sum game requiring a mixed strategy), they are not completely convinced by the work in current state. Three of the reviewers commented on the experiments not being rigorous and convincing enough in current form, and thus not (yet!) being able to recommend acceptance to ICLR. ",Paper Decision
hl-r2zcfr,r1lQQeHYPr,Embodied Multimodal Multitask Learning,Reject,This paper offers a new approach to cross-modal embodied learning that aims to overcome limited vocabulary and other issues.  Reviews are mixed.  I concur with the two reviewers who say the work is interesting but the contribution is not sufficiently clear for acceptance at this time.,Paper Decision
vmFSpeIS8p,r1gfQgSFDr,High Fidelity Speech Synthesis with Adversarial Networks,Accept (Talk),"The authors design a GAN-based text-to-speech synthesis model that performs competitively with state-of-the-art synthesizers.  The reviewers and I agree that this appears to be the first really successful effort at GAN-based synthesis.  Additional positives are that the model is designed to be highly parallelisable, and that the authors also propose several automatic measures of performance in addition to reporting human mean opinion scores.  The automatic measures correlate well (though far from perfectly) with human judgments, and in any case are a nice contribution to the area of evaluation of generative models.  It would be even more convincing if the authors presented human A/B forced-choice test results (in addition to the mean opinion scores), which are often included in speech synthesis evaluation, but this is a minor quibble.",Paper Decision
SNYq9Iegob,BkgM7xHYwH,Autoencoder-based Initialization for Recurrent Neural Networks with a Linear Memory,Reject,"The paper explores an initialization scheme for the recently introduced linear memory network (LMN) (Bacciu et al., 2019) that is better than random initialization and the approach is tested on various MNIST and TIMIT data sets with positive results.

Reviewer 3 raised concerns about the breadth of experiments and novelty. Reviewer 2 recognized that the model performs well on its MNIST baselines and had concerns about applicability to larger settings. Reviewer 1 acknowledges a very well written paper, but again raises concerns about the thoroughness of the experiments. The authors responded to all three reviewers, responding that the tasks were chosen to match existing work and that the approach is complementary to LSTMs to solve different tasks. Overall the reviewers did not re-adjust their ratings.

There remains questions on scalability and generality, which makes the paper not yet ready for acceptance. We hope that the reviews support the authors further research.",Paper Decision
BYpQE6-ZpG,HyezmlBKwr,Test-Time Training for Out-of-Distribution Generalization,Reject,"The paper is on a new approach approach to transductive learning. Reviewers were a bit on the fence. Their most important objection is that the performance improvements that the authors report almost entirely come from the ""online"" version, which basically gets to see the test distribution.  That contribution is nevertheless, in itself, potentially interesting, but I was surprised not to see comparison with simple transductive learning from semi-supervised learning, learning with cache, or domain adaptation, e.g., using knowledge of the target distribution to reweigh the training sample, or [0], on using an adversary to select a distribution consistent with sample statistics. I encourage the authors to add more baselines, analyze differences with existing approaches, and, if their approach is superior to existing approaches, resubmit elsewhere. 

[0] http://papers.nips.cc/paper/5458-robust-classification-under-sample-selection-bias.pdf",Paper Decision
b7BMFDzfOF,HJgb7lSFwS,Distance-based Composable Representations with Neural Networks,Reject,"The paper proposes an approach for learning class-level and individual-level (token-level) representations based on Wasserstein distances between data subsets.  The idea is appealing and seems to have applicability to multiple tasks.  The reviewers voiced significant concerns with the unclear writing of the paper and with the limited experiments.  The authors have improved the paper, but to my mind it still needs a good amount of work on both of these aspects.  The choice of wording in many places is imprecise.  The tasks are non-standard ones so they don't have existing published numbers to compare against; in such a situation I would expect to see more baselines, such as alternative class/instance representations that would show the benefit specifically of the Wasserstein distance-based approach.  I cannot tell from the paper in its current form whether or when I would want to use the proposed approach.  In short, despite a very interesting initial idea, I believe the paper is too preliminary for publication.",Paper Decision
Z9Ja6s6vb,Bkeb7lHtvH,At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?,Accept (Spotlight),"The paper considers the problem of training neural networks asynchronously, and the gap in generalization due to different local minima being accessible with different delays. The authors derive a theoretical model for the delayed gradients, which provide prescriptions for setting the learning rate and momentum.

All reviewers agreed that this a nice paper with valuable theoretical and empirical contributions.

",Paper Decision
5YHirJCSqt,BJxlmeBKwS,FRICATIVE PHONEME DETECTION WITH ZERO DELAY,Reject,"The reviewers appreciate the importance of the problem, and one reviewer particularly appreciated the gains in performance. However, two reviewers raised concerns about limited novelty and missing comparisons to prior work. While the rebuttal helped address these concerns, the novelty is still limited. The authors are encouraged to revise the presentation to clarify the novelty.",Paper Decision
nC_Gq-WDZl,BygkQeHKwB,"Walking on the Edge: Fast, Low-Distortion Adversarial Examples",Reject,"In this paper the authors highlight the role of time in adversarial training and study various speed-distortion trade-offs. They introduce an attack called boundary projection BP which relies on utilizing the classification boundary. The reviewers agree that searching on the class boundary manifold, is interesting and promising but raise important concerns about evaluations on state of the art data sets. Some of the reviewers also express concern about the quality of presentation and lack of detail. While the authors have addressed some of these issues in the response, the reviewers continue to have some concerns. Overall I agree with the assessment of the reviewers and do not recommend acceptance at this time.",Paper Decision
2ohlSmPGnD,Bkx1mxSKvB,Disentangling Trainability and Generalization in Deep Learning,Reject,"The paper investigates the trainability and generalization of deep networks as a function of hyperparameters/architecture, while focusing on wide nets of large depth; it aims to characterize regions of hyperparameter space where networks generalize well vs where they do not; empirical observations are demonstrated to support theoretical results. However, all reviewers agree that, while the topic of the paper is important and interesting, more work is required to improve the readability and clarify the exposition to support the proposed theoretical results.
 ",Paper Decision
aNi8w9akuD,HyeJmlrFvH,Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization,Reject,"This paper proposes a communication-efficient data-parallel SGD with quantization. The method bridges the gap between theory and practice. The QSGD method has theoretical guarantees while QSGDinf doesn't, but the latter gives better result. This paper proves stronger results for QSGD using a different quantization scheme which matches the performance of QSGDinf.

The reviewers find issues with the approach and have pointed some of them out. During the discussion period, we did discuss if reviewers would like to raise their scores. Unfortunately, they still have unresolved issues (see R1's comment). 
R1 made another comment recently that they were unable to add to their review:
""The proposed algorithm and the theoretical analysis does not include momentum. However, in the experiments, it is clearly stated that momentum (with a factor of 0.9) is used. Thus, it is unclear whether the experiments really validate the theoretical guarantees. And, it is also unclear how momentum is added for both NUQSGD and EF-SGD, since momentum is not mentioned in Algorithm 1 in this paper, or the paper of QSGD, or the paper of EF-SignSGD. (There is a version of SignSGD with momentum *without* error feedback, called SIGNUM).""

With the current score, the paper does not make the cut for ICLR, but I encourage the authors to revise the paper based on reviewers' feedback. For now, I recommend to reject this paper.",Paper Decision
ghAs9VZgN8,HkxCzeHFDB,Functional Regularisation for  Continual Learning with Gaussian Processes,Accept (Poster),"The authors introduce a framework for continual learning in neural networks based on sparse Gaussian process methods. The reviewers had a number of questions and concerns, that were adequately addressed during the discussion phase. This is an interesting addition to the continual learning literature. Please be sure to update the paper based on the discussion.",Paper Decision
TyBkcfDRc4,HJxRMlrtPH,Verification of Generative-Model-Based Visual Transformations,Reject,"The goal of verification of properties of generative models is very interesting and the contributions of this work seem to make some progress in this context. However, the current state of the paper (particularly, its presentation) makes it difficult to recommend its acceptance.",Paper Decision
oOs5o2CXG,Syg6fxrKDB,A Graph Neural Network Assisted Monte Carlo Tree Search Approach to Traveling Salesman Problem,Reject,"The paper is a contribution to the recently emerging literature on learning                                                        
based approaches to combinatorial optimization.                                                                                    
The authors propose to pre-train a policy network to imitate SOTA solvers for                                                      
TSPs.                                                                                                                              
At test time, this policy is then improved, in an alpha-go like manner, with                                                       
MCTS, using beam-search rollouts to estimate bootstrap values.                                                                     
                                                                                                                                   
The main concerns raised by the reviewers is lack of novelty (the proposed                                                         
algorithm is a straight forward application of graph NNs to MCTS) as well a the                                                    
experimental results.                                                                                                              
Although comparing well to other learning based methods, the algorithm is far                                                      
away from the performance of SOTA solvers.                                                                                         
                                                                                                                                   
Although well written, the paper is below acceptance threshold.                                                                    
The methodological novelty is low.                                                                                                 
The reported results are an order of magnitude away from SOTA solvers, while previous work                                         
has already reported the general feasibility of learned solvers to TPSs.                                                           
Furthermore, the overall contribution is somewhat unclear as the policy relies                                                     
on pre-training with solutions form existing solvers. ",Paper Decision
97nrRkPW3R,rJg3zxBYwH,Learning Likelihoods with Conditional Normalizing Flows ,Reject,"The authors propose a conditional normalizing flow approach to learning likelihoods. While reviewers appreciated the paper, in its present form it lacked a clear champion, and there were still some remaining concerns about novelty and clarity of presentation. The authors are encouraged to continue with this work and to account for reviewer comments in future revisions. Following up on the author response, a reviewer adds:
""Thanks for your clarification. I still disagree that the conditional flow architecture proposed should be considered as a novel contribution. The reason why I mentioned [1] or [2] was not because they follow the exact setting (coupling based conditional flow model) discussed in this paper. I wanted to highlight that the idea to use conditioning variables as an input to the transforming network (whether it is an autoregressive density function, autoregressive transforming network, or coupling layers) is quite universal (as we all know many of the existing codes implementing flow-based models includes additional keyword arguments 'context' to model conditioning). I'm not sure why the fact that the proposed framework is conditioning on high-dimensional variables makes a contribution. There seems to be no particular challenge in doing that and novel design choices to circumvent that (i.e., we can just use existing architectures with minor modifications).

I agree that the binary dequantization should be considered as a contribution, but as significant as to change my decision to accept. Thanks for the clarification on experiments. Considering this, I raise my rating to weak reject...

Another previous work I forgot to mention in the initial review is ""Structured output learning with the conditional generative flow"", Lu and Huang 2019, ICML 2019 invertible neural network workshop. This paper discusses the conditional flow based on a similar idea, and attacks high-dimensional structured output prediction. I think this should be cited in the paper.""
",Paper Decision
D5j0__OBk,S1ghzlHFPS,Informed Temporal Modeling via Logical Specification of Factorial LSTMs,Reject,"While reviewers find this paper interesting, they raised number of concerns including the novelty, writing, experiments, references and clear mention of the benefit. Unfortunately, excellent questions and insightful comments left by reviewers are gone without authors’ answers. ",Paper Decision
qYp1XTbCV8,HyljzgHtwS,Regularly varying representation for sentence embedding,Reject,"Three reviewers recommend rejection. After a good rebuttal, the first reviewer is more positive about the paper yet still feels the paper is not ready for publication. The authors are encouraged to strengthen their work and resubmit to a future venue.",Paper Decision
9O_s3zFz3J,rJgjGxrFPS,A Simple and Scalable Shape Representation for 3D Reconstruction,Reject,"This paper proposes to use PCS to replace the conventional decoder for 3D shape reconstruction. It shows competitive performance to the state of the art methods. While reviewer #3 is overall positive about this work, both reviewer #1 and #2 rated weak rejection. Reviewer #1 concerns that important details are missing, and the discussion of results is insufficient. Reviewer #3 has questions on the clarity of the presentation and comparison with SOTA methods. The authors provided response to the questions, but did not change the rating of the reviewers. The ACs agree that this work has merits. However, given the various concerns raised by the reviewers, this paper can not be accepted at its current state.",Paper Decision
QnYPVAe778,rJl5MeHKvB,Learning Through Limited Self-Supervision: Improving Time-Series Classification Without Additional Data via Auxiliary Tasks,Reject,"The paper addresses  an important problem of self-supervised learning in the context of time-series classification. However, all reviewers raised major concerns regarding the novelty of the approach and the quality of empirical evaluation, including insufficient comparison with the state-of-art and reproducibility issues. The reviewers agree that the paper, in its current state, does not path the ICLR acceptance threshold, and encourage the authors to improve the paper based on the provided suggestions.",Paper Decision
-jtws4dEnM,Byg5flHFDr,EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs,Reject,"The paper proposes a combination graph neural networks and graph generation model (GraphRNN) to model the evolution of dynamic graphs for predicting the topology of next graph given a sequence of graphs.

The problem to be addressed seems interesting, but lacks strong motivation. Therefore it would be better if some important applications can be specified.  

The proposed approach lacks novelty. It would be better to point out why the specific combination of two existing models is the most appropriate approach to address the task. 

The experiments are not fully convincing. Bigger and comprehensive datasets (with the right motivating applications) should be used to test the effectiveness of the proposed model. 

In short, the current version failed to raise excitement from readers due to the reasons above. A major revision addressing these issues could lead to a strong publication in the future. ",Paper Decision
DUkmbaQhu,B1ltfgSYwS,Few-Shot One-Class Classification via Meta-Learning,Reject,"The authors present a combination of few-shot learning with one-class classification model of problems. The authors use the existing MAML algorithm and build upon it to present a learning algorithm for the problem. As pointed out by the reviewers, the technical contributions of the paper are quite minimal and after the author response period the reviewers have not changed their minds. However, the authors have significantly changed the paper from its initial submission and as of now it needs to be reviewed again. I recommend authors to resubmit their paper to another conference. As of now, I recommend rejection.",Paper Decision
LyXwrtrXM2,SyeKGgStDB,Training a Constrained Natural Media Painting Agent using Reinforcement Learning ,Reject,Paper is withdrawn by authors.,Paper Decision
sq4LX4xL1,SkgOzlrKvH,The Role of Embedding Complexity in Domain-invariant Representations,Reject,"This paper studies the impact of embedding complexity on domain-invariant representations by incorporating embedding complexity into the previous upper bound explicitly.

The idea of embedding complexity is interesting, the exploration has some useful insight, and the paper is well-written. However, Reviewers and AC generally agree that the current version can be significantly improved in several ways:
- The proposed upper bound has several limitations such as looser than existing ones.
- The embedding complexity is only addressed implicitly, which shares similar idea with previous works.
- The claim of implicit regularization has not been explored in-depth.
- The proposed MDM method seems to be incremental and related closely with the embedding complexity.
- There is no analysis about the generalization when estimating this upper bound from finite samples.

There are important details requiring further elaboration. So I recommend rejection.",Paper Decision
CCZJxNqrtG,SklwGlHFvH,Learning Curves for Deep Neural Networks: A field theory perspective,Reject,"This paper studies deep neural network (DNN) learning curves by leveraging recent connections of (wide) DNNs to kernel methods such as  Gaussian processes. 

The bulk of the arguments contained in this paper are, thus, for the ""kernel regime"" rather than ""the problem of non-linearity in DNNs"", as one reviewer puts it. 
When it comes to scoring this paper, it has been controversial. However a lot of discussion has taken place. On the positive side, it seems that there is a lot of novel perspectives included in this paper. On the other hand, even after the revision, it seems that this paper is still very difficult to follow for non-physicists. 

Overall, it would be beneficial to perform a more careful revision of the paper such that it can be better appreciated by the targeted scientific community. 
",Paper Decision
NEBj3SHKM0,HklPzxHFwB,Zero-Shot Policy Transfer with Disentangled Attention,Reject,"This paper proposes a new method for zero-shot policy transfer in RL. The authors propose learning the policy over a disentangled representation that is augmented with attention. Hence, the paper is a simple modification of an existing approach (DARLA). The reviewers agreed that the novelty of the proposed approach and the experimental evaluation are limited. For this reason I recommend rejection.",Paper Decision
1cjSIaigOJ,BylUMxSFwS,Disentangled Cumulants Help Successor Representations Transfer to New Tasks,Reject,"The author propose a method to first learn policies for intrinsically generated goal-based tasks, and then leverage the learned representations to improve the learning of a new task in a generalized policy iteration framework.  The reviewers had significant issues about clarity of writing that were largely addressed in the rebuttal.  However, there were also concerns about the magnitude of the contribution (especially if it was added anything significant to the existing literature on GPI, successor features, etc), and the simplicity (and small number of) test domains.  These concerns persisted after the rebuttal and discussion.  Thus, I recommend rejection at this time.",Paper Decision
-D6vixOfr-,SyeLGlHtPS,"Learning vector representation of local content and matrix representation of local motion, with implications for V1",Reject,"The paper received mixed reviews. On one hand, there is interesting novelty in relation to biological vision systems. On the other hand, there are some serious experimental issues with the machine learning model. While reviewers initially raised concerns about the motivation of the work, the rebuttal addressed those concerns. However, concerns about experiments remained. ",Paper Decision
v0ccgYOJZR,S1xHfxHtPr,Online Learned Continual Compression with Stacked Quantization Modules,Reject,"The paper proposes a new problem setup as ""online continual compression"". The proposed idea is a combination of existing techniques and very simple, though interesting. Parts of the algorithm are not clear, and the hierarchy is not well-motivated. Experimental results seem promising but not convincing enough, since it is on a very special setting, the LiDAR experiment is missing quantitative evaluation, and different tasks might introduce different difficulties in this online learning setting. The ablation study is well designed but not discussed enough.",Paper Decision
Uuy-JaHvi,S1lHfxBFDH,Gumbel-Matrix Routing for Flexible Multi-task Learning,Reject,"This paper proposed to use Gumbel softmax to optimize the routing matrix in routing network for multitask learning.  All reviewers have a consensus on rejecting this paper.  The paper did not clearly explain how and why this method works, and the experiments are not sufficient.",Paper Decision
Mrprk-YZZ6,SJgSflHKDr,The Frechet Distance of training and test distribution predicts the generalization gap,Reject,"The authors discuss how to predict generalization gaps. Reviews are mixed, putting the submission in the lower half of this year's submissions. I also would have liked to see a comparison with other divergence metrics, for example, L1, MMD, H-distance, discrepancy distance, and learned representations (e.g., BERT, Laser, etc., for language). Without this, the empirical evaluation of FD is a bit weak. Also, the obvious next step would be trying to minimize FD in the context of domain adaptation, and the question is if this shouldn't already be part of your paper? Suggestions: The Amazon reviews are time-stamped, enabling you to run experiments with drift over time. See [0] for an example. 

[0] https://www.aclweb.org/anthology/W18-6210/",Paper Decision
XcWSXnGZZ,SJxNzgSKvH,Selective sampling for accelerating  training of deep neural networks,Reject,"The paper proposes a method to speed up training of deep nets by re-weighting samples based on their distance to the decision boundary. However, they paper seems hastily written and the method is not backed by sufficient experimental evidence.",Paper Decision
jpCj9mth2O,SJxmfgSYDB,Representing Unordered Data Using Multiset Automata and Complex Numbers,Reject,"Main summary: Paper is about generating feature representations for set elements using weighted multiset automata

Discussion:
reviewer 1: paper is well written but experimental results are not convincing
reviewer 2: well written but weak motivation
reviewer 3: well written but reviewer has some questions around the motivation of weighted automata machinery. 
Recommendation: all the reviewers agree its well written but the paper could be stronger with motivation and experiments, all reviewers agree. I vote Reject.",Paper Decision
mQm0s8ccJo,HkxQzlHFPr,Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out,Reject,"This paper proposes using first order logic to rule out superficial information for improved natural language inference. While the topic is of interest, reviewers find that the paper misses much of the previous literature on semantics which is highly relevant. 

I thank the authors for submitting this paper to ICLR. Please take the reviewers' comments, especially recommended references, to improve the paper for future submission.",Paper Decision
z5kJBDSDid,H1gXzxHKvH,Deep Nonlinear Stochastic Optimal Control for Systems with Multiplicative Uncertainties,Reject,"A nice paper, but quite some unclarities; it's unclear  in particular if the paper improves w.r.t. SOTA.  Esp. scaling is an issue here. Also, the understandability is below par and more work can make this into an acceptable submission.",Paper Decision
AlBAgatu-w,ByeGzlrKwH,Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network,Accept (Spotlight),"This paper has a few interesting contributions: (a) a bound for un-compressed networks in terms of the compressed network (this is in contrast to some prior work, which only gives bounds on the compressed network); (b) the use of local Rademacher complexity to try to squeeze as much as possible out of the connection; (c) an application of the bound to a specific interesting favorable condition, namely low-rank structure.

As a minor suggestion, I'd like to recommend that the authors go ahead and use their allowed 10th body page!",Paper Decision
e6-HrXDXw4,rJxGGlSKwH,Sentence embedding with contrastive multi-views learning,Reject,"This paper proposes a method to learn sentence representations that incorporates linguistic knowledge in the form of dependency trees using contrastive learning. Experiments on SentEval and probing tasks show that the proposed method underperform baseline methods.

All reviewers agree that the results are not strong enough to support the claim of the paper and have some concerns about the scalability of the implementation. They also agree that the writing of the paper can be improved (details included in their reviews below). 

The authors acknowledged these concerns and mentioned that they will use them to improve the paper for future work, so I recommend rejecting this paper for ICLR.",Paper Decision
KfodKMAat,BJgZGeHFPH,Dynamics-Aware Embeddings,Accept (Poster),"This paper studies how self-supervised objectives can improve representations for efficient RL. The reviewers are generally in agreement that the method is interesting, the paper is well-written, and the results are convincing. The paper should be accepted.",Paper Decision
r53gJhyrWv,BJgxzlSFvr,AN ATTENTION-BASED DEEP NET FOR LEARNING TO RANK,Reject,All three reviewers felt the paper should be rejected and no rebuttal was offered. So the paper is rejected.,Paper Decision
wuI6mcQBA,HkgeGeBYDB,RaPP: Novelty Detection with Reconstruction along Projection Pathway,Accept (Poster),"The paper proposes to extend the autoencoder loss in a deep generative model to include per-latent-layer loss terms.  Two variants are proposed: SAP (simple aggregation along pathway) and NAP (normalized aggregation along pathway). SAP is simply the sum of the squared norm, while NAP performs decorrelation and normalization of the magnitude.  This was viewed as novel by the reviewers, and the experiments supported the proposed approach.

In the post rebuttal phase, the inclusion of an ablation study has led to an upgrade in the reviewer recommendation.  As a result, there was a unanimous opinion that the paper is suitable for publication at ICLR.",Paper Decision
A2F7gqAPho,BJg1fgBYwH,SAFE-DNN: A Deep Neural Network with Spike Assisted Feature Extraction for Noise Robust Inference,Reject," The paper proposes to improve noise robustness of the network learned features, by augmenting deep networks with Spike-Time-Dependent-Plasticity (STDP). The new network show improved noise robustness with better classification accuracy on Cifar10 and ImageNet subset when input data have noise. While this paper is well written, a number of concerns are raised by the reviewers. They include that the proposed method would not be favored from computer vision perspective, it is not convincing why spiking nets are more robust to random noises, and the method fails to address works in adversarial perturbations and adversarial training. Also, Reviewer #2 pointed out the low level of methodological novelty. The authors provided response to the questions, but did not change the rating of the reviewers. Given the various concerns raised, the ACs recommend reject.",Paper Decision
3cdMnyN0iP,B1x1MerYPB,Putting Machine Translation in Context with the Noisy Channel Model,Reject,"The authors propose using a noisy channel formulation which allows them to combine a sentence level target-source translation model with a language model trained over target side document-level information. They use reranking of a 50-best list generated by a standard Transformer model for forward translation and show reasonably strong results.  The reviewers were concerned about the efficiency of this approach and the limited novelty as compared to the sentence-level noisy channel research Yu et al. 2017. The authors responded in depth, adding results with another baseline which includes backtranslated data. I feel that although this paper is interesting, it is not compelling enough for inclusion in ICLR. ",Paper Decision
hdnAi6jhjZ,BJxyzxrYPH,Deep geometric matrix completion:  Are we doing it right?,Reject,"This paper proposes a multiresolution spectral geometric loss called the zoomout loss to help with matrix completion, and show state-of-the-art results on several recommendation benchmarks, although experiments also show that the result improvements are not always dependent upon the geometric loss itself.
Reviewers find the idea interesting and the results promising but also have important concerns about the experiments not establishing how the approach truly works. Authors have clarified their explanations in the revisions and provided requested experiments (e.g., on the importance of the initialization size), however important reservations re. why the approach works are still not sufficiently addressed, and would require more iterations to fulfill the potential of this paper.
Therefore, we recommend rejection.",Paper Decision
LI7Lo5ef3,S1e0ZlHYDB,Progressive Compressed Records: Taking a Byte Out of Deep Learning Data,Reject,"Main content: Introduces Progressive Compressed Records (PCR), a new storage format for image datasets for machine learning training.
Discussion:
reviewer 4: Interesting application of progressive compression to reduce the disk I/O overhead. Main concern is paper could be clearer about setting. 
reviewer 5: (not knowledgable about area): well-written paper. concern is that related work could be better, including state of the art on the topic.
reviewer 2: likes the topic but discusses many areas for improvement (stronger exeriments, better metrics reported, etc.). this is probably the most experienced reviewer marking reject.
reviewer 3: paper is well written. Main issue is that exeriments are limited to image classification tasks, and it snot clear how the method works on larger scale.  
Recommendation: interesting idea but experiments could be stronger. I lean to Reject.",Paper Decision
ka03bK43qM,SJxTZeHFPH,The Intriguing Effects of Focal Loss on the Calibration of Deep Neural Networks,Reject,"The paper investigates the effect of focal loss on calibration of neural nets.

On one hand, the reviewers agree that this paper is well-written and the empirical results are interesting. On the other hand, the reviewers felt that there could be better evaluation of the effect of calibration on downstream tasks, and better justification for the choice of optimal gamma (e.g. on a simpler problem setup).

I encourage the others to revise the draft and resubmit to a different venue.  
",Paper Decision
VrdXmrpI5n,ryx6WgStPB,Hypermodels for Exploration,Accept (Poster),"This paper considers ensemble of deep learning models in order to quantify their epistemic uncertainty and use this for exploration in RL. The authors first show that limiting the ensemble to a small number of models, which is typically done for computational reasons, can severely limit the approximation of the posterior, which can translate into poor learning behaviours (e.g. over-exploitation). Instead, they propose a general approach based on hypermodels which can achieve the benefits of a large ensemble of models without the computational issues. They perform experiments in the bandit setting supporting their claim. They also provide a theoretical contribution, proving that an arbitrary distribution over functions can be represented by a linear hypermodel.

The decision boundary for this paper is unclear given the confidence of reviewers and their scores. However, the tackled problem is important, and the proposed approach is sound and backed up by experiments. Most of reviewers concerns seemed to be addressed by the rebuttal, with the exception of few missing references which the authors should really consider adding. I would therefore recommend acceptance.",Paper Decision
GM-O2ojRNJ,ryl3blSFPr,Denoising Improves Latent Space Geometry in Text Autoencoders,Reject,"This work presents a simple technique for improving the latent space geometry of text autoencoders. The strengths of the paper lie in the simplicity of the method, and results show that the technique improves over the considered baselines. However, some reviewers expressed concerns over the presented theory for why input noise helps, and did not address concerns that the theory was useful. The paper should be improved if Section 4 were instead rewritten to focus on providing intuition, either with empirical analysis, results on a toy task, or clear but high level discussion of why the method helps. The current theorem statements seem either unnecessary or make strong assumptions that don't hold in practice. As a result, Section 4 in its current form is not in service to the reader's understanding why the simple method works. 
Finally, further improvements to the paper could be made with comparisons to additional baselines from prior work as suggested by reviewers.",Paper Decision
CjzLx08ckf,Skeh-xBYDH,On Symmetry and Initialization for Neural Networks,Reject,"The two main concerns raised by reviewers is that whether the results are significant, and a potential issue in the proof. While the rebuttal clarified some steps in the proof, the main concerns about the significance remain. The authors are encouraged to make this significance more clear.

Note that one reviewer argued theoretical papers are not suitable for ICLR. This is false, as a theoretical understanding of neural networks remains a key research area that is of wide interest to the community. Consequently, this review was not considered in the final evaluation.",Paper Decision
ubtOjZsiZ,HkgsWxrtPB,Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies,Accept (Poster),"This work formulates and tackles a few-shot RL problem called subtask graph inference, where hierarchical tasks are characterized by a graph describing all subtasks and their dependencies. In other words, each task consists of multiple subtasks and completing a subtask provides a reward. The authors propose a meta-RL approach to meta-train a policy that infers the subtask graph from any new task data in a few shots. Empirical experiments are performed on different domains, including Startcraft II, highlighting the efficiency and scalability of the proposed approach.

Most concerns of reviewers were addressed in the rebuttal. The main remaining concerns about this work are that it is mainly an extension of Sohn et al. (2018), making the contribution somewhat incremental, and that its applicability is limited to problems where subtasks are provided. However, all reviewers being positive about this paper, I would still recommend acceptance. ",Paper Decision
3Orygi4zw,ByliZgBKPH,Policy path programming,Reject,"The reviewers were not convinced about the significance of this work. There is no empirical or theoretical result justifying why this method has advantages over the existing methods. The reviewers also raised concerns related to the scalability of the proposal. Since none of the reviewers were enthusiastic about the paper, including the expert ones, I cannot recommend acceptance of this work.",Paper Decision
wUcxxqypMK,B1gcblSKwB,Meta-Learning with Network Pruning for Overfitting Reduction,Reject,"This paper proposes a regularization scheme for reducing meta-overfitting. After the rebuttal period, the reviewers all still had concerns about the significance of the paper's contributions and the thoroughness of the empirical study. As such, this paper isn't ready for publication at ICLR. See the reviewer's comments for detailed feedback on how to improve the paper. ",Paper Decision
28kwLPRzOM,Byg9bxrtwS,Kernel and Rich Regimes in Overparametrized Models,Reject,"The paper studies how the size of the initialization of neural network weights affects whether the resulting training puts the network in a ""kernel regime"" or a ""rich regime"". Using a two-layer model they show, theoretically and practically, the transition between kernel and rich regimes. Further experiments are provided for more complex settings.

The scores of the reviewers were widely spread, with a high score (8) from a low confidence reviewer with a very short review. While the authors responded to the reviewer comments, two of the reviewers (importantly including the one recommending reject) did not further engage.

Overall, the paper studies an important problem, and provides insight into how weight initialization size can affect the final network. Unfortunately, there are many strong submissions to ICLR this year, and the submission in its current state is not yet suitable for publication.",Paper Decision
AMKp_qAFhg,rJecbgHtDH,A Boolean Task Algebra for Reinforcement Learning,Reject,"This paper considers the situation where a set of reinforcement learning tasks are related by means of a Boolean algebra.  The tasks considered are restricted to stochastic shortest path problems. The paper shows that learning goal-oriented value functions for subtasks enables the agent to solve new tasks (specified with boolean operations on the goal sets) in a zero-shot fashion.  Furthermore, the Boolean operations on tasks are transformed to simple arithmetic operations on the optimal action-value functions, enabling the zero short transfer to a new task to be computationally efficient. This approach to zero-shot transfer is tested in the four room domain without function approximation and a small video game with function approximation.

The reviewers found several strengths and weaknesses in the paper.  The paper was clearly written.  The experiments support the claim that the method supports zero-shot composition of goal-specified tasks.  The weaknesses lie in the restrictive assumptions.  These assumptions require deterministic transition dynamics, reward functions that only differ on the terminal absorbing states, and having only two different terminal reward values possible across all tasks.  These assumptions greatly restrict the applicability of the proposed method.  The author response and reviewer comments indicated that some aspects these restrictions can be softened in practice, but the form of composition described in this paper is restrictive.  The task restrictions also seem to limit the method's utility on general reinforcement learning problems.

The paper falls short of being ready for publication at ICLR.  Further justification of the restrictive assumptions is required to convince the readers that the forms of composition considered in this paper are adequately general. ",Paper Decision
J9xLvtkRFz,H1xFWgrFPS,Explanation  by Progressive  Exaggeration,Accept (Spotlight),"This paper presents an idea for interpolating between two points in the decision-space of a black-box classifier in the image-space, while producing plausible images along the interpolation path. The presentation is clear and the experiments support the premise of the model.
While the proposed technique can be used to help understanding how a classifier works, I have strong reservations in calling the generated samples ""explanations"". In particular, there is no reason for the true explanation of how the classifier works to lie in the manifold of plausible images. This constraint is more of a feature to please humans rather than to explain the geometry of the decision boundary.
I believe this paper will be well-received and I suggested acceptance, but I believe it will be of limited usefulness for robust understanding of the decision boundary of classifiers.",Paper Decision
z5z1tE1UOS,ryxtWgSKPB,Quantum Optical Experiments Modeled by Long Short-Term Memory,Reject,"The paper predicts properties of quantum states through RNNs.  The idea is nice, but the results are very limited and require more work.  It seems to be more suited for a conference focussing on quantum ML---even when the authors have an ML background.

All reviewers agree on a rejection, and their arguments are solid.  The authors offered no rebuttal.",Paper Decision
qD1q5Z6ta,S1l_ZlrFvS,Why do These Match? Explaining the Behavior of Image Similarity Models,Reject,"This submission proposes an explainability method for deep visual representation models that have been trained to compute image similarity. 

Strengths:
-The paper tackles an important and overlooked problem.
-The proposed approach is novel and interesting.

Weaknesses:
-The evaluation is not convincing. In particular (i) the evaluation is performed only on ground-truth pairs, rather than on ground-truth pairs and predicted pairs; (ii) the user study doesn’t disambiguate whether users find the SANE explanations better than the saliency map explanations or whether users tend to find text more understandable in general than heat maps. The user study should have compared their predicted attributes to the attribute prediction baseline; (iii) the explanation of Figure 4 is not convincing: the attribute is not only being removed. A new attribute is also being inserted (i.e. a new color). Therefore it’s not clear whether the similarity score should have increased or decreased; (iv) the proposed metric in section 4.2 is flawed: It matters whether similarity increases or decreases with insertion or deletion. The proposed metric doesn’t reflect that.
-Some key details, such as how the attribute insertion process was performed, haven’t been explained. 

The reviewer ratings were borderline after discussion, with some important concerns still not having been addressed after the author feedback period. Given the remaining shortcomings, AC recommends rejection.",Paper Decision
iFpAc-yVKO,rkeO-lrYwr,Mode Connectivity and Sparse Neural Networks,Reject,"This paper investigates theories related to networks sparsification, related to mode connectivity and the so-called lottery ticket hypothesis.  The paper is interesting and has merit, but on balance I find the contributions not sufficiently clear to warrant acceptance.  The authors made substantial changes to the paper which are admirable and which bring it to borderline status. 
",Paper Decision
gDoYF23FX,HyePberFvH,Monte Carlo Deep Neural Network Arithmetic,Reject,"The paper studies the impact of rounding errors on deep neural networks. The                                                       
authors apply Monte Carlos arithmetics to standard DNN operations.                                                                 
Their results indeed show catastrophic cancellation in DNNs and that the resulting loss of                                         
significance in the number representation correlates with decrease in validation                                                   
performance, indicating that DNN performances are sensitive to rounding errors.                                                    
                                                                                                                                   
Although recognizing that the paper addresses an important problem (quantized /                                                    
finite precision neural networks), the reviewers point out the contribution of                                                     
the paper is somewhat incremental.                                                                                                 
During the rebuttal, the authors made an effort to improve the manuscript based                                                    
on reviewer suggestions, however review scores were not increased.                                                                 
                                                                                                                                   
The paper is slightly below acceptance threshold, based on reviews and my own                                                      
reading, as the method is mostly restricted to diagnostics and cannot yet be used                                                  
to help training low-precision neural networks.",Paper Decision
UiUyNvGyv7,r1xI-gHFDH,How can we generalise learning distributed representations of graphs?,Reject,"The paper proposed a general framework to construct unsupervised models for representation learning of discrete structures. The reviewers feel that the approach is taken directly from graph kernels, and the novelty is not high enough. ",Paper Decision
5OPLk_z4sS,BJl8ZlHFwr,Relation-based Generalized Zero-shot Classification with the Domain Discriminator on the shared representation,Reject,"This paper proposes a relation-based model that extends VAE to explicitly alleviate the domain bias problem between seen and unseen classes in the setting of generalized zero-shot learning.

Reviewers and AC think that the studied problem is interesting, the reported experimental results are strong, and the writing is clear, but the proposed model and its scientific reasoning for convincing why the proposed method is valuable is somewhat limited. Thus the authors are encouraged to further improve in these directions. In particular:

- The idea of using a variant of the widely-used domain discriminator to make seen and unseen classes distinguishable is somewhat contradicted to the basic principle of zero-shot learning. How to trade off the balance between seen and unseen classes has been an important problem in generalized ZSL. These problems need further elaboration.

- The proposed model itself is not a real ""VAE"", making the value of an extensive derivation based on variational inference less prominent. 

- There is also the need to compare with the baselines mentioned by the reviewers. 

Overall, this is a borderline paper. Since the above concerns were not addressed convincingly in the rebuttal, I am leaning towards rejection.",Paper Decision
FbM9YFgsS1,BJxSWeSYPB,Self-supervised Training of Proposal-based Segmentation via Background Prediction,Reject,"This work proposes a self-supervised segmentation method: building upon Crawford and Pineau 2019, this work adds a Monte-Carlo based training strategy to explore object proposals.
Reviewers found the method interesting and clever, but shared concerns about the lack of a better comparison to Crawford and Pineau, as well as generally a lack of care in comparisons to others, which were not satisfactorily addressed by authors response.
For these reasons, we recommend rejection.",Paper Decision
wd-EqSBWJJ,S1lNWertDr,Decoupling Hierarchical Recurrent Neural Networks With Locally Computable Losses,Reject,"All reviewers gave this paper a score of 1.
The AC recommends rejection.",Paper Decision
p4aynpOYq-,HJe7bxBYvr,Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning,Reject,"This paper tackles the problem of safe exploration in RL. The proposed approach uses an imaginative module to construct a connectivity graph between all states using forward predictions. The idea then consists in using this graph to plan a trajectory which avoids states labelled as ""unsafe"".

Several concerns were raised and the authors did not provide any rebuttal. A major point is that the assumption that the approach has access to what are unsafe states, which is either unreasonable in practice or makes the problem much simpler. Another major point is the uniform data collection about every state-action pairs. This can be really unsafe and defeats the purpose of safe exploration following this phase. These questions may be due to a miscomprehension, indicating that the paper should be clarified, as demanded by reviewers. Finally, the experiments would benefit from additional details in order to be correctly understood.

All reviewers agree that this paper should be rejected. Hence, I recommend reject.",Paper Decision
P4HBDZALQ,Hkem-lrtvH,BayesOpt Adversarial Attack,Accept (Poster),"This paper proposes a query-efficient black-box attack that uses Bayesian optimization in combination with Bayesian model selection to optimize over the adversarial perturbation and the optimal degree of search space dimension reduction. The method can achieve comparable success rates with 2-5 times fewer queries compared to previous state-of-the-art black-box attacks. The paper should be further improved in the final version (e.g., including more results on ImageNet data).",Paper Decision
lDXI0VGDVg,SyeMblBtwr,CrossNorm: On Normalization for Off-Policy Reinforcement Learning,Reject,"This is certainly a boarderline paper. The reviewers agreed this paper provides a good explanation and empirical justification of why popular normalization schemes don't help in DRL. The paper then proposes a simple scheme and demonstrates how it improves learning in several domains. The main concerns are the nature of these gains and how broadly useful the new approach is. In many cases there appear to be somewhat clear wins in the middle of the learning curves, but by the end of each experiment the errorbars overlap. The most clear results are those with TD3. There are some oddities here: using half SD error bars and smoothing---both underline the concern about significance. 

The reviewers requested more experiments and the authors provided three more domains: two in which their method appears better. These are not widely used benchmarks and it was hard to compare the performance of the baselines with fan et al (different setup) to evaluate the claims. The paper nicely provides lots of insight and empirical wisdom in the appendix, explaining how they got the algorithms to perform well.   
",Paper Decision
NeSqmWx34,BJeGZxrFvS,A Simple Technique to Enable Saliency Methods to Pass the Sanity Checks,Reject,"This submission proposes a method to pass sanity checks on saliency methods for model explainability that were proposed in a prior work.

Pros:
-The method is simple, intuitive and does indeed pass the proposed checks.

Cons:
-The proposed method aims to pass the sanity checks, but is not well-evaluated on whether it provides good explanations. Passing these checks can be considered as necessary but not sufficient.
-All reviewers agreed that the evaluation could be improved and most reviewers found the evaluation insufficient.

Given the shortcomings, AC agrees with the majority recommendation to reject.
",Paper Decision
2TUEz6rFkb,B1eWbxStPH,Directional Message Passing for Molecular Graphs,Accept (Spotlight),"This paper studies Graph Neural Networks for quantum chemistry by incorporating a number of physics-informed innovations into the architecture. In particular, it considers directional edge information while preserving equivariance.

Reviewers were in agreement that this is an excellent paper with strong empirical results, great empirical evaluation and clear exposition. Despite some concerns about the limited novelty in terms of GNN methodology ( for instance, directional message passing has appeared in previous GNN papers, see e.g. https://openreview.net/forum?id=H1g0Z3A9Fm , in a different context). Ultimately, the AC believes this is a strong, high quality work that will be of broad interest, and thus recommends acceptance. ",Paper Decision
OfRwYSTzXM,HJe-blSYvH,Unsupervised Learning of Efficient and Robust Speech Representations,Reject,"The paper focuses on learning speech representations with contrastive predictive coding (CPC). As noted by reviewers, (i) novelty is too low (mostly making the model bidirectional) for ICLR (ii) comparison with existing work is missing.",Paper Decision
Eq55j-Xan0,BJx-ZeSKDB,Compositional Embeddings: Joint Perception and Comparison of Class Label Sets,Reject,"The authors propose a new type of compositional embedding (with two proposed variants) for performing tasks that involve set relationships between examples (say, images) containing sets of classes (say, objects).  The setting is new and the reviewers are mostly in agreement (after discussion and revision) that the approach is interesting and the results encouraging.  There is some concern, however, that the task setup may be too contrived, and that in any real task there could be a more obvious baseline that would do better.  For example, one task setup requires that examples be represented via embeddings, and no reference can be made to the original inputs; this is justified in a setting where space is a constraint, but the combination of this setting with the specific set query tasks considered seems quite rare.  The paper may be an example of a hammer in search of a nail.  The ideas are interesting and the paper is written well, and so the authors can hopefully refine the proposed class of problems toward more practical settings.",Paper Decision
_4uNzD_Rnj,HklxbgBKvr,Model-based reinforcement learning for biological sequence design,Accept (Poster),"The paper proposes a model based proximal policy optimization reinforcement learning algorithm for designing biological sequences. The policy of for a new round is trained on data generated by a simulator. The paper presents empirical results on designing sequences for transcription factor binding sites, antimicrobial proteins, and Ising model protein structures.

Two of the reviewers are happy to accept the paper, and the third reviewer was not confident. The paper has improved significantly during the discussion period, and the authors have updated the approach as well as improved the presented results in response to comments raised by the reviewers. This is a good example of how an open review process with a long discussion period can improve the quality of accepted papers.

A new method, several nice applications, based on a combination of two ideas (simulating a model to train a policy RL method, and discrete space search as RL). This is a good addition to the ICLR literature.",Paper Decision
bD1zs1XOHo,rklx-gSYPS,Learning to Optimize via Dual space Preconditioning,Reject,"Thanks for the detailed replies to the reviewers, which significantly helped us understand your paper better.
However, after all, we decided not to accept your paper due to weak justification and limited experimental validation. Writing should also be improved significantly. We hope that the feedback from the reviewers help you improve your paper for potential future submission.
",Paper Decision
S39Mmu6hZ,B1xybgSKwB,Self-Attentional Credit Assignment for Transfer in Reinforcement Learning,Reject,"The paper introduces a novel approach to transfer learning in RL based on credit assignment. The reviewers had quite diverse opinions on this paper. The strength of the paper is that it introduces an interesting new direction for transfer learning in RL. However, there are some questions regarding design choices and whether the experiments sufficiently validate the idea (i.e., the sensitivity to hyperparameters is a  question that is not sufficiently addressed). Overall, this research has great potential. However, a more extensive empirical study is necessary before it can be accepted.",Paper Decision
orGK3FEZfF,HJlk-eHFwH,AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion,Reject,"The paper has major presentation issues. The rebuttal clarified some technical ones, but it is clear that the authors need to improve the reading substantially, ,so the paper is not acceptable in its current form.",Paper Decision
3x4CbROGp,SJxAlgrYDr,City Metro Network Expansion with Reinforcement Learning,Reject,"The paper explores the use of RL (actor-critic) for planning the expansion of a metro subway network in a City. The reviewers felt that novelty was limited and there was not enough motivation on what is special about this application, and what lessons can be learned from this exercise.  ",Paper Decision
bKgiICH2iO,r1x0lxrFPS,BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations,Accept (Poster),Three reviewers suggest acceptance. Reviewers were impressed by the thoroughness of the author response. Please take reviewer comments into account in the camera ready. Congratulations!,Paper Decision
4E0bPZySHS,S1xRxgSFvH,ShardNet: One Filter Set to Rule Them All,Reject,"This submission proposes an interesting experiment/modification of CNNs. However, it looks like this contribution overlaps significantly with prior work (that the authors initially missed) and the comparison in the (revised) manuscript seem to not clearly delineate and acknowledge the similarities and differences.

I suggest the authors improve this aspect and try submitting this work to next venue. ",Paper Decision
hYlQMrsvmf,HJxTgeBtDr,Towards Interpretable Evaluations: A Case Study of Named Entity Recognition,Reject,"The paper diligently setup and conducted multiple experiments to validate their approach - bucketizating attributions of data and analyze them accordingly to discover deeper insights eg biases. However, reviewers pointed out that such bucketing is tailored to tasks where attributions are easily observed, such as the one of the focus in this paper -NER. While manuscript proposes this approach as ‘general’, reviewers failed to seem this point. Another reviewer recommended this manuscript to become a journal item rather than conference, due to the length of the page in appendix (17). There were some confusions around writings as well, pointed out by some reviewers. We highly recommend authors to carefully reflect on reviewers both pros and cons of the paper to improve the paper for your future submission. 
",Paper Decision
wMXTh4unAH,S1g6xeSKDS,Mixed-curvature Variational Autoencoders,Accept (Poster),"This paper studies generalizations of Variational Autoencoders to Non-Euclidean domains, modeled as products of constant curvature Riemannian manifolds. The framework allows to simultaneously learn the latent representations as well as the curvature of the latent domain. 

Reviewers were unanimous at highlighting the significance of this work at developing non-Euclidean tools for generative modeling. Despite the somewhat preliminary nature of the empirical evaluation, there was consensus that the paper puts forward interesting tools that might spark future research in this direction. Given those positive assessments, the AC recommends acceptance. 
",Paper Decision
hRkpIrnja,rJehllrtDS,Rethinking deep active learning: Using unlabeled data at model training,Reject,"This paper argues that incorporating unsupervised/semi-supervised learning into the training process can dramatically increase the performance of models. In particular, its incorporation can result in performance gains that dwarf the gains obtained by collecting data actively alone. The experiments effectively demonstrate this phenomenon. 

The paper is written with a tone that implicitly assumes that ""active learning for deep learning is effective"" and therefore it is a surprise and a challenge to the status quo that using unlabelled data in intelligent ways alone gets such a boost. On the contrary, reviewers found that active learning not working very well for deep learning is a well-known state of affairs. This is not surprising because the most effective theoretically justifiable active learning algorithms rely on finite capacity assumptions about the model class, which deep learning disobeys. 

Thus, the reviewers found the conclusions to lack novelty as the power of semi-supervised and unsupervised learning is well known. Reject. ",Paper Decision
Az2LN4BH1R,r1ghgxHtPH,Blurring Structure and Learning to Optimize and Adapt Receptive Fields,Reject,"The paper proposes an interesting idea of inserting Gaussian convolutions into ConvNet in order to increase and to adapt effective receptive fields of network units. The reviewers generally agree that the idea is interesting and that the results on CityScapes are promising. However, it is hard not to agree with Reviewer 3, that validation on a single dataset for a single task is not sufficient. This criticism is unaddressed. ",Paper Decision
shxyC0wba,BkxoglrtvH,Layerwise Learning Rates for Object Features in Unsupervised and Supervised Neural Networks And Consequent Predictions for the Infant Visual System,Reject,"This paper investigates the properties of deep neural networks as they learn, and how they may relate to human visual learning (e.g. how learning develops across regions of the infant brain). The paper received three reviews, all of which recommended Weak Reject. The reviewers generally felt the topic of the paper was very interesting, but overall felt that the insights that the paper revealed were relatively modest, and had concerns about the connections between DNN and human learning (e.g., the extent to which DNNs are biologically plausible -- including back propagation, batch normalization, random initialization, etc. -- and whether this matters for the conclusions of the present study). In response to comments, the authors undertook a significant revision to try to address these points of confusion. However, the reviewers were still skeptical and chose to keep their Weak Reject scores.

The AC agrees with reviewers that investigations of the similarity -- or not! -- between infant and deep neural networks is extremely interesting and, as the authors acknowledge, is a high risk but potentially very high reward research direction. However, in light of the reviews with unanimous Weak Reject decisions, the AC is not able to recommend acceptance at this time. I strongly encourage authors to continue this work and submit to another venue; this would seem to be a perfect match for CogSci conference, for example. We hope the reviews below help authors to improve their manuscript for this next submission.",Paper Decision
-MJSAJaZXj,r1xjgxBFPB,Continual Deep Learning by Functional Regularisation of Memorable Past,Reject,"This work tackles the problem of catastrophic forgetting by using Gaussian processes to identify ""memory samples"" to regularize learning.

Although the approach seems promising and well-motivated, the reviewers ultimately felt that some claims, such as scalability, need stronger justifications. These justifications could come, for example, from further experiments, including ablation studies to gain insights. Making the paper more convincing in this way is particularly desirable since the directions taken by this paper largely overlap with recent literature (as argued by reviewers).
",Paper Decision
-fUDu4ZUH4,Hyl9xxHYPr,Demystifying Inter-Class Disentanglement,Accept (Poster),"This paper proposes a novel method for class-supervised disentangled representation learning. The method augments an autoencoder with asymmetric noise regularisation and is able to disentangled content (class) and style information from each other. The reviewers agree that the method achieves impressive empirical results and significantly outperforms the baselines. Furthermore, the authors were able to alleviate some of the initial concerns raised by the reviewers during the discussion stage by providing further experimental results and modifying the paper text. By the end of the discussion period some of the reviewers raised their scores and everyone agreed that the paper should be accepted. Hence, I am happy to recommend acceptance.",Paper Decision
yK4J9sWXy6,r1lclxBYDS,On the implicit minimization of alternative loss functions when training deep networks,Reject,"The paper proposes an interesting setting in which the effect of different optimization parameters on the loss function is analyzed.  The analysis is based on considering cross-entropy loss with different softmax parameters, or hinge loss with different margin parameters.  The observations are interesting but ultimately the reviewers felt that the experimental results were not sufficient to warrant publication at ICLR.  The reviews unanimously recommended rejection, and no rebuttal was provided.",Paper Decision
qn_a7pjqMj,B1eYlgBYPH,A Deep Recurrent Neural Network via Unfolding Reweighted l1-l1 Minimization,Reject,"This paper presents a novel RNN algorithm based on unfolding a reweighted L1-L1 minimization problem. Authors derive the generalization error bound which is tighter than existing methods. 
All reviewers appreciate the theoretical contributions of the paper, particularly the derivation of generalization error bounds. However, at a higher-level, the overall idea is incremental because RNN by unfolding L1-L1 minimization problem (Le+,2019) and reweighted L1 minimization (Candes+,2008) are both known techniques. The proposed method is essentially a simple combination of them and therefore the result seems somewhat obvious. Also, I agree with reviewers that some experiments are not deep enough to support the theory. For example, for over-parameterization (large model parameters) issue, one can compare the models with the same number of parameters and observe how they generalize. 
Overall, this is the very borderline paper that provides a good theoretical contribution with limited conceptual novelty and empirical evidences. As a conclusion, I decided to recommend rejection but could be accepted if there is a room.
",Paper Decision
-is9Sw_9Qr,HygFxxrFvB,Differentially Private Mixed-Type Data Generation For Unsupervised Learning,Reject,"This provides a new method, called DPAutoGAN, for the problem of differentially private synthetic generation. The method uses private auto-encoder to reduce the dimension of the data, and apply private GAN on the latent space. The reviewers think that there is not sufficient justification for why this is a good approach for synthetic generation. They also think that the presentation is not ready for publication.",Paper Decision
WTOjR_bWd,SkeuexBtDr,Learning from Rules Generalizing Labeled Exemplars,Accept (Spotlight),"The paper addresses the problem of costly human supervision for training supervised learning methods.
The authors propose a joint approach for more effectively collecting supervision data from humans, by extracting rules and their exemplars, and a model for training on this data.
They demonstrate the effectiveness of their approach on multiple datasets by comparing to a range of baselines.

Based on the reviews and my own reading I recommend to accept this paper.
The approach makes intuitively a lot of sense and is well explained.
The experimental results are convincing. ",Paper Decision
5C4cMWL6ve,rkxdexBYPB,Group-Transformer: Towards A Lightweight Character-level Language Model,Reject,"This paper proposes using a lightweight alternative to Transformer self-attention called Group-Transformer. This is proposed in order to overcome difficulties in modelling long-distance dependencies in character level language modelling. They take inspiration from  work on group convolutions. They experiment on two large-scale char-level LM datasets which show positive results, but experiments on word level tasks fail to show benefits. I think that this work, though promising, is still somewhat incremental and has not shown to be widely applicable, and therefore I recommend that it is not accepted. ",Paper Decision
0vWK7djX8k,HylvleBtPB,Language-independent Cross-lingual Contextual Representations,Reject,"The paper proposes a method to learn cross-lingual representations by aligning monolingual models with the help of a parallel corpus using a three-step process: transform, extract, and reorder. Experiments on XNLI show that the proposed method is able to perform zero-shot cross-lingual transfer, although its overall performance is still below state-of-the-art jointly trained method XLM.

All three reviewers suggested that the proposed method needs to be evaluated more thoroughly (more datasets and languages). R2 and R4 raise some concerns around the complexity of the proposed method (possibly could be simplified further). R3 suggests a more thorough investigation on why the model saturates at 250,000 parallel sentences, among others.

The authors acknowledged reviewers' concerns in their response and will incorporate them in future work.

I recommend rejecting this paper for ICLR.",Paper Decision
M6gEmVoYAJ,r1lPleBFvH,Understanding the Limitations of Conditional Generative Models,Accept (Poster),"This paper presents theoretical results showing the conditional generative models cannot be robust. The paper also provide counter examples and some empirical evidence showing that the theory is reflected in practice. Some reviewers doubt how much of the theory holds in reality, but still they think that this paper could be a useful for the community. After the rebuttal period, R2 increased their score and it seems that with the current score the paper can be accepted.",Paper Decision
YGG3wayhq-,HJewxlHFwH,Skew-Explore: Learn faster in continuous spaces with sparse rewards,Reject,"While the reviewers generally appreciated the ideas presented in the paper and found the overall aims and motivation of the paper to be compelling, there were too many questions raised about the experiments and the soundness of the technical formulation to accept the paper at this time, and the reviewers did not feel that the authors had adequately addressed these issues in their responses. The main concerns were (1) with the correctness and rigor of the technical derivation, which the reviewers generally found to be somewhat questionable -- while the main idea seems reasonable, the details have a few too many question marks; (2) the experimental results have a number of shortcomings that make it difficult to fully understand whether the method really works, and how well.",Paper Decision
1PhjIJC0KT,rkgIllBtwB,Exploring the Correlation between Likelihood of Flow-based Generative Models and Image Semantics,Reject,"This paper discusses the (lack of) correlation between the image semantics and the likelihood assigned by flow-based models, and implications for out-of-distribution (OOD) detection.

The reviewers raised several important questions:
1) precise definition of OOD: definition of semantics vs typicality (cf. definition in Nalisnick et al. 2019 pointed by R1)
There was a nice discussion between authors and the reviewers. At a high level, there was some agreement in the end, but lack of precise definition may cause confusion. I think adding a precise definition will add more clarity and improve the paper.

2) novelty: similar observations have been made in earlier papers cf. Nalisnick et al. 2018. R3 also pointed a recent paper by Ren et al. 2019 which showed that likelihood can be dominated by background pixels. Older work has shown that the likelihood and sample quality are not necessarily correlated. The reviewers appreciate that this paper provides additional evidence, but weren't convinced that the new observations in this paper qualified for a full paper.

3) experiments on more datasets

Overall, while this paper explores an interesting direction, it's not ready for publication as is. I encourage the authors to revise the paper based on the feedback and submit to a different venue.",Paper Decision
LTYVxGqw2O,r1xHxgrKwr,Anomaly Detection Based on Unsupervised Disentangled Representation Learning in Combination with Manifold Learning,Reject,"The paper presents AnoDM (Anomaly detection based on unsupervised Disentangled representation learning and Manifold learning) that combine beta-VAE and t-SNE for anomaly detection. Experiment results on both image and time series data are shown to demonstrate the effectiveness of the proposed solution. 

The paper aims to attack a challenging problem. The proposed solution is reasonable. The authors did a job at addressing some of the concerns raised in the reviews. However, two major concerns remain: (1) the novelty in the proposed model (a combination of two existing models) is not clear; (2) the experiment results are not fully convincing. While theoretical analysis is not a must for all models, it would be useful to conduct thorough experiments to fully understand how the model works, which is missing in the current version. 

Given the two reasons above, the paper did not attract enough enthusiasm from the reviewers during the discussion. We hope the reviews can help improve the paper for a better publication in the future. 



",Paper Decision
jB_WnSCRH,HyerxgHYvH,Neural Arithmetic Unit by reusing many small pre-trained networks,Reject,"This paper proposes to train and compose neural networks for the purposes of arithmetic operations. All reviewers agree that the motivation for such a work is unclear, and the general presentation in the paper can be significantly improved. As such, I cannot recommend this paper in its current state for publication.
",Paper Decision
_6gQVmLRxq,rkxNelrKPB,On Stochastic Sign Descent Methods,Reject,"This paper proposes an analysis of signSGD in some special cases. SignGD has been shown to be of interest, whether because of its similarity to Adam or in quasi-convex settings.

The complaint shared by reviewers was the strength of the conditions. SGC is really strong, I have yet to see increasing mini-batch sizes to be used in practice (although there are quite a  few papers mentioning this technique to get a convergence rate) and the strength of the other two are harder to assess. With that said, the improvement compared to existing work such as Karimireddy et. al. 2019 is unclear.

I encourage the authors to address the comment of the reviewers and to submit an improved version to a later, or perhaps to a more theoretical, convergence.",Paper Decision
D3pfQB_9Z4,H1eNleBYwr,GENN: Predicting Correlated Drug-drug Interactions with Graph Energy Neural Networks,Reject,"This paper studies the use of a graph neural network for drug-to-drug interaction (DDI) prediction task (an instance of a link prediction task with drugs as vertices and interaction as edges). In particular, the authors apply structured prediction energy networks (SPEN) and model the dependency structure of the labels by minimising an energy function. The authors empirically validate the proposed approach against feedforward GNNs on two DDI prediction tasks. The reviewers feel that understanding drug-drug interactions is an important task and that the work is well motivated. However, the reviewers argued that the proposed methodology is not novel enough to merit publication at ICLR and that some conclusions are not supported by the empirical analysis. For the former, the benefits of the semi-supervised design need to be clearly and concisely presented. For the latter, providing a more convincing practical benefit would greatly improve the manuscript. As such, I will recommend the rejection of this paper at the current state.",Paper Decision
agCVrWiLFe,H1eVlgHKPr,Event Discovery for History Representation in Reinforcement Learning,Reject,The authors propose approaches to handle partial observability in reinforcement learning. The reviewers agree that the paper does not sufficiently justify the methods that are proposed and even the experimental performance shows that the proposed method is not always better than baselines.,Paper Decision
PvkQ8cMe9M,rke7geHtwH,Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning,Accept (Poster),"The authors present a novel stable RL algorithm for the batch off-policy setting, through the use of a learned prior.  Initially, reviewers had significant concerns about (1) reproducibility, (2) technical details, including the non-negativity of the lagrange multiplier, (3) a lack of separation between performance contributions of ABM and MPO, (4) baseline comparisons.  The authors satisfactorily clarified points (1)-(3) and the simulated baseline comparisons for (4) seem reasonable in light of how long the real robot experiments took, as reported by the authors.  Futhermore, the reviewers all agree on the contribution of the core ideas.  Thus, I recommend this paper for acceptance.",Paper Decision
eO0TSjvspL,BJxQxeBYwH,Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification,Reject,"This paper proposes to split the GNN operations into two parts and study the effects of each part. While two reviewers are positive about this paper, the other reviewer R1 has raised some concerns. During discussion, R1 responded and indicated that his/her concerns were not addressed in author rebuttal. Overall, I feel the paper is borderline and lean towards reject.",Paper Decision
phI1I6Oav,B1xGxgSYvH,Domain-Invariant Representations: A Look on Compression and Weights,Reject,"This paper provides a new theoretical framework for domain adaptation by exploring the compression and adaptability.

Reviewers and AC generally agree that this paper discusses about an important problem and provides new insight, but it is not a thorough theoretical work. The reviewers identified several key limitations of the theory such as unrealistic condition and approximation. Some important points still require more work to make the framework practical for algorithm design and computation. The presentation could also be improved.

Hence I recommend rejection.",Paper Decision
NOHiNs2Mq,HJlzxgBtwH,Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack,Reject,"This work presents a method for generating an (approximately) minimal adversarial perturbation for neural networks. During the discussion period, the AC raised additional concerns that were not originally addressed by the reviewers. The method is an iterative first order method for solving constrained optimization problems, however when considered as a new first order optimization method the contribution seems minimal. Most of the additions are rather straightforward---e.g. using a line search at each step to determine the optimal step size---and the reported gains over PGD are unconvincing. PGD can be considered as a ""universal"" first order optimizer [1], as such we should be careful that the reported gains are substantial and not just a question of tuning. Given that using a line search at each step increases the computational cost by a multiplicative factor, the comparison with PGD should take this into account.

The AC notes several plots in the Appendix show PGD having better performance (particularly on restricted Imagenet), and for others there remain questions on how PGD is tuned (for example the CIFAR-10 plots in Figure 5). One of two things explains the discrepancies in Figure 5: either PGD is finding a worse local optimum than FAB, or PGD has not converged to a local optimum. There needs to be provided experiments to rule out the second possibility, as this is evidence that PGD is not being tuned properly. Some standard things to check are the step size and number of steps. Additionally, enforcing a constant step size after projection is an easy way to improve the performance of PGD. For example, if the gradient of the loss is approximately equal to the normal vector of the constraint, then proj(x_i+ lambda * g) ~ x_i will result in an effective step size that is too low to make progress.

Finally, it is unclear what practical use there is for a method that finds an approximately minimum norm perturbation. There are no provable guarantees so this cannot be used for certification. Additionally, in order to properly assess the security and reliability of ML systems, it is necessary to consider larger visual distortions, occlusions, and corruptions (such as the ones in [2]) as these will actually be encountered in practice. 

1. https://arxiv.org/pdf/1706.06083.pdf
2. https://arxiv.org/abs/1807.01697",Paper Decision
M8Eje-Bmkv,Hkg-xgrYvH,Empirical Bayes Transductive Meta-Learning with Synthetic Gradients,Accept (Poster),"Three reviewers have assessed this paper and they have scored it 6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.",Paper Decision
P7wcJAFekV,rJxWxxSYvB,Spike-based causal inference for weight alignment,Accept (Poster),"All authors agree the paper is well written, and there is a good consensus on acceptance.  The last reviewer was concerned about a lack of diversity in datasets, but this was addressed in the rebuttal.",Paper Decision
ObEr38Q6Qd,BylWglrYPH,Symmetry and Systematicity,Reject,"Thanks for clarifying several issues raised by the reviewers, which helped us understand the paper.

After all, we decided not to accept this paper due to the weakness of its contribution. I hope the updated comments by the reviewers help you strengthen your paper for potential future submission.",Paper Decision
Fs4xmSjke,SJlxglSFPB,Efficacy of Pixel-Level OOD Detection for Semantic Segmentation,Reject,"This paper studies the problem of out-of-distribution (OOD) detection for semantic segmentation.

Reviewers and AC agree that the problem might be important and interesting, but the paper is not ready to publish in various aspects, e.g.,  incremental contribution and less-motivated/convincing experimental setups/results.

Hence, I recommend rejection.",Paper Decision
RG1EvURgtL,SJg1lxrYwS,PatchFormer: A neural architecture for self-supervised representation learning on images,Reject,"The paper presents a generative approach to learn an image representation along a self-supervised scheme.  

The reviews state that the paper is premature for publication at ICLR 2020 for the following reasons:
* the paper is unfinished (Rev#3); in particular the description of the approach is hardly reproducible (Rev#1);
* the evaluation is limited to ImageNet and needs be strenghtened (all reviewers)
* the novelty needs be better explained (Rev#1).
It might be interesting to discuss the approach w.r.t. ""Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles"", Noroozi and Favaro.

I recommend the authors to rewrite and better structure the paper (claim, state of the art, high level overview of the approach, experimental setting, discussion of the results, discussion about the novelty and limitations of the approach). 

",Paper Decision
mNvt_xyu-,SJlJegHFvH,Address2vec: Generating vector embeddings for blockchain analytics,Reject,The paper propose to analyze bitcoin addresses using graph embeddings. The reviewers found that the paper was too incomplete for publication. Important information such as a description of datasets and metrics was omitted.,Paper Decision
mG8yUNB1AS,HkgAJxrYwr,Attack-Resistant Federated Learning with Residual-based Reweighting,Reject,"The paper proposes an aggregation algorithm for federated learning that is robust against label-flipping, backdoor, and Gaussian noise attacks. The reviewers agree that the paper presents an interesting and novel method, however the reviewers also agree that the theory was difficult to understand and that the success of the methodology may be highly dependent on design choices and difficult-to-tune hyperparameters. ",Paper Decision
TdX0BPMmUw,rJxRJeStvB,Learning scalable and transferable multi-robot/machine sequential assignment planning via graph embedding,Reject,"Unfortunately, the reviewers of the paper are all not certain about their review, none of them being RL experts.  Assessing the paper myself—not being an RL expert but having experience—the authors have addressed all points of the reviewers thoroughly.  
",Paper Decision
azQBA1vbaM,HyxTJxrtvr,Learning a Spatio-Temporal Embedding for Video Instance Segmentation,Reject,"This paper proposes a spatio-temporal embedding loss for video instance segmentation. The proposed model (1) learns a per-pixel embedding such that the embeddings of pixels from the same instance are closer than embeddings of pixels from other instances, and (2) learns depth in a self-supervised way using a photometric reconstruction loss which operates under the assumption of a moving camera and a static scene. The resulting loss is a weighted sum of these attraction, repulsion, regularisation and geometric view synthesis losses.
The reviewers agree that the paper is well written and that the problem is well motivated. In particular, there is consensus that the 3D geometry and 2D instance representation should be considered jointly. However, due to the lack of technical novelty, the complexity of the final model, and the issues with the empirical validation of the proposed approach, we feel that the work is slightly below the acceptance bar.",Paper Decision
RY6KFk3nBC,Hkla1eHFvS,Efficient Exploration via State Marginal Matching,Reject,"The paper provides a nice approach to optimizing marginals to improve exploration for RL agents.  The reviewers agree that its improvements w.r.t. the state of the art do not merit a publication at ICLR.  Furthermore, additional experimentation is needed for the paper to be complete.",Paper Decision
Hg7ELoo9Oy,ryl3ygHYDB,Lookahead: A Far-sighted Alternative of Magnitude-based Pruning,Accept (Poster),"This paper introduces a pruning criterion which is similar to magnitude-based pruning, but which accounts for the interactions between layers. The reviewers have gone through the paper carefully, and after back-and-forth with the authors, they are all satisfied with the paper and support acceptance.",Paper Decision
_bSbiPBqP,ryxnJlSKvr,SCELMo: Source Code Embeddings from Language Models,Reject,"This paper improves DeepBugs by borrowing the NLP method ELMo as new representations. The effectiveness of the embedding is investigated using the downstream task of bug detection. 

Two reviewers reject the paper for two main concerns:
1 The novelty of the paper is not strong enough for ICLR as this paper mainly uses a standard context embedding technique from NLP.
2 The experimental results are not convincing enough and more comprehensive evaluation are needed. 

Overall, this novelty of this paper does not meet the standard of ICLR.
",Paper Decision
AA7icvy-ZA,B1esygHFwS,Detecting Change in Seasonal Pattern via Autoencoder and Temporal Regularization,Reject,"The paper proposes ATR-CSPD, which learns a low-dimensional representation of seasonal pattern, for detecting changes with clustering-based approaches. 

While ATR-CSPD is simple and intuitive, it lacks novel contribution in methodology. It is unclear how it is different from existing approaches. The evaluation and the writing could be improved significantly. 

In short, the paper is not ready for publication. We hope the reviews can help improve the paper for a strong submission in the future. ",Paper Decision
PwgaU_btCX,Hkl9JlBYvr,VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning,Accept (Poster),"This paper considers the problem of transfer learning among families of MDP, and proposes a variational Bayesian approach to learn a probabilistic model of a new problem drawn from the same distribution as previous tasks, which is then leveraged during action selection. 

After discussion, the three respondent reviewers converged to the opinion that the paper is novel and interesting, and well evaluated. (Reviewer 1 never responded to any questions the authors or me, so I have disregarded their review.) I am therefore recommending an accept.",Paper Decision
XEgYShJzuK,Bkl5kxrKDr,A Generalized Training Approach for Multiagent Learning,Accept (Talk),"This paper analyzes and extends learning methods based on Policy-Spaced Response Oracles (PSRO) through the application of alpha-rank.  In doing so, the paper explores connections with Nash equilibria, establishes convergence guarantees in multiple settings, and presents promising empirical results on (among other things) 3-to-5 player poker games.

Although this paper originally received mixed scores, after the rebuttal period all reviewers converged to a consensus. A revised version also includes new experiments from the MuJoCo soccer domain, and new poker results as well.  Overall, this paper provides a nice balance of theoretical support and practical relevance that should be of high impact to the RL community. ",Paper Decision
kM1GckIAm5,ByeqyxBKvS,Quantum Semi-Supervised Kernel Learning,Reject,"Three reviewers have assessed this paper and they have scored it 6/6/6 after rebuttal with one reviewer hesitating about the appropriateness of this submission to ML venues. The reviewers have raised a number of criticisms such as an incremental nature of the paper (HHL and LMR algorithms) and the main contributions lying more within the field of quantum computing than ML. The paper was discussed with reviewers, buddy AC and chairs. On balance, it was concluded that this paper is minimally below the acceptance threshold. We encourage authors to consider all criticism, improve the paper and resubmit to another venue as there is some merit to the proposed idea.
",Paper Decision
Mpb1Rjjd7H,S1et1lrtwr,Unsupervised Meta-Learning for Reinforcement Learning,Reject,"The paper discusses the relevant topic of unsupervised meta-learning in an RL setting. The topic is an interesting one, but the writing and motivation could be much clearer. I advise the authors to make a few more iterations on the paper taking into account the reviewers' comments and then resubmit to a different venue.",Paper Decision
lXj48wajb,SygKyeHKDH,Making Efficient Use of Demonstrations to Solve Hard Exploration Problems,Accept (Poster),"This paper tackles hard-exploration RL problems using learning from demonstrations. The idea is to combine the existing R2D2 algorithms with imitation learning from human demonstrations. Experiments are conducted on a new set of challenging tasks, highlighting limitations of strong current baseline while highlighting the strength of the proposed approach.

The contribution is two-folds: the proposed algorithm which clear outperforms previous SOTA agents and the set of benchmarks. All reviewers being positive about this paper, I therefore recommend acceptance.",Paper Decision
qWLi0UczvQ,B1gdkxHFDH,Training individually fair ML models with sensitive subspace robustness,Accept (Spotlight),"The paper addresses individual fairness scenario (treating similar users similarly) and proposes a new definition of algorithmic fairness that is based on the idea of robustness, i.e. by perturbing the inputs (while keeping them close with respect to the distance function), the loss of the model cannot be significantly increased.
All reviewers and AC agree that this work is clearly of interest to ICLR, however the reviewers have noted the following potential weaknesses: (1) presentation clarity -- see R3’s detailed suggestions e.g. comparison to Dwork et al, see R2’s comments on how to improve, (2) empirical evaluations -- see R1’s question about using more complex models, see R3’s question on the usefulness of the word embeddings. 
Pleased to report that based on the author respond with extra experiments and explanations, R3 has raised the score to weak accept. All reviewers and AC agree that the most crucial concerns have been addressed in the rebuttal, and the paper could be accepted - congratulations to the authors! The authors are strongly urged to improve presentation clarity and to include the supporting empirical evidence when preparing the final revision.",Paper Decision
LKKU3_PdW,BygdyxHFDS,Meta-learning curiosity algorithms,Accept (Poster),"This paper proposes meta-learning auxiliary rewards as specified by a DSL. The approach was considered innovative and the results interesting by all reviewers. The paper is clearly of an acceptable standard, with the main concerns raised by reviewers having been addressed (admittedly at the 11th hour) by the authors during the discussion period. Accept.",Paper Decision
giKeDQy9-n,rylwJxrYDS,vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations,Accept (Poster),"This paper proposes a new self-supervised pre-trained speech model that improves speech recognition performance. 
 The idea combines an earlier pre-training approach (wav2vec) with discretization followed by BERT-style masked reconstruction.  The result is a fairly complex approach, with not too much novelty but with a good amount of engineering and analysis, and ultimately very good performance.  The reviewers agree that the work deserves publication at ICLR, and the authors have addressed some of the reviewer concerns in their revision.  The complexity of the approach may mean that it is not immediately widely adopted by others, but it is a good proof of concept and may well inspire other related work.  I believe the ICLR community will find this work interesting.",Paper Decision
gRuZ-MtCg8,rJx8ylSKvr,Leveraging Entanglement Entropy for Deep Understanding of  Attention Matrix in Text Matching,Reject,"This paper advocates for the application of entanglement entropy from quantum physics to understand and improve the inductive bias of neural network architectures for question answering tasks. All reviewers found the current presentation of the method difficult to understand, and as a result it is difficult to determine what exactly the contribution of this work is. One suggestion for improving the manuscript is to minimize the references to quantum entanglement (where currently is it asserted without justification that entanglement entropy is a relevant concept for modeling question-answering tasks). Instead, presenting the method as applications of tensor decompositions for parameterizing neural network architectures would make the work more accessible to a machine learning audience, and help clarify the contribution with respect to related works [1]. 

1. http://papers.nips.cc/paper/8495-a-tensorized-transformer-for-language-modeling.pdf",Paper Decision
O71KGil1YK,rkgU1gHtvr,Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior Policies,Accept (Poster),"The authors present a method to address off-policy policy evaluation in the infinite horizon case, when the available data comes from multiple unknown behavior policies.  Their solution -- the estimated mixture policy -- combines recent ideas from both infinite horizon OPE and regression importance sampling, a recent importance sampling based method.  At first, the reviewers were concerned about writing clarity, feasibility in the continuous case, and comparisons to contemporary methods like DualDICE.  After the rebuttal period, the reviewers agreed that all the major issues had been addressed through clarifications, rewriting, code release, and additional empirical comparisons.  Thus, I recommend to accept this paper.",Paper Decision
qC-gDRaMcI,r1eU1gHFvH,Under what circumstances do local codes emerge in feed-forward neural networks,Reject,This paper studies when hidden units provide local codes by analyzing the hidden units of trained fully connected classification networks under various architectures and regularizers. The reviewers and the AC believe that the paper in its current form is not ready for acceptance to ICLR-2020. Further work and experiments are needed in order to identify an explanation for the emergence of local codes. This would significantly strengthen the paper.,Paper Decision
0pr-ULQpOZ,HkeryxBtPB,MMA Training: Direct Input Space Margin Maximization through Adversarial Training,Accept (Poster),"This work presents a new loss function that combines the usual cross-entropy term with a margin maximization term applied to the correctly classified examples. There have been a lot of recent ideas on how to incorporate margin into the training process for deep learning. The paper differs from those in the way that it computes margin. The paper shows that training with the proposed max margin loss results in robustness against some adversarial attacks.
There were initially some concerns about baseline comparisons; one of the reviewers requesting comparison against TRADES, and the other making comments on CW-L2. In response, authors ran additional experiments and listed those in their rebuttal and in the revised draft. This led some reviewers to raise their initial scores. At the end, majority of reviewers recommended accept. Alongside with them, I find extensions of classic large margin ideas to deep learning settings (when margin is not necessarily defined at the output layer) an important research direction for constructing deep models that are robust and can generalize. ",Paper Decision
9ueaOnplT,ByxHJeBYDB,Forecasting Deep Learning Dynamics with Applications to Hyperparameter Tuning,Reject,"This paper trains a transformer to extrapolate learning curves, and uses this in a model-based RL framework to automatically tune hyperparameters. This might be a good approach, but it's hard to know because the experiments don't include direct comparisons against existing hyperparameter optimization/adaptation techniques (either the ones based on extrapolating training curves, or standard ones like BayesOpt or PBT). The presentation is also fairly informal, and it's not clear if a reader would be able to reproduce the results. Overall, I think there's significant cleanup and additional experiments needed before publication in ICLR.
",Paper Decision
UsEOQVKJ4O,BJeVklHtPr,Batch Normalization has Multiple Benefits: An Empirical Study on Residual Networks,Reject,The paper is rejected based on unanimous reviews.,Paper Decision
pPaN6Fgsu,BJgNJgSFPS,Building Deep Equivariant Capsule Networks,Accept (Talk),"This paper combine recent ideas from capsule networks and group-equivariant neural networks to form equivariant capsules, which is a great idea. The exposition is clear and the experiments provide a very interesting analysis and results. I believe this work will be very well received by the ICLR community.",Paper Decision
w-BFRBRXkt,rylNJlStwB,Learning to Infer User Interface Attributes from Images,Reject,"The majority of reviewers suggest rejection, pointing to concerns about design and novelty. Perhaps the most concerning part to me was the consistent lack of expertise in the applied area. This could be random bad luck draw of reviewers, but more likely the paper is not positioned well in the ICLR literature. This means that either it was submitted to the wrong venue, or that the exposition needs to be improved so that the paper is approachable by a larger part of the ICLR community. Since this is not currently true, I suggest that the authors work on a revision.",Paper Decision
KylCrU_z4y,B1eXygBFPH,Attacking Graph Convolutional Networks via Rewiring,Reject,"This paper proposes a method for attacking graph convolutional networks, where a graph rewiring operation was introduced that affects the graph in a less noticeable way compared to adding/deleting edges. Reinforcement learning is applied to learn the attack strategy based on the proposed rewiring operation. The paper should be improved by acknowledging/comparing with previous work in a more proper way. In particular, I view the major innovation is on the rewiring operation and its analysis. The reinforcement learning formulation is similar to Dai et al (2018). This connection should be made more clear in the technical part. One issue that needs to be discussed on is that if you directly consider the triples as actions, the space will be huge. Do you apply some hierarchical treatment as suggested by Dai et al. (2018)?  The review comments should be considered to further improve too.",Paper Decision
yxG2CveYIu,Hyl7ygStwB,Incorporating BERT into Neural Machine Translation,Accept (Poster),"The authors propose a novel way of incorporating a large pretrained language model (BERT) into neural machine translation using an extra attention model for both the NMT encoder and decoder.   The paper presents thorough experimental design, with strong baselines and consistent positive results for supervised, semi-supervised and unsupervised experiments. The reviewers all mentioned lack of clarity in the writing and there was significant discussion with the authors. After improvements and clarifications, all reviewers agree that this paper would make a good contribution to ICLR and be of general use to the field. ",Paper Decision
2Gn_k1idq4,BkgGJlBFPS,Unsupervised Hierarchical Graph Representation Learning with Variational Bayes,Reject,"The paper presents an unsupervised method for graph representation, building upon Loukas' method for generating a sequence of gradually coarsened graphs. The contribution is an ""encoder-decoder"" architecture trained by variational inference, where the encoder produces the embedding of the nodes in the next graph of the sequence, and the decoder produces the structure of the next graph. 

One important merit of the approach is  that this unsupervised representation can be used effectively for supervised learning, with results quite competitive to the state of the art. 

However the reviewers were unconvinced by the novelty and positioning of the approach. The point of whether the approach should be viewed as variational Bayesian, or simply variational approximation was much debated between the reviewers and the authors. 

The area chair encourages the authors to pursue this very promising research, and to clarify the paper; perhaps the use of ""encoder-decoder"" generated too much misunderstanding. 
Another graph NN paper you might be interested in is ""Edge Contraction Pooling for Graph NNs"", by Frederik Diehl. 
",Paper Decision
s-uuWDHWmG,SklM1xStPB,Copy That! Editing Sequences by Copying Spans,Reject,"This paper proposes an addition to seq2seq models to allow the model to copy spans of tokens of arbitrary length in one step. The authors argue that this method is useful in editing applications where long spans of the output sequence will be exact copies of the input. Reviewers agreed that the problem is interesting and the solution technically sound. However, during the discussion phase there were concerns that the method was too incremental to warrant publication at ICLR. The work would be strengthened with a more thorough discussion of related work and additional experiments comparing with the relevant baselines as suggested by Reviewer 2.",Paper Decision
Bf_KGoGLn,SJlWyerFPS,DeepXML: Scalable & Accurate Deep Extreme Classification for Matching User Queries to Advertiser Bid Phrases,Reject,"The paper proposes a new method for extreme multi-label classification. However, this paper only combine some  well known tricks, the technical contributions are too limited. And there are many problems in the experiments, such as the reproducibility, the scal of data set and the results on well-known extreme data sets and so on. The authors are encouraged to consider the reviewer's comments to revise the paper.",Paper Decision
DybSK5aelg,rJxbJeHFPS,What Can Neural Networks Reason About?,Accept (Spotlight),"This paper proposes a framework which qualifies how well given neural architectures can perform on reasoning tasks. From this, they show a number of interesting empirical results, including the ability of graph neural network architectures for learn dynamic programming.

This substantial theoretical and empirical study impressed the reviewers, who strongly lean towards acceptance. My view is that this is exactly the sort of work we should be show-casing at the conference, both in terms of focus, and of quality. I am happy to recommend this for acceptance.",Paper Decision
9AvyA73Dd,B1e-kxSKDH,Structured Object-Aware Physics Prediction for Video Modeling and Planning,Accept (Poster),"The paper presents a method for modeling videos with object-centric structured representations. The paper is well written and clearly motivated. Using a Graph Neural Network for modeling latent physics is a sensible idea and can be beneficial for planning/control. Experimental results show improved performance over the baselines. After the rebuttal, many questions/concerns from the reviewers were addressed, and all reviewers recommend weak acceptance.",Paper Decision
oZfRXJSQw0,r1glygHtDB,A multi-task U-net for segmentation with lazy labels,Reject,"The paper proposes an architecture for semantic instance segmentation learnable from coarse annotations and evaluates it on two microscopy image datasets, demonstrating its advantage over baseline. While the reviewers appreciate the details of the architecture, they note the lack of evaluation on any of popular datasets and the lack of comparisons with baselines that would be more close to state-of-the-art. The authors do not address this criticism convincingly. It is not clear, why e.g. the Cityscapes or VOC Pascal datasets, which both have reasonably accurate annotations, cannot be used for the validation of the idea. If the focus is on the precision of the result near the boundaries, then one can always report the error near boundaries (this is a standard thing to do). Note that the performance of the baseline models is far from saturated near boundaries (i.e. the errors are larger than mistakes of annotation).

At this stage, the paper lacks convincing evaluation and comparison with prior art. Given that this is first and foremost application paper, lacking some very novel ideas (as pointed out by e.g. Rev1), better evaluation is needed for acceptance.",Paper Decision
02YqasPKDg,Bklg1grtDr,Neural Design of Contests and All-Pay Auctions using Multi-Agent Simulation,Reject,"This paper demonstrates a framework for optimizing designs in auction/contest problems. The approach relies on considering a multi-agent learning process and then simulating it. 

To a large degree there is agreement among reviewers that this approach is sensible and sound, however lacks substantial novelty. The authors provided a rebuttal which clarified the aspects that they consider novel, however the reviewers remained mostly unconvinced. Furthermore, it would help if the improvement over past approaches is demonstrated in a more convincing way, for example with increased scope experiments that also involve richer analysis.
",Paper Decision
Ss0rG1TIp7,H1gy1erYDH,CaptainGAN: Navigate Through Embedding Space For Better Text Generation,Reject,"This paper proposes a method to train generative adversarial nets for text generation. The paper proposes to address the challenge of discrete sequences using straight-through and gradient centering. The reviewers found that the results on COCO Image Captions and EMNLP 2017 News were interesting. However, this paper is borderline because it does not sufficiently motivate one of its key contributions: the gradient centering. The paper establishes that it provides an improvement in ablation, but more in-depth analysis would significantly improve the paper. I strongly encourage the authors to resubmit the paper once this has been addressed.",Paper Decision
gdm5DfP-yk,HyxJ1xBYDH,Learning-Augmented Data Stream Algorithms,Accept (Poster),"This paper theoretically analyzes the use of an oracle to predict various quantities in data stream models.  Building upon Hsu et al., (2019), the overriding goal is to examine the degree to which such an oracle is can provide memory and time improvements across broad streaming regimes.  In doing so, optimal bounds are derived in conjunction with a heavy hitter oracle.

Although the rebuttal and discussion period did not lead to a consensus in the scoring of this paper, two reviewers were highly supportive.  However, the primary criticism from the lone dissenting reviewer was based on the high-level presentation and motivation, and in particular, the impression that the paper read more like a STOC theory paper.  In this regard though, my belief is that the authors can easily tailor a revision to increase the accessibility to a wider ICLR audience.",Paper Decision
8cT-gD6ic1,HkxARkrFwB,word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement,Accept (Spotlight),"This paper proposes quantum-inspired methods for increasing the parametric efficiency of word embeddings. While a little heavy in terms of quantum jargon, and perhaps a little ignorant of loosely related work in this sub-field (e.g. see the work of Coecke and colleagues from 2008 onwards), the majority of reviewers were broadly convinced the work and results were of sufficient merit to be published.",Paper Decision
x795jnCPK,HJgRCyHFDr,On Weight-Sharing and Bilevel Optimization in Architecture Search,Reject,"Since there were only two official reviews submitted, I reviewed the paper to form a third viewpoint.  I agree with reviewer 2 on the following points, which support rejection of the paper:
1) Only CIFAR is evaluated without Penn Treebank;
2) The ""faster convergence"" is not empirically justified by better final accuracy with same amount of search cost; and
3) The advantage of the proposed ACSA over SBMD is not clearly demonstrated in the paper.

The scores of the two official reviews are insufficient for acceptance, and an additional review did not overturn this view.",Paper Decision
_AKqEaHJuY,BkxRRkSKwr,Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models,Accept (Spotlight),"The authors present a hierarchical explanation model for understanding the underlying representations produced by LSTMs and Transformers.  Using human evaluation, they find that their explanations are better, which could lead to better trust of these opaque models.

The reviewers raised some issues with the derivations, but the author response addressed most of these.  ",Paper Decision
LBNnCAztZ,H1lTRJBtwB,Compositional Transfer in Hierarchical Reinforcement Learning,Reject,"This paper is concerned with improving data-efficiency in multitask reinforcement learning problems. This is achieved by taking a hierarchical approach, and learning commonalities across tasks for reuse. The authors present an off-policy actor-critic algorithm to learn and reuse these hierarchical policies.

This is an interesting and promising paper, particularly with the ability to work with robots. The reviewers did however note issues with the novelty and making the contributions clear. Additionally, it was felt that the results proved the benefits of hierarchy rather than this approach, and that further comparisons to other approaches are required. As such, this paper is a weak reject at this point.",Paper Decision
mmtcopdgMs,HJlnC1rKPB,On the Relationship between Self-Attention and Convolutional Layers,Accept (Poster),"This paper studies the relationship between attention networks such as Transformers and convolutional networks. The paper shows that a special case of attention can be cast as convolution. However this link depends on using relative positional embeddings and generalization to other encodings are not given in the paper. The reviewers found the results correct, but we caution that the writing should better reflect the caveats of the approach.",Paper Decision
LhxIGI1UJB,SyxiRJStwr,Dynamic Scale Inference by Entropy Minimization,Reject,"This paper constitutes interesting progress on an important problem.  I urge the authors to continue to refine their investigations, with the help of the reviewer comments; e.g., the quantitative analysis recommended by AnonReviewer4.",Paper Decision
HroVTEjk8I,rkxs0yHFPH,SpikeGrad: An ANN-equivalent Computation Model for Implementing Backpropagation with Spikes,Accept (Poster),"This paper proposes a learning framework for spiking neural networks that exploits the sparsity of the gradient during backpropagation to reduce the computational cost of training. The method is evaluated against prior works that use full precision gradients and shown comparable performance. Overall, the contribution of the paper is solid, and after a constructive rebuttal cycle, all reviewers reached a consensus of weak accept. Therefore, I recommend accepting this submission.",Paper Decision
s3mlf1QDWY,ryl5CJSFPS,GENERALIZATION GUARANTEES FOR NEURAL NETS VIA HARNESSING THE LOW-RANKNESS OF JACOBIAN,Reject,"This submission investigates the properties of the Jacobian matrix in deep learning setup. Specifically, it splits the spectrum of the matrix into information (large singulars) and ``nuisance (small singulars) spaces. The paper shows that over the information space learning is fast and achieves zero loss. It also shows that generalization relates to how well labels are aligned with the information space.

While the submission certainly has encouraging analysis/results, reviewers find these contributions limited and it is not clear how some of the claims in the paper can be extended to more general settings. For example, while the authors claim that low-rank structure is suggested by theory, the support of this claim is limited to a case study on mixture of Gaussians. In addition, the provided analysis only studies two-layer networks. As elaborated by R4, extending these arguments to more than two layers does not seem straighforward using the tools used in the submission. While all reviewers appreciated author's response, they were not convinced and maintained their original ratings.
",Paper Decision
EZa-YwkGJY,rke5R1SFwS,Learning to Remember from a Multi-Task Teacher,Reject,"The paper addresses the setting of continual learning. Instead of focusing on catastrophic forgetting measured in terms of the output performance of the previous tasks, the authors tackle forgetting that happens at the level of the feature representation via a meta-learning approach. As rightly acknowledged by R2, from a meta-learning perspective the work is quite interesting and demonstrates a number of promising results. 
However the reviewers have raised several important concerns that placed this work below the acceptance bar:
 (1) the current manuscript lacks convincing empirical evaluations that clearly show the benefits of the proposed approach over SOTA continual learning methods; specifically the generalization of the proposed strategy to more than two sequential tasks is essential; also see R1’s detailed suggestions that would strengthen the contributions of this approach in light of continual learning;
(2) training a meta-learner to predict the weight updates with supervision from a multi-task teacher network as an oracle, albeit nicely motivated, is unrealistic in the continual learning setting -- see R1’s detailed comments on this issue. 
(3) R2 and R3 expressed concerns regarding i) stronger baselines that are tuned to take advantage of the meta-learning data and ii) transferability to the different new tasks, i.e. dissimilarity of the meta-train and meta-test settings. Pleased to report that the authors showed and discussed in their response some initial qualitative results regarding these issues. An analysis on the performance of the proposed method when the meta-training and testing datasets are made progressively dissimilar would strengthen the evaluation the proposed meta-learning approach. 
There is a reviewer disagreement on this paper. AC can confirm that all three reviewers have read the rebuttal and have contributed to a long discussion. Among the aforementioned concerns, (3) did not have a decisive impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed approach, and were viewed by AC as critical issues. AC suggests, that in its current state the manuscript is not ready for a publication and needs a major revision before submitting for another round of reviews. We hope the reviews are useful for improving and revising the paper.
",Paper Decision
D6996H75Y,ryxK0JBtPr,Gradient $\ell_1$ Regularization for Quantization Robustness,Accept (Poster),Reviewers uniformly suggest acceptance. Please take their comments into account in the camera-ready. Congratulations!,Paper Decision
zR_yRwryMQ,rJxt0JHKvS,Coloring graph neural networks for node disambiguation,Reject,"This paper presents an extension of MPNN which leverages the random color augmentation to improve the representation power of MPNN. The experimental results shows the effectiveness of colorization. A majority of the reviewers were particularly concerned about lacking permutation invariance in the approach as well as the large variance issue in practice, and their opinion stays the same after the rebuttal. The reviewers unanimously expressed their concerns on the large variance issue during the discussion period. Overall, the reviewers believe that the authors has not addressed their concerns sufficiently.",Paper Decision
FbV1U9mBU,H1l_0JBYwS,Spectral  Embedding of Regularized Block Models,Accept (Spotlight),"The paper proposes a nice and easy way to regularize spectral graph embeddings, and explains the effect through a nice set of experiments. Therefore, I recommend acceptance.",Paper Decision
BSRPRuBYvQ,SJeOAJStwB,On Federated Learning of Deep Networks from Non-IID Data: Parameter Divergence and the Effects of Hyperparametric Methods,Reject,"This paper studies the problem of federated learning for non-i.i.d. data, and looks at the hyperparameter optimization in this setting. As the reviewers have noted, this is a purely empirical paper. There are certain aspects of the experiments that need further discussion, especially the learning rate selection for different architectures. That said, the submission may not be ready for publication at its current stage.",Paper Decision
ZDsWSAVMkz,rJguRyBYvr,Improved Detection of Adversarial Attacks via Penetration Distortion Maximization,Reject,"A defense against of adversarial attacks is presented, which builds mostly on combining known methods in a novel way. While the novelty is somewhat limited, this would be fine if the results were unequivocally good and other parts of the problematic. However, reviewers were not entirely convinced by the results, and had a number of minor complaints with various parts of the paper.

In sum, this paper is not currently at a stage where it can be accepted.",Paper Decision
onVKCmjRJm,S1gwC1StwS,Barcodes as summary of objective functions' topology,Reject,"The main concern raised by the reviewers is that the paper is difficult to read and potentially unclear. Therefore, the area chair read the paper, and also found it fairly dense and challenging to read. While there may be important discoveries in the paper, the paper in its current form makes it too difficult to read. Since four reviewers (including the AC) struggled to understand the paper, we believe the presentation of the paper should be improved. In particular, the claims of the paper should be better put into context.",Paper Decision
t9FO16glvu,SylL0krYPS,Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control,Accept (Poster),"This paper considers adversarial attacks in continuous action model-based deep reinforcement learning. An optimisation-based approach is presented, and evaluated on Mujoco tasks.

There were two main concerns from the reviewers. The first was that the approach requires strong assumptions, but in the rebuttal some relaxations were demonstrated (e.g., not attacking every step). Additionally, there were issues raised with the choice of baselines, but in the discussion the reviewers did not agree on any other reasonable baselines to use.

This is a novel and interesting contribution nonetheless, which could open the field to much additional discussion, and so should be accepted.",Paper Decision
PfgC4fBSnS,SkxHRySFvr,LEARNING TO IMPUTE: A GENERAL FRAMEWORK FOR SEMI-SUPERVISED LEARNING,Reject,"There is insufficient support to recommend accepting this paper.  The reviewers unanimously criticize the quality of the exposition, noting that many key elements in the main development and experimental set up are not clear.  The significance of the contribution could be made stronger with some form of theoretical analysis.  The current paper lacks depth and insufficient justification for the proposed approach.  The submitted comments should be able to help the authors improve the paper.",Paper Decision
jqgpju4XN,Bklr0kBKvB,Geometry-aware Generation of Adversarial and Cooperative Point Clouds,Reject,This paper offers an improved attack on 3-D point clouds. Unfortunately the clarity of the contribution is unclear and on balance insufficient for acceptance.,Paper Decision
skWJaSbatc,HJxVC1SYwr,Crafting Data-free Universal Adversaries with Dilate Loss,Reject,"This paper focuses on finding universal adversarial perturbations, that is, a single noise pattern that can be applied to any input to fool the network in many cases. Further more, it focuses on the data-free setting, where such a perturbation is found without having access to data (images) from the distribution that train- and test data comes from. 

The reviewers were very conflicted about this paper. Among others, the strong experimental results and the clarity of writing and analysis were praised. However, there was also criticism of the amount of novelty compared to GDUAP, on the strong assumptions needed (potentially limiting the applicability), and on some weakness in the theoretical analysis. 

In the end, the paper seems in current form not convincing enough for me to recommend acceptance for ICLR.  ",Paper Decision
hdTI6noAe,Bkx4AJSFvB,Efficient Bi-Directional Verification of ReLU Networks via Quadratic Programming,Reject,"This article is concerned with sensitivity to adversarial perturbations. It studies the computation of the distance to the decision boundary from a given sample in order to obtain robustness certificates, and presents an iterative procedure to this end. This is a very relevant line of investigation. The reviewers found that the approach is different from previous ones (even if related quadratic constraints had been formulated in previous works). However, they expressed concerns with the presentation, missing details or intuition for the upper bounds, and the small size of the networks that are tested. The reviewers also mentioned that the paper could be clearer about the strengths and weaknesses of the proposed algorithm. The responses clarified a number of points from the initial reviews. However, some reviewers found that important aspects were still not addressed satisfactorily, specifically in relation to the justification of the approach to obtain upper bounds (although they acknowledge that the strategy seems at least empirically validated), and reiterated concerns about the scalability of the approach. Overall, this article ranks good, but not good enough. 
",Paper Decision
XM93tEDQra,HklE01BYDB,Improving Sample Efficiency in Model-Free Reinforcement Learning from Images,Reject,"The paper investigates how sample efficiency of image based model-free RL can be improved  by including an image reconstruction loss as an auxiliary task and applies it to soft actor-critic. The method is demonstrated to yield a substantial improvement compared to SAC learned directly from pixels, and comparable performance to other prior works, such as SLAC and PlaNet, but with a simpler learning setup. The reviewers generally appreciate the clarity of presentation and good experimental evaluation. However, all reviewers raise concerns regarding limited novelty, as auxiliary losses for RL have been studied before, and the contribution is mainly in the design choices of the implementation. In this view, and given that the results are on a par with SOTA, the contribution of this paper seems too incremental for publishing in this venue, and I’m recommending rejection. ",Paper Decision
hTvbEPHVDk,rJe7CkrFvS,Improving Exploration of Deep Reinforcement Learning using Planning for Policy Search,Reject,"The paper is about exploration in deep reinforcement learning. The reviewers agree that this is an interesting and important topic, but the authors provide only a slim analysis and theoretical support for the proposed methods. Furthermore, the authors are encouraged to evaluate the proposed method on more than a single benchmark problem.",Paper Decision
zzNZLRApya,SJlM0JSFDr,A Theoretical Analysis of  Deep Q-Learning,Reject,"The authors offer theoretical guarantees for a simplified version of the deep Q-learning algorithm. However, the majority of the reviewers agree that the simplifying assumptions are so many that the results do not capture major important aspects of deep Q-Learning (e.g. understanding good exploration strategies, understanding why deep nets are better approximators and not using neural net classes that are so large that can capture all non-parametric functions). For justifying the paper to be called a theoretical analysis of deep Q-Learning some of these aspects need to be addressed, or the motivation/title of the paper needs to be re-defined. ",Paper Decision
n2_XdJWxe5,SkgGCkrKvH,Decentralized Deep Learning with Arbitrary Communication Compression,Accept (Poster),"The authors present an algorithm CHOCO-SGD to make use of communication compression in a decentralized setting. This is an interesting problem, and the paper is well-motivated and well-written. On the theoretical side, the authors prove the convergence rate of the algorithm on non-convex smooth functions, which shows a nearly linear speedup. The experimental results on several benchmark datasets validate the algorithm achieves better performance than baselines. These can be made more convincing by comparing with more baselines (including DeepSqueeze and other centralized algorithms with a compression scheme), and on larger datasets. The authors should also clarify results on consensus.",Paper Decision
fpHaXPDNA,S1e-0kBYPB,Can I Trust the Explainer? Verifying Post-Hoc Explanatory Methods,Reject,"The paper proposes a framework for generating evaluation tests for feature-based explainers. The framework provides guarantees on the behaviors of each trained model in that non-selected tokens are irrelevant for each prediction,  and for each instance in the pruned dataset, one subset of clearly relevant tokens is selected. 

After reading the paper, I think there are a few issues with the current version of the paper: 

(1) the writing can be significantly improved: the motivation is unclear, which makes it difficult for readers to fully appreciate the work. It seems that each part of the paper is written by different persons, so the transition between different parts seems abrupt and the consistency of the texts is poor. For example, the framework is targeted at NLP applications, but in the introduction the texts are more focused on general purpose explainers. The transition from the RCNN approach to the proposed framework is not well thought-out, which makes the readers confused about what exactly is the proposed framework and what is the novelty.

(2) the claimed properties of the proposed framework are rather straightforward derivations. The technical novelty is not as high as claimed in the paper.

(3) The experiment results are not fully convincing. 

All the reviewers have read the authors' feedback and responded. It is agreed that the current version of the paper is not ready for publication. 
",Paper Decision
c0Bxya9na8,rkxZCJrtwS,D3PG: Deep Differentiable Deterministic Policy Gradients,Reject,"This paper proposes a hybrid RL algorithm that uses model based gradients from a differentiable simulator to accelerate learning of a model-free policy.  While the method seems sound, the reviewers raised concerns about the experimental evaluation, particularly lack of comparisons to prior works, and that the experiments do not show a clear improvement over the base algorithms that do not make use of the differentiable dynamics. I recommend rejecting this paper, since it is not obvious from the results that the increased complexity of the method can be justified by a better performance, particularly since the method requires access to a simulator, which is not available for real world experiments where sample complexity matters more.",Paper Decision
qa9zi55xK,r1xZAkrFPr,Deep Ensembles: A Loss Landscape Perspective,Reject,"Paper https://arxiv.org/abs/1802.10026 (Garipov et. al, NeurIPS 2018) shows that one can find curves between two independently trained solutions along which the loss is relatively constant. The authors of this ICLR submission claim as a key contribution that they show the weights along the path correspond to different models that make different predictions (""Note that prior work on loss landscapes has focused on mode-connectivity and low-loss tunnels, but has not explicitly focused on how diverse the functions from different modes are, beyond an initial exploration in Fort & Jastrzebski (2019)""). Much of the disagreement between two of the reviewers and the authors is whether this point had already been shown in 1802.10026.

It is in fact very clear that 1802.10026 shows that different points on the curve correspond to diverse functions. Figure 2 (right) of this paper shows the test error of an _ensemble_ of predictions made by the network for the parameters at one end of the curve, and the network described by \phi_\theta(t) at some point t along the curve: since the error goes down and changes significantly as t varies, the functions corresponding to different parameter settings along these curves must be diverse. This functional diversity is also made explicit multiple times in 1802.10026, which clearly says that this result shows that the curves contain meaningfully different representations.

In response to R3, the authors incorrectly claim that  ""Figure 2 in Garipov et al. only plots loss and accuracy, and does not measure function space similarity, between different initializations, or along the tunnel at all. Just by looking at accuracy and loss values, there is no way to infer how similar the predictions of the two functions are."" But Figure 2 (right) is actually showing the test error of an average of predictions of networks with parameters at different points along the curve, how it changes as one moves along the curve, and the improved accuracy of the ensemble over using one of the endpoints. If the functions associated with different parameters along the curve were the same, averaging their predictions would not help performance. 

Moreover, Figure 6 (bottom left, dashed lines) in the appendix of 1802.10026 shows the improvement in performance in ensembling points along the curve over ensembling independently trained networks. Section A6 (Appendix) also describes ensembling along the curve in some detail, with several quantitative results. There is no sense in ensembling models along the curve if they were the same model.

These results unequivocally demonstrate that the points on the curve have functional diversity, and this connection is made explicit multiple times in 1802.10026 with the claim of meaningfully different representations: “This result also demonstrates that these curves do not exist only due to degenerate parametrizations of the network (such as rescaling on either side of a ReLU); instead, points along the curve correspond to meaningfully different representations of the data that can be ensembled for improved performance.”  Additionally, other published work has built on this observation, such as 1907.07504 (UAI 2019), which performs Bayesian model averaging over the mode connecting subspace, relying on diversity of functions in this space; that work also visualizes the different functions arising in this space. 

It is incorrect to attribute these findings to Fort & Jastrzebski (2019) or the current submission.  It is a positive contribution to build on prior work, but what is prior work and what is new should be accurately characterized, and currently is not, even after the discussion phase where multiple reviewers raised the same concern. Reviewers appreciated the broader investigation of diversity and its effect on ensembling, and the more detailed study regarding connecting curves. In addition to the concerns about inaccurate claims regarding prior work and novelty (which included aspects of the mode connectivity work but also other works), several reviewers also felt that the time-accuracy trade-offs of deep ensembles relative to standard approaches were not clearly presented, and comparisons were lacking. It would be simple and informative to do an experiment showing a runtime-accuracy trade-off curve for deep ensembles alongside FGE and various Bayesian deep learning methods and mc-dropout. It's also possible to use for example parallel MCMC chains to explore multiple quite different modes like deep ensembles but for Bayesian deep learning. For the paper to be accepted, it would need significant revisions, correcting the accuracy of claims, and providing such experiments.",Paper Decision
9LKC9LU2w5,B1xxAJHFwS,A Finite-Time Analysis of  Q-Learning with Neural Network Function Approximation,Reject,"This was an extremely difficult paper to decide, as it attracted significant commentary (and controversy) that led to non-trivial corrections in the results.  One of the main criticisms is that the work is an incremental combination of existing results.  A potentially bigger concern is that of correctness: the main convergence rate was changed from 1/T to 1/sqrt{T} during the rebuttal and revision process.  Such a change is not trivial and essentially proves the initial submission was incorrect.  In general, it is not prudent to accept a hastily revised theory paper without a proper assessment of correctness in its modified form.  Therefore, I think it would be premature to accept this paper without a full review cycle that assessed the revised form.  There also appear to be technical challenges from the discussion that remain unaddressed.  Any resubmission will also have to highlight significance and make a stronger case for the novelty of the results.",Paper Decision
rhEuoMUMAx,r1geR1BKPr,MULTI-STAGE INFLUENCE FUNCTION,Reject,"This paper extends the idea of influence functions (aka the implicit function theorem) to multi-stage training pipelines, and also adds an L2 penalty to approximate the effect of training for a limited number of iterations.

I think this paper is borderline.  I also think that R3 had the best take and questions on this paper.

Pros:
 - The main idea makes sense, and could be used to understand real training pipelines better.
 - The experiments, while mostly small-scale, answer most of the immediate questions about this model.

Cons:
 - The paper still isn't all that polished.  E.g. on page 4: ""Algorithm 1 shows how to compute the influence score in (11). The pseudocode for computing the influence function in (11) is shown in Algorithm 1""
 - I wish the image dataset experiments had been done with larger images and models.

Ultimately, the straightforwardness of the extension and the relative niche applications mean that although the main idea is sound, the quality and the overall impact of this paper don't quite meet the bar.",Paper Decision
NUGd66WjqT,Hygy01StvH,Impact of the latent space on the ability of GANs to fit the distribution,Reject,"The reviewers have pointed out several major deficiencies of the paper, which the authors decided not to address.",Paper Decision
l6hr25do6z,Hye1RJHKwB,Training Generative Adversarial Networks from Incomplete Observations using Factorised Discriminators,Accept (Poster),"All three reviewers appreciate the new method (FactorGAN) for training generative networks from incomplete observations. At the same time, the quality of the experimental results can still be improved. On balance, the paper will make a good poster.",Paper Decision
tNfWVsRN3i,SkeAaJrKDS,Combining Q-Learning and Search with Amortized Value Estimates,Accept (Poster),"This paper proposes Search with Amortized Value Estimates (SAVE) that combines Q-learning and MCTS.  SAVE uses the estimated Q-values obtained by MCTS at the root node to update the value network, and uses the learned value function to guide MCTS.

The rebuttal addressed the reviewers’ concerns, and they are now all positive about the paper. I recommend acceptance.",Paper Decision
s4nFDDMG0,ryxC6kSYPr,Infinite-Horizon Differentiable Model Predictive Control,Accept (Poster),"This paper develops a linear quadratic model predictive control approach for safe imitation learning.  The main contribution is an analytic solution for the derivative of the discrete-time algebraic Riccati equation (DARE).  This allows an infinite horizon optimality objective to be used with differentiation-based learning methods.  An additional contribution is the problem reformulation with a pre-stabilizing controller and the support of state constraints throughout the learning process.  The method is tested on a damped-spring system and a vehicle platooning problem.

The reviewers and the author response covered several topics. The reviewers appreciated the research direction and theoretical contributions of this work.  The reviewers main concern was the experimental evaluation, which was originally limited to a damped spring system.  The authors added another experiment for a substantially more complex continuous control domain.  In response to the reviewers, the authors also described how this work relates to non-linear control problems.  The authors also clarified the ability of the proposed method to handle state-based constraints that are not handled by earlier methods.  The reviewers were largely satisfied with these changes.

This paper should be accepted as the reviewers are satisfied that the paper has useful contributions.",Paper Decision
o3xNC7m7aK,H1epaJSYDS,Anchor & Transform: Learning Sparse Representations of Discrete Objects,Reject,"The paper proposes a method to produce embeddings of discrete objects, jointly learning a small set of anchor embeddings and a sparse transformation from anchor objects to all the others. While the paper is well written, and proposes an interesting solution, the contribution seems rather incremental (as noted by several reviewers), considering the existing literature in the area.  Also, after discussions the usefulness of the method remains a bit unclear - it seems some engineering (related to sparse operations) is still required to validate the viability of the approach.
",Paper Decision
oPQZwXLK0,Skg2pkHFwS,Emergence of Collective Policies Inside Simulations with Biased Representations,Reject,"This paper presents an ensemble method for reinforcement learning.  The method trains an ensemble of transition and reward models.  Each element of this ensemble has a different view of the data (for example, ablated observation pixels) and a different latent space for its models.  A single (collective) policy is then trained, by learning from trajectories generated from each of the models in the ensemble.  The collective policy makes direct use of the latent spaces and models in the ensemble by means of a translator that maps one latent space into all the other latent spaces, and an aggregator that combines all the model outputs.  The method is evaluated on the CarRacing and VizDoom environments.  

The reviewers raised several concerns about the paper. The evaluations were not convincing with artificially weak baselines and only worked well in one of the two tested environments (reviewer 2). The paper does not adequately connect to related work on model-based RL (reviewer 1 and 2). The paper does not motivate its artificial setting (reviewer 2 and 1).  The paper's presentation lacks clarity from using non-standard terminology and notation without adequate explanation (reviewer 1 and 3).  Technical aspects of the translator component were also unclear to multiple reviewers (reviewers 1, 2 and 3).  The authors found the review comments to be helpful for future work, but provided no additional clarifications.

The paper is not ready for publication.",Paper Decision
cOtUbdqMZO,rke3TJrtPS,Projection-Based Constrained Policy Optimization,Accept (Poster),"The paper proposes a new algorithm for solving constrained MDPs called Projection Based Constrained Policy Optimization. Compared to CPO, it projects the solution back to the feasible region after each step, which results in improvements on some of the tasks considered. 

The problem addressed is relevant, as many tasks could have important constraints e.g. concerning fairness or safety. 

The method is supported through theory and empirical results. It is great to have theoretical bounds on the policy improvement and constraint violation of the algorithm, although they only apply to the intractable version of the algorithm (another approximate algorithm is proposed that is used in practice). The experimental evidence is a bit mixed, with the best of the proposed projections (based on the KL approach) sometimes beating CPO but also sometimes being beaten by it, both on the obtained reward and on constraint satisfaction. 

The method only considers a single constraint. I'm not sure how trivial it would be to add more than one constraint. The reviewers also mention that the paper does not implement TRPO as in the original paper, as in the original paper the step size in the direction of the natural gradient is refined with a line search if the original step size (calculated using the quadratic expansion of the expected KL) does violate the original constraints. (Line search on the constraint as mentioned by the authors would be a different issue). Futhermore, the quadratic expansion of the KL is symmetric around the current policy in parameter space. This means that starting from a feasible solution the trust region should always overlap with the constraint set when feasibility is maintained, going somewhat agains the argument for PCPO as opposed to CPO brought up by the authors in the discussion with R2. I would also show this symmetry in illustrations such as Fig 1 to aid understanding. 


",Paper Decision
JVJayhEGu3,BJliakStvH,Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning,Accept (Spotlight),The paper introduces a novel way of doing IRL based on learning constraints. The topic of IRL is an important one in RL and the approach introduced is interesting and forms a fundamental contribution that could lead to relevant follow-up work.,Paper Decision
ZnsDK63P4h,H1eKT1SFvH,Towards Effective 2-bit Quantization: Pareto-optimal Bit Allocation for Deep CNNs Compression,Reject,"This works presents a method for inferring the optimal bit allocation for quantization of weights and activations in CNNs. The formulation is sound and the experiments are complete. However, the main concern is that the paper is very similar to a recent work by the authors, which is not cited.",Paper Decision
VCj0bB3Pxu,HyxY6JHKwr,You Only Train Once: Loss-Conditional Training of Deep Networks,Accept (Poster),"The paper proposes and validates a simple idea of training a neural network for a parametric family of losses, using a popular AdaIN mechanism.
Following the rebuttal and the revision, all three reviewers recommend acceptance (though weakly). There is a valid concern about the overlap with an ICLR19-workshop paper with essentially the same idea, however the submission is broader in scope and validates the idea on several applications.",Paper Decision
r2PDRSwode,ryeYpJSKwr,Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization,Accept (Spotlight),"This paper explores the idea of using meta-learning for acquisition functions. It is an interesting and novel research direction with promising results. 

The paper could be strengthened by adding more insights about the new acquisition function and performing more comparisons e.g. to Chen et al. 2017. But in any case, the current form of the paper should already be of high interest to the community
",Paper Decision
Rj0ZE8tyv,B1xu6yStPH,Using Explainabilty to Detect Adversarial Attacks,Reject,"This paper proposes EXAID, a method to detect adversarial attacks by building on the advances in explainability (particularly SHAP), where activity-map-like explanations are used to justify and validate decisions. Though it may have some valuable ideas, the execution is not satisfying, with various issues raised in comments. No rebuttal was provided.",Paper Decision
_7XeEO8vrY,B1lda1HtvB,Feature Selection using Stochastic Gates,Reject,"The authors propose a method for feature selection in non linear models by using an appropriate continuous relaxation of binary feature selection variables. The reviewers found that the paper contains several interesting methodological contributions. However, they thought that the foundations of the methodology make very strong assumptions. Moreover the experimental evaluation is lacking comparison with other methods for non linear feature selection such as that of Doquet et al and Chang et al.",Paper Decision
JSVNFjgTo,HyewT1BKvr,SpectroBank: A filter-bank convolutional layer for CNN-based audio applications,Reject,"The paper proposed a parameterized convolution layer using predefined filterbanks. It has the benefit of less parameters to optimize and better interpretability. The original submission failed to inlcude many related work into the discussion which was addressed during the rebutal.

The main concerns for this paper is the limited novelty and insufficient experimental validation and comprisons: 
* There have been existing work using sinc parameterized filters, learnable Gammatones etc, which are very similar to the proposed method. Also in the rebutal, the authors acknowledged that ""We did not claim that cosine modulation was the novelty in our paper"" and it is ""just a way of simplifying implementation and dealing with real values instead of complex ones"" and ""addressing the question of convergence of parametric filter banks to perceptual scale"".
* Although the authors addressed the missing related work problem by including them into discussions, the expeirmental sections need more work to include comparisons to those methods and also more validations on difference datasets to address the concern on the generalization of the proposed method. ",Paper Decision
0fqZFeKUgp,SJev6JBtvH,Testing For Typicality with Respect to an Ensemble of Learned Distributions,Reject,"The paper proposes a new method for testing whether new data comes from the same distribution as training data without having an a-priori density model of the training data. This is done by looking at the intersection of typical sets of an ensemble of learned models. 

On the theoretical side, the paper was received positively by all reviewers. The theoretical results were deemed strong, and the ideas in the paper were considered novel. The problem setting was considered relevant, and seen as a good proposal to deal with the shortcoming of models on out of distribution data. 

However, the lack of empirical results on at least somewhat realistic datasets (e.g. MNIST) was commented on by all reviewers. The authors only present a toy experiment. The authors have explained their decision, but I agree with R1 that it would be appropriate in such situations to present the toy experiment next to a more realistic dataset. This also means that the effectiveness of the proposed method in real settings is as of yet unclear. Although the provided toy example was considered clear and illuminating, the clarity of the text could still be improved.

Although the reviewers had a spread in their final score, I think they would all agree that the direction this paper takes is very exciting, but that the current version of the paper is somewhat premature. Thus, unfortunately, I have to recommend rejection at this point. 

",Paper Decision
QmaaJ-k17w,BJe8pkHFwS,GraphSAINT: Graph Sampling Based Inductive Learning Method,Accept (Poster),"All three reviewers advocated acceptance. The AC agrees, feeling the paper is interesting. ",Paper Decision
2ntjJ4kyH3,H1g8p1BYvS,Adversarial Filters of Dataset Biases,Reject,"This paper proposes to address the issue of biases and artifacts in benchmark datasets through the use of adversarial filtering. That is, removing training and test examples that a baseline model or ensemble gets wright. 

The paper is borderline, and could have flipped to an accept if the target acceptance rate for the conference were a bit higher. All three reviewers ultimately voted weakly in favor of it, especially after the addition of the new out-of-domain generalization results. However, reviewers found it confusing in places, and R2 wasn't fully convinced that this should be applied in the settings the authors suggest. This paper raises some interesting and controversial points, but after some private discussion, there wasn't a clear consensus that publishing it as is would do more good than harm.",Paper Decision
VfCNrPDP5H,rJxBa1HFvS,Value-Driven Hindsight Modelling,Reject,"This paper studies the problem of estimating the value function in an RL setting by learning a representation of the value function. While this topic is one of general interest to the ICLR community, the paper would benefit from a more careful revision and reorganization following the suggestions of the reviewers.",Paper Decision
ERpMJZEjqI,H1gN6kSFwS,Learning Neural Causal Models from Unknown Interventions,Reject,"This paper proposes a metalearning objective to infer causal graphs from data based on masked neural networks to capture arbitrary conditional relationships. While the authors agree that the paper contains various interesting ideas, the theoretical and conceptual underpinnings of the proposed methodology are still lacking and the experiments cannot sufficiently make up for this. The method is definitely worth exploring more and a revision is likely to be accepted at another venue.",Paper Decision
iJ2_ZGdq1,rJg46kHYwH,Adaptive Generation of Unrestricted Adversarial Inputs,Reject,"This paper presents an interesting method for creating adversarial examples using a GAN.  Reviewers are concerned that ImageNet Results, while successfully evading a classifier, do not appear to be natural images.  Furthermore, the attacks are demonstrated on fairly weak baseline classifiers that are known to be easily broken.  They attack Resnet50 (without adv training), for which Lp-bounded attacks empirically seem to produce more convincing images.  For MNIST, they attack Wong and Kolter’s ""certifiable"" defense, which is empirically much weaker than an adversarially trained network, and also weaker than more recent certifiable baselines.
",Paper Decision
1tPaz9X-m_,BJeXaJHKvB,P-BN: Towards Effective Batch Normalization in the Path Space,Reject,"This paper addresses the extension of path-space-based SGD (which has some previously-acknowledged advantages over traditional weight-space SGD) to handle batch normalization. Given the success of BN in traditional settings, this is a reasonable scenario to consider.  The analysis and algorithm development involved exploits a reparameterization process to transition from the weight space to the path space.  Empirical tests are then conducted on CIFAR and ImageNet.

Overall, there was a consensus among reviewers to reject this paper, and the AC did not find sufficient justification to overrule this consensus.  Note that some of the negative feedback was likely due, at least in part, to unclear aspects of the paper, an issue either explicitly stated or implied by all reviewers.  While obviously some revisions were made, at this point it seems that a new round of review is required to reevaluate the contribution and ensure that it is properly appreciated.",Paper Decision
cJHZGnjZl,rJg76kStwH,Efficient Probabilistic Logic Reasoning with Graph Neural Networks,Accept (Poster),"This paper is far more borderline than the review scores indicate. The authors certainly did themselves no favours by posting a response so close to the end of the discussion period, but there was sufficient time to consider the responses after this, and it is somewhat disappointing that the reviewers did not engage.

Reviewer 2 states that their only reason for not recommending acceptance is the lack of experiments on more than one KG. The authors point out they have experiments on more than one KG in the paper. From my reading, this is the case. I will consider R2 in favour of the paper in the absence of a response.

Reviewer 3 gives a fairly clear initial review which states the main reasons they do not recommend acceptance. While not an expert on the topic of GNNs, I have enough of a technical understanding to deem that the detailed response from the authors to each of the points does address these concerns. In the absence of a response from the reviewer, it is difficult to ascertain whether they would agree, but I will lean towards assuming they are satisfied.

Reviewer 1 gives a positive sounding review, with as main criticism ""Overall, the work of this paper seems technically sound but I don’t find the contributions particularly surprising or novel. Along with plogicnet, there have been many extensions and applications of Gnns, and I didn’t find that the paper expands this perspective in any surprising way."" This statement is simply re-asserted after the author response. I find this style of review entirely inappropriate and unfair: it is not a the role of a good scientific publication to ""surprise"". If it is technically sound, and in an area that the reviewer admits generates interest from reviewers, vague weasel words do not a reason for rejection make.

I recommend acceptance.",Paper Decision
bqQIdMDJP2,SkxQp1StDH,Low-dimensional statistical manifold embedding of directed graphs,Accept (Poster),"The paper proposes an embedding for nodes in a directed graph, which takes into account the asymmetry. The proposed method learns an embedding of a node as an exponential distribution (e.g. Gaussian), on a statistical manifold. The authors also provide an approximation for large graphs, and show that the method performs well in empirical comparisons.

The authors were very responsive in the discussion phase, providing new experiments in response to the reviews. This is a nice example where a good paper is improved by several extra suggestions by reviewers. I encourage the authors to provide all the software for reproducing their work in the final version.

Overall, this is a great paper which proposes a new graph embedding approach that is scalable and provides nice empirical results.",Paper Decision
SRTv8_N6-D,BylfTySYvB,GATO: Gates Are Not the Only Option,Reject,"This paper proposes a modification of RNN that does not suffer from vanishing and exploding gradient problems. The proposed model, GATO partitions the RNN hidden state into two channels, and both are updated by the previous state. This model ensures that the state in one of the parts is time-independent by using residual connections. 

The reviews are mixed for this paper, but the general consensus was that the experiments could be better (baseline comparisons could have been fairer). The reviewers have low confidence in the revised/updated results. Moreover, it remains unclear what the critical components are that make things work. It would be great to read a paper and understand why something works and not that something works. 

Overall: Nice idea, but the paper is not quite ready yet.

",Paper Decision
c8I8ONyEoc,S1ef6JBtPr,Probabilistic View of Multi-agent Reinforcement Learning: A Unified Approach,Reject,"The paper takes the perspective of ""reinforcement learning as inference"", extends it to the multi-agent setting and derives a multi-agent RL algorithm that extends Soft Actor Critic. Several reviewer questions were addressed in the rebuttal phase, including key design choices. A common concern was the limited empirical comparison, including comparisons to existing approaches. ",Paper Decision
0HpAtHFe03,HJx-akSKPS,Neural Subgraph Isomorphism Counting,Reject,"This paper proposes a method called Dynamic Intermedium Attention Memory Network (DIAMNet) to learn the subgraph isomorphism counting for a given pattern graph P and target graph G. However, the reviewers think the experimental comparisons are insufficient. Furthermore,  the evaluation is only for synthetic dataset for which generating process is designed by the authors. If possible, evaluation on benchmark graph datasets would be convincing though creating the ground truth might be difficult for larger graphs.
",Paper Decision
v32WGpka0,rkg-TJBFPB,RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments,Accept (Poster),"This paper tackles the problem of exploration in deep reinforcement learning in procedurally-generated environments, where the same state is rarely encountered twice. The authors show that existing methods do not perform well in these settings and propose an approach based on intrinsic reward bonus to address this problem. More specifically, they combine two existing ideas for training RL policies: 1) using implicit reward based on latent state representations (Pathak et al. 2017) and 2) using implicit rewards based on difference between subsequent states (Marino et al. 2019).

Most concerns of the reviewers have been addressed in the rebuttals. Given that it builds so closely on existing ideas, the main weakness of this work seems to be the novelty. The strength of this paper resides in the extensive experiments and analysis that highlight the shortcomings of current techniques and provide insight into the behaviour of trained agents, in addition to proposing a strategy which improves upon existing methods.

The reviewers all agree that the paper should be accepted. I therefore recommend acceptance.",Paper Decision
WGvdHqD34S,SJlgTJHKwB,Continual Learning with Delayed Feedback,Reject,"This paper claims to present a model-agnostic continual learning framework which uses a queue to work with delayed feedback. All reviewers agree that the paper is difficult to follow. I also have a difficult time reading the paper. 

In addition, all reviewers mentioned there is no baseline in the experiments, which makes it difficult to empirically analyze the strengths and weaknesses of the proposed model. R2 and R3 also have some concerns regarding the motivation and claim made in the paper, especially in relation to previous work in this area.

The authors did not respond to any of the concerns raised by the reviewers. It is very clear that the paper is not ready for publication at a venue such as ICLR at the current state, so I recommend rejecting the paper.",Paper Decision
9pnD3ESmy,SklgTkBKDr,Neural Non-additive Utility Aggregation,Reject,"This paper presents two new architectures that model latent intermediate utilities and use non-additive utility aggregation to estimate the set utility based on the computed latent utilities. These two extensions are easy to understand and seem like a simple extension to the existing RNN model architectures, so that they can be implemented easily. However, the connection to Choquet integral is not clear and no theory has been provided to make that connection. Hence, it is hard for the reader to understand why the integral is useful here.  The reviewers have also raised objection about the evaluation which does not seem to be fair to existing methods. These comments can be incorporated to make the paper more accessible and the results more appreciable. ",Paper Decision
RyOltrjCHx,ByggpyrFPS,Bayesian Variational Autoencoders for Unsupervised Out-of-Distribution Detection,Reject,"This paper tackles the problem of detection out-of-distribution (OoD) samples. The proposed solution is based on a Bayesian variational autoencoder. The authors show that information-theoretic measures applied on the posterior distribution over the decoder parameters can be used to detect OoD samples. The resulting approach is shown to outperform baselines in experiments conducted on three benchmarks (CIFAR-10 vs SVNH and two based on FashionMNIST).

Following the rebuttal, major concerns remained regarding the justification of the approach. The reason why relying on active learning principles should allow for OoD detection would need to be clarified, and the use of the effective sample size (ESS) would require a stronger motivation. Overall, although a theoretically-informed OoD strategy is indeed interesting and relevant, reviewers were not convinced by the provided theoretical justifications. I therefore recommend to reject this paper.",Paper Decision
B-wu7JbbUM,S1lk61BtvB,"``""Best-of-Many-Samples"" Distribution Matching",Reject,"This paper proposed an improvement on VAE-GAN which draws multiple samples from the reparameterized latent distribution for each inferred q(z|x), and only backpropagates reconstruction error for the resulting G(z) which has the lowest reconstruction.  While the idea is interesting, the novelty is not high compared with existing similar works, and the improvement is not significant.",Paper Decision
BFDsTBwtn1,rkl03ySYDH,SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition,Accept (Poster),"The paper makes a reasonable contribution to generative modeling for unsupervised scene decomposition.  The revision and rebuttal addressed the primary criticisms concerning the qualitative comparison and clarity, which caused some of the reviewers to increase their rating.  I think the authors have adequately addressed the reviewer concerns.  The final version of the paper should still strive to improve clarity, and strengthen the evaluation and ablation studies.",Paper Decision
sB_v08TlTO,HyeCnkHtwH,Efficient generation of structured objects with Constrained Adversarial Networks,Reject,"This paper develops ideas for enabling the data generation with GANs in the presence of structured constraints on the data manifold. This problem is interesting and quite relevant to the ICLR community. The reviewers raised concerns about the similarity to prior work (Xu et al '17), and missing comparisons to previous approaches that study this problem (e.g. Hu et al '18) that make it difficult to judge the significance of the work. Overall, the paper is slightly below the bar for acceptance.",Paper Decision
QBmNQo6QO4,Hkxp3JHtPr,Deep Variational Semi-Supervised Novelty Detection,Reject,"This paper presents two novel VAE-based methods for semi-supervised anomaly detection (SSAD) where one has also access to a small set of labeled anomalous samples. The reviewers had several concerns about the paper, in particular completely addressing reviewer #3's comments would strengthen the paper.",Paper Decision
UeVfJPlYf,HJeT3yrtDr,Cross-Lingual Ability of Multilingual BERT: An Empirical Study,Accept (Poster),"This paper introduces a set of new analysis methods to try to better understand the reasons that multilingual BERT succeeds. The findings substantially bolster the hypothesis behind the original multilingual BERT work: that this kind of model discovers and uses substantial structural and semantic correspondences between languages in a fully unsupervised setting. This is a remarkable result with serious implications for representation learning work more broadly.

All three reviewers saw ways in which the paper could be expanded or improved, and one reviewer argued that the novelty and scope of the paper are below the standard for ICLR. However, I am inclined to side with the two more confident reviewers and argue for acceptance. I don't see any substantive reasons to reject the paper, the methods are novel and appropriate (even in light of the prior work that exists on this question), and the results are surprising and relevant a high-profile ongoing discussion in the literature on representation learning for language.",Paper Decision
SMltj4yOvN,Hkeh21BKPH,Towards Finding Longer Proofs,Reject,"This paper proposes a curriculum-based reinforcement learning approach to improve theorem proving towards longer proofs. While the authors are tackling an important problem, and their method appears to work on the environment it was tested in, the reviewers found the experimental section too narrow and not convincing enough. In particular, the authors are encouraged to apply their methods to more complex domains beyond Robinson arithmetic. It would also be helpful to get a more in depth analysis of the role of the curriculum. The discussion period did not lead to improvements in the reviewers’ scores, hence I recommend that this paper is rejected at this time. 
",Paper Decision
OvKG2N3mS,Bylh2krYPr,Probing Emergent Semantics in Predictive Agents via Question Answering,Reject,"This paper proposes question-answering as a general paradigm to decode and understand the representations that agents develop, with application to two recent approaches to predictive modeling. During rebuttal, some critical issues still exist, e.g., as Reviewer#3 pointed out, the submission in its current form lacks experimental analysis of the proposed conditional probes, especially the trade-offs on the reliability of the representation analysis when performed with a conditional probe as well as a clear motivation for the need of a language interface. The authors are encouraged to incorporate the refined motivation and add more comprehensive experimental evaluation for a possible resubmission.",Paper Decision
0OXaFIh1Q3,rkeqn1rtDH,Hierarchical Graph Matching Networks for Deep Graph Similarity Learning,Reject,"The submission proposes an architecture to learn a similarity metric for graph matching. The architecture uses node-graph information in order to learn a more expressive, multi-level similarity score. The hierarchical approach is empirically validated on a limited set of graphs for which pairwise matching information is available and is shown to outperform other methods for classification and regression tasks.

The reviewers were divided in their scores for this paper, but all noted that the approach was somewhat incremental and empirically motivated, without adequate analysis, theoretical justification, or extensive benchmark validation. 

Although the approach has value, more work is needed to support the method fully. Recommendation is to reject at this time.",Paper Decision
myLM_Km5oA,rJxq3kHKPH,A Simple Approach to the Noisy Label Problem Through the Gambler's Loss,Reject,"This paper focuses on mitigating the effect of label noise. They provide a new class of loss functions along with a new stopping criteria for this problem. The authors claim that these new losses improves the test accuracy in the presence of label corruption and helps avoid memorization. The reviewers raised concerns about (1) lack of proper comparison with many baselines (2) subpar literature review and (3) state that parts of the paper is vague. The authors partially addressed these concerns and have significantly updated the paper including comparison with some of the baselines. However, the reviewers were not fully satisfied with the new updates. I mostly agree with the reviewers. I think the paper has potential but requires a bit more work to be ready for publication and can not recommend acceptance at this time. I have to say that the authors really put a lot of effort in their response and significantly improved their submission during the discussion period. I recommend the authors follow the reviewers' suggestions to further improve the paper (e.g. comparing with other baselines) for future submissions",Paper Decision
1Fz7lg0p-N,Hygq3JrtwS,On the Reflection of Sensitivity in the Generalization Error,Reject,"The paper proposes a definition of the sensitivity of the output to random perturbations of the input and its link to generalization.

While both reviewers appreciated the timeliness of this research, they were taken aback by the striking similarity with the work of Novak et al. I encourage the authors to resubmit to a later conference with a lengthier analysis of the differences between the two frameworks, as they started to do in their rebuttal.",Paper Decision
SpKGoqpcUJ,H1eF3kStPS,Redundancy-Free Computation Graphs for Graph Neural Networks,Reject,"This paper proposes a new graph Hierarchy representation (HAG) which eliminates the redundancy during the aggregation stage and improves computation efficiency. It achieves good speedup and also provide theoretical analysis. There has been several concerns from the reviewers; authors' response addressed them partially. Despite this, due to the large number of strong papers, we cannot accept the paper at this time. We encourage the authors to further improve the work for a future version.  


",Paper Decision
AbjKCGwpnf,HJxKhyStPH,Toward Understanding The Effect of Loss Function on The Performance of Knowledge Graph Embedding,Reject,"The paper analyses the effect of different loss functions for TransE and argues that certain limitations of TransE can be mitigated by choosing more appropriate loss functions.  The submission then proposes TransComplEx to further improve results.  This paper received four reviews, with three recommending rejection, and one recommending weak acceptance.  A main concern was in the clarity of motivating the different models.  Another was in the relatively low performance of RotatE compared with [1], which was raised by multiple reviewers.  The authors provided extensive responses to the concerns raised by the reviewers.  However, at least the implementation of RotatE remains of concern, with the response of the authors indicating ""Please note that we couldn’t use exactly the same setting of RotatE due to limitations in our infrastructure.""  On the balance, a majority of reviewers felt that the paper was not suitable for publication in its current form.",Paper Decision
CnZiloV-j,SylO2yStDr,Reducing Transformer Depth on Demand with Structured Dropout,Accept (Poster),"This paper presents Layerdrop, which is a method for structured dropout which allows you to train one model, and then prune to a desired depth at test time. This is a simple method which is exciting because you can get a smaller, more efficient model at test time for free, as it does not need fine tuning. They show strong results on machine translation, language modelling and a couple of other NLP benchmarks. The reviews are consistently positive, with significant author and reviewer discussion. This is clearly an approach which merits attention, and should be included in ICLR.",Paper Decision
YqfqyWENWa,BJg_2JHKvH,Semi-Supervised Learning with Normalizing Flows,Reject,"This paper offers a novel method for semi-supervised learning using GMMs.  Unfortunately the novelty of the contribution is unclear, and the majority of the reviewers find the paper is not acceptable in present form.  The AC concurs.",Paper Decision
AuYsZoMBV,rJgD2ySFDr,Neural Communication Systems with Bandwidth-limited Channel,Reject,"There was some support for this paper, but it was on the borderline and significant concerns were raised. It did not compare to the exiting related literature on communications, compression, and coding. There were significant issues with clarity.",Paper Decision
BvbXerpLuY,SkeP3yBFDS,Reducing Computation in Recurrent Networks by Selectively Updating State Neurons,Reject,"This paper introduces a new RNN architecture which uses a small network to decide which cells get updated at each time step, with the goal of reducing computational cost.  The idea makes sense, although it requires the use of a heuristic gradient estimator because of the non-differentiability of the update gate.

The main problem with this paper in my view is that the reduction in FLOPS was not demonstrated to correspond to a reduction in wallclock time, and I don't expect it would, since the sparse updates are different for each example in each batch, and only affect one hidden unit at a time.  The only discussion of this problem is ""we compute the FLOPs for each method as a surrogate for wall-clock time, which is hardware-dependent and often fluctuates dramatically in practice.""  Because this method reduces predictive accuracy, the reduction in FLOPS should be worth it!

Minor criticism:
1) Figure 1 is confusing, showing not the proposed architecture in general but instead the connections remaining after computing the sparse updates.
",Paper Decision
FFwV3wZenW,SygD31HFvB,A Novel Analysis Framework of Lower Complexity Bounds for Finite-Sum Optimization,Reject,"The paper considers a lower bound complexity for the convex problems. The reviewers worry about whether the scope of this paper fit in ICLR, the initialization issues, and the novelty and some other problems.",Paper Decision
rsg7Mx14yE,Skx82ySYPH,Neural Outlier Rejection for Self-Supervised Keypoint Learning,Accept (Poster),This paper proposes a solid (if somewhat incremental) improvement on an interesting and well-studied problem. I suggest accepting it.,Paper Decision
JZQ1oKG3tv,H1gBhkBFDH,B-Spline CNNs on Lie groups,Accept (Poster),"The paper describes principles for endowing a neural architecture with invariance with respect to a Lie group. The contribution is that these principles can accommodate discrete and continuous groups, through approximation via a base family (B-splines). 

The main criticisms were related to the intelligibility of the paper and the practicality of the approach, implementation-wise. Significant improvements have been done and the paper has been partially rewritten during the rebuttal period.

Other criticisms were related to the efficiency of the approach, regarding how the property of invariance holds under the approximations done. These comments were addressed in the rebuttal and the empirical comparison with data augmentation also supports the merits of the approach.

This leads me to recommend acceptance. I urge the authors to extend the description and discussion about the experimental validation. 
",Paper Decision
H9crcczRGZ,rkxNh1Stvr,Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation with an I/O Kernel,Accept (Poster),"This paper presents a method to model uncertainty in deep learning regressors by applying a post-hoc procedure.  Specifically, the authors model the residuals of neural networks using Gaussian processes, which provide a principled Bayesian estimate of uncertainty.  The reviewers were initially mixed and a fourth reviewer was brought in for an additional perspective.  The reviewers found that the paper was well written, well motivated and found the methodology sensible and experiments compelling.  AnonReviewer4 raised issues with the theoretical exposition of the paper (going so far as to suggest that moving the theory into the supplementary and using the reclaimed space for additional clarifications would make the paper stronger).  The reviewers found the author response compelling and as a result the reviewers have come to a consensus to accept.  Thus the recommendation is to accept the paper.  

Please do take the reviewer feedback into account in preparing the camera ready version.  In particular, please do address the remaining concerns from AnonReviewer4 regarding the theoretical portion of the paper.  It seems that the methodological and empirical portions of the paper are strong enough to stand on their own (and therefore the recommendation for an accept).  Adding theory just for the sake of having theory seems to detract from the message (particularly if it is irrelevant or incorrect as initially pointed out by the reviewer).",Paper Decision
xbVi20gvP,HJem3yHKwH,EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness Against Adversarial Attacks,Accept (Poster),This paper proposed to apply emsembles of high precision deep networks and low precision ones to improve the robustness against adversarial attacks while not increase the cost in time and memory heavily.  Experiments on different tasks under various types of adversarial attacks show the proposed method improves the robustness of the models without sacrificing the accuracy on normal input.  The idea is simple and effective.  Some reviewers have had concerns on the novelty of the idea and the comparisons with related work but I think the authors give convincing answers to these questions.,Paper Decision
OYMnCO123b,HklXn1BKDH,Learning To Explore Using Active Neural SLAM,Accept (Poster),"The paper presents a method for visual robot navigation in simulated environments. The proposed method combines several modules, such as mapper, global policy, planner, local policy for point-goal navigation. The overall approach is reasonable and the pipeline can be modularly trained. The experimental results on navigation tasks show strong performance, especially in generalization settings. ",Paper Decision
MtIbxeI2r,rklMnyBtPB,Adversarial Robustness Against the Union of Multiple Perturbation Models,Reject,"Thanks to the authors for submitting the paper and providing further explanations and experiments. This paper aims to ensure robustness against several perturbation models simultaneously. While the authors' response has addressed several issues raised by the reviewers, the concern on the lack of novelty remains. Overall, there is not enough support among the reviewers for the paper to be accepted.",Paper Decision
nKybGpU3_5,SylzhkBtDB,Understanding and Improving Information Transfer in Multi-Task Learning,Accept (Poster),"Many existing approaches in multi-task learning rely on intuitions about how to transfer information. This paper, instead, tries to answer what does ""information transfer"" even mean in this context. Such ideas have already been presented in the past, but the approach taken here is novel, rigorous and well-explained.

The reviewers agreed that this is a good paper, although they wished to see the analysis conducted using more practical models. 

For the camera ready version it would help to make the paper look less dense.
",Paper Decision
w7yCvVN-vG,ryGWhJBtDB,Hyperparameter Tuning and Implicit Regularization in Minibatch SGD,Reject,"Authors provide an empirical evaluation of batch size and learning rate selection and its effect on training and generalization performance. As the authors and reviewers note, this is an active area of research with many closely related results to the contributions of this paper already existing in the literature. In light of this work, reviewers felt that this paper did not clearly place itself in the appropriate context to make its contributions clear. Following the rebuttal, reviewers minds remained unchanged. ",Paper Decision
YJswe4ejDS,SkxWnkStvS,Searching for Stage-wise Neural Graphs In the Limit,Reject,"This paper proposes a graphon-based search space for neural architecture search. Unfortunately, the paper as currently stands and the small effect sizes in the experimental results raise questions about the merits of actually employing such a search space for the specific task of NAS. The reviewers expressed concerns that the results do not convincingly support graphon being a superior search space as claimed in the paper. 
",Paper Decision
dsIQuI8DCU,S1xWh1rYwB,Restricting the Flow: Information Bottlenecks for Attribution,Accept (Talk),"All three reviewers strongly recommend accepting this paper. It is clear, novel, and a significant contribution to the field. Please take their suggestions into account in a camera ready version. Thanks!",Paper Decision
P1GpmaOruZ,H1gx3kSKPS,Stein Bridging: Enabling Mutual Reinforcement between Explicit and Implicit Generative Models,Reject,The paper proposes a generative model that jointly trains an implicit generative model and an explicit energy based model using Stein's method. There are concerns about technical correctness of the proofs and the authors are advised to look carefully into the points raised by the reviewers. ,Paper Decision
DA8TJOjW4U,Sygg3JHtwB,Step Size Optimization,Reject,The paper is rejected based on unanimous reviews.,Paper Decision
K23TsejCZ,H1xJhJStPS,Equilibrium Propagation with Continual Weight Updates,Reject,"Main content: paper introduces a new variant of equilibrium propagation algorithm that continually updates the weights making it unnecessary to save steady states. T

Summary of discussion:
reviewer 1: likes the idea but points out many issues with the proofs. 
reviewer 2: he really likes the novelty of paper, but review is not detailed, particularly discussing pros/cons. 
reviewer 3: likes the ideas but has questions on proofs, and also questions why MNIST is used as the evaluation tasks.
Recommendation: interesting idea but writing/proofs could be clarified better. Vote reject.

",Paper Decision
qR8g32awiF,BJgyn1BFwS,Global Adversarial Robustness Guarantees for Neural Networks,Reject,"The authors propose a framework for estimating ""global robustness"" of a neural network, defined as the expected value of ""local robustness"" (robustness to small perturbations) over the data distribution. The authors prove the the local robustness metric is measurable and that under this condition, derive a statistically efficient estimator. The authors use gradient based attacks to approximate local robustness in practice and report extensive experimental results across several datasets.

While the paper does make some interesting contributions, the reviewers were concerned about the following issues:
1) The measurability result, while technically important, is not surprising and does not add much insight algorithmically or statistically into the problem at hand. Outside of this, the paper does not make any significant technical contributions.
2) The paper is poorly organized and does not clearly articulate the main contributions and significance of these relative to prior work.
3) The fact that the local robustness metric is approximated via gradient based attacks makes the final results void of any guarantees, since there are no guarantees that gradient based attacks compute the worst case adversarial perturbation. This calls into question the main contribution claim of the paper on computing global robustness guarantees.

While some of the technical aspects of the reveiwers' concerns were clarified during the discussion phase, this was not sufficient to address the fundamental issues raised above.

Hence, I recommend rejection.",Paper Decision
NxEVwQUODU,HylAoJSKvH,A Stochastic Derivative Free Optimization Method with Momentum,Accept (Poster),"A new method for derivative free optimization including momentum and importance sampling is proposed.

All reviewers agreed that the paper deserves acceptance.

Acceptance is recommended.",Paper Decision
jZBh9cesdt,SygRikHtvS,Coresets for Accelerating Incremental Gradient Methods,Reject,"This paper investigates the practical and theoretical consequences of speeding up training using incremental gradient methods (such as stochastic descent) by calculating the gradients with respect to a specifically chosen sparse subset of data. 

The reviewers were quite split on the paper. 

On the one hand, there was a general excitement about the direction of the paper. The idea of speeding up gradient descent is of course hugely relevant to the current machine learning landscape. The approach was also considered novel, and the paper well-written. 

However, the reviewers also pointed out multiple shortcomings. The experimental section was deemed to lack clarity and baselines.  The results on standard dataset were very different from expected, causing worry about the reliability, although this has partially been addressed in additional experiments. The applicability to deep learning and large dataset, as well as the significance of time saved by using this method, were other worries.

Unfortunately, I have to agree with the majority of the reviewers that the idea is fascinating, but that more work is required for acceptance to ICLR.  ",Paper Decision
pKT6QAHqbv,BJgRsyBtPB,A Greedy Approach to Max-Sliced Wasserstein GANs,Reject,"The paper proposes a variant of the max-sliced Wasserstein distance, where instead of sorting, a greedy assignment is performed. As no theory is provided, the paper is purely of experimental nature. 

Unfortunately the work is too preliminary to warrant publication at this time, and would need further experimental or theoretical strengthening to be of general interest to the ICLR community.",Paper Decision
2QWpDiQ0b,HygaikBKvS,Off-Policy Actor-Critic with Shared Experience Replay,Reject,"The paper presents an off-policy actor-critic scheme where i) a buffer storing the trajectories from several agents is used (off-policy replay) and mixed with the on-line data from the current agent; ii) a trust-region estimator is used to select trajectories that are sufficiently close to the current policy (e.g. in the sense of a KL divergence).

As noted by the reviews, the results are impressive. 

Quite a few concerns still remain:
* After Fig. 1 (revised version), what matters is the shared replay, where the agent actually benefits from the experience of 9 other different agents; this implies that the population based training observes 9x more frames than the no-shared version, and the question whether the comparison is fair is raised;
* the trust-region estimator might reduce the data seen by the agent, leading it to overfit the past (Fig. 3, left);
* the influence of the $b$ hyper-parameter (the trust threshold) is not discussed. In standard trust region-based optimization methods, the trust region is gradually narrowed, suggesting that parameter $b$ here should evolve along time. 

",Paper Decision
zrbA8uGEx6,rkg6sJHYDr,Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems,Accept (Talk),"The authors introduce a framework for automatically detecting diverse, self-organized patterns in a continuous Game of Life environment, using compositional pattern producing networks (CPPNs) and population-based Intrinsically Motivated Goal Exploration Processes (POP-IMGEPs) to find the distribution of system parameters that produce diverse, interesting goal patterns.

This work is really well-presented, both in the paper and on the associated website, which is interactive and features source code and demos. Reviewers agree that it’s well-written and seems technically sound. I also agree with R2 that this is an under-explored area and thus would add to the diversity of the program.

In terms of weaknesses, reviewers noted that it’s quite long, with a lengthy appendix, and could be a bit confusing in areas. Authors were responsive to this in the rebuttal and have trimmed it, although it’s still 29 pages. My assessment is well-aligned with those of R2 and thus I’m recommending accept. In the rebuttal, the authors mentioned several interesting possible applications for this work; it’d be great if these could be included in the discussion. 

Given the impressive presentation and amazing visuals, I think it could make for a fun talk.
",Paper Decision
Xsw8PQb0R,rJe2syrtvS,The Ingredients of Real World Robotic Reinforcement Learning,Accept (Spotlight),"This is a very interesting paper which discusses practical issues and solutions around deploying RL on real physical robotic systems, specifically involving questions on the use of raw sensory data, crafting reward functions, and not having resets at the end of episodes.

Many of the issues raised in the reviews and discussion were concerned with experimental details and settings, as well as relation to different areas of related work. These were all sufficiently handled in the rebuttal, and all reviewers were in favour of acceptance.",Paper Decision
AZ_pu0JgN_,S1g2skStPB,Causal Discovery with Reinforcement Learning,Accept (Talk),"This paper proposes an RL-based structure search method for causal discovery. The reviewers and AC think that the idea of applying reinforcement learning to causal structure discovery is novel and intriguing. While there were initially some concerns regarding presentation of the results, these have been taken care of during the discussion period. The reviewers agree that this is a very good submission, which merits acceptance to ICLR-2020.",Paper Decision
t3TV3FqukD,BJlisySYPS,Modelling the influence of data structure on learning in neural networks,Reject,"The paper examines the idea that real world data is highly structured / lies on a low-dimensional manifold. The authors show differences in neural network dynamics when trained on structured (MNIST) vs. unstructured datasets (random), and show that ""structure"" can be captured by their new ""hidden manifold"" generative model that explicitly considers some low-dimensional manifold.

The reviewers perceived a lack of actionable insights following the paper, since in general these ideas are known, and for MNIST to be a limited dataset, despite finding the paper generally clear and correct.

Following the discussion, I must recommend rejection at this time, but highly encourage the authors to take the insights developed in the paper a bit further and submit to another venue. E.g. trying to improve our algorithms by considering the inductive bias of structure of the hidden manifold, or developing a systematic and quantifiable notion of structure for many different datasets that correlate with difficulty of training would both be great contributions.",Paper Decision
fMw0PHMXls,rJgsskrFwH,Scaling Autoregressive Video Models,Accept (Spotlight),"This paper presents an approach for scalable autoregressive video generation based on a three-dimensional self-attention mechanism. As rightly pointed out by R3, the proposed approach ’is individually close to ideas proposed elsewhere before in other forms ... but this paper does the important engineering work of selecting and combining these ideas in this specific video synthesis problem setting.’ 
The proposed method is relevant and well-motivated, and the experimental results are strong. All reviewers agree that experiments on the Kinetics dataset are particularly appealing. In the initial evaluation, the reviewers have raised several concerns such as performance metrics, ablation study, training time comparison, empirical evaluation of the baseline methods on Kinetics, that were addressed by the authors in the rebuttal. 
In conclusion, all three reviewers were convinced by the author’s rebuttal, and AC recommends acceptance of this paper – congratulations to the authors!",Paper Decision
O0LE7qmXz0,S1eqj1SKvr,TOWARDS FEATURE SPACE ADVERSARIAL ATTACK,Reject,"This paper provides an improved feature space adversarial attack.

However, the contribution is unclear in its significance, in part due to an important prior reference was omitted (song et al.) 

Unfortunately the paper is borderline, and not above the bar for acceptance in the current pool.",Paper Decision
NflYy4z4ZX,SyeYiyHFDH,Convergence Analysis of a Momentum Algorithm with Adaptive Step Size for Nonconvex Optimization,Reject,"The reviewers have reached consensus that while the paper is interesting, it could use more time.  We urge the authors to continue their investigations.",Paper Decision
X1S0P5GNA,SylKikSYDH,Compressive Transformers for Long-Range Sequence Modelling,Accept (Poster),"The paper proposes a ""compressive transformer"", an extension of the transformer, that keeps a compressed long term memory in addition to the fixed sized memory.  Both memories can be queried using attention weights.  Unlike TransfomerXL that discards the oldest memories, the authors propose to ""compress"" those memories.  The main contribution of this work is that that it introduces a model that can handle extremely long sequences. The authors also introduces a new language modeling dataset based on text from Project Gutenberg that has much longer sequences of words than existing datasets.  They provide comprehensive experiments comparing against different compression strategies and compares against previous methods, showing that this method is able to result in lower word-level perplexity. In addition, the authors also present evaluations on speech, and image sequences for RL.

Initially the paper received weak positive responses from the reviewers. The reviewers pointed out some clarity issues with details of the method and figures and some questions about design decisions. After rebuttal, all of the reviewers expressed that they were very satisfied with the authors responses and increased their scores (for a final of 2 accepts and 1 weak accept).

The authors have provided a thorough and well-written paper, with comprehensive and convincing experiments. In addition, the ability to model long-range sequences and dependencies is an important problem and the AC agrees that this paper makes a solid contribution in tackling that problem.  Thus, acceptance is recommended.",Paper Decision
Dj11MrLeXV,ryedjkSFwr,Global Momentum Compression for Sparse Communication in Distributed SGD,Reject,"The author propose a method called global momentum compression for sparse communication setting, and provided some theoretical results on the convergence rate. The convergence result is interesting, but the underlying assumptions used in the analysis appear very strong. Moreover, the proposed algorithm has limited novelty as it is only a minor modification. Another main concern is that the proposed algorithm shows little performance improvement in the experiments. Moreover, more related algorithms should be included in the experimental comparison.",Paper Decision
kLCrV6CyN,BkevoJSYPB,Differentiation of Blackbox Combinatorial Solvers,Accept (Spotlight),"This paper proposes a method for efficiently training neural networks combined with blackbox implementations of exact combinatorial solvers.

Reviewers and AC agree that it is a well written paper with a novel idea supported by good experimental results. Experimental results are of small scale and can be further improved, but the authors acknowledged this aspect well.

Hence, I recommend acceptance.",Paper Decision
FdwJq_Jl-P,rkxDoJBYPB,Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs,Accept (Poster),The submission presents an approach that leverages machine learning to optimize the placement and scheduling of computation graphs (such as TensorFlow graphs) by a compiler. The work is interesting and well-executed. All reviewers recommend accepting the paper.,Paper Decision
7uEbcwt6Ga,B1lDoJSYDH,Lagrangian Fluid Simulation with Continuous Convolutions,Accept (Poster),The paper proposes an approach for N-D continuous convolution on unordered particle set and applies it to Lagrangian fluid simulation. All reviewers found the paper to be a novel and useful contribution towards the problem of N-D continuous convolution on unordered particles. I recommend acceptance.  ,Paper Decision
4uudRwsglG,rkl8sJBYvH,Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks,Accept (Spotlight),This paper carries out extensive experiments on Neural Tangent Kernel (NTK) --kernel methods based on infinitely wide neural nets on small-data tasks. I recommend acceptance.,Paper Decision
ONZVM38Bum,B1eBoJStwr,"Semi-supervised semantic segmentation needs strong, high-dimensional perturbations",Reject,"This paper proposes a method for semi-supervised semantic segmentation through consistency (with respect to various perturbations) regularization. While the reviewers believe that this paper contains interesting ideas and that it has been substantially improved from its original form, it is not yet ready for acceptance to ICLR-2020. With a little bit of polish, this paper is likely to be accepted at another venue.",Paper Decision
mNbW2d3Ki3,B1gHokBKwS,Learning to Guide Random Search,Accept (Poster),"This paper develops a methodology to perform global derivative-free optimization of high dimensional functions through random search on a lower dimensional manifold that is carefully learned with a neural network.  In thorough experiments on reinforcement learning tasks and a real world airfoil optimization task, the authors demonstrate the effectiveness of their method compared to strong baselines.  The reviewers unanimously agreed that the paper was above the bar for acceptance and thus the recommendation is to accept.  An interesting direction for future work might be to combine this methodology with REMBO.  REMBO seems competitive in the experiments (but maybe doesn't work as well early on since the model needs to learn the manifold).  Learning both the low dimensional manifold to do the optimization over and then performing a guided search through Bayesian optimization instead of a random strategy might get the best of both worlds?  ",Paper Decision
hA7jyFYSW,SJlEs1HKDr,Attentive Sequential Neural Processes,Reject,"This manuscript outlines a method to improve the described under-fitting issues of sequential neural processes. The primary contribution is an attention mechanism depending on a context generated through an RNN network. Empirical evaluation indicates empirical results on some benchmark tasks.

In reviews and discussion, the reviewers and AC agreed that the results look promising, albeit on somewhat simplified tasks. It was also brought up in reviews and discussions that the technical contributions seem to be incremental. This combined with limited empirical evaluation suggests that this work might be preliminary for conference publication. Overall, the manuscript in its current state is borderline and would be significantly improved wither by additional conceptual contributions, or by a more thorough empirical evaluation.",Paper Decision
bqlxhX1Y2E,S1e4jkSKvB,The intriguing role of module criticality in the generalization of deep networks,Accept (Spotlight),"The paper analyses the importance of different DNN modules for generalization performance, explaining why certain architectures may be much better performing than others. All reviewers agree that this is an interesting paper with a novel and important contribution. ",Paper Decision
F-82D30Ew,rygEokBKPS,Yet another but more efficient black-box adversarial attack: tiling and evolution strategies,Reject,"This paper proposes a new black-box adversarial attack based on tiling and evolution strategies. While the experimental results look promising, the main concern of the reviewers is the novelty of the proposed algorithm, and many things need to be improved in terms of clarity and experiments. The paper does not gather sufficient support from the reviewers even after author response. I encourage the authors to improve this paper and resubmit to future conference.",Paper Decision
n48TE60Q7p,SJgXs1HtwH,TreeCaps: Tree-Structured Capsule Networks for Program Source Code Processing,Reject,"This paper proposes an application of capsule networks to code modeling.

I see the potential in this approach, but as the reviewers pointed out, in the current draft there are significant issues with respect to both clarity of motivating the work, and in the empirical results (which start at a much lower baseline than previous work). I am not recommending acceptance at this time, but would encourage the reviewers to clarify the issues raised in the reviews for future submission.",Paper Decision
dQwIjMh1wH,SJeQi1HKDH,Learning with Social Influence through  Interior Policy Differentiation,Reject,"The paper proposes a mechanism for obtaining diverse policies for solving a task by posing it as a multi-agent problem, and incentivizing the agents to be different from each other via maximizing total variation.

The reviewers agreed that this is an interesting idea, but had issues with the placement and exact motivations -- precisely what kind of diversity is the work after, why, and what accordingly related approaches does it need to be compared to.
Some reviewers also found the technical and exposition clarity to be lacking.

Given the consensus, I recommend rejection at this time, but encourage the authors to take the reviewers' feedback into account and resubmit to another venue.",Paper Decision
dIMVu9eEDe,SyxGoJrtPr,SPROUT: Self-Progressing Robust Training,Reject,"This paper proposes a new training technique to produce a learned model robust against adversarial attacks -- without explicitly training on example attacked images. The core idea being that such a training scheme has the potential to reduce the cost in terms of training time for obtaining robustness, while also potentially increasing the clean performance. The method does so by proposing a version of label smoothing and doing two forms of data augmentations (gaussian noise and mixup). 

The reviewers were mixed on this work. Two recommended weak reject while one recommended weak accept. All agreed that this work addressed an important problem and that the proposed solution was interesting. The authors and reviewers actively engaged in a discussion, in some cases with multiple back and forths. The main concern of the reviewers is the inconclusive experimental evidence. Though the authors did demonstrate strong performance on PGD attacks, the reviewers had concerns about some attack settings like epsilon and how that may unfairly disadvantage the baselines. In addition, the results on CW presented a different story than the results with PGD. 

Therefore, we do not recommend this work for acceptance in its current form. The work offers strong preliminary evidence of a potential solution to provide robustness without direct adversarial training, but more analysis and explanation of when each component of their proposed solution should increase robustness is needed. 
",Paper Decision
4DDFyn0A5m,Hyxfs1SYwH,Alleviating Privacy Attacks via Causal Learning,Reject,"Maintaining the privacy of membership information contained within the data used to train machine learning models is paramount across many application domains.  Moreover, this risk can be more acute when the model is used to make predictions using out-of-sample data.  This paper applies a causal learning framework to mitigate this problem, motivated by the fact that causal models can be invariant to the training distribution and therefore potentially more resistant to certain privacy attacks.  Both theoretical and empirical results are provided in support of this application of causal modeling.

Overall, during the rebuttal period there was no strong support for this paper, and one reviewer in particular mentioned lingering unresolved yet non-trivial concerns.  For example, to avoid counter-examples raised the reviewer, a deterministic labeling function must be introduced, which trivializes the distribution p(Y|X) and leads to a problematic training and testing scenario from a practical standpoint.  Similarly the theoretical treatment involving Markov blankets was deemed confusing and/or misleading even after careful inspection of all author response details.  At the very least, this suggests that another round of review is required to clarify these issues before publication, and hence the decision to reject at this time.",Paper Decision
t-pXqA-qMI,H1gZsJBYwH,Hybrid Weight Representation: A Quantization Method Represented with Ternary and Sparse-Large Weights,Reject,"The paper proposes a hybrid weighs representation method in deep networks. The authors propose to utilize the extra state in 2-bit ternary representation to encode large weight values. The idea is simple and straightforward. The main concern is on the experimental results. The use of mixed bit width for neural network quantization is not new, but the authors only compare with basic quantization method in the original submission. In the revised version of the paper, the proposed method performs significantly worse than recent quantization methods such as PACT and QIL. Moreover, writing can be improved, and parts of the paper need to be clarified.",Paper Decision
Q0qpdigPU,Hyx-jyBFPr,Self-labelling via simultaneous clustering and representation learning,Accept (Spotlight),"The paper focuses on supervised and self-supervised learning. The originality is to formulate the self-supervised criterion in terms of optimal transport, where the trained representation is required to induce $K$ equidistributed clusters. The formulation is well founded; in practice, the approach proceeds by alternatively optimizing the cross-entropy loss (SGD) and the pseudo-loss, through a fast version of the Sinkhorn-Knopp algorithm, and scales up to million of samples and thousands of classes.

Some concerns about the robustness w.r.t. imbalanced classes, the ability to deliver SOTA supervised performances, the computational complexity have been answered by the rebuttal and handled through new experiments. The convergence toward a local minimum is shown; however, increasing the number of pseudo-label optimization rounds might degrade the results. 

Overall, I recommend to accept the paper as an oral presentation. A more fancy title would do a better justice to this very nice paper (""Self-labelling learning via optimal transport"" ?).  ",Paper Decision
y-f3SvPFfm,HklliySFDS,Continual Learning with Gated Incremental Memories for Sequential Data Processing,Reject,"This manuscript describes a continual learning approach where individual instances consist of sequences, such as language modeling. The paper consists of a definition of a problem setting, tasks in that problem setting, baselines (not based on existing continual learning approaches, which the authors argue is to highlight the need for such techniques, but with which the reviewers took issue), and a novel architecture.

Reviews focused on the gravity of the contribution. R1 and R2, in particular, argued that the paper is written as though the problem/benchmark definition is the main contribution. R2 mentions that in spite of this, the methods section jumps directly into the candidate architecture. As mentioned above, several reviewers also took issue with the fact that existing CL techniques are not employed as baselines. The authors engaged with reviewers and promised updates, but did not take the opportunity to update their paper.

As many of the reviewers' comments remain unaddressed and the authors' updates did not materialize, I recommend rejection, and encourage the authors to incorporate the feedback they have received in a future submission.",Paper Decision
YerBdxWIm,HyxgoyHtDB,Policy Optimization by Local Improvement through Search,Reject,"Thanks for your detailed responses to the reviewers, which helped us a lot to better understand your paper.
However, given that the current manuscript still contains many unclear parts, we decided not to accept the paper. We hope that the reviewers' comments help you improve your paper for potential future submission.",Paper Decision
Mq7p_r9SG1,S1xJikHtDH,Improving Model Compatibility of Generative Adversarial Networks by Boundary Calibration,Reject,"The paper presents a method for increasing the ""model compatibility"" of Generative Adversarial Networks by adding a term to the loss function relating to classification boundaries. The reviewers recognized the importance of the problem, but several concerns were raised about the clarity of the paper, as well as the significance of the experimental results.",Paper Decision
Q1ImzIjMYQ,SJx0q1rtvS,Robust anomaly detection and backdoor attack detection via differential privacy,Accept (Poster),"Thanks for the submission. This paper leverages the stability of differential privacy for the problems of anomaly and backdoor attack detection. The reviewers agree that this application of differential privacy is novel. The theory of the paper appears to be a bit weak (with very strong assumptions on the private learner), although it reflects the basic underlying idea of the detection technique. The paper also provides some empirical evaluation of the technique.",Paper Decision
6CYya7dyCj,HkxCcJHtPr,CAT: Compression-Aware Training for bandwidth reduction,Reject,"This work propose a compression-aware training (CAT) method to allows efficient compression of  feature maps during inference. I read the paper myself. The proposed method is quite straightforward and looks incremental compared with existing approaches based on entropy regularization. 

",Paper Decision
rGUkXMM9-K,Ske6qJSKPH,Scheduling the Learning Rate Via Hypergradients: New Insights and a New Algorithm,Reject,"First, I'd like to apologize once again for failing to secure a third reviewer for this paper. To compensate, I checked the paper more thoroughly than standard.

The area of online adaptation of the learning rate is of great importance and I appreciate the authors' effort in that direction. The authors carefully abundantly cite the research on gradient-based hyperparameter optimization but I would have appreciated to also see past works on stochastic line search (for instance  ""A stochastic line-search method with convergence rate"") or statistical methods (""Using Statistics to Automate Stochastic Optimization"").

The issue with these methods is that, despite usually very positive claims in the paper, they are not that competitive against a carefully tuned fixed schedule and end up not being used in practice. Hence, it is critical to develop a convincing experimental section to assuage doubts. Unfortunately, the experimental section of this work is a bit lacking, as pointed by both reviewers. I would like to comment on two points specifically:
- First, no plot uses wall-clock time as the x-axis. Since the authors state that it can be up to 4 times as slow per iteration, the gains compared to a carefully tuned schedule are unclear.
- Second, the use of a single (albeit two variants) dataset also leads to skepticism. Datasets have vastly different optimization properties and, by not using a wide range of them, one can miss the true sensitivity of the proposed algorithm.

While I do not think that the paper is ready for publication, I feel like there is a clear path to an improved version that could be submitted to a later conference.",Paper Decision
s-h3mZabw,BkxackSKvH,Learning Entailment-Based Sentence Embeddings from Natural Language Inference,Reject,"This paper proposes a method for learning sentence embeddings such that entailment and contradiction relationships between sentence pairs can be inferred by a simple parameter-free operation on the vectors for the two sentences.

Reviewers found the method and the results interesting, but in private discussion, couldn't reach a consensus on what (if any) substantial valuable contributions the paper had proven. The performance of the method isn't compellingly strong in absolute or relative terms, yielding doubts about the value of the method for entailment applications, and the reviewers didn't see a strong enough motivation for the line of work to justify publishing it as a tentative or exploratory effort at ICLR.",Paper Decision
eKytb1tm1x,HJxp9kBFDS,Invariance vs Robustness of Neural Networks,Reject,"This paper examines the interplay between the related ideas of invariance and robustness in deep neural network models. Invariance is the notion that small perturbations to an input image (such as rotations or translations) should not change the classification of that image. Robustness is usually taken to be the idea that small perturbations to input images (e.g. noise, whether white or adversarial) should not significantly affect the model's performance. In the context of this paper, robustness is mostly considered in terms of adversarial perturbations that are imperceptible to humans and created to intentionally disrupt a model's accuracy. The results of this investigation suggests that these ideas are mostly unrelated: equivariant models (with architectures designed to encourage the learning of invariances) that are trained with data augmentation whereby input images are given random rotations do not seem to offer any additional adversarial robustness, and similarly using adversarial training to combat adversarial noise does not seem to confer any additional help for learning rotational invariance. (In some cases, these types of training on the one hand seem to make invariance to the other type of perturbations even worse.)

Unfortunately, the reviewers do not believe the technical results are of sufficient interest to warrant publication at this time. ",Paper Decision
kcLvb6eiDN,r1enqkBtwr,Asymptotic learning curves of kernel methods: empirical data v.s. Teacher-Student paradigm,Reject,"The paper studies, theoretically and empirically, the problem when generalization error decreases as $n^{-\beta}$ where $\beta$ is not $\frac{1}{2}$. It analyses a Teacher-Student problem where the Teacher generates data from a Gaussian random field. The paper provides a theorem that derives $\beta$ for Gaussian and Laplace kernels, and show empirical evidence supporting the theory using MNIST and CIFAR.

The reviews contained two low scores, both of which were not confident. A more confident reviewer provided a weak accept score, and interacted multiple times with the authors during the discussion period (which is one of the nice things about the ICLR review process). However, this reviewer also noted that ICLR may not be the best venue for this work.

Overall, while this paper shows promise, the negative review scores show that the topic may not be the best fit to the ICLR audience.",Paper Decision
yQCUg_xaq2,rklhqkHFDB,LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS,Reject,"The authors demonstrate how neural networks can be used to learn vectorial representations of a set of items given only triplet comparisons among those items.  The reviewers had some concerns regarding the scale of the experiments and strength of the conclusions:  empirically, it seemed like there should be more truly large-scale experiments considering that this is a selling point; there should have been more analysis and/or discussion of why/how the neural networks help; and the claim that deep networks are approximately solving an NP-hard problem seemed unimportant as they are routinely used for this purpose in ML problems.  With a combination of improved experiments and revised discussion/analysis, I believe a revised version of this paper could make a good submission to a future conference.",Paper Decision
v0hOoPNtCW,ByxoqJrtvr,Learning to Reach Goals Without Reinforcement Learning,Reject,"The authors present an algorithm that utilizes ideas from imitation learning to improve on goal-conditioned policy learning methods that rely on RL, such as hindsight experience replay.  Several issues of clarity and the correctness of the main theoretical result were addressed during the rebuttal period in way that satisfied the reviewers with respect to their concerns in these areas.  However, after discussion, the reviewers still felt that there were some fundamental issues with the paper, namely that the applicability of this method to more general RL problems (complex reward functions rather than signle state goals, time ) is unclear.  The basic idea seems interesting, but it needs further development, and non-trivial modifications, to be broadly applicable as an approach to problems that RL is typically used on.  Thus, I recommend rejection of the paper at this time.",Paper Decision
xJWyBk6ujM,S1eq9yrYvH,Subjective Reinforcement Learning for Open Complex Environments,Reject,"The authors propose a learning framework to reframe non-stationary MDPs as smaller stationary MDPs, thus hopefully addressing problems with contradictory or continually changing environments. A policy is learned for each sub-MDP, and the authors present theoretical guarantees that the reframing does not inhibit agent performance.

The reviewers discussed the paper and the authors' rebuttal. They were mainly concerned that the submission offered no practical implementation or demonstration of feasibility, and secondarily concerned that the paper was unclearly written and motivated. The authors' rebuttal did not resolve these issues.

My recommendation is to reject the submission and encourage the authors to develop an empirical validation of their method before resubmitting.",Paper Decision
rIS6lnLl9,SJeq9JBFvH,Deep probabilistic subsampling for task-adaptive compressed sensing,Accept (Poster),"This paper introduces a probabilistic data subsampling scheme that can be optimized end-to-end.  The experimental evaluation is a bit weak, focusing mostly on toy-scale problems, and I would have liked to see a discussion of bias in the Gumbel-max gradient estimator.  

It's also not clear how the free hyperparameters for this method were chosen, which makes me suspect they were tuned on the test set.

However, the overall idea is sensible, and the area seems under-explored.",Paper Decision
8jlAURBDwx,H1lK5kBKvr,Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations,Reject,"This paper proposes a semi-supervised method for reconstructing 3D faces from images via a disentangled representation. The method builds on previous work by Tran et al (2018, 2019). While some results presented in the paper show that this method works well, all reviewers agree that the authors should have provided more experimental evidence to convincingly demonstrate the benefits of their method. The reviewers are also unconvinced by how computationally expensive this method is or by the contributions of the unlabelled data to the performance of the proposed model. Given that the authors did not address the reviewers’ concerns, and for the reasons stated above, I recommend rejecting this paper.",Paper Decision
lCUVPPi_Kq,Bkxd9JBYPH,Representing Model Uncertainty of Neural Networks in Sparse Information Form,Reject,"This paper presents a variant of recently developed Kronecker-factored approximations to BNN posteriors. It corrects the diagonal entries of the approximate Hessian, and in order to make this scalable, approximates the Kronecker factors as low-rank.

The approach seems reasonable, and is a natural thing to try. The novelty is fairly limited, however, and the calculations are mostly routine. In terms of the experiments: it seems like it improved the Frobenius norm of the error, though it's not clear to me that this would be a good measure of practical effectiveness. On the toy regression experiment, it's hard for me to tell the difference from the other variational methods. It looks like it helped a bit in the quantitative comparisons, though the improvement over K-FAC doesn't seem significant enough to justify acceptance purely based on the results.

Reviewers felt like there was a potentially useful idea here and didn't spot any serious red flags, but didn't feel like the novelty or the experimental results were enough to justify acceptance. I tend to agree with this assessment.
",Paper Decision
9jjSWkFvGw,rJe_cyrKPB,GroSS Decomposition: Group-Size Series Decomposition for Whole Search-Space Training,Reject,"The authors use a Tucker decomposition to represent the weights of a network, for efficient computation. The idea is natural, and preliminary results promising. The main concern was lack of empirical validation and comparisons. While the authors have provided partial additional results in the rebuttal, which is appreciated, a thorough set of experiments and comparisons would ideally be included in a new version of the paper, and then considered again in review.
",Paper Decision
DCC3FVzwnE,SklD9yrFPS,Neural Tangents: Fast and Easy Infinite Neural Networks in Python,Accept (Spotlight),"This paper presents a software library for dealing with neural networks either in the (usual) finite limit or in the infinite limit. The latter is obtained by using the Neural Tangent Kernel theory. 

There is variance in the reviewers' scores, however there has also been quite a lot of discussion, which has been facilitated by the authors' elaborate rebuttal. The main points in favor and against are clear: on the positive side, the library is demonstrated well (especially after rebuttal) and is equipped with desirable properties such as usage of GPU/TPU, scalability etc. On the other hand, a lot of the key insights build heavily on prior work of Lee et al, 2019. However, judging novelty when it comes to a software paper is more tricky to do, especially given that not many such papers appear in ICLR and therefore calibration is difficult. This has been discussed among reviewers. 

It would help if some further theoretical insights were included in this paper; these insights could come by working backwards from the implementation (i.e. what more can we learn about infinite width networks now that we can experiment easily with them?).

Overall, this paper should still be of interest to the ICLR community.
",Paper Decision
qhsMxvSKZR,SJgw51HFDr,Sparse Weight Activation Training,Reject,The paper is proposed a rejection based on majority reviews.,Paper Decision
Pr4eFU2vJ,B1xwcyHFDr,Learning Robust Representations via Multi-View Information Bottleneck,Accept (Poster),"This paper extends the information bottleneck method to the unsupervised representation learning under the multi-view assumption. The work couples the multi-view InfoMax principle with the information bottleneck principle to derive an objective which encourages the representations to contain only the information shared by both views and thus eliminate the effect of independent factors of variations. Recent advances in estimating lower-bounds on mutual information are applied to perform approximate optimisation in practice. The authors empirically validate the proposed approach in two standard multi-view settings.
Overall, the reviewers found the presentation clear, and the paper well written and well motivated. The issues raised by the reviewers were addressed in the rebuttal and we feel that the work is well suited for ICLR. We ask the authors to carefully integrate the detailed comments from the reviewers into the manuscript. Finally, the work should investigate and briefly establish a connection to [1].

[1] Wang et al. ""Deep Multi-view Information Bottleneck"". International Conference on Data Mining 2019 (https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.5)",Paper Decision
3oNYtwtzz,Bke89JBtvB,Batch-shaping for learning conditional channel gated networks,Accept (Poster),"The paper describes a method to train a convolutional network with large capacity, where channel-gating (input conditioned) is implemented - thus, only parts of the network are used at inference time. The paper builds over previous work, with the main contribution being a ""batch-shaping"" technique that regularizes the channel gating to follow a beta distribution, combined with L0 regularization. The paper shows that ResNet trained with this technique can achieve higher accuracy with lower theoretical MACs. Weakness of the paper is that more engineering would be required to convert the theoretical MACs into actual running time - which would further validate the practicality of the approach.
",Paper Decision
vtSBjN2fdJ,rJg851rYwH,"Making the Shoe Fit: Architectures, Initializations, and Tuning for Learning with Privacy",Reject,"This paper presents experimental evidence that learning with privacy requires optimization of the model settings (architectures and initializations) that are not identical to those used when learning without privacy. While acknowledging potential usefulness of this work for practitioners, the reviewers expressed several important concerns such as (1) lack of SOTA baseline comparisons, (2) lack of clarity of the empirical evaluation protocols, (3) large models (that are widely used in practice) have not been studied in the paper, (4) low technical novelty. The authors have successfully addressed some of the concerns regarding (1) and (2). However (3) and (4) make it difficult to assess the benefits of the proposed approach for the community and were viewed by AC as critical issues. We hope the detailed reviews are useful for improving and revising the paper. ",Paper Decision
BXtAz0FOce,HygS91rYvH,Universal Adversarial Attack Using Very Few Test Examples,Reject,"The paper proposes to get universal adversarial examples using few test samples. The approach is very close to the Khrulkov & Oseledets, and the abstract for some reason claims that it was proposed independently, which looks like a very strange claim. Overall, all reviewers recommend rejection, and I agree with them.",Paper Decision
vzvObgEQ7V,rklr9kHFDB,Rotation-invariant clustering of neuronal responses in primary visual cortex,Accept (Talk),This paper is enthusiastically supported by all three reviewers. Thus an accept is recommended.,Paper Decision
T0ytmBXEG6,HJxV5yHYwB,Solving single-objective tasks by preference multi-objective reinforcement learning,Reject,"The paper considers planning through the lenses both of a single and multiple objectives. The paper then discusses the pareto frontiers of this optimization. While this is an interesting direction, the reviewers feel a more careful comparison to related work is needed.",Paper Decision
vJPGOWeB_,rkeNqkBFPB,Deep automodulators,Reject,"The manuscript proposes an autoencoder architecture incorporating two recent architectural innovations from the GAN literature (progressive growing & feature-wise modulation), trained with the adversarial generator-encoder paradigm with a novel cyclic loss meant to encourage disentangling, and procedure for enforcing layerwise invariances. The authors demonstrate coarse/fine visual transfer on generative modeling of face images, as well as generative modeling results on several Large Scale Scene Understanding (LSUN) datasets. 

Reviewers generally found the results somewhat compelling and the ideas valuable and well-motivated, but criticized the presentation clarity, lack of ablation studies, and that the claims made were not sufficiently supported by the empirical evidence. The authors revised, and while it was agreed that clarity was improved, some reviewers were still not satisfied with the level of clarity (the revision appeared at the very end of the discussion period, unfortunately not allowing for any further refinement). Ablation studies were added in the revised manuscript, which were appreciated, but seemed to suggest that the proposed loss function was of mixed utility: while style-mixing quantitatively improved, overall sample quality appeared to suffer.

As the reviewers remain unconvinced as to the significance of the contribution and the clarity of its presentation, I recommend rejection at this time, while encouraging the authors to further refine the presentation of their ideas for a future resubmission.",Paper Decision
srZ7H-3QtN,BkgNqkHFPr,Enhanced Convolutional Neural Tangent Kernels,Reject,"This paper was assessed by three reviewers who scored it as 6/3/6. 
The reviewers liked some aspects of this paper e.g., a good performance, but they also criticized some aspects of work such as inventing new names for existing pooling operators, observation that large parts of improvements come from the pre-processing step rather than the proposed method, suspected overfitting.  Taking into account all positives and negatives, AC feels that while the proposed idea has some positives, it also falls short of the quality required by ICLR2020, thus it cannot be accepted at this time. AC strongly encourages authors to go through all comments (especially these negative ones), address them and resubmit an improved version to another venue.

",Paper Decision
haDMmb_GW,H1g79ySYvB,Revisiting Gradient Episodic Memory for Continual Learning,Reject,"This paper proposes an extension of Gradient Episodic Memory (GEM) namely support examples, soft gradient constraints, and positive backward transfer. The authors argue that experiments on MNIST and CIFAR show that the proposed method consistently improves over the original GEM.

All three reviewers are not convinced with experiments in the paper. R1 and R3 mentioned that the improvements over GEM appear to be small. R2 and R3 also have some concerns without results with multiple runs. R3 has questions about hyperparameter tuning. The authors also appears to be missing recent developments in this area (e.g., A-GEM). The authors did not provide a rebuttal to these concerns.

I agree with the reviewers and recommend rejecting this paper.",Paper Decision
DoQRNUXgZL,rkem91rtDB,Inductive and Unsupervised Representation Learning on Graph Structured Objects,Accept (Poster),"The paper focuses on the problem of finding dense representations of graph-structured objects in an unsupervised manner. The authors propose a novel framework for solving this problem and show that it improves over competitive baselines. The reviewers generally liked the paper, although were concerned with the strength of the experimental results. During the discussion phase, the authors bolstered the experimental results. The reviewers are satisfied with the resulting paper and agree that it should be accepted.",Paper Decision
ueJfykS85b,SyxM51BYPB,A new perspective in understanding of Adam-Type algorithms and beyond,Reject,"In this paper, the authors draw upon online convex optimization in order to derive a different interpretation of Adam-Type algorithms, allowing them to identify the functionality of each part of Adam. Based on these  observations, the authors derive a new Adam-Type algorithm,  AdamAL and test it in 2 computer vision datasets using 3 CNN architectures. The main concern shared by all reviewers is the lack of novelty but also rigor both on the experimental and theoretical justification provided by the authors. After having read carefully the reviews and main points of the paper, I will side with the reviewers, thus not recommending acceptance of this paper. ",Paper Decision
IWKzm3Lt_1,HyeG9yHKPr,Causally Correct Partial Models for Reinforcement Learning,Reject,"The authors show that in a reinforcement learning setting, partial models can be causally incorrect, leading to improper evaluation of policies that are different from those used to collect the data for the model.  They then propose a backdoor correction to this problem that allows the model to generalize properly by separating the effects of the stochasticity of the environment and the policy.  The reviewers had substantial concerns about both issues of clarity and the clear, but largely undiscussed, connection to off-policy policy evaluation (OPPE).  

In response, the authors made a significant number of changes for the sake of clarity, as well as further explained the differences between their approach and the OPPE setting.  First, OPPE is not typically model-based.  Second, while an importance sampling solution would be technically possible, by re-training the model based on importance-weighted experiences, this would need to be done for every evaluation policy considered, whereas the authors' solution uses a fundamentally different approach of causal reasoning so that a causally correct model can be learned once and work for all policies.

After much discussion, the reviewers could not come to a consensus about the validity of these arguments.  Futhermore, there were lingering questions about writing clarity. Thus, in the future, it appears the paper could be significantly improved if the authors cite more of the off policy evaluation literature, in addition to their added textual clairifications of the relation of their work to that body of work.  Overall, my recommendation at this time is to reject this paper.",Paper Decision
7cwa_fkVn7,rkgb9kSKwS,Spectral Nonlocal Block for Neural Network,Reject,"This paper proposes a new formulation of the non-local block and interpret it from the graph view. The idea is interesting and the experimental results seems to be promising.

Reviewer has two major concerns. The first is the presentation, which is not clear enough. The second is the experimental design and analysis. The authors add more video dataset in the revision, but still lack comprehensive experimental analysis for video-based applications. 

Overall, the idea of non-local block from graph view is interesting. However, the presentation of the paper needs further polish and thus does not meet the standard of ICLR
",Paper Decision
qr0-XhhOa,BJlZ5ySKPH,U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation,Accept (Poster),"The paper proposes a new architecture for unsupervised image2image translation.
Following the revision/discussion, all reviewers agree that the proposed ideas are reasonable, well described, convincingly validated, and of clear though limited novelty. Accept.",Paper Decision
vCnL7BiIIy,BJe-91BtvH,Masked Based Unsupervised Content Transfer,Accept (Poster),"This paper extends the prior work on disentanglement and attention guided translation to instance-based unsupervised content transfer. The method is somewhat complicated, with five different networks and a multi-component loss function, however the importance of each component appears to be well justified in the ablation study. Overall the reviewers agree that the experimental section is solid and supports the proposed method well. It demonstrates good performance across a number of transfer tasks, including transfer to out-of-domain images, and that the method outperforms the baselines. For these reasons, I recommend the acceptance of this paper.",Paper Decision
f5ddPv_r1,rkgl51rKDB,Efficient meta reinforcement learning via meta goal generation,Reject,"This paper combines PEARL with HAC to create a hierarchical meta-RL algorithm that operates on goals at the high level and learns low-level policies to reach those goals. Reviewers remarked that it’s well-presented and well-organized, with enough details to be mostly reproducible. In the experiments conducted, it appears to show strong results.

However there was strong consensus on two major weaknesses that render this paper unpublishable in its current form: 1) the continuous control tasks used don’t seem to require hierarchy, and 2) the baselines don’t appear to be appropriate. Reviewers remarked that a vital missing baseline is HER, and that it’s unfair to compare to PEARL, which is a more general meta-RL algorithm. The authors don’t appear to have made revisions in response to these concerns.

All reviewers made useful and constructive comments, and I urge the authors to take them into consideration when revising for a future submission.",Paper Decision
ZEbtWjZuMc,B1elqkrKPH,Learning robust visual representations using data augmentation invariance,Reject,"This paper introduces an unsupervised learning objective that attempts to improve the robustness of the learnt representations. This approach is empirically demonstrated on cifar10 and tiny imagenet with different network architectures including all convolutional net, wide residual net and dense net.   Two of three reviewers felt that the paper was not suitable for publication at ICLR in its current form.  Self supervision based on preserving network outputs despite data transformations is a relatively minor contribution, the framing of the approach as inspired by biological vision notwithstanding.  Several references, including at a past ICLR include:
http://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf
and 
Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations.  In International Conference on Learning Representations (ICLR), 2018.",Paper Decision
wSEyBuSwHw,rJxyqkSYDH,A Simple Dynamic Learning Rate Tuning Algorithm For Automated Training of DNNs,Reject,"This paper proposes an automatic tuning procedure for the learning rate of SGD. Reviewers were in agreement over several of the shortcomings of the paper, in particular its heuristic nature. They also took the time to provide several ways of improving the work which I suggest the authors follow should they decide to resubmit it to a later conference.",Paper Decision
uWqxgeGU7f,Hkx1qkrKPr,DropEdge: Towards Deep Graph Convolutional Networks on Node Classification,Accept (Poster),"The paper proposes a very simple but thoroughly evaluated and investigated idea for improving generalization in GCNs. Though the reviews are mixed, and in the post-rebuttal discussion the two negative reviewers stuck to their ratings, the area chair feels that there are no strong grounds for rejection in the negative reviews. Accept.",Paper Decision
U62l4vJvvv,ByeAK1BKPB,Projected Canonical Decomposition for Knowledge Base Completion,Reject,"The paper proposes a tensor decomposition method that interpolates between Tucker and CP decompositions. The authors also propose an optimization algorithms (AdaImp) and argue that it has superior performance against AdaGrad in this tesnor decomposition task. The approach is evaluated on some NLP tasks.
The reviewers raised some concerns related to clarity, novelty, and strength of experiments. As part of addressing reviewers concerns, the authors reported their own results on MurP and Tucker (instead of quoting results from reference papers). While the reviewers greatly appreciated these experiments as well as authors' response to their questions and feedback, the concerns largely remained unresolved. In particular, R2 found the gain achieved by AdaImp not significantly large compared to Adagrad. In addition, R2 found very limited evaluation on how AdaImp outperforms Adagrad (thus little evidence to support that claim). Finally, AdaImp lacks any theoretical analysis (unlike Adagrad).",Paper Decision
hbKo5rF0Fw,Hke0K1HKwr,Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue,Accept (Spotlight),"This paper proposes a sequential latent variable model for the knowledge selection task for knowledge grounded dialogues. Experimental results demonstrate improvements over the previous SOTA in the WoW, knowledge grounded dialogue dataset, through both automated and human evaluation. All reviewers scored the paper highly, but they also made several suggestions for improving the presentation. Authors responded positively to all these suggestions and provided updated results and other stats. The paper will be a good contribution to ICLR.",Paper Decision
HfeiuWl9H,SJlpYJBKvH,Measuring the Reliability of Reinforcement Learning Algorithms,Accept (Spotlight),"Main content:

This paper provides a unified way to provide robust statistics in evaluating the reliability of RL algorithms, especially deep RL algorithms. Though the metrics are not particularly novel, the investigation should be useful to the broader community as it compares seven specific evaluation metrics, including 'Dispersion across Time (DT): IQR across Time', 'Short-term Risk across Time (SRT): CVaR on Differences', 'Long-term Risk across Time (LRT): CVaR on Drawdown', 'Dispersion across Runs (DR): IQR across Runs', 'Risk across Runs (RR): CVaR across Runs', 'Dispersion across Fixed-Policy Rollouts (DF): IQR across Rollouts' and 'Risk across Fixed-Policy Rollouts (RF): CVaR across Rollouts'. The paper further proposed ranking and also confidence intervals based on bootstrapped samples, and compared against continuous control and discrete actions algorithms on Atari and OpenAI Gym.

--

Discussion:

The reviews clearly agree on accepting the paper, with a weak accept coming from a reviewer who does not know much about this subarea. Comments are mostly just directed at clarifications and completeness of description, which the authors have addressed.

--

Recommendation and justification:

This paper should be accepted due to its useful contributions toward doing a better job of measuring performance of RL.",Paper Decision
6a6g97cBtW,H1enKkrFDB,Stable Rank Normalization for Improved Generalization in Neural Networks and GANs,Accept (Spotlight),"The authors propose stable rank normalization, which minimizes the stable rank of a linear operator and apply this to neural network training. The authors present techniques for performing the normalization efficiently and evaluate it empirically in a range of situations. The only issues raised by reviewers related to the empirical evaluation. The authors addressed these in their revisions. ",Paper Decision
mfmGtUPx9,ryestJBKPB,Graph Neural Networks for Soft Semi-Supervised Learning on Hypergraphs,Reject,"This paper proposes and evaluates using graph convolutional networks for semi-supervised learning of probability distributions (histograms). The paper was reviewed by three experts, all of whom gave a Weak Reject rating. The reviewers acknowledged the strengths of the paper, but also had several important concerns including quality of writing and significance of the contribution, in addition to several more specific technical questions. The authors submitted a response that addressed these concerns to some extent. However, in post-rebuttal discussions, the reviewers chose not to change their ratings, feeling that quality of writing still needed to be improved and that overall a significant revision and another round of peer review would be needed. In light of these reviews, we are not able to recommend accepting the paper, but hope the authors will find the suggestions of the reviewers helpful in preparing a revision for another venue. ",Paper Decision
CQaoL3CA_j,BylsKkHYvH,Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks,Accept (Poster),"This paper investigates the problem of using zero imputation when input features are missing. The authors study this problem, propose a solution, and evaluate on several benchmark datasets. The reviewers were generally positive about the paper, but had some questions and concerns about the experimental results. The authors addressed these concerns in the rebuttal. The reviewers are generally satisfied and believe that the paper should be accepted.",Paper Decision
Iur9nqT9Nl,Byg5KyHYwr,Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks,Reject,"This paper addresses the problem of exploration in challenging RL environments using self-imitation learning. The idea behind the proposed approach is for the agent to imitate a diverse set of its own past trajectories. To achieve this, the authors introduce a policy conditioned on trajectories. The proposed approach is evaluated on various domains including Atari Montezuma's Revenge and MuJoCo.

Given that the evaluation is purely empirical, the major concern is in the design of experiments. The amount of stochasticity induced by the random initial state alone does not lead to convincing results regarding the performance of the proposed approach compared with baselines (e.g. Go-Explore). With such simple stochasticity, it is not clear why one could not use a model to recover from it and then rely on an existing technique like Go-Explore. Although this paper tackles an important problem (hard-exploration RL tasks), all reviewers agreed that this limitation is crucial and I therefore recommend to reject this paper.",Paper Decision
SOOwYIQXMF,SJecKyrKPH,ICNN: INPUT-CONDITIONED FEATURE REPRESENTATION LEARNING FOR TRANSFORMATION-INVARIANT NEURAL NETWORK,Reject,"This paper proposes a CNN that is invariant to input transformation, by making two modifications on top of the TI-pooling architecture: the input-dependent convolutional filters, and a decoder network to ensure fully transformation invariant. Reviewer #1 concerns the limited novelty, unconvincing experimental results. Reviewer #2 praises the paper being well written, but is not convinced by the significance of the contributions. The authors respond to Reviewer #2 but did not change the rating. Reviewer #3 especially concerns that the paper is not well positioned with respect to the related prior work.  Given these concerns and overall negative rating (two weak reject and one reject), the AC recommends reject.",Paper Decision
NcnZ6gab-Y,SkeKtyHYPS,Data Augmentation in Training CNNs: Injecting Noise to Images,Reject,"This paper studies the effect of various data augmentation methods on image classification tasks. The authors propose the structural similarity as a measure of the magnitude of the various types of data augmentation noise they consider and argue that it is outperforms PSNR as a measure of the intensity of the noise. The authors performed an empirical analysis showing that speckle noise leads to improved CNN models on two subsets of ImageNet. While there is merit in thoroughly analysing data augmentation schemes for training CNNs, the reviewers argued that the main claims of the work were not substantiated and the raised issues were not addressed in the rebuttal. I will hence recommend rejection of this paper.
",Paper Decision
hn1-7_MRN,S1xKYJSYwS,VAENAS: Sampling Matters in Neural Architecture Search,Reject,"This paper proposes to represent the distribution w.r.t. which neural architecture search (NAS) samples architectures through a variational autoencoder, rather than through a fully factorized distribution (as previous work did). 

In the discussion, a few things improved (causing one reviewer to increase his/her score from 1 to 3), but it became clear that the empirical evaluation has issues, with a different search space being used for the method than for the baselines. There was unanimous agreement for rejection. I agree with this judgement and thus recommend rejection.",Paper Decision
HeE6xIIgJA,S1g_t1StDB,Self-Educated Language Agent with Hindsight Experience Replay for Instruction Following,Reject,"Two reviewers are borderline and one recommends rejection. The main criticism is the simplicity of language, scalability to a more complex problem, and questions about experiments. Due to the lack of stronger support, the paper cannot be accepted at this point. The authors are encouraged to address the reviewer's comments and resubmit to a future conference.",Paper Decision
5z8aGOxQ4,HJg_tkBtwS,Model-Agnostic Feature Selection with Additional Mutual Information,Reject,"The paper presents an approach to feature selection. Reviews were mixed and questions whether the paper has enough substance, novelty, the correctness of the theoretical contributions, experimental details, as well as whether the paper compares to the relevant literature.  ",Paper Decision
61KUonbIR0,ryevtyHtPr,Do Deep Neural Networks for Segmentation Understand Insideness?,Reject,"This paper investigates a notion of recognizing insideness (i.e., whether a pixel is inside a closed curve/shape in the image) with deep networks. It's an interesting problem, and the authors provide analysis on the limitations of existing architectures (e.g., feedforward and recurrent networks) and present a trick to handle the long-range relationships. While the topic is interesting, the constructed datasets are quite artificial and it's unclear how this study can lead to practically useful results (e.g., improvement in semantic segmentation, etc.). ",Paper Decision
BWCvP_iAZM,rygvFyrKwH,Adversarial Robustness as a Prior for Learned Representations,Reject,"The paper proposes recasting robust optimization as regularizer for learning representations by neural networks, resulting e.g. in more semantically meaningful representations. 

The reviewers found that the claimed contributions were well supported by the experimental evidence. The reviewers noted a few minor points regarding clarity that seem to have been addressed. The problems addressed are very relevant to the ICLR community (representation learning and adversarial robustness).

However, the reviewers were not convinced by the novelty of the paper. A big part of the discussion focused on prior work by the authors that is to be published at NeurIPS. This paper was not referenced in the manuscript but does reduce the novelty of the present submission. In contrast to the current submission, that paper focuses on manipulating the learned manipulations to solve image generation tasks, whereas the current paper focuses on the underlying properties of the representation. Since the underlying phenomenon had been described in the earlier paper and the current submission does not introduce a new approach / algorithm, the paper was deemed to lack the novelty for acceptance to ICLR. 

",Paper Decision
x1ZETd2h-j,HygDF1rYDB,Explaining Time Series by Counterfactuals,Reject,"The paper proposes a definition of and an algorithm for computing the importance                                                   
of features in time series classification / regression.                                                                            
The importance is defined as a finite difference version of standard sensitivity                                                   
analysis, where the distribution over finite perturbations is given by a                                                           
learned time series model.                                                                                                         
The approach is tested on simulated and real-world data sets.                                                                      
                                                                                                                                   
The reviewers note a lack of novelty in the paper and deem the contribution                                                        
somewhat incremental, although exposition and experiments have improved compared                                                   
to previous versions of the manuscript.                                                                                            
                                                                                                                                   
I recommend to reject this paper in its current form, taking into account on the reviews and my own                                
reading, mostly due t a lack of novelty.                                                                                           
Furthermore, the authors call their method a ""counterfactual"" approach.                                                            
I don't agree with this terminology.                                                                                               
No attempt is made to justify is by linking it to the relevant causal literature                                                   
on counterfactuals.                                                                                                                
The authors do indeed motivate their algorithm by considering how the classifier                                                   
output would change ""had an observation been different"" (a counterfactual), but                                                    
mathematical in their model this the same as asking ""what changes if the observation is                                            
different"" (interventional query). ",Paper Decision
xJgB3NAy9D,rkg8FJBYDS,Variational Diffusion Autoencoders with Random Walk Sampling,Reject,"This paper proposes to train latent-variable models (VAEs) based on diffusion maps on the data-manifold. While this is an interesting idea, there are substantial problems with the current draft regarding clarity, novelty and scalability. In its current form, it is unlikely that the proposed model will have  a substantial impact on the community.",Paper Decision
qEuv1CdS4S,S1g8K1BFwS,Probability Calibration for Knowledge Graph Embedding Models,Accept (Poster),"The paper proposes a novel method to calibrate a knowledge graph embedding method when ground truth negatives are not available. Essentially, the method relies on generating corrupted triples as negative examples to be used by known approaches (Platt scaling and isotonic regression). 

This is claimed as the first approach of probability calibration for knowledge graph embedding models, which is considered to be very relevant for practitioners working on knowledge graph embedding (although this is a narrow audience). The paper does not propose a wholly novel method for probability calibration. Instead, the value in experimental insights provided.

Some reviewers would have liked to see a more in-depth analysis, but reviewers appreciated the thoroughness of the results in the clear articulation of the findings and the fact that multiple datasets and models are studied. 

There was an animated discussion about this paper, but the paper seems a useful contribution to the ICLR community and I would like to recommend acceptance. ",Paper Decision
fmuqVMOM6,BkgStySKPB,Contrastive Multiview Coding,Reject,"This paper proposes to use contrastive predictive coding for self-supervised learning.  The proposed approach is shown empirically to be  more effective than existing self-supervised learning algorithms.  While the reviewers found the experimental results encouraging, there were some questions about the contribution as a whole, in particular the lack of theoretical justification.",Paper Decision
D2TghbVzY8,rkgNKkHtvB,Reformer: The Efficient Transformer,Accept (Talk),"Transformer models have proven to be quite successful when applied to a variety of ML tasks such as NLP.  However, the computational and memory requirements can at times be prohibitive, such as when dealing with long sequences.  This paper proposes locality-sensitive hashing to reduce the sequence-length complexity, as well as reversible residual layers to reduce storage requirements.  Experimental results confirm that the performance of Transformer models can be preserved even with these new efficiencies in place, and hence, this paper will likely have significant impact within the community.  

Some relatively minor points notwithstanding, all reviewers voted for acceptance which is my recommendation as well.  Note that this paper was also vetted by several detailed external commenters.  In all cases the authors provided reasonable feedback, and the final revision of the work will surely be even stronger.",Paper Decision
H7_03fz3S,S1gEFkrtvH,BasisVAE: Orthogonal Latent Space for Deep Disentangled Representation,Reject,"The paper proposes a new way to learn a disentangled representation by embedding the latent representation z into an explicit learnt orthogonal basis M. While the paper proposes an interesting new approach to disentangling, the reviewers agreed that it would benefit from further work in order to be accepted. In particular, after an extensive discussion it was still not clear whether the assumptions of Theorem 1 applied to VAEs, and whether Theorem 1 was necessary at all. In terms of experimental results, the discussions revealed that the method used supervision during training, while the baselines in the paper are all unsupervised. The authors are encouraged to add supervised baselines in the next iteration of the manuscript. For these reasons I recommend rejection.",Paper Decision
X4P5WYPW9O,BygXFkSYDH,Target-Embedding Autoencoders for Supervised Representation Learning,Accept (Talk),"The paper presents a general view of supervised learning models that are jointly trained with a model for embedding the labels (targets), which the authors dub target-embedding autoencoders (TEAs).  Similar models have been studied before, but this paper unifies the idea and studies more carefully various components of it.  It provides a proof for the specific case of linear models and a set of experiments on disease trajectory prediction tasks.  The reviewer concerns were addressed well by the authors and I believe the paper is now strong.  It would be even stronger if it included more tasks (and in particular some ""typical"" tasks that more of the community is focusing on), and the theoretical part is to my mind not a major contribution, or at least not as large as the paper implies, because it analyzes a much simpler model than anyone is likely to use TEAs for.",Paper Decision
e8xCALjLhP,BJlQtJSKDB,Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search,Accept (Talk),"The paper investigates parallelizing MCTS.                                                                                         
The authors propose a simple method based on only updating the exploration bonus                                                   
in (P)-UCT by taking into account the number of currently ongoing / unfinished                                                               
simulations.                                                                                                                       
The approach is extensively tested on a variety of environments, notably                                                            
including ATARI games.                                                                                                             
                                                                                                                                   
This is a good paper.                                                                                                              
The approach is simple, well motivated and effective.                                                                              
The experimental results are convincing and the authors made a great effort to                                                     
further improve the paper during the rebuttal period.                                                                              
I recommend an oral presentation of this work, as MCTS has become a                                                                
core method in RL and planning, and therefore I expect a lot of interest in the                                                    
community for this work.                                                                                                           
                         ",Paper Decision
BdYG9tZBDR,BklmtJBKDB,Conditional Flow Variational Autoencoders for Structured Sequence Prediction,Reject,"The novelty of the proposed work is a very weak factor, the idea has been explored in various forms in previous work.",Paper Decision
UAfCAtXDUk,HkeMYJHYvS,High-Frequency guided Curriculum Learning for Class-specific Object Boundary Detection,Reject,"This paper received all negative reviewers, and the scores were kept after the rebuttal. The authors are encouraged to submit their work to a computer vision conference where this kind of work may be more appreciated. Furthermore, including stronger baselines such as Acuna et al is recommended.",Paper Decision
sNbwqRtqb1,SJxzFySKwH,On the Equivalence between Positional Node Embeddings and Structural Graph Representations,Accept (Poster),"The paper shows the relationship between node embeddings and structural graph representations. By careful definition of what structural node representation means, and what node embedding means, using the permutation group, the authors show in Theorem 2 that node embeddings cannot represent any extra information that is not already in the structural representation. The paper then provide empirical experiments on three tasks, and show in a fourth task an illustration of the theoretical results.

The reviewers of the paper scored the paper highly, but with low confidence. I read the paper myself (unfortunately not with a lot of time), with the aim of increasing the confidence of the resulting decision. The main gap in the paper is between the phrases ""structural node representation"" and ""node embedding"", and their theoretical definitions. The analogy of distribution and its samples follows unsurprisingly from the definitions (8 and 12), but the interpretation of those definitions as the corresponding English phrases is not obvious by only looking at the definitions. There also seems to be a sleight of hand going on with the most expressive representations (Definitions 9 and 11), which is used to make the conditional independence statement of Theorem 2. The authors should clarify in the final version whether the existence of such a representation can be shown, or even better a constructive way to get it from data.

Given the significance of the theoretical results, the authors should improve the introduction of the two main concepts by:
- relating them to prior work (one way is to move Section 5 towards the front)
- explaining in greater detail why Definitions 8 and 12 correspond to the two concepts. For example expanding the part of the proof of Corollary 1 about SVD, to make clear what Definition 12 means.
- a corresponding simple example of Definition 8 to relate to a classical method.

The paper provides a nice connection between two disparate concepts. Unfortunately, the connection uses graph invariance and equivariance, which is unfamiliar to many of the ICLR audience. On balance, I believe that the authors can improve the presentation such that a reader can understand the implications of the connection without being an expert in graph isomorphism. As such, I am recommending an accept.

",Paper Decision
X6vMEhBtg,rkgbYyHtwB,Disagreement-Regularized Imitation Learning,Accept (Spotlight),"This paper presents an approach for interactive imitation learning while avoiding an adversarial optimization by using ensembles. The reviewers agreed that the contributions were significant and the results were compelling. Hence, the paper should be accepted.",Paper Decision
zvhfGspnu1,B1eZYkHYPS,Shifted Randomized Singular Value Decomposition,Reject,"The proposed algorithm is found to be a straightforward extension of the previous work, which is not sufficient to warrant publication in ICLR2020.",Paper Decision
O9HqIH3Gc,r1xxKJBKvr,PassNet: Learning pass probability surfaces from single-location labels. An architecture for visually-interpretable soccer analytics,Reject,"The paper proposes PassNet, which is an architecture that produces a 2D map of probability of successful completion of a soccer pass. The architecture has some similarities with UNet and has downsampling and upsampling modules with a set of skip-connections between them.

The reviewers raised several issues:
* Novelty compared to UNET
* Lack of ablation studies
* Uncertainty about what probabilities mean and issues regarding output interpretation.

The authors have tried to address these concerns in their rebuttal and provided additional experiments. They also argue that the application area (sport analytics) of the paper is novel. Even though the application area is interesting and might lead to new problems, this paper did not get enough support from reviewers to justify its acceptance.",Paper Decision
QGMBIDoC9y,H1ggKyrYwB,On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints,Reject,"The paper proposes a technique for incorporating prior knowledge as relations between training instances.

The reviewers had a mixed set of concerns, with one common one being an insufficient comparison with / discussion of related work. Some reviewers also found the clarity lacking, but were satisfied with the revision. One reviewer found the claim of the approach being general but only tested and valid for the VQA dataset problematic.

Following the discussion, I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to another venue.",Paper Decision
JZBoDYXEnV,SygeY1SYvr,Are Few-shot Learning Benchmarks Too Simple ?,Reject,"The paper is interested in assessing the difficulty of popular few-shot classification benchmarks (Omniglot and miniImageNet). A clustering-based meta-learning method is proposed (called Centroid Network), on which a metric is built (gap between the performance of Prototypical Networks and Centroid Networks). As noted by several reviewers, the proposed metric (critical for the paper) is however not motivated enough, nor convincing enough - after discussion, the logic in the metric reasoning seems to remain flawed.
",Paper Decision
J5FabpB6SM,H1lkYkrKDB,UNIVERSAL MODAL EMBEDDING OF DYNAMICS IN VIDEOS AND ITS APPLICATIONS,Reject,"The paper focuses on extracting the underlying dynamics of objects in video frames, for background/foreground extraction and video classification. Generally speaking, the presentation of the paper should be improved. Novelty should be clarified, contrasting the proposed approach with existing literature. All reviewers also agree the experimental section is also too weak in its current form.
",Paper Decision
k0yT7dKj2H,rJgCOySYwH,Function Feature Learning of Neural Networks,Reject,"This paper tackles an important problem: understanding if different NN solutions are similar or different. In the current form, however, the main motivation for the approach, and what the empirical results tell us, remains unclear. I read the paper after the updates and after reading reviews and author responses, and still had difficulty understanding the goals and outcomes of the experiments (such as what exactly is being reported as test accuracy and what is meant by: ""High test accuracy means that assumptions are reasonable.""). We highly recommend that the authors revisit the description of the motivation and approach based on comments from reviewers; further explain what is reported as test accuracy in the experiments; and more clearly highlight the insights obtain from the experiments. ",Paper Decision
WnqIRMPcn,r1eCukHYDH,Manifold Learning and Alignment with Generative Adversarial Networks,Reject,"This work proposes a GAN architecture that aims to align the latent representations of the generator with different interpretable degrees of freedom of the underlying data (e.g., size, pose).

Reviewers found this paper well-motivated and the proposed method to be technically sound. However, they cast some doubts about the novelty of the approach, specifically with respect to DMWGAN and MADGAN. The AC shares these concerns and concludes that this paper will greatly benefit from an additional reviewing cycle that addresses the remaining concerns. 
",Paper Decision
7A-2b_L_bH,ByeadyrtPB,Learning Deep-Latent Hierarchies by Stacking Wasserstein Autoencoders,Reject,"The paper received 6, 3, 1. The main criticism is the lack of quantitative evaluation/comparison. The rebuttal did not convince the last reviewer who strongly argues for a comparison. The authors are encouraged to add additional results and resubmit to a future venue.",Paper Decision
YKkJOBZnJ,rygT_JHtDr,Scalable Deep Neural Networks via Low-Rank Matrix Factorization,Reject,"The proposed paper presents low-rank compression method for DNNs. This topic has been around for a while, so the contribution is limited. Lebedev et. al paper in ICLR 2015 used CP-factorization to compress neural networks for Imagenet classification; in 2019, the idea has to be really novel in order to be presented on CIFAR datasets. The latency is not analyzed. 
So, I agree with reviewers.",Paper Decision
preNA0A6cy,rkgTdkrtPH,NoiGAN: NOISE AWARE KNOWLEDGE GRAPH EMBEDDING WITH GAN,Reject,"This paper proposes a noise-aware knowledge graph embedding (NoiGAN) by combining KG completion and noise detection through the GANs framework. The reviewers find that the idea is interesting, but the comparison to SOTA is largely missing. The paper can be improved by addressing the reviewer comments. ",Paper Decision
tF-ejoOfDt,ByxhOyHYwH,Fast Task Adaptation for Few-Shot Learning,Reject,"This paper develops a new few-shot image classification algorithm by using a metric-softmax loss for non-episodic training and a linear transformation to modify the model towards few-shot training data for task-agnostic adaptation.

Reviewers acknowledge that some of the results in the paper are impressive especially on domain sift settings as well as with a fine-tuning approach. However, they also raise very detailed and constructive concerns on the 1) lack of novelty, 2) improper claim of contribution, 3) inconsistent evaluation protocol with de facto ones in existing work. Author's rebuttal failed to convince the reviewers in regards to a majority of the critiques.

Hence I recommend rejection.",Paper Decision
e52kwC7Ozd,Bye2uJHYwr,Weighted Empirical Risk Minimization: Transfer Learning based on Importance Sampling,Reject,"This paper aims to address transfer learning by importance weighted ERM that estimates a density ratio from the given sample and some auxiliary information on the population. Several learning bounds were proven to promote the use of importance weighted ERM.

Reviewers and AC feel that the novelty of this paper is modest given the rich relevant literature and the practical use of this paper may be limited. The discussion with related theoretical work such as generalization bound of PU learning can be expanded significantly. The presentation can be largely improved, especially in the experiment part. The rebuttal is somewhat subjective and unconvincing to address the concerns.

Hence I recommend rejection.",Paper Decision
bzV3dNAsPw,Hkls_yBKDB,Neural Program Synthesis By Self-Learning,Reject,"The authors consider the problem of program induction from input-output pairs.                                                     
They propose an approach based on a combination of imitation learning from                                                         
an auto-curriculum for policy and value functions and alpha-go style tree search.                                                   
It is a applied to inducing assembly programs and compared to ablation                                                             
baselines.                                                                                                                         
                                                                                                                                   
This paper is below acceptance threshold, based on the reviews and my own                                                          
reading.                                                                                                                           
The main points of concern are a lack of novelty (the proposed approach is                                                         
similar to previously published approaches in program synthesis), missing                                                          
references to prior work and a lack of baselines for the experiments.",Paper Decision
8mLSR1giU,HyxjOyrKvr,Neural Epitome Search for Architecture-Agnostic Network Compression,Accept (Poster),"The paper proposed a novel way to compress arbitrary networks by learning epitiomes and corresponding transformations of them to reconstruct the original weight tensors. The idea is very interesting and the paper presented good experimental validations of the proposed method on state-of-the-art models and showed good MAdd reduction. The authors also put a lot of efforts addressing the concerns of all the reviewers by improving the presentation of the paper, which although can still be further improved, and adding more explanations and validations on the proposed method. Although there's still concerns on whether the reduction of MAdd really transforms to computation reduction, all the reviewers agreed the paper is interesting and useful and further development of such work would be useful too. ",Paper Decision
zYGganLx2C,SyecdJSKvr,Learning from Label Proportions with Consistency Regularization,Reject,"After reading the author's rebuttal, the reviewer still hold that the main contribution is just the simple combination of already known losses. And the paper need to pay more attention on the clarity of the paper.",Paper Decision
H-ehLtvS7,Bke9u1HFwB,Do recent advancements in model-based deep reinforcement learning really improve data efficiency?,Reject,"The paper makes broad claims, but the depth of the experiments is very limited to a narrow combination of algorithms.",Paper Decision
mIeQVxPRl_,BkeYdyHYPS,Evo-NAS: Evolutionary-Neural Hybrid Agent for Architecture Search,Reject,"Thanks to the authors for the revision and discussion. This paper provides a neural architecture search (NAS) method, called Evolutionary-Neural hybrid agents (Evo-NAS), which combines NN-based NAS and Aging EVO. While the authors' response addressed some of the reviewers' comments, during discussion period there is a new concern that the idea proposed here highly overlaps with the method of RENAS, which stands for Reinforced Evolutionary Neural Architecture Search. Reviewers acknowledge that this might discount the novelty of the paper. Overall, there is not sufficient support for acceptance.",Paper Decision
boqsHBeQww,B1xtd1HtPS,Quaternion Equivariant Capsule Networks for 3D Point Clouds,Reject,"This paper presents a capsule network to handle 3d point clouds which is equivariant to SO(3) rotations. It also provides the theoretical analysis to connect the dynamic routing approach to the Generalized Weiszfeld Iterations. The equivariant property of the method is demonstrated on classification and orientation estimation tasks of 3D shapes.
While the technical contribution of the method is sound, the main concern raised by the reviewers was the lack of details in the presentation of methodology and results. Although the authors have made substantial efforts to update the paper, some reviewers were still not convinced and thus the scores remained the same. The paper was on the very borderline, but because of the limited capacity, I regret that I have to recommend rejection. 
Invariances and equivariances are indeed important topics in representation learning, for which the capsule network is known as one of the promising approaches but still not well investigated compared to other standard architectures. I encourage authors to resubmit the paper taking in the reviewers' comments.",Paper Decision
hEZEkuBTlX,ByxduJBtPB,When Covariate-shifted Data Augmentation Increases Test Error And How to Fix It,Reject,"This paper describes situations whereby data augmentation (particularly drawn from a true distribution) can lead to increased generalization error even when the model being optimized is appropriately formulated. The authors propose ""X-regularization"" which requires that models trained on standard and augmented data produce similar predictions on unlabeled data. The paper includes a few experiments on a toy staircase regression problem as well as some ResNet experiments on CIFAR-10.  This paper received 2 recommendations for rejection, and one weak accept recommendation.  After the rebuttal phase, the author who recommended weak acceptance indicated their willingness to let the paper be rejected in light of the other reviews.  The reviewer highlighted: ""I think the authors could still to better to relate their theory to practice, and expand on the discussion/presentation of X-regularization.""  The main open issue is that the theoretical contributions of the paper are not sufficiently linked to the proposed algorithm.",Paper Decision
uKtHXmb2jX,BklDO1HYPS,Accelerated Variance Reduced Stochastic Extragradient Method for Sparse Machine Learning Problems,Reject,"This paper proposes a stochastic variance reduced extragradient algorithm. The reviewers had a number of concerns which I feel have been adequately addressed by the authors.

That being said, the field of optimizers is crowded and I could not be convinced that the proposed method would be used. In particular, (almost) hyperparameter-free methods are usually preferred (see Adam), which is not the case here.

To be honest, this work is borderline and could have gone either way but was rated lower than other borderline submissions.",Paper Decision
PT5Y5E4AQi,r1eUukrtwH,The Variational InfoMax AutoEncoder,Reject,"This paper describes a new generative model based on the information theoretic principles for better representation learning. The approach is theoretically related to the InfoVAE and beta-VAE work, and is contrasted to vanilla VAEs. The reviewers have expressed strong concerns about the novelty of this work. Some of the very closely related baselines (e.g. Zhao et al., Chen et al., Alemi et a) are not compared against, and the contributions of this work over the baselines are not clearly discussed. Furthermore, the experimental section could be made stronger with more quantitative metrics. For these reasons I recommend rejection.",Paper Decision
wteBshrWy,r1gIdySFPH,Skew-Fit: State-Covering Self-Supervised Reinforcement Learning,Reject,"This paper tackles the problem of exploration in RL. In order to maximize coverage of the state space, the authors introduce an approach where the agent attempts to reach some self-set goals. The empirically show that agents using this method uniformly visit all valid states under certain conditions. They also show that these agents are able to learn behaviours without providing a manually-defined reward function.

The drawback of this work is the combined lack of theoretical justification and limited (marginal) algorithmic novelty given other existing goal-directed techniques. Although they highlight the performance of the proposed approach, the current experiments do not convey a good enough understanding of why this approach works where other existing goal-directed techniques do not, which would be expected from a purely empirical paper. This dampers the contribution, hence I recommend to reject this paper.",Paper Decision
tDgK7z-ny8,rJeU_1SFvr,LOGAN:  Latent Optimisation for Generative Adversarial Networks,Reject,"The authors propose to overcome challenges in GAN training through latent optimization, i.e. updating the latent code, motivated by natural gradients. The authors show improvement over previous methods.  The work is well-motivated, but in my opinion, further experiments and comparisons need to be made before the work can be ready for publication.

The authors write that ""Unfortunately, SGA is expensive to scale because computing the second-order derivatives with respect to all parameters is expensive"" and further ""Crucially, latent optimization approximates SGA using only second-order derivatives with respect to the latent z and parameters of the discriminator and generator separately. The second-order terms involving parameters of both the discriminator and the generator – which are extremely expensive to compute – are not used. For latent z’s with dimensions typically used in GANs (e.g., 128–256, orders of magnitude less than the number of parameters), these can be computed efficiently. In short, latent optimization efficiently couples the gradients of the discriminator and generator, as prescribed by SGA, but using the much lower-dimensional latent source z which makes the adjustment scalable.""

However, this is not true. Computing the Hessian vector product is not that expensive. In fact, it can be computed at a cost comparable to gradient evaluations using automatic differentiation (Pearlmutter (1994)). In frameworks such as PyTorch, this can be done efficiently using double backpropagation, so only twice the cost.  Based on the above, one of the main claims of improvement over existing methods, which is furthermore not investigated experimentally, is false. 

It is unacceptable that the authors do not compare with SGA: both in terms of quality and computational cost since that is the premise of the paper. The authors also miss recent works that successfully ran methods with Hessian-vector products: https://arxiv.org/abs/1905.12103 https://arxiv.org/abs/1910.05852",Paper Decision
8sZznAaBz,ryeHuJBtPH,Hyper-SAGNN: a self-attention based graph neural network for hypergraphs,Accept (Poster),"This work introduces a new neural network model that can represent hyperedges of variable size, which is experimentally shown to improve or match the state of the art on several problems. 

Both reviewers were in favor of acceptance given the method's strong performance, and had their concerns resolved by the rebuttals and the discussion. I am therefore recommending acceptance. ",Paper Decision
nPz0qpRdgq,SJxSOJStPr,A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning,Accept (Poster),"This paper proposes an expansion-based approach for task-free continual learning, using a Bayesian nonparametric framework (a Dirichlet process mixture model).

It was well-reviewed, with reviewers agreeing that the paper is well-written, the experiments are thorough, and the results are impressive. Another positive is that the code has been released, meaning it’s likely to be reproducible.

The main concern shared among reviewers is the limited novelty of the approach, which I also share. Reviewers all mentioned that the approach itself isn’t novel, but they like the contribution of applying it to task-free continual learning. This wasn’t mentioned, but I’m concerned about the overlap between this approach and CURL (Rao et al 2019) published in NeurIPS 2019, which also deals with task-free continual learning using a generative, nonparametric approach. Could the authors comment on this in their final version?

In sum, it seems that this paper is well-done, with reproducible experiments and impressive results, but limited novelty. Given that reviewers are all satisfied with this, I’m willing to recommend acceptance. 

",Paper Decision
gUuumxFCUS,Hke4_JrYDr,Global-Local Network for Learning Depth with Very Sparse Supervision,Reject,"This paper proposes a deep network architecture for learning to predict depth from images with sparsely depth-labeled pixels. 

This paper was subject to some discussion, since the authors felt that the approach was interesting and the problem-well motivated. Some of the concerns about experimental evaluation (especially from R1) were resolved due the author's rebuttal, but ultimately the reviewers felt the paper was not yet ready for publication. ",Paper Decision
7OztopXgxO,SygEukHYvB,CEB Improves Model Robustness,Reject,"This paper proposes CEB, Conditional Entropy Bottleneck, as a way to improves the robustness of a model against adversarial attacks and noisy data. The model is tested empirically using several experiments and various datasets.

We appreciate the authors for submitting the paper to ICLR and providing detailed responses to the reviewers' comments and concerns. After the initial reviews and rebuttal, we had extensive discussions to judge whether the contributions are clear and sufficient for publication. In particular, we discussed the overlap with a previous (arXiv) paper and decided that the overlap should not be considered because it is not published at a conference or journal. Plus the paper makes additional contributions.

However, reviewers in the end did not think the paper showed sufficient explanation and proof of why and how this model works, and whether this approach improves upon other state-of-the-art adversarial defense approaches.

Again, thank you for submitting to ICLR, and I hope to see an improved version in a future publication.",Paper Decision
fzDEFA7Dx,HJx7uJStPH,Music Source Separation in the Waveform Domain,Reject,"The paper proposed a waveform-to-waveform music source separation system. Experimental justification shows the proposed model achieved the best SDR among all the existing waveform-to-waveform models, and obtained similar performance to spectrogram based ones. The paper is clearly written and the experimental evaluation and ablation study are thorough. But the main concern is the limited novelty, it is an improvement over the existing Wave-U-Net, it added some changes to the existing model architecture for better modeling the waveform data and compared masking vs. synthesis for music source separation.  ",Paper Decision
FBYDmXnMSN,Hkx7_1rKwS,On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach,Accept (Poster),"The submission proposes a novel solution for minimax optimization which has strong theoretical and empirical results as well as broad relevance for the community. The approach, Follow-the-Ridge, has theoretical guarantees and is compatible with preconditioning and momentum optimization strategies.

The paper is well-written and the authors engaged in a lengthy discussion with the reviewers, leading to a clearer understanding of the paper for all. The reviews all recommend acceptance. ",Paper Decision
Yw6Tpflcm,ryxGuJrFvS,Distributionally Robust Neural Networks,Accept (Poster),"This paper proposes distributionally robust optimization (DRO) to learn robust models that minimize worst-case training loss over a set of pre-defined groups. They find that increased regularization is necessary for worst-group performance in the overparametrized regime (something that is not needed for non-robust average performance).

This is an interesting paper and I recommend acceptance. The discussion phase suggested a change in the title which slightly overstated the paper's contributions (a comment which I agree with). The authors agreed to change the title in the final version. 
",Paper Decision
sUqMNEuHlP,B1eWOJHKvB,Kernel of CycleGAN as a principal homogeneous space,Accept (Poster),"This paper theoretically studied one of the fundamental issue in CycleGAN (recently gained much attention for image-to-image translation). The authors analyze the space of exact and approximated solutions under automorphisms.

Reviewers mostly agree with theoretical value of the paper. Some concerns on practical values are also raised, e.g., limited or no-surprising experimental results. In overall, I think this is a boarderline paper. But, I am a bit toward acceptance as the theoretical contribution is solid, and potentially beneficial to many future works on unpaired image-to-image translation.

",Paper Decision
LF2Cb7DMZq,B1eyO1BFPr,"Don't Use Large Mini-batches, Use Local SGD",Accept (Poster),"The authors propose a simple modification of local SGD for parallel training, starting with standard SGD and then switching to local SGD. The resulting method provides good results and makes a practical contribution. Please carefully account for reviewer comments in future revisions.",Paper Decision
jOnPym6Ro9,rklk_ySYPB,Provable robustness against all adversarial $l_p$-perturbations for $p\geq 1$,Accept (Poster),"This paper extends the degree to which ReLU networks can be provably resistant to a broader class of adversarial attacks using a MMR-Universal regularization scheme.  In particular, the first provably robust model in terms of lp norm perturbations is developed, where robustness holds with respect to *any* p greater than or equal to one (as opposed to prior work that may only apply to specific lp-norm perturbations).

While I support accepting this paper based on the strong reviews and significant technical contribution, one potential drawback is the lack of empirical tests with a broader cohort of representative CNN architectures (as pointed out by R1).  In this regard, the rebuttal promises that additional experiments with larger models will be added in the future to the final version, but obviously such results cannot be used to evaluate performance at this time.",Paper Decision
6-j3TrjYFk,S1xCPJHtDB,Model Based Reinforcement Learning for Atari,Accept (Spotlight),"This paper presents a model-based RL approach to Atari games based on video prediction. The architecture performs remarkably well with a limited amount of interactions.  This is a very significant result on a question that engages many in the research community.

Reviewers all agree that the paper is good and should be published. There is some disagreement about the novelty of it. However, as one reviewer states, the significance of the results is more important than the novelty. Many conference attendees would like to hear about it.

Based on this, I think the paper can be accepted for oral presentation.",Paper Decision
Q0bcibzWTB,BkgCv1HYvB,Generating Multi-Sentence Abstractive Summaries of Interleaved Texts,Reject,"This paper proposes an end-to-end approach for abstractive summarization of on-line discussions. The approach is contrary to the previous work that first disentangles discussions, and the summarizes them, and aims to tackle transfer of disentanglement errors in the pipeline. The proposed method is a hierarchical encoder - hierarchical decoder architecture. Experimental results on two corpora demonstrate the benefits of the proposed approach. The reviewers are concerned about the synthetic nature of the datasets, limited novelty given the previous work, lack of clear explanation of whether disentanglement is actually needed for summarization, and simpler baselines in comparison to the state-of-the-art. Hence, I recommend rejecting the paper.",Paper Decision
fXxg3D9dK,HkxTwkrKDB,On Universal Equivariant Set Networks,Accept (Poster),"This paper shows that DeepSets and PointNet, which are known to be universal for approximating functions, are also universal for approximating equivariant set functions. Reviewer are in agreement that this paper is interesting and makes important contributions. However, they feel the paper could be written to be more accessible.

Based on the reviews and discussions following author response, I recommend accepting this paper. I appreciate the authors for an interesting paper and look forward to seeing it at the conference.",Paper Decision
rXDkzMNLua,S1gTwJSKvr,OPTIMAL BINARY QUANTIZATION FOR DEEP NEURAL NETWORKS,Reject,"This paper proposes to quantize the weights of neural networks that can minimize the L_2 loss between the quantized values and the full-precision ones. The paper has limited novelty, as many of the solutions presented in the paper have already been discovered in the literature. During the discussion, the reviewers agree that it is an incremental contribution. Parts of the paper can also be clarified, particularly on the optimality of the solution, assumptions used in the approximation, and some of the experimental results. Experimental results can also be made more convincing by adding comparision with the more recent quantization methods.",Paper Decision
5bgvZ4BKJ,Bye3P1BYwr,Deep End-to-end Unsupervised Anomaly Detection ,Reject,"The authors propose an approach for anomaly detection in the setting where the training data includes both normal and anomalous data.  Their approach is a fairly straightforward extension of existing ideas, in which they iterate between clustering the data into normal vs. anomalous and learning an autoencoder representation of normal data that is then used to score normality of new data.  The results are promising, but the experiments are fairly limited.  The authors argue that their experimental settings follow those of prior work, but I think that for such an incremental contribution, more empirical work should be done, regardless of the limitations of particular prior work.",Paper Decision
SW4GeNFjG_,rke2P1BFwS,Tensor Decompositions for Temporal Knowledge Base Completion,Accept (Poster),"The authors propose a new algorithm based on tensor decompositions for the problem of knowledge base completion. They also introduce new regularisers to augment their method. They also propose an new dataset for temporal KB completion.  

All the reviews agreed that the paper addresses an important problem and presents interesting results. The authors diligently responded to reviewer queries and addressed most of the concerns raised by the reviewers. 

Since all the reviewers are in agreement, I recommend that this paper be accepted. ",Paper Decision
UQM2-EKkyu,BJlowyHYPr,CloudLSTM: A Recurrent Neural Model for Spatiotemporal Point-cloud Stream Forecasting,Reject,"The paper presents an approach to forecasting over temporal streams of permutation-invariant data such as point clouds. The approach is based on an operator (DConv) that is related to continuous convolution operators such as X-Conv and others. The reviews are split. After the authors' responses, concerns remain and two ratings remain ""3"". The AC agrees with the concerns and recommends against accepting the paper.",Paper Decision
HbBlKsMRoZ,SkloDJSFPH,Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling,Reject,"The paper presents a technique for approximately sampling from autoregressive models using something like a a proposal distribution and a critic. The idea is to chunk the output into blocks and, for each block, predict each element in the block independently from a proposal network, ask a critic network whether the block looks sensible and, if not, resampling the block using the autoregressive model itself. 
The idea in the paper is interesting, but the paper would benefit from
- a better relation to existing methods
- a better experimental section, which details the hyper-parameters of the algorithm (and how they were chosen) and which provides error bars on all plots (and tables)",Paper Decision
HRXoeWfG6B,HJgcvJBFvB,Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning,Accept (Poster),"This submission proposes an RL method for learning policies that generalize better in novel visual environments. The authors propose to introduce some noise in the feature space rather than in the input space as is typically done for visual inputs. They also propose an alignment loss term to enforce invariance to the random perturbation.

Reviewers agreed that the experimental results were extensive and that the proposed method is novel and works well.

One reviewer felt that the experiments didn’t sufficiently demonstrate invariance to additional potential domain shifts. AC believes that additional experiments to probe this would indeed be interesting but that the demonstrated improvements when compared to existing image perturbation methods and existing regularization methods is sufficient experimental justification of the usefulness of the approach.

Two reviewers felt that the method should be more extensively compared to “data augmentation” methods for computer vision tasks. AC believes that the proposed method is not only a data augmentation method given that the added loss tries to enforce representation invariance to perturbations as well. As such comparisons to feature adaptation techniques to tackle domain shift would be appropriate but it is reasonable to consider this line of comparison beyond the scope of this particular work.

Ac agrees with the majority opinion that the submission should be accepted.",Paper Decision
kxgDTmQY3,HyeqPJHYvH,Stochastic Latent Residual Video Prediction,Reject,"The paper proposes a method for learning a latent dynamics model for videos. The main idea is to learn a latent representation and model the dynamics of the latent features via residual connection motivated by ODE. The architectural choice of residual connection itself is not new as many prior works have employed ""skip connections"" in hidden representations but the notion of connecting this with ODE and factoring time as input into the residual function seems a new idea. The experimental results show the promise of the proposed method on moving MNIST, KTH, and BAIR datasets. The experiments on different frame rates are also nice.  In terms of weakness, the evaluation is performed on relatively simple domains (e.g., moving MNIST and KTH) with static backgrounds and the improvement on BAIR dataset (which is not considered as a difficult benchmark) in terms of FVD is not as clear. For the BAIR dataset, it's unclear how the proposed method will handle the interactions between the robot arm and background objects due to the modeling assumption (i.e., static background). In this sense, content swap results on BAIR dataset look quite anecdotal, and the significance is limited. For improvement, I would suggest adding evaluations on other challenging domains, such as Human 3.6M (where human motions are much more uncertain compared to KTH) and other Robot datasets with more complex robot-object interactions. Overall, the paper proposes an interesting architecture with promising results on relatively simple datasets, but the advantage over existing SOTA methods on challenging benchmarks is unclear yet.
",Paper Decision
gxAihoNTWM,H1gcw1HYPr,AlignNet: Self-supervised Alignment Module,Reject,"This paper proposes a network architecture which labels object with an identifier that it is trained to retain across subsequent instances of that same object.

After discussion, the reviewers agree that the approach is interesting, well-motivated and written, and novel. However, there was unanimous concern about the experimental evaluation, so the paper does not appear to be ready for publication just yet, and I am recommending rejection.",Paper Decision
wudeaXlvBi,HylKvyHYwS,Learning with Protection: Rejection of Suspicious Samples under Adversarial Environment,Reject,"The paper addresses the setting of learning with rejection while incorporating the ideas from learning with adversarial examples to tackle adversarial attacks. While the reviewers acknowledged the importance to study learning with rejection in this setting, they raised several concerns: (1) lack of technical contribution -- see R1’s and R2’s related references, see R3’s suggestion on designing c(x); (2) insufficient empirical evidence -- see R3’s comment about the sensitivity experiment on the strength of the attack, see R1’s suggestion to compare with a baseline that learns the rejection function such as SelectiveNet;  (3) clarity of presentation -- see R2’s suggestions how to improve clarity.
Among these, (3) did not have a substantial impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed approach, and were viewed by AC as critical issues.
AC can confirm that all three reviewers have read the author responses and have revised the final ratings. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.
",Paper Decision
YYopxreh3o,rkxKwJrKPS,QXplore: Q-Learning Exploration by Maximizing Temporal Difference Error,Reject,"There is insufficient support to recommend accepting this paper.  Although the authors provided detailed responses, none of the reviewers changed their recommendation from reject.  One of the main criticisms, even after revision, concerned the quality of the experimental evaluation.  The reviewers criticized the lack of important baselines, and remained unsure about adequate hyperparameter tuning in the revision.  The technical exposition lacked a sober discussion of limitations.  The paper would be greatly strengthened by the addition of a theoretical justification of the proposed approach.  In the end, the submitted reviews should be able to help the authors strengthen this paper.",Paper Decision
Dx2jD7gwx1,ryguP1BFwr,Walking the Tightrope: An Investigation of the Convolutional Autoencoder Bottleneck,Reject,"The paper investigates the effect of convolutional information bottlenecks to generalization. The paper concludes that the width and height of the bottleneck can greatly influence generalization, whereas the number of channels has smaller effect. The paper also shows evidence against a common belief that CAEs with sufficiently large bottleneck will learn an identity map. 

During the rebuttal period, there was a long discussion mainly about the sufficiency of the experimental setup and the trustworthiness of the claims made in the paper. A paper that empirically investigates an exiting method or belief should include extensive experiments of high quality in to enable general conclusions. I’m thus recommending rejection, but encourage the authors to improve the experiments and resubmitting.",Paper Decision
y8Yz3g8YNc,SJe_D1SYvr,Partial Simulation for Imitation Learning,Reject,"The paper introduces the concept of an Expert Induced MDP (eMDP) to address imitation learning settings where environment dynamics are part known / part unknown. Based on the formulation a model-based imitation learning approach is derived and the authors obtain theoretical guarantees. Empirical validation focuses on comparison to behavior cloning. Reviewers raised concerns about the size of the contribution. For example, it is unclear to what degree the assumptions made here would hold in practical settings.",Paper Decision
c8lmtADCE,B1xwv1StvS,Few-shot Learning by Focusing on Differences,Reject,"Main content:

[Blind review #3] The authors propose a metric based model for few-shot learning. The goal of the proposed technique is to incorporate a prior that highlight better the dissimilarity between closely related class prototype. Thus, the proposed paper is related to prototypical neural network (use of prototype to represent a class) but differ from it by using inner product scoring  as a similarity measure instead of the use of euclidean distance. There is also close similarity between the proposed method and matching network.

[Blind review #2] The stated contributions of the paper are: (1) a method for performing few-shot learning and (2) an approach for building harder few-shot learning datasets from existing datasets. The authors describe a model for creating a task-aware embedding for different novel sets (for different image classification settings) using a nonlinear self-attention-like mechanism applied to the centroid of the global embeddings for each class. The resulting embeddings are used per class with an additional attention layer applied on the embeddings from the other classes to identify closely-related classes and consider the part of the embedding orthogonal to the attention-weighted-average of these closely-related classes. They compare the accuracy of their model vs others in the 1-shot and 5-shot setting on various datasets, including a derived dataset from CIFAR which they call Hierarchical-CIFAR.

--

Discussion:

All reviews agree on a weak reject.

--

Recommendation and justification:

While the ideas appear to be on a good track, the paper itself is poorly written - as one review put it, more like notes to themselves, rather than a well-written document to the ICLR audience.",Paper Decision
k2F6zynkK6,BJxwPJHFwS,Robustness Verification for Transformers,Accept (Poster),"A robustness verification method for transformers is presented. While robustness verification has previously been attempted for other types of neural networks, this is the first method for transformers. 

Reviewers are generally happy with the work done, but there were complaints about not comparing with and citing previous work, and only analyzing a simple one-layer version of transformers. The authors convincingly respond to these complaints.

I think that the paper can be accepted, given that the reviewers' complaints have been addressed and the paper seems to be sufficiently novel and have practical importance for understanding transformers.",Paper Decision
8EatWJ_2_U,SJgIPJBFvH,Fantastic Generalization Measures and Where to Find Them,Accept (Poster),"This paper provides a valuable survey, summary, and empirical comparison of many generalization quantities from throughout the literature. It is comprehensive, thorough, and will be useful to a variety of researchers (both theoretical and applied).",Paper Decision
G4WBcHLfr7,SJlHwkBYDH,Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks,Accept (Poster),"Under the optimization formulation of adversarial attack, this paper proposes two methods to improve the transferability of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). NI-FGSM adapts Nesterov accelerated gradient into the iterative attacks to effectively look ahead and avoid the “missing” of the global maximum, and SIM optimizes the adversarial perturbations over the scale copies of the input images so as to avoid “overfitting” on the white-box model being attacked and generate more transferable adversarial examples. Empirical results demonstrate the effectiveness of the proposed methods. The ideas are sensible, and the empirical studies were strengthened during rebuttal.",Paper Decision
xZyM4u9x8d,SJeHwJSYvH,Learning De-biased Representations with Biased Representations,Reject,"This paper provides and analyzes an interesting approach to ""de-biasing"" a predictor from its training set.  The work is valuable, however unfortunately just below the borderline for this year.  I urge the authors to continue their investigations, for instance further addressing the reviewer comments below (some of which are marked as coming after the end of the feedback period).",Paper Decision
3ooTOTTjsE,HJgSwyBKvr,Weakly Supervised Disentanglement with Guarantees,Accept (Poster),"This paper first discusses some concepts related to disentanglement. The authors propose to decompose disentanglement into two distinct concepts: consistency and restrictiveness. Then, a calculus of disentanglement is introduced to reveal the relationship between restrictiveness and consistency. The proposed concepts are applied to analyze weak supervision methods. 

The reviewers ultimately decided this paper is well-written and has content which is of general interest to the ICLR community.",Paper Decision
qgQCJN2MA,BJe4PyrFvB,Imagining the Latent Space of a Variational Auto-Encoders,Reject,"The paper proposes a new method for improving generative properties of VAE model.  The reviewers unanimously agree that this paper is not ready to be published, particularly being concerned about the unclear objective and potentially misleading claims of the paper. Multiple reviewers pointed out about incorrect claims and statements without theoretical or empirical justification. The reviewers also mention that the paper does not provide new insights about VAE model as MDL interpretation of VAE it is not new.",Paper Decision
_4hYzqt6tc,ryx4PJrtvS,A Copula approach for hyperparameter transfer learning,Reject,"This paper tackles the problem of transferring learning between tasks when performing Bayesian hyperparameter optimization. In this setting, tasks can correspond to different datasets or different metrics. The proposed approach uses Gaussian copulas to synchronize the different scales of the considered tasks and uses Thompson Sampling from the resulting Gaussian Copula Process for selecting next hyperparameters.

The main weakness of the paper resides in the concerns raised about the experiments. First, the results are hard to interpret, leading to a misunderstanding of performances. Moreover, the considered baselines may not be adapted (they may be trivial). This might be due to a misunderstanding of the paper, which would align with the third major concern, that is the lack of clarity. These points could be addressed in a future version of the work, but it would need to be reviewed again and therefore would be too late for the current camera-ready.

Hence, I recommend rejecting this paper.",Paper Decision
N9d8kA54k,B1eXvyHKwS,THE EFFECT OF ADVERSARIAL TRAINING: A THEORETICAL CHARACTERIZATION,Reject,"This paper studies adversarial training in the linear classification setting, and shows a rate of convergence for adversarial training of o(1/log T) to the hard margin SVM solution under a set of assumptions. 

While 2 reviewers agree that the problem and the central result is somewhat interesting (though R3 is uncertain of the applicability to deep learning, I agree that useful insights can often be gleaned from studying the linear case), reviewers were critical of the degree of clarity and rigour in the writing, including notation, symbol reuse, repetitions/redundancies, and clarity surrounding the assumptions made.

No updates to the paper were made and reviewers did not feel their concerns were addressed by the rebuttals. I therefore recommend rejection, but would encourage the authors to continue refining their paper in order to showcase their results more clearly and didactically.",Paper Decision
KKPWxNnVBQ,S1gmvyHFDS,Provenance detection through learning transformation-resilient watermarking,Reject,"This paper offers an interesting and potentially useful approach to robust watermarking.  The reviewers are divided on the significance of the method.  The most senior and experienced reviewer was the most negative.  On balance, my assessment of this paper is borderline; given the number of more highly ranked papers in my pile, that means I have to assign ""reject"".",Paper Decision
teAI1SstlZ,SJefPkSFPr,Regulatory Focus: Promotion and Prevention Inclinations in Policy Search,Reject,"The authors take inspiration from regulatory fit theory and propose a new parameter for policy gradient algorithms in RL that can manage the ""regulatory focus"" of an agent.  They hypothesize that this can affect performance in a problem-specific way, especially when trading off between broad exploration and risk.  The reviewers expressed concerns about the usefulness of the proposed algorithm in practice and a lack of thorough empirical comparisons or theoretical results.  Unfortunately, the authors did not provide a rebuttal, so no further discussion of these issues was possible; thus, I recommend to reject.",Paper Decision
ypqPVEMrjG,BkeGPJrtwB,Fairness with Wasserstein Adversarial Networks,Reject,"This paper presents an approach to enforce statistical fairness notions using adversarial networks. The reviewers point out several issues of the paper, including 1) their approach does not provably enforce criteria such as demographic parity, 2) lack of novelty and 3) poor presentation.",Paper Decision
EGb5NlF92,SkezP1HYvS,Diagonal Graph Convolutional Networks with Adaptive Neighborhood Aggregation,Reject,All three reviewers are consistently negative on this paper. Thus a reject is recommended.,Paper Decision
OOXxnJSnER,Byg-wJSYDS,Discrepancy Ratio: Evaluating Model Performance When Even Experts Disagree on the Truth,Accept (Poster),"This paper tackles an interesting problem: ""How should we evaluate models when the test data contains noisy labels?"". This is a particularly relevant question in the medical imaging domain where expert annotators often disagree with each other. The paper proposes a new metric ""discrepancy ratio"" which computes the ratio how often the model disagrees with humans to how often humans disagree with each other. The paper shows that under certain noise models for the human annotations the discrepancy ratio can exactly determine when a model is more accurate than humans, whereas commonly used baselines such as comparing with the majority vote do not have this property. Reviewers were satisfied with the author rebuttal, particularly with the clarification that the goal of the metric is to accurately determine when model performance exceeds that of human annotators, and not to better rank models. The metric should be quite useful, assuming users are cautious of the limitations described by the authors.",Paper Decision
wArIlIZqV,B1xZD1rtPr,The Dual Information Bottleneck,Reject,"Main content:

Blind review #1 summarizes it well:

This paper introduces a variant of the Information Bottleneck (IB) framework, which consists in permuting the conditional probabilities of y given x and y given \hat{x} in a Kullback-Liebler divergence involved in the IB optimization criterion.

Interestingly, this change only results in changing an arithmetic mean into a geometric mean in the algorithmic resolution.

Good properties of the exponential families (existence of non-trivial minimal sufficient statistics) are preserved, and an analysis of the new critical points/information plane induced is carried out.

--

Discussion:

The reviews generally agree on the elegant mathematical result, but are critical of the fact that the paper lacks any empirical component whatsoever.

--

Recommendation and justification:

The paper would be good for ICLR if it had any decent empirical component at all; it is a shame that none was presented as this does not seem very difficult.",Paper Decision
dkKrN3OvEp,Hkexw1BtDr,Deep Auto-Deferring Policy for Combinatorial Optimization,Reject,"This paper proposes a new way to formulate the design of the deep reinforcement learning that automatically shrinks or expands decision processes.

The paper is borderline and all reviewers appreciate the paper and gives thorough reviews. However, it not completely convince that it is ready publication. 

Rejection is recommended. This can become a nice paper for next conference by taking feedback into account. ",Paper Decision
NMzDAf6POZ,rygePJHYPH,Towards trustworthy predictions from deep neural networks with fast adversarial calibration,Reject,"This paper proposes an algorithm to produce well-calibrated uncertainty estimates. The work accomplishes this by introducing two loss terms: entropy-encouraging loss and an adversarial calibration loss to encourage predictive smoothness in response to adversarial input perturbations. 

All reviewers recommended weak reject for this work with a major issue being the presentation of the work. Each reviewer provided specific examples of areas in which the paper text, figures, equations etc were unclear or missing details. Though the authors have put significant effort into responding to the specific reviewer mentions, the reviewers have determined that the manuscript would benefit from further revision for clarity. 

Therefore, we do not recommend acceptance of this work at this time and instead encourage the authors to further iterate on the manuscript and consider resubmission to a future venue. 
",Paper Decision
7I6ohZKYQl,Byg1v1HKDB,Abductive Commonsense Reasoning,Accept (Poster),"This paper presents a dataset, created using a combination of existing resources, crowdsourcing, and model-based filtering, that aims to tests models' understanding of typical progressions of events in everyday situations. The dataset represents a challenge for a range of state of the art models for NLP and commonsense reasoning, and also can be used productively as a training task in transfer learning.

After some discussion, reviewers came to a consensus that this represents an interesting contribution and a potentially valuable resource. There were some concerns—not fully resolved—about the implications of using model-based filtering during data creation, but these were not so serious as to invalidate the primary contributions of the paper.

While the thematic fit with ICLR is a bit weak—the primary contribution of the paper appears to be a dataset and task definition, rather than anything specific to representation learning—there are relevant secondary contributions, and I think that this work will be practically of interest to a reasonable fraction of the ICLR audience. ",Paper Decision
RMDYTxVXKY,Syx1DkSYwB,Variance Reduction With Sparse Gradients,Accept (Poster),Congratulations on getting your paper accepted to ICLR. Please make sure to incorporate the reviewers' suggestions for the final version.,Paper Decision
tumHqHLIlx,SklkDkSFPB,BlockSwap: Fisher-guided Block Substitution for Network Compression on a Budget,Accept (Poster),"Two reviewers recommend acceptance. One reviewer is negative, however, does not provide reasons for rejection. The AC read the paper and agrees with the positive reviewers. in that the paper provides value for the community on an important topic of network compression.",Paper Decision
ts9yuuqh4,S1eALyrYDH,RNA Secondary Structure Prediction By Learning Unrolled Algorithms,Accept (Talk),"This paper proposes a RNA structure prediction algorithm based on an unrolled inference algorithm. The proposed approach overcomes limitations of previous methods, such as dynamic programming (which does not work for molecular configurations that do not factorize), or energy-based models (which require a minimization step, e.g. by using MCMC to traverse the energy landscape and find minima).

Reviewers agreed that the method presented here is novel on this application domain, has excellent empirical evaluation setup with strong numerical results, and has the potential to be of interest to the wider deep learning community. The AC shares these views and recommends an enthusiastic acceptance. ",Paper Decision
3QRlH-tVV_,SJlRUkrFPS,Learning transport cost from subset correspondence,Accept (Poster),"The paper proposes an algorithm for learning a transport cost function that accurately captures how two datasets are related by leveraging side information such as a subset of correctly labeled points. The reviewers believe that this is an interesting and novel idea. There were several questions and comments, which the authors adequately addressed. I recommend that the paper be accepted.",Paper Decision
-5sy4JS1Q,B1xpI1BFDS,Semi-Supervised Few-Shot Learning with a Controlled Degree of Task-Adaptive Conditioning,Reject,"This paper proposes an approach to semi-supervised few-shot learning. In a discussion after the rebuttal phase, the reviewers were somewhat split on this paper, appreciating the advantages of the algorithm such as increased robustness to distractors and the ability to adapt with additional iterations, but were concerned that the contributions over Ren et al were not significant. Overall, the contributions of this paper don't quite warrant publication at ICLR.",Paper Decision
yDQYOxjhoR,HyenUkrtDB,Detecting Noisy Training Data with Loss Curves,Reject,"The paper proposes a new, stable metric, called Area Under Loss curve (AUL) to recognize mislabeled samples in a dataset due to the different behavior of their loss function over time. The paper build on earlier observations (e.g. by Shen & Sanghavi) to propose this new metric as a concrete solution to the mislabeling problem. 

Although the reviewers remarked that this is an interesting approach for a relevant problems, they expressed several concerns regarding this paper. Two of them are whether the hardness of a sample would also result in high AUL scores, and another whether the results hold up under realistic mislabelings, rather than artificial label swapping / replacing. The authors did anecdotally suggest that neither of these effects has a major impact on the results. Still, I think a precise analysis of these effects would be critically important to have in the paper. Especially since there might be a complex interaction between the 'hardness' of samples and mislabelings (an MNIST 1 that looks like a 7 might be sooner mislabeled than a 1 that doesn't look like a 7). The authors show some examples of 'real' mislabeled sentences recognized by the model but it is still unclear whether downweighting these helped final test set performance in this case. 

Because of these issues, I cannot recommend acceptance of the paper in its current state. However, based on the identified relevance of the problem tackled and the identified potential for significant impact I do think this could be a great paper in a next iteration. ",Paper Decision
SOz567xbqI,Syl38yrFwr,Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles,Reject,"This paper presents a differentially private mechanism, called Noisy ArgMax, for privately aggregating predictions from several teacher models. There is a consensus in the discussion that the technique of adding a large constant to the largest vote breaks differential privacy. Given this technical flaw, the paper cannot be accepted.",Paper Decision
lQWvXUknDH,ryxsUySFwr,Neural Network Out-of-Distribution Detection for Regression Tasks,Reject,"The paper investigates out-of-distribution detection for regression tasks.

The reviewers raised several concerns about novelty of the method relative to existing methods, motivation & theoretical justification and clarity of the presentation  (in particular, the discussion around regression vs classification). 

I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue.
",Paper Decision
ss-RU_lVAM,HkgsUJrtDB,Rényi Fair Inference,Accept (Poster),"The paper addresses the problem of fair representation learning. The authors propose to use Rényi correlation as a measure of (in)dependence between the predictor and the sensitive attribute and developed a general training framework to impose fairness with theoretical properties. The empirical evaluations have been performed using standard benchmarks for fairness methods and the SOTA baselines -- all this supports the main claims of this work's contributions. 
All the reviewers and AC agree that this work has made a valuable contribution and recommend acceptance. Congratulations to the authors! 
",Paper Decision
Wis-BzcTr,rkg98yBFDr,Reject Illegal Inputs: Scaling Generative Classifiers with Supervised Deep Infomax,Reject,"This paper combines a well-known, recently proposed unsupervised representation learning technique technique with a class-conditional negative log likelihood and a squared hinge loss on the class-wise conditional likelihoods, and proposes to use the resulting conditional density model for generative classification. The empirical work appears to validate the claim that their method leads to good out of distribution detection, and better performance using a rejection option. The adversarial defense results are less clear. Reporting raw logits is a strange choice, and difficult to interpret; the table is also difficult to read, and this method of reporting makes it difficult to compare against existing methods.

The reviewers generally remarked on presentation issues. R1 asked about the contribution of various loss terms, a matter I feel is underexplored in this work, and the authors mainly replied with a qualitative description of loss behaviour in the joint system, which I don't believe was the question. R1 also asked about the choice of thresholds and the issues of fairness of comparison regarding model capacity, neither of which seemed adequately addressed. R3 remarked on the clarity being lacking, and also that ""Generative modeling of representations is novel, afaik."" (It is not; see, for example, the VQ-VAE line of work where PixelCNN priors are fit on top of representations, and layer-wise pre-training works of the mid 2000s, where generative models were frequently fit on greedily trained feature representations, sometimes in conjunction with a joint generative model of class labels).  R2's review was very brief, and with a self-reported low confidence, but their concerns were addressed in a subsequent update.

There are three weaknesses which are my grounds for recommending rejection. First, this paper does a poor job of situating itself in the wider body of literature on classification with rejection, which dates to at least the 1970s (see Bartlett & Wengkamp, 2006 and the references therein). Second, the empirical work makes little comparison to other methods in the literature; baselines on clean data are self-generated, and the paper compares to no other adversarial defense proposals. In a minor drawback, ImageNet results are also missing; given that one of the purported advantages of the method is scalability, a large scale benchmark would have strengthened this claim. Third, no ablation study is undertaken that might give us insight into the role of each term of the loss. Given that this is a straightforward combination of well-understood techniques, a fully empirical paper ought to deliver more insight into the combination than this manuscript has.",Paper Decision
ky2WhDr-Xz,B1xcLJrYwH,Lean Images for Geo-Localization,Reject,"The submission studies the problem of geolocalizing a city based on geometric information encoded in so called ""lean"" images.  The reviewers were unanimous in their opinion that the submission does not meet the threshold for publication at ICLR.  Concerns included quality of writing, novelty with respect to existing literature (in particular see Review #2), and limited validation on one geographic area.  No rebuttal was provided.",Paper Decision
zNhs71lX3,rkeYL1SFvH,WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia,Reject,"The authors present an approach to large scale bitext extraction from Wikipedia. This builds heavily on previous work, with the novelty being somewhat minor efficient approximate K-nearest neighbor search and language agnostic parameters such as cutoffs. These techniques have not been validated on other data sets and it is unclear how well they generalise. The major contribution of the paper is the corpus created, consisting of 85 languages, 1620 language pairs and 135M parallel sentences, of which most do not include English. This corpus is very valuable and already in use in the field, but IMO ICLR is not the right venue for this kind of publication. There were four reviews, all broadly in agreement, and some discussion with the authors. 
",Paper Decision
F0tWpnIGyD,SkeYUkStPr,Deep Lifetime Clustering,Reject,"The authors propose a clustering algorithm for users in a system based on their lifetime distribution. The reviewers acknowledge the novelty of the proposed clustering algorithm, but one concern left unresolved is how the results of the analysis can be of use in the real world examples used. ",Paper Decision
OXspbi9VZF,BylKL1SKvr,Towards Understanding the Transferability of Deep Representations,Reject,"This paper studies the transfer of representations learned by deep neural networks across various datasets and tasks when the network is pre-trained on some dataset and subsequently fine-tuned on the target dataset. The authors theoretically analyse two-layer fully connected networks and provide an extensive empirical evaluation arguing that the loss landscape of  appropriately pre-trained networks is easier to optimise (improved Lipschitzness). 
Understanding the transferability of representations is an important problem and the reviewers appreciated some aspects of the extensive empirical evaluation and the initial theoretical investigation. However, we feel that the manuscript needs a major revision and that there is not enough empirical evidence to support the stated conclusions. As a result, I will recommend rejecting this paper in the current form. 
Nevertheless, as the problem is extremely important I encourage the authors to improve the clarity and provide more convincing arguments towards the stated conclusions by addressing the issues raised during the discussion phase.",Paper Decision
N6YZNAvJX,BJgd81SYwr,Meta Dropout: Learning to Perturb Latent Features for Generalization,Accept (Poster),"This paper proposes a type of adaptive dropout to regularize gradient based meta-learning models. The reviewers found the idea interesting and it is supported by improvements on standard benchmarks. The authors addressed several concerns of the reviewers during the rebutal phase. In particular, revisions added results against other regularization mthods. We recommend that further attention is given to ablations, in particular the baseline proposed by Reviewer 1.",Paper Decision
A0pUgJ715c,ByxdUySKvS,Adversarial AutoAugment,Accept (Poster),"This paper proposes a method to learn data augmentation policies using an adversarial loss. In contrast to AutoAugment where an augmentation policy generator is trained by RL (computationally expensive), the authors propose to train a policy generator and the target classifier simultaneously. This is done in an adversarial fashion by computing augmentation policies which increase the loss of the classifier. The authors show that this approach leads to roughly an order of magnitude improvement in computational cost over AutoAugment, while improving the test performance.
The reviewers agree that the presentation is clear and that the proposed method is sound, and that there is a significant practical benefit of using such a technique. As most of the concerns were addressed in the discussion phase, I will recommend acceptance of this paper. We ask the authors to update the manuscript to address the remaining (minor) concerns.
",Paper Decision
4WZsUiW01,HyxPIyrFvH,When Robustness Doesn’t Promote Robustness: Synthetic vs. Natural Distribution Shifts on ImageNet,Reject,"The authors show that models trained to satisfy adversarial robustness properties do not possess robustness to naturally occuring distribution shifts. The majority of the reviewers agree that this is not a surprising result especially for the choice of natural distribution shifts chosen by the authors (for instance it would be better if the authors compare to natural distribution shifts that look similar to the adversarial corruptions). Moreover, this is a survey study and no novel algorithms are presented, so the paper cannot be accepted on that merit either.",Paper Decision
DfU1kBE-ej,HyevIJStwH,Understanding Why Neural Networks Generalize Well Through GSNR of Parameters,Accept (Spotlight),"Quoting a reviewer for a very nice summary:

""In this work, the authors suggest a new point of view on generalization through the lens of the distribution of the per-sample gradients. The authors consider the variance and mean of the per-sample gradients for each parameter of the model and define for each parameter the Gradient Signal to Noise ratio (GSNR). The GSNR of a parameter is the ratio between the mean squared of the gradient per parameter per sample (computed over the samples) and the variance of the gradient per parameter per sample (also computed over the samples). The GSNR is promising as a measure of generalization and the authors provide a nice leading order derivation of the GSNR as a proxy for the measure of the generalization gap in the model.""

The majority of the reviewers vote to accept this paper. We can view the 3 as a weak signal as that reviewer stated in his review that he struggled to rate the paper because it contained a lot of math.",Paper Decision
Qb4Lm7XiRW,HJgLLyrYwB,State-only Imitation with Transition Dynamics Mismatch,Accept (Poster),"This paper addresses the setting of imitation learning from state observations only, where the system dynamics under which the demonstrations are performed differs from the target environment. The paper proposes to circumvent this dynamics shift with an algorithm whereby the target policy is trained to imitate its own past trajectories, re-ranked based on the similarity in state occupancies as judged by a WGAN critic.

The reviewers found the paper to be clearly written and enjoyable. The paper improved considerably through reviewers feedback. Notably, a behavior cloning from observations (BCO) baseline was added, which was stronger than the authors expected but still helped highlight the strength of the proposed method by comparison. R1 had a particularly productive multiple round exchange, clarifying the description of previous work, clarifying the details of the proposed procedure and strengthening the presentation of empirical evidence.

This work compellingly addresses an important problem, and in its final form is a polished piece of work. I recommend acceptance.",Paper Decision
FFSNqtyup1,rkeIIkHKvS,Measuring and Improving the Use of Graph Information in Graph Neural Networks,Accept (Poster),Two reviewers are positive about this paper while the other reviewer is negative. The low-scoring reviewer did not respond to discussions. I also read the paper and found it interesting. Thus an accept is recommended.,Paper Decision
hq08Bo79qc,rJx8I1rFwr,Meta-Learning by Hallucinating Useful Examples,Reject,"This paper describes a new approach to meta-learning with generating new useful examples.

The reviewers liked the paper but overall felt that the paper is not ready for publication as it stands.

Rejection is recommended. ",Paper Decision
2aPZHzwv7c,rylrI1HtPr,Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery,Reject,"This paper proposes to use the grey level co-occurrence matrix method (GLCM) for both the performance evaluation metric and an auxiliary loss function for single image super resolution. Experiments are conducted on X-ray images of rock samples. Three reviewers provide comments. Two reviewers rated reject while one rated weak reject. The major concerns include the lack of clear and detailed description, low novelty, limited experiment on only one database, unconvincing improvement over the prior work, etc. The authors agree that the limited experiment on one database does not demonstrate the generalization capability of the proposed method. The AC agrees with the reviewers’ comments, and recommend rejection.",Paper Decision
w8Ipi4yCb,BJxSI1SKDH,A Latent Morphology Model for Open-Vocabulary Neural Machine Translation,Accept (Spotlight),"This paper proposes a model for neural machine translation into morphologically rich languages by modeling word formation through a hierarchical latent variable model mimicking the process of morphological inflection. The model boils down to a VAE-like formulation with two latent representation: a continuous one (governed by a Gaussian) which captures lexical semantic aspects, and a discrete one (governed by the Kuma distribution) which captures the morphosyntactic function, shared among different surface forms. Even though the empirical improvements in terms of BLEU scores are fairly small, I find this a very elegant model which may foster interesting future research directions on latent models for NMT.

The reviewers had some concerns with some experimental details and model details that were properly addressed by the authors in their detailed response. In the discussion phase this alleviated the reviewers' concerns, which leads me to recommend acceptance. I urge the authors to follow the reviewer's recommendations to improve the final version of the paper. ",Paper Decision
szSFylL1gQ,SklVI1HKvH,Sample-Based Point Cloud Decoder Networks,Reject,The reviewers have raised several important concerns about the paper that the authors decided not to address.,Paper Decision
ZRTkeVkYK,S1gN8yrYwB,AUGMENTED POLICY GRADIENT METHODS FOR EFFICIENT REINFORCEMENT LEARNING,Reject,"The authors propose a hybrid model-free/model-based policy gradient method that attempts to reduce sample complexity without degrading asymptotic performance. They evaluate their approach on a collection of benchmark tests.

The reviewers raised concerns about limited novelty of the proposed approach and flaws in the evaluation. The authors need to compare to more baselines and ensure that the baseline algorithms are performing as previously reported. Even then, the reported improvements were small.

Given the issues raised by the reviewers, this paper is not ready for publication at ICLR.",Paper Decision
6n5hzSh9ma,HyeEIyBtvr,BETANAS: Balanced Training and selective drop for Neural Architecture Search,Reject,"This paper proposes a neural architecture search method that uses balanced sampling of architectures from the one-shot model and drops operators whose importance drops below a certain weight.

The reviewers agreed that the paper's approach is intuitive, but main points of criticism were:
- Lack of good baselines
- Potentially unfair comparison, not using the same training pipeline
- Lack of available code and thus of reproducibility. (The authors promised code in response, which is much appreciated. If the open-sourcing process has completed in time for the next version of the paper, I encourage the authors to include an anonymized version of the code in the submission to avoid this criticism.)

The reviewers appreciated the authors' rebuttal, but it did not suffice for them to change their ratings.
I agree with the reviewers that this work may be a solid contribution, but that additional evaluation is needed to demonstrate this. I therefore recommend rejection and encourage resubmission to a different venue after addressing the issues pointed out by the reviewers.",Paper Decision
0hzK1f5oJF,B1gX8JrYPr,Connecting the Dots Between MLE and RL for Sequence Prediction,Reject,"The authors construct a weighted objective that subsumes many of the existing approaches for sequence prediction, such as MLE, RAML, and entropy regularized policy optimization. By dynamically tuning the weights in the objective, they show improved performance across several tasks.

Although there were no major issues with the paper, reviewers generally felt that the technical contribution is fairly incremental and the empirical improvements are limited. Given the large number of high-quality submissions this year, I am recommending rejection for this submission.",Paper Decision
kncwFMK75M,B1gX8kBtPr,Universal Approximation with Certified Networks,Accept (Poster),"This work shows that there exist neural networks that can be certified by interval bound propagation. It provides interesting and surprising theoretical insights, although analysis requires the networks to be impractically large and hence does not directly yield practical advances. ",Paper Decision
cyuuQfZ2kn,SJgzLkBKPB,Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution,Accept (Poster),"A new method of calculating saliency maps for deep networks trained through RL (for example to play games) is presented. The method is aimed at explaining why moves were taken by showing which salient features influenced the move, and seems to work well based on experiments with Chess, Go, and several Atari games. 

Reviewer 2 had a number of questions related to the performance of the method under various conditions, and these were answered satisfactorily by the reviewers.

This is a solid paper with good reasoning and results, though perhaps not super novel, as the basic idea of explaining policies with saliency is not new. It should be accepted for poster presentation.
",Paper Decision
Vgm-PPd3W,rkeZIJBYvr,Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks,Accept (Talk),"The reviewers generally agreed that the paper presents a compelling method that addresses an important problem. This paper should clearly be accepted, and I would suggest for it to be considered for an oral presentation.

I would encourage the authors to take into account the reviewers' suggestions (many of which were already addressed in the rebuttal period) and my own suggestion.

The main suggestion I would have in regard to improving the paper is to position it a bit more carefully in regard to prior work on Bayesian meta-learning. This is an active research field, with quite a number of papers. There are two that are especially close to the VI method that the authors are proposing: Gordon et al. and Finn et al. (2018). For example, the graphical model in Figure 2 looks nearly identical to the ones presented in these two prior papers, as does the variational inference procedure. There is nothing wrong with that, but it would be appropriate for the authors to discuss this prior work a bit more diligently -- currently the relationship to these prior works is not at all apparent from their discussion in the related work section. A more appropriate way to present this would be to begin Section 3.2 by stating that this framework follows prior work -- there is nothing wrong with building on prior work, and the significant and important contribution of this paper is no way diminished by being up-front about which parts are inspired by previous papers.",Paper Decision
NMkwkr7Ean,SyeZIkrKwS,DyNet: Dynamic Convolution for Accelerating Convolution Neural Networks,Reject,"The paper proposed the use of dynamic convolutional kernels as a way to reduce inference computation cost, which is a linear combination of static kernels and fused after training for inference to reduce computation cost. The authors evaluated the proposed methods on a variety models and shown good FLOPS reduction while maintaining accuracy. 

The main concern for this paper is the limited novelty. There have been many works use dynamic convolutions as pointed out by all the reviewers. The most similar ones are SENet and soft conditional computation. Although the authors claim that soft conditional computation ""focus on using more parameters to make models to be more expressive while we focus on reducing redundant calculations"", the methods are pretty the same and moreover in the abstract of soft conditional computation they have ""CondConv improves the performance and inference cost trade-off"".",Paper Decision
HjUZRSKUJ,r1egIyBFPS,Deep Symbolic Superoptimization Without Human Knowledge,Accept (Poster),"This work introduces a neural architecture and corresponding method for simplifying symbolic equations, which can be trained without requiring human input. This is an area somewhat outside most of our expertise, but the general consensus is that the paper is interesting and is an advance. The reviewer's concerns have been mostly resolved by the rebuttal, so I am recommending an accept. ",Paper Decision
maRpdTbYwv,B1lgUkBFwr,Unsupervised domain adaptation with imputation,Reject,"This paper addresses the problem of performing unsupervised domain adaptation when some target domain data is missing is a potentially non-stochastic way. The proposed solution consists of applying a version of domain adversarial learning for adaptation together with an MSE based imputation loss learned using complete source data. The method is evaluated on both the standard digit recognition datasets and a real-world advertising dataset. 

The reviewers had mixed recommendations for this work, with two recommending weak reject and one recommending acceptance. The key positive point from R3 who recommended acceptance was that this work addresses a new problem statement which may be of practical importance. The other two reviewers expressed concerns over the contribution of the work and the validity of the problem setting. Namely, both R2 and R4 had significant confusion over the problem specification and/or under what conditions the proposed setting is valid. 

It is a difficult decision for this paper as there is a core disagreement between the reviewers. All reviewers seem to agree that the proposed solution is a combination of prior methods in a new way to address the specific problem setting of this work.  However, the reviewers differ in precisely whether they determine the proposed problem setting to be valid and justified. Due to this discrepancy, the AC does not recommend acceptance at this time. If the core contribution is to be an application of existing techniques to a new problem statement than that should be clarified and motivated further. 
",Paper Decision
_guXzMQ6d6,HJlxIJBFDr,Sample Efficient Policy Gradient Methods with Recursive Variance Reduction,Accept (Poster),"The paper introduces a policy gradient estimator that is based on stochastic recursive gradient estimator. It provides a sample complexity result of O(eps^{-3/2}) trajectories for estimating the gradient with the accuracy of eps.
This paper generated a lot of discussions among reviewers. The discussions were around the novelty of this work in relation to SARAH (Nguyen et al., ICML2017), SPIDER (Fang et al., NeurIPS2018) and the work of Papini et al. (ICML 2018). SARAH/SPIDER are stochastic variance reduced gradient estimators for convex/non-convex problems and have been studied in the optimization literature.
To bring it to the RL literature, some adjustments are needed, for example the use of importance sampling (IS) estimator. The work of Papini et al. uses IS, but does not use SARAH/SPIDEH, and it does not use step-wise IS.

Overall, I believe that even though the key algorithmic components of this work have been around, it is still a valuable contribution to the RL literature.
",Paper Decision
JoWxpSOYU9,S1x1IkHtPr,A Generative Model for Molecular Distance Geometry,Reject,"The paper presents a solution to generating molecule with three dimensional structure by learning a low-dimensional manifold that preserves the geometry of local atomic neighborhoods based on Euclidean distance geometry. 

The application is interesting and the proposed solution is reasonable. The authors did a good job at addressing most concerns raised in the reviews and updating the draft. 

Two main concerns were left unresolved: one is the lack of novelty in the proposed model, and the other is that some arguments in the paper are not fully supported. The paper could benefit from one more round of revision before being ready for publication. 

",Paper Decision
NRPZobaUUg,BJeAHkrYDS,Fast Task Inference with Variational Intrinsic Successor Features,Accept (Talk),"This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN.  After discussion of the author rebuttal, the reviewers all agreed on the significant contribution of the paper and that concerns about clarity were sufficiently addressed.  Thus, I recommend this paper for acceptance.",Paper Decision
-E_zJWTDtS,HyeaSkrYPH,Certified Defenses for Adversarial Patches,Accept (Poster),"This paper presents a certified defense method for adversarial patch attacks. The proposed approach provides certifiable guarantees to the attacks, and the reviewers particularly find its experiments results interesting and promising. The added new experiments during the rebuttal phase strengthened the paper. There still is a remaining concern that is novelty is limited as this paper could be viewed as the application of the original IBP to patch attacks, but the reviewers believe in that its empirical results are important.",Paper Decision
w_JckE1MG,SkgpBJrtvS,Contrastive Representation Distillation,Accept (Poster),"This paper presents a new distillation method with theoretical and empirical supports.

Given reviewers' comments and AC's reading, the novelty/significance and application-scope shown in the paper can be arguably limited. However, the authors extensively verified and compared the proposed methods and existing ones by showing significant improvements under comprehensive experiments. As the distillation method can enjoy a broader usage, I think the propose method in this paper can be influential in the future works.

Hence, I think this is a borderlines paper toward acceptance.",Paper Decision
s9nlt8thbT,Skl3SkSKDr,Generating valid Euclidean distance matrices,Reject,"This paper proposes a parametrisation of Euclidean distance matrices amenable to be used within a differentiable generative model. The resulting model is used in a WGAN architecture and demonstrated empirically in the generation of molecular structures. 

Reviewers were positive about the motivation from a specific application area (generation of molecular structures). However, they raised some concerns about the actual significance of the approach. The AC shares these concerns; the methodology essentially amounts to constraining the output of a neural network to be symmetric and positive semidefinite, which is in turn equivalent to producing a non-negative diagonal matrix (corresponding to the eigenvalues). As a result, the AC recommends rejection, and encourages the authors to include simple baselines in the next iteration. ",Paper Decision
pxupoAD8W,HJg3HyStwB,Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions,Reject,"The method proposed and explored here is to introduce small spatial distortions, with the goal of making them undetectable by humans but affecting the classification of the images. As reviewers point out, very similar methods have been tested before. The methods are also only tested on a few low-resolution datasets. 

The reviewers are unanimous in their judgement that the method is not novel enough, and the authors' rebuttals have not convinced the reviewers or me about the opposite.",Paper Decision
8P7NpCs8z5,rkliHyrFDB,Information Theoretic Model Predictive Q-Learning,Reject,"The authors develop a novel connection between information theoretic MPC and entropy regularized RL. Using this connection, they develop Q learning algorithm that can work with biased models. They evaluate their proposed algorithm on several control tasks and demonstrate performance over the baseline methods.

Unfortunately, reviewers were not convinced that the technical contribution of this work was sufficient. They felt that this was a fairly straightforward extension of MPPI. Furthermore, I would have expected a comparison to POLO. As the authors note, their approach is more theoretically principled, so it would be nice to see them outperforming POLO as a validation of their framework.

Given the large number of high-quality submissions this year, I recommend rejection at this time.",Paper Decision
aN2tKg-2-_,HklsHyBKDr,On Predictive Information Sub-optimality of RNNs,Reject,"Nice start but unfortunately not ripe.  The issues remarked by the reviewers were only partly addressed, and an improved version of the paper should be submitted at a future venue.",Paper Decision
XCcOXrLGo_,SklsBJHKDS,Model Inversion Networks for Model-Based Optimization,Reject,"This paper proposes Model Inversion Networks (MINs) to solve model optimization problems high-dimensional spaces. The paper received three reviews from experts working in this area. In a short review, R1 recommends Reject based on limited novelty compared to an ICDM 2019 paper. R2 recommends Weak Reject, identifying several strengths of the paper but also a number of concerns including unclear or missing technical explanations and need for some additional experiments (ablation studies). R3 recommends Weak Accept, giving the opinion that the idea the paper proposes is worthy of publication, but also identifying a number of weaknesses including a ""rushed"" experimental section that is missing details, need for additional quantitative experimental results, and some ""ad hoc"" parts of the formulation. The authors prepared responses that address many of these concerns, including a convincing argument that there is significant difference and novelty compared to the ICDM 2019. However, even if excluding R1's review, the reviews of R2 and R3 are borderline; the ACs read the paper and while they feel the work has significant merit, they agree with R2 and R3 that the paper needs additional work and another round of peer review to fully address R2 and R3's concerns. 

",Paper Decision
BuzOos2t2i,rJecSyHtDS,Learning to Recognize the Unseen Visual Predicates,Reject,"The paper proposes a new problem setting of predicate zero-shot learning for visual relation recognition for the setting when some of the predicates are missing, and a model that is able to address it.

All reviewers agreed that the problem setting is interesting and important, but had reservations about the proposed model. In particular, the reviewers were concerned that it is too simple of a step from existing methods. One reviewer also pointed towards potential comparisons with other zero-shot methods.

Following that discussion, I recommend rejection at this time but highly encourage the authors to take the feedback into account and resubmit to another venue.",Paper Decision
ZR3jbPaDAF,Skg5r1BFvB,"Continuous Control with Contexts, Provably",Reject,"This work considers the popular LQR objective but with [A,B] unknown and dynamically changing. At each time a context [C,D] is observed and it is assumed there exist a linear map Theta from [C,D] to [A,B]. The particular problem statement is novel, but is heavily influenced by other MDP settings and the also follows very closely to previous works. The algorithm seems computationally intractable (a problem shared by previous work this work builds on) and so in experiments a gross approximation is used. 

Reviewers found the work very stylized and did not adequately review related work. For example, little attention is paid to switching linear systems and the recent LQR advances are relegated to a list of references with no discussion. The reviewers also questioned how the theory relates to the traditional setting of LQR regret, say, if [C,D] were identity at all times so that Theta = [A,B]. 

This paper received 3 reviews (a third was added late to the process) and my own opinion influenced the decision. While the problem statement is interesting, the work fails to put the paper in context with the existing work, and there are some questions of algorithm methods.  ",Paper Decision
DxVrvy_Za,SyxKrySYPr,Stabilizing Transformers for Reinforcement Learning,Reject,"This paper proposes  architectural modifications to transformers, which are promising for sequential tasks requiring memory but can be unstable to optimize, and applies the resulting method to the RL setting, evaluated in the DMLab-30 benchmark.

While I thought the approach was interesting and the results promising, the reviewers unanimously felt that the experimental evaluation could be more thorough, and were concerned with the motivation behind of some of the proposed changes.
",Paper Decision
2-61ZCc3Ue,SJlKrkSFPH,A FRAMEWORK  FOR ROBUSTNESS CERTIFICATION  OF SMOOTHED CLASSIFIERS USING  F-DIVERGENCES,Accept (Poster),"This submission proposes a black-box method for certifying the robustness of smoothed classifiers in the presence of adversarial perturbations. This work goes beyond previous works in certifying robustness for arbitrary smoothing measures.

Strengths:
-Sound formulation and theoretical justification to tackle an important problem.

Weaknesses
-Experimental comparison was at times not fair.
-The presentation and writing could be improved.

These two weaknesses were sufficiently addressed during the discussion. All reviewers recommend acceptance.",Paper Decision
PV64LzvDS,SylurJHFPS,The Detection of Distributional Discrepancy for Text Generation,Reject,"The authors propose a novel metric to detect distributional discrepancy for text generation models and argue that these can be used to explain the failure of GANs for language generation tasks. The reviewers found significant deficiencies with the paper, including:

1) Numerous grammatical errors and typos, that make it difficult to read the paper.

2) Mischarcterization of prior work on neural language models, and failure to compare with standard distributional discrepancy measures studied in prior work (KL, total variation, Wasserstein etc.). Further, the necessity of the complicated procedure derived by the authors is not well-justified.

3) Failure to run experiments on standard banchmarks for image generation (which are much better studied applications of GANs) and confirm the superiority of the proposed metrics relative to standard baselines. 

The reviewers were agreed on the rejection decision and the authors did not participate in the rebuttal phase.

I therefore recommend rejection.",Paper Decision
sT8BSRAqgw,SyedHyBFwS,Relative Pixel Prediction For Autoregressive Image Generation,Reject,"All reviewers rated this submission as a weak reject and there was no author response.
The AC recommends rejection.",Paper Decision
SXGYD_tkBP,BylPSkHKvB,Natural- to formal-language generation using Tensor Product Representations,Reject,"The paper proposed a new seq2seq method to implement natural language to formal language translation.  Fixed length Tensor Product Representations are used as the intermediate representation between encoder and decoder.  Experiments are conducted on MathQA and AlgoList datasets and show the effectiveness of the methods.  Intensive discussions happened between the authors and reviewers.  Despite of the various concerns raised by the reviewers, a main problem pointed by both reviewer#3 and reviewer#4 is that there is a gap between the  theory and the implementation in this paper.  The other reviewer (#2) likes the paper but is less confident and tend to agree with the other two reviewers.",Paper Decision
7RlwxfdCQu,BJxvH1BtDS,Three-Head Neural Network Architecture for AlphaZero Learning,Reject,"The authors provide an empirical study of the recent 3-head architecture applied to AlphaZero style learning. They thoroughly evaluate this approach using the game Hex as a test domain.

Initially, reviewers were concerned about how well the hyper parameters for tuned for different methods. The authors did a commendable job addressing the reviewers concerns in their revision. However, the reviewers agreed that with the additional results showing the gap between the 2 headed architecture and the three-headed architecture narrowed, the focus of the paper has changed substantially from the initial version. They suggest that a substantial rewrite of the paper would make the most sense before publication.

As a result, at this time, I'm going to recommend rejection, but I encourage the authors to incorporate the reviewers feedback. I believe this paper has the potential to be a strong submission in the future.

",Paper Decision
LUTeBSo6ik,HJl8SkBYPr,Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget,Reject,"The authors leverage advances in semi-supervised learning and data augmentation to propose a method for active learning. The AL method is based on the principle that a model should consistently label across perturbation/augmentations of examples, and thus propose to choose samples for active learning based on how much the estimated label distribution changes based on different perturbations of a given example. The method is intuitive and the experiments provide some evidence of efficacy. However, during discussion there was a lingering question of novelty that eventually swayed the group to reject this paper. ",Paper Decision
axb_l2zDv6,BkgUB1SYPS,Interpretable Network Structure for Modeling Contextual Dependency,Reject,"This paper a theoretical interpretation of separation rank as a measure of a recurrent network's ability to capture contextual dependencies in text, and introduces a novel bidirectional NLP variant and tests it on several NLP tasks to verify their analysis. 

Reviewer 3 found that the paper does not provide a clear description of the method and that a focus on single message would have worked better. Reviewer 2 made a claim of several shortcomings in the paper relating to lack of clarity, limited details on method, reliance on a 'false dichotomy', and failure to report performance. Reviewer 1 found the goals of the work to be interesting but that the paper was not clear, that the proofs were not rigorous enough, and clarity of experiments. The authors responded to the all the comments. The reviewers felt that their comments were still valid and did not adjust their ratings.

Overall, the paper is not yet ready in its current form. We hope that the authors will find valuable feedback for their ongoing research.",Paper Decision
9vzZVL0Y5,HJlrS1rYwH,Policy Tree Network,Reject,"The consensus amongst the reviewers is that the paper discusses an interesting idea and shows significant promise, but that the presentation of the initial submission was not of a publishable standard. While some of the issues were clarified during discussion, the reviewers agree that the paper lacks polish and is therefore not ready. While I think Reviewer #3 is overly strict in sticking to a 1, as it is the nature of ICLR to allow papers to be improved through the discussion, in the absence of any of the reviewers being ready to champion the paper, I cannot recommend acceptance. I however have no doubt that with further work on the presentation of what sounds like a potentially fascinating contribution to the field, the paper will stand a chance at acceptance at a future conference.",Paper Decision
WDBg901Ci5,BJlBSkHtDS,Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks,Accept (Poster),"The paper proposed a new learnable activation function called Padé Activation Unit (PAU) based on parameterization of rational function. All the reviewers agree that the method is soundly motivated, the empirical results are strong to suggest that this would be a good addition to the literature. ",Paper Decision
iLf3K8pue,SkeBBJrFPH,Characterize and Transfer Attention in Graph Neural Networks,Reject,"This paper suggests that datasets have a strong influence on the effects of attention in graph neural networks and explores the possibility of transferring attention for graph sparsification, suggesting that attention-based sparsification retains enough information to obtain good performance while reducing computational and storage costs. 

Unfortunately I cannot recommend acceptance for this paper in its present form. Some concerns raised by the reviewers are: the analysis lacks theoretical insights and does not seem to be very useful in practice; the proposed method for graph sparsification lacks novelty; the experiments are not thorough to validate its usefulness. I encourage the authors to address these concerns in an eventual resubmission.
",Paper Decision
qu0UalVQqe,SJgVHkrYDH,Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering,Accept (Poster),"The paper proposed a multi-hop machine reading method for hotpotqa and squad-open datasets. The reviewers agreed that it is very interesting to learn to retrieve, and the paper presents an interesting solution. Some additional experiments as suggested by the reviewers will help improve the paper further. ",Paper Decision
enSC6Eok6n,rylXBkrYDS,A Baseline for Few-Shot Image Classification,Accept (Poster),"This paper introduces a simple baseline for few-shot image classification in the transductive setting, which includes a standard cross-entropy loss on the labeled support samples and a conditional entropy loss on the unlabeled query samples.

Both losses are known in the literature (the seminal work of entropy minimization by Bengio should be cited properly). However, reviewers are positive about this paper, acknowledging the significant contributions of a novel few-shot baseline that establishes a new state-of-the-art on well-known public few-shot datasets as well as on the introduced large-scale benchmark ImageNet21K. The comprehensive study of the methods and datasets in this domain will benefit the research practices in this area.

Therefore, I make an acceptance recommendation.",Paper Decision
vqKoVb0lZS,ByxQB1BKwH,Abstract Diagrammatic Reasoning with Multiplex Graph Networks,Accept (Poster),"This paper a new method of constructing graph neural networks for the task of reasoning to answer IQ style diagrammatic reasoning, in particular including Raven Progressive Matrices. The model first learns an object representation for parts of the  image and then tries to combine them together to represent relations between different objects of the image. Using this model they achieve SOTA results (ignoring a parallely submitted paper) on the PGM and Raven datasets. The improvement in SOTA is subtantial.

Most of the critique made for the paper is on writing style and presentation. The authors seem to have fixed several of these concerns in the newly uploaded version of the paper. I will further request the authors to revise the paper for readability. However, since the paper presents both an interesting modeling and improved empirical results, I recommend acceptance.",Paper Decision
APCxqwApje,SklGryBtwr,Environmental drivers of systematicity and generalization in a situated agent,Accept (Poster),"The paper studies out-of-sample generalisation that require an agent to respond to never-seen-before instructions by manipulating and positioning objects in a 3D Unity simulated room, and analyzes factors which promote combinatorial generalization in such environment. 

The paper is a very thought provoking work, and would make a valuable contribution to the line of works on systematic generalization in embodied agents. The draft has been improved significantly after the rebuttal. After the discussion, we agree that it is worthwhile presenting at ICLR.  ",Paper Decision
BDotz4oli,Skgfr1rYDH,SoftAdam: Unifying SGD and Adam for better stochastic gradient descent,Reject,"The reviewers all agreed that the proposed modification was minor. I encourage the authors to pursue in this direction, as they mentioned in their rebuttal, before resubmitting to another conference.",Paper Decision
gvQ94s6q0S,r1xMH1BtvB,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,Accept (Poster),"This paper investigates the tasks used to pretrain language models. The paper proposes not using a generative tasks ('filling in' masked tokens), but instead a discriminative tasked (recognising corrupted tokens). The authors empirically show that the proposed method leads to improved performance, especially in the ""limited compute"" regime. 

Initially, the reviewers had quite split opinions on the paper, but after the rebuttal and discussion phases all reviewers agreed on an ""accept"" recommendation. I am happy to agree with this recommendation based on the following observations:
- The authors provide strong empirical results including relevant ablations. Reviews initially suggested a limitation to classification tasks and a lack of empirical analysis, but those issues have been addressed in the updated version. 
- The problem of pre-training language model is relevant for the ML and NLP communities, and it should be especially relevant for ICLR. The resulting method significantly outperforms existing methods, especially in the low compute regime. 
- The idea is quite simple, but at the same time it seems to be a quite novel idea. ",Paper Decision
kP5i2Czt9P,SJxbHkrKDH,Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning,Accept (Poster),"The paper proposes a curriculum approach to increasing the number of agents (and hence complexity) in MARL.

The reviewers mostly agreed that this is a simple and useful idea to the MARL community. There was some initial disagreement about relationships with other RL + evolution approaches, but it got resolved in the rebuttal. Another concern was the slight differences in the environments considered by the paper compared to the literature, but the authors added an experiment with the unmodified version.

Given the positive assessment and the successful rebuttal, I recommend acceptance.",Paper Decision
z8RctS87so,SJe-HkBKDS,Amharic Text Normalization with Sequence-to-Sequence Models,Reject,"The paper proposes a text normalisation model for Amharic text. The model uses word classification, followed by a character-based GRU attentive encoder-decoder model. The paper is very short and does not present reproducible experiments. It also does not conform to the style guidelines of the conference. There has been no discussion of this paper beyond the initial reviews, all of which reject it with a score of 1. It is not ready to publish and the authors should consider a more NLP focussed venue for future research of this kind. 
",Paper Decision
TjaTTCcVEw,SJexHkSFPS,Thinking While Moving: Deep Reinforcement Learning with Concurrent Control,Accept (Poster),"This paper studies the setting in reinforcement learning where the next action must be sampled while the current action is still executing. This refers to continuous time problems that are discretised to make them delay-aware in terms of the time taken for action execution. The paper presents adaptions of the Bellman operator and Q-learning to deal with this scenario.

This is a problem that is of theoretical interest and also has practical value in many real-world problems. The reviewers found both the problem setting and the proposed solution to be valuable, particularly after the greatly improved technical clarity in the rebuttals. As a result, this paper should be accepted.",Paper Decision
XgeyUMBMJm,BJxeHyrKPB,RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH,Reject,"Agreement by the reviewers: although the idea is good, the paper is very hard to read and not accurately enough formulated to merit publication.  

This can be repaired, and the authors should try again after a thorough revision and rewrite.",Paper Decision
ExJmOU7UHz,SygkSkSFDB,On the expected running time of nonconvex optimization with early stopping,Reject,"The authors made no response to reviewers. Based on current reviews, the paper is suggested a rejection as majority.",Paper Decision
pvMiT0yc4,ryl1r1BYDS,Multiagent Reinforcement Learning in Games with an Iterated Dominance Solution,Reject,"The paper proofs that reinforcement learning (using two different algorithms) converge to iterative dominance solutions for a class of multi-player games (dominance solvable games). 

There was a lively discussion around the paper. However, two of the reviewers remain unconvinced of the novelty of the approach,  pointing to [1] and [2], with [1] only pertaining to supermodular games. The exact contribution over such existing results is currently not addressed in the manuscript.  There were also concerns about the scaling and applicability of the results, as dominance solvable games are limited. 

[1] http://www.parisschoolofeconomics.eu/docs/guesnerie-roger/milgromroberts90.pdf
[2] Friedman, James W., and Claudio Mezzetti. ""Learning in games by random sampling."" Journal of Economic Theory 98.1 (2001): 55-84.",Paper Decision
60wBNwWgB,HylA41Btwr,CP-GAN: Towards a Better Global Landscape of GANs,Reject,The paper is proposed a rejection based on majority reviews.,Paper Decision
Q45nRnJSGu,Hke0V1rKPS,Jacobian Adversarially Regularized Networks for Robustness,Accept (Poster),"This paper extends previous observations (Tsipars, Etmann etc) in relations between Jacobian and robustness and directly train a model that improves robustness using Jacobians that look like images. The questions regarding computation time (suggested by two reviewers, including one of the most negative reviewers) are appropriately addressed by the authors (added experiments). Reviewers agree that the idea is novel, and some conjectured why the paper’s idea is a very sensible one. We think this paper would be an interest for ICLR readers. Please address any remaining comments from the reviewers before the final copy.
",Paper Decision
lDo4NPzNoG,BJg641BKPH,Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems,Reject,"This article studies gradient optimization for classification problems with shallow networks with smooth activations, obtaining convergence and generalisation results under a separability assumption on the data. The results are obtained under much less stringent requirements on the width of the network than other related recent works. However, with results on convergence and generalisation having been established in other previous works, the reviewers found the contribution incremental. The responses clarified some of the distinctive challenges with the logistic loss compared with the squared loss that has been considered in other works, and provided examples for the separability assumption. Overall, the article makes important contributions in the case of classification problems. However, with many recent works addressing challenging problems in a similar direction, the bar has been set quite high. As pointed out by some of the reviewers, the contribution could gain substantially in relevance and make a more convincing case by addressing extensions to non smooth activations and deep models. ",Paper Decision
Lgv5ai_usX,BkeaEyBYDB,Improving Federated Learning Personalization via Model Agnostic Meta Learning,Reject,"The reviewers have reached consensus that while the paper is interesting, it could use more time.  We urge the authors to continue their investigations.",Paper Decision
Cqtd5aUKBs,SyxhVkrYvr,Towards Verified Robustness under Text Deletion Interventions,Accept (Poster),This paper deals with the under-sensitivity problem in natural language inference tasks.  An interval bound propagation (IBP) approach is applied to predict the confidence of the model when a subsets of words from the input text are deleted.  The paper is well written and easy to follow.  The authors give detailed rebuttal and 3 of the 4 reviewers lean to accept the paper.,Paper Decision
Bu5HQgvfx,Skx24yHFDr,Discovering Topics With Neural Topic Models Built From PLSA Loss,Reject,"This paper presents a neural topic model with the goal of improving topic discovery with a PLSA loss. Reviewers point out major limitations including the following:

1) Empirical comparison is done only with LDA when there are many newer models that perform much better.
2) Related work section is incomplete, especially for the newer models.
3) Writing is unclear in many parts of the paper.

For these reasons, I recommend that the authors make major improvements to the paper before resubmitting to another venue.",Paper Decision
GfyAvLt4aF,rJehVyrKwH,And the Bit Goes Down: Revisiting the Quantization of Neural Networks,Accept (Spotlight),"This paper addresses to compress the network weights by quantizing their values to some fixed codeword vectors. The paper is well written, and is overall easy to follow. The proposed algorithm is well-motivated, and easy to apply. The method can be expected to perform well empirically, which the experiments verify, and to have potential impact. On the other hand, the novelty is not very high, though this paper uses these existing techniques in a different setting.",Paper Decision
1AbRf1Niw,rkesVkHtDr,Meta-Learning Runge-Kutta,Reject,"Summary: This paper casts the problem of step-size tuning in the Runge-Kutta method as a meta learning problem. The paper gives a review of the existing approaches to step size control in RK method. Deriving knowledge from these approaches the paper reasons about appropriate features and loss functions to use in the meta learning update. The paper shows that the proposed approach is able to generalize sufficiently enough to obtain better performance than a baseline. 


The paper was lacking in advocates for its merits, and needs better comparisons with other baselines before it is ready to be published.",Paper Decision
WybA5-my0c,HyxjNyrtPr,RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis,Accept (Poster),"The paper has initially received mixed reviews, with two reviewers being weakly positive and one being negative. Following the author's revision, however, the negative reviewer was satisfied with the changes, and one of the positive reviewers increased the score as well. 

In general, the reviewers agree that the paper contains a simple and well-executed idea for recovering geometry in unsupervised way with generative modeling from a collection of 2D images, even though the results are a bit underwhelming. The authors are encouraged to expand the related work section in the revision and to follow our suggestion of the reviewers.",Paper Decision
AuyFGF4L7a,rkgqN1SYvr,Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks,Accept (Poster),"The paper shows that initializing the parameters of a deep linear network from the orthogonal group speeds up learning, whereas sampling the parameters from a Gaussian may be harmful.

The result of this paper can be interesting to the deep learning community. The main concern the reviewers raised is the huge overlap with the paper by Du & Hu (2019). It would have been nice to actually see whether the results for linear networks empirically also hold for nonlinear networks. ",Paper Decision
1QTqY2y8w1,BkgF4kSFPB,Hallucinative Topological Memory for Zero-Shot Visual Planning,Reject,"The submission presents an approach to visual planning. The work builds on semi-parametric topological memory (SPTM) and introduces ideas that facilitate zero-shot generalization to new environments. The reviews are split. While the ideas are generally perceived as interesting, there are significant concerns about presentation and experimental evaluation. In particular, the work is evaluated in extremely simple environments and scenarios that do not match the experimental settings of other comparable works in this area. The paper was discussed and all reviewers expressed their views following the authors' responses and revision. In particular, R1 posted a detailed justification of their recommendation to reject the paper. The AC agrees that the paper is not ready for publication in a first-tier venue. The AC recommends that the authors seriously consider R1's recommendations.",Paper Decision
Q4JkXERKIv,HkgYEyrFDr,Learning Good Policies By Learning Good Perceptual Models,Reject,"This paper investigates using ""curiosity"" to improve representation learning. This paper is not ready for publication. The main issues was the reviewers found the paper did not support the claim contributions in terms of (1) evaluating the new representations and improvement due to the representation, and (2) the novelty of the method compared to the long literature in this area. In general the reviewers found the empirical evidence unconvincing, and the too many missing details.

The results in this paper have many issues: claims of performance based on three runs; undefined error measures; bolding entries in tables which appear not significantly better without explanation; unclear/informal meta-parameter tuning. 

Finally, there are some terminology issues in this paper. I suggest an excellent paper on the topic: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3858647/  ",Paper Decision
LqWqdhJGoy,r1etN1rtPB,Implementation Matters in Deep RL: A Case Study on PPO and TRPO,Accept (Talk),"This paper provides a careful and well-executed evaluation of the code-level details of two leading policy search algorithms, which are typically considered implementation details and therefore often unstated or brushed aside in papers. These are revealed to have major implications for the performance of both algorithms.

The reviewers are all in agreement that this paper has important reproducibility and evaluation implications for the field, and adds substantially to our body of knowledge on policy gradient algorithms. I therefore recommend it be accepted.

However, a serious limitation is that only 3 random seeds were used to get average performance in the first, key experiment. Experiments are expensive, but that result is not meaningful without more runs, and arguably could be misleading rather than informative. The authors should increase the number of runs as much as possible, at least to 10 but ideally more.",Paper Decision
EYZztuKXx,ryxdEkHtPS,A Closer Look at Deep Policy Gradients,Accept (Talk),"The paper empirically studies the behaviour of deep policy gradient algorithms, and reveals several unexpected observations that are not explained by the current theory. All three reviewers are excited about this work and recommend acceptance.",Paper Decision
FmVXgU2BYJ,H1edEyBKDS,Plug and Play Language Models: A Simple Approach to Controlled Text Generation,Accept (Poster),"This paper proposes a simple plug-and-play language model approach to the problem of controlled language generation. The problem is important and timely, and the approach is simple yet effective. Reviewers had some discussions whether  1) there is enough novelty, 2) evaluation task really shows effectiveness, and 3) this paper will inspire future research directions. 

After discussions of the above points, reviewers are leaning more positive, and I reflect their positive sentiment by recommending it to be accepted. I look forward to seeing this work presented at ICLR.",Paper Decision
0KkqKZSgC,HkewNJStDr,Efficient High-Dimensional Data Representation Learning via Semi-Stochastic Block Coordinate Descent Methods,Reject,"All the reviewers reach a consensus to reject the current submission. 

In addition, there are two assumptions in the proof which seemed never included in Theorem conditions or verified in typical cases. 

1) Between Eq (16) and (17), the authors assumed the 'extended restricted strong convexity’ given by the un-numbered equation. 

2) In Eq. (25), the authors assume the existence of \sigma making the inequality true.

However those assumptions are neither explicitly stated in theorem conditions, nor verified for typical cases in applications, e.g. even the square or logistic loss. The authors need to address these assumptions explicitly rather than using them from nowhere.",Paper Decision
9abhtGXJdR,H1gDNyrKDS,Understanding and Robustifying Differentiable Architecture Search,Accept (Talk),"This paper studies the properties of Differentiable Architecture Search, and in particular when it fails, and then proposes modifications that improve its performance for several tasks. The reviews were all very supportive with three Accept opinions, and authors have addressed their comments and suggestions. Given the unanimous reviews, this appears to be a clear Accept. ",Paper Decision
JGqNbI89wN,B1g8VkHFPH,Rethinking the Hyperparameters for Fine-tuning,Accept (Poster),"This paper presents a guide for setting hyperparameters when fine-tuning from one domain to another. This is an important problem as many practical deep learning applications repurpose an existing model to a new setting through fine-tuning.  All reviewers were positive saying that this work provides new experimental insights, especially related to setting momentum parameters. Though other works may have previously discussed the effect of momentum during fine-tuning, this work presented new experiments which contributes to the overall understanding. Reviewer 3 had some concern about the generalization of the findings to other backbone architectures, but this concern was resolved during the discussion phase. The authors have provided detailed clarifications during the rebuttal and we encourage them to incorporate any remaining discussion or any new clarifications into the final draft. ",Paper Decision
suA_qhEthA,S1eL4kBYwr,UNITER: Learning UNiversal Image-TExt Representations,Reject,"This submission proposes an approach to pre-train general-purpose image and text representations that can be effective on target tasks requiring embeddings for both modes. The authors propose several pre-training tasks beyond masked language modelling that are more suitable for the cross-modal context being addressed, and also investigate which dataset/pretraining task combinations are effective for given target tasks.

All reviewers agree that the empirical results that were achieved were impressive.

Shared points of concern were:
- the novelty of the proposed pre-training schemes.
- the lack of insight into the results that were obtained.

These concerns were insufficiently addressed after the discussion period, particularly the limited novelty. Given the remaining concerns and the number of strong submissions to ICLR, this submission, while promising, does not meet the bar for acceptance.
",Paper Decision
pDjlsNktRv,Skl8EkSFDr,Self-Supervised GAN Compression,Reject,"The paper develops a new method for pruning generators of GANs. It has received a mixed set of reviews. Basically, the reviewers agree that the problem is interesting and appreciate that the authors have tried some baseline approaches and verified/demonstrated that they do not work. 

Where the reviewers diverge is on whether the authors have been successful with the new method. In the opinion of the first reviewer, there is little value in achieving low levels (e.g. 50%) of fine-grained sparsity, while the authors have not managed to achieve good performance with filter-level sparsity (as evidenced by Figure 7, Table 3 as well as figures in the appendices). The authors admit that the sparsity levels achieved with their approach cannot be turned into speed improvement without future work.

Furthermore, as pointed out by the first reviewer, the comparison with prior art, in particular with LIT method, which has been reported to successfully compress the same GAN, is missing and the results of LIT have been misrepresented. While the authors argue that their pruning is an ""orthogonal technique"", and can be applied on top of LIT, this is not verified in any way. In practice, combination of different compression techniques is known to be non-trivial, since they aim to explain the same types of redundancies.

Overall, while this paper comes close, the problems highlighted by the first reviewer have not been resolved convincingly enough for acceptance.",Paper Decision
JvylP8JlZ9,BylB4kBtwB,Retrieving Signals in the Frequency Domain with Deep Complex Extractors,Reject,The paper discusses audio source separation with complex NNs.  The approach is good and may increase an area of research.  But the experimental section is very weak and needs to be improved to merit publication.,Paper Decision
wdj9g4Nvs,BJgr4kSFDS,Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings,Accept (Poster),This paper proposes a new method to answering queries using incomplete knowledge bases. The approach relies on learning embeddings of the vertices of the knowledge graph. The reviewers unanimously found that the method was well motivated and found the method convincingly outperforms previous work.,Paper Decision
y4BBlXf4L,Byx4NkrtDS,Implementing Inductive bias for different navigation tasks through diverse RNN attrractors,Accept (Poster),"Navigation is learned in a two-stage process, where the (recurrent) network is first pre-trained in a task-agnostic stage and then fine-tuned using Q-learning. The analysis of the learned network confirms that what has been learned in the task-agnostic pre-training stage takes the form of attractors.

The reviewers generally liked this work, but complained about lack of comparison studies / baselines. The authors then carried out such studies and did a major update of the paper.

Given that the extensive update of the paper seems to have addressed the reviewers' complaints, I think this paper can be accepted.",Paper Decision
0WaOgwe3N_,BJe4V1HFPr,Disentangling Style and Content in Anime Illustrations,Reject,"This paper proposes a two-stage adversarial training approach for learning a disentangled representation of style and content of anime images. Unlike the previous style transfer work, here style is defined as the identity of a particular anime artist, rather than a set of uninterpretable style features. This allows the trained network to generate new anime images which have a particular content and are drawn in the style of a particular artist. While the approach works well, the reviewers voiced concerns about the method (overly complicated and somewhat incremental) and the quality of the experimental section (lack of good baselines and quantitative comparisons at least in terms of the disentanglement quality). It was also mentioned that releasing the code and the dataset would strengthen the appeal of the paper. While the authors have addressed some of the reviewers’ concerns, unfortunately it was not enough to persuade the reviewers to change their marks. Hence, I have to recommend a rejection.",Paper Decision
HZ7RrZy6Bh,H1lXVJStwB,Dynamic Instance Hardness,Reject,"All three reviewers, even after the rebuttal, agreed that the paper did not meet with bar for acceptance. A common complaint was lack of clarity being a major problem. Unfortunately, the paper cannot be accepted in its current form. The authors are encouraged to improve the presentation of their approach  and resubmit to a new venue.  ",Paper Decision
EnItGAmxLd,r1l7E1HFPH,Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning,Reject,"This paper extends recent multi-step dynamic programming algorithms to reinforcement learning with function approximation.  In particular, the paper extends h-step optimal Bellman operators (and associated k-PI and k-VI algorithms) to deep reinforcement learning.  The paper describes new extensions to DQN and TRPO algorithms.  This approach is claimed to reduce the instability of model-free algorithms, and the approach is tested on Atari and Mujoco domains. 

The reviewers noticed several limitations of the work.  The reviewers found little theoretical contribution in this work and they were unsatisfied with the empirical contributions.  The reviewers were unconvinced of the strength and clarity of the empirical results with the Atari and Mujoco domains along with the deep learning network architectures.  The reviewers suggested that simpler domains with a simpler function approximation scheme could enable more through experiments and more conclusive results.  The claim in the abstract of addressing the instabilities was also not adequately studied in the paper.

This paper is not ready for publication.  The primary contribution of this work is the empirical evaluation, and the evaluation is not sufficiently clear for the reviewers.",Paper Decision
7kJxOtZ2JW,BJx7N1SKvB,A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions,Reject,"In this work, the authors focus on the high-dimensional regime in which both the dataset size and the number of features tend to infinity. They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations.

Unfortunately, the reviewers could not reach a consensus as to whether this paper had sufficient novelty to merit acceptance at this time. Incorporating their feedback would move the paper closer towards the acceptance threshold.",Paper Decision
PagJTXqs6,ryefE1SYDr,LIA: Latently Invertible Autoencoder with Adversarial Learning,Reject,"A nice idea: the latent prior is replaced by a GAN.  A general agreement between all four reviewers to reject the submission, based on a not thorough enough description of the approach, and possibly not being novel.",Paper Decision
t2KDDhKNnC,BJgWE1SFwS,PCMC-Net: Feature-based Pairwise Choice Markov Chains,Accept (Poster),"This submission proposes to use neural networks in combination with pairwise choice markov chain models for choice modelling. The deep network is used to parametrize the PCMC and in so doing improve generalization and inference.

Strengths:
The formulation and theoretical justifications are convincing.
The improvements are non-trivial and the approach is novel.

Weaknesses:
The text was not always easy to follow.
The experimental validation is too limited initially. This was addressed during the discussion by adding an additional experiment.

All reviewers recommend acceptance.
",Paper Decision
hlMH9OLx7,B1gZV1HYvS,Multi-Agent Interactions Modeling with Correlated Policies,Accept (Poster),"The paper proposes an extension to the popular Generative Adversarial Imitation Learning framework that considers multi-agent settings with ""correlated policies"", i.e., where agents' actions influence each other. The proposed approach learns opponent models to consider possible opponent actions during learning. Several questions were raised during the review phase, including clarifying questions about key components of the proposed approach and theoretical contributions, as well as concerns about related work. These were addressed by the authors and the reviewers are satisfied that the resulting paper provides a valuable contribution. I encourage the authors to continue to use the reviewers' feedback to improve the clarity of their manuscript in time for the camera ready submission.",Paper Decision
Jd6y2fOXks,rJel41BtDH,Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning,Reject,"The paper focuses on semi-supervised learning and presents a pseudo labeling-based approach with i) mixup (Zhang et al. 2018); ii) keeping $k$ labelled examples in each minibatch.

The paper is clear and well-written; it presents a simple and empirically effective idea. Reviewers appreciate the nice proof of concept on the two-moons dataset, the fact that the approach is validated with different architectures. Some details would need to be clarified, e.g. about the dropout control.

A main contribution of the paper is to show that pseudo-labelling plus the combination of mixup and certainty (keeping $k$ labelled examples in each minibatch) can outperform the state of the art based on consistency regularization methods, while being simpler and computationally much less demanding. 

While the paper does a good job of showing that ""it works"", the reader however misses some discussion about ""why it works"". It is most interesting that the performances are not improving with $k$ (Table 1). An in-depth analysis of the trade-off between the uncertainty (through mix-up and the entropy of the pseudo-labels) and certainty, and how it impacts the performance, would be appreciated. You might consider monitoring how this trade-off evolves along learning; I suspect that evolving $k$ along the epochs might make sense;  the question is to find a simple way to control online this hyper-parameter.  

The area chair encourages the authors to continue this very promising path of research, and dig a little bit deeper, considering the question of optimizing the trade-off between certainty and uncertainty along the training trajectory.",Paper Decision
rxBB1m4Kk6,HylxE1HKwS,Once-for-All: Train One Network and Specialize it for Efficient Deployment,Accept (Poster),"The authors propose a new method for neural architecture search, except it's not exactly that because model training is separated from architecture, which is the main point of the paper. Once this network is trained, sub-networks can be distilled from it and used for specific tasks.

The paper as submitted missed certain details, but after this was pointed out by reviewers the details were satisfactorily described by the authors. 

The idea of the paper is original and interesting. The paper is correct and, after the revisions by authors, complete. In my view, this is sufficient for acceptance.",Paper Decision
1OapIup5YI,H1lxVyStPH,Generalized Convolutional Forest Networks for Domain Generalization and Visual Recognition,Accept (Poster),"The authors introduce an approach to learn a random forest model and a representation simultaneously. The basic idea is to modify the representation so that subsequent trees in the random forest are less correlated.  The authors evaluate the technique empirically and show some modest gains. While the reviews were mixed, the approach is quite different from the usual approaches published at ICLR  and so I think it's worth highlighting this work.
",Paper Decision
gyg254z4lz,S1xJ4JHFvS,Acutum: When Generalization Meets Adaptability,Reject,"The paper addresses an important problem of finding a good trade-off between generalization and convergence speed of stochastic gradient methods for training deep nets. However, there is a consensus among the reviewers, even after rebuttals provided by the authors, that  the contribution is somewhat limited and the paper may require additional work before it is ready to be published.",Paper Decision
oxehWUL-V0,BylyV1BtDB,FR-GAN: Fair and Robust Training,Reject,"This manuscript proposes an approach for fair and robust training of predictive modeling -- both of which are implemented using adversarial methods, i.e., an adversarial loss for fairness and an adversarial loss for robustness. The resulting model is evaluated empirically and shown to improve fairness and robustness performance.

The reviewers and AC agree that the problem studied is timely and interesting, as there is limited work on joint fairness and robustness. However, the reviewers were unconvinced about the novelty and clarity of the conceptual and empirical results. In reviews and discussion, the reviewers also noted insufficient motivation for the approach. ",Paper Decision
BKh_GvQpO,Sye0XkBKvS,SNODE: Spectral Discretization of Neural ODEs for System Identification,Accept (Poster),"This work proposes using spectral element methods to speed up training of ODE Networks for system identification. The authors utilize truncated series of Legendre polynomials to analyze the dynamics and then conduct experiments that shows their proposed scheme achieves an order of magnitude improvement in training speed compared to baseline methods. Reviewers raised some concerns (e.g. empirical comparison against adjoint methods in the multi-agent example) or asked for clarifications (e.g. details of time sampling of the data). The authors adequately addressed most of these concerns via rebuttal response as well as revising the initial submission. At the end, all reviewers recommended for accept based on contributions of this work on improving training speed of ODE Networks. R4 hopes that some of the additional concerns that are not yet reflected in the current revision, be addressed in the camera ready version. ",Paper Decision
xS_-Mv8nvg,BJl07ySKvS,Guiding Program Synthesis by Learning to Generate Examples,Accept (Poster),"The paper consider the problem of program induction from a small dataset of input-output pairs; the small amount of available data results a large set of valid candidate programs.
The authors propose to train an neural oracle by unsupervised learning on the given data, and synthesizing new pairs to augment the given data, therefore reducing the set of admissible programs.
This is reminiscent of data augmentation schemes, eg elastic transforms for image data.

The reviewers appreciate the simplicity and effectiveness of this approach, as demonstrated on an android UI dataset.
The authors successfully addressed most negative points raised by the reviewers in the rebuttal, except the lack of experimental validating on other datasets.

I recommend to accept this paper, based on reviews and my own reading.
I think the manuscript could be further improved by more explicitly discussing  (early in the paper) the intuition why the authors think this approach is sensible:
The additional information for more successfully infering the correct program has to come from somewhere; as no new information is eg given by a human oracle, it was injected by the choice of prior over neural oracles.
It is essential that the paper discuss this. ",Paper Decision
v5Uwg_gUR,rklTmyBKPH,Fast Neural Network Adaptation via Parameter Remapping and Architecture Search,Accept (Poster),"Main content: Paper proposes a fast network adaptation (FNA) method, which takes a pre-trained image classification network, and produces a network for the task of object detection/semantic segmentation

Summary of discussion:
reviewer1: interesting paper with good results, specifically without the need to do pre-training on Imagenet. Cons are better comparisons to existing methods and run on more datasets. 
reviewer2:  interesting idea on adapting source network network via parameter re-mapping that offers good results in both performance and training time.
reviewer3: novel method overall, though some concerns on the concrete parameter remapping scheme. Results are impressive
Recommendation: Interesting idea and good results. Paper could be improved with better comparison to existing techniques. Overall recommend weak accept.",Paper Decision
YEB3WXENzc,r1la7krKPS,Measuring Calibration in Deep Learning,Reject,"The authors propose two measures of calibration that don't simply rely on the top prediction. The reviewers gave a lot of useful feedback. Unfortunately, the authors didn't respond.",Paper Decision
Sb_vGTHCs1,H1lTQ1rFvS,R2D2: Reuse & Reduce via Dynamic Weight Diffusion for Training Efficient NLP Models,Reject,"This paper proposes a very interesting alternative to feed-forward network layers, based on Quaternion methods and Hamilton products, which has the benefit of reducing the number of parameters in the neural network (more than 50% smaller) without sacrificing performance. They conducted extensive experiments on language tasks (NMT and NLI, among others) using transformers and LSTMs. 

The paper appears to be clearly presented and have extensive results on a variety of tasks. However all reviewers pointed out that there is a lack of in-depth analysis and thus insight into why this approach works, as well as questions on the specific effects of regularization. These concerns were not addressed in the rebuttal period, instead leaving it to future work. My assessment is that, with further analysis, ablation studies, and comparison to alternative methods for reducing model size (quantization, etc), this paper has the potential to be quite impactful, and I look forward to future versions of this work. As it currently stands, however, I don’t believe it’s suitable for publication at ICLR.
",Paper Decision
reMm3MEhFU,rkl3m1BFDB,Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning,Accept (Poster),"This was a contentious paper, with quite a large variance in the ratings, and ultimately a lack of consensus. After reading the paper myself, I found it to be a valuable synthesis of common usage of saliency maps and a critique of their improper interpretation. Further, the demonstration of more rigorous methods of evaluating agents based on salience maps using case studies is quite illustrative and compelling. I think we as a field can agree that we’d like to gain better understanding our deep RL models. This is not possible if we don’t have a good understanding of the analysis tools we’re using.

R2 rightly pointed out a need for quantitative justification for their results, in the form of statistical tests, which the authors were able to provide, leading the reviewer to revise their score to the highest value of 8. I thank them for instigating the discussion.

R1 continues to feel that the lack of a methodological contribution (in the form of improving learning within an agent) is a weakness. However, I don’t believe that all papers at deep learning conferences have to have the goal of empirically “learning better” on some benchmark task or dataset, and that there’s room at ICLR for more analysis papers. Indeed, it’d be nice to see more papers like this.
 
For this reason, I’m inclined to recommend accept for this paper. However this paper does have weaknesses, in that the framework proposed could be made more rigorous and formal. Currently it seems rather adhoc and on a task-by-task basis (ie we need to have access to game states or define them ourselves for the task). It’s also disappointing that it doesn’t work for recurrent agents, which limits its applicability for analyzing current SOTA deep RL agents. I wonder if authors can comment on possible extensions that would allow for this.
",Paper Decision
k897WTvqj6,SyljQyBFDH,Meta-Learning Deep Energy-Based Memory Models,Accept (Poster),Four knowledgable reviewers recommend accept. Good job!,Paper Decision
v1KObiYty_,Syx9Q1rYvH,Mutual Information Maximization for Robust Plannable Representations,Reject,"The manuscript concerns a mutual information maximization objective for dynamics model learning, with the aim of using this representation for planning / skill learning. The central claim is that this objective promotes robustness to visual distractors, compared with reconstruction-based objectives. The proposed method is evaluated on DeepMind Control Suite tasks from rendered pixel observations, modified to include simple visual distractors. 

Reviewers concurred that the problem under consideration is important, and (for the most part) that the presentation was clear, though one reviewer disagreed, remarking that the method is only introduced on the 5th page. A central sticking point was whether the method would reliably give rise to representations that ignore distractors and preferentially encode task information. (I would note that a very similar phenomenon to the behaviour they describe has been empirically demonstrated before in Warde-Farley et al 2018, also on DM Control Suite tasks, where the most predictable/controllable elements of a scene are reliably imitated by a goal-conditioned policy trained against a MI-based reward). The distractors evaluated were criticized as unrealistically stochastic, that fully deterministic distractors may confound the procedure; while a revised version of the manuscript experimented with *less* random distractors, these distractors were still unpredictable at the scale of more than a few frames.

While the manuscript has improved considerably in several ways based on reviewer feedback, reviewers remain unconvinced by the empirical investigation, particularly the choice of distractors. I therefore recommend rejection at this time, while encouraging the authors to incorporate criticisms to strengthen a resubmission.",Paper Decision
XVeKRBcKp,Bkx5XyrtPS,Depth creates no more spurious local minima in linear networks,Reject,Paper shows that the question of linear deep networks having spurious local minima under benign conditions on the loss function can be reduced to the two layer case. This paper is motivated by and builds upon works that are proven for specific cases. Reviewers found the techniques used to prove the result not very novel in light of existing techniques. Novelty of technique is of particular importance to this area because these results have little practical value in linear networks on their own; the goal is to extend these techniques to the more interesting non-linear case. ,Paper Decision
e-cDXG8Tv8,HkgqmyrYDH,WORD SEQUENCE PREDICTION FOR AMHARIC LANGUAGE,Reject,"This paper presents a language model for Amharic using HMMs and incorporating POS tags. The paper is very short and lacks essential parts such as describing the exact model and the experimental design and results. The reviewers all rejected this paper, and there was no author rebuttal. This paper is clearly not appropriate for publication at ICLR. ",Paper Decision
mv94DUG1S,HygYmJBKwH,YaoGAN: Learning Worst-case Competitive Algorithms from Self-generated Inputs,Reject,"The authors propose an intriguing way to designing competitive online algorithms. However, the state of the paper and the provided evidence of the success of the proposed methodology is too preliminary to merit acceptance.",Paper Decision
WynniSQQ5z,HJeFmkBtvB,Annealed Denoising score matching: learning Energy based model in high-dimensional spaces,Reject,"This paper presents a variant of the Noise Conditional Score Network (NCSN) which does score matching using a single Gaussian scale mixture noise model. Unlike the NCSN, it learns a single energy-based model, and therefore can be compared directly to other models in terms of compression. I've read the paper, and the methods, exposition, and experiments all seem solid. Numerically, the score is slightly below the cutoff; reviewers generally think the paper is well-executed, but lacking in novelty and quality of results relative to Song & Ermon (2019). 
",Paper Decision
YWcJxsJx6i,SJx_QJHYDB,Finding Winning Tickets with Limited (or No) Supervision,Reject,"The paper studies finding winning tickets with limited supervision. The authors consider a variety of different settings. An interesting contribution is to show that findings on small datasets may be misleading. That said, all three reviewers agree that novelty is limited, and some found inconsistencies and passages that were hard to read: Based on this, it seems the paper doesn't quite meet the ICLR bar in its current form. ",Paper Decision
M-tH3al04L,HkxdQkSYDB,Graph Convolutional Reinforcement Learning,Accept (Poster),"The work proposes a graph convolutional network based approach to multi-agent reinforcement learning. This approach is designed to be able to adaptively capture changing interactions between agents. Initial reviews highlighted several limitations but these were largely addressed by the authors. The resulting paper makes a valuable contribution by proposing a well-motivated approach, and by conducting extensive empirical validation and analysis that result in novel insights. I encourage the authors to take on board any remaining reviewer suggestions as they prepare the camera ready version of the paper.",Paper Decision
tOhGpVMw39,HJePXkHtvS,Deep Generative Classifier for Out-of-distribution Sample Detection,Reject,"The paper presents a training method for deep neural networks to detect out-of-distribution samples under perspective of Gaussian discriminant analysis.

Reviewers and AC agree that some idea is given in the previous work (although it does not focus on training), and additional ideas in the paper are not super novel. Furthermore, experimental results are weak, e.g., comparison with other deep generative classifiers are desirable, as the paper focuses on training such deep models.

Hence, I recommend rejection.",Paper Decision
pZn9H9Ucnv,SyxDXJStPS,Reparameterized Variational Divergence Minimization for Stable Imitation,Reject,"The submission performs empirical analysis on f-VIM (Ke, 2019), a method for imitation learning by f-divergence minimization. The paper especially focues on a state-only formulation akin to GAILfO (Torabi et al., 2018b). The main contributions are:
1) The paper identifies numerical proplems with the output activations of f-VIM and suggest a scheme to choose them such that the resulting rewards are bounded.
2) A regularizer that was proposed by Mescheder et al. (2018) for GANs is tested in the adversarial imitation learning setting.
3) In order to handle state-only demonstrations, the technique of GAILfO is applied to f-VIM (then denoted f-VIMO) which inputs state-nextStates instead of state-actions to the discriminator.

The reviewers found the submitted paper hard to follow, which suggests a revision might make more apparent the author's contributions in later submissions of this work. ",Paper Decision
QLm8QXyaL,SJeUm1HtDH,Swoosh! Rattle! Thump! - Actions that Sound,Reject,"This paper investigates using sound to improve classification, motion prediction, and representation learning all from data generated by a real robot.

All the reviewers were intrigued by the work. The paper provides experiments on real robots (never a small task), and a data-set for the community, and a sequence of illustrative experiments. Because the paper combines existing techniques, its main contribution is the empirical demonstrations of the utility of using sound. Overall, it was not quite enough for the reviewers. The main issues were: (1) motion prediction is perhaps expected given the physical setup, (2) lack of comparison with other approaches, (3) lack of diversity in the demonstrations (10 objects, one domain).

The authors added two new experiments with a different setup, further demonstrating their claims. In addition the authors highlighted that the novelty of this task means there are no clear baselines (to which r3 agreed). The new experiments are briefly described in the response (and visuals on a website), but the authors did not update the paper. The new experiments could potentially significantly strength the paper. However, the terse description in the response and the supplied visuals made it difficult for the reviewers to judge their contribution.

Overall, this is certainly a very interesting direction. The results on real world data demonstrate promise, even if they are not the benchmarking style the community is used too.   ",Paper Decision
8Q2T8yT1vr,SJl47yBYPS,Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning,Reject,"The paper studies the role of entropy in maximum entropy RL, particularly in soft actor-critic, and proposes an action normalization scheme that leads to a new algorithm, called Streamlined Off-Policy (SOP), that does not maximize entropy, but retains or exceeds the performance of SAC. Independently from SOP, the paper also introduces Emphasizing Recent Experience (ERE) that samples minibatches from the replay buffer by prioritizing the most recent samples. After rounds of discussion and a revised version with added experiments, the reviewers viewed ERE as the main contribution, while had doubts regarding the claimed benefits of SOP. However, the paper is currently structured around SOP, and the effectiveness of ERE, which can be applied to any off-policy algorithm, is not properly studied. Therefore, I recommend rejection, but encourage the authors to revisit the work with an emphasis on ERE.",Paper Decision
BIO1gYTya,SkxV7kHKvr,TWIN GRAPH CONVOLUTIONAL NETWORKS: GCN WITH DUAL GRAPH SUPPORT FOR SEMI-SUPERVISED LEARNING,Reject,All three reviewers are consistently negative on this paper. Thus a reject is recommended.,Paper Decision
F9q68n0sGi,HJemQJBKDr,Continual Density Ratio Estimation (CDRE): A new method for evaluating generative models in continual learning,Reject,"The paper seems technically correct and has some novelty, but the relevance of the paper is questionable. Considering the selectiveness of ICLR, I cannot recommend the paper for acceptance at this point. 

In more detail: the authors propose a technique for estimating density rations between a target distribution of real samples and a distribution of samples generated by the model, without storing samples. The method seems to be technically well executed and verified. However, there was major concerns among multiple reviewers that the addressed problem does not seem relevant to the ICLR community. The question addressed seemed artificial, and it was not considered realistic (by R2 and also by R1 in the confidential discussion). R3 also expressed doubts at the usefulness of the method. 

Furthermore, some doubts were expressed regarding clarity (although opinions were mixed on that) and on the justification of the modification of the VAE objective to the continual setting. ",Paper Decision
CGRpO1wco0,BylQm1HKvB,CONTRIBUTION OF INTERNAL REFLECTION IN LANGUAGE EMERGENCE WITH AN UNDER-RESTRICTED SITUATION,Reject,"This paper is very different from most ICLR submissions, and appears to be addressing interesting themes.  However the paper seems poorly written, and generally unclear.  The motivation, task, method and evaluation are all unclear.  I recommend that the authors add explicit definitions, equations, algorithm boxes, and more examples to make their paper clearer.",Paper Decision
AEPFs0kn1_,Hklz71rYvS,Kernelized Wasserstein Natural Gradient,Accept (Spotlight),"This is a very interesting paper which extends natural gradient to output space metrics other than the Fisher-Rao metric (which is motivated by approximating KL divergence). It includes substantial mathematical and algorithmic insight. The method is shown to outperform various other optimizers on a neural net optimization problem that's artificially made ill-conditioned; while it's not clear how practically meaningful this setting is, it seems like a good way to study optimization. I think this paper will be of interest to a lot of researchers and could open up new research directions, so I recommend acceptance as an Oral.
",Paper Decision
uQ67MdQiX6,rygGQyrFvH,The Curious Case of Neural Text Degeneration,Accept (Poster),"This paper presents nucleus sampling, a sampling method that truncates the tail of a probability distribution and samples from a dynamic nucleus containing the majority of the probability mass. Likelihood and human evaluations show that the proposed method is a better alternative to a standard sampling method and top-k sampling.

This is a well-written paper and I think the proposed sampling method will be useful in language modeling. All reviewers agree that the paper addresses an important problem. 

Two reviewers have concerns regarding the technical contribution of the paper (i.e., nucleus sampling is a straightforward extension of top-k sampling), and whether it is enough for publications at a venue such as ICLR. R2 suggests to have a better theoretical framework for nucleus sampling. I think these are valid concerns. However, given the potential widespread application of the proposed method and the strong empirical results, I recommend to accept the paper.

Also, a minor comment, I think there is something wrong with your style file (e.g., the bottom margin appears too large compared to other submissions).",Paper Decision
xYRNUWTG6,HkeZQJBKDB,Universal approximations of permutation invariant/equivariant functions by deep neural networks,Reject,"The article studies universal approximation for the restricted class of equivariant functions, which can have a smaller number of free parameters. The reviewers found the topic important and also that the approach has merits. However, they pointed out that the article is very hard to read and that more intuitions, a clearer comparison with existing work, and connections to practice would be important. The responses did clarify some of the differences to previous works. However, there was no revision addressing the main concerns. 
",Paper Decision
N5TpEAKcyI,HkxWXkStDB,Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation,Reject,"The paper in its current form was just not well enough received by the reviewers to warrant an acceptance rating. It seems this work may have promise and the authors are encouraged to continue with this line of work.
",Paper Decision
rOcr1VHdj,SkgbmyHFDS,What Can Learned Intrinsic Rewards Capture?,Reject,"The authors present a metalearning-based approach to learning intrinsic rewards that improve RL performance across distributions of problems.  This is essentially a more computationally efficient approach to approaches suggested by Singh (2009/10).  The reviewers agreed that the core idea was good, if a bit incremental, but were also concerned about the similarity to the Singh et al. work, the simplicity of the toy domains tested, and comparison to relevant methods.  The reviewers felt that the authors addressed their main concerns and significantly improved the paper; however the similarity to Singh et al. remains, and thus the concerns about incrementalism.   Thus, I recommend this paper for rejection at this time.",Paper Decision
21Gokiocin,B1xgQkrYwS,"On Iterative Neural Network Pruning, Reinitialization, and the Similarity of Masks",Reject,"This is an observational work with experiments for comparing iterative pruning methods.

I agree with the main concerns of all reviewers:

(a) Experimental setups are of too small-scale or with easy datasets, so hard to believe they would generalize for other settings, e.g., large-scale residual networks. This aspect is very important as this is an observational paper.
(b) The main take-home contribution/message is weak considering the high-standard of ICLR.

Hence, I recommend rejection. 

I would encourage the authors to consider the above concerns as it could yield a valuable contribution.",Paper Decision
DSAGURM1td,BkgeQ1BYwS,Implicit Generative Modeling for Efficient Exploration,Reject,"There is insufficient support to recommend accepting this paper.  The authors provided detailed responses, but the reviewers unanimously kept their recommendation as reject.  The novelty and significance of the main contribution was not made sufficiently clear, given the context of related work.  Critically, the experimental evaluation was not considered to be convincing, lacking detailed explanation and justification, and a sufficiently thorough comparison to strong baselines, The submitted reviews should help the authors improve their paper.",Paper Decision
BZuJlWXyND,r1l1myStwr,Continuous Meta-Learning without Tasks,Reject,"In this paper the authors view meta-learning under a general, less studied viewpoint, which does not make the typical assumption that task segmentation is provided. In this context, change-point analysis is used as a tool to complement meta-learning in this expanded domain. 
 
The expansion of meta-learning in this more general and often more practical context is significant and the paper is generally well written. However, considering this particular (non)segmentation setting is not an entirely novel idea; for example the reviewers have already pointed out [1] (which the authors agreed to discuss), but also [2] is another relevant work. The authors are highly encouraged to incorporate results, or at least a discussion, with respect to at least [2]. It seems likely that inferring boundaries could be more powerful, but it is important to better motivate this for a final paper. 
 
Moreover, the paper could be strengthened by significantly expanding the discussion about practical usefulness of the approach. R3 provides a suggestion towards this direction, that is, to explore the performance in a situation where task segmentation is truly unavailable. 
 
[1] Rahaf et el. ""Task-Free Continual Learning"". 
[2] Riemer et al. ""Learning to learn without forgetting by maximizing transfer and minimizing interference"".
 
",Paper Decision
Aqpw4UFnyx,rJlk71rYvH,Counterfactual Regularization for Model-Based Reinforcement Learning,Reject,I agree with the reviewers that this paper has serious limitations in the experimental evaluation.,Paper Decision
ASh91iFZ1,r1xCMyBtPS,Multilingual Alignment of Contextual Word Representations,Accept (Poster),"This paper proposes a method to improve alignments of a multilingual contextual embedding model (e.g., multilingual BERT) using parallel corpora as an anchor. The authors show the benefit of their approach in a zero-shot XNLI experiment and present a word retrieval analysis to better understand multilingual BERT.

All reviewers agree that this is an interesting paper with valuable contributions. The authors and reviewers have been engaged in a thorough discussion during the rebuttal period and the revised paper has addressed most of the reviewers concerns.

I think this paper would be a good addition to ICLR so I recommend accepting this paper.",Paper Decision
oOM8PH7tVx,B1xRGkHYDS,A bi-diffusion based layer-wise sampling method for deep learning in large graphs,Reject,"This paper addresses the challenge of time complexity in aggregating neighbourhood information in GCNs. As we aggregate information from larger hops (deeper neighbourhoods) the number of nodes can increases exponentially thereby increasing time complexity. To overcome this the authors propose a sampling method which samples nodes layer by layer based on bidirectional diffusion between layers. They demonstrate the effectiveness of their approach on 3 large benchmarks.

While the ideas presented in the paper were interesting the reviewers raised some concerns which I have summarised fellow:

1) Novelty: The reviewers felt that the techniques presented were not very novel and is very similar to one existing work as pointed out by R4
2) Writing: The writing needs to be improved. The authors have already made an attempt towards this but it could be improved further
3) Comparisons with baselines: R4 has raised some concerns  the settings/configurations used for the baseline methods. In particular, the results for the baseline methods are lower than those reported in the original papers. I have read the author's rebuttal for this but I am not completely convinced about it. I would suggest that the authors address this issue in subsequent submissions

Based on the above reasons I recommend that the paper cannot be accepted. 

 ",Paper Decision
87Yf_X7sy,rJgRMkrtDr,Learning Video Representations using Contrastive Bidirectional Transformer,Reject,"This paper studies self-supervised video representations with a multi-modal learning process that the authors then use for performance on a variety of tasks. The main contribution of the paper is a successful effort to incorporate BERT-like models into vision tasks.

Reviewers acknowledged the extensive empirical evaluation and the good performance of the approach. However, they raised some concerns about the lack of clarity and the absence of analysis and interpretation of the results. The AC shares this view, and recommends rejection at this time, encouraging the authors to revise their work addressing these analysis and clarity questions.  ",Paper Decision
Y1Me_gVCgE,HJxnM1rFvr,HUBERT Untangles BERT to Improve Transfer across NLP Tasks,Reject,"The paper introduces additional layers on top BERT type models for disentangling of semantic and positional information.  The paper demonstrates (small) performance gains in transfer learning compared to pure BERT baseline.

Both reviewers and authors have engaged in a constructive discussion of the merits of the proposed method. Although the reviewers appreciate the ideas and parts of the paper the consensus among the reviewers is that the evaluation of the method is not clearcut enough to warrant publication.

Rejection is therefore recommended. Given the good ideas presented in the paper and the promising results the authors are encouraged to take the feedback into account and submit to the next ML conference.   ",Paper Decision
DsrJqMCeO3,HyxnMyBKwB,The Gambler's Problem and Beyond,Accept (Poster),"This paper studies the optimal value function for the gambler's problem, and presents some interesting characterizations thereof. The paper is well written and should be accepted.",Paper Decision
fpmxv3N5I0,S1esMkHYPr,GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation,Accept (Poster),"All reviewers agreed that this paper is essentially a combination of existing ideas, making it a bit incremental, but is well-executed and a good contribution.  Specifically, to quote R1:

""This paper proposes a generative model architecture for molecular graph generation based on autoregressive flows. The main contribution of this paper is to combine existing techniques (auto-regressive BFS-ordered generation of graphs, normalizing flows, dequantization by Gaussian noise, fine-tuning based on reinforcement learning for molecular property optimization, and validity constrained sampling). Most of these techniques are well-established either for data generation with normalizing flows or for molecular graph generation and the novelty lies in the combination of these building blocks into a framework. ... Overall, the paper is very well written, nicely structured and addresses an important problem. The framework in its entirety is novel, but the building blocks of the proposed framework are established in prior work and the idea of using normalizing flows for graph generation has been proposed in earlier work. Nonetheless, I find the paper relevant for an ICLR audience and the quality of execution and presentation of the paper is good.""",Paper Decision
W_ekdLMvLN,r1lczkHKPr,Off-policy Multi-step Q-learning,Reject,"The authors propose TD updates for Truncated Q-functions and Shifted Q-functions, reflecting short- and long-term predictions, respectively. They show that they can be combined to form an estimate of the full-return, leading to a Composite Q-learning algorithm. They claim to demonstrated improved data-efficiency in the tabular setting and on three simulated robot tasks.

All of the reviewers found the ideas in the paper interesting, however, based on the issues raised by Reviewer 3, everyone agreed that substantial revisions to the paper are necessary to properly incorporate the new results. As a result, I am recommending rejection for this submission at this time. I encourage the authors to incorporate the feedback from the reviewers, and believe that after that is done, the paper will be a strong submission. ",Paper Decision
QJEcfi_vQn,H1e5GJBtDr,Axial Attention in Multidimensional Transformers,Reject,"This paper proposes a self-attention-based autoregressive model called Axial Transformers for images and other data organized as high dimensional tensors. The Axial Attention is applied within each axis of the data to accelerate the processing.

Most of the authors claim that main idea behind Axial Attention is widely applicable, which can be used in many core vision tasks, such as detection and classification. However, the revision fails to provide more application for Axial attention.

Overall, the idea behind this paper is interesting but more convincing experimental results are needed.
",Paper Decision
jj1Izn74N,BkgOM1rKvr,The Surprising Behavior Of Graph Neural Networks,Reject,"The paper empirically investigates the behaviour of graph neural networks, as a function of topology, structural noise, and coupling between nodal attributes and structure. While the paper is interesting, reviewers in general felt that the presentation lacked clarity and aspects of the experiments were hard to interpret. The authors are encouraged to continue with this work, accounting for reviewer comments in subsequent versions.
",Paper Decision
7pz9M3rCw0,ByedzkrKvH,Double Neural Counterfactual Regret Minimization,Accept (Poster),"Double coúnterfactual regret minimization is an extension of neural counterfactual regret minimization that uses separate policy and regret networks (reminiscent of similar extensions of the basic RL formula in reinforcement learning). Several new algorithmic modifications are added to improve the performance. 

The reviewers agree that this paper is novel, sound, and interesting. One of the reviewers had a set of questions that the authors responded to, seemingly satisfactorily. Given that this seems to be a high-quality paper with no obvious issues, it should be accepted.",Paper Decision
NMEreOFQEY,BJe_z1HFPr,Resizable Neural Networks,Reject,"This paper offers likely novel schemes for image resizing.  The performance improvement is clear.  Unfortunately two reviewers find substantial clarity issues in the manuscript after revision, and the AC concurs that this is still an issue.  The paper is borderline but given the number of higher ranked papers in the pool is unable to be accepted unfortunately. ",Paper Decision
V9jLYhsgcK,BkeDGJBKvB,Multitask Soft Option Learning,Reject,"Apologies for only receiving two reviews. R2 gave a WR and R3 gave an A. Given the lack of 3rd review and split nature of the scores, the AC has closely scrutinized the paper/reviews/comments/rebuttal. Thoughts:
 - Paper is on interesting topic.
 - AC agrees with R2's concern about the evaluation not using more complex environments like Mujoco. Without evaluation on a standard benchmark, it is difficult to know objectively if the approach works. 
 - AC agrees with authors that the DISTRAL approach forms a strong baseline. 
 - Nevertheless, the experiments aren't super compelling either.
 - AC has some concerns about scaling issues w.r.t. model size & #tasks. 

The paper is very borderline, but the AC sides with R2's concerns and unfortunately feels the paper cannot be accepted without a stronger evaluation. With this, it would make a compelling paper. ",Paper Decision
fjSJ9keA-l,HklvMJSYPB,Adaptive Adversarial Imitation Learning,Reject,This paper extends adversarial imitation learning to an adaptive setting where environment dynamics change frequently. The authors propose a novel approach with pragmatic design choices to address the challenges that arise in this setting. Several questions and requests for clarification were addressed during the reviewing phase. The paper remains borderline after the rebuttal. Remaining concerns include the size of the algorithmic or conceptual contribution of the paper.,Paper Decision
3s4kQ3fChB,H1eUz1rKPr,Representation Learning with Multisets,Reject,"While the reviewers appreciated the problem to learn a multiset representation, two reviewers found the technical contribution to be minor, as well as limited experiments. The rebuttal and revision addressed concerns about the motivation of the approach, but the experimental issues remain. The paper would likely substantially improve with additional experiments.",Paper Decision
mZydQoihz5,rJeIGkBKPS,Improving Confident-Classifiers For Out-of-distribution Detection,Reject,"The paper improves the previous method for detecting out-of-distribution  (OOD) samples. 

Some theoretical analysis/motivation is interesting as pointed out by a reviewer. I think the paper is well written in overall and has some potential.

However, as all reviewers pointed out, I think experimental results are quite below the borderline to be accepted (considering the ICLR audience), i.e., the authors should consider non-MNIST-like and more realistic datasets. This indicates the limitation on the scalability of the proposed method. 

Hence, I recommend rejection.",Paper Decision
GGOcIMkHa,S1xSzyrYDB,Cyclic Graph Dynamic Multilayer Perceptron for Periodic Signals,Reject,"The reviewers all appreciated the area explored by this work but there was a consensus that it lacked a thorough presentation of existing works, as well as relevant baselines.

I encourage the authors to better position their work with respect to the existing literature for what should be a stronger submission for a future conference.",Paper Decision
lZUKw3nCEG,HJlHzJBFwB,Accelerating Monte Carlo Bayesian Inference via Approximating Predictive Uncertainty over the Simplex,Reject,"This paper proposes to speed up Bayesian deep learning at test time by training a student network to approximate the BNN's output distribution. The idea is certainly a reasonable thing to try, and the writing is mostly good (though as some reviewers point out, certain sections might not be necessary). The idea is fairly obvious, though, so the question is whether the experimental results are impressive enough by themselves to justify acceptance. The method is able to get close to the performance achieved by Monte Carlo estimators with much lower cost, although there is a nontrivial drop in accuracy. This is probably worth paying if it achieves 500x computation reduction as claimed in the paper, though the practical gains are probably much smaller since Monte Carlo methods are rarely used with 500 samples. Overall, this seems a bit below the bar for ICLR.
",Paper Decision
LVUaRKvEg8,rkxVz1HKwB,Certifiably Robust Interpretation in Deep Learning,Reject,"This paper discusses new methods to perform adversarial attacks on salience maps.

In its current form, this paper in its current form has unfortunately has not convinced several of the reviewers/commenters of the motivation behind proposing such a method. I tend to share the same opinion. I would encourage the authors to re-think the motivation of the work, and if there are indeed solid use cases to express them explicitly in the next version of the paper.",Paper Decision
0v9B8i1Au7,r1e4MkSFDr,Continuous Convolutional Neural Network forNonuniform Time Series,Reject,"This paper presents a continuous CNN model that can handle nonuniform time series data. It learns the interpolation kernel and convolutional architectures in an end-to-end manner, which is shown to achieve higher performance compared to naïve baselines. 
All reviewers scored Weak Reject and there was no strong opinion to support the paper during discussion. Although I felt some of the reviewers’ comments are missing the points, I generally agree that the novelty of the method is rather straightforward and incremental, and that the experimental evaluation is not convincing enough. Particularly, comparison with more recent state-of-the-art point process methods should be included. For example, [1-3] claim better performance than RMTPP. Considering that the contribution of the paper is more on empirical side and CCNN is not only the solution for handing nonuniform time series data, I think this point should be properly addressed and discussed. Based on these reasons, I’d like to recommend rejection. 

[1] Xiao et al., Modeling the Intensity Function of Point Process via Recurrent Neural Networkss, AAAI 2017.
[2] Li et al., Learning Temporal Point Processes via Reinforcement Learning, NIPS 2018.
[3] Turkmen et al, FastPoint: Scalable Deep Point Processes, ECML-PKDD 2019.
",Paper Decision
yvnRFvxKkw,SJeQGJrKwH,DS-VIC: Unsupervised Discovery of Decision States for Transfer in RL,Reject,"This work is interesting because it's aim is to push the work in intrinsic motivation towards crisp definitions, and thus reads like an algorithmic paper rather than yet another reward heuristic and system building paper. There is some nice theory here, integration with options, and clear connections to existing work.

However, the paper is not ready for publication. There were were several issues that could not be resolved in the reviewers minds (even after the author response and extensive discussion). The primary issues were: (1) There was significant confusion around the beta sensitivity---figs 6,7,8 appear misleading or at least contradictory to the message of the paper. (2) The need for x,y env states. (3) The several reviewers found the decision states unintuitive and confused the quantitative analysis focus if they given the authors primary focus is transfer performance. (4) All reviewers found the experiments lacking. Overall, the results generally don't support the claims of the paper, and there are too many missing details and odd empirical choices.  

Again, there was extensive discussion because all agreed this is an interesting line of work. Taking the reviewers excellent suggestions on board will almost certainly result in an excellent paper. Keep going!",Paper Decision
fJ34mCeZBO,BJgQfkSYDS,Neural Policy Gradient Methods: Global Optimality and Rates of Convergence,Accept (Poster),"The paper makes a solid contribution to understanding the convergence properties of policy gradient methods with over-parameterized neural network function approximators.  This work is concurrent with and not subsumed by other strong work by Agarwal et al. on the same topic.  There is sufficient novelty in this contribution to merit acceptance.  The authors should nevertheless clarify the relationship between their work and the related work noted by AnonReviewer2, in addition to addressing the other comments of the reviewers.",Paper Decision
fQF0GfyaJ,rJgffkSFPS,Multi-objective Neural Architecture Search via Predictive Network Performance Optimization,Reject,"This paper proposes to use Graph Convolutional Networks (GCNs) in Bayesian optimization for neural architecture search. While the paper title includes multi-objective, this component appears to only be a posthoc evaluation of the Pareto front of networks evaluated using a single-objective search -- this could be performed for any method that evaluates more than one network. Performance on NAS-Bench-101 appears to be very good. 

In the private discussion of reviewers and AC, several issues were raised, including whether the approach is compared fairly to LaNAS and whether the GCN will predict well for large search spaces. Also, unfortunately, no code is provided, making it unclear whether the work is reproducible. The reviewers unanimously agreed on a weak rejection score.

I concur with this assessment and therefore recommend rejection.
",Paper Decision
HMvkpVdyBY,rJgzzJHtDB,"Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference",Accept (Poster),"The authors develop a novel technique to train networks to be robust and accurate while still being efficient to train and evaluate. The authors propose ""Robust Dynamic Inference Networks"" that allows inputs to be adaptively routed to one of several output channels and thereby adjust the inference time used for any given input. They show 

The line of investigation initiated by authors is very interesting and should open up a new set of research questions in the adversarial training literature.

The reviewers were in consensus on the quality of the paper and voted in favor of acceptance. One of the reviewers had concerns about the evaluation in the paper, in particular about whether carefully crafted attacks could break the networks studied by the authors. However, the authors performed additional experiments and revised the paper to address this concern to the satisfaction of the reviewer.

Overall, the paper contains interesting contributions and should be accepted.",Paper Decision
QKf-ZlqnW,SJlbGJrtDB,Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers,Accept (Poster),The paper lies on the borderline. An accept is suggested based on majority reviews and authors' response.,Paper Decision
Lml9dqy7Ul,Skl-fyHKPH,A Mean-Field Theory for Kernel Alignment with Random Features in Generative Adverserial Networks,Reject,"This paper was assessed by three reviewers who scored it as 6/1/6. The main criticism included somewhat weak experiments due to the manual tuning of bandwidth, the use of old (and perhaps mostly solved/not challenging) datasets such as Mnist and Cifar10, lack of ablation studies. The other issue voiced in the review is that the proposed method is very close to a MMD-GAN with a kernel plus random features. Taking into account all positives and negatives, we regret to conclude that this submission falls short of the quality required by ICLR2020, thus it cannot be accepted at this time.

",Paper Decision
4_xxSSPLPE,Hkxbz1HKvr,Learning Key Steps to Attack Deep Reinforcement Learning Agents,Reject,"This paper considers adversarial attacks in deep reinforcement learning, and specifically focuses on the problem of identifying key steps to attack. The paper poses learning these key steps as an RL problem with a cost for the attacker choosing to attack.

The reviewers agreed that this was an interesting problem setup, and the ability to learn these attacks without heuristics is promising. The main concern, which was felt was not adequately addressed in the rebuttals, was that the results need to be more than just competitive with heuristic approaches.

The fact that the attack ratio cannot be reliably changed, even with varying $\lambda$ still presents a major hurdle in the evaluation of the proposed method.

For the aforementioned reasons, I recommend rejecting this paper.",Paper Decision
Ap1QRvBcr4,rkllGyBFPH,Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks,Accept (Poster),"This paper studies the training of over-parameterized two-layer neural networks by considering high-order Taylor approximation, and randomizing the network to remove the first order term in the network’s Taylor expansion. This enables the neural network training go beyond the recently so-called neural tangent kernel (NTK) regime. The authors also established the optimization landscape, generalization error and expressive power results under the proposed analysis framework. They showed that when learning polynomials, the proposed randomized networks with quadratic Taylor approximation outperform standard NTK by a factor of the input dimension. This is a very nice work, and provides a new perspective on NTK and beyond. All reviewers are in support of accepting this paper. ",Paper Decision
WzCPPkpoUF,SklgfkSFPH,On PAC-Bayes Bounds for Deep Neural Networks using the Loss Curvature,Reject,"The paper computes an ""approximate"" generalization bound based on loss curvature. Several expert reviewers found a long list of issues, including missing related work and a sloppy mix of formal statements and heuristics, without proper accounting of what could be gleaned from some many heuristic steps. Ultimately, the paper needs to be rewritten and re-reviewed. ",Paper Decision
2xmVSfCFEQ,HyeJf1HKvS,Deep Graph Matching Consensus,Accept (Poster),"The paper proposed an end-to-end network architecture for graph matching problems, where first a GNN is applied to compute the initial soft correspondence, and then a message passing network is applied to attempt to resolve structural mismatch. The reviewers agree that the second component (message passing) is novel, and after the rebuttal period, additional experiments were provided by the authors to demonstrate the effectiveness of this. Overall this is an interesting network solution for graph-matching, and would be a worthwhile addition to the literature.",Paper Decision
lYwrjnk21,B1lJzyStvS,Self-Supervised Learning of Appliance Usage,Accept (Poster),"Authors proposed a multi-modal unsupervised algorithm to uncover the electricity usage of different appliances in a home. The detection of appliance was done by using both combined electricity consumption data and user location data from sensors. The unit of detection was set to be a 25-second window centered around any electricity usage spike. Authors used a encoder/decode set up to model two different factors of usage: type of appliance and variety within the same appliance. This part of the model was trained by predicting actual consumption. Then only the type of appliance was used to predict the location of people in the house, which was also factored into appliance related and unrelated factors. Locations are represented as images to avoid complicated modeling of multiple people.

The reviewers were satisfied with the discussion after the authors, and therefore believe this work is of general interest to the ICLR community.",Paper Decision
0bhi5RGdN1,ryxC-kBYDS,Gaussian Conditional Random Fields for Classification,Reject,"Main content:

Blind review #2 summarizes it well:

The authors provide a method to modify GRFs to be used for classification. The idea is simple and easy to get through, the writing is clean. The method boils down to using a latent variable that acts as a ""pseudo-regressor"" that is passed through a sigmoid for classification. The authors then discuss learning and inference in the proposed model, and propose two different variants that differ on scalability and a bit on performance as well. The idea of using the \xi transformation for the lower bound of the sigmoid was interesting to me -- since I have not seen it before, its possible its commonly used in the field and hopefully the other reviewers can talk more about the novelty here. The empirical results are very promising, which is the main reason I vote for weak acceptance. I think the paper has value, albeit I would say its a bit weak on novelty, and I am not 100% convinced about the this conference being the right fit for this paper. The authors augment MRFs for classification and evaluate and present the results well. 

--

Discussion:

As blind review #1 points out:

Even from the experiments (including the new traffic one), it is unclear how much better the method is either because we don't know if the improvements are statistically significant and that in many of the results, unstructured models like RF or logistic regression are very competitive casting some doubt on whether these datasets were well suited for structured prediction.

--

This paper is a desk reject as review #2's points out that anonymity was broken by the inclusion of a code link that reveals the authorship, which is true as a simple search on the GitHub user ""andrijaster"" immediately brings us to https://arxiv.org/pdf/1902.00045.pdf which is a draft of this submission showing all author names.",Paper Decision
KtVpihTjd,rkgAb1Btvr,Fourier networks for uncertainty estimates and out-of-distribution detection,Reject,"This paper presents a new method for detecting out-of-distribution (OOD) samples.

A reviewer pointed out that the paper discovers an interesting finding and the addressed problem is important. On the other hand, other reviewers pointed out theoretical/empirical justifications are limited. 

In particular, I think that experimental supports why the proposed method is superior beyond the existing ones are limited. I encourages the authors to consider more scenarios of OOD detection (e.g., datasets and architectures) and more baselines as the problem of measuring the confidence of neural networks or detecting outliers have rich literature. This would guide more comprehensive understandings on the proposed method.

Hence, I recommend rejection.

",Paper Decision
hEnlPzmiLk,Syxp-1HtvB,Semantic Hierarchy Emerges in the Deep Generative Representations for Scene Synthesis,Reject,"The paper proposes to study what information is encoded in different layers of StyleGAN.  The authors do so by training classifiers for different layers of latent codes and investigating whether changing the latent code changes the generated output in the expected fashion.

The paper received borderline reviews with two weak accepts and one weak reject.  Initially, the reviewers were more negative (with one reject, one weak reject, and one weak accept).  After the rebuttal, the authors addressed most of the reviewer questions/concerns.  

Overall, the reviewers thought the results were interesting and appreciated the care the authors took in their investigations.  The main concern of the reviewers is that the analysis is limited to only StyleGAN.  It would be more interesting and informative if the authors applied their methodology to different GANs.  Then they can analyze whether the methodology and findings holds for other types of GANs as well. R1 notes that given the wide interest in StyleGAN-like models, the work maybe of interest to the community despite the limited investigation.  The reviewers also point out the writing can be improved to be more precise.

The AC agrees that the paper is mostly well written and well presented.  However, there are limitations in what is achieved in the paper and it would be of limited interest to the community.  The AC recommends that the authors consider improving their work, potentially broadening their investigation to other GAN architectures, and resubmit to an appropriate venue.",Paper Decision
ABEOL8rgIy,Hygab1rKDS,Quantum Algorithms for Deep Convolutional Neural Networks,Accept (Poster),"Four reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission. Especially, the authors should take care to make this paper accessible (understandable) to the ML community as ICLR is a ML venue (rather than quantum physics one). Failure to do so will likely discourage the generosity of reviewers toward this type of submissions in the future.",Paper Decision
2oj_M5PBi8,SyxTZ1HYwB,TWO-STEP UNCERTAINTY NETWORK FOR TASKDRIVEN SENSOR PLACEMENT,Reject,"This paper proposes a sensor placement strategy based on maximising the information gain. Instead of using Gaussian process, the authors apply neural nets as function approximators. A limited empirical evaluation is performed to assess the performance of the proposed strategy. 
The reviewers have raised several major issues, including the lack of novelty, clarity, and missing critical details in the exposition. The authors didn’t address any of the raised concerns in the rebuttal. I will hence recommend rejection of this paper.",Paper Decision
Ai-5rY9iPU,Skxn-JSYwr,EXPLOITING SEMANTIC COHERENCE TO IMPROVE PREDICTION IN SATELLITE SCENE IMAGE ANALYSIS: APPLICATION TO DISEASE DENSITY ESTIMATION,Reject,"This papers proposed a solution to the problem of disease density estimation using satellite scene images.  The method combines a classification and regression task.  The reviewers were unanimous in their recommendation that the submission not be accepted to ICLR.  The main concern was a lack of methodological novelty.  The authors responded to reviewer comments, and indicated a list of improvements that still remain to be done indicating that the paper should at least go through another review cycle.",Paper Decision
5FLyA-lhT,ryghZJBKPS,"Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds",Accept (Talk),"The paper provides a simple method of active learning for classification using deep nets. The method is motivated by choosing examples based on an embedding computed that represents the last layer gradients, which is shown to have a connection to a lower bound of model change if labeled. The algorithm is simple and easy to implement. The method is justified by convincing experiments. 

The reviewers agree that the rebuttal and revisions cleared up any misunderstandings.

This is a solid empirical work on an active learning technique that seems to have a lot of promise. Accept. 
",Paper Decision
70w3ljNMpb,B1eibJrtwr,Abstractive Dialog Summarization with Semantic Scaffolds,Reject,"This paper proposes an approach for abstractive summarization of multi-domain dialogs, called SPNet, that incrementally builds on previous approaches such as pointer-generator networks. SPNet also separately includes speaker role, slot and domain labels, and is evaluated against a new metric, Critical Information Completeness (CIC), to tackle issues with ROUGE. The reviewers suggested a set of issues, including the meaningfulness of the task, incremental nature of the work and lack of novelty, and consistency issues in the write up. Unfortunately authors did not respond to the reviewer comments. I suggest rejecting the paper.",Paper Decision
AZc4_CBOt,SklibJBFDB,Evaluating Semantic Representations of Source Code,Reject,"This paper presents a dataset to evaluate the quality of embeddings learnt for source code. The dataset consists of three different subtasks: relatedness, similarity, and contextual similarity. The main contribution of the paper is the construction of these datasets which should be useful to the community. However, there are valid concerns raised about the size of the datasets (which is pretty small) and the baselines used to evaluate the embeddings -- there should be a baselines using a contextual embeddings model like BERT which could have been fine-tuned on the source code data. If these comments are addressed, the paper can be a good contribution in an NLP conference. As of now, I recommend a Rejection.",Paper Decision
t1euIZwFz2,rJxqZkSFDB,Searching to Exploit Memorization Effect in Learning from Corrupted Labels,Reject,"This paper develops a method for sample selection that exploits the memorization effect. While the paper has been substantially improved from its original form, the paper still does not meet the quality bar of ICLR in terms of presentation of the results and experimental validation. The paper will benefit from a revision and resubmission to another venue.",Paper Decision
B7ZbWZM_af,SJeY-1BKDS,"Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness",Accept (Poster),"Main content:

Blind review #3 summarizes it well:

This paper presents results on Dictionary Learning through l4 maximization. The authors base this paper heavily off of the formulation and algorithm in Zhai et. al. (2019) ""Complete dictionary learning via l4-norm maximization over the orthogonal group"". The paper draws connections between complete dictionary learning, PCA, and ICA by pointing out similarities between the objectives functions that are optimized as well as the algorithms used. The paper further presents results on dictionary learning in the presence of different types of noise (AWGN, sparse corruptions, outliers) and show that the l4 objective is robust to different types of noise. Finally the authors apply different types of noise to synthetic and real images and show that the dictionaries that they learn are robust to the noise applied.

--

Discussion:

Reviews agree about the interesting work, including the connections of complete dictionary learning with classic PCA and ICA (after further clarification during the rebuttal period). Additional empirical strengthening during the rebuttal period also addressed a reviewer concern.

--

Recommendation and justification:

As review #3 wrote, ""Overall this paper makes significant contributions by extending the work in [Zhai et. al's (2019) ""Complete dictionary learning via l4-norm maximization over the orthogonal group""] to noisy dictionary learning settings"".",Paper Decision
4SVNjdLizJ,BygKZkBtDH,Balancing Cost and Benefit with Tied-Multi Transformers,Reject,"The paper proposed a method for training multiple transformers with tied parameters and enabling dynamic choice of the number of encoder and decoder layers. The method is evaluated in neural machine translation and shown to reduce decoding costs without compromising translation quality. The reviewers generally agreed that the proposed method is interesting, but raised issues regarding the significance of the claimed benefits and the quality of overall presentation of the paper. Based on a consensus reached in a post rebuttal discussion with the reviewers, I am recommending rejecting this paper.",Paper Decision
RhnCK9BuCm,ryxPbkrtvr,BOSH: An Efficient Meta Algorithm for Decision-based Attacks,Reject,"This paper proposes BOSH-attack, a meta-algorithm for decision-based attack, where a model that can be accessed only via label queries for a given input is attacked by a minimal perturbation to the input that changes the predicted label. BOSH improves over existing local update algorithms by leveraging Bayesian Optimization (BO) and Successive Halving (SH). It has valuable contributions. But various improvements as detailed in the review comments can be made to further strength the manuscript.",Paper Decision
y8aDfpoSc_,rJgDb1SFwB,MGP-AttTCN: An Interpretable Machine Learning Model for the Prediction of Sepsis,Reject,"The problem of introducing interpretability into sepsis prediction frameworks is one that I find a very important contribution, and I personally like the ideas presented in this paper. However, there are two reviewers, who have experience at the boundary of ML and HC, who are flagging this paper as currently not focusing on the technical novelty, and explaining the HC application enough to be appreciated by the ICLR audience. As such my recommendation is to edit the exposition so that it more appropriate for a general ML audience, or to submit it to an ML for HC meeting. Great work, and I hope it finds the right audience/focus soon. ",Paper Decision
5I3IDOFP6,rkgIW1HKPB,Unsupervised Representation Learning by Predicting Random Distances,Reject,"The reviewers agree that this is an interesting paper but it required major modifications. After rebuttal, thee paper is much improved but unfortunately not above the bar yet. We encourage the authors to iterate on this work again.",Paper Decision
SG32SGRk3R,ryg8WJSKPr,ConQUR: Mitigating Delusional Bias in Deep Q-Learning,Reject,"While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.

Concerns raised included the need for better motivation of the practicality of the approach, versus its computational cost. The need for improved evaluations was also raised.",Paper Decision
vfX6knnaa,BkgHWkrtPB,Where is the Information in a Deep Network?,Reject,"This paper is full of ideas. However, a logical argument is only as strong as its weakest link, and I believe the current paper has some weak links. For example, the attempt to tie the behavior of SGD to free energy minimization relies on unrealistic approximations. Second, the bounds based on limiting flat priors become trivial. The authors in-depth response to my own review was much appreciated, especially given its last minute appearance. Unfortunately, I was not convinced by the arguments. In part, the authors argue that the logical argument they are making is not sensitive to certain issues that I raised, but this only highlights for me that the argument being made is not very precise.  I can imagine a version of this work with sharper claims, built on clearly stated assumptions/conjectures about SGD's dynamics, RATHER THAN being framed as the consequences of clearly inaccurate approximations. The behavior of diffusions can be presented as evidence that the assumptions/conjectures (that cannot be proven at the moment, but which are needed to complete the logical argument) are reasonable. However, I am also not convinced that it is trivial to do this, and so the community must have a chance to review a major revision.",Paper Decision
-gdco-XJyV,H1gHb1rFwr,Extreme Values are Accurate and Robust in Deep Networks,Reject,"This manuscript proposed biologically-inspired modifications to convolutional neural networks including differences of Gaussians convolutional filter, a truncated ReLU, and a modified projected normalization layer. The authors' results indicate that the modifications improve performance as well as improved robustness to adversarial attacks.

The reviewers and AC agree that the problem studied is timely and interesting, and closely related to a variety of recent work on robust model architectures. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and importance of the results. In reviews and discussion, the reviewers noted issues with clarity of the presentation and sufficient justification of the approach and results. In the opinion of the AC,  the manuscript in its current state is borderline and could be improved with more convincing empirical justification.",Paper Decision
H1j5_z_qfo,BJlrZyrKDB,Statistically Consistent Saliency Estimation,Reject,"This submission proposes a statistically consistent saliency estimation method for visual model explainability.

Strengths:
-The method is novel, interesting, and passes some recently proposed sanity checks for these methods.

Weaknesses:
-The evaluation was flawed in several aspects.
-The readability needed improvement.

After the author feedback period remaining issues were:
-A discussion of two points is missing: (i) why are these models so sensitive to the resolution of the saliency map? How does the performance of LEG change with the resolution (e.g. does it degrade for higher resolution?)? (ii) Figure 6 suggests that SHAP performs best at identifying ""pixels that are crucial for the predictions"". However, the authors use Figure 7 to argue that LEG is better at identifying salient ""pixels that are more likely to be relevant for the prediction"". These two observations are contradictory and should be resolved.
-The evaluation is still missing some key details for interpreting the results. For example, how representative are the 3 images chosen in Figure 7? Also, in section 5.1 the authors don't describe how many images are included in their sanity check analysis or how those images were chosen.
-The new discussion section is not actually a discussion section but a conclusion/summary section.

Because of these issues, AC believes that the work is theoretically interesting but has not been sufficiently validated experimentally and does not give the reader sufficient insight into how it works and how it compares to other methods. Note also that the submission is also now more than 9 pages long, which requires that it be held to a higher standard of acceptance.

Reviewers largely agreed with the stated shortcomings but were divided on their significance.
AC shares the recommendation to reject.",Paper Decision
rcRPTcLTD_,HylNWkHtvB,Domain-Independent Dominance of Adaptive Methods,Reject,"This paper proposes an adaptive gradient method for optimization in deep learning called AvaGrad.  The authors argue that AvaGrad greatly simplifies hyperparameter search (over e.g. ADAM) and demonstrate competitive performance on benchmark image and text problems.  In thorough reviews, thorough author response and discussion by the reviewers (which are are all appreciated) a few concerns about the work came to light and were debated.  One reviewer was compelled by the author response to raise their recommendation to weak accept.  However, none of the reviewers felt strongly enough to champion the paper for acceptance and even the reviewer assigning the highest score had reservations.  A major issue of debate was the treatment of hyperparameters, i.e. that the authors tuned hyperparameters on a smaller problem and then assumed these would extrapolate to larger problems. In a largely empirical paper this does seem to be a significant concern.  The space of adaptive optimizers for deep learning is a crowded one and thus the empirical (or theoretical) burden of proof of superiority is high.  The authors state regarding a concurrent submission: ""when hyperparameters are properly tuned, echoing our results on this matter"", however, it seems that the reviewers disagree that the hyperparameters are indeed properly tuned in this paper.  It's due to these remaining reservations that the recommendation is to reject.  ",Paper Decision
HnAMBCWpHq,ByeVWkBYPH,Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors ,Reject,"Quoting from R3: ""This paper proposes and analyzes a new loss function for linear autoencoders (LAEs) whose minima directly recover the principal components of the data. The core idea is to simultaneously solve a set of MSE LAE problems with tied weights and increasingly stringent masks on the encoder/decoder matrices.""  With two weak acceptance recommendations and a recommendation for rejection, this paper is borderline in terms of its scores.

The approach and idea are interesting.  The main shortcoming of the paper, as highlighted by the reviewers, is that the approach and theoretical analysis are not properly motivated to solve an actual problem faced in real-world data.  The approach does not provide a better algorithm for recovering the eigenvectors of the data, nor is it proposed as part of a learning framework to solve a real-world problem.  Experiments are shown on synthetic data and MNIST.  As a stand-alone theoretical result, it leaves open questions as to the proposed utility.",Paper Decision
RgxhNQxYQn,ryxmb1rKDS,Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control,Accept (Poster),"This paper proposes a novel method for learning Hamiltonian dynamics from data. The data is obtained from systems subjected to an external control signal. The authors show the utility of their method for subsequent improved control in a reinforcement learning setting. The paper is well written, the method is derived from first principles, and the experimental validation is solid. The authors were also able to take into account the reviewers’ feedback and further improve their paper during the discussion period. Overall all of the reviewers agree that this is a great contribution to the field and hence I am happy to recommend acceptance.",Paper Decision
feJeNRVyd,Syx7WyBtwB,Interpretations are useful: penalizing explanations to align neural networks with prior knowledge,Reject,"The paper contains interesting ideas for giving simple explanations to a NN; however, the reviewers do not feel the contribution is sufficiently novel to merit acceptance.",Paper Decision
S5FRMZhf59,BygzbyHFvB,FreeLB: Enhanced Adversarial Training for Natural Language Understanding,Accept (Spotlight),"The paper proposes a new algorithm for adversarial training of language models.  This is an important research area and the paper is well presented, has great empirical results and a novel idea. ",Paper Decision
7x_6G9OVWG,rygf-kSYwH,Behaviour Suite for Reinforcement Learning,Accept (Spotlight),"This paper proposes a platform for benchmarking and evaluating reinforcement learning algorithms.  While reviewers had some concerns about whether such a tool was necessary given existing tools, reviewers who interacted with the tool found it easy to use and useful. Making such tools is often an engineering task and rarely aligned with typical research value systems, despite potentially acting as a public good. The success or failure of similar tools rely on community acceptance and it is my belief that this tool surpasses the bar to be promoted to the community at a top tier venue.  ",Paper Decision
9vWLSWYvh1,HJlWWJSFDH,Strategies for Pre-training Graph Neural Networks,Accept (Spotlight),All three reviewers are consistently positive on this paper. Thus an accept is recommended.,Paper Decision
eR3twRotB_,S1eWbkSFPS,"GRAPHS, ENTITIES, AND STEP MIXTURE",Reject,Two reviewers are concerned about this paper while the other one is slightly positive. A reject is recommended.,Paper Decision
HU6BjGlvDl,rkglZyHtvH,Refining the variational posterior through iterative optimization,Reject,"In this paper a method for refining the variational approximation is proposed.

The reviewers liked the contribution but a number reservations such as missing reference made the paper drop below the acceptance threshold. The authors are encouraged to modify paper and send to next conference.

Reject. ",Paper Decision
PvcZsr_j-,B1xeZJHKPB,Aggregating explanation methods for neural networks stabilizes explanations,Reject,"This paper describes a new method for explaining the predictions of a CNN on a particular image. The method is based on aggregating the explanations of several methods.   They also describe a new method of evaluating explanation methods which avoids manual evaluation of the explanations.

However, the most critical reviewer questions the contribution of the proposed method, which is simple.  Simple isn't always a bad thing, but I think here the reviewer has a point.  The new method for evaluating explanation methods is interesting, but the sample images given are also very simple -- how does the method work when the image is cluttered?   How about when the prediction is uncertain or wrong?",Paper Decision
c9TwXMtqy4,Byl1W1rtvH,Recurrent Hierarchical Topic-Guided Neural Language Models,Reject,"This paper was a very difficult case. All three original reviewers of the paper had never published in the area, and all of them advocated for acceptance of the paper. I, on the other hand, am an expert in the area who has published many papers, and I thought that while the paper is well-written and experimental evaluation is not incorrect, the method was perhaps less relevant given current state-of-the-art models. In addition, the somewhat non-standard evaluation was perhaps causing this fact to be masked. I asked the original reviewers to consider my comments multiple times both during the rebuttal period and after, and unfortunately none of them replied.

Because of this, I elicited two additional reviews from people I knew were experts in the field. The reviews are below. I sent the PDF to the reviewers directly, and asked them to not look at the existing reviews (or my comments) when doing their review in order to make sure that they were making a fair assessment. 

Long story short, Reviewer 4 essentially agreed with my concerns and pointed out a few additional clarity issues. Reviewer 5 pointed out a number of clarity issues and was also concerned with the fact that d_j has access to all other sentences (including those following the current sentence). I know that at the end of Section 2 it is noted that at test time d_j only refers to previous sentences, but if so there is also a training-testing disconnect in model training, and it seems that this would hurt the model results.

Based on this, I have decided to favor the opinions of three experts (me and the two additional reviewers) over the opinions of the original three reviewers, and not recommend the paper for acceptance at this time. In order to improve the paper I would suggest the following (1) an acknowledgement of standard methods to incorporate context by processing sequences consisting of multiple sentences simultaneously, (2) a more thorough comparison with state-of-the-art models that consider cross-sentential context on standard datasets such as WikiText or PTB. I would encourage the authors to consider this as they revise their paper.

Finally, I would like to apologize to the authors that they did not get a chance to reply to the second set of reviews. As I noted above, I did try to make my best effort to encourage discussion during the rebuttal period.",Paper Decision
7C_BgXicy,BJgkbyHKDS,Invertible generative models for  inverse problems: mitigating representation error and dataset bias,Reject,"This paper studies the empirical performance of invertible generative models for compressive sensing, denoising and in painting. One issue in using generative models in this area has been that they hit an error floor in reconstruction due to model collapse etc i.e. one can not achieve zero error in reconstruction. The reviewers raised some concerns about novelty of the approach and thoroughness of the empirical studies. The authors response suggests that they are not claiming novelty w.r.t. to the approach but rather their use in compressive techniques. My own understanding is that this error floor is a major problem and removing its effect is a good contribution even without any novelty in the techniques. However,  I do agree that a more thorough empirical study would be more convincing. While I can not recommend acceptance given the scores I do think this paper has potential and recommend the authors to resubmit to a future venue after a through revision.",Paper Decision
4Els73P1kQ,HJxyZkBKDr,NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search,Accept (Spotlight),This paper presents a new benchmark for architecture search. Reviewers put this paper in the top tier. I encourage the authors to also cite https://openreview.net/forum?id=SJx9ngStPH in their final version. ,Paper Decision
L-SEjQfizT,BkgRe1SFDS,Learning World Graph Decompositions To Accelerate Reinforcement Learning,Reject,"This paper introduces an approach for structured exploration based on graph-based representations.  While a number of the ideas in the paper are quite interesting and relevant to the ICLR community, the reviewers were generally in agreement about several concerns, which were discussed after the author response. These concerns include the ad-hoc nature of the approach, the limited technical novelty, and the difficulty of the experimental domains (and whether the approach could be applied to a more general class of challenging long-horizon problems such as those in prior works). Overall, the paper is not quite ready for publication at ICLR.",Paper Decision
23KomIHTGN,H1gCeyHFDS,Gram-Gauss-Newton Method: Learning Overparameterized Neural Networks for Regression Problems,Reject,"The article considers Gauss-Newton as a scalable second order alternative to train neural networks, and gives theoretical convergence rates and some experiments. The second order convergence results rely on the NTK and very wide networks. The reviewers pointed out that the method is of course not new, and suggested that comparison not only with SGD but also with methods such as Adam, natural gradients, KFAC, would be important, as well as additional experiments with other types of losses for classification problems and multidimensional outputs. The revision added preliminary experiments comparing with Adam and KFAC. Overall, I think that the article makes an interesting and relevant case that Gauss-Newton can be a competitive alternative for parameter optimization in neural networks. However, the experimental section could still be improved significantly. Therefore, I am recommending that the paper is not accepted at this time but revised to include more extensive experiments. 
",Paper Decision
RhO3c7PXhF,H1laeJrKDB,Controlling generative models with continuous factors of variations,Accept (Poster),"Following the revision and the discussion, all three reviewers agree that the paper provides an interesting contribution to the area of generative image modeling. Accept.",Paper Decision
qUy70mxnWz,SkxpxJBKwS,Emergent Tool Use From Multi-Agent Autocurricula,Accept (Spotlight),"This paper describes how multi-agent reinforcement learning at scale leads to the evolution of complex behaviors. Actually, ""at scale"" may be an understatement - a lot of computing power was used here. But the amount of compute used is not the point, rather the point is that complex and fascinating behavior can emerge from a long co-evolutionary process (though gradient-based RL is used here, the principle is the same) where the arms race forms an implicit curriculum. This is the existence proof that people in artificial life and adaptive behavior have been looking for for so long. 

Two reviewers were positive about the paper, with a third being negative because the paper does not give any new insights about how to do RL at scale. But that was not the stated aim of the paper, as the authors clarify in a response.

This paper will draw quite some attention and deserves an oral presentation.",Paper Decision
Rp7TWiM25,S1e3g1rtwB,The fairness-accuracy landscape of neural classifiers,Reject,"This manuscript investigates and characterizes the tradeoff between fairness and accuracy in neural network models. The primary empirical contribution is to investigate this tradeoff for a variety of datasets.

The reviewers and AC agree that the problem studied is timely and interesting. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty of the results. IN particular, it is not clear that the idea of a fairness/performance tradeoff is a new one. In reviews and discussion, the reviewers also noted issues with clarity of the presentation. In the opinion of the AC, the manuscript is not appropriate for publication in its current state. ",Paper Decision
aX7CipBXv7,Hke3gyHYwH,Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee,Accept (Poster),"This paper studies the effect of various regularization techniques for dealing with noisy labels. In particular the authors study various regularization techniques such as distance from initialization to mitigate this effect. The authors also provide theory in the NTK regime. All reviewers have positive assessment about the paper and think is clearly written with nice contributions but do raise some questions about novelty given that it mostly follows the normal NTK regime. I agree that the paper is nicely written and well-motivated. I do not think the theory developed here fully captures all the nuances of practical observations in this problem. In particular, with label noise this theory suggests that test performance is not dramatically affected by label noise when using regularization or early stopping where as in practice what has been observed (and even proven in some cases) is that the performance is completely unaffected with small label noise. I think this paper is a good addition to ICLR and therefore recommend acceptance but recommend the authors to more clearly articulate the above nuances and limitations of their theory in the final manuscript.  ",Paper Decision
CZAGa3NSC,rJlnxkSYPS,Unsupervised Clustering using Pseudo-semi-supervised Learning,Accept (Poster),"The authors addressed the issues raised by the reviewers, so I suggest the acceptance of this paper.",Paper Decision
B_JqXFdQ8H,rygixkHKDH,Geometric Analysis of Nonconvex Optimization Landscapes for Overcomplete Learning,Accept (Talk),"This paper investigates the use non-convex optimization for two dictionary learning problems, i.e., over-complete dictionary learning and convolutional dictionary learning. The paper provides theoretical results, associated with empirical experiments, about the fact that, that when formulating the problem as an l4 optimization, gives rise to a landscape with strict saddle points and as such, they can be escaped with negative curvature. As a result, descent methods can be used for learning with provable guarantees. All reviews found the work extremely interesting, highlighting the importance of the results that constitute ""a solid improvement over the prior understandings on over-complete DL"" and ""extends our understanding of provable methods for dictionary learning"". This is an interesting submission on non-convex optimization, and as such of interest to the ML community of ICLR . I'm recommending this work for acceptance.",Paper Decision
4-vfWjTydh,rkecl1rtwB,PairNorm: Tackling Oversmoothing in GNNs,Accept (Poster),"The paper proposes a way to tackle oversmoothing in Graph Neural Networks. The authors do a good job of motivating their approach, which is straightforward and works well. The paper is well written and the experiments are informative and well carried out. Therefore, I recommend acceptance. Please make suree thee final version reflects the discussion during the rebuttal.",Paper Decision
RDDEBlKs3P,S1ltg1rFDS,Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning,Accept (Poster),"This paper addresses an important and relevant problem in reinforcement learning: learning from off-policy data, taking into account the offsets in the visitation distribution of states. This has the promise of lowering variance even with long horizon roll-outs. Existing methods have required access to the behavior policy (or have required data from the stationary distribution). The novel proposed approach instead uses an alternative method, based on the fixed point of the ""backward flow"" operator, to calculate the importance ratios required for policy evaluation in discrete and continuous environments. 

In the initial version of the submission, several concerns were expressed regarding both the quality of the paper and clarity. The authors have updated the paper to address these concerns to the satisfaction of the reviewers, who are now unanimously in favor of acceptance. ",Paper Decision
z4Qv0-YCbu,SkeFl1HKwr,Empirical Studies on the Properties of Linear Regions in Deep Neural Networks,Accept (Poster),"This paper studies the properties of regions where a DNN with piecewise linear activations behaves linearly. They develop a variety of techniques to chracterize properties and show how these properties correlate with various parameters of the network architecture and training method.

The reviewers were in consensus on the quality of the paper: The paper is well written and contains a number of insights that would be of broad interest to the deep learning community.

I therefore recommend acceptance.",Paper Decision
3l0RPCKgnw,rJxtgJBKDr,SNOW: Subscribing to Knowledge via Channel Pooling for Transfer & Lifelong Learning of Convolutional Neural Networks,Accept (Poster),"This paper proposes a method, SNOW, for improving the speed of training and inference for transfer and lifelong learning by subscribing the target delta model to the knowledge of source pretrained model via channel pooling.

Reviewers and AC agree that this paper is well written, with simple but sound technique towards an important problem and with promising empirical performance. The main critique is that the approach can only tackle transfer learning while failing in the lifelong setting. Authors provided convincing feedbacks on this key point. Details requested by the reviewers were all well addressed in the revision.

Hence I recommend acceptance.",Paper Decision
9ckjYt9BJ,HJeOekHKwr,Smoothness and Stability in GANs,Accept (Poster),"The paper provides a theoretical study of what regularizations should be used in GAN training and why. The main focus is that the conditions on the discriminator that need to be enforced, to get the Lipshitz property of the corresponding function that is optimized for the generator. Quite a few theorems and propositions are provided. As noted by Reviewer3, this adds insight to well-known techniques: the Reviewer1 rightfully notes that this does not lead to any practical conclusion. 
Moreover, then training of GANs never goes to the optimal discriminator, that could be a weak point; rather than it proceeds in the alternating fashion, and then evolution is governed by the spectra of the local Jacobian (which is briefly mentioned). This is mentioned in future work, but it is not clear at all if the results here can be helpful (or can be generalized).
 At some point of the paper it gets to ""more theorems mode"" which make it not so easy and motivating to read. 
The theoretical results at the quantitative level are very interesting.  I have looked for a long time on Figure 1: does this support the claims? First my impression was it does not (there are better FID scores for larger learning rates). But in the end, I think it supports: the convergence for a smaller that $\gamma_0$ learning rate to the same FID indicated the convergence to the same local minima (probably). This is perfectly fine. Oscillations afterwards move us to a stochastic region, where FID oscillates. So, the theory has at least minor confirmation. 

",Paper Decision
d0nnx1hQdD,r1lOgyrKDS,Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation,Accept (Poster),"The paper presents a novel reinforcement learning-based algorithm for contextual sequence generation. Specifically, the paper presents experimental results on the application of the gradient ARSM estimator of Yin et al. (2019) to challenging structured prediction problems (neural program synthesis and image captioning). The method consists in performing correlated Monte Carlo rollouts starting from each token in the generated sequence, and using the multiple rollouts to reduce gradient variance. Numerical experiments are presented with promising performance. 

Reviewers were in agreement that this is a non-trivial extension of previous work with broad potential application. Some concerns about better framing of contributions were mostly resolved during the author rebuttal phase. Therefore, the AC recommends publication. ",Paper Decision
0hOd2-CaQ,BJewlyStDr,On Bonus Based Exploration Methods In The Arcade Learning Environment,Accept (Poster),"This paper presents a detailed comparison of different bonus-based exploration methods on a common evaluation framework (Rainbow) when used with the ATARI game suite. They find that while these bonuses help on Montezuma's Revenge (MR), they underperform relative to epsilon-greedy on other games. This suggests that architectural changes may be a more important factor than bonus-based exploration in recent advances on MR.

The reviewers commented that this paper makes no effort to present new techniques, and the insights discovered could be expanded on. Despite this, it is an interesting paper that is generally well argued and would be a useful contribution to the field. I recommend acceptance.",Paper Decision
xgKy_iXAt2,BkxDxJHFDr,Power up! Robust Graph Convolutional Network based on Graph Powering,Reject,"The paper identifies the limitation of graph neural networks and proposed new variants of graph neural works. However, the reviewers feel that the theory of the paper have some problems: 
1. A major concern is that the theoretical analyses in this paper are limited to graphs sampled from the SBM model. It is unclear how these analyses can be generalized to real graphs. 
2. The robustness definition is inconsistent. 
Furthermore, more extensive experiments on more datasets will also be helpful. ",Paper Decision
Knise7HamQ,ByeDl1BYvH,Global graph curvature,Reject,"This paper studies the problem of embedding graphs into continuous spaces.  The authors focus on determining the correct dimension and curvature to minimize distortion or a threshold loss of the embedding. The authors  consider a variety of existing notions of curvature for graphs, introduce a notion of global curvature for the entire graph, and how to efficiently compute it.

Reviewers were positive about the problem under study, but agreed that the current manuscript somewhat lacks a clear contribution. They also pointed out that the goal of using a global notion of curvature should be better motivated. For these reasons, the AC recommends rejection at this time. ",Paper Decision
GOXrTep2ho,BklIxyHKDr,Deep k-NN for Noisy Labels,Reject,"The paper proposed and analyze a k-NN method for identifying corrupted labels for training deep neural networks.

Although a reviewer pointed out that the noisy k-NN contribution is interesting, I think the paper can be much improved further due to the followings:

(a) Lack of state-of-the-art baselines to compare.
(b) Lack of important recent related work, i.e., ""Robust Inference via Generative Classifiers for Handling Noisy Labels"" from ICML 2019 (see https://arxiv.org/abs/1901.11300). The paper also runs a clustering-like algorithm for handling noisy labels, and the authors should compare and discuss why the proposed method is superior.
(c) Poor write-up, e.g., address what is missing in existing methods from many different perspectives as this is a quite well-studied popular problem.

Hence, I recommend rejection.",Paper Decision
Vgu2JZ1WMj,Skg8gJBFvr,Filling the Soap Bubbles: Efficient Black-Box Adversarial Certification with Non-Gaussian Smoothing,Reject,"The authors extend the framework of randomized smoothing to handle non-Gaussian smoothing distribution and use this to show that they can construct smoothed models that perform well against l2 and linf adversarial attacks. They show that the resulting framework can obtain state-of-the-art certified robustness results improving upon prior work.

While the paper contains several interesting ideas, the reviewers were concerned about several technical flaws and omissions from the paper:

1) A theorem on strong duality was incorrect in the initial version of the paper, though this was fixed in the rebuttal. However, the reasoning of the authors on the ""fundamental trade-off"" is specific to the particular framework they consider, and is not really a fundamental trade-off.

2) The justification for the new family of distributions constructed by the author is not very clear and the experiments only show marginal improvements over prior work. Thus, the significance of this contribution is not clear.

Some of the issues were clarified during the rebuttal, but the reviewers remained unconvinced about the above points.

Thus, the paper cannot be accepted in its current form.
",Paper Decision
UPsAqRDqP-,SyxBgkBFPS,Guided Adaptive Credit Assignment for Sample Efficient Policy Optimization,Reject,"The paper proposes a policy gradient algorithm related to entropy-regularized RL, that instead of the KL uses f-divergence to avoid mode collapse.

The reviewers found many technical issues with the presentation of the method, and the evaluation. In particular, the experiments are conducted on particular program synthesis tasks and show small margin improvements, while the algorithm is motivated by general sparse reward RL.

I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit an improved version elsewhere.",Paper Decision
Jx2yatJbi,r1eBeyHFDH,A Theory of Usable Information under Computational Constraints,Accept (Talk),All reviewers unanimously accept the paper.,Paper Decision
wBjXJnBrhM,BJlVeyHFwH,On the Invertibility of Invertible Neural Networks,Reject,"This submission analyses the numerical invertibility of analytically invertible neural networks and shows that analytical invertibility does not guarantee numerical invertibility of some invertible networks under certain conditions (e.g. adversarial perturbation).

Strengths:
-The work is interesting and the theoretical analysis is insightful.

Weaknesses:
-The main concern shared by all reviewers was the weakness of the experimental section including (i) insufficient motivation of the decorrelation task; (ii) missing comparisons and experimental settings.
-The paper clarity could be improved.

Both weaknesses were not sufficiently addressed in the rebuttal. All reviewer recommendations were borderline to reject.
",Paper Decision
U3sPdqwc7,SkeNlJSKvS,Shallow VAEs with RealNVP Prior Can Perform as Well as Deep Hierarchical VAEs,Reject,"This paper provides an interesting insight into the fitting of variational autoencoders.  While much of the recent literature focuses on training ever more expressive models, the authors demonstrate that learning a flexible prior can provide an equally strong model.  Unfortunately one review is somewhat terse.  Among the other reviews, one reviewer found the paper very interesting and compelling but did not feel comfortable raising their score to ""accept"" in the discussion phase citing a lack of compelling empirical results in compared to baselines.  Both reviewers were concerned about novelty in light of Huang et al., in which a RealNVP prior is also learned in a VAE.  AnonReviewer3 also felt that the experiments were not thorough enough to back up the claims in the paper.  Unfortunately, for these reasons the recommendation is to reject.  More compelling empirical results with carefully chosen baselines to back up the claims of the paper and comparison to existing literature (Huang et al) would make this paper much stronger.

",Paper Decision
wvxD7G-G4N,HJgEe1SKPr,GAN-based Gaussian Mixture Model Responsibility Learning,Reject,"This paper proposes to use GMM as the latent prior distribution of GAN. The reviewers unanimously agree that the paper is not well motivated, explanations are lacking and writing needs to be substantially improved. ",Paper Decision
oR6FABh8f3,BJlXgkHYvS,Information-Theoretic Local Minima Characterization and Regularization,Reject,"This paper proposes using the Fisher information matrix to characterize local minima of deep network loss landscapes to indicate generalizability of a local minimum. While the reviewers agree that this paper contains interesting ideas and its presentation has been substantially improved during the discussion period, there are still issues that remain unanswered, in particular between the main objective/claims and the presented evidence. The paper will benefit from a revision and resubmission to another venue.",Paper Decision
ySebnAYVww,BJg7x1HFvB,Well-Read Students Learn Better: On the Importance of Pre-training Compact Models,Reject," Though the reviewers thought the ideas in this paper were interesting, they questioned the importance and magnitude of the contribution.  Though it is important to share empirical results, the reviewers were not sure that there was enough for this paper to be accepted.",Paper Decision
FWywWnOTIf,BJeGlJStPr,IMPACT: Importance Weighted Asynchronous Architectures with Clipped Target Networks,Accept (Poster),"The authors propose a novel distributed reinforcement learning algorithm that includes 3 new components: a target network for the policy for stability, a circular buffer, and truncated importance sampling. The authors demonstrate that this improves performance while decreasing wall clock training time.

Initially, reviewers were concerned about the fairness of hyper parameter tuning, the baseline implementation of algorithms, and the limited set of experiments done on the Atari games. After the author response, reviewers were satisfied with all 3 of those issues.

I may have missed it, but I did not see that code was being released with this paper. I think it would greatly increase the impact of the paper at the authors release source code, so I strongly encourage them to do so.

Generally, all the reviewers were in consensus that this is an interesting paper and I recommend acceptance.",Paper Decision
ZG27KFOTGM,HkgMxkHtPH,UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING,Reject,"This paper proposed to improve the quality of underwater images, specifically color distortion and haze effect, by an unsupervised generative adversarial network (GAN). An end-to-end autoencoder network is used to demonstrate its effectiveness in comparing to existing works, while maintaining scene content structural similarity. Three reviewers unanimously rated weak rejection. The major concerns include unclear difference with respect to the existing works, incremental contribution, low quality of figures, low quality of writing, etc. The authors respond to Reviewers’ concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state.",Paper Decision
P-F2reYdG,r1lZgyBYwS,HiLLoC: lossless image compression with hierarchical latent variable models,Accept (Poster),"The paper proposes a lossless image compression consisting of a hierarchical VAE and using a bits-back version of ANS. Compared to previous work, the paper (i) improves the compression rate performance by adapting the discretization of latent space required for the entropy coder ANS (ii) increases compression speed by implementing a vectorized version of ANS (iii) shows that a model trained on a low-resolution imagenet 32 dataset can generalize its compression capabilities to higher resolution.

The authors addressed properly reviewers' concerns. Main critics which remain are (i) the method is not practical yet (long compression time) (ii) results are not state of the art - but the contribution is nevertheless solid.",Paper Decision
ixswxdHvoU,rJebgkSFDB,Learning to Learn Kernels with Variational Random Features,Reject,"The paper looks at meta learning using random Fourier features for kernel approximations. The idea is to learn adaptive kernels by inferring Fourier bases from related tasks that can be used for the new task. A key insight of the paper is to use an LSTM to share knowledge across tasks.

The paper tackles an interesting problem, and the idea to use a meta learning setting for transfer learning within a kernel setting is quite interesting. It may be worthwhile relating this work to this paper by Titsias et al. (https://arxiv.org/abs/1901.11356), which looks at a slightly different setting (continual learning with Gaussian processes, where information is shared through inducing variables).

Having read the paper, I have some comments/questions:
1. log-likelihood should be called log-marginal likelihood (wherever the ELBO shows up)
2. The derivation of the ELBO confuses me (section 3.1). First, I don't know whether this ELBO is at training time or at test time. If it was at training time, then I agree with Reviewer #1 in the sense that $p(\omega)$ should not depend on either $x$ or $\mathcal {S}$. If it is at test time, the log-likelihood term should not depend on $\mathcal{S}$ (which is the training set), because $\mathcal S$ is taken care of by $p(\omega|\mathcal S)$. However, critically, $p(\omega|\mathcal S)$ should not depend on $x$. I agree with Reviewer #1 that this part is confusing, and the authors' response has not helped me to diffuse this confusion (e.g., priors should not be conditioned on any data).
3. The tasks are indirectly represented by a set of basis functions, which are represented by $\omega^t$ for task $t$. In the paper, these tasks are then inferred using variational inference and an LSTM. It may be worthwhile relating this to the latent-variable approach by Saemundsson et al. (http://auai.org/uai2018/proceedings/papers/235.pdf) for meta learning. 
4. The expression ""meta ELBO"" is inappropriate. This is a simple ELBO, nothing meta about it. If we think of the tasks as latent variables (which the paper also states), this ELBO in equation (9) is a vanilla ELBO that is used in variational inference.
5. For the LSTM, does it make a difference how the tasks are ordered?
6. Experiments: Figure 3 clearly needs error bars, and MSEs need to be reported with error bars as well; 
6a) Figures 4 and 5 need error bars.
6b) Error bars should also be based on different random initializations of the learning procedure to evaluate the robustness of the methods (use at least 20 random seeds). I don't think any of the results is based on more than one random seed (at least I could not find any statement regarding this).
7. Table 1 and 2: The highlighting in bold is unclear. If it is supposed to highlight the best methods, then the highlighting is dishonest in the sense that methods, which perform similarly, are not highlighted. For example, in Table 1, VERSA or MetaVRF (w/o LSTM) could be highlighted for all tasks because the error bars are so huge (similar in Table 2).
8. One of the things I'm missing completely is a discussion about computational demand: How efficiently can we train the model, and how long does it take to make predictions? It would be great to have some discussion about this in the paper and relate this to other approaches. 
9. The paper evaluates also the effect of having an LSTM that correlates tasks in the posterior. The analysis shows that there are some marginal gains, but none of the is statistically significant. I would have liked to see much more analysis of the effect/benefit of the LSTM.

Summary: The paper addresses an interesting problem. However, I have reservations regarding some theoretical bits and regarding the quality of the evaluation. Given that this paper also exceeds the 8 pages (default) limit, we are supposed to ask for higher acceptance standards than for an 8-pages paper. Hence, putting everything together, I recommend to reject this paper.",Paper Decision
p3FCnyfAX,SkgWeJrYwr,Efficient Wrapper Feature Selection using Autoencoder and Model Based Elimination,Reject,"In this paper the authors propose a wrapper feature selection method that selects features based on 1) redundancy, i.e. the sensitivity of the downstream model to feature elimination, and 2) relevance, i.e. how the individual features impact the accuracy of the target task. The authors use a combination of the redundancy and relevance scores to eliminate the features. 

While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns that were viewed by AC as critical issues:
(1) all reviewers agreed that the proposed approach lacks theoretical justification or convincing empirical evaluations in order to show its effectiveness and general applicability -- see R1’s and R2’s requests for evaluation with more datasets/diverse tasks to assess the applicability and generality of the proposed model; see R1’s, R4’s concerns regarding theoretical analysis; 
(2) all reviewers expressed concerns regarding the technical issue of combining the redundancy and relevance scores -- see R4’s and R2’s concerns regarding the individual/disjoint calibration of scores; see R1’s suggestion to learn to reweigh the scores;
(3) experimental setup requires improvement both in terms of clarity of presentation and implementation -- see R1’s comment regarding the ranker model, see R4’s concern regarding comparison with a standard deep learning model that does feature learning for a downstream task; both reviewers also suggested to analyse how autoencoders with different capacity could impact the results.
Additionally R1 raised a concern regarding relevant recent works that were overlooked. 
The authors have tried to address some of these concerns during rebuttal, but an insufficient empirical evidence still remains a critical issue of this work. To conclude, the reviewers and AC suggest that in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.
",Paper Decision
CgcRRDdKTd,r1gelyrtwH,Physics-aware Difference Graph Networks for Sparsely-Observed Dynamics,Accept (Poster),"All reviewers agree that this research is novel and well carried out, so this is a clear accept. Please ensure that the final version reflect the reviewer comments and the new information provided during the rebuttal",Paper Decision
9fIYwx6fH-,HygegyrYwH,Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks,Accept (Poster),"This paper studies how much overparameterization is required to achieve zero training error via gradient descent in one hidden layer neural nets. In particular the paper studies the effect of margin in data on the required amount of overparameterization. While the paper does not improve in the worse case in the presence of margin the paper shows that sometimes even logarithmic width is sufficient. The reviewers all seem to agree that this is a nice paper but had a few mostly technical concerns. These concerns were sufficiently addressed in the response. Based on my own reading I also find the paper to be interesting, well written with clever proofs. So I recommend acceptance. I would like to make a suggestion that the authors do clarify in the abstract intro that this improvement can not be achieved in the worst case as a shallow reading of the manuscript may cause some confusion (that logarithmic width suffices in general).",Paper Decision
AYB0xheCUL,Hke1gySFvB,Enhancing Language Emergence through Empathy,Reject,"This paper introduces the idea of ""empathy"" to improve learning in communication emergence. The reviewers all agree that the idea is interesting and well described. However, this paper clearly falls short on delivering the detailed and sufficient experiments and results to demonstrate whether and how the idea works.

I thank the authors for submitting this research to ICLR and encourage following up on the reviewers' comments and suggestions for future submission. ",Paper Decision
MIj-EvXKF2,B1eCk1StPH,The Generalization-Stability Tradeoff in Neural Network Pruning,Reject,"The authors introduce a notion of stability to pruning and argue through empirical evaluation that pruning leads to improved generalization when it introduces instability. The reviewers were largely unconvinced, though for very different reasons. The idea that ""Bayesian ideas"" explain what's going on seems obviously wrong to me. The third reviewer seems to think there's a tautology lurking here and that doesn't seem to be true to me either. It is disappointing that the reviewers did not re-engage with the authors after the authors produced extensive rebuttals. Unfortunately, this is a widespread pattern this year. 

Even though I'm inclined to ignore aspects of these reviews, I feel that there needs to be a broader empirical study to confirm these findings. In the next iteration of the paper, I believe it may also be important to relate these ideas to [1]. It would be interesting to compare also on the networks studied in [1], which are more diverse.


[1] The Lottery Ticket Hypothesis at Scale (Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin) https://arxiv.org/abs/1903.01611",Paper Decision
jCnKEzykEE,HklCk1BtwS,Word embedding re-examined: is the symmetrical factorization optimal?,Reject,"The paper studies word embeddings using the matrix factorization framework introduced by Levy et al 2015. The authors provide a theoretical explanation for how the hyperparameter alpha controls the distance between words in the embedding and a method to estimate the optimal alpha.  The authors also provide experiments showing the alpha found using their method is close to the alpha that gives the highest performance on the word-similarity task on several datasets. 

The paper received 2 weak rejects and 1 weak accept.  The reviews were unchanged after the rebuttal, with even the review for weak accept (R2) indicating that they felt the submission to be of low quality.  Initially, reviewers commented that while the work seemed solid and provided insights into the problem of learning word embeddings, the paper needed to improve their positioning with respect to prior work on word embeddings and add missing citations.  In the revision, the authors improved the related work, but removed the conclusion.

The current version of the paper is still low quality and has the following issues
1. The paper exposition still needs improvement and it would benefit from another review pass
Following R3's suggestions, the authors have made various improvements to the paper, including modifying the terminology and contextualizing the work.  However, as R3 suggests, the paper still needs more rewriting to clearly articulate the contribution and how it relates to prior work throughout the paper.  In addition, the conclusion was removed and the paper still needs an editing pass as there are still many language/grammar issues.

Page 5: ""inherites"" -> ""inherits""
Page 5: ""top knn"" -> ""top k""

2. More experimental evaluation is needed.
For instance, R1 suggested that the authors perform additional experiments on other tasks (e.g. NER, POS Tagging).  The authors indicated that this was not a focus of their work as other works have already looked at the impact of alpha on other task.  While prior works has looked at the correlation of alpha vs performance on the task, they have not looked at whether alpha estimated the method proposed by the author will give good performance on these tasks as well.  Including such analysis will make this a stronger paper.

Overall, there are some promising elements in the paper but the quality of the paper needs to be improved.  The authors are encouraged to improve the paper by adding more experimental evaluation on other tasks, improving the writing, as well as incorporating other reviewer comments and resubmit to an appropriate venue. 
",Paper Decision
PpdNEoBky,BJeRykBKDH,Empowering Graph Representation Learning with Paired Training and Graph Co-Attention,Reject,"The paper proposes combining paired attention with co-attention. The reviewers have remarked that the paper is will written and that the experiments provide some new insights into this combination. Initially, some additional experiments were proposed, which were addressed by the authors in the rebuttal and the new version of the paper. However, ICLR is becoming a very competitive conference where novelty is an important criteria for acceptance, and unfortunately the paper was considered to lack the novelty to be presented at ICLR.",Paper Decision
9WcmfWTJKn,Bke61krFvS,Learning representations for binary-classification without backpropagation,Accept (Poster),"This paper provides a rigorous analysis of feedback alignment under two restrictions 1) that all, except the first, layers are constrained to realize monotone functions and 2) the task is binary classification. Overall, all reviewers agree that this is an interesting submission providing important results on the topic and as such all agree that it should feature at the ICLR program. Thus, I recommend acceptance. However, I ask the authors to take into account the reviewers' concerns and include a discussion about limitations (and general applicability) of this work.",Paper Decision
10I3AhtDV,BylTy1HFDS,Deep unsupervised feature selection,Reject,"This paper proposes Restricted AutoEncoders (REAs) for unsupervised feature selection, and applies and evaluates it in applications in biology. The paper was reviewed by three experts. R1 recommends Weak Reject, identifying some specific technical concerns as well as questions about missing and unclear experimental details. R2 recommends Reject, with concerns about limited novelty and unconvincing experimental results. R3 recommends Weak Accept saying that the overall idea is good, but also feels the contribution is ""severely undermined"" by a recently-published paper that proposes a very similar approach. Given that that paper (at ECMLPKDD 2019) was presented just one week before the deadline for ICLR, we would not have expected the authors to cite the paper. Nevertheless, given the concerns expressed by the other reviewers and the lack of an author response to help clarify the novelty, technical concerns, and missing details, we are not able to recommend acceptance. We believe the paper does have significant merit and hope that the reviewer comments will help authors in preparing a revision for another venue.",Paper Decision
TzL9ldw0Ni,Skeh1krtvH,WaveFlow: A Compact Flow-based Model for Raw Audio,Reject,"The paper presented a unified framework for constructing likelihood-based generative models for raw audio. It demonstrated tradeoffs between memory footprint, generation speech and audio fidelity. The experimental justification with objective likelihood scores and subjective mean opinion scores are matching standard baselines. The main concern of this paper is the novelty and depth of the analysis. It could be much stronger if there're thorough analysis on the benefits and limitations of the unified approach and more insights on how to make the model much better. ",Paper Decision
_xQ9Hvor9M,Ske31kBtPr,Mathematical Reasoning in Latent Space,Accept (Talk),"This paper was very well received by the reviewers with solid Accept ratings across the board.
The subject matter is quite interesting -  mathematical reasoning in latent space, and it was suggested by a reviewer that this could be a good candidate for an oral. The AC agrees and recommends acceptance as an oral. Some of the intuitions of what is being done in this paper could be better visualized and presented and I encourage the authors to think carefully about how to present this work if an oral presentation is granted by the PCs.",Paper Decision
9-54e4UqWf,rJxok1BYPr,Black Box Recursive Translations for Molecular Optimization,Reject,"This paper presents a simple method for improving molecular optimization with a learned model. The method operates by repeatedly feeding generated molecules back through an encoder decoder pair trained to maximize a desired property. Reviewers liked the simplicity of the method, and found it interesting but ultimately there were concerns about the metrics used to evaluate the method. Reviewers 3 and 4 both noted issues with the log P (and penalized log P) metric, noting that it is possible to artificially increase both metrics in a way that isn't useful in practice. During the discussion phase, Reviewer 4 constructed a specific example where simply adding long carbon chains to a molecule would yield a linear increase the penalized log P metric, and noted that the ""best molecules"" found by the method in Figure 3 also have extremely long carbon chains (long carbon chains are not generally desirable for drug discovery). 
I recommend the authors resubmit after finding a better way to evaluate that their method generates molecules with more useful properties for drug discovery.",Paper Decision
vfONePFDJh,B1eiJyrtDB,Improved Generalization Bound of Permutation Invariant Deep Neural Networks,Reject,"This work proves a generalization bound for permutation invariant neural networks (with ReLU activations). While it appears the proof is technically sound and the exact result is novel, reviewers did not feel that the proof significantly improves our understanding of model generalization relative to prior work. Because of this, the work is too incremental in its current form.",Paper Decision
JYfyp-RyP,B1gskyStwr,Frequency-based Search-control in Dyna,Accept (Poster),"The reviewers are unanimous in their evaluation of this paper, and I concur.",Paper Decision
jSwukAaPzq,SklcyJBtvB,Off-policy Bandits with Deficient Support,Reject,"This paper tackles the problem of learning off-policy in the contextual bandit problem, more specifically when the available data is deficient (in the sense that it does not allow to build reasonable counterfactual estimators). To address this, the authors introduce three strategies: 1) restricting the action space; 2) imputing missing rewards when lacking data; 3) restricting the policy space to policies with ""enough"" data. All three approaches are analyzed (statistical and computational properties) and evaluated empirically. Restricting the policy space appears to be particularly effective in practice.

Although the problem being solved is very relevant,  it is not clear how this work is positioned with respect to approaches solving similar problems in RL. For example, Batch constrained Q-learning ([1]) restricts action space, while Bootstrapping Error Accumulation ([2]) and SPIBB ([3]) restrict the policy class in batch RL. A comparison with these techniques in the contextual bandit settings, in addition to recent state-of-the-art off-policy bandit approaches (Liu et al. (2019), Xie et al. (2019)) is lacking. Moreover, given the newly added results (DR method by Tang et al. (2019)), it is not clear how the proposed approach improves over existing techniques. This should be clarified. I therefore recommend to reject this paper. 
",Paper Decision
ff3z7VSaf,Syxc1yrKvr,Implicit λ-Jeffreys Autoencoders: Taking the Best of Both Worlds,Reject,The paper received Weak Reject scores from all three reviewers. The AC has read the reviews and lengthy discussions and examined the paper. AC feels that there is a consensus that the paper does not quite meet the acceptance threshold and thus cannot be accepted. Hopefully the authors can use the feedback to improve their paper and resubmit to another venue.,Paper Decision
Ksdos3Z8aI,HyeYJ1SKDH,FLUID FLOW MASS TRANSPORT FOR GENERATIVE NETWORKS,Reject,"The submission is concerned with providing a transport based formulation for generative modeling in order to avoid the standard max/min optimization challenge of GANs. The authors propose representing the divergence with a fluid flow model, the solution of which can be found by discretizing the space, resulting in an alignment of high dimensional point clouds. 

The authors disagreed about the novelty and clarity of the work, but they did agree that the empirical and theoretical support was lacking, and that the paper could be substantially improved through better validation and better results - in particular, the approach struggles with MNIST digit generation compared to other methods.

The recommendation is to not accept the submission at this time.",Paper Decision
rKoJ3HJ8Uc,S1lukyrKPr,LEX-GAN: Layered Explainable Rumor Detector Based on Generative Adversarial Networks,Reject,"The paper is well-written and presents an extensive set of experiments. The architecture is a simple yet interesting attempt at learning explainable rumour detection models. Some reviewers worry about the novelty of the approach, and whether the explainability of the model is in fact properly evaluated. The authors responded to the reviews and provided detailed feedback. A major limitation of this work is that explanations are at the level of input words. This is common in interpretability (LIME, etc), but it is not clear that explanations/interpretations are best provided at this level and not, say, at the level of training instances or at a more abstract level. It is also not clear that this approach would scale to languages that are morphologically rich and/or harder to segment into words. Since modern approaches to this problem would likely include pretrained language models, it is an interesting problem to make such architectures interpretable. ",Paper Decision
yPuDDh114w,Skxuk1rFwB,Towards Stable and Efficient Training of Verifiably Robust Neural Networks,Accept (Poster),"This paper presents a method that hybridizes the strategies of linear programming and interval bound propagation to improve adversarial robustness.  While some reviewers have concerns about the novelty of the underlying ideas presented, the method is an improvement to the SOTA in certifiable robustness, and has become a benchmark method within this class of defenses.",Paper Decision
Abbs6rgJHt,SJxIkkSKwB,Learning in Confusion: Batch Active Learning with Noisy Oracle,Reject,"This paper proposes a new active learning algorithm based on clustering and then sampling based on an uncertainty-based metric. This active learning method is not particular to deep learning. The authors also propose a new de-noising layer specific to deep learning to remove noise from possibly noisy labels that are provided. These two proposals are orthogonal to one another and its not clear why they appear in the same paper.

Reviewers were underwhelmed by the novelty of either contribution. With respect to active learning, there is years of work on first performing unsupervised learning (e.g., clustering) and then different forms of active sampling. 

This work lacks sufficient novelty for acceptance at a top tier venue. Reject",Paper Decision
QwKxPzk7u5,HJx81ySKwr,Iterative energy-based projection on a normal data manifold for anomaly localization,Accept (Poster),"This paper proposed to use an autoencoder based approach for anomaly localization. The method shows promising on inpainting task compared with traditional auto-encoder.

First two reviewers recommend this paper for acceptance. The last review has some concerns about the experimental design and whether VAE is a suitable baseline. The authors provide reasonable explanation in rebuttal while the reviewer did not give further comments.

Overall, the paper proposes a promising approach for anomaly localization; thus, I recommend it for acceptance.
",Paper Decision
c4e976xo1,rJeBJJBYDB,Chart Auto-Encoders for Manifold Structured  Data,Reject,"This paper proposes to use more varied geometric structures of latent spaces to capture the manifold structure of the data, and provide experiments with synthetic and real data that show some promise in terms of approximating manifolds.
While reviewers appreciate the motivation behind the paper and see that angle as potentially resulting in a strong paper in the future, they have concerns that the method is too complicated and that the experimental results are not fully convincing that the proposed method is useful, with also not enough ablation studies. Authors provided some additional results and clarified explanations in their revisions, but reviewers still believe there is more work required to deliver a submission warranting acceptance in terms of justifying the complicated architecture experimentally.
Therefore, we do not recommend acceptance.",Paper Decision
ovskqci4JH,B1erJJrYPH,Optimizing Loss Landscape Connectivity via Neuron Alignment,Reject,"This paper studies the loss landscape of neural networks by taking into consideration the symmetries arising from the parametrisation. Specifically, given two models $\theta_1$, $\theta_2$, it attempts to connect $\theta_1$ with the equivalence of class of $\theta_2$ generated by weight permutations. 
Reviewers found several strengths in this work, from its intuitive and simple idea to the quality of the experimental setup. However, they also found important shortcomings in the current manuscript, chief among them the lack of significance of the results. As a result, this paper unfortunately cannot be accepted in its current form. The chairs encourage the authors to revise their work by taking the reviewer feedback into consideration. ",Paper Decision
CflqCeKKIW,BJe4JJBYwS,CROSS-DOMAIN CASCADED DEEP TRANSLATION,Reject,"The paper addresses image translation by extending prior models, e.g. CycleGAN, to domain pairs that have significantly different shape variations. The main technical idea is to apply the translation directly on the deep feature maps (instead of on the pixel level). 
While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns:
(1) ill-posed formulation of the problem and what is desirable, (2) using fine-tuned/pre-trained VGG features, (3) computational cost of the proposed approach, i.e. training a cascade of pairs of translators (one pair per layer). 
AC can confirm that all three reviewers have read the author responses. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.
",Paper Decision
CwtCXuKewH,Hyg4kkHKwH,V1Net: A computational model of cortical horizontal connections,Reject,"The paper proposes a neurally inspired model that is a variant of conv-LSTM called V1net. The reviewers had trouble gleaning the main contributions of the work. Given that it is hard to obtain state of art results in neurally inspired architectures, the bar is much higher to demonstrate that there is value in pursuing these architectures. There are not enough convincing results in the paper to show this. I recommend rejection.",Paper Decision
8efzvgGukX,r1eX1yrKwB,Distribution Matching Prototypical Network for Unsupervised Domain Adaptation,Reject,"This paper addresses the problem of unsupervised domain adaptation and proposes explicit modeling of the source and target feature distributions to aid in cross-domain alignment. 

The reviewers all recommended rejection of this work. Though they all understood the paper’s position of explicit feature distribution modeling, there was a lack of understanding as to why this explicit modeling should be superior to the common implicit modeling done in related literature. As some reviewers raised concern that the empirical performance of the proposed approach was marginally better than competing methods, this experimental evidence alone was not sufficient justification of the explicit modeling. There was also a secondary concern about whether the two proposed loss functions were simultaneously necessary. 

Overall, after reading the reviewers and authors comments, the AC recommends this paper not be accepted. ",Paper Decision
0WCcfVjiy9,H1lQJ1HYwS,Deep amortized clustering,Reject,"This paper introduces a new clustering method, which builds upon the work introduced by Lee et al, 2019 - contextual information across different dataset samples is gathered with a transformer, and then used to predict the cluster label for a given sample. All reviewers agree the writing should be improved and clarified. The novelty is also on the low side, given the previous work by Lee et al. Experiments should be more convincing.
",Paper Decision
WqbKpPMPUf,HygXkJHtvB,Using Objective Bayesian Methods to Determine the Optimal Degree of Curvature within the Loss Landscape,Reject,"There has been significant discussion in the literature on the effect of the properties of the curvature of minima on generalization in deep learning.  This paper aims to shed some light on that discussion through the lens of theoretical analysis and the use of a Bayesian Jeffrey's prior.  It seems clear that the reviewers appreciated the work and found the analysis insightful.  However, a major issue cited by the reviewers is a lack of compelling empirical evidence that the claims of the paper are true.  The authors run experiments on very small networks and reviewers felt that the results of these experiments were unlikely to extrapolate to large scale modern models and problems.  One reviewer was concerned about the quality of the exposition in terms of the writing and language and care in terminology.  Unfortunately, this paper falls below the bar for acceptance, but it seems likely that stronger empirical results and a careful treatment of the writing would make this a much stronger paper for future submission.",Paper Decision
Dtn5Jqm5F,ByxGkySKwH,Towards neural networks that provably know when they don't know,Accept (Poster),"This paper tackles the problem of confidence on neural network predictions for out-of-distribution (OOD) samples. The authors propose an approach for training neural networks such that the OOD prediction is uniform across classes. The approach requires samples from in- and out-of distribution and relies on a mixture of Gaussians for modelling the distributions, allowing to obtain theoretical guarantees on detecting OOD samples (unlike existing techniques).

The main concerns of the reviewers have been addressed during the rebuttal. If this approach does not outperform state-of-the-art in practice, providing such theoretical guarantees is an important contribution.

All reviewers agree that this paper should be accepted. I therefore recommend acceptance.",Paper Decision
qYM_YwEhon,Sklf1yrYDr,BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning,Accept (Poster),"This paper proposed an improved ensemble method called BatchEnsemble, where the weight matrix is decomposed as the element-wise product of a shared weigth metrix and a rank-one matrix for each member.  The effectiveness of the proposed methods has been verified by experiments on a list of various tasks including image classification, machine translation, lifelong learning and uncertainty modeling.  The idea is simple and easy to follow.  Although some reviewers thought it lacks of in-deep analysis, I would like to see it being accepted so the community can benefit from it.",Paper Decision
NE3C5d_yK,H1gWyJBFDr,Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions,Reject,All three reviewers are consistently negative on this paper. Thus a reject is recommended.,Paper Decision
-cTQT4VjPy,rJeW1yHYwH,Inductive representation learning on temporal graphs,Accept (Poster),"The major contribution of this paper is the use of random Fourier features as temporal (positional) encoding for dynamic graphs. The reviewers all find the proposed method interesting, and believes that this is a paper with reasonable contributions. One comment pointed out that the connection between Time2Vec and harmonic analysis has been discussed in the previous work, and we suggest the authors to include this discussion/comparison in the paper.",Paper Decision
ZQkRCxJ3HQ,Bkel1krKPS,Attention on Abstract Visual Reasoning,Reject,"This work proposes a new architecture for abstract visual reasoning called ""Attention Relation Network"" (ARNe), based on Transformer-style soft attention and relation networks, which the authors show to improve on the ""Wild Relation Network"" (WReN). The authors test their network on the PGM dataset, and demonstrate a non-trivial improvement over previously reported baselines. 

The paper is well written and makes an interesting contribution, but the reviewers expressed some criticisms, including technical novelty, unfinished experiments (and lack of experimental details), and somewhat weak experimental results, which suggest that the proposed ARNe model does not work well when training with weaker supervision without meta-targets. Even though the authors addressed some concerns in their revised version (namely, they added new experiments in the extrapolation split of PGM and experiments on the new RAVEN dataset), I feel the paper is not yet ready for publication at ICLR. 
",Paper Decision
YBAFeS0OS8,SJegkkrYPS,Starfire: Regularization-Free Adversarially-Robust Structured Sparse Training,Reject,"This paper concerns a training procedure for neural networks which results in sparse connectivity in the final resulting network, consisting of an ""early era"" of training in which pruning takes place, followed by fixed connectivity training thereafter, and a study of tradeoffs inherent in various approaches to structured and unstructured pruning, and an investigation of adversarial robustness of pruned networks.

While some reviewers found the general approach interesting, all reviewers were critical of the lack of novelty, clarity and empirical rigour. R2 in particular raised concerns about the motivation, evaluation of computational savings (that FLOPS should be measured directly), and felt that the discussion of adversarial robustness was out of place and ""an afterthought"".

Reviewers were unconvinced by rebuttals, and no attempts were made at improving the paper (additional experiments were promised, but not delivered). I therefore recommend rejection. ",Paper Decision
ONZA6dxbvd,Hkee1JBKwB,Convolutional Tensor-Train LSTM for Long-Term Video Prediction,Reject,"This paper proposes Conv-TT-LSTM for long-term video prediction. The proposed method saves memory and computation by low-rank tensor representations via tensor decomposition and is evaluated in Moving MNIST and KTH datasets.

All reviews argue that the novelty of the paper does not meet the standard of ICLR. In the rebuttal, the authors polish the experiment design, which fails to change any reviewer’s decision.

Overall, the paper is not good enough for ICLR.",Paper Decision
6VUB512HR,H1gyy1BtDS,An Information Theoretic Approach to Distributed Representation Learning,Reject,"The authors study generalization in distributed representation learning by describing limits in accuracy and complexity which stem from information theory. 

The paper has been controversial, but ultimately the reviewers who provided higher scores presented weaker and fewer arguments. By recruiting an additional reviewer it became clearer that, overall the paper needs a little more work to reach ICLR standards. The main suggestions for improvements have to do with improving clarity in a way that makes the motivation convincing and the practicality more obvious. Boosting the experimental results is a complemental way of increasing convincingness, as argued by reviewers.
",Paper Decision
riZnDqu_jI,HJlA0C4tPS,A Probabilistic Formulation of Unsupervised Text Style Transfer,Accept (Spotlight),"This paper proposes an unsupervised text style transfer model which combines a language model prior with an encoder-decoder transducer. They use a deep generative model which hypothesises a latent sequence which generates the observed sequences. It is trained on non-parallel data and they report good results on unsupervised sentiment transfer, formality transfer, word decipherment, author imitation, and machine translation. The authors responded in depth to reviewer comments, and the reviewers took this into consideration. This is a well written paper, with an elegant model and I would like to see it accepted at ICLR. ",Paper Decision
iDsaMKJy5,HyxCRCEKwB,ROBUST GENERATIVE ADVERSARIAL NETWORK,Reject,"This work proposes a robust variant of GAN, in which the generator and discriminator compete with each other in a worst-case setting within a small Wasserstein ball. Unfortunately, the reviewers have raised some critical concerns in terms of theoretical analysis and empirical support. The authors did not submit rebuttals in time. We encourage the authors to improve the work based on reviewer's comments. ",Paper Decision
0GOvsrFwQh,BJeTCAEtDB,Feature Map Transform Coding for Energy-Efficient CNN Inference,Reject,"The paper proposed the use of a lossy transform coding approach to to reduce the memory bandwidth brought by the storage of intermediate activations. It has shown the proposed method can bring good memory usage while maintaining the the accuracy.
The main concern on this paper is the limited novelty. The lossy transform coding is borrowed from other domains and only the use of it on CNN intermediate activation is new, which seems insufficient. ",Paper Decision
vAFRmcIOJI,SJgaRA4FPH,"Generative Models for Effective ML on Private, Decentralized Datasets",Accept (Poster),"The paper provides methods for training generative models by combining federated learning techniques with differentiable privacy. The paper also provides two concrete applications for the problem of debugging models. Even though the method in the paper seems to be a standard combination of DP deep learning and federated learning, the paper is well-written and presents interesting use cases.",Paper Decision
Cjd07CDw07,rylT0AVtwH,Learning from Partially-Observed Multimodal Data with Variational Autoencoders,Reject,"This submission proposes a VAE-based method for jointly inferring latent variables and data generation. The method learns from partially-observed multimodal data.

Strengths:
-Learning to generate from partially-observed data is an important and challenging problem.
-The proposed idea is novel and promising.

Weaknesses:
-Some experimental protocols are not fully explained.
-The experiments are not sufficiently comprehensive (comparisons to key baselines are missing).
-More analysis of some surprising results is needed.
-The presentation has much to improve.

The method is promising but the mentioned weaknesses were not sufficiently addressed during discussion. AC agrees with the majority recommendation to reject.
",Paper Decision
E8WAc3bqm8,SJl3CANKvB,A SIMPLE AND EFFECTIVE FRAMEWORK FOR PAIRWISE DEEP METRIC LEARNING,Reject,"The reviewers agree that this is a reasonable paper but somewhat derivative. The authors discussed the contribution further in the rebuttal, but even in light of their comments, I consider the significance of this work too low for acceptance.",Paper Decision
DDZb-1uWSu,r1e30AEKPr,A Group-Theoretic Framework for Knowledge Graph Embedding,Reject,"This paper presents a rigorous mathematical framework for knowledge graph embedding. The paper received 3 reviews. R1 recommends Weak Reject based on concerns about the contributions of the paper; the authors, in their response, indicate that R1 may have been confused about what the contributions were meant to be. R2 initially recommended Reject, based on concerns that the paper was overselling its claims, and on the clarity and quality of writing. After the author response, R2 raised their score to Weak Reject but still felt that their main concerns had gone unanswered, and in particular that the authors seemed unwilling to tone down their claims. R3 recommends Weak Reject, indicating that they found the paper difficult to follow and gave some specific technical concerns. The authors, in their response, express confusion about R3's comments and suggest that R3 also did not understand the paper. However, in light of these unanimous Weak Reject reviews, we cannot recommend acceptance at this time.  We understand that the authors may feel that some reviewers did not properly understand or appreciate the contribution, but all three reviewers are researchers working at highly-ranked institutions and thus are fairly representative of the attendees of ICLR; we hope that their points of confusion and concern, as reflected in their reviews, will help authors to clarify a revision of the paper for another venue. 

",Paper Decision
L34Ot-8mE,SJloA0EYDr,A⋆MCTS: SEARCH WITH THEORETICAL GUARANTEE USING POLICY AND VALUE FUNCTIONS,Reject,"This paper proposed an extension of the Monte Carlos Tree Search to find the optimal policy. The method combines A* and MCTS algorithms to prioritize the state to be explored. Compare with traditional MCTS based on UCT, A* MCTS seem to perform better.

One concern of the reviewers is the paper's presentation, which is hard to follow. The second concern is the strong restriction of assumption, which make the setting too simple and unrealistic. The rebuttal did not fully address these problems.

This paper needs further polish to meet the standard of ICLR.
",Paper Decision
Vb-xRUGnoP,SkgsACVKPH,Picking Winning Tickets Before Training by Preserving Gradient Flow,Accept (Poster),This paper proposes a method to improve the training of sparse network by ensuring the gradient is preserved at initialization. The reviewers found that the approach was well motivated and well explained. The experimental evaluation considers challenging benchmarks such as Imagenet and includes strong baselines. ,Paper Decision
50oUezAM1n,Byg9AR4YDB,Exploring Cellular Protein Localization Through Semantic Image Synthesis,Reject,"This paper proposes a dedicated deep models for analysis of multiplexed ion beam imaging by time-of-flight (MIBI-TOF).

The reviewers appreciated the contributions of the paper but not quite enough to make the cut.

Rejection is recommended. ",Paper Decision
r7uQN3HAQg,Byx5R0NKPr,Learning Calibratable Policies using Programmatic Style-Consistency,Reject,"The reviewers generally reached a consensus that the work is not quite ready for acceptance in its current form. The central concerns were about the potentially limited novelty of the method, and the fact that it was not quite clear how good the annotations needed to be (or how robust the method would be to imperfect annotations). This, combined with an evaluation scenario that is non-standard and requires some guesswork to understand its difficulty, leaves one with the impression that it is not quite clear from the experiments whether the method really works well. I would recommend for the authors to improve the evaluation in the next submission.",Paper Decision
BiZnkni1lo,H1x9004YPr,Contextual Temperature for Language Modeling,Reject,"With an average post author response score of 4 - two weak rejects and one weak accept, it is just not possible for the AC to recommend acceptance. The author response was not able to shift the scores and general opinions of the reviewers and the reviewers have outlined their reasoning why their final scores remain unchanged during the discussion period.",Paper Decision
dSb6fWeC0N,H1eY00VFDB,Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks,Reject,"This paper introduces a further regularizer, retrospection loss, for training neural networks, which leverages past parameter states.  The authors added several ablation studies and extra experiments during the rebuttal, which are helpful to show that their method is useful. However, this is still one of those papers that essentially proposes an additional heuristic to train deep news, which is helpful but not clearly motivated from a theoretical point of view (despite the intuitions). Yes, it provides improvements across tasks but these are all relatively small, and the method is more involved. Therefore, I am recommending rejection.",Paper Decision
jTC0ChX6YM,rkgt0REKwS,Curriculum Loss: Robust Learning and Generalization  against Label Corruption,Accept (Poster),"This paper studies learning with noisy labels by integrating the idea of curriculum learning.

All reviewers and AC are happy with novelty, clear write-up and experimental results.

I recommend acceptance. 
",Paper Decision
2VAO0AhxO,H1gdAC4KDB,Adversarially Robust Generalization Just Requires More Unlabeled Data,Reject,"This work starts with a decomposition of the adversarial risk into two terms: the first is the usual risk, while the second is a stability term, that captures the possible effect of an adversarial perturbation. The insight of this work is that this second term can be dealt with using unlabelled data, which is often in plentiful supply. Unfortunately, the same ideas was developed concurrently and independently by several groups of authors.

The reviewer all agreed that this particular version was not ready for publication. In two cases, the authors compared the work unfavorably with concurrent independent work. I will note that the main bound somewhat ignores the issue of overfitting that the second term deals with via the Rademacher bound. Unless one assumes one has unlimited unlabeled data, could one not get an arbitrarily biased view of robustness from the sample. Seems like a gap to fill.",Paper Decision
pJMaaKC6sx,HJlvCR4KDS,Why Does the VQA Model Answer No?: Improving Reasoning through Visual and Linguistic Inference,Reject,"This paper is good, with relatively positive support from the reviewers. However, there were also several legitimate issues raised, for example regarding the semantics of a negative answer and associated explanations. Though this paper cannot be accepted at this time, we hope the feedback here can help improve a future version, as all reviewers agree this is a valuable line of work.",Paper Decision
PJ-uOnwj5,SyeD0RVtvS,DeepSFM: Structure From Motion Via Deep Bundle Adjustment,Reject,"Main content:  Physical driven architecture of DeepSFM to infer the structures from motion
Discussion:
reviewer 1: well-motivated model with good solid experimental results. not clear about the LM optimization in BA-Net is memory inefficient 
reviewer 2: main issue is the experiments could be improved.
reviewer 3: well written but again experimental section is lacking
Recommendation: Good paper and results, but all 3 reviewers agree experiments could be improved. Rejection is recommended.",Paper Decision
Vrn4yhXIMq,rylvAA4YDB,IsoNN: Isomorphic Neural Network for Graph Representation Learning and Classification,Reject,"This paper proposes a method to learn graph features by means of neural networks for graph classification.
The reviewers find that the paper needs to improve in terms of novelty and experimental comparisons. ",Paper Decision
eOHHzlmHKa,HklUCCVKDB,Uncertainty-guided Continual Learning with Bayesian Neural Networks,Accept (Poster),"While prior work has shown the potential of using uncertainty to tackle catastrophic forgetting (e.g. by appropriate updates to the posterior), this paper goes further and proposes a strategy to adapt the learning rate based on the uncertainty. This is a very reasonable idea since, in practice, learning rate control is one of the simplest and most understood techniques to fight catastrophic forgetting. 
The overall approach ends up being a well-motivated strategy for controlling the learning rate of the parameters according to a notion of their ""importance"". Of course now the question is if this work uses a good proxy for ""importance"" so further ablation studies would help, but the current results already show a clear benefit. 
",Paper Decision
N3aWNtoM73,HygrAR4tPS,On Empirical Comparisons of Optimizers for Deep Learning,Reject,"This paper examines classifiers and challenges a (somewhat widely held) assumption that adaptive gradient methods underperform simpler methods.

This paper sparked a *large* amount of discussion, more than any other paper in my area. It was also somewhat controversial.

After reading the discussion and paper itself, on one hand I think this makes a valuable contribution to the community. It points out a (near-) inclusion relationship between many adaptive gradient methods and standard SGD-style methods, and points out that rather obviously if a particular method is included by a more general method, the more general method will never be worse and often will be better if hyperparameters are set appropriately.

However, there were several concerns raised with the paper. For example, reviewer 1 pointed out that in order for Adam to include Momentum-based SGD, it must follow a specialized learning rate schedule that is not used with Adam in practice. This is pointed out in the paper, but I think it could be even more clear. For example, in the intro ""For example, ADAM (Kingma and Ba, 2015) and RMSPROP (Tieleman and Hinton, 2012) can approximately simulate MOMENTUM (Polyak, 1964) if the ε term in the denominator of their parameter updates is allowed to grow very large."" does not make any mention of the specialized learning rate schedule.

Second, Reviewer 1 was concerned with the fact that the paper does not clearly qualify that the conclusion that more complicated optimization schedules do better depends on extensive hyperparameter search. This fact somewhat weakens one of the main points of the paper.

I feel that this paper is very much on the borderline, but cannot strongly recommend acceptance. I hope that the authors take the above notes, as well as the reviewers' other comments into account seriously and try to reflect them in a revised version of the paper.",Paper Decision
7CKNyNrAk7,B1xBAA4FwH,On Evaluating Explainability Algorithms,Reject,"The paper proposes metrics for comparing explainability metrics.

Both reviewers and authors have engaged in a thorough discussion of the paper and feedback. The reviewers, although appreciating aspects of the paper, all see major issues with the paper. 

All reviewers recommend reject.  ",Paper Decision
zTUF0GNypS,r1lHAAVtwr,Deep Hierarchical-Hyperspherical Learning (DH^2L),Reject,"The paper proposes a hierarchical diversity promoting regularizer for neural networks. Experiments are shown with this regularizer applied to the last fully-connected layer of the network, in addition to L2 and energy regularizers on other layers. Reviewers found the paper well-motivated but had concerns on writing/readability of the paper and that it provides only marginal improvements over existing simple regularizers such as L2. I would encourage the authors to look for scenarios where the proposed regularizer can show clear improvements and resubmit to a future venue. ",Paper Decision
OGiTNr_XP,SkxV0RVYDH,Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders,Reject,"This paper proposes an outlier detection method that maps outliers to low probability regions of the latent space. The novelty is in proposing a weighted reconstruction error penalizing the mapping of outliers into high probability regions. The reviewers find the idea promising.
They have also raised several questions. It seems the questions are at least partially addressed in the rebuttal, and as a result one of our expert reviewers (R5) has increased their score from WR to WA. But since we did not have a champion for this paper and its overall score is not high enough, I can only recommend a reject at this stage.",Paper Decision
KFaHMBgQjm,HJxN0CNFPB,Ladder Polynomial Neural Networks,Reject,"This paper proposes a new type of Polynomial NN called Ladder Polynomial NN (LPNN) which is easy to train with general optimization algorithms and can be combined with techniques like batch normalization and dropout.  Experiments show it works better than FMs with simple classification and regression tasks, but no experiments are done in more complex tasks. All reviewers agree the paper addresses an interesting question and makes some progress but the contribution is limited and there are still many ways to improve.",Paper Decision
ahHW2xn_S,SJgmR0NKPr,Training Recurrent Neural Networks Online by Learning Explicit State Variables,Accept (Poster),"The paper proposes an alternative to BPTT for training recurrent neural networks based on an explicit state variable, which is trained to improve both the prediction accuracy and the prediction of the next state. One of the benefits of the methods is that it can be used for online training, where BPTT cannot be used in its exact form. Theoretical analysis is developed to show that the algorithm converges to a fixed point. Overall, the reviewers appreciate the clarity of the paper, and find the theory and the experimental evaluation to be reasonably well balanced. After a round of discussion, the authors improved the paper according to the reviews. The final assessments are overall positive, and I’m therefore recommending accepting this paper.",Paper Decision
uzhmydBe0i,rygfC0VKPS,Improved Modeling of Complex Systems Using Hybrid Physics/Machine Learning/Stochastic Models,Reject,"All reviewers agree that the paper is to be rejected, provided strong claims that were not answered. In this form (especially with such a title) it could not be published (it is more of a technical/engineering interest).",Paper Decision
mTLUj-1oO3,S1xGCAVKvr,LEARNING  TO LEARN  WITH  BETTER  CONVERGENCE,Reject,"This paper proposes an improved (over Andrychowicz et al) meta-optimizer that tries to to learn better strategies for training deep machine learning models. The paper was reviewed by three experts, two of whom recommend Weak Reject and one who recommends Reject. The reviewers identify a number of significant concerns, including degree of novelty and contribution, connections to previous work, completeness of experiments, and comparisons to baselines. In light of these reviews and since the authors have unfortunately not provided a response to them, we cannot recommend accepting the paper.",Paper Decision
EQWJGfxtIl,BkxGAREYwB,Deep Expectation-Maximization in Hidden Markov Models via Simultaneous Perturbation Stochastic Approximation,Reject,"The authors propose to use numerical differentiation to approximate the Jacobian while estimating the parameters for a collection of Hidden Markov Models (HMMs). Two reviewers provided detailed and constructive comments, while unanimously rated weak rejection. Reviewer #1 likes the general idea of the work, and consider the contribution to be sound. However, he concerns the reproducibility of the work due to the niche database from e-commerce applications. Reviewer #2 concerns the poor presentation, especially section 3. The authors respond to Reviewers’ concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state.",Paper Decision
zNpQVyeUqj,S1l-C0NtwS,Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework,Accept (Poster),"Reviewer worries include: whether the approach scales to distant language pairs, overselling of the paper as a ""framework"", a few citations and comparisons missing. I agree and encourage the authors not to use the word ""framework"" here. I would also encourage the authors to evaluate on more interesting language pairs, and analyze what vocabularies are relocated, as well as what their method is better at compared to previous work. ",Paper Decision
Gu5CMac20,BygZARVFDH,Compositional Visual Generation with Energy Based Models,Reject,"This submission proposes an image generation technique for composing concepts by combining their associated distributions. 

Strengths:
-The approach is interesting and novel.

Weaknesses:
-Several reviewers were not convinced about the correctness of the formulations for negation and disjunction.
-The experimental validation of the disjunction and negation approaches is insufficient.
-The paper clarity and exposition could be improved. The authors addressed this in the discussion but concerns remain.

Given the weaknesses, AC shares R3’s recommendation to reject.",Paper Decision
YM7d5VIXRo,H1leCRNYvS,Hierarchical Bayes Autoencoders,Reject,"This paper introduces a probabilistic generative model which mixes a variational autoencoder (VAE) with an energy based model (EBM). As mentioned by all reviewers (i) the motivation of the model is not well justified (ii) experimental results are not convincing enough. In addition (iii) handling sets is not specific to the proposed approach, and thus claims regarding sets should be revised.
",Paper Decision
swIhDdaSR,r1g1CAEKDH,Wyner VAE: A Variational Autoencoder with Succinct Common Representation Learning,Reject,"This paper adds a new model to the literature on representation learning from correlated variables with some common and some ""private"" dimensions, and takes a variational approach based on Wyner's common information.  The literature in this area includes models where both of the correlated variables are assumed to be available as input at all times, as well as models where only one of the two may be available; the proposed approach falls into the first category.  Pros:  The reviewers generally agree, as do I, that the motivation is very interesting and the resulting model is reasonable and produces solid results.  Cons:  The model is somewhat complex and the paper is lacking a careful ablation study on the components.  In addition, the results are not a clear ""win"" for the proposed model.  The authors have started to do an ablation study, and I think eventually an interesting story is likely to come out of that.  But at the moment the paper feels a bit too preliminary/inconclusive for publication.",Paper Decision
KZUTMHOLy3,SJxyCRVKvB,Granger Causal Structure Reconstruction from Heterogeneous Multivariate Time Series,Reject,"This paper proposes a solution to learn Granger temporal-causal network for multivariate time series by adding attention named prototypical Granger causal attention in LSTM. 

The work aims to address an important problem. The proposed solution seems effective empirically. However, two major issues have not been fully addressed in the current version: (1) the connection between Granger causality and the attention mechanism is not fully justified; (2) the complex design overkills the whole concept of Granger causality (since its popularity is due to the simplicity). 

The paper would be a strong publication in the future if the two issues can be addressed in a satisfactory way. ",Paper Decision
5KIwNomVcC,H1eJAANtvr,CGT: Clustered Graph Transformer for Urban Spatio-temporal Prediction,Reject,"This paper proposes an approach to handle the problem of unsmoothness while modeling spatio-temporal urban data. However all reviewers have pointed major issues with the presentation of the work, and whether the method's complexity is justified. ",Paper Decision
LFTe5E3kwi,HJgC60EtwB,Robust Reinforcement Learning for Continuous Control with Model Misspecification,Accept (Poster),"The authors provide a framework for improving robustness (if the model of the dynamics is perturbed) into the RL methods, and provide nice experimental results, especially in the updated version. I am happy to see that the discussion for this paper went in a totally positive and constructive way which lead to a) constructive criticism of the reviewers b) significant changes in the paper c) corresponding better scores by the reviewer. Good work and obvious accept.",Paper Decision
JxeoXn4ld8,r1gRTCVFvB,Decoupling Representation and Classifier for Long-Tailed Recognition,Accept (Poster),"This paper presents an approach for the long-tailed image classification, where the class frequencies during (supervised) training of an image classifier are heavily skewed, so that the classifier underfits on under-represented classes. The authors' responses to the reviews clarified most of their  concerns, although some reviewers pointed out that some of the details regarding experiments such as the construction of the validation set and the selection of balanced/imbalanced sets remain unclear. Overall, we believe this paper contains interesting observations to be shared.",Paper Decision
-rj6i2NYI,r1xapAEKwS,SDGM: Sparse Bayesian Classifier Based on a Discriminative Gaussian Mixture Model,Reject,"This paper presents a method for merging a discriminative GMM with an ARD sparsity-promoting prior.  This is accomplished by nesting the ARD prior update within a larger EM-based routine for handling the GMM, allowing the model to automatically remove redundant components and improve generalization.  The resulting algorithm was deployed on standard benchmark data sets and compared against existing baselines such as logistic regression, RVMs, and SVMs.

Overall, one potential weakness of this paper, which is admittedly somewhat subjective, is that the exhibited novelty of the proposed approach is modest.  Indeed ARD approaches are now widely used in various capacities, and even if some hurdles must be overcome to implement the specific marriage with a discriminative GMM as reported here, at least one reviewer did not feel that this was sufficient to warrant publication.  Other concerns related to the experiments and comparison with existing work.  For example, one reviewer mentioned comparisons with Panousis et al., ""Nonparametric Bayesian Deep Networks with Local Competition,"" ICML 2019 and requested a discussion of differences.  However, the rebuttal merely deferred this consideration to future work and provided no feedback regarding similarities or differences.  In the end, all reviewers recommended rejecting this paper and I did not find any sufficient reason to overrule this consensus.",Paper Decision
T2kvFLucPj,HJlTpCEKvS,Which Tasks Should Be Learned Together in Multi-task Learning?,Reject,"An approach to make multi-task learning is presented, based on the idea of assigning tasks through the concepts of cooperation and competition. 

The main idea is well-motivated and explained well. The experiments demonstrate that the method is promising. However, there are a few  concerns regarding fundamental aspects, such as: how are the decisions affected by the number of parameters? Could ad-hoc algorithms with human in the loop provide the same benefit, when the task-set is small? More importantly, identifying task groups for multi-task learning is an idea presented in prior work, e.g. [1,2,3]. This important body of prior work is not discussed at all in this paper.

[1] Han and Zhang. ""Learning multi-level task groups in multi-task learning""
[2] Bonilla et al. ""Multi-task Gaussian process prediction""
[3] Zhang and Yang. ""A Survey on Multi-Task Learning""
",Paper Decision
xf0aoJQQH,r1lh6C4FDr,COMBINED FLEXIBLE ACTIVATION FUNCTIONS FOR DEEP NEURAL NETWORKS,Reject,"Main content: Proposes combining flexible activation functions 

Discussion:
reviewer 1: main issue is unfamiliar with stock dataset, and CIFAR dataset has a bad baseline.
reviewer 2: main issue is around baselines and writing. 
reviewer 3: main issue is paper does not compare with NAS.

Recommendation: All 3 reviewers vote reject. Paper can be improved with stronger baselines and experiments. I recommend Reject.",Paper Decision
5NH_wiG8cb,S1xipR4FPB,Teacher-Student Compression with Generative Adversarial Networks,Reject,"This paper uses GAN for data augmentation to improve the performance of knowledge distillation.

Reviewers and AC commonly think the paper suffers from limited novelty and insufficient experimental supports/details.

Hence, I recommend rejection.",Paper Decision
xbpqULHUzW,Skg9aAEKwH,Visual Hide and Seek,Reject,"This paper proposes a technique for training embodied agents to play Visual Hide and Seek where a prey must navigate in a simulated environment in order to avoid capture from a predator. The model is trained to play this game from scratch without any prior knowledge of its visual world, and experiments and visualizations show that a representation of other agents automatically emerges in the learned representation. Results suggest that, although agent weaknesses make the learning problem more challenging, they also cause useful features to emerge in the representation.

While reviewers found the paper explores an interesting direction, concerns were raised that many claims are unjustified. For example, in the discussion phase a reviewer asked how can one infer ""hider learns to first turn away from the seeker then run away"" from a single transition frequency? Or, the rebuttal mentions ""The agent with visibility reward does not get the chance to learn features of self-visibility because of the limited speed hence the model received samples with significantly less variation of its self-visibility, which makes learning to discriminate self-visibility difficult"". What is the justification for this? There could be more details in the paper and I'd also like to know if these findings were reached purely by looking at the histograms or by combining visual analysis with the histograms.

I suggest authors address these concerns and provide quantitative results for all of the claims in an improved iteration of this paper. ",Paper Decision
5UyWnRz28x,Hyg5TRNtDH,Unsupervised Temperature Scaling: Robust Post-processing Calibration for Domain Shift,Reject,"The paper proposes a method called unsupervised temperature scaling (UTS) for improving calibration under domain shift.

The reviewers agree that this is an interesting research question, but raised concerns about clarity of the text, depth of the empirical evaluation, and validity of some of the assumptions. While the author rebuttal addressed some of these concerns, the reviewers felt that the current version of the paper is not ready for publication.

I encourage the authors to revise and resubmit to a different venue.",Paper Decision
n3ovhjtUFE,B1e5TA4FPr,Pareto Optimality in No-Harm Fairness,Reject,"This manuscript outlines procedures to address fairness as measured by disparity in risk across groups. The manuscript is primarily motivated by methods that can achieve ""no-harm"" fairness, i.e., achieving fairness without increasing the risk in subgroups.

The reviewers and AC agree that the problem studied is timely and interesting. However, in reviews and discussion, the reviewers noted issues with clarity of the presentation, and sufficient justification of the results. The consensus was that the manuscript in its current state is borderline, and would have to be significantly improved in terms of clarity of the discussion, and possibly improved methods that result in more convincing results.
",Paper Decision
upmwJ8lai,BJxt60VtPr,Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping,Accept (Poster),"The authors propose to learn space-aware 3D feature abstractions of the world given 2.5D input, by minimizing 3D and 2D view contrastive prediction objectives. The work builds upon Tung et al. (2019) but extends it by removing some of the limitations, making it thus more general. To do so, they learn an inverse graphics network which takes as input 2.5D video and maps to a 3D feature maps of the scene. The authors present experiments on both real and simulation datasets  and their proposed approach is tested on feature learning, 3D moving object detection, and 3D motion estimation with good performance. All reviewers agree that this is an important problem in computer vision and the papers provides a working solution. The authors have done a good job with comparisons and make a clear case about their superiority of their model (large datasets, multiple tasks). Moreover, the rebuttal period has been quite productive, with the authors incorporating reviewers' comments in the manuscript, resulting thus in a stronger submission. Based in reviewer's comment and my own assessment, I think this paper should get accepted, as the experiments are solid with good results that the CV audience of ICLR would find relevant. 
",Paper Decision
hFYWTrdljP,S1lOTC4tDS,Dream to Control: Learning Behaviors by Latent Imagination,Accept (Spotlight),"This paper presents an approach to model-based reinforcement learning in high-dimensional tasks. The approach involves learning a latent dynamics model, and performing rollouts thereof with an actor-critic model to learn behaviours. This is extensively evaluated on 20 visual control tasks.

This paper was favourably received, but there were concerns around it being incremental (relative to PlaNet and SVG). The authors highlighted the differences in the rebuttal, clarifying the novelty of this work. 

Given the interesting ideas presented, and the convincing results, this paper should be accepted.",Paper Decision
gt4tBvyhlZ,H1guaREYPr,From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech,Accept (Poster),"The authors propose a conditional GAN-based approach for generating faces consistent with given input speech.  The technical novelty is not large, as the approach is mainly putting together existing ideas, but the application is a fairly new one and the experiments and results are convincing.  The approach might also have broader applicability beyond this task.",Paper Decision
r1bonkIpQi,HylwpREtDr,Active Learning Graph Neural Networks via Node Feature Propagation,Reject,"The authors propose a method of selecting nodes to label in a graph neural network setting to reduce the loss as efficiently as possible. Building atop Sener & Savarese 2017 the authors propose an alternative distance metric and clustering algorithm. In comparison to the just mentioned work, they show that their upper bound is smaller than the previous art's upper bound. While one cannot conclude from this that their algorithm is better, at least empirically the method appears to have a advantage over state of the art.

However, reviewers were concerned about the assumptions necessary to prove the theorem, despite the modifications made by the authors after the initial round. 

The work proposes a simple estimator and shows promising results but reviewers felt improvements like reducing the number of assumptions and potentially a lower bound may greatly strengthen the paper.",Paper Decision
weHzFx0JGh,B1lPaCNtPB,"Real or Not Real, that is the Question",Accept (Spotlight),"The paper proposes a novel GAN formulation where the discriminator outputs discrete distributions instead of a scalar. The objective uses two ""anchor"" distributions that correspond to real and fake data. There were some concerns about the choice of these distributions but authors have addressed it in their response. The empirical results are impressive and the method will be of interest to the wide generative models community. ",Paper Decision
Bb8Co6P16F,rJgDT04twH,Deep Reinforcement Learning with Implicit Human Feedback,Reject,"The paper explores the idea of using implicit human feedback, gathered via EEG, to assist deep reinforcement learning. This is an interesting and at least somewhat novel idea. However, it is not clear that there is a good argument why it should work, or at least work well. The experiments carried are more exploratory than anything else, and it is not clear that much can be learned from the results. It's a proof of concept more than anything else, of the type that would work well for a workshop paper. More systematic empirical work would be needed for a good conference paper.

The authors did not provide a rebuttal to reviewers, but rather agreed with their comments and that the paper needs more work. In light of this, the paper should be rejected and we wish the authors best of luck with a new version of the paper.
",Paper Decision
jwPPFwUhT,BJlITC4KDB,Multi-Sample Dropout for Accelerated Training and Better Generalization,Reject,"This paper proposes a multi-sample variant of dropout, claiming that it accelerates training and improves generalization. CIFAR10/100, ImageNet and SVHN results are presented, along with a few ablations.

Reviewers were in agreement that the novelty of the contribution appears to be very limited, the evidence for the claims is not strong, and that the applicability of the method for achieving efficiency gains is limited to architectures that only apply dropout very late in processing, precluding applicability to models that employ dropout throughout. Importantly, comparisons to Fast Dropout (Wang 2013) seem highly relevant and are missing.

While the reviewers acknowledged some of the criticisms, virtually no arguments were offered to rebut them and no updates were made to address them. I therefore recommend rejection.",Paper Decision
-2u2Au4kY4,r1gIa0NtDH,MelNet: A Generative Model for Audio in the Frequency Domain,Reject,"The paper proposed an autoregressive model with a multiscale generative representation of the spectrograms to better modeling the long term dependencies in audio signals. The techniques developed in the paper are novel and interesting. The main concern is the validation of the method. The paper presented some human listening studies to compare long-term structure on unconditional samples, which as also mentioned by reviewers are not particularly useful. Including justifications on the usefulness of the learned representation for any downstream task would make the work much more solid. ",Paper Decision
iBea-HH5K,S1gV6AVKwB,Cross Domain Imitation Learning,Reject,"The authors propose a novel approach for imitation learning in settings where demonstrations are unaligned with the task (e.g., differ in terms of state and action space). The proposed approach consists of alignment and adaptation steps and theoretical insights are provided on whether given MDPs can be aligned. Reviewers were positive about the ideas presented in the paper, and several requests for clarification were well addressed by the authors during the rebuttal phase. Key evaluation issues remained unresolved. In particular, it was unclear to what degree performance differences were purely caused by issues in alignment, and reviewers did not see sufficient evidence to support claims about performance on the full cross domain learning setting.",Paper Decision
S9fnCUaUEV,HkeQ6ANYDB,Blending Diverse Physical Priors with Neural Networks,Reject,"This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work, and I urge the authors to continue to develop refinements and extensions.",Paper Decision
IRHnh8xXjJ,H1x-pANtDB,A closer look at network resolution for efficient network design,Reject,"Main content:new training regime for multi-resolution slimmable networks. 

Discussion:
reviewer 4: believes the main contribution of mutual learning from width and resolution is a bit weak
reviewer 1: incremental work, details/baselines missing in experimental section
reviewer 2: (least detailed): well-written with good results
Recommendation: I agree with reviewer 1, 4 that the experimental section could be improved. Leaning to reject. 

",Paper Decision
RXHopZ3Yp,Hye-p0VFPB,Efficient Systolic Array Based on Decomposable MAC for Quantized Deep Neural Networks,Reject,"This paper presents an energy-efficient architecture for quantized deep neural networks based on decomposable multiplication using MACs. Although the proposed approach is shown to be somehow effective, two reviewers pointed out that the very similar idea was already proposed in the previous work, BitBlade [1]. As the authors did not submit a rebuttal to defend this critical point, I’d like to recommend rejection. I recommend authors to discuss and clarify the difference from [1] in the future version of the paper. 

[1] Sungju Ryu, Hyungjun Kim, Wooseok Yi, Jae-Joon Kim. BitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation. DAC'2019
",Paper Decision
_bd33aNvCI,rkxWpCNKvS,Improved Image Augmentation for Convolutional Neural Networks by Copyout and CopyPairing,Reject,The reviewers have issues with novelty and quality of exposition. I recommend rejection.,Paper Decision
urt33-dYGB,rylxpA4YwH,On the Evaluation of Conditional GANs,Reject,"The paper presents an extension of FID for conditional generation settings. While it's an important problem to address, the reviewers were concerned about the novelty and advantage of the proposed method over the existing methods. The evaluation is reported on toy datasets, and the significance is limited.",Paper Decision
5Ctu3QFSb,r1gx60NKPS,JAUNE: Justified And Unified Neural language Evaluation,Reject,"The authors tackle the questions of automatic metrics for assessing document similarity and propose the use of Transformed-based language models as a critic providing scores to samples. As a note, ideas like these have been also adopted in Computer Vision with the use of the Inception score as a proxy the quality of generated images. The authors ask great questions in the paper and they clearly tackle a very important problem, that of automatic measures for assessing text quality. While their first indications are not negative, this paper lacks the rigor and depth of experiments of a conference paper that would convince the research community to abandon BLEU and ROUGE in lieu of some other metric. It's perhaps a good workshop paper or a short paper at a *CL conference. Specifically, we would need more tasks where BLEU/ROUGE is the standard measure and so how the proposed measure correlates better with humans,  so cases where word overlap is in theory a good proxy of similarity assuming reference sentence (e.g., logical entailment is not such a prototypical task). MT is a first step towards that, but summarization is also a necessary I would say. Other questions of interest relate to the type of LM (does it only need to be Roberta?) and the quality of LM (what if i badly tune my LM?)  On a more personal note: We all know that BLEU is not a good metric (especially for document-level judgements) and every now and then there have been proposals to replace BLEU that do correlate better (e.g., http://ccc.inaoep.mx/~villasen/bib/Regression%20for%20machine%20translation%20evaluation.pdf) . However, BLEU is still here due to each simplicity. Please keep pushing this research and I’m looking forward to seeing more experimental evidence.",Paper Decision
yT9mqpO8KY,B1gkpR4FDB,Statistical Adaptive Stochastic Optimization,Reject,"The paper proposes an approach to automatically tune the learning rate by using a statistical test that detects the stationarity of the learning dynamics. It also proposes a robust line search algorithm to reduce the need to tune the initial learning rate. The statistical test uses a test function which is taken to be a quadratic function in the paper for simplicity, although any choice of test function is valid. Although the method itself is interesting, the empirical benefits over SGD/ADAM seem to be minor. 
",Paper Decision
rDePEOO_O,BklC2RNKDS,Scalable Neural Learning for Verifiable Consistency with Temporal Specifications,Reject,"This submission proposes a deep network training method to verify desired temporal properties of the resultant model.

Strengths:
-The proposed approach is valid and has some interesting components.

Weaknesses:
-The novelty is limited.
-The experimental validation could be improved.

Opinion on this paper was mixed but the more confident reviewers believed that novelty is insufficient for acceptance.",Paper Decision
iVNq9sylqt,S1gR2ANFvB,Model Comparison of Beer data classification using an electronic nose,Reject,"The paper has received all negative scores. Furthermore, one of the reviewers identified an anonymity violation. This is a reject.",Paper Decision
949sCp3d0d,B1gR3ANFPS,Non-linear System Identification from Partial Observations via Iterative Smoothing and Learning,Reject,"The paper is about nonlinear system identification in an EM-style learning framework. The idea is to use nonlinear programming for the E step (finding a MAP estimate) and then refine the model parameters. In flavor, this approach is similar to the work by Roweis and Ghahramani. 

However, this paper does not offer any new insights whatsoever and the (very short) methods section arrives at proposing to compute the maximum a posteriori estimate (eq. 5). While the motivation for this given in the paper is a bit hard to understand it is of course a very well-known and useful estimator. Besides the maximum likelihood estimator this is one of the most commonly used point estimators, see any textbook on statistical signal processing. There has been quite a bit of work in the signal processing community over the last 10 years, and a good overview can be found here:
https://web.stanford.edu/~boyd/papers/pdf/rt_cvx_sig_proc.pdf
This should give evidence that this is indeed a standard way of solving the problem and it does work really well. Given that we have so fast and good optimizers these days it is common to solve Kalman filtering/smoothing problems via this optimization problem.
The paper does not contain any analysis at all. The experiments do of course show that the method works (when there is low noise). Again, we know very well that the MAP estimate is a decent estimator for unimodal problems. The MAP estimator can also be made to work well for noisy situations.

As for the comments that the sequential Monte Carlo methods do not work in higher dimensions that is indeed true. However, there are now algorithms that work in much higher dimensions than those considered by the authors of this paper, e.g.
https://ieeexplore.ieee.org/document/8752074
which also contains an up-to-date survey on the topic. Furthermore, when it comes to particle smoothing there are also much more efficient smoothers than 10 years ago. The area of particle smoothing has also evolved rapidly over the past years. 

Summary:
The paper makes use of the well-known MAP estimator for learning nonlinear dynamical systems (states and parameters). This is by now a standard technique in signal processing. There are several throw-away comments on SMC that are not valid and that are not grounded in the intense research of that field over the past decade.
",Paper Decision
Fe5tLSJs7X,ryga2CNKDH,Evaluating Lossy Compression Rates of Deep Generative Models,Reject,"The paper proposed a method to evaluate latent variable based generative models by estimating the compression in the latents (rate) and the distortion in the resulting reconstructions. While reviewers have clearly appreciated the theoretical novelty in using AIS to get an upper bound on the rate, there are concerns on missing empirical comparison with other related metrics (precision-recall) and limited practical applicability of the method due to large computational cost. Authors should consider comparing with PR metric and discuss some directions that can make the method practically as relevant as other related metrics. ",Paper Decision
TpTmF7C6xg,Hkx6hANtwH,LambdaNet: Probabilistic Type Inference using Graph Neural Networks,Accept (Poster),"This paper proposes an approach to type inference in dynamically typed languages using graph neural networks. The reviewers (and the area chair) love this novel and useful application of GNNs to a practical problem, the presentation, the results. Clear accept.",Paper Decision
rEaGuI3kDS,r1eh30NFwB,Variational Autoencoders with Normalizing Flow Decoders,Reject,"The paper received mixed reviews: WR (R1,R3) and WA (R2). AC has carefully read reviews and rebuttal and examined the paper. Unfortunately, the AC sides with R1 & R3, who are more experienced in this field than R2, and feels that paper does not quite meet the acceptance threshold. The authors should incorporate the comments of the reviewers and resubmit to another venue. ",Paper Decision
pzbI9CDNw,Skln2A4YDB,Model-Augmented Actor-Critic: Backpropagating through Paths,Accept (Poster),"The authors propose a novel model-based reinforcement learning algorithm. The key difference with previous approaches is that the authors use gradients through the learned model. They present theoretical results on error bounds for their approach and a monotonic improvement theorem. In the small sample regime, they show improved performance over previous approaches.

After the revisions, reviewers raised a few concerns:
The results are only for 100,000 steps, which does not support the claim that the models achieves the same asymptotic performance as model – free algorithms would.
The results would be stronger as the experiments were run with more than 3 random seats.
In the revised version of the text, it's unclear if the authors are using target networks.

Overall, I think the paper introduces some interesting ideas and shows improved performance over existing approaches. I recommend acceptance on the condition that the authors tone down their claims or back them up with empirical evidence. Currently, I don't see evidence for the claim that the method achieves similar asymptotic performance to model free algorithms or the claim that their approach allows for longer horizons than previous approaches.",Paper Decision
Mc4YmrEgS0,Sygn20VtwH,Metagross: Meta Gated Recursive Controller Units for Sequence Modeling,Reject,"This paper proposes a recurrent architecture based on a recursive gating mechanism. The reviewers leaned towards rejection on the basis of questions regarding novelty, analysis, and the experimental setting. Surprisingly, the authors chose not to engage in discussion, as all reviewers seems pretty open to having their minds changed. If none of the reviewers will champion the paper, and the authors cannot be bothered to champion their own work, I see no reason to recommend acceptance.",Paper Decision
69V28tjSN,ryxjnREFwH,Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension,Accept (Spotlight),"Main content:

Blind review #1 summarizes it well:

This paper presents a semantic parser that operates over passages of text instead of a structured data source.  This is the first time anyone has demonstrated such a semantic parser (Siva Reddy and several others have essentially used unstructured text as an information source for a semantic parser, similar to OpenIE methods, but this is qualitatively different).  The key insight is to let the semantic parser point to locations in the text that can be used in further symbolic operations.  This is excellent work, and it should definitely be accepted.  I have a ton of questions about this method, but they are good questions.  

--

Discussion:

The reviews all agree on a generally positive assessment, and focus on details that have been addressed, rather than major problems.

--

Recommendation and justification:

This paper should be accepted. Even though novelty in terms of fundamental machine learning components is minimal, but the architecture employing neural models to do symbolic work is a good contribution in a crucial direction (especially in the theme of ICLR).",Paper Decision
5o6FyNBW9x,B1lj20NFDS,Variational Autoencoders for Highly Multivariate Spatial Point Processes Intensities,Accept (Poster),"This paper presents a novel VAE-based model for multivariate spatial point process which can realize efficient inference by amortization and handle missing points via smooth intensity estimation. Authors also provide interesting theoretical analysis to connect their method to a popular VAE-based collaborative filtering method.
Overall, all reviewers appreciate the methodological and theoretical contributions of the paper. During the reviewer discussion, one reviewer decided to update to the score to Weak Acceptance. There are still some concerns regarding experimental validation, I think the paper provides enough theoretical contribution to the community and would like to recommend acceptance. ",Paper Decision
GHn2B5XQvA,Skeq30NFPr,Stochastic Mirror Descent on Overparameterized Nonlinear Models,Reject,"This paper takes results related to the convergence and implicit regularization of stochastic mirror descent, as previously applied within overparameterized linear models, and extends them to the nonlinear case.  Among other things, conditions are derived for guaranteeing convergence to a global minimizer that is (nearly) closest to the initialization with respect to a divergence that depends upon the mirror potential.  Overall the paper is well-written and likely at least somewhat accessible even for non-experts in this field.

That being said, two reviewers voted to reject while one chose accept; however, during the rebuttal period the accept reviewer expressed a somewhat borderline sentiment.  As for the reviewers that voted to reject, a common criticism was the perceived similarity with reference (Azizan and Hassibi, 2019), as well as unsettled concerns about the reasonableness of the assumptions involved (e.g., Assumption 1).  With respect to the former, among other similarities the proof technique from both papers relies heavily on Lemma 6.  It was then felt that this undercut the novelty somewhat.   

Beyond this though, even the accept reviewer raised an unsettled issue regarding the ease of finding an initialization point close to the manifold that nonetheless satisfies the conditions of Assumption 1.  In other words, as networks become more complex such that points are closer to the manifold of optimal solutions, further non-convexity could be introduced such that the non-negativity of the stated divergence becomes more difficult to achieve.  While the author response to this point is reasonable, it feels a bit like thoughtful speculation forged in the crunch time of a short rebuttal period, and possibly subject to change upon further reflection.  In this regard a less time-constrained revision could be beneficial (including updates to address the other points mentioned above), and I am confident that this work can be positively received at another venue in the near future.",Paper Decision
ML68zfPjyT,HJeqhA4YDS,Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators,Accept (Poster),"This paper studies the question of why a network trained to reproduce a single image often de-noises the image early in training. This an interesting question and, post discussion, all three reviewers agree that it will be of general interest to the community and is worth publishing. Therefore I recommend it be accepted. ",Paper Decision
I-3H8Eii5Q,HylthC4twr,Frequency Analysis for Graph Convolution Network,Reject,"This paper studies two-layer graph convolutional networks and two-layer multi-layer perceptions and develops quantitative results of their effect in signal processing settings. The paper received 3 reviews by experts working in this area. R1 recommends Weak Accept, indicating that the paper provides some useful insight (e.g. into when graph neural networks are or are not appropriate for particular problems) and poses some specific technical questions. In follow up discussions after the author response, R1 and authors agree that there are some over claims in the paper but that these could be addressed with some toning down of claims and additional discussion. R2 recommends Weak Accept but raises several concerns about the technical contribution of the paper, indicating that some of the conclusions were already known or are unsurprising. R2 concludes ""I vote for weak accept, but I am fine if it is rejected."" R3 recommends Reject, also questioning the significance of the technical contribution and whether some of the conclusions are well-supported by experiments, as well as some minor concerns about clarity of writing. In their thoughtful responses, authors acknowledge these concerns.  Given the split decision, the AC also read the paper. While it is clear it has significant merit, the concerns about significance of the contribution and support for conclusions (as acknowledged by authors) are important, and the AC feels a revision of the paper and another round of peer review is really needed to flesh these issues out. ",Paper Decision
7O7QiO5vFI,rkeu30EtvS,Network Deconvolution,Accept (Spotlight),"This paper presents a feature normalization method for CNNs by decorrelating channel-wise and spatial correlation simultaneously. Overall all reviewers are positive to the acceptance and I support their opinions. The idea and implementation is relatively straightforward but well-motivated and reasonable. Experiments are well-organized and intensive, providing enough evidence to convince its effectiveness in terms of final accuracy and convergence speed. Also, it’s analogy to biological center-surrounded structure is thought provoking. The novelty of the method seems somewhat incremental considering that there already exists a channel-wise decorrelation method, but I think the findings of the paper are interesting and valuable enough for ICLR community and would like to recommend acceptance.
Minor comments: I recommend authors to mention about zero-component analysis (ZCA) normalization, which has been a standard input normalization method for CIFAR datasets. I guess it is quite similar to the proposed method considering 1x1 convolution. Also, comparison with other recent normalization methods (e.g., Group Norm) would be useful. 
",Paper Decision
QQjsBl-6F,SJgdnAVKDH,Revisiting Self-Training for Neural Sequence Generation,Accept (Poster),"This paper analyzes self-training for sequence-to-sequence models and proposes a noisy version of self training. An empirical study shows the proposed noisy version improves results for machine translation and summarization tasks.

All reviewers appreciate the interesting contributions of the research, as well as clear writing. They also offer several comments for the revision of the paper. 

We look forward to seeing this paper presented at the conference!",Paper Decision
1mjEPEjums,SkxOhANKDr,Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense,Reject,"This paper presents a method to defend neural networks from adversarial attack. The proposed generative cleaning network has a trainable quantization module which is claimed to be able to eliminate adversarial noise and recover the original image. 
After the intensive interaction with authors and discussion, one expert reviewer (R3) admitted that the experimental procedure basically makes sense and increased the score to Weak Reject. Yet, R3 is still not satisfied with some details such as the number of BPDA iterations, and more importantly, concludes that the meaningful numbers reported in the paper show only small gains, making the claim of the paper less convincing. As authors seem to have less interest in providing theoretical analysis and support, this issue is critical for decision, and there was no objection from other reviewers. After carefully reading the paper myself, I decided to support the opinion and therefore would like to recommend rejection. 
",Paper Decision
VgHcd0kOv,S1lvn0NtwH,Mutual Exclusivity as a Challenge for Deep Neural Networks,Reject,"This paper presents an understudied bias known to exist in the learning patterns of children, but not present in trained NN models.  This bias is the mutual exclusivity bias: if the child already knows the word for an object, they can recognize that the object is likely not the referent when a new word is introduced.  So that is, the names of objects are mutually exclusive. 

The authors and reviewers had a healthy discussion. In particular, Reviewer 3 would have liked to have seen a new algorithm or model proposed, as well as an analysis of when ME would help or hurt.  I hope these ideas can be incorporated into a future submission of this paper.",Paper Decision
n2WNhczPSW,SJeD3CEFPH,Meta-Q-Learning,Accept (Talk),"This paper’s contribution is twofold: 1) it proposes a new meta-RL method that leverages off-policy meta-learning by importance weighting, and 2) it demonstrates that current popular meta-RL benchmarks don’t necessarily require meta-learning, as a simple non-meta-learning algorithm (TD3) conditioned on a context variable of the trajectory is competitive with SoTA meta-learning approaches. 

The reviewers all agreed that the approach is interesting and the contributions are significant. I’d like to thank the reviewers for engaging in a spirited discussion about this paper, both with each other and with the authors. There was also a disagreement about the semantics of whether the approach can be classified as “meta-learning”, but in my opinion this argument is orthogonal to the practical contributions. After the revisions and rebuttal, reviewers agreed that the paper was improved and increased their ratings as a result, with all recommending accept.

There’s a good chance this work will make an impactful contribution to the field of meta-reinforcement learning and therefore I recommend it for an oral presentation.
",Paper Decision
uUkTQ1oxmr,H1gL3RVtwr,CURSOR-BASED ADAPTIVE QUANTIZATION FOR DEEP NEURAL NETWORK,Reject,"This paper presents a method to compress DNNs by quantization. The core idea is to use NAS techniques to adaptively set quantization bits at each layer. The proposed method is shown to achieved good results on the standard benchmarks. 
Through our final discussion, one reviewer agreed to raise the score from ‘Reject’ to ‘Weak Reject’,  but still on negative side. Another reviewer was not satisfied with the author’s rebuttal, particularly regarding the appropriateness of training strategy and evaluation. Moreover, as reviewers pointed out, there were so many unclear writings and explanations in the original manuscript. Although we admit that authors made great effort to address the comments, the revision seems too major and need to go through another complete peer reviewing. As there was no strong opinion to push this paper, I’d like to recommend rejection. ",Paper Decision
KN0yKKJfOM,Bye8hREtvB,Natural Image Manipulation for Autoregressive Models Using Fisher Scores,Reject,"The paper proposes learning a latent embedding for image manipulation for PixelCNN by using Fisher scores projected to a low-dimensional space.
The reviewers have several concerns about this paper:
* Novelty
* Random projection doesn’t learn useful representation
* Weak evaluations
Since two expert reviewers are negative about this paper, I cannot recommend acceptance at this stage.
",Paper Decision
DqFCUHarBE,Hklr204Fvr,Towards a Deep Network Architecture for Structured Smoothness,Accept (Poster),"The AC has carefully looked at the paper/comments/discussion in order to arrive at this meta-review.

Looking over the paper, the FGL layer is an interesting idea, but its utility is only evaluated in a limited setting (fMRI data), rather that other types of images/data. Also, the approach seems to work on some of the fMRI datasets, on others the performance is on par with the baselines. 

Overall, the paper is borderline but the AC believes the paper would be a good contribution to the conference.",Paper Decision
4iv2us5Wol,HJxEhREKDH,On the Global Convergence  of Training Deep Linear ResNets,Accept (Poster),This paper provides further analysis of convergence in deep linear networks. I recommend acceptance. ,Paper Decision
9hggU6cY9,HJeVnCEKwH,A Closer Look at the Optimization Landscapes of Generative Adversarial Networks,Accept (Poster),This is an interesting contribution that sheds some light on a well-studied but still poorly understood problem. I think it might be of interest to the community.,Paper Decision
KtGtnI6RmP,BkxX30EFPS,Perceptual Generative Autoencoders,Reject,"The authors present a new training procedure for generative models where the target and generated distributions are first mapped to a latent space and the divergence between then is minimised in this latent space. The authors achieve state of the art results on two datasets.

All reviewers agreed that the idea was vert interesting and has a lot of potential. Unfortunately, in the initial version of the paper the main section (section 3) was not very clear with confusing notation and statements. I thank the authors for taking this feedback positively and significantly revising the writeup. However, even after revising the writeup some of the ideas are still not clear. In particular, during discussions between the AC and reviewers it was pointed out that the training procedure is still not convincing. It was not clear whether the heuristic combination of the deterministic PGA parts of the objective (3) with the likelihood/VAE based terms (9) and (12,13), was conceptually very sound. Unfortunately, most of the initial discussions with the authors revolved around clarity and once we crossed the ""clarity"" barrier there wasn't enough time to discuss the other technical details of the paper. As a result, even though the paper seems interesting, the initial lack of clarity went against the paper. 

In summary, based on the reviewer comments, I recommend that the paper cannot be accepted. 
",Paper Decision
r3T5OLjge,B1xm3RVtwB,Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning,Accept (Spotlight),"The method presented, the simplified action decoder, is a clever way of addressing the influence of exploratory actions in multi-agent RL. It's shown to enable state of the art performance in Hanabi, an interesting and relatively novel cooperative AI challenge. It seems, however, that the method has wider applicability than that.

All reviewers agree that this is good and interesting work. Reviewer 2 had some issues with the presentation of the results and certain assumptions, but the authors responded so as to alleviate any concerns.

This paper should definitely be accepted, if possible as oral.

 



",Paper Decision
fHagPsJ-vD,r1xMnCNYvB,"JAX MD: End-to-End Differentiable, Hardware Accelerated, Molecular Dynamics in Pure Python",Reject,"The paper is about a software library that allows for relatively easy simulation of molecular dynamics. The library is based on JAX and draws heavily from its benefits.

To be honest, this is a difficult paper to evaluate for everyone involved in this discussion. The reason for this is that it is an unconventional paper (software) whose target application centered around molecular dynamics. While the package seems to be useful for this purpose (and some ML-related purposes), the paper does not expose which of the benefits come from JAX and which ones the authors added in JAX MD. It looks like that most of the benefits are built-in benefits in JAX. Furthermore, I am missing a detailed analysis of computation speed (the authors do mention this in the discussion below and in a sentence in the paper, but this insufficient). Currently, it seems that the package is relatively slow compared to existing alternatives. 

Here are some recommendations:
1. It would be good if the authors focused more on ML-related problems in the paper, because this would also make sure that the package is not considered a specialized package that overfits to molecular dynamics.
2. Please work out the contribution/delta of JAX MD compared to JAX.
3. Provide a thorough analysis of the computation speed
4. Make a better case, why JAX MD should be the go-to method for practitioners.

Overall, I recommend rejection of this paper. A potential re-submission venue could be JMLR, which has an explicit software track.",Paper Decision
Cq69L7LnIp,r1xGnA4Kvr,Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks,Accept (Poster),"""Sleep"" is introduced as a way of increasing robustness in neural network training. To sleep, the network is converted into a spiking network and goes through phases of more and less intense activation. The results are quite good when it comes to defending against adversarial examples. Reviewers agree that the method is novel and interesting. Authors responded to the reviewers' questions (one of the reviewers had a quite extensive set of questions) satisfactorily, and improved the paper significantly in the process. I think the paper should be accepted on the grounds of novelty and good results.",Paper Decision
n18iTdrYgn,SJxZnR4YvB,Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication,Accept (Poster),"This paper tackles the problem of regret minimization in a multi-agent bandit problem, where distributed learning bandit algorithms collaborate in order to minimize their total regret. More specifically, the work focuses on efficient communication protocols and the regret corresponds to the communication cost. The goal is therefore to design protocols with little communication cost. The authors first establish lower bounds on the communication cost, and then introduce an algorithm with provable near-optimal regret.

The only concern with the paper is that ICLR may not be the appropriate venue given that this work lacks representation learning contributions. However, all reviewers being otherwise positive about the quality and contributions of this work, I would recommend acceptance.",Paper Decision
FqKuzO3BHh,r1genAVKPB,Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?,Accept (Spotlight),"The authors challenge the idea that good representation in RL lead are sufficient for learning good policies with an interesting negative result -- they show that there exist MDPs which require an exponential number of samples to learn a near-optimal policy even if a good-but-not-perfect representation is given to the agent for both value-based and policy-based learning.  Reviewers had some minor technical questions which were clarified sufficiently by the authors, leading to a consensus of the contribution and quality of this work.  Thus, I recommend this paper for acceptance.",Paper Decision
jiwyZ7_wxA,Bkxe2AVtPS,Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks,Accept (Poster),"Main description:  paper focuses on training neural networks using 8-bit floating-point numbers (FP8). The goal is highly motivated: training neural networks faster, with smaller memory footprint and energy consumption.

Discussions
reviewer 3:  gives a very short review and is not knowledagble in this area (rating is weak accept)
reviewer 4: well written and convincing paper, some minor technical flaws (not very knowledgable)
reviewer 1: interesting paper but argues not very practical (not very knowledgable)
reviewer 2: this is the most thorough and knowledable review, and here the authors like the scope of the paper and its interest to ICLR. 
Recommendation: going mainly by reviewer 2, i vote to accept this as a poster",Paper Decision
Jy9Cuw_ym,HyxJhCEFDS,Intriguing Properties of Adversarial Training at Scale,Accept (Poster),This paper studies the properties of adversarial training in the large scale setting. The reviewers found the properties identified by the paper to be of interest to the ICLR community - in particular the robustness community. We encourage the authors to release their models to help jumpstart future work building on this study.,Paper Decision
SgfYjlVkDJ,rklJ2CEYPH,Point Process Flows,Reject,"The paper proposed to use  normalizing flow to model point processes. However, the reviews find that the paper is incremental. There have been several works using deep generative models to temporal data, and the proposed method is a simple combination of well-established existing works without problem-specific adaptation. ",Paper Decision
yt5UcfQVh,SJx0oAEYwH,Cover Filtration and Stable Paths in the Mapper,Reject,"The paper proposes a filtration based on the covers of data sets and demonstrates its effectiveness in recommendation systems and explainable machine learning. The paper is theory focused, and the discussion was mainly centered around one very detailed and thorough review. The main concerns raised in the reviews and reiterated at the end of the rebuttal cycle was lack of clarity, relatively incremental contribution, and limited experimental evaluation. Due to my limited knowledge of this particular field, I base my recommendation mostly on R1's assessment and recommend rejecting this submission.",Paper Decision
9HBrs1VcZ4,Byx0iAEYPH,Fully Polynomial-Time Randomized Approximation Schemes for Global Optimization of High-Dimensional Folded Concave Penalized Generalized Linear Models,Reject,"Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects.
However, the novelty of this paper is rather marginal and given the  high competition at ICLR2020, this paper is unfortunately below the bar.
We hope that the reviewers' comments are useful for improving the paper for potential future publication.
",Paper Decision
MUcbBSnK54,H1g6s0NtwS,Learning Neural Surrogate Model for Warm-Starting Bayesian Optimization,Reject,"This paper is concerned with warm-starting Bayesian optimization (i.e. starting with a better surrogate model) through transfer learning among related problems. 

While the key motivation for warm-starting BO is certainly important (although not novel), there are important shortcomings in the way the method is developed and demonstrated. Firstly, the reviewers questioned design decisions, such as why combine NNs and GPs in this particular way or why the posterior variance of the hybrid model is not calculated. Moreover, there are issues with the experimental methodology that do not allow extraction of confident conclusions (e.g. repeating the experiments for different initial points is highly desirable). Finally, there are presentation issues. The authors replied only to some of these concerns, but ultimately the shortcomings seem to persist and hint towards a paper that needs more work.
",Paper Decision
vxKV0_7ABn,Hkl6i0EFPH,Scalable Differentially Private Data Generation via Private  Aggregation  of  Teacher Ensembles,Reject,"This paper addresses the problem of differential private data generator. The paper presents a novel approach called G_PATE which builds on the existing PATE framework. The main contribution is in using a student generator with an ensemble of teacher discriminators and in proposing a new private gradient aggregation mechanism which ensures differential privacy in the information flow from discriminator to generator.

Although the idea is interesting, there are significant concerns raised by the reviewers about the experiments and analysis done in the paper which seem to be valid and have not been addressed yet in the final revision. I believe upon making significant changes to the paper, this could be a good contribution. Thus, as of now, I am recommending a Rejection.",Paper Decision
c769NN_F9h,SJg2j0VFPB,Knowledge Graph Embedding: A Probabilistic Perspective and Generalization Bounds,Reject,"The paper provides a generalization error bound, which extends the results from PU learning, for the problem of knowledge graph completion. The authors assume a missing at random setting, and provide bounds on the triples (two nodes and an edge) that could be mistakes. Then the paper provides a maximum likelihood interpretation, as well as relations to existing knowledge graph completion methods. The problem setting is interesting, and the writing clear.

This discussion was extensive, with reviewers and authors following the spirit of ICLR and having a constructive discussion which resulted in improvements to the paper. However, there seems to be still some remaining improvements to be made in terms of clarity of presentation, as well as precision of the theoretical arguments.

Unfortunately, there are many strong submissions, and the paper as it currently stands does not satisfy the quality threshold of ICLR.",Paper Decision
nULDhM5_GG,rkehoAVtvS,Adversarial Paritial Multi-label Learning,Reject,"The paper considers a problem of clearly practical importance: multi-label classification where the ground truth label sets are noisy, specifically they are known (or at least assumed) to be a superset of the true ground truth labels. Learning a classifier in this setting require simultaneous identification of irrelevant labels. The proposed solution is a 4-part neural architecture, wherein a multi-label classifier is composed with a disambiguation or ""cleanup"" network, which is used as conditioning input to a conditional GAN which learns an inverse mapping, trained via an adversarial loss and also a least squares reconstruction loss (""generation loss""). 

Reviews were split 2 to 1 in favour of rejection, and the discussion phase did not resolve this split, as two reviewers did not revisit their assessments. R2 and R3 were concerned about the overall novelty and degeneracy of the inverse mapping problem. R1 increased their score after the rebuttal phase as they felt their concerns were addressed in comments (regarding issues surrounding the related work, the possibility of trivial solutions, and intuition for why the adversarial objective helps), but these were not addressed in the text as no updates were made.

I agree with the authors that PML is an important problem (one that receives perhaps less attention than it should from our community), and their empirical validation seems to support that their method outperforms (marginally, in many cases) methods from the literature. While the ablation study offers preliminary evidence that the inverse mapping is responsible for some of the gains, there are a lot of moving parts here and the authors haven't done a great job of motivating why this should help, or investigating why it in fact does. Based on the scores and my own reading of the paper, I'd recommend rejection at this time.

My own comments for the authors: I'd urge efforts to clarify the motivation for learning the inverse mapping, in particular adversarially (rather than just with the generation loss) in the text of the paper as you have in your rebuttals, and to improve the notation (the use of both D-tilde and D is confusing, and the omega notation seems unnecessary). I'm also not entirely clear whether the generator is stochastic or not, as the notation doesn't mention a randomly sampled latent variable (the traditional ""z"" here is a conditioning vector). Either way, the answer should be made more explicit.",Paper Decision
odZHvkfmx,Syejj0NYvr,Adversarial Interpolation Training: A Simple Approach for Improving Model Robustness,Reject,"Reviewers agree that the proposed method is interesting and achieves impressive results. Clarifications were needed in terms of motivating and situating the work. Thee rebuttal helped, but unfortunately not enough to push the paper above the threshold. We encourage the authors to further improve the presentation of their method and take into accounts the comments in future revisions.",Paper Decision
RQIVVZPpOZ,Syxss0EYPS,Agent as Scientist: Learning to Verify Hypotheses,Reject,"The authors propose an agent that can act in an RL environment to verify hypotheses about it, using hypotheses formulated as triplets of pre-condition, action sequence, and post-condition variables. Training then proceeds in multiple stages, including a pretraining phase using a reward function that encourages the agent to learn the hypothesis triplets. 

Strengths: Reviewers generally agreed it’s an important problem and interesting approach

Weaknesses: There were some points of convergence among reviewer comments: lack of connection to existing literature (ie to causal reasoning and POMDPs), and concerns about the robustness of the results (which were only reporting the max seeds).  Two reviewers also found the use of natural language to unnecessarily complicate their setup. Overall, clarity seemed to be an issue. Other comments concerned lack of comparisons, analyses, and suggestions for alternate methods of rewarding the agent (to improve understandability).  

The authors deserve credit for their responsiveness to reviewer comments and for the considerable amount of additional work done in the rebuttal period. However, these efforts ultimately didn’t satisfy the reviewers enough to change their scores. Although I find that the additional experiments and revisions have significantly strengthened the paper, I don't believe it's currently ready for publication at ICLR. I urge the authors to focus on clearly presenting and integrating these new results in a future submission, which I look forward to.
",Paper Decision
lxIfPfkqs,rJgqjREtvS,CRNet: Image Super-Resolution Using A Convolutional Sparse Coding  Inspired Network,Reject,"All three reviewers agreed that the paper should not be accepted. No rebuttal was offered, thus the paper is rejected.",Paper Decision
ir53kmmQyi,B1g5sA4twr,Deep Double Descent: Where Bigger Models and More Data Hurt,Accept (Poster),"This paper experimentally analyzes the double descent phenomenon for deep models. While, as the reviewers have mentioned, this phenomenon has been observed for some time, some of its specificities still elude us. As a consequence, I am happy to see this paper presented to ICLR.

That being said, given the original lack of proper references as well as the recent public announcements about this paper giving it visibility, I want to make it absolutely clear that this paper is accepted with the assumption that proper credit will be given to past work and that efforts will be made to draw connections between all these works.",Paper Decision
3-P6WGQ-0b,ByxKo04tvr,Multigrid Neural Memory,Reject,This paper investigates convolutional LSTMs with a multi-grid structure. This idea in itself has very little innovation and the experimental results are not entirely convincing.,Paper Decision
RZlvTM1gKO,H1lFsREYPS,ASGen: Answer-containing Sentence Generation to Pre-Train Question Generator for Scale-up Data in Question Answering,Reject,"Thanks for an interesting discussion. The paper introduces a sound question generation technique for QA. Reviewers are moderately positive, with low confidence. Some issues remain unresolved, though: While the UniLM comparison is currently not apples-to-apples, for example, nothing prevents the authors from using their method to pretrain UniLM. Currently, QA results are low-ish, and it is hard to accept a paper based solely on BLEU scores (questionable metric) for question generation (the task is but a means to an end). Moreover, the authors do not really discuss how their method relates to previous work (see Review 2 and the related work cited there; there's more, e.g., [0]). I also find it a little problematic that the paper completely ignores all work prior to 2017: The NLP community started organizing workshops on question generation in 2010. [1]",Paper Decision
X8j4e55gOu,rJgFjREtwr,Distribution-Guided Local Explanation for Black-Box Classifiers,Reject,"This paper proposed a method to estimate the instance-wise saliency map for image classification, for the purpose of improving the faithfulness of the explainer. Based on the U-net, two modifications are proposed in this work. While reviewer #3 is overall positive about this work, both Reviewer #1 and #2 rated weak reject and raised a number of concerns. The major concerns include the modifications either already exist or suffer potential issue. Reviewer #2 considered that the contributions are not enough for ICLR, and the performance improvement is marginal. The authors provided detailed responses to the reviewers’ concerns, which help to make the paper stronger, but did not change the rating. Given the concerns raised by the reviewers, the ACs agree that this paper can not be accepted at its current state.",Paper Decision
W5MOblSDJ,HklOo0VFDH,Decoding As Dynamic Programming For Recurrent Autoregressive Models,Accept (Poster),"This paper proposes an approximate inference approach for decoding in autoregressive models, based on the method of auxiliary coordinates,  which uses iterative factor graph approximations of the model.  The approach leads to nice improvements in performance on a text infilling task.  The reviewers were generally positive about this paper, though there was a concern that more baselines are needed and discussion was very limited following the author responses.  I tend to agree with the authors that their results are convincing on the infilling task.  The impact of the paper is a bit limited by the lack of experiments on more standard decoding tasks, which, as the authors point out, would be challenging as their approach is computationally demanding.  Overall I believe this would be an interesting contribution to the ICLR community.",Paper Decision
jgXzJRjl9X,Hkl_sAVtwr,Compressed Sensing with Deep Image Prior and Learned Regularization,Reject,"This paper proposes a compressed sensing (CS) method which employs deep image prior (DIP) algorithm to recovering signals for images from noisy measurements using untrained deep generative models.  A novel learned regularization technique is also introduced. Experimental results show that the proposed methods outperformed the existing work. The theoretical analysis of early stopping is also given. All reviewers agree that it is novel to combine the deep learning method with compressed sensing. The paper is well written and overall good. However the reviewers also proposed many concerns about method and the experiments, but the authors gave no rebuttal almost no revisions were made on the paper. I would suggest the author to consider the reviewers' concern seriously and resubmit the paper to another conference or journal.",Paper Decision
B01tR0Mwk,HJewiCVFPB,Gradient Surgery for Multi-Task Learning,Reject,"This paper presents a method for improving optimization in multi-task learning settings by minimizing the interference of gradients belonging to different tasks. 

While the idea is simple and well-motivated, the reviewers felt that the problem is still not studied adequately. The proofs are useful, but there is still a gap when it comes to practicality.

The rebuttal clarified some of the concerns, but still there is a feeling that (a) the main assumptions for the method need to be demonstrated in a more convincing way, e.g. by boosting the experiments as suggested with other MTL methods (b) by placing the paper better in the current literature and minimizing the gap between proofs/underlying assumptions and practical usefulness. 
",Paper Decision
xavPsH7wew,r1gPoCEKvH,SINGLE PATH ONE-SHOT NEURAL ARCHITECTURE SEARCH WITH UNIFORM SAMPLING,Reject,"This paper introduces a simple NAS method based on sampling single paths of the one-shot model based on a uniform distribution. Next to the private discussion with reviewers, I read the paper in detail. 

During the discussion, first, the reviewer who gave a weak reject upgraded his/her score to a weak accept since all reviewers appreciated the importance of neural architecture search and that the authors' approach is plausible. 
Then, however, it surfaced that the main claim of novelty in the paper, namely the uniform sampling of paths with weight-sharing, is not novel: Li & Talwalkar already introduced a uniform random sampling of paths with weight-sharing in the one-shot model in their paper ""Random Search and Reproducibility in NAS"" (https://arxiv.org/abs/1902.07638), which was on arXiv since February 2019 and has been published at UAI 2019. This was their method ""RandomNAS with weight sharing"".

The authors actually cite that paper but do not mention RandomNAS with weight sharing. This may be because their paper also has been on arXiv since March 2019 (6 weeks after the one above), and was therefore likely parallel work. Nevertheless, now, 9 months later, the situation has changed, and the authors should at least point out in their paper that they were not the first to introduce RandomNAS with weight sharing during the search, but that they rather study the benefits of that previously-introduced method.

The only real novelty in terms of NAS methods that the authors provide is to use a genetic algorithm to select the architecture with the best one-shot model performance, rather than random search. This is a relatively minor contribution, discussed literally in a single paragraph in the paper (with missing details about the crossover operator used; please fill these in). Also, this step is very cheap, so one could potentially just run random search longer. Finally, the comparison presented may be unfair: evolution uses a population size of 50, and Figure 2 plots iterations. It is unclear whether each iteration for random search also evaluated 50 samples; if not, then evolution got 50x more samples than random search. The authors should fix this in a new version of the paper.

The paper also appears to make some wrong claims in Section 2. For example, the authors write that gradient-based NAS methods like DARTS inherit the one-shot weights and fine-tune the discretized architectures, but all methods I know of actually retrain from scratch rather than fine-tuning. Also, equation (3) is not what DARTS does; that does a bi-level optimization.
In Section 3, the authors say that their single-path strategy corresponds to a dropout rate of 1. I do not think that this is correct, since a dropout rate of 1 drops every connection (and does not leave one remaining). All of these issues should be rectified.

The paper reports good results on ImageNet. Unfortunately, these may well be due to using a better training pipeline than other works, rather than due to a better NAS method (no code is available, so there is no way to verify this). On the other hand, the application to mixed-precision quantization is novel and interesting.

AnonReviewer2 asked about the correlation of the one-shot performance and the final evaluation performance, and this question was not answered properly by the authors. This question is relevant, because this correlation has been shown to be very low in several works (e.g., Sciuto et al: ""Evaluating the search phase of Neural Architecture Search"" (https://arxiv.org/abs/1902.08142), on arXiv since February 2019 and a parallel ICLR submission). In those cases, the proposed approach would definitely not work.

The high scores the reviewers gave were based on the understanding that uniform sampling in the one-shot model was a novel contribution of this paper. Adjusting for that, the real score is much lower and right at the acceptance threshold. After a discussion with the PCs, due to limited capacity, the recommendation is to reject the current version. I encourage the authors to address the issues identified by the reviewers and in this meta-review and to submit to a future venue. ",Paper Decision
3mUv9BsMQK,S1l8oANFDH,Synthesizing Programmatic Policies that Inductively Generalize,Accept (Poster),"The authors consider control tasks that require ""inductive generalization"", ie                                                     
the ability to repeat certain primitive behaviors.                                                                                 
They propose state-machine machine policies, which switch between low-level                                                      
policies based on learned transition criteria.                                                                                     
The approach is tested on multiple continuous control environments and compared to                                                   
RL baselines as well as an ablation.                                                                                                          
                                                                                                                                   
The reviewers appreciated the general idea of the paper.                                                                           
During the rebuttal, the authors addressed most of the issues raised in the                                                        
reviews and hence reviewers increased their score.                                                                                 
                                                                                                                                   
The paper is marginally above acceptance.                                                                                          
On the positive side: Learning structured policies is clearly desirable but                                                        
difficult and the paper proposes an interesting set of ideas to tackle this                                                       
challenge.                                                                                                                         
My main concern about this work is:                                                                                                
The approach uses the true environment simulator, as the                                                                           
training relies on gradients of the reward function.                                                                               
This makes the tasks into planning and not an RL problems; this needs to be                                                       
highlighted, as it severly limits its applicability of the proposed approach.                                                                              
Furthermore, this also means that the comparison to the model-free PPO baselines                                                   
is less meaningful.                                                                                                                
The authors should clear mention this.                                                                                             
Overall however, I think there are enough good ideas presented here to warrant                                                     
acceptance. ",Paper Decision
aPQQYnrGKU,r1eIiCNYwS,Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention,Accept (Poster),This work examines a problem that is of considerable interest to the community and does a good job of presenting the work. The AC recommends acceptance.,Paper Decision
-QFzvWn3Q2,SylUiREKvB,Variational Hyper RNN for Sequence Modeling,Reject,"The paper proposes a neural network architecture that uses a hypernetwork (RNN or feedforward) to generate weights for a network (variational RNN), that models sequential data. An empirical comparison of a large number of configurations on synthetic and real world data show the promise of this method.

The authors have been very responsive during the discussion period, and generated many new results to address some reviewer concerns. Apart from one reviewer, the others did not engage in further discussion in response to the authors updating their paper.

The paper provides a tweak to the hypernetwork idea for modeling sequential data. There are many strong submissions at ICLR this year on RNNs, and the submission in its current state unfortunately does not pass the threshold.",Paper Decision
JQLVzB7tgd,HklBjCEKvH,Generalization through Memorization: Nearest Neighbor Language Models,Accept (Poster),"This paper proposes an idea of using a pre-trained language model on a potentially smaller set of text, and interpolating it with a k-nearest neighbor model over a large datastore. The authors provide extensive evaluation and insightful results. Two reviewers vote for accepting the paper, and one reviewer is negative. After considering the points made by reviewers, the AC decided that the paper carries value for the community and should be accepted.",Paper Decision
J4zGVDFgq,S1gSj0NKvB,Comparing Rewinding and Fine-tuning in Neural Network Pruning,Accept (Talk),Reviewers unanimously accepted this paper. ,Paper Decision
omkM0NLc-k,SklViCEFPH,Simple is Better: Training an End-to-end Contract Bridge Bidding Agent without Human Knowledge,Reject,"This paper proposes a new training method for an end-to-end contract bridge bidding agent. Reviewers R2 and R3 raised concerns regarding limited novelty and also experimental results not being convincing. R2's main objection is that the paper has ""strong SOTA performance with a simple model, but empirical study are rather shallow.""

Based on their recommendations, I recommend to reject this paper.
",Paper Decision
A5QD2HHQS,BJlNs0VYPB,The Sooner The Better: Investigating Structure of Early Winning Lottery Tickets,Reject,This paper does extensive experiments to understand the lottery ticket hypothesis. The lottery ticket hypothesis is that there exist sparse sub-networks inside dense large models that achieve as good accuracy as the original model. The reviewers have issues with the novelty and significance of these experiments. They felt that it didn't shed new scientific light. They felt that epochs needed to do early detection was still expensive. I recommend doing further studies and submitting it to another venue.,Paper Decision
KL0AP8h-fr,HklmoRVYvr,Long History Short-Term Memory for Long-Term Video Prediction,Reject,"The paper proposes a new recurrent unit which incorporates long history states to learn longer range dependencies for improved video prediction. This history term corresponds to a linear combination of previous hidden states selected through a soft-attention mechanism and can be directly added to ConvLSTM equations that compute the IFO gates and the new state. The authors perform empirical validation on the challenging KTH and BAIR Push datasets and show that their architecture outperforms existing work in terms of SSIM, PSNR, and VIF.
The main issue raised by the reviewers is the incremental nature of the work and issues in the empirical evaluation which do not support the main claims in the paper. After the rebuttal and discussion phase the reviewers agree that these issues were not adequately resolved and the work doesn’t meet the acceptance bar. I will hence recommend the rejection of this paper. Nevertheless, we encourage the authors improve the manuscript by addressing the remaining issues in the empirical evaluation.",Paper Decision
fOo9MWSmX4,S1xXiREKDB,Adversarial training with perturbation generator networks,Reject,"This paper proposes to use the GAN (i.e., minimax) framework for adversarial training, where another neural network was introduced to generate the most effective adversarial perturbation by finding the weakness of the classifier. The rebuttal was not fully convincing on why the proposed method should be superior to existing attacks.",Paper Decision
KJye-gIiC5,rJeQoCNYDS,Single Episode Policy Transfer in Reinforcement Learning,Accept (Poster),"This is an interesting paper that is concerned with single episode transfer to reinforcement learning problems with different dynamics models, assuming they are parameterised by a latent variable. Given some initial training tasks to learn about this parameter, and a new test task, they present an algorithm to probe and estimate the latent variable on the test task, whereafter the inferred latent variable  is used as input to a control policy.

There were several issues raised by the reviewers. Firstly, there were questions with the number of runs and the baseline implementations, which were all addressed in the rebuttals. Then, there were questions around the novelty and the main contribution being wall-clock time. These issues were also adequately addressed.

In light of this, I recommend acceptance of this paper.",Paper Decision
5yjKtkaCxW,BygfiAEtwS,Inducing Stronger Object Representations in Deep Visual Trackers,Reject,"This paper proposes to learn a visual tracking network for an object detection loss as well as the ordinary tracking objective for enhancing the reliability of the tracking network.  The reviewers were unanimous in their opinion that the paper should not be accepted to ICLR in its current form.  A main concern is that the proposed method shows improvement over a relatively weak base system.  Although the author response proposed to include additional analysis, but the reviewers felt that without the additional analysis already included it was not possible to change the overall review score.",Paper Decision
D3waFvsOgg,SkgGjRVKDS,Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization,Accept (Poster),"This work introduces Moving Average Batch Normalization (MABN) method to address performance issues of batch normalization in small batch cases. The method is theoretically analyzed and empirically verified on ImageNet and COCO.
Some issues were raised by the reviewers, such as restrictive nature of some of the assumptions in the analysis as well as performance degradation due lack of centralizing feature maps. Nevertheless, all the reviewers found the contributions of this paper interesting and important, and they all recommended accept.
",Paper Decision
uyGglX0qW,HJe-oRVtPB,STABILITY AND CONVERGENCE THEORY FOR LEARNING RESNET: A FULL CHARACTERIZATION,Reject,"The article studies the stability of ResNets in relation to initialisation and depth. The reviewers found that this is an interesting article with important theoretical and experimental results. However, they also pointed out that the results, while good, are based on adaptations of previous work and hence might not be particularly impactful. The reviewers found that the revision made important improvements, but not quite meeting the bar for acceptance, pointing out that the presentation and details in the proofs could still be improved. ",Paper Decision
KrZbySgxv4,HklWsREKwr,Training Deep Neural Networks with Partially Adaptive Momentum,Reject,"This paper extends Adam by adding another hyperparameter that allows the second moments to be raised to a power p other than 1/2. This certainly seems worth trying. The paper is well written, and the experiments seem reasonably complete. But some of the reviewers and I feel like the contribution is a bit obvious and incremental. The ""small learning rate dilemma"" needs a bit more justification: since the denominator has a different scale, the learning rates for different values of p are not directly comparable. It could very well be that Adam's learning rate has to be set too small due to some outlier dimensions, but showing this would require some evidence. From the experiments, it does seem like there's some practical benefit, though it's not terribly surprising that adding an additional hyperparameter will result in improved performance. The reviewers think the theoretical analysis is a straightforward extension of prior work (though I haven't checked myself). Overall, it doesn't seem to me like the contribution is quite enough for publication at ICLR.
",Paper Decision
bmukdcHZsa,ryxgsCVYPr,NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension,Accept (Poster),"This paper extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. All reviewers agree that this approach is interesting (verification and validation) and experiments are solid. One of the reviewers raised concerns are promptly answered by authors, raising the average score to accept. 
",Paper Decision
fAg2ck3d9B,HylloR4YDr,Learning Latent Representations for Inverse Dynamics using Generalized Experiences,Reject,"Solid, but not novel enough to merit publication.  The reviewers agree on rejection, and despite authors' adaptation, the paper requires more work and broader experimentation for publication.",Paper Decision
3o0KRjMUCO,Sklgs0NFvr,Learning The Difference That Makes A Difference With Counterfactually-Augmented Data,Accept (Spotlight),"This paper introduces the idea of a counterfactually augmented dataset, in which each example is paired with a manually constructed example with a different label that makes the minimal possible edit to the original example that makes that label correct. The paper justifies the value of these datasets as an aid in both understanding and building classifiers that are robust to spurious features, and releases two small examples.

On my reading, this paper presents a very substantially new idea that is relevant to a major ongoing debate in the applied machine learning literature: How do we build models that learn some intended behavior, where the primary evidence we have of that behavior comes in the form of datasets with spurious correlations/artifacts.

One reviewer argued for rejection on the grounds that dataset papers are not appropriate for publication at a main conference. I don't find that argument compelling, and I'm also not sure that it's accurate to call this paper primarily a dataset paper. We could not reach a complete consensus after further discussion. The other reviews raised some additional concerns about the paper, but the revised manuscript appears to have address them to the extent possible.",Paper Decision
6uCHmWKy_o,HJgkj0NFwr,Differentiable Architecture Compression,Reject,The paper is on the borderline. A rejection is proposed due to the percentage limitation of ICLR.  ,Paper Decision
Y6qohXMNn,Hkl1iRNFwS,The Early Phase of Neural Network Training,Accept (Poster),"This paper studies numerous ways in which the statistics of network weights evolve during network training.  Reviewers are not entirely sure what conclusions to make from these studies, and training dynamics can be strongly impacted by arbitrary choices made in the training process.  Despite these issues, the reviewers think the observed results are interesting enough to clear the bar for publication.",Paper Decision
L79S-TI9yw,rJl05AVtwB,Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks,Reject,The submission is proposed a rejection based on majority review.,Paper Decision
NgABgojUX,ryl0cAVtPH,On The Difficulty of Warm-Starting Neural Network Training,Reject,"The paper addresses the question of why warm starting could result in worse generalization ability than training from scratch.  The reviewers agree that increasing the circumstances in which warm starting could be applied is of interest, in particular to reduce training time and computational resources.  However, the reviewers were unanimous in their opinion that the paper is not suitable for publication at ICLR in its current form.  Concerns included that the analysis was not sufficiently focused and the experiments too small scale.  As the analysis component of the paper was considered to be limited, the experimental results were insufficient on the balance to push the paper to an acceptable state.",Paper Decision
YPYO9DUiwz,HJgCcCNtwH,NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks,Reject,"This work proposes new initialization and layer topologies for training a priori sparse networks. Reviewers agreed that the direction is interesting and that the paper is well written. Additionally the theory presented on the toy matrix reconstruction task helped motivate the proposed approach. However, it is also necessary to validate the new approach by comparing with existing sparsity literature on standard benchmarks. I recommend resubmitting with the additional experiments suggested by the reviewers.",Paper Decision
nZ-cM6aGZ3,Bkga90VKDB,Distilled embedding: non-linear embedding factorization using knowledge distillation,Reject,"This paper proposes to further distill token embeddings via what is effectively a simple autoencoder with a ReLU activation. All reviewers expressed concerns with the degree of technical contribution of this paper. As Reviewer 3 identifies, there are simple variants (e.g. end-to-end training with the factorized model) and there is no clear intuition for why the proposed method should outperform its variants as well as the other baselines (as noted by Reviewer 1). Reviewer 2 further expresses concerns about the merits of the propose approach over existing approaches, given the apparently small effect size of the improvement (let alone the possibility that the improvement may not in fact be statistically significant).
",Paper Decision
4kLBcHijCo,HylpqA4FwS,RNNs Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?,Accept (Poster),"In this paper, the authors propose the incremental RNN, a novel recurrent neural network architecture that resolves the exploding/vanishing gradient problem. While the reviewers initially had various concerns, the paper has been substantially improved during the discussion period and all questions by the reviewers have been resolved. The main idea of the paper is elegant, the theoretical results interesting, and the empirical evaluation extensive. The reviewers and the AC recommend acceptance of this paper to ICLR-2020.",Paper Decision
Fub5wqPRKc,r1ln504YvH,Actor-Critic Approach for Temporal Predictive Clustering,Reject,"This paper proposes a reinforcement learning approach to clustering time-series data. The reviewers had several questions related to clarity and concerns related to the novelty of the method, the connection to RL, and experimental results. While the authors were able to address some of these questions and concerns in the rebuttal, the reviewers believe that the paper is not quite ready for publication.",Paper Decision
6PHmpIITG4,SkxoqRNKwr,Adversarial Privacy Preservation under Attribute Inference Attack,Reject,"While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.

The most significant concerns raised were about the strength of the experiments, and choice of appropriate baselines.",Paper Decision
LKIFZTFC7,Hklo5RNtwS,Behavior-Guided Reinforcement Learning,Reject,"The authors introduce the idea of using Wasserstein distances over latent ""behavioral spaces"" to measure the similarity between two polices, for use in RL algorithms.  Depending on the choice of behavioral embedding, this method produces different regularizers for policy optimization, in some cases recovering known algorithms such as TRPO.  This approach generalizes ideas of similarity used in many common algorithms like TRPO, making these ideas widely applicable to many policy optimization approaches.  The reviewers all agree that the core idea is interesting and would likely be useful to the community.  However, a primary concern that was not sufficiently resolved during the rebuttal period was the experimental evaluation -- both the ability of the experiments to be replicated, as well as whether they provide sufficient insight into how/why the algorithm performs.  Thus, I recommend rejection of this paper at this time.",Paper Decision
9r8If1oa_V,Bkgq9ANKvB,Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates,Reject,"Thank you very much for the detailed feedback to the reviewers, which helped us better understand your paper.
Thanks also for revising the manuscript significantly; many parts were indeed revised. 
However, due to the major revision, we find more points to be further discussed, which requires another round of reviews/rebuttals.
For this reason, we decided not to accept this paper.
We hope that the reviewers' comments are useful for improving the paper for potential future publication.
",Paper Decision
gAHR7n3M_,HJe9cR4KvB,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,Reject,"While the revised paper was better and improved the reviewers assessment of the work, the paper is just below the threshold for acceptance. The authors are strongly encouraged to continue this work.",Paper Decision
CIwDpoJ9ZX,SklKcRNYDH,Extreme Tensoring for Low-Memory Preconditioning ,Accept (Poster),"Post author rebuttal the score of this paper increased.
Discussions with reviewers were substantive and the AC recommends acceptance.",Paper Decision
mE43mrSImO,SJlYqRNKDS,Blockwise Adaptivity:  Faster Training and Better Generalization in Deep Learning,Reject,"The authors propose an adaptive block-wise coordinate descent method and claim faster convergence and lower generalization error. While the reviewers agreed that this method may work well in practice, they had several concerns about the relevance of the theory and strength of the empirical results. After considering the author responses, the reviewers have agreed that this paper is not yet ready for publication. ",Paper Decision
yDAni7zAxO,BkxdqA4tvB,Collapsed amortized variational inference for switching nonlinear dynamical systems,Reject,This is an interesting paper on an important topic.  The reviewers identified a variety of issues both before and after the feedback period; I urge the authors to consider their comments as they continue to refine and extend their work.,Paper Decision
MYQvPh7Uno,H1e_cC4twS,Non-Autoregressive Dialog State Tracking,Accept (Poster),"(Please note that I am basing the meta-review on two reviews plus my own thorough read of the paper)
This paper proposes an interesting adaptation of the non-autoregressive neural encoder-decoder models previously proposed for machine translation to dialog state tracking. Experimental results demonstrate state-of-the-art for the MultiWOZ, multi-domain dialog corpus. The reviewers suggest that while the NA approach is not novel, author's adaptation of the approach to dialog state tracking and detailed experimental analysis are interesting and convincing. Hence I suggest accepting the paper as a poster presentation.",Paper Decision
zYv51yqavK,BJlOcR4KwS,Channel Equilibrium Networks,Reject,"The paper proposed Channel Equilibrium (CE) to overcome the over-sparsity problem in CNNs using 'BN+ReLU'. Experiments on ImageNet and COCO show its effectiveness by introducing little computational complexity. However the reviewers pointed a number of problems in the writing and the clarity of the paper. Although the authors addressed all the se concerns in details and agreed to make revisions in the paper, it's better for the authors to submit the revised version to another opportunity.",Paper Decision
n1hGYrgf6,B1eP504YDr,Independence-aware Advantage Estimation,Reject,"Policy gradient methods typically suffer from high variance in the advantage function estimator. The authors point out independence property between the current action and future states which implies that certain terms from the advantage estimator can be omitted when this property holds. Based on this fact, they construct a novel important sampling based advantage estimator. They evaluate their approach on simple discrete action environments and demonstrate reduced variance and improved performance.

Reviewers were generally concerned about the clarity of the technical exposition and the positioning of this work with respect to other estimators of the advantage function which use control variates. The authors clarified differences between their approach and previous approaches using control variance and clarified many of the technical questions that reviewers asked about.

I am not convinced by the merits of this approach. While, I think the fundamental idea is interesting, the experiments are limited to simple discrete environments and no comparison is made to other control variate based approaches for reducing variance. Furthermore, due to the function approximation which introduces bias, the method should be compared to actor critic methods which directly estimate the advantage function. Finally, one of the advantages of on policy policy gradient methods is its simplicity. This method introduces many additional steps and parameters to be learned. The authors would need to demonstrate large improvements in sample efficiency on more complex tasks to justify this added complexity. At this time, I do not recommend this paper for acceptance.",Paper Decision
k7USU4g358,Bkxv90EKPB,Bayesian Meta Sampling for Fast Uncertainty Adaptation,Accept (Poster),"This paper presents a meta-learning algorithm that represents uncertainty both at the meta-level and at the task-level. The approach contains an interesting combination of techniques. The reviewers raised concerns about the thoroughness of the experiments, which were resolved in a convincing way in the rebuttal. Concerns about clarity remain, and the authors are *strongly encouraged* to revise the paper throughout to make the presentation more clear and understandable, including to readers who do not have a meta-learning background. See the reviewer's comments for further details on how the organization of the paper and the presentation of the ideas can be improved.",Paper Decision
OuhbDBefb,rkeUcRNtwS,Salient Explanation for Fine-grained Classification,Reject,"This paper is interested in finding salient areas in a deep learning image classification setting. The introduced method relies on masking images using Gaussian Gaussian light and shadow (GLAS) and estimating its impact on output.

As noted by all reviewers, the paper is too weak for publication in its current form:
- Novelty is very low.
- Experimental section not convincing enough, in particular some metrics are missing.
- The writing should be improved.",Paper Decision
9PjfTh7pUC,rklHqRVKvH,Harnessing Structures for Value-Based Planning and Reinforcement Learning,Accept (Talk),"The paper shows empirical evidence that the the optimal action-value function Q* often has a low-rank structure. It uses ideas from the matrix estimation/completion literature to provide a modification of value iteration that benefits from such a low-rank structure.
The reviewers are all positive about this paper. They find the idea novel and the writing clear.
There have been some questions about the relation of this concept of rank to other definitions and usage of rank in the RL literature.
The authors’ rebuttal seem to be satisfactory to the reviewers. Given these, I recommend acceptance of this paper.",Paper Decision
C8_f8GFcla,S1g490VKvB,The Dynamics of Signal Propagation in Gated Recurrent Neural Networks,Reject,"Using ideas from mean-field theory and statistical mechanics, this paper derives a principled way to analyze signal propagation through gated recurrent networks.  This analysis then allows for the development of a novel initialization scheme capable of mitigating subsequent training instabilities.  In the end, while reviewers appreciated some of the analytical insights provided, two still voted for rejection while one chose accept after the rebuttal and discussion period.  And as AC for this paper, I did not find sufficient evidence to overturn the reviewer majority for two primary reasons.

First, the paper claims to demonstrate the efficacy of the proposed initialization scheme on multiple sequence tasks, but the presented experiments do not really involve representative testing scenarios as pointed out by reviewers.  Given that this is not a purely theoretical paper, but rather one suggesting practically-relevant initializations for RNNs, it seems important to actually demonstrate this on sequence data people in the community actually care about.  In fact, even the reviewer who voted for acceptance conceded that the presented results were not too convincing (basically limited to toy situations involving Cifar10 and MNIST data).

Secondly, all reviewers found parts of the paper difficult to digest, and while a future revision has been promised to provide clarity, no text was actually changed making updated evaluations problematic.  Note that the rebuttal mentions that the paper is written in a style that is common in the physics literature, and this appears to be a large part of the problem.  ICLR is an ML conference and in this respect, to the extent possible it is important to frame relevant papers in an accessible way such that a broader segment of this community can benefit from the key message.  At the very least, this will ensure that the reviewer pool is more equipped to properly appreciate the contribution.  My own view is that this work can be reframed in such a way that it could be successfully submitted to another ML conference in the future.",Paper Decision
Qt_JZF7kWT,SyxV9ANFDH,Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality,Accept (Poster),The authors propose a modification of the statistical recurrent unit for modelling mutliple time series and show that it can be very useful in practice for identifying granger causality when the time series are non-linearly related. The contributions are primarily conceptual and empirical. The reviewers agree that this is a useful contribution in the causality literature.,Paper Decision
XpXgxYs-Rc,rJgE9CEYPS,Discriminability Distillation in Group Representation Learning,Reject,"This paper proposes discriminability distillation learning (DDL) for learning group representations. The core idea is to learn a discriminability weight for each instance which are a member of a group, set or sequence. The discriminability score is learned by first training a standard supervised base model and using the features from this model, computing class-centroids on a proxy set, and computing the iter and intra-class distances. A function of these distance computations are then used as supervision for a distillation style small network (DDNet) which may predict the discriminability score (DDR score). A group representation is then created through a combination of known instances, weighted using their DDR score. The method is validated on face recognition and action recognition.

This work initially received mixed scores, with two reviewers recommending acceptance and two recommending rejection. After reading all the reviews, rebuttals, and discussions, it seems that a key point of concern is low clarity of presentation. During the rebuttal period, the authors have revised their manuscript and interacted with reviewers. One reviewer has chosen to update their recommendation to weak acceptance in response. The main unresolved issues are related to novelty and experimental evaluation. Namely, for novelty comparison and discussion against attention based approaches and other metric learning based approaches would benefit the work, though the proposed solution does present some novelty. For the experiments there was a suggestion to evaluate the model on more complex datasets where performance is not already maxed out. The authors have provided such experiments during the rebuttal period.

Despite the slight positive leanings post rebuttal, the ACs have discussed this case and determine the paper is not ready for publication.",Paper Decision
bguekeQ5vI,B1eQcCEtDB,"Calibration, Entropy Rates, and Memory in Language Models",Reject,"This paper shows empirically that the state-of-the-art language models have a problem of increasing entropy when generating long sequences. The paper then proposes a method to mitigate this problem. As the authors re-iterated through their rebuttal, this paper approaches this problem theoretically, rather than through a comprehensive set of empirical comparisons.

After discussions among the reviewers, this paper is not recommended to be accepted. Some skepticism and concerns remain as to whether the paper makes sufficiently clear and proven theoretical contributions.

We all appreciate the approach and potential of this paper and encourage the authors to re-submit a revision to a future related venue.",Paper Decision
kSlI-l19e,ryxf9CEKDr,Efficient Saliency Maps for Explainable AI,Reject,"The paper presents an efficient approach to computer saliency measures by exploiting saliency map order equivalence (SMOE), and visualization of individual layer contribution by a layer ordered visualization of information. 

The authors did a good job at addressing most issues raised in the reviews. In the end, two major concerns remained not fully addressed: one is the motivation of efficiency, and the other is how much better SMOE is compared with existing statistics. I think these two issue also determines how significance the work is. 

After discussion, we agree that while the revised draft pans out to be a much more improved one, the work itself is nothing groundbreaking. Given many other excellent papers on related topics, the paper cannot make the cut for ICLR. ",Paper Decision
Nqg-yyai43,BkgzqRVFDr,Reinforcement Learning with Probabilistically Complete Exploration,Reject,"This was a borderline paper, with both pros and cons.  In the end, it was not considered sufficiently mature to accept in its current form.  The reviewers all criticized the assumptions needed, and lamented the lack of clarity around the distinction between reinforcement learning and planning.  The paper requires a clearer contribution, based on a stronger justification of the approach and weakening of the assumptions.  The submitted comments should be able to help the authors strengthen this work.",Paper Decision
Z-hWwUcg,H1ebc0VYvH,Unaligned Image-to-Sequence Transformation with Loop Consistency,Reject,"The main concern raised by the reviewers is limited experimental work, and there is no rebuttal.",Paper Decision
Kz6c7rAlct,rkegcC4YvS,Removing the Representation Error of GAN Image Priors Using the Deep Decoder,Reject,"The paper introduces a method for removing what they call representation error and apply the method to super resolution and compressive sensing. 

The reviewers have provided constructive feedback. The reviewers like aspects of the paper but are also concerned with various shortcomings. The consensus is that the paper is not ready for publication as it stands.

Rejection is therefore recommended with strong encouragement to keep working on the method and submit elsewhere.",Paper Decision
JGy5iT1Z_l,rJxlc0EtDr,MEMO: A Deep Network for Flexible Combination of Episodic Memories,Accept (Poster),"The authors introduce a new associative inference task from cognitive psychology, show shortcomings of current memory-augmented architectures, and introduce a new memory architecture that performs better with respect to the task. The reviewers like the motivation and thought the experimental results were strong, although they also initially had several questions and pointed to areas of the paper which lacked clarity. The authors updated the paper in response to the reviewer's questions and increased the clarity of the paper. The reviewers are satisfied and believe the paper should be accepted.",Paper Decision
_jlqGvwoJx,SJxy5A4twS,Superbloom: Bloom filter meets Transformer,Reject,"This paper presents to integrate the codes based on multiple hashing functions with Transformer networks to reduce vocabulary sizes in input and output spaces. Compared to non-hashed models, it enables training more complex and powerful models with the same number of overall parameters, thus leads to better performance. 
Although the technical contribution is limited considering hash-based approach itself is rather well-known and straightforward, all reviewers agree that some findings in the experiments are interesting. On the cons side, two reviewers were concerned about unclear presentation regarding the details of the method. More importantly, the proposed method is only evaluated on non-standard tasks without comparison to other previous methods. Considering that the main contribution of the paper is in empirical side, I agree it is necessary to evaluate the method on more standard benchmarking tasks in NLP where there should be many other state-of-the-art methods of model compression. For these reasons, I’d like to recommend rejection. ",Paper Decision
gvwBo0vKo6,Hye190VKvH,Longitudinal Enrichment of Imaging Biomarker Representations for Improved Alzheimer's Disease Diagnosis,Reject,"This paper proposes to overcome the issue of inconsistent availability of longitundinal data via the combination of leveraging principal components analysis and locality preserving projections. All three reviewers express significant reservations regarding the technical writing in the paper. As it stands, this paper is not ready for publication. 
",Paper Decision
xtD-4hfVb1,HJgCF0VFwr,Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks,Accept (Poster),"This paper proposes a novel approach for pruning deep neural networks using non-parametric statistical tests to detect 3-way interactions among two nodes and the output. While the reviewers agree that this is a neat idea, the paper has been limited in terms of experimental validation. The authors provided further experimental results during the discussion period and the reviewers agree that the paper is now acceptable for publication at ICLR-2020. ",Paper Decision
Tb5ZiIQwT,SJlRF04YwB,Generating Semantic Adversarial Examples with Differentiable Rendering,Reject,"The authors present a way for generating adversarial examples using discrete perturbations, i.e., perturbations that, unlike pixel ones, carry some semantics. Thus, in order to do so, they assume the existence of an inverse graphics framework. Results are conducted in the VKITTI dataset. Overall, the main serious concern expressed by the reviewers has to do with the general applicability of this method, since it requires an inverse graphics framework, which all-in-all is not a trivial task, so it is not clear how such a method would scale to more “real” datasets. A secondary concern has to do with the fact that the proposed method seems to be mostly a way to perform semantic data-augmentation rather than a way to avoid malicious attacks. In the latter case, we would want to know something about the generality of this method (e.g., what happens a model is trained for this attacks but then a more pixel-based attack is applied). As such, I do not believe that this submission is ready for publication at ICLR. However, the technique is an interesting idea it would be interesting if a later submission would provide empirical evidence about/investigate the generality of this idea. ",Paper Decision
e7I1HD0LMt,r1xpF0VYDS,Quantum algorithm for finding the negative curvature direction,Reject,"There was some support for the ideas presented, but this paper was on the borderline, and ultimately not able to be accepted for publication at ICLR.

Concerns raised included level of novelty, and clarity of the exposition to an ML audience.",Paper Decision
FsRc1dXHfa,SJe3KCNKPr,Dual-module Inference for Efficient Recurrent Neural Networks,Reject,"This paper presents an efficient RNN architecture that dynamically switches big and little modules during inference. In the experiments, authors demonstrate that the proposed method achieves favorable speed up compared to baselines, and the contribution is orthogonal to weight pruning. 
All reviewers agree that the paper is well-written and that the proposed method is easy to understand and reasonable. However, its methodological contribution is limited because the core idea is essentially the same as distillation, and dynamically gating the modules is a common technique in general. Moreover, I agree with the reviewers that the method should be compared with more other state-of-the-art methods in this context. Accelerating or compressing DNNs are intensively studied topics and there are many approaches other than weight pruning, as authors also mention in the paper. As the possible contribution of the paper is more on the empirical side, it is necessary to thoroughly compare with other possible approaches to show that the proposed method is really a good solution in practice. For these reasons, I’d like to recommend rejection. 
",Paper Decision
k_V3X1csgZ,rJl3YC4YPH,GUIDEGAN:  ATTENTION  BASED  SPATIAL  GUIDANCE FOR  IMAGE-TO-IMAGE TRANSLATION,Reject,"The paper proposes to augment the conditional GAN discriminator with an attention mechanism, with the aim to  help the generator, in the context of image to image translation. The reviewers raise several issues in their reviews. One theoretical concern has to do with how the training of the attention mechanism (which seems to be collaborative) would interact with the minimax, zero-sum nature of a GAN objective; another with the discrepancy in how the attention map is used during training and testing. The experimental results were not significant enough, and the reviewers also recommend additional experiment results to clearly demonstrate the benefit of the method. ",Paper Decision
VbIIy0sBW5,SkgjKR4YwH,MixUp as Directional Adversarial Training,Reject,"This paper builds a connection between MixUp and adversarial training. It introduces untied MixUp (UMixUp), which generalizes the methods of MixUp. Then, it also shows that DAT and UMixUp use the same method of MixUp for generating samples but use different label mixing ratios. Though it has some valuable theoretical contributions, I agree with the reviewers that it’s important to include results on adversarial robustness, where both adversarial training and MixUp are playing an important role.",Paper Decision
Pwy8N0o4U,HyljY04YDB,Towards Interpretable Molecular Graph Representation Learning,Reject,"The paper introduces a new pooling approach ""Laplacian pooling"" for graph neural networks and applies this to molecular graphs. While the paper has been substantially improved from its original form, there are still various concerns regarding performance and interpretability that remain unanswered. In its current form the paper is not ready for acceptance to ICLR-2020.",Paper Decision
12P2ecUPw_,S1ecYANtPr,Representation Learning Through Latent Canonicalizations,Reject,"This paper proposes a method to allow models to generalize more effectively through the use of latent linear transforms.

Overall, I think this method is interesting, but both R2 and R4 were concerned with the experimental evaluation being too simplistic, and the method not being applicable to areas where a good simulator is not available. This seems like a very valid concern to me, and given the high bar for acceptance to ICLR, I would suggest that the paper is not accepted at this time. I would encourage the authors to continue with follow-up experiments that better showcase the generality of the method, and re-submit a more polished draft to a conference in the near future.",Paper Decision
ThBtwPVv_,S1e5YC4KPS,Winning Privately: The Differentially Private Lottery Ticket Mechanism,Reject,"This paper provides an approach to improve the differentially private SGD method by leveraging a differentially private version of the lottery mechanism, which reduces the number of parameters in the gradient update (and the dimension of the noise vectors). While this combination appears to be interesting, there is a non-trivial technical issue raised by Reviewer 3 on the sensitivity analysis in the paper. (R3 brought up this issue even after the rebuttal.) This issue needs to be resolved or clarified for the paper to be published.",Paper Decision
2RGSEpCm-I,ryeFY0EFwS,Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization,Accept (Poster),"The paper proposes an intuitive causal explanation for the generalization properties of GD methods. The reviewers appreciated the insights, with one reviewer claiming that there was significant overlap with existing work.

I ultimately decided to accept this paper as I believe intuitive explanations are critical to the propagation of ideas. That being said, there is a tendency in this community to erase past, especially theoretical, work, for that very reason that theoretical work is less popular.

Hence, I want to make it clear that the acceptance of this paper is based on the premise that the authors will incorporate all of reviewer 3's comments and give enough credit to all relevant work (namely, all the papers cited by the reviewer) with a proper discussion on the link between these.",Paper Decision
GWFqCqZ-IB,Byx_YAVYPH,Jelly Bean World: A Testbed for Never-Ending Learning,Accept (Poster),"This paper proposes a flexible environment for studying never ending learning. During the discussion period, all reviewers found the paper to be borderline.

Pros:
- we don't have good lifelong or never-ending RL environments, and this paper seems to provide one
- includes a number of interesting features such as multiple input modalities, non-episodic interactions, flexible task definitions

Cons:
- procedurally generated, toy environment
- unclear if the environment reflects the characteristics of real world NEL problems

In the balance, I think the environments add value to the RL community, and being presented at ICLR would increase its visibility.",Paper Decision
DCcY4ZUKAE,Bkl8YR4YDB,Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs,Reject,"The authors address the problem of training an NMT model on a really massive parallel data set of 40 billion Chinese-English sentence pairs, an order of magnitude bigger than other cz-en experiments. To address noise and training time problems they propose pretraining + a couple of different ways of creating a fine-tuning data set. Two of the reviewers assert that the technical contribution is thin, and the results are SOTA but not really as good as you might hope with this amount of data. This combined with the fact that the data set is not released, makes me think that this paper is not a good fit with ICLR and would more appropriate for an application focussed conference. The authors engaged strongly with the reviewers, adding more backtranslation results. The reviewers took their responses into account but did not change their scores. ",Paper Decision
xv68sTToQT,rJlUt0EYwS,Learning from Explanations with Neural Execution Tree,Accept (Poster),"This paper proposing a framework for augmenting classification systems with explanations was very well received by two reviewers, and on reviewer labeling themselves as ""perfectly neutral"". I see no reason not to recommend acceptance.",Paper Decision
LB86Nt4JaL,H1lBYCEFDB,A Coordinate-Free Construction of Scalable Natural Gradient,Reject,The authors analyze the natural gradient algorithm for training a neural net from a theoretical perspective and prove connections to the K-FAC algorithm. The paper is poorly written and contains no experimental evaluation or well established implications wrt practical significance of the results.,Paper Decision
r7d_Zq8Zh,rkgHY0NYwr,Discovering Motor Programs by Recomposing Demonstrations,Accept (Poster),"The work presents a novel and effective solution to learning reusable motor skills.  The urgency of this problem and the considerable rebuttal of the authors merits publication of this paper, which is not perfect but needs community attention.",Paper Decision
Vfh0M9AG5p,rJlNKCNtPB,Adaptive Learned Bloom Filter (Ada-BF): Efficient Utilization of the Classifier,Reject,"The paper improves the Bloom filter learning by utilizing the complete spectrum of the scores regions. 

The paper is nicely written with strong motivation and theoretical analysis of the proposed model. The evaluation could be improved: all the experiments are only tested on the small datasets, which makes it hard to assess the practicality of the proposed method. The paper could lead to a strong publication in the future if the issue on evaluation can be addressed. 
",Paper Decision
m1gGATS98,SJlVY04FwH,Convergence of Gradient Methods on Bilinear Zero-Sum Games,Accept (Poster),All reviewers found the work interesting but worried about the extension to non-bilinear games. This is a point the authors should explicitly address in their work before publication.,Paper Decision
KASHz9JtHB,B1gXYR4YDH,DSReg: Using Distant Supervision as a Regularizer,Reject,"This paper proposes a way to handle the hard-negative examples (those very close to positive ones) in  NLP, using a distant supervision approach that serves as a regularization.   The paper addresses an important issue and is well written; however, reviewers pointed put several concerns, including testing the approach on the state-of-art neural nets, and making experiments more convincing by testing on larger problems.
 
",Paper Decision
fo3bFRQd8p,rylztAEYvr,Iterative Target Augmentation for Effective Conditional Generation,Reject,"This paper proposes a training scheme to enhance the optimization process where the outputs are required to meet certain constraints. The authors propose to insert an additional target augmentation phase after the regular training. For each datapoint, the algorithm samples candidate outputs until it find a valid output according the an external filter. The model is further fine-tuned on the augmented dataset. 

The authors provided detailed answers and responses to the reviews, which the reviewers appreciated. However, some significant concerns remained, and  due to a large number of stronger papers, this paper was not accepted at this time.",Paper Decision
mGdyvxNGYH,H1ezFREtwH,Composing Task-Agnostic Policies with Deep Reinforcement Learning,Accept (Poster),"This paper considers deep reinforcement learning skill transfer and composition, through an attention model that weighs the contributions of several base policies conditioned on the task and state, and uses this to output an action. The method is evaluated on several Mujoco tasks.

There were two main areas of concern. The first was around issues with using equivalent primitives and training times for comparison methods. The second was around the general motivation of the paper, and also the motivation for using a BiRNN. These issues were resolved in a comprehensive discussion, leaving this as an interesting paper that should be accepted.",Paper Decision
-TjmqiTGTz,HJxMYANtPH,The Local Elasticity of Neural Networks,Accept (Poster),"This paper presents a new phenomenon referred to as the ""local elasticity of neural networks"". The main argument is that the SGD update for nonlinear network at a local input x does not change the predictions at a different input x' (see Fig. 2). This is then connected to similarity using nearest-neighbor and kernel methods. An algorithm is also presented.

The reviewers find the paper intriguing and believe that this could be interesting for the community. After the rebuttal period, one of the reviewers increased their score.

I do agree with the view of the reviewers, although I found that the paper's presentation can be improved. For example, Fig. 1 is not clear at all, and the related work section basically talks about many existing works but does not discuss why they are related to this work and how this work add value to this existing works. I found Fig. 2 very clear and informative. I hope that the authors could further improve the presentation. This should help in improving the impact of the paper.

With the reviewers score, I recommend to accept this paper, and encourage the authors to improve the presentation of the paper.",Paper Decision
aoyivhzi49,rklbKA4YDS,Gradient-Based Neural DAG Learning,Accept (Poster),"In this paper, the authors propose a novel approach for learning the structure of a directed acyclic graph from observational data that allows to flexibly model nonlinear relationships between variables using neural networks. While the reviewers initially had concerns with respect to the positioning of the paper and various questions regarding theoretical results and experiments, these concerns have been addressed satisfactorily during the discussion period.  The paper is now acceptable for publication in ICLR-2020. ",Paper Decision
tstH_r8GK,BylWYC4KwH,On Concept-Based Explanations in Deep Neural Networks,Reject,"This paper introduces an unsupervised concept learning and explanation algorithm, as well as a concept of ""completeness"" for evaluating representations in an unsupervised way.

There are several valuable contributions here, and the paper improved substantially after the rebuttal.  It would not be unreasonable to accept this paper.  But after extensive post-review discussion, we decided that the completeness idea was the most valuable contribution, but that it was insufficiently investigated.

To quote R3, who I agree with: "" I think the paper could be strengthened considerably with a rewrite that focuses first on a shortcoming of existing methods in finding complete solutions. I also think their explanations for why PCA is not complete are somewhat speculative and I expect that studying the completeness of activation spaces in invertible networks would lead to some relevant insights""

",Paper Decision
nJ0UtpCVh1,rklxF0NtDr,Policy Message Passing: A New Algorithm for Probabilistic Graph Inference,Reject,"This paper was reviewed by 3 experts, who recommend Weak Reject, Weak Reject, and Reject. The reviewers were overall supportive of the work presented in the paper and felt it would have merit for eventual publication. However, the reviewers identified a number of serious concerns about writing quality, missing technical details, experiments, and missing connections to related work. In light of these reviews, and the fact that the authors have not submitted a response to reviews, we are not able to accept the paper. However given the supportive nature of the reviews, we hope the authors will work to polish the paper and submit to another venue.",Paper Decision
icjZna-df,rJleFREKDr,Learning to Control Latent Representations for Few-Shot Learning of Named Entities,Reject,"This work proposes to use policy-gradient RL to learn to read and write actions over memory locations using as reward the entropy reduction of memory location distribution. The authors perform experiments on NER in Stanford Dialogue task, that are framed though as few-shot learning. The reviewers have pointed out shortcomings of the paper with regards to its novelty, narrow contribution in combination thin experimental setup (the authors only look into one dataset and one task with minimal comparison to previous work and no ablation studies as to understand the behaviour of the model) and clarity (method description seems to be lacking some crucial components of the model). As such, I cannot recommend acceptance but I hope the authors will use the reviewers comments to transform this into a strong submission for a later conference.",Paper Decision
dXXMlZYieV,S1xJFREKvB,Amortized Nesterov's Momentum: Robust and Lightweight  Momentum for Deep Learning,Reject,"This paper introduces a variant of Nesterov momentum which saves computation by only periodically recomputing certain quantities, and which is claimed to be more robust in the stochastic setting. The method seems easy to use, so there's probably no harm in trying it. However, the reviewers and I don't find the benefits persuasive. While there is theoretical analysis, its role is to show that the algorithm maintains the convergence properties while having other benefits. However, the computations saved by amortization seem like a small fraction of the total cost, and I'm having trouble seeing how the increased ""robustness"" is justified. (It's possible I missed something, but clarity of exposition is another area the paper could use some improvement in.) Overall, this submission seems promising, but probably needs to be cleaned up before publication at ICLR.
",Paper Decision
xACNwEN81N,SyeyF0VtDr,Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph,Reject,"The paper proposes a recurrent and autorgressive architecture to model temporal knowledge graphs and perform multi-time-step inference in the form of future link prediction. However, the reviewers feel that the papers are more of a straight application of current techniques. Furthermore, a better presentation of the experimental section will also help improve the paper.  ",Paper Decision
zKmocj4bdN,BylA_C4tPr,Composition-based Multi-Relational Graph Convolutional Networks,Accept (Poster),"This paper proposes and evaluates a formulation of graph convolutional networks for multi-relation graphs. The paper was reviewed by three experts working in this area and received three Weak Accept decisions. The reviewers identified some concerns, including novelty with respect to existing work and specific details of the experimental setup and results that were not clear. The authors have addressed most of these concerns in their response, including adding a table that explicitly explains the contribution with respect to existing work and clarifying the missing details. Given the unanimous Weak Accept decision, the ACs also recommend Accept as a poster.",Paper Decision
8uFQZhbNxq,HJe6uANtwH,Capsules with Inverted Dot-Product Attention Routing,Accept (Poster),"This work presents a routing algorithm for capsule networks, and demonstrates empirical evaluation on CIFAR-10 and CIFAR-100. The results outperform existing capsule networks and are at-par with CNNs. Reviewers appreciated the novelty, introducing a new simpler routing mechanism, and achieving good performance on real world datasets. In particular, removing the squash function and experimenting with concurrent routing was highlighted as significant progress. There were some concerns (e.g. claiming novelty for inverted dot-product attention) and clarification questions (e.g. same learning rate schedule for all models). The authors provided a response and revised the submission , which addresses most of these concerns. At the end, majority of reviewers recommended accept. Alongside with them, I acknowledge the novelty of using layer norm and parallel execution, and recommend accept.
",Paper Decision
h3QgGdLjF,H1xauR4Kvr,The Discriminative Jackknife: Quantifying Uncertainty in Deep Learning via Higher-Order Influence Functions,Reject,"In this work, the authors develop a method for providing frequentist confidence intervals for a range of deep learning models with coverage guarantees.  While deep learning models are being used pervasively, providing reasonable uncertainty estimates from these models remains challenging and an important open problem.  Here, the authors argue that frequentist statistics can provide confidence intervals along with rigorous guarantees on their quality.  They develop a jack-knife based procedure for deep learning.  The reviews for this paper were all borderline, with two weak accepts and two weak rejects (one reviewer was added to provide an additional viewpoint).  The reviewers all thought that the proposed methodology seemed sensible and well motivated.  Among the cited issues, major topics of discussion were the close relation to related work (some of which is very recent, Giordano et al.) and that the reviewers felt the baselines were too weak (or weakly tuned).  The reviewers ultimately did not seem convinced enough by the author rebuttal to raise their scores during discussion and there was no reviewer really willing to champion the paper for acceptance.  Unfortunately, this paper falls below the bar for acceptance.  It seems clear that there is compelling work here and addressing the reviewer comments (relation to related work, i.e. Robbins, Giordano and stronger baselines) would make the paper much stronger for a future submission.",Paper Decision
zdtLaZneJO,BkxadR4KvS,Insights on Visual Representations for Embodied Navigation Tasks,Reject,"The general consensus amongst the reviewers is that this paper is not quite ready for publication, and needs to dig a little deeper in some areas.  Some reviewers thought the contributions are unclear, or unsupported.  I hope these reviews will help you as you work towards finding a home for this work.",Paper Decision
nFzLSbBuz,ryen_CEFwr,"Unsupervised Disentanglement of Pose, Appearance and Background from Images and Videos",Reject,"The paper proposes an approach for unsupervised learning of keypoint landmarks from images and videos by decomposing them into the foreground and static background. The technical approach builds upon related prior works such as Lorenz et al. 2019 and Jakab et al. 2018 by extending them with foreground/background separation. The proposed method works well for static background achieving strong pose prediction results. The weaknesses of the paper are that (1) the proposed method is a fairly reasonable but incremental extension of existing techniques; (2) it relies on a strong assumption on the property of static backgrounds; (3) video prediction results are of limited significance and scope. In particular, the proposed method may work for simple data like KTH but is very limited for modeling videos as it is not well-suited to handle moving backgrounds, interactions between objects (e.g., robot arm in the foreground and objects in the background), and stochasticity. ",Paper Decision
O_X3TrTzQu,BJx3_0VKPB,On the Unintended Social Bias of Training Language Generation Models with News Articles,Reject,"The reviewers had a hard time fully identifying the intended contribution behind this paper, and raised concerns that suggest that the experimental results are not sufficient to justify any substantial contribution with the level of certainty that would warrant publication at a top venue. The authors have not responded, and the concerns are serious, so I have no choice but to reject this paper despite its potentially valuable topic.",Paper Decision
R92XOH8Hyl,rJeidA4KvS,Role-Wise Data Augmentation for Knowledge Distillation,Reject,"This paper studies Population-Based Augmentation in the context of knowledge distillation (KD) and proposes a role-wise data augmentation schemes for improved KD. While the reviewers believe that there is some merit in the proposed approach, its incremental nature and inherent complexity require a cleaner exposition and a stronger empirical evaluation on additional data sets. I will hence recommend the rejection of this manuscript in the current state. Nevertheless, applying PBA to KD seems to be an interesting direction and we encourage the authors to add the missing experiments and to carefully incorporate the reviewer feedback to improve the manuscript.",Paper Decision
2t1eQ2FDk,rJe5_CNtPB,Attention Forcing for Sequence-to-sequence Model Training,Reject,"The paper proposed an attention-forcing algorithm that guides the sequence-to-sequence model training to make it more stable. But as pointed out by the reviewers, the proposed method requires alignment which is normally unavailable. The solution to address that is using another teacher-forcing model, which can be expensive. 

The major concern about this paper is the experimental justification is not sufficient:
* lack of evaluations of the proposed method on different tasks;
* lack of experiments on understanding how it interact with existing techniques such as scheduled sampling etc;
* lack of comparisons to related existing supervised attention mechanisms. 
",Paper Decision
UlmecEFG1I,rJg9OANFwS,Topic Models with Survival Supervision: Archetypal Analysis and Neural Approaches,Reject,"The paper proposes two approaches to topic modeling supervised by survival analysis. The reviewers find some problems in novelty,  algorithm and experiments, which is not ready for publish.",Paper Decision
ML25Cf24I,S1xtORNFwH,FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary,Accept (Poster),"The paper proposes to compress convolutional neural networks via weight sharing across filters of each convolution layer. A fast convolution algorithm is also designed for the convolution layer with this approach. Experimental results show (i) effectiveness in CNN compression, (ii) acceleration on the tasks of image classification, object detection and neural architecture search. While the authors addressed most of reviewers' concerns, the weakness of the paper which remains is that no wall-clock runtime numbers (only FLOPS) are reported - so efficiency of the approach in practice in uncertain.
",Paper Decision
KugaNxpndx,r1lF_CEYwS,On the Need for Topology-Aware Generative Models for Manifold-Based Defenses,Accept (Poster),"This paper studies the role of topology in designing adversarial defenses. Specifically , the authors study defense strategies that rely on the assumption that data lies on a low-dimensional manifold, and show theoretical and empirical evidence that such defenses need to build a topological understanding of the data.

Reviewers were initially positive, but had some concerns pertaining to clarity and limited experimental setup. After a productive rebuttal phase, now reviewers are mostly in favor of acceptance, thanks to the improved readibility and clarity. Despite the small-scale experimental validation, ultimately both reviewers and AC conclude this paper is worthy of publication.  ",Paper Decision
aJtXl1ZqJZ,SkgKO0EtvS,Neural Execution of Graph Algorithms,Accept (Poster),It seems to be an interesting contribution to the area. I suggest acceptance.,Paper Decision
poAy7d1u9A,Sked_0EYwB,Objective Mismatch in Model-based Reinforcement Learning,Reject,"As the reviewers point out, this paper has potentially interesting ideas but it is in too preliminary state for publication at ICLR.  ",Paper Decision
Reubjvlnja,S1e__ANKvB,Molecular Graph Enhanced Transformer for Retrosynthesis Prediction,Reject,"Several approaches can be used to feed structured data to a neural network, such as convolutions or recurrent network. This paper proposes to combine both roads, by presenting molecular structures to the network using both their graph structured and a serialized representation (SMILES), that are processed by a framework combining the strenth of Graph Neural Network and the sequential transformer architecture.

The technical quality of the paper seems good, with R1 commenting on the performance relative to SOTA seq2seq based methods and R3 commenting on the benefits of using more plausible constraints. The problem of using data with complex structure is highly relevant for ICLR.

However, the novelty was deemed on the low side. As a very competitive conference, this is one of the key aspects necessary for successful ICLR papers. All reviewers agree that the novelty is too low for the current (high) bar of ICLR. ",Paper Decision
B5KfLgT1_z,HkePOCNtPH,Non-Sequential Melody Generation,Reject,"All the reviewers pointed out issues with the experiments, which the rebuttal did not address. The paper seems interesting, and the authors are encouraged to improve it.",Paper Decision
1hPAy5wXO,SJlPOCEKvH,Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning,Reject,"This work explores weight pruning for BERT in three broad regimes of transfer learning: low, medium and high.

Overall, the paper is well written and explained and the goal of efficient training and inference is meaningful. Reviewers have major concerns about this work is its technical innovation and value to the community: a reuse of pruning to BERT is not new in technical perspective, the marginal improvement in pruning ratio compared to other compression method for BERT, and the introduced sparsity that hinders efficient computation for modern hardware such as GPU. The rebuttal failed to answer a majority of these important concerns.

Hence I recommend rejection.",Paper Decision
ZOL-q3c446,S1xLuRVFvr,Visual Explanation for Deep Metric Learning,Reject,"This submission proposes a method for providing visual explanations for why two images match by highlighting image regions that most contribute to similarity.

Reviewers agreed that the problem is interesting but were divided on the degree of novelty of the proposed approach.

AC shares R1’s concern that localization accuracy is not satisfactory as a quantitative measure of the quality of the explanations. In particular, it pre-supposes what the explanations ought to be, i.e. that a good explanation means good localization. A small user-study would be more convincing. A more convincing evaluation would also include a study of explanation of image pairs with different degrees of similarity (e.g. images that are dissimilar as well as images with the same object).

AC also shares R2’s concern about the validity of the model diagnosis application. This discussion also relies on the assumption that better localization of the whole object means a better explanation. Further, the highlighted regions in Figure 5 are very similar. Once again, a user study would help to indicate whether these results really do improve explainability.

Reviewers also had concerns about missing details and, while the authors did improve this, key details are still missing. For example, the localization method that was used was only referenced but should be described in the paper itself.

Given that several concerns remain, AC recommends rejection.
",Paper Decision
jtIkO1-IN,SygLu0VtPH,Deep Innovation Protection,Reject,"This paper is a very borderline case. Mixed reviews. R2 score originally 4, moved to 5 (rounded up to WA 6), but still borderline. R1 was 6 (WA) and R3 was 3 (WR).  R2 expert on this topic, R1 and R3 less so. AC has carefully read the reviews/rebuttal/comments and looked closely at the paper. AC feels that R2's review is spot on and that the contribution does not quite reach ICLR acceptance level, despite it being interesting work. So the AC feels the paper cannot be accepted at this time. But the work is definitely interesting -- the authors should improve their paper using R2's comments and resubmit.  ",Paper Decision
5E4GKcNZ5x,HkeSdCEtDS,Alternating Recurrent Dialog Model with Large-Scale Pre-Trained Language Models,Reject,"This paper proposes an alternating dialog model based on transformers and GPT-2, that model each conversation side separately and aim to eliminate human supervision. Results on two dialog corpora are either better than or comparable to state-of-the-art. Two of the reviewers raise concerns about the novel contributions of the paper, and did not change their scores after authors' rebuttal. Furthermore, one reviewer raises concerns about the lack of detailed experiments aiming to explain where the improvements come from. Hence,  I suggest rejecting the paper.",Paper Decision
hvXizPVN_c,SkeHuCVFDr,BERTScore: Evaluating Text Generation with BERT,Accept (Poster),"Thanks for an interesting discussion. The authors present a supposedly task-independent evaluation metric for generation tasks with references that relies on BERT or similar pretrained language models and a BERT-internal alignment. Reviewers are moderately positive. I encourage the authors to think about a) whether their approach scales to language pairs where wordpieces are less comparable; b) whether second order similarly, e.g., using RSA, would be better than alignment-based similarity; c) whether this metric works in the extremes, e.g., can it distinguish between bad output and super-bad output (where in both cases alignment may be impossible), and can it distinguish between good output and super-good output (where BERT scores may be too biased by BERT's training objective). ",Paper Decision
VQSBEcGWti,HkxSOAEFDB,Octave Graph Convolutional Network,Reject,"Two reviewers are negative on this paper while the other one is slightly positive. Overall, the paper does not make the bar of ICLR and thus a reject is recommended.",Paper Decision
Ia9scpfvjn,rJlVdREKDS,Learning from Imperfect Annotations: An End-to-End Approach,Reject,"The paper introduces a novel way of jointly modeling annotator competencies and learning from imperfect annotations. Reviewers were moderately positive. One reviewer mentioned Carpenter (2002) and subsequent work. One prominent example of this line of work, which the authors do not cite, is: https://www.isi.edu/publications/licensed-sw/mace/ - from 2013. I encourage the authors to cite this paper. In the discussion, the authors point out this type of work is not *end-to-end* in their sense. However, there's, to the best of my knowledge, a relatively big body of literature on end-to-end approaches that the authors completely ignore, e.g., [0-3]. In the absence of a discussion of this work, it is hard to accept the paper. 

[0] https://link.springer.com/article/10.1007/s10994-013-5411-2
[1] https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405343
[2] http://www.cs.utexas.edu/~atn/nguyen-acl17.pdf
[3] https://arxiv.org/pdf/1803.04223.pdf",Paper Decision
PN7vqUphQ6,SklE_CNFPr,Zeroth Order Optimization by a Mixture of Evolution Strategies,Reject,"The paper proposes an adaptive sampling mechanism for zeroth order optimization that samples perturbed points from a mixture distribution with asymptotic convergence guarantees. The reviewers raised issues regarding the clarity of presentation, potential problems with the proofs, and simplicity of the experimental setup. The authors did not provide a response. Overall, the reviewers agree that the quality of the paper is not sufficient for publishing, and therefore I recommend rejection.",Paper Decision
KyWUSdnj2,ryxQuANKPB,Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History,Accept (Poster),"This work proposes use of two pre-trained FST models to explicitly incorporate semantic and strategic/tactic information from dialog history into non-collaborative (negotiation) dialog systems. Experiments on two datasets from prior work show the advantage of this model in automated and human evaluation. While all reviewers found the work interesting, they made many suggestions regarding the presentation. Author'(s) rebuttal included explanations and changes to the presentation. Hence, I suggest acceptance as a poster presentation.",Paper Decision
endRhWx8EK,S1eQuCVFvB,Machine Truth Serum,Reject,"This paper proposes a family of new methods, based on Bayesian Truth Serum, that are meant to build better ensembles 
from a fixed set of constituent models.

Reviewers found the problem and the general research direction interesting, but none of the three of them were convinced that the proposed methods are effective in the ways that the paper claims, even after some discussion. It seems as though this paper is dealing with a problem that doesn't generally lend itself to large improvements in results, but reviewers weren't satisfied that the small observed improvements were real, and urged the authors to explore additional settings and baselines, and to offer a full significance test.",Paper Decision
-LmwgMEna,BJxG_0EtDS,"Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control",Accept (Poster),"This paper studies optimal control with low-dimensional representation.  The paper presents interesting progress, although I urge the authors to address all issues raised by reviewers in their revisions.",Paper Decision
jCX0jNGOUv,r1lGO0EKDH,GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding,Accept (Talk),"The authors present an approach for learning graph embeddings by first fusing the graph to generate a new graph with encodes structural information as well as node attribution information. They then iteratively merge nodes based spectral similarities to  obtain coarser graphs. They then use existing methods to learn embeddings from this coarse graph and progressively refine the embeddings to finer graphs. They demonstrate the performance of their method on standard graph datasets. 

This paper has received positive reviews from all reviewers. The authors did a good job of addressing the reviewers' concerns and managed to convince the reviewers about their contributions. I request the authors to take the reviewers suggestions into consideration while preparing the final draft of the paper and recommend that the paper be accepted.",Paper Decision
O57CvsvLEQ,rJlf_RVKwr,Sensible adversarial learning,Reject,"Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects.
However, there is still room for improvement; for example, convergence to a good solution needs to be further investigated.
Given the  high competition at ICLR2020, this paper is unfortunately below the bar.
We hope that the reviewers' comments are useful for improving the paper for potential future publication.",Paper Decision
EGdrL5lcgr,BJe-_CNKPH,Attention Interpretability Across NLP Tasks,Reject,"This paper investigates the degree to which we might view attention weights as explanatory across NLP tasks and architectures. Notably, the authors distinguish between single and ""pair"" sequence tasks, the latter including NLI, and generation tasks (e.g., translation). The argument here is that attention weights do not provide explanatory power for single sequence tasks like classification, but do for NLI and generation. Another notable distinction from most (although not all; see the references below) prior work on the explainability of attention mechanisms in NLP is the inclusion of transformer/self-attentive architectures. 

Unfortunately, the paper needs work in presentation (in particular, in Section 3) before it is ready to be published.",Paper Decision
sb5iZ94_dc,BJlxdCVKDB,MoET: Interpretable and Verifiable Reinforcement Learning via Mixture of Expert Trees,Reject,"This paper aims at making a deep RL policy interpretable and verifiable by distilling the policy represented by a deep neural network into an ensemble of decision trees. This should be done without hurting the performance of the policy. The authors achieve this by extending the existing Viper algorithm. The resulting approach can imitate the deep policy better compared with Viper while preserving verifiability. Experiments show that the proposed method improves in terms of cumulative reward and error rate over Viper in four benchmark tasks.

The amount of improvement over the original Viper is not convincing given the presented results. Moreover, reviewers uniformly agree that the contribution of this work is incremental. I therefore recommend to reject this paper.",Paper Decision
3ijXNyY7GH,rygxdA4YPS,AdaScale SGD: A Scale-Invariant Algorithm for Distributed Training,Reject,"Main summary: Novel rule for scaling learning rate, known as gain ration, for which the effective batch size is increased.

Discussion: 
reviewer 2: main concern is reviewer can't tell if it's better of worse than linear learning rate scaling from their experiment section.
reviewer 3: novlty/contribution is a bit too low for ICLR.
reviewer 1: algorthmic clarity lacking.
Recommendation: all 3 reviewers recommend reject, I agree.",Paper Decision
3fvRdDU6i,SkgJOAEtvr,INTERNAL-CONSISTENCY CONSTRAINTS FOR EMERGENT COMMUNICATION,Reject,"This work examines how internal consistency objectives can help emergent communication, namely through possibly improving ability to refer to unseen referents and to generalize across communicative roles. Experimental results support the second hypothesis but not the first.
Reviewers agree that this is an exciting object of study, but had reservations about the rationale for the first hypothesis (which was ultimately disproven), and for how the second hypothesis was investigated (lack of ablations to tease apart which part was most responsible for improvement, unsatisfactory framing). These concerns were not fully addressed by the response.
While the paper is very promising and the direction quite interesting, this cannot in its current form be recommended for acceptance. We encourage authors to carefully examine reviewers' suggestions to improve their work for submission to another venue.",Paper Decision
wWg-QUvUW_,Bylkd0EFwr,Bio-Inspired Hashing for Unsupervised Similarity Search,Reject,"This paper introduces a biologically inspired locally sensitive hashing method, a variant of FlyHash. While the paper contains interesting ideas and its presentation has been substantially improved from its original form during the discussion period, the paper still does not meet the quality bar of ICLR due to its limitations in terms of experiments and applicability to real-world scenarios.",Paper Decision
LzwcVP0Qg,SJlRDCVtwr,Simplicial Complex Networks,Reject,"The aper introduces simplicial complex networks, a new class of
neural networks based on the idea of the subdivision of a simplicial
complex. The paper is interesting and brings ideas of algebraic topology to inform the design of new neural network architectures. 

Reviewer 1 was positive about the ideas of this paper, but had several concerns about clarity, scalablity and the sense that the paper might still be in an early phase. Reviewer 2 had similar concerns about clarity, comparisons, and usefulness. Although there were no responses form the author, the discussion explored the paper further, but continued to think the idea is still in its early phase.

The paper is not currently ready for acceptance, and we hope the authors will find useful feedback for their ongoing reasearch. ",Paper Decision
KjPnxd3BFj,Byx0PREtDH,BEYOND SUPERVISED LEARNING: RECOGNIZING UNSEEN ATTRIBUTE-OBJECT PAIRS WITH VISION-LANGUAGE FUSION AND ATTRACTOR NETWORKS,Reject,"The paper focuses on attribute-object pairs image recognition, leveraging some novel ""attractor network"".

At this stage, all reviewers agree the paper needs a lot of improvements in the writing. There are also concerns regarding (i) novelty: the proposed approach being two encoder-decoder networks; (ii) lack of motivation for such architecture (iii) possible flow in the approach (are the authors using test labels?) and (iv) weak experiments.",Paper Decision
nwXozTCfmP,SJx0PAEFDS,Underwhelming Generalization Improvements From Controlling Feature Attribution,Reject,"This paper studies the effect of training image classifier with masked images to exclude distraction regions in the image and avoid formation of spurious correlation between them and predicted labels. The paper proposes actdiff regularizer and demonstrates that it prevents such overfitting phenomenon on synthetic data.  However, there was no success on real data. This is important as it shows that the improvement reported in some saliency-map based approaches in the literature may be due to other regularization effects such as cutout.

This was a unique submission in my batch, as it embraces its negative results. Among our internal discussions, all reviewers that and we all believe that negative results are important and should be encouraged. However, in order for the negative results to be sufficiently insightful for the entire community, they need to be examined under well-organized experiments. This is the aspect that the reviewers think the paper needs to improve on.  In particular, R2 believes the paper could consider a larger set of possible regularizations as well as a broader range of  applications. The insights in such setting may then lead to solid insights on why the current approaches are not very helpful, and in which direction the follow-up researches should focus on.",Paper Decision
rVnf6FnjN7,B1x6w0EtwH,Graph Constrained Reinforcement Learning for Natural Language Action Spaces,Accept (Poster),"This paper applies reinforcement learning to text adventure games by using knowledge graphs to constrain the action space. This is an exciting problem with relatively little work performed on it. Reviews agree that this is an interesting paper, well written, with good results. There are some concerns about novelty but general agreement that the paper should be accepted. I therefore recommend acceptance.",Paper Decision
nkEhVPRTig,BkgTwRNtPB,Solving Packing Problems by Conditional Query Learning,Reject,"This paper proposes an end-to-end deep reinforcement learning-based algorithm for the 2D and 3D bin packing problems. Its main contribution is conditional query learning (CQL) which allows effective decision over mutually conditioned action spaces through policy expressed as a sequence of conditional distributions. Efficient neural architectures for modeling of such a policy is proposed. Experiments validate the effectiveness of the algorithm through comparisons with genetic algorithm and vanilla RL baselines.

The presentation is clear and the results are interesting, but the novelty seems insufficient for ICLR. The proposed model is based on transformer with the following changes:
* encoder: position embedding is removed, state embedding is added to the multi-head attention layer and feed forward layer of the original transformer encoder;
* decoder: three decoders one for the three steps, namely selection, rotation and location.
* training: actor-critic algorithm",Paper Decision
AXbd8GGfOp,S1x2PCNKDB,Task-Relevant Adversarial Imitation Learning,Reject,"This paper attempts to improve adversarial imitation learning (GAIL) by encouraging the discriminator to focus on task-dependent features.

An advantage of this paper is that it not only improves upon GAIL, but it is doing so after first demonstrating and analyzing an existing issue. 

On the other hand, the presentation of the paper and breadth of experiments could be significantly improved further than the updated version. It would also be necessary to clarify whether the baseline is vanilla PG or D4PG.

A major point for discussion was the selection of the invariance set. The ablation studies and explanation provided during the rebuttal period towards this point are helpful, but somehow we still do not have the full picture to understand well how this method compares to existing literature.
",Paper Decision
7Qys7zNwP,ryghPCVYvH,Generative Restricted Kernel Machines,Reject,"The paper proposes a way to use kernel method for multi-view generation. The points are mapped into a common subspace (with CNN feature extractor and kernel on top), and then a generation procedure from a latent point is given. 
I found the paper not easy to ready and follow; the idea of using CNN + kernel methods have been around for some years (for example, see ""Impostor networks"" by Lebedev et. al), and explicit feature map shows that kernel is just an additional layer to the network. Overall, the approach is straightforward, the generation can be quite slow and the benefits are not clear. The reviewers are mildly negative, so I think this time this paper can not be accepted.",Paper Decision
OovmtzsYN3,r1eowANFvr,Towards Fast Adaptation of Neural Architectures with Meta Learning,Accept (Poster),"This paper introduces T-NAS, a neural architecture search (NAS) method that can quickly adapt architectures to new datasets based on gradient-based meta-learning. It is a combination of the NAS method DARTS and the meta-learning method MAML.

All reviewers had some questions and minor criticisms that the authors replied to, and in the private discussion of reviewers and AC all reviewers were happy with the authors' answers. There was unanimous agreement that this is a solid poster. 

Therefore, I recommend acceptance as a poster.",Paper Decision
dd6R98Oh1,BJl9PRVKDS,A Functional Characterization of Randomly Initialized Gradient Descent in Deep ReLU Networks,Reject,"This article sets out to study the advantages of depth and overparametrization in neural networks from the perspective of function space, with results on univariate shallow fully connected ReLU networks and some experiments on deep networks. 
The article presents results on the concentration /dispersion of the slope / break point distribution of the functions represented by shallow univariate ReLU networks for parameters from various distributions. The reviewers found that the article contains interesting analysis, but that the presentation could be improved. The revision clarified some aspects and included some experiments illustrating breakpoint distributions in relation to the curvature of some target functions. However, the reviewers did not find this convincing enough, pointing out that the analysis focuses on a very restrictive setting and that that presentation of the article still could be improved. The discussion of implicit regularisation in section 2.4 seems promising, but it would benefit from a clearer motivation, background, and discussion. ",Paper Decision
pOn44eTmxU,H1x5wRVtvS,Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling,Accept (Poster),"This paper proposes a bidirectional joint image-text model using a variational hetero-encoder (VHE) randomized generative adversarial network (GAN). The proposed VHE-GAN model encodes an image to decode its associated text. Three reviewers have split reviews. Reviewer #3 is overall positive about this work. Reviewer #1 rated weak acceptance, while request more comparison with latest works. Reviewer  #2 rated weak reject raised concerns on the motivation of the approach, the lack of ablation and lack of comparison with the latest work. During the rebuttal, the authors provide additional comparison and ablation, which seem to address the major concerns. Given the overall positive feedback and the quality of rebuttal, the AC recommends acceptance.",Paper Decision
V-A8y1vmbi,HJgcw0Etwr,Toward Understanding Generalization of Over-parameterized Deep ReLU network trained with SGD in Student-teacher Setting,Reject,"The article studies a student-teacher setting with over-realised student ReLU networks, with results on the types of solutions and dynamics. The reviewers found the line of work interesting, but they also raised concerns about the novelty of the presented results, the description of previous works, settings and claims, and experiments. The revision clarified some of the definitions, the nature of the observations, experiments, and related works, including a change of the title. However, the reviewers still were not convinced, in particular with the interpretation of the results, and keep their original ratings. With many points that were raised in the original reviews, the article would benefit from a more thorough revision. ",Paper Decision
SIspqR_iKR,S1gFvANKDS,Asymptotics of Wide Networks from Feynman Diagrams,Accept (Spotlight),"This submission presents bounds on the training dynamics (including gradient evolution) for deep linear (and in some cases nonlinear) networks as a function of the width of the layers or number of convolutional layers. The work also presents experimental results that provide evidence that the bounds are tight.

Strengths:
The work provides interesting insights into these training dynamics, particularly for the wide-but-not-infinite setting, which is less studied.
The work also adapts cluster graphs and Feynman diagrams to derive these bounds, which could be useful tools for researchers in this field.

Weaknesses:
The validity and applicability of some of the results for nonlinear networks was not entirely clear at first but has been clarified in the revision.

The reviewer consensus was to accept this submission.
",Paper Decision
w2ARLc1Qn7,BkgYPREtPr,Symplectic Recurrent Neural Networks,Accept (Spotlight),"This paper proposes a novel architecture for learning Hamiltonian dynamics from data. The model outperforms the existing state of the art Hamiltonian Neural Networks on challenging physical datasets. It also goes further by proposing a way to deal with observation noise and a way to model stiff dynamical systems, like bouncing balls. The paper is well written, the model works well and the experimental evaluation is solid. All reviewers agree that this is an excellent contribution to the field, hence I am happy to recommend acceptance as an oral.",Paper Decision
wJ06WN4M2,Byg_vREtvB,Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks,Reject,"The authors consider distilling posterior expectations for Bayesian neural networks. While reviewers found the material interesting, and the responses thoughtful, there were questions about the practical utility of the work. Evaluations of classification favour NLL (and typically do not show accuracy), and regression (which was considered in the original Bayesian Dark Knowledge paper) is not considered. In general, it is difficult to assess and interpret how the approach is working, and in what application regime it would be a gold standard, e.g., with respect to downstream tasks. The authors are encouraged to continue with this work, taking reviewer comments into account in a final version.",Paper Decision
GGvIZE6Jb1,HygwvC4tPH,Learning Cross-Context Entity Representations from Text,Reject,"The paper describes an approach for learning context dependent entity representations that encodes fine-grained entity types. The paper includes some good empirical results and observations, but the proposed approach is very simple but lacks technical novelty needed to top ML conference; the clarify of the presentation can also be improved. ",Paper Decision
knAaaNAuZ,HJxwvCEFvH,SPECTRA: Sparse Entity-centric Transitions,Reject,"This paper introduces a model that learns a slot-based representation and its transition model to predict the representation changes over time. While all the reviewers agree that this paper is focusing on an important problem, they expressed multiple concerns regarding the novelty of the approach as well as lacking experiments. It certainly is missing multiple important relevant works, thereby overclaiming at a few places. The authors provided a short general response to compare their approach with some of the previous works and conduct stronger experiments for a future submission. We believe this paper is not at the stage to be published at this point.",Paper Decision
A3xXDp_zGL,SkgvvCVtDS,DeepSimplex: Reinforcement Learning of Pivot Rules Improves the Efficiency of Simplex Algorithm in Solving Linear Programming Problems,Reject,"This paper present a learning method for speeding up of LP, and apply it to the TSP problem.

Reviewers and AC agree that the idea is quite interesting and promising. However, I think the paper is far from being ready to publish in various aspects:

(a) much more editorial efforts are necessary
(b) the TPS application of small scale is not super appealing 

Hence, I recommend rejection.",Paper Decision
J11LknIzYn,HkeUDCNFPS,Learning Temporal Abstraction with Information-theoretic Constraints for Hierarchical Reinforcement Learning,Reject,"This paper presents a novel hierarchical reinforcement learning framework, based on learning temporal abstractions from past experience or expert demonstrations using recurrent variational autoencoders and regularising the representations.

This is certainly an interesting line of work, but there were two primary areas of concern in the reviews: the clarity of details of the approach, and the lack of comparison to baselines. While the former issue was largely dealt with in the rebuttals, the latter remained an issue for all reviewers.

For this reason, I recommend rejection of the paper in its current form.",Paper Decision
c8b-LRNzTx,S1eIw0NFvr,Selective Brain Damage: Measuring the Disparate Impact of Model Pruning,Reject,"This work investigates neural network pruning through the lens of its influence over specific exemplars (which are found to often be lower quality or mislabelled images) and how removing them greatly helps metrics.
The insight from the paper is interesting, as recognized by reviewers. However, experiments do not suggest that the findings shown in the paper would generalize to more pruning methods. Nor do the authors give directions for tackling the ""hard exemplar"" problem. Authors' response did provide justifications and clarifications, however the core of the concern remains.
Therefore, we recommend rejection.",Paper Decision
uJ7ROpbYgo,BJlSPRVFwS,Asynchronous Stochastic Subgradient Methods for General Nonsmooth Nonconvex Optimization,Reject,"This paper considers an interesting theoretical question. However, it would add to the strength of the paper if it was able to meaningfully connect the considered model as well as derived methodology to the challenges and performance that arise in practice. ",Paper Decision
Rr7kPxGOE,H1ervR4FwH,Improved Structural Discovery and Representation Learning of Multi-Agent Data,Reject,"The work addresses the problem of inferring group structure from unstructured data in multi-agent learning settings, proposing a novel approach that has key computational / run time advantages over a prior approach. A key limitation raised by reviewers is the limited quantitative evaluation and comparison to previous approaches, as well as a resulting set of general insights into advantages of the proposed approach compared to prior work (beyond computational benefits). While some of the key limitations were addressed in the rebuttal, the contribution in its current form remains too narrow. The paper is not ready for publication at ICLR at this stage.",Paper Decision
drJCq17qTk,HJeEP04KDH,Quantized Reinforcement Learning (QuaRL),Reject,"The paper investigates quantization for speeding up RL. While the reviewers agree that the idea is a good one (it should definitely help), they also have a number of concerns about the paper and presentation. In particular, the reviewers feel that the authors should have provided more insight into the challenges of quantization in RL and the tradeoffs involved. After having read the rebuttals, the reviewers believe that the authors are on the right track, but that the paper is still not ready for publication. If the authors take the reviewer comments and concerns seriously and update their paper accordingly, the reviewers believe that this could eventually result in a strong paper.",Paper Decision
KDtuQwA8N,HJx4PAEYDH,R-TRANSFORMER: RECURRENT NEURAL NETWORK ENHANCED TRANSFORMER,Reject,"The submission proposes a variant of a Transformer architecture that does not use positional embeddings to model local structural patterns but instead adds a recurrent layer before each attention layer to maintain local context. The approach is empirically verified on a number of domains.

The reviewers had concerns with the paper, most notably that the architectural modification is not sufficiently novel or significant to warrant publication, that appropriate ablations and baselines were not done to convincingly show the benefit of the approach, that the speed tradeoff was not adequately discussed, and that the results were not compared to actual SOTA results.

For these reasons, the recommendation is to reject the paper.",Paper Decision
VWSBwKM9kz,rJeXDANKwr,NADS: Neural Architecture Distribution Search for Uncertainty Awareness,Reject,"This paper introduces a neural architecture search method that is geared towards yielding good uncertainty estimates for out-of-distribution (OOD) samples.

The reviewers found that the OOD prediction results are strong, but criticized various points, including the presentation of the OOD results, novelty as a NAS paper, missing citations to some recent papers, and a lack of baselines with simpler ensembles.
The authors improved the presentation of their OOD results and provided new experiments, which causes one reviewer to increase his/her score from a weak reject to an accept. The other reviewers appreciated the rebuttal, but preferred not to change their scores from a weak reject and a reject, mostly due to lack of novelty as a NAS paper.

I also read the paper, and my personal opinion is that it would definitely be very novel to have a good neural architecture search for handling uncertainty in deep learning; it is by no means the case that ""NAS for X"" is not interesting just because there are now a few papers for ""NAS for Y"". As long as X is relevant (which uncertainty in deep learning definitely is), and NAS finds a new state-of-the-art, I think this is great. For such an ""application"" paper of the NAS methodology, I do not find it necessary to introduce a novel NAS method, but just applying an existing one would be fine. The problem is more that the paper claims to introduce a new method, but that that method is too similar to existing ones, without a comparison; actually just using an existing NAS method would therefore make the contribution and the emphasis on the application domain clearer. 
I have one small question to the authors about a part that I did not understand: to optimize WAIC (Eq 1), why is it not optimal to just set the parameterization \phi such that the variance is minimized, i.e., return a delta distribution p_\phi that always returns the same architecture (one with a strong prediction)? Surely, that's not what the authors want, but wouldn't that minimize WAIC? I hope the authors will clarify this in a future version.

In the private discussion of reviewers and AC, the most positive reviewer emphasized that the OOD results are strong, but admitted that the mixed sentiment is understandable since people who do not follow OOD detection could miss the importance and context of the results, and that the paper could definitely improve its messaging. The other reviewers' scores remained at 1 and 3, but the reviewers indicated that they would be positive about a future version of the paper that fixed the identified issues. My recommendation is to reject the paper and encourage the authors to continue this work and resubmit an improved version to a future venue.",Paper Decision
vlc0JagntR,ryg7vA4tPB,Rigging the Lottery: Making All Tickets Winners,Reject,"A somewhat new approach to growing sparse networks.  Experimental validation is good, focussing on ImageNet and CIFAR-10, plus experiments on language modelling.  Though efficient in computation and storage size, the approach does not have a theoretical foundation.  That does not agree with the intended scope of ICLR.  I strongly suggest the authors submit elsewhere.",Paper Decision
9YeGsBVwgQ,H1lfwAVFwr,CAPACITY-LIMITED REINFORCEMENT LEARNING: APPLICATIONS IN DEEP ACTOR-CRITIC METHODS FOR CONTINUOUS CONTROL,Reject,"This paper presents Capacity-Limited Reinforcement Learning (CLRL) which builds on methods in soft RL to enable learning in agents with limited capacity.

The reviewers raised issues that were largely around three areas: there is a lack of clear motivation for the work, and many of the insights given lack intuition; many connections to related literature are missing; and the experimental results remain unconvincing.

Although the ideas presented in the paper are interesting, more work is required for this to be accepted. Therefore at this point, this is unfortunately a rejection.",Paper Decision
d3nKPyBQdY,BklMDCVtvr,Discovering the compositional structure of vector representations with Role Learning Networks,Reject,"This work builds directly on McCoy et al. (2019a) and add a RNN that can replace what was human generated hypotheses to the role schemes. The final goal of ROLE is to analyze a network by identifying ‘symbolic structure’. The authors conduct sanity check by conducting experiments with ground truth, and extend the work further to apply it to a complex model. I wonder under what definition of ‘interpretable’ authors have in mind with the final output (figure 2) - the output is very complex. It remains questionable if this will give some ‘insight’ or how would humans parse this info such that it is ‘useful’ for them in some way. 

Overall, though this is a good paper, due to the number of strong papers this year, it cannot be accepted at this time. We hope the comments given by reviewers can help improve a future version. 
",Paper Decision
CWlv4Bg2i,HJgfDREKDB,Higher-Order Function Networks for Learning Composable 3D Object Representations,Accept (Poster),"The submission presents an approach to single-view 3D reconstruction. The approach is quite creative and involves predicting the weights of a network that is then applied to a point set. The presentation is good. The experimental protocol is well-informed and the results are convincing. The reviewers' concerns have largely been addressed by the authors' responses and the revision. In particular, R2, who gave a ""3"", posted ""I would now advise to raise my score (3 previously) to a be in line with the 6: Weak Accept given by the other reviewers."" This means that all three reviewers recommend accepting the paper. The AC agrees.",Paper Decision
r0bvJ0UsDh,rkx-wA4YPS,Adapting to Label Shift with Bias-Corrected Calibration,Reject,"This was a borderline paper, but in the end two of the reviewers remain unconvinced by this paper in its current form, and the last reviewer is not willing to argue for acceptance. The first reviewer's comments were taken seriously in making a decision on this paper. As such, it is my suggestion that the authors revise the paper in its current form, and resubmit, addressing some of the first reviewers comments, such as discussion of utility of the methodology, and to improve the exposition such that less knowledgable reviewers understand the material presented better. The comments that the first reviewer makes about lack of motivation for parts of the presented methodology is reflected in the other reviewers comments, and I'm convinced that the authors can address this issue and make this a really awesome submission at a future conference.

On a different note, I think the authors should be congratulated on making their results reproducible. That is definitely something the field needs to see more of.",Paper Decision
K4IFjuHyF,SygWvAVFPr,Neural Module Networks for Reasoning over Text,Accept (Poster),"This work extends the previously introduced NMN for VQA for handling reasoning over text using symbolic reasoning components that can perform counting, sorting etc and can be compositionally combined. Moreover, to successfully train the model, the authors introduce a simple unsupervised auxiliary loss for training the IE components as well heuristically incorporating inductive biases in the behaviour on couple of components. All reviews agreed that this is a challenging topic and an interesting approach to symbolic reasoning over text. At the same time, reviewers did point that experiments are borderline thin, since the authors start with DROP and drop questions that are not particularly suited for symbolic reasoning, resulting in a substantially smaller dataset. Despite the fact that the experiments could probably be stronger, I’m recommending acceptance cause this topic is very interesting and this is a good paper to raise discussions at ICLR,",Paper Decision
-55OfVU8qF,B1xewR4KvH,MANIFOLD FORESTS: CLOSING THE GAP ON NEURAL NETWORKS,Reject,"This work explores how to leverage structure of this input in decision trees, the way this is done for example in convolutional networks.
All reviewers agree that the experimental validation of the method as presented is extremely weak. Authors have not provided a response to answer the many concerns raised by reviewers.
Therefore, we recommend rejection.",Paper Decision
aqGf8e_0ax,rJgJDAVKvB,Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees,Accept (Spotlight),All reviewers unanimously accept the paper.,Paper Decision
kofGORi9z0,ryx1wRNFvB,Improved memory in recurrent neural networks with sequential non-normal dynamics,Accept (Poster),"This paper proposes to explore nonnormal matrix initialization in RNNs.  Two reviewers recommended acceptance and one recommended rejection.  The reviewers recommending acceptance highlighted the utility of the approach, its potential to inspire future work, and the clarity and quality of writing and accompanying experiments.  One reviewer recommending weak acceptance expressed appreciation of the quality of the rebuttal and that their concerns were largely addressed.  The reviewer recommending rejection was primarily concerned with the novelty of the method.  Their review suggested the inclusion of an additional citation, which was included in a revised version for the rebuttal but not with a direct comparison of results.  On the balance, the paper has a relatively high degree of support from the reviewers, and presents an interesting and potentially useful initialization in a clear and well-motivated way.",Paper Decision
9SVUJqEIbG,S1lJv0VYDr,Model Imitation for Model-Based Reinforcement Learning,Reject,"This paper addresses challenges in offline model learning, i.e., in the setting where some trajectories are given and can be used for learning a model, which in turn serves to train an RL agent or plan action sequences in simulation. A key issue in this setting is that of compounding errors: as the simulated trajectory deviates from observed data, errors build up, leading to suboptimal performance in the target domain. The paper proposes a distribution matching approach that considers trajectory sequence information and provides theoretical guarantees as well as some promising empirical results.

Several issues were raised by reviewers, including missing references, clarity issues, questions about limitations of the theoretical analysis, and limitations of the empirical validation. Many of the issues raised by reviewers were addressed by the authors during the rebuttal phase.

At the same time, several issues remain. First, the authors committed to adding results for additional tasks (initially deemed too easy or too hard to show differences). Even if the tasks show little separation between methods, these would be important data points to include as they support additional comparisons with prior and future work. The AC has to assess the paper without taking promised additional results into account. Second, questions about the results for Ant are not sufficiently addressed. The plot shows no learning. The author response mentions initialization but this is not deemed a sufficient explanation. Given the remaining questions, my assessment is that the quality and contribution of the submission are not yet ready for publication at the current stage.",Paper Decision
MB6KhXyJR,H1eRI04KPB,Likelihood Contribution based Multi-scale Architecture for Generative Flows,Reject,"The authors propose a multi-scale architecture for generative flows that can learn which dimensions to pass through more flow layers based on a heuristic that judges the contribution to the likelihood. The authors compare the technique to some other flow based approaches. The reviewers asked for more experiments, which the authors delivered. However, the reviewers noted that a comparison to the SOTA for CIFAR in this setting was missing. Several reviewers raised their scores, but none were willing to argue for acceptance. ",Paper Decision
60S2eLVFVa,BylT8RNKPH,A Base Model Selection Methodology for Efficient Fine-Tuning,Reject,"This paper proposes to speed up finetuning of pretrained deep image classification networks by predicting the success rate of a zoom of pre-trained  networks without completely running them on the test set. The idea is that a sensible measure from the output layer might well correlate with the performance of the network. All reviewers consider this is an important problem and a good direction to make the effort. However, various concerns are raised and all reviewers unanimously rate weak reject. The major concerns include the unclear relationship between the metrics and the fine-tuning performance, non- comprehensive experiments, poor writing quality. The authors respond to Reviewers’ concerns but did not change the major concerns. The ACs concur the concerns and the paper can not be accepted at its current state.",Paper Decision
fAMy9BrMX,H1lTUCVYvH,Rethinking Curriculum Learning With Incremental Labels And Adaptive Compensation,Reject,"While the reviewers appreciated the ideas presented in the paper and their novelty, there were major concerns raised about the experimental evaluation. Due to the serious doubts that the reviewers raised about the effectiveness of the proposed approach, I do not think that the paper is quite ready for publication at this time, though I would encourage the authors to revise and resubmit the work at the next opportunity.",Paper Decision
oejiKPKQap,SJl28R4YPr,Graph Neural Networks for Reasoning 2-Quantified Boolean Formulas,Reject,"This work investigates the use of graph NNs for solving 2QBF . The authors provide empirical evidence that for this type of satisfiability decision problem, GNNs are not able to provide solutions and claim this is due to the message passing mechanism that cannot afford for complex reasoning. Finally, the authors propose a number of heuristics that extend GNNs and show that these improve their performance.

2-QBF problem is used as a playground since, as the authors also point, their complexity is in between  that of predicate and propositional logic. This on its own is not bad,  as it can be used as a minimal environment for the type of investigation the authors are interested. That being said, I find a number a number of flaws in the current form of the paper (some of them pointed by R3 as well), with the main issue being that of lack experimental rigor. Given the restricted set of problems the authors consider, I think the experiments on identifying pathologies of GNNs on this setup could have gone more in depth. Let me be specific. 

1) The bad performance is attributed to message-passing. However, this feels anecdotal at the moment and authors do not provide firm conclusions about that. The only evidence they provide is that performance becomes better with more message-passing iterations they allow. This is a hint though to dive deeper rather than a firm conclusion. For example do we know if the finding about sensitivity to  message-passing  is due to the small size of the network or the training procedure? 
2) To add on that, there is virtually no information on the paper about the specifics of the experimental setup, so the reader cannot be convinced that the negative results do not arise from a bad experimental configuration (e.g., small size of network).
3) Moreover, the negative results here, as the authors point, seem to contradict previous work, providing negative results against GNNs.  Again, this is a valuable contribution if that is indeed the case, but again the paper does not provide enough evidence. In lieu of a convincing set of experiments, the paper could provide a proof (as also asked by R3). However with no proof and not strong empirical evidence that this result does not feel ready to get published at ICLR.

Overall, I think this paper with a bit more rigor could be a very good submission for a later conference. However, as it stands I cannot recommend acceptance. 
",Paper Decision
FRAMa9WKOZ,SJlh8CEYDB,Learn to Explain Efficiently via Neural Logic Inductive Learning,Accept (Poster),"This paper proposes a differentiable inductive logic programming method in the vein of recent work on the topic, with efficiency-focussed improvements. Thanks the very detailed comments and discussion with the reviewers, my view is that the paper is acceptable to ICLR. I am mindful of the reasons for reluctance from reviewer #3 — while these are not enough to reject the paper, I would strongly, *STRONGLY* advise the authors to consider adding a short section providing comparison to traditional ILP methods and NLM in their camera ready.",Paper Decision
lhcB_89a2l,ryxn8RNtvr,NormLime: A New Feature Importance Metric for Explaining Deep Neural Networks,Reject,"The paper aims to extract the set of features explaining a class, from a trained DNN classifier.

The proposed approach relies on LIME (Ribeiro et al. 2016), modified as follows: i) around a point x, a linearized sparse approximation of the classifier is found (as in LIME); ii) for a given class, the importance of a feature aggregates the relative absolute weight of this feature in the linearized sparse approximations above; iii) the explanation is made of the top features in terms of importance.

This simple modification yields visual explanations that significantly better match the human perception than the SOTA competitors. 

The experimental setting based on the human evaluation via a Mechanical Turk setting is the second contribution of the approach. The feature importance measure is also assessed along a Keep and Retrain mechanism, showing that the approach selects actually relevant features in terms of prediction. 
Incidentally, it would be good to see the sensitivity of the method to parameter $k$ (in Eq. 1).

As noted by Rev#1, NormLIME is simple (and simplicity is a strength) and it demonstrates its effectiveness on the MNIST data. However, as noted by Rev#4, it is hard to assess the significance of the approach from this only dataset. 

It is understood that the Mechanical Turk-based assessment can only be used with a sufficiently simple problem.  However, complementary experiments on ImageNet for instance, e.g., showing which pixels are retained to classify an image as a husky dog, would be much appreciated to confirm the merits and investigate the limitations of the approach.
",Paper Decision
I-dzn5yAVm,rygoURNYvS,Pre-trained Contextual Embedding of Source Code,Reject,"The paper presents  CuBERT (Code Understanding BERT), which is BERT-inspired pretraining/finetuning setup, for source code contextual embedding. The embedding results are tested on classification tasks to demonstrate the effectiveness of CuBERT. 

This is an interesting application paper that extends existing models to source code analysis. The authors did a good job at motivating the applications, describing the proposed models and discussing the experiments. The authors also agree to share all the datasets and source code so that the experiment results can be replicated and compared with by other researchers. 

One major concern is the lack of strong baselines. All reviewers are concerned about this issue. The paper could lead to a good publication in the future if the issues can be addressed. ",Paper Decision
2dEm9zBfh,rkgiURVFDS,Certified Robustness to Adversarial Label-Flipping Attacks via Randomized Smoothing,Reject,"The authors develop a certified defense for label-flipping attacks (where an adversary can flip labels of a small number of training set samples) based on the randomized smoothing technique developed for certified defenses to adversarial perturbations of the input. The framework applies to least-squares classifiers acting on pretrained features learned by a deep network. The authors show that the resulting framework can obtain significant improvements in certified accuracy against targeted label flipping attacks for each test example.

While the paper makes some interesting contributions, the reviewers had the following shared concerns regarding the paper:
1) Reality of threat model: The threat model assumes that the adversary has access to the model and all of the training data (so as to choose which labels to flip), which is very unlikely in practice. 
2) Limitation to least squares on pre-trained features: The only practical instantiation of the framework presented in the paper is on least squares classifiers acting on pre-trained features learned by a deep network.

In the rebuttal phase, the authors clarified some of the more minor concerns raised by the reviewers, but the above concerns remained.

Overall, I feel that this paper is borderline - If the authors extend the applicability of the framework (for example relaxing the restriction on pre-training the deep features) and motivating the threat model more strongly, this could be an interesting paper.",Paper Decision
98KiOKgzxI,Ske5UANYDB,Benefit of Interpolation in Nearest Neighbor Algorithms,Reject,"The authors show that data interpolation in the context of nearest neighbor algorithms, can sometime strictly improve performance. The paper is poorly written for an ICLR audience and the added value compared to extensive prior work in the area is not clearly demonstrated.",Paper Decision
kEEvsoSxo,BkgqL0EtPH,{COMPANYNAME}11K: An Unsupervised Representation Learning Dataset for Arrhythmia Subtype Discovery,Reject,"This paper introduces a new ECG dataset. While I appreciate the efforts to clarify several points raised by the reviewers, I still believe this contribution to be of limited interest to the broad ICLR community. As such, I suggest this paper to be submitted to a more specialised venue.",Paper Decision
JV1v6PLD3J,ryxF80NYwS,Neural Clustering Processes,Reject,"This paper uses neural amortized inference for clustering processes to automatically tune the number of clusters based on the observed data. The main contribution of the paper is the design of the posterior parametrization based on the DeepSet method.  The reviewers feel that the paper has limited novelty since it mainly follows from existing methodologies. Also, experiments are limited and not all comparisons are made. ",Paper Decision
gV0ujR7cQf,ByxY8CNtvr,Improving Neural Language Generation with Spectrum Control,Accept (Poster),"Main content:

Blind review #2 summarizes it well:

Summary: This paper deals with the representation degeneration problem in neural language generation, as some prior works have found that the singular value distribution of the (input-output-tied) word embedding matrix decays quickly. The authors proposed an approach that directly penalizes deviations of the SV distribution from the two prior distributions, as well as a few other auxiliary losses on the orthogonality of U and V (which are now learnable). The experiments were conducted on small and large scale language modeling datasets as well as the relatively small IWSLT 2014 De-En MT dataset.

Pros:
+ The paper is well-written with great clarity. The dimensionality of the involved matrices (and their decompositions) are clearly provided, and the approach is clearly described. The authors also did a great job providing the details of their experimental setup.
+ The experiments seem to show consistent improvements over the baseline methods (at least the ones listed by the authors) on a relatively extensive set of tasks (e.g., of both small and large scales, of two different NLP tasks). Via WT2 and WT103, the authors also showed that their method worked on both LSTM and Transformers (which it should, as the SVD on word embedding should be independent of the underlying architecture).
+ I think studying the expressivity of the output embedding matrix layer is a very interesting (and important) topic for NLP. (e.g., While models like BERT are widely used, the actual most frequently re-used module of BERT is its pre-trained word embeddings.)

--

Discussion:

The reviewers agree that it is a very well written paper, and this is important as a conference paper to illuminate readers.

The one main objection is that spectrum control regularization was previously proposed and applied to GANs (Jiang et al ICLR 2019). However the authors convincingly point out that the technique is widely used, not only for GANs, and that application to neural language generation has quite different characteristics requiring a different, new approach: ""our proposed prior distributions as shown in Figure 2 in our paper are fundamentally different from the singular value distributions learned using their penalty functions (See Figure 1 and Table 7 in Jiang et al.’s paper). Figure 1 in their paper suggests that their penalty function, i.e., D-optimal Reg, will encourage all the singular values close to 1, which is well aligned with their motivation for training GAN. However, if we use such penalty function to train neural language models, the learned word representations will lose the power of modeling contextual information, and can result in much worse results than the baseline methods.""

--

Recommendation and justification:

I concur with the majority of reviewers that this paper is a weak accept. Though not revolutionary, it is well written, has usefully broad application, and is supported well empirically.",Paper Decision
GU5dn4luyo,B1guLAVFDB,Span Recovery for Deep Neural Networks with Applications to Input Obfuscation,Accept (Poster),The authors propose a way to recover latent factors implicitly constructed by a neural net with black box access to the nets output. This can be useful for identifying possible adversarial attacks. The majority of reviewers agrees that this is a solid technical and experimental contribution.,Paper Decision
nsJsm8okuG,rkguLC4tPB,Unknown-Aware Deep Neural Network,Reject,"This paper proposes the unknown-aware deep neural network (UDN), which can discover out-of-distribution samples for CNN classifiers. Experiments show that the proposed method has an improved rejection accuracy while maintaining a good classification accuracy on the test set. Three reviewers have split reviews. Reviewer #2 provides positive review for this work, while indicating that he is not an expert in image classification. Reviewer #1 agrees that the topic is interesting, yet the experiment is not so convincing, especially with limited and simple databases. Reviewer #3 shared the similar concern that the experiments are not sufficient. Further, R3 felt that the main idea is not well explained. The ACs concur these major concerns and agree that the paper can not be accepted at its current state.",Paper Decision
rLBkW3BIh5,Syeu8CNYvS,MODELLING   BIOLOGICAL   ASSAYS   WITH ADAPTIVE DEEP KERNEL LEARNING,Reject,"This work applies deep kernel learning to the problem of few shot regression for modeling biological assays. To deal with sparse data on new tasks, the authors propose to adapt the learned kernel to each task. Reviews were mixed about the method and experiments, some reviewers were satisfied with the author rebuttal while others did not support acceptance during the discussion period. Some reviewers ultimately felt that the experimental results were too weak to warrant publication. On the binding task the method is comparable with simpler baselines, and some felt that the gains on antibacterial were unconvincing. 
Other reviewers felt that there remained simpler baselines to compare with, for example ablating the affects of learning the kernel with simple hand picking one. While authors commented they tried this, there were no details given on the results or what exactly they tried. 

Based on the reviewer discussion, the work feels too preliminary in its current form to warrant publication in ICLR. However, given that there are clearly some interesting ideas proposed in this work, I recommend resubmitting with stronger experimental evidence that the method helps over baselines.",Paper Decision
XdS3cXZwV,rkePU0VYDr,A Perturbation Analysis of Input Transformations for Adversarial Attacks,Reject,"This paper presents an analysis on different methods of noise injection in adversarial examples, using gaussian noise for example. There are important issues raised by reviewers 1 & 2 about some conclusions not being well supported by the experiments and the utility/importance of some conclusions. After a discussion among reviewers, as of now all 3 reviewers stand by the decision that substantial improvements, and analysis can be made in the paper. Thus, Im recommending a Rejection.",Paper Decision
2sH993qYbi,HJeIU0VYwB,ADA+: A GENERIC FRAMEWORK WITH MORE ADAPTIVE EXPLICIT ADJUSTMENT FOR LEARNING RATE,Reject,"In this paper, the authors proposed a general framework, which uses an explicit function as an adjustment to the actual learning rate, and presented a more adaptive specific form Ada+. Based on this framework, they analyzed various behaviors brought by different types of the function. Empirical experiments on benchmarks demonstrate better performance than some baseline algorithms. The main concern of this paper is: (1) lack of justification or interpretation for the proposed framework; (2) the performance of the proposed algorithm is on a par with Padam; (3) missing comparison with some other baselines on more benchmark datasets. Plus, the authors did not submit response.  I agree with the reviewers’ evaluation.",Paper Decision
swpkmu9nm,Bke8UR4FPB,Oblique Decision Trees from Derivatives of ReLU Networks,Accept (Poster),"This paper leverages the piecewise linearity of predictions in ReLU neural networks to encode and learn piecewise constant predictors akin to oblique decision trees. The reviewers think the paper is interesting, and the idea is clever. The paper can be further improved in experiments. This includes comparison to ensembles of traditional trees or (in some cases) simple ReLU networks. Also  the tradeoffs other than accuracy between the method and baselines are also interesting. ",Paper Decision
_HBKhbOtRZ,BJerUCEtPB,Smooth Kernels Improve Adversarial Robustness and Perceptually-Aligned Gradients,Reject,"The authors propose a regularized for convolutional kernels that seeks to improve adversarial robustness of CNNs and produce more perceptually aligned gradients. While the topic studied by the paper is interesting, reviewers pointed out several deficiencies with the empirical evaluation that call into question the validity of the claims made by the authors. In particular:

1) Adversarial evaluation protocol: There are several red flags in the way the authors perform adversarial evaluation. The authors use a pre-defined adversarial attack toolbox (Foolbox) but are unable to produce successful attacks even for large perturbation radii - this suggests that the attack is not tuned properly. Further, the authors present results over the best case performance over several attacks, which is dubious since the goal of adversarial evaluation is to reveal the worst case performance of the model. 

2) Perceptual alignment: The claim of perceptually aligned gradients also does not seem sufficiently justified given the experimental results, since the improvement over the baseline is quite marginal. Here too, the authors report failure of a standard visualization technique that has been successfully used in prior work, calling into question the validity of these results.

The authors did not participate in the rebuttal phase and the reviewers maintained their scores after the initial reviews. 

Overall, given the significant flaws in the empirical evaluation, I recommend that the paper be rejected. I encourage the authors to rerun their experiments following the feedback from reviewers 1 and 3 and resubmit the paper with a more careful empirical evaluation.",Paper Decision
lqdGqI4fcO,r1gNLAEFPS,Neural ODEs for Image Segmentation with Level Sets,Reject,"This paper addresses the classic medial image segmentation by combining Neural Ordinary Differential Equations (NODEs) and the level set method. The proposed method is evaluated on kidney segmentation and salient object detection problems. Reviewer #1 provided a brief review concerning ICLR is not the appropriate venue for this work. Reviewer #2 praises the underlying concept being interesting, while pointing out that the presentation and experiments of this work is not ready for publication yet. Reviewer #3 raises concerns on whether the methods are presented properly. The authors did not provide responses to any concerns. Given these concerns and overall negative rating (two weak reject and one reject), the AC recommends reject.",Paper Decision
KqnNBBz_m6,SJgVU0EKwS,Precision Gating: Improving Neural Network Efficiency with Dynamic Dual-Precision Activations,Accept (Poster),"The submission proposes an approach to accelerate network training by modifying the precision of individual weights, allowing a substantial speed up without a decrease in model accuracy. The magnitude of the activations determines whether it will be computed at a high or low bitwidth.

The reviewers agreed that the paper should be published given the strong results, though there were some salient concerns which the authors should address in their final revision, such as how the method could be implemented on GPU and what savings could be achieved.

Recommendation is to accept.",Paper Decision
-4I5-m6NF7,BJxVI04YvB,PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction,Accept (Poster),"This paper describes a method for bounding the confidence around predictions made by deep networks. Reviewers agree that this result is of technical interest to the community, and with the added reorganization and revisions described by the authors, they and the AC agree the paper should be accepted. ",Paper Decision
SS4yLLIFT,SkeXL0NKwH,Low Rank Training of Deep Neural Networks for Emerging Memory Technology,Reject,"The reviewers generally agreed that the novelty of the work was very limited. This is not necessarily a deal-breaker for a largely applied contribution, but for an applied paper, the evaluation of the actual application on edge devices is not present. So if the main contribution is the application, and there is no evaluation of this application, then it does not seem like the paper is really complete. As such, I cannot recommend it for acceptance.",Paper Decision
z1JJ9TYRvf,H1gX8C4YPr,DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames,Accept (Poster),"The authors present and implement a synchronous, distributed RL called Decentralized Distributed Proximal Policy Optimization. The proposed technique was validated for pointgoal visual navigation task on recently introduced Habitat challenge 2019 and got the state of art performance.

Two reviews recommend this paper for acceptance with only some minor comments, such as revising the title. The Blind Review #2 has several major concerns about the implementation details. In the rebuttal, the authors provided the source code to make the results reproducible.

Overall, the paper is well written with promising experimental results. I also recommend it for acceptance.
",Paper Decision
DWWOyAyYBN,ryxz8CVYDH,Learning to Learn by Zeroth-Order Oracle,Accept (Poster),"This paper proposes to extend learning to learn framework based on zeroth-order optimization. Generally, the paper is well presented and easy to follow. The core idea is to incorporate another RNN to adaptively to learn the Gaussian sampling rule.  Although the method does not seem to have a strong theorical support, its effectiveness is evaluated in the well-organized experiments including realistic tasks like black-box adversarial attack. 
All reviewers including two experts in this field admit the novelty of the methods and are positive to the acceptance. I’d like to support their opinions and recommend accepting the paper.
As R#1 still finds some details unclear, please try to clarify these points in the final version of the paper.",Paper Decision
8J2VNxGK1V,HJlWIANtPH,Neural Embeddings for Nearest Neighbor Search Under Edit Distance,Reject,"This paper presents an approach to improving the calculation of embeddings for nearest-neighbor search with respect to edit distance.

Reading the reviews, it seems that the paper is greatly improved over its previous version, but still has significant clarity issues. Given that these issues remain even after one major revision, I would suggest that the paper not be accepted for this ICLR, but that the authors carefully revise the paper for clarity and submit to a following submission opportunity. It may help to share the paper with others who are not familiar with the research until they can read it once and understand the method well.

I have quoted Reviewer 3 below in the author discussion, where there are some additional clarity issues that may help being resolved:

----------

Some specifics are clear now with their new edition. 
* The [relationship between] cgk' & cgk not as clear as it could be. For example the algorithms are designed for bits. So one should assume that they are applying it on the bits of the characters. But this should be clarified in the manuscript.
* Also still backpropagating through f' is not clear to me.
* And in the text for inference they still say: ""We randomly select 100 queries and use the remainder of the dataset as the base set"" which should be ""the remainder excluding the training set"" or ""including?"".",Paper Decision
Xqc_D0kFwG,ryxW804FPH,ADAPTING PRETRAINED LANGUAGE MODELS FOR LONG DOCUMENT CLASSIFICATION,Reject,"This paper investigates ways of using pretrained transformer models like BERT for classification tasks on documents that are longer than a standard transformer can feasibly encode. 

This seems like a reasonable research goal, and none of the reviewers raised any concerns that seriously questioned the claims of the paper. However, neither of the more confident reviewers were convinced by the experiments in the paper (even after some private discussion) that the methods presented here represent a useful contribution. 

This is not an area that I (the area chair) know well, but it seems as though there aren't any easy fixes to suggest: Additional discussion of the choice of evaluation data (or new data), further ablations, and general refinement of the writing could help.",Paper Decision
YIIbXTrMDV,SJeeL04KvH,Robust Federated Learning Through Representation Matching and Adaptive Hyper-parameters,Reject,"This manuscript proposes strategies to improve both the robustness and accuracy of federated learning. Two proposals are online reinforcement learning for adaptive hyperparameter search, and local distribution matching to synchronize the learning trajectories of different local models. 

The reviewers and AC agree that the problem studied is timely and interesting, as it addresses known issues with federated learning. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and clarity of the conceptual and empirical results. Taken together, the AC's opinion is that the paper may not be ready for publication.",Paper Decision
zIPrpEwbT,BklxI0VtDB,ROS-HPL: Robotic Object Search with Hierarchical Policy Learning and Intrinsic-Extrinsic Modeling,Reject,"This paper introduces a two-level hierarchical reinforcement learning approach, applied to the problem of a robot searching for an object specified by an image.  The system incorporates a human-specified subgoal space, and learns low-level policies that balance the intrinsic and extrinsic rewards.  The method is tested in simulations against several baselines.

The reviewer discussion highlighted strengths and weaknesses of the paper.  One strength is the extensive comparisons with alternative approaches on this task.  The main weakness is the paper did not adequately distinguish between which aspects of the system were generic to HRL and which aspects are particular to robot object search.  The paper was not general enough to be understood as a generic HRL method. It was also ignoring much relevant background knowledge (robot mapping and navigation) if the paper is intended to be primarily about robot object search.  The paper did not convince the reviewers that the proposed method was desirable for either hierarchical reinforcement learning or for robot object search.

This paper is not ready for publication as the contribution was not sufficiently clear to the readers. 
",Paper Decision
OnGRdBQk4G,ryex8CEKPr,Knockoff-Inspired Feature Selection via Generative Models,Reject,"This manuscript proposes feature selection inspired by knockoffs, where the generative models are implemented using modern deep generative techniques. The resulting procedure is evaluated in a variety of empirical settings and shown to improve performance.

The reviewers and AC agree that the problem studied is timely and interesting, as knockoffs combined with generative models have recently shown promise for inferential problems. However, the reviewers were unconvinced about the motivation of the work, and the strength of the empirical evaluation results. In the option of the AC, this work might be improved by focusing (both conceptually and empirically) on applications where inferential variable selection is most relevant e.g. causal settings, healthcare applications, and so on.",Paper Decision
aPvShV3HR,SJx1URNKwH,MetaPix: Few-Shot Video Retargeting,Accept (Poster),"Three reviewers have assessed this paper and they have scored it 6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.",Paper Decision
B53UQddfwa,SkxJ8REYPH,SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum,Accept (Poster),"This paper presents a new approach, SlowMo, to improve communication-efficient distribution training with SGD. The main method is based on the BMUF approach and relies on workers to periodically synchronize and perform a momentum update. This works well in practice as shown in the empirical results. 

Reviewers had a couple of concerns regarding the significance of the contributions. After the rebuttal period some of their doubts were clarified. Even though they find that the solutions of the paper are an incremental extension of existing work, they believe this is a useful extension. For this reason, I recommend to accept this paper.",Paper Decision
1mMED5Pmro,rke2HRVYvH,Stochastic Prototype Embeddings,Reject,"The consensus of reviewers is that this paper is not acceptable in present form, and the AC concurs.",Paper Decision
-v8Nt4xX2n,rJl5rRVFvH,Way Off-Policy Batch Deep Reinforcement Learning of Human Preferences in Dialog,Reject,"This paper offers a possibly novel approach to regularizing policy learning to make it suitable for large-scale divergence in the underlying domain.  Unfortunately all the reviewers are unanimous that the paper is not acceptable in present form.  Insufficient clarity regarding the contribution relative to several references, some of which were missing from the submitted version, is perhaps the most significant issue in the view of the AC.",Paper Decision
2RgTqCB4qq,ByxtHCVKwB,Targeted sampling of enlarged neighborhood via Monte Carlo tree search for TSP,Reject,"This paper contributes to the recently emerging literature about applying reinforcement learning methods to combinatorial optimization problems.
The authors consider TSPs and propose a search method that interleaves greedy local search with Monte Carlo Tree Search (MCTS).
This approach does not contain learned function approximation for transferring knowledge across problem instances, which is usually considered the main motivation for applying RL to comb opt problems.

The reviewers state that, although the approach is a relatively straight-forward combination of two existing methods, it is in principle somewhat interesting. 
However, the experiments indicate a large gap to SOTA solvers for TSPs. 
No rebuttal was submitted.

In absence of both SOTA results and methodological novelty, as assessed by the reviewers and my owm reading, I recommend to reject the paper in its current form.",Paper Decision
8ziCCZEmMA,H1xKBCEYDr,Black-box Adversarial Attacks with Bayesian Optimization,Reject,"The paper proposes a Bayesian optimization approach to creating adversarial examples. The general idea has been in the air for some years, and over the last year especially there have been a number of approaches using BayesOpt for this purpose. Reviewers raised concerns about differences between this approach and related work, and practical challenges in general for using BayesOpt in this domain (regarding dimensionality, etc.). The authors provided thoughtful responses, although some of these concerns still remain. The authors are encouraged to address all comments carefully in future revisions, which a sufficiently substantial that the paper would benefit from additional review.


",Paper Decision
KZy-KhXbv,BJedHRVtPB,Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving,Accept (Poster),Three knowledgable reviewers give a positive evaluation of the paper. The decision is to accept.,Paper Decision
gHPe4WTsZc,S1g_S0VYvr,Learning to Combat Compounding-Error in Model-Based Reinforcement Learning,Reject,"The paper received mixed reviews: R (R3), WA (R2), A (R1). AC has read the reviews, rebuttal and paper. AC is concerned about the short planning horizon, which seems like a major issue: (i) as R1 notes, most MPC algorithms use much longer horizons as they find it helps performance and (ii) the claim of the approach to be able to pick the planning horizon is moot if its dynamic range is small.  Overall, the paper is very borderline. The idea is interesting but without addressing longer horizons, the contribution is limited. Under guidance from the PCs, the AC feels that the paper just falls below the acceptance threshold and thus cannot be accepted unfortunately. The work is definitely interesting however and should be revised for a future submission. 

",Paper Decision
DO0dqEX1E,BylDrRNKvH,Understanding Attention Mechanisms,Reject,"This paper aims to theoretically understand the the benefit of attention mechanisms. The reviewers agreed that better understanding of attention mechanisms is an important direction. However, the paper studies a weaker form of attention which does not correspond well to the attention models using in the literature. The paper should better motivate why the theoretical results for this restrained model would carry over to more realistic mechanisms.",Paper Decision
FfXAUjCbpn,H1lDSCEYPH,Beyond GANs: Transforming without a Target Distribution,Reject,"This paper presents a new generative modeling approach to transform between data domains via a neuron editing technique. The authors address the scenario of source to target domain translation that can be applied to a new source domain. While the reviewers acknowledged that the idea of neuron editing is interesting, they have raised several concerns that were viewed by AC as critical issues: (1) given the progress that have been made in the field, an empirical comparison with SOTA GANs models is required to assess the benefits/competitiveness of the proposed approach -- see R1’s comments, also [StarGAN by Choi et al, CVPR 2018], (2) the literature review is incomplete and requires a major revision -- see R1’s and R3’s suggestions, also [CYCADA by Hoffman et al, ICML 2018], (3) presentation clarity -- see R1’s and R2’s comments. AC suggests, in its current state the manuscript is not ready for a publication. We hope the detailed reviews are useful for improving and revising the paper. ",Paper Decision
NlZ1JWpOen,HJx8HANFDH,Four Things Everyone Should Know to Improve Batch Normalization,Accept (Poster),"This paper proposes techniques to improve training with batch normalization. The paper establishes the benefits of these techniques experimentally using ablation studies. The reviewers found the results to be promising and of interest to the community. However, this paper is borderline due in part due to the writing (notation issues) and because it does not discuss related work enough. We encourage the authors to properly address these issues before the camera ready.",Paper Decision
9dFBtRxa83,ByeUBANtvB,Learning to solve the credit assignment problem,Accept (Poster),"Initial reviews of this paper cited some concerns about a lack of comparison to SOTA and baselines, and also some debate over claims of what is (or is not) ""biologically plausible.""  However, after extensive back-and-forth between the authors and reviewers these issues have been addressed and the paper has been improved.  There is now consensus among authors that this paper should be accepted.  I would like to thank the reviewers and authors for taking the time to thoroughly discuss this paper.",Paper Decision
v5KM3iP2TY,HJlISCEKvB,Improving Multi-Manifold GANs with a Learned Noise Prior,Reject,"This paper introduces a modified GAN architecture that looks a lot like a mixture of experts, to address the problem of learning multiple disconnected manifolds.  They show this method helps on 2D toy experiments, and artificial tasks where different datasets are combined, but not on CIFAR.  They also introduced a new variant of FID that they claim is more sensitive to the improvements made by their model.

R2 didn't seem to think too hard about the paper, and R3 seemed a bit dismissive.

Overall the idea seems sensible but the particulars of this approach aren't all that well-motivated in my opinion, especially since the cost of the generator is increased.  Why not just use a mixture of Gaussians in the original untransformed space?

I also found the toy experiments unconvincing, particularly the claim that a standard GAN couldn't learn a mixture of 3 Gaussians.  Learning a mixture of 8 Gaussians was one of the results in the unrolled GAN paper, for instance.

The results on the mixed datasets experiments seem encouraging, but I'm afraid that proposing a new GAN architecture in 2019 requires even more baselines than the authors compared against, and the fact that the task was artificially constructed undercuts its importance.",Paper Decision
cpaZXwDPR,HylrB04YwH,Overparameterized Neural Networks Can Implement Associative Memory,Reject,"The paper shows that overparameterized autoencoders can be trained to memorize a small number of training samples, which can be retrieved via fixed point iteration. After rounds of discussion with the authors, the reviewers agree that the idea is interesting and overall quality of writing and experiments is reasonable, but they were skeptical regarding the significance of the finding and impact to the field and thus encourage studying the phenomenon further and resubmitting in a future conference. I thus recommend rejecting this submission for now.",Paper Decision
HINRSzHIyS,B1grSREtDH,Bayesian Residual Policy Optimization: Scalable Bayesian Reinforcement Learning with Clairvoyant Experts,Reject,"This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work (see in particular the updates from AnonReviewer1), and I urge the authors to continue to develop refinements and extensions.",Paper Decision
TqHOhtRYn,rylVHR4FPB,Sampling-Free Learning of Bayesian Quantized Neural Networks,Accept (Poster),"This paper proposes Bayesian quantized networks and efficient algorithms for learning and prediction of these networks. The reviewers generally thought that this was a novel and interesting paper.  There were a few concerns about the clarity of parts of the paper and the experimental results. These concerns were addressed during the discussion phase, and the reviewers agree that the paper should be accepted.",Paper Decision
i5IG-Z8iRd,ryeEr0EFvS,A Hierarchy of Graph Neural Networks Based on Learnable Local Features,Reject,"This paper proposes a modification to GCNs that generalizes the aggregation step to multiple levels of neighbors, that in theory, the new class of models have better discriminative power. The main criticism raised is that there is lack of sufficient evidence to distinguish this works theoretical contribution from that of Xu et al. Two reviewers also pointed out the concerns around experiment results and suggested to includes more recent state of the art SOTA results. While authors disagree that the contributions of their work is incremental, reviewers concerns are good samples of the general readers of this paper— general readers may also read this paper as incremental. We highly encourage authors to take another cycle of edits to better distinguish their work from others before future submissions. 
",Paper Decision
C4KIlfcXPD,rJeXS04FPH,DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling,Accept (Poster),"The authors design a deep model architecture for learning word embeddings with better performance and/or more efficient use of parameters.  Results on language modeling and machine translation are promising.  Pros:  Interesting idea and nice results.  New model may have some independent value beyond NLP.  Cons:  Empirical comparisons could be more thorough.  For example, it is not clear (to me at least) what would be the benefits of this approach applied to whole words versus a competitor using subword units.",Paper Decision
PcKIIay7Cm,rJg7BA4YDr,NEURAL EXECUTION ENGINES,Reject,"This paper investigates the problem of building a program execution engine with neural networks. While the reviewers find this paper to contain interesting ideas, the technical contributions, scope of experiments, and the presentation of results would need to be significantly improved in order for this work to reach the quality bar of ICLR.",Paper Decision
zOP1lwF384,BygfrANKvB,Learning to Make Generalizable and Diverse Predictions for Retrosynthesis,Reject,"The authors present a new approach to improve performance for retro-synthesis using a seq2seq model, achieving significant improvement over the baseline. There are a number of lingering questions regarding the significance and impact of this work. Hence, my recommendation is to reject. ",Paper Decision
nmh0ODe2j,SyezSCNYPB,Disentangled GANs for Controllable Generation of High-Resolution Images,Reject,"The paper presents a model combining AC-GAN and StyleGAN for semi-supervised learning of disentangled generative adversarial networks. It also proposes new datasets of 3d images as benchmarks. The main claim is that the proposed model can achieve strong disentanglement property by using 1-5% of the annotations on the factors of variation. The technical contribution is moderate but the architecture itself is not highly novel. While the proposed method seems to work for controlled/synthetic datasets, overall technical contribution seems incremental and it's unclear whether it can perform well on larger-scale, real datasets. The experimental results on CelebA don't look convincing enough.
",Paper Decision
aa6gblT3Nl,BkgZSCEtvr,Continuous Graph Flow,Reject,Novelty of the proposed model is low. Experimental results are weak.,Paper Decision
1KpU2HKn2G,BkxgrAVFwH,Wasserstein-Bounded Generative Adversarial Networks,Reject,"The paper presents a framework named Wasserstein-bounded GANs which generalizes WGAN. The paper shows that WBGAN can improve stability.

The reviewers raised several questions about the method and the experiments, but these were not addressed.

I encourage the authors to revise the draft and resubmit to a different venue.",Paper Decision
9bFX0tdWI5,rkgyS0VFvr,DBA: Distributed Backdoor Attacks against Federated Learning,Accept (Poster),"Thanks for the discussion, all. This paper proposes an attack strategy against federated learning. Reviewers put this in the top tier, and the authors responded appropriately to their criticisms. ",Paper Decision
qg34Yr5Lvx,Skl1HCNKDr,Learning Generative Models using Denoising Density Estimators,Reject,"The majority of reviewers suggest that this paper is not yet ready for publication. The idea presented in the paper is interesting, but there are concerns about what experiments are done, what papers are cited, and how polished the paper is. This all suggests that the paper could benefit from a bit more time to thoughtfully go through some of the criticisms, and make sure that everything reviewers suggest is covered.",Paper Decision
H_yw7egdJ4,BJx040EFvH,Fast is better than free: Revisiting adversarial training,Accept (Poster),"This paper provides a surprising result: that randomization and FGSM can produce robust models faster than previous methods given the right mix of cyclic learning rate, mixed precision, etc. This paper produced a fair bit of controversy among both the community and the reviewers to the point where there were suggestions of bugs, evaluation problems, and other issues leading to the results. In the end, the authors released the code (and made significant updates to the paper based on all the feedback). Multiple reviewers checked the code and were happy. There was an extensive author response, and all the reviewers indicated that their primary concerns were address, save concerns about the sensitivity of step-size and the impact of early stopping.

Overall, the paper is well written and clear. The proposed approach is simple and well explained. The result is certainly interesting, and this paper will continue to generate fruitful debate. There are still things to address to improve the paper, listed above. I strongly encourage the authors to continue to improve the work and make a more concerted effort to carefully discuss the impacts of early stopping.
",Paper Decision
x-0rXp0qws,r1l0VCNKwB,LOSSLESS SINGLE IMAGE SUPER RESOLUTION FROM LOW-QUALITY JPG IMAGES,Reject,"Main summary:  Sngle image super-resolution network that can generate high-resolution images from the corresponding C-JPG images

Discussions
reviewer 3: reviewer has a few issues including, claim the method is lossless, want more information about JPG revovering step
reviewer 1: (not knowledgable): paper is well written and reviewer gives very few cons
reviewer 2: main concerns are wrt novelty and technically sound
Recommendation: the 2 more knowledgable reviwers mark this as Reject, I agree.",Paper Decision
PIOVA1p81,HJg6VREFDH,iWGAN: an Autoencoder WGAN for Inference,Reject,"This paper proposes a new way to stabilise GAN training.

The reviews were very mixed but taken together below acceptance threshold.

Rejection is recommended with strong motivation to work on the paper for next conference. This is potentially an important contribution. ",Paper Decision
OWy8iidNeV,SklnVAEFDB,BERT-AL: BERT for Arbitrarily Long Document Understanding,Reject,"This paper proposes a hybrid LSTM-Transformer method to use pretrained Transformers like BERT that have a fixed maximum sequence lengths on texts longer than that limit.

The consensus of the reviewers is that the results aren't sufficient to justify the primary claims of the paper, and that—in addition—the missing details and ablations cast doubt on the reliability of those results. This is an interesting research direction, but substantial further experimental work would be needed to turn this into something that's ready for publication at a top venue.",Paper Decision
9ubfoVqniD,SJeoE0VKDS,Novelty Search in representational space for sample efficient exploration,Reject,"The two most experienced reviewers recommended the paper be rejected.  The submission lacks technical depth, which calls the significance of the contribution into question.  This work would be greatly strengthened by a theoretical justification of the proposed approach.  The reviewers also criticized the quality of the exposition, noting that key parts of the presentation was unclear.  The experimental evaluation was not considered to be sufficiently convincing.  The review comments should be able to help the authors strengthen this work.",Paper Decision
15rFYiQSUk,SyxjVRVKDB,Switched linear projections and inactive state sensitivity for deep neural network interpretability,Reject," This paper proposes a method to capture patterns of the so called “off” neurons using a newly proposed metric. The idea is interesting and worth pursuing. However, the paper needs another round of modification to improve both writing and experiments. ",Paper Decision
zqwm3tx1F3,Ske9VANKDH,An Optimization Principle Of Deep Learning?,Reject,The paper is rejected based on unanimous reviews.,Paper Decision
bNjWM4VtYi,Byl5NREFDr,Thieves on Sesame Street! Model Extraction of BERT-based APIs,Accept (Poster),"Two knowledgable reviewers recommend accepting the paper, and the less familiar reviewer is also positive. The final decision is to accept the paper. It's an interesting and timely topic with insightful results.",Paper Decision
d1m4vN94go,BygFVAEKDH,Understanding Knowledge Distillation in Non-autoregressive Machine Translation,Accept (Poster),"Main content:

Blind review #3 summarized it well, as follows:

This paper studies knowledge distillation in the context of non-autoregressive translation. In particular, it is well known that in order to make NAT competitive with AT, one needs to train the NAT system on a distilled dataset from the teacher model. Using initial experiments on EN=>ES/FR/DE, the authors argue that this necessity arises from the overly-multimodal nature of the output distribution, and that the AT teacher model produces a less multimodal distribution that is easier to model with NAT. 

Based on this, the authors propose two quantities that estimate the complexity (conditional entropy) and faithfulness (cross entropy vs real data), and derive approximations to these based on independence assumptions and an alignment model.  The translations from the teacher output are indeed found to be less complex, thereby facilitating easier training for the NAT student model.

--

Discussion:

Questions were mostly about how robust the results were on other language pairs and random starting points. Authors addressed questions reasonably.

One low review came from a reviewer who admitted not knowing the field, and I agree with the other two reviewers.

--

Recommendation and justification:

I think papers that offer empirically support for scientific insight (giving an ""a-ha!"" reaction), rather than massive engineering efforts to beat the state of the art, are very worthwhile in scientific conferences. This paper meets that criteria for acceptance.",Paper Decision
m24isqGWA,rkltE0VKwH,Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning,Reject,"The authors present a method that utilizes intrinsic rewards to coordinate the exploration of agents in a multi-agent reinforcement learning setting.   The reviewers agreed that the proposed approach was relatively novel and an interesting research direction for multiagent RL.  However, the reviewers had substantial concerns about writing clarity, the significance of the contribution of the propose method, and the thoroughness of evaluation (particularly the number of agents used and limited baselines).  While the writing clarity and several technical points (including addition ablations) were addressed in the rebuttal, the reviewers still felt that the core contribution of the work was a bit too marginal.  Thus, I recommend this paper to be rejected at this time.",Paper Decision
rE8VRHGt_c,HJg_ECEKDr,Generative Teaching Networks: Accelerating Neural Architecture Search by Learning  to Generate Synthetic Training Data,Reject,"Overview:
This paper introduces a method to distill a large dataset into a smaller one that allows for faster training. The main application of this technique being studied is neural architecture search, which can be sped up by quickly evaluating architectures on the generated data rather than slowly evaluating them on the original data.

Summary of discussion:
During the discussion period, the authors appear to have updated the paper quite a bit, leading to the reviewers feeling more positive about it now than in the beginning. In particular, in the beginning, it appears to have been unclear that the distillation is merely used as a speedup trick, not to generate additional information out of thin air. The reviewers' scores left the paper below the decision boundary, but closely enough so that I read it myself. 

My own judgement:
I like the idea, which I find very novel. However, I have to push back on the authors' claims about their good performance in NAS. This has several reasons:

1. In contrast to what is claimed by the authors, the comparison to graph hypernetworks (Zhang et al) is not fair, since the authors used a different protocol: Zhang et al sampled 800 networks and reported the performance (mean +/- std) of the 10 judged to be best by the hypernetwork. In contrast, the authors of the current paper sampled 1000 networks and reported the performance of the single one judged to be best. They repeated this procedure 5 times to get mean +/- std. The best architecture of 1000 is of course more likely to be strong than the average of the top 10 of 800.

2. The comparison to random search with weight sharing (here: 3.92% error) does not appear fair. The cited paper in Table 1 is *not* the paper introducing random search + weight sharing, but the neural architecture optimization paper. The original one reported an error of 2.85% +/- 0.08% with 4.3M params. That paper also has the full source code available, so the authors could have performed a true apples-to-apples comparison. 

3. The authors' method requires an additional (one-time) cost for actually creating the 'fake' training data, so their runtimes should be increased by the 8h required for that.

4. The fact that the authors achieve 2.42% error doesn't mean much; that result is just based on scaling the network up to 100M params. (The network obtained by random search also achieves 2.51%.)

As it stands, I cannot judge whether the authors' approach yields strong performance for NAS. In order to allow that conclusion, the authors would have to compare to another method based on the same underlying code base and experimental protocol. Also, the authors do not make code available at this time. Their method has a lot of bells and whistles, and I do not expect that I could reproduce it. They promise code, but it is unclear what this would include: the generated training data, code for training the networks, code for the meta-approach, etc? This would have been much easier to judge had the authors made the code available in anonymized fashion during the review.

Because of these reasons, in terms of making progress on NAS, the paper does not quite clear the bar for me. The authors also evaluated their method in several other scenarios, including reinforcement learning. These results appear to be very promising, but largely preliminary due to lack of time in the rebuttal phase.  

Recommendation:
The paper is very novel and the results appear very promising, but they are also somewhat preliminary. The reviewers' scores leave the paper just below the acceptance threshold and my own borderline judgement is not positive enough to overrule this. I believe that some more time, and one more iteration of reorganization and review, would allow this paper to ripen into a very strong paper. For a resubmission to the next venue, I would recommend to either perform an apples-to-apples comparison for NAS or reorganize and just use NAS as one of several equally-weighted possible applications. In the current form, I believe the paper is not using its full potential.",Paper Decision
u0GJlWsq_,Hye_V0NKwr,Locality and Compositionality in Zero-Shot Learning,Accept (Poster),"This paper investigates the role of locality (ability to encode only information specific to locations of interest) and compositionality (ability to be expressed as a combination of simpler parts) in Zero-Shot Learning (ZSL). Main contributions of the paper are (i) compared to previous ZSL frameworks, the proposed approach is that the model is not allowed to be pretrained on another dataset (ii) a thorough evaluation of existing methods.

Following discussions, weaknesses are (i) the proposed method (CMDIM) isn't sufficiently different or interesting compared to existing methods (ii) the paper does not do an in-depth discussion of locality and compositionality. The empirical evaluation being extensive, the accept decision is chosen.
",Paper Decision
H5nMEwnWQJ,rklw4AVtDH,Optimistic Adaptive Acceleration for Optimization,Reject,"The paper introduces a variant of AMSGrad (""Optimistic-AMSGrad""), which integrates an estimate of the future gradient into the optimization problem. While the method is interesting, reviewers agree that novelty is on the low side. The motivation of the approach should also be clarified. The experimental section should be made stronger; in particular, reporting convincing wall-clock running time advantages is critical for validating the viability of the proposed approach. 

",Paper Decision
_o8AzJts-O,ByePEC4KDS,Situating Sentence Embedders with Nearest Neighbor Overlap,Reject,"This paper proposes to analyze the space of known sentence-to-vector functions by comparing the ways in which they induce nearest neighbor lists in a text corpus.

The primary results of the study are somewhat unclear, and the reviewers do not find the method to be novel enough—or sufficiently well motivated a priori—to warrant publication in spite of these results.",Paper Decision
l2GKf6cXhy,BklLVAEKvH,Generalized Clustering by Learning to Optimize Expected Normalized Cuts,Reject,"This paper proposes a deep clustering method based on normalized cuts.  As the general idea of deep clustering has been investigated a fair bit, the reviewers suggest a more thorough empirical validation.  Myself, I would also like further justification of many of the choices within the algorithm, the effect of changing the architecture.",Paper Decision
2xGCbkP3_E,H1gB4RVKvB,Recurrent neural circuits for contour detection,Accept (Poster),"All the reviewers recommend accept, and the found the paper interesting and novel. ",Paper Decision
xoa23UcmPK,HJxrVA4FDS,Disentangling neural mechanisms for perceptual grouping,Accept (Spotlight),All the reviewers recommend acceptance. The reviews found the paper to be interesting with substantial insights. ,Paper Decision
ppONLmvUA,SJlVVAEKwS,Adversarial Imitation Attack,Reject,"This paper proposes to use a generative adversarial network to train a substitute that replicates (imitates) a learned model under attack. It then shows that the adversarial examples for the substitute can be effectively used to attack the learned model. The proposed approach leads to better success rates of attacking than other substitute-training approaches that require more training examples. The condition to get a well-trained imitation model is that a sufficient number of queries are obtained from the target model. This paper has valuable contributions by developing an imitation attacker. However, some key issues remain. In particular, I agree with R1 that the average number of queries per image is relatively high, even during training. In the rebuttal, the authors made the assumption that “suppose their method could make an infinite number of queries for target models”, which is unfortunately not realistic. Another point that I found confusing: at testing, I don’t see how you can use the imitation model D to generate adversarial samples (D is a discriminative model, not a generator); it should be G, right?
",Paper Decision
QScSo_rdNA,Hkl4EANFDH,Regularizing Trajectories to Mitigate Catastrophic Forgetting,Reject,"The submission proposes a 'co-natural' gradient update rule to precondition the optimization trajectory using a Fisher information estimate acquired from previous experience. This results in reduced sensitivity and forgetting when new tasks are learned. 

The reviews were mixed on this paper, and unfortunately not all reviewers had enough expertise in the field. After reading the paper carefully, I believe that the paper has significance and relevance to the field of continual learning, however it will benefit from more careful positioning with respect to other work as well as more empirical support. The application to the low-data-regime is interesting and could be expanded and refined in a future submission. 

The recommendation is for rejection.",Paper Decision
vs1rbj96G,rkxXNR4tvH,Semantic Pruning for Single Class Interpretability,Reject,"The authors propose to use pruning to study/interpret learned CNNs. The reviewers believed the results were not surprising and/or had no practical relevance. Unlike in many cases, two of the reviewers acknowledged reading the rebuttals, but were unswayed.",Paper Decision
sjPo1lLu6G,r1xfECEKvr,Analyzing the Role of Model Uncertainty for Electronic Health Records,Reject,"The paper considers an important problem in medical applications of deep learning, such as variability/stability of  model's predictions in face of various perturbations in the model (e.g., random seed), and evaluates different approaches to capturing model uncertainty. However, it appears to be little innovation in terms of machine-learning methodology, so ICLR might not be the best venue for this work, while perhaps other venues focused more on medical applications might be a better fit. 
 ",Paper Decision
9pJqh4a5oo,rygG4AVFvH,Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation,Accept (Poster),"This paper proposes to optimize the code optimal code in DNN compilers using adaptive sampling and reinforcement learning. This method achieves  significant speedup in compilation time and execution time. The authors made strong efforts in addressing the problems raised by the reviewers, and promised to make the code publicly available, which is of particular importance for works of this nature.   
",Paper Decision
bFGI4yCUI5,SygfNCEYDH,Weakly-supervised Knowledge Graph Alignment with Adversarial Learning,Reject,"Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects.
This paper potentially discusses an interesting problem, and the concern raised by Review #2 was addressed in the revised paper.
However,  given the  high competition at ICLR2020, this paper is unfortunately below the bar.
We hope that the reviewers' comments are useful for improving the paper for potential future publication.

The ",Paper Decision
tVq1bBESvl,SylWNC4FPH,Auto Completion of User Interface Layout Design Using Transformer-Based Tree Decoders,Reject,"The paper introduces an interesting application of GNNs, but the reviewers find that the contribution is too limited and the motivation is too weak.",Paper Decision
2FKyNMplY,rkeZNREFDr,Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation,Reject,"This paper proposes to learn self-explaining neural networks using a feature leveling idea.  Unfortunately, the reviewers have raised several concerns on the paper, including insufficiency of novelty, weakness on experiments, etc. The authors did not provide rebuttal. We hope the authors can improve the paper in future submission based on the comments.  
",Paper Decision
Teuck8hyTB,SJleNCNtDH,Intrinsic Motivation for Encouraging Synergistic Behavior,Accept (Poster),"The authors address the important issue of exploration in reinforcement learning. In this case, they propose to use reward shaping to encourage joint-actions whose outcomes deviate from the sequential counterpart. Although the proposed intrinsic reward is targeted at a particular family of two-agent robotic tasks, one can imagine generalizing some of the ideas here to other multi-agent learning tasks.

The reviewers agree that the paper is of interest to the ICLR audience.",Paper Decision
RP5jcRxbI,BklxN0NtvB,Noisy Machines: Understanding noisy neural networks and enhancing robustness to analog hardware errors using distillation,Reject,"This paper argues that NNs deployed to hardware needs to robust to additive noise and introduces two methods to achieve this.

The reviewers liked aspects of the paper and the paper is borderline. However, all in all sufficient reservations were raised to put the paper below the threshold. The criticism was constructive and can be used in an updated version submitted to next conference.

Rejection is recommended.",Paper Decision
AkrvZGFC0y,S1e1EAEFPB,Perceptual Regularization: Visualizing and Learning Generalizable Representations,Reject,"This paper proposes a new mechanism to visualize the latent space of a neural network. The idea is simple and the paper includes several experiments to test the effectiveness of the method. However, the method bears similarity to previous work and the evaluation does not sufficiently show quantitative improvements over other introspection techniques. The reviewers found this was a substantial problem and for this reason the paper is not ready for publication. The paper should improve its discussion of prior work and better establish its place in this regard.",Paper Decision
J-Jq5dWiWe,BJlJVCEYDB,Neural networks with motivation,Reject,"This paper proposes a deep RL framework that incorporates motivation as input features, and is tested on 3 simplified domains, including one which is presented to rodents. 

While R2 found the paper well-written and interesting to read, a common theme among reviewer comments is that it’s not clear what the main contribution is, as it seems to simultaneously be claiming a ML contribution (motivation as a feature input helps with certain tasks) as well as a neuroscientific contribution (their agent exhibited representations that clustered similarly to those in animals). In trying to do both, it’s perhaps doing both a disservice. 

I think it’s commendable to try to bridge the fields of deep RL and neuroscience, and this is indeed an intriguing paper. However any such paper still needs to have a clear contribution. It seems that the ML contributions are too slight to be of general practical use, while the neuroscientific contributions are muddled somewhat. The authors several times mentioned the space constraints limiting their explanations. Perhaps this is an indication that they are trying to cover too much within one paper. I urge the authors to consider splitting it up into two separate works in order to give both the needed focus. 

I also have some concerns about the results themselves. R1 and R3 both mentioned that the comparison between the non-motivated agent and the motivated agent wasn’t quite fair, since one is essentially only given partial information. It’s therefore not clear how we should be interpreting the performance difference. Second, why was the non-motivated agent not analyzed in the same way as the motivated agent for the Pavlovian task? Isn’t this a crucial comparison to make, if one wanted to argue that the motivational salience is key to reproducing the representational similarities of the animals?  (The new experiment with the random fixed weights is interesting, I would have liked to see those results.) For these reasons and the ones laid out in the extensive comments of the reviewers, I’m afraid I have to recommend reject.
",Paper Decision
5XNRtVCmqy,HJxR7R4FvS,RaCT: Toward Amortized Ranking-Critical Training For Collaborative Filtering ,Accept (Poster),"The reviewers generally agreed that the application and method are interesting and relevant, and the paper should be accepted.

I would encourage the authors to carefully go through the reviewers' suggestions and address them in the final.",Paper Decision
lTD-xLRqmc,H1eA7AEtvS,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,Accept (Spotlight),"This paper proposes three modifications of BERT type models two of which is concerned with parameter sharing and one with a new auxiliary loss. New SOTA on downstream tasks are demonstrated. 

All reviewers liked the paper and so did a lot of comments. 

Acceptance is recommended.",Paper Decision
d2KRXpvndz,BklTQCEtwH,Curriculum Learning for Deep Generative Models with Clustering,Reject,"The paper proposes a curriculum learning approach to training generative models like GANs. The reviewers had a number of questions and concerns related to specific details in the paper and experimental results. While the authors were able to address some of these concerns, the reviewers believe that further refinement is necessary before the paper is ready for publication.",Paper Decision
Pk5u_ZzrB,SklTQCNtvS,Sign-OPT: A Query-Efficient Hard-label Adversarial Attack,Accept (Poster),"The reviewers had several concerns with the paper related to novelty and comparisons with other approaches. During the discussion phase, these concerns were adequately addressed.",Paper Decision
_WrZSKAvuC,S1xnXRVFwH,Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP,Accept (Poster),"This paper explores the application of the lottery ticket hypothesis to NLP and RL problems for better initialisations of deep networks and reduced model sizes. This is evaluated in a variety of settings, including continuous control and ATARI games for RL, and LSTMs and Transformers for NLP, showing very positive results.

The main issue raised by the reviewers was the lack of algorithmic novelty in the paper. Despite this, I believe the paper to present an important contribution that could stimulate much additional research. The paper is well written and the results are rigorous and interesting. For these reasons I recommend acceptance.",Paper Decision
M1H8ysZA5,rkenmREFDr,Learning Space Partitions for Nearest Neighbor Search,Accept (Poster),"This paper proposes a new framework for improved nearest neighbor search by learning a space partition of the data, allowing for better scalability in distributed settings and overall better performance over existing benchmarks.

The two reviewers who were most confident were both positive about the contributions and the revisions. The one reviewer who recommended reject was concerned about the metric used and whether comparison with baselines was fair. In my opinion, the authors seem to have been very receptive to reviewer comments and answered these issues to my satisfaction. After author and reviewer engagement, both R1 and myself are satisfied with the addition of the new baselines and think the authors have sufficiently addressed the major concerns. For the final version of the paper, I’d urge the authors to take seriously R4’s comment regarding clarity and add algorithmic details as per their suggestion.
",Paper Decision
x4Kyjmgyef,Hyes70EYDB,Visual Interpretability Alone Helps Adversarial Robustness,Reject,"This work focuses on how one can design models with robustness of interpretations. While this is an interesting direction, the paper would benefit from a more careful treatment of its technical claims.

",Paper Decision
rtfzk8xzD1,B1lsXREYvr,One-Shot Neural Architecture Search via Compressive Sensing,Reject,"This paper proposed to use a compressive sensing approach for neural architecture search, similar to Harmonica for hyperparameter optimization. 

In the discussion, the reviewers noted that the empirical evaluation is not comparing apples to apples; the authors could not provide a fair evaluation. Code availability is not mentioned. The proof of theorem 3.2 was missing in the original submission and was only provided during the rebuttal. All reviewers gave rejecting scores, and I also recommend rejection. ",Paper Decision
c5TE1cG7fs,rkgqm0VKwB,End-to-end named entity recognition and relation extraction using pre-trained language models,Reject,"This paper presents an end-to-end technique for named entity recognition, that uses pre-trained models so as to avoid long training times, and evaluates it against several baselines. The paper was reviewed by three experts working in this area. R1 recommends Reject, giving the opinion that although the paper is well-written and results are good, they feel the technique itself has little novelty and that the main reason the technique works well is using BERT. R2 recommends Weak Reject based on similar reasoning, that the approach consists of existing components (albeit combined in a novel way) and suggest some ablation experiments to isolate the source of the good performance. R3 recommends Weak Accept but feels it is ""unsurprising"" that BERT allows for faster training and higher accuracy. In their response, authors emphasize that the application of pretraining to named entity recognition is new, and that theirs is a methodological advance, not purely a practical one (as R1 suggests and other reviews imply). They also argue it is not possible to do a fair ablation study that removes BERT, but make an attempt. The reviewers chose to keep their scores after the response. Given the split decision, the AC also read the paper. It is clear the paper has significant merit and significant practical value, as the reviews indicate. However, given that three expert reviewers -- all of whom are NLP researchers at top institutions -- feel that the contribution of the paper is weak (in the context of the expectations of ICLR) makes it not possible for us to recommend acceptance at this time. ",Paper Decision
ZevoAHi5Aw,Hklcm0VYDS,How noise affects the Hessian spectrum in overparameterized neural networks,Reject,"The study of the impact of the noise on the Hessian is interesting and I commend the authors for attacking this difficult problem. After the rebuttal and discussion, the reviewers had two concerns:
- The strength of the assumptions of the theorem
- Assuming the assumptions are reasonable, the conclusions to draw given the current weak link between Hessian and generalization.

I'm confident the authors will be able to address these issues for a later submission.",Paper Decision
S1royhTc6,rkgFXR4KPr,A Simple Recurrent Unit with Reduced Tensor Product Representations,Reject,"This paper has been reviewed by three reviewers and received scores such as 3/3/6. The reviewers took into account the rebuttal in their final verdict. The major criticism concerned the somewhat ad-hoc notion of interpretability, the analysis of vanishing/exploding gradients in  TPRU is experimental lacking theory. Finally,  all reviewers noted the paper is difficult to read and contains grammar issues etc. which does not help. On balance, we regret that this paper cannot be accepted to ICLR2020.

",Paper Decision
gQpucrwCor,BJeFQ0NtPS,Parallel Neural Text-to-Speech,Reject,"The paper proposed a non-autoregressive attention based encoder-decoder model for text-to-sepectrogram using attention distillation. It is shown to bring good speedup to conventional autoregressive ones. The paper further adopted VAE for the vocoder training which trains from scratch although performs worse than existing method (e.g. ClariNet). 

The main concerns for this paper come from the unclear presentation:
* As the reviewer pointed out, there're some misleading claims that the speedup gains was obtained without the consideration of the full context (i.e. not including the whole inference time).
* The paper failed to clear present the architectures developed/used in the paper and the differences from those used in the literature. The reviewers suggested the use of diagram to aid the presentation.
* The two contributions are unbalanced presented. Due to the complexities involved, it's better to explain things in more details. 
The authors acknowledged the reviewers comments during rebuttal, but did not make any changes to the paper.",Paper Decision
BWAo9wobN,B1gd7REFDB,Context-Aware Object Detection With Convolutional Neural Networks,Reject,"The paper proposes a contextual reasoning module following the approach proposed by the NIPS 2011 paper for object detection. Although the reviewers find the proposed approach reasonable, the experimental results are weak and noisy. Multiple reviewers believe that the paper will benefit from another review cycle, pointing out that the authors response confirmed that multiple additional (or redoing of) experiments are needed. 
",Paper Decision
eNIoRUovnm,HJeO7RNKPr,DeepV2D: Video to Depth with Differentiable Structure from Motion,Accept (Poster),"This work proposes a CNN architecture for joint depth and camera motion estimation from videos. The paper presents a differentiable formulation of the problem to allow its end-to-end learning, and the reviewers unanimously find the proposed approach reasonable and agree that this is a solid paper. Some of the reviewers find the method itself to be too mechanical, but they all agree that this is a well-engineered solution.",Paper Decision
5fNG1rKKzB,HJew70NYvH,TPO: TREE SEARCH POLICY OPTIMIZATION FOR CONTINUOUS ACTION SPACES,Reject,"The paper proposes a tree search based policy optimization methods for continuous action state spaces. The paper does not have a theoretical guarantee, but has empirical results.

Reviewers brought up issues such as lack of using other policy optimizations methods (SAC, RERPI, etc.), sample inefficiency, and unclear difference with some other similar papers. Even though the authors have provided a rebuttal to address these issues, all the reviewers remain negative. So I can only recommend rejection at this stage.",Paper Decision
ydF7lzeeCk,HkxwmRVtwH,Gaussian Process Meta-Representations Of Neural Networks,Reject,"The authors propose an approach to Bayesian deep learning, by representing neural network weights as latent variables mapped through a Kronecker factored Gaussian process. The ideas have merit and are well-motivated. Reviewers were primarily concerned by the experimental validation, and lack of discussion and comparisons with related work. After the rebuttal, reviewers still expressed concern regarding both points, with no reviewer championing the work.

One reviewer writes: ""I have read the authors' rebuttal. I still have reservation regarding the gain of a GP over an NN in my original review and I do not think the authors have addressed this very convincingly -- while I agree that in general, sparse GP can match the performance of GP with a sufficiently large number of inducing inputs, the proposed method also incurs extra approximations so arguing for the advantage of the proposed method in term of the accurate approximate inference of sparse GP seems problematic.""

Another reviewer points out that the comment in the author rebuttal about Kronecker factored methods (Saatci, 2011) for non-Gaussian likelihoods and with variational inference being an open question is not accurate: SV-DKL (https://arxiv.org/abs/1611.00336) and other approaches (http://proceedings.mlr.press/v37/flaxman15.pdf) were specifically designed to address this question, and are implemented in popular packages. Moreover, there is highly relevant additional work on latent variable representations for neural network weights, inducing priors on p(w) through p(z), which is not discussed or compared against (https://arxiv.org/abs/1811.07006, https://arxiv.org/abs/1907.07504). The revision only includes a minor consideration of DKL in the appendix. 

While the ideas in the paper are promising, and the generally thoughtful exchanges were appreciated, there is clearly related work that should be discussed in the main text, with appropriate comparisons. With reviewers expressing additional reservations after rebuttal, and the lack of a clear champion, the paper would benefit from significant revisions in these directions. 

Note: In the text, it says:
""However, obtaining p(w|D) and p(D) exactly is intractable when N is large or when the network is large and as such, approximation methods are often required.""
One cannot exactly obtain p(D), or the predictive distribution, regardless of N or the size of the network; exact inference is intractable because the relevant integrals cannot be expressed in closed form, since the parameters are mapped through non-linearities, in addition to typically non-Gaussian likelihoods.",Paper Decision
A3WVKXXuiD,H1eD7REtPr,CAN ALTQ LEARN FASTER: EXPERIMENTS AND THEORY,Reject,"The reviewers attempted to provide a fair assessment of this work, albeit with varying qualifications.  Nevertheless, the depth and significance of the technical contribution was unanimously questioned, and the experimental evaluation was not considered to be convincing by any of the assessors.  The criticisms are sufficient to ask the authors to further strengthen this work before it can be considered for a top conference.",Paper Decision
Sb86bpVR-C,r1g87C4KwB,The Break-Even Point on Optimization Trajectories of Deep Neural Networks,Accept (Spotlight),"This is an interesting study analyzing learning trajectories and their dependence on hyperparameters, important for better understanding of learning in deep neural networks.  All reviewers agree that the paper has a useful message to the ICLR community, and appreciate changes made by the authors in response to the initial reviews.",Paper Decision
PyfLO-MWu,SJxIm0VtwH,Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets,Accept (Poster),"This work proposes a new adaptive method for solving certain min-max problems.

The reviewers all appreciated the work and most of their concerns were addressed in the rebuttal. Given the current interest in both adaptive methods and min-max problems, this work is suited for publication at ICLR.",Paper Decision
Vtf6emHHVv,BygSXCNFDB,Exploration Based Language Learning for Text-Based Games,Reject,"The paper applies the Go-Explore algorithm to text-based games and shows that it is able to solve text-based game with better sample efficiency and generalization than some alternatives.  The Go-Explore algorithm is used to extract high reward trajectories that can be used to train a policy using a seq2seq model that maps observations to actions.

Paper received 1 weak accept and 2 weak rejects.  Initially the paper received three weak rejects, with the author response and revision convincing one reviewer to increase their score to a weak accept.

Overall, the authors liked the paper and thought that it was well-written with good experiments.
However, there is concern that the paper lacks technical novelty and would not be of interest to the broader ICLR community (beyond those that are interested in text-based games).  Another concern reviewers expressed was that the proposed method was only compared against baselines with simple exploration strategies and that baselines with more advanced exploration strategies should be included.

The AC agrees with above concerns and encourage the authors to improve their paper based on the reviewer feedback, and to consider resubmitting to a venue that is more focused on text-based games (perhaps an NLP conference).",Paper Decision
T98wSnilP,HJlSmC4FPS,Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks,Accept (Poster),"This paper focuses on studying neural network-based denoising methods. The paper makes the interesting observation that most existing denoising approaches have a tendency to overfit to knowledge of the noise level. The authors claim that simply removing the bias on the network parameters enables a variety of improvements in this regard and provide some theoretical justification for their results. The reviewers were mostly postive but raised some concerns about generalization beyond Gaussian noise and not ""being very well theoretically motivated"". These concerns seem to have at least partially been alleviated during the discussion period. I agree with the reviewers. I think the paper looks at an important phenomena for denoising (role of variance parameter) and is well suited to ICLR. I recommend acceptance. I suggest that the authors continue to further improve the paper based on the reviewers' comments.",Paper Decision
D7LHRKEVk3,S1lEX04tPr,CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning,Accept (Poster),"This paper was generally well received by reviewers and was rated as a weak accept by all.
The AC recommends acceptance.
",Paper Decision
7UYuZE32uQ,Skl4mRNYDr,"Deep Imitative Models for Flexible Inference, Planning, and Control",Accept (Poster),"This paper proposes to build an 'imitative model' to improve the performance for imitation learning. The main idea is to combine the model-based RL type of work to the imitation learning approach. The model is trained using a probabilistic method and can help the agent imitate goals that were previously not easy to achieve with previous works.

Reviewers 2 and 3 strongly agree that the paper should be accepted. R3 has increased their score after the rebuttal, and the authors' response helped in this case. Based on reviewers score, I recommend to accept this paper.",Paper Decision
GGbEO_asHy,r1eVX0EFvH,Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness,Reject,"The paper considers the relationship betwee:

- perturbations to an input x which change predictions of a model but not the ground truth label
- perturbations to an input x which do not change a model's prediction but do chance the ground truth label. 

The authors show that achieving robustness to the former need not guarantee robustness to the latter. 

While these ideas are interesting, the reviewers would like to see a tighter connection between the two forms of robustness developed. ",Paper Decision
d8vmM9LwzD,ryeQmCVYPS,Defective Convolutional Layers Learn Robust CNNs,Reject,"The reviewers wondered about the practical application of this method, given that the performance was lower.  The reviewers were also surprised by some of your claims and wanted you to explore them more deeply.  

On the positive side, the reviewers found your experiments to be very thorough.  You also performed additional experiments during the rebuttal period.  We hope that those experiments will help you to build a better paper as you work towards publishing this work.",Paper Decision
vskvz-0-Av,BkeMXR4KvS,DASGrad: Double Adaptive Stochastic Gradient,Reject,"The reviewers were confused by several elements of the paper, as mentioned in their reviews and, despite the authors' rebuttal, still have several areas of concerns.

I encourage you to read the reviews carefully and address the reviewers' concerns for a future submission.",Paper Decision
_SR8Jk5Rh,rygG7AEtvB,Finding Mixed Strategy Nash Equilibrium for Continuous Games through Deep Learning,Reject,"The paper presents an algorithm to compute mixed-strategy Nash equilibria for continuous action space games. While the paper has some novelty, reviewers are generally unimpressed with the assumptions made, and the quality of the writing. Reviewers were also not swayed by the responses from the authors. Additionally, it could be argued that the paper is somewhat peripheral to the topic of the conference.¨

On balance, I would recommend reject for now; the paper needs more work.",Paper Decision
Ehwwk01bkD,r1lZ7AEKvB,The Logical Expressiveness of Graph Neural Networks,Accept (Spotlight),The paper focuses on characterizing the expressiveness of graph neural networks. The reviewers were satisfied that the authors answered their questions suffciiently and uniformly agree that this is a strong paper that should be accepted.,Paper Decision
6Ta2M-tgB,rkg-mA4FDr,Pre-training Tasks for Embedding-based Large-scale Retrieval,Accept (Poster),"This paper conducts a comprehensive study on different retrieval algorithms and show that the two-tower Transformer models with properly designed pre-training tasks can largely improve over the widely used BM-25 algorithm. In fact, the deep learning based two tower retrieval model is already used in the IR field. The main contribution lies in the comprehensive experimental evaluation.

Blind Review #3 has a major misunderstanding of the paper; hence his review will be excluded. The other two reviewers tend to accept the paper with several minor comments.

As the authors promise to release the code as a baseline for further works, I agree to accept the paper.
",Paper Decision
V6x1Xb529f,rJggX0EKwS,The Benefits of Over-parameterization at Initialization in Deep ReLU Networks,Reject,"The article studies benefits of over-parametrization and theoretical properties at initialization in ReLU networks. The reviewers raised concerns about the work being very close to previous works and also about the validity of some assumptions and derivations. Nonetheless, some reviewers mentioned that the analysis might be a starting point in understanding other phenomena and made some suggestions. However, the authors did not provide a rebuttal nor a revision. ",Paper Decision
PI5wHzc5MN,Byekm0VtwS,A Training Scheme for the Uncertain Neuromorphic Computing Chips,Reject,"The paper is proposing uncertainty of the NN’s in the training process on analog-circuits based chips. As one reviewer emphasized, the paper addresses important and unique research problem to run NN on chips. Unfortunately, a few issues are raised by reviewers including presentation, novelly and experiments. This might be partially be mitigated by 1) writing motivation/intro in most lay person possible way 2) give easy contrast to normal NN (on computers) to emphasize the unique and interesting challenges in this setting. We encourage authors to take a few cycles of edition, and hope this paper to see the light soon.
",Paper Decision
4TpMS-im8P,rkly70EKDH,Mildly Overparametrized Neural Nets can Memorize Training Data Efficiently,Reject,"The paper studies the amount of over-parameterization needed for a quadratic 2 /3 layer neural network to memorize a separable training data set with arbitrary labels. While the reviewers agree that this paper contains interesting results, the review process uncovered highly related prior work, which requires a major revision to put the current paper into perspective and generally various clarifications. The paper will benefit from a revision and resubmission to another venue, and is in its current form not ready for acceptance at ICLR-2020.",Paper Decision
fRfSiaseoP,r1e0G04Kvr,Deep Graph Translation,Reject,"This paper studies a problem of graph translation, which aims at learning a graph translator to translate an input graph to a target graph using adversarial training framework. The reviewers think the problem is interesting. However, the paper needs to improve further in term of novelty and writing. ",Paper Decision
RsgOysWLDh,ByxRM0Ntvr,Are Transformers universal approximators of sequence-to-sequence functions?,Accept (Poster),"The paper provides a proof that Transformer networks (a popular deep learning model) are universal approximators for sequence-to-sequence functions. The theorem relies on the idea of contextual mappings (Definition 3.1), which models the attention layers. The results provide an important starting point for understanding a very widely used architecture.

As with many theoretical papers, the reviewers provided several suggestions as to which are important parts to be presented in the main paper. The authors were very responsive during the discussion period, updating the structure of the paper significantly. This shows nice evidence supporting the need for a long discussion period for ICLR. One reviewer upgraded their score (to 8), which is not reflected in the system.

This is an excellent paper, providing much needed theoretical analysis of a popular neural architecture. Clear accept.

",Paper Decision
EnRpKaETA,rkgAGAVKPr,Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples,Accept (Poster),"While the reviewers have some outstanding issues regarding the organization and clarity of the paper, the overall consensus is that the proposed evaluation methods is a useful improvement over current standards for meta-learning.",Paper Decision
V1tA0y4XRl,BJlaG0VFDH,Decoupling Weight Regularization from Batch Size for Model Compression,Reject,"This paper proposes to apply regularizers such as weight decay or weight noise only periodically, rather than every epoch. It investigates how the ""non-regularization period"", or period between regularization steps, interacts with other hyperparameters. 

Overall, the writing feels somewhat scattered, and it is hard to identify a clear argument for why the NRP should help. Certainly one could save computation this way, but regularizers like weight decay or weight noise incur only a small computational cost anyway. One explicit claim from the paper is that a higher NRP allows larger regularization. There's a sense in which this is demonstrated, though not a very interesting sense: Figure 4 shows that the weight decay strength should be adjusted proportionally to the NRP. But varying the parameters in this way simply results in an unbiased (but noisier) estimate of gradients of exactly the same regularization penalty, so I don't think there's much surprising here.

Similarly, Section 3 argues that a higher NRP allows for larger stochastic perturbations, which makes it easier to escape local optima. But this isn't demonstrated experimentally, nor does it seem obvious that stochasticity will help find a better local optimum.

Overall, I think this paper needs substantial cleanup before it's ready to be published at a venue such as ICLR.
",Paper Decision
Ucbi2sC76,r1g6MCEtwr,Zero-Shot Out-of-Distribution Detection with Feature Correlations,Reject,"The paper proposes a new scoring function for OOD detection based on calculating the total deviation of the pairwise feature correlations. This is an important problem that is of general interest in our community.

Reviewer 2 found the paper to be clear, provided a set of weaknesses relating to lack of explanations of performance and more careful ablations, along with a set of strategies to address them. Reviewer 1 recognized the importance of being useful for pretrained networks but also raised questions of explanation and theoretical motivations. Reviewer 3 was extremely supportive, used the authors' code to highlight the difference between far-from-distribution behaviour versus near-distribution OOD examples. The authors provided detailed responses to all points raised and provided additional eidence. There was  no convergence of the review recommendations.

The review added much more clarity to the paper and it is no a better paper. The paper demonstrates all the features of a good paper, but unfortunately didn't yet reach the level for acceptance for the next conference. ",Paper Decision
ESN0GM5NVn,rJehf0VKwS,Proactive Sequence Generator via Knowledge Acquisition,Reject,"This paper shows a nice idea to transfer knowledge from larger sequence models to small models. However, all the reivewers find that the contribution is too limited and the experiments are insufficient. All the reviewers agree to reject.",Paper Decision
534w7qBV3A,HJxiMAVtPH,Multi-scale Attributed Node Embedding,Reject,"This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work, and I urge the authors to continue to develop refinements and extensions.",Paper Decision
uXf-9R1UCV,S1xsG0VYvB,Understanding the functional and structural differences across excitatory and inhibitory neurons,Reject,"This paper explores the role of excitatory and inhibitory neurons, and how their properties might differ based on simulations.  A few issues were raised during the review period, and I commend the authors for stepping up to address these comments and run additional experiments.  It seems, though, that the reviewer's worries were born out in the results of the additional experiments: ""1. The object classification task is not really relevant to elicit the observed behavior and 2. Inhibitory neurons are not essential (at least when training with batch norm).""  I hope the authors can make improvements in light of these observations, and discuss their implications in a future version of this paper. ",Paper Decision
7_VANJ06hg,r1e9GCNKvH,One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation,Accept (Poster),"Based on current unanimous reviews, the paper is accepted.",Paper Decision
lXC0OY80j1,rJgqMRVYvr,Differentially Private Meta-Learning,Accept (Poster),"Thanks to the authors for the submission. This paper studies differentially private meta-learning, where the algorithm needs to use information across several learning tasks to protect the privacy of the data set from each task. The reviewers agree that this is a natural problem and the paper presents a solution that is essentially an adoption of differentially private SGD. There are several places the paper can improve. For the experimental evaluation, the authors should include a wider range of epsilon values in order to investigate the accuracy-privacy trade-off. The authors should also consider expanding the existing experiments with other datasets. ",Paper Decision
HjIt3MdOF,rJxYMCEFDr,Leveraging Adversarial Examples to Obtain Robust Second-Order Representations,Reject,The authors propose a method to train a neural network that is robust to visual distortions of the input image. The reviewers agree that the paper lacks justification of the proposed method and experimental evidence of its performance.,Paper Decision
6O_zaNzS4F,HkxYzANYDB,CLEVRER: Collision Events for Video Representation and Reasoning,Accept (Spotlight),The reviewers are unanimous in their opinion that this paper offers a novel approach to causal learning.  I concur.,Paper Decision
N6tgYPin8_,rJeuMREKwS,Using Logical Specifications of Objectives in Multi-Objective Reinforcement Learning,Reject,"The reviewers generally agreed that the technical novelty of the work was limited, and the experimental evaluation was insufficient to make up for this, evaluating the method only on relatively simple toy tasks. As much, I do not think that the paper is ready for publication at this time.",Paper Decision
z6UJ8Id5cX,Hke_f0EYPH,Efficient Training of Robust and Verifiable Neural Networks,Reject,"This paper studies the problem of certified robustness to adversarial examples. It first demonstrates that many existing certified defenses can be viewed under a unified framework of regularization. Then, it proposes a new double margin-based regularizer to obtain better certified robustness.  Overall, it has major technical issues and the rebuttal is not satisfying.",Paper Decision
6uPQy0dT9b,H1ldzA4tPr,Learning Compositional Koopman Operators for Model-Based Control,Accept (Spotlight),"This paper proposes using object-centered graph neural network embeddings of a dynamical system as approximate Koopman embeddings, and then learning the linear transition matrix to model the dynamics of the system according to the Koopman operator theory. The authors propose adding an inductive bias (a block diagonal structure of the transition matrix with shared components) to limit the number of parameters necessary to learn, which improves the computational efficiency and generalisation of the proposed approach. The authors also propose adding an additional input component that allows for external control of the dynamics of the system. The reviewers initially had concerns about the experimental section, since the approach was only tested on toy domains. The reviewers also asked for more baselines. The authors were able to answer some of the questions raised during the discussion period, and by the end of it all reviewers agreed that this is a solid and novel piece of work that deserves to be accepted. For this reason I recommend acceptance.",Paper Decision
HAF3Z91Edq,SJgwzCEKwH,Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness,Accept (Poster),"This paper investigates improving robustness to adversarial examples by using mode connectivity in the loss function. The paper received three reviews by experts working in related areas. In a strongly positive review, R1 recommends Accept, but gives some specific technical questions. The authors submitted a response to these questions; in post-review comments, R1 was satisfied and maintained the highly positive review. R2 recommended Weak Reject and also asked specific technical questions, including some additional details on experiments, statistical significance, etc. The author response also convincingly responded to these concerns. R3 recommended Weak Accept but suggested improving the writing, which authors have done in their revision. Given that R1 and R3 are highly positive and R2's concerns were addressed in the response and revision, we now recommend (weak) Accept.",Paper Decision
7rCLpcGzG,SJgwf04KPr,Confidence-Calibrated Adversarial Training: Towards Robust Models Generalizing Beyond the Attack Used During Training,Reject,"This paper proposes a confidence-calibrated adversarial training (CCAT). The key idea is to enforce that the confidence on adversarial examples decays with their distance to the attacked examples. The authors show that CCAT can achieve better natural accuracy and robustness. After the author response and reviewer discussion, all the reviewers still think more work (e.g., improving the motivation to better position this work, conducting a fair comparison with adversarial training which does not have adversarial example detection component) needs to be done to make it a strong case. Therefore, I recommend reject.",Paper Decision
sZViNxICvy,rkxUfANKwB,All SMILES Variational Autoencoder for Molecular Property Prediction and Optimization,Reject,"The paper proposes All SMILES VAE which can capture the chemical properties of small molecules and also optimize the structures of these molecules. The model achieves significantly performance improvement over existing methods on the Zinc250K and Tox21 datasets. 

Overall it is a very solid paper - it addresses an important problem, provides detailed description of the proposed method and shows promising experiment results. The work could be a landmark piece, leading to major impacts in the field. However, given its potential,  the paper could benefit from major revisions of the draft. Below are some suggestions on improving the work:
1. The current version contains a lot of materials. It tries to strike the balance between machine learning methodology and details of the application domain. But the reality is that the lack of architecture details and some sloppy definitions of ML terms make it hard for readers to fully appreciate the methodology novelty. 

2. There is still room for improvement in experiments. As suggested in the review, more datasets should be used to evaluate the proposed model. Since it is hard to provide theoretic analysis of the proposed model,  extensive experiments should be provided. 

3. The complexity analysis is not fully convincing. Some fair comparison with the alternative approaches should be provided. 

In summary, it is a paper with big potentials. The current version is a step away from being ready for publication. We hope the reviews can help improve the paper for a strong publication in the future. ",Paper Decision
kRrWnvUrJ,SyeUMRNYDr,Generating Dialogue Responses From A Semantic Latent Space,Reject,"This paper proposes a response generation approach that aims to tackle the generic response problem. The approach is learning a latent semantic space by maximizing the correlation between features extracted from prompts and responses. The reviewers were concerned about the lack of comparison with previous papers tackling the same problem, and did not change their decision (i.e., were not convinced) even after the rebuttal. Hence, I suggest a reject for this paper.",Paper Decision
myzUxy6lvs,ryxUMREYPr,Is There Mode Collapse? A Case Study on Face Generation and Its Black-box Calibration,Reject,"This paper studies the problem of mode collapse in GANs. The authors present new metrics to judge the model's diversity of the generated faces. The authors present two black-box approaches to increasing the model diversity. The benefit of using a black box approach is that the method does not require access to the weights of the model and hence it is more easily usable than white-box approaches. However, there are significant evaluation problems and lack of theoretical and empirical motivation on why the methods proposed by the paper are good. The reviewers have not changed their score after having read the response and there is still some gaps in evaluation which can be improved in the paper. Thus, I'm recommending a Rejection.",Paper Decision
VF_pFYuiMj,SJeNz04tDS,Overlearning Reveals Sensitive Attributes,Accept (Poster),"This paper introduces the problem of overlearning, which can be thought of as unintended transfer learning from a (victim) source model to a target task that the source model’s creator had not intended its model to be used for. The paper raises good points about privacy legislation limitations due to the fact that overlearning makes it impossible to foresee future uses of a given dataset.

Please incorporate the revisions suggested in the reviews to add clarity to the overlearning versus censoring confusion addressed by the reviewers.",Paper Decision
nWgcJS8p_Q,H1g4M0EtPS,Gaussian MRF Covariance Modeling for Efficient Black-Box Adversarial Attacks,Reject,"This paper presents a Markov Random Fields (MRF) for generating adversarial examples in a black-box setting, where only it has access to loss function evaluations. The method exploits the structure of input data to model the covariance structure of the gradients. Empirically, the resulting method uses fewer queries than the current state of the art to achieve comparable performance. Overall, the paper has valuable contributions. The main issue is on empirical evaluation, which can be strengthened, e.g., by including results with multi-step methods and more thorough analysis of the estimated gradients.",Paper Decision
zIrgeChvN4,Bke7MANKvS,A Kolmogorov Complexity Approach to Generalization in Deep Learning,Reject,"This is an interesting paper that aims to redefine generalization based on the difference between the training error and the inference error (measured on the empirical sample set), rather than the test error. The authors propose to improve generalization in image classification by augmenting the input with encodings of the image using a source code, and learn this encoding using the compression distance, an approximation of the Kolmogorov complexity. They show that training in this fashion leads to performance that is more robust to corruption and adversarial perturbations that exist in the empirical sample set. 

Reviewers agree on the importance of this topic and the novelty of the approach, but there continue to exist sharp disagreement in the ratings. Most have concerns about the formalism and clarity in the presentation. Especially given that the paper is 10 pages, it should be evaluated against a more rigorous standard, which doesn't appear to be met. I encourage the authors to consider a rewrite with a goal towards clarity for a more general ML audience and resubmit for a future conference.
",Paper Decision
GEsHWZ-9cW,B1lXfA4Ywr,Towards Modular Algorithm Induction,Reject,"The reviewers all agreed that although there is a sensible idea here, the method and presentation need a lot of work, especially their treatment of related methods.",Paper Decision
sH9vCYg_1,BkgzMCVtPB,Optimal Strategies Against Generative Attacks,Accept (Talk),"This paper concerns the problem of defending against generative ""attacks"": that is, falsification of data for malicious purposes through the use of synthesized data based on ""leaked"" samples of real data. The paper casts the problem formally and assesses the problem of authentication in terms of the sample complexity at test time and the sample budget of the attacker. The authors prove a Nash equillibrium exists, derive a closed form for the special case of multivariate Gaussian data, and propose an algorithm called GAN in the Middle leveraging the developed principles, showing an implementation to perform better than authentication baselines and suggesting other applications.

Reviewers were overall very positive, in agreement that the problem addressed is important and the contribution made is significant. Most criticisms were superficial. This is a dense piece of work, and presentation could still be improved. However this is clearly a significant piece of work addressing a problem of increasing importance, and is worthy of acceptance.",Paper Decision
2NjCDZRM7o,H1eWGREFvB,Stein Self-Repulsive Dynamics: Benefits from Past Samples,Reject,"This paper proposes a new sampling mechanism which uses a self-repulsive term to increase the diversity of the samples.

The reviewers had concerns, most of which were addressed in the rebuttal. Unfortunately, none of the reviewers genuinely championed the paper. Since there were a lot of good submissions this year, we had to make decisions on the borderline papers and this lack of full support means that this submission will be rejected.

I highly encourage you to keep updating the manuscript and to rebusmit it to a later conference.",Paper Decision
fJjrUOyUG,ryebG04YvB,Adversarially robust transfer learning,Accept (Poster),"This paper presents an empirical study towards understanding the transferability of robustness (of a deep model against adversarial examples) in the process of transfer learning across different tasks.

The paper received divergent reviews, and an in-depth discussion was raised among the reviewers.

+ Reviewers generally agree that the paper makes an interesting study to the robust ML community. The paper provides a nice exploration of the hypothesis that robust models learn robust intermediate representations, and leverages this insight to help in transferring robustness without adversarial training on every new target domain. 

- Reviewers also have concerns that, as an experimental paper, it should perform a larger study on different datasets and transfer problems to eliminate the bias to specific tasks, and explore the behavior when the task relatedness increases or decreases.

AC agrees with the reviewers and encourages the authors to incorporate these constructive suggestions in the revision, in particular, explore more tasks with different task relatedness.

I recommend acceptance, assuming the comments will be fully addressed.",Paper Decision
AVnirYQLEA,S1glGANtDr,Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation,Accept (Spotlight),"The paper proposes a doubly robust off-policy evaluation method that uses both stationary density ratio as well as a learned value function in order to reduce bias.
The reviewers unanimously recommend acceptance of this paper.",Paper Decision
_B8vEgJA0h,BkggGREKvS,Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning,Reject,"After reading the reviews and discussing this paper with the reviewers, I believe that this paper is not quite ready for publication at this time. While there was some enthusiasm from the reviewers about the paper, there were also major concerns raised about the comparisons and experimental evaluation, as well as some concerns about novelty. The major concerns about experimental evaluation center around the experiments being restricted to continuous action settings where there is a limited set of baselines (see R3). While I see the authors' point that the method is not restricted to this setting, showing more experiments with more baselines would be important: the demonstrated experiments do strike me as somewhat simplistic, and the standardized comparisons are limited.

This might not by itself be that large of an issue, if it wasn't for the other problem: the contribution strikes me as somewhat ad-hoc. While I can see the intuition behind why these two auxiliary objectives might work well, since there is only intuition, then the burden in terms of showing that this is a good idea falls entirely on the experiments. And this is where in my opinion the work comes up short: if we are going to judge the efficacy of the method entirely on the experimental evaluation without any theoretical motivation, then the experimental evaluation does not seem to me to be sufficient.

This issue could be addressed either with more extensive and complete experiments and comparisons, or a more convincing conceptual or theoretical argument explaining why we should expect these two particular auxiliary objectives to make a big difference.",Paper Decision
S208Bl_r13,HkeJzANFwS,Contextual Text Style Transfer,Reject,"The paper proposes a new style transfer task, contextual style transfer, which hypothesises that the document context of the sentence is important, as opposed to previous work which only looked at sentence context. A major contribution of the paper is the creation of two new crowd-sourced datasets, Enron-Context and Reddit-Context focussed on formality and offensiveness. The reviewers are skeptical that it was context that has really improved results on the style transfer tasks. The authors responded to all the reviewers but there was no further discussion. I feel like this paper has not convinced me or the reviewers of the strength of its contribution and, although interesting, I recommend for it to be rejected. ",Paper Decision
uhQlKYXk3B,SylR-CEKDS,Modeling question asking using neural program generation,Reject,"The authors explore different ways to generate questions about the current state of a “Battleship” game. Overall the reviewers feel that the problem setting is interesting, and the program generation part is also interesting. However, the proposed approach is evaluated in tangential tasks rather than learning to generate question to achieve the goal. Improving this part is essential to improve the quality of the work. 

",Paper Decision
KpX5uMudwp,S1eRbANtDB,Learning to Link,Accept (Poster),All reviewers come to agreement that this is a solid paper worth publishing at ICLR; the authors are encouraged to incorporate additional comments suggested by reviewers.,Paper Decision
bvpCNULqJ,SJlRWC4FDB,Adversarial Attacks on Copyright Detection Systems,Reject,"This paper shows a case study of an adversarial attack on a copyright detection system. The paper implements a music identification method with a simple convolutional neural network, and shows that it is possible to fool such CNN with an adversarial learning. After the discussion period, two among three reviewers incline to the rejection of the paper. Although the majority of the reviewers agree that this is an interesting problem with an important application, they also find many of their concerns remain unaddressed. These include the generality of the finding as the current paper is more like a proof-of-concept that black/white-box attack can work for copyright system. The reviewers are also concerned that the technique solution/finding is not novel as it is very similar to prior work in other domains (e.g., image classification). One reviewer was particularly concerned about that the user study is missing, making it difficult to judge whether the quality of the modified audio is reasonable or not.",Paper Decision
kfhAyorNoi,BJl6bANtwH,Detecting Extrapolation with Local Ensembles,Accept (Poster),"This paper presents an ensembling approach to detect underdetermination for extrapolating to test points. The problem domain is interesting and the approach is simple and useful. While reviewers were positive about the work, they raised several points for improvement. The authors are strongly encouraged to include the discussion here in the final version.",Paper Decision
6ygjR6QOg,B1lnbRNtwr,Global Relational Models of Source Code,Accept (Poster),"The paper investigates hybrid NN architectures to represent programs, involving both local (RNN, Transformer) and global (Gated Graph NN) structures, with the goal of exploiting the program structure while permitting the fast flow of information through the whole program.

The proof of concept for the quality of the representation is the performance on the VarMisuse task (identifying where a variable was replaced by another one, and which variable was the correct one). Other criteria regard the computational cost of training and number of parameters.

Varied architectures, involving fast and local transmission with and without attention mechanisms, are investigated, comparing full graphs and compressed (leaves-only) graphs. The lessons learned concern the trade-off between the architecture of the model, the computational time and the learning curve. It is suggested that the Transformer learns from scratch to connect the tokens as appropriate; and that interleaving RNN and GNN allows for more effective processing, with less message passes and less parameters with improved accuracy.

A first issue raised by the reviewers concerns the computational time (ca 100 hours on P100 GPUs); the authors focus on the performance gain w.r.t. GGNN in terms of computational time (significant) and in terms of epochs. Another concern raised by the reviewers is the moderate originality of the proposed architecture. I strongly recommend that the authors make their architecture public; this is imo the best way to evidence the originality of the proposed solution. 

The authors did a good job in answering the other concerns, in particular concerning the computational time and the choice of the samples. I thus recommend acceptance. ",Paper Decision
knvcLk5Usn,rkx3-04FwB,MONET: Debiasing Graph Embeddings via the Metadata-Orthogonal Training Unit,Reject,"This work presents a method for debiasing graph embeddings. The main concerns for the work were originally identified by Reviewer 3, who pointed out that the method is only capable of linear debiasing. Authors responded by updating the manuscript in several places to mention this limitation as well as adding Table 3 to the Appendix showing that SVM's with non-linear kernels are still able to identify bias in the embeddings. Reviewers agreed that this addition improved the manuscript, however some reviewers still had concerns about the revised manuscript. This AC has several recommendations for improving the paper. First additional revision is needed to better address the limitations of linear debiasing, for example Table 1 still reads ""MONET is successful in removing all metadata information from the topology embeddings – the links in the graph are no longer an effective predictor of political party"".  Statements like this are a bit misleading, as the embeddings will still be biased with respect to a non-linear classifiers (as evident by Table 3). Additionally, updating Table 1 and related experiments to measure embedding bias with respect to non-linear classifiers would help clarify the limitations for perspective readers. Second, the paper should be updated to address remaining concerns that the linear debiasing assumption limits the applicability of the method. One could either discuss or demonstrate additional applications of the method that work even with the linear assumption, extend MONET so it can improve model bias with respect to non-linear classifiers, or show that MONET still outperforms baselines when the non-linear assumption is violated.",Paper Decision
UWK--eoa4b,HJg2b0VYDr,Selection via Proxy: Efficient Data Selection for Deep Learning,Accept (Poster),"This paper proposes to perform sample selection for deep learning - which can be very computationally expensive - using a smaller and simpler proxy network. The paper shows that such proxies are faster to train and do not substantially harm the accuracy of the final network.

The reviewers were all in agreement that the problem is important, and that the paper is comprehensive and well executed. I therefore recommend it should be accepted.",Paper Decision
yoFT62NzZl,ryesZANKPB,Meta Learning via Learned Loss,Reject,"Despite the new ideas in this paper, reviewers feel that it needs to be revised for clarification, and that experimental results are not convincing.  I have down-weighted the criticisms of Reviewer 2 because I agree with the authors' rebuttal.  However, there is still not enough support among the remaining reviews to justify acceptance. ",Paper Decision
4qqiCFUOgu,Byg5ZANtvH,Short and Sparse Deconvolution --- A Geometric Approach,Accept (Poster),"The work considers sparse and short blind deconvolution problem, which is to inverse a convolution of a sparse source (such as spikes at cell locations in microscopy) with a short (of limited spatial size) kernel or point spread function, not known in advance. This is posed as a bilinear lasso optimization problem. The work applies a non-linear optimization method with some practical improvements (such as data-driven initialization, momentum, homotopy continuation).

The paper extends the work by Kuo et al. (2019) by providing a practical algorithm for solving those inverse problems. A focus of the paper is to solve the bilinear lasso instead of the approximate bilinear lasso, because this approximation is poor for coherent problems. Having read the rebuttal and the paper, I believe the authors addressed the issues raised by Reviewer #2 in a sufficient way.

small things:
- it would be good to define $\iota$ (zero-padding operator) in (1)
- it would be good to define $p, p_0$ just below (3). They seem to be appearing out of the blue without any direct relation to anything mentioned prior in section 2.
- it would be good to cite some older/historic references for various optimization methods , e.g. [1] below. 


[1] Richter & deCarlo 
Continuation methods: Theory and applications
IEEE Transactions on Systems, Man, and Cybernetics, 1983
https://ieeexplore.ieee.org/abstract/document/6313131",Paper Decision
gMrg3-TgNr,SkxcZCNKDS,"If MaxEnt RL is the Answer, What is the Question?",Reject,"This paper studies maximum entropy reinforcement learning in more detail. Maximum entropy is a popular strategy in modern RL methods and also seems used in human and animal decision making. However, it does not lead to optimize expected utility. The authors propose a setting in which maximum entropy RL is an optimal solution. 

The authors were quite split on the paper, and there has been an animated discussion between the reviewers among each other and with the authors. 

The technical quality is good, although one reviewer commented on the restricted setting of the experiments (bandit problems). The authors have addressed this by adding an additional experiment. Futhermore, two reviewers commented that the clarity of the paper could be improved. 

A larger part of the discussion (also the private discussion) revolved around relevance and significance, especially of the meta-pomdp setting that takes up a large part of the manuscript. 
- A reviewer mentioned that after reading the paper, it does not become more clear why maximum entropy RL works well in practice. The discussion even turned to why MaxEntropyRL might be *unreasonable* from the point of view of needing a meta-POMDP with Markov assumptions, which doesn't help shed light on its empirical success. The meta-POMDP setting does not seem to reflect the use cases where maximum entropy RL has done well in emperical studies. 
- Another reviewer mentioned that earlier papers have investigated maximum entropy RL, and that the paper tries to offer a new perspective with the Meta-POMDP setting. The discussion of this discussion was not deemed complete in current state and needs more attention (splitting the paper into two along these lines is a possibility mooted by two of the reviewers). A particular example was the doctor-patient example, where in the meta-POMDP setting the doctor would repeatedly attempt to cure a fixed sampled illness, rather than e.g. solving for a new illness each time. 

Based on the discussion, I would conclude that the topic broached by the paper is very relevant and timely, however, that the paper would benefit from a round of major revision and resubmission rather than being accepted to ICLR in current form. ",Paper Decision
FetHMoUGn,rygFWAEFwS,Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well,Accept (Poster),"The authors proposed a simple and effective approach to parallel training based on stochastic weight averaging. Moreover, the authors have carefully addressed the reviewer comments in the discussion period, particularly the relation to local SGD, to the satisfaction of reviewers. Local SGD mimics sequential SGD with noise induced by lack of synchronization, whereas SWAP averages multiple samples from a stationary distribution, and synchronizes at the end. Please clarify these points and carefully account for reviewer comments in the final version. Overall, the proposed approach will make an excellent addition to the program, both elegant and practically useful.",Paper Decision
RV8RTElbzG,SJxFWRVKDr,Characterizing Missing Information in Deep Networks Using Backpropagated Gradients,Reject,"This paper proposes the use of gradient of the loss evaluated at the example with respect to the model parameters as the feature representation of that example. The authors performed an empirical analysis on anomaly detection benchmarks to demonstrate the practical benefits of the proposed method. While the reviewers find the idea interesting, the consensus is that the proposed method lacks justification, and that the main claims were not substantiated. While the reviewers proposed several key points of improvement, the raised issues were not addressed in the rebuttal. I will hence recommend rejection of this paper.
",Paper Decision
xLOiH_IJi,Hkl_bCVKDr,Scaleable input gradient regularization for adversarial robustness,Reject,"(1) the authors emphasize the theoretical contribution and claims the bound are tighter. However, they did not directly compare with any certified robust methods, or previous bounds to support the argument. 
HM, not sure, need to check this

(2) The empirical results look suboptimal. The authors did not convince me why they sampled 1000 images for test for a small CIFAR-10 dataset. The proposed method is 10% less robust comparing to Madry's in table 1.  
Seems ok, understand authors response

1) The theoretical analysis are not terribly new, which is just a straightforward application of first-order Taylor expansion. This idea could be traced back to the very first paper on adversarial examples FGSM (Goodfellow et al 2014).
True

2) The novelty of the paper is to replace exact gradient (w.r.t input) by their finite difference and use it as a regularization. However, there is a misalignment between the theory and the proposed algorithm. The theory only encourages input gradient regularization, regardless to how it is evaluated, and previous studies have shown that this is not a very effective way to improve robustness. According to the experiments, the main empirical improvement comes from the finite difference implementation but the benefit of finite difference is not justified/discussed by the theory. Therefore, the empirical improvement are not supported by the theory. Authors have briefly respond to this issue in the discussion but I believe a more rigorous analysis is needed.  
This seems okay based on author response

3) Moreover, the empirical performance does not achieve state-of-the-art result. Indeed, there is a non-negligible  gap (12%) between the obtained performance and some well-known baseline. Thus the empirical contribution is also limited. 
Yea, for some cases",Paper Decision
oT72q94FQy,HJe_Z04Yvr,Adjustable Real-time Style Transfer,Accept (Poster),"This paper offers an innovative approach to adjusting style transfer parameters.
The reviewers were consistent, and all recommend acceptance.  I concur. 
",Paper Decision
mygBatm-N5,Skxw-REFwS,Unsupervised Progressive Learning and the STAM Architecture,Reject,"


The paper presents a semi-supervised data streaming approach. The proposed architecture is made of a layer-wise k-means structure (more specifically a epsilon-means approach, where the epsilon is adaptively defined from the distortion percentile). Each layer is associated a scope (patch dimensions); each patch of the image is associated its nearest cluster center (or a new cluster is created if needed); new cluster centers are adjusted to fit the examples (Short Term Memory); clusters that have been visited sufficiently many time are frozen (Long Term Memory). Each cluster is associated a label distribution from the labelled examples. The label for each new image is obtained by a vote of the clusters and layers.

Some reviews raise some issues about the robustness of the approach, and its sensitivity w.r.t. hyper-parameters. Some claims (""the distribution associated to a class may change with time"") are not experimentally confirmed; it seems that in such a case, the LTM size might grow along time; a forgetting mechanism would then be needed to enforce the tractability of classification. 

Some claims (the mechanism is related to how animal learn) are debatable, as noted by Rev#1; see hippocampal replay.

The area chair thinks that a main issue with the paper is that the Unsupervised Progressive Learning is considered to be a new setting (""none of the existing approaches in the literature are directly applicable to the UPL problem""), preventing the authors from comparing their results with baselines.

However, after a short bibliographic search, some related approaches exist under another name:
* Incremental Semi-supervised Learning on Streaming Data, Pattern Recognition 88, Li et al., 2018;
* Incremental Semi-Supervised Learning from Streams for Object Classification, Chiotellis et al., 2018;
* Online data stream classification with incremental semi-supervised learning, Loo et al., 2015.

The above approaches seem able to at least accommodate the Uniform UPL scenario. I therefore encourage the authors to consider some of the above as baselines and provide a comparative validation of STAM.",Paper Decision
7S5DU03Dhd,HyxwZRNtDr,Wasserstein Robust Reinforcement Learning,Reject,"While the reviewers agreed that the problem of learning robust policies is an important one, there were a number of major concerns raised about the paper, and as a result I would recommend that the paper not be accepted at this time. The important points are: (1) limited novelty in light of prior work in this area (see R2 and R3); (2) a number of missing comparisons (see R2). There is also a bit of confusion in the reviews, which I think stems from a somewhat unclear statement in the paper of the problem formulation. While there is nothing wrong with assuming access to a parameterized simulator and studying robustness under parametric variation, this is of course a much stronger assumption than some prior work on robust reinforcement learning. Clarity on this point is crucial, and there are a large number of prior methods that can likely do well in this setting (e.g., based on system ID, etc.).",Paper Decision
6mhmO5c9p8,ryxIZR4tvS,Knowledge Hypergraphs: Prediction Beyond Binary Relations,Reject,"The paper proposes two methods for link prediction in knowledge hypergraphs. The first method concatenates the embedding of all entities and relations in a hyperedge. The second method combines an entity embedding, a relation embedding, and a weighted convolution of positions. The authors demonstrate on two datasets (derived by the authors from Freebase), that the proposed methods work well compared to baselines. The paper proposes direct generalizations of knowledge graph approaches, and unfortunately does not yet provide a comprehensive coverage of the possible design space of the two proposed extensions.

The authors should be commended for providing the source code for reproducibility. One of the reviewers (who was unfortunately also the most negative), was time pressed. Unfortunately, the discussion period was not used by the reviewers to respond to the authors' rebuttal of their concerns.

Even discounting the most negative review, this paper is on the borderline, and given the large number of submissions to ICLR, it unfortunately falls below the acceptance threshold in its current form.


",Paper Decision
SXIN6VrCtY,HJgLZR4KvH,Dynamics-Aware Unsupervised Discovery of Skills,Accept (Talk),"This is a very interesting paper on unsupervised skill learning based on the predictability of skill effects, with the incorporation of these ideas into model-based RL.

This is a clear accept, based on the clarity of the ideas presented and the writing, as well as the thorough and convincing experiments.",Paper Decision
CJ3LZwBBN,HJlU-AVtvS,A Fine-Grained Spectral Perspective on Neural Networks,Reject,"The authors develop a spectral analysis on the boolean cube for the neural ""conjugate kernel"" (CK) and ""tangent kernel"" (NTK). The analysis sheds light into inductive biases of neural networks, such as whether they are biased to simple functions.  

This work contains rigorous analysis and theory which is useful for further discussions. However, the theory and insights do not feel complete. One important drawback is that the analysis is limited by the boolean cube setting; this also means that it is more difficult to link theory to practical scenarios. This has been discussed a lot during the rebuttal and among reviewers. Empirical validation has attempted to deal with these concerns, but it would be useful to have this validation coming from theory, or at least have further relevant theoretical insights. This could happen by further building on the theorem provided in the rebuttal for eigenvalue behavior when d is large.",Paper Decision
9_LKMSAY9B,BygSZAVKvr,Energy-Aware Neural Architecture Optimization with Fast Splitting Steepest Descent,Reject,"This paper extends previous work on searching for good neural architectures by iteratively growing a network, including energy-aware metrics during the process. There was discussion about the extent of the novelty of this work and how well it was evaluated, and in the end the reviewers felt it was not quite ready for publicaiton.",Paper Decision
suzfRykvw,HkgrZ0EYwB,Unpaired Point Cloud Completion on Real Scans using Adversarial Training,Accept (Poster),"This paper presents an unsupervised method for completing point clouds obtained from real 3D scans based on GAN. Generally, the paper is well-organized, and its contributions and experimental supports are clearly presented, from which all reviewers got positive impressions.
Although the technical contribution of the method seems marginal as it is essentially a combination of established methods, it well fits in a novel and practical application scenario, and its useful is convincingly demonstrated in intensive experiments. We conclude that the paper provides favorable insights covering the weakness in technical novelty, so I’d like to recommend acceptance. 
",Paper Decision
oDVIYe8W6M,HJxV-ANKDH,Efficient Riemannian Optimization on the Stiefel Manifold via the Cayley Transform,Accept (Poster),"This paper presents a method for optimizing parameter matrices of deep learning objectives while enforcing orthonormality constraints.  While advantageous in certain respects, such constraints can be expensive to maintain when using existing methods.  To address this issue, an new algorithm is proposed based on the Cayley Transform and analyzed in terms of convergence.  After the discussion period two reviewers supported acceptance while one still voted for rejection.  Consequently, in recommending acceptance here for a poster, it is worth examining the significance of unresolved concerns.

First, the reject reviewer raised the valid point that the convergence proof relies on the assumption of Lipschitz continuous gradients, and yet the experiments use ReLU activation functions that do not satisfy this criteria.  In my view though, it is sometimes reasonable to derive useful theory under the assumption of Lipschitz continuous derivatives that nonetheless provides insight into the case where these derivatives may not be Lipschitz on a set of measure zero (which would be the case with ReLU activations).  So while ideally it might be nice to extend the theory to remove this assumption, the algorithm seems to work fine with ReLU activations in practice.  And this seems reasonable given the improbability of any iterate exactly hitting the measure-zero points where the gradients are discontinuous.  Beyond this issue, some criticisms were mentioned in terms of how and where the timing comparisons were presented.  However, I believe that these issues can be easily remedied in a final revision.",Paper Decision
gNhvfe-LjV,H1lNb0NtPH,DIME: AN INFORMATION-THEORETIC DIFFICULTY MEASURE FOR AI DATASETS,Reject,"This paper proposes a measure of inherent difficulty of datasets. While reviewers agree that there are good ideas in this paper that is worth pursuing, several concerns has been risen by reviewers, which are mostly acknowledged by the authors. We look forward to seeing an improved version of this paper soon!
",Paper Decision
Eyl9TpHg8T,SyxXWC4KPB,Structured consistency loss for semi-supervised semantic segmentation,Reject,This submission proposes to combine the CutMix data augmentation of Yun et al 2019 with the standard consistency loss of  and the  structured consistency loss of Liu et al 2019 and applies the resulting approach to the Cityscapes dataset.  The reviewers were unanimous that the paper is not suitable for publication at ICLR due to a lack of novelty in the method.  No rebuttal was provided.,Paper Decision
vhIa3h0bri,Bkl7bREtDr,AMRL: Aggregated Memory For Reinforcement Learning,Accept (Poster),"This paper introduces a way to augment memory in recurrent neural networks with order-independent aggregators. In noisy environments this results in an increase in training speed and stability. The reviewers considered this to be a strong paper with potential for impact, and were satisfied with the author response to their questions and concerns.",Paper Decision
mt64CPWVf,B1gXWCVtvr,Adapting Behaviour for Learning Progress,Reject,"The paper introduces a non-stationary bandit strategy for adapting the exploration rate in Deep RL algorithms. They consider exploration algorithms with a tunable parameter (e.g. the epsilon probability in epsilon-greedy) and attempt to adjust this parameter in an online fashion using a proxy to the learning progress. The proposed approach is empirically compared with using fixed exploration parameters and adjusting the parameter using a bandit strategy that doesn't model the learning process.

Unfortunately, the proposed approach is not theoretically grounded and the experiments lack comparison with good baselines in order to be convincing. A comparison with other, provably efficient, non-stationary bandit algorithms such as exponential weight methods (Besbes et al 2014) or Thompson sampling (Raj & Kalyani 2017), which are cited in the paper, is missing. Moreover, given the whole set of results and how they are presented, the improvement due to the proposed method is not clear. In light of these concerns I recommend to reject this paper.",Paper Decision
VRWzAXKCba,BkgMbCVFvr,Pretraining boosts out-of-domain robustness for pose estimation,Reject,"The paper presents a new dataset, containing around 8k pictures of 30 horses in different poses. This is used to study the benefits of pretraining for in- and out-of-domain images.

The paper is somewhat lacking in novelty. Others have studied the same type of pre-training in the past using other datasets, which makes the dataset the main novelty. But reviewers raised many questions about the dataset, in particular about how many of the frames of the same horse might be similar, and of how few horses there are; few enough to potentially not make the results statistically meaningful. The authors replied to these questions more by appealing to standards in other fields than by explaining why this is a good choice. Apart from these crucial weaknesses, however, the research appears good.

This is a pretty clear reject based on lack of novelty and oddities with the dataset.",Paper Decision
B-1LS68jK,SkxG-CVFDH,GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning,Reject,"The authors integrate an interpolation based regularization to develop a graph neural network for semi-supervised learning. While reviewers enjoyed the paper, and the authors have provided a thoughtful response, there were remaining questions about clarity of presentation and novelty remaining after the rebuttal period. The authors are encouraged to continue with this work, accounting for reviewer comments in future revisions.",Paper Decision
FGKfaRpltq,Syx-bCEFPS,Synthetic vs Real: Deep Learning on Controlled Noise,Reject,"Thanks for your detailed feedback to the reviewers, which helped us a lot to better understand your paper.
However, given high competition at ICLR2020, we think the current manuscript is premature and still below the bar to be accepted to ICLR2020.
We hope that the reviewers' comments are useful to improve your manuscript for potential future submission.",Paper Decision
IHCqr5By6n,SJeW-A4tDS,Detecting malicious PDF using CNN,Reject,"This submission addresses the problem of detecting malicious PDF files. The proposed solution trains existing CNN architectures on a collected dataset and verifies improved performance over available antivirus software. 

There were a number of concerns raised about this work. The main concern the reviewers had with this submission is lack of novelty. The issue is that the paper tackles a standard supervised classification problem which has been extensively explored in the literature and applies an off-the-shelf classification model. Though the particular application has seen less attention in the ICLR community, the problem setting and solution are well known. Thus, the contribution of the work is not sufficient for acceptance. ",Paper Decision
a1zJS2QinU,Byxl-04KvH,NESTED LEARNING FOR MULTI-GRANULAR TASKS,Reject,"This paper proposes a model architecture and training procedure for multiple nested label sets of varying granularities and shows improvements in efficiency over simple baselines in the number of fine-grained training labels needed to reach a given level of performance.

Reviewers did not raise any serious concerns about the method that was presented, but they were also not convinced that it represented a sufficiently novel or impactful contribution to an open problem. Without any reviewer advocating for the paper, even after discussion, I have no choice but to recommend rejection.

I'm open to the possibility that there is substantial technical value here, but I think this work would be well served by more extensive comparisons and a potentially revamped motivation to try to make the case for it that value more directly.",Paper Decision
K-evbMGKR6,HkgxW0EYDS,Scalable Model Compression by Entropy Penalized Reparameterization,Accept (Poster),"The paper describes a simple method for neural network compression by applying Shannon-type encoding. This is a fresh and nice idea, as noted by reviewers. A disadvantage is that the architectures on ImageNet are not the most efficient ones. Also, the review misses several important works on low-rank factorization of weights for the compression (Lebedev et. al, Novikov et. al).   But overall, a good paper.",Paper Decision
exTZhGdKTu,SkxybANtDB,Dynamic Time Lag Regression: Predicting What & When,Accept (Poster),"The paper proposes a Bayesian approach for time-series regression when the explanatory time-series influences the response time-series with a time lag. The time lag is unknown and allowed to be non-stationary process. Reviewers have appreciated the significance of the problem and novelty of the proposed method, and also highlighted the importance of the application domain considered by the paper. ",Paper Decision
ge6BRZsgr,S1lRg0VKDr,On summarized validation curves and generalization,Reject,"The reviewers reached a consensus that the paper is preliminary and has a very limited contribution. Therefore, I cannot recommend acceptance at this time.",Paper Decision
TGt6hB_MHN,Hke0lRNYwS,Convolutional Bipartite Attractor Networks,Reject,"This paper proposes to reintroduce bipartite attractor networks and update them using ideas from modern deep net architectures. 

After some discussions, all three reviewers felt that the paper did not meet the ICLR bar, in part because of an insufficiency of quantitative results, and in part because the extension was considered pretty straightforward and the results unsurprising, and hence it did not meet the novelty bar. I therefore recommend rejection. ",Paper Decision
MbIPg3Nmji,BkeaxAEKvB,New Loss Functions for Fast Maximum Inner Product Search,Reject,"While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.

Concerns were raised as to the generality of the approach, thoroughness of experiments, and clarity of the exposition.",Paper Decision
iVhxIkKCnF,BylalAEtvB,Lipschitz Lifelong Reinforcement Learning,Reject,"While there was some support for this paper, there was not enough support to accept it for publication at ICLR.

The following concern is characteristic of the concerns raised by the reviewers: ""The ""main contribution of this paper is hard to discern, but the ideas presented are interesting."" Other reviewers said it was ""hard to read"" and not ready for publication.
",Paper Decision
Ifzq7yLeF1,B1x2eCNFvH,Local Label Propagation for Large-Scale Semi-Supervised Learning,Reject,"The paper introduces an approach for semi-supervised learning based on local label propagation. While reviewers appreciate learning a consistent embedding space for prediction and label propagation, a few pointed out that this paper does not make it clear how different it is from preview work (Wu et al, Iscen et al., Zhuang et al.), in addition to complexity calculation, or pseudo-label accuracy. These are important points that weren’t included to the degree that reviewers/readers can understand, and reviewers seem to not change their minds after authors wrote back. This suggests the paper can use additional cycles of polishing/editing to make these points clear. We highly recommend authors to carefully reflect on reviewers both pros and cons of the paper to improve the paper for your future submission. 
",Paper Decision
Tnz_Ol2xLb,S1lslCEYPB,Improved Mutual Information Estimation,Reject,"This paper centers on an unbiased variant of the Mutual Information Neural Estimation (procedure), using the so-called ""eta trick"" applied to the Donsker-Varadhan lower bound on the KL divergence. The paper's contribution is mainly theoretical though experiments are presented on synthetic Gaussian-distributed data as well as CIFAR10 and STL10 classification experiments (from learned representations).

R1's criticism of the theoretical contributions centers on fundamental limitations on finite sample estimation of the MI, contending that the bounds simply aren't meaningful in high-dimensional settings, and that the empirical work centers on synthetic data and self-generated baselines rather than comparisons to reported numbers in the literature; they were unswayed by the author response, which contended that these criticisms were based on pessimistic worst-case analysis and that ""mild assumptions on the mutual information and function class"" could render better finite-sample bounds. Some of R3's concerns were addressed by the author rebuttal and associated updates, but remained critical of the presentation, in particular regarding the dual function, and downgraded their score.

Because R2 disclosed that they were outside of their area of strong expertise, a 4th reviewer was sought (by this stage, the paper was the revised version). Concerns about clarity persisted, with R4 remarking that a section was ""a collection of different remarks without much coherence, some of which are imprecisely stated"". R4 felt variance and sample complexity should be dealt with experimentally, though this was not directly addressed in the author response. R4 also remarked that the plots were difficult to read and questioned the utility of supervised representation learning benchmarks at assessing the quality of MI estimation, given recent evidence in the literature.

The theoretical contributions of this submission are slightly outside the bounds of my own expertise, but consensus among three expert reviewers appears to be that the clarity of exposition leaves much to be desired, and I concur with their assessment that the empirical investigation is insufficiently rigorous and does not draw clear comparisons to existing work in this area. I therefore recommend rejection.",Paper Decision
0G4iTPlp6,rJeqeCEtvH,Semi-Supervised Generative Modeling for Controllable Speech Synthesis,Accept (Poster),"The authors propose to enforce interpretability and controllability on latent variables, like affect and speaking rate, in a speech synthesis model by training in a semi-supervised way, with a small amount of labeled data with the variables of interest labeled.   The idea is sensible and the results are very encouraging, and the authors have addressed the initial concerns brought up by the reviewers.",Paper Decision
4VpbizTRe3,Hkg5lAEtvS,Towards Physics-informed Deep Learning for Turbulent Flow Prediction,Reject,"The reviewers all agree that this is an interesting paper with good results. The authors' rebuttal response was very helpful. However, given the competitiveness of the submissions this year, the submission did not make it. We encourage the authors to resubmit the work including the new results obtained during the rebuttal.",Paper Decision
a4gzivpZp,SJeYe0NtvH,Neural Text Generation With Unlikelihood Training,Accept (Poster),"This paper introduces a new objective for text generation with neural nets.  The main insight is that the standard likelihood objective assigns excessive probability to sequences containing repeated and frequent words.  The paper proposes an objective that penalizes these patterns.  This technique yields better text generation than alternative methods according to human evaluations.

The reviewers found the paper to be written clearly. They found the problem to be relevant and found the proposed solution method to be both novel and simple.  The experiments were carefully designed and the results were convincing.  The reviewers raised several concerns on particular details of the method.  These concerns were largely addressed by the authors in their response.  Overall, the reviewers did not find the weaknesses of the paper to be serious flaws.

This paper should be published. The paper provides a clearly presented solution for a relevant problem, along with careful experiments. 

",Paper Decision
clmQ9gWiKr,rkgOlCVYvB,Pure and Spurious Critical Points: a Geometric Study of Linear Networks,Accept (Poster),This paper studies the landscape of linear networks and its critical point. The authors utilize geometric properties of determinantal varieties to derive interesting results on the landscape of linear networks. The reviewers raised some concerns about the fact that many of the results stated here can already be achieved using other techniques and therefore had some concerns about the novelty of these results. The authors provided a detailed response addressing these concerns. One reviewer however still had some concerns about the novelty. My own understanding of the paper is that while some of these results can be obtained using other approaches the proof techniques (brining ideas from algebraic geometry) is novel and could be rather useful. While at this point it is not clear that the techniques generalize to the nonlinear case I think algebraic geometry perspective have a good potential and provide some diversity in the theoretical techniques. As a result I recommend acceptance if possible.,Paper Decision
O8hzint9or,H1l_gA4KvH,Surrogate-Based Constrained Langevin Sampling With Applications to Optimal Material Configuration Design,Reject,"The paper is not overly well written and motivated. A guiding thread through the paper is often missing. Comparisons with constrained BO methods would have improved the paper as well as a more explicit link to multi-objective BO. It could have been interesting to evaluate the sensitivity w.r.t. the number of samples in the Monte Carlo estimate. What happens if the observations of the function are noisy? Is there a natural way to deal with this?
Given that the paper is 10+ pages long, we expect a higher quality than an 8-pages paper (reviewing and submission guidelines). ",Paper Decision
FHYKL83VmV,BJluxREKDB,Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning,Accept (Poster),"This paper proposes a new method to learning heuristics for quantified boolean formulas through RL. The focus is on a method called backtracking search algorithm. The paper proposes a new representation of formulas to scale the predictions of this method.

The reviewers have an overall positive response to this paper. R1 and R2 both agree that the paper should be accepted, and have given some minor feedback to improve the paper. R3 initially was critical of the paper, but the rebuttal helped to clarify their doubt. They still have one more comment and I encourage the authors to address this in the final version of the paper.

R3 meant to increase their score but somehow this is not reflected in the current score. Based on their comments though, I am assuming the scores to be 6,8,6 which makes the cut for ICLR. Therefore, I recommend to accept this paper.",Paper Decision
nhLtfGOGgt,S1gvg0NYvH,Mean Field Models for Neural Networks in Teacher-student Setting,Reject,"This paper studies the evolution of the mean field dynamics of a two layer-fully connected and Resnet model. The focus is in a realizable or student/teacher setting where the labels are created according to a planted network. The authors study the stationary distribution of the mean-field method and use this to explain various observations. I think this is an interesting problem to study. However, the reviewers and I concur that the paper falls short in terms of clearly putting the results in the context of existing literature and demonstrating clear novel ideas. With the current writing of the paper is very difficult to surmise what is novel or new. I do agree with the authors' response that clearly they are looking at some novel aspects not studied by the previous work but this was not revised during the discussion period. Therefore, I do not think this paper is ready for publication. I suggest a substantial revision by the authors and recommend submission to future ML venues. ",Paper Decision
sdCEWQY5b,Hkxvl0EtDH,A Causal View on Robustness  of Neural Networks,Reject,"This paper attempts to present a causal view of robustness in classifiers, which is a very important area of research.
However, the connection to causality with the presented model is very thin and, in fact, mathematically unnecessary. Interventions are only applied to root nodes (as pointed out by R4) so they just amount to standard conditioning on the variable ""M"". The experimental results could be obtained without any mention to causal interventions.",Paper Decision
Vkaoa0A2hL,ryeUg0VFwr,Striving for Simplicity in Off-Policy Deep Reinforcement Learning,Reject,All the reviewers recommend rejecting the submission. There is no basis for acceptance.,Paper Decision
0CafPDEql,rJgLlAVYPr,White Box Network: Obtaining a right composition ordering of functions,Reject,"This paper presents White Box Network (WBN), which allows for composing function blocks from a given set of functions to construct a target function. The main idea is to introduce a selection layer that only selects one element of the previous layer as an input to a function block.  The reviewers were unanimous in their opinion that the paper is not suitable for publication at ICLR in its current form.  There were significant concerns about the clarity in writing, and reviewers have provided detailed discussion should the authors wish to improve the paper.",Paper Decision
5G8P29mA0Z,SyxrxR4KPS,Deep neuroethology of a virtual rodent,Accept (Spotlight),"This paper is somewhat unorthodox in what it sets out to do: use neuroscience methods to understand a trained deep network controlling an embodied agent. This is exciting, but the actual training of the virtual rodent and the performance it exhibits is also impressive in its own right. All reviewers liked the papers. The question that recurred among all reviewers was what was actually learned in this analysis. The authors responded to this convincingly by listing a number of interesting findings. 

I think this paper represents an interesting new direction that many will be interested in.",Paper Decision
bVGuAAaTMv,HklSeREtPB,Emergence of functional and structural properties of the head direction system by optimization of recurrent neural networks,Accept (Spotlight),"This paper studies properties that emerge in an RNN trained to report head direction, showing that several properties in natural neural circuits performing that function are detected. 
All reviewers agree that this is quite an interesting paper. While there are some reservations as to the value of letting a property of interest emerge as opposed to simply hand-coding it in, this approach is seen as powerful and valuable by many people, in that it suggests a higher plausibility that the emerging properties are actually useful when optimizing for that function -- a claim which hand-coding would not make possible. Reviewers have also provided valuable suggestions and requests for clarifications, and authors have responded by improving the presentation and providing more insights.
Overall, this is a solid contribution that will be of interest to the part of the ICLR audience that is interested in biological systems.",Paper Decision
r-CadCiZMD,BJl4g0NYvB,Causal Induction from Visual Observations for Goal Directed Tasks,Reject,"The submission presents an approach to uncovering causal relations in an environment via interaction. The topic is interesting and the work is timely. However, the experimental setting is quite simplistic and the approach makes strong assumptions that limit its applicability. The reviewers are split. R2 raised their rating from 3 to 6 following the authors' responses and revision, but R1 maintained their rating of 3 and posted a response that justifies this position. In light of the limitations of the work, the AC recommends against accepting the submission.",Paper Decision
A5CcZTlshz,Hkx7xRVYDr,Duration-of-Stay Storage Assignment under Uncertainty,Accept (Spotlight),"Thanks to the authors for the submission and the active discussion. The paper applies deep learning to the duration-of-stay estimation problem in the warehouse storage application. The authors provide problem formulation and describe the pipeline of their solutions, including datasets preparation and loss functions design. The reviewers agree that this is a good application paper that showcases how deep learning can be useful for a real-world problem. The release of the dataset can also be a nice contribution. A major debate during the discussion is whether this paper is in scope of ICLR given that it is mostly a straightforward application existing techniques. After several rounds of discussion, reviewers think that this should fit under the category ""applications in vision, ... , computational biology, and others."" Overall, this paper can be a good example of applying deep learning to real-world problems.
",Paper Decision
_QI7ffqaiU,BkxXe0Etwr,CAQL: Continuous Action Q-Learning,Accept (Poster),All three reviewers gave scores of Weak Accept. AC has read the reviews and rebuttal and agrees that the paper makes a solid contribution and should be accepted.,Paper Decision
8XE8gaX5-,r1eQeCEYwB,GRAPH ANALYSIS AND GRAPH POOLING IN THE SPATIAL DOMAIN,Reject,"The authors identify a limitation of aggregating GNNs, which is that global structure can be mostly lost. They propose a method which combines a graph embedding with the spatial convolution GNN and show that the resulting GNN can better distinguish between similar local structures. 

The reviewers were mixed in their scores. The proposed approach is clearly motivated and justified and may be relelvant for some graphnet researchers, but the approach is only applicable in some circumstances - in other cases it may be desirable to ignore global structure. This, plus the high computational complexity of the proposed approach, mean that the significance is weaker. Overall the reviewers felt that the contribution was not significant enough and that the results were not statistically convincing.  Decision is to reject.",Paper Decision
Qy8E9ksjm-,Hkxzx0NtDB,Your classifier is secretly an energy based model and you should treat it like one,Accept (Talk),"This paper uses energy based model to interpret standard discriminative classifier and demonstrates that energy based model training of the joint distribution improves calibration, robustness, and out-of-distribution detection while generating samples with better quality than GAN-based approaches. The reviewers are very excited about this work, and the energy-based perspective of generative and discriminative learning. There is a unanimous agreement to strongly accept this paper after author response.",Paper Decision
2qoIFs-0i,rylMgCNYvS,On the Linguistic Capacity of Real-time Counter Automata,Reject,"This paper presents an analysis of the languages that can be accepted by a counter machine, motivated by recent work that suggests that counter machines might be a good formal model from which to approach the analysis of LSTM representations.

This is one of the trickiest papers in my batch. Reviewers agree that it represents an interesting and provocative direction, and I suspect that it could yield valuable discussion at the conference. However, reviewers were not convinced that the claims made (or implied) _about LSTMs_ are motivated, given the imperfect analogy between them and counter machines. The authors promise some empirical evidence that might mitigate these concerns to some extent, but the paper has not yet been updated, so I cannot take that into account. 

As a very secondary point, which is only relevant because this paper is borderline, LSTMs are no longer widely used for language tasks, so discussion about the capacity of LSTMs _for language_ seems like an imperfect fit for an machine learning conference with a fairly applied bent.",Paper Decision
mUBpUzTcGf,HJxWl0NKPB,Combining MixMatch and Active Learning for Better Accuracy with Fewer Labels,Reject,"This paper extends state of the art semi-supervised learning techniques (i.e., MixMatch) to collect new data adaptively and studies the benefit of getting new labels versus adding more unlabeled data. Active learning is incorporated in a natural and simple (albeit, unsurprising) way and the experiments are convincing that this approach has merit.

While the approach works, reviewers were concerned about the novelty of the combination given that its somewhat obvious and straightforward to accomplish. Reviewers were also concerned that the space of both semi-supervised learning algorithms and active learning algorithms was not sufficiently exhaustively studied. As one reviewer points out: neither of these ideas are new or particular to deep learning.

Due to lack of novelty, this paper is not suited for a top tier conference. ",Paper Decision
rPClWhIXrf,BJxWx0NYPr,Adaptive Structural Fingerprints for Graph Attention Networks,Accept (Poster),This paper is consistently supported by all three reviewers during initial review and discussions. Thus an accept is recommended.,Paper Decision
2oVWAln-H2,ByxxgCEYDS,Inductive Matrix Completion Based on Graph Neural Networks,Accept (Spotlight),"This paper proposes a novel technique for matrix completion, using graphical neighborhood structure to side-step the need for any side-information.

Post-rebuttal, the reviewers converged on a unanimous decision to accept. The authors are encouraged to review to address reviewer comments.",Paper Decision
zIFMPNShG_,H1lxeRNYvB,Neural Operator Search,Reject,"This paper proposes an extension of the search space of neural architecture search to include dynamic convolutions, teacher nets among others. The method is evaluated on CIFAR-10 and Imagenet with a similar setup as other architecture search methods. The reviewers found that the results did not convincingly show that the proposed improvements were better than other ways of improving neural architecture search such as rroxylessNAS.",Paper Decision
oN42aMToV,rklklCVYvB,Time2Vec: Learning a Vector Representation of Time,Reject,"This paper investigates and evaluates learning high-dimensional embeddings of time, which is useful for a variety of applications. This paper received 4 reviews (due to a missing review, we requested several emergency reviews). R1 recommends Weak Accept, calling the method simple but saying it could be of wide interest and utility in practice. R3 recommends Reject, identifying concerns about the significance of the contribution, caused by the simplicity of the approach, the connection to existing work, and missing comparisons to baselines. In a short review, R4 recommends Accept with several positive comments. In a long, thoughtful review, R5 recommends Weak Reject, due to concerns and questions about the theoretical motivation and depth of experiments. The authors have submitted detailed responses that have addressed many of the questions of the reviewers; however, R3 feels the response does not address their concerns, and R5 is closer to accepting but still feels additional improvements in presentation and experimentation are needed.

Given the split decision, the AC also read the paper. The AC agrees with R1 and R4 that this is an interesting problem and the approach here may be useful in practice, but shares concerns with R3 and R5 about the depth of contribution with respect to existing work, and need for additional experimental validation against stronger baselines. ",Paper Decision
NaJrBoLmaC,HklkeR4KPB,ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring,Accept (Poster),"This works improves the MixMatch semi-supervised algorithm along the two directions of distribution alignment and augmentation anchoring, which together make the approach more data-efficient than prior work.
All reviewers agree that the impressive empirical results in the paper are its main strength, but express concern that the method is overly complicated and hacking together many known pieces, as well as doubt as to the extent of the contribution of the augmentation method itself, with requests for better augmentation controls.
While some of these concerns have not been addressed by authors in their response, the strength of empirical results seems enough to justify an acceptance recommendation.",Paper Decision
6aocbQPRn,Hkekl0NFPr,Conditional Learning of Fair Representations,Accept (Spotlight),"This paper provides a new algorithm for learning fair representation for two different fairness criteria--accuracy parity and equalized odds. The reviewers agree that the paper provides novel techniques, although the experiments may appear to be a bit weak. Overall, this paper gives new contributions to the fair representation learning literature.

The authors should consider citing and discussing the relationship with the following work:
A Reductions Approach to Fair Classification., ICML 2018",Paper Decision
licXSaUKkC,r1eCy0NtDH,Mean-field Behaviour of Neural Tangent Kernel for Deep Neural Networks,Reject,"This paper focuses on studying the impact of initialization and activation functions on the Neural Tangent Kernel (NTK) type analysis. The authors claim to make a connection between NTK and edge of chaos analysis. The reviewers had some concern about (1) impact of smooth activations ""any NTK-based training method for DNNs should use a Smooth Activation Function from the class S and the network should be initialized on the EOC"" (2) proofs of residual networks (3) and why mixing NTK with EOC is interesting. Some of these concerns were addressed in the response. I do share the reviewer concerns about (2). The authors need to give a clear proof. I think this combination of NTK and EOC could be interesting but needs to be better motivated. As a result I do not recommend publication.",Paper Decision
E7s56FBrAS,BylRkAEKDH,TabNet: Attentive Interpretable Tabular Learning,Reject,"This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work, and I urge the authors to continue to develop refinements and extensions.",Paper Decision
54CM5XuiMv,ryeT10VKDH,Adapt-to-Learn: Policy Transfer in Reinforcement Learning,Reject,"This paper considers inter-domain policy transfer in reinforcement learning. The proposed approach involves adapting existing policies from a source task to a target task by adding a cost related to the difference between the dynamics and trajectory likelihoods of the two tasks.

There are three major problems with this paper as it stands, as pointed out by the reviewers. Firstly, the ""KL divergence"" is not a real KL divergence and seems to be only empirically motivated. Then, there are issues with the derivative of the policy gradient. Finally, the theory is not well connected to the proposed algorithm. The rebuttals not only failed to convince the reviewer that raised these issues, but another reviewer lowered their score as a result of these raised points.

This is a really interesting idea with compelling experiments, but must be rejected at this point for the aforementioned reasons.",Paper Decision
Y_Hx7sgEFF,B1l6y0VFPr,Identity Crisis: Memorization and Generalization Under Extreme Overparameterization,Accept (Poster),"The paper studies the effect of various hyperparameters of neural networks including architecture, width, depth, initialization, optimizer, etc. on the generalization and memorization. The paper carries out a rather through empirical study of these phenomena. The authors also rain a model to mimic identity function which allows rich visualization and easy evaluation.  The reviewers were mostly positive but expressed concern about the general picture. One reviewer also has concerns about ""generality of the observed phenomenon in this paper"". The authors had a thorough response which addressed many of these concerns. My view of the paper is positive. I think the authors do a great job of carrying out careful experiments. As a result I think this is a good addition to ICLR and recommend acceptance.",Paper Decision
0KB1BPApx,H1e31AEYwB,Stiffness: A New Perspective on Generalization in Neural Networks,Reject,"While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.

Concerns raised include lack of sufficient motivation for the approach, and problems with clarity of the exposition.",Paper Decision
mtJtiJNB_u,rkg3kRNKvH,Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities,Reject,"This paper presents an analysis of the kind of knowledge captured by pre-trained word embeddings. The authors show various kinds of properties like relation between entities and their description, mapping high-level commands to discrete commands etc. The problem with the paper is that almost all of the properties shown in this work has already been established in existing literature. In fact, the methods presented here are the baseline algorithms to the identification of different properties presented in the paper.

The term common-sense which is used often in the paper is mischaracterized. In NLP literature, common-sense is something that is implicitly understood by humans but which is not really captured by language. For example, going to a movie means you need parking is something that is well-understood by humans but is not implied by the language of going to the movie. The phenomenon described by the authors is general language processing.

Towards the end the evaluation criteria for embedding proposed is also a well-established concept, its just that these metrics are not part of the training mechanism as yet. So if the contribution was on showing how those metrics can be integrated in training the embeddings, that would be a great contribution.

I agree with the reviewer's critics and recommend a rejection as of now.",Paper Decision
CjJQWzzPS,Skg3104FDS,First-Order Preconditioning via Hypergradient Descent,Reject,"This paper has been assessed by three reviewers who scored it as 3/3/3, and they did not increase their scores after the rebuttal. The main criticism lies in novelty of the paper, lack of justification for MM^T formulation, speed compared to gradient descent (i.e. theoretical analysis plus timing). Other concerns point to overlaps with Baydin et al. 2015 and the question about the validity of Theorem 1. On balance, this paper requires further work and it cannot be accepted to ICLR2020.",Paper Decision
855mfqVWoH,B1eoyAVFwH,Feature Partitioning for Efficient Multi-Task Architectures,Reject,"This paper considers how to create efficient architectures for multi-task neural networks. R1 recommends Weak Reject, identifying concerns about the clarity of writing, unsupported claims, and missing or unclear technical details. R2 recommends Weak Accept but calls this a ""borderline"" case, and has concerns about experiments and comparisons to baselines. R3 also has concerns about experiments and baselines, and feels the approach is somewhat ad hoc. The authors submitted a response that addressed some of these issues, but the authors chose to maintain their decisions. The AC feels the paper has merit but given these slightly negative to borderline reviews, we cannot recommend acceptance at this time. We hope the reviewer comments help the authors to prepare a revision for another venue.",Paper Decision
iRWQxcYTQv,SkejkR4KDr,Layer Flexible Adaptive Computation Time for Recurrent Neural Networks,Reject,"All reviewers assessed this paper as a weak reject.
The AC recommends rejection.",Paper Decision
-8uRqSQMcq,Skgq1ANFDB,Curvature-based Robustness Certificates against Adversarial Examples,Reject,"This paper presents a upper bound on the curvature of a deep network. After the discussion, the author has addressed some concerns of reviwers, but the results are not very strong, there is some limitation on the applications. There is no strong support for this paper. Due to the high standard of ICLR, the acceptance of the paper need strong results in terms of theory or experiments.",Paper Decision
hRRPc5KK1m,Byx91R4twB,Adversarial Video Generation on Complex Datasets,Reject,"This paper addresses the tasks of video generation and prediction and shows impressive results on the datasets such as Kinetics-600. There is a reviewer disagreement on this paper. AC can confirm that all three reviewers have read the rebuttal and have contributed to a long discussion. The reviewers have raised the following concerns that were viewed as critical issues when making the final decision: R1 and R3 expressed the concerns regarding limited technical novelty of the proposed approach in light of the prior works, e.g. MoCoGAN and TGANv2. R3 suggests, that the proposed method shows advantage that might be due to the large computational resources available to train the model. Providing a comparison of the proposed model and the relevant baselines on the Kinetics dataset is desirable to access the benefits of the proposed approach (R1). 
AC also agrees with the R2 about the potential impact this work could have in the community. However, given that the reviewers have raised important concerns and have given suggestions, the paper needs too many revisions for acceptance at this time. We hope the reviews are useful for improving and revising the paper.",Paper Decision
qzh86zABmF,HkgtJRVFPS,Topological Autoencoders,Reject,"This paper introduces a new variant of autoencoders with an topological loss term.

The reviewers appreciated part of the paper and it is borderline. However, there are enough reservations to argue for it will be better for the paper to updated and submitted to next conference.

Rejection is recommended.  ",Paper Decision
VRA5vYada,HkeO104tPB,Reinforcement Learning without Ground-Truth State,Reject,"This paper considers the problem of reinforcement learning with goal-conditioned agents where the agents do not have access to the ground truth state.  The paper builds on the ideas in hindsight experience replay (HER), a method that relabels past trajectories with a goal set in hindsight.  This hindsight mechanism enables indicator reward functions to be useful even with image inputs.  Two technical contributions are reward balancing (balancing positive and negative experience) and reward filtering (a heuristic for removing false negatives).  The method is tested on multiple tasks including a novel RopePush task in simulation. 

The reviewers discussed strengths and limitations of the paper.  One strength was that the writing was clear for the reviewers. One limitation was the paper's novelty, as most of these ideas are already present in HER with the exception of reward filtering.  Another major concern was that the experiments were not sufficiently informative.  The simulation tasks did not adequately distinguish the proposed method from the baseline (in two of the three tasks) and the third task (RopePush) was simplified substantially (using invisible robot arms).  The real world task did not require the pixel observations.  The analysis of the method was also found to be somewhat limited by the reviewers, though this was partially addressed by the authors.

This paper is not yet ready for publication since the proposed method has insufficient supporting evidence.  A more thorough experiment could provide stronger evidence by showing a regime where the proposed method performs better than alternatives.",Paper Decision
yyWlCpuSTl,HJe_yR4Fwr,Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin,Accept (Poster),"This works presents a new and interesting notion of margin for deep neural networks (that incorporates representation at all layers). It then develops generalization bounds based on the introduced margin. The reviewers pointed some concerns, including some notation issues, complexity in case of residual networks, removal of exponential dependence on depth,  and dependence on a hard to compute quantity - \kapp^{adv}. Some of these concerns were addressed by the authors. At the end, most of the reviewers find the notion of all-layer margin introduced in this paper a very novel and promising idea for characterizing generalization in deep networks. Agreeing with reviewers, I recommend accept. However, I request the authors to accommodate remaining comments /concerns raised by R1 in the final version of your paper. In particular, in your response to R1 you mentioned for one case you saw improvement even with dropout, but that is not mentioned in the revision; Please include related details in the draft.
",Paper Decision
QVBARu6e4,BJx_JAVKDB,In-Domain Representation Learning For Remote Sensing,Reject,"As the reviewers point out, this paper requires a major revision and fleshing out of the claimed contribution before it is suitable for conference presentation.",Paper Decision
kXBkmdmI66,BJevJCVYvB,Training Neural Networks for and by Interpolation,Reject,"This paper uses the interpolation property to design a new optimization algorithm for deep learning, which computes an adaptive learning-rate in closed form at each iteration. The authors also analyzed the convergence rate of the proposed algorithm in the stochastic convex optimization setting. Experiments on several benchmark neural networks and datasets verify the effectiveness of the proposed algorithm. This is a borderline paper and has been carefully discussed. The main objection of the reviewers include: (1) The interplay between regularization and the interpolation property is not clear; and (2) the proposed algorithm  is no better than SGD in any of the benchmarks except one, where SGD's learning rate is set to be a constant. After the author response, this paper still does not gather sufficient support. So I encourage the authors to improve this paper and resubmit it to future conference.",Paper Decision
K7h6M2h9eT,ByeL1R4FvS,Unsupervised Data Augmentation for Consistency Training,Reject,"The paper shows that data augmentation methods work well for consistency training on unlabeled data in semi-supervised learning.

Reviewers and AC think that the reported experimental scores are interesting/strong, but scientific reasoning for convincing why the proposed method is valuable is limited. In particular, the authors are encouraged to justify novelty and hyper-parameters used in the paper. This is because I also think that it is not too surprising that more data augmentations in supervised learning are also effective in semi-supervised learning. It can be valuable if more scientific reasoning/justification is provided.

Hence, I recommend rejection.",Paper Decision
ThDp14kXir,Hyl8yANFDB,Assessing Generalization in TD methods for Deep Reinforcement Learning,Reject,"This paper received three reviews. R1 recommends Weak Reject, and identifies a variety of concerns about the motivation, presentation, clarity and soundness of results, and experimental design (e.g. choice of metrics). In a short review, R2 recommends Weak Accept, but indicates they are not an expert in this area. R3 also recommends Weak Accept, but identifies concerns also centering around clarity and completeness of the paper as well as some specific technical questions. In their response, authors address these issues, and have a constructive back-and-forth conversation with R1, who remains unconvinced about significance of the empirical results and thus the conclusion of the overall paper. After the discussion period, R3 indicated that they weakly favored acceptance but agreed that the paper had significant presentation issues and would not strongly advocate for it. R1 advocated for Reject, given the concerns identified in their reviews and followup comments. Given the split decision, the AC also read the paper. While the work clearly has merit, we agree with R1's comment that it is overall a ""potentially interesting idea, but the justification and presentation/quantification of results is not good enough in the submitted paper,"" and feel the paper really needs a revision and another round of peer review before publication. ",Paper Decision
s6V1EtuDo1,rkeS1RVtPS,Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning,Accept (Talk),"This paper proposes a novel stochastic gradient Markov chain Monte Carlo method incorporating a cyclical step size schedule (cyclical SG-MCMC).  The authors argue that this step size schedule allows the sampler to cross modes (when the step size is large) and locally explore modes (when the step size is smaller).  SG-MCMC is a very promising method for Bayesian deep learning as it is both scalable and easily to incorporate into existing models.  However, the stochastic setting often leads to the sampler getting stuck in a local mode due to a requirement of a small step size (which itself is often due to leaving out the Metropolis-Hastings accept / reject step).   The cyclic learning rate intuitively helps the sampler escape local modes.  This property is demonstrated on synthetic problems in comparison to existing SG-MCMC baselines.  The authors demonstrate improved negative log likelihood on larger scale deep learning benchmarks, which is appreciated as the related literature often restricts experiments to small scale problems.  The reviewers all found the paper compelling and argued for acceptance and thus the recommendation is to accept.  Some questions remain for future work.  E.g. all experiments were performed using a very low temperature, which implies that the methods are not sampling from the true Bayesian posterior.  Why is such a low temperature needed for reasonable performance?  In any case a very nice paper.",Paper Decision
9hObwT1yUB,rJgSk04tDH,Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?,Reject,"This paper seeks to analyse the important question around why hierarchical reinforcement learning can be beneficial. The findings show that improved exploration is at the core of this improved performance. Based on these findings, the paper also proposes some simple exploration techniques which are shown to be competitive with hierarchical RL approaches.

This is a really interesting paper that could serve to address an oft speculated about result of the relation between HRL and exploration. While the findings of the paper are intuitive, it was agreed by all reviewers that the claims are too general for the evidence presented. The paper should be extended with a wider range of experiments covering more domains and algorithms, and would also benefit from some theoretical results.

As it stands this paper should not be accepted.",Paper Decision
X0nlKU13z,r1xNJ0NYDH,The Effect of Neural Net Architecture on Gradient Confusion & Training Performance,Reject,"This paper introduces the concept of gradient confusion to show how the neural network architecture affects the speed of training. The reviewers' opinion on this paper varies widely, also after the discussion phase.  The main disagreement is on the significance of this work, and whether the concept of gradient confusion adds something meaningful to the existing literature with respect to understanding deep networks. The strong disagreement on this paper suggest that the paper is not quite ready yet for ICLR, but that the authors should make another iteration on the paper to strengthen the case for its significance.
",Paper Decision
Y9L1-n6ZI8,H1lmyRNFvr,Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space,Accept (Poster),"Paper received reviews of A, WA, WR. AC has carefully read all reviews/responses. R1 is less experienced in this area. AC sides with R2,R3 and feels paper should be accepted. Interesting topic and interesting problem. Authors are encouraged to strengthen experiments in final version. ",Paper Decision
BuDufNvwG,SJeXJANFPr,Regularizing Deep Multi-Task Networks using Orthogonal Gradients,Reject,"This paper proposes a training approach that orthogonalizes gradients to enable better learning across multiple tasks. The idea is simple and intuitive.

Given that there is past work following the same kind of ideas, it would be need to further: 
(a) expand the experimental evaluation section with comparisons to prior work and, ideally, demonstrate stronger results.
 (b) study in more depth the assumptions behind gradient orthogonality for transfer. This would increase impact on top of past literature by explaining, besides intuitions, why gradient orthogonality helps for transfer in the first place.
",Paper Decision
mD_yhn82mk,BklXkCNYDB,Fast Training of Sparse Graph Neural Networks on Dense Hardware,Reject,"While there was some interest in the ideas presented, this paper was on the borderline, and was ultimately not able to be accepted for publication at ICLR.

Reviewers raised concerns as to the novelty, generality, and practicality of the approach, which could have been better demonstrated via experiments.",Paper Decision
V6thUKigye,Hyez1CVYvr,Simultaneous Classification and Out-of-Distribution Detection Using Deep Neural Networks,Reject,"The paper proposes a method for out-of-distribution (OOD) detection for neural network classifiers.

The reviewers raised several concerns about novelty, choice of baselines and the experimental evaluation. While the author rebuttal addressed some of these concerns, I think the paper is still not ready for acceptance as is. 

I encourage the authors to revise the paper and resubmit to a different venue.",Paper Decision
u9SwzOjXYb,rkgMkCEtPB,Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML,Accept (Poster),"Paper received mixed reviews: WR (R1), A (R2 and R3). AC has read reviews/rebuttal and examined paper. AC agrees that R1's concerns are misplaced and feels the paper should be accepted. 
",Paper Decision
uMqAxKxXXP,SJlbyCNtPr,"Long-term planning, short-term adjustments",Reject,"This paper proposes a reinforcement learning algorithm for continuous action domains that combines a short-horizon model-based objective and a long-horizon value estimate.  The stated benefits to this approach are the ability to modify the model-based objective without extensive retraining.  This model-based objective can also capture custom constraints and implements a linear dynamics model that is used in conventional control theory.  The proposed method was evaluated on two domains (the mountain car domain and a custom crane domain) and compared to a continuous action space method (DDPG).  

This discussion of this paper highlighted both strengths and weaknesses.  The reviewers said the presentation was clear.  The reviewers also appreciated the relevance of the problem.  The primary weakness was the evaluation of the method. One repeated concern from the reviewers was having only one standard domain for evaluation (mountain car).  Another concern was the absence of other model-based algorithms, which was addressed by the author response.   

This paper is not yet ready to be published, despite its possible benefits,  due to the lack of evidence for this method on more continuous action problems.   ",Paper Decision
SkpXWoAZAN,Hyg-JC4FDr,Imitation Learning via Off-Policy Distribution Matching,Accept (Poster),"This work addresses new insights in the imitation learning setting, and shows how a popular type of approach can be extended in a principled way to the off-policy learning setting. Several requests for clarification were addressed in the rebuttal phase, in particular regarding the empirical evaluation in off-policy settings. The authors improved the empirical validation and overall clarity of the paper. The resulting manuscript provides valuable new insights, in particular in its principled connections, and extension to previous work.",Paper Decision
bMSXeuQfN,BklekANtwr,Unsupervised Learning of Automotive 3D Crash Simulations using LSTMs,Reject,"
The paper proposes to train LSTMs to encode car crashes (a temporal sequence of 3D mesh representations).  Decoder LSTMs can then be used to 1) reconstruct the input or 2) predict the future sequence of structural geometry.  The authors propose to use a spectral feature representation based on prior work as input into the encoding LSTM.  The main contribution of the paper (based on the author response) is the introduction of this spectral feature representation to the ML community.  The authors used single 3D truck model to generate 205 simulations, of which 105 was used for training, and 100 for testing.  The authors presented reconstruction errors and TSNE visualization of the LSTM's reconstruction weights.

Discussion Summary:
The paper got three weak rejects.  The response provided by the authors failed to convince any of the reviewers to adjust their scores.  The authors did not provide a revision based on the reviewer comments.

Overall, the reviewers found the problem statement to be interesting.  However, they had concerns about the following:
1. It's unclear what is the main technical contribution of the work. 
Several of the reviewers pointed out the lack of technical novelty.  From the writing, it's unclear if the proposed spectral feature representation is taken directly from prior work or there was some additional innovation in this submission.  Based on the author response, it seems the proposed feature representation is taken directly from prior work as the authors themselves acknowledge that the submission is taking two known ideas and combining them.  This can be made more explicit in the paper itself.  

2. Lack of comparison with existing work and experimental analysis
There is no comparison against existing work on predicting 3D structure deformation over time. While the proposed representation is interesting, the is no comparison with other methods or other alternative representations.  Without any comparisons it is difficult to judge how the reconstruction error corresponding to actual reconstruction quality.  How much error is acceptable?  The submissions also fails to elucidate when the proposed representation should be used.  Is it better than alternative representations (use 3D mesh directly? use point clouds? use alternate basis functions?) 

3. What is being learned by the model?  
R3 pointed out that the authors mention that the model is trained in just half an hour and questioned whether the dynamics function is trivial to learn and that the only two parts of the 3D structure is analyzed.  The authors responded that the ""coarse"" dynamic is easier to learn than the ""fine"" scale dynamics.  Is what is learned by the model sufficient?  How well would a model that just modeled the car as a rigid object and predicted the position do?  The lack of comparison against baselines and alternative methods/representations makes it difficult to judge usefulness of the representation/approach that is presented.

4. The paper also has minor typos. 
Page 5: ""treat the for beams"" --> ""treat the four beams""
Page 7: ""marrked"" --> ""marked""

Overall the paper addresses a interesting problem domain, and introduces a interesting representation to the ML community, but fails to do a proper experimental analysis showing how the representation compares to alternatives.  Since the paper does not claim the novelty of the representation as its contribution, it is essential that it performs a thorough investigation of the task and perform empirical studies comparing the proposed representation/method against baselines and alternatives.",Paper Decision
cbtsY4Plom,H1gx1CNKPH,Augmenting Transformers with KNN-Based Composite Memory,Reject,"This paper augments transformer encoder-decoder networks architecture with k nearest neighbors to fetch knowledge or information related to the previous conversation, and demonstrates improvements through manual and automated evaluation. Reviewers note the fact that the approach is simple and clean and results in significant improvements, however, the approach is incremental over the previous work (including https://arxiv.org/pdf/1708.07863.pdf). Furthermore, although the authors improved the article in the light of reviewer suggestions (i.e., rushed analysis, not so clear descriptions) and some reviewers increased their scores, none of them actually marked the paper as an accept or a strong accept.",Paper Decision
faLD4p_Kv,SyglyANFDr,SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning,Reject,"This paper proposes a modification of SGD to do distributionally-robust optimization of deep networks.  The main idea is sensible enough, however, the inadequate handling of baselines and relatively toy nature of the experiments means that this paper needs more work to be accepted.",Paper Decision
aIddMG1_KL,S1lyyANYwr,Constrained Markov Decision Processes via Backward Value Functions,Reject,"The paper considers the setting of constrained MDPs and proposes using backward value functions to keep track of the constraints.

All reviewers agreed that the idea of backward value functions is interesting, but there were a few technical concerns raised, and the reviewers remained unconvinced after the rebuttal. In particular, there were doubts whether the method actually makes sense for the considered problem (the backward VF averaging constraints over all trajectories, instead of only considering the current one), and a concern about insufficient baseline comparisons.

I recommend rejection at this time, but encourage the authors to take the feedback into account, make the paper more crisp, and resubmit to a future venue.",Paper Decision
P0xEBTu8KK,S1ly10EKDS,Reanalysis of Variance Reduced Temporal Difference Learning,Accept (Poster),"The paper studies the variance reduced TD algorithm by Konda and Prashanth (2015). The original paper provided a convergence analysis that had some technical issues. This paper provides a new convergence analysis, and shows the advantage of VRTD to vanilla TD in terms of reducing the bias and variance. Several of the five reviewers are expert in this area and all of them are positive about it. Therefore, I recommend acceptance of this work.",Paper Decision
KgiqHQ-CZU,S1lACa4YDS,Meta-Learning for Variational Inference,Reject,"The paper proposes a meta-learning algorithm to learn the divergence measure of variational inference as well as the initialization of the variational parameters (which reduces optimization steps of VI). Improved performance by the learned divergence against hand-designed ones are empirically shown on: Gaussian mixture approximation, Bayesian neural regression, and p-VAE based recommender.
Reviewers initally raised some concerns on hyperparameters selection, weakness of experiments, and motivation for the proposed scheme. The authors responded by adding additional experiments (MNIST) as well as some new sections in their appendix about details of their method or the baselines.
The reviewers greatly appreciated the response and commonly believed that the revised version is significantly improved over the initial draft  and the improvements of the draft. As a result of that, some reviewers increased their scores. However, some of their concerns did not resolve. In particular, R1 questions the impact of the work and importance of learning divergence measure (referring to GAN or VQ-VAE for obtaining realistic samples). Also R1 finds evaluation based on MNIST unsatisfactory, as it is commonly considered as a toy dataset. To motivate the method, it is suggested that the authors think about real applications which can highlight the benefits of their method in practice. Similar concerns are shared by R2 after authors' response. In particular, R2 is not convinced about motivation and the necessity of using meta-learning for learning the divergence. I suggest authors improve on issues around motivation and support the impact of their scheme in a more practical setting.",Paper Decision
5Buy44OZ_2,Hye00pVtPS,CONFEDERATED MACHINE LEARNING ON HORIZONTALLY AND VERTICALLY SEPARATED MEDICAL DATA FOR LARGE-SCALE HEALTH SYSTEM INTELLIGENCE,Reject,"This manuscript proposes a strategy for fitting predictive models on data separated across nodes, with respect to both samples and features.

The reviewers and AC agree that the problem studied is timely and interesting, and were impressed by the size and scope of the evaluation dataset (particularly for a medical application). However, reviewers were unconvinced about the novelty and clarity of the conceptual and empirical results. On the conceptual end, the AC also suggests that the authors look into closely related work on split learning (https://splitlearning.github.io/) which has also been applied to medical data settings.",Paper Decision
Gri2RAt5tN,BygpAp4Ywr,Defending Against Adversarial Examples by Regularized Deep Embedding,Reject,"The paper suggests a new way to defend against adversarial attacks on neural networks. Two of the reviewers were negative, one of them (the most experienced in the subarea) strongly negative. One reviewer is weakly positive. The main two concerns of the reviewers are insufficient comparisons with SOTA and lack of clarity. The authors' response, though detailed, has not convinced the reviewers and has not alleviated their concerns. 
",Paper Decision
4kE-ALhpG,SygpC6Ntvr,Minimizing FLOPs to Learn Efficient Sparse Representations,Accept (Poster),"This paper studies methods for using weight sparsification to reduce the computational load of network inference.  While there is not absolute consensus on whether this paper should be accepted, one of the main criticisms of this paper is that sparse compute is not always realistic or efficient on a GPU.  While this may be true of the current SOTA in hardware, emerging computing platforms and CPU libraries may handle sparse networks quite well.  For this reason, I am willing to down-weight this criticism. Based on the remaining comments, this paper has the merit to be accepted, even if it is a bit forward looking in terms of the hardware platforms it targets.

",Paper Decision
ITNJKX0OLk,S1gTAp4FDB,Neural-Guided Symbolic Regression with Asymptotic Constraints,Reject,"This paper proposes 1) using neural-guided Monte-Carlo Tree Search to search for expressions that match a dataset and 2) Augments the loss to match the asymptotics of the true function when these are given.

The use of MCTS sounds more sensible than standard evolutionary search.  The augmented loss could make sense but seems extremely niche, requiring specific side information about the problem being solved.

Overall, the task is so niche that I don't think it'll be of wide interest.  It's not clear that it's solving a real problem.",Paper Decision
GVE5FVawo,HJg3Rp4FwH,Policy Optimization In the Face of Uncertainty,Reject,"The main contribution of this work is introducing the uncertainty-aware value function prediction into model-based RL, which can be used to balance the risk and return empirically. 

The reviewers generally agree that this paper addresses an interesting problem, but there are some concerns that remain (see reviewer comments). 

I also want to highlight that in terms of empirical results, it is insufficient to present results for 3 different random seeds. To highlight any kind of robustness, I suggest *at least* 10-20 different random seeds; otherwise the findings can/will be misleading. ",Paper Decision
dUxCoAxeOS,B1gi0TEFDB,Understanding Top-k Sparsification in Distributed Deep Learning,Reject,"This paper investigates gradient sparsification using top-k for distributed training. Starting with empirical studies, the authors propose a distribution for the gradient values, which is used to derive bounds on the top-k sparsification. The top-k approach is further improved using a procedure that is easier to parallelize.

The reviewers and AC agree that the problem studied is timely and interesting. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the rigor and novelty of the results, and perhaps issues with unstated assumptions. In reviews and discussion, the reviewers also noted issues with clarity of the presentation, some of which were corrected after rebuttal. In the opinion of the AC, the manuscript is not appropriate for publication in its current state. ",Paper Decision
XEA_rN2DDE,Byes0TNFDS,Entropy Penalty: Towards Generalization Beyond the IID Assumption,Reject,"The paper proposes an entropy penalty related to information bottleneck to deep neural network regression problems. The reviewers had a number of questions and concerns about the paper, which the authors did not address. In light of this, the reviewers agree that the paper is not yet ready for publication. Please carefully read and address the reviewer's concerns in future iterations of this paper.",Paper Decision
_43qIGz_8B,rkgc06VtwH,Improving Semantic Parsing with Neural Generator-Reranker Architecture,Reject,"This paper presents and evaluates a technique for semantic parsing, and in particular proposes a model to re-rank the candidates generated by beam search. The paper was reviewed by 3 experts and received Reject, Weak Reject, and Weak Reject opinions. The reviews identified strengths of the paper but also significant concerns, mostly centered around the experimental evaluation (including choice of datasets, lack of direct comparison to baselines, need for more methodical and quantitative analysis, need for additional analysis, etc.) and some questions about the design of the technical approach. The authors submitted responses that addressed some of these concerns, but indicated that additional experimentation would be needed to address all of them. In light of these reviews, we are not able to recommend acceptance at this time, but I hope authors use the detailed, constructive feedback to improve the paper for another venue.",Paper Decision
LtGgnFsDTH,S1xqRTNtDr,Learning a Behavioral Repertoire from Demonstrations,Reject,"This paper proposes a way to lean context-dependent policies from demonstrations, where the context represents behavior labels obtained by annotating demonstrations with differences in behavior across dimensions and the reduced in 2 dimensions. Results are conducted in the domain of StarCraft. The main concerns from the reviewers related to the paper’s novelty (as pointed by R2) and experiments (particularly the lack of comparison with other methods and the evaluation of only 4 out of the 62 behaviour clusters, as pointed by R3). As such, I cannot recommend acceptance, as current results do not provide strong empirical evidence about the superiority of the method against other alternatives.",Paper Decision
AxUfguy4mC,BkeqATVYwr,GRAPH NEIGHBORHOOD ATTENTIVE POOLING,Reject,All three reviewers are consistently negative on this paper. Thus a reject is recommended.,Paper Decision
NKFxMJSKH,S1gKA6NtPS,Deep symbolic regression,Reject,"This paper suggests using RNN and policy gradient methods for improving symbolic regression. The reviewers could not reach a consensus, and due to concerns about the clarity of the paper and the extensiveness of the experimental results, the paper does not appear to currently meet the level of publication. 

Also, while not mentioned in the reviews, there appears to be some work on symbolic regression aided by deep learning, (see for example, https://twhughes.github.io/pdfs/cs221_final.pdf, which was found by searching ""symbolic regression deep learning"")---I would thus also recommend the authors do a more thorough literature search for future revisions. ",Paper Decision
6n_Oms4iui,ryxtCpNtDS,Autoencoders and Generative Adversarial Networks for Imbalanced Sequence Classification,Reject,"This paper presents a synthetic oversampling method for sequence-to-sequence classification problems based on autoencoders and generative adversarial networks. 

All reviews reject the paper for two main reasons:
1 The novelty of the paper is not enough for ICLR as the idea of utilizing GAN for data sampling is common now.
2 The experimental is not convincing as authors did not compare with other leading oversampling methods.

The rebuttal did not well answer these two questions; thus I choose to reject the paper.
",Paper Decision
jlAm0R8rA,SyxdC6NKwH,Uncertainty-Aware Prediction for Graph Neural Networks,Reject,"The authors propose a way to produce uncertainty measures in graph neural networks. However, the reviewers find that the methods proposed lack novelty and are incremental additions to prior work.",Paper Decision
5arbrXnfUI,HJlPC6NKDH,Training Deep Neural Networks by optimizing over nonlocal paths in hyperparameter space,Reject,"This paper uses a variant of parallel tempering to tune the subset of neural net hyperparameters which control the amount of noise and/or rate of diffusion (e.g. learning rate, batch size). It's certainly an appealing idea to run multiple chains in parallel and periodically propose swaps between them. However, I'm not persuaded about the details. The argumentation in the paper is fairly informal, and it uses ideas from optimization and MCMC somewhat interchangeably. Since the individual chains aren't sampling from any known stationary distribution, it's not clear to me what MH-based swaps will achieve. 

The authors are upset with one of the reviews and think it misrepresents their paper. However, I find myself agreeing with most of the reviewer's points. Furthermore, as a general principle, the availability of code doesn't by itself make a paper reproducible. One should be able to reproduce it without the code, and one shouldn't need to refer to the code for important details about the algorithm.

Another limitation (pointed out by various reviewers) is that there aren't any comparisons against prior work on hyperparameter optimization. Overall, I think there are some promising and appealing ideas in this submission, but it needs to be cleaned up before it's ready for publication at ICLR.
",Paper Decision
iqI1GsjMaB,rJlwAa4YwS,Lattice Representation Learning,Reject,"This paper presents a new view of latent variable learning as learning lattice representations.

Overall, the reviewers thought the underlying ideas were interesting, but both the description and the experimentation in the paper were not quite sufficient at this time. I'd encourage the authors to continue on this path and take into account the extensive review feedback in improving the paper!",Paper Decision
UZSQLxHM-z,ryxPRpEtvH,Omnibus Dropout for Improving The Probabilistic Classification Outputs of ConvNets,Reject,"The paper investigates how to improve the performance of dropout and proposes an omnibus dropout strategy to reduce the correlation between the individual models.

All the reviewers felt that the paper requires more work before it can be accepted. In particular, the reviewers raised several concerns about novelty of the method relative to existing methods, significance of performance improvements and clarity of the presentation. 

I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue.
",Paper Decision
Iwo5nTEwdh,HJl8AaVFwB,Deep Multiple Instance Learning for Taxonomic Classification of Metagenomic read sets,Reject,"The work proposes a modification to existing architectures applied to predict taxonomic labels from metagenomic sequences. Reviewers agreed that the problem was well motivated, but that current experiments lack comparisons with existing standard baselines in the area. I recommend the authors update their work to included the additional experiments suggested by the reviewers.",Paper Decision
13QDi3MegV,HyxLRTVKPH,Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints,Accept (Poster),"This paper formalizes the problem of training deep networks in the presence of a budget, expressed here as a maximum total number of optimization iterations, and evaluates various budget-aware learning schedules, finding simple linear decay to work well. 

Post-discussion, the reviewers all felt that this was a good paper. There were some concerns about the lack of theoretical justification for linear decay, but these were overruled by the practical use of these papers to the community. Therefore I am recommending it be accepted.",Paper Decision
9EG0VM0Gf,SyxS0T4tvS,RoBERTa: A Robustly Optimized BERT Pretraining Approach,Reject,"This paper conducts an extensive study of training BERT and shows that its performance can be improved significantly by choosing a better training setup (e.g., hyperparameters, objective functions). I think this paper clearly offers a better understanding of the importance of tuning a language model to get the best performance on downstream tasks. However, most of the findings are obvious (careful tuning helps, more data helps). I think the novelty and technical contributions are rather limited for a conference such as ICLR. These concerns are also shared by all the reviewers. The review scores are borderline, so I recommend to reject the paper.",Paper Decision
T8O4Vdqhx1,HkgH0TEYwH,Deep Semi-Supervised Anomaly Detection,Accept (Poster),"Issues raised by the reviewers have been addressed by the authors, and thus I suggest the acceptance of this paper.",Paper Decision
WB_0syHnz,HJe4Cp4KwH,GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation,Reject,"While considerable effort went into improving the paper during the author response period, the concerns outlined by reviewer 2 remain and the aggregate score across reviewers reflects this issue. The AC recommends rejection with strong encouragement to resubmit this work to another high quality venue upon further revision to the work.",Paper Decision
e8fjNf8K9_,BygNAa4YPH,Out-of-distribution Detection in Few-shot Classification,Reject,"This paper presents a method for out-of-distribution detection under the condition of access to only a few positive labeled samples. The main contribution as summarized by reviewers and authors is the new proposed benchmark and problem statement. 

All reviewers are in agreement that this paper is not ready for publication in its current form. The main concern is around the validity of the problem statement. The reviewers seek more clarity motivating the proposed scenario. Though the authors argue that as few-shot recognition is very difficult and may benefit from strategies like active learning, it is not directly clear how out of distribution detection is the best approach. In addition, R3 seeks clarification on the similarity to existing work. 

Considering the unanimous opinions of the reviewers and all author rebuttal text, the AC does not recommend acceptance of this work. We encourage the authors to focus their revisions on the explanation and motivation of this new benchmark and submit to a future venue. 
",Paper Decision
HmFOnYDNdx,HkxQRTNYPH,Mirror-Generative Neural Machine Translation,Accept (Talk),"This paper proposes a novel method for considering translations in both directions within the framework of generative neural machine translation, significantly improving accuracy.

All three reviewers appreciated the paper, although they noted that the gains were somewhat small for the increased complexity of the model. Nonetheless, the baselines presented are already quite competitive, so improvements on these datasets are likely to never be extremely large.

Overall, I found this to be a quite nice paper, and strongly recommend acceptance, perhaps as an oral presentation.",Paper Decision
LF1xdc47TQ,H1lXCaVKvS,Frustratingly easy quasi-multitask learning,Reject,"One of the reviewers pointed out similarity to existing very recent work which would require significant reframing of the current paper. Hence, this work is below the bar at the moment.",Paper Decision
60weOraUqt,S1efAp4YvB,Interpreting video features: a comparison of 3D convolutional networks and convolutional LSTM networks,Reject,"The paper addresses interpretability in the video data domain. The authors study and compare the saliency maps for 3D CNNs and convolutional LSTMs networks, analysing what they learn, and how do they differ from one another when capturing temporal information. To search for the most informative part in a video sequence, the authors propose to adapt the meaningful perturbations approach by Fong & Vedaldi (2017) to the video domain using temporal mask perturbations. 
While all reviewers and AC acknowledge the importance and potential usefulness of studying and comparing different generative models in continual learning, they raised several important concerns that place this paper below the acceptance bar: 
(1) in an empirical study paper, an in-depth analysis and insightful evaluations are required to better understand the benefits and shortcomings of the available and proposed models (R5 and R2). Specifically: 
(i) providing a baseline comparison to assess the benefits of the proposed approach -- please see R5’s suggestions on the baseline methods; 
(ii) analyzing how the proposed approach can elucidate meaningful differences between 3D CNNs and LSTMs (R5, R2). The authors discussed in their rebuttal some of these questions, but a more detailed analysis is required to fully understand the benefits of this study. 
(2) R5 and R2 raised an important concern that the temporal mask generation developed in this work is grounded on the generation of the spatial masks, which is counterintuitive when analysing the temporal dynamics of the NNs - see R5’s suggestions on how to improve. 
Also R5 has raised concerns regarding the qualitative analysis of the Grad-CAM visualizations. Happy to report that the authors have addressed these concerns in the rebuttal, namely reporting the results in Table 2 and providing an updated discussion. R1 has raised a concern about the importance of the sub-sampling in the CNN framework, which was partially addressed in the rebuttal. 
To conclude, the AC suggest that in its current state the manuscript is not ready for a publication and needs a major revision before submitting for another round of reviews. We hope the reviews are useful for improving and revising the paper.
",Paper Decision
D88GreM8KY,BJeGA6VtPS,TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks,Reject,"This paper presents a very creative threat model for neural networks.  The proposed attack requires systems-level intervention by the attacker, which prompts the reviewers to question how realistic the attack is, and whether it is well motivated by the authors.  After conversing with the reviewers on this topic, they have not changed their mind about these issues.  As an AC, I think the threat model is both interesting and potentially realistic in some scenarios, however I agree with the reviewers that the motivation for the threat model could be more powerful.  For example the authors could focus more on realistic types of malicious behaviors that a developer could embed into a neural network.  I also think there's lots of opportunities for a range of applications that exploit the type of ""two nets in one"" behavior that the authors study.  Despite the interesting ideas in this paper, the post-rebuttal scores are not strong enough to accept it.  I encourage the authors to address some of these presentation issues, and resubmit this interesting paper to another venue.",Paper Decision
a0Ez9m5PZ,ryl-RTEYvB,Robust Learning with Jacobian Regularization,Reject,"Three reviewers have reviewed this submission and scored it as 6/3/3. After rebuttal, the reviewers remained unconvinced. The main criticisms concerns the Jacobian  regularizaton [1] being known which makes the contributions of this submission  look diluted. Additionally, there were concerns over results (degradation) on CIFAR10 and ImageNet and other minor issues.
For these reasons, this paper cannot be accepted by ICLR2020.",Paper Decision
ir875TV2Bj,BygWRaVYwH,Generalized Inner Loop Meta-Learning,Reject,"The reviewers agree that the technical innovations presented in this paper are not great enough to justify acceptance.  The authors correctly point out to the reviewers that the ICLR CFP states that the topics of ""implementation issues, parallelization, software platforms, hardware” are acceptable.  I would point out that most papers in these spaces describe *technical innovations* that enable improvements in ""parallelization, software platforms, hardware"" rather than implementations of these improvements.   However, it is certainly true that a software package is an acceptable (although less common) basis for a publication, provided is it sufficiently unique and impactful.  After pointing this out to the reviewers and collecting opinions, the reviewers do not feel the combined technical and software contributions of this paper are enough to justify acceptance. 
 ",Paper Decision
bnS--LV1x,SygW0TEFwH,Sign Bits Are All You Need for Black-Box Attacks,Accept (Poster),"This paper presents a novel black-box adversarial attack algorithm, which exploits a sign-based rather than magnitude-based, gradient estimator for black-box optimization. It also adaptively constructs queries to estimate the gradient. The proposed approach outperforms many state-of-the-art black-box attack methods in terms of  query complexity. There is a unanimous agreement to accept this paper.",Paper Decision
Z1n_2e_E2G,B1elCp4KwH,Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech,Accept (Talk),"The paper is extremely well-written with a clear motivation (Section 1). The approach is novel. But I think the paper's biggest strength is in its very thorough experimental investigation. Their approach is compared to other very recent speech discretization methods on the same data using the same (ABX) evaluation metric. But the work goes further in that it systematically attempts to actually understand what types of structures are captured in the intermediate discrete layers, and it is able to answer this question convincingly. Finally, very good results on standard benchmarks are achieved.

To authors: Please do include the additional discussions and results in the final paper. ",Paper Decision
QQ9iYQvTSP,S1elRa4twS,Pre-training as Batch Meta Reinforcement Learning with tiMe ,Reject,"The reviewers reached a unanimous consensus that the paper could not be accepted for publication in its current form. There were a number of concerns raised regarding (1) the clarity of the writing; (2) the comparisons, especially to prior work; (3) the details of the experimental setup.",Paper Decision
tdWJ7FsJOs,SkgC6TNFvr,Reinforced active learning for image segmentation,Accept (Poster),"Authors propose a novel scheme to perform active learning on image segmentation. This structured task is highly time consuming for humans to perform and challenging to model theoretically as to potentially apply existing active learning methods. Reviewers have remaining concerns over computation and that the empirical evaluation is not overwhelming (e.g., more comparisons). Nevertheless, the paper appears to bring new ideas to the table for this important problem.    ",Paper Decision
xin4glD4Xh,SklR6aEtwH,Neural Architecture Search by Learning Action Space for Monte Carlo Tree Search,Reject,"This paper proposes an MCTS method for neural architecture search (NAS). Evaluations on NAS-Bench-101 and other datasets are promising. Unfortunately, no code is provided, which is very important in NAS to overcome the reproducibility crisis.

Discussion:
The authors were able to answer several questions of the reviewers. I also do not share the concern of AnonReviewer2 that MCTS hasn't been used for NAS before; in contrast, this appears to be a point in favor of the paper's novelty. However, the authors' reply concerning Bayesian optimization and the optimization of its acquisition function is strange: using the ConvNet-60K dataset with 1364 networks, it does not appear to make sense to use only 1% or even only 0.01% of the dataset size as a budget for optimizing the acquisition function. The reviewers stuck to their rating of 6,3,3.

Overall, I therefore recommend rejection. ",Paper Decision
9imcaj1Tnn,Skep6TVYDB,Gradientless Descent: High-Dimensional Zeroth-Order Optimization,Accept (Spotlight),The paper considers an interesting algorithm on zeorth-order optimization and contains strong theory. All the reviewers agree to accept.,Paper Decision
s6RF4Keewg,Hkx6p6EFDr,Equivariant Entity-Relationship Networks,Reject,"This paper defines a parameter-tying scheme for a general feed-forward network with the equivalence properties of relational data. Most reviewers raised a few concerns around the experiments, baselines, datasets used and motivation. A few pointed out that the paper is hard to read - for a person without heavy database theory literature, which includes most of ICLR readers. While this paper may read well for the folks in the domain, authors should consider revising the paper to be more inclusive so that it can be read more widely. The motivation of the problem was also another point that many reviewers have mentioned (perhaps related to the language issues above) that some noted that you may not always want equivariance in relational DB and other noted that the paper did not sufficiently demonstrate the advantage of the proposed methods. Reviewers also univocally commented on experiments - many voiced the lack of baselines (not even any simple one). Authors wrote back to defend that there is no similar method and even simple tensor factorization isn’t applicable. That makes me wonder - is there really no single simple method you can compare with? If nobody had solution for this problem, is this a problem worth solving? Reviewers also encouraged to use larger (beyond Kaggle dataset) real-world datasets to strengthen the paper. All the points raised by reviewers suggests that this paper can benefit from another round of nontrivial editing before it’s ready for the show.
",Paper Decision
dT3Toc3NC,B1xhpa4FvS,Modeling Fake News in Social Networks with Deep Multi-Agent Reinforcement Learning,Reject,"The paper aims to model fake news by drawing tools from multi-agent reinforcement learning. After the discussion period, there is a consensus among the reviewers that the paper lacks novel technical contributions. The reviewers also acknowledge that paper also doesn't quite deliver a practical solution as claimed by the authors.",Paper Decision
IAJ-4bfgVy,HylsTT4FvB,"On the ""steerability"" of generative adversarial networks",Accept (Poster),All three reviewers agree that the paper provide an interesting study on the ability of generative adversarial networks to model geometric transformations and a simple practical approach to how such ability can be improved. Acceptance as a poster is recommended.,Paper Decision
KO_ZxVfAjv,HylcapVtvB,Improving Differentially Private Models with Active Learning,Reject,"This paper provides an active-learning approach to improve the performance of an existing differentially private classifier with public labeled data. Where the paper provides a new approach, there is a consensus among the reviewers that the paper does not provide a strong enough contribution for acceptance. The authors can potentially improve the submission by including a more comprehensive comparison with the PATE framework and improving its overall presentation.",Paper Decision
scY8IKeJyh,Hye5TaVtDH,Matrix Multilayer Perceptron,Reject,"This paper introduces a novel architecture and loss for estimating PSD matrices using neural networks.  There is some theoretical justification for the architecture, and a small-scale but encouraging experiment.

Overall, I think there is a sensible contribution here, but there are so many architectural and computational choices presented together at once that it's hard to tell what the important parts are.

The main problems with this paper are:
1) The scalability of the approach O(N^3)
2) The derivation of the architecture and gradient computations wasn't clear about what choices were available and why.  Several alternative choices were mentioned but not evaluated.  I think the authors also need to improve their understanding of automatic differentiation.  Backprop through eigendecomposition is already available in most autodiff packages.  It was claimed that a certain kind of matrix derivative provided better generalization, which seems like a strong claim to make in general.
3) The experimental setup seemed contrived, except for the heteroskedastic regression experiments, which lacked competitive baselines.  Why were the GP and MLPs homoskedastic?

As a matter of personal preference, I found that having 4 different ""H""s differing only in font and capitalization for the network architecture was hard to keep track of.

I agree that R1 had some unjustified comments and R2's review was contentless.  I apologize for these inadequate reviews. ",Paper Decision
0pqkdyrXlq,rJxFpp4Fvr,"Feature-Robustness, Flatness and Generalization Error for Deep Neural Networks",Reject,"The authors propose a notion of feature robustness, provide a straightforward decomposition of risk in terms of this robustness measure, and then provide some empirical evidence for their perspective. Across the board, the reviewers raised issues with missing related work, which the authors then addressed. I will point out that some things the authors say about PAC-Bayes are false. E.g., in the rebuttal the authors say that PAC-Bayes is limited to 0-1 error. It is generally trivial to obtain bounds for bounded loss. For unbounded loss functions, there are bounds based on, e.g., sub gaussian assumptions. 

Despite improvements in connections with related work, reviewers continued to find the theoretical contributions to be marginal. Even the empirical contributions were found to be marginal.",Paper Decision
JgZ_IL9Gvu,BkeOp6EKDH,TriMap: Large-scale Dimensionality Reduction Using Triplets,Reject,"This paper proposes a new dimensionality reduction technique that tries to preserve the global structure of the data as measured by the relative distances between triplets. As Reviewer 1 noted, the construction of the TriMap algorithm is fairly heuristic, making it difficult to determine how TriMap ought to behave “better” than existing dimensionality reduction approaches other than through qualitative assessment. Here, I share Reviewer 2’s concern that the qualitative behavior of TriMap is difficult to distinguish from existing methods in many of the figures. 
",Paper Decision
uzs-mPa4_,rkgO66VKDS,LEARNED STEP SIZE QUANTIZATION,Accept (Poster),"Main content: Paper is about training low precision networks to a high-accuracy.

Discussion:
reviewer 2: impressive results, main questions are around some clarity in the experiments tried, but sounds like authors addressed most of this in rebuttal.
reviewer 1: well written paper, but authors think some technical details could be clarified. 
reviewer 3:  well written but experimental section could be improved.
Recommendation: all reviewers are in consensus, well written paper but some experiments/technical details could be improved. i vote poster.",Paper Decision
rDiH2krpwA,H1gDaa4YwS,Learning General and Reusable Features via Racecar-Training,Reject,Both reviewers (we apologize for the lack of a 3rd review) did not feel the paper should be accepted. The rebuttal offered did not change the reviewer scores. So the paper cannot be accepted unfortunately. But the authors should use the feedback to improve their paper and resubmit.,Paper Decision
-9DJ8Apjn,BJg866NFvB,Estimating counterfactual treatment outcomes over time through adversarially balanced representations,Accept (Spotlight),Reviewers uniformly suggest acceptance. Please look carefully at reviewer comments and address in the camera-ready. Great work!,Paper Decision
J7y9cWT3FY,BJgLpaEtDS,Poincaré Wasserstein Autoencoder,Reject,"The paper received 3, 3, 6. All reviewers agree that the method is technically interesting. The main concern shared by the reviewers are the experiments which are somewhat underwhelming. The AC believes that this is a solid technical paper that needs a little bit more work. The authors are encouraged to strengthen their evaluation and resubmit to a future conference.",Paper Decision
MfoSlTdAxB,rklraTNFwB,Robust Instruction-Following in a Situated Agent via Transfer-Learning from Text,Reject,"The paper examines whether it is possible to train agents to follow synthetic instructions that perceives and modifies a 3D scene based on a first-person viewpoint, and have the trained agents follow natural language instructions provided by humans.

The paper received two weak rejects and one weak accept.  The main concerns voiced by the reviewers are:
1. Lack of variety in natural language
One of the key claims of the paper is that previous work on instruction following can only handle instructions generated from templates and cannot handle ambiguous expressions used by real people, and that the contribution of this work is that it can handle such expresssions.  However, as pointed out by R1, the language considered in this work is very simplistic in form (close to being template based) with the main variation coming from synonyms.  Even the free-form natural instructions that are collected, are done so with very specific instructions that restrict diversity of language (e.g don't use colors or other properties of the object). R1 also point out that there are prior work that handles much more diverse language.

2. Limited technical novelty and questions about how much the proposed CMSA method actually contribute

3. Overclaims and lack of precision when using terminology
There is concern that the task that is addressed is not actually that complex.  The environments are simple (with just 2 objects) and not that realistic.  Tackling 2 tasks is barely ""multi-task"", and commonly, ""manipulation"" refers to low-level grasping/picking up of objects which is not how it is used here.

While the paper has many strong elements and is mostly well written, considerable improvements still need to be made for the paper to have claims it can support.  It is currently below the bar for acceptance. The authors are encouraged to improve their paper and resubmit to an appropriate venue.
",Paper Decision
9RpaSZI3D8,S1lSapVtwS,Stochastic Conditional Generative Networks with Basis Decomposition,Accept (Poster),"Main content: BasiGAN, a novel method for  introducing stochasticity in conditional GANs
Summary of discussion:
reviewer1: interesting work and results on GANs. Reviewer had a question on pre-defned basis but i think it was answered by the authors. 
reviewer3: interesting and novel work on GANS, wel-written paper and improves on SOTA. The main uestion is around bases again like reviewer 1, but it seems the authors have addressed this.
reviewer4: Novel interesting work. Main comments are around making Theorem 1 more theoretically correct, which it sounds like the authors addressed.
Recommendation: Poster. Well written and novel paper and authors addressed a lot of concerns. ",Paper Decision
aDpW1npAHW,BklBp6EYvB,Task-Based Top-Down Modulation Network for Multi-Task-Learning Applications,Reject,"The paper is interested in multi-task learning. It introduces a new architecture which condition the model in a particular manner: images features and task ID features are fed to a top-down network which generates task-specific weights, which are then used in a bottom-up network to produce final labels. The paper is experimental, and the contribution rather incremental, considering existing work in the area. Experimental section is currently not convincing enough, given marginal improvements over existing approaches - multiple runs as well as confidence intervals would help in that respect.
",Paper Decision
tlATYHK8aq,rylVTTVtvH,Tensor Graph Convolutional Networks for Prediction on Dynamic Graphs,Reject,"The paper proposes a tensor-based extension to graph convolutional networks for prediction over dynamic graphs. 

The proposed model is reasonable and achieves promising empirical results. After discussion, it is agreed that while the problem of handling dynamic graphs is interesting and challenging, the proposed tensor method lacks novelty, the theoretical analysis is artificial, and the empirical study does not cover enough benchmarks. 

The current version of the paper is not ready for publication. Addressing the issues above could lead to a strong publication in the future. ",Paper Decision
rcXTG5A1P,ryxQ6T4YwB,GraphNVP: an Invertible Flow-based Model for Generating Molecular Graphs,Reject,"The authors propose an invertible flow-based model for molecular graph generation. The reviewers like the idea but have several concerns: in particular, overfitting in the model, need for more experiments and missing related work. It is important for authors to address them in a future submission",Paper Decision
qTwCNtNzYD,BJgza6VtPB,Language GANs Falling Short,Accept (Poster),"Main content:

Blind review #1 summarizes it well:

Recently many language GAN papers have been published to overcome the so called exposure bias, and demonstrated improvements  in natural language generation in terms of sample quality, some works propose to assess the generation in terms of diversity, however, quality and diversity are two conflicting measures that are hard to meet. This paper is a groundbreaking work that proposes receiver operating curve or Pareto optimality for quality and diversity measures, and shows that simple temperature sweeping in MLE generates the best quality-diversity curves than all language GAN models through comprehensive experiments. It points out a good target that language GANs should aims at. 

--

Discussion:

The main reservation was the originality of the idea of using temperature sweep in the softmax. However, it turns out this idea came from the authors in the first place, which they have not been able to state directly due to the anonymity requirement. Per the program chair's instruction to direct this to the area chair, I think this has been handled correctly.

--

Recommendation and justification:

This paper should be accepted. It provides readers with insight in that it illuminates a misconception of how important exposure bias has been assumed to be, and provides a less expensive MLE based way to train than GAN counterparts.",Paper Decision
AwBH-h5xE,BkxfaTVFwH,GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations,Accept (Poster),"This paper offers a new method for scene generation.  While there is some debate on the semantics of ‘generative’ and ‘3d’, on balance the reviewers were positive and more so after rebuttal.  I concur with their view that this paper deserves to be accepted.",Paper Decision
N1DJKizW8O,SylGpT4FPS,Last-iterate convergence rates for min-max optimization,Reject,"This provides a simple analysis of an existing algorithm for min-max optimization under some favorable assumptions.  The paper is clean and nice, though unfortunately lands just below borderline.

I urge the authors to continue their interesting work, and amongst other things address the reviewer comments, for example those on stochastic gradient descent. ",Paper Decision
8CVG2PiTZf,Bke-6pVKvB,Poisoning Attacks with Generative Adversarial Nets,Reject,"This paper proposes a GAN-based approach to producing poisons for neural networks.  While the approach is interesting and appreciated by the reviewers, it is a legitimate and recurring criticism that the method is only demonstrated on very toy problems (MNIST and Fashion MNIST).  During the rebuttal stage, the authors added results on CIFAR, although the results on CIFAR were not convincing enough to change the reviewer scores; the SOTA in GANs is sufficient to generate realistic images of cars and trucks (even at the ImageNet scale), while the demonstrated images are sufficiently far from the natural image distribution on CIFAR-10 that it is not clear whether the method benefits from using a GAN.   It should be noted that a range of poisoning methods exist that can effectively target CIFAR, and SOTA methods (e.g., poison polytope attacks and backdoor attacks) can even target datasets like ImageNet and CelebA.",Paper Decision
9WJxoyJ3gt,HJgepaNtDS,Learnable Group Transform For Time-Series,Reject,"This paper received two weak rejects (3) and one accept (8).  In the discussion phase, the paper received significant discussion between the authors and reviewers and internally between the reviewers (which is tremendously appreciated).  In particular, there was a discussion about the novelty of the contribution and ideas (AnonReviewer3 felt that the ideas presented provided an interesting new thought-provoking perspective) and the strength of the empirical results.  None of the reviewers felt really strongly about rejecting and would not argue strongly against acceptance.   However, AnonReviewer3 was not prepared to really champion the paper for acceptance due to a lack of confidence.  Unfortunately, the paper falls just below the bar for acceptance.  Taking the reviewer feedback into account and adding careful new experiments with strong results would make this a much stronger paper for a future submission.",Paper Decision
4fMleifPuQ,Bkle6T4YvB,From English to Foreign Languages: Transferring Pre-trained Language Models,Reject,"This paper proposes a method to transfer a pretrained language model in one language (English) to a new language. The method first learns word embeddings for the new language while keeping the the body of the English model fixed, and further refines it in a fine-tuning procedure as a bilingual model. Experiments on XNLI and dependency parsing demonstrate the benefit of the proposed approach.

R3 pointed out that the paper is missing an important baseline, which is a bilingual BERT model. The authors acknowledged this in their rebuttal and ran a preliminary experiment to obtain a first set of results. However, since the main claim of the paper depends on this new experiment, which was not finished by the end of the rebuttal period, it is difficult to accept the paper in its current state. In an internal discussion, R1 also agreed that this baseline is critical to support the paper.

As a result, I recommend to reject this paper for ICLR. I encourage the authors to update their paper with the new experiment for submission to future conferences (given consistent results).",Paper Decision
7HUOXMKov,SkeyppEFvS,CoPhy: Counterfactual Learning of Physical Dynamics,Accept (Spotlight),The reviewers are unanimous in their opinion that this paper offers a novel approach to learning naïve physics.  I concur.,Paper Decision
aUzptc1sf,Bygka64KPH,Semi-Supervised Few-Shot Learning with Prototypical Random Walks,Reject,"This paper proposed a semi-supervised few-shot learning method, on top of Prototypical Networks, wherein a regularization term that involves a random walk from a prototype to unlabeled samples and back to the same prototype.  SotA results were obtained in several experiments by using this method.  All reviewers agreed that the novelty of the paper is not such high compared with Haeusser et al. (2017) and the analysis and the experiments could be improved.",Paper Decision
SlCZeX6Ydy,S1ekaT4tDB,Why Convolutional Networks Learn Oriented Bandpass Filters: A Hypothesis,Reject,"This paper proposes an alternative explanation of the emergence of oriented bandpass filters in convolutional networks: rather than reflecting observed structure in images, these filters would be a consequence of the convolutional architecture itself and its eigenfunctions. 
Reviewers agree that the mathematical angle taken by the paper is interesting, however they also point out that crucial prior work making the same points exists, and that more thorough insights and analyses would be needed to make a more solid paper.
Given the closeness to prior work, we cannot recommend acceptance in this form.",Paper Decision
eGcPQTXG-5,B1lCn64tvS,Improving SAT Solver Heuristics with Graph Networks and Reinforcement Learning,Reject,"SAT is NP-complete (Karp, 1972) due its intractable exhaustive search. As such, heuristics are commonly used to reduce the search space. While usually these heuristics rely on some in-domain expert knowledge, the authors propose a generic method that uses RL to learn a branching heuristic. The policy is parametrized by a GNN, and at each step selects a variable to expand and the process repeats until either a satisfying assignment has been found or the problem has been proved unsatisfiable. The main result of this is that the proposed heuristic results in fewer steps than VSIDS,  a commonly used heuristic. 

All reviewers agreed that this is an interesting and well-presented submission. However, both R1 and R2 (rightly according to my judgment) point that at the moment the paper seems to be conducting an evaluation that is not entirely fair. Specifically, VSIDS has been implemented within a framework optimized for running time rather than number of iterations, whereas the proposed heuristic is doing the opposite. Moreover, the proposed heuristic is not stressed-test against larger datasets. So, the authors take a heuristic/framework that has been optimized to operate specifically well on large datasets (where running time is what ultimately makes the difference) scale it down to a smaller dataset and evaluate it on a metric that the proposed algorithm is optimized for. At the same time, they do not consider evaluation in larger datasets and defer all concerns about scalability to the one of industrial use vs answering ML questions related to whether or not it is possible to  “stretch existing RL techniques to learn a branching heuristic”. This is a valid point and not all techniques need to be super scalable from iteration day 0, but this being ML, we need to make sure that our evaluation criteria are fair and that we are comparing apples to apples in testing hypotheses. As such, I do not feel comfortable suggesting acceptance of this submission, but I do sincerely hope the authors will take the reviewers' feedback and improve the evaluation protocols of their manuscript, resulting in a stronger future submission.",Paper Decision
pomwaSThW7,SJeC2TNYwB,Unsupervised Out-of-Distribution Detection with Batch Normalization,Reject,"The authors observe that batch normalization using the statistics computed from a *test* batch significantly improves out-of-distribution detection with generative models.  Essentially, normalizing an OOD test batch using the test batch statistics decreases the likelihood of that batch and thus improves detection of OOD examples.  The reviewers seemed concerned with this setting and they felt that it gives a significant advantage over existing methods since they typically deal with single test example.  The reviewers thus wanted empirical comparisons to methods designed for this setting, i.e. traditional statistical tests for comparing distributions.  Despite some positive discussion, this paper unfortunately falls below the bar for acceptance.  The authors added significant experiments and hopefully adding these and additional analysis providing some insight into how the batchnorm is helping would make for a stronger submission to a future conference.",Paper Decision
3-3e_XkYz,B1x62TNtDS,Understanding the Limitations of Variational Mutual Information Estimators,Accept (Poster),"This paper presents a critical appraisal of variational mutual information estimators, and suggests a slight variance-reducing improvement based on clipping density ratio estimates, and prove that this reduces variance (at the cost of bias). They also propose a set of criteria they term ""self-consistency"" for evaluation of MI estimators and, and show convincingly that variational MI estimators fall short with respect to these.

Reviewers were generally positive about the contribution, and were happy with improvements made. While somewhat limited in scope, I believe this is nonetheless a valuable contribution to the conversation surrounding mutual information objectives that have become popular recently. I therefore recommend acceptance.",Paper Decision
nDzIMR8yGs,S1x63TEYvr,Latent Question Reformulation and Information Accumulation for Multi-Hop Machine Reading,Reject,"This paper proposes a novel approach, Latent Question Reformulation Network (LQR-net), a multi-hop and parallel attentive network designed for question-answering tasks that require multi-hop reasoning capabilities. Experimental results on the HotPotQA dataset achieve competitive results and outperform the top system in terms of exact match and F1 scores. However, reviewers note the limited setting of the experiments on the unrealistic, closed-domain setting of this dataset and suggested experimenting with other data (such as complex WebQuesitons). Reviewers were also concerned about the scalability of the system due to the significant amount of computations. They also noted several previous studies were not included in the paper. Authors acknowledged and made changes according to these suggestions. They also included experiments only on the open-domain subset of the HotPotQA in their rebuttal, unfortunately the results are not as good as before. Hence, I suggest rejecting this paper.",Paper Decision
WTuw-NPOV,HJenn6VFvB,Hamiltonian Generative Networks,Accept (Spotlight),"The paper introduces a novel way of learning Hamiltonian dynamics with a generative network. The Hamiltonian generative network (HGN) learns the dynamics directly from data by embedding observations in a latent space, which is then transformed into a phase space describing the system's initial (abstract) position and momentum. Using a second network, the Hamiltonian network, the position and momentum are reduced to a scalar, interpreted as the Hamiltonian of the system, which can then be used to do rollouts in the phase space using techniques known from, e.g., Hamiltonian Monte Carlo sampling. An important ingredient of the paper is the fact that no access to the derivatives of the Hamiltonian is needed. 

The reviewers agree that this paper is a good contribution, and I recommend acceptance.",Paper Decision
bwYyOVAfnF,Bkln2a4tPB,Customizing Sequence Generation with Multi-Task Dynamical Systems,Reject,"This work proposes a dynamical systems model to allow the user to better control sequence generation via the latent z. Reviewers all agreed the that the proposed method is quite interesting. However, reviewers also felt that current evaluations were weak and were ultimately unconvinced by the author rebuttal. I recommend the authors resubmit with a stronger set of experiments as suggested by Reviewers 2 and 3.",Paper Decision
-2oG8guO3X,BkgnhTEtDS,Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection,Accept (Poster),"The paper extracts feature interactions in recommender systems and studies the effect of these interactions on the recommendations. While the focus is on recommender systems the authors claim that the ideas can be generalised to other domains also. 

All reviewers found the empirical results and analysis thereof to be very interesting and useful. This paper saw a healthy discussion between the authors and reviewers and all reviewers agreed that this paper makes a useful contribution. I recommend that the authors address all the concerns of the reviewers in the final version of the paper. ",Paper Decision
cOuMf9Aba,Hkgs3aNYDS,Quantum Expectation-Maximization for Gaussian Mixture Models,Reject,"The reviewers were unanimous that this submission is not ready for publication at ICLR in its current form. 

Concerns raised include a significant lack of clarity, and the paper not being self-contained.",Paper Decision
lVkL64AWO_,BJg9hTNKPH,Behavior Regularized Offline Reinforcement Learning,Reject,"This paper is an empirical studies of methods to stabilize offline (ie, batch) RL methods where the dataset is available up front and not collected during learning. This can be an important setting in e.g. safety critical or production systems, where learned policies should not be applied on the real system until their performance and safety is verified. Since policies leave the area where training data is present, in such settings poor performance or divergence might result, unless divergence from the reference policy is regularized. This paper studies various methods to perform such regularization. 

The reviewers are all very happy about the thoroughness of the empirical work. The work only studies existing methods (and combination thereof), so the novelty is limited by design. The paper was also considered well written and easy to follow. The results were very similar between the considered regularizers, which somehow limits the usefulness of the paper as practical guideline (although at least now we know that perhaps we do not need to spend a lot of time choosing the best between these). Bigger differences were observed between ""value penalties"" versus ""policy regularization"". This seems to correspond to theoretical observations by Neu et al (https://arxiv.org/abs/1705.07798, 2017), which is not cited in the manuscript. Although unpublished, I think that work is highly relevant for the current manuscript, and I'd strongly recommend the authors to consider its content. Some minor comments about the paper are given below.

On the balance, the strong point of the paper is the empirical thoroughness and clarity, whereas novelty, significance, and theoretical analysis are weaker points. Due to the high selectivity of ICLR, I unfortunately have to recommend rejection for this manuscript.

I have some minor comments about the contents of the paper:
- The manuscript contains the line:  ""Under this definition, such a behavior policy πb is always well-defined even
if the dataset was collected by multiple, distinct behavior policies"". Wouldn't simply defining the behavior as a mixture of the underlying behavior policies (when known) work equally well?
- The paper mentions several earlier works that regularize policies update using the KL from a reference policy (or to a reference policy). The paper of Peters is cited in this context, although there the constraint is actually on the KL divergence between state-action distributions, resulting in a different type of regularization.",Paper Decision
EBg87bO6X,B1xq264YvH,Encoder-Agnostic Adaptation for Conditional Language Generation,Reject,"This paper proposes a method to use a pretrained language model for language generation with arbitrary conditional input (images, text). The main idea, which is called pseudo self-attention, is to incorporate the conditioning input as a pseudo history to a pretrained transformer. Experiments on class-conditional generation, summarization, story generation, and image captioning show the benefit of the proposed approach.

While I think that the proposed approach makes sense, especially for generation from multiple modalities, it would be useful to see the following comparison in the case of conditional generation from one modality (i.e., text-text such as in summarization and story generation). How does the proposed approach compare to a method that simply concatenates these input and output? In Figure 1(c), this would be having the encoder part be pretrained as well, as opposed to randomly initialized, which is possible if the input is also text. I believe this is what R2 is suggesting as well when they mentioned a GPT-2 style model, and I agree this is an important baseline.

This is a borderline paper. However, due to space constraint and the above issues, I recommend to reject the paper.",Paper Decision
O9HULKUsGf,BJxt2aVFPr,Optimizing Data Usage via Differentiable Rewards,Reject,"The paper proposes an iterative learning method that jointly trains both a model and a scorer network that places a non-uniform weights on data points, which estimates the importance of each data point for training.  This leads to significant improvement on several benchmarks.  The reviewers mostly agreed that the approach is novel and that the benchmark results were impressive, especially on Imagenet.  There were both clarity issues about methodology and experiments, as well as concerns about several technical issues.  The reviewers felt that the rebuttal resolved the majority of minor technical issues, but did not sufficiently clarify the more significant methodological concerns. Thus, I recommend rejection at this time.",Paper Decision
mz21SjX9OT,Bylthp4Yvr,Dropout: Explicit Forms and Capacity Control,Reject,"The authors study dropout for matrix sensing and deep learning, and show that dropout induces a data-dependent regularizer in both cases. In both cases, dropout controls quantities that yield generalization bounds. 

Reviewers raised several concerns, and several of these were vehemently rebutted. The rhetoric of the back and forth slid into unfortunate territory, in my opinion, and I'd prefer not to see this sort of thing happen. On the one hand, I can sympathize with the reviewers trying to argue that (un)related work is not related work. On the other hand, it's best to be generous, or you run into this sort of mess.

In the end, even the expert reviewers were unswayed. I suspect the next version of this paper may land more smoothly.

While many of the technical issues are rebutted, one that caught my attention pertained to the empirical work. Reviewer #4 noticed that the empirical evaluations do not meet the sample complexity requirements for the bounds to be valid (nevermind loose). The response suggests this is simply a fact of making the bounds looser, but I suspect it may also change their form in this regime, potentially erasing the empirical findings. I suggest the authors carefully consider whether all assumptions are met, and relay this more carefully to readers.",Paper Decision
AgSFZfn2jk,r1ltnp4KwS,Training Interpretable Convolutional Neural Networks towards Class-specific Filters,Reject,"The paper proposes a method to make the filters of the last conv layer more class-specific. The motivation for this is to improve upon the interpretability of the CNN, which is empirically shown by comparing the class activation maps (CAMs) of regular CNN and the proposed LSG-CNN. While the idea is interesting, one of the concerns from reviewers is about limited applicability of the method, at least the way it is shown in experiments -- a concern that I tend to agree with. As primary goal of the work is improving interpretability of CNNs, authors should test LSG-CNN with some more recent methods for producing the saliency maps other than CAM to convincingly establish the value of the method. Authors also mention lack of hyperparameter tuning and the use of SGD with limited training epochs as a reason for the drop in accuracy. It will be worth spending some effort so the accuracy matches the standard benchmarks -- this will help in arguing more convincingly about practical benefit of the method. ",Paper Decision
zemZKR2ztM,rJeO3aVKPB,Faster Neural Network Training with Data Echoing,Reject,"This paper presents a simple trick of taking multiple SGD steps on the same data to improve distributed processing of data and reclaim idle capacity. The underlying ideas seems interesting enough, but the reviewers had several concerns.

1. The method is a simple trick (R2). I don't think this is a good reason to reject the paper, as R3 also noted, so I think this is fine.
2. There are not clear application cases (R3). The authors have given a reasonable response to this, in indicating that this method is likely more useful for prototyping than for well-developed applications. This makes sense to me, but both R3 and I felt that this was insufficiently discussed in the paper, despite seeming quite important to arguing the main point.
3. The results look magical, or too good to be true without additional analysis (R1 and R3). This concerns me the most, and I'm not sure that this point has been addressed by the rebuttal. In addition, it seems that extensive hyperparameter tuning has been performed, which also somewhat goes against the idea that ""this is good for prototyping"". If it's good for prototyping, then ideally it should be a method where hyperparameter tuning is not very necessary.
4. The connections with theoretical understanding of SGD are not well elucidated (R1). I also agree this is a problem, but perhaps not a fatal one -- very often simple heuristics prove effective, and then are analyzed later in follow-up papers.

Honestly, this paper is somewhat borderline, but given the large number of good papers that have been submitted to ICLR this year, I'm recommending that this not be accepted at this time, but certainly hope that the authors continue to improve the paper towards a final publication at a different venue.
",Paper Decision
SzAGDRncc9,Hyx_h64Yvr,Kronecker Attention Networks,Reject,"This submission has been assessed by three reviewers who scored it as 3/3/3. The main criticism includes lack of motivation for sections 3.1 and 3.2, comparisons to mere regular self-attention without encompassing more works on this topic, a connection between Theorem 1 and the rest of the paper seems missing. Finally, there exists a strong resemblance to another submission by the same authors which is also raises the questions about potentially a dual submission. Even excluding the last argument, lack of responses to reviewers does not help this case. Thus, this paper cannot be accepted by ICLR2020.",Paper Decision
a128u8vaC,Byxv2pEKPH,"Farkas layers: don't shift the data, fix the geometry",Reject,"This paper proposes a new normalization scheme that attempts to prevent all units in a ReLU layer from being dead. The experimental results show that this normalization can effectively be used to train deep networks, though not as well as batch normalization. A significant issue is that the paper does not sufficiently establish that their explanation for the success of Farkas layer is valid. For example, do networks usually have layers with only inactive units in practice?",Paper Decision
X9ao1AbUse,SyxL2TNtvr,Unsupervised Model Selection for Variational Disentangled Representation Learning,Accept (Poster),"The authors address the important and understudied problem of tuning of unsupervised models, in particular variational models for learning disentangled representations.  They propose an unsupervised measure for model selection that correlates well with performance on multiple tasks.  After significant fruitful discussion with the reviewers and resulting revisions, many reviewer concerns have been addressed.  There are some remaining concerns that there may still be a gap in the theoretical basis for the application of the proposed measure to some models, that for different downstream tasks the best model selection criteria may vary, and that the method might be too cumbersome and not quite reliable enough for practitioners to use it broadly.  All of that being said, the reviewers (and I) agree that the approach is sufficiently interesting, and the empirical results sufficiently convincing, to make the paper a good contribution and hopefully motivation for additional methods addressing this problem.",Paper Decision
auFGi97Ew,HkxU2pNYPH,Sticking to the Facts: Confident Decoding for Faithful Data-to-Text Generation,Reject,"This paper proposes to improve the faithfulness of data-to-text generation models, through an attention-based confidence measure and a variational approach for learning the model.  There is some reviewer disagreement on this paper.  All agree that the problem is important and ideas interesting, while some reviewers feel that the methods are insufficiently justified and/or the results unconvincing.  In addition, there is not much technical novelty here from a machine learning perspective; the contribution is to a specific task.  Overall I think this paper would fit in much better in an NLP conference/journal.",Paper Decision
GdrBW7YY1g,rJeB36NKvB,How much Position Information Do Convolutional Neural Networks Encode?,Accept (Spotlight),"This paper analyzes the weights associated with filters in CNNs and finds that they encode positional information (i.e. near the edges of the image).  A detailed discussion and analysis is performed, which shows where this positional information comes from.  

The reviewers were happy with your paper and found it to be quite interesting.  The reviewers felt your paper addressed an important (and surprising!) issue not previously recognized in CNNs.",Paper Decision
O58nz2IAIQ,HkgB2TNYPS,A Theoretical Analysis of the Number of Shots in Few-Shot Learning,Accept (Poster),"The reviewers generally found the paper's contribution to be valuable and informative, and I believe that this paper should be accepted for publication and a poster presentation. I would strongly recommend to the authors to carefully read over the reviews and address any comments or concerns that were not yet addressed in the rebuttal.",Paper Decision
hYshZWVBFE,H1gS364FwS,Event extraction from unstructured Amharic text,Reject,"This paper performs event extraction from Amharic texts. To this end, authors prepared a novel Amharic corpus and used a hybrid system of rule-based and learning-based systems.
Overall, while all reviewers admit the importance of addressing low-resource language and the value of the novel Amharic corpus, they are not satisfied with the quality of the current paper as a scientific work. 
Most importantly, although the attempt of even extraction might be new on Amharic, there have been many works on other languages. It should be clearly presented what are the non-trivial language-specific challenges on Amharic and how they are solved, otherwise it seems just an engineering of existing techniques on a new dataset. Also, all reviewers are fairly concerned about the presentation and clarity of the paper. Unfortunately, no revised paper is uploaded and we cannot confirm how authors' response is reflected. For those reasons, I would like to recommend rejection. 
",Paper Decision
EI5nwSy7W8,SJlVn6NKPB,Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach,Reject,"The authors present a method for learning representations of remote sensing images from multiple views. The main ideas is to use the InfoNCE loss to learn from multiple views of the data. 

The reviewers had a few concerns about this work which were not adequately addressed by the authors. I have summarised these below and would strongly recommend that the authors address these in subsequent submissions:

1) Experiments on a single dataset and a very specific task: Authors should present a more convincing argument about why the chosen dataset and task are challenging and important to demonstrate the main ideas presented in their work. Further, they should also report results on additional datasets suggested by the reviewers. 
2) Comparisons with existing works: The reviewers suggested several existing works for comparison. The authors agreed that these were relevant and important but haven't done this comparison yet. Without such a comparison it is hard to evaluate the main contributions of this work. 

Based on the above objections raised by the reviewers, I recommend that the paper should not be accepted.",Paper Decision
7sZVLPSJ_,H1lmhaVtvr,Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery,Accept (Poster),"The authors present a method to learn the expected number of time steps to reach any given state from any other state in a reinforcement learning setting.  They show that these so-called dynamical distances can be used to increase learning efficiency by helping to shape reward.  After some initial discussion, the reviewers had concerns about the applicability of this method to continuing problems without a clear goal state, learning issues due to the dependence of distance estimates on policy (and vice versa), experimental thoroughness, and a variety of smaller technical issues.  While some of these were resolved, the largest outstanding issue is whether the proper comparisons were made to existing work other than DIAYN.  The authors appear to agree that additional baselines would benefit the paper, but are uncertain whether this can occur in time.  Nonetheless, after discussion the reviewers all appeared to agree on the merit of the core idea, though I strongly encourage the authors to address as many technical and baseline issues as possible before the camera ready deadline.  In summary, I recommend this paper for acceptance.",Paper Decision
IpRj76H1MR,SJeX2aVFwH,Project and Forget: Solving Large Scale Metric Constrained Problems,Reject,"Quoting from Reviewer2: ""The paper considers the problem of optimizing convex functions under metric constraints. The main challenge is that expressing all metric constraints on n points requiries O(n^3) constraints. The paper proposes a “project and forget” approach which is essentially is based on cyclic Bregman projections but with a twist that some of the constraints are forgotten.""  The reviewers were split on this submission, with two arguing for weak acceptance and one arguing for rejection.  Purely based on scores, this paper is borderline.  It was pointed out by multiple reviewers that the method is not very novel.  In particular it effectively works as an active set method.  It appears to be very effective in this setting, but the basic algorithm does not differ in structure from any active set method, for which removal of inactive constraints is considered standard (see even the wikipedia page on active set methods).",Paper Decision
yNHIuV5RC,rkgz2aEKDr,On the Variance of the Adaptive Learning Rate and Beyond,Accept (Poster),"The paper considers an important topic of the warmup in deep learning, and investigates the problem of the adaptive learning rate. While the paper is somewhat borderline, the reviewers agree that it might be useful to present it to the  ICLR community.",Paper Decision
2wdl3eKpFE,rJxG3pVKPB,"Translation Between Waves,  wave2wave",Reject,"The paper considers the task of sequence to sequence modelling with multivariate, real-valued time series.
The authors propose an encoder-decoder based architecture that operates on fixed windows of the original signals.

The reviewers unanimously criticise the lack of novelty in this paper and the lack of comparison to existing baselines.
While Rev #1 positively highlights human evaluation contained in the experiments, they nevertheless do not think this paper is good enough for publication as is.
The authors did not submit a rebuttal.

I therefore recommend to reject the paper.",Paper Decision
jNm-GJX8vd,HyxG3p4twS,Quantifying the Cost of Reliable Photo Authentication via High-Performance Learned Lossy Representations,Accept (Poster),"The paper introduces a new image compression approach that preserves the patterns indicating image manipulation. The reviewers appreciate the idea and the method. Please take into account the suggestions of Reviewer1, when preparing the final version.",Paper Decision
l5L_FQdEwO,Byl-264tvr,Improving End-to-End Object Tracking Using Relational Reasoning,Reject,"The authors propose an end-to-end object tracker by exploiting the attention mechanism. Two reviewers recommend rejection, while the last reviewer is more positive. The concerns brought up are novelty (last reviewer), and experiments (second reviewer). Furthermore, the authors seem to overclaim their contribution. There indeed are end-to-end multi-object trackers, see Frossard & Urtasun's work for example. This work needs to be cited, and possibly a comparison is needed. Since the paper did not receive favourable reviews and there are additional citations missing, this paper cannot be accepted in current form. The authors are encouraged to strengthen their work and resubmit to a future venue.",Paper Decision
4Aype8ZL9i,HygW26VYwS,Attention Privileged Reinforcement Learning for Domain Transfer,Reject,"This paper tackles the problem of transferring an RL policy learned in simulation to the real world (sim2real). More specifically, the authors address the situation where the agent can access privileged information available during simulation, for example access to exact states instead of compressed representations. They perform experiments in various simulated domains where different aspects of the environment are modified to evaluate generalization.

Major concerns remain following the rebuttal. First, it is not clear how realistic it is to assume access to such privileged information in practice. Second, the experiments are not convincing since the algorithms do not appear to have reached convergence in the presented results. Finally, a sim2real work would highly benefit from real-world experiments.

In light of the above issues, I recommend to reject this paper.",Paper Decision
Y8RaLJ2XJ4,BJge3TNKwH,Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations,Accept (Spotlight),The paper addresses an important problem (preventing catastrophic forgetting in continual learning) through a novel approach based on the sliced Kramer distance. The paper provides a novel and interesting conceptual contribution and is well written. Experiments could have been more extensive but this is very nice work and deserves publication.,Paper Decision
5XXJl5iMZ,Hkex2a4FPr,On Variational Learning of Controllable Representations for Text without Supervision,Reject,"This paper analyzes the behavior of VAE for learning controllable text representations and uses this insight to introduce a method to constrain the posterior space by introducing a regularization term and a structured reconstruction term to the standard VAE loss. Experiments show the proposed method improves over unsupervised baselines, although it still underperforms supervised approaches in text style transfer.

The paper had some issues with presentation, as pointed out by R1 and R3. In addition, it missed citations to many prior work. Some of these issues had been addressed after the rebuttal, but I still think it needs to be more self contained (e.g., include details of evaluation protocols in the appendix, instead of citing another paper). 

In an internal discussion, R1 still has some concerns regarding whether the negative log likelihood is less affected by manipulations in the constrained space compared to beta-VAE. In particular, the concern is about whether the magnitude of the manipulation is comparable across models, which is also shared by R3. R1 also think some of the generated samples are not very convincing. 

This is a borderline paper with some interesting insights that tackles an important problem. However, due to its shortcoming in the current state, I recommend to reject the paper.",Paper Decision
jbc0bHlS-R,Sklyn6EYvH,Disentangled Representation Learning with Sequential Residual Variational Autoencoder,Reject,"This paper that defines a “Residual learning” mechanism as the training regime for variational autoencoder. The method gradually activates individual latent variables to reconstruct residuals.

There are two main concerns from the reviewers. First, residual learning is a common trick now, hence authors should provide insights on why residual learning works for VAE. The other problem is computational complexity. Currently, reviews argue that it seems not really fair to compare to a bruteforce parameter search. The authors’ rebuttal partially addresses these problems but meet the standard of the reviewers.

Based on the reviewers’ comments, I choose to reject the paper.
",Paper Decision
KuLk5umrx,Bke13pVKPS,"Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization",Reject,"This paper proposes a GA-based method for optimizing the loss function a model is trained on to produce better models (in terms of final performance). The general consensus from the reviewers is that the paper, while interesting, dedicates too much of its content to analyzing one such discovered loss (the Baikal loss), and that the experimental setting (MNIST and Cifar10) is too basic to be conclusive. It seems this paper can be so significantly improved with some further and larger scale experiments that it would be wrong to prematurely recommend acceptance. My recommendation is that the authors consider the reviewer feedback, run the suggested further experiments, and are hopefully in the position to submit a significantly stronger version of this paper to a future conference.",Paper Decision
KLRPkI5Z8l,Hke12T4KPS,Using Hindsight to Anchor Past Knowledge in Continual Learning,Reject,"This paper proposes a continual learning method that uses anchor points for experience replay. Anchor points are learned with gradient-based optimization to maximize forgetting on the current task. Experiments MNIST, CIFAR, and miniImageNet show the benefit of the proposed approach.

As noted by other reviewers, there are some grammatical issues with the paper. 

It is missing some important details in the experiments. It is unclear to me whether the five random seeds how the datasets (tasks) are ordered in the experiments. Do the five random seeds correspond to five different dataset orderings? I think it would also be very interesting to see the anchor points that are chosen in practice. This issue is brought up by R4, and the authors responded that anchor points do not correspond to classes. Since the main idea of this paper is based on anchor points, it would be nice to analyze further to get a better understanding what they represent.

Finally, the authors only evaluate their method on image classification. While I believe the technique can be applied in other domains (e.g., reinforcement learning, natural language processing) with some modifications, without providing concrete empirical evidence in the paper, the authors need to clearly state that their proposed method is only evaluated on image classification and not sell it as a general method (yet).

The authors also miss citations to some prior work on memory-based parameter adaptation and its variants.

Regardless all of the above issues, this is still a borderline paper. However, due to space constraint, I recommend to reject this paper for ICLR.",Paper Decision
nmsavO3KX1,Hke0oa4KwS,Empirical confidence estimates for classification by deep neural networks,Reject,"The paper proposes to model uncertainty using expected Bayes factors, and empirically show that the proposed measure correlates well with the probability that the classification is correct.

All the reviewers agreed that the idea of using Bayes factors for uncertainty estimation is an interesting approach. However, the reviewers also found the presentation a bit hard to follow. While the rebuttal addressed some of these concerns, there were still some remaining concerns (see R3's comments). 

I think this is a really promising direction of research and I appreciate the authors' efforts to revise the draft during the rebuttal (which led to some reviewers increasing the score). This is a borderline paper right now but I feel that the paper has the potential to turn into a great paper with another round of revision. I encourage the authors to revise the draft and resubmit to a different venue.",Paper Decision
KvC6pvYjWM,Syg6jTNtDH,Learning Numeral Embedding,Reject,"This paper proposes better methods to handle numerals within word embeddings.

Overall, my impression is that this paper is solid, but not super-exciting. The scope is a little bit limited (to only numbers), and it is not by any means the first paper to handle understanding numbers within word embeddings. A more thorough theoretical and empirical comparison to other methods, e.g. Spithourakis & Riedel (2018) and Chen et al. (2019), could bring the paper a long way.

I think this paper is somewhat borderline, but am recommending not to accept because I feel that the paper could be greatly improved by making the above-mentioned comparisons more complete, and thus this could find a better place as a better paper in a new venue.",Paper Decision
sJLWCl0YyC,Skgaia4tDH,Localized Generations with Deep Neural Networks for Multi-Scale Structured Datasets,Reject,"The paper presents a structured VAE, where the model parameters depend on a local structure (such as distance in feature or local space), and it uses the meta-learning framework to adjust the dependency of the model parameters to the local neighborhood.

The idea is natural, as pointed by Rev#1. It incurs an extra learning cost, as noted by Rev#1 and #2, asking for details about the extra-cost. The authors' reply is (last alinea in first reply to Rev#1): we did not comment (...) because in essence, using neighborhoods in a naive way is not affordable. 
The area chair would like to know the actual computational time of Local VAE compared to that of the baselines.  

More details (for instance visualization) about the results on Cars3D and NORB would also be needed to better appreciate the impact of the locality structure. The fact that the optimal value (wrt Disentanglement) is rather low ($10^{-2}$) would need be discussed, and assessed w.r.t. the standard deviation.  

In summary, the paper presents a good idea. More details about its impacts on the VAE quality, and its computation costs, are needed to fully appreciate its merits. ",Paper Decision
YfCeyhV6Nz,HkghoaNYPB,AlgoNet: $C^\infty$ Smooth Algorithmic Neural Networks,Reject,"The paper does not provide theory or experiment to justify the various proposed relaxations. In its current form, it has very limited scope.",Paper Decision
9PXvytFAJ,HJghoa4YDB,Temporal-difference learning for nonlinear value function approximation in the lazy training regime,Reject,"This paper provides convergence results for Non-linear TD under lazy training.

This paper tackles the important and challenging task of improving our theoretical understanding of deep RL. We have lots of empirical evidence Q-learning and TD can work with NNs, and even empirical work that attempts to characterize when we should expect it to fail. Such empirical work is always limited and we need theory to supplement our empirical knowledge. This paper attempts to extend recent theoretical work on the convergence of supervised training of NN to the policy evaluation setting with TD.

The main issue revolves around the presentation of the work. The reviewers found the paper difficult to read (ok for theory work). But, the paper did not clearly discuss and characterize the significance of the work: how limited is the lazy training regime, when would it be useful? Now that we have this result, do we have any more insights for algorithm design (improving nonlinear TD), or comments about when we expect NN policy evaluation to work? 

This all reads like: the paper needs a better intro and discussion of the implications and limitations of the results, and indeed this is what the reviewers were looking for. Unfortunately the author response and paper submitted were lacking in this respect. Even the strongest advocates of the work found it severely lacking explanation and discussion.  They felt that the paper could be accepted, but only after extensive revision.

The direction of the work is important. The work is novel, and not a small undertaking. However, to be published the authors should spend more time explaining the framework, the results, and the limitations to the reader.

",Paper Decision
5c6l18ro9,H1l3s6NtvH,A Bayes-Optimal View on Adversarial Examples,Reject,"The paper studies how adversarial robustness and Bayes optimality relate in a simple gaussian mixture setting. The paper received two recommendations for rejection and one weak accept. One of the central complaints was whether the study had any bearing on ""real world"" adversarial examples. I think this is a fair concern, given how limited the model appears on the surface, although perhaps the model is a good model of any local ""piece"" of a decision boundary in a real problem. That said, I do not agree with the strong rejection (1) in most places. The weak reject asked for some experiments. The revision produced these experiments, but I'm not sure how convincing these are since only one robust training method was used, and it's not clear that it's the best one could do among SOTA methods. For whatever reason, the reviewers did not update their scores. I am not certain that they reviewed the revision, despite my prodding.",Paper Decision
Q2fKFbtLYe,B1gjs6EtDr,Efficient Content-Based Sparse Attention with Routing Transformers,Reject,"This paper proposes a new model, the Routing Transformer, which endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention from O(n^2) to O(n^1.5). The model attained very good performance on WikiText-103 (in terms of perplexity) and similar performance to baselines (published numbers) in two other tasks.

Even though the problem addressed (reducing the quadratic complexity of self-attention) is extremely relevant and the proposed approach is very intuitive and interesting, the reviewers raised some concerns, notably: 
- How efficient is the proposed approach in practice. Even though the theoretical complexity is reduced, more modules were introduced (e.g., forced clustering, mix of local heads and clustering heads, sorting, etc.)
- Why is W_R fixed random? Since W_R is orthogonal, it's just a random (generalized) ""rotation"" (performed on the word embedding space). Does this really provide sensible ""routing""?
- The experimental section can be improved to better understand the impact of the proposed method. Adding ablations, as suggested by the reviewers, would be an important part of this work.
- Not clear why the work needs to be motivated through NMF, since the proposed method uses k-means.

Unfortunately several points raised by the reviewers (except R2) were not addressed in the author rebuttal, and therefore it is not clear if some of the raised issues are fixable in camera ready time, which prevents me from recommend this paper to be accepted.

However, I *do* think the proposed approach is very interesting and has great potential, once these points are clarified. The gains obtained in WikiText-103 are promising. Therefore, I strongly encourage the authors to resubmit this paper taking into account the suggestions made by the reviewers. ",Paper Decision
1ukhIkO_oS,S1ejj64YvS,Good Semi-supervised VAE Requires Tighter Evidence Lower Bound,Reject,"The paper proposes to combine a VAE model with the Optimal Transport to approximate some components of the model. The authors evaluate their approach on semi-supervised problems and claim to obtain very competitive results compared to literature. Unfortunately, the paper would benefit substantially from revisions to make it easier to follow. For this reason, the paper is not ready for publication in this venue at this time.",Paper Decision
0GAksCzRjs,B1gqipNYwH,Option Discovery using Deep Skill Chaining,Accept (Poster),"This paper tackles the problem of autonomous skill discovery by recursively chaining skills backwards from the goal in a deep learning setting, taking the initial conditions of one skill to be the goal of the previous one. The approach is evaluated on several domains and compared against other state of the art algorithms.

This is clearly a novel and interesting paper. Two minor outstanding issues are that the domains are all related to navigation, and it would be interesting to see the approach on other domains, and that the method involves a fair bit of engineering in piecing different methods together. Regardless, this paper should be accepted.",Paper Decision
EBqIotZzpq,SJeqs6EFvB,HOPPITY: LEARNING GRAPH TRANSFORMATIONS TO DETECT AND FIX BUGS IN PROGRAMS,Accept (Spotlight),"This paper presents a learning-based approach to detect and fix bugs in JavaScript programs. By modeling the bug detection and fix as a sequence of graph transformations, the proposed method achieved promising experimental results on a large JavaScript dataset crawled from GitHub.

All the reviews agree to accept the paper for its reasonable and interesting approach to solve the bug problems. The main concerns are about the experimental design, which has been addressed by the authors in the revision. 

Based on the novelty and solid experiments of the proposed method, I agreed to accept the paper as other revises.
",Paper Decision
41Hn_5nZ2H,rJlqoTEtDB,PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization,Reject,"After reading the author's rebuttal, the reviewers still think that this is an incremental work, and the theory and experiments .are inconsistent. The authors are encouraged to consider the the reivewer's comments to improve the paper.",Paper Decision
O-knKXhTY,Syetja4KPH,Deep Randomized Least Squares Value Iteration,Reject,"This paper combines DQN and Randomized value functions for exploration. 

All the reviewers agreed the paper is not yet ready for publication. The experiments lack appropriate baselines and thus it is unclear how this new approach improves exploration in Deep RL. The reviewers also found some of the algorithmic design decisions unintuitive and unexplained. The authors main response was the objective was to improve and compare against vanilla DQN. This could be a valid goal, but it requires clear motivation (perhaps the focus is on simply algorithms that are commonly used in applications or something). Even then comparisons with other methods would be of interest to quantify how much the base algorithm is improved, and to justify empirically all the design decisions that went into building such an improvement (performance vs complexity of implementation etc).

The reviewers gave nice suggestions for improvements.  This is a good area of study: keep going!",Paper Decision
wL6Kx4bkX4,HygtiTEYvS,Self-Supervised Policy Adaptation,Reject,"The submission proposes to improve generalization in RL environments, by addressing the scenario where the observations change even though the underlying environment dynamics do not change. The authors address this by learning an adaptation function which maps back to the original representation. The approach is empirically evaluated on the Mountain Car domain. 

The reviewers were unanimously unimpressed with the experiments, the baselines, and the results. While they agree that the problem is well-motivated, they requested additional evidence that the method works as described and that a simpler approach such as fine-tuning would not be sufficient. 

The recommendation is to reject the paper at this time.",Paper Decision
nogpUfaTQ,SkeuipVKDH,RTC-VAE: HARNESSING THE PECULIARITY OF TOTAL CORRELATION  IN LEARNING DISENTANGLED REPRESENTATIONS,Reject,"This paper highlights the problem of penalizing the total correlation of sampled latent variables for unsupervised learning of disentangled representations. Authors prove a theorem on how sample representations with bounded total correlation may have arbitrarily large total correlation when computed with the underlying mean. As a fix, the authors propose RTC-VAE method that penalizes total covariance of sampled latent variables.

R2 appreciated the simplicity of the idea, making it easy to understand and implement, but raises serious concerns on empirical evaluation of the method. Specifically, very limited datasets (initially dsprites and 3d shapes) and with no evaluation of disentanglement performance and no comparison against other disentangling methods like DIP-VAE-1. While the authors added another dataset (3d face) in their revised versions, the concerns about disentanglement performance evaluation and its comparison against baselines remained as before, and R2 was not convinced to raise the initial score.

Similarly, while R1 and R3 appreciate author's response, they believe the response was not convincing enough for them, and maintained their initial ratings.

Overall, the submission has room for improvement toward a clear evaluation of the proposed method against related baselines.",Paper Decision
3jR4YK2-Fq,HJgdo6VFPH,OmniNet: A unified architecture for multi-modal multi-task learning,Reject,"This paper presents OmniNet, an architecture based on the popular transformer for learning on data from multiple modalities and predicting on multiple tasks.  The reviewers found the paper well written, technically sound and empirically thorough.  However, overall the scores fell below the bar for acceptance and none of the reviewers felt strongly enough to 'champion' the paper for acceptance.  The primary concern cited by the reviewers was a lack of strong baselines, i.e. comparison to other methods for multi-task learning.  Unfortunately, as such the recommendation is to reject.  However, adding a thorough comparison to existing literature empirically and in the related work would make this a much stronger submission to a future conference.",Paper Decision
a-tjlf_TDF,rJlDoT4twr,Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition,Reject,"This paper presents a unified probabilistic approach for deep continual learning by combining generative and discriminative models into one framework that solves the following problems: catastrophic forgetting, and identifying out of distribution and open set examples. The method termed, OCDVAE in the paper achieves closer or better to SOTA results in different evaluation tasks. 

The reviewers had several concerns about the presentation of the paper and some errors in the equations, all of which seem to have been fixed in the latest upload made by the authors. Blind review #3 was delayed as the original reviewer refused to review the paper and this review was then obtained by someone else after the new upload of the paper, so this review looks at the new version of the paper. I would recommend the authors to incorporate suggestions provided by reviewer #3 in the final version of the paper including expanding on the related work section. 

However, as of now I recommend to reject the paper.",Paper Decision
45SZRyirvS,Syxwsp4KDB,TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising,Reject,"This paper proposes an abstractive text summarization model that takes advantage of lead bias for pretraining on unlabeled corpora and a combination of reconstruction and theme modeling loss for finetuning. Experiments on NYT, CNN/DM, and Gigaword datasets demonstrate the benefit of the proposed approach. 

I think this is an interesting paper and the results are reasonably convincing. My only concern is regarding a parallel submission that contains a significant overlap in terms contributions, as originally pointed out by R2 (https://openreview.net/forum?id=ryxAY34YwB). All of us had an internal discussion regarding this submission and agree that if the lead bias is considered a contribution of another paper this paper is not strong enough. 

Due to space constraint and the above concern, along with the issue that the two submissions contain a significant overlap in terms of authors as well, I recommend to reject this paper.",Paper Decision
oJkWWID8dp,SJeLopEYDH,V4D: 4D Convolutional Neural Networks for Video-level Representation Learning,Accept (Poster),"This paper proposes video-level 4D CNNs and the corresponding training and inference methods for improved video representation learning. The proposed model achieves state-of-the-art performance on three action recognition tasks. 
Reviewers agree that the idea well motivated and interesting, but were initially concerned with positioning with respect to the related work, novelty, and computational tractability. As these issues were mostly resolved during the discussion phase, I will recommend the acceptance of this paper. We ask the authors to address the points raised during the discussion to the manuscript, with a focus on the tradeoff between the improved performance and computational cost.",Paper Decision
0mLpjLV8t,BygIjTNtPr,ODE Analysis of Stochastic Gradient Methods with Optimism and Anchoring  for Minimax Problems and GANs,Reject,"Motivated by GANs, the authors study the convergence of stochastic subgradient                                                     
descent on convex-concave minimax games.                                                                                           
They introduced an improved ""anchored"" SGD variant, that provably converges                                                        
under milder assumptions that the base algorithm.                                                                                  
It is applied to training GANs on MNIST and CIFAR-10, partially showing                                                            
improvements over alternative training methods.                                                                                    
                                                                                                                                   
A main point of criticism that the reviewers identify is the strength of the                                                       
assumptions needed for the analysis.                                                                                               
Furthermore, the experimental results were deemed weak as the reported scores                                                      
are far away from the SOTA, and only simple baselines were compared against.  ",Paper Decision
Ihic84YeDr,rylHspEKPr,Learning to Represent Programs with Property Signatures,Accept (Poster),"The authors propose improved techniques for program synthesis by introducing the idea of property signatures. Property signatures help capture the specifications of the program and the authors show that using such property signatures they can synthesise programs more efficiently.

I think it is an interesting work. Unfortunately, one of the reviewers has strong reservations about the work. However, after reading the reviewer's comments and the author's rebuttal to these comments I am convinced that the initial reservations of R1 have been adequately addressed. Similarly, the authors have done a great job of addressing the concerns of the other reviewers and have significantly updated their paper (including more experiments to address some of the concerns). Unfortunately R1 did not participate in subsequent discussions and it is not clear whether he/she read the rebuttal. Given the efforts put in by the authors to address different concerns of all the reviewers and considering the positive ratings given by the other two reviewers I recommend that this paper be accepted. 

Authors,
Please include all the modifications done during the rebuttal period in your final version. Also move the comparison with DeepCoder to the main body of the paper.",Paper Decision
WDRs2ck0o,HkgBsaVtDB,Unified recurrent network for many feature types,Reject,"main summary: sparse time LSTM

discussions;
reviewer 4: technical description of the proposed method insufficient,
reviewer 2, 3: same paper sent to ICLR 2019 and rejected
recommendation: rejected, based on all reviewers comments",Paper Decision
O-D14JmFuO,Bye4iaEFwr,Improving Dirichlet Prior Network for Out-of-Distribution Example Detection,Reject,"In this work the authors build on the Dirichlet prior network of Malinin & Gales, replacing the loss function and adding a regularization term which improve training in the setting with a significant number of classes.   Improving uncertainty for deep learning is a challenging but very important problem.  The reviewers of this paper gave two weak rejects (one is of low confidence) and one weak accept.  They found the paper well written, easy to follow and well motivated but somewhat incremental and not entirely empirically justified.  None of the reviewers were willing to strongly champion the paper for acceptance.  Unfortunately as such the paper falls below the bar for acceptance.  It appears that the authors significantly added to the experiments in the discussion phase and hopefully that will make the paper much stronger for a future submission.",Paper Decision
12gc0pR1wr,BylNoaVYPS,Variational Autoencoders for Opponent Modeling in Multi-Agent Systems,Reject,"The present work addresses the problem of opponent modeling in multi-agent learning settings, and propose an approach based on variational auto-encoders (VAEs). Reviewers consider the approach natural and novel empirical results area presented to show that the proposed approach can accurately model opponents in partially observable settings. Several concerns were addressed by the authors during the rebuttal phased. A key remaining concern is the size of the contribution. Reviewers suggest that a deeper conceptual development, e.g., based on empirical insights, is required.",Paper Decision
syk9jam7zM,S1gXiaEYvr,Prototype Recalls for Continual Learning,Reject,"The reviewers have provided extensive comments, we encourage the authors to take them into account seriously in further iterations of this work.",Paper Decision
KP7A3dMjZX,SJg7spEYDS,Generative Ratio Matching Networks,Accept (Poster),"The paper proposes a training method for generative adversarial network that avoids solving a zero-sum game between the generator and the critic, hence leading to more stable optimization problems. It is similar to MMD-GAN, in which MMD is computed on a projected low-dim space, but the projection is trained to match the density ratio between the observed and the latent space.
The reviewers raised several questions. Most of them have been addressed after several rounds of discussions. Overall, they are all positive about this paper, so I recommend acceptance. I encourage the authors to incorporate those discussions in their revised paper.",Paper Decision
IByNbfUjSE,r1gzoaNtvr,Emergence of Compositional Language with Deep Generational Transmission,Reject,"This paper explores the emergence of language in environments that demand agents communicate, focusing on the compositionality of language, and the cultural transmission of language.

Reviewer 1 has several suggestions about new experiments that are possible. The AC does think there is value in many of the suggested experiments, if not to run, then just to acknowledge their possibility and leave for future work. The reviewers also point to some previous work that is very similar.  E.g. ""Ease-of-Teaching and Language Structure from Emergent Communication"", Funshan Li et al",Paper Decision
MDjMDrTjH,BkxzsT4Yvr,Deep Gradient Boosting -- Layer-wise Input Normalization of Neural Networks,Reject,"The paper introduces a neat idea that an SGD update can be written as a solution of the linear least squares problem with a given backpropagated output; this is generalized to a larger batch size, giving a sort of ""block"" gradient-type update. Some notes that the columns of $O_t$ have to be scaled are made, but not clear why. The paper then goes into the experiments, and then gets back to the fast approximation of DGB. It really looks like bad organization of the paper, which was noted. 
The reviewers agree that the actual computational improvements are marginal, and all recommend rejection. As a recommendation, I would suggest to restructure the paper for a more coherent view, and also the improvements in Top-1 are not very stimulating. The general view is interesting, but it is not clear what insight it brings.",Paper Decision
XxEAzVCsdu,BJlbo6VtDH,A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models,Reject,"This paper proposes a generalized way to generate sequences from undirected sequence models.

Overall, I believe a framework like this could definitely be a valuable contribution, but as Reviewer 1 and Reviewer 3 noted, the paper is a bit lacking both in theoretical analysis and strong empirical results. I don't think that this is a bad paper at all, but it feels like the paper needs a little bit of an extra push to tighten up the argumentation and/or results before warranting publication at a premier venue such as ICLR. I'd suggest the authors continue to improve the paper and aim to re-submit at revised version at a future conference. ",Paper Decision
vpVHZttBm,SJx-j64FDr,In Search for a SAT-friendly Binarized Neural Network Architecture,Accept (Poster),"This paper studies how the architecture and training procedure of binarized neural networks can be changed in order to make it easier for SAT solvers to verify certain properties of them.

All of the reviewers were positive about the paper, and their questions were addressed to their satisfaction, so all reviewers are in favor of accepting the paper. I therefore recommend acceptance.",Paper Decision
EzhVlfecRj,Skgeip4FPr,Neural networks are a priori biased towards Boolean functions with low entropy,Reject,"This article studies the inductive bias in a simple binary perceptron without bias, showing that if the weight vector has a symmetric distribution, then the cardinality of the support of the represented function is uniform on 0,...,2^n-1. Since the number of possible functions with support of extreme cardinality values is smaller, the result is interpreted as a bias towards such functions. Further results and experiments are presented. The reviewers found this work interesting and mentioned that it contributes to the understanding of neural networks. However, they also expressed concerns about the contribution relying crucially on 0/1 variables, and that for example with -1/1 the effect would disappear, implying that the result might not be capturing a significant aspect of neural networks. Another concern was whether the results could be generalised to other architectures. The authors agreed that this is indeed a crucial part of the analysis, and for the moment pointed at empirical evidence for the appearance of this effect in other cases. The reviewers also mentioned that the motivation was not very clear, that some of the derivations were difficult to follow (with many results presented in the appendix), and that the interpretation and implications were not sufficiently discussed (in particular, in relation to generalization, missing a more detailed discussion of training). This is a good contribution and the revision made important improvements on the points mentioned above, but not quite reaching the bar. ",Paper Decision
m33UKm5RxE,H1lyiaVFwB,DUAL ADVERSARIAL MODEL FOR GENERATING 3D POINT CLOUD,Reject,"The paper introduces a new method for 3d point cloud generation based upon auto encoders and GANs.

Two reviewers voted for accept and one reviewer for outright reject. Both authors and reviewers posted thorough responses. Based upon these it is judged best to not accept the paper in the present. The authors should take the feedback into account in a an updated version of the paper.

Rejection is recommended.  ",Paper Decision
bfu6GlPwMw,BJlyi64FvB,Wider Networks Learn Better Features,Reject,"This paper investigated the effect of network width on learned features using activation atlases. From the current view of deep learning, the novelty of the paper is limited.

As all reviews rejected the paper and the authors gave up rebuttal, I choose to reject the paper.
",Paper Decision
kFwnBhj3MH,SyxC9TEtPH,Conditional Invertible Neural Networks for Guided Image Generation,Reject,"The paper presents an extension of flow-based invertible generative models to a conditional setting. The key idea is fairly simple modification of the original architecture, but authors also propose techniques for down-sampling with Haar wavelets. The experimental results on class-conditional MNIST generation and colorization are promising. However, in terms of weakness, the technical novelty seems somewhat limited although it's a reasonable extension. In addition, the experimental results lack evaluation on general conditional image generation tasks with more widely used benchmarks (e.g., class-conditional generation setting for real images, such as CIFAR and ImageNet; attribute-conditional or image-to-image translation settings; etc.). In other words, colorization seems like a niche task. The baselines compared are not the strongest models. For example, the diversity of 
cGANs can be significantly improved by simple plug-in modifications (e.g., DSGAN) to any existing GAN architectures, and those methods were demonstrated on broader benchmarks. So I view the experimental validation somewhat limited in scope and significance. While this work presents a reasonable extension of conditional invertible generative models with promising results, I believe that more work needs to be done to be publishable at a top-tier conference.

Diversity-Sensitive Conditional Generative Adversarial Networks
https://arxiv.org/abs/1901.09024

Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis
https://arxiv.org/abs/1903.05628
* exactly the same idea as DSGAN above.
",Paper Decision
DPVrpJe7d,S1xCcpNYPr,Cost-Effective Testing of a Deep Learning Model through Input Reduction,Reject,"This paper presents a method which creates a representative subset of testing examples so that the model can be tested quickly during the training. The procedure makes use of the famous HGS selection algorithm which identifies and then eliminates the redundant and obsolete test cases based on two criteria: (1) structural coverage as measured by the number of neurons activated beyond a certain threshold, and (2) distribution mismatch (as measured by KL divergence) of the last layer activations. The algorithm has two-phases: (1) a greedy subset selection based on the coverage, and (2) an iterative phase were additional test examples are added until the KL divergence (as defined above) falls below some threshold. 
This approach is incremental in nature -- the resulting multi-objective optimisation problem is not a significant improvement over BOT. After the discussion phase, we believe that the advantages over BOT were not clearly demonstrated and that the main drawback of BOT (requiring the number of samples) is not hindering practical applications. Finally, the empirical evaluation is performed on very small data sets and I do not see an efficient way to apply it to larger data sets where this reduction could be significant. Hence, I will recommend the rejection of this paper. To merit acceptance to ICLR the authors need to provide a cleaner presentation (especially of the algorithms), with a focus on the incremental improvements over BOT, an empirical analysis on larger datasets, and a detailed look into the computational aspects of the proposed approach.
",Paper Decision
VtMf73VgVd,H1ep5TNKwr,Hebbian Graph Embeddings,Reject,"The paper learns an embedding on the nodes of the graph, iteratively aligning the vector associated to a node with that of its neighbor nodes (based on the Hebbian rule). 

The reviews state that the approach is interesting though very natural/straightforward, and that it might go too far to call it ""Hebbian"" (Rev#2) - you might want also to see it as a Self-Organizing Map for graphs. 

A main criticism was about the comparison with the state of the art (all reviewers). The authors did add empirical comparisons with the suggested VGAE and SEAL, and phrase it nicely as ""our algorithm outperforms SEAL on one out of four data sets"". Looking at the revised paper, this is true: the approach is outperformed by SEAL on 3 out of 4 datasets.

Another criticism regards the insufficient analysis of the results (e.g. through visualization, studying the clusters obtained along different runs, etc). 
This aspect is not addressed in the revised version.

An excellent point is the scalability of the approach, which is worth emphasizing.

I thus encourage the authors to rewrite and polish the paper, improving the positioning of the proposed approach w.r.t. the state of the art, and providing a more thorough analysis of the results.

",Paper Decision
pewV34cNX,r1xa9TVFvH,NeuralUCB: Contextual Bandits with Neural Network-Based Exploration,Reject,"As the reviewers have pointed out and the authors have confirmed, the original version of this paper was not a significant leap beyond combining recent understanding of Neural Tangent Kernels and previous techniques for kernelized bandits. In a revision, the authors updated their draft to allow the point at which gradients are centered around, theta_0, to now equal theta_t. This seems like a more reasonable algorithm and it is satisfying that the authors were able to maintain their regret bound for this dynamic setting. However, the revision is substantial and it seems unreasonable to expect reviewers to read the revised results in detail--the reviewers also felt it may be unfair to other ICLR submissions. All reviewers believe the paper has introduced valuable contributions to the area but should go under a full review process at a future venue. A reviewer would also like to see a comparison to Kernel UCB run on the true NTK (or a good approximation thereof). ",Paper Decision
_8RzzyrrT8,BJepcaEtwB,Meta-Graph: Few shot Link Prediction via Meta Learning,Reject,"This paper presents a new link prediction framework in the case of small amount labels using meta learning methods. The reviewers think the problem is important, and the proposed approach is a modification of meta learning to this case. However, the method is not compared to other knowledge graph completion methods such as TransE, RotaE, Neural Tensor Factorization in benchmark dataset such as Fb15k and freebase.  Adding these comparisons can make the paper more convincing. ",Paper Decision
w1RLEPftxC,H1lhqpEYPr,Actor-Critic Provably Finds Nash Equilibria of Linear-Quadratic Mean-Field Games,Accept (Poster),"The authors propose an actor-critic method for finding Nash equilibrium in linear-quadratic mean field games and establish linear convergence under some assumptions. There were some minor concerns about motivation and clarity, especially with regards to the simulator. In an extensive and interactive rebuttal, the authors were able to argue that their results/methods, which appear to be rather specialized to the LQ setting, offer insight/methods beyond the LQ setting.",Paper Decision
Ndn2zhMfK7,Bkx29TVFPr,An implicit function learning approach for parametric modal regression,Reject,"The paper proposes an implicit function approach to learning the modes of multimodal regression. The basic idea is interesting, and is clearly related to density estimation, which the paper does not discuss. 

Based on the reviews and the fact that the authors did not submit a helpful rebuttal, I recommend rejection.",Paper Decision
tG1zbTUaSh,SkgscaNYPS,The asymptotic spectrum of the Hessian of DNN throughout training,Accept (Poster),"This paper studies the spectrum of the Hessian through training, making connections with the NTK limit. While many of the results are perhaps unsurprising, and more empirically driven, together the paper represents a valuable contribution towards our understanding of generalization in deep learning. Please carefully account for the reviewer comments in the final version.",Paper Decision
Do9mqq9fUx,SJe9qT4YPr,RISE and DISE: Two Frameworks for Learning from Time Series with Missing Data,Reject,"The paper attacks the important problem of learning time series models with missing data and proposes two learning frameworks, RISE and DISE, for this problem. The reviewers had several concerns about the paper and experimental setup and agree that this paper is not yet ready for publication. Please pay careful attention to the reviewer comments and particularly address the comments related to experimental design, clarity, and references to prior work while editing the paper.",Paper Decision
ozBpXh05D,B1x996EKPS,Fast Machine Learning with Byzantine Workers and Servers,Reject,"This paper is concerned with learning in the context of so-called Byzantine failures. This is relevant for for example distributed computation of gradients of mini-batches and parameter updates. The paper introduces the concept and Byzantine servers and gives theoretical and practical results for algorithm for this setting.

The reviewers had a hard time evaluating this paper and the AC was unable to find an expert reviewer. Still, the feedback from the reviewers painted a clear picture that the paper did not do enough to communicate the novel concepts used in the paper.

Rejection is recommended with a strong encouragement to use the feedback to improve the paper for the next conference.",Paper Decision
Lwturqal5,Byx55pVKDB,How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks,Reject,"
The paper investigates how the softmax activation hinders the detection of out-of-distribution examples.

All the reviewers felt that the paper requires more work before it can be accepted. In particular, the reviewers raised several concerns about theoretical justification, comparison to other existing methods, discussion of connection to existing methods and scalability to larger number of classes.

I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue.

",Paper Decision
eWkLLI5xLC,HJxK5pEYvr,Tree-Structured Attention with Hierarchical Accumulation,Accept (Poster),This paper incorporates tree-structured information about a sentence into how transformers process it. Results are improved. The paper is clear. Reviewers liked it. Clear accept.,Paper Decision
VlxKilPd5g,B1gF56VYPH,"Deep 3D Pan via local adaptive ""t-shaped"" convolutions with global and local adaptive dilations",Accept (Poster),"Two reviewers recommend acceptance while one is negative. The authors propose t-shaped kernels for view synthesis, focusing on stereo images. AC finds the problem and method interesting and the results to be sufficiently convincing to warrant acceptance.",Paper Decision
bKIuvP943r,ryedqa4FwS,MANAS: Multi-Agent Neural Architecture Search,Reject,"This paper introduces a NAS algorithm based on multi-agent optimization, treating each architecture choice as a bandit and using an adversarial bandit framework to address the non-stationarity of the system that results from the other bandits running in parallel.

Two reviewers ranked the paper as a weak accept and one ranked it as a weak reject. The rebuttal answered some questions, and based on this the reviewers kept their ratings. The discussion between reviewers and AC did not result in a consensus. The average score was below the acceptance threshold, but since it was close I read the paper in detail myself before deciding.

Here is my personal assessment:

""
Positives:
1. It is very nice to see some theory for NAS, as there isn't really any so far. The theory for MANAS itself does not appear to be very compelling, since it assumes that all but one bandit is fixed, i.e., that the problem is stationary, which it clearly isn't. But if I understand correctly, MANAS-LS does not have that problem. (It would be good if the authors could make these points more explicit in future versions.)

2. The absolute numbers for the experimental results on CIFAR-10 are strong.

3. I welcome the experiments on 3 additional datasets.

Negatives:
1. The paper crucially omits a comparison to random search with weight sharing (RandomNAS-WS) as introduced by Li & Talwalkar's paper ""Random Search and Reproducibility for Neural Architecture Search"" (https://arxiv.org/abs/1902.07638), on arXiv since February and published at UAI 2019. This method is basically MANAS without the update step, using a uniform random distribution at step 3 of the algorithm, and therefore would be the right baseline to see whether the bandits are actually learning anything. RandomNAS-WS has the same memory improvements over DARTS as MANAS, so this part is not new. Similarly, there is GDAS as another recent approach with the same low memory requirement: http://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Searching_for_a_Robust_Neural_Architecture_in_Four_GPU_Hours_CVPR_2019_paper.html
This is my most important criticism.

2. I think there may be a typo somewhere concerning the runtimes of MANAS. It would be extremely surprising if MANAS truly takes 2.5 times longer when run with 20 cells and 500 epochs than when run with 8 cells and 50 epochs. It would make sense if MANAS gets 2.5 slower when just going from 8 to 20 cells, but when going from 50 to 500 epochs the cost should go up by another factor of 10. And the text states specifically that ""for datasets other than ImageNet, we use 500 epochs during the search phase for architectures with 20 cells, 400 epochs for 14 cells, and 50 epochs for 8 cells"". Therefore, I think either that text is wrong or MANAS got 10x more budget than DARTS.

3. Figure 2 shows that on Sport-8, MANAS actually does *significantly worse* when searching on 14 cells than on 8 cells (note the different scale of the y axis). It's also slightly better with 8 cells on MIT-67. I recommend that the authors discuss this in the text and offer some explanation, rather than have the text claim that 14 cells are better and the figure contradict this. Only for MANAS-LS, the 14-cell version actually works better.

4. The authors are unclear about whether they compare to random search or random sampling. These are two different approaches. Random sampling (as proposed by Sciuto et al, 2019) takes a single random architecture from the search space and compares to that. Standard random search iteratively samples N random architectures and evaluates them (usually on some proxy metric), selecting and retraining the best one found that way. The number N is chosen for random search to use the same computational resources as the method being compared. The authors call their method random search but then appear to be describing random sampling.

Also, with several recent papers showcasing problems in NAS evaluation (many design decisions affect NAS performance), it would be a big plus to have code available to ensure reproducibility. Many ICLR papers are submitted with an anonymized code repository, and if possible, I would encourage the authors to do this for a future version.
""

The prior rating based on the reviewers was slightly below the acceptance threshold, and my personal judgement did not push the paper above the acceptance threshold. I encourage the authors to improve the paper by addressing the reviewer's points and the points above and resubmit to a future venue. Overall, I believe this is very interesting work and am looking forward to a future version.",Paper Decision
ZCcoumT7i_,BygPq6VFvS,Enhancing Attention with Explicit Phrasal Alignments,Reject,"This paper proposes a phrase-based attention method to model word n-grams (as opposed to single words) as the basic attention units. Multi-headed phrasal attentions are designed within the Transformer architecture to perform token-to-token and token-to-phrase mappings. Some improvements are shown in English-German, English-Russian and English-French translation tasks on the standard WMT'14 test set, and on the one-billion-word language modeling benchmark.

While the proposed approach is interesting and takes inspiration in the notion of phrases used in phrase-based machine translation, with some positive empirical results, the technical novelty of this paper is rather limited, and the experiments could be more solid. While it is understandable that lack of computational resources made it hard to experiment with larger models (e.g. Transformer-big), perhaps it would be interesting to try on language pairs with fewer resources (smaller datasets), where base models are more competitive.",Paper Decision
D73iDiSRml,B1xv9pEKDS,LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning,Reject,"This paper proposes a two-stage distillation from pretrained language models, where the knowledge distillation happens in both the pre-training and the fine-tune stages.  Experiments show improvement on BERT, GPT and MASS.  All reviewers pointed that the novelty of the work is very limited.",Paper Decision
_uHA3IVXU,Syl89aNYwS,Robust saliency maps with distribution-preserving decoys,Reject,"This submission proposes a method to explain deep vision models using saliency maps that are robust to certain input perturbations.

Strengths:
-The paper is clear and well-written.
-The approach is interesting.

Weaknesses:
-The motivation and formulation of the approach (e.g. coherence vs explanation and the use of decoys) was not convincing.
-The validation needs additional experiments and comparisons to recent works.

These weaknesses were not sufficiently addressed in the discussion phase. AC agrees with the majority recommendation to reject.",Paper Decision
z4j4TXaTk,r1e8qpVKPS,Role of two learning rates in convergence of model-agnostic meta-learning,Reject,"This paper theoretically and empirically studies the inner and outer learning rate of the MAML algorithm and their role in convergence. While the paper presents some interesting ideas and add to our theoretical understanding of meta-learning algorithms, the reviewers raised concerns about the relevance of the theory. Further the empirical study is somewhat preliminary and doesn't compare to prior works that also try to stabilize the MAML algorithm, further bringing into question its usefulness. As such, the current form of the paper doesn't meet the bar for ICLR.",Paper Decision
LgMdEK4iNK,rJeIcTNtvS,Low-Resource Knowledge-Grounded Dialogue Generation,Accept (Poster),"The paper considers the problem of knowledge-grounded dialogue generation with low resources. The authors propose to disentangle the model into three components that can be trained on separate data, and achieve SOTA on three datasets.

The reviewers agree that this is a well-written paper with a good idea, and strong empirical results, and I happily recommend acceptance.",Paper Decision
OY4uYTwyvg,rygHq6EFwr,GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation,Reject,"This paper studies the “suspended animation limit” of various graph neural networks (GNNs) and provides some theoretical analysis to explain its cause. To overcome the limitation, the authors propose Graph Residual Network (GRESNET) framework to involve nodes’ raw features or intermediate representations throughout the graph for all the model layers. The main concern of the reviewers is: the assumption made for theoretical analysis that the fully connected layer is identical mapping is too stringent. The paper does not gather sufficient support from the reviewers to merit acceptance, even after author response and reviewer discussion.  I thus recommend reject.",Paper Decision
3LwRINrwzV,SklVqa4YwH,Realism Index: Interpolation in Generative Models With Arbitrary Prior,Reject,"This paper introduces a realism metric for generated covariates and then leverage this metric to produce a novel method of interpolating between two real covariates. The reviewers found the method novel and were satisfied with the response form the authors to their concerns. However, Reviewer 4 did have reservations about the response to his/her points 3 and 4. Moreover, in the discussion period it was decided that while the method was well justified by intuition and theory, the empirical evaluation—which is the what matters at the end of the day—was unconvincing.  ",Paper Decision
CJxzxuzknN,ryeN5aEYDH,"Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities",Reject,"The reviewers all believe that this paper is not yet ready for publication. All agree that this is an important application, and an interesting approach. The methodological novelty, as well as other parts of exposition, involving related work, or further discussion of what this solution means for patients, is right now not completely convincing to reviewers. My recommendation is to work on making sure the exposition best explains the methodology, and making sure this venue is the best for the submitted line of work.",Paper Decision
wSPuo7okhR,BylVcTNtDS,A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning,Accept (Poster),"The reviewers were generally in agreement that the paper presents a valuable contribution and should be accepted for publication. However, I would strongly encourage the authors to carefully read over the reviews and address the suggestions and concerns insofar as possible for the final.",Paper Decision
KgDlu8OMy,Bkg75aVKDH,Training Provably Robust Models by Polyhedral Envelope Regularization,Reject,"The authors develop a new technique for training neural networks to be provably robust to adversarial attacks. The technique relies on constructing a polyhedral envelope on the feasible set of activations and using this to derive a lower bound on the maximum certified radius. By training with this as a regularizer, the authors are able to train neural networks that achieve strong provable robustness to adversarial attacks.

The paper makes a number of interesting contributions that the reviewers appreciated. However, two of the reviewers had some concerns with the significance of the contributions made:
1) The contributions of the paper are not clearly defined relative to prior work on bound propagation (Fast-Lin/KW/CROWN). In particular, the authors simply use the linear approximation derived in these prior works to obtain a bound on the radius to be certified. The authors claim faster convergence based on this, but this does not seem like a very significant contribution.

2) The improvements on the state of the art are marginal.

These were discussed in detail during the rebuttal phase and the two reviewers with concerns about the paper decided to maintain their score after reading the rebuttals, as the fundamental issues above were not 

Given these concerns, I believe this paper is borderline - it has some interesting contributions, but the overall novelty on the technical side and strength of empirical results is not very high.",Paper Decision
6Hprl72mPq,HJlQ96EtPr,FleXOR: Trainable Fractional Quantization,Reject,"This work studies parameter quantization using binary codes and proposes an encryption algorithm/architecture to compress quantized weights and achieve fractional numbers of bits per weight, and to perform decryption using XOR gates. The authors conduct experiments on datasets including ImageNet to evaluate their scheme.
Much of the concern from reviewers relates to baseline comparison and details around that. Specifically, R1 believes that the submission could have a bigger impact if authors could conduct more thorough experiments, e.g. compressing more widely-used and challenging architecture of ResNet-50, or trying tasks such as image detection (Mask R-CNN). The authors' responded to that and mentioned their choice of the current experimental setting is to facilitate comparison with previous works (baselines), which use similar experimental settings. Nevertheless, the baseline methods could have been attempted by the authors on broader tasks, or more widely-used architectures could have been investigated by authors on the baseline methods. As a result, R1 was not convinced. To ensure the paper receives the attention it deserves, I recommend considering a more thorough evaluation of the proposed method against baseline methods.",Paper Decision
7LJ_Rv1GB,r1ezqaEFPr,Multi-Task Learning via Scale Aware Feature Pyramid Networks and Effective Joint Head,Reject,"All three reviewers gave scores of Weak Reject. Only a brief rebuttal was offered, which did not change the scores. Thus the paper connect be accepted. ",Paper Decision
xTVs_Gu3jo,r1l-5pEtDr,AdaX: Adaptive Gradient Descent with Exponential Long Term Memory,Reject,"This paper analyzes the non-convergence issue in Adam in a simple non-convex case. The authors propose a new adaptive gradient descent algorithm based on exponential long term memory, and analyze its convergence in both convex and non-convex settings. The major weakness of this paper pointed out by many reviewers is its experimental evaluation, ranging from experimental design to missing comparison with strong baseline algorithms. I agree with the reviewers’ evaluation and thus recommend reject.",Paper Decision
uf525O9HLb,BJl-5pNKDB,On Computation and Generalization of Generative Adversarial Imitation Learning,Accept (Poster),"The paper provides a theoretical analysis of the recent and popular Generative Adversarial Imitation Learning (GAIL) approach. Valuable new insights on generalization and convergence are developed, and put GAIL on a stronger theoretical foundation. Reviewer questions and suggestions were largely addressed during the rebuttal.",Paper Decision
CA5RoG3anA,rkeZ9a4Fwr,Disentangling Improves VAEs' Robustness to Adversarial Attacks,Reject,"This work a ""Seatbelt-VAE"" algorithm to improve the robustness of VAE against adversarial attacks. The proposed method is promising but the paper appears to be hastily written and leave many places to improve and clarify. This paper can be turned into an excellent paper with another round of throughout modification. 


",Paper Decision
ElJH0lSuMT,S1el9TEKPB,Sparsity Meets Robustness: Channel Pruning for the Feynman-Kac Formalism Principled Robust Deep Neural Nets,Reject,The paper is rejected based on unanimous reviews.,Paper Decision
Rrine5bsTk,Bkeeca4Kvr,FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES,Accept (Poster),The authors propose a method for few-shot learning for graph classification. The majority of reviewers agree on the novelty of the proposed method and that the problem is interesting. The authors have addressed all major concerns.,Paper Decision
jnsVUDsBm,BJgy96EYvr,Influence-Based Multi-Agent Exploration,Accept (Spotlight),"The paper presents a new take on exploration in multi-agent reinforcement learning settings, and presents two approaches, one motivated by information theoretic, the other by decision theoretic influence on other agents. Reviewers consider the proposed approach ""pretty elegant, and in a sense seem fundamental"", the experimental section ""thorough"", and expect the work to ""encourage future work to explore more problems in this area"". Several questions were raised, especially regarding related work, comparison to single agent exploration approaches, and several clarifying questions. These were largely addressed by the authors, resulting in a strong submission with valuable contributions.",Paper Decision
qBe2DsrRX8,BklRFpVKPH,Demonstration Actor Critic,Reject,"The paper proposes to combine RL and Imitation Learning. It defines a regularized reward function that minimizes the KL distance between the policy and the expert action. The formulation is similar to the KL regularized MDPs, but with the difference that an additional indicator function based on the support of the expert’s distribution is multiplied to the regularized term.

Several issues have been brought up by the reviewers, including:
* Comparison with pre-deep learning literature on the combination of RL and imitation learning
* Similarity to regularized MDP framework
* Assumption 1 requiring a stochastic expert policy, contradicting the policy invariance claim
* Difficulty of learning the indicator function of the support of the expert’s data distribution

Some of these issues have been addressed, but at the end of the day, one of the expert reviewers was not convinced that the problem of learning an indicator function is going to be easy at all. The reviewer believes that learning such a function requires ""learning a harsh approximation of the density of visits of the expert for every state which is a quite hard task, especially in stochastic environments.” 

Another issue is related to the policy invariance under the optimal expert policy. In most MDPs, the optimal policy is not stochastic and does not satisfy Assumption 1, so the optimal policy invariance proof seems to contradict Assumption 1.

Overall, it seems that even though this might become a good paper, it requires some improvements. I encourage the authors to address the reviewers’ comments as much as possible.",Paper Decision
dh14GiB-7,HklRKpEKDr,Deep Coordination Graphs,Reject,"This work extends previous work (Castellini et al) with parameter sharing and  low-rank approximations, for pairwise communication between agents. 
However the work as presented here is still considered too incremental, in particular when compared to Castellini et al.
The advances such as parameter sharing and low-rank approximation are good but not enough of a contribution. Authors' efforts to address this concern did not change reviewers' judgment.
Therefore, we recommend rejection.",Paper Decision
RDIQtXNkno,SJxRKT4Fwr,"Cross-Dimensional Self-Attention for Multivariate, Geo-tagged Time Series Imputation",Reject,"The paper proposes a solution based on self-attention RNN to addressing the missing value in spatiotemporal data. 

I myself read through the paper, followed by a discussion with the reviewers. We agree that the model is reasonable, and the results are promising. However, there is still some room for improvement:
1. The self-attention mechanism is not new. The specific way proposed in the paper is an interesting tweak of existing models, but not brand new per se. Most importantly, it is unclear if the proposed way is the optimal one and where the performance improvement comes from. As the reviewer suggested, more thorough empirical analysis should be performed for deeper insights of the model. 

2. The datasets were adopted from existing work, but most of them do not have such complex models as the one proposed in the paper. Therefore, the suggestion for bigger datasets is valid. 

Given the considerations above, we agree that while the paper has a lot of good materials, the current version is not ready yet. Addressing the issues above could lead to a good publication in the future. ",Paper Decision
4r93aL6xQo,BylTta4YvB,How Well Do WGANs Estimate the Wasserstein Metric?,Reject,"There is insufficient support to recommend accepting this paper.  Generally the reviewers found the technical contribution to be insufficient, and were not sufficiently convinced by the experimental evaluation.  The feedback provided should help the authors improve their paper.",Paper Decision
AeWBEF7_Kv,BJl6t64tvr,Revisiting the Generalization of Adaptive Gradient Methods,Reject,"The paper combines several recent optimizer tricks to provide empirical evidence that goes against the common belief that adaptive methods result in larger generalization errors. The contribution of this paper is rather small: no new strategies are introduced and no new theory is presented. The paper makes a good workshop paper, but does not meet the bar for publication at ICLR.
",Paper Decision
ArfAd6UI48,rylnK6VtDH,Multiplicative Interactions and Where to Find Them,Accept (Poster),"This paper provides a unifying perspective regarding a variety of popular DNN architectures in terms of the inclusion of multiplicative interaction layers.  Such layers increase the representational power of conventional linear layers, which the paper argues can induce a useful inductive bias in practical scenarios such as when multiple streams of information are fused.  Empirical support is provided to validate these claims and showcase the potential of multiplicative interactions in occupying broader practical roles.

All reviewers agreed to accept this paper, although some concerns were raised in terms of novelty, clarity, and the relationship with state-of-the-art models.  However, the author rebuttal and updated revision are adequate, and I believe that this paper should be accepted.",Paper Decision
AvSsmGIvAR,rJejta4KDS,SELF-KNOWLEDGE DISTILLATION ADVERSARIAL ATTACK,Reject,"This paper proposes an attack method to improve the transferability of adversarial examples under black-box attack settings.

Despite the simplicity of the proposed idea, reviewers and AC commonly think that the paper is far from being ready to publish in various aspects: (a) the presentation/writing quality, (b) in-depth analysis and (c) experimental results.

Hence, I recommend rejection.",Paper Decision
vwg2k1G3h,rJxotpNYPS,DIVA: Domain Invariant Variational Autoencoder,Reject,"This paper addresses the problem of domain generalization. The proposed solution, DIVA, introduces a domain invariant variational autoencoder. The latent space can be decomposed into three components: category specific, domain specific, and residual. The authors argue that each component is necessary to capture all relevant information while keeping the latent space interpretable.

This work received mixed scores. Two reviewers recommended weak reject while one reviewer recommended weak accept. There was extensive discussion between the reviewers and authors as well as amongst the reviewers. All reviewers agreed this is an important problem statement and that this work offers a compelling initial approach and experiments for domain generalization. There was disagreement as to whether the contributions as is was sufficient for acceptance. Some reviewers were concerned over similarity to [ref1], this work appears close to the time of ICLR submission and is therefore considered concurrent. However, despite this, there was significant confusion over the proposed solution and whether it is uniquely useful for domain generalization or for other areas like adaptation or transfer learning with reviewers arguing that experiments in these other settings would have helped showcase the benefits of the proposed approach. In addition, there was inconclusive evidence as to whether the two latent components were necessary. 

Considering all discussions, reviews, and rebuttals the AC does not recommend this work for acceptance. The contribution and proposed solution needed substantial clarification and the experiments need additional analysis to explain under what conditions each latent component is needed either to improve performance or for interpretability.
",Paper Decision
yyJKFg7sXb,SJlsFpVtDB,Continual Learning with Bayesian Neural Networks for Non-Stationary Data,Accept (Poster),"This paper introduces an algorithm for online Bayesian learning of both streaming and non-stationary data.  The algorithmic choices are heuristic but motivated by sensible principles.  The reviewers' main concerns were with novelty, but because the paper was well-written and addressing an important problem they all agreed it should be accepted.",Paper Decision
Jhvw2ffPf,BJgctpEKwr,RPGAN: random paths as a latent space for GAN interpretability,Reject,"The paper received mixed scores: Weak Reject (R1 and R2) and Accept (R3). AC has closely read the reviews/comments/rebuttal and examined the paper. After the rebuttal, R2's concerns still remain. AC sides with R2 and feels that the generated interpretations are not convincing, and that the conclusions drawn are not fully supported. Thus the paper just falls below the acceptance threshold, unfortunately. The work has merits however and the authors should revise their paper to incorporate the constructive feedback.",Paper Decision
23W1iqagLq,rye5YaEtPr,SAdam: A Variant of Adam for Strongly Convex Functions,Accept (Poster),"The reviewers all appreciated the results. They expressed doubts regarding the discrepancy between the assumptions made and the reality of the loss of deep networks.

I share these concerns with the reviewers but also believe that, due to the popularity of Adam, a careful analysis of a variant is worthy of publication.",Paper Decision
rdclXO67Yn,B1xtFpVtvB,Improving the Generalization of Visual Navigation Policies using Invariance Regularization,Reject,All the reviewers recommend rejecting the submission. There is no basis for acceptance.,Paper Decision
xf91Fi33Qj,r1e_FpNFDr,Generalization bounds for deep convolutional neural networks,Accept (Poster),"The authors present several theorems bounding the generalization error of a class of conv nets (CNNs) with high probability by 
    
      O(sqrt(W(beta + log(lambda)) + log(1/delta)]/sqrt(n)), 

where W is the number of weights, beta is the distance from initialization in operator norm, lambda is the margin, n is the number of data, and the bound holds with prob. at least 1-delta. (They also present a bound that is tighter when the empirical risk is small.)

The bounds are ""size free"" in the sense that they do not depend on the size of the *input*, which is assumed to be, say, a d x d image. While there is dependence on the number of parameters, W, there is no implicit dependence on d here.

The paper received the following feedback:

1. Reviewer 3 mostly had clarifying questions, especially with respect to (essentially independent) work by Wei and Ma. Reviewer 3 also pressed the authors to discuss how the bounds compared in absolute terms to the bounds of Bartlett et al. The authors stated that they did not have explicit constants to make such a comparison. Reviewer 3 was satisfied enough to raise their score to a 6.

2. Reviewer 1 admitted they were not experts and raised some issues around novelty/simplicity. I do not think the simplicity of the paper is a drawback. The reviewers unfortunately did not participate in the rebuttal, despite repeated attempts.

3. Reviewer 2 argued for weak reject, despite an interaction with the authors. The reviewer raised the issue of bounds based on control of the Lipschitz constant. The conversation was slightly marred by a typo in the reviewers original comment. I don't believe the authors ultimately responded to the reviewer's point. There was another discussion about simultaneously work and compression-based bounds. I would agree with the authors that they need not have cited simultaneous work, especially since the details are quite different. Ultimately, this reviewer still argued for rejection (weakly).

After the rebuttal period ended, the reviewers raised some further concerns with me. I tried to assess these on my own, and ended up with my own questions.

I raise these in no particular order. Each of them may have a simple resolution. In that case, the authors should take them as possible sources of confusion. Addressing them may significantly improve the readability of the paper.

i. Lemma A.3. The order of quantification is poorly expressed and so I was not confident in the statement. In particular, the theorem starts \forall \eta >0 \exists C, .... but then C is REINTRODUCED later, subsequent to existential quantification over M, B, and d and so it seems there is dependence. If there is no dependence, this presentation is sloppy and should be fixed.

ii. Lemma A.4, the same dependence of C on M, B and d holds here and this is quite problematic for the later applications. If this constant is independent of these quantities, then the order of quantifiers has been stated incorrectly. Again, this is sloppy if it is wrong. If it's correct, then we need to know how C grows.

Based on other claims by the authors, it is my understanding that, in both cases, the constant C does not depend on M, B, or d. Regardless, the authors should clarify the dependence. If C does in fact depend on these quantities, and the conclusions change, the paper should be retracted.

iii. Proof of Lemma 2.3. I'd remind the reader that the parametrization maps the unit ball to G. 

iv. The bound depends on control of operator norms and empirical margins. It is not clear how these interact and whether, for margin parameters necessary to achieve small empirical margin risk, the bounds pick up dependence on other aspects of the learning problem (e.g., depth). I think the only way to assess this would be to investigate these quantities empirically, say, by varying the size and depth of the network on a fixed data set, trained to achieve the same empirical risk (or margin).

I'll add that I was also disappointed that the authors did not attempt to address any of the issues by a revision of the actual paper. In particular, the authors promise several changes that would have been straightforward to make in the two weeks of rebuttal. Instead, the reviewers and myself are left to imagine how things would change. I see at least two promises:

A. To walk back some of the empirical claims about distance from initialization that are based on somewhat flimsy empirical evaluations. I would add to this the need to investigate how the margin and operator norms depend on depth empirically.

B. Attribute Dziugate and Roy for establishing the first bounds in terms of distance from initialization, though their bounds were numerical. I think a mention of simultaneously work would also be generous, even if not strictly necessary.
",Paper Decision
2QdfhKRESc,BJedt6VKPS,"Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks",Reject,"This paper proposes a new design space for initialization of neural networks motivated by balancing the singular values of the Hessian. Reviewers found the problem well motivated and agreed that the proposed method has merit, however more rigorous experiments are required to demonstrate that the ideas in this work are significant progress over current known techniques. As noted by Reviewer 2, there has been substantial prior work on initialization and conditioning that needs to be discussed as they relate to the proposed method. The AC notes two additional, closely related initialization schemes that should be discussed [1,2]. Comparing with stronger baselines on more recent modern architectures would improve this work significantly.

[1]: https://nips.cc/Conferences/2019/Schedule?showEvent=14216
[2]: https://arxiv.org/abs/1901.09321.",Paper Decision
b0Xl3w0o2P,HygDF6NFPB,A Fair Comparison of Graph Neural Networks for Graph Classification,Accept (Poster),"The paper provides a careful, reproducible empirical comparison of 5 graph neural network models on 9 datasets for graph classification. The paper shows that baseline methods that use only node features (either counting node types, or summing node features) can be competitive. The authors also provide some guidelines for ways to improve reproducibility in empirical comparisons of graph classification.

The authors responded well to the issued raised during review, and updated the paper during the discussion period. The reviewers improved their score, and while there were reservations about the comprehensiveness of the set of experiments, they all agreed that the paper provides a solid empirical contribution to the literature.

As machine learning becomes increasingly popular, papers that perform a careful empirical survey of baselines provide an important sanity check that future work can be built upon. Therefore, this paper, while not covering all possible graph neural network questions, provides an excellent starting point for future work to extend.

",Paper Decision
2ZzD4djzx,rylvYaNYDH,Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents,Accept (Poster),"This paper proposes a tool to visualizing the behaviour of deep RL agents, for example to observe the behaviour of an agent in critical scenarios. The idea is to learn a generative model of the environment and use it to artificially generate novel states in order to induce specific agent actions. States can then be generated such as to optimize a given target function, for example states where the agent takes a specific actions or states which are high/low reward. They evaluate the proposed visualization on Atari games and on a driving simulation environment, where the authors use their approach, to investigate the behaviour of different deep RL agents such as DQN.

The paper is very controversial. On the one hand, as far as we know, this is the first approach that explicitly generates states that are meant to induce specific agent behaviour, although one could relate this to adversarial samples generation. Interpretability in deep RL is a known problem and this work could bring an interesting tool to the community. However, the proposed approach lacks theoretical foundations, thus feels quite ad-hoc, and results are limited to a qualitative, visual, evaluation. At the same time, one could say that the approach is not more ad hoc than other gradient saliency visualization approaches, and one could argue that the lack of theoretical soundness is due to the difficulty of defining good measures of interpretability and that apply well to image-based environments.

Nonetheless, this paper is a step in the good direction in a field that could really benefit from it. ",Paper Decision
JKgWnUubmK,SkxLFaNKwB,Computation Reallocation for Object Detection,Accept (Poster),"The submission applies architecture search to object detection architectures. The work is fairly incremental but the results are reasonable. After revision, the scores are 8, 6, 6, 3. The reviewer who gave ""3"" wrote after the authors' responses and revision that ""Authors' responses partly resolved my concerns on the experiments. I have no object to accept this paper. [sic]"". The AC recommends adopting the majority recommendation and accepting the paper.",Paper Decision
qzM8xZN2H,SJeItTEKvr,MULTI-LABEL METRIC LEARNING WITH BIDIRECTIONAL REPRESENTATION DEEP NEURAL NETWORKS,Reject,"All reviewers agreed that this submission is still premature to be accepted to ICLR2020.
We hope the review comments are useful for improving your paper for potential future submission.",Paper Decision
Bf7hRLoIn8,ByeSYa4KPS,Sparse Networks from Scratch: Faster Training without Losing Performance,Reject,"This paper presents a method for training sparse neural networks that also provides a speedup during training, in contrast to methods for training sparse networks which train dense networks (at normal speed) and then prune weights.

The method provides modest theoretical speedups during training, never measured in wallclock time.   The authors improved their paper considerably in response to the reviews.  I would be inclined to accept this paper despite not being a big win empirically, however a couple points of sloppiness pointed out (and maintained post-rebuttal) by R1 tip the balance to reject, in my opinion.  Specifically: 

1) ""I do not agree that keeping the learning rate fixed across methods is the right approach.""  This seems like a major problem with the experiments to me.

2) ""I would request the authors to slightly rewrite certain parts of their paper so as not to imply that momentum decreases the variance of the gradients in general.""  I agree.",Paper Decision
BXD-MFJ_4F,BklHF6VtPB,Modeling Winner-Take-All Competition in Sparse Binary Projections,Reject,"This paper proposes a WTA models for binary projection.  While there are notable partial contributions, there is disagreement among the reviewers.   I am most persuaded by the concern expressed that the experiments are not done on datasets that are large enough to be state-of-the-art compared to other random projection investigations.",Paper Decision
004KdyPaY3,HygHtpVtPH,Laplacian Denoising Autoencoder,Reject,"The main idea proposed by the work is interesting. The reviewers had several concerns about applicability and the extent of the empirical work. The authors responded to all the comments, added more experiments, and as reviewer 2 noted, the method is interesting because of its ability to handle local noise. Despite the author's helpful responses, the ratings were not increased, and it is still hard to assess the exact extent of how the proposed approach improves over state of the art.   Because some concerns remained, and due to a large number of stronger papers, this paper was not accepted at this time.",Paper Decision
DxSIb7XWRj,rkxEKp4Fwr,Training Data Distribution Search with Ensemble Active Learning,Reject,This paper proposes an ensemble-based active learning approach to select a subset of training data that yields the same or better performance. The proposed method is rather heuristic and lacks novel technical contribution that we expect for top ML conferences. No theoretical justification is provided to argue why the proposed method works. Additional studied are needed to fully convincingly demonstrate the benefit of the proposed method in terms computational cost. ,Paper Decision
UT5c0E6gOU,BklEFpEYwS,Meta-Learning without Memorization,Accept (Spotlight),"The paper introduces the concept of overfitting in meta learning and proposes some solutions to address this problem. Overall, this is a good paper. It would be good if the authors could relate this work to meta learning approaches, which are based on hierarchical (Bayesian) modeling for learning a task embedding.

[1] Hausman et al. (ICLR 2018): Learning an Embedding Space for Transferable Robot Skills 
https://openreview.net/pdf?id=rk07ZXZRb
[2] Saemundsson et al. (UAI 2018): Meta Reinforcement Learning with Latent Variable Gaussian Processes
http://auai.org/uai2018/proceedings/papers/235.pdf
",Paper Decision
stdCLb7Z5,S1g7tpEYDS,From Variational to Deterministic Autoencoders,Accept (Poster),"This paper proposes an extension to deterministic autoencoders, namely instead of noise injection in the encoders of VAEs to use deterministic autoencoders with an explicit regularization term on the latent representations. While the reviewers agree that the paper studies an important question for the generative modeling community, the paper has been limited in terms of theoretical analysis and experimental validation. The authors, however, provided further experimental results to support the claims empirically during the discussion period and the reviewers agree that the paper is now acceptable for publication in ICLR-2020. ",Paper Decision
oVPgWgNkJc,H1gfFaEYDS,Adversarially Robust Representations with Smooth Encoders,Accept (Poster),This paper proposes an novel way of expanding our VAE toolkit by tying it to adversarial robustness. It should be thus of interest to the respective communities.,Paper Decision
pperiADu3,SJgMK64Ywr,AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures,Accept (Poster),"The submission applies architecture search to find effective architectures for video classification. The work is not terribly innovative, but the results are good. All reviewers recommend accepting the paper.",Paper Decision
XsRIyxQJzk,SklfY6EFDH,Representation Quality Explain Adversarial Attacks,Reject,"The reviewers found the aim of the paper interesting (to connect representation quality with adversarial examples). However, the reviewers consistently pointed out writing issues, such as inaccurate or unsubstantiated claims, which are not appropriate for a scientific venue. The reviewers also found the experiments, which are on simple datasets, unconvincing.",Paper Decision
ZQNCMFufj5,rylZKTNYPr,Inferring Dynamical Systems with Long-Range Dependencies through Line Attractor Regularization,Reject,"The paper proposes  an interesting idea to leave a very simple form for piecewise-linear RNN, but separate units in to two types, one of which acts as memory. The ""memory"" units are penalized towards the linear attractor parameters, i.e. making elements of $A$ close to 1 and off-diagonal of $W$ close to $1$. 
The benchmarks are presented that confirm the efficiency of the model.
The reviewer opinion were mixed; one ""1"", one ""3"" and one ""6""; the Reviewer1 is far too negative and some of his claims are not very constructive, the ""positive"" reviewer is very short. Finally, the last reviewer raised a question about the actual quality on the results. This is not addressed. Although there is a motivation for such partial regularization, the main practical question is how many ""memory neurons"" are needed. I looked through the paper - this addressed only in the supplementary, where the value of $M_{reg}$ is mentioned (=0.5 M). For $M_{reg} = M$ it is the L2 penalty; what happens if the fraction is 0.1, 0.2, ... and more? A very crucial hyperparameter (and of course, smart selection of it can not be worse than L2RNN). This study is lacking. In my opinion, one can also introduce weights and sparsity constraints on them (in order to detect the number of ""memory"" neurons more-or less automatically). Although I feel this paper has a potential, it is not still ready for publication and could be significantly improved.",Paper Decision
7roo50sba3,HkgbKaEtvB,End-To-End Input Selection for Deep Neural Networks,Reject,"This paper proposes to address the high bandwidth cost when transferring data between server and user for machine learning applications. The input data is augment with channel and spatial mask so that the file transfer cost is reduced. While the reviewers agree that this is a well motivated and interesting problem to study, a number of concerns are raised, including loosely specified performance/size trade-off, how this work is compared to related work, low novelty relative to a few key missing references. The authors respond to Reviewers’ concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state.",Paper Decision
IyNIqO03jK,rJeeKTNKDB,Hierarchical Graph-to-Graph Translation for Molecules,Reject,"Two reviewers are negative on this paper while the other reviewer is slightly positive. Overall, the paper does not make the bar of ICLR. A reject is recommended.",Paper Decision
EPDpHgJbe1,HJgJtT4tvB,ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning,Accept (Poster),"Main content:

Blind review #1 summarizes it well:

This paper presents a new reading comprehension dataset for logical reasoning. It is a multi-choice problem where questions are mainly from GMAT and LSAT, containing 4139 data points. The analyses of the data demonstrate that questions require diverse types of reasoning such as finding necessary/sufficient assumptions, whether statements strengthen/weaken the argument or explain/resolve the situation. The paper includes comprehensive experiments with baselines to identify bias in the dataset, where the answer-options-only model achieves near half (random is 25%). Based on this result, the test set is split into the easy and hard set, which will help better evaluation of the future models. The paper also reports the numbers on the split data using competitive baselines where the models achieve low performance on the hard set.

--

Discussion:

While the authors agree this is an important direction, there are reservations concerning the small size of the dataset, that have not been fully addressed.

--

Recommendation and justifcation:

I still believe this paper should be accepted as the existing datasets for reading comprehension are inadequate and it is important for the field not to be climbing the wrong hill.",Paper Decision
A_3Ru-_Cw,SJlyta4YPS,DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine,Reject,"The authors address the problem of CTR prediction by using a Transformer based encoder to capture interactions between features. They suggest simple modifications to the basic Multiple Head Self Attention (MSHA) mechanism and show that they get the best performance on two publicly available datasets. 

While the reviewers agreed that this work is of practical importance, they had a few objections which I have summarised below: 
1) Lack of novelty: The reviewers felt that the adoption of MSHA for the CTR task was straightforward. The suggested modifications in the form of Bilinear similarity and max-pooling were viewed as incremental contributions. 
2) Lack of comparison with existing work: The reviewers suggested some additional baselines (Deep and Cross) which need to be added (the authors have responded that they will do so later).
3) Need to strengthen experiments: The reviewers appreciated the ablation studies done by the authors but requested for more studies to convincingly demonstrate the effect of some components. One reviewer also pointed that the authors should control form model complexity to ensure an apples-to-apples comparison (I agree that many papers in the past have not done this but going froward I have a hunch that many reviewers will start asking for this) . 

IMO, the above comments are important and the authors should try to address them in subsequent submissions.

Based on the reviewer comments and lack of any response from the authors, I recommend that the paper in it current form cannot be accepted. ",Paper Decision
vGutMyhG4O,H1l0O6EYDH,A NEW POINTWISE CONVOLUTION IN DEEP NEURAL NETWORKS THROUGH EXTREMELY FAST AND NON PARAMETRIC TRANSFORMS,Reject,"This paper presents an approach to utilize conventional frequency domain basis such as DWHT and DCT to replace the standard point-wise convolution, which can significantly reduce the computational complexity. The paper is generally well-written and easy to follow. However, the technical novelty seems limited as it is basically a simple combination of CNNs and traditional filters. Moreover, as reviewers suggested, it is our history and current consensus in the community that learned representations have significantly outperformed traditional pre-defined features or filters as the training data expands. I do understand the scientific value of revisiting and challenging that belief as commented by R1, but in order to provoke meaningful discussion, experiments on large-scale dataset like ImageNet are definitely necessary. For these reasons, I think the paper is not ready for publication at ICLR and would like to recommend rejection.",Paper Decision
07DbXqfBd,rJeA_aVtPB,Decaying momentum helps neural network training,Reject,"This paper proposes a new decaying momentum rule to improve existing optimization algorithms for training deep neural networks, including momentum SGD and Adam. The main objections from the reviewers include: (1) its novelty is limited compared with prior work; (2) the experimental comparison needs to be improved (e.g., the baselines might not be carefully tuned, and learning rate decay is not applied, while it usually boosts the performance of all the algorithms a lot). After reviewer discussion, I agree with the reviewers’ evaluation and recommend reject.",Paper Decision
vYHO8j_5Bv,S1xCuTNYDr,Regularizing Black-box Models for Improved Interpretability,Reject,"This paper investigates a promising direction on the important topic of interpretability; the reviewers find a variety of issues with the work, and I urge the authors to refine and extend their investigations.",Paper Decision
yUuhK3jPri,ryx6daEtwr,GPNET: MONOCULAR 3D VEHICLE DETECTION BASED ON LIGHTWEIGHT WHEEL GROUNDING POINT DETECTION NETWORK,Reject,"This paper aims to estimate the 3D location and orientation of vehicle from a 2D image. Instead of using a CNN-based 3D detection pipeline, the authors propose to detect the vehicle’s wheel grounding points and then using the ground plane constraint for the estimation. All three reviewers provided unanimous rating of rejection. Many concerns are raised by the reviewers, including poor generalization to new situations, small improvement over prior work, low presentation quality, the lack of detailed description of the experiments, etc. The authors did not respond to the reviewers’ comments. The AC agrees with the reviewers’ comments, and recommend rejection.",Paper Decision
6hkDr7JEGR,H1xTup4KPr,Needles in Haystacks: On Classifying Tiny Objects in Large Images,Reject,"This paper proposes performs an empirical study to evaluate CNN-based object classifier for the case where the object of interest is very small relative to the size of the image. Two synthetic databases are used to conduct the experiments, through which the authors made a number of observations and conclusions. The reviewers concern that the databases used are too structured or artificial, and one of the two databases is very small as well. On top of that, only one network architecture is used for evaluation. Furthermore, the conclusion from two databases seem inconsistent as well. The authors provided detailed responses to the reviewers' comments but were not able to change the overall rating of the paper. Given these concerns, as well as no methodological contribution, there are general concerns from all reviewers that the contributions of this work is not sufficient for ICLR. The ACs concur the concerns and the paper can not be accepted at its current state.",Paper Decision
pzaA-1Wmgn,HylhuTEtwr,The advantage of using Student's t-priors in variational autoencoders,Reject,"The consensus among all reviewers was to reject this paper, and the authors did not provide a rebuttal.",Paper Decision
_-wzDAie7v,SJgndT4KwB,Finite Depth and Width Corrections to the Neural Tangent Kernel,Accept (Spotlight),"This paper aims to study the mean and variance of the neural tangent kernel (NTK) in a randomly initialized ReLU network. The purpose is to understand the regime where the width and depth go to infinity together with a fixed ratio. The paper does not have a lot of numerical experiments to test the mathematical conclusions. In the discussion the reviewers concurred that the paper is interesting and has nice results but raised important points regarding the fact that only the diagonal elements are studied. This I think is the major limitation of this paper. Another issue raised was lack of experimental work validating the theory. Despite the limitations discussed above, overall I think this is an interesting and important area as it sheds light on how to move beyond the NTK regime. I also think studying this limit is very important to better understanding of neural network training. I recommend acceptance to ICLR.",Paper Decision
0QLXhEUQu,HygsuaNFwr,Order Learning and Its Application to Age Estimation,Accept (Poster),"This paper addresses a promising method for order learning and applies the new ideas of multiple-chain learning and anchor selection to age estimation and aesthetic regression. The decision regarding instance class is made by comparing it with anchor instances in the same chain and maximizing the consistency among the comparison results. In a multi-chain setting, each chain may correspond to a higher-level attribute class, for example, gender or ethnic group. Supervised and unsupervised learning of multiple ordered chains is proposed.  As rightly acknowledged by R4: “What more promising is the unsupervised chains, which could automatically search for a more optimal multi-chain division scheme than the pre-defined data division.”
All three reviewers and AC agree that the proposed approach is interesting and shows promising results. There are several potential weaknesses and suggestions to further strengthen this work:
(1) more quantitative results are needed for assessing the benefits of this approach (R3, R4) -- see R3’s request to complete the results for FG-Net, to include the results for CLAP2016 and a comparison with the SOTA method BridgeNet. Pleased to report that the authors have revised the manuscript and have included performance of the arithmetic scheme as well as the geometric scheme for FG-Net. Also the authors have provided some initial evaluations of BridgeNet and promised to report the final results as well as the results for CLAP2016 in the final version. 
(2) R3 and R4 have expressed concerns regarding using the geometric ratio of the class distances in age estimation and that the improvement may be caused by the data distribution that favours it (R4) or because the baseline methods are not fine-tuned in the same manner (R3). The authors have partially addressed this concern in the rebuttal. 
There is a large body of work in computer vision that is focused on relative comparison of samples based on attributes (e.g. age) that is not clearly articulated in the discussions / baseline comparisons (1CH) -- see the seminal work [Relative attributes by Parikh and Grauman, ICCV2011] and the follow up works. 
Considering the author response, the AC decided that the most crucial concerns have been addressed in the revision and that the paper could be accepted, but the authors are strongly urged to include additional results that were promised in the rebuttal for the final revision. 
",Paper Decision
1Ry-6dYPd,HJe5_6VKwS,Model-based Saliency for the Detection of Adversarial Examples,Reject,"This submission proposes a method for detecting adversarial attacks using saliency maps.

Strengths:
-The experimental results are encouraging.

Weaknesses:
-The novelty is minor.
-Experimental validation of some claims (e.g. robustness to white-box attacks) is lacking.

These weaknesses were not sufficiently addressed in the discussion phase. AC agrees with the majority recommendation to reject.
",Paper Decision
-0ePms0SIF,H1lKd6NYPS,Online Meta-Critic Learning for Off-Policy Actor-Critic Methods,Reject,"There was extension discussion of the paper between the reviewers. It's clear that the reviewers appreciated the main idea in the paper, and the notion of an ""online"" meta-critic that accelerates the RL process is definitely very appealing. However, there were unanswered questions about what the method is actually doing that make me reticent to recommend acceptance at this point. I would refer the authors to R3 and R1 for an in-depth discussion of the issues, but the short summary is that it's not clear whether, if, and how the meta-loss in this case actually converges, and what the meta-critic is actually doing. In the absence of a theoretical understanding for what the modification does to accelerate RL, we are left with the empirical experiments, and there it is necessary to consider alternative hypotheses and perform detailed ablation analyses to understand that the method really works for the reasons stated by the authors (and not some of the alternative explanations, see e.g. R3). While there is nothing wrong with a result that is primarily empirical, it is important to analyze that the empirical gains really are happening for the reasons claimed, and to carefully study convergence and asymptotic properties of the algorithm. The comparatively diminished gains with the stronger algorithms (TD3 and especially SAC) make me more skeptical. Therefore, I would recommend that the paper not be accepted at this time, though I encourage the authors to resubmit with a more in-depth experimental evaluation.",Paper Decision
kc7UvE4ROE,HJlY_6VKDr,BUZz: BUffer Zones for defending  adversarial examples in image classification,Reject,"This paper formalizes the concept of buffer zones, and proposes a defense method based on a combination of deep neural networks and simple image transformations. The authors argue that the proposed method based on buffer zones is robust against state-of-the-art black box attacks methods.This paper, however, falls short of (1) unjustified claims (e.g., buffer zones are widened when the models are diverse); (2) incomplete literature survey and related work; (3) similar ideas are well-known in the literature, (4) unfair experimental evaluations and many others. Even after author response, it still does not gather support from the reviewers. Thus I recommend reject.",Paper Decision
8eg96tALJJ,B1eY_pVYvB,Efficient and Information-Preserving Future Frame Prediction and Beyond,Accept (Poster),"This paper introduces a new approach that consists of the invertible autoencoder and a reversible predictive module (RPM) for video future-frame prediction.

Reviewers agree that the paper is well-written and the contributions are clear. It achieves new state-of-the-art results on a diverse set of video prediction datasets and with techniques that enable more efficient computation and memory footprint. Also, the video representation learned in a self-supervised way by the approach can have good generalization ability on downstream tasks such as object detection. The concerns of the paper were relatively minor, and were successfully addressed in the rebuttal.

AC feels that this work makes a solid contribution with well-designed model and strong empirical performance, which will attain wide interests in the area of video future-frame prediction and self-supervised video representation learning.

Hence, I recommend accepting this paper.",Paper Decision
6YgWl2deDI,rked_6NFwH,Path Space for Recurrent Neural Networks with ReLU Activations,Reject,The scores of the reviewers are just far to low to warrant an acceptance recommendation from the AC.,Paper Decision
BmljCBMJ8,SJldu6EtDS,Wasserstein Adversarial Regularization (WAR) on label noise,Reject,"This article proposes a regularisation scheme to learn classifiers that take into account similarity of labels, and presents a series of experiments. The reviewers found the approach plausible, the paper well written, and the experiments sufficient. At the same time, they expressed concerns, mentioning that the technical contribution is limited (in particular, the Wasserstein distance has been used before in estimation of conditional distributions and in multi-label learning), and that it would be important to put more efforts into learning the metric. The author responses clarified a few points and agreed that learning the metric is an interesting problem. There were also concerns about the competitiveness of the approach, which were addressed in part in the authors' responses, albeit not fully convincing all of the reviewers. This article proposes an interesting technique for a relevant type of problems, and demonstrates that it can be competitive with extensive experiments. ``Although this is a reasonably good article, it is not good enough, given the very high acceptance bar for this year's ICLR. 
",Paper Decision
_xzlypBWa1,rJlDO64KPH,Self-Supervised Speech Recognition via Local Prior Matching,Reject,"The paper proposed local prior matching that utilizes a language model to rescore the hypotheses generate by a teacher model on unlabeled data, which are then used to training the student model for improvement. The experimental results on Librispeech is thorough. But two concerns on this paper are: 1) limited novelty: LM trained on large tex data is already used in weak distillation and the only difference is the use of multiply hypotheses. As pointed out by the reviewers, the method is better understood through distillation even though the authors try to derive it from Bayesian perspective. 2) Librispeech is a medium sized dataset, justifications on much larger dataset for ASR would make it more convincing. ",Paper Decision
3OPk5qQLcN,HJlP_pEFPH,SRDGAN: learning the noise prior for Super Resolution with Dual Generative Adversarial Networks,Reject,"All reviewers agree that the authors have done a great job identifying weaknesses with the current SOTA in super-resolution.   However, there is also agreement that the proposed approach may be too simple to accurately capture a range of real camera distortions, and more comparisons to the SOTA are needed.   While this paper certainly has merits and opens the door for strong work in the future, there is not enough support to accept the paper in its current form.",Paper Decision
X9ETHEEm8s,S1xI_TEtwS,Amata: An Annealing Mechanism for Adversarial Training Acceleration,Reject,"The paper proposes a modification for adversarial training in order to improve the robustness of the algorithm by developing an annealing mechanism for PGD adversarial training. This mechanism gradually reduces the step size and increases the number of iterations of PGD maximization. One reviewer found the paper to be clear and competitive with existing work, but raised concerns of novelty and significance. Another reviewer noted the significant improvements in training times but had concerns about small scale datasets. The final reviewer liked the optimal control formulation, and requested further details. The authors provided detailed answers and responses to the reviews, although some of these concerns remain. The paper has improved over the course of the review, but due to a large number of stronger papers, was not accepted at this time.",Paper Decision
mBf6NtdnJB,r1lUdpVtwB,Context Based Machine Translation With Recurrent Neural Network For English-Amharic Translation ,Reject,"The authors propose a model which combines a neural machine translation system and a context-based machine translation model, which combines some aspects of rule and example based MT.  This paper presents work based on obsolete techniques, has relatively low novelty, has problematic experimental design and lacks compelling performance improvements. The authors rebutted some of the reviewers claims, but did not convince them to change their scores. ",Paper Decision
u64yOLZM8g,H1xSOTVtvH,Robust Domain Randomization for Reinforcement Learning,Reject,"The paper presents a technique for learning RL agents to generalize well to unseen environments.

All reviewers and AC think that the paper has some potential but is a bit below the bar to be accepted due to the following facts:

(a) Limited experiments, i.e., consider more appealing baselines/scenarios and provide more experimental details.
(b) The proposed method/idea is simple/reasonable, but not super novel, i.e., not enough considering the ICLR high standard (potentially enough for a workshop paper).

Hence, I think this is a borderline paper toward rejection.

",Paper Decision
C5rPJ2sLzP,HygrdpVKvr,NAS evaluation is frustratingly hard,Accept (Poster),"Summary:
This paper provides comprehensive empirical evidence for some of the systemic issues in the NAS community, for example showing that several published NAS algorithms do not outperform random sampling on previously unseen data and that the training pipeline is more important in the DARTS space than the exact choice of neural architecture. I very much appreciate that code is available for reproducibility.

Reviewer scores and discussion: 
The reviewers' scores have very high variance: 2/3 reviewers gave clear acceptance scores (8,8), very much liking the paper, whereas one reviewer gave a clear rejection score (1). In the discussion between the reviewers and the AC, despite the positive comments of the other reviewers, AnonReviewer 2 defended his/her position, arguing that the novelty is too low given previous works. The other reviewers argued against this, emphasizing that it is an important contribution to show empirical evidence for the importance of the training protocol (note that the intended contribution is *not* to introduce these training protocols; they are taken from previous work).

Due to the high variance, I read the paper myself in detail. Here are my own two cents:
- It is not new to compare to a single random sample. Sciuto et al clearly proposed this first; see Figure 1 (c) in https://arxiv.org/abs/1902.08142 
- The systematic experiments showing the importance of the training pipeline are very useful, providing proper and much needed empirical evidence for the many existing suggestions that this might be the case. Figure 3 is utterly convincing.
- Throughout, it would be good to put the work into perspective a bit more. E.g., correlations have been studied by many authors before. Also, the paper cites the best practice checklist in the beginning, but does not mention it in the section on best practices (my view is that this paper is in line with that checklist and provides important evidence for several points in it; the checklist also contains other points not being discussed in this paper; it would be good to know whether this paper suggests any new points for the checklist).

Recommendation:
Overall, I firmly believe that this paper is an important contribution to the NAS community. It may be viewed by some as ""just"" running some experiments, but the experiments it shows are very informative and will impact the community and help guide it in the right direction. I therefore recommend acceptance (as a poster).",Paper Decision
9yfQzdAVy,BJgEd6NYPH,Ellipsoidal Trust Region Methods for Neural Network Training,Reject,"This paper interprets adaptive gradient methods as trust region methods, and then extends the trust regions to axis-aligned ellipsoids determined by the approximate curvature. It's fairly natural to try to extend the algorithms in this way, but the paper doesn't show much evidence that this is actually effective. (The experiments show an improvement only in terms of iterations, which doesn't account for the computational cost or the increased batch size; there doesn't seem to be an improvement in terms of epochs.) I suspect the second-order version might also lose some of the online convex optimization guarantees of the original methods, raising the question of whether the trust-region interpretation really captures the benefits of the original methods. The reviewers recommend rejection (even after discussion) because they are unsatisfied with the experiments; I agree with their assessment.
",Paper Decision
CcBVtUNuwD,r1lEd64YwH,Learning Semantically Meaningful Representations Through Embodiment,Reject,"What is investigated is what kind of representations are formed by embodied agents; it is argued that these are different than from non-embodied arguments. This is an interesting question related to foundational AI and Alife questions, such as the symbol grounding problem. Unfortunately, the empirical investigations are insufficient. In particular, there is no comparison with a non-embodied control condition. The reviewers point this out, and the authors propose a different control condition, which unfortunately is not sufficient to test the hypothesis.

This paper should be rejected in its current form, but the question is interesting and hopefully the authors will do the missing experiments and submit a new version of the paper.",Paper Decision
miwWQqXxm,B1eX_a4twH,Superseding Model Scaling by Penalizing Dead Units and Points with Separation Constraints,Reject,This paper proposes constraints to tackle the problems of dead neurons and dead points. The reviewers point out that the experiments are only done on small datasets and it is not clear if the experiments will scale further. I encourage the authors to carry out further experiments and submit to another venue.,Paper Decision
aj1h43xJ25,S1emOTNKvS,Robust Graph Representation Learning via Neural Sparsification,Reject,"This submission proposes a graph sparsification mechanism that can be used when training GNNs.

Strengths:
-The paper is easy to follow.
-The proposed method is sound and effective.

Weaknesses:
-The novelty is limited.

Given the limited novelty and the number of strong submissions to ICLR, this submission, while promising, does not meet the bar for acceptance.",Paper Decision
OORew_JXfd,rkezdaEtvH,Hyperbolic Discounting and Learning Over Multiple Horizons,Reject,"While there was some support for the ideas presented in this paper, it was on the borderline, and ultimately did not make the cut for publication at ICLR.

Concerns were raised as to the significance of the contribution, beyond that of past work.",Paper Decision
Gaqy29z7ls,HJlfuTEtvB,CLN2INV: Learning Loop Invariants with Continuous Logic Networks,Accept (Poster),"This paper implements a novel architecture for inferring loop invariants in verification (though the paper bridges to compilers).  The idea is novel and the paper is well executed.  It is not the usual topic for ICLR, but not presents an important application of deep learning done well, and it has interesting implications for program synthesis.  Therefore, I recommend acceptance.",Paper Decision
-LovxiaeFf,Syl-_aVtvH,Federated User Representation Learning,Reject,"This manuscript personalization techniques to improve the scalability and privacy preservation of federated learning. Empirical results are provided which suggests improved performance.

The reviewers and AC agree that the problem studied is timely and interesting, as the tradeoffs between personalization and performance are a known concern in federated learning.  However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and clarity of the conceptual and empirical results. Reviewers were also unconvinced by the provided empirical evaluation results. ",Paper Decision
P3SptFT8uw,BJeguTEKDB,INSTANCE CROSS ENTROPY FOR DEEP METRIC LEARNING,Reject,"The paper proposes a new objective function called ICE for metric learning.

There was a substantial discussion with the authors about this paper. The two reviewers most experienced in the field found the novelty compared to the vast existing literature lacking, and remained unconvinced after the discussion. Some reviewers also found the technical presentation and interpretations to need improvement, and this was partially addressed by a new revision.

Based on this discussion, I recommend a rejection at this time, but encourage the authors to incorporate the feedback and in particular place the work in context more fully, and resubmit to another venue.",Paper Decision
O5-NRxbiO,BJlguT4YPr,Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base,Accept (Poster),"This paper proposes an approach to representing a symbolic knowledge base as a sparse matrix, which enables the use of  differentiable neural modules for inference. This approach scales to large knowledge bases and is demonstrated on several tasks.   

Post-discussion and rebuttal, all three reviewers are in agreement that this is an interesting and useful paper. There was intiially some concern about clarity and polish, but these have been resolved upon rebuttal and discussion. Therefore I recommend acceptance. ",Paper Decision
KueUc5lryc,HJxJdp4YvS,Variational pSOM: Deep Probabilistic Clustering with Self-Organizing Maps,Reject,"The authors present a deep model for probabilistic clustering and extend it to handle time series data.   The proposed method beats existing deep models on two datasets and  the representations learned in the process are also interpretable.

Unfortunately, despite detailed responses by the authors, the reviewers felt that some of their main concerns were not addressed. For example, the authors and the reviewers are still not converging on whether SOM-VAE uses a VAE or an autoencoder. Further, the discussion about the advantages of VAE over AE is still not very convincing. Currently the work is positioned as a variational clustering method but the reviewers feel that it is a clustering method which uses a VAE (yes, I understand that this difference is subtle but needs to be clarified). 

The reviewers read the responses of the author and during discussions with the AC suggested that there were still not convinced about some of their initial questions. Given this, at this point I would prefer going by the consensus of the reviewers and recommend that this paper cannot be accepted.",Paper Decision
CkMBuad13,HklJdaNYPH,Augmenting Self-attention with Persistent Memory,Reject,"This paper proposes a modification to the Transformer architecture in which the self-attention and feed-forward layer are merged into a self-attention layer with ""persistent"" memory vectors. This involves concatenating the contextual representations with global, learned memory vectors, which are attended over. Experiments show slight gains in character and word-level language modeling benchmarks. 

While the proposed architectural changes are interesting, they are also rather minor and had a small impact in performance and in number of model parameters. The motivation of the persistent memory vector as replacing the FF-layer is a bit tenuous since Eqs 5 and 9 are substantially different. Overall the contribution seems a bit thin for a ICLR paper. I suggest more analysis and possibly experimentation in other tasks in a future iteration of this paper.",Paper Decision
gP34QhvIYJ,B1l0wp4tvr,Information Plane Analysis of Deep Neural Networks via Matrix--Based Renyi's Entropy and Tensor Kernels,Reject,"This paper considers the information plane analysis of DNNs. Estimating mutual information is required in such analysis which is difficult task for high dimensional problems. This paper proposes a new ""matrix–based Renyi’s entropy coupled with ´tensor kernels over convolutional layers"" to solve this problem. The methods seems to be related to an existing approach but derived using a different ""starting point"". Overall, the method is able to show improvements in high-dimensional case.

Both R1 and R3 have been critical of the approach. R3 is not convinced that the method would work for high-dimensional case and also that no simulation studies were provided. In the revised version the authors added a new experiment to show this. R3's another comment makes an interesting point regarding ""the estimated quantities evolve during training, and that may be interesting in itself, but calling the estimated quantities mutual information seems like a leap that's not justified in the paper."" I could not find an answer in the rebuttal regarding this.

R1 has also commented that the contribution is incremental in light of existing work. The authors mostly agree with this, but insist that the method is derived differently.

Overall, I think this is a reasonable paper with some minor issues. I think this can use another review cycle where the paper can be improved with additional results and to take care of some of the doubts that reviewers' had this time. 

For now, I recommend to reject this paper, but encourage the authors to resubmit at another venue after revision.",Paper Decision
AKVb60TfqP,HklRwaEKwB,"Ridge Regression: Structure, Cross-Validation, and Sketching",Accept (Spotlight),"The paper studies theoretical properties of ridge regression, and in particular how to correct for the bias of the estimator. 

The reviewers appreciated the contribution and the fact that you updated the manuscript to make it clearer.

I however advise the authors to think about the best way to maximize impact for the ICLR audience, perhaps by providing relevant examples from the ML literature.",Paper Decision
d63t9IH680,rylCP6NFDB,Hindsight Trust Region Policy Optimization,Reject,"The paper pursues an interesting approach, but requires additional maturation.  The experienced reviewers raise several concerns about the current version of the paper.  The significance of the contribution was questioned.  The paper missed key opportunities to evaluate and justify critical aspects of the proposed approach, via targeted ablation and baseline studies.  The quality and clarity of the technical exposition was also criticized.  The comments submitted by the reviewers should help the authors strengthen the paper. ",Paper Decision
bQK6vyRbkV,SkxpDT4YvS,Policy Optimization with Stochastic Mirror Descent,Reject,"This paper proposes a new policy gradient method based on stochastic mirror descent and variance reduction. Both theoretical analysis and experiments are provided to demonstrate the sample efficiency of the proposed algorithm. The main concerns of this paper include: (1) unclear presentation in both the main results and the proof; and (2) missing baselines (e.g., HAPG) in the experiments. This paper has been carefully discussed but even after author response and reviewer discussion, it does not gather sufficient support.

Note: the authors disclosed their identity by adding the author names in the revision during the author response. After discussion with PC chair, the openreview team helped remove that revision during the reviewer discussion to avoid desk reject. 
",Paper Decision
AbdGevkyPC,Bke6vTVYwH,Graph convolutional networks for learning with few clean and many noisy labels,Reject,The paper combines graph convolutional networks with noisy label learning. The reviewers feel that novelty in the work is limited and there is a need for further experiments and  extensions. ,Paper Decision
UN16ATEFU,ryenvpEKDr,A Constructive Prediction of the Generalization Error Across Scales,Accept (Poster),"The paper presents a very interesting idea for estimating the held-out error of deep models as a function of model and data set size. The authors intuit what the shape of the error should be, then they fit the parameters of a function of the desired shape and show that this has predictive power. I find this idea quite refreshing and the paper is well written with good experiments. Please make sure that the final version contains the cross-validation results provided during the rebuttal.",Paper Decision
DfKlU7Bb_K,ryx2wp4tvS,MLModelScope: A Distributed Platform for ML Model Evaluation and Benchmarking at Scale,Reject,"The paper proposes a platform for benchmarking, and in particular hardware-agnostic evaluation of machine learning models. This is an important problem as our field strives for more reproducibility.

This was a very confusing paper to discuss and review, since most of the reviewers (and myself) do not know much about the area. Two of the reviewers found the paper contributions sufficient to be (weakly) accepted. The third reviewer had many issues with the work and engaged in a lengthy debate with the authors, but there was strong disagreement regarding their understanding of the scope of the paper as a Tools/Systems submission.

Given the lack of consensus, I must recommend rejection at this time, but highly encourage the authors to take the feedback into account and resubmit to a future venue.",Paper Decision
rRlb9w2Vc,HygiDTVKPr,A Mention-Pair Model of Annotation with Nonparametric User Communities,Reject,"Thanks to the reviewers and the authors for an interesting discussion. The reviewers are mixed, learning toward positive, but a few shortcomings were left unaddressed: (i) Turning the task into a mention-pair classification problem ignores the mention detection step, and synergies from joint modeling are lost. (ii) Lee et al. (2018) has been surpassed by some margin by BERT and spanBERT, models ignored in this paper. (iii) Several approaches to aggregating structured annotations have already been introduced, e.g., for sequence labelling tasks. [0] Overall, the limited novelty, the missing baselines, and the missing related work lead me to not favor acceptance at this point. 

[0] https://www.aclweb.org/anthology/P17-1028/",Paper Decision
PAYB-UNwI8,HJeiDpVFPr,An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality,Accept (Poster),"This paper proposes a neural network approach to approximate distances, based on a representation of norms in terms of convex homogeneous functions. The authors show universal approximation of norm-induced metrics and present applications to value-function approximation in RL and graph distance problems. 

Reviewers were in general agreement that this is a solid paper, well-written and with compelling results. The AC shares this positive assessment and therefore recommends acceptance. ",Paper Decision
CjciZGGTfv,SJl9PTNYDS,NPTC-net: Narrow-Band Parallel Transport Convolutional Neural Network on Point Clouds,Reject,All the reviewers recommend rejecting the paper. There is no basis for acceptance.,Paper Decision
HlfRE4e0O6,SJe5P6EYvS,Mogrifier LSTM,Accept (Talk),"This paper presents a new twist on the typical LSTM that applies several rounds of gating on the history and input, with the end result that the LSTM's transition function is effectively context-dependent. The performance of the model is illustrated on several datasets.

In general, the reviews were positive, with one score being upgraded during the rebuttal period. One of the reviewers complained that the baselines were not adequate, but in the end conceded that the results were still worthy of publication.

One reviewer argued very hard for the acceptance of this paper ""Papers that are as clear and informative as this one are few and far between. ... As such, I vehemently argue in favor of this paper being accepted to ICLR.""",Paper Decision
Le8cZX3Lun,rJx9vaVtDS,Individualised Dose-Response Estimation using Generative Adversarial Nets,Reject,"This paper addresses the problem of estimating treatment responses involving a continuous dosage parameter.  The basic idea is to learn a GAN model capable of generating synthetic dose-response curves for each training sample, which then facilitates the supervised training of an inference model that estimates these curves for new cases.  For this purpose, specialized architectures are also proposed for the GAN, which involves a multi-task generator network and a hierarchical discriminator network.  Empirical results demonstrate improvement over existing methods.

While there is always a chance that reviewers may underappreciate certain aspects of a submission, the fact that there was a unanimous decision to reject this work indicates that the contribution must be better marketed to the ML community.  For example, after the rebuttal one reviewer remained unconvinced regarding explanations for why the proposed method is likely to learn the full potential outcome distribution.  Among other things, another reviewer felt that both the proposed DRGAN model, and the GANITE framework upon which it is based, were not necessarily working as advertised in the present context.",Paper Decision
qjHcmfbTZm,BJeKwTNFvB,Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video,Accept (Poster),"The submission presents an approach to estimating physical parameters from video. The approach is sensible and is presented fairly well. The main criticism is that the approach is only demonstrated in simplistic ""toy"" settings. Nevertheless, the reviewers recommend (weakly) accepting the paper and the AC concurs.",Paper Decision
LyX4pb3lC,rkeYvaNKPr,Trajectory representation learning for Multi-Task NMRDPs planning,Reject,"The paper considers a special case of decision making processes with                                                               
non-Markovian reward functions, where conditioned on an unobserved task-label                                                      
the reward function becomes Markovian.                                                                                             
A semi-supervised loss for learning trajectory embeddings is proposed.                                                             
The approach is tested on a multi-task grid-world environment and ablation                                                         
studies are performed.                                                                                                             
                                                                                                                                   
The reviewers mainly criticize the experiments in the paper. The environments                                                      
studied are quite simple, leaving it uncertain if the approach still works in                                                      
more complex settings.                                                                                                             
Apart from ablation results, no baselines were presented although the setting is                                                   
similar to continual learning / multi-task learning (with unobserved task label)                                                   
where prior work does exist.                                                                                                       
Furthermore, the writing was found to be partially lacking in clarity, although                                                    
the authors addressed this in the rebuttal.                                                                                        
                                                                                                                                   
The paper is somewhat below acceptance threshold, judging from reviews and my own                                                  
reading, mostly due to lack of convincing experiments. Furthermore, the general setting                                            
considered in this paper seems quite specific, and therefore of limited impact.",Paper Decision
_uCSxPPz7Z,SkgODpVFDr,Incorporating Horizontal Connections in Convolution by Spatial Shuffling,Reject,"The paper is well-motivated by neuroscience that our brains use information from outside the receptive field of convolutive processes through top-down mechanisms. However, reviewers feel that the results are not near the state of the art and the paper needs further experiments and need to scale to larger datasets. ",Paper Decision
xlU-k6aiZ4,r1x_DaVKwH,Is Deep Reinforcement Learning Really Superhuman on Atari? Leveling the playing field,Reject,"This paper proposes a new benchmark that compares performance of deep reinforcement learning algorithms on the Atari Learning Environment to the best human players.  The paper identifies limitations of past evaluations of deep RL agents on Atari. The human baseline scores commonly used in deep RL are not the highest known human scores.  To enable learning agents to reach these high scores, the paper recommends allowing the learning agents to play without a time limit.  The time limit in Atari is not always consistent across papers, and removing the time limit requires additional software fixes due to some bugs in the game software.  These ideas form the core of the paper's proposed new benchmark (SABER). The paper also proposes a new deep RL algorithm that combines earlier ideas. 

The reviews and the discussion with the authors brought out several strengths and weaknesses of the proposal.  One strength was identifying the best known human performance in these Atari games.  
However, the reviewers were not convinced that this new benchmark is useful.  The reviewers raised concerns about using clipped rewards, using games that received substantially different amounts of human effort, comparing learning algorithms to human baselines instead of other learning algorithms, and also the continued use of the Atari environment. Given all these many concerns about a new benchmark, the newly proposed algorithm was not viewed as a distraction.

This paper is not ready for publication. The new benchmark proposed for deep reinforcement learning on Atari was not convincing to the reviewers.  The paper requires further refinement of the benchmark or further justification for the new benchmark.",Paper Decision
zQ-WZvGFpS,SJxDDpEKvH,Counterfactuals uncover the modular structure of deep generative models,Accept (Poster),"This paper provides a fresh application of tools from causality theory to investigate modularity and disentanglement in learned deep generative models. It also goes one step further towards making these models more transparent by studying their internal components. While there is still margin for improving the experiments, I believe this paper is a timely contribution to the ICLR/ML community.
This paper has high-variance in the reviewer scores. But I believe the authors did a good job with the revision and rebuttal. I recommend acceptance.",Paper Decision
T1IB0wwzex,rJxwDTVFDB,Pushing the bounds of dropout,Reject,"The reviewers have uniformly had significant reservations for the paper. Given that the authors did not even try to address them, this suggests the paper should be rejected.",Paper Decision
mB8fsy5XVE,SyevDaVYwr,Confidence Scores Make Instance-dependent Label-noise Learning Possible,Reject,"While two reviewers  rated this paper as an accept, reviewer 3 strongly believes there are unresolved issues with the work as summarized in their post-rebuttal review. This work seems very promising and while the AC will recommend rejection at this time, the authors are strongly encouraged to resubmit this work.",Paper Decision
_3S-MwFwL,B1lLw6EYwB,Gap-Aware Mitigation of Gradient Staleness,Accept (Poster),"The authors propose a novel approach for measuring gradient staleness and use this measure to penalize stale gradients in an asynchronous stochastic gradient set up. Following previous work, they provide a convergence proof for their approach. Most importantly, they provide extensive evaluations comparing against previous approaches and show impressive gains over previous work.

After the author response, the primary concerns from reviewers is regarding the gap between the proposed method and single worker SGD/synchronous SGD. I feel that the authors have made compelling arguments that ASGD is an important optimization paradigm to consider, so their improvements in narrowing the gap are of interest to the community. There were some concerns about the novelty of the theory, and my impression is that theorem is straightforward to prove based on assumptions and previous work, however, I view the main contribution of the paper as empirical.

This paper is borderline, but I think the impressive empirical results over existing work on ASGD is a worthwhile contribution and others will find it interesting, so I am recommending acceptance.",Paper Decision
54vkvzMXum,ryg8wpEtvB,Evaluating and Calibrating Uncertainty Prediction in Regression Tasks,Reject,"The paper investigates calibration for regression problems. The paper identifies a shortcoming of previous work by Kuleshov et al. 2018 and proposes an alternative. 

All the reviewers agreed that while this is an interesting direction, the paper requires more work before it can be accepted. In particular, the reviewers raised several concerns about motivation, clarity of the presentation and lack of in-depth empirical evaluation.

I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue.
",Paper Decision
36PJm4Zrz9,BygSP6Vtvr,Ensemble Distribution Distillation,Accept (Poster),"The paper investigates how to distill an ensemble effectively (using a prior network) in order to reap the benefits of uncertainty estimation provided by ensembling (in addition to the accuracy gains provided by ensembling). 

Overall, the paper is nicely written, and makes a valuable contribution. The authors also addressed most of the initial concerns raised by the reviewers. I recommend the paper for acceptance, and encourage the authors to take into account the reviewer feedback when preparing the final version.",Paper Decision
d0rnS5fOgH,SkxSv6VFvS,Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation,Accept (Poster),"In my opinion, this paper is borderline (but my expertise is not in this area) and the reviewers are too uncertain to be of help in making an informed decision.",Paper Decision
jlUKyDqAVO,H1gEP6NFwr,On the Tunability of Optimizers in Deep Learning,Reject,"The paper proposed a new metric to define the quality of optimizers as a weighted average of the scores reached after a certain number of hyperparameters have been tested.

While reviewers (and myself) understood the need to better be able to compare optimizers, they failed to be convinced by the proposed solutions. In particular (setting aside several complaints of the reviewers with which I disagree), by defining a very versatile metric, this paper lacks a strong conclusion as the ranking of optimizers would clearly depend on the instantiation of that metric.

Although that is to be expected, by the very behaviour of these optimizers, it makes it unclear what the added value of the metric is. As one reviewer pointed out,  all the points made could have been similarly made with other, more common plots.

Ultimately, it wasn't clear to me what the paper was trying to achieve beyond defining a mathematical formula encompassing all ""standard"" evaluation metric, which I unfortunately see of limited value.",Paper Decision
EhFs4dgxm,rJgVwTVtvS,Gradient Perturbation is Underrated for Differentially Private Convex Optimization,Reject,"In this paper, the authors showed that for differentially private convex optimization, the utility guarantee of both DP-GD and  DP-SGD is determined by the expected curvature rather than the worst-case minimum curvature. Based on this motivation, the authors justified the advantage of gradient perturbation over other perturbation methods. This is a borderline paper, and has been discussed after author response. The main concerns of this paper include (1) the authors failed to show any loss function that can satisfy the expected curvature inequality; (2) the contribution of this paper is limited, since all the proofs in the paper are just small tweak of existing proofs; (3) this paper does not really improve any existing gradient perturbation based differentially private methods. Due to the above concerns, I have to recommend reject.",Paper Decision
k1CkfLQql0,SygXPaEYvH,VL-BERT: Pre-training of Generic Visual-Linguistic Representations,Accept (Poster),The paper proposed a new pretrained language model which can take visual information into the embeddings. Experiments showed state-of-the-art results on three downstream tasks. The paper is well written and detailed comparisons with related work are given. There are some concerns about the clarity and novelty raised by the reviewers which is answered in details and I think the paper is acceptable.,Paper Decision
oecYLGMfCE,SkgQwpVYwH,"Credible Sample Elicitation by Deep Learning, for Deep Learning",Reject,"The primary contribution of this manuscript is a conceptual and theoretical solution to the sample elicitation problem, where agents are asked to report samples. The procedure is implemented using score functions to evaluate the quality of the samples.

The reviewers and AC agree that the problem studied is timely and interesting, as there is limited work on credible sample elicitation in the literature. However, the reviewers were unconvinced about the motivation of the work, and the clarity of the conceptual results. There is also a lack of empirical evaluation. IN the opinion of the AC, this manuscript, while interesting, can be improved by significant revision for clarity and context, and revisions should ideally include some empirical evaluation.",Paper Decision
xxJgAvD97D,SkeGvaEtPr,Neural Markov Logic Networks,Reject,"This paper on extending MLNs using NNs is borderline acceptable: one reviewer is strongly opposed, although I confess I don't really understand their response to the rebuttal or see what the issue with novelty is (a position shared by the other reviewers). I'm not sure how to weigh this review, but there is not a lot of signal in favour of rejection aside from the rating.

The remaining two reviews are in favour of acceptance, with their enthusiasm only bounded by the lack of scalability of the method, something they appreciate the authors are upfront about. My view is this paper brings something new to the table which will interest the community, but doesn't oversell the result.

Given the distribution of papers in my area, this one is just a little too borderline to accept, but this is primarily a reflection of the number of high-quality papers reviewed and the limited space of the conference. I have no doubt this paper will be successful at another conference, and it's a bit of a shame we were not in a position to accept it to this one.",Paper Decision
w1e9xMTmlT,r1xGP6VYwH,Optimistic Exploration even with a Pessimistic Initialisation,Accept (Poster),"The paper propose a scheme to enable optimistic initialization in the deep RL setting, and shows that it's helpful.

The reviewers agreed that the paper is well-motivated and executed, but had some minor reservations (e.g. about the proposal scaling in practice). In an example of a successful rebuttal two of the reviewers raised their scores after the authors clarified the paper and added an experiment on Montezuma's revenge.

The paper proposes a useful, simple and practical idea on the bridge between tabular and deep RL, and I gladly recommend acceptance.",Paper Decision
oAKcm2cgGf,SJlbvp4YvS,Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning,Reject,"The authors propose to extend model-based/model-free hybrid methods (e.g., MVE, STEVE) to stochastic environments. They use an ensemble of probabilistic models to model the environment and use a lower confidence bound of the estimate to avoid risk. They found that their proposed method yields state-of-the-art performance over previous methods.

The valid concerns by Reviewers 1 & 4 were not addressed by the authors and although the authors responded to Reviewer 3, they did not revise the paper to address their concerns. The ideas and results in this paper are interesting, but without addressing the valid concerns raised by reviewers, I cannot recommend acceptance.",Paper Decision
LhX3XGIQVL,BkeWw6VFwr,Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing,Accept (Poster),"The paper extends the work on randomized smoothing for certifiably robust classifiers developed in prior work to a weaker specification requiring that the set of top-k predictions remain unchanged under adversarial perturbations of the input (rather than just the top-1). This enables the authors to achieve stronger results on robustness of classifiers on CIFAR10 and ImageNet (where the authors report the top-5 accuracy).

This is an interesting extension of certified defenses that is likely to be relevant for complex prediction tasks with several classes (ImageNet and beyond), where top-1 robustness may be difficult and unrealistic to achieve.

The reviewers were in consensus on acceptance and minor concerns were alleviated during the rebuttal phase.

I therefore recommend acceptance.",Paper Decision
8daT0dl44,BJexP6VKwH,Generalized Domain Adaptation with Covariate and Label Shift CO-ALignment,Reject,"This paper proposes a method to address the covariate shift and label shift problems simultaneously. 

The paper is an interesting attempt towards an important problem. However, Reviewers and AC commonly believe that the current version is not acceptable due to several major misconceptions and misleading presentations. In particular:
- The novelty of the paper is not very significant.
- The main concern of this work is that its shift assumption is not well justified.
- The proposed method may be problematic by using the minimax entropy and self-training with resampling.
- The presentation has many errors that require a full rewrite.

Hence I recommend rejection.",Paper Decision
WnQr_I9M1T,r1glDpNYwS,LabelFool: A Trick in the Label Space,Reject,"Thanks for the discussion with reviewers, which improved our understanding of your paper significantly.
However, we concluded that this paper is still premature to be accepted to ICLR2020. We hope that the detailed comments by the reviewers help improve your paper for potential future submission.",Paper Decision
sDUxUQZUc,HJekvT4twr,RGTI:Response generation via templates integration for End to End dialog,Reject,"This paper describes a method to incorporate multiple candidate templates to aid in response generation for an end-to-end dialog system. Reviewers thought the basic idea is novel and interesting. However, they also agree that the paper is far from complete, results are missing, further experiments are needed as justification, and the presentation of the paper is not very clear. Given the these feedback from the reviews, I suggest rejecting the paper.",Paper Decision
ktVqAvJ3XC,rJe1DTNYPH,Towards Disentangling Non-Robust and Robust Components in Performance Metric,Reject,"All reviewers suggest rejection. Beyond that, the more knowledgable two have consistent questions about the motivation for using the CCKL objective. As such, the exposition of this paper, and justification of the work could use improvement, so that experienced reviewers understand the contributions of the paper.",Paper Decision
UyZubnFHZ7,HJx0U64FwS,A Mechanism of Implicit Regularization in Deep Learning,Reject,"This paper analyzes a mechanism of the implicit regularization caused by nonlinearity of ReLU activation, and suggests that the learned DNNs interpolate almost linearly between data points, which leads to the low complexity solutions in the over-parameterized regime. The main objections include (1) some claims in this paper are not appropriate; (2) lack of proper comparison with prior work; and many other issues in the presentation. I agree with the reviewers’ evaluation and encourage the authors to improve this paper and resubmit to future conference.
",Paper Decision
lBfbXDTSz4,Bkl086VYvH,Feature-map-level Online Adversarial Knowledge Distillation,Reject,"The paper received scores of WR (R1) WR (R2) WA (R3), although R3 stated that they were borderline. The main issues were (i) lack of novelty and (ii) insufficient experiments. The AC has closely look at the reviews/comments/rebuttal and examined the paper. Unfortunately, the AC feels that with no-one strongly advocating for acceptance, the paper cannot be accepted at this time. The authors should use the feedback from reviewers to improve their paper. ",Paper Decision
Jey-ZZZ7rV,HJlAUaVYvH,Optimising Neural Network Architectures for Provable Adversarial Robustness,Reject,"The authors propose a novel method to estimate the Lipschitz constant of a neural network, and use this estimate to derive architectures that will have improved adversarial robustness. While the paper contains interesting ideas, the reviewers felt it was not ready for publication due to the following factors:

1) The novelty and significance of the bound derived by the authors is unclear. In particular, the bound used is coarse and likely to be loose, and hence is not likely to be useful in general.

2) The bound on adversarial risk seems of limited significance, since in practice, this can be estimated accurately based on the adversarial risk measured on the test set.

3) The paper is poorly organized with several typos and is hard to read in its present form.

The reviewers were in consensus and the authors did not respond during the rebuttal phase.

Therefore, I recommend rejection. However, all the reviewers found interesting ideas in the paper. Hence, I encourage the authors to consider the reviewers' feedback and submit a revised version to a future venue.",Paper Decision
DIe-_Bx2n6,BylaUTNtPS,Recurrent Independent Mechanisms,Reject,"This paper has, at its core, a potential for constituting a valuable contribution. However, there was a shared belief among reviewers (that I also share) that the paper still has much room for improvement in terms of presentation and justification of the claims. I hope that the authors will be able to address the feedback they received to make this submission get where it should be.
",Paper Decision
iGX8LHiF_,S1l6ITVKPS,An Explicitly Relational Neural Network Architecture,Reject,"This paper proposes a model that can learn predicates (symbolic relations) from pixels and can be trained end to end.  They show that the relations learned generate a representation that generalizes well, and provide some interpretation of the model.

Though it is reasonable to develop a model with synthetic data, the reviewers did wonder if the findings would generalize to new data from real situations.  The authors argue that a new model should be understood (using synthetic data) before it can reasonably be applied to natural data.  I hope the reviews have shown the authors which areas of the paper need further explanation, and that the use of a synthetic dataset needs to strong justification, or perhaps show some evidence that the method will probably work on real data (e.g. how it could be extended to natural images).",Paper Decision
6yMAsGAygH,HJxhUpVKDr,Branched Multi-Task Networks: Deciding What Layers To Share,Reject,"The authors present an approach to multi-task learning. Reviews are mixed. The main worries seem to be computational feasibility and lack of comparison with existing work. Clearly, one advantage to Cross-stitch networks over the proposed approach is that their approach learns sharing parameters in an end-to-end fashion and scales more efficiently to more tasks. Note: The authors mention SluiceNets in their discussion, but I think it would be appropriate to directly compare against this architecture - or DARTS [https://arxiv.org/abs/1806.09055], maybe - since the offline RSA computations only seem worth it if better than *anything* you can do end-to-end. I would encourage the authors to map out this space and situate their proposed method properly in the landscape of existing work. I also think it would be interesting to think of their approach as an ensemble learning approach and look at work in this space on using correlations between representations to learn what and how to combine. Finally, some work has suggested that benefits from MTL are a result of easier optimization, e.g., [3]; if that is true, will you not potentially miss out on good task combinations with your approach?

Other related work: 
[0] https://www.aclweb.org/anthology/C18-1175/
[1] https://www.aclweb.org/anthology/P19-1299/
[2] https://www.aclweb.org/anthology/N19-1355.pdf - a somewhat similar two-stage approach
[3] https://www.aclweb.org/anthology/E17-2026/",Paper Decision
11lSkS8kbN,rke3U6NtwH,MxPool: Multiplex Pooling for Hierarchical Graph Representation Learning,Reject,All three reviewers are consistently negative on this paper. Thus a reject is recommended.,Paper Decision
JWRbBs2-kH,SJgs8TVtvr,Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations,Reject,"The paper proposes a VAE with a mixture-of-experts decoder for clustering and generation of high-dimensional data. Overall, the reviewers found the paper well-written and structured , but in post rebuttal discussion questioned the overall importance and interest of the work to the community.  This is genuinely a borderline submission. However, the calibrated average score currently falls below the acceptance threshold, so I’m recommending rejection, but strongly encouraging the authors to continue the work, better motivating the importance of the work, and resubmitting.",Paper Decision
Dkv_4uHCmU,rkej86VYvB,Temporal Difference Weighted Ensemble For Reinforcement Learning,Reject,"The paper proposes a method to combine the decision of an ensemble of RL agents. It uses an uncertainty measure based on the TD error, and suggests a weighted average or weighted voting mechanism to combine their policy or value functions to come up with a joint decision.
The reviewers raised several concerns, including whether the method works in the stochastic setting, whether it favours deterministic parts of the state space, its sensitivity to bias, and unfair comparison to a single agent setting.
There is also a relevant PhD dissertation (Elliot, 2017), which the authors surprisingly refused to discuss and cite because apparently it was not published at any conference. A PhD dissertation is a citable reference, if it is relevant. If it is, a good scholarship requires proper citation.

Overall, even though the proposed method might potentially be useful, it requires further investigations. Two out of three reviewers are not positive about the paper in its current form. Therefore, I cannot recommend acceptance at this stage.

Elliott, Daniel L., The Wisdom of the crowd : reliable deep reinforcement learning through ensembles of Q-functions, PhD Dissertation, Colorado State University, 2017",Paper Decision
ZO8GdunhB,rJlcLaVFvB,Effect of top-down connections in Hierarchical Sparse Coding,Reject,"This paper introduces a new architecture for sparse coding.

The reviewers gave long and constructive feedback that the authors in turn responded at length on. There is consensus among the reviewers that despite contributions this paper in its current form is not ready for acceptance.

Rejection is therefore recommended with encouragement to make updated version for next conference.  

",Paper Decision
oHNuQ1t2SM,B1x9ITVYDr,"Compressive Recovery Defense: A Defense Framework for $\ell_0, \ell_2$ and $\ell_\infty$ norm attacks.",Reject,"After reading the author's response, all the reviwers still think that this paper is a simple extension of gradient masking, and can not provide the robustness in neural networks.",Paper Decision
OnMe7ECmD,BJxYUaVtPB,Match prediction from group comparison data using neural networks,Reject,"This paper investigates neural networks for group comparison -- i.e., deciding if one group of objects would be preferred over another. The paper received 4 reviews (we requested an emergency review because of a late review that eventually did arrive). R1 recommends Weak Reject, based primarily on unclear presentation, missing details, and concerns about experiments. R2 recommends Reject, also based on concerns about writing, unclear notation, weak baselines, and unclear technical details. In a short review, R3 recommends Weak Accept and suggests some additional experiments, but also indicates that their familiarity with this area is not strong. R4 also recommends Weak Accept and suggests some clarifications in the writing (e.g. additional motivation future work). The authors submitted a response and revision that addresses many of these concerns. Given the split decision, the AC also read the paper; while we see that it has significant merit, we agree with R1 and R2's concerns, and feel the paper needs another round of peer review to address the remaining concerns.",Paper Decision
0H0lt9rD_0,SklOUpEYvB,Identifying through Flows for Recovering Latent Representations,Accept (Poster),"Main content:

Blind review #1 summarizes it well:

This paper is about learning an identifiable generative model, iFlow, that builds upon a recent result on nonlinear ICA. The key idea is providing side information to identify the latent representation, i.e., essentially a prior conditioned on extra information such as labels and restricting the mapping to flows for being able to compute the likelihood. As the loglikelihood of a flow model is readily available, a direct approach can be used for learning that optimizes both the prior and the observation model.	

--

Discussion:

Reviewer questions were mostly about clarification, which the authors addressed during the rebuttal period.

--

Recommendation and justification:

All reviewers agree the paper is a weak accept based on degree of depth, novelty, and impact.",Paper Decision
VPsJwbUEBF,ryxOUTVYDH,Robust training with ensemble consensus,Accept (Poster),"This paper proposes an ensemble method to identify noisy labels in the training data of supervised learning.  The underlying hypothesis is that examples with label noise require memorization.  The paper proposes methods to identify and remove bad training examples by retaining only the training data that maintains low losses after perturbations to the model parameters.  This idea is developed in several candidate ensemble algorithms.  One of the proposed ensemble methods exceeds the performance of state-of-the-art methods on MNIST, CIFAR-10 and CIFAR-100.

The reviewers found several strengths and a few weaknesses in the paper.  The paper was well motivated and clear.  The proposed solution was novel and plausible.  The experiments were comprehensive.  The reviewers identified several parts of the paper that could be more clear or where more detail could be provided, including a complexity analysis and 
extended experiments.  The author response addressed the reviewer questions directly and also in a revised document.  In the discussion phase, the reviewers were largely satisfied that their concerns were addressed.

This paper should be accepted for publication as the paper presents a clear problem and solution method along with convincing evidence of method's merits.
",Paper Decision
zrftqffDE4,rklPITVKvS,BRIDGING ADVERSARIAL SAMPLES AND ADVERSARIAL NETWORKS,Reject,"This paper proposes incorporating adversarial training on real images to improve the stability of GAN training. The key idea relies on the observation that GAN training already implicitly does a form of adversarial training on the generated images and so this work proposes adding adversarial training on real images as well. In practice, adversarial training on real images is performed using FGSM and experiments are conducted on CelebA, CiFAR10, and LSUN reporting using standard generative metrics like FID.

Initially all reviewers were in agreement that this work should not be accepted. However, in response to the discussion with the authors Reviewer 2 updated their score from weak reject to weak accept. The other reviewers recommendation remained unchanged. The core concerns of reviewers 3 and 1 is limited technical contribution and unconvincing experimental evidence. In particular, concerns were raised about the overlap with [1] from CVPR 2019. The authors argue that their work is different due to the focus on the unsupervised setting, however, this application distinction is minor and doesn’t result in any major algorithmic changes. With respect to experiments, the authors do provide performance across multiple datasets and architectures which is encouraging, however, to distinguish this work it would have been helpful to provide further study and analysis into the aspects unique to this work -- such as the settings and type of adversarial attack (as mentioned by R3) and stability across GAN variants. 

After considering all reviewer and author comments, the AC does not recommend this work for publication in its current form and recommends the authors consider both additional experiments and text description to clarify and solidify their contributions over prior work.

[1] Liu, X., & Hsieh, C. J. (2019). Rob-gan: Generator, discriminator, and adversarial attacker. CVPR 2019.
",Paper Decision
CgNlsVf8GF,HkxIIaVKPB,Unsupervised-Learning of time-varying features,Reject,"This work proposes a VAE-based model for learning transformations of sequential data (the main here intuition is to have the model learn changes between frames without learning features that are constant within a time-sequence). All reviewers agreed that this is a very interesting submission, but have all challenged the novelty and rigor of this paper, asking for more experimental evidence supporting the strengths of the model. After having read the paper, I agree with the reviewers and I currently see this one as a weak submission without potentially comparing against other models or showing whether the representations learned from the proposed model lead in downstream improvements in a task that uses this representations.",Paper Decision
JHpeyguEDo,B1l8L6EtDS,Self-Adversarial Learning with Comparative Discrimination for Text Generation,Accept (Poster),"This paper proposes a method for improving training of text generation with GANs by performing discrimination between different generated examples, instead of solely between real and generated examples.

R3 and R1 appreciated the general idea, and thought that while there are still concerns, overall the paper seems to be interesting enough to warrant publication at ICLR. R2 has a rating of ""weak reject"", but I tend to agree with the authors that comparison with other methods that use different model architectures is orthogonal to the contribution of this paper.

In sum, I think that this paper would likely make a good contribution to ICLR and recommend acceptance.",Paper Decision
4DwCnKn-t,rkerLaVtDr,A General Upper Bound for Unsupervised Domain Adaptation,Reject,"Given two distributions, source and target, the paper presents an upper bound on the target risk of a classifier in terms of its source risk and other terms comparing the risk under the source/target input distribution and target/source labeling function. In the end, the bound is shown to be minimized by the true labeling function for the source, and at this minimum, the value of the bound is shown to also control the ""joint error"", i.e., the best achievable risk on both target and source by a single classifier. 

The point of the analysis is to go beyond the target risk bound presented by Ben-David et al. 2010 that is in terms of the discrepancy between the source and target and the performance of the source labeling function on the target or vice versa, whichever is smaller. Apparently, concrete domain adaptation methods ""based on"" the Ben-David et al. bound do not end up controlling the joint error. After various heuristic arguments, the authors develop an algorithm for unsupervised domain adaptation based on their bound in terms of a two-player game.

Only one reviewer ended up engaging with the authors in a nontrivial way. This review also argued for (weak) acceptance. Another reviewer mostly raised minor issues about grammar/style and got confused by the derivation of the ""general"" bound, which I've checked is ok. The third reviewer raised some issues around the realizability assumption and also asked for better understanding as to what aspects of the new proposal are responsible for the improved performance, e.g., via an ablation study.

I'm sympathetic to reviewer 1, even though I wish they had engaged with the rebuttal. I don't believe the revision included any ablation study. I think this would improve the paper. I don't think the issues raised by reviewer 3 rise to the level of rejection, especially since their main technical concern is due to their own confusion. Reviewer 2 argues for weak acceptance. However, if there was support for this paper, it wasn't enough for reviewers to engage with each other, despite my encouragement, which was disappointing.",Paper Decision
IyIJZ08Qol,SkxBUpEKwH,Vid2Game: Controllable Characters Extracted from Real-World Videos,Accept (Poster),"This paper proposes to extract a character from a video, manually control the character, and render into the background in real time.  The rendered video can have arbitrary background and capture both the dynamics and appearance of the person. All three reviewers praises the visual quality of the synthesized video and the paper is well written with extensive details. Some concerns are raised. For example, despite an excellent engineering effort, there is few things the reader would scientifically learn from this paper. Additional ablation study on each component would also help the better understanding of the approach. Given the level of efforts, the quality of the results and the reviewers’ comments, the ACs recommend acceptance as a poster.",Paper Decision
KzPOC9wM_,ryg48p4tPH,Action Semantics Network: Considering the Effects of Actions in Multiagent Systems,Accept (Poster),"The authors address the challenge of sample-efficient learning in multi-agent systems. They propose a model that distinguishes actions in terms of their semantics, specifically in terms of whether they influence the acting agent and environment or whether they influence other agents. This additional structure is shown to substantially benefit learning speed when composed with a range of state of the art multi-agent RL algorithms. During the rebuttal, technical questions were well addressed and the overall quality of the paper improved. The paper provides interesting novel insights on how the proposed structure improves learning.",Paper Decision
oURz9rU3kg,Skl4LTEtDS,Growing Action Spaces,Reject,"This paper presents a novel approach to learning in problems which have large action spaces with natural hierarchies. The proposed approach involves learning from a curriculum of increasingly larger action spaces to accelerate learning. The method is demonstrated on both small continuous action domains, as well as a Starcraft domain.

While this is indeed an interesting paper, there were two major concerns expressed by the reviewers. The first concerns the choice of baselines for comparison, and the second involves improving the discussion and intuition for why the hierarchical approach to growing action spaces will not lead to the agent missing viable solutions. The reviewers felt that neither of these were adequately addressed in the rebuttal, and as such it is to be rejected in its current form.",Paper Decision
iJdt1g4EBt,rkgQL6VFwr,Learning Generative Image Object Manipulations from Language Instructions,Reject,"The submission proposes to train a model to modify objects in an image using language (the modified image is the effect of an action). The model combines CNN, RNN, Relation Nets and GAN and is trained and evaluated on synthetic data, with some examples of results on real images.

The paper received relatively low scores (1 reject and 2 weak rejects).  The authors did not provide any responses to the reviews and did not revise their submission.  Thus there was no reviewer discussion and the scores remained unchanged.

The reviewers all agreed that the submission addressed an interesting task, but there was no special insight in how the components were put together, and the work was limited in the experimental results.  Comparisons against additional baselines (AE, VAE), and ablation studies or examinations of how the components can be varied is needed.

The paper is currently too weak to be accepted at ICLR.  The authors are encouraged to improve their evaluation and resubmit to an appropriate venue.",Paper Decision
aqpFu_20Il,B1em8TVtPr,Discourse-Based Evaluation of Language Understanding,Reject,"This paper proposes a new benchmark to evaluate natural language processing models on discourse-related tasks based on existing datasets that are not available in other benchmarks (SentEval/GLUE/SuperGLUE). The authors also provide a set of baselines based on BERT, ELMo, and others; and estimates of human performance for some tasks.

I think this has the potential to be a valuable resource to the research community, but I am not sure that it is the best fit for a conference such as ICLR. R3 also raises a valid concern regarding the performance of fine-tuned BERT that are comparable to human estimates on half of the tasks (3 out of 5), which slightly weakens the main motivation of having this new benchmark. 

My main suggestion to the authors is to have a very solid motivation for the new benchmark, including the reason of inclusion for each of the tasks. I believe that this is important to encourage the community to adopt it. For something like this, it would be nice (although not necessary) to have a clean website for submission as well. I believe that someone who proposes a new benchmark needs to do as best as they can to make it easy for other people to use it.

Due to the above issues and space constraint, I recommend to reject the paper.",Paper Decision
F03E44pYtE,rJxX8T4Kvr,Learning Efficient Parameter Server Synchronization Policies for Distributed SGD,Accept (Poster),"The authors consider a parameter-server setup where the learner acts a server communicating updated weights to workers and receiving gradient updates from them. A major question then relates in the synchronisation of the gradient updates, for which couple of *fixed* heuristics exists that trade-off accuracy of updates (BSP) for speed (ASP) or even combine the two allowing workers to be at most k steps out-of-sync. Instead, the authors propose to learn a synchronisation policy using RL. The authors perform results on a simulated and real environment. Overall, the RL-based method seems to provide some improvement over the fixed protocols, however the margin between the fixed and the RL get smaller in the real clusters. This is actually the main concern raised by the reviewers as well (especially R2) -- the paper in its initial submission did not include the real cluster results, rather these were added at the rebuttal. I find this to be an interesting real-world application of RL and I think it provides an alternative environment for testing RL algorithms beyond simulated environments.   As such, I’m recommending acceptance. However, I do ask the authors to be upfront with the real cluster results and move them in the main paper.
",Paper Decision
0efUxCCWx,B1lGU64tDr,Relational State-Space Model for Stochastic Multi-Object Systems,Accept (Poster),"The paper proposed what is termed Relational State Space Model (R-SSM) that can be used for modeling interacting time-series data. The model essentially consists of a set of (nonlinear) state space models whose states are jointly evolved in a way that take into account a known interaction structure between them (the relational part, even though technically it is just a coupling structure -- the term relational structure in the past has been used for models with objects and classes, for example see the difference between ""coupled HMM"" vs ""relational HMM""). The authors also proposed a graph normalizing flow operation to model the joint state evolution. The main weakness of the paper is in the complexity of the model. However, from a modeling point of view, R-SSM seems suitable in situation when the interaction structure is known, and this is demonstrated in the experimental results when comparing against the baselines. ",Paper Decision
l2jFTePF3g,B1gzLaNYvr,TSInsight: A local-global attribution framework for interpretability in time-series data,Reject,"Main content:

Blind review #2 summarizes it well:

The aim of this work is to improve interpretability in time series prediction. To do so, they propose to use a relatively post-hoc procedure which learns a sparse representation informed by gradients of the prediction objective under a trained model. In particular, given a trained next-step classifier, they propose to train a sparse autoencoder with a combined objective of reconstruction and classification performance (while keeping the classifier fixed), so as to expose which features are useful for time series prediction.  Sparsity, and sparse auto-encoders, have been widely used for the end of interpretability. In this sense, the crux of the approach is very well motivated by the literature.

--

Discussion:

All reviews had difficulties understanding the significance and novelty, which appears to have in large part arisen from the original submission not having sufficiently contextualized the motivation and strengths of the approach (especially for readers not already specialized in this exact subarea).

--

Recommendation and justification:

The reviews are uniformly low, probably due to the above factors, and while the authors' revisions during the rebuttal period have improved the objections, there are so many strong submissions that it would be difficult to justify override the very low reviewer scores.",Paper Decision
FwSk7xRhS,HklZUpEtvr,"OPTIMAL TRANSPORT, CYCLEGAN, AND PENALIZED LS FOR UNSUPERVISED LEARNING IN INVERSE PROBLEMS",Reject,"This paper provides a novel approach for addressing ill-posed inverse problems based on a formulation as a regularized estimation problem and showing that this can be optimized using the CycleGAN framework. While the paper contains interesting ideas and has been substantially improved from its original form, the paper still does not meet the quality bar of ICLR due to a critical gap between the presented theory and applications. The paper will benefit from a revision and resubmission to another venue.",Paper Decision
26H4y_bCsO,HylZIT4Yvr,Structural Language Models for Any-Code Generation,Reject,"This paper proposes a new method for code generation based on structured language models.

After viewing the paper, reviews, and author response my assessment is that I basically agree with Reviewer 4. (Now, after revision) This work seems to be (1) a bit incremental over other works such as Brockschmidt et al. (2019), and (2) a bit of a niche topic for ICLR. At the same time it has (3) good engineering effort resulting in good scores, and (4) relatively detailed conceptual comparison with other work in the area. Also, (5) the title of ""Structural Language Models for Code Generation"" is clearly over-claiming the contribution of the work -- as cited in the paper there are many language models, unconditional or conditional, that have been used in code generation in the past. In order to be accurate, the title would need to be modified to something that more accurately describes the (somewhat limited) contribution of the work.

In general, I found this paper borderline. ICLR, as you know is quite competitive so while this is a reasonably good contribution, I'm not sure whether it checks the box of either high quality or high general interest to warrant acceptance. Because of this, I'm not recommending it for acceptance at this time, but definitely encourage the authors to continue to polish for submission to a different venue (perhaps a domain conference that would be more focused on the underlying task of code generation?)",Paper Decision
pgy2MAgq8L,SJxeI6EYwS,Simple and Effective Stochastic Neural Networks,Reject,This paper proposes to use stacked layers of Gaussian latent variables with a maxent objective function as a regulariser. I agree with the reviewers that there is very little novelty and the experiments are not very convincing.,Paper Decision
FPYfXekenb,HkeeITEYDr,Robust Reinforcement Learning with Wasserstein Constraint,Reject,"This paper studies the robust reinforcement learning problem in which the constraint on model uncertainty is captured by the Wasserstein distance. The reviewers expressed concerns regarding novelty with respect to prior work, the presentation or the results, and unconvincing experiments. In its current form the paper is not ready for acceptance to ICLR-2020.",Paper Decision
sRuyvt0nxA,BylJUTEKvB,Cross-Iteration Batch Normalization,Reject,"This paper proposes cross-iteration batch normalization, which is a strategy for maintaining statistics across iterations to improve the applicability of batch normalization on small batches of data. 

The reviewers pointed out some strong points but also some weak points about the paper. The paper was judged to be novel and theoretically sound, and the paper was judged to be well-written. 

However, there were some doubts regarding the relevance and significance of the work. Reviewers commented on being unconvinced by the utility of the approach, it being unclear when the proposed method is beneficial, and the relative small magnitude of the empirical improvement. 

On the balance, the paper seems decent but not completely convincing. This means that with the current high competitiveness and selectivity of ICLR I unfortunately cannot recommend the manuscript for acceptance. ",Paper Decision
vAPRKbxUK9,SyxJU64twr,Model Ensemble-Based Intrinsic Reward for Sparse Reward Reinforcement Learning,Reject,"This paper considers the challenge of sparse reward reinforcement learning through intrinsic reward generation based on the deviation in predictions of an ensemble of dynamics models. This is combined with PPO and evaluated in some Mujoco domains.

The main issue here was with the way the sparse rewards were provided in the experiments, which was artificial and could lead to a number of problems with the reward structure and partial observability. The work was also considered incremental in its novelty. These concerns were not adequately rebutted, and so as it stands this paper should be rejected.",Paper Decision
8v1lef4nl,H1eArT4tPH,The Effect of Residual Architecture on the Per-Layer Gradient of Deep Networks,Reject,"This paper studies the statistics of activation norms and Jacobian norms for randomly-initialized ReLU networks in the presence (and absence) of various types of residual connections. Whereas the variance of the gradient norm grows with depth for vanilla networks, it can be depth-independent for residual networks when using the proper initialization.

Reviewers were positive about the setup, but also pointed out important shortcomings on the current manuscript, especially related to the lack of significance of the measured gradient norm statistics with regards to generalisation, and with some techinical aspects of the derivations. For these reasons, the AC believes this paper will strongly benefit from an extra iteration. ",Paper Decision
LNcAEsznct,HkxAS6VFDB,Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN,Reject,"The authors propose a hardware-agnostic metric called effective signal norm (ESN) to measure the computational cost of convolutional neural networks. They then demonstrate that models with fewer parameters achieve far better accuracy after quantization. The main novelty is on the metric ESN. However, ESN is based on ideal hardware, and thus not suitable for existing hardware. Assumptions made in the paper are hard to be proved. Experimental results are not convincing, and related pruning methods are not compared. Finally, the paper is not written clearly, and the structure and some arguments are confusing.",Paper Decision
-3fOKs3Ml,SyepHTNFDS,Graph Residual Flow for Molecular Graph Generation,Reject,The authors propose a graph residual flow model for molecular generation.  Conceptual novelty is limited since it is simple extension and there isn't much improvement over state of art.,Paper Decision
8VS3NxKUPj,B1x6BTEKwr,Piecewise linear activations substantially shape the loss surfaces of neural networks,Accept (Poster),"Quoting R3: ""This paper studies the theoretical property of neural network's loss surface. The main contribution is to prove that the loss surface of every neural network (with arbitrary depth) with piecewise linear activations has infinite spurious local minima.""

There were split reviews, with two reviewers recommending acceptance and one recommending rejection.  During a robust rebuttal and discussion phase, both R2 and R3's appreciation for the work was strengthened.  The authors also provided a robust response to R1, whose main concerns included (i) that the paper's analysis is limited to piecewise linear activation functions, (ii) technical questions about the difficulty of proving theorem 2, which appear to have been answered in the discussion, and (iii) concerns about the strength of the language employed.

On the balance, the reviewers were positively impressed with the relevance of the theoretical study and its contributions.  Genuine shortcomings and misunderstandings were systematically resolved during the rebuttal process.",Paper Decision
6AyyRQtSLb,HyxnH64KwS,The problem with DDPG: understanding failures in deterministic environments with sparse rewards,Reject,"This paper provides an extensive investigation of the robustness of Deep Deterministic Policy Gradient algorithm.

Papers providing extensive and qualitative empirical studies, illustrative benchmark domains, identification of problems with existing methods, and new insights can be immensely valuable, and this paper is certainly in this direction, if not quite there yet. 

The vast majority of this paper investigates one deep learning algorithm in designed domain. There is some theory but it's relegated to the appendix. There are a few issues with this approach: (1) there is no concrete evidence that this is a general issue beyond the provided example (more on that below). (2) Even in the designed domain the problem is extremely rare. (3) The study and perhaps even the issue is only shown for one particular architecture (with a whole host of unspecified meta-parameter details). Why not just use SAC it works? DDPG has other issues, why is it of interest to study and fix this particular architecture? The motivation that it is the first and most popular algorithm is not well developed enough to be convincing. (4) There is really no reasoning to suggest that the particular 1D is representative or interesting in general.

The authors including Mujoco results to address #1. But the error bars overlap, its completely unclear if the baseline was tuned at all---this is very problematic as the domains were variants created by the authors. If DDPG was not tuned for the variant then the plots are not representative. In general, there are basically no implementation details (how parameters were tested, how experiments were conducted)or general methodological details given in the paper. Given the evidence provided in this paper its difficult to claim this is a general and important issue. 

I encourage the authors to look at John Langfords hard exploration tasks, and broaden their view of this work general learning mechanisms. ",Paper Decision
YEklyZ7ike,B1liraVYwr,LocalGAN: Modeling Local Distributions for Adversarial Response Generation,Reject,"This paper tackles neural response generation with Generative Adversarial Nets (GANs), and to address the training instability problem with GANs, it proposes a local distribution oriented objective. The new objective is combined with the original objective, and used as a hybrid loss for the adversarial training of response generation models, named as LocalGAN. Authors responded with concerns about reviewer 3's comments, and I agree with the authors explanation, so I am disregarding review 3, and am relying on my read through of the latest version of the paper. The other reviewers think the paper has good contributions, however they are not convinced about the clarity of the presentations and made many suggestions (even after the responses from the authors).  I suggest a reject, as the paper should include a clear presentation of the approach and technical formulation (as also suggested by the reviewers).",Paper Decision
3r0lgh0tIJ,SkxcSpEKPS,Generative Adversarial Networks For Data Scarcity Industrial Positron Images With Attention,Reject,"The paper studies Positron Emission Tomography (PET) in medical imaging. The paper focuses on the challenges created by gamma-ray photon scattering, that results in poor image quality. To tackle this problem and enhance the image quality, the paper suggests using generative adversarial networks. Unfortunately due to poor writing and severe language issues, none of the three reviewers were able to properly assess the paper [see the reviews for multiple examples of this]. In addition, in places, some important implementation details were missing.

The authors chose not to response to reviewers' concerns. In its current form, the submission cannot be well understood by people interested in reading the paper, so it needs to be improved and resubmitted. ",Paper Decision
HWOYPBiZI4,rJxcBpNKPr,OvA-INN: Continual Learning with Invertible Neural Networks,Reject,"This paper is board-line but in the end below the standards for ICLR. Firstly this paper could use significant polishing. The text has significant grammar and style issues: incorrect words, phrases and tenses; incomplete sentences; entire sections of the paper containing only lists, etc. The paper is in need of significant editing.

This of course is not enough to merit rejection, but there are concerns about the contribution of the new method, experiment details, and the topic of study. The results are reported from either a single run or unknown number of runs of the learning system, which is not acceptable even if the we suspect the variance is low. The proposed approach relies on pre-training a feature extractor which in many ways side-steps the forgetting/interference problem rather than what we really need: new algorithms that processes the training data in ways the mitigate interference by learning representations. In general the reviewers found it very difficult to access the fairness of the comparisons dues do differences between how different methods make use of stored data and pre-training. The reviewers highlighted the similarity between the propose approach and recent work in angle of generative modeling / out of distribution (OOD) detection which suggests that the proposed approach has limited utility (as detailed by R1) and that OOD baselines were missing. Finally, the CL problem formulation explored here, where task identifiers are available during training and data is i.i.d, is of limited utility. Its hard to imagine how approaches that learn individual networks for each task could scale to more realistic problem formulations.

All reviewers agreed the paper's experiments were borderline and the paper has substantial issues. There are too many revisions to be done.",Paper Decision
KEgPBoofqD,S1gqraNKwB,Contextual Inverse Reinforcement Learning,Reject,"The authors introduce a framework for inverse reinforcement learning tasks whose reward functions are dependent on context variables and provide a solution by formulating it as a convex optimization problem.  Overall, the authors agreed that the method appears to be sound.  However, after discussion there were lingering concerns about (1) in what situations this framework is useful or advantageous, (2) how it compares to existing, modern IRL algorithms that take context into account, and (3) if the theoretical and experimental results were truly useful in evaluating the algorithm.  Given that these issues were not able to be fully resolved, I recommend that this paper be rejected at this time.",Paper Decision
QAeaHlwsJ,rketraEtPr,Learning Time-Aware Assistance Functions for Numerical Fluid Solvers,Reject,"This paper provides a data-driven approach that learns to improve the accuracy of numerical solvers. It solves an important problem and provides some promising direction. However, the presented paper is not novel in terms of ML methodology. The presentation can be significantly improved for ML audience (e.g., it would be preferred to explicitly state the problem setting in the beginning of Section 3).",Paper Decision
M-W6t2k1i0,B1lOraEFPB,Transition Based Dependency Parser for Amharic Language Using Deep Learning,Reject,"The paper builds a transition-based dependency parser for Amharic, first predicting transitions and then dependency labels. The model is poorly motivated, and poorly described. The experiments have serious problems with their train/test splits and lack of baseline. The reviewers all convincingly argue for reject. The authors have not responded. ",Paper Decision
xG7X-klkPm,B1g_BT4FvS,Samples Are Useful? Not Always: denoising policy gradient updates using variance explained,Reject,"The authors aim to improve policy gradient methods by denoising the gradient estimate. They propose to filter the transitions used to form the gradient update based on a variance explains criterion. They evaluate their method in combination with PPO and A2C, and demonstrate improvements over the baseline methods.

Initially, reviewers were concerned about the motivation and explanation of the method. The authors revised the paper by clarifying the motivation and providing a justification based on the options framework. Furthermore, the authors included additional experiments investigating the impact of their approach on the gradient estimator, showing that with their filtering, the gradient estimator had larger magnitude.

Reviewers found the justification via the options framework to be a stretch, and I agree. The authors should explain how the options framework leads to dropping gradient terms. At the moment, the paper describes an algorithm using the options framework, however, they don't connect the policy gradients of that algorithm to their method. Furthermore, the authors should more clearly verify the claims about reducing noise in the gradient estimate. While the additional experiments on the norm are nice, the authors should go further. For example, if the claim is that the variance of the gradient estimator is reduced, then that should be verified. Finally, there are many approaches for reducing the variance of the policy gradient (Grathwohl et al. 2018, Wu et al 2018, Liu et al. 2018) and no comparisons are made to these approaches.

Given the remaining issues, I recommend rejection for this paper at this time, however, I encourage the authors to address these issues and submit to a future venue.
",Paper Decision
VoolRM9eFA,BkePHaVKwS,Learning Surrogate Losses,Reject,"Unfortunately, this was a borderline paper that generated disagreement among the reviewers.  After high level round of additional deliberation it was decided that this paper does not yet meet the standard for acceptance.  The paper proposes a potentially interesting approach to learning surrogates for non-differentiable and non-decomposable loss functions.  However, the work is a bit shallow technically, as any supporting theoretical justification is supplied by pointing to other work.  The paper would be stronger with a more serious and comprehensive analysis.  The reviewers criticized the lack of clarity in the technical exposition, which the authors attempted to mitigate in the rebuttal/revision process.  The paper would benefit from additional clarity and systematic presentation of complete details to allow reproduction.",Paper Decision
BKg3yceJiQ,SylwBpNKDr,Boosting Network: Learn by Growing Filters and Layers via SplitLBI,Reject,"This paper considers how to learn the structure of deep network by beginning with a simple network and then progressively adding layers and filters as needed. The paper received three reviews by expert working in this area. R1 recommends Weak Reject due to concerns about novelty, degree of contribution, clarity of technical exposition, and experiments. R2 recommends Weak Accept and has some specific suggestions and questions. R3 recommends Weak Reject, also citing concerns with experiments and writing. The authors submitted a response that addressed many of these comments, but R1 and R3 continue to have concerns about contribution and the experiments, while R2 maintains their Weak Accept rating. Given the split decision, the AC also read the paper. While we believe the paper has significant merit, we agree with R1 and R3 on the need for additional experimentation, and believe another round of peer review would help clarify the writing and contribution. We hope the reviewer comments will hep authors prepare a revision for a future venue.",Paper Decision
NazKJQg5M,SkxUrTVKDH,Split LBI for Deep Learning: Structural Sparsity via Differential Inclusion Paths,Reject,"This paper investigates an existing method for fitting sparse neural networks, and provides a novel proof of global convergence.  Overall, this seems like a sensible, if marginal, contribution.  However, there were serious red flags regarding the care which which the scholarship was done which make me deem the current submission unsuitable for publication.  In particular, two points raised by R4, which were not addressed even after the rebuttal:

1) ""One important issue with the paper is that it blurs the distinction between prior work and the new contribution. For example, the subsection on Split Linearized Bregman Iteration in the ""Methodology"" section does not contain anything new compared to [1], and this is not clear enough to the reader.""

2) ""The newly-written conclusion is still incorrect, stating again that Split LBI achieves SOTA performance on ImageNet.""

I believe that R3's high score is due to not noticing these unsupported or misleading claims.

",Paper Decision
5oh8LNTeE,HJeLBpEFPB,Unsupervised Universal Self-Attention Network for Graph Classification,Reject,All three reviewers are consistently negative on this paper. Thus a reject is recommended.,Paper Decision
uUMV2qRsgw,SJgBra4YDS,"Manifold Modeling in Embedded Space: A Perspective for Interpreting ""Deep Image Prior""",Reject,"The paper proposes a combination of a delay embedding as well as an autoencoder to perform representation learning. The proposed algorithm shows competitive performance with deep image prior, which is a convnet structure. The paper claims that the new approach is interpretable and provides explainable insight into image priors.

The discussion period was used constructively, with the authors addressing reviewer comments, and the reviewers acknowledging this an updating their scores.

Overall, the proposed architecture is good, but the structure and presentation of the paper is still not up to the standards of ICLR. The current presentation seems to over-claim interpretability, without sufficient theoretical or empirical evidence.
",Paper Decision
PHUUTSFtxj,ByeNra4FDB,Novelty Detection Via Blurring,Accept (Poster),"The paper proposes a new method for out-of-distribution detection by combining random network distillation (RND) and blurring (via SVD). The proposed idea is very simple but achieves strong empirical performance, outperforming baseline methods in several OOD detection benchmarks. There were many detailed questions raised by the reviewers but they got mostly resolved, and all reviewers recommend acceptance, and this AC agrees that it is an interesting and effective method worth presenting at ICLR. ",Paper Decision
AEYc3w1FKc,rkeNr6EKwB,Small-GAN: Speeding up GAN Training using Core-Sets,Reject,"The paper proposes to use greedy core set sampling to improve GAN training with large batches. Although the problem is clear and the solution works, reviewers have raised several concerns. One concern is that the technical novelty is limited; another (in the first version) that even a simpler version of gradient accumulation can solve the  main task (rather that computing core-sets). In the end, some discussion was done, with quite a few additions and experiments done by the authors. The final concern that seemingly was not addressed: the gradient accumulation seems to give the same numbers as large batches, thus you can 'mimic' large batch sizes with smaller ones and gradient accumulation, making the main claim of the paper questionable. The achievement of SOTA is good, but it is not clear wether it is due to the proposed technique, or rather smart tuning of a larger set of hyperparameters. Thus, I would agree with the concern of Reviewer1.",Paper Decision
rCDocmc_3r,BkgXHTNtvS,Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow ReLU Networks,Accept (Poster),"This article investigates the optimization landscape of shallow ReLU networks, showing that for sufficiently narrow networks there are data sets for which there is no descent paths to the global minimiser. The topic and the nature of the results is very interesting. The reviewers found that this article makes important contributions in a relevant line of investigation and had generally positive ratings. The authors' responses addressed questions from the initial reviews, and the discussion helped identifying questions for future study departing from the present contribution.  ",Paper Decision
qi-YR3wl6,H1gmHaEKwB,Data-Independent Neural Pruning via Coresets,Accept (Poster),"The rebuttal period influenced R1 to raise their rating of the paper.
The most negative reviewer did not respond to the author response.
This work proposes an interesting approach that will be of interest to the community.
The AC recommends acceptance.",Paper Decision
kQEke-_G1L,ryxmrpNtvH,Deeper Insights into Weight Sharing in Neural Architecture Search,Reject,"This paper provides a series of empirical evaluations on a small neural architecture search space with 64 architectures. The experiments are interesting, but limited in scope and limited to 64 architectures trained on CIFAR-10. It is unclear whether lessons learned on this search space would transfer to large search spaces. One upside is that code is available, making the work reproducible.

All reviewers read the rebuttal and participated in the private discussion of reviewers and AC, but none of them changed their mind. All gave a weak rejection score.

I agree with this assessment and therefore recommend rejection.",Paper Decision
WknBMp7_lr,HyezBa4tPB,Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems,Reject,"This paper proposes adding a Dirichlet distribution as a wrapper on top of a black box classifier in order to better capture uncertainty in the predictions.  This paper received four reviews in total with scores (1,1,1,6).  The reviewer who gave the weak accept found the paper well written, easy to follow and intuitive.  The other reviewers, however, were primarily concerned about the empirical evaluation of the method.  They found the baselines too weak and weren't convinced that the method would work well in practice.  The reviewers also cited a lack of comparison to existing literature for their scores.  One reviewer noted that while the method addresses aleatoric uncertainty, it doesn't provide any mechanism for epistemic uncertainty, which would be necessary for the applications motivating the work.  

The authors did not provide a response and thus there was no discussion. ",Paper Decision
DT_VrYM0XF,r1l-HTNtDB,S2VG: Soft Stochastic Value Gradient method,Reject,"The authors consider improvements to model-based reinforcement learning to improve sample efficiency and computational speed. They propose a method which they claim is simple and elegant and embeds the model in the policy learning step, this allows them to compute analytic gradients through the model which can have lower variance than likelihood ratio gradients. They evaluate their method on Mujoco with limited data.

All of the reviewers found the presentation confusing and below the bar for an acceptable submission. Although the authors tried to explain the algorithm better to the reviewers, they did not find the presentation sufficiently improved. I agree that the paper has substantial room for improvement around clarity. Reviewers also asked that experiments be run for more time steps. I agree that this would be an important addition as many model-based reinforcement learning approaches perform worse asymptotically model free approaches and it would be interesting to see how the proposed approach does. A reviewer pointed out that equation 2 is missing a term, and indeed I believe that is true. The authors response is not correct, they likely refer to an equation in SVG where the state is integrated out. Finally, the method does not compare to state-of-the-art model-based approaches, claiming that they use ensembles or uncertainty to improve performance. The authors would need to show that adding either of these to their approach attains similar performance to state-of-the-art approaches.

At this time, this paper is below the bar for acceptance.",Paper Decision
iwUlSHLUzz,SJxWS64FwH,Deep Network Classification by Scattering and Homotopy Dictionary Learning,Accept (Poster),After the rebuttal period the ratings on this paper increased and it now has a strong assessment across reviewers. The AC recommends acceptance.,Paper Decision
lwznwaQdE_,BkleBaVFwB,Scalable Generative Models for Graphs with Graph Attention Mechanism,Reject,"The paper proposed an efficient way of generating graphs.  Although the paper claims to propose simplified mechanism, the reviewers find that the generation task to be relatively very complex, and the use of certain module seems ad-hoc. Furthermore, the results on the new metric is at times inconsistent with other prior metrics. The paper can be improved by addressing those concerns concerns. ",Paper Decision
qTYdE7-4cD,BkllBpEKDH,Continuous Adaptation in Multi-agent Competitive Environments,Reject,"This paper studies whether adopting strategy adaptation mechanisms helps players improve their performance in zero-sum stochastic games (in this case baseball). Moreover they study two questions in particular, a) whether adaptation techniques are helpful when faced with a small number of iterations and 2) what’s the effect of different initial strategies when both teams adopt the same adaptation technique. Reviewers expressed concerns regarding the fact that the author’s adaptation techniques improve upon initial strategies, which seems to indicate that their initial strategies were not Nash (despite the use of CFR). In the lack of theory of why this seems to happen at the current setup (and whether indeed the initial strategies are Nash and why do the improve), stronger empirical evidence from more rigorous experiments seem somewhat necessary for recommending acceptance of this paper.",Paper Decision
Tex4z872T,BkglSTNFDB,Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP,Accept (Poster),"In this paper, the authors extended Q-learning with UCB exploration bonus by Jin et al. to infinite-horizon MDP with discounted rewards without accessing a generative model, and proved nearly optimal regret bound for finite-horizon episodic MDP. The authors also proved PAC-type sample complexity of exploration, which matches the lower bound up to logarithmic factors. Overall this is a solid theoretical reinforcement learning work.  After author response, we reached a unanimous agreement to accept this paper.",Paper Decision
0TO9kXgLd,SJlJSaEFwS,Robust Cross-lingual Embeddings from Parallel Sentences ,Reject,"The authors propose a new approach to learning cross-lingual embeddings from parallel data. For an overview of this literature, see [0]. Reviews are mixed, and some objections seem unresolved. The authors also ignore a new line of research in which pretrained language models are used to align vocabularies across languages, e.g., [1-2] The paper would also benefit from a discussion of massively parallel resources such as JW300 and WikiMatrix. Finally, it feels odd not to compare to distilled representations from NMT architectures, e.g., [3]. 

[0] http://www.morganclaypoolpublishers.com/catalog_Orig/product_info.php?products_id=1419
[1] https://www.aclweb.org/anthology/N19-1162.pdf
[2] https://www.aclweb.org/anthology/K19-1004.pdf
[3] https://arxiv.org/abs/1901.07291",Paper Decision
SoCkDiPJn-,rJe04p4YDB,Semi-supervised Learning by Coaching,Reject,"Authors propose a new method of semi-supervised learning and provide empirical results. Reviewers found the presentation of the method confusing and poorly motivated. Despite the rebuttal, reviewers still did not find clarity on how or why the method works as well as it does.",Paper Decision
XqVtarN0Wn,SJgCEpVtvr,DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS,Reject,"The paper is develops a self-training framework for graph convolutional networks where we have partially labeled graphs with a limited amount of labeled nodes. The reviewers found the paper interesting. One reviewer notes the ability to better exploit available information and raised questions of computational costs. Another reviewer felt the difference from previous work was limited, but that the good results speak for themselves. The final reviewer raised concerns on novelty and limited improvement in results. The authors provided detailed responses to these queries, providing additional results.

The paper has improved over the course of the review, but due to a large number of stronger papers, was not accepted at this time.",Paper Decision
DMVgMvn62z,H1gpET4YDB,Blockwise Self-Attention for Long Document Understanding,Reject,"This paper proposes blockwise masked attention mechanisms to sparsify Transformer architectures, the main motivation being reducing the memory usage with long sequence inputs. The resulting model is called BlockBERT. The paper falls in a trend of recent papers compressing/sparsifying/distilling Transformer architectures, a very relevant area of research given the daunting resources needed to train these models.

While the proposed contribution is very simple and interesting, it also looks a rather small increment over prior work, namely Sparse Transformer and Adaptive Span Transformer, among others. Experiments are rather limited and the memory/time reduction is not overwhelming (18.7-36.1% less memory, 12.0-25.1% less time), while final accuracy is sometimes sacrificed by a few points. No comparison to other adaptively sparse attention transformer architectures (Correia et al. EMNLP 19 or Sukhbaatar et al. ACL 19) which should as well provide memory reductions due to the sparsity of the gradients, which require less activations to be cached. I suggest addressing this concerns in an eventual resubmission of the paper.",Paper Decision
nIhTlSojRa,HkgaETNtDB,Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models,Accept (Poster),"This paper presents mixout, a regularization method that stochastically mixes parameters of a pretrained language model and a target language model. Experiments on GLUE show that the proposed technique improves the stability and accuracy of finetuning a pretrained BERT on several downstream tasks.

The paper is well written and the proposed idea is applicable in many settings. The authors have addressed reviewers concerns' during the rebuttal period and all reviewers are now in agreement that this paper should be accepted.

I think this paper would be a good addition to ICLR and recommend to accept it.
",Paper Decision
8v_zpWBAzV,rJehNT4YPr,I Am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively,Accept (Poster),"This paper proposes a new way of comparing classifiers, which does not use fixed test set and adaptively sample it from an arbitrarily large corpus of unlabeled images, i.e. replacing the conventional test-set-based evaluation methods with a more flexible mechanism. The main proposal is to build a test set adaptively in a manner that captures how classifiers disagree, as measured by the wordnet tree. As noted by R2, this work has the potential to be of interest to a broad audience and can motivate many subsequent works. 

While the reviewers acknowledged the importance of this work, they raised several concerns: (1) the proposed approach is immature to be considered for benchmarking yet (R1,R4), (2) selecting k and studying its influence on the performance ( R1, R3, R4), (3) the proposed approach requires data annotation which might not be straightforward -- (R3, R4).  The authors provided a detailed rebuttal addressing the reviewer concerns.

There is reviewer disagreement on this paper. The comments from R3 were valuable for the discussion, but at the same time too brief to be adequately addressed by the authors. The comments from emergency reviewer were helpful in making the decision. AC decided to recommend acceptance of the paper seeing its valuable contributions towards re-thinking the evaluation of current SOTA models.
",Paper Decision
POb6Prd9u,SJxhNTNYwB,Black-Box Adversarial Attack with Transferable Model-based Embedding,Accept (Poster),This paper proposes a new black-box adversarial attack approach which learns a low-dimensional embedding using a pretrained model and then performs efficient search in the embedding space to attack target networks. The proposed approach can produce perturbation with semantic patterns that are easily transferable and improve the query efficiency in black-box attacks. All reviewers are in support of the paper after author response. I am very happy to recommend accept. ,Paper Decision
xkEECFtfbc,SJgn464tPB,Stabilizing Off-Policy Reinforcement Learning with Conservative Policy Gradients,Reject,"This paper proposes a new algorithmic approach to reduced variance in off-policy, policy gradient updates.

Multiple reviewers were concerned with both the soundness of the proposed approach, and the cost of using rollouts. In particular, the interaction between the target policy and the behavior policy, and how they are swapped was unclear, where the algorithms in the paper do not match the code provided.

The results show apparent reduction in variance across runs compared with TD3: clear improvements in two domains, minor improvements , and/or an increase in variance in others. In some domains there was decrease in mean performance. The reviewers wanted comparisons with other baseline methods (even in terms of variance across runs).

It is difficult to evaluate the results in this paper, as the performance is averaged over only 5 runs, and runs which result in ""failure"" are discarded from analysis. The authors explain this was done in the original TD3 code, and one can sympathise in following common practices in the literature. However, the consensus of the reviewers and the AC was that this choice was not well defended, obscures a key difficulty of the learning problem, and makes algorithms look considerably stronger then they actually are. This is particularly confounding in a paper about improving the robustness of learning algorithms. This is not acceptable empirical practice and we strongly encourage the authors to discontinue this.

The reviewers gave nice suggestions including changing the pitch of the paper, and including results in noisy tasks. To reduce the burden of doing more scientific experiments, we suggest the authors start with small or even designed problems to carefully study robustness of learning and the potential improvements due to their algorithm. After this is done in a statistically significant way, it would be natural to move to more demonstration style scaled up results.",Paper Decision
fVnqGoAxk,Syx9ET4YPB,Do Image Classifiers Generalize Across Time?,Reject,"This paper proposed to evaluate the robustness of CNN models on similar video frames. The authors construct two carefully labeled video databases. Based on extensive experiments, they conclude that the state of the art classification and detection models are not robust when testing on very similar video frames. While Reviewer #1 is overall positive about this work, Reviewer #2 and #3 rated weak reject with various concerns. Reviewer #2 concerns limited contribution since the results are similar to our intuition. Reviewer #3 appreciates the value of the databases, but concerns that the defined metrics make the contribution look huge. The authors and Reviewer #3 have in-depth discussion on the metric, and Reviewer #3 is not convinced. Given the concerns raised by the reviewers, the ACs agree that this paper can not be accepted at its current state.",Paper Decision
jgHnavHtUA,SJl5Np4tPr,Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation,Accept (Spotlight),"This submission addresses the problem of few-shot classification. The proposed solution centers around metric-based models with a core argument that prior work may lead to learned embeddings which are overfit to the few labeled examples available during learning. Thus, when measuring cross-domain performance, the specialization of the original classifier to the initial domain will be apparent through degraded test time (new domain) performance. The authors therefore, study the problem of domain generalization in the few-shot learning scenario. The main algorithmic contribution is the introduction of a feature-wise transformation layer. 

All reviewers suggest to accept this paper. Reviewer 3 says this problem statement is especially novel. Reviewer 1 and 2 had concerns over lack of comparisons with recent state-of-the-art methods. The authors responded with some additional results during the rebuttal phase, which should be included in the final draft. 

Overall the AC recommends acceptance, based on the positive comments and the fact that this paper addresses a sufficiently new problem statement.
",Paper Decision
cdm8ijukh9,rkxtNaNKwr,Evolutionary Reinforcement Learning for Sample-Efficient Multiagent Coordination,Reject,"This work has a lot of promise; however, the author response was not sufficient to address the concerns expressed by reviewer 1, leading to an aggregate rating that is just not sufficient to justify an acceptance recommendation. The AC recommends rejection.",Paper Decision
vrhf8or8S8,H1lKNp4Fvr,A shallow feature extraction network with a large receptive field for stereo matching tasks,Reject,"The paper proposed the use of a shallow layers with large receptive fields for feature extraction to be used in stereo matching tasks. It showed on the KITTI2015 dataset this method leads to large model size reducetion while maintaining a comparable performance.

The main conern on this paper is the lack of technical contributions:
* The task of stereo matching is very specialized one, simply presenting the model size reduction and performance is not interesting to general readers. Adding more analysis that help understanding why the proposed method helps in this particular task and for what kind of tasks a shallow feature instead a deeper one is perferred. In that way, the paper would be addressing much wider audiences. 
* The discussions on related work is not thorough enough, lacking of analysis of pros and cons between different methods.",Paper Decision
jqZYjNxLZw,BkxtNaEYDr,Learning Boolean Circuits with Neural Networks,Reject,"The paper puts forward a theoretical investigation of the learnability of                                                          
tree-structured Boolean circuits with neural networks.                                                                             
The authors identify *local correlations*, ie correlation of each internal                                                         
target circuit gate with the target output, as critical property for                                                               
characterizing learnability by layerwise training.                                                                                 
                                                                                                                                   
The reviewers agree that the paper is well written and content to be correct                                                       
(to the best of their knowledge).                                                                                                  
However, they have reservations about the strength of the assumptions about the                                                    
target functions as well as the layerwise training procedure.                                                                      
                                                                                                                                   
I think this paper is slightly below acceptance threshold for ICLR, which is a quite                                               
applied conference.                                                                                                                
The assumptions are quite strong, ie local correlations and the topology of the                                                    
circuit to be known as well as layerwise training, and possibly too far removed                                                    
from current deep learning practice.",Paper Decision
Flqnlcg2Pa,B1lPETVFPS,Towards Principled Objectives for Contrastive Disentanglement,Reject,"The paper proposes new regularizations on contrastive disentanglement. After reading the author's response,  all the reviewers still think that the contribution is too limited and all agree to reject.",Paper Decision
ZAwj9Pyym,HkePNpVKPB,Compositional languages emerge in a neural iterated learning model,Accept (Poster),"This paper examines the correspondence between topological similarity of languages (correlation between the message space and object space) and ability to learn quickly in a situation of emergent communication between agents.

While this paper is not without issues, it does seem to present a nice contribution that all of the reviewers appreciated to some extent. I think it will spark further discussions in this area, and thus can recommend it for acceptance.",Paper Decision
swMAvtPNLo,rJeINp4KwH,Population-Guided Parallel Policy Search for Reinforcement Learning,Accept (Poster),"The  paper proposes a new approach to multi-actor RL, which ensure diversity and performance of the population of actors, by distilling the policy of the best performing agent in a soft way and maintaining some distance between the agents. The authors show improved performance over several state-of-the-art mono-actor algorithms and over several other multi-actor RL algorithms.  Initially, reviewers were concerned with magnitude of the contribution/novelty, as well as some technical issues (e.g. the beta update), and relative lack of baseline comparisons.  However, after discussion the reviewers largely agree that their main concerns have been addressed.  Therefore, I recommend this paper for acceptance.",Paper Decision
kF8rX0M39,r1lL4a4tDB,Variational Recurrent Models for Solving Partially Observable Control Tasks,Accept (Poster),"The authors propose to decompose control in a POMDP into learning a model of the environment (via a VRNN) and learning a feed-forward policy that has access to both the environment and environment model. They argue that learning the recurrent environment model is easier than learning a recurrent policy. They demonstrate improved performance over existing state-of-the-art approaches on several PO tasks.

Reviewers found the motivation for the proposed approach convincing and the experimental results proved the effectiveness of the method. The authors response resolved reviewers concerns, so as a result, I recommend acceptance.",Paper Decision
ZHxza-ZX2O,rygBVTVFPB,Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning,Reject,"This paper proposes using RL to solve PDEs, with application to solving conservation laws. It is quite borderline, with one reviewer weakly recommending acceptance, one finding the paper interesting but the application not sufficiently novel, and one confessing they have not understood the paper.

I concur with R2 this is a difficult subject matter, but the other reviewers seem satisfied with the clarity of the presentation. R3 seems to believe the paper sufficiently proves the concept to warrant publication. I confess I do not understand R1's argument for lack of novelty, despite my pushing for further detail. I see this as a novel application of RL methods, and R1 admits this will be seen as novel for a PDE conference. I am in favour of a certain degree of interdisciplinarity at ICLR, and believe this paper could bring a bit of subject matter diversity to the programme. However, due to the number of high quality submissions in my area, I'm afraid this one must be rejected due to limited space. The authors are encouraged to submit to another ML conference after addressing (or having addressed) some of the action items from the more sympathetic reviewers.",Paper Decision
puSotMhhn0,rkl44TEtwH,Composable Semi-parametric Modelling for Long-range Motion Generation,Reject,"The submission presents a semi-parametric approach to motion synthesis. The reviewers expressed concerns about the presentation, the relationship to existing work, and the scope of the results. After the authors' responses and revision, concerns remain. The AC also notes that the submission is 10 pages long. The AC recommends rejecting the submission.",Paper Decision
FXiuXBGXA2,BJlEEaEFDS,Towards an Adversarially Robust Normalization Approach,Reject," This paper presents an empirical analysis of the reasons behind BatchNorm vulnerability to adversarial inputs, based on the hypothesis that such vulnerability may be caused by using different statistics during the inference stage as compared to the training stage. While the paper is interesting and clearly written, reviewers point out insufficient empirical evaluation in order to make the claim more convincing.",Paper Decision
_Y9_l3b_VY,Syg7VaNYPB,Generative Latent Flow,Reject,The authors propose generative latent flow which uses autoencoder to learn latent representations and normalizing flows to map that distribution. The reviewers feel that there is limited novelty since it is a straightforward combination of existing ideas. ,Paper Decision
CyBEqwmqV,SJeQEp4YDH,GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification,Accept (Poster),"This work addresses the problem of detecting an adversarial attack. This is a challenging problem as the detection mechanism itself is also vulnerable to attack. The paper proposes asymmetrical adversarial training as a robust solution. This approach partitions the feature space according to the output of the robust classifier and trains an adversarial example detector per partition. The paper demonstrates improvements over state-of-the-art detection techniques. 

All three reviewers recommend acceptance of this work. Some positive points include the paper being well-written with strong experimental evidence. One potential difficulty with the proposed approach is the additional computational cost associated with a per class adversarial attack detector. The authors have responded to this concern by claiming that the straightforward version of their approach is K times slower (10 in the case of 10 classes), but their integrated version is 2x slower as they only run the detector associated with the example-specific class prediction. We encourage the authors to include a discussion on computational cost in the final version. In addition, there was a community comment about black-box testing which will be of relevance to many in the community. The authors have already provided additional experiments to address this question as well as code to reproduce the new experiment. 

Overall, the paper addresses an important problem with a two-step solution of training a robust model and detecting potentially perturbed samples per class. This is a novel solution with comprehensive experiments and therefore recommend acceptance. 
",Paper Decision
lfe0dU7juD,r1e74a4twH,CZ-GEM:  A  FRAMEWORK  FOR DISENTANGLED REPRESENTATION LEARNING,Reject,"The paper addresses the problem of learning disentangled representations in supervised and unsupervised settings. 

In general, the problem of representation learning in of course a core problem in ICLR. However, in the set-up described by the authors, R2 commented on the the set-up for supervised being a bit unnatural in as detailed labels need to be given (somewhat confusingly, the labels are called control variates in the paper).

Several reviewers commented on the novelty of the paper being on the low side, with R2 commenting the contribution being fairly small, and R3 noting similarities to stackgan.

There were also some comments on quality, and clarity. On the topic of technical quality, R2 did note that the authors present extensive results, but R3 mentions that the case for the disentanglement improving is not sufficiently supported. In terms of clarity, there was some initial confusing about e.g. the inference procedure, though the authors addressed these issues in the discussion. ",Paper Decision
lXbGBMe4d,HkxzNpNtDS,Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning,Reject,"The paper proposes a multitask navigation model that can be trained on both vision-language navigation (VLN) and navigation from dialog history (NDH) tasks. The authors provide experiments that demonstrate that their model can outperformance single-task baseline models.

The paper received borderline scores with two weak accept and one weak reject.  Overall, the reviewers found the paper to be well-written and easy to understand, with thorough experiments.

The reviewers had minor concerns about the following:
1. The generalizability of the work.  No results are reported on the test set, only on val.
2. The gains for val unseen are pretty small and there are other models (e.g. Ke et al, Tan et al) that have better results.  Would the proposed environment-agnostic multitask learning be able to improve those models as well?  Or is the gains limited to having a weak baseline?
3. It's unclear if the gains are due to the multitasking or just having more data available to train on.
4. There are some minor issues with the misspellings/typos.  Some examples are given:
Page 1: ""Manolis Savva* et al"" --> ""Savva et al""
Page 5: ""x_1, x2, ..., x_3"" --> Should the x_3 be something like x_k where k is the length of the utterance?

The AC agrees with the reviewers that the paper is interesting and is mostly solid work.  The AC also feels that there are some valid concerns about the generalizability of the work and that the paper would benefit from a more careful consideration of the issues raised by the reviewers.  The authors are encouraged to refine the work and resubmit.",Paper Decision
TLsRXk8Tym,H1efEp4Yvr,Global Concavity and Optimization in a Class of Dynamic Discrete Choice Models,Reject,"The authors develop theoretical results showing that policy gradient methods converge to the globally optimal policy for a class of MDPs arising in econometrics. The authors show empirically that their methods perform on a standard benchmark.

The paper contains interesting theoretical results. However, the reviewers were concerned about some aspects:
1) The paper does not explain to a general ML audience the significance of the models considered in the paper - where do these arise in practical applications? Further, the experiments are also limited to a small MDP - while this may be a standard benchmark in econometrics, it would be good to study the algorithm's scaling properties to larger models as is standard practice in RL.

2) The implications of the assumptions made in the paper are not explained clearly, nor are the relative improvements of the authors' work relative to prior work. In particular, one reviewer was concerned that the assumptions could be trivially satisfied and the authors' rebuttal did not clarify this sufficiently.

Thus, I recommend rejection but am unsure since none of the reviewers nor I am an expert in this area.",Paper Decision
T-9UMCY_P-,Syg-ET4FPS,Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information,Accept (Talk),"The paper extends posterior sampling to the multi-agent RL setting, and develops a novel algorithm with convergence guarantees to a Nash Equilibrium strategy in two-player zero sum games. Reviewers raised several questions, many of which were well addressed by the authors and which helped further clarify the approach and contribution of the paper. The paper is timely in that novel connections between Game Theory and RL are being explored in fruitful ways, and the paper provides valuable new insights and directions for future research.",Paper Decision
o-fdEHpo94,BJeWVpNtwr,On the Pareto Efficiency of Quantized CNN,Reject,This paper studies the trade-off between the model size and quantization levels in quantized CNNs by varying different channel width multipliers. The paper is well  motivated and draws interesting observations but can be improved in terms of evaluation. It is a borderline case and rejection is made due to the high competition. ,Paper Decision
E5lxdTfRo2,B1lxV6NFPH,BANANAS: Bayesian Optimization with Neural Networks for Neural Architecture Search,Reject,"This paper uses Bayesian optimization with neural networks for neural architecture search. 
One of the contributions is a path-based encoding that enumerates every possible path through a cell search space. This encoding is shown to be surprisingly powerful, but it will not scale to large cell-based search spaces or non-cell-based search spaces. The availability of code, as well as the careful attention to reproducibility is much appreciated and a factor in favor of the paper.

In the discussion, it surfaced that a comparison to existing Bayesian optimization approaches using neural networks would have been possible, while the authors initially did not think that this would be the case. The authors promised to include these comparisons in the final version, but, as was also discussed in the private discussion between reviewers and AC, this is problematic since it is not clear what these results will show. Therefore, the one reviewer who was debating about increasing their score did in the end not do so (but would be inclined to accept a future version with a clean and thorough comparison to baselines). 

All reviewers stuck with their score of ""weak reject"", leaning to borderline. I read the paper myself and concur with this judgement. I recommend rejection of the current version, with an encouragement to submit to another venue after including a comparison to BO methods based on neural networks.",Paper Decision
BZdKH35SH_,SkexNpNFwS,Potential Flow Generator with $L_2$ Optimal Transport Regularity for Generative Models,Reject,"This paper proposes applying potential flow generators in conjunction with L2 optimal transport regularity to favor solutions that ""move"" input points as little as possible to output points drawn from the target distribution.  The resulting pipeline can be effective in dealing with, among other things, image-to-image translation tasks with unpaired data.  Overall, one of the appeals of this methodology is that it can be integrated within a number of existing generative modeling paradigms (e.g., GANs, etc.).

After the rebuttal and discussion period, two reviewers maintained weak reject scores while one favored strong acceptance.  With these borderline/mixed scores, this paper was discussed at the meta-review level and the final decision was to side with the majority, noting that a revision which fully addresses reviewer comments could likely be successful at a future venue.  As one important lingering issue, R1 pointed out that the optimality conditions of the proposed approach are only enforced on sampled trajectories, not actually on the entire space.  The rebuttal concedes this point, but suggests that the method still seems to work.  But as an improvement, the suggestion is made that randomly perturbed trajectories could help to mitigate this issue.  However, no experiments were conducted using this modification, which could be helpful in building confidence in the reliability of the overall methodology.

Additionally, from my perspective the empirical validation could also be improved to help solidify the contribution in a revision.  For example, the image-to-image translation experiments with CelebA were based on a linear (PCA) embedding and feedforward networks.  It would have been nice to have seen a more sophisticated setup for this purpose (as discussed in Section 5), especially for a non-theoretical paper with an ostensibly practically-relevant algorithmic proposal.  And consistent with reviewer comments, the paper definitely needs another pass to clean up a number of small grammatical mistakes.",Paper Decision
s8sTlbxcj,HJeg46EKPr,Integrative Tensor-based Anomaly Detection System For Satellites,Reject,"The paper applies tensor analysis techniques to anomaly detection from satellite data. The proposed solution is simple and seems to achieve good results. However, there is limited novelty in methodology and no sufficient experiments have been conducted to explain the performance gain. The paper is not ready for publication in ICLR but could be suitable for an application oriented venue. ",Paper Decision
d-0NsuthcE,Skgy464Kvr,Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions,Accept (Poster),"This paper presents a mechanism for capsule networks to defend against adversarial examples, and a new attack, the reconstruction attack. The differing success of this attacks on capsnets and convnets is used to argue that capsnets find features that are more similar to what humans use.

Reviewers generally like the paper, but took instance with the strength of the claim (about the usefulness of the examples) and argued that the paper might not be as novel as it claims.

Still, this seems like a valuable contribution that should be published.",Paper Decision
RJBYXtm-J,rJx1Na4Fwr,MACER: Attack-free and Scalable Robust Training via Maximizing Certified Radius,Accept (Poster),"The submission proposes a robustness certification technique for smoothed classifiers for a given l_2 attack radius.

Strengths:
-The majority opinion is that this work is a non-trivial extension of prior work to provide radius certification.
-The work is more efficient that strong recent baselines and provides better performance.
-It successfully achieves this while avoiding adversarial training, which is another novel aspect.

Weaknesses:
-There were some initial concerns about missing experiments and unfair comparisons but these were sufficiently addressed in the discussion.

AC shares the majority opinion and recommends acceptance.",Paper Decision
r2-uZTcpRb,rJx0Q6EFPB,TinyBERT: Distilling BERT for Natural Language Understanding,Reject,"This paper proposes a new distillation-based method for using large pretrained models like BERT to produce much *smaller* fine-tuned target-task models. 

This paper is low-borderline: It has merit and meets our basic standards, but owing to capacity limitations we had to give preference to papers we see as having a higher potential impact. Reviewers had some concerns about experimental design, but those seem to have been fully resolved after discussion. Reviewers were not convinced, even after some discussion, that the method and results were sufficiently novel and effective to have a substantial impact on the state of practice in this area.",Paper Decision
dW1qEjyuuy,HklCmaVtPS,UW-NET: AN INCEPTION-ATTENTION NETWORK FOR UNDERWATER IMAGE CLASSIFICATION,Reject,The reviewers have issues with the lack of enough experimental results as well as with novelty of the solution proposed. I recommend rejection.,Paper Decision
0uvZfmQaJf,ByxT7TNFvH,Semantically-Guided Representation Learning for Self-Supervised Monocular Depth,Accept (Poster),"The paper proposes a using pixel-adaptive convolutions to leverage semantic labels in self-supervised monocular depth estimation. Although there were initial concerns of the reviewers regarding the technical details and limited experiments, the authors responded reasonably to the issues raised by the reviewers. Reviewer2, who gave a weak reject rating, did not provide any answer to the authors comments. We do not see any major flaws to reject this paper.",Paper Decision
tl6hE5oLCD,HJepXaVYDr,Stochastic AUC Maximization with Deep Neural Networks,Accept (Poster),The paper proposed using stochastic AUC for dealing with imbalanced data. This paper provides useful insights and experiments on this important problem. I recommend acceptance.,Paper Decision
xiOkvVySEx,S1enmaVFvS,Data-Driven Approach to Encoding and Decoding 3-D Crystal Structures,Reject,"This paper presents an encoder-decoder based approach to construct a compressed latent space representation of each molecule. Then a second neural network segments the output and assigns an atomic number. Unlike previous works using 1D or 2D representations, the proposed method focuses on the 3D representations.

The reviewers have several major concerns. Firstly, the novelty of the paper seems to be limited as the proposed method mainly use the existing techniques. Secondly, there is no clear baseline to compare with. Finally, there is no clear quantitative results to measure the proposed method. The rebuttal did not well address these problems.

Overall, this paper did not meet the standard of ICLR and I choose to reject the paper.
",Paper Decision
KA65jj-wX,BJgnXpVYwS,Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity,Accept (Talk),"Gradient clipping is increasingly popular and it's nice to see a paper theoretically exploring its nice performance. All reviewers appreciated the work and the results.

Please make sure to incorporate all of their comments for the final version.",Paper Decision
gxqc5iNpa6,SJx37TEtDH,Why ADAM Beats SGD for Attention Models	,Reject,"This paper tries to explain why Adam is better than sgd for training attention model. In specific, it first provides some empirical and theoretical evidence that a heavy-tailed distribution of the noise in stochastic gradients is the cause of SGD's worse performance. Then the authors studied a clipped variant of SGD that circumvents this issue, and revisited Adam through the lens of clipping. Overall, this paper conveys some interesting ideas. On the other hand, the theorems proved in this paper do not provide additional insight besides the intuition and the experiments are weak (hyperparameters are not carefully tuned). So even after author response, it still does not gather sufficient support from the reviewers. This is a borderline paper, and due to a rather limited number of papers the conference can accept, I encourage the authors to improve this paper and resubmit it to future conference.
",Paper Decision
WRsFikw0p,HyxoX6EKvB,Reflection-based Word Attribute Transfer,Reject,"This paper proposes a way to transform word vectors based on a binary attribute (e.g. male/female) based on reflection, with the property that applying the reflection operator twice, the vector for a word is left unchanged.  By identifying parameterized mirror planes for each word, the proposed method can leave neutral words left unchanged.

The paper received 3 weak accepts.  There was initially one reject, but the revisions convinced the reviewer to update their score to a weak accept.  Overall, the reviewers appreciated the idea of reflection-based binary word attribute transfer.   suggestions, the authors made small improvements to the writing, added missing citations, as well as additional results for another word embedding (GloVE) and another dataset (antonyms).  One of the main remaining weakness of the work, is still the small dataset.  Although somewhat alleviated by the inclusion of the antonym dataset, this is still a weakness of the paper.  

The AC agrees that the paper has an nice idea and is well presented.  However, the work is limited in scope and is likely to be of limited interest to the ICLR community and would be more appreciated in the NLP community.  The authors are encouraged to improve upon the work, and resubmit to an appropriate venue.  
",Paper Decision
rIcDsAYwjb,rygjmpVFvB,Difference-Seeking Generative Adversarial Network--Unseen Sample Generation,Accept (Poster),The authors propose a way to generate unseen examples in GANs by learning the difference of two distributions for which we have access. The majority of reviewers agree on the originality and practicality of the idea.,Paper Decision
EFj5MHtyvT,B1l5m6VFwr,EINS: Long Short-Term Memory with Extrapolated Input Network Simplification,Reject,The paper proposed new version of LSTM which is claimed to abandon the redundancies in LSTM.  It is weak both in theory and experiments.  All reviewers gave clear rejects and the AC agree.,Paper Decision
L7NNrTRwaP,BJgqQ6NYvB,FasterSeg: Searching for Faster Real-time Semantic Segmentation,Accept (Poster),"This paper presents neural architecture search for semantic segmentation, with search space that integrates multi-resolution branches. The method also uses a regularization to overcome the issue of learned networks collapsing to low-latency but poor accuracy models. Another interesting contribution is a collaborative search procedure to simultaneously search for student and teacher networks in a single run. All reviewers agree that the proposed method is well-motivated and shows promising empirical results. Author response satisfactorily addressed most of the points raised by the reviewers. I recommend acceptance. ",Paper Decision
UVjzdwO1h1,SJetQpEYvB,LEARNING EXECUTION THROUGH NEURAL CODE FUSION,Accept (Poster),"This paper presents a method to learn representations of programs via code and execution.

The paper presents an interesting method, and results on branch prediction and address pre-fetching are conclusive. The only main critiques associated with this paper seemed to be (1) potential lack of interest to the ICLR community, and (2) lack of comparison to other methods that similarly improve performance using other varieties of information. I am satisfied by the authors' responses to these concerns, and believe the paper warrants acceptance.",Paper Decision
vjvsq9gA7,HJedXaEtvS,Editable Neural Networks,Accept (Poster),"This paper proposes a method which patches/edits a pre-trained neural network's predictions on problematic data points. They do this without the need for retraining the network on the entire data, by only using a few steps of stochastic gradient descent, and thereby avoiding influencing model behaviour on other samples. The post patching training can encourage reliability, locality and efficiency by using a loss function which incorporates these three criteria weighted by hyperparameters. Experiments are done on CIFAR-10 toy experiments, large-scale image classification with adversarial examples, and machine translation. The reviews are generally positive, with significant author response, a new improved version of the paper, and further discussion. This is a well written paper with convincing results, and it addresses a serious problem for production models, I therefore recommend that it is accepted. ",Paper Decision
E2Fee4sHfK,HkedQp4tPr,Parallel Scheduled Sampling,Reject,"The paper proposes a parallelization approach for speeding up scheduled sampling, and show significant improvement over the original.  The approach is simple and a clear improvement over vanilla schedule sampling.  However, the reviewers point out that there are more recent methods to compare against or combine with, and that the paper is a bit thin on content and could have addressed this.  The proposed approach may well combine well with newer techniques, but I tend to agree that this should be tested.",Paper Decision
ihmfUxggG,rygPm64tDH,Learning Explainable Models Using Attribution Priors,Reject,"This work claims two primary contributions: first a new saliency method ""expected gradients"" is proposed, and second the authors propose the idea of attribution priors to improve model performance by integrating domain knowledge during training. Reviewers agreed that the expected gradients method is interesting and novel, and experiments such as Table 1 are a good starting point to demonstrate the effectiveness of the new method. However, the claimed ""novel framework, attribution priors"" has large overlap with prior work [1]. One suggestion for improving the paper is to revise the introduction and experiments to support the claim ""expected gradients improve model explainability and yield effective attribution priors"" rather than claiming to introduce attribution priors as a new framework. One possibility for strengthening this claim is to revisit experiments in [1] and related follow-up work to demonstrate that expected gradients yield improvements over existing saliency methods. Additionally, current experiments in Table 1 only consider integrated gradients as a baseline saliency method, there are many others worth considering, see for example the suite of methods explored in [2]. 

Finally, I would add that the current section on distribution shift provides an overly narrow perspective on model robustness by only considering robustness to additive Gaussian noise. It is known that it is easy to improve robustness to Gaussian noise by biasing the model towards low frequency statistics in the data, however this typically results in degraded robustness to other kinds of noise types. See for example [3], where it was observed that adversarial training degrades model robustness to low frequency noise and the fog corruption. If the authors wish to pursue using attribution priors for improving robustness to distribution shift, it is important that they evaluate on a more varied suite of corruptions/noise types [4]. Additionally, one should compare against strong baselines in this area [5].

1. https://arxiv.org/abs/1703.03717
2. https://arxiv.org/abs/1810.03292
3. https://arxiv.org/abs/1906.08988
4. https://arxiv.org/abs/1807.01697
5. https://arxiv.org/abs/1811.12231
",Paper Decision
w6qeEwpYFM,rygw7aNYDS,Efficient Inference and Exploration for Reinforcement Learning,Reject,"This paper examined a pure exploration method for efficient action value estimates in tabular reinforcement learning.  The paper is on the theoretical properties of value estimates in the large sample regime.  The method is shown to outperform baseline algorithms for this task in tabular reinforcement learning.

The reviewers were divided on the merits of this work.  The use of the central limit theorem was viewed as elegant, and the results were thought to be potentially useful.  However, the reviewers several limitations.  They found the assumption of a communicating MDP to be overly restrictive (reviewer 1).  The algorithm may be computationally inefficient (reviewer 2).  The nature of ""exploration"" in this work is not the conventional meaning in reinforcement learning (reviewer 3).

The paper is not yet ready for publication at ICLR.  The theoretical results do not clearly convey insights for reinforcement learning with function approximation, and the reviewers are also not in agreement that the current results are applicable to a general MDP setting.",Paper Decision
_H0U8Glemg,HJeIX6EKvr,Leveraging inductive bias of neural networks for learning without explicit human annotations,Reject,"The authors present an approach to learning from noisy labels. The reviews were mixed and several issues remain unresolved. I do not accept the following as a valid response: ""We fully agree that noisily collected labels are common for many problems other than image classification. However, the focus of our paper is image classification, and we thus concentrate on classification problems related to the widely popular CIFAR-10 and ImageNet classification problems."" ICLR is a conference on theoretical and applied ML, and the fact that a technique has not been used for image classification before, does not mean you bring something to the table by doing so. The NLP literature is abundant with interesting work on label noise and should obviously be considered related work. That said, there's also missing references directly related to the connection between early stopping/regularization and label bias correction, including: 

[0] https://arxiv.org/pdf/1904.11238.pdf
[1] https://arxiv.org/pdf/1705.03419.pdf
[2] http://proceedings.mlr.press/v80/ma18d/ma18d.pdf

See also this paper submitted to this conference: https://openreview.net/forum?id=SJldu6EtDS",Paper Decision
0gKpzAqYRt,Bke8764twr,Bias-Resilient Neural Network,Reject,"The paper addressed the problem of machine bias when training machine learning models. The authors propose an approach based on representation learning with adversarial training. As opposed to the majority of previous works that trying to create a representation from which it is not possible to predict the sensitive feature (bias), the authors propose to minimize the dependency between the learned features and the sensitive feature with adversarial training. While acknowledging that the proposed model is addressing an important problem and is potentially useful, the reviewers and AC note the following potential weaknesses:  
(1) limited technical contribution -- the proposed approach is similar to a number of works published in machine learning and computer vision before the submission deadline that were overlooked by the authors. Specifically: i) adversarial training for learning fair representations [Edwards and Storkey, Censoring Representations with an Adversary, ICLR 2016], [Beutel, et al 2017, Data decisions and theoretical implications when adversarially learning fair representations], ii) learning fair representation by minimizing the dependency between the latent representation and the sensitive attributes [The variational fair autoencoder, ICLR 2016 by Louizos et al.; Fairness Constraints: Mechanisms for Fair Classification, by Zafar et al, 2015] or by minimizing the mutual information between feature embedding and bias [Learning Not to Learn: Training Deep Neural Networks with Biased Data, CVPR 2019]. 
(2) Limited empirical evidence -- the baseline methods used in the evaluation are not sufficient to assess the benefits of the proposed approach over the existing SOTA methods mentioned above. In fact, none of the baseline methods used in the evaluation tackle machine bias (via adversarial training or minimizing statistical dependence). 
(3) It would be beneficial to also report fairness metrics, e.g. equality of opportunity, statistical parity, to assess the effectiveness of bias removal. R1 has raised some concerns regarding empirical evidence -- see the point about mixed results. Also R2 has reported concerns regarding controversial results in experiment 4.2 and suggested ways to justify when and why the results of the CNNs baseline are close to the BR-Net. Addressing these concerns would strengthen the contributions of the proposed method.    

Among these, (3) did not have a decisive impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed approach, and were viewed by AC as critical issues. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.
",Paper Decision
j4oFzv7WK-,SJgBQaVKwH,Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis,Reject,"This paper investigates variational models of speech for synthesis, and in particular ways of making them more controllable for a variety of synthesis tasks (e.g. prosody transfer, style transfer).  They propose to do this via a modified VAE objective that imposes a learnable weight on the KL term, as well as using a hierarchical decomposition of latent variables.  The paper shows promising results and includes a good amount of analysis, and should be very interesting for speech synthesis researchers.  However, there is not much novelty from a machine learning perspective.  Therefore, I think the paper is not a great fit for ICLR and is better suited for a speech conference/journal.",Paper Decision
Kk6JiG3VBQ,HJgS7p4FPH,Accelerating Reinforcement Learning Through GPU Atari Emulation,Reject,"The paper presented a detailed discussion on the implementation of a library emulating Atari games on GPU for efficient reinforcement learning. The analysis is very thoroughly done. The major concern is whether this paper is a good fit to this conference. The developed library would be useful to researchers and the discussion is interesting with respect to system design and implementation, but the technical depth seems not sufficient.",Paper Decision
K3tRBTyOr,rklB76EKPr,Can gradient clipping mitigate label noise?,Accept (Poster),This paper studies the effect of clipping on mitigating label noise. The authors demonstrate that standard gradient clipping does not suffice for achieving robustness to label noise. The authors suggest a noise-robust alternative. In the discussion the reviewers raised some interesting questions and technical detailed but mostly agreed that the paper is well-written with nice contributions. I concur with the reviewers that this is a nicely written paper with good contributions. I recommend acceptance but recommend the authors continue to improve their paper based on the reviewers' suggestions.,Paper Decision
2FHgt5pFGV,r1eVXa4KvH,Concise Multi-head Attention Models,Reject,"This paper studies tradeoffs in the design of attention-based architectures. It argues and formally establishes that the expressivity of an attention head is determined by its dimension and that fixing the head dimension, one gains additional expressive power by using more heads.

Reviewers were generally positive about the question under study here, but raised important concerns about the significance of the results and the take-home message in the current manuscript. The AC shares these concerns, and recommends rejection, while encouraging the authors to address the concerns raised during this discussion. ",Paper Decision
GD6P6w1Xc,S1e4Q6EtDH,Tensorized Embedding Layers for Efficient Model Compression,Reject,"This paper has been reviewed by three reviewers and received scores: 6/3/8. While two reviewers were reasonably positive, they also did not provide a very compelling reviews (e.g. one rev. just reiterated the rationale behind tensor model compression and the other admitted the paper is of limited novelty). Perhaps the shortest review (and perhaps the most telling) prompts authors to the fact that the model compression with tensor decompositions is quite common in the literature these days. One example could be T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor by Kossaifi et al. Very likely the authors will find many more recent developments on model compression with/without tensor decomp. For a good paper in this topic, authors should carefully consider various tensor factorizations (Tucker, TT, tensor rings, t-product and many more) and consider theoretical contributions and guarantees. Taking into account all pros and cons, this submissions falls marginally short of the ICLR 2020 threshold but the authors are encouraged to work on further developments.",Paper Decision
g881K34zhI,HygQ7TNtPr,Rethinking Neural Network Quantization,Reject,"The submission proposes methodology for quantizing neural networks.  The reviewers were unanimous in their opinion that the paper is not suitable for publication at ICLR.  Concerns included novelty over previous works, comparatively weak baseline comparisons, and overly restrictive assumptions.",Paper Decision
F5KL4g70Fq,HyeX7aVKvr,Zero-shot task adaptation by homoiconic meta-mapping,Reject,"The authors presents a method for adapting models to new tasks in a zero shot manner using learned meta-mappings.  The reviewers largely agreed that this is an interesting and creative research direction.  However, there was also agreement that the writing was unclear in many sections, that the appropriate metalearning baselines were not compared to, and that the power of the method was unclear due to overly simplistic domains.  While the baseline issue was mostly cleared up in rebuttal and discussion, the other issues remain.  Thus, I recommend rejection at this time.",Paper Decision
1SxJsPqaQF,ryefmpEYPr,iSparse: Output Informed Sparsification of Neural Networks,Reject,"Thank you very much for your feedback to the reviewers, which helped us a lot to better understand your paper.
However, the paper is still premature to be accepted to ICLR2020. We hope that the detailed reviewers' comments help you improve your paper for potential future submission.
",Paper Decision
tFKqvND6zj,SJgzXaNFwS,HyperEmbed:  Tradeoffs Between Resources and Performance in NLP Tasks with Hyperdimensional Computing enabled embedding of n-gram statistics ,Reject,"The reviewers were unanimous that this submission is not ready for publication at ICLR in its present form.

Concerns raised included lack of relevant baselines, and lack of sufficient justification of the novelty and impact of the approach.",Paper Decision
GUm-Vn9oTI,BJlzm64tDH,Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model,Accept (Poster),"This submission proposes a secondary objective when learning language models like BERT that improves the ability of such models to learn entity-centric information. This additional objective involves predicting whether an entity has been replaced. Replacement entities are mined using wikidata.

Strengths:
-The proposed method is simple and shows significant performance improvements for various tasks including fact completion and question answering.

Weaknesses:
-The experimental settings and data splits were not always clear. This was sufficiently addressed in a revised version.
-The paper could have probed performance on tasks involving less common entities.

The reviewer consensus was to accept this submission.
",Paper Decision
LdOBXwpzD,H1e-X64FDB,"Fast Linear Interpolation for Piecewise-Linear Functions, GAMs, and Deep Lattice Networks",Reject,"This paper proposes an efficient implementation of piecewise linear functions.

While this paper tackles a problem of large apparent interest, as noted by the reviewers the paper (1) is pretty far from the domain of the average ICLR paper, and (2) not written with the high standards of clarity that would make it accessible to the average ICLR reader. I am not impugning on the merits of the paper itself, but would suggest that the authors both take the reviewer's advice with regards to the clarity issues (among other) and consider submitting to the Systems for ML workshop, a systems conference, a compilers conference, or some other venue with a larger percentage of qualified readers (and reviewers).",Paper Decision
AoAAt75GL2,HygbQaNYwr,Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system,Reject,This paper proposes to introduce perturbation biases as a counter-measure against adversarial perturbations. The perturbation biases are additional bias terms that are trained by a variant of gradient ascent. Serious issues were raised in the comments. No rebuttal was provided.,Paper Decision
mxD7HMlBEc,HJel76NYPS,Collaborative Generated Hashing for Market Analysis and Fast Cold-start Recommendation,Reject,"The presented work has worse accuracy than existing (and not all the baselines are given correctly) and does not provide the running time comparison. All reviewers recommend rejection, and I am with them.",Paper Decision
WOzj4OLEqd,rJeg7TEYwB,Pruned Graph Scattering Transforms,Accept (Poster),"Main content: Authors developed graph scattering transforms (GST) with a pruning algorithm, with the aim to reduce the running time and space cost, improve robustness to perturbations on input graph signal, and encourage flexibility for domain adaption. 
Discussion:
reviewer 1: likes the idea, considers it to be elegant and work well. some questions regarding the proofs in the paper but it sounds like authors have addressed concerns.
reviewer 2: solid paper and results, has questons on stability results, like reviewer 2.
reviewer 3: likes the idea, including good sufficient theoretical analysis and algorthmic stability. concern is around complexity analysis but sounds like the authors have addressed the concerns.
Recommendation: Well written solid paper with good proofs. Authors addressed any reviewer concerns and all 3 reviewres vote weak accept. This is good for poster.",Paper Decision
Lk4m9JvHck,B1x1ma4tDr,DDSP: Differentiable Digital Signal Processing,Accept (Spotlight),This paper proposes a novel differentiable digital signal processing in audio synthesis. The application is novel and interesting. All the reivewers agree to accept it. The authors are encouraged to consider the reviewer's suggestions to revise the paper.,Paper Decision
14KLzvQRtm,BkeJm6VtPH,Continual Learning via Neural Pruning,Reject,There are several concerns with the brittleness and reproducibility of the proposed approach and experiments.,Paper Decision
g0RvFiw63t,rylkma4twr,Min-Max Optimization without Gradients: Convergence and Applications to Adversarial ML,Reject,"This paper proposes convergence results for zeroth-order optimization.

One of the main complaints was that ZO has limited use in ML. I appreciate the authors' response that there are cases where gradients are not easily available, especially for black-box attacks.

However, I find the limited applicability an issue for ICLR and I encourage the authors to find a conference that is more suited to that work.",Paper Decision
6BLZXeFTk,BJgAf6Etwr,XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering,Reject,"The authors provide an analysis of a cross-lingual data augmentation technique which they call XLDA. This consists of replacing a segment of an input text with its translation in another language. They show that when fine-tuning, it is more beneficial to train on the cross-lingual hypotheses than on the in-language pairs, especially for low resource languages such as Greek, Turkish and Urdu. The paper explores an interesting idea however they lack comparison with other techniques such as backtranslation and XLM models, and would benefit from a wider range of tasks. I feel like this paper is more suitable for an NLP-focussed venue. ",Paper Decision
sa9S-YWOQF,BJlAzTEKwS,Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning,Reject,"The reviewers generally expressed considerable reservation about the novelty of the proposed method. After reading the reviews in detail and looking at the paper, I'm inclined to agree that the contribution is rather incremental. Using normalizing flows for representing policies in RL has been studied in a number of prior works, including with soft actor-critic, and I think the novelty in this work is limited in relation to prior work. Therefore, I cannot recommend acceptance at this time.",Paper Decision
v8ZVBwsy_,BkxpMTEtPB,GLAD: Learning Sparse Graph Recovery,Accept (Poster)," The paper proposes a neural network architecture to address the problem of estimating a sparse precision matrix from data, which can be used for inferring conditional independence if the random variables are gaussian. The authors propose an Alternating Minimisation procedure for solving the l1 regularized maximum likelihood which can be unrolled and parameterized. This method is shown to converge faster at inference time than other methods and it is also far more effective in terms of training time compared to an existing data driven method.

Reviewers had good initial impressions of this paper, pointing out the significance of the idea and the soundness of the setup. After a productive rebuttal phase the authors significantly improved the readibility and successfully clarified the remaining concerns of the reviewers. This AC thus recommends acceptance. ",Paper Decision
OvsQ_u7sg8,S1xaf6VFPB,PDP: A General Neural Framework for Learning SAT Solvers,Reject,"The authors present a neural framework for learning SAT solvers that takes the form of probabilistic inference. The whole process consists of propagation, decimation and prediction steps, so unlike other prior work like Neurosat that learns to predict sat/unsat and only through this binary signal,  this work presents a more modular approach, which is learned via energy minimization and it aims at predicting assignments (the assignments are soft which give rise to a differentiable loss). On the other hand, at test time the method returns the first soft assignment whose hard version (obtained by thresholding) satisfies the formula.  Reviewers found this to be an interesting submission, however there were some concerns regarding (among others) comparison to previous work.  

Overall, this submission has generated a lot of discussion among the reviewers (also regarding how this model actually operates)  and it is currently borderline without a strong champion. Due to the concerns raised and the limited space in the conference's program, unfortunately I cannot recommend this work for acceptance.
",Paper Decision
FbXEKYY8Xf,rJlnfaNYvB,Adaptive Loss Scaling for Mixed Precision Training,Reject,"This work proposes to improve mixed precision training by adaptively scaling the loss based on statistics from previous activations to minimize underflow during training. However, the method is designed rather heuristically and can be improved with stronger theoretical support and improved representation of the paper. 
",Paper Decision
22i8hPtNqL,B1l3M64KwB,How many weights are enough : can tensor factorization learn efficient policies ?,Reject,"In this paper dense layers in deep neural networks representing policies are replaced by tensor regression layers, also by a scattering layer, and second-order optimization is considered. The paper does not have a single consistent message, and combines different techniques for unclear reason. Important related work is not cited. The presentation was found unclear by the reviewers. ",Paper Decision
b3LFvGfBb5,ByljMaNKwB,Domain Aggregation Networks for Multi-Source Domain Adaptation,Reject,"Thanks for the detailed replies to the reviewers.
Their score was slightly improved, this paper is still below the bar given high competition of ICLR2020.
For this reason, we decided not to accept this paper.",Paper Decision
ixOQerMM-a,ryljMpNtwr,Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming,Reject,"This paper proposes a benchmark for assessing the impact of image quality degradation (e.g. simulated fog, snow, frost) on the performance of object detection models. The authors introduce corrupted versions of popular object detection datasets, namely PASCAL-C, COCO-C and Cityscapes-C, and an evaluation protocol which reveals that the current models are not robust to such corruptions (losing as much as 60% of the performance). The authors then show that a simple data augmentation scheme significantly improves robustness. The reviewers agree that the manuscript is well written and that the proposed benchmark reveals major drawbacks of current detection models. However, two critical issues with the paper paper remain, namely lack of novelty in light of Geirhos et al., and how to actually use this benchmark in practice. I will hence recommend the rejection of this paper in the current state. Nevertheless, we encourage the authors to address the raised shortcomings (the new experiments reported in the rebuttal are a good starting point). ",Paper Decision
soD47Jxqn,rJe9fTNtPS,AHash: A Load-Balanced One Permutation Hash,Reject,"This paper proposes a load-balanced hashing called AHash that balances the load of hashing bins to avoid empty bins that appear in some minwise hashing methods.
Reviewers found the work interesting and well-motivated. Authors addressed some clarity issues in their rebuttal. However the impact appeared quite limited, and the experimental validation limited to few realistic experiments that did not alleviate this concern.
We thus recommend rejection.",Paper Decision
DVBlUw7Iox,SJg9z6VFDr,Ordinary differential equations on graph networks,Reject,"This paper introduces a few ideas to potentially improve the performance of neural ODEs on graph networks.  However, the reviewers disagreed about the motivations for the proposed modifications.  Specifically, it's not clear that neural ODEs provide a more advantageous parameterization in this setting than standard discrete networks.

It's also not clear at all why the authors are discussion graph neural networks in particular, as all of their proposed changes would apply to all types of network.

Another major problem I had with this paper was the assertion that the running the original system backwards leads to large numerical error.  This is a plausible claim, but it was never verified.  It's extremely easy to check (e.g. by comparing the reconstructed initial state at t0 with the true original state at t0, or by comparing gradients computed by different methods).  It's also not clear if the authors enforced the constraints on their dynamics function needed to ensure that a unique solution exists in the first place.",Paper Decision
awOI1TxL-,ryeFzT4YPr,"Lift-the-flap: what, where and when for context reasoning",Reject,"The authors present the task lift-the-flap where an agent (artificial or human) is presented with a blurred image and a hidden item. The agent can de-blur the parts of the image by clicking on it. The authors introduce a model for this task (ClickNet) and they compare this against others.
As reviewers point, this paper presents an interesting set of experiments and analyses. Overall, this type of work can be quite influential as it gives an alternative way to improve our models by unveiling human strategies and using those as inductive biases for our models. That being said, I find the conclusions of this paper quite narrow for the general audience of ICLR (as R2 and R3 also point), as authors look into an artificial task and show ClickNet performs well. But what have we learned beyond that? How do we use these results to improve either our models or our understanding of these models? I believe these are the type of questions that  are missing from the current version of the paper and that if answered would greatly increase its impact and relevance to the ICLR community. At the moment though, I cannot recommend this paper for acceptance.
",Paper Decision
FaQTLflCSN,HketzTNYwS,"Unifying Question Answering, Text Classification, and Regression via Span Extraction",Reject,"(I acknowledge reading authors' recent note on decaNLP.)

This paper proposes a span extraction approach (SpExBERT) to unify question answering, text classification and regression. Paper includes a significant number of experiments (including low-resource and multi-tasking experiments) on multiple benchmarks. The reviewers are concerned about lack of support on author's claims from the experimental results due to seemingly insignificant improvements and lack of analysis regarding the results. Hence, I suggest rejecting the paper.",Paper Decision
mT6r2axivF,Syx_f6EFPr,Supervised learning with incomplete data via sparse representations,Reject,"This was a difficult paper to decide, given the strong disagreement between reviewer assessments.  After the discussion it became clear that the paper tackles some well studied issues while neglecting to cite some relevant works.  The significance and novelty of the contribution was directly challenged, yet I could not see a convincing case presented to mitigate these criticisms.  The paper needs to do a better job of placing the work in the context of the existing literature, and establishing the significance and novelty of its main contributions.",Paper Decision
a7msfpKvM4,rkl_f6EFPS,The Probabilistic Fault Tolerance of Neural Networks in the Continuous Limit,Reject," This paper focuses on the problem of robustness in the network with random loss of neurons.  However, reviewers had issues with insufficient clarity of the presentation, and lack of discussion about closely related dropout approach.

 
 ",Paper Decision
W8a02AliI7,rylDzTEKwr,Variational Hashing-based Collaborative Filtering with Self-Masking,Reject,There was a clear consensus amongst reviewers that the paper should not be accepted. This view was not changed by the rebuttal. Thus the paper is rejected. ,Paper Decision
tL5FEgnkZr,B1evfa4tPB,Neural Network Branching for Neural Network Verification ,Accept (Talk),"The authors develop a strategy to learn branching strategies for branch-and-bound based neural network verification algorithms, based on GNNs that imitate strong branching. This allows the authors to obtain significant speedups in branch and bound based neural network verification algorithms relative to strong baselines considered in prior work.

The reviewers were in consensus and the quality of the paper and minor concerns raised in the initial reviews were adequately addressed in the rebuttal phase. 

Therefore, I strongly recommend acceptance.",Paper Decision
3uhJ1KKgm,SylUzpNFDS,SoftLoc: Robust Temporal Localization under Label Misalignment,Reject,"Main content:

Blind review #3 summarizes it well:

This paper proposes a new loss for training models that predict where events occur in a sequence when the training sequence has noisy labels. The central idea is to smooth the label sequence and prediction sequence and compare these rather than to force the model to treat all errors as equally serious.

The proposed problem seems sensible, and the method is a reasonable approach. The evaluations are carried out on a variety of different tasks (piano onset detection, drum detection, smoking detection, video action segmentation).

--

Discussion:

The reviewers were concerned about the relatively low level of novelty, simplicity of the proposed approach (which the authors argue could be seen as a feature rather than a flaw, given its good performance), and inadequate motivation.

--

Recommendation and justification:

After the authors' revision in response to the reviews, this paper could be a weak accept if not for the large number of stronger submissions.",Paper Decision
GF4_KfLKNf,rJgUfTEYvH,VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation,Accept (Poster),"The authors explore the use of flow-based models for video prediction. The idea is interesting. The paper is well-written. It is a good paper worthwhile presenting in ICLR.

For final version, we suggest that the authors can significantly improve the experiments: (1) report results on human motion datasets; (2) include the results by the FVD metric. 
",Paper Decision
ZstkImK21,Skerzp4KPS,Adaptive Data Augmentation with Deep Parallel Generative Models,Reject,"This paper proposes a data augmentation method based on Generative Adversarial Networks by training several GANs on subsets of the data which are then used to synthesise new training examples in proportion to their estimated quality as measured by the Inception Score. The reviewers have raised several critical issues with the work, including motivation (it can be harder to train a generative model than a discriminative one), novelty, complexity of the proposed method, and lack of comparison to existing methods. Perhaps the most important one is the inadequate empirical evaluation. The authors didn’t address any of the raised concerns in the rebuttal. I will hence recommend the rejection of this paper.",Paper Decision
-KLrF3so0p,SJxHMaEtwB,Domain-invariant Learning using Adaptive Filter Decomposition,Reject,"The paper proposes a domain-adaptive filter decomposition method via separating domain-specific and cross-domain features, towards learning invariant representations for unsupervised domain adaptation.

Overall, this well-written paper is well motivated with a better technique for learning invariant representations using convolutional filters. Nonetheless, reviewers still have major concerns: 1) the novelty of the paper may be marginal given the significant line of recent work on learning domain-invariant representations; 2) when the label distributions differ, learning invariant representations can only lead to worse target generalizations; 3) the provided theory has an unclear connection to the presented filter decomposition method. The paper can be strengthened by further discussions on how to mitigate the aforementioned negative results. 

Hence I recommend rejection.",Paper Decision
NZw-7ayVJz,HJgEMpVFwB,Adversarial Policies: Attacking Deep Reinforcement Learning,Accept (Poster),"This paper demonstrates that for deep RL problems one can construct adversarial examples where the examples don't really need to be even better than the best opponent. Surprisingly, sometimes, the adversarial opponent is less capable than normal opponents which the victim plays successfully against, yet they can disrupt the policies. The authors present a physically realistic threat model and demonstrate that  adversarial policies can exist in this threat
model.

The reviewers agree with this paper presents results (proof of concept) that is ""timely"" and the RL community will benefit from this result. Based on reviewers comment, I recommend to accept this paper. ",Paper Decision
LN2Ogebpqn,rkeNfp4tPr,Escaping Saddle Points Faster with Stochastic Momentum,Accept (Poster),"This paper studies the impact of using momentum to escape saddle points. They show that a heavy use of momentum improves the convergence rate to second order stationary points. The reviewers agreed that this type of analysis is interesting and helps understand the benefits of this standard method in deep learning. The authors were able to address most of the concerns of the reviewers during rebutal, but is borderline due to lingering concerns about the presentation of the results. We encourage the authors to give more thought to the presentation before publication.",Paper Decision
PqqcEiCKne,H1emfT4twB,Few-shot Text Classification with Distributional Signatures,Accept (Poster),"This paper proposes a meta-learning approach for few-shot text classification. The main idea is to use an attention mechanism over the distributional signatures of the inputs to weight word importance. Experiments on text classification datasets show that the proposed method improves over baselines in 1-shot and 5-shot settings.

The paper addresses an important problem of learning from a few labeled examples. The proposed approach makes sense and the results clearly show the strength of the proposed approach.

R1 had some questions regarding the proposed method and experimental details. I believe this have been addressed by the authors in their rebuttal.

R2 suggested that the authors clarified their experimental setup with respect to prior work and improved the clarity of their paper. The authors have made some adjustments based on this feedback, including adding new sections in the appendix.

R3 had concerns regarding the contribution of the approach and whether it trades variance for bias. The authors have addressed most of these concerns and R3 has updated their review accordingly.

I think all the reviewers gave valuable feedbacks that have been incorporated by the authors to improve their paper. While the overall scores remain low, I believe that they would have been increased had R1 and R2 reassessed the revised submission. I recommend to accept this paper.
",Paper Decision
efyk8XdIiW,r1e7M6VYwH,RotationOut as a Regularization Method for Neural Network,Reject,"All of the reviewers agree the paper has an interesting idea (using rotations of the representation as regularization). However, the reviewers also agree the empirical gains are too insignificant. While the paper shows results on CIFAR, the reviewers mentioned a few other ways to improve performance, such as more complex and unconstrained datasets. These additional experiments would make the effectiveness of proposed approach more convincing.",Paper Decision
a09IwQo5a,B1xGGTEtDH,Universal Approximation with Deep Narrow Networks,Reject,"This article studies universal approximation with deep narrow networks, targeting the minimum width. The central contribution is described as providing results for general activation functions. The technique is described as straightforward, but robust enough to handle a variety of activation functions. The reviewers found the method elegant. The most positive position was that the article develops non trivial techniques that extend existing universal approximation results for deep narrow networks to essentially all activation functions. However, the reviewers also expressed reservations mentioning that the results could be on the incremental side, with derivations similar to previous works, and possibly of limited interest. In all, the article makes a reasonable theoretical contribution to the analysis of deep narrow neural networks. Although this is a reasonably good article, it is not good enough, given the very high acceptance bar for this year's ICLR. 
",Paper Decision
mW5OiXM6M8,SJefGpEtDB,A Dynamic Approach to Accelerate Deep Learning Training,Reject,"The submission proposes a dynamic approach to training a neural net which switches between half and full-precision operations while maintaining the same classifier accuracy, resulting in a speed up in training time. Empirical results show the value of the approach, and the authors have added additional sensitivity analysis by sweeping over hyperparameters. 

The reviewers were concerned about the novelty of the approach as well as the robustness of the claims that accuracy can be maintained even in the accelerated, dynamic regime. After discussion there were still concerns about the sensitivity analysis and the significance of the results.

The recommendation is to reject the paper at this time.",Paper Decision
rkRJ3ATVJ,SJezGp4YPr,Geometric Insights into the Convergence of Nonlinear TD Learning,Accept (Poster),"This paper takes steps towards a theory of convergence for TD(0) with non-linear function approximation.  The paper provides two theoretical results.  One result bounds the error when training the sum of linear and homogenous parameterized functions.  The second result shows global convergence when the environment dynamics are sufficiently reversible  and the differentiable function approximation is sufficiently well-conditioned.  The paper provides additional insight using a family of environments with partially reversible dynamics.

The reviewers commented on several aspects of this work.  The reviewers wrote that the presentation was clear and that the topic was relevant.  The reviewers were satisfied with the correctness of the results.  The reviewers liked the result that state value function estimation error is bounded when using homogeneous functions. They also noted that the deep networks in common use are not homogeneous so this result does not apply directly. The result showing global convergence of TD(0) with partial reversibility was also appreciated. Finally, the reviewers liked the family of examples.

This paper is acceptable for publication as the presentation was clear, the results are solid, and the research direction could lead to additional insights.",Paper Decision
y4WyRChjEI,H1lWzpNKvr,Efficient Multivariate Bandit Algorithm with Path Planning,Reject,"This paper tackles the multivariate bandit problem (akin to a factorial experiment) where the player faces a sequence of decisions (that can be viewed as a tree) before obtaining a reward. The authors introduce a framework combining Thompson Sampling with path planning in trees/graphs. More specifically, they consider four path planning strategies, leading to four approaches. The resulting approaches are empirically evaluated on synthetic settings.

Unfortunately, the proposed approaches lack theoretical justification and the current experiments are not strong enough to support the claims made in the paper. Given that most of reviewer's concerns remained valid after rebuttal, I recommend to reject this paper.",Paper Decision
5qyN6HfWO1,rke-f6NKvS,Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling,Accept (Poster),"The paper introduces Value Iteration with Negative Sampling (VINS) algorithm as a method to accelerate RL using expert demonstrations. VINS learns an initial value function that has a smaller value at states not encounter during the demonstrations.

The reviewers raised several issues regarding the assumptions, theoretical results, and experiments. The method seems to be most natural for robotic control problems. Nonetheless, it seems that the rebuttal addressed most of the concerns, and two of the reviewers increased their scores accordingly. Since we have three Weak Accepts, I believe this paper can be accepted at the conference.",Paper Decision
_f3lJJ7KUU,H1exf64KwH,Exploring Model-based Planning with Policy Networks,Accept (Poster),"This paper proposes a model-based policy optimization approach that uses both a policy and model to plan online at test time. The paper includes significant contributions and strong results in comparison to a number of prior works, and is quite relevant to the ICLR community. There are a couple of related works that are missing [1,2] that combine learned policies and learned models, but generally the discussion of prior work is thorough. Overall, the paper is clearly above the bar for acceptance.

[1] https://arxiv.org/pdf/1703.04070.pdf
[2] https://arxiv.org/pdf/1904.05538.pdf",Paper Decision
sXBtLuT-g6,H1lefTEKDS,Benchmarking Model-Based Reinforcement Learning,Reject,"This paper provide an extensive set of benchmarks for Deep Model-based RL algorithms.

This paper contains a large number of algorithms, environments, and empirical results. The reviewers all recognized the need for such a study to provide some clarity, insights, and common standards. The reviewers we concerned about several aspects of the implementation of the effort. (1) All the performance is based on 4 runs, smoothed curves, and default errors (often extensively overlapping). The paper cites Henderson et al, and yet does not follow the advice laid out therein. (2) The results were fairly inconclusive---perhaps to be expected---we didn't learn much (more on this below). (3) The paper has communication issues.

The overall approach taken was a bit perplexing. Some algorithms we given access to the dynamics. The reward functions were converted to diff. forms, and early stopping in a domain specific way was employed. This all seems like simplifying the problem in different ways so that some methods can be competitive, but it is not at all clear why. If we take the typical full rl problem and limit domain knowledge, many of these approaches cannot be applied and others will fail. Those are the results we want. One could actually view these choices are unfair to more general algorithms---algorithms that need diff rewards pay no price for this assumption. This also leads to funny things, for example, like using position as the reward in mountain car (totally non-standard, and invalid without discounting). The paper claims a method can solve MC, but that is unclear from the graph. The paper motivates the entire enterprise based on the claimed lack of standardization in the literature, but then proceeds to redefine classic control tasks with little discussion or explanation. 

The paper has communication issues. For example, all the domains are use continuous actions (and the others in the response highlight that is their main focus), but this is never stated in the paper. The paper refers to and varies ""environment length"", but this was not defined in the paper and has no obvious meaning. The tasks are presumably discounted but the the value of gamma is not specified anywhere in the paper (could be there, but I searched for a while). Pages of parameter settings in the appendix with many not discussed or their ranges justified.

This paper is ambitious, but I urge the authors to perhaps limit the scope and do less, and consider a slightly broader audience in both the writing and experiment design.   
",Paper Decision
Ki4tlY21qy,SylkzaEYPS,Encoder-decoder Network as Loss Function for Summarization,Reject,"This paper presents an encoder-decoder based architecture to generate summaries. The real contribution of the paper is to use  a recoder matrix which takes the output from an existing encoder-decoder network and tries to generate the reference summary again. The output here is basically the softmax layer produced by the first encoder-decoder network which then goes through a feed-forward layer before being fed as embeddings into the recoder. So, since there is no discretization, the whole model can be trained jointly. (the original loss of the first encoder-decoder model is used as well anyway).

I agree with the reviewers here, that this whole model can in fact be viewed as a large encoder-decoder model, its not really clear where the improvements come from. Can you just increase the number of parameters of the original encoder-decoder model and see if it performs as good as the encoder-decoder + recoder? The paper also does not achieve SOTA on the task as there are other RL based papers which have been shown to perform better, so the choice of the recorder model is also not empirically justified. I recommend rejection of the paper in its current form.",Paper Decision
h3k9-sCef,BJg1f6EFDB,On Identifiability in Transformers,Accept (Poster),"This paper investigates the identifiability of attention distributions in the context of Transformer architectures. The main result is that, if the sentence length is long enough, difference choices of attention weights may result in the same contextual embeddings (i.e. the attention weights are not identifiable). A notion of ""effective attention"" is proposed that projects out the null space from attention weights. 

In the discussion period, there were some doubts about the technical correctness of the identifiability result that were clarified by the authors. The attention matrix A results from a softmax transformation, therefore each of its rows is constrained to be in the probability simplex -- i.e. we have A >= 0 (elementwise) and A1 = 1. In the present version of the paper, when analyzing the null space of T (Eqs. 4 and 5) this constraint on A is not taken into account. In particular, in Eq. 5 the existence of a \tilde{A} in the null space of T is not clear at all, since for (A + \tilde{A})T = AT to hold we would need to require, besides A >= 0 and A1 = 1, that A + \tilde{A} >= 0 and A1 + \tilde{A}1 = 1, i.e.

\tilde{A} <= A (elementwise)
\tilde{A}1 = 0

The present version of the paper does not make it clear that the intersection of the null space of T with these two constraints is non-empty in general -- which would be necessary for attention not to be identifiable, one of the main points of the paper. 

The authors acknowledged this concern and provided a proof. I suggest the following simplified version of their proof:

We're looking for a vector \tilde{A} satisfying

(1) \tilde{A}’*T = 0 (to be in the null space of T)

and 

(2) \tilde{A}’*1 = 0 
(3) \tilde{A} >= -A

(to make sure A + \tilde{A} are in the probability simplex).

Conditions (1) and (2) are equivalent to require \tilde{A} to be in the null space of [T; 1]. It is fine to assume this null space exists for a general T (it will be a linear subspace of dimension ds - dv - 1). 

To take into account condition (3) here’s a simpler proof: since A is a probability vector coming from a softmax transformation (hence it is strictly > 0 elementwise), there is some epsilon > 0 such that any point in the ball centered on 0 with radius epsilon is >= -A.
 
Since the null space of [T; 1] contains 0, any point \tilde{A} in the intersection of this null space with the epsilon-ball above satisfies (1), (2), and (3). This should work for any ds - dv > 1  and as long as A is not a one-hot distribution (otherwise it collapses to a single point \tilde{A} = 0).

I am less convinced about the justification to use an “effective attention” which is not in the probability simplex, though (not even in the null space of [T; 1] but only null(T)). That part deserves more clarification in the paper. 

I recommend acceptance of this paper provided these clarifications are provided and the proof is included in the final version. 

",Paper Decision
ZBoyu95zf7,H1e0Wp4KvH,Automated curriculum generation through setter-solver interactions,Accept (Poster),"The authors introduce a method to automatically generate a learning curriculum (of goals) in a sparse reward RL setting, examining several criteria for goal setting to induce a useful curriculum.  The reviewers agreed that this was an exciting research direction but also had concerns about baseline comparisons, clarity of some technical points, hyperparameter tuning (and the effect on the strength of empirical results), and computational tractability.  After discussion, the reviewers felt most of these points were sufficiently addressed.  Thus, I recommend acceptance at this time.",Paper Decision
EcPd-K0HU,SkgRW64twr,Deep Multi-View Learning via Task-Optimal CCA,Reject,"The main contribution of this paper is the training of a supervised model jointly with deep CCA for improving the representations learned in a setting where the training data is multi-view.  The claimed technical contribution is modifications to deep CCA to enable it to play nicely with the minibatch gradient-based training used for the supervised loss.  Pros:  This is an important problem with many applications.  Cons:  The novelty is minimal.  Some previous work has done joint training of supervised models with CCA, and some has addressed training deep CCA in a stochastic setting.  The reviewers (and I) are unconvinced that the differences from previous work are sufficient, and the paper does not carefully compare with the previous work.  The contribution to the tasks may be quite significant, however, so the paper may fit in well in an application-oriented conference/journal.",Paper Decision
HxS2JDJ94a,Skx6WaEYPH,Bandlimiting Neural Networks Against Adversarial Attacks,Reject,The reviewers recommend rejection due to various concerns about novelty and experimental validation. The authors have not provided a response.,Paper Decision
rZIA_JamId,BkepbpNFwr,Progressive Memory Banks for Incremental Domain Adaptation,Accept (Poster),"This paper introduces an RNN based approach to incremental domain adaptation in natural language processing, where the RNN is progressively augmented with the parameterized memory bank which is shown to be better than expanding the RNN states.

Reviewers and AC acknowledge that this paper is well written with interesting ideas and practical value. Domain adaptation in the incremental setting, where domains come in a streaming way with only the current one accessible, can find some realistic application scenarios. The proposed extensible attention mechanism is solid and works well on several NLP tasks. Several concerns were raised by the reviewers regarding the comparative and ablation studies, which were well resolved in the rebuttal. The authors are encouraged to generalize their approach to other application domains other than NLP to show the generality of their approach.

I recommend acceptance.",Paper Decision
VbKOJXEoeR,HJxhWa4KDr,MMD GAN with Random-Forest Kernels,Reject,"Reviewers raise the serious issue that the proof of Theorem 2 is plagiarized from Theorem 1 of ""Demystifying MMD GANs"" (https://arxiv.org/abs/1801.01401). With no response from the authors, this is a clear reject.
",Paper Decision
XhTgmuod40,B1l2bp4YwS,What graph neural networks cannot learn: depth vs width,Accept (Poster),"This paper provides a theoretical background for the expressive power of graph convolutional networks. The results are obviously useful, and the discussion went in the positive way. All reviewers recommend accepting, and I am with them.",Paper Decision
zoILtdvbw,B1gn-pEKwH,"INFERENCE, PREDICTION, AND ENTROPY RATE OF CONTINUOUS-TIME, DISCRETE-EVENT PROCESSES",Reject,"The authors present a Bayesian model for time series which are represented as discrete events in continuous time and describe methods for doing parameter inference, future event prediction and entropy rate estimation for such processes. 
However, the reviewers find that the novelty of the paper is not high enough, and without sufficient acknowledgement and comparison to existing literature.",Paper Decision
1iFupR0wyE,SJgob6NKvH,RTFM: Generalising to New Environment Dynamics via Reading,Accept (Poster),"This paper proposes RTFM, a new model in the field of language-conditioned policy learning. This approach is promising and important in reinforcement learning because of the difficulty to learn policies in new environments. 

Reviewers appreciate the importance of the problem and the effective approach. After the author response which addressed some of the major concerns, reviewers feel more positive about the paper. They comment, though, that presentation could be clearer, and the limitations of using synthetic data should be discussed in depth.

I thank the authors for submitting this paper.",Paper Decision
v1HhRCE7j,BJl9ZTVKwB,MIM: Mutual Information Machine,Reject,"The paper proposes an autoencoder framework for learning joint distributions over observations and latent states. The reviewers expressed concerns regarding the motivation for this work, the presentation with respect to prior work, and unconvincing experiments. In its current form the paper is not ready for acceptance to ICLR-2020.",Paper Decision
rJ--IC4fNe,rkgKW64FPH,Constant Time Graph Neural Networks,Reject,"There was some interest in the ideas presented, but this paper was on the borderline and ultimately not able to be accepted for publication at ICLR.

The primary reviewer concern was about the level of novelty and significance of the contribution. This was not sufficiently demonstrated.",Paper Decision
eV9ov5k4WI,SkgtbaVYvH,AutoLR: A Method for Automatic Tuning of Learning Rate,Reject,"The authors propose a method for automatic tuning of learning rates. The reviewers liked the idea but felt that there are much more extensive experiments to be done especially better baselines. Also, clarifying what aspect is automated is important, because no method can be truly automatic: they all have some hyperparameters. ",Paper Decision
2SzUVrg5Y-,HJgFW6EKvH,Generating Robust Audio Adversarial Examples using Iterative Proportional Clipping,Reject,"This paper proposes a method called iterative proportional clipping (IPC) for generating adversarial audio examples that are imperceptible to humans. The efficiency of the method is demonstrated by generating adversarial examples to attack the Wav2letter+ model. Overall, the reviewers found the work interesting, but somewhat incremental and analysis of the method and generated samples incomplete, and I’m thus recommending rejection.",Paper Decision
DPNAoVqto,rkxuWaVYDB,Optimal Attacks on Reinforcement Learning Policies,Reject,"This paper studies the problem of devising optimal attacks in deep RL to minimize the main agent average reward. In the white-box attack setting, optimal attacks amounts to solving a Markov Decision Process, while in black-box attacks, optimal attacks can be trained using RL techniques. Empirical efficiency of the attacks was demonstrated. It has valuable contributions on studying the adversarial robustness on deep RL. However, the current motivation and setup needs to be made clearer, and so is not being accepted at this time. We hope for these comments to help improve a future version.",Paper Decision
dQD823QA3w,B1ldb6NKDr,Multi-Agent Hierarchical Reinforcement Learning for Humanoid Navigation,Reject,"This paper presents an approach combining multi-agent with hierarchical RL in a custom-made simulated humanoid robotics setting.

Although it is an interesting premise and has a compelling motivation (multi-agent, real-world interaction, humanoid robotics), the reviewers had some trouble pinpointing what the significant contributions are. Partly this is due to lack of clarity in the presentation, such as with overlong sections (eg 5.2), unclear descriptions, mistakes in the text, etc. Reviewers also remarked that this paper might be trying to do too much, without performing the necessary experiments/comparisons and analyses needed to interpret the contributions of each component. 

This work is definitely promising and has the potential to make a nice contribution, given some additional care (experiments, analyses) and rewriting/polishing. As it is, it’s probably a bit premature for publication at ICLR.
",Paper Decision
ywTg9HO2d,H1lDbaVYvH,SMiRL: Surprise Minimizing RL in Entropic Environments,Reject,"This paper proposes augmentation of the state exploration strategy that is interesting and has a potential to lead to improvement. However, the current presentation makes it difficult to properly assess that. In particular, the way the authors convey both the underlying intuition and its implementation is fairly vague and does not build confidence in the grounding of the underlying methodology.",Paper Decision
YRkUmkR57L,rklv-a4tDB,Mesh-Free Unsupervised Learning-Based PDE Solver of Forward and Inverse problems,Reject,"This paper proposes modifying the training loss for neural net-based PDE solvers, by adding an L_infty (max) term to the standard L_2 loss.  The motivation for this loss is sensible in that it matches the definition of a strong solution, but this is only a heuristic motivation, and is missing a theoretical analysis.

This paper's lack of novelty and polish, as well as the lack of clarity in the implementation details, makes this a narrow reject.",Paper Decision
dBFcz2-DU_,SyxIWpVYvr,Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models,Accept (Poster),"I have read the paper and the reviews carefully. Despite the numerical scores, I think this paper is above the bar for ICLR, and recommend acceptance.

This paper addresses the now-well-known problem that generative models often assign higher likelihoods to out-of-distribution examples, rendering likelihoods useless for OOD detection. They diagnose this as resulting from differences in compressibility of the input, and propose to compensate for this by comparing the log-likelihood to the description length from a strong image compressor. They show this performs well against a variety of OOD detection methods.

The idea is a natural one, and certainly should have been one of the first things tried in addressing this phenomenon. I'm a little surprised it hasn't been done before, but none of the reviewers or I are aware of a prior reference, so AFAIK it's novel. One reviewer believes the contribution is small; while it's simple, I think the field will benefit from a careful implementation and testing of this approach.

Multiple reviewers raise the concern of whether generative models' bias towards low-complexity inputs is just a matter of needing better generative models. I don't think so: even arbitrarily good generative models will still be limited by the inherent compressibility of an input (e.g. as measured by Kolmogorov complexity).

I'm also not concerned about the lack of an explicit threshold; if one has proposed a good score function, there are many ways one could choose a threshold, depending on the task.
",Paper Decision
5klWZJpK_9,r1e8WTEYPB,Sparse and Structured Visual Attention,Reject,"This paper presents sparse attention mechanisms for image captioning. In addition to recent sparsemax based method, authors proposed to extend it by incorporating structural constraints in 2D images, which is called TVMAX. The proposed methods are shown to improve the quality of captioning, particularly in terms of fewer erroneous repetitions, and obtain better human evaluation scores. 
Through reviewer discussion, one reviewer updated the score to rejection. A major concern raised by the reviewers is that the motivation of introducing sparse attention is not clear, and the reason why it improves the quality (particularly, why it can reduce repetition) is not convincing. While we understand it is plausible for long sequences as in text domain, we are not convinced that it is really necessary for image captioning problems. Although authors seem to have some ideas, we cannot see how they will be reflected in the paper so I’d like to recommend rejection.
I recommend authors to polish the paper with a clearer description of the motivation and high-level analysis of the method as well as testing on other visual tasks to show its generality. 
",Paper Decision
EleP33_Rrd,H1x8b6EtvH,Network Pruning for Low-Rank Binary Index,Reject,"The submission proposes a method to improve over a standard binary network pruning strategy by the inclusion of a structured matrix product to encourage network weight sparsification that can have better memory and computational properties.  The idea is well motivated, but there were reviewer concerns about the quality of writing and in particular the quality of the experiments.  The reviewers were unanimous that the paper is not suitable for acceptance at ICLR, and no rebuttal was provided.",Paper Decision
tfZ5ZZLFB,rkgrbTNtDr,Style-based Encoder Pre-training for Multi-modal Image Synthesis,Reject,"The submission describes a new two-stage training scheme for multi-modal image-to-image translation. The new scheme is compared to a single-stage end-to-end baseline, and the advantage of the new scheme is demonstrated empirically. All three reviewers appreciate the proposed contribution and the quality improvement it brings over the baseline. At the same time, the reviewers see the contribution as incremental and not sufficient for an ICLR paper. The author response and paper adjustment have not changed the opinion of the reviewers, so the overall recommendation is to reject.",Paper Decision
pJNEw8WpXG,HygHbTVYPB,LDMGAN: Reducing Mode Collapse in GANs with Latent Distribution Matching,Reject,"This paper proposes to mitigate mode collapse in GANs by encouraging distribution matching in the latent space. Reviewers 1 and 3 expressed concerns that the methodology is too incremental in the context of the existing literature (VEEGAN, VAE-GAN, AAE). This, combined with the lack of up-to-date baselines, makes it difficult to access the significance of the proposed modifications. The quality and precision of the writing can also be improved to meet the standards of publication at a top-tier conference. 
",Paper Decision
d6_sdLCIL,Hye4WaVYwr,Bootstrapping the Expressivity with Model-based Planning,Reject,"The paper provides some insight why model-based RL might be more efficient than model-free methods. It provides an example that even though the dynamics is simple, the value function is quite complicated (it is in a fractal). Even though the particular example might be novel and the construction interesting, this relation between dynamics and value function is not surprising, and perhaps part of the folklore. The paper also suggests a model-based RL methods and provides some empirical results.

The reviewers find the paper interesting, but they expressed several concerns about the relevance of the particular example, the relation of the theory to empirical results, etc. The authors provided a rebuttal, but the reviewers were not convinced. Given that we have two Weak Rejects and the reviewer who is Weak Accept is not completely convinced, unfortunately I can only recommend rejection of this paper at this stage.",Paper Decision
5hSHNqUQkA,ryl4-pEKvB,DeepAGREL: Biologically plausible deep learning via direct reinforcement,Reject,"The paper proposes an RL-based algorithm for training neural networks that is able to match the performance of backprop on CIFAR and MNIST datasets.

The reviewers generally found the algorithm and motivations interesting, but some had issue with the imprecision of the notion of ""biologically plausible"" used by the authors. One reviewer had issues with missing discussion of related work and also doubts about the meaningfulness of the experiments, since the networks were quite shallow.

For this type of paper, clarity and precision of exposition is crucial in my opinion, and so I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to a future venue.",Paper Decision
EmbcEIvTsg,ByxXZpVtPB,Homogeneous Linear Inequality Constraints for Neural Network Activations,Reject,"The authors propose a framework for incorporating homogeneous linear inequality constraints on neural network activations into neural network architectures. The authors show that this enables training neural networks that are guaranteed to satisfy non-trivial constraints on the neurons in a manner that is significantly more scalable than prior work, and demonstrate this experimentally on a generative modelling task.

The problem considered in the paper is certainly significant (training neural networks that are guaranteed to satisfy constraints arises in many applications) and the authors make some interesting contributions. However, the reviewers found the following issues that make it difficult to accept the paper in its present form:
1) The setting of homogeneous linear equality constraints is not well-motivated and the significance of being able to impose such constraints is not clearly articulated in the paper. The authors would do well to prepare a future revision documenting use-cases motivated by practical applications and add these to the paper.
2) The experimental evaluation is not sufficiently thorough: the authors evaluate their method on an artificial constraint involving a ""checkerboard pattern"" on MNIST. Even in this case, the training method proposed by the authors seems to suffer from some issues, and more thorough experiments need to be conducted to confirm that the training method can perform well across a variety of datasets and constraints.

Given these issues, I recommend rejection. However, I encourage the authors to revise their work on this important topic and prepare a future version including practical examples of the constraints and experiments on a variety of prediction tasks.

",Paper Decision
4nHjf-733z,HyxQbaEYPr,Leveraging Simple Model Predictions for Enhancing its Performance,Reject,The authors propose a sample reweighting scheme that helps to learn a simple model with similar performance as a more complex one. The authors contained critical errors in their original submission and the paper seems to lack in terms of originality and novelty of the proposed method.,Paper Decision
AL1xoecrHP,B1gm-a4tDH,Modeling treatment events in disease progression,Reject,The proposed method has very weak novelty.,Paper Decision
8lGbrm6Ry,ryxMW6EtPB,DG-GAN: the GAN with the duality gap,Reject,"This paper proposes looking at the duality gap to measure performance. However, the metric is just an upperbound on the true metric of interest, and therefore its value can be ambiguous. 

The reviewers found the paper to be in an unacceptable form and was clearly hastily prepared. They were also skeptical about the novelty of the result as well as the comprehensiveness of the experiments.

This paper would require extensive revisions before any potential acceptance. Reject   ",Paper Decision
SZhSCtceLk,rygMWT4twS,Stochastic Gradient Descent with Biased but Consistent Gradient Estimators,Reject,"This paper analyzes the convergence of SGD with a biased yet consistent gradient estimator. The main result is that this biased estimator results in the same convergence rate as does using unbiased ones. The main application is on learning representations on graphs (e.g., GCNs), and FastGCN is a closely related work. I agree that this paper has valuable contributions, but it can be further strengthened by considering the review comments, such as on the key assumptions.",Paper Decision
52iBqqdy3,BJgWbpEtPr,One-way prototypical networks,Reject,"This paper extends prototypical networks to few shot 1-way classification. The idea is to introduce a null class to compare against with a null prototype. The reviewers found the idea sound and interesting. However, the response was mixed because the reviewers were not convinced of the significance of the improvements. Furthermore, there were questions raised about the motivation that were not sufficiently addressed in the rebuttal. Batch normalization layers will not necessarily lead to zero mean if the trainable offset is not disabled. The authors did not clarify whether they disable this offset. I encourage the authors to resubmit after addressing the issues raised by the reviewers.",Paper Decision
y6h5v6TsHq,Hke-WTVtwr,Encoding word order in complex embeddings,Accept (Spotlight),"This paper describes a new language model that captures both the position of words, and their order relationships.  This redefines word embeddings (previously thought of as fixed and independent vectors) to be functions of position.  This idea is implemented in several models (CNN, RNN and Transformer NNs) to show improvements on multiple tasks and datasets.

One reviewer asked for additional experiments, which the authors provided, and which still supported their methodology.   In the end, the reviewers agreed this paper should be accepted.",Paper Decision
1_TCjkGw6k,Bylx-TNKvH,Functional vs. parametric equivalence of ReLU networks,Accept (Poster),"This work proves that the weights of feed-forward ReLU networks are determined, up to a specified set of symmetries, by the functions they define. Reviewers found the paper easy to read and the proof technically sound. There was some debate over the motivation for the paper, Reviewer 1 argues that there is no practical significance for the result, a point that the authors do not deny. I appreciate the concerns raised by Reviewer 1, theorists in machine learning should think carefully about the motivation for their work. However, while there is no clear practical significance of this work, I believe there is value to accepting it. Because the considered question concerns a sufficiently fundamental property of neural networks, and the proof is both easy to read and provides insights into a well studied class of models, I believe many researchers will find value in reading this paper.",Paper Decision
bPP9sDslEY,rkx1b64Fvr,A New Multi-input Model with the Attention Mechanism for Text Classification,Reject,"This paper proposes a CNN-based text classification model that uses words, characters, and labels as its input. It also presents an attention block to replace the pooling operation that is typically used in a CNN. The proposed method is evaluated on six benchmark classification datasets, achieving reasonably good results.

While the proposed method performs reasonably well compared to baselines in the papers, all reviewers pointed out that there is no discussion or comparison with existing SotA based on pretrained models (e.g., BERT, XLNet), which would strengthen the main claim of the paper. All three reviewers also suggested that the writing of the paper could be improved. The authors did not respond to these reviews, so there was little discussion needed to arrive at a consensus.

I agree with all reviewers and recommend to reject the paper.",Paper Decision
f3j3SVgMBm,B1lyZpEYvH,Multi-Dimensional Explanation of Reviews,Reject,"This paper proposes a neural network model for predicting multi-aspect sentiment and generating masks that can justify the predictions. The positive aspects of the paper include improved results over the state-of-the-art.

Reviewers found the technical novelty limited, and the experiments short of being fully convincing. After the author rebuttal, there were discussions between the reviewers and the AC, and the reviewers still thought the paper is not fully convincing given these limitations.

I thank the authors for their submission and detailed responses to the reviewers and hope to see this research in a future venue.",Paper Decision
2X4hzmAJk6,Skek-TVYvr,A Uniform Generalization Error Bound for Generative Adversarial Networks,Reject,"The authors received reviews from true experts and these experts felt the paper was not up to the standards of ICLR. 

Reviewer 3 and Reviewer 1 disagree as to whether the new notion of generalization error is appropriate. I think both cases can be defended. I think the authors should aim to sharpen their argument in this regard.Several reviewers at one point remark that the results follow from standard techniques: shouldn't this be the case? I believe the actual criticism being made is that the value of these new results do not go above and beyond existing ones. There is also the matter of what value should be attributed to technical developments on their own. On this matter, the reviewers seem to agree that the derivations lean heavily on prior work. ",Paper Decision
OZSvmQrcj,HkeAepVKDH,QGAN: Quantize Generative Adversarial Networks to Extreme low-bits,Reject,"main summary:  method for quantizing GAN

discussion:
reviewer 1: well-written paper, but reviewer questions novelty
reviewer 2: well-written, but some details are missing in the paper as well as comparisons to related work
reviewer 3: well-written and interesting topic, related work section and clarity of results could be improved
recommendation: all reviewers agree paper could be improved by better comparison to related work and better clarity of presentation. Marking paper as reject.",Paper Decision
06V352WC2Y,H1gax6VtDB,Contrastive Learning of Structured World Models,Accept (Talk),"This paper presents an approach to learn state representations of the scene as well as their action-conditioned transition model, applying contrastive learning on top of a graph neural network. The reviewers unanimously agree that this paper contains a solid research contribution and the authors' response to the reviews further clarified their concerns.",Paper Decision
dhLFEO9xv,SygagpEKwB,Disentangling Factors of Variations Using Few Labels,Accept (Poster),"This paper addresses the problem of learning disentangled representations and shows that the introduction of a few labels corresponding to the desired factors of variation can be used to increase the separation of the learned representation. 

There were mixed scores for this work. Two reviewers recommended weak acceptance while one reviewer recommended rejection. All reviewers and authors agreed that the main conclusion that the labeled factors of variation can be used to improved disentanglement is perhaps expected. However, reviewers 2 and 3 argue that this work presents extensive experimental evidence to support this claim which will be of value to the community. The main concerns of R1 center around a lack of clear analysis and synthesis of the large number of experiments. Though there is a page limit we encourage the authors to revise their manuscript with a specific focus on clarity and take-away messages from their results. 

After careful consideration of all reviewer comments and author rebuttals the AC recommends acceptance of this work. The potential contribution of the extensive experimental evidence warrants presentation at ICLR. However, again, we encourage the authors to consider ways to mitigate the concerns of R1 in their final manuscript. 
",Paper Decision
0nIsh9jka7,r1lnxTEYPS,Detecting Out-of-Distribution Inputs to Deep Generative Models Using Typicality,Reject,"This paper tackles the problem of detecting out of distribution (OoD) samples. To this end, the authors propose a new approach based on typical sets, i.e. sets of samples whose expected log likelihood approximate the model's entropy. The idea is then to rely on statistical testing using the empirical distribution of model likelihoods in order to determine whether samples lie in the typical set of the considered model. Experiments are provided where the proposed approach show competitive performance on MNIST and natural image tasks.

This work has major drawbacks: novelty, theoretical soundness, and robustness in settings with model misspecification. Using the typicality notion has already been explored in Choi. et al. 2019 (for flow-based model), which dampers the novelty of this work. The conditions under which the typicality notion can be used are also not clear, e.g. in the small data regime. Finally, the current experiments are lacking a characterization of robustness to model misspecification. Given these limitations, I recommend to reject this paper.
",Paper Decision
mADsHcnUaL,S1gnxaVFDB,EDUCE: Explaining model Decision through Unsupervised Concepts Extraction,Reject,"This paper introduces a method for building interpretable classifiers, along with a measure of ""concept accuracy"" to evaluate interpretability, and primarily applies this method to text models, but includes a proof of concept on images in the appendix.

The main contributions are sensible enough, but the main problems the reviewers had were:
A) The performance of the proposed method
B) The lack of human evaluation of interpretability, and 
C) Lack of background and connections to other work.

The authors improved the paper considerably during the rebuttal period, and might have addressed point C) satisfactorily, but only after several back and forths, and at this point it's too late to re-evaluate the paper.  I expect that a more polished version of this paper would be acceptable in a future conference.

I mostly ignored R1's review as they didn't seem to put much thought into their review and didn't respond to requests for clarifications.",Paper Decision
Zpxizc-jUm,B1esx6EYvr,"A critical analysis of self-supervision, or what we can learn from a single image",Accept (Poster),"This paper studies the effectiveness of self-supervised approaches by characterising how much information they can extract from a given dataset of images on a per-layer basis. Based on an empirical evaluation of RotNet, BiGAN, and DeepCluster, the authors argue that the early layers of CNNs can be effectively learned from a single image coupled with strong data augmentation. Secondly, the authors also provide some empirical evidence that supervision might still necessary to learn the deeper layers (even in the presence of millions of images for self-supervision). 
Overall, the reviews agree that the paper is well written and timely given the growing popularity of self-supervised methods. Given that most of the issues raised by the reviewers were adequately addressed in the rebuttal, I will recommend acceptance. We ask the authors to include additional experiments requested by the reviewers (they are valuable even if the conclusions are not perfectly aligned with the main message).
",Paper Decision
jOeOc1sGGH,r1gixp4FPH,Accelerating SGD with momentum for over-parameterized learning,Accept (Poster),"The authors provide an empirical and theoretical exploration of Nesterov momentum, particularly in the over-parametrized settings. Nesterov momentum has attracted great interest at various times in deep learning, but its properties and practical utility are not well understood. This paper makes an important step towards shedding some light on this approach for training models with a large number of parameters. ",Paper Decision
lsqKxT81X,Syx5eT4KDS,Discrete InfoMax Codes for Meta-Learning,Reject,"The reviewers were unanimous that this submission is not ready for publication at ICLR in its current form.

Concerns raised included that the method was not sufficiently general, including in choice of experiments reported, and the lack of discussion of some lines of significantly related work.",Paper Decision
A-GEi32Mm5,rJe9lpEFDH,The Geometry of Sign Gradient Descent,Reject,The paper is rejected based on unanimous reviews.,Paper Decision
04taF0PLu,r1ltgp4FwS,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,Reject,"The paper presents an architecture for conditional video generation tasks with temporal self-supervision and temporal adversarial learning. The proposed architecture is reasonable but looks somewhat complicated. In terms of technical novelty, the so-called ""ping-pong"" loss looks interesting and novel, but other parts are more-or-less some combinations of existing techniques. Experimental results show promise of the proposed method against selected baselines for video super-resolution (VSR) and unpaired video-to-video translation tasks (UVT). In terms of weakness, (1) the technical novelty is not very high; (2) the final loss is a combination of many losses with many hyperparameters; (3) experimentally the proposed method is not compared against recent SOTA methods on VSR and UVT. 

The proposed method should be compared against more recent SOTA baselines for VSR tasks (see examples of references below):

EDVR: Video Restoration with Enhanced Deformable Convolutional Networks
https://arxiv.org/abs/1905.02716

Progressive Fusion Video Super-Resolution Network via Exploiting Non-Local Spatio-Temporal Correlations
ICCV 2019

Recurrent Back-Projection Network for Video Super-Resolution
CVPR 2019

The same comment would apply for baselines for UVT tasks:

Mocycle-GAN: Unpaired Video-to-Video Translation
https://arxiv.org/abs/1908.09514

Preserving Semantic and Temporal Consistency for Unpaired Video-to-Video Translation
https://arxiv.org/abs/1908.07683

Particularly for UVT, the evaluated dataset seems limited in terms of scope as well (i.e., evaluations on more popular benchmarks, such as Viper would be needed for further validation). Overall, given that the contribution of this work is an empirical performance with a rather complex architecture/loss, more comprehensive empirical evaluations on SOTA baselines are warranted.
",Paper Decision
0CcdATXEpG,S1xFl64tDr,Interpretable Complex-Valued Neural Networks for Privacy Protection,Accept (Poster),"The reviewers are unanimous in their opinion that this paper offers a novel approach to secure edge learning.  I concur.  Reviewers mention clarity, but I find the latest paper clear enough.",Paper Decision
GwAGDmO8bO,SylOlp4FvH,V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control,Accept (Poster),"This paper proposes an extension of MPO for on-policy reinforcement learning. The proposed method achieved promising results in a relatively hyper-parameter insensitive manner.

One concern of the reviewers is the lack of comparison with previous works, such as original MPO, which has been partially addressed by the authors in rebuttal. In addition, Blind Review #3 has some concerns with the fairness of the experimental comparison, though other reviews accept the comparison using standardized benchmark.

Overall, the paper proposes a promising extension of MPO; thus, I recommend it for acceptance.
",Paper Decision
fjoR62EzaL,rklOg6EFwS,Improving Adversarial Robustness Requires Revisiting Misclassified Examples,Accept (Poster),"This paper presents modifications to the adversarial training loss that yield improvements in adversarial robustness.  While some reviewers were concerned by the lack of mathematical elegance in the proposed method, there is consensus that the proposed method clears a tough bar by increasing SOTA robustness on CIFAR-10. ",Paper Decision
XgDWTjS7hm,SJgvl6EFwH,InfoCNF: Efficient Conditional Continuous Normalizing Flow Using Adaptive Solvers,Reject,"This paper presents a conditional CNF based on the InfoGAN structure to improve ODE solvers. Reviewers appreciate that the approach shows improved performances over the baseline models. 

Reviewers all note, however, that this paper is weak in clearly defining the problem and explaining the approach and the results. While the authors have addressed some of the reviewers concerns through their rebuttal, reviewers still remain concerned about the clarity of the paper.

I thank the authors for submitting to ICLR and hope to see a revised paper at a future venue.",Paper Decision
grAfSt5iTJ,r1lUl6NFDH,Mirror Descent View For Neural Network Quantization,Reject,"The paper proposes to use the mirror descent algorithm for the binary network. It is easy to read. However, novelty over ProxQuant is somehow limited. The theoretical analysis is weak, in that there is no analysis on the convergence and neither how to choose the projection for mirror mapping construction. Experimental results can also be made more convincing, by adding comparisons with bigger datasets, STOA networks, and ablation study to demonstrate why mirror descent is better than proximal gradient descent in this application.",Paper Decision
oL2x3wiUt,rkg8xTEtvB,Hierarchical Disentangle Network for Object Representation Learning,Reject,"The authors propose a new method for learning hierarchically disentangled representations. One reviewer is positive, one is between weak accept and borderline and two reviewers recommend rejection, and keep their assessment after rebuttal and a discussion. The main criticism is the lack of disentanglement metrics and comparisons. After reading the paper and the discussion, the AC tends to agree with the negative reviewers. Authors are encouraged to strengthen their work and resubmit to a future venue.",Paper Decision
nTY68F4q8a,Bklrea4KwS,Deep Multiple Instance Learning with Gaussian Weighting,Reject,The authors propose a novel MIL method that uses a novel approach to normalize the instance weights. The majority of reviewers found the paper lacking in novelty and sufficient experimental performance evidence.,Paper Decision
4Ea1-4ujV,rygHe64FDS,Zeno++: Robust Fully Asynchronous SGD,Reject,"Main content:

Blind review #2 summarizes it well:

This paper investigates the security of distributed asynchronous SGD. Authors propose Zeno++, worker-server asynchronous implementation of SGD which is robust to Byzantine failures. To ensure that the gradients sent by the workers are correct, Zeno++ server scores each worker gradients using a “reference” gradient computed on a “secret” validation set.  If the score is under a given threshold, then the worker gradient is discarded. 

Authors provide convergence guarantee for the Zeno++ optimizer for non-convex function. In addition, they provide an empirical evaluation of Zeno++ on the CIFAR10 datasets and compare with various baselines.

--

Discussion:

Reviews are generally weak on the limited novelty of the approach compared with Zeno, but the rebuttal of the authors on Nov 15 is fair (too long to summarize here).

--

Recommendation and justification:

I do not feel strongly enough to override the weak reviews (but if there is room in the program I would support a weak accept).",Paper Decision
Va_IHM9Xhv,HJgExaVtwr,DivideMix: Learning with Noisy Labels as Semi-supervised Learning,Accept (Poster),"This paper proposes an algorithm for noisy labels by adopting an idea in the recent semi-supervised learning algorithm.

As two problems of training noisy labels and semi-supervised ones are closely related, it is not surprising to expect such results as pointed out by reviewers. However, reported thorough experimental results are strong and I think this paper can be useful for practitioners and following works. 

Hence, I recommend acceptance.",Paper Decision
tFQQpRi6Ja,r1lfga4KvS,Extreme Value k-means Clustering,Reject,"This paper explores extending k-means to allow to clusters with non-convex shapes.

This paper introduces a new algorithm, relying on empirical comparisons to illustrate its contribution. The main issue with the paper is that the empirical claims do not support that the new method is indeed better. The paper claims the new method outperforms the competitors in most cases. However, the original submission reported median performance and when the authors provided mean performance and additional baseline methods (at the reviewers' request) there appear to be little evidence to support the claim. In addition there are no measures of significance provided. The authors provided no commentary to help the reviewers understand the new results. There might be some important speed gains at the cost of final performance, but on the evidence provided we are not able to evaluate the cost in final performance.

The text changes size after section 5.3 and is 9% smaller. Watch out for this formatting issue in future submissions

",Paper Decision
TC4VKQJDVN,rylfl6VFDH,Adaptive network sparsification with dependent variational beta-Bernoulli dropout,Reject,"This paper introduces a new adaptive variational dropout approach to balance accuracy, sparsity and computation. 

The method proposed here is sound, the motivation for smaller (perhaps sparser) networks is easy to follow. The paper provides experiments in several data-sets and compares against several other regularization/pruning approaches, and measures accuracy, speedup, and memory. The reviewers agreed on all these points, but overall they found the results unconvincing. They requested (1) more baselines (which the authors added), (2) larger tasks/datasets, and (3) more variety in network architectures.  The overall impression was it was hard to see a clear benefit of the proposed approach, based on the provided tables of results.

The paper could sharpen its impact with several adjustments. The results are much more clear looking at the error vs speedup graphs. Presenting ""representative results"" in the tables was confusing, especially considering the proposed approach rarely dominated across all measures. It was unclear how the variants of the algorithms presented in the tables were selected---explaining this would help a lot. In addition, more text is needed to help the reader understand how improvements in speed, accuracy, and memory matter. For example in LeNet 500-300 is a speedup of ~12 @ 1.26 error for BB worth-it/important compared a speedup of ~8 for similar error for L_0? How should the reader think about differences in speedup, memory and accuracy---perhaps explanations linking to the impact of these metrics to their context in real applications. I found myself wondering this about pretty much every result, especially when better speedup and memory could be achieved at the cost of some accuracy---how much does the reduction in accuracy actually matter? Is speed and size the dominant thing? I don't know.

Overall the analysis and descriptions of the results are very terse, leaving much to the reader to figure out. For example (fig 2 bottom right). If a result is worth including in the paper it's worth explaining it to the reader. Summary statements like ""BB and DBB either achieve significantly smaller error than the baseline methods, or significant speedup and memory saving at similar error rates."" Is not helpful where there are so many dimensions of performance to figure out. The paper spends a lot of time explaining what was done in a matter of fact way, but little time helping the reader interpret the results.

There are other issues that hurt the paper, including reporting the results of only 3 runs, sometimes reporting median without explanation, undefined metrics like speedup ,%memory (explain how they are calculated), restricting the batchsize for all methods to a particular value without explanation, and overall somewhat informal and imprecise discussion of the empirical methodology.

The authors did a nice job responding to the reviewers (illustrating good understanding of the area and the strengths of their method), and this could be a strong paper indeed if the changes suggested above were implemented. Including SSL and SVG in the appendix was great, but they really should have been included in the speedup vs error plots throughout the paper. This is a nice direction and was very close. Keep going!",Paper Decision
QtxdEfLPMv,S1efxTVYDr,Data-dependent Gaussian Prior Objective for Language Generation,Accept (Talk),"This paper addresses the problem of poor generation quality in models for text generation that results from the use of the maximum likelihood (ML) loss, in particular the fact that the ML loss does not differentiate between different ""incorrect"" generated outputs (ones that do not match the corresponding training sequence).  The authors propose to train text generation models with an additional loss term that measures the distance from the ground truth via a Gaussian distribution based on embeddings of the ground-truth tokens.  This is not the first attempt to address drawbacks of ML training for text generation, but it is simple and intuitive, and produces improvements over the state of the art on a range of tasks.  The reviewers are all quite positive, and are in agreement that the author responses and revisions have improved the paper quality and addressed initial concerns.  I think this work will be broadly appreciated by the ICLR audience.  One negative point is that the writing quality still needs improvement.",Paper Decision
mQj1Ir792m,Syl-xpNtwS,Learning Representations in Reinforcement Learning: an Information Bottleneck Approach,Reject,"The authors propose to use the information bottleneck to learn state representations for RL. They optimize the IB objective using Stein variational gradient descent and combine it with A2C and PPO. On a handful of Atari games, they show improved performance.

The reviewers primary concerns were:
*Limited evaluation. The method was only evaluated on a handful of the Atari games and no comparison to other representation learning methods was done.
*Using a simple Gaussian embedding function would eliminate the need for amortized SVGD. The authors should compare to that alternative to demonstrate the necessity of their approach.

The ideas explored in the paper are interesting, but given the issues with evaluation, the paper is not ready for acceptance at this time.",Paper Decision
G8dIIqBdj,BkgZxpVFvH,LSTOD: Latent Spatial-Temporal Origin-Destination prediction model and its applications in ride-sharing platforms,Reject,"The paper proposes a  deep learning architecture for forecasting Origin-Destination (OD) flow. The model integrates several existing modules including spatiotemporal graph convolution and periodically shifted attention mechanism. 

The reviewers agree that the paper is not written well, and the experiments are also not executed well. Overall, we recommend rejection.",Paper Decision
t-pVBAtsF,S1xxx64YwH,Ecological Reinforcement Learning,Reject,"This paper investigates how the properties of an environment affect the success of reinforcement learning, and in particular finds that random dynamics and non-episodic learning makes learning easier, even though these factors make learning more difficult when applied individually. The paper was reviewed by three experts who gave Reject, Weak Reject, and Weak Reject recommendations. The main concerns are about missing connections to related work, overstating some contributions, and experimental details. While the author response addressed many of these issues, reviewers felt another round of peer review is really needed before this paper can be accepted; R2's post-rebuttal comments give some specific, constructive, concrete suggestions for preparing a revision.",Paper Decision
6MS2k57_Y3,BJlkgaNKvr,Towards Understanding the Regularization of Adversarial Robustness on Neural Networks,Reject,"The paper investigates why adversarial training can sometimes degrade model performance on clean input examples. 

The reviewers agreed that the paper provides valuable insights into how adversarial training affects the distribution of activations. On the other hand, the reviewers raised concerns about the experimental setup as well as the clarity of the writing and felt that the presentation could be improved. 

Overall, I think this paper explores a very interesting direction and such papers are valuable to the community. It's a borderline paper currently but I think it could turn into a great paper with another round of revision. I encourage the authors to revise the draft and resubmit to a different venue. 

 ",Paper Decision
jNFRgGx3Vh,S1gyl6Vtvr,MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning,Reject,"This paper presents a method to learn a pruned convolutional network during conventional training.  Pruning the network has advantages (in deployment) of reducing the final model size and reducing the required FLOPS for compute.  The method adds a pruning mask on each layer with an additional sparsity loss on the mask variables. The method avoids the cost of a train-prune-retrain optimization process that has been used in several earlier papers.  The method is evaluated on CIFAR-10 and ImageNet with three standard convolutional network architectures.  The results show comparable performance to the original networks with the learned sparse networks. 

The reviewers made many substantial comments on the paper and most of these were addressed in the author response and subsequent discussion.  For example, Reviewer1 mentioned two other papers that promote sparsity implicitly during training (Q3), and the authors acknowledged the omission and described how those methods had less flexibility on a target metric (FLOPS) that is not parameter size.  Many of the author responses described changes to an updated paper that would clarify the claims and results (R1: Q2-7, R2:Q3).

However, the reviewers raised many concerns for the original paper and they did not see an updated paper that contains the proposed revisions.  Given the numerous concerns with the original submission, the reviewers wanted to see the revised paper to assess whether their concerns had been addressed adequately. Additionally, the paper does not have a comparison experiment with state-of the art results, and the current results were not sufficiently convincing for the reviewers.  Reviewer1 and author response to questions 13--15 suggest that the experimental results with ResNet-34 are inadequate to show the benefits of the approach, but results for the proposed method with the larger ResNet-50 (which could show benefits) are not yet ready. 

The current paper is not ready for publication.
",Paper Decision
xFjm1GjXSi,rkgCJ64tDB,Scale-Equivariant Neural Networks with Decomposed Convolutional Filters,Reject,"This paper presents a CNN architecture equivariant to scaling and translation which is realized by the proposed joint convolution across the space and scaling groups. All reviewers find the theorical side of the paper is sound and interesting. Through the discussion based on authors’ rebuttal, one reviewer decided to update the score to Weak Accept, putting this paper on the borderline. However, some concerns still remain. Some reviewers are still not convinced regarding the novelty of the paper, particularly in terms of the difference from (Chen+,2019). Also, they agree that experiments are still very weak and not convincing enough. Overall, as there was no opinion to champion this paper, I’d like to recommend rejection this time. 
I encourage authors to polish the experimentations taking in the reviewers’ suggestions. 
",Paper Decision
vWeubcuJnD,S1eRya4KDB,A novel Bayesian estimation-based word embedding model for sentiment analysis,Reject,"This paper proposes a method to improve word embedding by incorporating sentiment probabilities. Reviewer appreciate the interesting and simple approach and acknowledges improved results in low-frequency words.

However, reviewers find that the paper is lacking in two major aspects:
1) Writing is unclear, and thus it is difficult to understand and judge the contributions of this research.
2) Perhaps because of 1, it is not convincing that the improvements are significant and directly resulting from the modeling contributions.

I thank the authors for submitting this work to ICLR, and I hope that the reviewers' comments are helpful in improving this research for future submission.",Paper Decision
JXBb6KtsT4,SJlpy64tvB,Attacking Lifelong Learning Models with Gradient Reversion,Reject,"The paper investigates questions around adversarial attacks in a continual learning algorithm, i.e., A-GEM. While reviewers agree that this is a novel topic of great importance, the contributions are quite narrow, since only a single model (A-GEM) is considered and it is not immediately clear whether this method transfers to other lifelong learning models (or even other models that belong to the same family as A-GEM). This is an interesting submission, but at the moment due to its very narrow scope, it seems more appropriate as a workshop submission investigating a very particular question (that of attacking A-GEM). As such, I cannot recommend acceptance.",Paper Decision
LUTNx7tR4,H1g6kaVKvH,Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient,Reject,"The submission is concerned with the catastrophic forgetting problem of continual learning, and proposes a gradient-based method which uses buffers of data seen previously to integrate the angles of the gradients and thereby mitigate forgetting. Empirical results are given on several benchmarks. 

The reviewers were impressed with the thorough validation and strong results, but noticed that the much simpler MEGA-D baseline did almost as well. Given this, they were not convinced that the proposed approach was necessary. Although the authors provided a strong rebuttal and an additional ablation, the reviewers did not feel that their concerns were met.

My recommendation is to reject the submission at this time.",Paper Decision
FVwmhQo6P,rJl31TNYPr,Fooling Detection Alone is Not Enough: Adversarial Attack against Multiple Object Tracking,Accept (Poster),"The authors agree after reading the rebuttal that attacks on MOT are novel.  While the datasets used are small, and the attacks are generated in digital simulation rather than the physical world, this paper still demonstrates an interesting attack on a realistic system.",Paper Decision
QwpN3Lfjxm,S1eik6EtPB,Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness,Reject,"This submission studies an interesting problem. However, as some of the reviewers point out, the novelty of the proposed contributions is fairly limited.",Paper Decision
uicjB8uX5j,S1xjJpNYvB,Domain-Agnostic Few-Shot Classification by Learning Disparate Modulators,Reject,"This paper addresses the problem of few-shot classification across multiple domains. The main algorithmic contribution consists of a selection criteria to choose the best source domain embedding for a given task using a multi-domain modulator. 

All reviewers were in agreement that this paper is not ready for publication. Some key concerns were the lack of scalability (though the authors argue that this may not be a concern as all models are only stored during meta-training, still if you want to incorporate many training settings it may become challenging) and low algorithmic novelty. The issue with novelty is that there is inconclusive experimental evidence to justify the selection criteria over simple methods like averaging, especially when considering novel test time domains. The authors argue that since their approach chooses the single best training domain it may not be best suited to generalize to a novel test time domain. 

Based on the reviews and discussions the AC does not recommend acceptance. The authors should consider revisions for clarity and to further polish their claims providing any additional experiments to justify where appropriate.
",Paper Decision
TDvtFBHBRW,SJg5J6NtDr,"Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards",Accept (Poster),"The paper proposed a meta-learning approach that learns from demonstrations and subsequent RL tasks.
The reviewers found this work interesting and promising. There have been some concerns regarding the clarity of presentation, which seems to be addressed in the revised version. Therefore, I recommend acceptance for this paper.",Paper Decision
POn535UERJ,rkecJ6VFvr,Logic and the 2-Simplicial Transformer,Accept (Poster),"This paper extends the Transformer, implementing higher-dimensional attention generalizing the dot-product attention. The AC agrees that Reviewer3's comment that generalizing attention from 2nd- to 3rd-order relations is an important upgrade, that the mathematical context is insightful, and that this could lead to the further potential development. The readability of the paper still remains as an issue, and it needs to be address in the final version of the paper.",Paper Decision
Kwft8zr-RL,S1gKkpNKwH,Reinforcement Learning with Chromatic Networks,Reject,"This paper describes a method for learning compact RL policies suitable for mobile robotic applications with limited storage.  The proposed pipeline is a scalable combination of efficient neural architecture search (ENAS) and evolution strategies (ES).  Empirical evaluations are conducted on various OpenAI Gym and quadruped locomotion tasks, producing policies with as little as 10s of weight parameters, and significantly increased compression-reward trade-offs are obtained relative to some existing compact policies.

Although reviewers appreciated certain aspects of this paper, after the rebuttal period there was no strong support for acceptance and several unsettled points were expressed.  For example, multiple reviewers felt that additional baseline comparisons were warranted to better calibrate performance, e.g., random coloring, wider range of generic compression methods, classic architecture search methods, etc.  Moreover, one reviewer remained concerned that the scope of this work was limited to very tiny model sizes whereby, at least in many cases, running the uncompressed model might be adequate.",Paper Decision
v_Ze4Oqm-,HkldyTNYwH,AE-OT: A NEW GENERATIVE MODEL BASED ON EXTENDED SEMI-DISCRETE OPTIMAL TRANSPORT,Accept (Poster),"The authors present a different perspective on the mode collapse and mode mixture problems in GAN based on some recent theoretical results. 

This is an interesting work. However, two reviewers have raised some concerns about the results and hence given a low rating of the paper. After reading the reviews and the rebuttal carefully I feel that the authors have addressed all the concerns of the reviewers. In particular, at least for one reviewer I felt that there was a slight misunderstanding on the reviewer's part which was clarified in the rebuttal. The concerns of R1 about a simpler baseline have also been addressed by the authors with the help of additional experiments. I am convinced that the original concerns of the reviewers are addressed. Hence, I recommend that this paper be accepted. 

Having said that, I strongly recommend that in the final version, the authors should be a bit more clear in motivating the problem. In particular, please make it clear that you are only dealing with the generator and do not have an adversarial component in the training. Also, as suggested by R3 add more intuitive descriptions to make the paper accessible to a wider audience.

",Paper Decision
mcGvegdLgY,Skld1aVtPB,Deep Mining: Detecting Anomalous Patterns in Neural Network Activations with Subset Scanning,Reject,"The paper investigates the use of the subset scanning to the detection of anomalous patterns in the input to a neural network. The paper has received mixed reviews (one positive and two negatives). The reviewers agree that the idea is interesting, has novelty, and is worth investigating. At the same time they raise issues about the clarity and the lack of comparisons with baselines. Despite a very detailed rebuttal, both of the negative reviewers still feel that addressing their concerns through paper revision would be needed for acceptance.",Paper Decision
2iDo4Rm_C,SklOypVKvS,A Data-Efficient Mutual Information Neural Estimator for Statistical Dependency Testing,Reject,"The paper deal with a mutual information based dependency test. 

The reviewers have provided extensive and constructive feedback on the paper. The authors have in turn given detailed response withsome new experiments and plans for improvement. 

Overall the reviewers are not convinced the paper is ready for publication.  ",Paper Decision
jGAgYJPaNT,Skgvy64tvr,Enhancing Adversarial Defense by k-Winners-Take-All,Accept (Spotlight),"This paper presents new non-linearity function which specially affects regions of the model which are densely valued. The non-linearity is simple: it retains only top-k highest units from the input, while truncating the rest to zero. This also makes the models more robust to adversarial defense which depend on the gradients. The non-linearity function is shown to have better adversarial robustness on CIFAR-10 and SVHN datasets. The paper also presents theoretical analysis for why the non-linearity is a good function.

The authors have already incorporated major suggestions by the reviewers and the paper can make significant impact on the community. Thus, I recommend its acceptance.",Paper Decision
op722O20zF,S1lDkaEFwS,Thwarting finite difference adversarial attacks with output randomization,Reject,"This paper proposes a defense technique against query-based attacks based on randomization applied to a DNN's output layer. It further shows that for certain types of randomization, they can bound the probability of introducing errors by carefully setting distributional parameters. It has some valuable contributions; however, the rebuttal does not fully address the concerns.",Paper Decision
xUFYIlcPDL,SkeIyaVtwB,Exploration in Reinforcement Learning with Deep Covering Options,Accept (Poster),"This paper considers options discovery in hierarchical reinforcement learning. It extends the idea of covering options, using the Laplacian of the state space discover a set of options that reduce the upper bound of the environment's cover time, to continuous and large state spaces. An online method is also included, and evaluated on several domains.

The reviewers had major questions on a number of aspects of the paper, including around the novelty of the work which seemed limited, the quantitative results in the ATARI environments, and problems with comparisons to other exploration methods. These were all appropriately dealt with in the rebuttals, leaving this paper worthy of acceptance.",Paper Decision
4B-9IdtLzW,ryxUkTVYvH,Towards Controllable and Interpretable Face Completion via  Structure-Aware and Frequency-Oriented Attentive GANs,Reject,"This work performs fast controllable and interpretable face completion, by proposing a progressive GAN with frequency-oriented attention modules (FOAM).  The proposed FOAM encourages GANs to highlight more to finer details in the progressive training process. This paper is well written and is easy to understand. While reviewer #1 is overall positive about this work, the reviewer #2 and #141 rated weak reject with various concerns, including unconvincing experiments, very common framework, limited novelty, and the lack of ablation study. The authors provided response to the questions, but did not change the rating of the reviewers. Given the various concerns raised, the ACs agree that this paper can not be accepted at its current state.",Paper Decision
KaiHtgXXB,HkxBJT4YvB,Learning Disentangled Representations for CounterFactual Regression,Accept (Poster),The paper proposes a new way of estimating treatment effects from observational data. The text is clear and experiments support the proposed model.,Paper Decision
iuSJLs5Uyj,SJeS16EKPr,Learning relevant features for statistical inference,Reject,"This manuscript proposes an approach for estimating cross-correlations between model outputs, related to deep CCA. Authors note that the procedure improves results when applied to supervised learning problems.

The reviewers have pointed out the close connection to previous work on deep CCA, and the author(s) have agreed. The reviewers agree that the paper has promise if properly expanded both theoretically and empirically.",Paper Decision
LvAPTEfDW,SJgNkpVFPr,VILD: Variational Imitation Learning with Diverse-quality Demonstrations,Reject,"The paper proposes a new imitation learning algorithm that explicitly models the quality of demonstrators.

All reviewers agreed that the problem and the approach were interesting, the paper well-written, and the experiments well-conducted. However, there was a shared concern about the applicability of the method to more realistic settings, in which the model generating the demonstrations does not fall under the assumptions of the method. The authors did add a real-world experiment during the rebuttal, but the reviewers were suspicious of the reported InfoGAIL performance and were not persuaded to change their assessment.

Following this discussion, I recommend rejection at this time, but it seems like a good paper and I encourage the authors to do a more careful validation experiment, and resubmit to a future venue.",Paper Decision
gulyk6F2ha,SylVJTNKDr,Entropy Minimization In Emergent Languages,Reject,"This paper studies the information-theoretic complexity for emergent languages when pairs of neural networks are trained to solve a two player communication game. One of the primary claims of the paper was that under common training protocols, networks were biased towards low entropy solutions. During the discussion period, one reviewer shared an ipython notebook investigating the experiments shown in Figure 1. There it was discovered that low entropy solutions were only obtained for networks which were themselves initialized at low entropy configurations. When networks are initialized at high entropy configurations, the converged solution would remain high entropy. This experiment raises questions about the validity of the claim that there was ""pressure"" towards low entropy solutions to the task. Therefore, a more careful analysis of the phenomenon is required. ",Paper Decision
73Dhvd8L8P,ryl71a4YPB,A Unified framework for randomized smoothing based certified defenses,Reject,"After the rebuttal, the reviewers agree that this paper would benefit from further revisions to clarify issues regarding the motivation of the DP-based security definition,  any relationship it may have to standard definitions of privacy, and the role of dimensionality in the theoretical guarantees.",Paper Decision
intYQZqp3z,rJgQkT4twH,Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification,Accept (Poster),"This paper presents a case study of training a video classifier and subsequently analyzing the features to reduce reliance on spurious artifacts. The supervised learning task is zebrafish bout classification which is relevant for biological experiments. The paper analyzed the image support for the learned neural net features using a previously developed technique called Deep Taylor Decomposition. This analysis showed that the CNNs when applied to the raw video were relying on artifacts of the data collection process, which spuriously increased classification accuracies by a ""clever Hans"" mechanism. By identifying and removing these artifacts, a retrained CNN classifier was able to outperform an older SVM classifier. More importantly, the analysis of the network features enabled the researchers to isolate which parts of the zebrafish motion were relevant for the classification.

The reviewers found the paper to be well-written and the experiments to be well-designed. The reviewers suggested a some changes to the phrasing in the document, which the authors adopted. In response to the reviewers, the authors also clarified their use of ImageNet for pre-training and examined alternative approaches for building saliency maps.

This paper should be published as the reviewers found the paper to be a good case study of how model interpretability can be useful in practice. ",Paper Decision
dIBw3_LrAU,rJeGJaEtPH,MIST: Multiple Instance Spatial Transformer Networks,Reject,"Two reviewers are negative on this paper while the other one is slightly positive. Overall, this paper does not make the bar of ICLR. A reject is recommended.",Paper Decision
xz98kScBG,rklz16Vtvr,ISBNet: Instance-aware Selective Branching Networks,Reject,"This paper proposes a method for finding neural architecture which, through the use of selective branching, can avoid processing portions of the network on a per-data-point basis. 

While the reviewers felt that the idea proposed was technically interesting and well-presented, they had substantial concerns about the evaluation that persisted post-rebuttal, and lead to a consensus rejection recommendation.",Paper Decision
pPGqUwVvxM,HJlMkTNYvH,MODiR: Multi-Objective Dimensionality Reduction for Joint Data Visualisation,Reject,"There is a consensus among reviewers that the paper should not be accepted. No rebuttal was provided, so the paper is rejected. ",Paper Decision
TtAjBPDPY,H1lZJpVFvr,Robust Local Features for Improving the Generalization of Adversarial Training,Accept (Poster),"Earlier work suggests that adversarial examples exploit local features and that more robust models rely on global features. The authors propose to exploit this insight by performing data augmentation in adversarial training, by cutting and reshuffling image block. They demonstrate the idea empirically and witness interesting gains. I think the technique is an interesting contribution, but empirically and as a tool.
",Paper Decision
k3K0KJkT-j,rkxZyaNtwB,Online and stochastic optimization beyond Lipschitz continuity: A Riemannian approach,Accept (Spotlight),"This is a mostly theoretical paper concerning online and stochastic optimization for convex loss functions that are not Lipschitz continuous. The authors propose a method for replacing the Lipschitz continuity condition with a more general Riemann-Lipschitz continuity condition, under which they are able to provide regret bounds for the online mirror descent algorithm, as well as extending to the stochastic setting. They follow up by evaluating their algorithm on Poisson inverse problems. 

The reviewers all agree that this is a well-written paper that makes a clear contribution. To the best of our knowledge, the theory and derivations are correct, and the authors were highly responsive to reviewers’ (minor) comments. I’m therefore happy to recommend acceptance.",Paper Decision
4IIz4JjqHT,Syee1pVtDS,Distributed Online Optimization with Long-Term Constraints,Reject,"The paper proposes  a decentralized algorithm with regret for distributed online convex optimization problems. The reviewers worry about the assumptions and the theoretical settings, they also find that the experimental evaluation  is insufficient.",Paper Decision
4SSL2-RHtL,ryxgJTEYDr,Reinforcement Learning with Competitive  Ensembles of Information-Constrained Primitives,Accept (Poster),"In contrast to many current hierarchical reinforcement learning approaches, the authors present a decentralized method that learns low level policies that decide for themselves whether to act in the current state, rather than having a centralized higher level meta policy that chooses between low level policies.  The reviewers primarily had minor concerns about clarity, reward scaling, and several other issues that were clarified by the authors.  The only outstanding concern is that of whether transfer/pretraining is required for the experiments to work or not.  While this is an interesting question that I would encourage authors to address as much as possible, it does not seem like a dealbreaker in light of the reviewers' agreement on the core contribution.  Thus, I recommend this paper for acceptance.",Paper Decision
iepMdWY-ZP,rylJkpEtwS,Learning the Arrow of Time for Problems in Reinforcement Learning,Accept (Poster),"This paper develops the notion of the arrow of time in MDPs and explores how this might be useful in RL. All the reviewers found the paper thought provoking, well-written, and they believe the work could have significant impact. The paper does not fit the typical mold: it presents some ideas and uses illustrative experiments to suggest the potential utility of the arrow without nailing down a final algorithm or make a precise performance claim. Overall it is a solid paper, and the reviewers all agreed on acceptance.

There are certainly weaknesses in the work, and there is a bit of work to do to get this paper ready. R2 had a nice suggestion of a baseline based on simply learning a transition model (its described in the updated review)---please include it. The description of the experimental methodology is a bit of a mess. Most of the experiments in the paper do not clearly indicate how many runs were conducted or how errorbars where computed or what they represent.  It is likely that only a handful of runs were used, which is surprising given the size of some of the domains used. In many cases the figure caption does not even indicate which domain the data came from. All of this is dangerously close to criteria for rejection; please do better.

Readability is also known as empowerment and it would be good to discuss this connection. In general the paper was a bit light on connections outlining how information theory has been used in RL. I suggest you start here (http://www2.hawaii.edu/~sstill/StillPrecup2011.pdf) to improve this aspect. Finally, the paper has a very large appendix (~14 oages) with many many more experiments and theory. I am still not convinced that the balance is quite right. This is probably a journal or long arxiv paper. Maybe this paper should be thought of as a nectar version of a longer standalone arxiv paper.

Finally, relying on effectiveness of random exploration is no small thing and there is a long history in RL of ideas that would work well, given it is easy to gather data that accurately summarizes the dynamics of the world (e.g. proto-value, funcs). Many ideas are effective given this assumption. The paper should clearly and honestly discuss this assumption, and provide some arguments why there is hope.",Paper Decision
IEvzkBmrU,Hye1kTVFDS,The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget,Accept (Poster),"Existing implementation of information bottleneck need access to privileged information which goes against the idea of compression. The authors propose variational bandwidth bottleneck which estimates the value of the privileged information and then stochastically decided whether to access this information or not. They provide a suitable approximation and show that their  method improves generalisation in RL while reducing access to expensive information.

These paper received only two reviews. However, both the reviews were favourable. During discussions with the AC the reviewers acknowledged that most of their concerns were addressed. R2 is still concerned that VBB does not result in improvement in terms of sample efficiency. I request the authors to adequately address this in the final version. Having said that, the paper does make other interesting contributions, hence I recommend that this paper should be accepted.",Paper Decision
b6DIvscGx,S1x0CnEtvB,AutoGrow: Automatic Layer Growing in Deep Convolutional Networks,Reject,"This paper proposes a neural architecture search method based on greedily adding layers with random initializations. The reviewers all recommend rejection due to various concerns about the significance of the contribution, novelty, and experimental design. They checked the author response and maintained their ratings.
",Paper Decision
B1VOlpM_gu,H1eCR34FPB,Sequence-level Intrinsic Exploration Model for Partially Observable Domains,Reject,"This paper introduces a new architecture based on intrinsic rewards, to deal with partially observable and sparse reward domains.

The reviewers found the novelty of the work not particularly high, and had concerns about the general utility of the method based on the empirical evidence. This paper has numerous issues and could use significant revision in terms of writing, connections to literature, experiment design, and clarity of results. 

Much of the discussion focused on the scaling parameter. From an algorithmic point of view, the scaling parameter is very problematic. It is domain specific and when tuned per domain resulted in very different values. The ablation study showed that only two settings in one domain led to good performance, whereas the other resulted in no learning (for some reason the other two values were not plotted). 

There are concerns that the baselines were not completely fair. In many cases different domains were used to compare against RND and ICM, and there appears to be no tuning of these baselines for the new domains---this a problem due to the inherent bias in favor one's own method. In the solaris domain which was used in the RND paper, the results don't appear to match the RND paper, and in vizdoom the performance numbers are difficult to compare for ICM because a different metric is used---even if you don't like their performance numbers at least report them once so we can be confident the baselines are well calibrated. One reviewer pointed out the meta-parameters where different for RND than the published previous, but the paper does not describe what approach was used to tune those parameters and this is not acceptable. We cannot have much confidence that these results are reflective of those methods. Finally, there is no comment on how the performance numbers were computed and no description of how the errorbars where computed or what they represent. 

The paper focuses on partially observable domains, the evidence that this method is effective in closer to Markov settings is unclear. The Atari experiments do not yield significant results by large (solairs looks as if there is no learning occurring at all---a no comment about it in the text to explain). The paper claims evidence the approach can work well in both cases, but it was not even indicated if frame-stacking was used in the Atari experiments. In fact, the result was only alluded too in the conclusion---there was no reference in the main text to a specific result in the appendix. Text is very challenging to read. The language is informal and imprecise, and the paper frequently uses terms incorrectly or in different ways through (e.g., the use of the term novelty throughout)

This is clearly an interesting direction. The authors should keep working, but this paper is not ready for publication. I urge the authors to dig deeper in the literature to gain a more nuanced understanding of the topic. Barto et al's excellent paper on the topic is a great place to start: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3858647/ ",Paper Decision
-rSDsvz9lq,SkgTR3VFvH,Pipelined Training with Stale Weights of Deep Convolutional Neural Networks,Reject,"The paper proposed a new pipelined training approach to better utilize the memory and computation power to speed up deep convolutional neural network training. The authors experimentally justified that the proposed pipeline training, using stale weights without weights stacking or micro-batching, is simpler and does converge on a few networks. 

The main concern for this paper is the missing of convergence analysis of the proposed method as requested by the reviewers. The authors brought up the concern of the limited space in the paper, which can be addressed by putting convergence analysis into appendix. From a reader perspective, knowing the convergence property of the methods is much more important than knowing it works for a few networks on a particular dataset.    ",Paper Decision
ju6JR17Bnp,rklnA34twH,Universal Learning Approach for Adversarial Defense,Reject,"The reviewers attempted to give this paper a fair assessment, but were unanimous in recommending rejection.  The technical quality of motivation was questioned, while the experimental evaluation was not found to be clear or convincing.  Hopefully the feedback provided can help the authors improve their paper.",Paper Decision
fSkl0ZNAZm,H1lj0nNFwB,The Implicit Bias of Depth: How Incremental Learning Drives Generalization,Accept (Poster),"The paper studies the role of depth on incremental learning, defined as a favorable learning regime in which one searches through the hypothesis space in increasing order of complexity. Specifically, it establishes a dynamical depth separation result, whereby shallow models require exponetially smaller initializations than deep ones in order to operate in the incremental learning regime. 

Despite some concerns shared amongst reviewers about the significance of these results to explain realistic deep models (that exhibit nonlinear behavior as well as interactions between neurons) and some remarks about the precision of some claims, the overall consensus -- also shared by the AC -- is that this paper puts forward an interesting phenomenon that will likely spark future research in this important direction. The AC thus recommends acceptance. ",Paper Decision
SAJB044NG,Byg9A24tvB,Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness,Accept (Poster),"This paper proposes an alternative loss function, the max-mahalanobis center loss, that is claimed to improve adversarial robustness. 

In terms of quality, the reviewers commented on the convincing experiments and theoretical results, and were happy to see the sample density analysis. 

In terms of clarity, the reviewers commented that the paper is well-written. 

The problem of adversarial robustness is relevant to the ICLR community, and the proposed approach is a novel and significant contribution in this area. 

The authors have also convincingly answered the questions of the authors and even provided new theoretical and experimental results in their final upload. ",Paper Decision
tYBc5fZvri,SygcCnNKwr,Measuring Compositional Generalization: A Comprehensive Method on Realistic Data,Accept (Poster),"Main content:

Blind review #1 summarizes it well:

This paper first introduces a method for quantifying to what extent a dataset split exhibits compound (or, alternatively, atom) divergence, where in particular atoms refer to basic structures used by examples in the datasets, and compounds result from compositional rule application to these atoms. The paper then proposes to evaluate learners on datasets with maximal compound divergence (but minimal atom divergence) between the train and test portions, as a way of testing whether a model exhibits compositional generalization, and suggests a greedy algorithm for forming datasets with this property. In particular, the authors introduce a large automatically generated semantic parsing dataset, which allows for the construction of datasets with these train/test split divergence properties. Finally, the authors evaluate three sequence-to-sequence style semantic parsers on the constructed datasets, and they find that they all generalize very poorly on datasets with maximal compound divergence, and that furthermore the compound divergence appears to be anticorrelated with accuracy.

--

Discussion:

Blind review #1 is the most knowledgeable in this area and wrote ""This is an interesting and ambitious paper tackling an important problem. It is worth noting that the claim that it is the compound divergence that controls the difficulty of generalization (rather than something else, like length) is a substantive one, and the authors do provide evidence of this.""

--

Recommendation and justification:

This paper deserves to be accepted because it tackles an important problem that is overlooked in current work that is evaluated on datasets of questionable meaningfulness. It adds insight by focusing on the qualities of datasets that enable testing how well learning algorithms do on compositional generalization, which is crucial to intelligence.",Paper Decision
YoU-IV7c5,HJgK0h4Ywr,Theory and Evaluation Metrics for Learning Disentangled Representations,Accept (Poster),"This manuscript proposes and evaluates new metrics for measuring the quality of disentangled representations for both supervised and unsupervised settings. The contributions include conceptual definitions and empirical evaluation.

In reviews and discussion, the reviewers and AC note missing or inadequate empirical evaluation with many available methods for learning disentangled representations. On the writing, reviewers mentioned that the conciseness of the manuscript could be improved. The reviewers also mentioned incomplete references and discussion of prior work, which should be improved.",Paper Decision
F9Xx1ImJqe,ByxtC2VtPB,Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks,Accept (Poster),"This paper proposed a mixup inference (MI) method, for  mixup-trained models, to better defend adversarial attacks.  The idea is novel and is proved to be effective on CIFAR-10 and CIFAR-100.  All reviewers and the AC agree to accept the paper.",Paper Decision
9ZmvdOWJQ-,rkeuAhVKvB,Dynamically Pruned Message Passing Networks for Large-scale Knowledge Graph Reasoning,Accept (Poster),"The paper presents a graph neural network model inspired by the consciousness prior of Bengio (2017) and implements it by means of two GNN models: the inattentive and the attentive GNN, respectively IGNN and AGNN. The reviewers think 
- The idea of learning an input-dependent subgraph using GNN seems new. 
- The proposed way to reduce the complexity by restricting the attention horizon sounds interesting. ",Paper Decision
h3JRl_YLaO,rkl_Ch4YwS,A TWO-STAGE FRAMEWORK FOR MATHEMATICAL EXPRESSION RECOGNITION,Reject,"One reviewer is positive, while the others recommend rejection. The authors did not submit a rebuttal, thus the reviewers kept their original assessment.",Paper Decision
oltx8YV7Bw,r1xwA34KDB,Learning Invariants through Soft Unification,Reject,"The main concern raised by reviewers is the limited experiments, which are on simple tasks and missing some baselines to state-of-the-art methods. While the overall approach is interesting, the reviewers found the empirical evidence to be fairly unconvincing. ",Paper Decision
JxwLtAha3,H1xPR3NtPB,Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction,Accept (Poster),"This paper presents results of looking at the inside of pre-trained language models to capture and extract syntactic constituency. Reviewers initially had neutral to positive comments, and after the author rebuttal which addressed some of the major questions and concerns, their scores were raised to reflect their satisfaction with the response and the revised paper. Reviewer discussions followed in which they again expressed that they became more positive that the paper makes novel and interesting contributions.

I thank the authors for submitting this paper to ICLR and look forward to seeing it at the conference..",Paper Decision
KW13-Oq2ux,HJgBA2VYwH,FSPool: Learning Set Representations with Featurewise Sort Pooling,Accept (Poster),"Overall, this paper got strong scores from the reviewers (2 accepts and 1 weak accept).  The paper proposes to address the responsibility problem, enabling encoding and decoding sets without worrying about permutations.  This is achieved using permutation-equivariant set autoencoders and an 'inverse' operation that undoes the sorting in the decoder.  The reviewers all agreed that the paper makes a meaningful contribution and should be accepted.  Some concerns regarding clarity of exposition were initially raised but were addressed during the rebuttal period.  I recommend that the paper be accepted.",Paper Decision
H375wzv7I,rJgHC2VKvB,Recurrent Neural Networks are Universal Filters,Reject,"Based on the Bayesian approach to filtering problem, the paper proves that RNN are universal approximators for the filtering problem.  Two reviewers, however, have doubts about the novelty and difficulty to get the result. Although I do not fully agree that Reviewer3 that the proof is just ""DNN can fit anything"" - it is not this case, but the concerns of Reviewer2 are more strong, especially about the usage of the term ""recurrent neural network"". The paper is purely theoretical and does not have any numerical experiments, which probably makes it too weak for ICLR in this form. However, I encourage the authors to continue to work on the subject, since the approach looks very interesting but it still very far from practice.",Paper Decision
0Jl3t5ayC5,HJxNAnVtDS,On the Convergence of FedAvg on Non-IID Data,Accept (Talk),"This manuscript analyzes the convergence of federated learning wit hstragellers, and provides convergence rates. The proof techniques involve bounding the effects of the non-identical distribution due to stragglers and related issues. The manuscript also includes a thorough empirical evaluation. Overall, the reviewers were quite positive about the manuscript, with a few details that should be improved. ",Paper Decision
qPcnzDdBW5,BklVA2NYvH,Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability,Reject,"The authors propose a framework for improving the robustness of neural networks to adversarial perturbations via optimal control techniques (Lyapunov Stability and the Pontryagin Maximum Principle, in particular). By considering a continuous-time limit of the training process, the authors use the PMP to derive udpate rules for the neural network weights that would result in a robust network. While the approach is interesting, the paper has some serious deficiencies that make it unacceptable for publication in its current form:

1. Quality of empirical evaluation: The authors only report final numbers on CIFAR-10 for a fixed set of adversarial attacks. It has been observed repeatedly in the adversarial robustness literature that adversarial evaluation of neural networks has to be done carefully to guard against possible underestimation of the worst-case attack. In particular, the specific details of the adversarial attacks used (number of steps, number of initializations, performance under larger perturbation radii) that are necessary to trust the results are not given (see https://arxiv.org/pdf/1902.06705.pdf for example).

2. Unclear novelty: The authors do not sufficiently explain the novelty in their approach relative to prior work (particular prior work that has used optimal control ideas in this context).

3. Computational cost: The authors do not give sufficient details to judge the computational overhead of their method to judge how much more expensive it is to train with their approach relative to standard or adversarial training.

While one reviewer voted for a weak accept, the other reviewers were in consensus on rejection. The authors did not respond during the rebuttal phase and hence the reviews were unchanged.

In summary, I vote for rejection. However, I think this paper has potentially interesting ideas that should be carefully developed and evaluated in a future revision.",Paper Decision
1cr9S0Bbtu,Syx7A3NFvH,Multi-agent Reinforcement Learning for Networked System Control,Accept (Poster),"The paper focuses on multi-agent reinforcement learning applications in network systems control settings. A key consideration is the spatial layout of such systems, and the authors propose a problem formulation designed to leverage structural assumptions (e.g., locality). The authors derive a novel approach / communication protocol for these settings, and demonstrate strong performance and novel insights in realistic applications. Reviewers particularly commended the realistic applications explored here. Clarifying questions about the setting, experiments, and results were addressed in the rebuttal, and the resulting paper is judged to provide valuable novel insights.",Paper Decision
rRCRBFSTl-,HJlXC3EtwB,Learning to Anneal and Prune Proximity Graphs for Similarity Search,Reject,"The paper proposes a method to prune edges in proximity graphs for faster similarity search. The method works by making the graph edges annealable and optimizing over the weights. The paper tackles an important and practically relevant problem as also acknowledged by the reviewers. However there are some concerns about empirical results, in particular about missing comparisons with tree-structure based algorithms (perhaps with product quantization for high dimensional data), and about modest empirical improvement on two of the three datasets used in the paper, which leaves room for convincing empirical justification of the method. Authors are encouraged to take the reviewers' comments into account and resubmit to a future venue. 

",Paper Decision
asM72hm9wS,B1gXR3NtwS,Deep Bayesian Structure Networks,Reject,"The authors develop stochastic variational approaches to learn Bayesian ""structure distributions"" for neural networks. While the reviewers appreciated the updates to the paper made by the authors, there will still a number of remaining concerns. There were particularly concerns about the clarity of the paper (remarking on informality of language and lack of changes in the revision with respect to comments in the original review), and the fairness of comparisons. Regarding comparisons, one reviewer comments: ""I do not agree that the comparison with DARTS is fair because the authors remove the options for retraining in both DARTS and DBSN. The reason DARTS trains using one half of the data and validate on the other is that it includes a retraining phase where all data is used. Therefore fair comparison should use the same procedure as DARTS (including a retraining phrase). At the very least, to compare methods without retraining, results of DARTS with more data (e.g., 80%) for training should be reported."" The authors are encouraged to continue with this work, carefully accounting for reviewer comments in future revisions.",Paper Decision
SYLj1sHTj,H1gzR2VKDH,Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation,Accept (Poster),"This paper proposes a method that uses subgoals for planning when using video prediction. The reviewers thought that the paper was clearly written and interesting. The reviewer questions and concerns were mostly addressed during the discussion phase, and the reviewers are in agreement that the paper should be accepted.",Paper Decision
yAZrLt9M7d,BklfR3EYDH,Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction,Reject,"The paper is interesting in video prediction, introducing a hierarchical approach: keyframes are first predicted, then intermediate frames are generated. While it is acknowledge the authors do a step in the right direction, several issues remain: (i) the presentation of the paper could be improved (ii) experiments are not convincing enough (baselines, images not realistic enough, marginal improvements) to validate the viability of the proposed approach over existing ones.
",Paper Decision
u1Rbfnelj,Byg-An4tPr,Differential Privacy in Adversarial Learning with Provable Robustness,Reject,"The authors propose a framework for relating adversarial robustness, privacy and utility and show how one can train models to simultaneously attain these properties. The paper also makes interesting connections between the DP literature and the robustness literature thereby porting over composition theorems to this new setting.

The paper makes very interesting contributions, but a few key points require some improvement:
1) The initial version of the paper relied on an approximation of the objective function in order to obtain DP guarantees. While the authors clarified how the approximation impacts model performance in the rebuttal and revision, the reviewers still had concerns about the utility-privacy-robustness tradeoff achieved by the algorithm.

2) The presentation of the paper seems tailored to audiences familiar with DP and is not easy for a broader audience to follow.

Despite this limitations, the paper does make significant novel contributions on an improtant problem (simultaneously achieveing privacy, robustness and utility) and could be of interest. 

Overall, I consider this paper borderline and vote for rejection, but strongly encourage the authors to improve the paper wrt the above concerns and resubmit to a future venue.",Paper Decision
TQZihR6z0x,H1l-02VKPB,Topology-Aware Pooling via Graph Attention,Reject,"This paper proposes to incorporate graph topology into pooling operations on graphs, to better define the notion of locality  necessary for pooling.  While the paper tackles an important problems, and seems to be also well-written, the reviewers agree that there are several issues regarding the contribution and empirical results that need to be addressed before this paper is ready for publication.",Paper Decision
1YUguJ4jxH,BJglA3NKwS,Siamese Attention Networks,Reject,"The submission presents a Siamese attention operator that lowers the computational costs of attention operators for applications such as image recognition. The reviews are split. R1 posted significant concerns with the content of the submission. The concerns remain after the authors' responses and revision. One of the concerns is the apparent dual submission with ""Kronecker Attention Networks"". The AC agrees with these concerns and recommends rejecting the submission.",Paper Decision
QezBNp1huV,rkxxA24FDr,Neural Stored-program Memory,Accept (Poster),"This paper presents the neural stored-program memory, which is a key-value memory that is used to store weights for another neural network, analogous to having programs in computers. They provide an extensive set of experiments in various domains to show the benefit of the proposed method, including synthetic tasks and few-shot learning experiments.

This is an interesting paper proposing a new idea. We discuss this submission extensively and based on our discussion I recommend accepting this submission. 

A few final comments from reviewers for the authors:
- Please try to make the paper a bit more self-contained so that it is more useful to a general audience. This can be done by either making more space in the main text (e.g., reducing the size of Figure 1, reducing space between sections, table captions and text, etc.) or adding more details in the Appendix. Importantly, your formatting is a bit off. Please use the correct style file, it will give you more space. All reviewers agree that the paper are missing some important details that would improve the paper.
- Please cite the original fast weight paper by Malsburg (1981).
- Regarding fast-weights using outer products, this was actually first done in the 1993 paper instead of the 2016 and 2017 papers.",Paper Decision
qfn9Msr6s0,S1exA2NtDB,ES-MAML: Simple Hessian-Free Meta Learning,Accept (Poster),"This paper introduces an evolution strategy for solving the MAML problem. Following up on some other evolutionary methods as alternatives for RL algorithms, this ES-MAML algorithm appears to be quite stable and efficient. The idea makes sense, and the experiments appear strong.

The scores of the reviews showed a lot of variance: 1,6,8. Therefore, I asked a 4th reviewer for a tie-breaking review, and he/she gave another 8. The rejecting reviewer mostly took objection to the fact that learning rates / step sizes were not tuned consistently, which can easily change the relative ranking of different ES algorithms. Here, I agree with the authors' rebuttal: the fact that even a simple ES algorithm performs well is very promising, and further tuning would only strengthen that result. Nevertheless, it would be useful to assess the algorithm's sensitivity w.r.t. its learning rate / step size.

In summary, I agree with the tie breaking review and recommend acceptance as a poster.",Paper Decision
ZmXK-yH3sC,B1eyA3VFwS,Enforcing Physical Constraints in Neural Neural Networks through Differentiable PDE Layer,Reject,"This paper introduces an FFT-based loss function to enforce physical constraints in a CNN-based PDE solver.  The proposed idea seems sensible, but the reviewers agreed that not enough attention was paid to baseline alternatives, and that a single example problem was not enough to understand the pros and cons of this method.",Paper Decision
Yh9euqKrt,rkeJRhNYDH,TabFact: A Large-scale Dataset for Table-based Fact Verification,Accept (Poster),"This paper presents a new dataset for fact verification in text from tables. The task is to identify whether a given claim is supported by the information presented in the table. The authors have also presented two baseline models, one based on BERT and based on symbolic reasoning which have an ok performance on the dataset but still very behind the human performance. The paper is well-written and the arguments and experiments presented in the paper are sound.

After reviewer comments, the authors have incorporated major changes in the paper. I recommend an Accept for the paper in its current form.",Paper Decision
BCgo9YeOe,B1lC62EKwr,Evidence-Aware Entropy Decomposition For  Active Deep Learning,Reject,"The authors propose a new perspective on active learning by borrowing concepts from subjective logic. In particular, they model uncertainty as a combination of dissonance and vacuity; two orthogonal forms of uncertainty that may invite additional labels for different reasons. The concepts introduced are not specific to deep learning but are generally applicable. Experiments on 2d data and a couple standard datasets are provided.

The derivation of the model is intuitive but it's not clear that it is ""better"" than any other intuitively derived model for active learning. With the field of active learning having such a long history, the field has moved towards a standard of expecting theoretical guarantees to distinguish a new method from the rest; this paper provides none. Instead anecdotal examples and small experiments are performed. Like other reviews, I am extremely skeptical about the use of KDE which is known to have essentially no inferential ability in high dimensions (such as in deep learning situations where presumably images are involved). It is hard not to feel as though deep learning is somewhat of a red herring in this paper. 

I recommend the authors lean into understanding the method from a perspective beyond anecdotes and experiments if they wish for this method to gain traction. ",Paper Decision
OhkRUXxy_2,SylR6n4tPS,Learning to Generate Grounded Visual Captions without Localization Supervision,Reject,"This paper proposes a cyclical training scheme for grounded visual captioning, where a localization model is trained to identify the regions in the image referred to by caption words, and a reconstruction step is added conditioned on this information. This extends prior work which required grounding supervision. 

While the proposed approach is sensible and grounding of generated captions is an important requirement, some reviewers (me included) pointed out concerns about the relevance of this paper's contributions. I found the authors’ explanation that the objective is not to improve the captioning accuracy but to refine its grounding performance without any localization supervision a bit unconvincing -- I would expect that better grounding would be reflected in overall better captioning performance, which seems to have happened with the supervised model of Zhou et al. (2019). In fact, even the localization gains seem rather small: “The attention accuracy for localizer is 20.4% and is higher than the 19.3% from the decoder at the end of training.” Overall, the proposed model is an incremental change on the training of an image captioning system, by adding a localizer component, which is not used at test time. The authors' claim that “The network is implicitly regularized to update its attention mechanism to match with the localized image regions” is also unclear to me -- there is nothing in the loss function that penalizes the difference between these two attentions, as the gradient doesn’t backprop from one component to another. Sharing the LSTM and Language LSTM doesn’t imply this, as the localizer is just providing guidance to the decoder, but there is no reason this will help the attention of the original model. 

Other natural questions left unanswered by this paper are:
- What happens if we use the localizer also in test time (calling the decoder twice)? Will the captions improve? This experiment would be needed to assess the potential of this method to help image captioning.
- Can we keep refining this iteratively?
- Can we add a loss term on the disagreement of the two attentions to actually achieve the said regularisation effect?

Finally, the paper [1] (cited by the authors) seems to employ a similar strategy (encoder-decoder with reconstructor) with shown benefits in video captioning.

[1] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Reconstruction network for video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7622–7631, 2018.

I suggest addressing some of these concerns in a revised version of the paper.",Paper Decision
0TlkzlVy27,Bylp62EKDH,Extreme Triplet Learning: Effectively Optimizing Easy Positives and Hard Negatives,Reject,"The authors propose a novel distance metric learning approach. Reviews were mixed, and while the discussion was interesting to follow, some issues, including novelty, comparison with existing approaches, and impact, remain unresolved, and overall, the paper does not seem quite ready for publication. ",Paper Decision
6Wezr_C6UK,HkgTTh4FDH,Implicit Bias of Gradient Descent based Adversarial Training on Separable Data,Accept (Poster),"This paper provides theoretical guarantees for adversarial training.  While the reviews raise a variety of criticisms (e.g., the results are under a variety of assumptions), overall the paper constitutes valuable progress on an emerging problem.",Paper Decision
MPn_KALmoh,S1l66nNFvB,Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis,Reject,"This paper presents an auxiliary module to boost the representation power of GNNs. The new module consists of virtual supernode, attention unit, and warp gate unit. The usefulness of each component is shown in well-organized experiments.
This is the very borderline paper with split scores. While all reviewers basically agree that the empirical findings in the paper are interesting and could be valuable to the community, one reviewer raised concern regarding the incremental novelty of the method, which is also understood by other reviewers. The impression was not changed through authors’ response and reviewer discussion, and there is no strong opinion to champion the paper. Therefore, I’d like to recommend rejection this time. 
",Paper Decision
nmNvI4YmYa,BJena3VtwS,The Visual Task Adaptation Benchmark,Reject,"The authors present a new benchmark for evaluating a plethora of models on a variety of tasks. In terms of scores, the paper received a borderline rating, with two reviews being rather superficial unfortunately. The last reviewer was positive. The reviewers generally agreed that the benchmark is interesting and carries value, and the AC agrees. Authors certainly invested a significant effort in designing the benchmark and performing a detailed analysis over several tasks and methods. However, the effort seems more engineering in nature and insights are somewhat lacking. For an experimental paper, presenting the results is interesting yet not sufficient. A much more in-depth analysis and discuss would warrant a deeper understanding of the results and open directions for future work. This part is currently underwhelming. ",Paper Decision
-_kc4rdxr,Hyl9ahVFwH,Learning Similarity Metrics for Numerical Simulations,Reject,"The authors present a Siamese neural net architecture for learning similarities among field data generated by numerical simulations of partial differential equations. The goal would be to find which two field data are more similar to each. One use case mentioned is the debugging of new numerical simulators, by comparing them with existing ones. 

The reviewers had mixed opinions on the paper. I agree with a negative comment of all three reviewers that the paper lacks a bit on the originality of the technique and the justification of the new loss proposed, as well as the fact that no strong explicit real world use case was given. I find this problematic especially given that similarities of solutions to PDEs is not a mainstream topic of the conference. Hence a good real world example use of the method would be more convincing.",Paper Decision
qox93zVkjW,Hyg9anEFPS,Image-guided Neural Object Rendering,Accept (Poster),"The paper presents a new variation of neural (re) rendering of objects, that uses a set of two deep ConvNets to model non-Lambertian effects associated with an object. The paper has received mostly positive reviews. The reviewers agree that the contribution is well-described, valid and valuable. The method is validated against strong baselines including Hedman et al., though Reviewer4 rightfully points out that the comparison might have been more thorough. 

One additional concern not raised by the reviewers is the lack of comparison with [Thies et al. 2019], which is briefly mentioned but not discussed. The authors are encouraged to provide a corresponding comparison (as well as additional comparisons with Hedman et al) and discuss pros and cons w.r.t. [Thies et al] in the final version.",Paper Decision
lxm1cSLL,Byx9p2EtDH,MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics,Reject,"The paper considers the case where policies have been learned in several environments - differing only according to their transition functions. The goal is to achieve a policy for another environment on the top of the former policies. The approach is based on learning a state-dependent combination (aggregation) of the former policies, together with a ""residual policy"". On the top of the aggregated + residual policies is defined a Gaussian distribution. The approach is validated in six OpenAI Gym environments. Lesion studies show that both the aggregation of several policies (the more the better, except for the computational cost) and the residual policy are beneficial. 

Quite a few additional experiments have been conducted during the rebuttal period according to the reviewers' demands (impact of the quality of the initial policies; comparing to fine-tuning an existing source policy).

A key issue raised in the discussion concerns the difference between the sources and the target environment. It is understood that ""even a small difference in the dynamics"" can call for significantly different policies. Still, the point of bridging the reality gap seems to be not as close as the authors think, for training the aggregation and residual modules requires hundreds of thousands of time steps - which is an issue in real-world robotics.

I encourage the authors to pursue this promising line of research; the paper would be definitely very strong with a proof of concept on the sim-to-real transfer task.",Paper Decision
TG8eEFB3TH,ryeK6nNFDr,Effective and Robust Detection of Adversarial Examples via Benford-Fourier Coefficients,Reject,"This paper presents a new metric for adversarial attack's detection. The reviewers find the idea interesting, but the some part has not been clearly explained, and there are questions on the reproducibility issue of the experiments. ",Paper Decision
hv0TZQNchU,B1lFa3EFwB,Stablizing Adversarial Invariance Induction by Discriminator Matching,Reject,"The paper proposes a modification to improve adversarial invariance induction for learning representations under invariance constraints. The authors provide both a formal analysis and experimental evaluation of the method. The reviewers generally agree that the experimental evaluation is rigorous and above average, but the paper lacks clarity making it difficult to judge the significance of it. Therefore, I recommend rejection, but encourage the authors to improve the presentation and resubmit.",Paper Decision
Q5AuUWHdt5,SkevphEYPB,POP-Norm: A Theoretically Justified and More Accelerated Normalization Approach,Reject,"The authors propose an alternative to batch norm, which they call POP-norm, and provide theoretical justification for POP-norm in nonconvex optimization on the basis of variance reduction. They then present empirical arguments.

One of the most cogent reviewers believed the theoretical results were known and the empirical arguments unconvincing because the method is similar to batch norm up to a change in learning rate and some minor differences.

Unfortunately, the reviewers did not engage with the author rebuttals at all. The authors seem to have addressed most points. However, if the reviewers are unwilling to engage, despite multiple emails, there's not much I can do, short of redoing the whole process from scratch. And I'll take the lack of engagement as lack of interest by the reviewers. Not being an expert in optimization myself, I'm not going to override the scores. I do know enough to know that there are standard bounds for both convex and nonconvex optimization that improve with decreased variance. ",Paper Decision
PTh1gsfegq,Bkgwp3NtDH,Programmable Neural Network Trojan for Pre-trained Feature Extractor,Reject,"This paper proposes a general framework for constructing Trojan/Backdoor attacks on deep neural networks. The authors argue that the proposed method can support dynamic and out-of-scope target classes, which are particularly applicable to backdoor attacks in the transfer learning setting. This paper has been very carefully discussed. While the idea is interesting and could be of interest to the broader community, all reviewers agree that it lacks of experimental comparison with existing methods for backdoor attacks on benchmark problems. The paper needs to be significantly revised before publication. I encourage the authors to improve this paper and resubmit to future conference.",Paper Decision
GMRR4k39MY,B1x8anVFPr,On Layer Normalization in the Transformer Architecture,Reject,"This paper investigates layer normalization and learning rate warmup in transformers, demonstrating that placing layer norm inside the residual connection (pre-LN) leads to better behaved gradients than post-LN placement. Doing so allows the learning rate warm-up stage to be removed, leading to faster training.

Reviewers were mildly positive about the submission, commenting on the interesting insight provided about transformers, as well as the clear, focused motivation and contribution.

However they also stated that it seem rather incremental of a contribution, as pre-LN placement has been introduced before, and found it confusingly written at times.

R2 clearly read it very closely, and had many detailed comments and discussions with authors and other reviewers. They had concerns about the relationship of this work with gradient clipping. The authors deserve credit for quickly investigating this in further experiments. Interestingly, the found that even with gradient clipping, post-LN models still needed the learning rate warm-up stage, although this issue went away with smaller clipped values or much lower learning rates. Overall, R2 appears to find the paper’s motivation very compelling, but the insights incomplete and not fully satisfactory, while all reviewers find the novelty rather limited.

I think a future submission that forges closer connections between the empirical findings and the theoretical interpretations would be of a great interest to the community, but in its current form is probably unsuitable for publication at ICLR 2020.
",Paper Decision
wiYJqg68l,BJlS634tPr,PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search,Accept (Spotlight),"This paper proposes an improvement to the popular DARTS approach, speeding it up by performing the search in a subset of channels. The improvements are robust, and code is available for reproducibility.

The rebuttal cleared up initial concerns, and after the (private) discussion among reviewers now all reviewers give accepting scores. Because the improvements seem somewhat incremental and only applied to DARTS, R3 argued against an oral, and even the most positive reviewer agreed that a poster format would be best for presentation. 

I therefore strongly recommend recommendation, as a poster.",Paper Decision
AheRSFOxic,BJeS62EtwH,Knowledge Consistency between Neural Networks and Beyond,Accept (Poster),"This paper presents a method for extracting ""knowledge consistency"" between neural networks and  understanding their representations. 

Reviewers and AC are positive on the paper, in terms of insightful findings and practical usages, and also gave constructive suggestions to improve the paper. In particular, I think the paper can gain much attention for ICLR audience.  

Hence, I recommend acceptance.",Paper Decision
SNMQrJU0GW,HygN634KvH,Temporal Probabilistic Asymmetric Multi-task Learning,Reject,"The authors propose a method for multi-task learning with time series data. The reviewers found the paper interesting, but the majority found the description of the method in the paper confusing and several technical details missing. Moreover, the reviewers were not convinced that the technique used for uncertainty quantification of the features at each stage of the time series is well founded.",Paper Decision
5QiVO7XdD1,rJx4p3NYDB,Lazy-CFR: fast and near-optimal regret minimization for extensive games with imperfect information,Accept (Poster),"The paper proposed an regret based approach to speed up counterfactural regret minimization. The reviewers find the proposed approach interesting. However, the method require large memory. More experimental comparisons and comparisons pointed out by reviewers and public comments will help improve the paper. ",Paper Decision
HrLxeynTpG,BJxVT3EKDH,Corpus Based Amharic Sentiment Lexicon Generation,Reject,"This papers addresses the problem of creating sentiment lexicon for a resource limited language (Amharic). This task is time consuming and requires skilled annotators. Hence the authors propose a method for constructing this automatically from News corpora. They start with a seed list of sentiment bearing words and then add new words to this list based on their PPMO scores with existing words. 

While the reviewers agreed that this work is of practical importance, they had a few objections which I have summarised below: 
1) Lack of novelty: The work has very few new ideas
2) Lack of comparison with existing work: Several missing citations have been pointed out by the reviewers
3) Weak experiments: The experimental section needs to be strengthened with more comparisons to existing work as well as proving the results for at least one more language. 
4) Organisation of the paper: The paper needs to be restructured for better presentation. In particular,  the Results and Discussions section does not really contain any discussions.
5) Grammatical errors: Please proofread the paper thoroughly and fix all grammatical and typo errors.

Based on the reviewer comments and lack of any response from the authors, I recommend that the paper in it current form cannot be accepted. 
",Paper Decision
SttNyKHM11,H1lma24tPB,Principled Weight Initialization for Hypernetworks,Accept (Talk),"All the reviewers agreed that this was a sensible application of mostly existing ideas from standard neural net initialization to the setting of hypernetworks.  The main criticism was that this method was used to improve existing applications of hypernets, instead of extending their limits of applicability.",Paper Decision
hqnl9ON83-,BkgXT24tDS,Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks,Accept (Poster),"This paper presents a quantization scheme with the advantage of high computational efﬁciency. The experimental results show that the proposed scheme outperforms SOTA methods and is competitive with the full-precision models. The reviewers initially raised some concerns including baseline ResNet performance,  detailed comparison of the quantization size, and comparison with ResNet50. Authors addressed these concerns in the rebuttal and revised the draft to accommodate the requested items. The reviewers appreciated the revision and find it highly improved. Their overall recommendation is toward accept, which I also support.",Paper Decision
ZahrBJpef,BJxGan4FPB,Transfer Alignment Network for Double Blind Unsupervised Domain Adaptation,Reject,"This paper tackles the problem of how to adapt a model from a source to a target domain when both data is not available simultaneously (even unlabeled) to a single learner. This is of relevance for certain privacy preserving applications where one setting would like to benefit from information learned in a related setting but due to various factors may not be willing to directly share data. The proposed solution is a transfer alignment network (TAN) which consists of two autoencoders (each trained independently on the source and the target) and an aligner which has the task of mapping the latent codes of one domain to the other. 

All three reviewers expressed concerns for this submission. Of greatest concern was the experimental setting. The datasets chosen were non-standard and there was no prior work to compare against directly so the results presented are difficult to contextualize. The authors have responded to this concern by specifying the existing domain adaptation benchmarks are more challenging and require more complex architectures to handle the “more complex data manifolds”. The fact that existing benchmark datasets may be more complex the the dataset explored in this work is a concern. The authors should take care to clarify whether their proposed solution may only be applicable to specific types of data. In addition, the authors claim to address a new problem setting and therefore cannot compare directly to existing work. One suggestion is if using new data, report performance of existing work under the standard setting to give readers some grounding for the privacy preserving setting. Another option would be to provide scaffold results in the standard UDA setting but with frozen feature spaces. Another option would be to ablate the choice of L2 loss for learning the transformer and instead train using an adversarial loss, L1 loss etc. There are many ways the authors could both explore a new problem statement and provide convincing experimental evidence for their solution. The AC encourages the authors to revise their manuscript, paying special attention to clarity and experimental details in order to further justify their proposed work. ",Paper Decision
_StETeCIc8,H1gza2NtwH,Towards understanding the true loss surface of deep neural networks using random matrix theory and iterative spectral methods,Reject,"The reviewers all appreciated the importance of the topic: understanding the local geometry of loss surfaces of large models is viewed as critical to understand generalization and design better optimization methods.

However, reviewers also pointed out the strength of the assumptions and the limitations of the empirical study. Despite the claim that these assumptions are weaker than those made in prior work, this did not convince the reviewers that the conclusion could be applied to common loss landscapes.

I encourage the authors to address the points made by the reviewers and submit an updated version to a later conference.",Paper Decision
Dd67Q5P5gX,HylWahVtwB,Neural Architecture Search in Embedding Space,Reject,"This paper proposes a method for neural architecture search in embedding space. This is an interesting idea, but its novelty is limited due to its similarity to the NAO approach. Also, the empirical evaluation is too limited; comparisons should have been performed to NAO and other contemporary NAS methods, such as DARTS. 

Due the factors above, all reviewers gave rejecting scores (3,3,1). The rebuttal did not remove the main issues, resulting in the reviewers sticking to their scores. I therefore recommend rejection.",Paper Decision
7rDBEs7On6,BkgWahEFvr,Enhancing Transformation-Based Defenses Against Adversarial Attacks with a Distribution Classifier,Accept (Poster),"This paper investigates tradeoffs between preserving accuracy on clean samples and increasing robustness on adversarial samples by using transformations and majority votes. Observations on the distribution of the induced softmax show that existing methods could be improved by leveraging information from that distribution to correct predictions, as confirmed by experiments.
The problem space is important and reviewers find the approach interesting. Authors have provided some necessary clarifications during rebuttal and additional experiments. While some reservations remain, this paper's premise and its experimental results appear sufficiently interesting to justify an acceptance recommendation.",Paper Decision
sLUhqC63w4,BJleph4KvS,HaarPooling: Graph Pooling with Compressive Haar Basis,Reject,"This paper presents a new graph pooling method, called HaarPooling. Based on the hierarchical HaarPooling, the graph classification problems can be solved under the graph neural network framework.

One major concern of reviewers is the experiment design. Authors add a new real world dataset in revision. Another concern is computational performance. The main text did not give a comprehensive analysis and the rebuttal did not fully address these problems.

Overall, this paper presents an interesting graph pooling approach for graph classification while the presentation needs further polish. Based on the reviewers’ comments, I choose to reject the paper.
",Paper Decision
x9fTmuOQEK,HkxeThNFPH,Safe Policy Learning for Continuous Control,Reject,"The paper is about learning policies in RL while ensuring safety (avoid constraint violations) during training and testing. 

For this meta review, I ignore Reviewer #3 because that review is useless. The discussion between the authors and Reviewer #1 was useful.

Overall, the paper introduces an interesting idea, and the wider context (safe learning) is very relevant. However, I also have some concerns.
One of my biggest concerns is that the method proposed here relies heavily on linearizations to deal with nonlinearities. However, the fact that this leads to approximation errors is not being acknowledged much. There are also small things, such as the (average) KL divergence between parameters, which makes no sense to me because the parameters don't have distributions (section 3.1). 

In terms of experiments, I appreciate that the authors tested the proposed method on multiple environments. The results, however, show that safety cannot be guaranteed. For example, in Figure 1(c), SDDPG clearly violates the constraints. The figures are also misleading because they show the summary statistics of the trajectories (mean and standard deviation). If we were to look at individual trajectories, we would find trajectories that violate the constraints. This fact is brushed under the carpet in the evaluation, and the paper even claims that ""our algorithms quickly stabilize the constraint cost below the threshold"". This may be true on average, but not for all trajectories. A more careful analysis and a more honest discussion would have been useful. In the robotics experiment, I would like to understand why we allow for any collisions. Why can't we set $d_0=0$, thereby disallowing for collisions. The threshold in the paper looks pretty arbitrary.  Again, the paper states that  ""Figure 4a and Figure 4b show that the Lyapunov-based PG algorithms have higher success rates"". This is a pretty optimistic interpretation of the figure given the size of the error bars. 

There are some points in the conclusion, I also disagree with:
1) ""achieve safe learning"": Given that some trajectories violate the constraints, ""safe"" is maybe a bit of an overstatement
2) ""better data efficiency"": compared to what?
3) ""scalable to tackle real-world problems"": I disagree with this one as well because for all experiments you will need to run an excessive number of trials, which will not be feasible on a real-world system (assuming we are talking about robots).

Overall, I think the paper has some potential, but it needs some more careful theoretical analysis (e.g., effect of linearization errors) and some better empirical analysis. 

Additionally, given that the paper is at around 9 pages (including the figures in the appendix, which the main paper cites), we are supposed to have higher standards on acceptance than an 8-pages paper.

Therefore, I recommend to reject this paper.",Paper Decision
fWlGYKe2B,HkxJpnVtPr,A Stochastic Trust Region Method for Non-convex Minimization,Reject,"This paper proposes a stochastic trust region method for local minimum finding based on variance reduction, which achieves better oracle complexities than some of the previous work. This is a borderline paper and has been carefully discussed. The main concern of the reviewers is that this paper falls short of proper experiment evaluation to support their theoretical analysis. In detail, the authors proved a globally sublinear convergence rate to a local minimum, yet the experiments demonstrate a linear or even quadratic convergence starting from the initialization. There is a big gap between the theoretical analysis and experiments, which is probably due to the experimental design. In addition, the authors did not submit a revision during the author response (while it is optional), so it is unclear whether a major revision is required to address all the reviewers’ comments. In fact, one reviewer thinks that a major revision is needed. I agree with the reviewers’ evaluation and encourage the authors to improve this paper before future submission.",Paper Decision
GYF--OXJ-M,Bkgk624KDB,Learning Effective Exploration Strategies For Contextual Bandits,Reject,"This paper introduces MELEE, a meta-learning procedure for contextual bandits. In particular, MELEE learns how to explore by training on datasets with full-information about what every reward each action would obtain (e.g., using classification datasets). The idea is strongly related to imitation learning, and a regret bound is demonstrated for the procedure that comes from that literature. Experiments are performed. 

Perhaps due to the generality in which the algorithm was presented, reviewers found some parts of the work unintuitive and difficult to follow. The work may greatly benefit from having an explicit running example for F and pi and how it evolves during training. Some reviewers were not impressed by the experimental results relative to epsilon-greedy. Yes, epsilon-greedy is a strong baseline, but MELEE introduces significant technical debt and data infrastructure so it seems fair to expect a sizable bump over epsilon-greedy or else why is it worth it?

Perhaps with revisions and experiments within a domain that justify its complexity, this paper may be suitable at another venue. But it is not deemed acceptable at this time, Reject.  ",Paper Decision
y74Ry1hL2,ryx0nnEKwH,Improving Batch Normalization with Skewness Reduction for Deep Neural Networks,Reject,The paper proposes a novel mechanism to reduce the skewness of the activations. The paper evaluates their claims on the CIFAR-10 and Tiny Imagenet dataset. The reviewers found the scale of the experiments to be too limited to support the claims. Thus we recommend the paper be improved by considering larger datasets such as the full Imagenet. The paper should also better motivate the goal of reducing skewness.,Paper Decision
Z39eSiMEp-,ryeRn3NtPH,Adversarial Inductive Transfer Learning with input and output space adaptation,Reject,"The paper proposes an adversarial inductive transfer learning method that handles distribution changes in both input and output spaces.

While the studied problem is interesting, reviewers have major concerns about the incremental modeling contribution, the lack of comparative study to existing methods and ablation study to disentangling different modules. Overall, the current study is less convincing from either theoretical analysis or experimental results.

Hence I recommend rejection.",Paper Decision
rZ7A9Zmysh,Hkgpnn4YvH,Graph Neural Networks For Multi-Image Matching,Reject,"The paper proposes a method for learning multi-image matching using graph neural networks. The model is learned by making use of cycle consistency constraints and geometric consistency, and it achieves a performance that is comparable to the state of the art. While the reviewers view the proposed method interesting in general, they raised issues regarding the evaluation, which is limited in terms of both the chosen datasets and prior methods. After rounds of discussion, the reviewers reached a consensus that the submission is not mature enough to be accepted for this venue at this time. Therefore, I recommend rejecting this submission.",Paper Decision
HRmOKgprCB,Byla224KPr,An Empirical Study on Post-processing Methods for Word Embeddings,Reject,"This paper explores a post-processing method for word vectors to ""smooth the spectrum,"" and show improvements on some downstream tasks. 

Reviewers had some questions about the strength of the results, and the results on words of differing frequency. The reviewers also have comments on the clarity of the paper, as well as the exposition of some of the methods.

Also, for future submissions to ICLR and other such conferences, it is more typical to address the authors comments in a direct response rather than to make changes to the document without summarizing and pointing reviewers to these changes. Without direction about what was changed or where to look, there is a lot of burden being placed on the reviewers to find your responses to their comments.",Paper Decision
N49tMPeeBF,B1l6nnEtwr,AN EFFICIENT HOMOTOPY TRAINING ALGORITHM FOR NEURAL NETWORKS,Reject,"The work proposes to learn neural networks using a homotopy-based continuation method. Reviewers found the idea interesting, but the manuscript poorly written, and lacking in experimental results. With no response from the authors, I recommend rejecting the paper.",Paper Decision
hWjik-_Iw,HyxnnnVtwB,High performance RNNs with spiking neurons,Reject,"This paper presents a new mechanism to train spiking neural networks that is more suitable for neuromorphic chips.
While the text is well written and the experiments provide an interesting analysis, the relevance of the proposed neuron models to the ICLR/ML community seems small at this point. My recommendation is that this paper should be submitted to a more specialised conference/workshop dedicated to hardware methods.",Paper Decision
YrUjsMc6Cq,SJl3h2EYvS,CLAREL: classification via retrieval loss for zero-shot learning,Reject,"This paper demonstrates that per-image semantic supervision, as opposed to class-only supervision, can benefit zero-shot learning performance in certain contexts.  Evaluations are conducted using CUB and FLOWERS fine-grained zero-shot data sets.  In terms of evaluation, the paper received mixed final scores (two reject, one accept).

During the rebuttal period, both reject reviewers considered the author responses, but in the end did not find the counterarguments sufficiently convincing.  For example, one reviewer maintained that in its present form, the paper appeared too shallow without additional experiments and analyses to justify the suitability of the contrastive loss used for obtaining embeddings applied to zero-shot learning.  Another continued to believe post-rebuttal that reference Reed et al., (2016) undercut the novelty of the proposed approach.

And consistent with these sentiments, even the reviewer who voted for acceptance alluded to the limited novelty of the proposed approach; however, the author response merely states that a future revision will clarify the novelty.  But this then requires another round of reviewing to determine whether the contribution is sufficiently new, especially given that all reviewers raised this criticism in one way or another.  Furthermore, the rebuttal also mentions the inclusion of some additional experiments, but again, we don't know how these will turn out.

Based on these considerations then, the AC did not see sufficient justification for accepting a paper with aggregate scores that are otherwise well below the norm for successful ICLR submissions.",Paper Decision
qqm61SYQdh,HJli2hNKDH,Observational Overfitting in Reinforcement Learning,Accept (Poster),The paper proposes a way to analyze overfitting to non-relevant parts of the state space in RL and proposes a framework to measure this type of generalization error. All reviewers agree that the formulation is interesting and useful for practical RL.,Paper Decision
0MJxGt3Yv2,rkxoh24FPH,On Mutual Information Maximization for Representation Learning,Accept (Poster),"This paper exams the role of mutual information (MI) estimation in representation learning. Through experiments, they show that the large MI is not predictive of downstream performance, and the empirical success of  methods like InfoMax may be more attributed to the inductive bias in  the choice of architectures of discriminators, rather than accurate MI estimation. The work is well appreciated by the reviewers. It forms a strong contribution and may motivate subsequent works in the field. 
",Paper Decision
LnBDFM0v-D,Bkxonh4Ywr,Localizing and Amortizing: Efficient Inference for Gaussian Processes,Reject,"This paper presents a method for speeding up Gaussian process inference by leveraging locality information through k-nearest neighbours. 

The key idea is well-motivated intuitively, however the way in which it is implemented seems to introduce new complications. One such issue is KNN overhead in high dimensions, but R1 outlines other potential issues too. Moreover, the method's merit is not demonstrated in a convincing way through the experiments. The authors have provided a rebuttal for those issues, but it does not seem to solve the concerns entirely.
",Paper Decision
LANN56zuC6,BJe932EYwS,PNAT: Non-autoregressive Transformer by Position Learning,Reject,"This paper presents a non-autoregressive NMT model which predicts the positions of the words to be produced as a latent variable in addition to predicting the words. This is a novel idea in the field of several other papers which are trying to do similar things, and obtains good results on benchmark tasks. The major concerns are systematic comparisons with the FlowSeq paper which seems to have been published before the ICLR submission deadline. The reviewers are still not convinced by the empirical performance comparison as well as speed comparisons. With some more work this could be a good contribution. As of now, I am recommending a Rejection.",Paper Decision
CZ_w9ys_63,S1x522NFvS,On unsupervised-supervised risk and one-class neural networks,Reject,"This paper makes a connection between one-class neural networks and the unsupervised approximation of the binary classifier risk under the hinge loss. An important contribution of the paper is the algorithm to train a binary classifier without supervision by using class prior and the hypothesis that class conditional classifier scores have normal distribution. The technical contribution of the paper is novel and brings an increased understanding into one-class neural networks. The equations and the modeling present in the paper are sound and the paper is well-written.

However, in its current form, as pointed out by the reviewers, the experimental section is rather weak and can be substantially improved by adding extra experiments as suggested by reviewers #1, #2. Since its submission the paper has not yet been updated to incorporate these comments. Thus, for now, I recommend rejection of this paper, however on improvements I'm sure it can be a good contribution in other conferences.",Paper Decision
J3iKVKxIu,BJeKh3VYDH,Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds,Accept (Spotlight),"This paper provides an improved method for deep learning on point clouds.  Reviewers are unanimous that this paper is acceptable, and the AC concurs. ",Paper Decision
lBe-95xSYW,HJlF3h4FvB,Distillation $\approx$ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized NN,Reject,"This paper tries to bridge early stopping and distillation.

1) In Section 2, the authors empirically show more distillation effect when early stopping.
2) In Section 3, the authors propose a new provable algorithm for training noisy labels.

In the discussion phase, all reviewers discussed a lot. In particular, a reviewer highlights the importance of Section 3. On the other hand, other reviewers pointed out ""what is the role of Section 2"", as the abstract/intro tends to emphasize the content of Section 2.

I mostly agree all pros and cons pointed out by reviewers. I agree that the paper proposed an interesting idea for refining noisy labels with theoretical guarantees. However, the major reason for my reject decision is that the current write-up is a bit below the borderline to be accepted considering the high standard of ICLR, e.g., many typos (what is the172norm in page 4?) and misleading intro/abstract/organization. In overall, it was also hard for me to read the paper. I do believe that the paper should be much improved if the authors make more significant editorial efforts considering a more broad range of readers. 

I have additional suggestions for improving the paper, which I hope are useful.

* Put Section 3 earlier (i.e., put Section 2 later) and revise intro/abstract so that the reader can clearly understand what is the main contribution. 
* Section 2.1 is weak to claim more distillation effect when early stopping. More experimental or theoretical study are necessary, e.g., you can control temperature parameter T of knowledge distillation to provide the ""early stopping"" effect without actual ""early stopping"" (the choice of T is not mentioned in the draft as it is the important hyper-parameter).
* More experimental supports for your algorithm should be desirable, e.g., consider more datasets, state-of-the-art baselines, noisy types, and neural architectures (e.g., NLP models).
* Softening some sentences for avoiding some potential over-claims to some readers.
",Paper Decision
plYOp0wfL,rklFh34Kwr,Bayesian Inference for Large Scale Image Classification,Reject,"This paper proposes a variant of Hamiltonian Monte Carlo for Bayesian inference in deep learning.

Although the reviewers acknowledge the ambition, scope and novelty of the paper they still have a number of reservations regarding experimental results and claims (regarding need for hyperparameter tuning). The overall score consequently falls below acceptance.

Rejection is recommended. These reservations made by the referees should definitely be addressable before next conference deadline so looking forward to see the paper published asap.",Paper Decision
cHjF4ozOIG,rJld3hEYvS,Ranking Policy Gradient,Accept (Poster),"The paper introduces a novel and effective approach to policy optimization.  The overall contribution is sufficient to merit acceptance.  Nevertheless, the authors should improve the presentation and experimental evaluation in line with the reviewer criticisms.  The criticisms of AnonReviewer2 in particular should not be neglected.  Regarding the theory, I agree with AnonReviewer3 that the UNOP assumption is too limiting.  The paper would be much stronger if this assumption could be significantly weakened, or better justified.",Paper Decision
VIOBObL07F,r1eOnh4YPB,How Does Learning Rate Decay Help Modern Neural Networks?,Reject,"This paper seeks to understand the effect of learning rate decay in neural net training. This is an important question in the field and this paper also proposes to show why previous explanations were not correct. However, the reviewers found that the paper did not explain the experimental setup enough to be reproducible. Furthermore, there are significant problems with the novelty of the work due to its overlap with works such as (Nakiran et al., 2019), (Li et al. 2019) or (Jastrzębski et al. 2017).",Paper Decision
pkanhS4rLC,rkgPnhNFPB,Random Matrix Theory Proves that Deep Learning Representations of GAN-data Behave as Gaussian Mixtures,Reject,"The paper theoretically shows that the data (embedded by representations learned by GANs) are essentially the same as a high dimensional Gaussian mixture. The result is based on a recent result from random matrix theory on the covariance matrix of data, which the authors extend to a theorem on the Gram matrix of the data. The authors also provide a small experiment comparing the spectrum and principle 2D subspace of BigGAN and Gaussian mixtures, demonstrating that their theorem applies in practice.

Two of the reviews (with confident reviewers) were quite negative about the contributions of the paper, and the reviewers unfortunately did not participate in the discussion period.

Overall, the paper seems solid, but the reviews indicate that improvements are needed in the structure and presentation of the theoretical results. Given the large number of submissions at ICLR this year, the paper in its current form does not pass the quality threshold for acceptance.",Paper Decision
YLchM1IawG,r1xPh2VtPB,SVQN: Sequential Variational Soft Q-Learning Networks,Accept (Poster),"The paper proposes a novel model-free solution to POMDPs, which proposes a unified graphical model for hidden state inference and max entropy RL. The method is principled and provides good empirical results on a set of experiments that relatively comprehensive. I would have liked to see more POMDP tasks instead of Atari, but the results are good. Overall this is good work.",Paper Decision
OVEpOfB95M,B1gUn24tPr,Classification Attention for Chinese NER,Reject,"The paper is interested in Chinese Name Entity Recognition, building on a BERT pre-trained model. All reviewers agree that the contribution has limited novelty. Motivation leading to the chosen architecture is also missing. In addition, the writing of the paper should be improved.
",Paper Decision
dO52g5MhP,rJlUhhVYvS,Understanding Isomorphism Bias in Graph Data Sets ,Reject,"Thanks to reviewers and authors for an interesting discussion. It seems the central question is whether learning to identify correct bijections should be part of graph classification problems, or whether this leads to bias and overfitting. Reviews are generally negative, putting this in the lower third of the submissions. The paper, however, inspired an interesting discussion, and I would encourage the authors to continue this line of work, addressing the question of bias and overfitting more directly, possibly going beyond dataset evaluation and, for example, thinking about how to evaluate whether training on non-isomorphic graphs leads to better off-training set generalization.",Paper Decision
VXQUH7whHs,Byl8hhNYPS,Neural Machine Translation with Universal Visual Representation,Accept (Spotlight),"This paper proposes using visual representations learned in a monolingual setting with image annotations into machine translation. Their approach obviates the need to have bilingual sentences aligned with image annotations, a very restricted resource. An attention layer allows the transformer to incorporate a topic-image lookup table. Their approach achieves significant improvements over strong baselines. The reviewers and the authors engaged in substantive discussions. This is a strong paper which should be included in ICLR. 

",Paper Decision
dN57YMIaym,rJeB22VFvS,Towards More Realistic Neural Network Uncertainties,Reject,"This paper proposes two contributions to improve uncertainty in deep learning.  The first is a Mahalanobis distance based statistical test and the second a model architecture.  Unfortunately, the reviewers found the message of the paper somewhat confusing and particularly didn't understand the connection between these two contributions.   A major question from the reviewers is why the proposed statistical test is better than using a proper scoring rule such as negative log likelihood.  Some empirical justification of this should be presented.",Paper Decision
ZbvR6yXQeg,BJxH22EKPS,Understanding Architectures Learnt by Cell-based Neural Architecture Search,Accept (Poster),"The paper reports interesting NAS patterns, supported by empirical and theoretical evidence that the pattern arises due to smooth loss landscape. Reviewers generally agree the this paper would be of interest for the NAS researchers. Some questions raised by reviewers are answered by authors with a few more extra experiments. We highly recommend authors to carefully reflect on reviewers both pros and cons of the paper before camera ready.
",Paper Decision
-REXrkTfGa,SJlNnhVYDr,Soft Token Matching for Interpretable Low-Resource Classification,Reject,"The authors focus on low-resource text classifications tasks augmented with ""rationales"". They propose a new technique that improves performance over existing approaches and that allows human inspection of the learned weights.

Although the reviewers did not find any major faults with the paper, they were in consensus that the paper should be rejected at this time. Generally, the reviewers' reservations were in terms of novelty and extent of technical contribution.

Given the large number of submissions this year, I am recommending rejection for this paper.
",Paper Decision
Q8Vy0M0hUb,r1gV3nVKPS,Beyond Classical Diffusion: Ballistic Graph Neural Network,Reject,"This submission has been assessed by three reviewers who scored it 3/1/3, and they have remained unconvinced after the rebuttal. The main issues voiced are the difficult readability of the paper, cryptic at times due to a mix of physical and DL notations, and a lack of sufficient experimentation to support all claims. The reviewers acknowledge the authors' efforts to resolve the main issues but find these efforts insufficient. Thus, this paper cannot be accepted to ICLR2020.",Paper Decision
-iSzuKbPdb,BJe7h34YDS,Understanding and Stabilizing GANs' Training Dynamics with Control Theory,Reject,"This paper suggests stabilizing the training of GANs using ideas from control theory. The reviewers all noted that the approach was well-motivated and seemed convinced that that the problem was a worthwhile one. However, there were universal concerns about the comparisons with baselines and performance over previous works on Stabilizing GAN training and the authors were not able to properly address them.",Paper Decision
VhH7thgG2,S1lXnhVKPr,Variance Reduced Local SGD with Lower Communication Complexity,Reject,The paper presents a novel variance reduction algorithm for SGD. The presentation is clear. But the theory is not good enough. The reivewers worry about the converge results and the technical part is not sound.,Paper Decision
sXLQ2RJVrP,rygfnn4twS,AutoQ: Automated Kernel-Wise Neural Network Quantization ,Accept (Poster),"This paper proposes a network quantization method which is based on kernel-level quantization. The extension from layer-level to kernel-level is straightforward, and so the novelty is somewhat limited given its similarity with HAQ. Nevertheless, experimental results demonstrate its efficiency in real applications. The paper can be improved by clarifying some experimental details, and have further discussions on its relationship with HAQ.",Paper Decision
S8EnleS94,SkxW23NtPH,GDP: Generalized Device Placement for Dataflow Graphs,Reject,"This paper presents a new reinforcement learning based approach to device placement for operations in computational graphs and demonstrates improvements for large scale standard models.

The paper is borderline with all reviewers appreciating the paper even the reviewer with the lowest score. The reviewer with the lowest score is basing the score on minor reservation regarding lack of detail in explaining the experiments.

Based upon the average score rejection is recommended. The reviewers' comments can help improve the paper and it is definitely recommended to submit it to the next conference.

",Paper Decision
NiXjfjM_ff,H1ebhnEYDH,White Noise Analysis of Neural Networks,Accept (Spotlight),All the reviewers found the paper to contain an interesting idea with insightful experiments. The rebuttal further improved confidence of the reviewers. The paper is accepted.,Paper Decision
dYBT7SGqoB,HyxehhNtvS,Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization,Reject,"This paper studies the problem of optimization for neural networks, by comparing the optimization problem in parameter space with the corresponding problem in function space. It argues that overparametrised models leads to a convex problem formulation leading to global optimality. 

All reviewers agreed that this paper lacks mathematical rigor and novelty relative to the current works on overparametrised neural networks. Its arguments need to be substantially reworked before it can be considered for publication, and as a consequence the AC recommends rejection. ",Paper Decision
jxi4e-JIVt,SkxgnnNFvH,Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring,Accept (Poster),"The paper presents a new architecture that achieves the advantages of both Bi-encoder and Cross-encoder architectures. The proposed idea is reasonable and well-motivated, and the paper is clearly written. The experimental results on retrieval and dialog tasks are strong, achieving high accuracy while the computational efficiency is orders of magnitude smaller than Cross-encoder. All reviewers recommend acceptance of the paper and this AC concurs.",Paper Decision
94E-wXPD4z,HJxJ2h4tPr,HighRes-net: Multi-Frame Super-Resolution by Recursive Fusion,Reject,"This paper proposes a multi-frame super-resolution method including recursive fusion for co-registration and registration loss to solve the problem where the super-resolution results and the high-resolution labels are not pixel-wise aligned. While reviewer #1 is positive about this paper, reviewer #2 and #3 rated weak reject and reject respectively. Both reviewer #2 and #3 have extensive experience in the topic of image super-resolution. The major concerns raised by the reviewers include the lack of many references, the comparison of recursive fusion with related work, limited test databases, using a single translational motion for the SR images, and limited novelty on the network modules.  The authors provided detailed response to the concerns, however they did not change the overall rating of the reviewers. While the ACs agree that this work has merits, given the various concerns raised by the reviewers, this paper can not be accepted at its current state.",Paper Decision
SvrhL6S1nD,BJe1334YDH,A Learning-based Iterative Method for Solving Vehicle Routing Problems,Accept (Poster),"   The paper proposed the use of a combination of RL-based iterative improvement operator to refine the solution progressively for the capacitated vehicle routing problem. It has been shown to outperform both classical non-learning based and SOTA learning based methods. The idea is novel and the results are impressive, the presentation is clear. Also the authors addressed the concern of lacking justification on larger tasks by including an appendix of additional experiments. ",Paper Decision
wCdCtZ9WQ_,rJxAo2VYwr,Transferable Perturbations of Deep Feature Distributions,Accept (Poster),"This paper considers black box adversarial attacks based on perturbations of the intermediate layers of a neural network classifier, obtained by training a binary classifier for each target class.

Reviewers were happy with the novelty of the approach as well as the presentation, described the presentation as rigorous and were pleased with the situation of this method relative to the literature. R3 had concerns about evaluation, success rate, and that the procedure was ""cumbersome"".  Some of their concerns were addressed in rebuttal, but remained steadfast that the method was too cumbersome to be practical.

I agree with R1 & R2 that this approach is novel and interesting and disagree with R3 that it is too impractical. The paper could be stronger with the addition of adversarial training experiments (and I disagree with the authors that ""there are currently no whitebox attacks that do well at attacking AT models"", this is very much not the case), but I concur with R1 & R2 that this is interesting work that may stimulate further exploration, enough so to warrant acceptance.",Paper Decision
7nWDfJUor,BJlRs34Fvr,Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets,Accept (Spotlight),"This paper makes the observation that, by adjusting the ratio of gradients from skip connections and residual connections in ResNet-family networks in a projected gradient descent attack (that is, upweighting the contribution of the skip connection gradient), one can obtain more transferable adversarial examples. This is evaluated empirically in the single-model black box transfer setting, against a wide range of models, both with and without countermeasures.

Reviewers praised the novelty and simplicity of the method, the breadth of empirical results, and the review of related work. Concerns were raised regarding a lack of variance reporting, strength of the baselines vs. numbers reported in the literature,  and the lack of consideration paid to the threat model under which an adversary employs an ensemble of source models, as well as the framing given by the original title and abstract. All of these appear to have been satisfactorily addressed, in a fine example of what ICLR's review & revision process can yield. It is therefore my pleasure to recommend acceptance.",Paper Decision
2bHGAFc_A-,Hyepjh4FwB,ProtoAttend: Attention-Based Prototypical Learning,Reject,"This paper proposes an interpretable machine learning method, ProtoAttend, that bases decisions on few relevant ""prototypes."" The proposed method uses an attention mechanism (possibly sparse, via sparsemax) that relates the encoded representations to samples in order to determine prototypes. The resulting model enables similarity-based interpretability, confidence estimation by quantifying the mismatch across prototype labels, and can be used for distribution mismatch detection. 

While the proposed model is interesting, the reviewers raised several concerns regarding the choice of prototypes and the evaluation of human interoperation. The paper would benefit from more experiments besides the provided user studies to check if the provided prototypes can help human users correctly guess the model prediction. I encourage the authors to address these suggestions in a future resubmission.",Paper Decision
ANNM3kncdg,HJeTo2VFwH,A Signal Propagation Perspective for Pruning Neural Networks at Initialization,Accept (Spotlight),"This is a strong submission, and I recommend acceptance. The idea is an elegant one: sparsify a network at initialization using a distribution that achieves approximate orthogonality of the Jacobian for each layer. This is well motivated by dynamical isometry theory, and should imply good performance of the pruned network to the extent that the training dynamics are explainable in terms of a linearization around the initial weights. The paper is very well written, and all design decisions are clearly motivated. The experiments are careful, and cleanly demonstrate the effectiveness of the technique. The one shortcoming is that the experiments don't use state-of-the-art modern architectures, even thought that ought to have been easy to try. The architectures differ in ways that could impact the results, so it's not clear to what extent the same principles describe SOTA neural nets. Still, this is overall a very strong submission, and will be of interest to a lot of researchers at the conference.

",Paper Decision
_RbuvlBtnG,rkl2s34twS,Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution,Reject,"The authors proposed a new problem setting called Wildly UDA (WUDA) where the labels in the source domain are noisy. They then proposed the ""butterfly"" method, combining co-teaching with pseudo labeling and evaluated the method on a range of WUDA problem setup. In general, there is a concern that Butterfly as the combination between co-teaching and pseudo labeling is weak on the novelty side. In this case the value of the method can be assessed by strong empirical result.  However as pointed out by Reviewer 3, a common setup (SVHN<-> MNIST) that appeared in many UDA paper was missing in the original draft. The author added the result for SVHN<-> MNIST  as  a response to review 3, however they only considered the UDA setting, not WUDA, hence the value of that experiment was limited. In addition, there are other UDA methods that achieve significantly better performance on SVHN<->MNIST that should be considered among the baselines. For example DIRT-T (Shu et al 2018) has a second phase where the decision boundary on the target domain is adjusted, and that could provide some robustness against a decision boundary affected by noise.

Shu et al (2018) A DIRT-T Approach to Unsupervised Domain Adaptation. ICLR 2018. https://arxiv.org/abs/1802.08735

I suggest that the authors consider performing the full experiment with WUDA using SVHN<->MNIST, and also consider the use of stronger UDA methods among the baseline. ",Paper Decision
dYr0Gp9aYo,Sye2s2VtDr,Automatically Learning Feature Crossing from Model Interpretation for Tabular Data,Reject,"The authors propose a simple but effective method for feature crossing using interpretation inconsistency (as defined by the authors).

I think this is a good work and the authors as well as the reviewers participated well in the discussions. However, there is still disagreement about the positioning of the paper. In particular, all the reviewers  felt that additional baselines should be tried. While the authors have strongly rebutted the necessity of these baselines the reviewers are not convinced about it. Given the strong reservations of the all the 3 reviewers at this point I cannot recommend the acceptance of this paper. I strongly suggest that in subsequent submissions the authors should position their work better and perhaps compare with some of the related works recommended by the reviewers.",Paper Decision
zoRa96wO-F,Hklso24Kwr,Continual Learning with Adaptive Weights (CLAW),Accept (Poster),"The paper proposes a new variational-inference-based continual learning algorithm with strong performance.

There was some disagreement in the reviews, with perhaps the one shared concern being the complexity of the proposed method. One reviewer brought up other potentially related work, but this was convincingly rebutted by the authors. Finally, one reviewer had an issue with the simplicity with the networks in the experiments, but the authors rightly pointed out that the architectures were simply designed to match those from the baselines.

Continual learning has been an active area for quite some time and convincingly achieving SOTA in a new way is a strong contribution, and will be of interest to the community. Progress in a field is sometimes made by iteratively simplifying an initially complex solution, and this work lays in a brick in that direction. For these reasons, I recommend acceptance.



",Paper Decision
ZgP0xuLbI,Skg9jnVFvH,Progressive Upsampling Audio Synthesis via Effective Adversarial Training,Reject,"Inspired by WaveGAN, this paper proposes a PUGAN to synthesizes high-quality audio in a raw waveform. The paper is well motivated. But all the reviewers find that the paper is lack of clarity and details, and there are some problems in the experiments.",Paper Decision
g0T-7bVtWH,Syl5o2EFPB,Learning Compact Reward for Image Captioning,Reject,"The paper proposed a refined AIRL method to deal with the reward ambiguity problem in image captioning, wherein the main idea is to refine the loss function in word level instead in sentence level, and introduce a conditional term in the loss function to mitigate mode collapse problem.  The results show the proposed method improves the performance and achieves state-of-the-art performance.  However there are concerns from the reviewers that the motivation of the work was not well explained and some inprecise parts exist in the paper.  The concept of ""reward ambiguity problem"" is not properly addressed according the opinion of reviewer2.  I would like to see these concerns be well addressed before the paper can be accepted.  ",Paper Decision
YMdjzSlBl,BJxqohNFPB,S-Flow GAN,Reject,"The submission proposes a new GAN-based method for translating from semantic maps of (synthetic) images/videos (from computer graphics) to photo-realistic images/videos with the aid of edge maps. The main innovation is the inclusion of edge maps to the generator, where the edge maps are initially computed using the spatial Laplacian operator, and later output from their DNED network. According to the authors, the edge map allows them to generate images with fine details and to generate output images at higher resolutions.  The authors use their method to generate both single images as well as videos. 

The submission received relatively low scores (2 rejects and 1 weak reject).  This was unchanged after the rebuttal (the authors did not submit a revised version of their paper).  The reviewers voiced concerns about the following:
1. Limited novelty
All of the reviewers indicated that they felt the novelty of the proposed approach of not high as the work seemed to make only small modifications on prior work.  In the author response, the authors provided some details on where they felt their innovation to be.  The paper can be improved by building on those and having experiments/examples to probe those claims in more detail. 

2. Application to other datasets
The proposed method is demonstrated only on two datasets of driving scenarios (Cityscapes and Synthia).  It is unclear how the method will generalize to other types of inputs.  Experiments on other datasets will demonstrate whether the proposed approach can work well for other types of images.

3. The overall quality of the writing.  
The overall quality of the writing is poor and hard to follow in places. The paper should also include more discussion of domain adaptation in the related work section.  It's possible that with improved writing that situates the work and explains the novel aspects of the work better, that the concern about limited novelty will be partially alleviated.   The paper also needs an editing pass as there are many grammar/spelling/capitalization issues.

Page 2: ""We make Three"" --> ""We make three""
Page 3: ""as can bee seen in fig 3.2"" --> ""as can be seen in ..."" (it's unclear which figure ""fig 3.2"" refers to, as figures are labeled Figure 1, Figure 2, etc)
Page 4: Equation (3), symbol e is not explained (it is presumably the edge map)
Page 7: ""bellow"" --> ""below""

Overall, there are interesting elements in this paper and the reviewers noted that the generated results look good.  However, the paper will need to be improved considerably.   The authors are encouraged to improve their work and submit to an appropriate venue.
",Paper Decision
fZRy-vhh8a,rJlYsn4YwS,Gradient-free Neural Network Training by Multi-convex Alternating Optimization,Reject,"The paper proposes a new learning algorithm for deep neural networks that first reformulates the problem as a multi-convex and then uses an alternating update to solve. The reviewers are concerned about the closeness to previous work, comparisons with related work like dlADMM, and the difficulty of the dataset. While the authors proposed the possibility of addressing some of these issues, the reviewers feel that without actually addressing them, the paper is not yet ready for publication. ",Paper Decision
R7GqKNBYNi,BkxFi2VYvS,Semi-supervised Semantic Segmentation using Auxiliary Network,Reject,"The paper presents a semi-supervised learning approach to handle semantic classification (pixel-level classification). The approach extends Hung et al. 18, using a confidence map generated by an auxiliary network, aimed to improve the identification of small objects.

The reviews state that the paper novelty is limited compared to the state of the art; the reviewers made several suggestions to improve the processing pipeline (including all images, including the confidence weights). 
The reviews also state that the paper needs be carefully polished. 

The area chair hopes that the suggestions about the contents and writing of the paper will help to prepare an improved version of the paper. 
",Paper Decision
c-pkwcmnF8,HygOjhEYDH,Intensity-Free Learning of Temporal Point Processes,Accept (Spotlight),"This submission proposes a new paradigm for modelling temporal point processes by using deep learning to learn to mix log-normal distributions in order to directly model the conditional distribution of event time intervals themselves.

Strengths of the paper:
-Introduces a new modelling paradigm that can lead to further research in this direction, for an important problem.
-Extensive experimentation validates the approach quantitatively.
-Easy to read.

Weaknesses:
-Several reviewers wanted more details on how the mixing parameter K was tuned. This was adequately addressed during the discussion period.

The reviewer consensus was to accept this submission.
",Paper Decision
3c4p7cm2Xx,r1gdj2EKPB,Scalable and Order-robust Continual Learning with Additive Parameter Decomposition,Accept (Poster),"The submission addresses the problem of continual learning with large numbers of tasks and variable task ordering and proposes a parameter decomposition approach such that part of the parameters are task-adaptive and some are task-shared. The validation is on omniglot and other benchmarks.

The reviews were mixed on this paper, but most reviewers were favorably impressed with the problem setup, the scalability of the method, and the results. The baselines were limited but acceptable. The recommendation is to accept this paper, but the authors are advised to address all the points in the reviews in their final revision.",Paper Decision
hIiTSTnvzy,rkxDon4Yvr,Discriminator Based Corpus Generation for General Code Synthesis,Reject,"This paper proposes a method to automatically generate corpora for training program synthesis systems.

The reviewers did seem to appreciate the core idea of the paper, but pointed out a number of problems with experimental design that preclude the publication of the paper at this time. The reviewers gave a number of good comments, so I hope that the authors can improve the paper for publication at a different venue in the future.",Paper Decision
QOxFzMPiI,S1ewjhEFwr,Storage Efficient and Dynamic Flexible Runtime Channel Pruning via Deep Reinforcement Learning,Reject,"Main content: Proposes a deep RL unified framework to manage the trade-off between static pruning to decrease storage requirements and network flexibility for dynamic pruning to decrease runtime costs
Summary of discussion:
reviewer1: Reviewer likes the proposed DRL approach, but writing and algorithmic details are lacking
reviewer2: Pruning methods are certainly imortant, but there are details missing wrt the algorithm in the paper. 
reviewer3: Presents a novel RL algorithm, showing good results on CIFAR10 and ISLVRC2012. Algorithmic details and parameters are not clearly explained. 
Recommendation: All reviewers liked the work but the writing/algorithmic details are lacking. I recommend Reject. ",Paper Decision
20LZpQD4al,BJevihVtwB,BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS,Reject,"This paper introduces a closed-form expression for the Stein’s unbiased estimator for the prediction error, and a boosting approach based on this, with empirical evaluation. While this paper is interesting, all reviewers seem to agree that more work is required before this paper can be published at ICLR.  ",Paper Decision
-NP6xc-7K,B1xIj3VYvr,Weakly Supervised Clustering by Exploiting Unique Class Count,Accept (Poster),"The paper proposes a weakly supervised learning algorithm, motivated by its application to histopathology. Similar to the multiple instance learning scenario, labels are provided for bags of instances. However instead of a single (binary) label per bag, the paper introduces the setting where the training algorithm is provided with the number of classes in the bag (but not which ones). Careful empirical experiments on semantic segmentation of histopathology data, as well as simulated labelling from MNIST and CIFAR demonstrate the usefulness of the method. The proposed approach is similar in spirit to works such as learning from label proportions and UU learning (both which solve classification tasks).
http://www.jmlr.org/papers/volume10/quadrianto09a/quadrianto09a.pdf
https://arxiv.org/abs/1808.10585

The reviews are widely spread, with a low confidence reviewer rating (1). However it seems that the high confidence reviewers are also providing higher scores and better comments. The authors addressed many of the reviewer comments, and seeked clarification for certain points, but the reviewers did not engage further during the discussion period.

This paper provides a novel weakly supervised learning setting, motivated by a real world semantic segmentation task, and provides an algorithm to learn from only the number of classes per bag, which is demonstrated to work on empirical experiments. It is a good addition to the ICLR program.",Paper Decision
iYi7pI4Bg7,BJeUs3VFPH,Domain Adaptation via Low-Rank Basis Approximation,Reject,"Three reviewers have scored this paper  as 1/1/3 and they have not increased their rating after the rebuttal and the paper revision. The main criticism revolves around the choice of datasets, missing comparisons with the existing methods, complexity and practical demonstration of speed. Other concerns touch upon a loose bound and a weak motivation regarding the low-rank mechanism in connection to DA. On balance, the authors resolved some issues in the revised manuscripts but reviewers remain unconvinced about plenty other aspects, thus this paper cannot be accepted to ICLR2020.",Paper Decision
-YR1hHxFNo,HyeSin4FPB,Learning to Control PDEs with Differentiable Physics,Accept (Spotlight),"The paper proposes a method to control dynamical systems described by a partial differential equations (PDE). The method uses a hierarchical predictor-corrector scheme that divides the problem into smaller and simpler temporal subproblems. They illustrate the performance of their method on 1D Burger’s PDE and 2D incompressible flow.
The reviewers are all positive about this paper and find it well-written and potentially impactful. Hence, I recommend acceptance of this paper.",Paper Decision
PaUeEgYYFp,H1lBj2VFPS,Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware,Accept (Poster),"This paper considers the question of how to quantize deep neural networks, for processors operating on low-precision integers.  The authors propose a methodology and have evaluated it thoroughly. The reviewers all agree that this question is important in practice, though there was disagreement about how novel a contribution this paper is specifically, and on its clarity. The clarity questions were resolved on rebuttal, so I lean to accepting the paper.",Paper Decision
YV6Zo3uGmR,rklEj2EFvB,Estimating Gradients for Discrete Random Variables by Sampling without Replacement,Accept (Spotlight),"The authors derive a novel, unbiased gradient estimator for discrete random variables based on sampling without replacement. They relate their estimator to existing multi-sample estimators and motivate why we would expect reduced variance. Finally, they evaluate their estimator across several tasks and show that is performs well in all of them.

The reviewers agree that the revised paper is well-written and well-executed. There was some concern about that effectiveness of the estimator, however, the authors clarified that ""it is the only estimator that performs well across different settings (high and low entropy). Therefore it is more robust and a strict improvement to any of these estimators which only have good performance in either high or low entropy settings."" Reviewer 2 was still not convinced about the strength of the analysis of the estimator, and this is indeed quantifying the variance reduction theoretically would be an improvement.

Overall, the paper is a nice addition to the set of tools for computing gradients of expectations of discrete random variables. I recommend acceptance.

",Paper Decision
Cp0gXjcBB,ryg7jhEtPB,On importance-weighted autoencoders,Reject,"The authors argue that directly optimizing the IS proposal distribution as in RWS is preferable to optimizing the IWAE multi-sample objective. They formalize this with an adaptive IS framework, AISLE, that generalizes RWS, IWAE-STL and IWAE-DREG. 

Generally reviewers found the paper to be well-written and the connections drawn in this paper interesting. However, all reviewers raised concerns about the lack of experiments (Reviewer 3 suggested several experiments that could be done to clarify remaining questions) and practical takeaways. 

The authors responded by explaining that ""the main ""practical"" takeaway from our work is the following: If one is interested in the bias-reduction potential offered by IWAEs over plain VAEs then the adaptive importance-sampling framework appears to be a better starting point for designing new algorithms than the specific multi-sample objective used by IWAE. This is because the former retains all of the benefits of the latter without inheriting its drawbacks."" I did not find this argument convincing as a primary advantage of variational approaches over WS is that the variational approach optimizes a unified objective. At least in principle, this is a serious drawback of the WS approaches. Experiments and/or a discussion of this is warranted.

This paper is borderline, and unfortunately, due to the high number of quality submissions this year, I have to recommend rejection at this point.
",Paper Decision
gTKMSOwuxq,BylXi3NKvS,FALCON: Fast and Lightweight Convolution for Compressing and Accelerating CNN,Reject,"The submission presents an approach to accelerating convolutional networks. The framework is related to depthwise separable convolutions. The reviews are split. R3 expresses concerns about the experimental evaluation and results. The AC agrees with these concerns. The AC also notes that the submission is 10 pages long. Taking all factors into account, the AC recommends against accepting the paper.",Paper Decision
eRTJ8s0k7,BkxfshNYwB,Mincut Pooling in Graph Neural Networks,Reject,"Two reviewers are negative on this paper while the other reviewer is positive. Overall, the paper does not make the bar of ICLR. A reject is recommended.",Paper Decision
YwodKKKb1E,r1xbj2VKvr,Dual Graph Representation Learning,Reject,"This work proposes context-aware representation of graph nodes leveraging attention over neighbors (as already done in previous work). Reviewers concerns about lack of novelty, lack of clarity of paper and lack of comparison to state of the art methods have not been addressed at all.
We recommend rejection.",Paper Decision
5FV3-PH-i0,Ske-ih4FPS,Unsupervised Few Shot Learning via Self-supervised Training,Reject,"This paper proposes an approach for unsupervised meta-learning for few-shot learning that iteratively combines clustering and episodic learning. The approach is interesting, and the topic is of interest to the ICLR community. Further, it is nice to see experiments on a more real world setting with the Market1501 dataset.
However, the paper lacks any meaningful comparison to prior works on unsupervised meta-learning. While it is accurate that the architecture used and/or assumptions used in this paper are somewhat different from those in prior works, it's important to find a way to compare to at least one of these prior methods in a meaningful way (e.g. by setting up a controlled comparison by running these prior methods in the experimental set-up considered in this work). Without such as comparison, it's impossible to judge the significance of this work in the context of prior papers.
The paper isn't ready for publication at ICLR.",Paper Decision
XddFBEQEs0,Sylgsn4Fvr,"To Relieve Your Headache of Training an MRF, Take AdVIL",Accept (Poster),"The paper proposes a black box algorithm for MRF training, utilizing a novel approach based on variational approximations of both the positive and negative phase terms of the log likelihood gradient (as R2 puts it, ""a fairly creative combination of existing approaches""). 

Several technical and rhetorical points were raised by the reviewers, most of which seem to have been satisfactorily addressed, but all reviewers agreed that this was a good direction. The main weakness of the work is that the empirical work is very small scale, mainly due to the bottleneck imposed by an inner loop optimization of the variational distribution q(v, h). I believe it's important to note that most truly large scale results in the literature revolve around purely feedforward models that don't require expensive to compute approximations; that said, MNIST experiments would have been nice. 

Nevertheless, this work seems like a promising step on a difficult problem, and it seems that the ideas herein are worth disseminating, hopefully stimulating future work on rendering this procedure less expensive and more scalable.",Paper Decision
HdwvzSiGK7,HJggj3VKPH,On the Dynamics and Convergence of Weight Normalization for Training Neural Networks,Reject,"The goal of this paper is to study the dynamics of convergence of neural network training when weight normalization is used. This is an important and interesting area. The authors focus on analyzing such effect based on a recent theoretical trend which studies neural network dynamics based on the so called neural tangent kernel (NTK). The authors show an interesting phenomena of length-direction decoupling. The reviewers raise various points some of which have been addressed by the authors in their response. Two main points not yet clearly addressed is (1) what is the novelty of the theoretical framework given existing literature and (2) what are the benefits of weight normalization based on this theory (e.g. generalization etc. ). The authors suggest improved convergence rate and overparameterization dependence (i.e. that with weight normalization the required width is decreased) as a possible advantage. However, as pointed out by reviewer 3 there are existing results which already obtain better results without weight normalization (the authors' response that this is only true in randomized scenarios is actually not accurate). Based on above I do not think the paper is ready for publication. That said I think this is a nice direction and well-written paper. I recommend the authors revise and resubmit to a future venue. Some suggestions for improvements in case this is helpful (1) improve literature review and discussion of existing results (2) identify clear benefits to weight normalization. I doubt that improving overparameterization in existing form is one of them unless you provide a lower-bound (I suspect one can eventually obtain even linear overparameterization i.e. number of parameters proportional to number of training data even in the NTK regime without weight normalization. The suggestion by the reviewer at looking at generalization might be a good direction to pursue. 

    ",Paper Decision
ggUCBDSikz,SJl1o2NFwS,Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View,Reject,"In this work, the authors interpret the Transformer as a numerical ODE modelling multi-particle convection. Guided by this connection, the authors take the Transformer that uses a feed forward net over attentions, and create a variant of transformer which instead uses an FFN-attention-FFN layer, thus the name macaron net. The authors present experiments in the GLUE dataset and in two MT datasets, and they overall report improved performance using their variant of Transformer. Thus, the main selling point of the paper is how seeing Transformer under his new light can potentially improve results through the construction of better models. The main criticisms from the authors is that  this story is not entirely convincing because the proposed variant departs a bit from the theory (R1 and comment about the Strang-Marchuk splitting) and the papers does not consider an evaluation of accuracy of Macaron in solving the underlying set of ODEs (comment from R3). As such, I cannot recommend acceptance of this paper -- I believe another set of revisions would increase the impact of this paper.",Paper Decision
Q_EUd531i,H1lac2Vtwr,SesameBERT: Attention for Anywhere,Reject,"This paper proposes a few architectural modifications to the BERT model for language understanding, which are meant to apply during fine-tuning for target tasks. 

All three reviewers had concerns about the motivation for at least one of the proposed methods, and none of three reviewers found the primary experimental results convincing: The proposed methods yield a small improvement on average across target tasks, but one that is not consistent across tasks, and that may not be statistically significant.

The authors clarified some points, but did not substantially rebut any of the reviewers concerns. Even though the reviewers express relatively low confidence, their concerns sound serious and uncontested, so I don't think we can accept this paper as is.",Paper Decision
nWL6IA0Oi6,rklp93EtwH,Automated Relational Meta-learning,Accept (Poster),"This paper proposes to deal with task heterogeneity in meta-learning by extracting cross-task relations and constructing a meta-knowledge graph, which can then quickly adapt to new tasks. The authors present a comprehensive set of experiments, which show consistent performance gains over baseline methods, on a 2D regression task and a series of few-shot classification tasks. They further conducted some ablation studies and additional analyses/visualization to aid interpretation.

Two of the reviewers were very positive, indicating that they found the paper well-written, motivated, novel, and thorough, assessments that I also share. The authors were very responsive to reviewer comments and implemented all actionable revisions, as far as I can see. The paper looks to be in great shape. I’m therefore recommending acceptance.  ",Paper Decision
vpJUj0AYD,BJepq2VtDB,Training Deep Networks with Stochastic Gradient Normalized by Layerwise Adaptive Second Moments,Reject,"The paper presented an adaptive stochastic gradient descent method with layer-wise normalization and decoupled weight decay and justified it on a variety of tasks. The main concern for this paper is the novelty is not sufficient. The method is a combination of LARS and AdamW with slight modifications. Although the paper has good empirically evaluations, theoretical convergence proof would make the paper more convincing. ",Paper Decision
ukaBTxuHmT,HyxhqhVKPB,Moniqua: Modulo Quantized Communication in Decentralized SGD,Reject,"This papers proposed an interesting idea for distributed decentralized training with quantized communication. The method is interesting and elegant. However, it is incremental, does not support arbitrary communication compression, and does not have a convincing explanation why modulo operation makes the algorithm better. The experiments are not convincing. Comparison is shown only for the beginning of the optimization where the algorithm does not achieve state of the art accuracy. Moreover, the modular hyperparameter is not easy to choose and seems cannot help achieve consensus.",Paper Decision
w7du8FqOnw,H1xscnEKDr,Defending Against Physically Realizable Attacks on Image Classification,Accept (Spotlight),"This paper studies the problem of defending deep neural network approaches for image classification from physically realizable attacks. It first demonstrates that adversarial training with PGD attacks and randomized smoothing exhibit limited effectiveness against three of the highest profile physical attacks. Then, it proposes a new abstract adversarial model, where an adversary places a small adversarially crafted rectangle in an image, and develops two approaches for efficiently computing the resulting adversarial examples. Empirical results show the effectiveness. Overall, a good paper. The rebuttal is convincing.",Paper Decision
PB5z5DeAtX,H1eo9h4KPH,Certifying Distributional Robustness using Lipschitz Regularisation,Reject,"This works relates adversarial robustness and Lipschitz constant regularization. After the rebuttal period reviewers still had some concerns. In particular it was felt that Theorem 1 could likely be deduced from known results in optimal transport, and it would be nice to make this connection explicit. There were still concerns about scalability. The authors are encouraged to continue with this work, considering the above points in future revisions.
",Paper Decision
6HXWtX56QV,Hyx5qhEYvH,A SPIKING SEQUENTIAL MODEL: RECURRENT LEAKY INTEGRATE-AND-FIRE,Reject,"This work extends Leaky Integrate and Fire (LIF)  by proposing a recurrent version.
All reviewers agree that the work as submitted is way too preliminary. Prior art is missing many results, presentation is difficult to follow and incomplete and contains errors. Even if these concerns were addressed, the benefit of the proposed method is unclear. Authors have not responded.
We thus recommend rejection.",Paper Decision
FNpR7ygHS,r1ecqn4YwB,N-BEATS: Neural basis expansion analysis for interpretable time series forecasting,Accept (Poster),The paper received positive recommendation from all reviewers. Accept.,Paper Decision
6QYfjRQeDv,H1e552VKPr,Subgraph Attention for Node Classification and Hierarchical Graph Pooling,Reject,"Initially, two reviewers gave high scores to this paper while they both admitted that they know little about this field. The other review raised significant concerns on novelty while claiming high confidence. During discussions, one of the high-scoring reviewers lowered his/her score. Thus a reject is recommended.",Paper Decision
0kRf-11Av,Skltqh4KvB,Are there any 'object detectors' in the hidden layers of CNNs trained to identify objects or scenes?,Reject,"This paper conducted a number of empirical studies to find whether units in object-classification CNN can be used as object detectors. The claimed conclusion is that there are no units that are sufficient powerful to be considered as object detectors. Three reviewers have split reviews. While reviewer #1 is positive about this work, the review is quite brief. In contrast, Reviewer #2 and #3 both rate weak reject, with similar major concerns. That is, the conclusion seems non-conclusive and not surprising as well. What would be the contribution of this type of conclusion to the ICLR community? In particular, Reviewer #2 provided detailed and well elaborated comments. The authors made efforts to response to all reviewers’ comments. However, the major concerns remain, and the rating were not changed. The ACs concur the major concerns and agree that the paper can not be accepted at its current state.",Paper Decision
CdVLFxjlSj,S1eYchEtwH,Learning Human Postural Control with Hierarchical Acquisition Functions,Reject,"The paper proposes hierarchical Bayesian optimization (HiBO) for learning control policies from a small number of environment interaction and applies it to the postural control of a humanoid. Both reviewers raised issues with the clarity of presentation, as well as contribution and overall fit to this venue. The authors’ response helped to clarify these issues only marginally. Therefore, primarily due to lack of clarity, I recommend rejecting this paper, but encourage the authors to improve the presentation as per the reviewers’ suggestions and resubmitting.",Paper Decision
t3DUavRvOl,SJlOq34Kwr,Unsupervised Intuitive Physics from Past Experiences,Reject,"While the reviewers found the paper interesting, all the reviewers raised concerns about the fairly simple experimental settings, which makes it hard to appreciate the strengths of the proposed method. During rebuttal phase, the reviewers still felt this weakness was not sufficiently addressed.",Paper Decision
fbwGjAUcLD,Syld53NtvH,Expected Tight Bounds for Robust Deep Neural Network Training,Reject,"The authors propose a new technique for training networks to be robust to adversarial perturbations. They do this by computing bounds on the impact of the worst case adversarial attack, but that only hold under strong assumptions on the distribution of the network weights. While these bounds are not rigorous, the authors show that they can produce networks that improve the robustness-accuracy tradeoff on image classification tasks.

While the idea proposed by the authors is interesting, the reviewers had several concerns about this paper:
1) The assumptions required for the bounds to hold are unrealistic and unlikely to hold in practice, especially for convolutional neural networks.
2) The comparisons are not presented in a fair manner that allow the reader to interpret the difference between the nature of certificates computed by the authors and those computed in prior work.
3) The empirical gains are not substantial if one normalizes for the non-rigorous nature of the certificates computed (given that they only hold under hard-to-justify assumptions).

The rebuttal phase clarified some issues in the paper, but the fundamental flaws with the approach remain unaddressed. Thus, I recommend rejection and suggest that the authors revisit the assumptions and develop more convincing arguments and/or experiments justifying them for practical deep learning scenarios.",Paper Decision
TMOhk4hbEr,B1xDq2EFDH,Analytical Moment Regularizer for Training Robust Networks,Reject,"This paper received two weak and one strong reject from the reviewers.  The major issues cited were 1) a lack of strong enough baselines or empirical results, 2) Novelty with respect to ""Certified adversarial robustness via randomized smoothing"" and 3) a limitation to Gaussian noise perturbations.  Unfortunately, as a result the reviewers agreed that this work was not ready for acceptance.  Adding stronger empirical results and a careful treatment of related work would make this a much stronger paper for a future submission.",Paper Decision
SZx6xDCUut,B1xw9n4Kwr,Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula,Reject,"This paper focuses on understanding the role of model architecture on convergence behavior and in particular on the speed of training. The authors study the gradient flow of training via studying an ODE's coefficient matrix H. They study the effect of H in terms of possible paths in the network. The reviewers all agreed that characterizing the behavior in terms of path is nice. However, they had concerns about novelty with respect to existing work on NTK. Other comments by reviewers include (1) poor literature review (2) subpar exposition and (3) hand-wavy and rack of rigor in some results. While some of these concerns were alleviated during the discussion. Reviewers were not fully satisfied.  I general agree with the overall assessment of the reviewers. The paper has some interesting ideas but suffers from lack of clarity and rigor. Therefore, I can not recommend acceptance in the current form.",Paper Decision
RUrWsk2bO,rkeIq2VYPr,Deep Learning of Determinantal Point Processes via Proper Spectral Sub-gradient,Accept (Poster),"Most reviewers seems in favour of accepting this paper, with the borderline rejection being satisfied with acceptance if the authors take special heed of their comments to improve the clarity of the paper when preparing the final version. From examination of the reviews, the paper achieves enough to warrant publication. Accept.",Paper Decision
qt8rN439b8,HylLq2EKwS,Collaborative Filtering With A Synthetic Feedback Loop,Reject,"The paper proposes to learn a ""virtual user"" while learning a ""recommender"" model, to improve the performance of the recommender system. A reinforcement learning algorithm is used for address the problem the authors defined. Multiple reviewers raised several concerns regarding its technical details including the feedback signal F, but the authors have not responded to any of the concerns raised by the reviewers. The lack of authors involvement in the discussion suggest that this paper is not at the stage to be published.",Paper Decision
Z2es9fQ2-,HygSq3VFvH,Self-Supervised State-Control through Intrinsic Mutual Information Rewards,Reject,"The paper considers a setting where the state of a (robotics) environment can be divided roughly into ""context states"" (such as variables under the robot's direct control) and ""states of interest"" (such as the state variables of an object to be manipulated), and learn skills by maximizing a lower bound on the mutual information between these two components of the state. Experimental results compare to DDPG/SAC, and show that the learned discriminator is somewhat transferable between environments.

Reviewers found the assumptions necessary on the degree of domain knowledge to be quite strong and domain-specific, and that even after revision, the authors were understating the degree to which this was necessary. The paper did improve based on reviewer feedback, and while R3 was more convinced by the follow-up experiments (though remarked that requiring environment variations to obtain new skills was a ""significant step backward from things like [Diversity is All You Need]""), the other reviewers remained unconvinced regarding domain knowledge and in particular how it interacts with the scalability of the proposed method to complex environments/robots.

Given the reviewers' concerns regarding applicability and scalability, I recommend rejection in its present form. A future revision may be able to more convincingly demonstrate that limitations based on domain knowledge are less significant than they appear.",Paper Decision
9z66ZRXMet,H1eH9hNtwr,Stagnant zone segmentation with U-net,Reject,"The paper proposed U-net for segmentation of stagnant zones in computed tomography. Technical contribution of the paper is severely limited, and is not of the quality expected of publications in this venue. The paper is not anonymized and violates the double blind review rule. I'm thus recommending rejection.",Paper Decision
C_JdAmsOuT,BJeB5hVtvB,Distance-Based Learning from Errors for Confidence Calibration,Accept (Poster),"All reviewers voted to accept this paper.
The AC recommends acceptance.",Paper Decision
zD0rk_qegV,BylEqnVFDB,Curvature Graph Network,Accept (Poster),"The paper presents a novel graph convolutional network by integrating the curvature information (based on the concept of Ricci curvature). The key idea is well motivated and the paper is clearly written. Experimental results show that the proposed curvature graph network methods outperform existing graph convolution algorithms. One potential limitation is the computational cost of computing the Ricci curvature, which is discussed in the appendix. Overall, the concept of using curvature in graph convolutional networks seems like a novel and promising idea, and I also recommend acceptance.",Paper Decision
GoLydDLnVP,S1gNc3NtvB,Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer,Reject,"The authors present a method that optimizes a differentiable neural computer with evolutionary search, and which can transfer abstract strategies to novel problems.  The reviewers all agreed that the approach is interesting, though were concerned about the magnitude of the contribution / novelty compared to existing work, clarity of contributions, impact of pretraining, and simplicity of examples.  While the reviewers felt that the authors resolved the many of their concerns in the rebuttal, there was remaining concern about the significance of the contribution.  Thus, I recommend this paper for rejection at this time.",Paper Decision
iM49PGcZVH,B1em9h4KDS,Generative Imputation and Stochastic Prediction,Reject,"The paper proposes a method that does uncertainty modeling over missing data imputation using a framework based on generative adversarial network. While the method shows some empirical improvements over the baselines, reviewers have found the work incremental in terms of technical novelty over the existing GAIN approach which renders it slightly below the acceptance threshold for the main conference, particularly in case of space constraints in the program. ",Paper Decision
B0lXUsBfT,Byg79h4tvB,PROTOTYPE-ASSISTED ADVERSARIAL LEARNING FOR UNSUPERVISED DOMAIN ADAPTATION,Reject,"The paper focuses on adversarial domain adaptation, and proposes an approach inspired from the DANN. The contribution lies in additional terms in the loss, aimed to i) align the source and target prototypes  in each class (using pseudo labels for target examples); ii) minimize the variance of the latent representations for each class in the target domain. 

Reviews point out that the expected benefits of target prototypes might be ruined if the pseudo-labels are too noisy; they note that the specific problem needs be more clearly formalized and they regret the lack of clarity of the text. The sensitivity w.r.t. the hyper-parameter values needs be assessed more thoroughly. 

One also notes that SAFN is one of the baseline methods; but its best variant (with entropic regularization) is not considered, while the performance thereof is on par or greater than that of PACFA for ImageCLEF-Da; idem for AdapSeg (consider its multi-level variant) or AdvEnt with MinEnt. 

For these reasons, the paper seems premature for publication at ICLR 2020. ",Paper Decision
kjZZHhM0SZ,ryeG924twB,Learning Expensive Coordination: An Event-Based Deep RL Approach,Accept (Poster),"This paper tackles the challenge of incentivising selfish agents towards a collaborative goal. In doing so, the authors propose several new modules. 

The reviewers commented on experiments being extremely thorough. One reviewer commented on a lack of ablation study of the 3 contributions, which was promptly provided by the authors. The proposed method is also supported by theoretical derivations. The contributions appear to be quite novel, significantly improving performance of the studied SMGs.

One reviewer mentioned the clarity being compromised by too much material being in the appendix, which has been addressed by the authors moving some main pieces of content to the main text. 

Two reviewer commented on the relevance being lower because of the problem not being widely studied in RL. I would disagree with the reviewers on this aspect, it is great to have new problem brought to light and have fresh and novel results, rather than having yet another paper work on Atari. I also think that the authors in their rebuttal made the practical relevance of their problem setting sufficiently clear with several practical examples. ",Paper Decision
BZIvm0T2T,HJxf53EtDr,Unifying Graph Convolutional Networks as Matrix Factorization,Reject,"The paper makes an interesting attempt at connecting graph convolutional neural networks (GCN) with matrix factorization (MF) and then develops a MF solution that achieves similar prediction performance as GCN. 

While the work is a good attempt, the work suffers from two major issues: (1)  the connection between GCN and other related models have been examined recently. The paper did not provide additional insights; (2) some parts of the derivations could be problematic. 

The paper could be a good publication in the future if the motivation of the work can be repositioned. ",Paper Decision
Xx6IwfdRuX,Skgb5h4KPH,Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks,Reject,"Borderline decision.  The idea is nice, but the theory is not completely convincing.  That makes the results in this paper not be significant enough.",Paper Decision
6w-pZzuxl2,SkeWc2EKPH,Model-free Learning Control of Nonlinear Stochastic Systems with Stability Guarantee,Reject,"The authors propose a method to guarantee the stability of a learnt continuous controller by optimizing the objective through a Lyapunov critic. The method is demonstrated on low dimensional continuous control problems such as cart pole. 

The reviewers were mixed in their opinion of the paper, especially after the authors' rebuttal. The concerns center around some of the authors' claims regarding theoretical results, in particular that stability guarantees can be asserted for a model-free controller. This claim seems to be incorrect especially on novel data where stability cannot be guaranteed, thus indicating that 'robust controller' might be a better description. There are also concerns about the novelty and the contributions of the paper. Overall, the method is promising but the claims need to be carefully written. The recommendation is to reject the paper at this time.",Paper Decision
gW9KbIXgmG,Skgxcn4YDS,LAMOL: LAnguage MOdeling for Lifelong Language Learning,Accept (Poster),This paper proposes a new method for lifelong learning of language using language modeling. Their training scheme is designed so as to prevent catastrophic forgetting. The reviewers found the motivation clear and that the proposed method outperforms prior related work. Reviewers raised concerns about the title and the lack of some baselines which the authors have addressed in the rebuttal and their revision.,Paper Decision
wPTYJz9HH0,HkxlcnVFwB,GenDICE: Generalized Offline Estimation of Stationary Values,Accept (Talk),"The authors develop a framework for off-policy value estimation for infinite horizon RL tasks, for estimating the stationary distribution of a Markov chain. Reviewers were uniformly impressed by the work, and satisfied by the author response. Congratulations! 
",Paper Decision
0YK3-6_zbm,B1l1qnEFwH,Deep Audio Prior,Reject,This paper proposes to use CNN'S prior to deal with the tasks in audio processing. The motivation is weak and the presentation is not clear.  The technical contribution is trivial.,Paper Decision
s-Ujxf_4W,ryxAY34YwB,Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization,Reject,"This paper proposes a method to leverage the Lead (i.e., first sentence of an article) in training a model for abstractive news summarization. 

Reviewers' initial recommendations were weak reject to weak accept, pointing out the limitations of the paper including 1) little novelty in modeling, 2) weak evaluation, and 3) lack of deep analysis. After the author rebuttal and revised paper, one of the reviewers increased the score and were leaning toward weak accept. 

However, reviewers noted that there was significant overlap with another submission, and we discussed that it would be best to accept one of the two, incorporating the contributions of both papers. Hence, I recommend that this paper not be accepted, and perhaps some of the non-overlapping contents of this paper can be included in the other, accepted paper.

Thank you for submitting this paper. I enjoyed reading it.",Paper Decision
BekNMrbbZm,HygpthEtvr,ProxSGD: Training Structured Neural Networks under Regularization and Constraints,Accept (Poster),"This paper proposes a new gradient-based stochastic optimization algorithm by adapting theory for proximal algorithms to the non-convex setting. 

The majority of reviewers voted for accept. The authors are encouraged to revise with respect to reviewer comments.",Paper Decision
jmhKuC1PH,Byl3K2VtwB,Unsupervised Learning of Node Embeddings by Detecting Communities,Reject,"The authors present an approach to learn node embeddings by minimising the mincut loss which ensures that the network simultaneously learns node representations and communities. To ensure scalability, the authors also propose an iterative process using mini-batches. 

I think this is a good paper with interesting results.  However, I would suggest that the authors try to make it more accessible to a larger audience (2 reviewers have indicated that they had difficulty in following the paper). For example, while Theorem 1 and Theorem 2 are interesting they could have been completely pushed to the Appendix and it would have sufficed to say that your work/results are grounded in well-proven theorems as mentioned in 1 and 2. 

I agree that the authors have done a good job of responding to reviewers' queries and addressed the main concerns. However, since the reviewers have unanimously given a low rating to this paper, I do not feel confident about overriding their rating and accepting this paper. Hence, at this point I will have to recommend that this paper cannot be accepted. This paper has good potential and the authors should submit it to another suitable venue soon.",Paper Decision
joFl7S6as3,ryxnY3NYPS,Diverse Trajectory Forecasting with Determinantal Point Processes,Accept (Poster),"The paper proposes an approach for forecasting diverse object trajectories using determinantal point processes (DPP). Past trajectory is mapped to a latent code and a conditional VAE is used to generate the future trajectories. Instead of using log-likelihood of DPP, the propose method optimizes expected cardinality as a measure for diversity. While there are some concerns about the core method being incremental in novelty over some existing DPP based methods, the context of the paper is different from these papers (ie, diverse trajectories in continuous space) and reviewers have appreciated the empirical improvements over the baselines, in particular over DPP-NLL and DPP-MAP in latent space. ",Paper Decision
gsh6tfUcK2,H1loF2NFwr,Evaluating The Search Phase of Neural Architecture Search,Accept (Poster),"This is one of several recent parallel papers that pointed out issues with neural architecture search (NAS). It shows that several NAS algorithms do not perform better than random search and finds that their weight sharing mechanism leads to low correlations of the search performance and final evaluation performance. Code is available to ensure reproducibility of the work.

After the discussion period, all reviewers are mildly in favour of accepting the paper. 

My recommendation is therefore to accept the paper. The paper's results may in part appear to be old news by now, but they were not when the paper first appeared on arXiv (in parallel to Li & Talwalkar, so similarities to that work should not be held against this paper).",Paper Decision
D2dBz03-x1,HklsthVYDH,Learning to Defense by Learning to Attack,Reject,"This paper considers solving the minimax formulation of adversarial training, where it proposes a new method based on a generic learning-to-learn (L2L) framework. Particularly, instead of applying the existing hand-designed algorithms for the inner problem, it learns an optimizer parametrized as a convolutional neural network. A robust classifier is learned to defense the adversarial attack generated by the learned optimizer. The idea is using L2L is sensible. However, main concerns on empirical studies remain after rebuttal. ",Paper Decision
FSkewIv756,B1e9Y2NYvS,On Robustness of Neural Ordinary Differential Equations,Accept (Spotlight),"This paper studies the robustness of NeuralODE, as well as propose a new variant. The results suggest that the neuralODE can be used as a building block to build robust deep networks. The reviewers agree that this is a good paper for ICLR, and based on their recommendation I suggest to accept this paper.",Paper Decision
RnUD1qipu,HyetFnEFDS,Diving into Optimization of Topology in Neural Networks,Reject,"This paper proposes an approach for architecture search by framing it as a differentiable optimization over directed acyclic graphs. While the reviewers appreciated the significance of architecture search as a problem and acknowledged that the paper proposes a principled approach for this problem, there were concerns about lack of experimental rigor, and limited technical novelty over some existing works. ",Paper Decision
34fTue2NkT,HyxFF34FPr,FoveaBox: Beyound Anchor-based Object Detection,Reject,"The paper proposes a method for object detection by predicting category-specific object probability and category-agnostic bounding box coordinates for each position that's likely to contain an object. The proposed idea is interesting and the experimental results show improvement over RetinaNet and other baselines. However, in terms of weakness, (1) conceptually speaking it's unclear whether the proposed method is a big departure from the existing frameworks; and (2) although the authors are claiming SOTA performance, the proposed method seems to be worse than other existing/recent work. Some example references are listed below (more available here: https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector). 

[1] Scale-Aware Trident Networks for Object Detection
https://arxiv.org/abs/1901.01892

[2] GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond
https://arxiv.org/abs/1904.11492

[3] CBNet: A Novel Composite Backbone Network Architecture for Object Detection
https://arxiv.org/abs/1909.03625

[4] EfficientDet: Scalable and Efficient Object Detection
https://arxiv.org/abs/1911.09070

References [3] and [4] are concurrent works so shouldn't be a ground of rejection per se, but the performance gap is quite large. Compared to [1] and [2] which have been on arxiv for a while (+5 months) the performance of the proposed method is still inferior. Despite considering that object detection is a very competitive field, the conceptual/technical novelty and overall practical significance seem limited for ICLR. For a future submission, I would suggest that a revision of this paper being reviewed in a computer vision conference, rather than ML conference.
",Paper Decision
o3iKFU8Xu,BJeuKnEtDH,Cascade Style Transfer,Reject,"This work combines style transfer approaches either in a serial or parallel fashion, and shows that the combination of methods is more powerful than isolated methods.
The novelty in this work is extremely limited and not offset by insightful analysis or very thorough experiments, given that most results are qualitative. Authors have not provided a public response.
Therefore, we recommend rejection.",Paper Decision
MyMfDOV9eq,H1gdF34FvS,Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning,Reject,"This paper caused a lot of discussions before and after the rebuttal. The concerns are related to the novelty of this paper, which seems to be relatively limited. Since we do not have a champion among positive reviewers, and the overall score is not high enough, I cannot recommend its acceptance at this stage.
",Paper Decision
aUamXx1lkI,rkgdYhVtvH,Unifying Graph Convolutional Neural Networks and Label Propagation,Reject,The authors attempt to unify graph convolutional networks and label propagation and propose a model that unifies them. The reviewers liked the idea but felt that more extensive experiments are needed. The impact of labels needs to be specially studied more in-depth.,Paper Decision
kXKxC_civf,BkxDthVtvS,Equivariant neural networks and equivarification,Reject,"This paper proposes a way to construct group equivariant neural networks from pre-trained non-equivariant networks. The equivarification is done with respect to known finite groups, and  can be done globally or layer-wise. The authors discuss their approach in the context of the image data domain. The paper is theoretically sound and proposes a novel perspective on equivarification, however, the reviewers agree that the experimental section should be strengthened and connections with other approaches (e.g. the work by Cohen and Welling) should be made clearer. The reviewers also had concerns about the computational cost of the equivarification method proposed in this paper. While the authors’ revision addressed some of the reviewers’ concerns, it was not enough to accept the paper this time round. Hence, unfortunately I recommend a rejection.",Paper Decision
h1qOb8Lj2m,BJx8YnEFPH,Data Valuation using Reinforcement Learning,Reject,"The paper suggests an RL-based approach to design a data valuation estimator. The reviewers agree that the proposed method is new and promising, but they also raised concerns about the empirical evaluations, including not comparing with other approaches of data valuation and limited ablation study.
The authors provided a rebuttal to address these concerns. It improves the evaluation of one of the reviewers, but it is difficult to recommend acceptance given that we did not have a champion for this paper and the overall score is not high enough.",Paper Decision
K45sX6jgnq,BJx8Fh4KPB,RL-LIM: Reinforcement Learning-based Locally Interpretable Modeling,Reject,"The paper aims to find locally interpretable models, such that the local models are fit (w.r.t. the ground truth) and faithful (w.r.t. the global underlying black box model).
The contribution of the paper is that the local model is trained from a subset of points, selected via an optimized importance weight function. The difference compared to Ren et al. (cited) is that the IW function is non-differentiable and optimized using Reinforcement Learning. 

A first concern (Rev#1, Rev#2) regards the positioning of the paper w.r.t. RL, as the actual optimization method could be any black-box optimization method: one wants to find the IW that maximizes the faithfulness. The rebuttal makes a good job in explaining the impact of using a non-differentiable IW function.

A second concern (Rev#2) regards the interpretability of the IW underlying the local interpretable model. 

There is no doubt that the paper was considerably improved during the rebuttal period. However, the improvements raise additional questions (e.g. about selecting the IW depending on the distance to the probes). I encourage the authors to continue on this promising line of search. ",Paper Decision
EHmfrSLVu,BJlrF24twB,BackPACK: Packing more into Backprop,Accept (Talk),"The paper efficiently computes quantities, such as variance estimates of the gradient or various Hessian approximations, jointly with the gradient, and the paper also provides a software package for this. All reviewers agree that this is a very good paper and should be accepted.",Paper Decision
IdKl8YyaTq,rylBK34FDS,DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures,Accept (Poster),"The authors propose a scale-invariant sparsity measure for deep networks. The experiments are extensive and convincing, according to reviewers. I recommend acceptance.",Paper Decision
70BgVnKFfK,BkeHt34Fwr,Regional based query in graph active learning,Reject,"The paper proposes a method for performing active learning on graph convolutional networks. In particular, instead of performing uncertainty-based sampling based on an individual node level, the authors propose to look at regional based uncertainty. They propose an efficient algorithm based on page rank. Empirically, they compare their method to several other leading methods, comparing favorably.  

Reviewers found the work poorly organized and difficult to read. The idea to use region based estimates is intuitive but feels like nothing more than just that. It's not clear if there is a mathematical basis to justify such a method (e.g. an analysis of sample complexity as has been accomplished in other graph active learning problems, Dasarathy, Nowak, Zhu 2015). 

The idea requires further study and justification, and the paper needs an improved exposition. Finally, the authors were not anonymized on the PDF. ",Paper Decision
rHvmfDP_tr,SJg4Y3VFPS,Group-Connected Multilayer Perceptron Networks,Reject,"The authors propose Group Connected Multilayer Perceptron Networks which allow expressive feature combinations to learn meaningful deep representations. They experiment with different datasets and show that the proposed method gives improved performance. 

The authors have done a commendable job of replying to the queries of the reviewers and addresses many of their concerns. However, the main concern still remains: The improvements are not very significant on most datasets except the MNIST dataset. I understand the author's argument that other papers have also reported small improvements on these datasets and hence it is ok to report small improvements. However, the reviewers and the AC did not find this argument very convincing. Given that this is not a theoretical paper and that the novelty is not very high (as pointed out by R1) strong empirical results are accepted.  Hence, at this point, I recommend that the paper cannot be accepted.

",Paper Decision
-m8rYQ2X3N,BklEF3VFPB,Towards Stable and comprehensive Domain Alignment: Max-Margin Domain-Adversarial Training,Reject,"This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.

Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.

Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",Paper Decision
uLuvWgzZx,SJg7KhVKPH,Depth-Adaptive Transformer,Accept (Poster),"This paper presents an adaptive computation time method for reducing the average-case inference time of a transformer sequence-to-sequence model. 

The reviewers reached a rough consensus: This paper makes a proposes a novel method for an important problem, and offers reasonably compelling evidence for that method. However, the experiments aren't *quite* sufficient to isolate the cause of the observed improvements, and the discussion of related work could be clearer.

I acknowledge that this paper is borderline (and thank R3 for an extremely thorough discussion, both in public and privately), but I lean toward acceptance: The paper doesn't have any fatal flaws, and it brings some fresh ideas to an area where further work would be valuable.",Paper Decision
SCUCEuju-4,r1lfF2NYvH,InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization,Accept (Spotlight),"This paper proposes a graph embedding method for the whole graph under both unsupervised and semi-supervised setting. It can extract a fixed length graph-level representation with good generalization capability. All reviewers provided unanimous rating of weak accept. The reviewers praise the paper is well written and is value to different fields dealing with graph learning. There are some discussions on the novelty of the approach, which was better clarified after the response from the authors. Overall this paper presents a new effort in the active topic of graph representation learning with potential large impact to multiple fields. Therefore, the ACs recommend it to be an oral paper.",Paper Decision
_VPelBLUb,HJezF3VYPB,Federated Adversarial Domain Adaptation,Accept (Poster),"This paper studies an interesting new problem, federated domain adaptation, and proposes an approach based on dynamic attention, federated adversarial alignment, and representation disentanglement.

Reviewers generally agree that the paper contributes a novel approach to an interesting problem with theoretical guarantees and empirical justification. While many professional concerns were raised by the reviewers, the authors managed to perform an effective rebuttal with a major revision, which addressed the concerns convincingly. AC believes that the updated version is acceptable.

Hence I recommend acceptance.",Paper Decision
dUTT3jReHa,HJgzt2VKPB,CATER: A diagnostic dataset for Compositional Actions & TEmporal Reasoning,Accept (Talk),"The paper proposed a new synthetically generated video dataset (CATER) for benchmarking temporal reasoning. The dataset is based on the CLEVR dataset and provides videos make up of primitive actions (""rotate"", ""pick-place"", ""slide"", ""contain"") that can be combined to form for complex actions.
The paper also benchmarks a variety of methods on three proposed tasks (atomic action classification, composite action classification, and 'snitch' localization) and demonstrates that while it is possible to get high performance on atomic action classification, the other two task are still challenging and requires temporal modeling.  

Overall, all reviewers found the paper to be well written and easy to follow, with care given to the dataset construction, as well as the task definitions and experiment setup and analysis.  The paper received strong scores from all reviewers (3 accepts).  Based on the reviewer comments, the authors further improved the paper by adding additional relevant datasets for comparison and providing missing details pointed out by the reviewers.  After the rebuttal, the reviewers remained positive.",Paper Decision
xgZsIBbaqS,BklWt24tvH,Learning Structured Communication for Multi-agent Reinforcement Learning,Reject,"The paper focuses on large-scale multi-agent reinforcement learning and proposes Learning Structured Communication (LSC) to deal issues of scale and learn sample efficiently. Reviewers are positive about the presented ideas, but note remaining limitations. In particular, the empirical validation does not lead to sufficiently novel insights, and additional analysis is needed to round out the paper.",Paper Decision
mXF7S_db0s,BygZK2VYvB,Utilizing Edge Features in Graph Neural Networks via Variational Information Maximization,Reject,"This paper proposed an auxiliary loss based on mutual information for graph neural network. Such loss is to maximize the mutual information between edge representation and corresponding edge feature in GNN ‘message passing’ function. GNN with edge features have already been proposed in the literature. Furthermore,  the reviewers think the paper needs to improve further in terms of explain more clearly the motivation and rationale behind the method. ",Paper Decision
PlHKoJaPMN,BJlgt2EYwr,Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters,Reject,"This paper studies Differentiable Neural Architecture Search, focusing on a problem identified with the approximated gradient with respect to architectural parameters, and proposing an improved gradient estimation procedure. The authors claim that this alleviates the tendency of DARTS to collapse on degenerate architectures consisting of e.g. all skip connections, presently dealt with via early stopping.

Reviewers generally liked the theoretical contribution, but found the evidence insufficient to support the claims. Requests for experiments by R1 with matched hyperparameters were granted (and several reviewers felt this strengthened the submission), though relegated to an appendix, but after a lengthy discussion reviewers still felt the evidence was insufficient.

R1 also contended that the authors were overly dogmatic regarding ""AutoML"" -- that the early stopping heuristic was undesirable because of the additional human knowledge involved. I appreciate the sentiment but find this argument unconvincing -- while it is true that a great deal of human knowledge is still necessary to make architecture search work, the aim is certainly to develop fool-proof automatic methods. 

As reviewers were still unsatisfied with the empirical investigation after revisions and found that the weight of the contribution was insufficient for a 10 page paper, I recommend rejection at this time, while encouraging the authors to take seriously the reviewers' requests for a systematic study of the source of the empirical gains in order to strengthen their paper for future submission.",Paper Decision
ScmRHSF_g,BygJKn4tPr,Effective Mechanism to Mitigate Injuries During NFL Plays ,Reject,"All reviewers recommend reject, and there is no rebuttal.",Paper Decision
m9OgIvFJHk,H1ekF2EYDH,TechKG: A Large-Scale Chinese Technology-Oriented Knowledge Graph,Reject,"This paper presents a large-scale automatically extracted knowledge base in Chinese which contains information about entities and their relations present in academic papers. The authors have collected several papers that come from around 38 different domains. As such this is a dataset creation paper where the authors have used existing methodologies to perform relation extraction in Chinese.

After having read the reviews and followup replies by authors, the main criticisms of the paper still hold. In addition to the lack of technical contribution, I feel that the writing of the paper can be improved a lot, for example, I would like to see a table with some example entities and relations extracted. That said, with further improvements this paper could potentially be a good contribution to LREC which is focused on dataset creation.

In its current form, I recommend the paper to be rejected.",Paper Decision
AfCQMgT8b,r1lkKn4KDS,Learning Reusable Options for Multi-Task Reinforcement Learning,Reject,"This paper presents a novel option discovery mechanism through incrementally learning reusasble options from a small number of policies that are usable across multiple tasks.

The primary concern with this paper was with a number of issues around the experiments. Specifically, the reviewers took issue with the definition of novel tasks in the Atari context. A more robust discussion and analysis around what tasks are considered novel would be useful. Comparisons to other option discovery papers on the Atari domains is also required.

Additionally, one reviewer had concerns on the hard limit of option execution length which remain unresolved following the discussion.

While this is really promising work, it is not ready to be accepted at this stage.",Paper Decision
y2_yusx4li,Bkg0u3Etwr,Maxmin Q-learning: Controlling the Estimation Bias of Q-learning,Accept (Poster),The authors propose the use of an ensembling scheme to remove over-estimation bias in Q-Learning. The idea is simple but well-founded on theory and backed by experimental evidence. The authors also extensively clarified distinctions between their idea and similar ideas in the reinforcement learning literature in response to reviewer concerns.,Paper Decision
YgYtVAX3IP,S1lAOhEKPS,X-Forest: Approximate Random Projection Trees for Similarity Measurement,Reject,"This paper proposes a new method for measuring pairwise similarity between data points. The method is based on the idea that similarity between two data points to be the probability (over the randomness in constructing the trees) that they are close in a Random Projection tree. 

Reviewers found important limitations in this work, pertaining to clarity of mathematical statements and novelty. Unfortunately, the authors did not provide a rebuttal, so these concerns remain. Moreover, the program committee was made aware of the striking similarities between this submission and the preprint https://arxiv.org/abs/1908.10506 from Yan et al., which by itself would be grounds for rejection due to concerns of potential plagiarism. 
As a result, the AC recommends rejection at this time. ",Paper Decision
pP5IhY-F64,Bygadh4tDB,Low Bias Gradient Estimates for Very Deep Boolean Stochastic Networks,Reject,"Straight-Through is a popular, yet not theoretically well-understood, biased gradient estimator for Bernoulli random variables. The low variance of this estimator makes it a highly useful tool for training large-scale models with binary latents. However, the bias of this estimator may cause divergence in training, which is a significant practical issue. The paper develops a Fourier analysis of the Straight-Through estimator and provides an expression for the bias of the estimator in terms of the Fourier coefficients of the considered function. 

The paper in its current form is not good enough for publication, and the reviewers believe that the paper contains significant mistakes when deriving the estimator. Furthermore, the Fourier analysis seems unnecessary. ",Paper Decision
1rM4d0ErY,BJl2_nVFPB,Automatically Discovering and Learning New Visual Categories with Ranking Statistics,Accept (Poster),"The paper defines a methodology to discover unknown classes in a semi-supervised learning setting, based on: i) defining a proper representation based on self-supervision on all samples; ii) defining equivalence classes on the unlabelled samples, based on ranking statistics; iii) training supervised heads aimed to predict the labels (when available) and the equivalence class indices (when unlabelled). 

All reviewers agree that the ranking statistics-based heuristics is a quite innovative element of the paper. The extensive and careful experimental validation, with the ablation studies, establishes the merits of all ingredients. 

Therefore, I propose acceptance of this paper.
",Paper Decision
6l2YiXfJP,r1x3unVKPS,Support-guided Adversarial Imitation Learning,Reject,"The submission proposes a method for adversarial imitation learning that combines two previous approaches - GAIL and RED - by simply multiplying their reward functions. The claim is that this adaptation allows for better learning - both handling reward bias and improving training stability. 

The reviewers were divided in their assessment of the paper, criticizing the empirical results and the claims made by the authors. In particular, the primary claims of handling reward bias and reducing variance seem to be not well justified, including results which show that training stability only substantially improves when SAIL-b, which uses reward clipping, is used. 

Although the paper is promising, the recommendation is for a reject at this time. The authors are encouraged to clarify their claims and supporting experiments and to validate their method on more challenging domains.",Paper Decision
bWPj2DB5Q7,rJlnOhVYPS,Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification,Accept (Poster),"The paper proposes an unsupervised framework for domain adaptation in the context of person re-identification to reduce the effect of noisy labels. They use refined soft labels and propose a soft softmax-triplet loss to support learning with these soft labels. 

All reviewers have unanimously agreed to accept the paper and appreciated the comprehensive experiments on four datasets and ablation studies which give some insights about the proposed method. I agree with the assessment of the reviewers and recommend that this paper be accepted.",Paper Decision
WK_YXDRGsb,rJljdh4KDH,Multi-Scale Representation Learning  for Spatial Feature Distributions using Grid Cells,Accept (Spotlight),"This paper proposes to follow inspiration from NLP method that use position embeddings and adapt them to spatial analysis  that also makes use of both absolute and contextual information, and presents a representation learning approach called space2vec to capture absolute positions and spatial relationships of places. Experiments show promising results on real data compared to a number of existing approaches.
Reviewers recognize the promise of this approach and suggested a few additional experiments such as using this spatial encoding as part of other tasks such as image classification, as well as clarification and further explanations on many important points. Authors performed these experiments and incorporated the results in their revisions, further strengthening the submission. They also provided more analyses and explanations about the granularity of locality and motivation for their approach, which answered the main concerns of reviewers.
Overall, the revised paper is solid and we recommend acceptance.",Paper Decision
RtT8q6UrUE,r1eiu2VtwH,Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data,Accept (Poster),"This paper proposes Neural Oblivious Decision Ensembles, a formulation of ensembles of decision trees that is end-to-end differentiable and can use multi-layer representation learning. The reviewers are in agreement that this is a novel and useful tool, although there was some mild concern about the extent of the improvement over other methods. Post-discussion, I am recommending the paper be accepted.",Paper Decision
P03-2VrbWj,H1eqOnNYDH,Data augmentation instead of explicit regularization,Reject,"The paper explores the setting of *just* using data augmentation without an additional regularization term included.  The submission claims that comparatively good performance can be achieved with data augmentation alone.  The reviewers unanimously felt that the submission was not suitable for publication at ICLR.  The reasons included skepticism that augmentation without regularization is a useful setting to explore, as well as concerns about the experiments used to support the conclusions in the paper.  In particular, there were concerns that the experiments do not match best practice and that the error rates were too high.  Finally, there were concerns about the clarity of definitions of ""implicit"" and ""explicit"" regularization.",Paper Decision
QkPzyN_Bu,S1xKd24twB,SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards,Accept (Poster),"The authors present a simple alternative to adversarial imitation learning methods like GAIL that is potentially less brittle, and can skip learning a reward function, instead learning an imitation policy directly.  Their method has a close relationship with behavioral cloning, but overcomes some of the disadvantages of BC by encouraging the agent via reward to return to demonstration states if it goes out of distribution.  The reviewers agree that overcoming the difficulties of both BC and adversarial imitation is an important contribution.  Additionally, the authors reasonably addressed the majority of the minor concerns that the reviewers had.  Therefore, I recommend for this paper to be accepted.",Paper Decision
mlj8P0rSMs,SJeF_h4FwB,Label Cleaning with Likelihood Ratio Test,Reject,"This paper addresses a very interesting topic, and the authors clarified various issues raised by the reviewers.
However, given the high competition of ICLR2020, this paper is unfortunately still below the bar.
We hope that the detailed comments from the reviewers help you improve the paper for potential future submission.",Paper Decision
913RlcGax,BJgdOh4Ywr,Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks,Reject,"The main concern raised by reviewers is limited novelty, poor presentation, and limited experiments. All the reviewers appreciate the difficulty and importance of the problem. The rebuttal helped clarify novelty, but the other concerns remain.",Paper Decision
Xr8Y-NwyZi,S1ldO2EFPr,Graph Neural Networks Exponentially Lose Expressive Power for Node Classification,Accept (Spotlight),"The paper provides a theoretical analysis of graph neural networks, as the number of layers goes to infinity. For the graph convolutional network, they relate the expressive power of the network with the graph spectra. In particular for Erdos-Renyi graphs, they show that very deep graphs lose information, and propose a new weight normalization scheme based on this insight.

The authors responded well to reviewer comments. It is nice to see that the open review nature has also resulted in a new connection. Unfortunately one of the reviewers did not engage further in the discussion with respect to the author rebuttals.

Overall, the paper provides a nice theoretical analysis of a widely used graph neural network architecture, and characterises its behaviour on a popular class of graphs. The fact that the theory provides a new approach for weight normalization is a bonus.",Paper Decision
pyypzkoXPB,HJx_d34YDB,VIDEO AFFECTIVE IMPACT PREDICTION WITH MULTIMODAL FUSION AND LONG-SHORT TEMPORAL CONTEXT,Reject,"There is no author response for this paper. The paper addresses the affective analysis of video sequences in terms of continual emotions of valence and arousal. The authors propose a multi-modal approach (combining modalities such as audio, pose estimation, basic emotions and scene analysis) and a multi-scale temporal feature extractor (to capture short and long temporal context via LSTMs) to tackle the problem. All the reviewers and AC agreed that the paper lacks (1) novelty, as the proposed approach is a combination of the existing well-studied techniques without explanations why and when this could be advantageous beyond the considered task, (2) clarity and motivation -- see R2’s and R3’s concerns and suggestions on how to improve. We hope the reviews are useful for improving the paper.
",Paper Decision
LYZsytwSvz,r1evOhEKvH,Graph inference learning for semi-supervised classification,Accept (Poster),"The authors propose a graph inference learning framework to address the issues of sparse labeled data in graphs. The authors use structural information and node attributes to define a structure relation which is then use to infer unknown labels from known labels. The authors demonstrate the effectiveness of their approach on four benchmark datasets.

The approach presented in the paper is sound and the empirical results are convincing. All reviewers have given a positive rating for this paper. Two reviewers had some initial concerns about the paper but after the rebuttal they have acknowledged the answers given by the authors and adjusted their scores. R1 still has concerns about the motivation of the paper and I request the authors to adequately address this in their final version.",Paper Decision
CnoDJzWRFk,BygPO2VKPH,Sparse Coding with Gated Learned ISTA,Accept (Spotlight),"The paper extends LISTA by introducing gain gates and overshoot gates, which respectively address underestimation of code components and compensation of small step size of LISTA. The authors theoretically analyze these extensions and backup the effectiveness of their proposed algorithm with encouraging empirical results. All reviewers are highly positive on the contributions of this paper, and appreciate the rigorous theory which is further supported by convincing experiments. All three reviewers recommended accept.
",Paper Decision
PXinSegCOe,SJeLO34KwS,Dimensional Reweighting Graph Convolution Networks,Reject,"As Reviewer 2 pointed out in his/her response to the authors' rebuttal, this paper (at least in current state) has significant shortcomings that need to be addressed before this paper merits acceptance.",Paper Decision
lqKzFYDjK,rylUOn4Yvr,ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE,Reject,"The paper proposes a gradient rescaling method to make deep neural network training more robust to label noise. The intuition of focusing more on easier examples is not particularly new, but empirical results are promising. On the weak side, no theoretical justification is provided, and the method introduces extra hyperparameters that need to be tuned. Finally, more discussions on recent SOTA methods (e.g., Lee et al. 2019) as well as further comprehensive evaluations on various cases, such as asymmetric label noise, semantic label noise, and open-set label noise, would be needed to justify and demonstrate the effectiveness of the proposed method. ",Paper Decision
roCsz5_dND,BJlLdhNFPr,Explaining A Black-box By Using A Deep Variational Information Bottleneck Approach,Reject,"The authors present a system-agnostic interpretable method based on the idea of that provides a brief (=compressed) but comprehensive (=informative) explanation. Their system is build upon the idea of VIB. The authors compare against 3 state-of-the-art interpretable machine learning methods and the evaluation is terms of interpretability (=human understandable) and fidelity (=accuracy of approximating black-box model). Overall, all reviewers agreed that the topic of model interpretability is an important one and the novel connection between IB and interpretable data-summaries is a very natural one.  

This manuscript has generated a lot of discussion among the reviewers during the rebuttal and there are a number of concerns that are currently preventing me from recommending this paper for acceptance. The first concern relates to the lack of comparison against attention methods (I agree with the authors that this is a model-specific solution whereas they propose a model-agnostic one), however attention is currently the elephant in room and the first thing someone thinks of when thinking of interpretability. As such, the authors should have presented such a comparison. The second concern relates to the human evaluation protocol which could be significantly improved  (Why 100 samples from all models but 200 for VIBI? Given the small set of results, are these model differences significant? Similarly, assuming that we have multiple annotations per sample, what is the variance in the annotations?).

This paper is currently borderline and given reviewers' concerns and the limited space in the conference program I cannot recommend acceptance of this paper. ",Paper Decision
rCF1Q2gBJ,rJgBd2NYPH,Learning deep graph matching with channel-independent embedding and Hungarian attention,Accept (Poster),"This paper proposed a new graph matching approach. The main contribution is a Hungarian attention mechanism, which dynamically generates links in computational graph. The resulting matching algorithm is tested on vision tasks.

The main concern of reviews is that the general matching algorithm is only tested on vision tasks. The authors partially addressed this problem by providing new experimental results with only geometric edge features. Other comments of Blind Review #2 are about some minor questions, which have also been answered by the authors.

Overall, this paper proposed a promising graph match approach and I tend to accept it. 
",Paper Decision
RhQmFLLhDd,rklVOnNtwH,Out-of-Distribution Detection Using Layerwise Uncertainty in Deep Neural Networks,Reject,"The paper proposes a method for OOD detection which leverages the uncertainties associated with the features at the intermediate layers (and not just the output layer).

All the reviewers agreed that while this is an interesting direction, the paper requires more work before it can be accepted. In particular, the reviewers raised several concerns about other relevant baselines, some of the reported empirical results, and clarity of the explanation.

I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue.
",Paper Decision
CUaVYR84b7,SJx4O34YvS,Semantics Preserving Adversarial Attacks,Reject,"This paper describes a method for generating adversarial examples from images and text such that they maintain the semantics of the input.

The reviewers saw a lot of value in this work, but also some flaws.  The review process seemed to help answer many questions, but a few remain: there are some questions about the strength of the empirical results on text after the author's updates. Wether the adversarial images stay on the manifold is questioned (are blurry or otherwise noisy images ""on manifold""?).  One reviewer raises good questions about the soundness of the comparison to the Song paper.

I think this review process has been very productive, and I hope the authors will agree.  I hope this feedback helps them to improve their paper.",Paper Decision
MCjH9hMEbB,HJe7unNFDH,Scaling Up Neural Architecture Search with Big Single-Stage Models,Reject,"This paper presents a NAS method that avoids having to retrain models from scratch and targets a range of model sizes at once. The work builds on Yu & Huang (2019) and studies a combination of many different techniques.
Several baselines use a weaker training method, and no code is made available, raising doubts concerning reproducibility.

The reviewers asked various questions, but for several of these questions (e.g., running experiments on MNIST and CIFAR) the authors did not answer satisfactorily. Therefore, the reviewer asking these questions also refuses to change his/her rating. 

Overall, as AnonReviewer #1 points out, the paper is very empirical. This is not necessarily a bad thing if the experiments yield a lot of insight, but this insight also appears limited. Therefore, I agree with the reviewers and recommend rejection.",Paper Decision
3RxM_EGaLz,H1gz_nNYDS,AutoSlim: Towards One-Shot Architecture Search for Channel Numbers,Reject,"The paper presents a simple one-shot approach on searching the number of channels for deep convolutional neural networks. It trains a single slimmable network and then iteratively slim and evaluate the model to ensure a minimal accuracy drop. The method is simple and the results are promising. 

The main concern for this paper is the limited novelty. This work is based on slimmable network and the iterative slimming process is new, but in some sense similar to DropPath. The rebuttal that PathNet ""has not demonstrated results on searching number of channels, and we are among the first few one-shot approaches on architectural search for number of channels"" seem weak.",Paper Decision
4cjMdXZJ0,r1gzdhEKvH,Neural Linear Bandits: Overcoming Catastrophic Forgetting through Likelihood Matching,Reject,"Reviewers found the problem statement having merit, but found the solution not completely justifiable. Bandit algorithms often come with theoretical justification because the feedback is such that the algorithm could be performing horribly without giving any indication of performance loss. With neural networks this is obviously challenging given the lack of supervised learning guarantees, but reviewers remain skeptical and prefer not to speculate based on empirical results. ",Paper Decision
7jxCANGwsP,S1gfu3EtDr,EgoMap: Projective mapping and structured egocentric memory for Deep RL,Reject,"This paper presents a spatially structured neural memory architecture that supports navigation tasks.  The paper describes a complex neural architecture that integrates visual information, camera parameters, egocentric velocities, and a differentiable 2D map canvas.  This structure is trained end-to-end with A2C in the VizDoom environment.  The strong inductive priors captured by these geometric transformations is demonstrated to be effective on navigation-related tasks in the experiments in this environment.

The reviewers found many strengths and a few weaknesses in this paper.  One strength is that the paper pulls together many related ideas in the mapping literature and combines them in one integrated system.  The reviewers liked the method's ability to leverage semantic reasoning and spatial computation.  They liked the careful updating of the maps and the use of projective geometry.  

The reviewers were less convinced of the generality of this method.  The lack of realism in these simulated environments left the reviewers unconvinced that the benefits observed from using projective geometry in this setting will continue to hold in more realistic environments.   The use of fixed geometric transformations with RGBD inputs instead of learned transformations also makes this approach less general than a system that could handle RGB inputs.  Finally, the reviewers noted that the contributions of this paper are not well aligned with the paper's claims.

This paper is not yet ready for publication as the paper's claims and experiments were not sufficiently convincing to the reviewers. ",Paper Decision
vodTpHk4B,BJe-unNYPr,Accelerated Information Gradient flow,Reject,"The paper makes its contribution by deriving an accelerated gradient flow for the Wasserstein distances. It is technically strong and demonstrates it applicability using examples fo Gaussian distributions and logistic regression.

Reviewer 3 provided a deep technical assessment, pointing out the relevance to our ML community since these ideas are not yet widespread, but had concerns about the clarity of the paper. Reviewer 2 had similar concerns about clarity, and was also positive about its relevance to the ML community. The authors provided details responses to the technical questions posed by the reviewers. The AC believes that such work is a good fit for the conference. The reviewers felts that this paper does not yet achieve the aim of making this work more widespread and needs more focus on communication.

This is a strong paper and the authors are encouraged to address the accessibility questions. We hope the review offers useful points of feedback for their future work.",Paper Decision
4mgEyD8emX,BJxg_hVtwH,StructPool: Structured Graph Pooling via Conditional Random Fields,Accept (Poster),"The paper proposed an operation called StructPool for graph-pooling by treating it as node clustering problem (assigning a label from 1..k to each node) and then use a pairwise CRF structure to jointly infer these labels. The reviewers all think that this is a well-written paper, and the experimental results are adequate to back up the claim that StructPool offers advantage over other graph-pooling operations. Even though the idea of the presented method is simple and it does add more (albeit by a constant factor) to the computational burden of graph neural network, I think this would make a valuable addition to the literature.",Paper Decision
rjZ3LFLNmC,BylldnNFwS,On the Decision Boundaries of Deep Neural Networks: A Tropical Geometry Perspective,Reject,"This paper studies the decision boundaries of a certain class of
neural networks (piecewise linear, non-linear activation functions)
using tropical geometry, a subfield of algebraic geometry that leverages piece-wise linear structures. 
Building on earlier work, such piecewise linear networks are shown to be represented as
a tropical rational function. This characterisation is used to explain different phenomena of neural
network training, such as the 'lottery ticket hypothesis', network
pruning, and adversarial attacks.

This paper received mixed reviews, owing to its very specialized area. Whereas R1 championed the submission 
for its technical novelty, the other reviewers felt the current exposition is too inaccessible and some application areas are not properly addressed. The AC shares these concerns, recommends rejection and strongly encourages the authors to address the reviewers concerns in the next iteration. 
",Paper Decision
YZ8xf1k5vA,ByxJO3VFwB,Probabilistic modeling the hidden layers of deep neural networks,Reject,"This paper makes a claim that the iid assumption for NN parameters does not hold. The paper then expresses the joint distribution as a Gibbs distribution and PoE. Finally, there are some results on SGD as VI. Reviewers have mixed opinion about the paper and it is clear that the starting point of the paper (regarding iid assumption) is unclear. I myself read through the paper and discussed this with the reviewer, and it is clear that there are many issues with this paper.

Here are my concerns:
- The parameters of DNN are not iid *after* training. They are not supposed to be. So the empirical results where the correlation matrix is shown does not make the point that the paper is trying to make.
- I agree with R2 that the prior is subjective and can be anything, and it is true that the ""trained"" NN may not correspond to a GP. This is actually well known which is why it is difficult to match the performance of a trained GP and trained NN.
- The whole contribution about connection to Gibbs distribution and PoE is not insightful. These things are already known, so I don't know why this is a contribution.
- Regarding connection between SGD and VI, they do *not* really prove anything. The derivation is *wrong*. In eq 85 in Appendix J2, the VI problem is written as KL(P||Q), but it should be KL(Q||P). Then this is argued to be the same as Eq. 88 obtained with SGD. This is not correct.

Given these issues and based on reviewers' reaction to the content, I recommend to reject this paper. ",Paper Decision
xReF5YaNbB,H1eCw3EKvH,On the Weaknesses of Reinforcement Learning for Neural Machine Translation,Accept (Poster),"In my opinion, the main strength of this work is the theoretical analysis and some observations that may be of great interest to the NLP community in terms of better analyzing the performance of RL (and ""RL-like"") methods as optimizers. The main weakness, as pointed out by R3, the limited empirical analysis.

I would urge the authors to take R3's advice and attempt insofar as possible to broaden the scope of the empirical analysis in the final. I believe that this is important for the paper to be able to make its case convincingly.

Nonetheless, I do think that the paper makes a significant contribution that will be of interest to the community, and should be presented at ICLR. Therefore, I would recommend for it to be accepted.",Paper Decision
4NGYY_v5o,rygRP2VYwB,Stochastically Controlled Compositional Gradient for the Composition problem,Reject,All the reivewers find the similarity between this paper and the references in terms of the algorithm and the proof. The theoretical results may not better than the existing results.,Paper Decision
BPC3AJAY1n,rkgpv2VFvr,Sharing Knowledge in Multi-Task Deep Reinforcement Learning,Accept (Poster),"This paper considers the benefits of deep multi-task RL with shared representations, by deriving bounds for multi-task approximate value and policy iteration bounds. This shows both theoretically and empirically that shared representations across multiple tasks can outperform single task performance.

There were a number of minor concerns from the reviewers regarding relation to prior work and details of the analysis, but these were clarified in the discussion. This paper adds important theoretical analysis to the literature, and so I recommend it is accepted.",Paper Decision
gGqW8uewU,rkg6PhNKDr,HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE?,Reject,"The authors demonstrate that starting from the 3rd epoch, freezing a large fraction of the weights (based on gradient information), but not entire layers, results in slight drops in performance.

Given existing literature, the reviewers did not find this surprising, even though freezing only some of a layers weights has not been explicitly analyzed before. Although this is an interesting observation, the authors did not explain why this finding is important and it is unclear what the impact of such a finding will be. The authors are encouraged to expand on the implications of their finding and theoretical basis for it. Furthermore, reviewers raised concerns about the extensiveness of the empirical evaluation.

This paper falls below the bar for ICLR, so I recommend rejection.",Paper Decision
IyHNewEA-V,HkehD3VtvS,"Deep Reasoning Networks:  Thinking Fast and Slow, for Pattern De-mixing",Reject,"The paper received mixed reviews of WR (R1), WR (R2) and WA (R3). AC has carefully read all the reviews/rebuttal/comments and examined the paper. AC agrees with R1 and R2's concerns, specifically around overclaiming around reasoning. Also AC was unnerved, as was R2 and R3, by the notion of continuing to train on the test set (and found the rebuttal unconvincing on this point). Overall, the AC feels this paper cannot be accepted. The authors should remove the unsupported/overly bold claims in their paper and incorporate the constructive suggestions from the reviewers in a revised version of the paper.",Paper Decision
AAubawzJ-G,HkenPn4KPH,When Does Self-supervision Improve Few-shot Learning?,Reject,"Reviewers agree that this paper contains interesting results and simple, but good ideas. However, a few severe concerns were raised by reviewers. Most prominent one was the experiment set up - authors use a pre trained ResNet101 (which has seen many classes of Imagenet) for testing which makes is unclear how well their proposed method would work for unlabeled pool of dataset that classifiers has never seen.
 While authors claim that their the dataset used for testing was disjoint from Imagenet, a reviewer pointed out that dogs dataset, bird datasets both state that they overlap with Imagenet. A few other concerns are raised (need more meaningful metric in Figure 4d, which wasn’t addressed in rebuttal). We look forward to seeing an improved version of the paper in your future submissions. ",Paper Decision
RhBgS3THES,HygnDhEtvr,Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation,Accept (Poster),"The reviewers found this paper on improving NLG using a graph-to-sequence architecture interesting and the results impressive. While I would personally have preferred to see further evaluation of this model on another NLG task, I think it would be overstepping in my role as AC to go against the reviewer consensus. The paper is clearly acceptable.",Paper Decision
hBs16wkOeP,HyloPnEKPr,Context-aware Attention Model for Coreference Resolution,Reject,"Main content:

Blind review #2 summarizes it well:

This paper extends the neural coreference resolution model in Lee et al. (2018) by 1) introducing an additional mention-level feature (grammatical numbers), and 2) letting the mention/pair scoring functions attend over multiple mention-level features. The proposed model achieves marginal improvement (0.2 avg. F1 points) over Lee et al., 2018, on the CoNLL 2012 English test set.

--

Discussion:

All reviewers rejected.

--

Recommendation and justification:

The paper must be rejected due to its violation of blind submission (the authors reveal themselves in the Acknowledgments).

For information, blind review #2 also summarized well the following justifications for rejection:

I recommend rejection for this paper due to the following reasons:
- The technical contribution is very incremental (introducing one more features, and adding an attention layer over the feature vectors). 
- The experiment results aren't strong enough. And the experiments are done on only one dataset.
- I am not convinced that adding the grammatical numbers features and the attention mechanism makes the model more context-aware.",Paper Decision
ryZbbeOxeH,HkgsPhNYPS,SELF: Learning to Filter Noisy Labels with Self-Ensembling,Accept (Poster),The authors addressed the issues raised by the reviewers; I suggest to accept this paper.,Paper Decision
QUmQEK9py,BJgcwh4FwS,Neural Maximum Common Subgraph Detection with Guided Subgraph Extraction,Reject,"This paper proposed graph neural networks based approach for subgraph detection. The reviewers find that the overall the paper is interesting, however further improvements are needed to meet ICLR standard: 
1. Experiments on larger graph. Slight speedup in small graphs are less exciting.  
2. It seems there's a mismatch between training and inference. 
3. The stopping criterion is quite heuristic. 
",Paper Decision
1CBLNgZcA,HJxcP2EFDS,Amharic Negation Handling,Reject,"Main content:

This paper presents negation handling approaches for Amharic sentiment classification.

--

Discussion:

All reviewers agree the paper is poorly written, uses outdated approaches, and requires better organization and formatting.

--

Recommendation and justification:

This paper after more work might be better submitted in an NLP workshop on low resource languages, rather than ICLR which is more focused on new machine learning methods.",Paper Decision
nw9oWMz2_z,rygtPhVtDS,Noise Regularization for Conditional Density Estimation,Reject,"This paper has a mixture of weak reviews, the majority of which lean towards reject. All reviews mention a lack of novelty, and 2 of 3 a lack of support in experiments. While the authors argue, perhaps legitimately, for the novelty of the paper with respect to current literature, this is not convincing in the exposition. I recommend that the authors improve the justification for the novelty of their methodology, and strengthen the experiments to convince reviewers. As it stands, this paper is not quite ready for publication.",Paper Decision
a-jjRCjTH_,BylKwnEYvS,Star-Convexity in Non-Negative Matrix Factorization,Reject,"The paper derives results for nonnegative-matrix factorization along the lines of recent results on SGD for DNNs, showing that the loss is star-convex towards randomized planted solutions.

Overall, the paper is relatively well written and fairly clear.  The reviewers agree that the theoretical contribution of the paper could be improved (tighten bounds) and that the experiments can be improved as well. In the context of other papers submitted to ICLR I therefore recommend to reject the paper.

",Paper Decision
bgsP3IHtDQ,rJgFDnEYPr,Count-guided Weakly Supervised Localization Based on Density Map,Reject,"This work proposes a new regularization method for weakly supervised localization based on counting.
Reviewers agree that this is an interesting topic but the experimental validation is weak (qualitative, lack of baselines), and the contribution too incremental.
Therefore, we recommend rejection.",Paper Decision
s6kX83JFzL,HyeuP2EtDB,Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization,Reject,"The paper proposes an algorithm for zero-shot generalization in RL via learning a scoring a function from.

The reviewers had mixed feelings, and many were not from the area. A shared theme was doubts about the significance of the experimental setting, and also the generality of the approach.

As this is my field, I read the paper, and recommend rejection at this time. The proposed method is quite laborious and requires quite a bit of assumptions on the environments to work, as well as fine tuning parameters for each considered task (number of regions, etc). I also agree that the evaluation is not convincing -- stronger baselines need to be considered and the experiments to better address the zero-shot transfer aspect that the paper is motivated by. I encourage the authors to take the review feedback into account and submit a future version to another venue.",Paper Decision
NO7fh8U9_Y,HkeuD34KPH,SSE-PT: Sequential Recommendation Via Personalized Transformer,Reject,"The paper proposes to improve sequential recommendation by extending SASRec (from prior work) by adding user embedding with SSE regularization.  The authors show that the proposed method outperforms several baselines on five datasets.

The paper received two weak accepts and one reject.  Reviewers expressed concerns about the limited/scattered technical contribution.  Reviewers were also concerned about the quality of the experiment results and need to compare against more baselines.  After examining some related work, the AC agrees with the reviewers that there is also many recent relevant work such as BERT4Rec that should be cited and discussed.  It would make the paper stronger if the authors can demonstrate that adding the user embedding to another method such as BERT4Rec can improve the performance of that model.  Regarding R3's concerns about the comparison against HGN, the authors indicates there are differences in the length of sequences considered and that some method may work better for shorter sequences while their method works better for longer sequences.  These details seems important to include in the paper. 

In the AC's opinion, the paper quality is borderline and the work is of limited interest to the ICLR community.  Such would would be more appreciated in the recommender systems community.  The authors are encouraged to improve the paper with improved discussion of more recent work such as BERT4Rec, add comparisons against these more recent work, incorporate various suggestions from the reviewers, and resubmit to an appropriate venue.",Paper Decision
tpqZaPwrv,rJxvD3VKvr,Wide Neural Networks are Interpolating Kernel Methods: Impact of Initialization on Generalization,Reject,"This paper proves that fully-connected wide ReLU-NNs trained with squared loss can be decomposed into two parts: (1) the minimum complexity solution of an interpolating kernel method, and (2) a term depends heavily on the initialization. The main concerns of the reviewers include (1) the contribution are not significant at all given prior work; (2) flawed proof,  and (3) lack the comparison with prior work. Even the authors addressed some of the concerns in the revision, it still does not gather sufficient support from the reviewers after author response. Thus I recommend reject.",Paper Decision
MCAKzZPaAv,SJlDDnVKwS,Improving Evolutionary Strategies with Generative Neural Networks,Reject,"Evolutionary strategies are a popular class of method for black-box gradient-free optimization and involve iteratively fitting a distribution from which to sample promising input candidates to evaluate.  CMA-ES involves fitting a Gaussian distribution and has achieved state-of-the-art performance on a variety of black-box optimization benchmarks when the underlying function is cheap to evaluate.  In this work the authors replace this distribution instead with a much more flexible deep generative model (i.e. NICE). They demonstrate empirically that this method is effective on a number of synthetic global optimization benchmarks (e.g. Rosenbrock) and three direct policy search reinforcement learning problems.  The reviewers all believe the paper is above borderline for acceptance.  However, two of the reviewers said they were on the low end of their respective scores (i.e. one wanted to give a 5 instead of a 6 and another a 7 instead of 8.)  A major issue among the reviewers was the experiments, which they noted were simple and not very convincing (with one reviewer disagreeing).  The synthetic global optimization problems do seem somewhat simple.  In the RL problems, it's not obvious that the proposed method is statistically significantly better, i.e. the error bars are overlapping considerably.   Thus the recommendation is to reject.  Hopefully stronger experiments and incorporating the reviewer comments in the manuscript will make this a stronger paper for a future conference.",Paper Decision
ol7NdtAi3,BJlLvnEtDB,Analysis and Interpretation of Deep CNN Representations as Perceptual Quality Features,Reject,"This paper aims to analyze CNN representations in terms of how well they measure the perceptual severity of image distortions.  In particularly, (a) sensitivity to changes in visual frequency and (b) orientation selectivity was used. Although the reviewers agree that this paper presents some interesting initial findings with a promising direction, the majority of the reviewers (three out of four) find that the paper is incomplete, raising concerns in terms of experimental settings and results. Multiple reviewers explicitly asked for additional experiments to confirm whether the presented empirical results can be used to improve results of an image generation. Responding to the reviews, the authors added a super-resolution experiment in the appendix, which the reviewers believe is the right direction but is still preliminary.

Overall, we believe the paper reports interesting findings but it will require a series of additional work to make it ready for the publication.",Paper Decision
ZKtIF9TsKC,BkxUvnEYDH,Program Guided Agent,Accept (Spotlight),"This paper provides a fascinating hybridization approach to incorporating programs as priors over policies which are then refined using deep RL. The reviewers were, at the end of the discussion, all in favour of acceptance (with the majority strongly in favour). An excellent paper I hope to see included in the conference.",Paper Decision
Twvf8XCqaF,BklSwn4tDH,Prestopping: How Does Early Stopping Help Generalization Against Label Noise?,Reject,"This paper focuses on avoiding overfitting in the presence of noisy labels. The authors develop a two phase method called pre-stopping based on a combination of early stopping and a maximal safe set. The reviewers raised some concern about illustrating maximal safe set for all data sets and suggest comparisons with more baselines. The reviewers also indicated that the paper is missing key relevant publications. In the response the authors have done a rather through job of addressing the reviewers comments. I thank them for this. However, given the limited time some of the reviewers comments regarding adding new baselines could not be addressed. As a result I can not recommend acceptance because I think this is key to making a proper assessment. That said, I think this is an interesting with good potential if it can outperform other baselines and would recommend that the authors revise and resubmit in a future venue.",Paper Decision
mlzOkObmMb,BklSv34KvB,"Carpe Diem, Seize the Samples Uncertain ""at the Moment"" for Adaptive Batch Selection",Reject,"The authors propose a new mini-batch selection method for training deep NNs. Rather than random sampling, selection is based on a sliding window of past model predictions for each sample and uncertainty about those samples. Results are presented on MNIST and CIFAR.

The reviewers agreed that this is an interesting idea which was clearly presented, but had concerns about the strength of the experimental results, which showed only a modest benefit on relatively simple datasets. In the rebuttal period, the authors added an ablation study and additional results on Tiny-ImageNet. However, the results on the new dataset seem very marginal, and R1 did not feel that all of their concerns were addressed. I’m inclined to agree that more work is required to prove the generalizability of this approach before it’s suitable for acceptance.
",Paper Decision
__jo5mffx6,Syx4wnEtvH,Large Batch Optimization for Deep Learning: Training BERT in 76 minutes,Accept (Poster),"This paper presents a range of methods for over-coming the challenges of large-batch training with transformer models.  While one reviewer still questions the utility of training with such large numbers of devices, there is certainly a segment of the community that focuses on large-batch training, and the ideas in this paper will hopefully find a range of uses. ",Paper Decision
