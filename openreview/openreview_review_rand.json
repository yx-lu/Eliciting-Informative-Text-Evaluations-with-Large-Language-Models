[{"uid": "H1gUil1TYS", "paper_title": "On Symmetry and Initialization for Neural Networks", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies the problem of learning the class of symmetric boolean function, that is, functions that depend only on |x| = \\sum_i x_i. The paper shows that with proper initialization, one-hidden layer over-parametrized networks can learn this class of functions. The main observation that the authors make is that the last layer weights are updated as in the Perceptron algorithm and as long as the first layer has learned a large-margin representation, the first-layer weights do not change much. The authors experimentally validate their theory and additionally show that random initialization fails to converge to a low test-error solution while their special initialization works.\n\nOverall, the main complexity arises from handling training of both layers and this is cleverly analyzed. However I am leaning towards rejection as the underlying problem does not seem well-motivated. The class of symmetric boolean functions can be modeled as a univariate function by mapping x -> |x| which is easy to solve in the no noise setting analyzed by the paper. Also, in terms of learning with neural networks, as the authors point out, one can learn this class by training only the last layer. It is unclear why this setup warrants the use of a neural network for training. The problem would be more challenging and interesting for the class of symmetric (permutation invariant) functions on the real domain where using symmetry in the architecture/initialization can potentially give gains.\n\nWriting - Proofs are mostly clear however it would help to add more details in the proof of the main theorem (especially to argue about the use of the Perceptron convergence theorem for the changing representations). Also, the introduction needs to further motivate the setup and its relevance to neural networks.\n\nRepresentation - Regarding the representation for indicators using ReLUs, one could use a simpler and more standard representation. In prior work the indicator is represented using a difference of ReLUs, 1[|x| >= i] = ReLU(|x| - i + 1) - ReLU(|x| - i) and 1[|x| = i] = 1[|x| >= i] - 1[|x| >= i+1] = ReLU(|x| - i + 1) - 2 ReLU(|x| - i) + ReLU(|x| - i - 1). Now one can express \\sum_{i \\in A} 1[|x| = i] by summing up these indicators and adding a bias term of -0.5 will make the sign be the correct value. Note that this would overall require only n + 2 hidden units with the weights now being bounded by constants. This would still have a margin of \\Omega(1/n). Is there a particular reason for the choice of representation in the paper?\n\nExperiments - The plots are hard to parse and inconsistent. Firstly, it would be better to use line plots instead of scatter plots to highlight the trend. Secondly, the x-axis needs to be sampled more frequently. The number of epochs seem to be varying in the experiments, please make that consistent. Lastly, the important plots need to be moved to the main paper. Are the experiments for multiple runs or just a single run?", "belong_id": "Skeh-xBYDH"}, {"uid": "Bkg_Pp2rcB", "paper_title": "On Symmetry and Initialization for Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "PAPER SUMMARY: This paper studies the problem of training a single hidden layer neural network to represent an arbitrary symmetric function. These are functions $f : \\{0,1\\}^n \\to \\{-1, 1\\}$ which are invariant to permutations in the input coordinates. The authors' main result (Theorem 1) shows that if you take a single hidden layer network with $O(n)$ hidden units and initialize the weights in a particular way, then for any symmetric $f$, SGD training will converge to an empirical risk minimizer with guaranteed small generalization error. On the other hand, the authors' experiments suggest that arbitrary symmetric functions are not learnable from random initialization. Taken together, these results point to the importance of designing network architectures/ initializations that respect the structure in the function class you're trying to represent.\n\nREVIEW SUMMARY: I lean towards rejecting this paper however, because I am not convinced of the results' significance. We already know how to learn symmetric functions (see Exercise 3.26 in Mohri et al., 2018). The authors' results show that we can inject this knowledge into a neural network at initialization, and then run SGD without making things too much worse. I do not see how these ideas might apply to more substantial learning problems where our prior knowledge is less precise. Moreover, while the proofs are clearly presented overall, I have one concern with a key step in Lemma 4.\n\nMAJOR COMMENTS:\n\n1) The key property of symmetric functions is that their output depends only on $|x|$. Thus, one can first extract 'cardinality features' $x \\mapsto |x|$, after which learnability follows by standard generalization theory results (as the authors note in the proof of Theorem 1).\n\nThe basic idea of Theorem 1 then seems to be to realize this feature map as the hidden layer of a single hidden layer ReLU network (this is essentially what the initialization does) and then show that running SGD will not move the weights too far from the initialization (Lemma 4).\n\n(a) First, I think it would be helpful to the reader if the authors could make this intuition more explicit. In the submission the authors do not give much explanation for the choice of initialization.\n\n(b) Second, because this is a learning problem we already know how to solve, the results seems a little contrived. I do not see how these ideas could extend to more challenging cases where our prior knowledge of symmetry (e.g. translation invariance) does not by itself lead to an algorithm with efficient learnability guarantees.\n\n2) I could not follow one step in the proof of Lemma 4 (used to show that SGD does not move the weights too far from the initialization). Why does Theorem 2 imply that the number of updates is at most $20 R^2 / \\gamma^2$? In Theorem 2, $R$ is fixed whereas in Lemma 4 it varies with $t$. To me this seems important, since without a bound on the number of steps it is unclear how you can control how far the embeddings move.\n\nMINOR COMMENTS\n\n3) In the statement of Lemma 4, linear separability of $V$ should be with respect to some fixed partition $Y$?.\n\n4) In Figure 5, why is empirical error not decreasing over epochs?\n\n5) I think the figures referenced in the text should be in the paper, not the appendix.\n\nMohri, M., Rostamizadeh, A., & Talwalkar, A. (2018). Foundations of machine learning. MIT press.", "belong_id": "Skeh-xBYDH"}, {"uid": "ryeOBfijdB", "paper_title": "Demonstration Actor Critic", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThis paper studied reinforcement learning from demonstration. Given a set of \nexpert demonstrations, this work provides a policy-dependent reward shaping objective that\ncan utilize demonstration information and preserves policy optimality, policy improvement,\nand the convergence of policy iteration at the same time, under the assumption\nthat expert policy is optimal and stochastic.\nThe main advantage of the proposed method is that the reward shaping function\nis related to the current policy. \nA practical algorithm based on theoretical derivation is provided. The authors conducted sufficient experimental results to demonstrate that the proposed method is effective,\ncomparing with a set of advanced baselines.\n\nI recommend acceptance: \nPrevious works on RLfD usually empirically incorporated a regularization\nto the RL objective, while those works didn't discuss whether this\nregularization will lead to sub-optimal policy or not. This paper discussed\nhow to use the demonstration information to do exploration and maintain\npolicy invariance at the same time, with a relatively strong assumption.\nUsing the framework from SAC, the\nalgorithm is shown to converge to the optimal via policy iteration, in\ntabular case. This work also developed a practical expert policy\nsupport estimation algorithm to measure the uncertainty of\nexpert policy. Utilizing the adversarial training framework, \nthe explicit computation of expert policy is avoided. \nThe authors conducted sufficient experiments to demonstrate\nthe effectiveness of the proposed method, compared with the state-of-the-art in RLfD. \n\n\nTechnical concerns:\nThe stochasticity assumption of expert policy in Asm. 1 can be contradicted\nwith that expert policy is optimal in policy invariance proof.  \nThis paper works on a problem of infinite horizon discounted MDP. \nAccording to Puterman [1994], Theorem 6.2.7, there always exists a\ndeterministic stationary policy \\pi that is optimal. Or intuitively,\nif we find the optimal value function via Bellman optimality equation,\nthe optimal policy is acting greedily (deterministic). The provided theorems are\nnot compatible with the MDP where only deterministic optimal policy exists.\nIt is not clear that in what type of MDPs the optimal stochastic policy exists and\nit can satisfy Asm. 1. \nCould the authors clearly specify the applicable problem settings?\n\nIf the asm 1 is satisfied, what is the necessity to incorporate the\nindicator function in Eq 4.? Since p(s) > 0 for all s, following strong stochasticity policy.\nFor any trajectory \\tau, p(\\tau) = p(s)\\Pi_t p(s_{t+1}|s_{t}, a_t)\\pi_E(a_t|s_t) > 0. \n\nThe proof of Theorem 2 is similar to the proof in Proposition 1, [1], though in a\ndifferent context. It would be better to have a citation?\n\nExperiments:\nIt would be more convincing to show the performance of behavior cloning policy using\nexpert trajectories. \n\n[1] Goodfellow, Ian, et al. 'Generative adversarial nets.' Advances in neural information processing systems. 2014.\n\n", "belong_id": "BklRFpVKPH"}, {"uid": "rkx4lgkTFB", "paper_title": "Demonstration Actor Critic", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method for doing RL from demonstrations in continuous control tasks. It combines both an augmented reward for minimizing the KL between the policy and the expert actions as well as directly minimizing that KL in the policy. Results on 5 sparse reward mujoco tasks show that it out-performs other related methods.\n\n\nThe motivation for the paper is difficult to follow. They claim that using demonstration data in a supervised manner 'cannot generalize supervision signal over those states unseen in the demonstrations,' but most of these approaches are using neural networks and definitely are generalizing those signals to other states. Whether they're generalizing accurately or not is a different question. In contrast, they say that reward shaping approaches do not suffer that problem because they evaluate trajectories rather than states, but there will still be a problem of generalizing to new trajectories. \n\nThe abstract is even more confusing as it tries to jump straight into the issues with these approaches without any explanation. I don't think there's enough space in the abstract to go into that level of detail.\n\nThe description of DQfD and DDPGfD in the related work is not accurate. They're described as 'treating demonstration data as self-generated data,' but in fact they both add supervised losses to more closely match the demonstrated data. https://ieeexplore.ieee.org/document/8794074 is another method built on DDPG that has both a critic and actor loss like yours and would make a useful comparison.\n\nThe related work section should also discuss and compare/contrast to GAIL, I was surprised that wasn't in there, especially since you also use a discriminator to differentiate expert and agent actions. \n\nThe end of the related work section is not very clear, you say these methods are problematic because 'the adopted shaping reward yields no direct dependence on the current policy' but there's no explanation or motivation for why that would be a problem. \n\nAssumption 1 seems like a very strong assumption that would not be true for many human experts. \n\nFor the experiments, I wonder about the impact of only using sparse reward tasks. Converting the tasks to sparse reward in this way makes them partially observable, and then potentially the expert demonstrations are required to overcome that partial observability. How do the methods compare on the unmodified tasks? There was nothing specific in your algorithm that meant it should specifically address sparse reward tasks. What about tasks that are naturally sparse reward? \n\nOverall, the algorithm is interesting and the results are nice. The motivation and related work need to be made clearer to situate this work with the other related works. And the experiments should go beyond these tasks that have been modified to have sparse rewards. \n\n\nThe revised version of the paper addresses many of my concerns about the motivation, related works, and comparisons with GAIL, so I'm updating my score to Weak Accept. ", "belong_id": "BklRFpVKPH"}, {"uid": "rJgI4xBycr", "paper_title": "Demonstration Actor Critic", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to mix reinforcement learning and imitation learning to boost the learning of an actor critic architecture. The authors use a regularized reward function that minimizes the divergence between the policy of the expert and the one followed by the agent. They use demonstrations obtained from a trained agent and experiment their method on several mujoco tasks. \n\nI have many concerns about this paper. First, The state of the art is missing important pre-deep-learning references such as: \n\n1. Direct Policy Iteration with Demonstrations: Chemali and Lazaric\n2. Learning from limited demonstrations, Beomjoon et al.\n3. Residual Minimization Handling Expert Demonstrations, Piot et al.\n\nThen, they make a mistake by saying that DQfD only considers transitions from the expert as self-generated and placed in the replay buffer. DQfD actually uses the same additional structured classification loss than Piot et al. [3] (except that they use boosted trees instead of deep networks, DQfD and Piot et al. are the same algorithm).\n\nAlso, the proposed solution here is equivalent to regularizing the MDP with a KL divergence  w.r.t. to an initial policy that would be the one of the expert. It is already studied in several works and more generally it comes with some assumptions on the policy update. It is generally studied in \n\n4. A theory of regularized MDP, Geist et al\n\nThey actually propose exactly the same framework as a special case in the appendix of that paper. \n\nIn addition to not be very novel, I think the method has some flaws. The authors use demonstrations coming from a pre-trained network which is known to make the imitation learning part much easier. Especially if it comes from an RL agent using similar deep RL algorithms (which is the case here). Finally, they only test on mujoco tasks which are very specific tasks with deterministic dynamics and very dense rewards  around states visited by the optimal strategy so initializing with an expert policy that is learned from demonstrations of a similar network of course helps. I would be more impressed by experiments on stochastic environments and sparse rewards. \n\nFinally, there is a concurrent work submitted to the same conference. Of course the authors could not know but Id like to have their impression about how their work is different.\n\nhttps://openreview.net/forum?id=BJg9hTNKPH&noteId=BJg9hTNKPH", "belong_id": "BklRFpVKPH"}, {"uid": "ByeFqVKpdr", "paper_title": "BRIDGING ADVERSARIAL SAMPLES AND ADVERSARIAL NETWORKS", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a new GAN training that additionally feeds adversarial examples as the real samples to the discriminator D. A key motivation here is to regularize the target real distribution simulated by D to be robust to adversarial perturbations. Experimental results show that the proposed GAN training generally improves the generation performance from the vanilla GAN training in CIFAR-10, CelebA and LSUN datasets. \n\nIn overall, I liked its clear motivation and the simplicity of the method. Experimental results are also presented clearly, and shows a significant improvement. One of my main concerns, however, is that robustifying D in GAN training is not a new idea for some readers [1], so they need more clarification on the novelty of the proposed method, e.g. by discussing about it in related work or by comparing the performance. \n\n- In regarding robust optimization, I think [2] could cover a lot of practices considered in this paper. Some questions listed here are relevant to this point:\n    (a) The paper only considers to use FGSM in adversarial training part, but FGSM training on large epsilon usually leads to overfitting [2] on CIFAR-10. I wonder if the authors have tried PGD counterpart in their method.\n    (b) It seems that the method uses both natural and adversarial examples in adversarial training, as in [3]. Instead, there is another (and perhaps more common) type of adversarial training [2] that uses only adversarial examples for the training. What happens if this training is applied to the method?\n\n- The method makes an additional parameter updates (Ep. 3 and 8) for adversarial training. Could this step make additional gain to the method by training more, i.e. perhaps it is a bit unfair to the vanilla training?\n\n- It would strengthen the claim if the paper could present whether the robustness of D is indeed increased, e.g. by comparing adversarial accuracies?\n\n- Table 1: I feel there should be more discussion about why the reproduced values are so different compared to that of previously reported values, as they might confuse the readers to convince the claimed results.\n\n- Eq. 7: The first term in the right hand side have to be E[max (log D(x - d))] instead of E[(log D(x - d))]?\n\n[1] Liu, X., & Hsieh, C. J. (2019). Rob-gan: Generator, discriminator, and adversarial attacker. CVPR 2019.\n[2] Madry, A. et al. (2017). Towards deep learning models resistant to adversarial attacks. ICLR 2018.\n[3] Goodfellow, I. et al. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.", "belong_id": "rklPITVKvS"}, {"uid": "rJx0AWlwKB", "paper_title": "BRIDGING ADVERSARIAL SAMPLES AND ADVERSARIAL NETWORKS", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents an interesting idea based on introducing adversarial noise on real samples during GAN training. This novel approach may improve GAN training and have potentially large impact, but the paper in its current form is slightly below the standard of ICLR due to its lack of clarity.\n\nWhile it is very interesting to apply adversarial noise in real data, this approach is not clearly motivated or explained. For example, at the beginning of page 2, why As a consequence, training will become unstable when generated distribution approximates target distribution because the gradient given by non-robust discriminator around real samples contains more adversarial noise? One may think that, on the contrary, the adversarial noise in generated samples would approximate that in real data when the generator distribution approximate the true data distribution. Similarly, after eq. 6, why Consequently, gradient given by discriminator may vanish when discriminator becomes stronger than generator without capacity constrained.? I would think the gradient would explode in this case given all the regularisation on gradients.\n\nIn addition, the term non-robust discriminator has been used several times in important places, but is not clearly defined. Properties used to justify the approach, such as symmetric and balanced need to be explained. For example, would it be possible to illustrate or measure the imbalance of discriminator?\n\nThe overhead of the algorithm needs to be stated more precisely. (after eq. 8) It is unclear to me that backward propagation of Equation 5 with respect to x with negligible computation overhead. Would this require back-prop through the entire discriminator, which can be very deep thus costly? It would be helpful to provide an estimation of runtime overhead supported by experiments.\n\nIn algorithm 1, why is it necessary to perform discriminator update twice? How about skipping step 4 (eq. 12)? I think this may better mirror the adversarial update for generated samples, which only involves updating generator parameters once.\n\nIn section 3.4, I am not sure it is similar to unrolled GAN when the perturbation is zero. Unrolled GAN required backprop through discriminator update, which I dont think is the case here\n\nFinally, the experimental results are confusing. In Table 1, why the reproduced results are already much better than those in the original papers? A solid baseline is necessary for any further assessment. Further more, since the DCGAN and ResNet baseline models did not use recent regularisation approaches such as spectral-norm, it is hard to assess whether the proposed method can work without those existing techniques. It would be helpful to use at least a SN-GAN baseline.\n\nThe analysis also need to be improved. It is hard to interpret the histograms and L1 norms in Figure 5 as related to informative gradient or semantic information as claimed in sectiion 4.4. Quantitative measurements such as correlation or mutual information may justify these claims.\n\nOverall, I think the idea presented is certainly promising, but needs further development to be qualified for acceptance.\n\nNit:\n\nThe 1-line derivation of eq. 6 can be incorporated into the main text.\n\nA transpose in eq.10 is missing.\n\nUpdate:\n\nI read the authors' rebuttal, which addressed many of my concerns and presented additional empirical results. The improvement over SN-GAN baseline is particularly convincing. I therefore believe the final camera-ready version can be a valuable contribution to the field, and would like to recommend accepting this paper.", "belong_id": "rklPITVKvS"}, {"uid": "HylxPVL3Yr", "paper_title": "BRIDGING ADVERSARIAL SAMPLES AND ADVERSARIAL NETWORKS", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper tries to propose a new method to stabilize the training procedure of GAN. To this end, they use adversarial samples of real data to train the discriminator, and claim that it is helpful to reduce the adversarial noise contained in the gradient. Although training GAN with adversarial samples of discriminator is somewhat novel, the method and experiments are not convincing. \nI do not recommend the acceptance based on the limited contribution of this paper. The following is a detailed evaluation. \n\n1. The paper uses vague description such as This approach can improve the robustness of discriminator and reduce adversarial noise contained in gradient without convincing justification. Please give a formal description or notation of adversarial noise contained in gradient, and discuss how to remove the effect of adversarial noise in principle instead of extensively testing adversarial training of discriminator. \n\n2. The experiment is not convincing and the improvement is not significant. The author running adversarial training on CIFAR10 dataset with FGSM and the perturbation is tested from 0.2/255 ~ 4/255. The performance (FID score) is a bit sensitive to the amount of perturbation level. Moreover, this The improvement over DCGAN is quite limited given previous works such as WGAN-GP. Combined together, the result is not convincing (it seems to be a heavy tuning result rather than a principled solution).", "belong_id": "rklPITVKvS"}, {"uid": "BJgUq9uoKH", "paper_title": "Improving Model Compatibility of Generative Adversarial Networks by Boundary Calibration", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this work authors consider a problem of 'model compatibility' of GANs, i.e. usefullness of the generated samples for classification tasks. Proposed 'Boundary Calibration' GAN attempts to tackle this issue by adding non-adversarial terms to discriminator, obtained as outputs of the classifiers trained on the original data. For evaluation, it is proposed to compare accuracies obtained by classifiers trained on generated and on real data (termed 'relative acurracy'). Experiments show that the proposed methods improve such scores.\n\nPros:\n- considered problem seems to be important GAN application which has not yet received too much attention.\n- proposed method seems to improve the accuracy of classifiers trained on generated data.\n\nCons:\n- the paper is poorly written, has multiple typos and often it is unclear what authors mean.\n- the proposed evaluation does not exactly measure the potential improvements from training classifiers with generated data. It would make sense to provide information if generated data can improve classifier's scores, if added to the training data (or small part of it).\n- it is unclear whether or not the proposed technique does not affect the sample quality, as no quantitative metrics are provided.\n\nDetails:\n1. Quantitative scores for quality of generated samples are not provided. Authors instead provide few samples and state that it is difficult to detect the difference. Although sample quality is not the main task here, it certainly is important - otherwise we could train generators solely against the classifier 'boundary calibration' loss terms - this, however would likely lead to adversarial examples. Combining two losses often leads to trade-offs, hence showing that we can improve 'model compatibility' without the loss of sample quality is actually crucial. Metrics such as Inception score [1], FID [2] or KID [3] are desired, especially given relatively poor quality of cifar and mnist samples from all models.\n2.The main metric used is the ratio between accurracies of classifiers (trained on real and generated data). It is hence difficult to tell if the classifiers that were used were trained reasonably and achieved reasonable scores.\n3. In the abstract, authors claim that 'GANs often prefer generating easier synthetic data that are far from boundaries of the classifiers'. Although for some GAN settings the generators might be biased to do so, in general this claim is unfounded, as GANs optimize divergences that are agnostic to classifier boundaries.\n4. It is unclear what kind of classifier-output is used as an input to MMD. Are these continuous logits, discrete class numbers, or one-hot-encoded class identities?\n5. Authors use WGAN and MMDGAN with gradient penalty. It is unclear how gradient penalty is applied to MMDGAN as what should be penalized is the witness function, which is different than in WGAN-GP [4], see e.g. [3].\n6. It is unclear how embeddings of class information are concatenated to discriminator inputs (p.5).\n7. It is unclear to what extent feature selection is deterministic. Authors argue in Section 5.4 that the intersection of top-k features selected from two models should be large. It would be good to provide the same statistics for features selected twice on the same sample.\n\nOverall, the paper currently does not match the quality requirements of ICLR, however it has potential for improvement if the mentioned issues are addressed.\n\nTypos/unclear expressions:\n[p1] 'may not willing' >> 'may not be willing'\n[p1]'with the property similar to the original data is demanding'  >> properties, demanded/in demand\n[p2] 'Although GANs are versatile as aforementioned' - strange wording\n[p2] 'The pioneered work' >> 'pioneering work'\n[p2] 'information of models' >> 'information from the models'\n[p2] effects >> affects\n[p3] related works >> related work\n[p3] distribution of label >> distribution of labels\n[p3] 'generated dataset adopt ' >> 'generated dataset will adopt '\n[p3] 'To known about the boundary' >> ' To know the boundary'/'To include the information about the boundary'\n[p3] 'the a distance'\n[p3] 'the problem to distinguish whether two sets of samples' \n[p4] 'If they are close the sets might be sampled from the same distribution' (?)\n[p4] tries to minimized the MMD\n[p4] would not leads to\n[p6, Table 2 caption] number of estimator used\n[p6] at Appendix\n[p6] depresses the model compatibility (?)\n[p6] can providing\n[p7] to known how\n[p8] our work open >> our work will open\n\n\n------------------------------------- Revision -------------------------------------\n\nAlthough some issue seem to have been clarified, two of my main concerns, i.e. the proposed evaluation and the text quality, have not been resolved. Additionally, as pointed out by reviewer #4, the results seem somewhat incremental. For these reasons, I decided to keep the rating unchanged.", "belong_id": "S1xJikHtDH"}, {"uid": "HJlLQ8ERYH", "paper_title": "Improving Model Compatibility of Generative Adversarial Networks by Boundary Calibration", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper aims at training a GAN that can generate data matches the real data distribution well especially at the boundaries of the classifiers. A Boundary-Calibration loss (BC-loss) base on multi pretrained classifiers is introduced to match the statistics between the distributions of original data and generated data. The motivation is interesting. The story is clearly explained. However, the experiments part is weak.  \n\nThere are several typo and mistakes. The experiments only show that the proposed method got a good performance, but the analysis of the reason is not shown. The reason to name the loss as Boundary-Calibration loss (BC-loss) should be explained and the experiments should show some effect on the boundary areas. Some concerns are listed below,\n\n1.\tIf the generated dataset exhibits very good property as the real dataset, it means the data is to some extent perfectly foreseen, and there is little to no privacy, is it contrary to the aim of not leaking the real dataset?\n2.\tIt is more interesting to see the difference between the distribution of real data and the generated data, However the author only show a simple toy data distribution comparison, I would like to see more comprehensive results about the distribution differences on real dataset , e.g. the TSNE embedding? \n3.\tEquation (3) is not correct.\n4.\tThe author said that image quality of MNIST and CIFAR10 are not improved, then why the classification results are improved?  there should have some differences existed among different compared methods, it would be more convincing if you can show it out.\n5.\tWhat kind of  generator do you use for the UCI data?  How do you settle the output problem?  Since some of the data are continuous and some are discrete.\n", "belong_id": "S1xJikHtDH"}, {"uid": "HkgBnP1b2r", "paper_title": "Improving Model Compatibility of Generative Adversarial Networks by Boundary Calibration", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper the authors propose a method for improving 'model compatibility' in GANs. For this reason they add to the loss of the generation procedure a term that depends on the maximum mean discrepancy between the following datasets: (1) the output of a classifier with input the real dataset, (2) the output of the same classifier with input GAN-generated samples. They authors show that in essentially all the datasets they tried, the model compatibility of the produces generator is increased after adding the aforementioned cost, while the visual quality of the data is not decreased.\n\nStrengths:\n\n- The low model compatibility of GANs is a very important disadvantage and hence improving this aspect of GANs is a relevant problem.\n\nWeaknesses - Comments:\nA. The increase in the model compatibility is very mild. Especially in CIFAR-10, the increase in very small.\n\nB. In MNIST the increase is larger than CIFAR-10 but the initial model compatibility using vanilla GANs is smaller. The reason might be that for MNIST much simpler classification algorithms have been used. This maybe suggests that the proposed method affects more the model compatibility of methods that achieve lower model compatibility in before the addition of the extra cost term.\n\nMinor Comments:\n1. In equation (1) it looks strange that the summation is over A but A does not appear at all in the summand. I suggest you replace h and h' with A(D) and A(D') so that this is clear.\n2. In Theorem 2, \\hat{L}_G is used but for the proof the authors have replaces \\hat{M} with M. There should be a comment for that. In general I believe that Theorem 2 is almost trivial and does not add value to this clearly experimental paper.", "belong_id": "S1xJikHtDH"}, {"uid": "rJlW3Szi_r", "paper_title": "Unsupervised Learning of Efficient and Robust Speech Representations", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nOverview:\n\nThis work uses contrastive predictive coding (CPC) to learn unsupervised speech representations on large amounts of unlabelled speech data and then uses the resulting features in downstream speech recognition systems. Unlabelled data is obtained from several sources (spanning different languages). Supervised systems are then built on top of these features and sample-efficiency and cross-domain robustness is investigated using English data sets. Finally, the approach is applied to four African languages.\n\nStrengths:\n\nFirstly, the paper is very clearly written and motivated. Secondly, a very relevant problem is tackled in a systematic way; compared to transcribed resources, unlabelled resources are much easier to collect and more widely available. This paper shows that these unlabelled resources can be of great benefit in downstream tasks and on languages where few resources are available. Thirdly, the experiments are carried out very systematically to support the claims of the paper: that bidirectional CPC-based feature learning improves same efficiency (they show that much less labelled data is required to achieve the same performance as when using more substantial labelled data with conventional features), and that it improves robustness to out-of-domain data. They perform these experiments on both English and truly low-resource languages.\n\nWeaknesses:\n\nThere are two main weaknesses to the paper. Firstly, as the authors note themselves, unsupervised CPC-based speech feature learning was developed and considered in previous work, and has also been subsequently investigated by others. The main technical contribution is therefore only in changing the unidirectional architecture to bidirectional. Secondly, the paper does a very poor job of linking this work with previous work. The work in [1] is very related. In Section 5, the ZeroSpeech challenges are mentioned briefly (with a single citation), but over the last decade there has been substantial work in this community specifically looking at exactly the main problem addressed in this paper (unsupervised speech representation learning). It would be of great benefit to situate this work within that context, and I would recommend that the paper at least mention [2] to [9].\n\nOverall assessment:\n\nAlthough technical novelty is limited (first weakness), I think there is novelty in the paper's systematic experimental investigation, including ASR experiments on truly low-resource languages. The conclusions of this work also has practical implications for the ASR community. The second weakness can be addressed by amending Section 5. I therefore assign a 'Weak Accept' to the paper.\n\nQuestions, suggestions, typos, grammar and style:\n\n- In Figure 1, it might be useful to indicate the autoregressive nature of the context vectors by adding arrows in-between the $c$ blocks on the top left. (In the text it says an RNN is used.)\n- p.7: '... are suitable for driving recognition different languages ...'. A typo or grammatically incorrect sentence.\n- p. 9: 'Tts without t' -> 'TTS without T'\n- p. 9: 'african' -> 'African' (check all citations for capitalization)\n\nMissing references:\n\n1. https://arxiv.org/abs/1904.03240\n2. A. Jansen et al. A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition. ICASSP, 2013.\n3. Badino, L., Canevari, C., Fadiga, L., & Metta, G. (2014). An auto-encoder based approach to unsupervised learning of subword units. in ICASSP.\n4. Versteegh, M., Anguera, X., Jansen, A. & Dupoux, E. (2016). The Zero Resource Speech Challenge 2015: Proposed Approaches and Results. In SLTU-2016 Procedia Computer Science, 81, (pp 67-72).\n5. Renshaw, D et al. (2015). A Comparison of Neural Network Methods for Unsupervised Representation Learning on the Zero Resource Speech Challenge. Interspeech.\n6. R. Thiolliere et al. A  hybrid  dynamic  time  warping-deep  neural  network  architecture  forunsupervised acoustic modeling. Interspeech. 2015\n7. https://arxiv.org/abs/1811.08284\n8. https://arxiv.org/abs/1702.01360\n9. https://arxiv.org/abs/1709.07902\n", "belong_id": "HJe-blSYvH"}, {"uid": "HJlqbVhpYH", "paper_title": "Unsupervised Learning of Efficient and Robust Speech Representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes an unsupervised method for learning representations of speech signals using contrastive predictive coding. \nThe authors provide results for the speech recognition task, in which they trained their model on up to 8000 hours of speech. The authors provide results on several English benchmark datasets in addition to four low-resource African language datasets. \nThe authors compared their method to the traditional signal processing representations and show that the proposed method is superior. \n\nMy main concern with this submission is its novelty.\nThe proposed method was previously explored in [1] and presented similar results. If I understand it correctly, the main novelty in this work is the usage of bi-directional models together with more data. However, it is not clear what made the improvements. Considering the fact that such an approach was suggested recently by [1], a detailed comparison with uni-directional models is needed.\nFor example, in Table 2, the authors provide results for WSJ dataset, however, with no LM decoding. Can the authors provide experiments of WSJ while using LM similarly to [1]?  Moreover, if the authors wanted to eliminate the effect of LM as they stated in the paper, why not calculating Character Error Rates instead or in addition to Word Error Rates? Again, as done in [1], and in many other papers in the field [2]. \n\nAdditionally, in Table 1 and Table 5, the error rates seem pretty high, especially for the baseline model, did the authors investigated different architectures/stronger ones for these tasks? Different representations such as LogFilterBanks / MFCCs?\n\nI'm willing to increase my score, in case the authors will address my concerns. However, at the moment, I do not see much novelty in this paper comparing to previous work. Additionally, the authors are missing an essential comparison to previous work so we could better understand the contribution of this paper. \n\nMinor comments: 'using a simpler convolutional architecture than is common' -> should be rephrased.\n\n\n[1] Schneider, Steffen, et al. 'wav2vec: Unsupervised Pre-training for Speech Recognition.' arXiv preprint arXiv:1904.05862 (2019).\n\n[2] Adi, Yossi, et al. 'To Reverse the Gradient or Not: an Empirical Comparison of Adversarial and Multi-task Learning in Speech Recognition.' ICASSP, 2019.\n", "belong_id": "HJe-blSYvH"}, {"uid": "rJezAFHRFS", "paper_title": "Unsupervised Learning of Efficient and Robust Speech Representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper investigates an unsupervised learning approach based on bi-directional contrasive predictive coding (CPC) to learning speech representations.  The speech representations learned using 1k and 8k hours unlabeled data based on CPC are shown to be helpful in semi-supervised learning ASR tasks in terms of sample efficiency, WER and cross-domain robustness. The reported work is interesting and may have value to the speech community.  Regarding the paper, I have the following concerns. \n\n1.  In terms of semi-supervised learning ASR, I think any proposed approach should compare with the 'naive' way of doing it. That is, use a high-performance ASR model to decode the unlabeled data and use the decoded pseudo-truth as the ground truth to train an acoustic model with an appropriate capacity.  In my experience,  many of the 'novel' approaches can not outperform this 'naive' method.  I would like to see this as a baseline for the semi-supervised learning experiments. \n\n2. In sec. 3.1 on the setting of unsupervised learning, the authors state that 'all audio signals have a sampling rate of 16KHz'. This is obviously not true for the Switchboard data in Table 6 in Appendix A, which has a sampling rate of 8KHz as they are telephony signals.   The authors should clarify. \n\n3.  It is not clear to me why the authors use two different ASR models (DeepSpeech2 small and TDNN). Why not stick to one architecture but adjust the model capacity?  \n\n4.   I wonder if the latent features learned by CPC can be complementary to the conventional features such as logmel ? How does it perform if  the two are simply concatenated as the input to the acoustic model? \n\nP.S.  rebuttal read. I will stay with my score.", "belong_id": "HJe-blSYvH"}, {"uid": "BkxVHrpKYS", "paper_title": "Influence-Based Multi-Agent Exploration", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the problem of designing effective exploration strategies in multi-agent domains. The key idea is to define one agent's exploration in terms of its interactions with other agents. This leads to two auxiliary exploration objectives, which measure how one agent's actions affect the dynamics and value of another agent's actions. The paper does an admirable job comparing the proposed method against a number of baselines, where the proposed method performs significantly better. Visualizations and ablation experiments nicely illustrate the contributions of various components of the method.\n\nI am leaning towards accepting the paper. To the best of my knowledge, the broad idea of applying information theory to multi-agent exploration, in addition to the specific instantiation described in the paper, is novel. I expect that this paper will encourage future work to explore more problems in this area. The experiments are quite thorough. My main reservation is a lack of comparisons to single agent exploration methods. As noted in Section 3, we can view multi-agent domains as just a special type of single agent domain. How would curiosity-based exploration, such as [Burda 2018, Pathak 17], or mutual information-based exploration, such as [Gregor 16, Eysenbach 18, Achaim 18], compare to the proposed method?\n\nI have a few reservations about the clarity of presentation, but I think those are easily addressed. My remaining concern is that the results are on somewhat toy tasks, but I think that is par for this area of research.\n\nOverall, I would strongly argue for accepting this paper if comparisons to single-agent exploration methods were added. I would consider decreasing my review if another reviewer found quite similar prior work, or if significant bugs were found in the mathematical derivation (I have not carefully checked all the proofs in the appendix.).\n\nMinor comments\n* 'transition-dependent' -- what does this mean?\n* 'while tend' -- missing a subject\n* 'struggle in many real-world scenarios with sparse rewards' -- please add a citation\n* 'intrinsic value function of agent i, I_{-i|i}^\\pi is \\beta > 0 is a weighting' -- I think part of this sentence was accidentally deleted.\n* Eq 5: What is the difference between I and MI?\n* 'We call ...' -- What is the a_2^V0I^\\pi_{-i|i} term?\n* Nitpick: Use `` for the start of quotes\n* Appendix B1: How is Eq 22 obtained from Eq 21?\n\n-------------------UPDATE AFTER AUTHOR RESPONSE---------------------\nThe authors have done a great job address my two concerns (similarity to prior work and empirical comparisons with single-agent exploration). I therefore increase my vote to 'accept.'", "belong_id": "BJgy96EYvr"}, {"uid": "rJg6aatjYH", "paper_title": "Influence-Based Multi-Agent Exploration", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Update: I thank the authors for their response and I will maintain my score, my main hesitation being the overall clarity and readability of the paper. \n\nSummary:  \nThis paper proposes the use of two intrinsic rewards for exploration in MARL settings. The first one is an information-theoretic influence (EITI) bonus and a decision-theoretic influence (EDTI)  bonus. EITI uses mutual information to capture the influence of one agent on the transition dynamics of others,  while EDTI uses an intrinsic reward called Value of Interaction (VoI) to quantify the influence of one agents behavior on expected returns of other agents.\n\nMain Comments:\nOverall, I think this paper would be a good contribution for ICLR 2020 and I lean towards accepting it. The experimental section is thorough, the authors include relevant ablations, baselines and popular algorithms used in MARL settings. The use of the decision-theoretic influence is novel as far as I can tell and it also seems to be quite effective on the tasks used for evaluation. Although the method uses a series of approximations and assumptions, I believe most of them are clearly stated and fairly well-motivated (plus they are not very far from those of other recent work in the deep MARL literature). I also appreciated the fact that the authors explicitly derived the main mathematical results used in the paper.\n\nI only have some minor comments and questions regarding some assumptions and notation.\n\nI  also encourage the authors to proof-read the paper as some parts of it are a bit hard to follow. I would very much like to see a more an edited version of this paper with more precise language.\n\nCan you discuss in more detail the difference between EITI and the intrinsic reward based on social influence used in Jacques et al. (2018)? They seem to be quite similar conceptually and the related work part related to this is rather vague. Please clarify the distinction.  \n\nMinor Questions / Comments:\nThere are a few typos throughout the paper:\n\n1. On page 3 after equation (3), I believe part of the sentence that should describe the I term in the equation is missing. \n\n2. The phrase right after equation (16) which defines the EDTI reward does not seems to not match the  above expression. Can you please explain why the transition would be conditioned on the influence term? While reviewing, Ive been assuming this was just a mistake in writing but please double check and clarify. \n\n3. On page 4, after equation (8), you refer to  a_1, a_1 and s_2 which do not appear in the above equation. Can you please use the same notation or motivate your choice for referring to those variables instead?\n\n \n", "belong_id": "BJgy96EYvr"}, {"uid": "HJx2JQyg9S", "paper_title": "Influence-Based Multi-Agent Exploration", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes methods for incentivizing exploration in multi-agent RL.  There are two approaches that are proposed, both framed as influence maximization (of either the state transitions or the decisions of the other agents).  The scaling to multiple agents is done via decomposing to pairwise interactions. This influence objective is the appended to the standard intrinsic motivation objective for single agent RL.\n\nThe proposed approaches are pretty elegant, and in a sense seem fundamental.  I'm not an expert in this particular area, so I don't know how novel these ideas are.  (See related work comments below.)\n\nThe empirical results seem quite strong, although (being a a non-expert), I can't tell if they're constructed to be good for the proposed approaches.  There isn't much discussion of limitations and/or experiments breaking the proposed approach.\n\nI found the related work discussion a bit incomplete.  Can the authors comment directly on related MARL work, such as Foerster et al., AAAI 2018?  What are the specific points of contrast?", "belong_id": "BJgy96EYvr"}, {"uid": "HJgnXst2tr", "paper_title": "Conservative Uncertainty Estimation By Fitting  Prior Networks", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work introduces a simple technique to obtain uncertainty estimates for deep neural networks. This is achieved by having a set of random networks (i.e. neural networks where their parameters are randomly initialized) and then computing an uncertainty value based on the difference in the predictions between those random networks and networks that are trained to mimic them on a finite collection of points. The authors further show that this method results into uncertainties that are conservative, meaning that they are higher than the uncertainty of a hypothetical posterior, and concentrate, i.e. they converge towards zero when we get more and more data. The authors further draw connections to ensemble methods and discuss how such a method can be effectively realized in practice. They then evaluate their approach on an out-of-distribution detection task, they measure the calibration of their uncertainty estimates and finally perform a small ablation study for their concentration result. \n\nThis work is interesting as it seems to provide a simple way to obtain reasonable uncertainty estimates. For this reason it can potentially serve as a strong baseline for this field. The theoretical considerations also help in providing some guarantees about such an approach. Having said that, in my opinion the writing could use some more work in order to make things more clear as some critical experimental details and baselines are missing and thus do not make the method as convincing. Furthermore, I also believe that some clarifications on the theoretical aspects of this work, will help in boosting its quality. More specifically:\n\n- How exactly do you apply your method on the classification scenario? Do you select an arbitrary hidden layer of the classification model for the prior and predictor network architectures or the output logits / softmax probabilities?  In appendix A you mention the architecture but not precisely how it is employed. I believe this can be an important piece of information in order to decipher the importance of e.g. the output dimensionality on the uncertainty quality, as higher dimensional outputs might be harder to approximate thus could induce a larger squared error and hence uncertainty.\n- What is the average training error of the predictor networks for the out-of-distribution task and subsampling ablation task, i.e. how far away from concentration were the priors? \n- An effect that I found weird is the following: what happens for the out-of-distribution examples when the predictor networks can perfectly predict the prior network outputs? Wouldnt that then imply that the uncertainty would be zero for any input (even an out-of-distribution one), as the prior network and predictor network always agree? One could imagine that for e.g. simple priors and with sufficiently dense sampling of the domain of the function this can happen in practice.\n- For the conservatism you show that your uncertainty estimate is higher, on average, than the posterior variance when you sample points from the model itself. In a sense this guarantee translates to the actual data when the prior is correct. How do those conservatism guarantees translate to the case when there is model misspecification, i.e. when the prior is not correct? Perhaps a small toy example would be informative.\n- For the predictor networks as described in figure 2; do you train both the green and red parts of the network or only the red parts and keep the green part fixed to the values you used for the prior f? (This helps in understanding how easy / difficult is the task of the predictor network).\n- What is the accuracy on the actual in-distribution prediction task for the RP and baselines? What did B correspond to for the dropout networks? Was it the number of dropout samples you averaged over to get the final predictive?\n- How sensitive are the results on the actual initialization strategy of the prior network? It would be good to see e.g. some form of performance / init variance curve in order to decipher the sensitivity. \n\nOther comments\n- It is worth pointing out that [1] showed that Monte-Carlo dropout performs approximate MAP inference, which seems more plausible than the approximate Bayesian inference perspective of [2].\n- In the introduction you argue that Bayesian neural networks rely on procedures different from standard supervised learning and thus most ML pipelines are not optimized for them in practice. Could you elaborate a bit about this statement? Variationally trained BNNs with e.g. the reparametrization trick [3, 4] are straightforward since you can just use backpropagation to update their (variational) parameters.\n- What is the x-axis for Figure 3 for the baselines? (I take it that for RP it is the \\hat{sigma}^2(x)).\n- I believe that a comparison against a simple variationally trained BNN would make the results more convincing.\n\nMisc\n- Second page, Figure 1, top two plota -> Figure 1, top two plots\n- Third page, [...] introduced in equation 2 denotes the posterior covariance [....] -> [...] introduced in equation 2 denotes the posterior variance [...]\n- Fifth page, [...] this makes it is reasonable for W large enough [...] -> [...] this makes it reasonable for W large enough [...]\n- Sixth page, Corollary 1 and proposition 2; where is corollary 1? Do you mean Proposition 1?\n- Seventh page, [...] inspired by, an builds on, [...] -> inspired by, and builds on, [...]\n- Ninth page montonicity -> monotonicity\n\nOverall, I tend to accept this work, although, depending on the author rebuttal and other discussions, I am willing to change my rating accordingly.\n\n[1] Eric Nalisnick, Jose Miguel Hernandez-Lobato, Padhraic Smyth, Dropout as a Structured Shrinkage Prior, 2019\n[2] Yarin Gal, Zoubin Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, 2016\n[3] Diederik P. Kingma, Max Welling, Auto-Encoding Variational Bayes, 2014\n[4] Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, Stochastic Backpropagation and Approximate Inference in Deep Generative Models, 2014", "belong_id": "BJlahxHYDS"}, {"uid": "B1xA7Po2tS", "paper_title": "Conservative Uncertainty Estimation By Fitting  Prior Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overview:\nThis paper introduces a new method for uncertainty estimation which utilizes randomly initialized networks. Essentially, instead of training a single predictor that outputs means and uncertainty estimates together, authors propose to have two separate models: one that outputs means, and one that outputs uncertainties. The later one consists of two networks: a randomly initialized prior which is fixed and is not trained, and a predictor, which is then trained to predict the output of the randomly initialized prior applied to the training samples.  \nAuthors show that under some reasonable assumptions the resulting estimates are conservative and concentrated (i.e. bounded and converge to zero with more data).\n\nWriting quality: \nOverall, the paper is relatively well-written, although it might be at times hard to follow, especially for someone who is not familiar with the original work that used randomized prior functions (Burda18, Osband 18, 19). \n\nEvaluation:\nThe method is experimentally evaluated on a task of out-of-distribution detection on CIFAR+SVHN, and seems to perform on-par or better than the baselines (including standard deep ensembles and dropout networks). In addition, there are experiments that demonstrate that the model is performing relatively well in terms of calibration (whether the model predictive behaviour makes sense as the model confidence changes).\n\nDecision:\nI find the core idea behind the paper quite interesting, however, as indicated by authors themselves, it has already been studied in a slightly different context (RL, works by Burda et. al, Osband et. al). That said, authors do provide additional insides for the supervised settings, and also analyse theoretically the behaviour of uncertainty estimates. \nOverall, I cannot say I am fully convinced that the paper should be accepted as is (also see questions below), but generally I am positive about this work, and hence the final score: weak accept.\n\nAdditional comments / questions:\n(somewhat minor) p1: While deep ensembles ..., where the individual ensembles are trained on different data - here and related text, it should probably be individual models / individual networks. Generally, I am not convinced that these are strong arguments against deep ensembles.\n\n(minor) p2-p3: 2. Preliminaries - I am not sure if this section adds much to the understanding, it would seem more natural to spend more time explaining the intuitions behind the net\n\n(kind of major) p3. prior - The explanation of why using a randomly initialized network makes sense is not very strict. I kind of get the general idea, but it is not clear to me why not use something less expensive, e.g. just random projections, and why do we actually need a full network. Intuitively it seems quite strange to waste a lot of capacity to fit to essentially fit a set of random weights: is it something that allows the network to avoid easily learning the random prior? And, more generally, can this also be considered as a trick to de-correlate individual predictors? I believe these points should be discussed in more detail.\n\n<update>\nI would like to thank authors for verbose response and the revised version: it is a bit more clear. \nI stand by my original rating.\n</update>\n\n\n\n\n\n\n", "belong_id": "BJlahxHYDS"}, {"uid": "HkgK3JURYB", "paper_title": "Conservative Uncertainty Estimation By Fitting  Prior Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper shows that the MSE of a deep network trained to match fixed random network is a conservative estimate of uncertainty in expectation over many such network pairs. An experiment compares this previously proposed method to other approaches for uncertainty estimation on CIFAR 10. \n\nStrengths:\n- Obtaining uncertainty estimates for predictions of deep neural networks is an important and open research question.\n- Proposition 1 is an interesting result, although the paper does not seem to discuss its significance and implications enough.\n\nWeaknesses:\n- Proposition 1 is described as 'our uncertainty estimates are never too small'. However, as the name of the proposition suggests, it seems to only hold in expectation over models trained, which is a quite different statement.\n- Proposition 2 seems to simply state that a small network can be distilled into a large network. Maybe I missed part of the reasoning here? Otherwise, it should probably not be highlighted as a major contribution.\n- The experimental evaluation is very limited, training only on CIFAR 10. While the experiments add little value to the community, this may be acceptable for a mostly theoretical paper.\n\nClarity:\n- The paper was difficult to read and unclear in explanations. It could help to define notation upfront instead of introducing shorthands on the way.\n- In Figure 3, the Y axis limits should be fixed across seen and unseen histograms for the same method. The current presentation is a bit misleading here, as the presented method seems to have moved the most mass under this chart scaling.\n\nComments:\n- As found in prior work cited in the submission, the method tends to perform well with just one network pair. This raises the question whether the contribution of the paper that holds in expectation over many pairs and the empirical success of the approach are connected.\n- The marginal posterior variance \\sigma^2(x_\\star) appears in various forms with hat, tilde, and different subscripts. It maybe worth assigning different letters to these to avoid confusion.", "belong_id": "BJlahxHYDS"}, {"uid": "ryew8BLsFH", "paper_title": "Attention Privileged Reinforcement Learning for Domain Transfer", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The topic addressed by the paper is domain adaptation and transfer learning in the text context of deep reinforcement learning, in particular the sim2real problem, where a policy is learned in simulation and should be transferred to a physical agent in a real-world scenario. The work builds on the existing asymmetric DDPG formulation (Pinto et al., 2017), which exploits the fact that full states are sometimes available in simulated environments but not during deployment. In Pinto et al., this is addressed by learning an actor taking as input observations, and a critic which has access to the state.\n\nThe contribution of the paper is an extension of Pinto et al. by adding additional communication between the state head (which learns a critic) and the observation head (which learns the policy). This is done through an attention mechanism, now very classical in deep learning, which weights the different elements of the (state) input. The particularity here is that the attention mechanism is trained on the critic, which takes the full state as input, and then transferred to the observed input of the policy. The problem is that the state is very different from the observations, which are images. An alignment module expands the distribution over state variables to a distribution over observed objects.\n\nOne of my main concerns here is lack of justification and lack of clarity. While the abstract of the paper and introduction section are well written, and wants us to learn more about this interesting idea, the paper kind of falls apart in the subsequent chapters. The authors mention alignment of attention, but the exact motivation for the algorithm, i.e. the motivation for its formulation, are never provided. The description itself is also far from clear, as key design choices are not introduced and we discover them, or guess them, from equations. We dont know, for instance, what the observations are, and think that they are images. However, apparently an object mask detector is run over the input images, as segmentation maps are input to the state-object alignment module, which needs to distribute the attention over states to attention over objects.\n\nThis also means that the formulation is quite specific to the task at hand, since it seems to be object-centric.\n\nFigure 1 is another example of the same problem: There are many arrows, but their signification is unclear. Dashed lines are indicated to mean alignment, but alignment is not a standard term, for instance comparable to a computation connection, or a loss signal.\n\nA similar confusion is found in the equations themselves, as h(s) is mapped to a vector c, but this vector decomposes into different objects of the environment. What is T? It being upper case could mean a matrix or a scalar value, but we are not sure. From the loss function we see that this seems to be vector (a distribution) of the same size as the attention vector h_o ... which is over what?\n\nBasically, as I understand it, the alignment loss minimizes some structured mapping between some attention vector over objects on the observation side and some attention vector over states on the critics side. This seems to be a weak loss signal, as the attention transformation needs to be learned together with the attention mechanism itself (and of course the actor and the critic).\n\nAt the beginning of section 3, the method is introduced of having an asymmetric part and a symmetric part, but I dont see this, as this would require learning 4 predictors and not 2 (an actor and a critic). An asymmetric model, as given in Pinto et al., uses two predictors, an actor and a critic, with different loss functions and, more importantly, different inputs. Here, the authors claim that the observation module itself is asymmetric ... but how can something by asymmetric if it contains only an actor? Same, the critic is supposed to be symmetric ... but without an actor?\n\nAs I see it, w.r.t to the question whether the model is symmetric or asymmetric, the formulation is identical to Pinto et al., thus asymmetric. The difference lies in the attention mechanism and the alignment module.\n\nAt some point in the paper, this method is mentioned to be self-supervised ... I am not sure which aspect of this work could be called self-supervised, but I agree that the definition of this relatively new term is sometimes ambiguous.\n\nOn the other hand, I think the method should be compared to classical self-supervision in RL, which corresponds to predicting information which is available during training but not available as input (depth prediction etc.). Here, a natural baseline seems to be a symmetric model (same input for actor and critic: observations) and to predict the state of the model and self-supervise it during training but not deployment.\n\nThe evaluation is unfortunately not convincing.\n\nTwo important baselines are missing:\n-\tSelf-supervision, as mentioned above\n-\tPinto et al., on which this method is based.\n\nThere are some hics in the results ... for instance the curves in Figure 2 show learning in progress, they are not yet converged. Also, the standard deviations are quite big.\n\nAnother downside is that, although sim2real is used as a motivation for this paper (since this is the most typical scenario where states are available during training but not during testing), the experiments have not been performed using physical agents. Testing is performed on degraded simulated agents. I do understand that physical environments are harder to manage than simulated ones, but tiny physical environments can be obtained for reasonable prices, and this would have made the paper much stronger and more aligned with its core motivation. Simulated noise cannot replace a physical environment.\n\nNo details have been given on the additional distractor objects unseen during training.\n\nThe Walker 2D environment is described as modified, but how exactly, and why?\n\nSection 2.1 (definition of POMDPs) seems to be unnecessary and can be deleted.\n", "belong_id": "HygW26VYwS"}, {"uid": "B1gwk6-RFH", "paper_title": "Attention Privileged Reinforcement Learning for Domain Transfer", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Overall the method is interesting but I am not overly convinced the method provides a significant improvement over simpler methods that are not compared to the work here. Also, the generalization and extrapolation experiments need more description. As well, the work talks about how this method can be used to accelerate learning for transfer to real-robotic systems but does not have an example of this.\n\nMore detailed comments:\n- When you are referencing a paper that is very similar to your method and you build off of it is good to link to the published version ( the reference for Asymmetric DDPG was published at RSS2018.)\n- While the method is very interesting I feel that one of the obvious baselines that would be good to try is to train a model that maps o -> s (image to state). This is a very simple method and appears to accomplish the goals for the authors. A discussion on why this would not be good enough and some results to show that this performs poor would be good addition.\n- Related to the last point why are object-specific segmentation maps needed? It seems like the method alone was not capable enough to learn from pixels alone. Also, Can these segmentation maps be visually described? It is not very clear. Where do the segmentation maps come from?\n- Can you describe the shared replay buffer in more detail? What does no shared replay buffer mean?\n- The caption in Figure 2 is rather sparse and comes before the explication of the different methods in the figure. This figure needs more explanation. What are the other versions of April? What is the baseline? There does not appear to be a significant improvement? Can you show more of a qualitative improvement via videos of the policy performance?\n- It appears the method does not work as well on the Atari games can the authors provide some insights into why the method does not offer as much of an improvement? Is there no Attentive DQN for Pong, natural?\n- I do not understand what experiment is being performed in section 4.3.2. Maybe it would help if you explained what a canonical image is? Where do you get the compressed state information?\n- In 4.4 you show that APRiL does not do better than the baseline? What is the baseline?\n- In section 4.5 you describe how the method generalizes to the task with additional distractors. Can you describe specifically how this experiment is designed? How do the distractors compare to the other objects in the scene? are their colors changed? How many are there? This section needs a lot more detail to understand how much the reader can surmise about the methods ability to generalize.\n- In the same section, the authors say the image segmentation is crucial. Does this imply that maybe we should just put an image segmentation network in between the observation and the policy inputs? What is using this additional attention mechanism better than that simpler method?\n- For Table 1: It is interesting that the baseline does better than the APRiL no share or no sup. It might be important to perform an exploration as to why having those features reduces performance and the baseline appears more robust.\n- It would be nice to see this work applied to more tasks. For now the reacher and walking are the most interesting but those tasks are also somewhat basic. How would this method work on the walker 3d where there is even more partial observation?\n- Can the authors explain more how they have deduced from the attention map that the attention has figured out how to indirectly discover where the Jaco links are without explicit attention? I do not see this.\n- For figure 4, how the attention is visualized could be explained better. Are the white regions receiving all the attention? Also, What makes the difference between a Interpolation example and a Extrapolation example? Is it just more objects in the scene?\n- The authors should also reference 'Sim-to-Real Transfer of Robotic Control with Dynamics Randomization'.", "belong_id": "HygW26VYwS"}, {"uid": "rJxee2j0tB", "paper_title": "Attention Privileged Reinforcement Learning for Domain Transfer", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary - Building on top of the domain randomization principle (used to train policies robust to domain-variations) to learn policies which transfer well to new domains, the paper proposes an approach to improve and speed-up learning / training over randomized environments. The paper operates in a settings where the policy to be transferred only has access to observations -- images, etc -- and not the complete underlying state of a (simulated) environment. The underlying idea is to -- (1) maintain two sets of actor-critic networks - a symmetric pair where the actor has access to the underlying state and an asymmetric pair where the actor has access only to image observations; (2) evenly gather experiences from behavioral policies of both actors and store them in a shared replay buffer and (3) learn to align the attention placed by the policies over objects in the environment for the state and observation based actors. The idea is to leverage privileged information about the state (which is strictly more informative compared to observations) to learn robust observation based policies. Experimental results indicate the proposed approach improves generalization performance compared to several ablations of the same on both in-distribution and out-of-distribution environments.\n\nStrengths\n\n- The paper is generally well-written and easy to follow. The authors generally do a good job of motivating the proposed approach by leveraging access to privileged information in the environment / simulation / renderer during training to get more robust observation policies to transfer to novel settings. The proposed approach is presented after appropriately grounding the problem setting and preliminaries and the authors clearly state and evaluate on the axes of research questions they care about.\n\n- The proposed approach is somewhat novel and extends prior work on using shared replay buffers and asymmetric actor critic methods to accelerate training. Although the specific focus is on aligning object-level attention from a state-based actor and an observation based actor, the authors adopt design choices that help in preventing degenerate solutions -- for instance, the object-weighted squared error loss component to learn the observation attention module.\n\n- The experimental results more-or-less support the claims of the paper (however, only in comparison with ablations and 1 baseline) in the sense we see improvements in terms of average returns and sample-efficiency plots. Furthermore, the authors conduct ablations to understand which components of the proposed approach contribute significantly in different environments -- for both extrapolated and interpolated environments.\n\n- Sec 4.6 presents interesting analysis of the learned attention (observation) attention mechanism for both interpolated and extrapolated environments. It seems that attention is generally placed over relevant aspects (objects / links) of the environment and the associated takeaways seem feasible.\n\nWeaknesses\n\n- Having said that, there some weaknesses / questions which if addressed would make the paper stronger and help in increasing the rating of the paper.\n\n- Access to the object-specific attention maps seems to be an assumption that might not scale well across simulators. More realistic / richer simulations (say, 3D reconstructions of indoor rooms) may not always offer this much privileged information -- one might have access to fine-grained reconstructions from multiple viewpoints, but not object level maps. This, combined with the fact that major gains have only been demonstrated over the specified continuous control domains (ignoring the Atari results), makes me slightly concerned about the scalability of the proposed approach in terms of more real-world + applicable domain-transfer scenarios. Maybe a more general approach that learns to match some intermediate representations of the state and observation based actors is a more general approach. Can the authors comment on this?\n\n- Including the entropy loss while training the attention mechanism for the state based actor is justified only through feasible interpretations of the attention visualizations for the JacoReach experiments. Im curious how important is it for the state-based attention to be sparse? Does it actually affect performance if the entropy loss is not included while learning the state-based attention module?\n\n- Although the paper mentions experience is gathered evenly over the behavioral policies of both the actors its slightly unclear how that is being performed without actually referring to the algorithm pseudocode in the appendix. I would encourage the authors to include / move the same to the main paper. It makes the entire pipeline much easier to grasp.\n\n- Transfer to novel environments (not necessarily with domain-shifts) has also been studied in context of providing exploration incentives (see InfoBot - https://arxiv.org/abs/1901.10902) in addition to a (sparse / dense) episodic reward. I would be curious to see how well does APRiL compare to such approaches in a setting where both are applicable - say the MultiRoomNXSY set of experiments in InfoBot (pointed above). This is to understand if the gains obtained from APRiL are very specific to domain-shifts or are largely applicable to any form of novel environment transfer. Can the authors comment on this?\n\nReasons for rating\n\nBeyond the above points of discussion, I dont have major weaknesses to point \tout. I generally like the paper. The authors do a good job of identifying the sliver in which they make their contribution and motivate the same appropriately. My major point of concern is centered is around the fact that APRiL (probably) assumes access to much privileged information which may not be generally available across all kinds of environments, etc. My rating of the paper is based on the strengths and weaknesses highlighted above. Addressing / Responding to those appropriately would definitely help in improving the rating of the paper.", "belong_id": "HygW26VYwS"}, {"uid": "HkeufHm1cB", "paper_title": "Critical initialisation in continuous approximations of binary neural networks", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper addresses a very important and relevant topic of initialisation of weights of neural networks. It builds up on highly celebrated results of Poole, Schoenholz and others, using the language of mean field theory and an approach rooted in Dynamical Systems.\nWhat the authors propose is an extension of the approach to other settings. The paper is very scientific and math-heavy. A good practice in such cases is to adhere to a format of a scientific mathematical paper and organise the material using Theorema, Lemmata, Propositions and Corollaries , Definitions and Proofs. Such language and framework exists for a reason - to structure the material and make the paper readable. The paper as is a stream of equations and discussion making it very unclear what the point is.\nIn order for this paper to be suitable for publication the reviewer would like to strongly suggest:\n- Organise the material in a way that would make it clear what is claimed, what is proven etc.\n- Make more specific what the added value of the paper is.\nAlso - for the contribution of the paper to be less incremental it would be valuable to add more formality to the original results, for example - Gaussian approximation is claimed without any sort of verification of assumptions of any version of CLT or Law of Large Numbers.", "belong_id": "rylmoxrFDH"}, {"uid": "SkxNKaup9B", "paper_title": "Critical initialisation in continuous approximations of binary neural networks", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this paper, the authors investigate the training dynamics of binary neural networks when using continuous surrogates for training. In particular, they study what properties the network should have at initialisation to best train. They do so via mean field approximations in the limit of very wide networks.\n\nThe authors provide concrete advice: the mean of the stochastic weights should be close to +/-1 at initialisation. Being able to give such advice to ML practitioners is of great value, but since this advice feels counter intuitive (naively, the mean of a binary +/-1 activation is typically the output of a tanh, and initialising this close to +/-1 means that gradient are going to be roughly 0, and can easily be exactly 0 in low precision computes). Right now, the justification of this initialisation is hard to find in the paper. This point would deserve a dedicated section, giving a summary of the argument, with references to more technical parts of the paper.\n\nThe presentation should also be improved as we can find the following issues:\n* Missing letters, repeated words or even missing figures (Fig 5 and 7 in the appendix).\n* Authors need to explain what is Edge of Chaos and give some reference (for example  C. G. Langton. Computation at the edge of chaos. Physica D, 42, 1990). This will make the paper more accessible to researchers less familiar with the theory but interested in its practical applications.\n* Please, better explain figures, for example Figure 1: need to explain better what this plot is. Why are there both dotted and solid lines? What are the shaded regions: is that some confidence interval? But which one, and how many experiments were used for those plots?", "belong_id": "rylmoxrFDH"}, {"uid": "r1xtybOgir", "paper_title": "Critical initialisation in continuous approximations of binary neural networks", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper provides an in-depth exploration of stochastic binary networks, continuous surrogates, and their training dynamics with some potentially actionable insights on how to initialize weights for best performance. This topic is relevant and the paper would have more impact if its structure, presentation  and formalism could be improved. Overall it lacks clarity in the presentation of the results, the assumptions made are not always clearly stated and the split between previous work and original derivations should be improved. \n\nIn particular in section 2.1, the author should state what exactly the mean-field approximation is and at which step it is required (e.g. independence is assumed to apply the CLT but this is not clearly stated). Section 3 should also clearly state the assumptions made. That section just follows the background section where different works treating different cases are mentioned and it is important to restate here which cases this paper specifically considers. Aside from making assumptions clearer, it would be helpful to highlight the specific contributions of the paper so we can easily see the distinctions between straightforward adaptations of previous work and new contributions.\n\nSpecific questions:\n\nIt might be worth double checking the equation between eq. (2) and eq. (3) , the boundary case (l=0) does not make sense to me, in particular what is S^0 ?.\n\nWhat does the term hat{p}(x^l) mean in the left hand side of eq.(3)? \n\nIn eq. (7) (8) why use the definition symbol := ?\n\nAt the beginning of section 3.1, please indicate what matcal(M) precisely refers to. Using the term P(mathcal(M) = M_ij) does not make much sense if the intent is to use a continuous distribution for the means. \n\nJust after eq. (9), please explain what Xi_{c*} means. \n\nSmall typo: Eq. (10) is introduced as can be read from the vector equation 31, what is eq. (31)?\n\nIn section 5.2, why reducing the training set size to 25% of MNIST?\n", "belong_id": "rylmoxrFDH"}, {"uid": "H1g4TD85Fr", "paper_title": "Self-labelling via simultaneous clustering and representation learning", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary & Pros\n- This paper proposes a representation learning method based on clustering. The proposed method performs clustering and representation learning alternatively and simultaneously. This approach requires only a few domain-specific prior (precisely, CNN prior) while self-supervised learning requires more prior domain knowledge.\n- Compared to the previous work, DeepCluster, this paper uses the same objective for clustering and representation learning. For clustering, the objective can be formulated as an optimal transport problem and it can be efficiently solved. This approach provides desired properties such as convergence.\n- This paper shows the proposed method outperforms DeepCluster and it achieves comparable performance with SOTA methods in the representation learning literature.\n\nConcerns #1: More analysis should be provided.\n- The author claimed that the proposed method has better convergence properties than DeepCluster. To verify that, more experimental or theoretical supports should be provided. For example, the convergence rate might be checked as Figure 2(b) in DeepCluster paper using NMI against the previous iteration.\n- If the number of each class is imbalanced, the equipartition constraint might degrade the quality of the label assignment. Thus, ablation studies about the imbalance setting on small-sized datasets such as CIFAR should be provided. I think K-means could prevent such imbalance issues, so in this case, DeepCluster might perform well.\n\nConcerns #2: Performance is still far from SOTA.\n- As reported in Section 4.3, the proposed method still underperforms SOTA methods significantly. The SOTA method can be considered as a combination of (instance-wise) clustering and self-supervision. Thus, such a combination should be tried for improving performance.\n\nConcerns #3: How to guarantee this approach finds good semantic representations?\n- In this approach, the model generates a task via clustering, so it might suffer from unsuitable solutions even under the equipartition constraint. If we use a very deeper architecture and a larger size of embedding, then the main optimization problem (3) might be solved before correct label assignments. Moreover, at the first iteration, the labels might be totally random, and then clustering quality is also zero. How to guarantee the clustering quality is gradually improved while training?\n\nThe proposed method provides meaningful gain compared to the previous work, DeepCluster. I think this direction against self-supervised learning is important because it requires relatively smaller domain knowledge. However,  I'm not sure how the proposed method can converge stably and efficiently. So I think it would be better if more analysis about the convergence is given in a rebuttal.\n", "belong_id": "Hyx-jyBFPr"}, {"uid": "B1gMQL-2tB", "paper_title": "Self-labelling via simultaneous clustering and representation learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper develops a novel self-supervised learning method by combing clustering and representation learning together.\nDifferent from other methods, the two tasks are optimized within the same objective function. Under the weak assumption that the number of samples should be similar across different clusters, the authors further develop a modified Sinkhorn-Knopp algorithm to solve the problem. Experiments on real-world image data demonstrate the effectiveness of the developed solution. In general, the whole paper is well written and the developed solution is interesting. However, I have the following comments:\n\n1. I am still confused about the difference between self-supervised learning and clustering when we do not have labeled data. From my point of view, they are actually the same thing. The authors are suggested to provide more explanations about the differences.\n2. The used assumption that samples are uniformed distributed across different cluster are too strong in practice. In many real-world scenarios, the size of the cluster often very a lot, I was wondering how the proposed method can tackle this issue.\n3. Experiments are only conducted on the image dataset, which is not quite convincing. The authors are suggested to use the datasets that are normally used in the clustering research to further demonstrate the effectiveness of the method.\n4. It is quite surprising that conventional clustering evaluation metrics such as Normalized Mutual Information and Adjusted Rand Index are not used in the experiments.\n5. How about the time complexity of the developed algorithm? Can it be scaled to large datasets? Further complexity analyzes are suggested.", "belong_id": "Hyx-jyBFPr"}, {"uid": "SylFxBcV5S", "paper_title": "Self-labelling via simultaneous clustering and representation learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThe paper proposes a self-supervised learning procedure to train deep neural networks within an unsupervised learning setting. The authors build their work on a pretext task that consists in maximizing the information between the input data samples and labels that are basically obtained by a self-labeling procedure that clusters them in K distinct classes. This is similar to what was done in DeepCluster, a previous self-supervised algorithm that self-labels samples through clustering. However, differently from that approach, the current method does not introduce any additional clustering cost functions. Instead it implicitly achieves self-labeling by simply adding the constraint that label assignments equally partition the dataset. This constraint acts as a 'regularizer' that allows the authors to minimize the cross-entropy loss between inputs and pseudo-labels while avoiding the degenerate trivial solution where all samples are assigned to the same pseudo-label. As a result, the authors are able to derive a self-labeling method that optimizes the same cross-entropy loss as the classification task. That is done by remarking that minimizing the loss function over the pseudo-label assignments (under the equal partition constraint) can be formulated as an optimal transport problem that can be solved efficiently with a fast version of the Sinkhorn-Knopp algorithm. In practice, what the authors do at training time is to alternate between 1) minimizing the average cross-entropy loss by training the neural network (feature extractor + linear classification head) given a fixed pseudo-labels assignment, and 2) optimizing the pseudo-label assignments implemented as a matrix Q of posterior distribution of labels given the sample index, which, as said, can be done efficiently with a KL-regularized version of Sinkhorn. Moreover, this last step can be carried out simultaneously for multiple distinct classification heads (with possibly different number of labels), each sharing the same feature extractor but inducing a different matrix Q. At this point, the number of classification heads can be treated as a hyperparameter of the algorithm.\nThe authors then go on to show that this new algorithm is competitive with current state of the art method with several architectures in terms of providing a good feature extractor for downstream image classification, detection and segmentation tasks. They for instance consistently beat DeepCluster, the main direct competitor, on classification and detection tasks.\nThey also conduct ablation studies that provide interesting insights on the functioning of their algorithms and the effects of the multiple classification heads, the number of clusters, and the quality of the learned assignment. Intriguingly, on ImageNet they find for example that AlexNet obtains better performance at validation when it's trained from scratch on labels obtained with their self-labeling procedure, as opposed to the original labels.\n\nDecision:\nIn my opinion this paper should be a clear accept. The paper is well written, presents an elegant idea in a clear and straight-forward manner, and is solidly built on top of the current literature on self-supervised learning for image processing, which is also very well summarized.\nThe feature extractors obtained with the proposed algorithm are convincingly tested and validated on several downstream tasks (like classification on ImageNet, PascalVOC classification, detection and segmentation), and that is done for several base architectures, obtaining performances that are competitive with state-of-the-art. In addition, a series of careful ablation studies help in gleaning some scientific understanding on the method.\n\nMinor comments:\n- The authors refer to traditional clustering like k-means as being 'generative', which a little confusing. Clustering algorithms can be derived within a probabilistic framework by positing a generative model of the data, however, strictly speaking, k-means is not by itself is not a generative approach. It's a minor point, but it could be helpful to be more precise about this, in order to avoid possible confusion. The main point that the authors want to make in this regard is that their framework eschews having to posit an additional clustering cost function.", "belong_id": "Hyx-jyBFPr"}, {"uid": "r1lmLcNaYB", "paper_title": "Four Things Everyone Should Know to Improve Batch Normalization", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors discuss four techniques to improve Batch Normalization, including inference example weighing, medium batch size, weight decay, the combination of batch and group normalization. Equipped with the proposed techniques, the authors obtain promising results when training deep models with various batch sizes. However, the novelty of this paper seems very limited and more experiments are required.\n\nPlease see my detailed comments below.\n\nPositive points:\n1. The proposed inference example weighing method yields promising results and does not require any re-training.\n2. The combination of batch and group normalization makes it possible to train deep models with very small batch size.\n3. By combining all the techniques, the proposed method yields promising performance when training deep models with different batch sizes.\n\nNegative points:\n\n1. Some notations are very confusing. For example, the authors use B to represent the size of a minibatch. However, why do the authors only consider B-1 samples in Eq. (2), i.e., selecting the minimum possible output among x_1, ..., x_{B-1}?\n\n2. The proposed inference example weighing method seems very similar to Batch Renormalization. Both methods seek to use a linear function to combine the batch statistics and the moving statistics. What is the essential difference between these methods?\n\n3. What model do the authors use in the experiment of Figure 2? Why do the authors conduct experiments on different datasets in Section 3.1 (including ImageNet) and Section 3.2 (excluding ImageNet)? It would be stronger to provide ImageNet results in Section 3.2.\n\n4. The authors draw different conclusions about the usage of weight decay from a recent work (He et al, CVPR2019). The CVPR paper reports that training \\gamma and \\beta without weight decay on ResNet-50 yields significant performance improvement. However, this paper shows that training ResNet-50 with weight decay improves the performance. Please comment on the differences in the conclusions.\n\nReference: 'Bag of tricks for image classification with convolutional neural networks.' CVPR, 2019.\n\n5. The authors only report ImageNet results of the proposed inference example weighing method. However, all the experiments in Section 4 are performed on three small datasets. It is necessary and important to provide ImageNet results to show the effectiveness of the other three techniques in Section 4. \n\n6. Note that training deep models with non-i.i.d. minibatches is a typical case to evaluate normalization methods, e.g., Batch Renormalization. Specifically, examples in a minibatch are not sampled independently. What would happen if the authors apply the proposed techniques to the non-i.i.d. case?\n\n7. Some closely related work should be discussed in the paper, such as\n\n[1] 'Decorrelated Batch Normalization.' CVPR, 2018.\n[2] 'Double Forward Propagation for Memorized Batch Normalization.' AAAI, 2018.\n[3] 'Differentiable Dynamic Normalization for Learning Deep Representation.' ICML, 2019.\n[4] 'Iterative Normalization: Beyond Standardization towards Efficient Whitening.' CVPR, 2019.\n\nMinor issues:\n1. In Section 1, the third contribution is not a complete sentence.\n\n2. There are many typos in the paper.\n(1) In Section 2, Layer Normalization, which has found use in many natural language processing tasks. Should which has found use be which has been used?\n(2) In Section 3.1, Batch Normalization has a disparity in function between training inference. between training inference should be between training and inference.\n(3) In Section 3.1, we need only figure out ... should be we only need to figure out ...\n\n", "belong_id": "HJx8HANFDH"}, {"uid": "B1lHRzu6YS", "paper_title": "Four Things Everyone Should Know to Improve Batch Normalization", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces four techniques to improve the deep network model through modifying Batch Normalization (BN). The inspirations are from the gaps between train&test and between batches in multi-gpu training, comparison to other normalization methods, and weight decay in regularizing convolution weights training. The paper studies each techniques with the support from experiments. The paper is easy to follow. The techniques seem effective.\n\nThe paper mentions 'theory' multiple times, but lacks sufficient justification to support these 'theories'. So one suggestion is to replace 'theory' with a soft word.\n\nExperimental evidence seems sufficient and there are some theoretical derivations, but it looks incremental that the paper presents some techniques in improving Batch Normalization only. In general, the paper is of values to the community.", "belong_id": "HJx8HANFDH"}, {"uid": "r1glmJ2ptH", "paper_title": "Four Things Everyone Should Know to Improve Batch Normalization", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper performs an empirical study of four batch-normalization improvements and proposes a new normalization technique for small batch sizes, based on group and batch normalizations. Among others, the authors address the inconsistency between the train and the test stages and the problem of small batch sizes. The authors conducted an empirical ablation study of the four techniques and proposed an intuition when each method should be used.\n\nConcerns:\n(1) The comparison with baselines in section 4.2 seems to be unclear. Fig.5 shows the performance of normalization methods for different batch sizes. Batch Normalization, however, has the same performance for all batch sizes. The authors refer to this baseline as idealized Batch Normalization. Additional elaboration on what does this means is required. \n(2) It would also be beneficial to see the comparison with the original Ghost Batch Normalization in the final evaluation (section 4.2), since this method, according to section 3.2, was capable of the significant improvement for Caltech-256 dataset.\n(3) In section 3.1, the authors provide an intuition of why can the discrepancy between test and train phases hurts the performance of a model. The empirical evaluation of this effect is needed to justify this intuition.\n\nOverall, the newly proposed method is a minor update, and novelty is limited. However, the thorough empirical study of existing improvement techniques would be a good addition to the conference.\n\nMinor comments:\n1. share the y-axis in Fig.4 between different ghost batch sizes.\n\nI would also recommend authors to include the following papers to the related work section:\n1. Riemannian approach to batch normalization [https://arxiv.org/abs/1709.09603]\n\n----------\n\nRespond to the rebuttal. Clarification on the concern (3):\n\nI agree that, in general, a discrepancy between training and testing can hurt a model. The paper showed that the output of a batch normalization layer is theoretically unbounded during testing. However, it would be beneficial to see numerically if it indeed the case on a real test set (the output range is wider than the one during training).\n\n", "belong_id": "HJx8HANFDH"}, {"uid": "Bylh6JURtH", "paper_title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a novel deep reinforcement learning algorithm at the intersection of model-based and model-free reinforcement learning: Risk Averse Value Expansion (RAVE). Overall, this work represents a significant but incremental step forwards for this 'hybrid'-RL class of algorithms. However, the paper itself has significant weaknesses in its writing, analysis, and presentation of ideas.\n\nThe main strength of this work is its empirical section. The proposed algorithm is fairly compared against many relevant baselines on a variety of continuous control tasks, on both the Mujoco and Roboschool simulators, and demonstrates significant performance increases across all of them. The visualization in Figure 4 is interesting, and provides good insight into the issues with DDPG and STEVE, and the reasons for the success of RAME. Based on these results, the authors have convinced that RAME is a state-of-the-art algorithm.\n\nHowever, in spite of its strong performance on benchmarks, I believe that this paper needs a significant overhaul/rewrite before publication.\n\nOne major criticism I have is on the authors' treatment of the different types of uncertainty. The introductory sections spend a large amount of time on the differences between aleatoric and epistemic uncertainty, and various other related concepts. But when it comes to the actual new algorithm, that the authors only barely touched upon the (very important) point that V[Q^DVE] is actually a *mix* of epistemic and aleatoric uncertainties. When minimizing this term, it's not clear whether epistemic or aleatoric uncertainty is actually being reduced. (Based on the experiments, which report only expected values, not CVaR or anything of the sort, it seems the authors care only about reducing epistemic uncertainty; if so, it's unclear why they choose to minimize a term which includes aleatoric uncertainty too.) A more principled understanding of this quantity seems essential to this line of work. Similarly, 'risk' typically refers to aleatoric uncertainty, but the risk-senstive component of RAME computes the confidence lower bound w.r.t. a mix of aleatoric and epistemic uncertainty. Calling this algorithm 'risk sensitive' is likely to generate confusion, in my opinion.\n\nA few other points on presentation of ideas:\n- Switching from a deterministic to a stochastic model is a trivial extension of MVE/STEVE, and way too much time is spent on this point (even going so far as to name a new algorithm!). Section 4 contains no new insight, beyond 'deterministic models can be bad in stochastic environments', which is obvious. Consider re-thinking your experiments for this section to help readers better understand *what* goes wrong.\n- In my opinion, RAME is as much a successor to TD3 as it is to STEVE. Taking the lower outcome of an ensemble of size 2 is equivalent to applying a penalty based on the stdev of the outcome distribution, with =1. The introduction of the paper should be re-written to clearly point out that that RAME ~= TD3 + STEVE.\n- I'd also like to see a lot more ablations, on at least a few environments. There are at least four factors that need to be teased apart: 1) deterministic vs stochastic model 2) lower confidence bound penalty 3) adaptive  for LCBP 4) STEVE reweighting. Does RAME require all of these elements to perform well? How do these elements interact?\n\nSome feedback on the writing:\n- There are various small factual errors. For example, in the very first paragraph, the Dyna algorithm is attributed to Kuturach et al, instead of Sutton (1990). (The algorithm from Kuturach is ME-TRPO.)\n- The notation is not very good; there are lots of symbols flying around everywhere, and it makes the ideas (which are fundamentally very simple) a bit difficult to parse. For example, naming each parameter-set individually, everywhere, is unnecessary.\n- There are some strange non-sequiturs and overall lack of flow.\n\nI think that this work has a lot of potential, and am especially impressed by the empirical results. I recommend rejection in its current form, but hope to see a revised version of this work appear at a conference in the near future.", "belong_id": "SJlbvp4YvS"}, {"uid": "S1etZbgy9B", "paper_title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work , the authors combine model-based value expansion(MVE) with\nmodel-free reinforcement learning and also take into account the high-order stochastic characteristics\nof the environment to make the value expansion algorithm in modeling error propagation of dynamics risk-averse. \nThey propose a novel Hybrid-RL method, namely the Risk Averse Value Expansion(RAVE), that uses an ensemble of probabilistic dynamics models to generate imaginative rollouts and to model risk aversion\nof risks by estimating the lower confidence bound of the ensemble. They showed that by taking the risk (in terms of variance) of the dynamic propagation error into the account, RAVE can achieve comparable performance with other state-of-the-art baselines in benchmark experiments including MuJoCo and robo-school. Also they showed that RAVE reduced the variance of the return and thus prevented catastrophic consequences such as falling.\n\nI found this work interesting as the authors try to take the uncertainty of probabilistic dynamics model into account for estimating the value function and its confidence bounds in model-free RL. Utilizing such confidence interval of values, they can have a way of doing exploration while being risk-averse. While similar approaches of modeling the mean and variance of Q functions can be found in some existing work, such as boot-strapped DQN (Osband'16), and efficient exploration via Bayesian DQN (Azizzadenesheli'18), none of these work model the variance of the Q-function using the error propagation of the dynamics model. Through extensive experiments, it has also demonstrated the effectiveness of RAVE in terms of achieving good performance, approximation error in value estimation, as well as robustness to failure in standard mujoco benchmarks, which provides readers some detailed understanding on how this risk-averse uncertainty modeling/propagation technique helps in exploration. However, while i found this idea interesting, it appears to me that the current work is still quite preliminary (without theoretical concentration inequality based variance bounds for guiding exploration), and empirically, it would be great to compare this method with the aforementioned bootstrapping/bayesian based approaches. My other major comment is on claiming the proposed method to be 'risk-averse', because in RL, risk-averse methods are commonly known to not only optimize the expected return but also to provide guarantees to other moments of the return, such as variance of CVaR. However, while the method used variance of the expected value (due to the error propagation of the dynamics) for exploration, I am not seeing the risk-averse optimization criteria being studied here. Therefore calling the RL method risk-averse might be a little mis-leading.", "belong_id": "SJlbvp4YvS"}, {"uid": "Ske-fbzGqr", "paper_title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the problem of high function approximation errors facing the stochastic environments when trying to combining model-based reinforcement learning (RL) with model-free RL. The paper begins by showing that previous methods like model-based value expansion (MVE) and stochastic ensemble value expansion (STEVE) can perform even worse than the pure model-free DDPG algorithm in a rather noisy environment. Following the ideas of MVE and STEVE, It then proposes a risk averse value expansion (RAVE) to replace the target Q function in the actor-critic algorithm, which is built upon an ensemble of probabilistic models (PE) and adopt the lower confidence bound as a surrogate of the target value as in the risk-sensitive RL. A simple yet intuitive approach for adaptively selecting the confidence bound \\alpha is also proposed. The experiments show that RAVE does improve over the state-of-the-art algorithms in several different environments, with a better draw-down control. In general, this paper is well-written and the idea of RAVE is novel as far as I know. But since I'm not very familiar with the specific literature of combing model-based and model-free RL, and since the idea of RAVE is relatively straightforward (but admittedly practically powerful and theoretically interesting), I choose to give a conservative accept to account the possibility that some existing works have followed very similar approaches. \n\nSome minor comments: 1) What is the loss of the negative log-likelihood in (4) for? 2) The authors may want to explain clearly what is the variance indicating in (8) more clearly, although one can guess that it is closely related to (5). ", "belong_id": "SJlbvp4YvS"}, {"uid": "SylI6P9iqr", "paper_title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\nThis paper expands on previous work on hybrid model-based and model-free reinforcement learning. Specifically, it expands on the ideas in Model-based Value Expansion (MVE) and Stochastic Ensemble Value Expansion (STEVE) with a dynamically-scaled variance bias term to increase risk aversion over the course of learning, which the authors call Risk Averse Value Expansion (RAVE). Experimental results indicate notable improvements over their selected model-free and hybrid RL baselines on continuous control tasks in terms of initial learning efficiency (how many environment steps are needed to achieve a particular level of performance), asymptotic performance (how high the performance is given the same large number of environment steps), and avoidance of negative outcomes (how infrequently major negative outcomes are encountered over the course of training).\n\nReview\nThe core contribution of the paper is an extension to STEVE that uses an idea from risk-averse RL of biasing the underlying estimators away from high-variance predictions, and adds a dynamic weight to that bias.\n\nStrengths\n- The addition of a risk-aversion term to STEVE is a good contribution to the literature on safe RL. While it may be possible to criticize this contribution as somewhat trivial, I am disinclined to do so, as finding simple ideas that are effective is the hallmark of an important contribution, in my opinion.\n- Under the assumption that the baselines are fair, the empirical results show substantial improvements for a good selection of challenging tasks along three metrics: initial efficiency, asymptotic performance, and avoidance of negative outcomes.\n- The paper provides a careful and detailed section on the relevant preliminaries, giving precise notation that is expanded step-by-step from basic actor-critic approaches all the way through the description of RAVE. (Although see my comment below about possibly making the notation more concise.)\n- Overall, the papers presentation is clear and natural. (Although see my comment below about a strong need for proofreading.)\n\nWeaknesses\n- The paper opens with a claim that MBRL has worse performance than MFRL in environments with noisy environments and long trajectories. However, recent work [1] has shown that MBRL can outperform MFRL on many of the tasks considered in this paper using far fewer environment steps, even when training directly from pixels.\n- The experiments are all based on observations of the true state vectors rather than pixels, which dramatically simplifies working with imagination rollouts, as the dimensionality is so much lower. This hides the high computational cost and modeling challenges involved in using imagination-based MBRL or hybrid RL approaches in real-world environments, and (in my opinion) is a major shortcoming of this line of research (not only of this paper). Since the DDPG family of algorithms doesnt have to do rollouts, they have a strong advantage over this type of approach on more realistic settings where the true state is unknown and must be inferred from high-dimensional observations. If experiments are not going to be done using pixels, the discussion should at least mention the trade-offs involved between the proposed algorithm and the baselines in that setting.\n- The core baseline of the paper is DDPG, which is unnecessarily weak. D4PG [2] came out a year and a half ago and has the same high-level properties as DDPG, but outperforms it on all of the Mujoco tasks considered in this paper. Additionally, it includes some of the ideas presented in this work, but in a model-free setting, including a distributional treatment and multi-step rollouts, so it is easy to imagine that some of the experimental gains presented here would be erased when using it as a baseline.\n- The use of ensembling is another confounder in the experiments. Ensembling models almost always yields an improvement, so any technique that relies on some form of ensembling needs to additionally demonstrate that the gains presented are not solely due to ensembling. In this case, the experiments with STEVE and RAVE should minimally be performed with different numbers of models in their ensembles.\n- (Minor) Similarly, the prediction horizon can have a large impact, and only STEVE and RAVE consider prediction horizons other than 1. Showing how the horizon affects performance would help make the comparisons more fair.\n- Figure 1 shows a bias problem in some current approaches in a clear toy problem, but does not show whether RAVE addresses that problem. Even though this section precedes the presentation of RAVE, it is necessary to show that the proposed solution actually helps on the toy problem.\n- There are a number of problems with the proposal for adaptively computing the alpha parameter. In general, such adaptive hyperparameters add a great deal of complexity to hyperparameter tuning, so such suggestions should be either strongly motivated by theory or by empirical results. Neither seems to be the case here.\n  - Only one proposal for adaptive alpha (equation 13) is considered. Its justification is plausible, but it would be more convincing if other dynamic approaches were considered in the experiments. For example:\n    - alpha(env_step) = alpha * env_step / max_env_step\n    - alpha(s_t,a_t) = alpha * min{1.0, 1 / (Z * ||E[fhat(s_t,a_t)] - s_t+1||^2)}\n    - The opposite of the proposed approach, where alpha starts high and gets lower over the course of training.\n  - Indeed, a potentially useful quantity during evaluation, when s_t+1 is unknown, would be based on the variance of the predicted next state, rather than the difference of the expectation of the predicted next state and the true next state. E.g.:\n    - alpha(s_t,a_t) = alpha / Var[fhat(s_t,a_t)]\n    This formulation says that the precision of the prediction determines the confidence of the model, which is also intuitively reasonable (to me, anyway) and doesnt rely on knowing the future.\n  - (Minor) The adaptive variant appears to be only slightly better than alpha=0.5, but requires two hyperparameters that are unspecified in the main body of the paper  alpha and Z. The appendix lists those parameters, but no discussion is made on how much tuning was done to determine that pair of parameters.\n  - (Minor) A plot of how the dynamic value of alpha changes during training would be useful, at least in the appendix.\n\nRecommendation\nIn light of my comments above, I cannot currently recommend the acceptance of this paper at ICLR. However, I think that the core idea is likely to hold up under more careful experimental comparisons. If the authors submit a revised draft that addresses the substance of my concerns, I would be very likely to increase my rating. In particular, I would like to see much more careful experimental treatment of the idea, so that readers can have high confidence about the circumstances where RAVE is likely to be a good choice.\n\n[1] Hafner et al., Learning Latent Dynics for Planning from Pixels. ICML 2019. https://arxiv.org/abs/1811.04551\n[2] Barth-Maron et al., Distributed Distributional Deterministic Policy Gradients. ICLR 2018. https://arxiv.org/abs/1804.08617\n\n\nOther Comments and Questions\n- This paper needs a lot of proofreading. A few examples of errors that should be fixed before publication:\n  - equation. 2: This should read Equation 2, equation 2, Eq. 2 or eq. 2. The period in the last two options indicates that letters have been elided. The mistake of using a period where no elision has happened occurs throughout the paper.\n  - prophesy: This is not the correct word. Just say prediction.\n  - image: This is often used when the correct word would be either imagine or imagination.\n  There are many other errors that could be fixed easily with the help of a native English speaker.\n- The mathematical notation in sections 3 through 5 is precise, but it is also a bit heavy. Consider whether there would be any ambiguity added if, for example, {\\hat Q}^DVE_{\\zeta_s,\\zeta_r,\\zeta_d,\\theta,\\phi} were instead notated {\\hat Q}^DVE_{\\zeta,\\theta,\\phi}.\n- Figure 2 adds nothing of value to the paper and should be removed.\n- Consider comparing on DMControl, which is the same set of tasks as Mujoco, but the scores are standardized such that each task has a maximum reward of 1000 per episode.\n- Why does DDPG get worse over time on the RoboSchool tasks? Without a clear explanation, it looks like a bug, and a bug like that calls into question the rest of the DDPG results as well.\n", "belong_id": "SJlbvp4YvS"}, {"uid": "BylXkCeatH", "paper_title": "JAUNE: Justified And Unified Neural language Evaluation", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper argues that BLEU and ROUGE, two metrics that are used for the evaluation of machine translation and text summarization systems, are flawed, and proposes a new JAUNE metric to replace it.\n\nThe authors train a regressor on the STS-B dataset, and show that their model (which is using sentence embeddings from RoBERTa) corresponds better to the ground truth similarity labels than then scaled (but otherwise unchanged) BLEU scores. This is probably not surprising, given the small size and specific nature of the STS-B task and dataset. \n\nI could agree with many of the problems that the authors describe, but the proposed solution seems to be a very specific solution that works on a given dataset (for which supervised training data is available), but I do not think it will generalize well to unseen test data in different domains. I also do not understand how the BLEU score can simply be rescaled from 0-5 - how do you determine the maximum BLEU score before rescaling?\n\nThe paper should be proofread by a native speaker, many sentences are unclear, and spacing as well as punctuation are used in weird ways.\n", "belong_id": "r1gx60NKPS"}, {"uid": "HyeKi-nlcH", "paper_title": "JAUNE: Justified And Unified Neural language Evaluation", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\n=== Summary ===\n\nThe authors motivate the development of new (automatic) metrics to evaluate language generation by using similarity with a given reference: standard metrics like BLEU, ROUGE or METEOR have been shown to have poor correlation with human judgment on a number of tasks and are vulnerable to changes in word re-ordering, semantics-changing word replacement, and syntactic transformations.\n  \nThey then propose a multi-dimensional evaluation criteria to evaluate sentence similarity based on semantic similarity (something that correlates with human judgments of the same), logical equivalence and fluency.\n  \nThe paper then goes on to describe possible directions to tackle several key problems in evaluation: evaluating semantic similarity by using models trained on the GLUE benchmark, evaluating logical equivalence using models trained on the MNLI corpus and fluency based on the CoLA corpus.\n\n=== Decision ===\n\nThe problem this paper seeks to tackle is clearly one of great\nimportance in the field, but I find it hard to argue that this paper\nsignificantly contributes to the existing body of work (more on this\nbelow) and as a result I vote to reject this paper.\n\nThere are two possible contributions for this paper: a set of criteria for what makes a good evaluation metric and the concrete proposals to implement these criteria.\n  \nFor the first, I find the proposed criteria to be overly generic and not helpful at providing additional clarity on what makes for a good evaluation: for example, how is semantic similarity different from logical consistency? Does it make sense to compare the semantic similarity of two sentences if one of them isn't even near grammatical? A lot of prior work already argue the shortcomings of the existing metrics this paper is making, e.g. Conroy and Dang (2008), Liu et al. (2016), Novikova et al. (2017). I think it would be valuable to present new axes to decompose the evaluation problem, but more work is needed to clarify and develop the axes presented in this paper.\n  \nFor the second possible contribution, the idea of evaluating language generation along dimensions is not novel and in fact quite standard in the NLP community. The challenge has been showing that there are subset of tasks that can be used a reliable metrics across different domains and systems. Unfortunately, this paper does not actually evaluate its own proposals, making it hard to evaluate how effective its proposals are.  \n", "belong_id": "r1gx60NKPS"}, {"uid": "H1xKiqomqH", "paper_title": "JAUNE: Justified And Unified Neural language Evaluation", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Observing shortcomings of BLEU and ROUGE, the paper proposes, JAUNE, a set of criteria for a good evaluation metric. These criteria include: high correlation with human judgement; being able to distinguish similar but contradicting statements; penalizing grammatical errors, and hard to game.\n\nThe paper, as its current form, is not ready for publishing. Some suggestions and comments:\n\n- Please carefully check the paper and fix typos and confusing sentences. I was collecting these errors but eventually stopped. Some examples. Sec. 2.3: punctuation missing between 'RUSE' and 'this method', comma missing after 'a discrete space'; Sec. 4.1.1: 'made to ,for example'....\n\n- The motivation of the paper is unclear. Is your criticism only about BLEU and ROUGE, or the state of the arts in NLP evaluation in general? To make JAUNE appealing, one has to argue that the state of the arts in NLP evaluation is ineffective. For this, the paper needs to review a boarder range of metrics beyond just BLUE and ROUGE. \n\n- While the authors suggest a data-driven metric, it reads to me like a model-driven metric (RoBERTAa specifically). Doesn't it systematically bias towards a certain family of metrics? \n\n- Better and more comprehensive experimental results are highly desired. ", "belong_id": "r1gx60NKPS"}, {"uid": "SJx_brSjKH", "paper_title": "Classification-Based Anomaly Detection for General Data", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "UPDATE:\nI acknowledge that Ive read the author responses as well as the other reviews.\n\nI appreciate the clarifications, additional experiments, and overall improvements made to the paper. I updated my score to 6 Weak Accept. \n\n\n####################\n\nThis paper proposes a deep method for anomaly detection (AD) that unifies recent deep one-class classification [6] and transformation-based classification [3, 4] approaches. The proposed method transforms the data to $M$ subspaces via $M$ random affine transformations and identifies with each such transformation a cluster centered around some centroid (set as the mean of the respectively transformed samples). The training objective of the method is defined by the triplet loss [5] which learns to separate the subspaces via maximizing the inter-class as well as minimizing the intra-class variation. The anomaly score for a sample is finally given by the sum of log-probabilities, where each transformation-/cluster-probability is derived from the distance to the cluster center. Using random affine transformations, the proposed method is applicable to general data types in contrast to previous works that only consider geometric transformations (rotation, translation, etc.) on image data [3, 4]. The paper conclusively presents experiments on CIFAR-10 and four tabular datasets (Arrhythmia, Thyroid, KDD, KDD-Rev) that indicate a superior detection performance of the proposed method over baselines and deep competitors.\n\nI think this paper is not yet ready for acceptance due to the following main reason: \n(i) The experimental evaluation needs clarification and should be extended to judge the significance of the empirical results.\n\n(i) I think the comparison with state-of-the-art deep competitors [6, 4] should consider at least another image dataset besides CIFAR-10, e.g. Fashion-MNIST or the recently published MVTec [1] for AD. On CIFAR-10, do you also consider geometric transformations however using your triplet loss or are the reported results from random affine transformations? I think reporting both would be insightful to see the difference between image-specific and random affine transformations.\nOn the tabular datasets, how do deep networks perform in contrast to the final linear classifier reported on most datasets? Especially when only using a final linear classifier, the proposed method is very similar to ensemble learning on random subspace projections. Figure 1 (right) shows an error curve that is also typical for ensemble learning (decrease in mean error and reduction in overall variance). I think this should be discussed and ensemble baselines [2] should be considered for a fair comparison. Table 2 also seems incomplete with the variances missing for some methods?\nFurther clarifications are needed. How many transformations $M$ do you consider on the specific datasets? How is hyperparameter $s$ chosen?\nFinally, I think the claim that the approach is robust against training data contamination is too early from only comparing against the DAGMM method on KDDCUP (Is Figure 1 (left) wrong labeled? As presented DAGMM shows a lower classification error).\n\nOverall, I think the paper proposes an interesting unification and generalization of existing state-of-the-art approaches [6, 4], but I think the experimental evaluation needs to be more extensive and clarified to judge the potential significance of the results. The presentation of the paper also needs some polishing as there are many typos and grammatical errors in the current manuscript (see comments below).\n\n\n####################\n*Additional Feedback*\n\n*Positive Highlights*\n1. Well motivated anomaly detection approach that unifies existing state-of-the-art deep one-class classification [6] and transformation-based classification [3, 4] approaches that indicates improved detection performance and is applicable to general types of data.\n2. The work is well placed in the literature. All relevant and recent related work is included in my view.\n\n*Ideas for Improvement*\n3. Extend and clarify the experimental evaluation as discussed in (i) to infer statistical significance of the results.\n4. I think many details from the experimental section could be moved to the Appendix leaving space for the additional experiments.\n5. Maybe add some additional tabular datasets as presented in [2, 7].\n6. Maybe clarify Classification-based AD vs. Self-Supervised AD a bit more since unfamiliar readers might be confused with supervised classification.\n7. Improve the presentation of the paper (fix typos and grammatical errors, improve legibility of plots)\n8. Some practical guidance on how to choose hyperparameter $s$ would be good. This may just be a default parameter recommendation and showing that the method is robust to changes in s with a small sensitivity analysis.\n\n*Minor comments*\n9. The set difference is denoted with a backslash not a forward slash, e.g. $R^L \\setminus X$.\n10. citet vs citep typos in the text (e.g. Section 1.1, first paragraph  ... Sakurada & Yairi (2014); ...)\n11. Section 1.1: ADGMM introduced by Zong et al. (2018) ...  DAGMM introduced by Zong et al. (2018) ....\n12. Eq. (1): $T(x, \\tilde{m})$ in the first denominator as well.\n13. Section 2, 4th paragraph: $T(x, \\tilde{m}) \\in R^L \\setminus X_{\\tilde{m}}$.\n14. $m$, $\\tilde{m}$, and $m'$ are used somewhat inconsistently in the text.\n15. Section 3: Note, that it is defined everywhere.?\n16. Section 4: 'If $T$ is chosen deterministicaly ...' >> 'If $T$ is chosen deterministically ...'\n17. Section 5, first sentence: ... to validate the effectiveness our distance-based approach ...  ... to validate the effectiveness of our distance-based approach ....\n18. Section 5.1: We use the same same architecture and parameter choices of Golan & El-Yaniv (2018) ...  We use the same architecture and parameter choices as Golan & El-Yaniv (2018) ...\n19. Section 5.2: Following the evaluation protocol of Zong et al. Zong et al. (2018) ...  Following the evaluation protocol of Zong et al. (2018) ....\n20. Section 5.2: Thyroid is a small dataset, with a low anomally to normal ratio ...  Thyroid is a small dataset, with a low anomaly to normal ratio ....\n21. Section 5.2, KDDCUP99 paragraph: Tab. ?? reference error.\n22. Section 5.2, KDD-Rev paragraph: Tab. ?? reference error.\n\n\n####################\n*References*\n\n[1] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ada comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 95929600, 2019.\n[2] J. Chen, S. Sathe, C. Aggarwal, and D. Turaga. Outlier detection with autoencoder ensembles. In SDM, pages 9098, 2017.\n[3] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018.\n[4] I. Golan and R. El-Yaniv. Deep anomaly detection using geometric transformations. In NIPS, 2018.\n[5] X. He, Y. Zhou, Z. Zhou, S. Bai, and X. Bai. Triplet-center loss for multi-view 3d object retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 19451954, 2018.\n[6] L. Ruff, R. A. Vandermeulen, N. Gornitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Muller, and M. Kloft. Deep one-class classification. In International Conference on Machine Learning, pages 43934402, 2018.\n[7] L. Ruff, R. A. Vandermeulen, N. Gornitz, A. Binder, E. Muller, K.-R. Muller, and M. Kloft. Deep semi-supervised anomaly detection. arXiv preprint arXiv:1906.02694, 2019.", "belong_id": "H1lK_lBtvS"}, {"uid": "HkgZFGqjKH", "paper_title": "Classification-Based Anomaly Detection for General Data", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a novel approach to classification-based anomaly detection for general data. Classification-based anomaly detection uses auxiliary tasks (transformations) to train a model to extract useful features from the data. This approach is well-known in image data, where auxiliary tasks such as classification of rotated or flipped images have been demonstrated to work effectively. The paper generalizes to the task by using the affine transformation y = Wx+b. A novel distance-based classification is also devised to learn the model in such as way that it generalizes to unseen data. This is achieved by modeling the each auxiliary task subspace by a sphere and by using the distance to the center for the calculation of the loss function. The anomaly score then becomes the product of the probabilities that the transformed samples are in their respective subspaces. The paper provides comparison to SOT methods for both Cifar10 and 4 non-image datasets. The proposed method substantially outperforms SOT on all datasets. A section is devoted to explore the benefits of this approach on adversarial attacks using PGD. It is shown that random transformations (implemented with the affine transformation and a random matrix) do increase the robustness of the models by 50%. Another section is devoted to studying the effect of contamination (anomaly data in the training set). The approach is shown to degrade more gracefully than DAGMM on KDDCUP99. Finally, a section studies the effect of the number of tasks on the performance, showing that after a certain number of task (which is probably problem-dependent), the accuracy stabilizes.\n\n\nPROS:\n\n* A general and novel approach to anomaly detection with SOT results.\n\n* The method allows for any type of classifier to be used. The authors note that deep models perform well on the large datasets (KDDCUP) while shallower models are sufficient for smaller datasets.\n\n* The paper is relatively well written and easy to follow, the math is clearly laid out.\n\n\nCONS:\n\n* The lack of a pseudo-code algorithm makes it hard to understand and reproduce the method\n\n* Figure 1 (left) has inverted colors (DAGMM should be blue - higher error).\n* Figure 1 (right) - it is unclear what the scale of the x-axis is since there is only 1 label. Also the tick marks seem spaced logarithmically, which, if i understand correctly, is wrong.\n\n* The paragraph 'Number of operations' should be renamed 'Number of tasks' to be consistent. Also the sentence 'From 16 ...' should be clarified, as it seems to contrast accuracy and results, which are the same entity. The concept of 'stability of results' is not explained clearly. It would suffice to say: 'From 16 tasks and larger, the accuracy remains stable'.\n\n* In section 6, the paragraph 'Generating many tasks' should be named 'Number of tasks', to be consistent with the corresponding paragraph in section 5.2. Also the first sentence should be: 'As illustrated in Figure 1 (right), increasing the number of tasks does result in improved performance but the trend is not linear and beyond a certain threshold, no improvements are made. And again the concept of 'stability' is somewhat misleading here. The sentence '...it mainly improves the stability of the results' is wrong. The stability is not improved, it is just that the performance trend is stable.\n\n* The study on the number of tasks should be carried on several datasets. Only one dataset is too few to make any claims on the accuracy trends as the number of task is increased.\n\n* The authors should coin an acronym to name their methods.\n\nOverall this paper provides a novel approach to classification-based semi-supervised anomaly detection of general data. The results are very encouraging, beating SOT methods by a good margin on standard benchmarks.\n", "belong_id": "H1lK_lBtvS"}, {"uid": "HJgOSSS6Fr", "paper_title": "Classification-Based Anomaly Detection for General Data", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Review: The paper proposes a technique for anomaly detection. It presents a novel method that unifies the current classification-based approaches to overcome generalization issues and outperforms the state of the art. This work also generalizes to non-image data by extending the transformation functions to include random affine transformations. A lot of important applications of anomaly detection are based on tabular data so this is significant. The normal data is divided into M subspaces where there are M different transformations, the idea is to then learn a feature space using triplet loss that learns supervised clusters with low intra-class variation and high inter-class variation. A score is computed (using the probabilities based on the learnt feature space) on the test samples to obtain their degree of anomalousness. The intuition behind this self-supervised approach is that learning to discriminate between many types of geometric transformations applied to normal images can help to learn cues useful for detecting novelties. \n\nPros:\n- There is an exhaustive evaluation and comparison across different types of data with the existing methods along with the SOTA. \n- It is interesting to see how random transformations indeed helped to achieve adversarial robustness.\n- The method is generalized to work on any type of data with arbitrary number of random tasks. It can even be used in a linear setting if needed for small datasets.\n\nCons:\n- While I liked that an analysis was done to see the robustness of the method on the contaminated data, I would be interested to see a more rigorous comparison in this fully unsupervised setting. \n\n\nComments/Question:\nDoes the selection of the transformation types affect the method performance at all? \n\nIn the Results section on Page 7, there are a couple of ?? instead of table numbers.\n", "belong_id": "H1lK_lBtvS"}, {"uid": "Hyey1w5AtS", "paper_title": "Mincut Pooling in Graph Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a solution to the important problem of pooling in graph neural networks. The method relies on minimizing a surrogate function inside the standard SGD loop and in conjunction with the optimization of the model parameters - such loss function aiming at optimizing the minCut on the graph. By that it aims to effective achieve a soft clustering of nodes that are both well connected and that have similar embeddings. This in an elegant choice, somewhat resembling the DiffPool method since it's also end-to-end trainable. However it adds the local graph connectivity information due to the minCut loss (and related orthogonality penalty to achieve non trivial solutions on the relaxed minCut continuous problem). Such local graph connectivity is indeed important information to consider when carrying out pooling.\nResults show good performance improvement on different tasks of graph clustering, node and whole graph classification. The paper is well written and clear to read. The math is solid and the concept is well substantiated by results.\nI found no mention about code release and I would solicit the authors to release the code to reproduce the experiments.", "belong_id": "BkxfshNYwB"}, {"uid": "H1gU4xse9B", "paper_title": "Mincut Pooling in Graph Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose a differentiable pooling method for graph data, known as minCUTpool. It learns a clustering assignment matrix using MLPs and then add regularization terms to encourage the clustering results close to the minCUT. The experimental results show that the regularization terms can help improve the performance.\n\nCons:\n1. The novelty is limited. Compared with existing work DiffPool, the proposed method is improving the Diffpool by adding two regularization terms. In addition, the main regularization $L_c$ is already proposed in previous studies. \n2. The motivation is not clear. Why should we apply minCut for graph pooling? Intuitively, how is the minCUT related to graph representation learning? The minCut can identify dense graph components but why these dense components should be different clusters in graph pooling? In addition, the author claim cluster together nodes which have similar features. How could minCut terms lead to such conclusion? \n3.  Important baselines are missing, such as Sortpool (Zhang et al, An end-to-end deep learning architecture for graph classification, AAAI 2018), Self-attention pool (Lee et al, Self-Attention Graph Pooling, ICML 2019). \n4. The graph classification results are not convincing enough. In the original Top-K paper (Gao et al , Graph U-Net, ICML2019), the reported results for Proteins and DD datasets are 77.68%, 82.43%, which are significantly better than the results reported in this paper. ", "belong_id": "BkxfshNYwB"}, {"uid": "BJgfMuCt5S", "paper_title": "Mincut Pooling in Graph Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a graph pooling method by utilizing the Mincut regularization loss. It is an interesting idea and performs well in a number of tasks. However, due to the limitation of novelty and poor organizations, this paper cannot meet the standard of ICLR. The detailed reasons why I give a weak reject are listed as follows:\n\n1. Even though the proposed minCUT pool is interesting, the contribution is not enough to get published in the ICLR. If I understand correctly, the only difference is the unsupervised loss, compared with the previous work, Diffpool [1].\n \n2. The paper needs to be reorganized to demonstrate its contribution. The proposed method section only has around 1.5 pages, making it difficult to understand the proposed method clearly. Therefore, more details and analyses about the proposed method should be included to support and clarify the idea.\n \n3. The paper needs to be improved for its theoretical derivations and proof. For example, it is not clear why Equation (6) is correct, which is the main contribution of this paper. The authors provide intuitive thoughts but there are not theoretical derivations and proof. The term $L_c$ comes from Equation (2) but why is it correct to only compute the trace?\n \n4. Some experiments cannot support the claim very well.  For example, the graph clustering experiments are not convincing. The goal of graph pooling is to learn high-level graph embeddings but not perform graph clustering. It is not proper to evaluate the graph pooling method using graph clustering tasks. Or, the author should clarify the motivation to do this experiment. If the model is trained for graph classification or node classification, then why should the node clusters lead to high NMI or CS scores?\n\n[1]. Ying et al., Hierarchical Graph Representation Learning with Differentiable Pooling, NIPS 2018\n\n\n==========Update===========\n\nI have read authors response and other reviews. While the authors address some of my concerns, I still believe the contribution/novelty is limited. I am sticking to my score.", "belong_id": "BkxfshNYwB"}, {"uid": "B1lawgdQtH", "paper_title": "Analytical Moment Regularizer for Training Robust Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a regularizer to encourage the robustness to the random contamination for training deep neural networks. The idea is straightforward and intuitive, but not that exciting. The experiments show some improvement. However, I have a few serious concerns:\n\n(1) Why do we care about the random noise, especially Gaussian noise? There has been a large amount of literature on training robust network, but they are also for the robustness to adversarial examples. The Gaussian noise is too simple and easy to defend. We can even apply a simple denoiser to preprocess the data, which does not even involve training a sophisticated neural network.\n\n(2) The proposed moment regularizer is very delicate. I do not think it can generalize to other noises or contaminations. This is because for other noises, the moment approximation can be fairly loose.\n\n(3) The Alexnet was proposed in 2011. Consider that it is already late 2019, the authors INDEED need to do experiments using more advanced and recent models, e.g., ResNet34/50 or even powerful ones, e.g, ResNeXt, DenseNet, Wide ResNet.", "belong_id": "B1xDq2EFDH"}, {"uid": "B1lFqwX6YB", "paper_title": "Analytical Moment Regularizer for Training Robust Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I am not fully convinced by the robustness result in Figure 3. In MNIST, the proposed method is worse than data augmentation. In CIFAR-10, the proposed method does perform better, however, \\tilde{N} for data augmentation is chosen as 2, which is too small in my opinion. The data augmentation's robustness is similar to the baseline. I don't know if there are any issues in the training but data augmentation with \\tilde{N}=2 is expected to be ineffective to improve robustness. For CIFAR-100, the improvement is marginal and can only be achieved when \\sigma is large.\n\nIs the GNR score in Table 1 calculated on all examples or only on correctly classified examples? If it is calculated on all examples, I think it would be better to also report the result of the correctly classified examples. Because at the end we care how the models can correctly and robustly make the classification.\n\nI think it would be better if the authors can have some discussion about the potential connections between the proposed method and the method in [1]. [1] proved that if a model can classify well under Gaussian noise, it is possible to turn it into a classifier that is certifiably robust (I don't think the proposed method is certifiable) to adversarial perturbations in l_2 ball. The training method in [1] is Gaussian data augmentation. An experimental comparison between the proposed method and the method in [1] is necessary. I am not sure how faster the proposed method can be. \n\n[1] Cohen, Jeremy M., Elan Rosenfeld, and J. Zico Kolter. 'Certified adversarial robustness via randomized smoothing.' ICML 2019\n\n---------------------------\nupdate after rebuttal:\nI appreciate the authors' feedback. However, I am not convinced that the proposed method is highly effective (Table 1 and Figure 3) and I still think the contribution is kind of marginal (especially given [1]). So it is hard to recommend acceptance.\n\n", "belong_id": "B1xDq2EFDH"}, {"uid": "H1gqLof79B", "paper_title": "Analytical Moment Regularizer for Training Robust Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a regularization method for achieving robustness to noisy inputs, with relatively less computation compared to standard data augmentation approaches. Specifically, the authors analyze the analytic expression of the loss on the noisy inputs, and using Jensens inequality, propose to minimize a surrogate loss over the expectation of noisy inputs. To minimize the loss over the expectation, the authors impose a regularization over the first moment of the network weights. The authors validate the model with the proposed regularization technique for its robustness against Gaussian attack and other types of attacks, whose results show that the model is robust. \n\nPros\n- The general idea of the regularization that replaces the generation of noisy samples and optimization over it is conceptually appealing and seems practically useful.\n- The derivation of the moment-based regularization makes sense. \n- The proposed regularizer seems to be effective to a certain degree, on the sets of experiments done by the authors.\n\nCons\n- Experimental validation seems highly inadequate due to lack of baselines. Thus it is difficult to assess the degree of robustness the proposed model achieves. The authors should perform extensive evaluation against state-of-the-art techniques against multiple types of attacks, in order to demonstrate the effectiveness of the proposed method.\n- While the authors emphasize the computational efficiency of the method, the authors do not report computational cost or actual runtime.  \n- The types of non-Gaussian attacks should be better described. Which ones use L-infinity attacks and which use L2 attacks?\n- Figure 3 doesnt seem like a very favorable result to the proposed model, since we are generally more concerned with adversarial examples generated with small perturbations, as large perturbations may change the input semantics.\n\nIn sum, while I like the overall idea and find the work novel and potentially practical, it is difficult to properly evaluate the work due to lack of comparison against state-of-the-art data augmentation methods for achieving robustness. Therefore I temporarily give this paper a weak reject, but may change the rating with more experimental results provided in the rebuttal. ", "belong_id": "B1xDq2EFDH"}, {"uid": "BJxNl7DdOH", "paper_title": "Ergodic Inference: Accelerate Convergence by Optimisation", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new combination of Markov chain Monte Carlo (MCMC) and variational inference (VI) for improving approximate inference. The main contribution is the optimization objective that allows improving the quality of samples obtained from the combination of VI and MCMC. Specifically, the authors minimize the 'approximate' version of the Kullback-Leibler (KL) divergence between the distribution of MCMC + VI and the true distribution. The authors validate the effectiveness of their formulation through experiments on 6 synthetic benchmarks and generative modeling of MNIST (experiments on Bayesian neural networks are also provided in the appendix). \n\nOverall, I think the paper provides a solid contribution towards combining MCMC and VI by proposing a way to optimize the MCMC part. The experiments validate the method by showing consistent improvement over existing methods. However, I believe the justification behind the proposed formulation, i.e., Equation (4) and (5), needs to be improved before being published at the conference.\n\nFirst, for Equation (4), the explanation behind 'replacing' H(P_{T}) with ELBO w.r.t. P_{0} is confusing. Specifically, it is reasoned that ELBO w.r.t. P_{t} only increase after MCMC steps. This statement is misleading since the replacement was done for H(P_{T}), not the ELBO w.r.t. P_{T}. \n\nI also think the Equation (5) is not properly justified. it is stated that the constraint is needed for preventing P_{T} to be closer to P_{0}. However, nothing is stated about the reason on why P_{T} gets closer to \\pi when Equation (5) is satisfied.  Note that even if the expected log-likelihood of the distribution is high, it does not necessarily mean that the distribution is more similar. \n\nMinor comments:\n- I was unable to understand why the algorithm is named 'ergodic' inference. Both HVI and the proposed EI rely on the ergodic property of Markov chain for improving the variational distribution. I hope the authors could better illustrate on this point. I also think the term 'ergodic approximation' in page 3. is hard to understand.\n- I (weakly) suggest changing y-axis of Figure 5. to log-scale for better readability. It almost seems that the brown plot does not converge in Fig 5-(a).\n- The paper could have been strengthened by performing experiments on more challenging datasets, e.g., CIFAR-10 or CIFAR-100. \n", "belong_id": "HkxZVlHYvH"}, {"uid": "Bkgsjnv_Yr", "paper_title": "Ergodic Inference: Accelerate Convergence by Optimisation", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The presented method is very useful to deep learning in the era of uncertainty modelling, which requires the use of Bayesian inference arguments. It's a valuable improvement upon variational inference, it's novel, and the derivations are correct. The presentation is elaborate and covers all expected aspects. The literature review is up to date. \nThe experimental results are diverse enough and convincing. The authors have considered both proof of concept experiments and deep learning architectures. The comparisons are valid. \n", "belong_id": "HkxZVlHYvH"}, {"uid": "HJlkXwkAtS", "paper_title": "Ergodic Inference: Accelerate Convergence by Optimisation", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper presents a new hybrid method to unify MCMC and VI. The key idea is to interpret a finite-length MCMC/HMC chain as a parametric procedure, whose parameters can be optimized via a VI-motivated objective. Specifically, the authors propose to modify the well-known ELBO (which is now non-trivial due to the intractable entropy) to form a new constrained and tractable objective. The presented techniques are tested on synthetic datasets and with the experiments of a VAE on MNIST. \n\nThe presented technique is interesting. However, there are several concerns of mine that should be addressed, as detailed below.\n\nThe notations of \\pi and \\pi^* are very confusing. I guess \\pi represents the marginal distribution of the last state of the MCMC chain, while \\pi^* is the target distribution. Is that right? Please clarify their meanings. \n\nThere are related works that combine MCMC and VI, such as [1]. What are the advantages of the proposed method compared to that method? \n[1] Francisco J. R. Ruiz and Michalis K. Titsias. A Contrastive Divergence for Combining Variational Inference and MCMC. International Conference on Machine Learning (ICML). 2019.\n\nIn equation 4, given fixed P_0 and a long enough MCMC chain, P_T will decorrelate with P_0. How to prevent P_T from collapsing to a delta function? Also intuitively, there should be a weight balancing the two terms of the loss; why a weight of 1 is used?\n\nIn equation 8, the function g_{phi} is not continuous because of the indicator function 1(). How do you back-propagate through that function? In the paragraph before Section 3.3, how would you defend the adopted stop-gradient trick?\n\nIn the paragraph before Figure 1, how to choose the hyperparameter h? It might not be suitable to set h as the entropy of the prior, as in practice prior and posterior might be different dramatically.\n", "belong_id": "HkxZVlHYvH"}, {"uid": "rJxxKMssdB", "paper_title": "MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper introduces a novel multi-source policy transfer problem, where we want to utilize policies from multiple source domains with different dynamics to improve the performance of the policy on our target dynamics. \n\nThe paper addresses the problem by adaptively aggregating the deterministic actions produced by source policies to maximize the expected return in the target environment. The method further trains an auxiliary network to predict a residual to revise the predicted action when some source policies are not useful or even adversarial.\n\n In my understanding, the paper assumes that the source and target domain shared the same task (reward structure) but only differs in dynamics. Also, the two domains share similar state and action spaces since the policy accepts the target states and predict actions in the target environment. This may limit the usage of the method.\n\nThe paper proposes to use residual learning as an auxiliary to compensate for the sub-optimal expressiveness of the source policies, which is novel and interesting.\n\nThe paper performs experiments on multiple environments. But the source and target domains only vary in some parameters of the agents. The domain gap seems small for these experiments. The paper needs a metric to measure the domain gap between source and target dynamics and report how the domain gap influences the proposed method and the baselines according to the metric.\n\nOne of my major concerns is that the limitation of the method with the same state and action space of the source and target domains. Also, there is no theoretical or intuitive analysis of how large the domain gap can be. This problem can be impractical for real-world applications with restrict limitations.\n\nPost-rebuttal:\n\nMy major concern is that the method is a naive combination of previous works and the paper is more like an engineering work. The method is also a weighted sum of source policies. There is no insight why the combination can work.\n\nNo assumption on source policies is given. That means I can get any random policy to learn a combination. This is like learning a policy from scratch by reinforcement learning. With better source policies, we can achieve better initialization for RL.\n\nThe paper is also related to hierarchical reinforcement learning, where the target reinforcement learning step is like building a high-level policy. \n\nThe work still requires lots of steps to train in the target domain, which does not fit to the real application of transfer RL. We hope transfer learning can adapt to the target environment fast.", "belong_id": "Byx9p2EtDH"}, {"uid": "rJxD1HJpYH", "paper_title": "MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a transfer reinforcement learning method that learns from existing source policies. The method aggregates deterministic actions produced by a collection of source policies to maximize expected return in the target environment. Unlike prior work it does not assume access to source environments nor source policy performance.\n\nThe method is intuitive and simple (simply a weighted sum over the actions of source policies). The paper is well-written in that it clearly explains the method and intuitions. The authors show results on a collection of different environments that include continuous and discrete action spaces. I appreciate the additional work put in to evaluate the distribution of performance. The method is well-ablated and addresses variants in which there is no reweighting and in which the residual is estimated independently of the state.\n\nI have some questions regarding the experiments:\n\n- In Table 1, do the authors have intuitions for why sometimes RPL is worse than MLP?\n- I'd like to see results comparing MULTIPOLAR with only bad sources with a randomly initialized policy\n- Given that source policies are needed for this to work, I'd like to see comparisons in which one continues to finetune an existing source policy. I know that the assumption here is that one does not have access to the internals of the source policies, but it would be nice to see how the performance compares.\n\nMy main concern has to do with the applicability of this method, since it seems to make strong assumptions on how different the domain dynamics are between source and target environments.", "belong_id": "Byx9p2EtDH"}, {"uid": "HkeRltpV5B", "paper_title": "MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper authors propose a method for transfer reinforcement learning (RL). Specifically they are claiming that RL agents can transfer knowledge between each other about the environment dynamics. In order to showcase their approach they have come up with a new transfer RL task that makes use of some source policies trained under a diverse set of environment dynamics. Their key contributions to solve the task involve a decision aggregation framework that is able to build on top of relevant policies while suppressing irrelevant ones and an auxiliary network that predicts the residuals around the aggregated actions.\n\nI recommend the paper to be accepted since they have an innovative contribution that pushes the needle on the transfer RL literature although I do not think the contribution is substantial. The set of experiments covers a wide range of different standard RL tasks and they provide enough evidence that the approach works. I find it interesting that they are able to extend the approach to the discrete action tasks. \n\nI would however recommend providing more experimental results that provides evidence that the target policy can recover the right policy when the target environment dynamics is the same as one of the source environments.", "belong_id": "Byx9p2EtDH"}, {"uid": "S1eJGUy6Yr", "paper_title": "Differentiable learning of numerical rules in knowledge graphs", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an interesting extension to the Neural LP framework for learning numerical rules in knowledge graphs. The proposed method can handle predicates involving the comparison of the numerical attribute values. The authors demonstrate its effectiveness on both synthetic knowledge graphs and the parts of existing knowledge graphs which consider numerical values.\n\nI recommend the paper to be rejected in its current form for the following 3 reasons:\n\n(1) Although the idea of making numerical rules differentiable is interesting, the current proposed method can only deal with one form of numerical predicate, which is numerical comparison. The limitation to such a special case makes the paper somewhat incremental. \n\n(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications. Although the proposed method does a good job in synthetic experiments, outperforming existing methods by a large margin, its performance on the numerical variants of Freebase/DBPedia dataset does not show consistent significant improvement. The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.\n\n(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks. A good place to start with is to visualize(print out) the learned numerical rules and see if they make any sense. The experiment section needs significant improvement, especially when there is space left.\n\n\nThe authors can consider improving the paper based on the above drawbacks. I encourage the authors to re-submit the paper once it's improved. \n", "belong_id": "rJleKgrKwS"}, {"uid": "BJekrJ9atS", "paper_title": "Differentiable learning of numerical rules in knowledge graphs", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed several extensions to the Neural LP work. Specifically, this paper addresses several limitations, including numerical variables, negations, etc. To efficiently compute these in the original Neural LP framework, this paper proposed several computation tricks to accelerate, as well as to save memory. Experiments on benchmark datasets show significant improvements over previous methods, especially in the case where numerical variables are required. \n\nI think overall the paper is written clearly, with good summarization of existing works. Also I like the simple but effective tricks for saving the computation and memory.\n\nOne main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general. For example, if rules contain quantifiers, how would this be extended? \n\nMinor comments:\n\n1) 4.1,  O(n^2/2) -- just put O(n^2) or simply write as n^2/2.\n2) How are the rules from in Eq (2)? i.e., how is \\beta_i selected for each i? In the extreme case it would be all the permutations.\n3) I would suggest a different name other than Neural-LP-N, as it is somewhat underselling this work. Also it makes Table 2 not that easy to read.\n", "belong_id": "rJleKgrKwS"}, {"uid": "SJx_7NveqH", "paper_title": "Differentiable learning of numerical rules in knowledge graphs", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an extension of NeuralLP that is able to learn a very restricted (in terms of expressiveness) set of logic rules involving numeric properties. The basic idea behind NeuralLP is quite simple: traversing relationships in a knowledge graph can be done by multiplicating adjacency matrices, and which rules hold and which ones don't can be discovered by learning an attention distribution over rules from data.\n\nThe idea is quite clever: relationships between numeric data properties of entities, such as age and heigh, can also be linked by relationships such as \\leq and \\geq, and those relations can be treated in the same way as standard knowledge graph relationship by the NeuralLP framework.\n\nA major drawback in applying this idea is that the corresponding relational matrix is expensive to both materialise, and use within the NeuralLP framework (where matrices are mostly sparse). To this end, authors make this process tractable by using dynamic programming and by defining such a matrix as a dynamic computation graph by means of the cumsum operator.  Furthermore, authors also introduce negated operators, also by defining the corresponding adjacency matrices by means of computation graphs.\n\nAuthors evaluate on several datasets - two real world and two synthetic - often showing more accurate results than the considered baselines.\n\n\nOne thing that puts me off is that, in Table 2, AnyBurl (the single one baseline authors considered other than the original NeuralLP) yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.\n\nAnother concern I have is that the expressiveness of the learned rules can be somehow limited, but this paper seems like a good star towards learning interpretable rules involving multiple modalities.\n\n\nMissing references - authors may want to consider citing https://arxiv.org/abs/1906.06187 as well in Sec. 2 - it seems very related to this work.\n", "belong_id": "rJleKgrKwS"}, {"uid": "rJxI645i_r", "paper_title": "FLAT MANIFOLD VAES", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Notes: \n\n  -This paper suggests the use of VAEs with stronger priors along with more powerful regularization of the decoder (especially its curvature).  This lowering of curvature is seen as 'flattening'.  \n\n  -Paper uses the normal VAE ELBO.  \n\n  -The normal prior over-regularizes the approximate posterior.  One proposal is to use a 'hierarchical prior': integral(p(z|zeta)*p(zeta), zeta) where zeta is a normal distribution.  So basically a function can transform the prior.  \n\n  -Importance weighting with q(z|x) has been proposed as a way to define a valid learning objective for this setting.  \n\n  -Another objective using lagrangian is called 'VHP-VAE'.  \n\n  -This paper extends VHP-VAE with jacobian regularization, which is approximated (the paper doesn't say so but I think it's a first order taylor expansion).  \n\n  -Paper also uses mixup in the latent space to provide regularization at points farther from the data.  \n\n  -With this mixup objective the mixing is also done to consider extrapolations in addition to interpolations.  \n\n  -The resulting latent space does indeed look much better (Figure 1).  \n\n  -The condition number is also way better (2a, 2b).  \n\n  -In figure 2, the background color indicates the degree of magnification (so the VAE-VHP has greater variability in distances?)  I found this figure a bit hard to itnerpret.  \n\nComments: \n\n  -This paper cites Mixup but there are two more papers to consider here: Manifold Mixup (ICML 2019) and Adversarial Mixup Resynthesis (Neurips 2019) which both considered mixing in a latent space.  AMR considered in an autoencoder, and Manifold Mixup is also relevant because its theoretical analysis explicitly considers flattening although in a somewhat difference sense (and both are different from what's done here).  \n\n  -The object tracking experiments don't seem very convincing to me (just looking at table 2 at least).  \n\nReview: \n\n  This paper considers augmenting the hierarchical VHP-VAE with a criteria in which the jacobian is approximately regularized at interpolations and extrapolations between different points in z space.  The experiments suggest this is an important problem with VHP-VAE and also that it's successfully addressed.  ", "belong_id": "SkgWIxSFvr"}, {"uid": "HJlXO5WwYr", "paper_title": "FLAT MANIFOLD VAES", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summary of paper:\nThe paper is concerned with the geometry of latent spaces in VAEs. In particular, it is argued that since geodesics (shortest paths) in the Riemannian interpretation of latent spaces are expensive to compute, then it might be beneficial to regularize the decoder (generator) to be flat, such that geodesics are straight lines. One such regularization is proposed.\n\nReview:\nI have several concerns with the paper:\n\n1) Geodesics are never motivated:\nThe paper provides no motivation for why geodesics are interesting objects in the first place, so it is not clear to me what the authors are even trying to approximate.\n\n2) Under the usual motivation, the work is flawed:\nThe usual motivation for geodesics is that they should follow the trend of the data (e.g. go through regions of high density). Since no other motivation is provided, I will assume this to be the motivation of the paper as well. The paper propose to use a flexible prior and then approximate geodesics by straight lines. Beyond the most simple linear models, then this cannot work. If the prior is flexible, then straight lines will hardly ever constitute paths through regions of high density. The core idea of the work, thus, seem to be in conflict with itself.\n\n3) A substantial bias is ignored:\nThe paper consider the Riemannian metric associated with the *mean* decoder. Due to regularization, holes in the data manifold will be smoothly interpolated by the mean decoder, such that geodesics under the associated metric will systematically be attracted to holes in the data manifold. Hauberg discuss this issue in great length here:\n\n  https://arxiv.org/abs/1806.04994\n\nHere it is also demonstrated that geodesics under the mean decoder tend to be straight lines (which is also what the authors observe). Taking the stochasticity of the VAE decoder into account drastically change the behavior of geodesics to naturally follow the trend of the data.\n\n4) Related work is mischaracterized:\nPrevious work on the geometry of latent spaces largely fall into two categories: those that treat the decoder as deterministic and those that treat it as being stochastic. In the cited papers Arvanitidis et al and Tosi et al consider stochastic decoders, while the other consider deterministic decoders. Given that geodesics have significantly different behavior in the two cases, it is odd that the difference is never discussed in the paper.\n\n5) It is not clear to me what the experiments actually show:\n\n-- I did not understand the sentence (page 5): 'The model is more invariant if the condition number is smaller...' What does it mean to be 'more invariant' ? And how is invariance (to what) related to the condition number of the metric?\n\n-- Figure 3 show example geodesics, but only geodesics going between clusters (I have no idea how such geodesics should look). If I look at the yellow cluster of Fig3a, then it seems clear  to me that geodesics really should be circular arcs, yet this is being approximated with straight lines. Are the ground truth geodesics circular? At the end, it seems like the shown examples are the least informative ones, and that intra-cluster geodesics would carry much more meaning.\n\n-- What am I supposed to learn from the 'Smoothness' experiment (page 7) ? My only take-away is currently that the proposed regularization does what it is asked to do. It is not clear to me if what it aims to do is desirable? Does the experiment shed light on the desirability of the regularizer or is it more of a 'unit test' that show that the regularizer is correctly implemented?\n\n-- In the 'Geodesic' experiment (page 7) I don't agree with the choice of baseline. If I understand correctly, the baseline approximate geodesics with shortest paths over the neighbor graph (akin to Isomap). However, there is no reason to believe that the resulting paths bare any resemblance to geodesics under the studied Riemannian metric. The above-mentioned paper by Hauberg provide significant evidence that these baseline geodesics are not at all related to the actual geodesics of the studied metric. The only sensible baseline I can think of is the expensive optimization-based geodesics.\n\n== rebuttal ==\nI have read the rebuttal and discussed with the authors, and I retain my original score.", "belong_id": "SkgWIxSFvr"}, {"uid": "SJeN-ZNl5r", "paper_title": "FLAT MANIFOLD VAES", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1.\tThe idea of explicitly forcing the encoding space to be flat by putting constraint on metric tensor is simple but neat.\n2.\tThe use of Jacobi regularization in Eq. (9) is effective but the choice of using interpolation to extend this in the entire decoding space is kind of adhoc. Can authors please justify?\n3.\tNot sure how authors put the Lipschitz continuity constraint on f. Please explain. \n4.\tThe title of Flat manifold VAEs is misleading as it potentially means VAEs for flat manifold \n5.\tI wonder what will happen if you put an unfolding constraint in the encoding space like LLE, ISOMAP etc.. The loss function is data driven so this should give atleast similar behavior.\n6.\tOverall I like the experimental setup, but the tracking experiment is kind of distracting. The authors may want to remove this experiment.\n7.\tIn Fig. 7, the authors have shown with and without  Jacobi normalization which I am really not convinced with, need better explanation.  \n", "belong_id": "SkgWIxSFvr"}, {"uid": "HkxJSbehYS", "paper_title": "Out-of-Distribution Detection Using Layerwise Uncertainty in Deep Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper tackles out-of-distribution samples detection via training VAE-like networks. The key idea is to inject learnable Gaussian noise to each layer across the network in the hope that the variance of the noise correlates well with the uncertainty of the input features. The network is trained to minimize the empirical loss subject to noise perturbation. The paper is well written, and the background is introduced clearly.\n \nAs I understand it, the goal of *out-of-distribution sample detection* is to train a deep network that simultaneously generalizes well and also be discriminative to outliers. However, its not clear to me why the proposed method server this purpose; empirical results are not convincing either. My major concerns are as follows:\n \nFirst of all, from my intuition, it would be much easier to train deterministic networks than their counterparts with randomness. Empirically, researchers also often observe near-zero training loss for large deterministic networks such as Dense-BC trained on simple CIFAR/SVHN datasets. Especially, in this case, the training goal is simply to map higher-dimensional inputs to lower-dimensional classification categories. That being said, one would expect the variances go to zero at convergence to achieve lower empirical loss in the case of no additional diversity (or uncertainty) promotion terms. \n \nIt is not clear to me how to avoid degenerate solutions at convergence \nwhile maintaining good testing performance with the proposed training strategy. \nFrom the empirical results, it also appears that all models reported might not be fully optimized? \nThe baseline results are significantly worse than those reported in previous work.  \nSpecifically, \nin table 1, the testing accuracy of Dense-BC trained on CIFAR-100 is only 71.6.\nIn table 2, the reported testing accuracy on CIFAR-10 using Dense-BC is 92.4.\n \nHowever, the results of DenseNet-BC (k=12, L=100, table 2) reported in the original paper are:\nCIFAR10  94.0  (also leave 5K examples as validation set)\nCIFAR100 75.9\n  \nMeanwhile, the reported accuracy of WRN-40-4 trained on CIFAR-10 and CIFAR-100 are 89.6 and 66.0, respectively. However, the corresponding baseline numbers in the original WRN paper are much higher,  \nCIFAR-10  95.03\nCIFAR-100 77.11 \n\nCould the authors comment on that?\n\nReferences:\nGao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger.\nDensely Connected Convolutional Networks\nhttps://arxiv.org/abs/1608.06993\n\nSergey Zagoruyko, Nikos Komodakis. \nWide Residual Networks.  \nhttps://arxiv.org/pdf/1605.07146.pdf\n", "belong_id": "rklVOnNtwH"}, {"uid": "r1gOgic6FS", "paper_title": "Out-of-Distribution Detection Using Layerwise Uncertainty in Deep Neural Networks", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "** post rebuttal start **\n\nAfter reading reviews and authors' response, I decided not to change my score.\nHowever, I feel that this paper is somewhat under-evaluated initially, so I hope the authors have an opportunity in another venue with their revision.\n\n\nDetailed comments:\n\n1.1. I recommend to add an algorithm box describing the learning scheme. It is not end-to-end learning, so it is hard to catch (and potentially, replicate) the learning part. I am also a bit skeptical about the convergence (with non-zero \\sigma), as Reviewer 2 has a concern about it.\n\n1.3. 'We hypothesized that the value of the uncertainty is different depending on whether the inputs are OOD or in-distribution inputs. The results of the ablation study listed in Table1 demonstrate that this hypothesis is true.'\n2. 'In order to use the data uncertainty, we used the value of \\sigma.'\n-> Table 1 proves that your proposal (playing with \\sigma) is effective, but it does not mean that \\sigma is the uncertainty which is only essential component for detecting OOD. I recommend the authors to validate their hypothesis, maybe by conducting more experiments to show that the role of \\mu and \\sigma is as expected. At least, if \\mu is proven to have no effect on OOD detection by some experiment, then it can be a clue.\n\n\nMinor comment: I hope ICLR papers are cited as ICLR papers at least in ICLR submissions, not arXiv preprint.. Alemi's paper is ICLR'17 paper, for example.\n\n** post rebuttal end **\n\n\n\n- Summary: This paper proposes to train an OOD detection model from a portion of modified latent vectors; more specifically, similar to VAE, they assume unimodal Gaussian distributed latent space at each layer and use the collection of standard deviation to train an OOD detector. Experimental results on several OOD benchmarks with different backbone networks show that their method outperforms ODIN (Liang et al., 2017).\n\n\n- Decision and supporting arguments:\nWeak reject.\n\n1. Though the idea of extracting uncertainty is interesting, but I think the motivation and explanation is not enough, so I couldn't find a rationale why we should do this. I have several questions that I couldn't find an answer in the submission, could you answer them?\n1.1. Are the classification loss and OOD detection loss optimized jointly?\n1.2. Is it reasonable to assume unimodal Gaussian distribution over all latent spaces without a carefully designed learning objective? More specifically, to make it learnable, don't you need a learning objective other than the conventional cross-entropy loss, e.g., 'Bayes by backprop' proposed in the early work (Blundell, 2015)?\n1.3. Why only the standard deviation values are useful for the OOD detection performance? If they are really useful, how the standard deviation values are related to the OOD detection performance?\n\nBlundell et al. Weight Uncertainty in Neural Networks. In ICML, 2015.\n\n2. More ablation study is required to verify the effectiveness of their method. Again, I am not sure why \\mu and \\sigma should be split, and why \\mu should be discarded for the OOD detection part.\n\n3. The architecture design of CNN in Figure 7 also looks arbitrary.\n\n4. Comparison with more state-of-the-art methods is required. ODIN (Liang et al., 2017) is a powerful method but it is somewhat old and many recent works actually combine their method with ODIN for better performance. Why don't you compare the proposed method with the Mahalanobis distance-based classifier (Lee et al., 2018)? They also estimate the uncertainty by measuring the Mahalanobis distance on the feature spaces & combine them for better OOD detection.\n\n\n- Comments:\n1. I couldn't find any statement about the classification accuracy, does the proposed model have a good classification performance as well? Since {a half of the model capacity is spent to split \\mu and \\sigma} and {it should take account of uncertainty in the forward pass}, I am not sure it maintains a good classification performance, compared to the standard classification model with the same capacity.", "belong_id": "rklVOnNtwH"}, {"uid": "HJg4ZAZ0tH", "paper_title": "Out-of-Distribution Detection Using Layerwise Uncertainty in Deep Neural Networks", "experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper considers the problem of out-of-distribution (OOD) sample detection while solving a classification task. The authors tackle the problem of OOD detection with exploiting uncertainty while passing a test sample through the neural network. They treat outputs of (some) layers in a NN as random Gaussian-distributed variables and measure uncertainty as variance of these Gaussians. Then when uncertainty is high, OOD is detected.\n\nThe overall idea behind the paper could be interesting, but its realisation in the current form is questionable. \n\nThe paper seems totally misusing the reparametrisation trick and stochastic outputs of layers in NNs. Eq. (2) is not the objective of variational inference that seems to be required for stochastic outputs and the reparametrisation trick as presented before the equation. The objective misses the KL-divergence term! Without it what would stop a neural net to set sigmas to 0 and forget about the stochasticity altogether? Not to mention that the current objective is not mathematically justified.\nIf there is no mix and error in eq. (2) and the networks were trained using this loss (and based on provided code they were using this loss), my wild guess of explaining why this may give best results in the experiments is that the models were trained for surprisingly small number of epochs. Therefore, a hypothesis would be that this small number of epochs did not allow the networks to switch sigmas to 0. \n\nUntil the authors can clarify and justify the objective, I will vote for rejection only based on this ground.\n\nHowever, there are other issues in the paper as well. First of all, its clarity. It seems that the paper requires a lot of polishing.  The first paragraph of this review is based on my assumptions from the paper since I am not completely sure I understand it correctly. More about the clarity issues below\n\nFor strong evaluation, comparison with Malinin & Gales (2018) work seems to be important since it was the only work also using uncertainties for OOD detection in related work. Also related work section does not look like an exhaustive overview.\n\nSome of the detailed comments:\n1.\tIn other words, in-distribution samples possess more features that convolutional filters react to than OOD samples  first of all, this sentence is not easy to parse. Secondly, it is unclear, why this should be true. If OOD samples are still natural images, they would contain edges just the same as in-distribution samples. That is the power of deep learning enabling transfer learning, that low-level features are the transferable across different data and tasks. Therefore, the claim that Therefore, the uncertainties of the features will be larger when the inputs are in-distribution samples requires more elaboration and arguments\n2.\tThe arguments of the next paragraph regarding uncertainty of deeper layers should be larger for OOD samples are not very convincing either.  It is either requires a definition what the authors mean here as uncertainty, or it is not necessarily true that absence of fixed regions for embeddings leads to higher uncertainty. \n3.\t3rd and 4th paragraphs in Introduction have too many repetitions of phrases between each other. Compare, e.g. the first sentences of the paragraphs or the last sentences. \n4.\tOne cause of the abovementioned problem is that their approaches and similarly the next paragraph: their approaches stylistically sound wrong. It is appropriate in the previous paragraph since there is a link to previous studies. It seems that these approaches or the existing approaches would be a better choice for this and the next paragraph.\n5.\tEach uncertainty is easily estimated after training the discriminative model by computing the mean and the variance of their features using a reparameterization trick  conventional discriminative models do not estimate mean and variances of the features. The issue of estimating uncertainty is addressed by several special methods such as Bayesian variational methods used in the referred papers. Therefore, in order to use a reparametrisation trick one need to firstly choose a special class of models, which is not obvious from the text.  \n6.\tMoreover, UFEL is robust to hyperparameters such as the number of in-distribution classes and the validation dataset.  the size of the validation dataset? In any case neither size of the validation dataset nor the validation dataset itself are not hyperparameters (should not be hyperparameters for out-of-distribution detection). The number of classes can hardly be called a hyperparameter also. \n7.\tdepends on the difference in the Dirichlet distribution of the categorical parameter <...> In our work, the distribution of the logit of the categorical parameters  what is/are this/these categorical parameter(s)?\n8.\tFurther, they estimate the parameter of the Dirichlet distribution using a DNN and train the model with in-distribution and OOD datasets  this sentence may mislead to impression that the proposed method does not need OOD dataset for training, which does not seem to be the case, since \\lambda and \\theta are trained based on OOD samples\n9.\tbecause they will not be relevant to the classification accuracy  who are they?\n10.\tand \\epsilon is the Gaussian noise > the standard Gaussian noise\n11.\twhere z^0 = x  it seems this should be placed somewhere earlier when z^l is introduced since z^0 is not used in eq.(2) after which this text is placed\n12.\tIt is unclear how \\lambda^l and CNN \\theta are learnt\n13.\tIt is unclear how the values of features d(x) are used to detect OOD samples\n14.\tcomparison methods, and models  not clear what models mean here\n15.\tMissing references to datasets in the main text. At least reference to Appendix A.2 is required\n16.\tWe used 5,000 validation images split from each training dataset and chose the parameter that can obtain  which parameter? \n17.\tAll the hyperparameters of ODIN  a reader does not know yet that ODIN is used for comparison\n18.\twhich consists of 100 OOD images from the test dataset and 1,000 images from the in-distribution validation set  it is a bit confusing to call OOD dataset as a test dataset in this context\n19.\tWe tuned the parameters of the CNN in Equation 4 using 50 validation training images taken from the 100 validation images. The best parameters were chosen by validating the performance using the rest of 50 validation images.  this is confusing. What parameters do the authors talk about in the second sentence if not the parameters of the CNN?\n20.\tWe used TNR at 95% TPR, AUROC, AUPR, and accuracy (ACC),  Some elaboration is required, at least the reference to Appendix A.1. What is the changing threshold for AUROC and AUPR? Why AUPR-In and AUPR-Out are considered and only a single AUROC is considered. What is the positive class for AUROC?\n23.\tFor LeNet5, we increased the number of channels of the original LeNet5 to improve accuracy  do the authors mean that they allowed RGB images as input rather than greyscale? If yes, this explicit explanation would be preferable \n24.\tWe inserted the reparameterization trick  not the best word choice. Reparametrisation trick is a computational/implementation trick/method and it is hard to say that it can be inserted into a network. I believe what the authors mean is that they inserted mean/std outputs instead of point outputs. Conceptually, this means that the output of the corresponding layers is considered to be stochastic rather than deterministic. \nAlso, it is unclear when the authors say they insert it to the softmax layer. According to Section 3 the softmax layer is never considered to output means and stds.\n25.\tThe numbers of epochs for training NNs are very small for LeNet and WideResNet in the experiments. Did the models manage to converge during this short training?\n\nMinor:\n1.\tThese data were also used -> this data", "belong_id": "rklVOnNtwH"}, {"uid": "Byxg4B70KB", "paper_title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a new pruning technique that utilizes the statistical dependency between the corresponding nodes and outputs. The dependency is measured by a kernel based dependency measure which is closely related to MMD. The test statistics derived from the dependency measure have an asymptotic distribution which can be written as a weighted sum of chi-square random variables. The proposed method is numerically investigated using some datasets such as MNIST and CIFAR 10.\n\nPruning is one of important problems for practical deep learning operations. This paper gives an interesting idea for the pruning techniques. I think the idea is novel.\n\nOn the other hand, I also have the following concerns:\n- Although applying the kernel type information measure is an interesting idea, its computational complexity would be large. It is not obvious that it works for large datasets such as ImageNet even if the sub-sampling technique is applied.\n- The numerical experiments are conducted in small datasets. How it works in larger datasets such as ImageNet and (more importantly) how it is compared with SOTA pruning methods in more difficult datasets. Actually, performance comparison is not done for the CIRAR10-VGG16 setting.\n- The asymptotic distribution can be seen as a corollary of existing researches for MMD and HSIC.\n\nFor these reasons, I was not completely convinced with the effectiveness of the proposed method.\n\n\nMinor comment:\n- The test statistics is more like HSIC. It would be nice if there were comments on the connection to HSIC.\n\n===\nUpdate: The computational cost for this method seems not so much demanding, and could be applied to large data-set. The  resultant performance also seems useful. I think more convincing comparisons are needed. However, its idea seems interesting and its practicality is ensured to some extent. Thus, I have raised  my score.", "belong_id": "HJgCF0VFwr"}, {"uid": "ByefJP6lqH", "paper_title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new way of evaluating the importance of neural connections which can be used for better model compression. The approach uses a non-parametric statistical test to detect the three way interaction among the two nodes and final output. Some small scale experiments show that the approach achieves better compression rate given the same test error.\n\nThe approach seems interesting in the sense that, unlike existing techniques, it explicitly measures the three way interaction among the two nodes and the output. Also, it removes the average effects and only considers (non-linear) correlation after removing the mean. The explicit link to the final output and removing average effects allows the method to remove more weights without decreasing the loss by much.\n\nHowever, the drawback of the approach is the significantly increased computation due to 1) quadratic complexity for the kernel methods; 2) unable to cache computation among different pairs of nodes (for example, gradient-based approach can compute importance for all node connections in one forward-backward pass). This limits the applicability of the approach to more interesting cases of larger models (for example, models that work on ImageNet) where model compression is of more urgent need. As a result, the significance and impact of the approach is also limited.\n\nThe experiment results are interesting in that it shows the proposed approach can achieve better compression rates given the same test error tolerance on a few small datasets. However, as mentioned above, these experiments are less convincing than more complicated models on larger datasets, such as ImageNet where model compression has greater impact. In addition, the test errors on smaller datasets are easier to achieve, sometimes tuning the optimization settings such as learning rates can result in significant improvement. Therefore, such results are more like preliminary.\n\nThe paper is general clear and well written. The experiment section should include more important information such as what kernel bandwidth is used for Gaussian kernel, which greatly affects performance. Also, the main text introducing the proposed statistical test is a bit verbose, and dense in unnecessary notations. I think it can be made more succinct by presenting a high level idea (removing mean, three way correlations) first and then the final results. Some intermediate results can be put into Appendix.", "belong_id": "HJgCF0VFwr"}, {"uid": "BJlTymjYcH", "paper_title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a nonparametric score test to estimate the importance of network connections on the final output of Deep Neural Networks. They derive this by modeling the log-transformed joint density of each connection and final output in a tensor product reproducing kernel Hilbert space. They finally derive an asymptotic distribution of the proposed test statistics which only depends on the eigenvalues of the kernel. This importance test is applied to ranking the importance of each connection in Multilayer Perceptron Networks, and Convolutional Networks and sparsifying the networks by removing the least important connections. The method is applied post-training to fully trained networks but evaluated by retraining the sparsified network from scratch. The method is demonstrated in experiments on compressing three networks. \n\nI believe this paper is a borderline accept. It provides a more statistically principled method to examine the importance of connections in a Neural Network and rank them compared to existing compression methods. Due to this ranking, there is a clear method to easily adjust the target compression rate. They are able to achieve high lossless compression rates. However, the benefits shown in the empirical results could be more convincing. It lacks baselines on more complex networks and could benefit from more empirical analysis of the theoretical benefits and properties of this approach.\n\nPros: \nThe method is able to maintain accuracy while achieving high compression rates on MNIST and CIFAR10. It does better than the baselines compared to the small networks. They show a capability for increasing generalizability by decreasing error rates. \n\nIt provides a well derived statistically principled method to examine the importance of connections and rank them.\n\nIndicates an additional use to visualize the importance of features.\n\nCons:\nThe empirical results could be clearer. It lacks baselines for larger models on Cifar10. Could you compare it with the published results of other algorithms? How does it do on larger networks like Imagenet?\nComputational efficiency is mentioned but could be examined in greater detail.  How long does it take to run and how is that affected by model size?\n\nThe experimental setting is somewhat unclear. The baseline Louizos et al. (2017)  was designed to optimize group sparsity/speed, but the experimental results here only examine the compression rate. Was the baseline run to optimize speed or sparsity?\n\nIt would be interesting to examine the correlation of importance score with actual impact on network performance. This might be done with a comparison with random pruning or pruning higher importance connections. It might be useful to examine the performance of the networks after pruning nodes of differing importance without full retraining or just fine-tuning. It is unclear how important full retraining is in this method.\n\nIt would be interesting to visualize the importance of features at different depths in the deep convolutional networks.\n\nMinor suggestions: In the introduction, you mention l0 and l1 norm methods, but cite Han et al. (2015) which compares l1 and l2 norm and found l2 norm to be better overall.\n\ntypos:\nIn Abstract: nonparemtric scoring test  -> nonparametric scoring test  \nIn 4.4: sample averarge -> sample average\nIn 4.5: ASYMPTOTICALLY DISTRIBUTION  ->   ASYMPTOTIC DISTRIBUTION ", "belong_id": "HJgCF0VFwr"}, {"uid": "rkxcWM73FH", "paper_title": "Do Image Classifiers Generalize Across Time?", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents new datasets based on ImageNet and Youtube-BB to assert networks performance consistency across time. Compared to previous work, it uses human labeler to further validate the dataset and discard frames that are deemed too different from the reference one. It provides results on image classification and detection using popular network architectures. Based on these results, the paper claims an accuracy drop of 10 to 16%.\n\nThe main contribution of this paper is to introduce a new, human annotated dataset for robustness assessment of image classifiers. In itself, it is valuable work, but it is not clear if the contribution is important enough for ICLR. However, I would still be ok with accepting the paper (better datasets are always useful) if it was not for the way the results are reported. I do not specifically have issues with 'more stringent robustness metric' but it should not be used to claim incredible results (like an accuracy drop of 10 to 16% instead of 3% for previous work (Real et al. 2017)).\n\nThere is one thing for sure: using 'accuracy drop' in this context is just misleading. The underlying concept to which 'accuracy' refers is _not_ the 'maximum error made by the network over the whole set of images'. By this definition of accuracy, if the number of images around the reference frame were 100, missing a _single one_ each time (that is 99% of actual accuracy) would result, according to this peculiar redefinition, to a _0%_ accuracy. This is actually highlighted in Appendix G.1: the 'accuracy' trend can only go down, since every supplemental frame brings one more chance to fail and obtain a 0% accuracy for this set of perturbed images.\nSame thing goes for the detection, where the only frame that matters among all is the one _minimizing_ the AP. Same thing in Table 4, where the 'accuracy' of the Original column means one thing (the amount of correctly identified images over the total number of images) while the 'accuracy' of the Perturbed column right next to it means something completely different. Same thing in Table 2, which even provides a 'delta' between two unrelated metrics.\nI cannot see how this can be justified. Sure, there could be some usage for such strict metric, but again, this is _not_ accuracy and cannot be compared to any previous results. Having a more stringent metric is one thing, but in this case it just seems like a justification to get high drop numbers.\n\nKeeping that in mind, these are the actual conclusions we can make from the paper:\n1) Human reviewers removed or changed about 20% of the frames\n2) This resulted in a relative accuracy improvement of about 4% for the reference frame (Table 4, column Original). The improvement for the perturbed frames are not actually provided.\n3) The comparison (and improvements) to previous work due to the dataset cleaning remains unclear.\n4) Comparison between different networks and training procedures\n\nOverall, the paper presents impressive numbers but does not actually back them up. I am open to eventually consider acceptance given the value of the datasets, but the paper would then require a significant overhaul to remove all confusing aspects.", "belong_id": "Syx9ET4YPB"}, {"uid": "Skx8zfYntr", "paper_title": "Do Image Classifiers Generalize Across Time?", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper targets on the evaluation of model robustness on similar video frames. The authors build two carefully labeled video datasets, and extensive experiments are conducted to show that the state-of-the-art classification and detection models are not robust enough when dealing with very similar video frames.  The results are similar to my intuitive feelings.\n\nThe authors propose acc_orig (the average acc) and acc_pmk (which chooses the worse one in nearest 2k frames) to amplify the gap. I personally think acc_pmk is too stringent. I wonder if there is still large gaps if we choose a random frame in the nearest 2k frames.\n\nThe authors have tried fine-tuning and data augmentation techniques to improve the robustness, although the performance is improved, the gap between acc_orig and acc_pmk does not change much.\n\nThe paper has done many work to analyze the robustness of image classification and detection models, however, the results are expected and no effective methods are proposed to improve the results. Overall, the contribution is limited. ", "belong_id": "Syx9ET4YPB"}, {"uid": "S1eNC80O5B", "paper_title": "Do Image Classifiers Generalize Across Time?", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\nIn this paper, the authors curated two datasets: ImageNet-Vid and Youtube-BB in order to create human-reviewed perceptibly similar sets (Imagenet-Vid-Robust and YTBB-Robust). The obtained datasets are evaluated over 45 different models pre-trained on ImageNet in order to see their drop in accuracy on natural perturbations. Three detection models are also evaluated and show that not only classification models are sensitive to these perturbations, but that it also yields to localization errors.\n\nComments\nThe paper is clear, well organized, well written and easy to follow.\nThe authors present two novel datasets grouped in sets of perceptibly similar images and answer to the following hypothesis: Can the perturbations occurring naturally in videos be a realistic robustness challenge?\nThe thorough evaluation over the curated datasets shows pretty well that the changes in the model prediction are indeed due to a lack of robustness of the models themselves rather than the difference occurring from one frame to the other (occlusion etc).\nThe authors mention the curation was done with the help of expert human annotators. Details could be added as to how the annotators are considered experts and what process they went through (mturk? Handmade application to select the frames?).\nOverall I think the paper adds an interesting contribution, with the datasets themselves which can be used for image similarity tasks for example\nAlthough the contribution of the paper is important, it seems limited for the conference with no novel method proposed. \n\nTypo\nSection 3, l 4: using use -> using\n", "belong_id": "Syx9ET4YPB"}, {"uid": "r1g2yTe2Yr", "paper_title": "FINBERT:  FINANCIAL SENTIMENT ANALYSIS   WITH PRE-TRAINED LANGUAGE MODELS", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a domain adaptation type of task via proposing fine-tuning of pre-trained models such as BERT on data from financial domains. The paper starts off with a good motivation about requiring some kind of domain adaptation particularly when performing tasks such as sentiment analysis on data sets from the financial domain. However, there is not much novelty in this paper.\n\n1)The authors do not propose any new model architectures. Even if we were to argue the novelty is in terms of their empirical work, there are some flaws/missing details in the experiments.\n2)In table 1 authors present agreement amongst annotators, it would be nice if in addition to mentioning the source of the data, the authors included what metric was used to attain agreement. I had to read the original paper releasing the data set to figure this out.\n3)Table 4 presents results that do not seem significant. It is hard to conclude if a certain pre-training strategy worked for sure.\n\nOn the whole I am very lukewarm on this paper. I find this paper lacking in novelty. Seems like an ambitious class project turned into an ICLR submission.", "belong_id": "HylznxrYDr"}, {"uid": "HkeWvWtCtr", "paper_title": "FINBERT:  FINANCIAL SENTIMENT ANALYSIS   WITH PRE-TRAINED LANGUAGE MODELS", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2529", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an analysis of the BERT language model on financial text. FinBERT is evaluated on two datasets from the financial domain: a sentiment prediction dataset (classification with 3 different classes) and a sentiment score prediction (the score is a float number between -1 and 1). \n\nI find the phrasing 'FinBERT is a language model based on BERT' misleading; I think FinBERT is BERT trained on financial text. There is no modification that is done to the original BERT model.\n\nThe paper presents several experiments using BERT as the language model and fine-tuning for the financial tasks. FinBERT is compared to a few baselines such as LSTMs with ElMO embeddings and ULMfit. I find interesting that the model performs better on the subset of the dataset for which there is perfect agreement between the annotators.\n\nI also find the results on training on financial data interesting. The results seem to indicate that further training on financial text does not seem to result in additional improvement when compared to original BERT.\n\nWhile I find the analysis and the experiments presented in the paper interesting, the novelty of the paper is rather low. There is no new idea introduced in this paper, it contains a series of experiments with BERT on financial text and tasks.\n\n", "belong_id": "HylznxrYDr"}, {"uid": "H1xnFZHVqH", "paper_title": "FINBERT:  FINANCIAL SENTIMENT ANALYSIS   WITH PRE-TRAINED LANGUAGE MODELS", "experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a method for financial sentiment analysis based on the texts obtained from news. The method is based on an existing method BERT (Devlin et al. 2018).  The authors have performed thorough experimental studies of the BERT method on an existing dataset TRC2-financial, a subset of TRC2 consisting of 1.8M news articles. Although the results may be of interest to communities working in this area, there are no or little novel contributions. By reading section 2 (only one page), which describes the method used, I have the impression that the authors took the method BERT and then applied this to the TRC2-financial dataset and then reported the results and also discussed some parameter choices in the BERT method. Therefore, the only value about this paper is the experimental results. Apart from this, there are no other contributions or insights to the methods/problems. In addition, section 2 is over-brief and very unclear, and it only contains a brief summary of the BERT method. For these reasons, I think the paper should be rejected for lacking novelty and writing quality. ", "belong_id": "HylznxrYDr"}, {"uid": "r1gkpwkhcH", "paper_title": "FINBERT:  FINANCIAL SENTIMENT ANALYSIS   WITH PRE-TRAINED LANGUAGE MODELS", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper described the application of BERT in the field of financial sentiment analysis. Authors find that when fine-tuned with in-domain data, BERT outperforms the state-of-the-art, demonstrating that language model pre-training can transfer knowledge learned from unsupervised large corpus to new domain with minimum effort. Experiments are conducted to explore 1) the utility of different in-domain dataset for further pre-training; 2) strategies to avoid catastrophic forgetting, and 3) effectiveness of fine-tuning a subset of the full model. \n\nI am in favor of rejecting this paper and my reasons are as follows:\n\nFirst, this paper may lack deeper innovation, although it demonstrates a good application of the BERT models in financial domain. For example, the framework of general-domain LM pretraining, to in-domain LM pretraining and finally in-domain classifier fine-tuning, as well as techniques of catastrophic forgetting were already proposed in Howard & Ruder 2018. Therefore, I think this paper may be more suitable for other (finance) application-oriented venues.\n\nSecond, the dataset used in evaluation is of small size (for example, Financial PhraseBank test set has one 1K). Thus, even though the paper is about transfer learning to domains without large data, I find it might be more convincing to draw a solid conclusion with a larger test set.\n\nThis paper is well organized and easy to follow. It may be beneficial to clarify in a few places (if space permits):\n1) Some description or statistics of the data may be helpful (e.g., average sentence length or some examples);\n2) Citations to Elmo and ULMFit can be made more explicit. Authors did cite Peters 2018 and Howard 2018 at the beginning of the paper, but may want to explicitly associate them with Elmo and ULMFit when these two terms first occur respectively;\n3) For table 2, does the all data or data with 100% agreement include training data (80%) or just the test data (20%)?\nThe difference between FinBERT(-domain) and ULMFit can be explicitly contrasted in the paper. Is the former initialized with BERT while latter with ULMFit?\n", "belong_id": "HylznxrYDr"}, {"uid": "HJxR4YcnYS", "paper_title": "Learning to Coordinate Manipulation Skills via Skill Behavior Diversification", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper aims to achieve multi-agent coordination by composing diverse skills learned by augmenting individual subtask objectives with DIAYN-style diversity bonuses. Once individual diverse skills are learned for the subtasks, the agents are combined by a meta-agent to coordinate multiple distinct robots to achieve a shared goal.\n\nThis is a good application of low-level skill learning to multi-agent coordination. I have settled on a weak acceptance, because the approach is simple and seems scalable, but the acceptance is weak because the method relies on specifying the subtasks in advance.\n\nThe approach is well-motivated in that learning individual skills in isolation is generally more tractable than learning their combined application from scratch, and the building blocks of this system are well-chosen. The results demonstrate the importance of the diversity objective, and find a good sweet spot for the diversity weight.\n\nI do have some criticisms, related primarily to the decision to pre-train with both a continuously-parameterized diversity conditioning as well as a discrete set of concrete subtasks. Because these subtasks must be specified in advance, this limits the wide applicability of the resulting approach to those that can be broken down a-priori into components. Did the authors consider using the DIAYN objective on its own to encourage sufficiently diverse behaviors? If this didn't work, would perhaps a larger latent skill vector, or a large discrete set of DIAYN skills, have made it work?\n\nI also don't see the size of the latent skill embedding reported anywhere. How big is this vector; that information should be added to the paper.\n\nHowever, the approach is generally good. I think the paper would be improved if it included a strong baseline that uses DIAYN only (no a-priori subtasks), so we can evaluate how important that expert knowledge is to the final performance.", "belong_id": "ryxB2lBtvH"}, {"uid": "H1eyGV_aYB", "paper_title": "Learning to Coordinate Manipulation Skills via Skill Behavior Diversification", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper provides a specific way of incorporating temporal abstraction into the multi-agent reinforcement learning (MARL) setting. Specifically, this method first discovers diversified skills for every single agent and then train a meta-policy to choose among skills for all agents. \n\nOverall, this paper is well presented so I can understand it well. Unfortunately, this paper didn't give me too much scientific insight. As maybe this is because I don't know too much about MARL, I would like to ask the author to help me address the following questions. \n\nMy first key question is, should we treat temporal abstraction (TA) under the multi-agent setting different from it under the single-agent setting? If they are the same, why do we bother discussing TA under the multi-agent setting? Why not just discuss it under the simpler single-agent setting? If they are not, what are the differences? \n\nThe second key question is if TA under the multi-agent system is special, then why the DIAYN method, which is proposed under the single-agent setting, could be directly used in the multi-agent setting? Why should we consider the DIAYN method, instead of other skills discovery methods? \n\nFurthermore, I would also like the author to help me address three more concrete questions. \n\n1. Section 1, paragraph 2, the author wrote: 'However, all these approaches are focused on working with a single end-effector or agent with learned primitive skills, and learning to coordinate has not been addressed.' Does the author mean there is no temporal abstraction method for multiple collaborative agents? \n\n2. Section 3.3, the author wrote, ' the prior distribution p(z) is Gaussian.' I.e., Z is continuous r.v. I would like to know how the author could learn q(z|s) to approximate p(z|s), which is an arbitrary continuous distribution. Maybe I am wrong, but I don't see a way to do this. \n\n3. In algorithm 1, a skill, once being chosen, will be executed for T_{low} steps, where T_{low} is fixed and pre-defined by the algorithm designer. I would like to hear to author analyzing the pros and cons of this critical design choice.", "belong_id": "ryxB2lBtvH"}, {"uid": "r1eDu0Dk5S", "paper_title": "Learning to Coordinate Manipulation Skills via Skill Behavior Diversification", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents a hierarchical reinforcement learning method for coordination of multiple cooperative agents with pre-learned adaptable skills. These skills are learned via a maximum entropy objective where diversity of behaviour given each skill is maximised and controlled via a latent conditioning vector. This allows controllability of the variation in skill execution by the meta-policy via changing the skill-specific latent vectors. The paper presents empirical results in manipulation (pick-push-place and moving a long bar by coordinating two Jaco arms) and locomotion (two Ants pushing a large block to a goal location). The method proposed outperforms the baselines reported. \n\nOverall, this paper addresses an interesting problem and can be impactful with the caveat for some clarifications and analysis. Given that the authors address my concerns, I would be willing to increase my score.\n\nThe main novelty of this work lies in how one can learn sub-skills that can be leveraged and adapted for down-stream tasks. The problem setting used to test the method is a multi-agent setting where it is crucial that skills are adapted to enable cooperation. I found the environments and the problem setting generally interesting and important for testing the proposed method. I have a few concerns that I listed below:\n\n\n1) I found the notations at times inconsistent and confusing. It would have helped to see some more details on the diagram (Figure 2) to understand how everything fits together. \n\n2) The set of skills for the two agents are selected by the meta-policy in every T_low steps. It looks like in the Jaco environments T_low = 1. Can you comment on this? This seems slightly concerning since it seems like the meta-controller is treating these skills as primitive actions rather than temporally extended behaviour.\n\n3) Looking at the training curves in Figure 4, there seems to be a really high variance in performance of the method. Can you comment on this as this seems concerning. Could you run more seeds to improve this?\n\n4) It is nice to see in section 4.5 how the hyper-parameters balancing diversity in combination with external reward (equation 4) is tuned and how sensitive that is to achieving adaptability for downstream tasks. The only criticism I have is that it is difficult to understand from 'Episode reward' on y-axis what the success rate is (similar to Figure 4)? It wouldve been nice to report results in a consistent way throughout the paper for these environments. \n\n5) Given that all the tasks in the experiments are cooperative multi-agent settings, I would have liked to see more in depth discussion regarding alternative multi-agent methods. The multi-agent baseline provided (which is using a decentralized policy with a shared critic, inspired by Lowe et al., 2017) seems fair, but I wonder if there has been more recent work in this direction that could have been highlighted? \n\n", "belong_id": "ryxB2lBtvH"}, {"uid": "ryxkuCyxFr", "paper_title": "DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine", "experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose a model for Click-Through Rate Prediction using a model consisting of an embedding layer, a Transformer stack, a Factorization Machine, and a DNN. \n\nI have several major concerns about the submission:\n2. Relevance: This work is extremely application specific, the application is not relevant to this community.\n1. Clarity and writing: The contributions which are relevant to the ICLR community are not explained well and the paper needs copy-editing for English grammar\n4. Novelty: While seemingly showing good results on some benchmarks, the model is a mix of many components and it's not clear which components actually improve performance and would be worth further study. \n\n\nMinor comments:\n\nApplying the DNN directly on top of the embeddings, and having a parallel stack of Encoder-FM, is not well explained. What does it mean that 'DNN aims at bit-wise level' if the DNN receives the same embedding features as the encoder, which supposedly 'learn[s] at vector wise level'? \n\nReferences to datasets are missing\n\nAblation study is limited, and has surprising results. E.g. even completely removing self-attention barely makes a dent in how well the method compares to other published work, moving it from rank 1 to rank 2. Otherwise only small tweaks with even more minor effects are made. What about removing e.g. the FM, other major components?\n\nThe biggest architectural innovations here are the bi-linear attention mechanism and max-pooling self attention. They are hard to interpret in this context. It's not clear how they would perform in a simpler architecture (e.g. vanilla BERT or Transformer) and in the context of a more standard benchmark. That study would have a lot more relevance to this community than the present one. \n", "belong_id": "SJlyta4YPS"}, {"uid": "S1ltPsiCFB", "paper_title": "DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This papers proposes DeepEnFM approach for CTR prediction task. In detail, Transformer encoder is applied on top of embeddings to generate new projected embeddings. Such transformer encoder is composed of self-attention with bilinear (to replace dot) and multi-head, which is followed by a mx pooling layer and then a FC layer. Position encoding is utilized then. Besides, some resnet-style trick in placed in the middle. Such encoder output is fed into FM and raw embeddings are feed into DNN part. These two parts are then used for final prediction. Some experimental results show the improvement of the proposed method over other methods.\n\nThe major questions are:\n\n*  The assumption of The field embedding size is very low in CTR is not reasonable. Do we have any study to verify this hypothesis?\n* Regarding to above hypothesis, i think it doesnt hold for all the CTR prediction tasks. Computation cost will be dramatically increased when embedding size increases because of bilinear between key and query and the FC on top of self-attention.\n* The novelty of the proposed method needs to justified to reach the bar of ICLR. The major reason is that 1) the proposed method just replaces MHSA with two changes, i.e., bilinear + max pooling, 2) other tricks such as resnet-style connection, layer norm and position encoding have been adopted everywhere.\n* The gain of proposed method is not so clear though the author test to remove each component from the architecture. As the change of encoder part is on top of MHSA, but there is no experiment to show the gain compared to using original MHSA instead of newly proposed bilinear + max pooling. I suggest to do this for better understanding the gain of changes.\n\n", "belong_id": "SJlyta4YPS"}, {"uid": "SJeEerv-5B", "paper_title": "DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper applies Multi-Head Self-Attention (MHSA) to a CTR prediction model with some small changes. The empirical results on two public datasets show it improves performance over some baselines.\n\nFirst of all, the novelty of the proposed algorithm is limited in that it mainly applies existing mulit-head self-attention. The paper does include some small modifications to MHSA and achieves better performance, such as bi-linear similarity and max-pooling. However, the nature of these changes seems more incremental.\n\nThe experiment section is very detailed and the paper conducts several ablation studies to understand which components contribute the most, which is nice. However, the paper is missing several important baselines, for example, Deep & Cross [1], which makes the results less convincing.\n\nAnother issue with the paper is that it does not control the model capacity when comparing performance. It is usually the case that increasing model capacity leads to better performance. Given that MHSA and bi-linear similarity have increased a lot of model parameters, it is more fair to compare performance across models with similar capacity. In fact, in [1], they show the logloss on Criteo dataset can be as low as 0.4423 when using large enough parameters.\n\nMinor: in the ablation study, it shows head = 1 has the best performance. In this case, why max-pooling is needed? \n\nReference:\n[1] Wang, R., Fu, B., Fu, G. and Wang, M., 2017, August. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 (p. 12). ACM.", "belong_id": "SJlyta4YPS"}, {"uid": "BkxhBqBBFB", "paper_title": "How many weights are enough : can tensor factorization learn efficient policies ?", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper suggests three different disconnected ideas for improving the number of parameters of deep vision models for playing ATARI and to improve the training speed.\n- Tensor-regression layer to replace fully connected layers\n- Wavelet-scattering layer to replace the first convolutional layer\n- Second order optimization (K-FAC)\nAll the ideas mentioned in this paper are existing ones (although properly attributed), so the novelty of this work is relatively low. \nThe paper mentions that this particular combination is 'novel', but it is not clear is there is any significant synergy between these methods and why it should be considered interesting in this particular setup.\nAlso the paper conflates sample-efficiency with parameter-efficiency. However, there is no indication that any of these methods address sample-efficiency which would be an interesting and useful contribution.\nAlso the experiments are neither very conclusive nor are they easy to interpret. For example in the pong case, there is no discernable effect of the compression ratio as the highest and lowest compression give the best (and comparable) results. Also the results come without confidence intervals.\n\nSo, in general, I would consider this paper to be an uninspired combination of pre-existing ideas with weak and inconclusive experimental results: a clear reject.", "belong_id": "B1l3M64KwB"}, {"uid": "HyeUam4aKB", "paper_title": "How many weights are enough : can tensor factorization learn efficient policies ?", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigates a list of methods to reduce the number of weights for deep RL architecture under the low-data regime. These methods include tensor regression, wavelet scattering, as well as second-order optimization (K-FAC). The experiments on the Atari games shows that by using tensor regression to replace the dense layer of the neural nets and using K-FAC for the optimization, one can reduce around 10 times of parameters without losing too much of performance. \n\nHowever, I have some concerns on the novelty of this work and therefore Im giving this paper a weak reject. Here are the reasons:\n\nTo begin with, leveraging tensor structure of the neural nets to reduce number of parameters while maintaining similar level or getting even better results have been done before, for example: Tensorizing Neural Networks (Novikov et al, 2015), Learning compact recurrent neural networks with block-term tensor decomposition (Ye et al., 2018) etc. Although the use of tensor regression might be new, the core idea is still to leverage the low rank property of the tensor and obtain a compression of the weight tensors. Moreover, why use Tucker decomposition specifically for the tensor regression? It has been proposed that using tensor train (TT) decomposition can also get very good results (see Garipov et. al. Ultimate tensorization: compressing convolutional and FC layers alike). Is it possible to investigate the use of TT decomposition for the dense layer of the deep RL architecture? Therefore the novelty for this aspect seems a bit weak for me.\n\nThe second method the authors have attempted is to swap the convolution layer of the deep RL architecture with wavelet scattering. For one particular game (demon_attack), this approach seems to outperform every other methods by a large margin. However the experiment shows that for the rest of the Atari games, there is a huge drop (45%) of performance. Therefore the significance of this approach is rather thin for me. Maybe some further investigation of the game demon_attack is needed to understand why using scattering in this game in particular gives such a huge performance boost. \n\nThirdly, as an approximation of the second order optimization, K-FAC does not really concern with the main theme of the paper, which is an investigation of potential weights reduction methods. It is great that the authors applied this techniques and seems to have great results. However, as the authors pointed out, K-FAC has been wildly applied in the deep RL literature, and the authors did not propose new extension for the K-FAC method, therefore the contribution of this matter is also quite thin. \n\nLast but not least, the writing of the paper is a bit clumsy, and I was having a hard time to figure out what exactly is the proposed method. I think this paper might need some rework on the writing to describe the idea of the authors in a more clear way for the publication. Due to these reasons, Im giving this paper a weak reject. \n\nSome writing comments and potential writing errors (did not affect the decision):\nPage 3, first line of Tensor regression layer, the shape of the tensor X seems to be a typo. \nAlso here, the definition of <X, Y>_N in the paper is to sum over the dimension of I_1...I_N, then the shape of <X, Y>_N should be K_1*...*K_x*L_1*...*L_y, without the I_N in the middle. \nAlso in this section, the authors mentioned Tucker decomposition for the tensor regression. However the phrasing of this sentence needs a bit rework. The usage of For instance here, gives the readers a feeling that Tucker is just one possible way of doing this decomposition, but not necessarily the actual decomposition for the reported experiments. \nIn 2.3, there is a lack of definition for  \\Lambda_1 and \\Lambda_2. In addition, it would be better for the general readers to add a few definitions for the terminologies in this section. For example, circular convolution, wavelet filter banks etc. I guess people with corresponding background will understand it with no problem, however I do find myself a bit lost in this section with these terminologies. \n2.4 line 6, with A and. B smaller, architecture-dependent matrices. I think it should be with A and B being...\n3.1, line 5, This is all the more pressing that...., I did not understand this sentence. \nIn page 6, line 3, there is a lack of definition for compression rate. Is it the compression rate w.r.t only the last dense layer, or w.r.t the whole network?\nFigure 4 is lacking y-axis and x-axis labels. \n4.2, last bullet point, However, one must not forget that the conv layers one learns must be somehow be well adapted..., I get what you are saying, but the sentence is a bit clumsy. \nTable 1 and 2, the row name Average is lacking definition. \n\n Overall it is a good attempt to reduce the number of weights in the deep RL architecture, but I do think the novelty of this work is a bit thin and the three contributions were not tied together with the main theme of the paper. Therefore, Im giving this work a weak reject. \n", "belong_id": "B1l3M64KwB"}, {"uid": "ryxByGUaKS", "paper_title": "How many weights are enough : can tensor factorization learn efficient policies ?", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper aims at parsimonious reinforcement learning by employing 3\ndifferent techniques: using tensor regression layers (Kossaifi et al.,\n2017b), wavelet scattering (Mallat 2011) and using K-FAC (Kingma & Ba,\n2014) as the optimization method. Learning models with fewer\nweights is important not only in reinforcement learning but also\nin all other machine learning areas. With the combination of tensor\nregression layers and K-FAC, the proposed methods give comparable\nperformance on several Atari games against 2 other methods, SimpPLe\nand data-efficient Rainbow, while using 2 to 10 times fewer\ncoefficients than data-efficient Rainbow. The use of wavelet\nscattering provides improvement on 1 out of 26 Atari games. The paper\nalso points out an interesting concentration of eigenvalues of dense\nlayer of a deep RL agent which provides motivation for low-rank\npresentations.\n\nWhile the savings in terms of coefficients is positive, the obtained\nresults are of little surprise. Tensor regression layers and K-FAC are\nused as is without any modification while space savings and efficiency\nhave been reported in corresponding references. The performances of\nwavelet scattering for the reported tasks are weak (better in only one\ngame) and the space saving is not clear. The proposed improvements\nseem to be tailored to tasks with image inputs and hence reported\nresults are only on Atari games (possibly with sparse and low-rank\nimages). It is not clear if the proposed techniques can be applied to\na wider set of reinforcement learning tasks\n(e.g. https://gym.openai.com/envs/#mujoco).\n\n\nIt would be interesting to see if we can apply the proposed methods to\nother more diverse RL tasks. The performance of wavelet scattering does indeed need\nmore investigation and improvement. It would also be interesting to\ncompare the distributions of the eigenvalues of the tensor layers\nversus the dense layers in deep RL which may provide insights on the\nachieved savings and the compression trade-off.\n\n\nI have read  the authors' rebuttals. The reviews point to a number of directions \nwhere the contributions could be made more significant.\n\n", "belong_id": "B1l3M64KwB"}, {"uid": "H1x0kqZJ5S", "paper_title": "Capsules with Inverted Dot-Product Attention Routing", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a simple and effective routing algorithm for capsule networks. The paper is well written. A nice analysis of the proposed routing algorithm is provided. Experiments of varying the routing iterations demonstrate the stableability of proposed routing algorithm compared to others.\n\nHere are some issues:\n1. Would the authors release the code for reproducing the results in the paper? It will be helpful for future research in this area.\n\n2. In Fig.5, it would be better to give some brief explanations about why CasNet (Matrix) occupies much more memory while possessing less parameters.", "belong_id": "HJe6uANtwH"}, {"uid": "rygSM4-q9r", "paper_title": "Capsules with Inverted Dot-Product Attention Routing", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis paper presents a new simpler routing mechanism for capsule networks and achieves good performance on real world data sets making use of this new capsule structure along with a restnet backbone. Strong performance on the cifar10 and cifar100 datasets are presented and the network outperforms earlier versions of capsule networks. This new structure also performs well on an augmented MNIST dataset of overlapping digits (similar to the one used by Sabour et al 2017). \n\nOverall the paper is well written and presents solid results. The paper also presents a thorough comparison of two earlier versions of capsules which is a worthwhile contribution in its own right.\n\nThe paper could be improved by clearing up a few ambiguities:\n\n- is the learning rate schedule the same for all three models? in figure 4 it looks like the learning rate is decayed at two distinct points for your model, but only one distinct point for both the EM and Dynamic routing models.  \n\n-'Notably, the prediction becomes random guess when the iteration number increases to 5.' this sentence is a little confusing. Do you mean when the iteration number the performance is equivalent to not random assignments?  \n\n- This new algorithm requires that the capsules in L+1 have initialized poses with which to compare agreement between the poses in L. This is initial value seems like it may greatly effect the performance of the model. In the paper it is set to 0 and not expanded upon. It would be interesting to see if randomizing this value, or learning a bias for it would effect performance.   \n\n-unlike the two previous versions of capsules, the inverted dot product capsules show in figure 4 sudden huge decreases in test accuracy while training. These moments seem to be overcome quite quickly and the model ends up outperforming the other two. But it would be worth mentioning this behavior and perhaps attempting to explain it.\n", "belong_id": "HJe6uANtwH"}, {"uid": "SklNZ1259S", "paper_title": "Capsules with Inverted Dot-Product Attention Routing", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Authors improve upon dynamic routing between capsules by removing the squash function (norm normalization) and apply a layerNorm normalization instead. Furthermore, they experiment with concurrent routing rather than sequential routing (route all caps layers once, then all layers concurrently again and again). This is an interesting development since provides better gradient in conjunction with layerNorm. They report results on Cifar10 and Cifar100 and achieve similar to CNN (resnet) performance.\n\nFirst, I want to point out that inverted attention is exactly what happens in dynamic routing (sabour et al 2017), proc. 1 line 4,5, and 7. In dynamic routing the dot product with the next layer capsule is calculated and then normalized over all next layer capsules. The only difference that I notice between alg. 1 here and proc. 1 there is replacement of squash with layer norm. There is no 'reconstructing the layer bellow' in Dynamic routing as authors suggest in intro. \n\nSecond, the Capsules are promised to have better viewpoint generalizability than CNNs while having comparable performance. Replacing the 1 convolution layer with a ResNet backbone and replacing the activation with a classifier on top seems reducing the proposed CapsNet to the level of CNNs in terms of Viewpoint Generalization. Why should someone use this network rather than the ResNet itself? Fewer number of parameters by itself is not interesting, the reason it is reported usually is that it indicates lower memory consumption or fewer flops. Is that the case when comparing the baseline ResNet with the proposed CapsNet? Otherwise, a set of experiments showcasing the viewpoint generalizability of proposed CapsuleNetworks might only justify the switch between resnets to the proposed capsnets.\n\nThirdly, Fig. 4 top images seems to indicate all 3 routing procedures are following the same Learning Rate schedule. In the text it is said that optimization hyperparameters are tuned individually. Did authors tune learning rate schedule individually?\n\nForth, the proper baseline for the current study is the dynamic routing CapsNet. Why the multiMNIST experiment lacks comparison with dynamic routing capsnet?\n\nFor the reasons above, the manuscript in its current format is not ready for publication.\n\n------------------------------------------------------rebuttal\nThank you for your response. I acknowledged the novel contributions of this work. My comment was that some claims in the paper are not right. i.e. 'inverted dot-product attention' is not new and 'reconstructing the layer bellow' does not happen in Sabour et al . Parallel execution + layer norm definitely is novel and significant.\n\nRegarding the LR-schedule, I am not sure how fair it is to use same hyper-params tuned for the proposed method on the baselines. \n\nRegarding the viewpoint, the diverseMultiMNIST is two over lapping MNIST digits shifted 6 pixels. There is no rotation or scale in this dataset. An example experiment verifying the viewpoint generalizability of the proposed model is training on MNIST testing on AFFNIST. \n", "belong_id": "HJe6uANtwH"}, {"uid": "S1liW_icKH", "paper_title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work tried to address the covariate shift problem in imitation learning, which is due to the mismatch between training and test state distribution and may cause compounding errors. \n\nThe authors proposed the algorithm called value iteration with negative sampling (VINS) of which the main ideas can be summarized as follows. First, value iteration is used on expert trajectories with negative sampling. Specifically, the states that are randomly perturbed from experts states were used to enforce (4.1) and (4.2) in the submission (called *conservative extrapolation* requirements in the submission). By doing so, the state values outside the support of expert state visitation distribution become less than those inside the support. In the meantime, temporal-difference (TD) error was minimized to satisfy Bellman consistency among state values. The second main idea of this work is to use *self-correctable policy*, where the approximate dynamics and behavioral-cloning (BC) policy were used to select the action which is expected to give higher value at the successor states. \n\nTo consolidate their ideas, the authors proved Theorem 4.4 showing that state visitation distribution of resultant policy is approximately close to that of an expert under a few assumptions. Empirically, they considered two experiments. In the first experiment, assuming that the environment simulation is not allowed, the performance of VINS was compared with that of BC over 5 tasks, and VINS achieved higher success rates. In the second experiment, assuming the simulation is allowed, VINS was compared with existing methods such as HER+BC, GAIL and Nair et al 18 and shown to be much more sample-efficient compared to the selected baselines. \n\nAlthough the theoretical and empirical contributions of this work are clear to me when the environment simulation is not allowed (as shown in the first experiment in Table 1), I think the second experiment, which allows the environment simulation (as shown in Figure 3), is misleading, and this is why I vote weak reject for this work. For instance, we can simply think about GAIL with BC initialization, but it seems to me that GAIL with random initialization was used in the second experiment (since authors mentioned GAIL in OpenAI baselines was used without hyperparameter tuning (https://github.com/openai/baselines/blob/master/baselines/gail/run_mujoco.py#L53)). In addition to it, there have been some recent works on the sample efficiency of imitation learning with environment simulation which are not included as baselines in this work:\n\n[1] GMMIL (Kim and Park, 2018, Imitation Learning via Kernel Mean Embedding) - cost learning with maximum mean discrepancy minimization leads to sample-efficient training\n\n[2] BGAIL (Jeon et al, 2019, Bayesian Approach to Generative Adversarial Imitation Learning) - Bayesian cost is helpful for sample-efficient imitation learning\n\n[3] DAC (Kostrikov et al, 2019, Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning) - Solving reward bias in imitation learning with simulation and using off-policy RL algorithm to enhance the sample efficiency\n\n[4] Sasaki et al, 2019, Sample Efficient Imitation Learning for Continuous Control - Bypassing cost learning and introducing off-policy RL to enhance the sample efficiency. \n\nEspecially, off-policy imitation learning methods [3], [4] are shown to be extremely sample-efficient compared to GAIL. I think the authors should have compared the performance of VINS + RL with those baselines in the second experiment if they tried to emphasize the sample efficiency of VINS + RL. Otherwise, they should have focused more on the initialization effect of VINS and BC. For example, one can consider the convergence speed of GAIL to the expert performance when policies were initialized with either VINS or BC.", "belong_id": "rke-f6NKvS"}, {"uid": "Bylgr7yRFr", "paper_title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work presents the value iteration with negative sampling (VINS) algorithm, a method for accelerating reinforcement learning using expert demonstrations.  In addition to learning an expert policy through behavioral cloning, VINS learns an initial value function which is biased to assign smaller expected values to states not encountered during demonstrations.  This is done by augmenting the demonstration data with states that have been randomly perturbed, and penalizing the value targets for these states by a factor proportional to their Euclidean distance to the original state.  In addition to the policy and value function, VINS also learns a one-step dynamics model used to select actions against the learned value function.  As the value function learned in VINS is only defined with respect to the current state, action values are estimated by sampling future states using the learned model, and computing the value of these sampled states.\n\nEmpirical results on a set of robotic control tasks demonstrate that VINS requires significantly less interaction with the environment to learn a good policy than existing, state of the art approaches given the same set of demonstrations.  While the paper presents a novel and highly effective approach, there are some apparent limitations to the algorithms which should be highlighted, and there is room for improvement in the empirical evaluations.\n\nIt is unclear that VINS would generalize well beyond robotic control domains.  For one, its theoretical guarantees depend on the local reversibility of the dynamics, that is, for small deviations from the desired state, it is possible to return to that state in a single step.  This isn't too significant a restriction, as the ability to recover quickly from small mistakes would seem to be a necessary for any method to be able to provide similar guarantees about its behavior.  The bigger issue is the use of the Euclidean metric (or any fixed metric) in the definition of conservative extrapolation.  Basically, a state is said to be similar to the states observed during the demonstrations if it is close, under the Euclidean metric, to at least one demonstrated state.  This is a reasonable approach in robotic control tasks, where Euclidean distance is a good measure of how similar two configurations are to one another, but it would seem to be unsuitable for domains where the observation space consists of images or other high-dimensional representations.  In those cases, a useful notion of similarity would likely have to be learned from the data.  In such domains, one might imagine that the conservative value function would simply learn to distinguish between real observations, and those that have been perturbed by random noise, which would never be observed in the actual task.\n\nWhile experimental results demonstrate a very significant advantage for VINS both in terms of sample complexity and final performance, results are presented for only two tasks, 'pick-and-place' and 'push', while VINS outperforms the alternatives on these tasks, it is worth noting that its initial performance (without additional environment interact) is not dramatically superior to pure behavioral cloning.  It would be helpful to see how well VINS compares against the alternatives for a much smaller number of demonstrations, say 5-20, a regime where we would expect initial performance to be poor.", "belong_id": "rke-f6NKvS"}, {"uid": "H1gCYBFkcB", "paper_title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis paper tackles an issue imitation learning approaches face. More specifically, policies learned in this manner can often fail when they encounter new states not seen in demonstrations. The paper proposes a method for learning value functions that are more conservative on unseen states, which encourages the learned policies to stay within the distribution of training states. Theoretical results are derived to provide some support for the approach. A practical algorithm is also presented and experiments on continuous control tasks display the effectiveness of the method, with particularly good results on imitation learning followed by reinforcement learning.\n\nThe proposed algorithm makes use of a natural intuition, that states visited by the expert probably have higher values, and the paper generally does a good job of supporting the approach through theory and experiments. Although the experiments seem sound, certain experimental details are not completely clear. The theory may also have some restrictive assumptions, limiting its significance.\nOverall, I am divided about this paper. While this submission has the elements of a good paper and the presentation is great, certain concerns make me hesitant to recommend acceptance. I would be willing to increase my score if those points are addressed. \n\nTheory:\n1) My main concern is the applicability of the theorem, due to Assumption 4.2. While the intuition is that there is an action that corrects the next state towards the demonstration states, the theoretical condition is more restrictive. In particular, the following part (paraphrasing): 'there exists an action a_cx that is close to a^bar and that makes a correction towards U'. This condition implies that there are correcting actions near any action a^bar, which sems unrealistic in most cases. For example, in a driving task, say s^bar is a state such that moving back to U require the vehicle to move to the left. Then, consider the action a^bar of steering towards the right (with some angle). There could be no action near a^bar that makes the vehicle turn left towards U. Note that this is not necessarily a pathological situation as described in the text.\n\n2) Also concerning assumption 4.2, I do not see why s^bar' is included in the quantifier of the statement since it is not used afterwards; after 'there exists an action...' no mention of s^bar' is made.\n\n3) The projection function may not be well-defined if there are multiple states that are closest to the one being projected.\n\n4) It could be said explicitly that the expert policy is assumed to be deterministic. Currently, this is not said outright.\n\nExperiments:\n1) It seems like VINS relies heavily on the assumption that the environment is deterministic to learn an effective model. Was VINS tried in stochastic environments? \n\n2) Data augmentation is used for VINS. This seems like an unfair advantage compared to the baseline competitors since sample efficiency is a key concern to reinforcement learning algorithms.  To make the comparisons fair, either it should be removed or the other algorithms should also receive additional data. How is the performance of VINS without this addition?\n\n3) A description of how the hyperparameters were chosen and their final values would be needed for reproducibility. Also, a discussion of the importance of the hyperparameters and their sensitivity would be informative. For example, I was curious to know the value of \\alpha in Algorithm 2 compared to the ranges the actions could take. \n\n4) I am not convinced that using Q functions would necessarily fail. On p.6, the paragraph 'can we learn conservatively-extrapolated Q-function' gives some reasoning why this could fail, that we may not want to penalize unseen actions. This is in opposition to the BCQ algorithm [1], which explicitly tries to avoid unseen actions and still has good performance. Trying a variant with Q(s,a) could be worthwhile. \nI am not exactly sure if I understood Appendix A properly but, from my understanding, I do not think the argument made there necessarily invalidates using Q functions. It seems to apply mostly to deterministic expert policies and also Q(s,a) could still have reasonable values due to function approximation (even if the particular action 'a' is not seen in demonstrations). \n\n5) Which RL algorithms were used for the imitation learning + RL set of experiments?\n\n6) For table 1, are the results also averaged over different sets of demonstrations? \n\n7) Are error bars one standard deviation or one standard error (divided by sqrt(n))?\n\n8) For figure 3, using RL without imitation learning would serve as a good additional baseline \n\n9) Ablation study: Trying no negative sampling with a perfect model could isolate the effect of negative sampling. \n\n10) Ablation study: What is the no behavior cloning and perfect model experiment trying to show?\n\n11) I think the name of the algorithm should be modified as 'value iteration' refers to a specific dynamic programming algorithm for learning value functions, while the proposed algorithm does not resemble this at all. \n\nMinor comments and typos (no impact on score):\n- Using the cross-entropy method as in QTOpt [2] could be used to pick actions in a more refined manner.\n- There is a large amount of blank space on p.8\n- p.3 'At the test time' -> 'At test time'\n- p.4 'entire states space' -> 'entire state space', 'burned to warm up them' -> 'burned to warm them up'\n- p.9  'option 2 by search the action uniformly.'  -> 'option 2 to search the actions uniformly'\n\n[1] 'Off-Policy Deep Reinforcement Learning without Exploration' by Fujimoto et al.\n[2] 'QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation' by Kalashnikov et al.\n", "belong_id": "rke-f6NKvS"}, {"uid": "H1xdnWUVtB", "paper_title": "Generative Adversarial Networks For Data Scarcity Industrial Positron Images With Attention", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The topic of the paper is a GAN framework to enhance PET images in industrial inspection, as far as I understand by transfer learning from a medical PET database. Unfortunately, I am unable assess the paper due to serious language problems. The text is incoherent and not understandable, it is impossible to decipher what is actually proposed.\n\nAdditional Comments:\n\nThe text reads like a machine translation gone wrong, including weird terms like 'migration learning' (transfer learning?), 'antagonistic generation network' / 'countermeasure generation network' / 'confrontation network' (GAN?).\n\nReferences are also mangled and undecipherable. And seemingly also not always well-chosen - even if I cannot map it to a paper due to bibtex problems, it appears implausible to reference a 2015 paper for something as basic as principal component analysis.\n\nIt seems that the only experiments are on simulated PET data. In may view that would be insufficient for a largely empirical application paper.\n\nThe paper claims to be quantitatively better than the baselines, but has, by far, the highest Frechet Inception distance. To my understanding, FID is a distance, lower is better.\n\n", "belong_id": "SkxcSpEKPS"}, {"uid": "BkxnKw5pFr", "paper_title": "Generative Adversarial Networks For Data Scarcity Industrial Positron Images With Attention", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work proposes a method to transfer information from PET imaging data from the medical domain, where data is highly available, to the industrial one, where data is scarce, in the context of non-destructive material quality evaluation. The basic idea seems interesting, but unfortunately in the present form he paper is very difficult to appreciate, as it lacks of important details concerning methodology, experimental results, and comparison with respect to the state-of-the-art. Moreover, the manuscript still appears in a draft form. Sentences are often broken, the text presents many typos and grammar mistakes, and the citations are not understandable. \n\nMissing methodological details are grouped in the following parts of this review.\n\nSection 3.2 Encoder\nParagraph 2: The authors claim that they introduce the knowledge of migration learning without explaining it. Where does this concept come from? Is there a literature about it or is this a new concept?\n\nMedical images are fitted though a variational auto encoder (VAE) (citation missing). The encoder description is minimal and lacks of implementation details (see Eq. 3), while the decoder description is missing throughout the paper. \n\nEq. (2): the authors write that they obtain a distribution p(x) according to Eq. (2), but this equation is just the formula for the sample mean, where p(x) is sampled and not computed.\n\nEq. (3): f1 and f2 are never made explicit in the paper so we do not know if they are linear or non-linear functions. The prior p(Z) is decomposed as a summation of posteriors p(Z|X) and the choice to have these posteriors equal to N(0,1) (1-dimensional, which is unusual) regardless of the data point X is not explained.\n\nSection 3.3 Feature extraction memory module\nFeature extraction from industrial images is done through principal component analysis (PCA). In the same paragraph is written that features are extracted through convolution neural networks (CNN). So it is not clear if there is an arbitrary choice to use PCA or CNN, and what are the conditions when this happens.\n\nEq. (6): the score between z_t (medical image distribution) and y_s (industrial image feature vector) is computed as scalar product dot(z_t , W_a * y_s). The key link is the linear mapping W_a, which is never made explicit in the paper. How do the authors compute W_a ?\n\nSection 4.1 Implementation details\nthe networks called identification network and front-end network are not well defined. They may refer to the  VAE, the CNN, the Adversarial Nets. There is too much ambiguity. A captioned figure can help in resolving the ambiguities.\n\nSection 4.2 Dataset\nIn the first paragraph the authors cite a dataset of CT images, while the main focus of the paper is on PET images. How the CT images comes into play?\n\nSection 4.3 Evaluation\nThe authors compare their method with respect to other three methods, namely VAE, DCGAN, and WGAN.\nThe implementation details of the competing methods are not described so we cannot be sure about the fairness of the comparison.\n\n\nOther considerations\nIntroduction, 4th paragraph: imaging quality is higher With respect to what? Usually PET images have the worst quality in the medical domain.\nIntroduction, 3rd-to-last paragraph: We use the medical CT image .... Should be PET images\n\nRelated work: This section is a list of works and the relation with the current paper is not highlighted.\nlung cancer cakes what are they?\n\nParagraph 3.2: excessive pursuit of quality why is it bad? migration learning do they mean transfer learning? Equation 2 and 3 in relation to a clustering problem never pointed out before in the paper.\n\nParagraph 4.1. What is the meaning of Adam algorithm(=0.5) ?\n\nFigures 1 and 2 have very minimal captions. What do they represent?\n\nCitation formatting problem: name and surname are switched. Journal/conferences often omitted.", "belong_id": "SkxcSpEKPS"}, {"uid": "SkgMSQgJ9S", "paper_title": "Generative Adversarial Networks For Data Scarcity Industrial Positron Images With Attention", "experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper applies GAN to the field of nondestructive testing for specific industries. This paper is more like a technical report rather than a formal paper. It seems to me that this paper should be submitted to other computer vision conferences of even specific areas while not in ICLR. \n\nIssues:\n\n* Bad format or organization. The authors are suggested that they should give a subtitle of each categories of work in the related work section. The table and picture in this paper are arbitrarily designed and take too much space. In some equations, you need to put commas to separate different equations.\n\n* Acknowledgements reveal personal information in the paper. It's not allowed in ICLR submission that would review the identity of the authors. This is a highly critical issue. \n\n* Bad writing. I can barely understand what the authors are talking about. For example, 'In this paper, we propose adversarial networks of positron image memory module based on attention mechanism'. Are you referring you proposed a new GAN model in the paper? Or you proposed a positron image memory module? ...\n\n* What are the functions of the metrics used in the experimental part? The higher, the better? Or the lower, the better? Besides, you need some analysis to illustrate the significance of your results.\n\nConsidering these issues demonstrated in the paper, I recommend rejection.", "belong_id": "SkxcSpEKPS"}, {"uid": "BJg1cbsTtB", "paper_title": "Imitation Learning of Robot Policies using Language, Vision and Motion", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work uses imitations learning (from synthetic data) to train a deep model which takes a natural language instruction, and a visual representation of a robot's environment, and outputs a trajectory for the robot to follow which executes this instruction.  The work focuses on a robotic pick-and-place task, where the instruction indicates which of the available bins an item should be placed in.  In addition to the trajectory model, a second model is trained which allows the agent to predict whether a given command is actually feasible (i.e. whether the target bin exists).  Empirical results show a reasonably high success rate in placing objects in the bin specified by the instruction, though there is still room for improvement in cases where the shape o a combination of features is important to the selection of the correct bin. \n\nRather than mapping directly from instructions and observations to control signals, the model trained in this work translates from an instruction, and an image of the agent's environment, to the parameters of a DMP controller.  The network therefore outputs the entire motion for the task in a single inference pass.  This approach would have advantages and disadvantages.  The DMP formulation ensures that the resulting trajectory is relatively smooth.  It also means that the network outputs a distinct goal configuration, which the DMP should reach (assuming the goal is feasible) regardless of the other motion parameters.  The use of a DMP output space, however, limits the model to generating relatively simple, goal-directed motions, and does not allow the agent to adapt to changes in the layout of the environment (which would only be observed in the static visual input).\n\nAs other work has considered visual instruction following (e.g. Misra et. al. 'Mapping Instructions and Visual Observations to Actions with Reinforcement Learning') it would strengthen this work considerably to see a direct comparison between this method and existing approaches.  It is likely that the approach presented in this work is better suited to the specific problem of robot control, but it would be helpful to see if learning a low-level control policy directly can be successful in this context.\n\nThe work needs to expand on the discussion in the second paragraph of section 4, where human annotators were used to generate natural language instructions for different tasks.  The paper suggests that this data was not used directly to train the model, but was instead used to build a template for generating natural language instructions.  What this template looks like, and how it was constructed based on the human-generated data, remains unclear, and needs to be described in much more detail.", "belong_id": "Bkg5LgrYwS"}, {"uid": "BylouRApKH", "paper_title": "Imitation Learning of Robot Policies using Language, Vision and Motion", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "*Summary \n\nThe paper describes a new end-to-end imitation learning method combining language, vision, and motion.\nA neural network architecture called Multimodal Policy Network is proposed. That can extract internal representations from language and vision to condition the generated motions. \nIt enables an end-user to influence a robot's policy through verbal communication.\nThe experiments demonstrate the generalization performance of the method. That can generate behaviors towards different goals depending on different sentences. \n\n*Decision and supporting arguments\n\nI think the paper is just below the borderline. The reason is as follows.\n\nThe concern is about evaluation. They demonstrated the method could work, and the robot can move to appropriate goals. However, there is no comparative methods in the experiment.\nRelated to this point, the problem was not identified in the Introduction.\nThe authors might assume that introducing language into behavioral cloning itself is qualitatively new work. However, such a study has a long history. \nFor example, please refer to Tani's pioneering works.\nSugita, Yuuya, and Jun Tani. 'Learning semantic combinatoriality from the interaction between linguistic and behavioral processes.' Adaptive behavior 13.1 (2005): 33-52.\n\nThe author should specify a current challenge or problem in pre-existing studies about imitation learning with language input, clarify their claim, and give empirical support for the claim.   \n", "belong_id": "Bkg5LgrYwS"}, {"uid": "HJecYd1RFS", "paper_title": "Imitation Learning of Robot Policies using Language, Vision and Motion", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper addresses the problem of using multiple modalities for learning from demonstration. Approaches that take in task or joint space data to learn a policy for replicating that task are numerous. Doing the same with multiple modalities involved, in particular vision, language and motion, has only been recently considered, so this is a timely paper. \n\nThe core contribution is pretty well summarised by the architecture in figure 1, which involves a combination of encodings of the words and sentences, images and parameters of a DMP in order to generate movement commands from a high level instruction. \n\nUnless I have missed something in the experimental setup, all of the considered task variations are movement commands of the form <Move> to <Object>. The network setup allows for synonyms of two kinds, so <Move> can be replaced by numerous verbal synonyms such as advance and go, and the object can be specified in terms of shapes, colors and so on, but otherwise this is the only specification of the task. This has been addressed in the recent literature using neural network architectures similar to the one being proposed here, e.g., see the following papers. These papers already solve the proposed problem and provide similar explanations. It would be helpful to see comparative discussion with respect to those methods and a clear statement of novelty with respect to such prior work:\n[R1] M. Burke, S. Penkov, S. Ramamoorthy, From explanation to synthesis: Compositional program induction for learning from demonstration, Robotics: Science and Systems (R:SS), 2019.\n[R2] Y. Hristov, D. Angelov, A.Lascarides, M. Burke, S. Ramamoorthy, Disentangled Relational Representations for Explaining and Learning from Demonstration, Conference on Robot Learning (CoRL), 2019. \n\nAn interesting feature in R2 that the authors do not explicitly address here is the issue of relational specifications in the language, e.g., in addition to saying 'move to the red bowl', we may also wish to say 'place on top of red block'. In the way that MPN is currently set up to map from the language input directly to hyperparameters of the DMP, and considering the embedding structure, it is not clear if MPN is capable of handling such specifications. If so, the claim of generalisation on the language input should be stated more clearly.\n\nThe ablation study is setup somewhat differently than what I would have expected. The authors consider the effect of changing the training set size and if the language input includes synonyms or not. Those two aspects seem to produce the expected results. It would also be interesting to see an ablation study in the sense of replacing or removing aspects of the architecture to see its relative effect on the overall model performance. So, for instance, if one did not have a DMP with the hyperparameters being estimated by a network and instead had a more straightforward encoding of where to move to - does it make a difference and how much? Likewise, how much performance benefit, if any, is being derived from an uninterpreted image I being combined as described in the embedding as opposed to an alternative that detects an objects and combines that position differently. The paper would have been stronger if such architectural choices were better justified and also demonstrated in the experiments.\n\n\n", "belong_id": "Bkg5LgrYwS"}, {"uid": "rkgz2PwHYB", "paper_title": "Optimizing Data Usage via Differentiable Rewards", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an iterative method that jointly trains the model and a scorer network that places a non-uniform distribution over data sets.  The paper proposes a gradient method to learn the scorer network based on reinforcement learning, which is novel as to what the reviewer knows.\n\nThere are several concerns/questions:\n\n1) The paper doesnt define the D_{dev} clearly. How is D_{dev} chosen? Is it a subset of D_{train}? \n\n2) In section 2.1, why smaller development set D_{dev} is much closer to the P_{test}(X,Y)? P_{test}(X,Y) is supposed to be not observed during training?\n\n3) In Eq (5), if D_{dev} is s subset of D_{train}, if \\theta* is the minimal of J, it means the gradient \nat  \\theta* is 0. To calculate the gradient of J with respect to \\psi, by chain rule, it need to calculate gradient to \\theta* first then \\theta* to \\psi. If gradient of \\theta* is 0, the product is also 0? So the \\psi will not be updated if D_{dev}  is sufficiently similar to D_{train} ?\n\n4) In Section 2.3, it omits the second order Hessian term. How does that influence the performance? \n\n5) it mentions without significant computing overhead in abstract, which is not demonstrated elsewhere.\n\n6) In the experiments, table 1, it seems the major improvement comes from retrain and TCS rather than DDS? In figure 3, it is better to show the weights of an image without DDS and comparing that with DDS.\n\n7) The paper contains many typos such as Eqn.11 is not defined in the main paper, the Eqn ?? Appears in the appendix, tha minimizes etc.\n\nIn general, the idea of the paper is natural and the results seem promising. I am looking forward to the reply to my questions/concerns. \n\n#############\n\nI have read the author's feedback. I think the clarity of both methodology and experiment does not reach the acceptance level and would maintain my current rating. \n", "belong_id": "BJxt2aVFPr"}, {"uid": "r1lqmp15YS", "paper_title": "Optimizing Data Usage via Differentiable Rewards", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a reinforcement learning approach towards using data that present best correlation with a validation sets gradient signal. The broader point of this paper is that there is inevitably some distribution shift going from train to test set - and the validation set can be a small curated set whose distribution is closer to the testing distribution than what the training dataset's distribution is. \n\nThe problem setup bears relationship to several areas including domain adaptation/covariate shift problems, curriculum learning based approaches amongst others. One assumption that I see which needs to be understood more is equation (6) - wherein, somehow, there is a Markov assumption used to zero out the contribution of the scoring network on parameters unto previous time step. Trying to understand the implications of this assumption (how the performance varies with/without this assumption) would be instructive for understanding potential shortcomings of this framework.\n\nI think the paper is well written, handles an important question. That said, I am not too aware of recent work in this area to make a decisive judgement on this papers novelty/contributions. ", "belong_id": "BJxt2aVFPr"}, {"uid": "Skgt9dG-9H", "paper_title": "Optimizing Data Usage via Differentiable Rewards", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper introduces a simple idea to optimize the weights of a weighted empirical training distributions. The goal is to optimize the population risk, and the idea is to optimize a distribution over the training examples to maximize the cosine similarity between training set gradients and validation set gradients. The distribution over the training set is parameterized by a neural network taking as arguments the\n\nStrengths:\n- The method is quite simple.\n- The results appear to be strong, although I am less familiar with the NMT baselines. The imagenet results seem quite strong to me.\n\nWeaknesses:\n- I couldn't find a particularly clear description of the scoring networks architecture. Given that it observes the whole dataset, this seems like a critical choice that could have a big impact on the complexity of this approach. At the very least, this should be clearly reported, and I recommend a more thorough investigation of this choice.\n- The authors report that their method takes 1.5x to 2x longer to run than the uniform baseline. Yet, they ran all methods for the same number of steps / epochs. It seems to me that a fairer comparison might be letting all methods enjoy the same total budget measure roughly by wall time.\n\nQuestions:\n- I didn't follow why the computation of the per example gradient grad l(x_i, y_i, theta_t-1) is so onerous. Isn't that computed on line 5 already?", "belong_id": "BJxt2aVFPr"}, {"uid": "H1l5NeGTFS", "paper_title": "Crafting Data-free Universal Adversaries with Dilate Loss", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper is well written and easy to follow. In this paper, a new data-free method is proposed to create universal adversarial perturbation without using data. There are some similarities with GDUAP though, authors also make some crucial improvements. They perform Euclidean norm maximization before the non-linearity of each layer, which not only has theoretical backing but also brings better performance in practice. Meanwhile, they optimize the perturbations in each layer in a sequential manner instead of joint optimization to avoid chances of reaching local minima solutions.\n\nThe authors provide a detailed theoretical analysis and systematic experimental results to demonstrate their arguments, which is convincing. Whats more, the proposed method achieves state-of-the-art data-free fooling rates on the large-scale dataset, which strongly demonstrates the effectiveness of their method.\n\nIn section 3.2, (the top of page 4) which becomes additive if column vectors in W1X are in the same orthant as W1p. We relax this criteria and favour the case of making the vectors as close as possible by\nCould the authors provide more discussions about it?", "belong_id": "HJxVC1SYwr"}, {"uid": "BJxB-51Rtr", "paper_title": "Crafting Data-free Universal Adversaries with Dilate Loss", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a white-box (known network architecture, known network weight) data free (without need to access the data) adversarial attacking method. The main idea is to find a perturbation that maximizes the activations at different layers jointly. But the optimization is done sequentially, treating each layers activation (before ReLU) as a linear transformation output.\n\nThe method is compared with existing methods (only one existing approach for the problem, GDUAP by Mopuri et al. 2018) in terms of the fool rate. It shows significant improvement. Ablation study is carried out to compare with baselines like perturbation maximizing only first layer activation, only last layer activation, etc. Also on some other settings (black-box testing, less data) the proposed method outperforms GDUAP. \n\nThe problem of data-free white-box attack is very interesting and does make sense. The proposed method achieve significant improvement over the previous one (GDUAP). I do have the following concerns though.\n\n1), the novelty of the proposed idea seems relatively limited. The proposed idea seeks perturbation maximizing activations over all layers. It incur perturbation before ReLU. But overall, the flavor of the idea is not significantly different from GDUAP, despite the significant performance boost. \n\n2), it was mentioned that compare with GDUAP, this paper has more theoretical analysis. But this is not very convincing to me. There are many steps of approximation/relaxation from the original problem (Equation (1)) to the final formula (Equation (10)). Many assumptions are made over the steps. It is OK to use these steps to derive a heuristic. But these steps can hardly be called 'theoretical analysis'.\n\nI am particularly uncomfortable with Equation (5), which is the basis of the main idea. It assumes that all data in $W_1X$ are in the same orthant as $W_1p$. But this is unrealistic as different data in X will for sure incur different activation patterns. Did I misunderstand anything?\n\n3) I do like the experimental results. It looks impressive. But the baselines are really limited (granted, there are not many existing approaches). There is only one task (image classification). How about other tasks like segmentation etc shown in Mopuri et al. 2018? Also it would be nice to also show the results of other UAP methods, as it gives us a better sense of the gap between with and without data.\n\n4) I wonder how will the attack affect some model which has been trained with some defense mechanism, e.g., adversarial training.\n\nTypo:\nEquation (5), RHS missing a max\n\n", "belong_id": "HJxVC1SYwr"}, {"uid": "ByeI1QS99B", "paper_title": "Crafting Data-free Universal Adversaries with Dilate Loss", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a data free method for generating universal adversarial examples. Their method finds an input that maximizes the output of each layer by maximizing the dilation loss. They gave a well motivated derivation going from the data matrix, the data mean and to data free. The experiments results seems solid as the numbers show that their method is much better in many cases.\n\nI have 2 main issues:\n\n* The fooling rate experiments does not seem to control for how much distortion there really is. How do you make sure that different methods have similar level of distortion and not just similar l_\\inf.  Given that the authors says most of their method saturates all values, it is not clear that the baselines and competition really has a similar level of distortion. The fooling rate for random seems rather high. Why is random noise not mostly ignored by the model?\n\n* while the method is data free. It needs complete access to the model and relies on properties of ReLu. I am not sure how realistic this setting is, and how this compares to methods that has black box access to the model. While it is interesting, the paper did not establish that universal adversarial perturbation is well-motivated and why data free is more important that model free or targeted perturbations.  An attacker probably always see the input and probably wants to make it misclassified into a particular class, instead of just making the model wrong. \n\n ", "belong_id": "HJxVC1SYwr"}, {"uid": "r1xcIWR35r", "paper_title": "Crafting Data-free Universal Adversaries with Dilate Loss", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThis paper proposed a method to generate universal adversarial perturbations without training data. This task is timely and practical. The proposed method maximizes the norm of the output before nonlinearity at any layer to craft the universal perturbation. A sequential dilation algorithm is designed to calculate UAPs. The experiments show that the proposed method outperforms GDUAP.\n\nMy major concern is that there is not much novelty in the proposed method compared with GDUAP. The dilate loss function (4) is similar to the objective function (3) in the GDUAP paper. This paper provides a theoretical explanation of the dilate loss function and an improvement on the non-linearity function, which, however, is not convincing. Equation 10 is derived based on many strong assumptions. See the comments below.\n\nPros:\n-\tThe theoretical analysis is clear.\n-\tThe proposed method performs better than GDUAP in the data-free and black-box setting.\n-\tThe writing is good. The paper is easy to follow.\n\nCons:\n-\tThe theoretical analysis is based on many strong assumptions/criteria. For example:\no\tTo derive equation (5), W1X and W1p must be in the same orthant. It is unclear how to satisfy the criteria In the algorithm. \no\tIn Lemma 1, problem (5) approximates problem (6) only if x has a very large projection on the first singular vector of W. However, x and W are fixed and independent of p. This assumption largely depends on the dataset and the weights of the model.\no\tIt would be better if the authors show that in what cases these assumptions can be satisfied.\n-\tOther factors such as batch normalization and max pooling used in Inception v3, may also affect the linearity of the model. It would be better if the authors provide theoretical analysis or an ablation study on these factors. \n-\tWhats the design principle behind Algorithm 1? Why can this algorithm solve the sub-optimal problem? The weights of different layers are not closely related. In the initialization part, why can we start learning p from the result of the previous layer? Would it be possible that the performance is improved due to the algorithm instead of the dilate loss?\n-\tThe proposed method performs worse than GDUAP does in some less data settings.\n-\tThe results in Table 4 and 5 are inconsistent. These two experiments use the same dataset (Imagenet) and the same number of images (D=64).\n\n", "belong_id": "HJxVC1SYwr"}, {"uid": "rylKU1IVKr", "paper_title": "Neural ODEs for Image Segmentation with Level Sets", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to utilize Neural ODEs (NODEs) and the Level Set Method (LSM) for the task of image segmentation.  The argument is that the NODE can be used to learn the force function in an LSM and solve the contour evolution process. The authors propose two architectures and demonstrate promising performance on a few image segmentation benchmarks. \n\nWhile I like the attempt to combine deep learning with traditional CV algorithms, I do not think this submission is suitable for acceptance. My major critique is that both the two proposed models (Figure 2) are essentially two fully differentially NODE based architecture trained in a purely supervised way (by minimizing MSE). This makes the flavor of the proposed methods drastically different from what an LSM does, whereas the latter heavily rely on rich priors embedded in the design of the force function. To be more specific, the image evolution method (Figure 2(b)) essentially has nothing to do LSM. The contour evolution method bares more similarity to LSM but only in the sense that it learns to iteratively refine a contour estimation with an NODE. In the experimental evaluations, it seems that the image evolution model works favorably compared to the contour evolution method. This suggests that the main benefit of the proposed method comes from applying an NODE based architecture to a supervised learning task, rather than the inductive prior brought by LSM.\n\nGiven this, I think a much more proper way of presenting this work should be from the view of applying an NODE to the supervised image segmentation task. This reduces the novelty but increases clarity, and may still make the work a useful empirical reference for these benchmarks.\n\nSome more detailed comments:\n1. Equation 3&4 are not super easy to follow. For example, \\gamma^{(1)} is not defined or explained in main text.\n2. A related work section will be useful, especially for readers who are not familiar with LSM and NODE related literature.\n3. Experiments with more careful control needed. For example, from Table 1, it seems that the baseline Unet 15M performs inferior to the 5M model (explanation needed too), while Table 2 only compares against the 15M model. This makes it difficult to interpret the results.", "belong_id": "r1gNLAEFPS"}, {"uid": "BJx6B8qpYS", "paper_title": "Neural ODEs for Image Segmentation with Level Sets", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to apply the Neural ODE framework (Chen et al 2018) for image segmentation. The method relies on contour delineation through Level Sets. Since contour estimation requires to solve an ODE, this naturally allows to apply the work presented in (Chen et al 2018). The method is here applied in two segmentation tasks: kidney segmentation and salient object detection.\n\nThe concept underlying the paper is interesting, and leverages on very recent advances in the field. The idea of learning the dynamics required to evolve segmentation contours is original and certainly appealing. Unfortunately the content of this work seems quite preliminary in terms of presentation and experiments.\n\nFirst, the methodology is only sketched, while the motivation underlying the modelling rationale is often missing. For example, it is not clear what is the difference between 'image evolution' and 'contour evolution' models, besides the implementation details, and what motivates the definition of these two different modelling approaches in parallel. \nSecond, the experimental paradigm is controversial. The application on medical imaging is overly simplistic, as the authors do not consider the original 3D image stack, but rather the set of corresponding 2D slices modelled independently. The paper seems to ignore the large variety of body organ segmentation methods already available to the community, most of them working in 3D (e.g. [1-4]). The paper should necessarily compare with respect to these approaches and, even more importantly, with respect to standard level sets methods. \n\nFrom the practical perspective, the proposed method builds upon the results obtained with the UNet, and therefore is characterised by an additional computational burden. Given that the the training of neural ODE is not straightforward and computational expensive, the use of this model for achieving a tiny accuracy improvement seems overkill for this kind of application. Moreover, the segmentation accuracy is still computed slice-by-slice in the 2D images, and no information is available for the consistency of the reconstruction in 3D (regularity over the vertical axis).  \n\nFinally, the results reported in Table 3 are not clear, why the metrics of the competing methods are approximated (e.g. ~ 0.7), while for the proposed methods are given up to the 3rd decimal term?\n\n\n[1] 3D Kidney Segmentation from Abdominal Images Using Spatial-Appearance Models. Fahmi Khalifa, Ahmed Soliman, Adel Elmaghraby, Georgy Gimel'farb, and Ayman El-Baz 1. \n\n[2] Automatic Detection and Segmentation of Kidneys in 3D CT Images Using Random Forests. Remi Cuingnet, Raphael Prevost, David Lesage, Laurent D. Cohen, Benoit Mory, Roberto Ardon. Medical Image Computing and Computer-Assisted Intervention  MICCAI 2012: 15th International Conference, Nice, France, October 1-5, 2012, Proceedings, Part III\n\n[3] Multi-organ localization with cascaded global-to-local regression and shape prior. Medical image analysis. Gauriau, R., Cuingnet, R., Lesage, D., & Bloch, I. (2015), 23(1), 70-83.\n\n[4] Joint Classification-Regression Forests for Spatially Structured Multi-object Segmentation. Ben Glocker, Olivier Pauly, Ender Konukoglu, Antonio Criminisi. ECCV 2012 pp 870-881", "belong_id": "r1gNLAEFPS"}, {"uid": "SyljiI4e5r", "paper_title": "Neural ODEs for Image Segmentation with Level Sets", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes to integrate Neural ODEs into image segmentation using level sets. I think the paper makes a good methodological contribution, however I don't think that ICLR is the best conference to publish this as the topic (image rather semantic segmentation) is too narrow and in my humble opinion, it won't attract the interest of ICLR audience. Moreover, it seems to me that the saliency object detection experiment is not a very convincing one as the methods compared are a bit old (mainly from 2015-2016).  I strongly recommend the authors try to publish this at MICCAI focusing on kidney segmentation or any other related medical imaging application.", "belong_id": "r1gNLAEFPS"}, {"uid": "rylPJuDatH", "paper_title": "Unsupervised Learning of Automotive 3D Crash Simulations using LSTMs", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary.\nLong short-term memory (LSTM) networks are trained to learn the underlying dynamics of 3D structures -- An encoder LSTM encodes a sequence of 3D mesh representations and two decoder LSTMs reconstruct the input itself and predict the future structural geometry. Such structural deformation sequences are collected from a simulation and used to train and evaluate the model. \n\nStrengths.\n1. Deep neural network architecture is applied to learn a 3D mesh structure deformation. Its approach looks reasonable to me -- (i) a high-level (user-specified) feature is extracted to represent 3D structural mesh, and (ii) LSTMs are trained in this feature space. \n\n2. The paper shows that LSTM can learn the underlying dynamics of 3D structural deformation (but since it does not provide any comparison with other work, I cannot determine the prediction error is within a reasonable bound for this task).\n\nWeaknesses.\n1. Weak technical and theoretical novelty.\nAn existing LSTM-based seq-to-seq style architecture is simply chosen and applied to the 3D mesh reconstruction and prediction task. LSTM networks have been successfully applied to various tasks, thus it would be less impactful even if a paper shows LSTM works well in the specific task. \n\n2. Missing comparison with existing work.\n(As far as I know) the task of predicting 3D structure deformation has long been studied and various simulation and analysis tools have widely used in academia and industries. The paper, however, does not thoroughly compare or cite any prior work in this area. As a reader from a different area, I would like to see more thorough analysis to support (i) what are the main bottlenecks of the conventional approaches, (ii) how the proposed data-driven approach can help to address these issues, and (iii) how the proposed approach can further extend.\n\nFor example, the paper mentions (as the main reason to use the deep neural network) that Since one simulation run takes a couple of hours on a compute cluster, running a large number of simulation is not feasible (in the 1st paragraph of Introduction). Can the authors provide an analysis of flops (floating-point operations per second)?\n\n3. Simulated data for training. \n205 samples are collected from a simulation to train and validate the proposed model and half an hour has taken to train this model. Judging from the common practices, does this imply the underlying dynamics function (of a simulation) is trivial to learn? \n\nAlso, only two parts (i.e. the left and right structural beam) of the whole 3D structure are analyzed. This implies that relations between any other parts are not considered and only a sole part is individually analyzed.\n\nMinor concerns.\n- Typo (In Figure 1 caption, undo).\n- Due to the encoder having 1000 layers?\n- Labels are needed in Figures.", "belong_id": "BklekANtwr"}, {"uid": "SJxVoXsJqr", "paper_title": "Unsupervised Learning of Automotive 3D Crash Simulations using LSTMs", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents an LSTM model to predict the evolution for 3D models for crash tests. The core model is a video prediction model (Srivastava et al. 2015). Instead of using the original 3D geometry, the authors propose to predict a feature representation based on prior work (Brezis & Gomez-Castro 2017, Bohn et al. 2013, Teran & Garcke 2019).\n\nStrength:\n + Interesting problem\n + Good writing\nAreas of improvement:\n - Technical contribution\n - Experiments\n\nThe paper is quite easy to read, although it would help to clearly separate prior work from the actual technical contributions. The problem of simulating 3D meshes instead of raw pixels is interesting.\n\nMy main issue with the paper was that I had a hard time seeing a contribution. It is unclear what is technically new, or if everything was just copied from prior work. It was also unclear which parts of the technical approach mattered? Is the LSTM the right solution, or are there other simple baselines to compare to?\n\nFinally, the experiments are not very intuitive. It is unclear what impact the presented technique has on the down-stream tasks of crash testing. Does it improve the current simulation of crash testing? Is the prediction error acceptable to replace certain safety tests?", "belong_id": "BklekANtwr"}, {"uid": "HyeC3PS8cr", "paper_title": "Unsupervised Learning of Automotive 3D Crash Simulations using LSTMs", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to use autoencoder LSTM to learn car 3D crash simulation in an unsupervised manner.\n\nBasically, it proposes to apply autoencoder LSTM, which is an existing and well-studied technique, to a very special task of car crash simulation. Neither a new method nor an in-depth investigation of the task itself is carried out. It is hard to identify contributions that are significant enough to support the publication in top conferences such as ICLR. Considering the fact that autoencoder LSTM is wildly employed in solving various unsupervised learning tasks, readers can hardly acquire knowledge in terms of novelty.", "belong_id": "BklekANtwr"}, {"uid": "B1eIQQVUtS", "paper_title": "Making Sense of Reinforcement Learning and Probabilistic Inference", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper criticizes the RL as inference paradigm by highlighting its limitations and shows that a variant of this framework - the K-learning algorithm (O'Donoghue et. al., 2018) does not have these limitations. The paper first clarifies some points of confusion regarding RL as inference, namely the fact that RL was originally an inference problem all along. A simple example is used to demonstrate that the RL as inference framework (Levine, 2018) fails to choose the optimal actions that resolve epistemic uncertainty, whereas the K-learning algorithm does select the optimal action. Further, a connection is made which reveals that K-learning is an approximate version of Thompson sampling - the strategy of using as single posterior sample of parameters given data for greedy actions which originated in bandit settings. Some empirical results are provided highlighting the cases where Soft Q-learning (Levine, 2018) fails but Thompson sampling and K-learning do not.\n\nI vote for accepting this paper as it brings to light an important limitation of the popular RL as Inference framework with a didactic example which, to the best of my knowledge, has not been shown before.\n\nThe paper does a great job at succinctly introducing a simple bandit problem where the bayes-optimal policy is to take a first action that is supposed to immediately resolve all epistemic uncertainty and then exploit the optimal action repeatedly for future plays. However, this simple problem is designed in such a way that there are several other sub-optimal actions which make the RL as inference algorithm have an exponentially low probability of selecting the optimal action. This implies that RL as inference, unlike Thompson sampling, does not in fact take into account epistemic uncertainty.\n\nFeedback to authors:\n- The introduction of family of MDPs caused a lot of confusion about the problem setting. I was not sure if a new MDP is sampled from \\phi at every episode in L or a single MDP is sampled and kept the same throughout. This was clarified later on in the middle of section 2.1, but it could have been introduced more carefully earlier on,\n- The tables 1-3 summarizing algorithms are useful but it would be great if there could be a side by side comparison of all three in a single table.\n- The notation is very dense and I see that efforts were made to avoid this, but it still feels inaccessible.\n- I am not sure of the role of experiments in section 4.3, if there is no comparison to K-learning. I understand that the authors leave it to future work but then the experiments feel out of place.\n- ... RL as inference has inspired many interesting and novel techniques, as well as delivered algorithms with good performance on problems where exploration is not the bottleneck (Gregor et al., 2016). I think this sentence is false, Gregor et. al. do not employ RL as inference anywhere in their paper. Also, I dont think the point of their paper was to show good performance on any problem. Maybe this was mixed up with Eysenbach, 2018, a successor paper which uses RL as inference?\n\n \n\nReferences:\nO'Donoghue, Brendan. 'Variational Bayesian Reinforcement Learning with Regret Bounds.' arXiv preprint arXiv:1807.09647 (2018).\n\nLevine, Sergey. 'Reinforcement learning and control as probabilistic inference: Tutorial and review.' arXiv preprint arXiv:1805.00909 (2018).\n\nGregor, Karol, Danilo Jimenez Rezende, and Daan Wierstra. 'Variational intrinsic control.' arXiv preprint arXiv:1611.07507 (2016).\n\nEysenbach, Benjamin, et al. 'Diversity is all you need: Learning skills without a reward function.' arXiv preprint arXiv:1802.06070 (2018).", "belong_id": "S1xitgHtvS"}, {"uid": "S1gAoVaTYH", "paper_title": "Making Sense of Reinforcement Learning and Probabilistic Inference", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper at hand presents an alternative view on reinforcement learning as probabilistic inference (or equivalently maximum entropy reinforcement learning). With respect to other formulations of this view (e.g. Levine, 2018; I am referring to the references of the paper here), the paper identifies a shortcoming in the disregard of the agents epistemic uncertainty (which seems to refer to the uncertainty with respect to the underlying MDP). It is argued, that algorithms based on the prevailing probabilistic formulation (e.g. soft Q-learning) suffer from suboptimal exploration.\nThe paper thus compares maximum entropy RL to K-learning (ODonoghue, 2018), which is taken to address the issue of suboptimal exploration due to its temperature scheduling and its inclusion of state-action pair counts in the reward signal. \n\nAs its technical contribution, the paper re-interprets K-learning via the latent variable denoting optimality employed in Levine (2018) and introduces a theorem bounding the distance between the policies of Thompson sampling and K-learning. Empirical validation of the claims is provided via experiments on an engineered bandit problem and the tabular MDP (i.e. DeepSea from Osband et al., 2017), as well as via soft Q-learning results on the recently suggested bsuite (Osband et al., 2019).\n\nI consider this paper a weak reject. This is in light of me finding it very hard to follow the papers main claims and arguments, even though it positions itself as communicating connections (making sense) in prior work, rather than presenting a novel algorithm. While this is in part due to the complicated issue and math being discussed (and the paper probably catering to a very narrow audience), the paper in its current state does seem to hinder understanding as well.\n\nOn the positive side, I do appreciate the intention of the paper, namely to connect RL as probabilistic inference, Thompson sampling and K-learning. In my opinion, this can be taken as a valuable addition to the current understanding of these approaches. Also, I like the experiments as they are specifically constructed to support the claims of the paper.\nOn the negative side, vague language, missing assumptions and lax notation seem to hinder the understanding of the paper to a considerable extend: e.g. it is stated, that we connect the resulting algorithm with [...] K-learning. However, I do not recognize a new algorithm being provided. Instead the paper argues in favor of K-learning. The assumptions that come with K-learning are not mentioned. The restriction of K-learning to tabular RL is taken to be understood implicitly (whereas RL as probabilistic inference seems applicable with function approximation also, which is not mentioned in the comparison). The paper always talks of shortcomings (plural) of RL as probabilistic inference, but only provides one argument (suboptimal exploration) with respect to this. RL as probabilistic inference is introduced in a different form as in prior literature (i.e. Equation 6), while the derivation in the Appendix spanning the differences in notation being hard to follow due to (maybe minor?) notational issues (e.g. x and y seem to have replaced s and a; further down there is a reference to Equation 7, however probably it is meant to be 8 and even that with some leap in notation).\nThe paper would benefit from better proof-reading, where mistakes in a very dense argumentation make it hard to follow (e.g. I do not understand the sentence The K-learning expectation (7) is with respect to the posterior over Q[...] to give a parametric approximation to the probability of optimality.)\n\nLiterature wise, the paper draws heavily from two unpublished papers (Levine, 2018; ODonoghue, 2018). While this makes it harder to arrive at a high confidence level with respect to the papers claims, I would not argue this to be critical.\nI would consider raising my score, if the authors would improve the accessibility of the paper by polishing the argumentation and notation. \n\nConfidence: low. It is very likely, that I have misunderstood key arguments and derivations. Also, I did not attempt to follow all of the technical derivations.\n\n\n======\npost rebuttal comment:\n\n\nI changed the score of my review in light of the rebuttal.\nThe changes made to the paper overall address my concerns.\nI do consider the additional explanations and re-phrasings as well as the improved notation a nice improvement of the paper.\nWhile I did not read all of the appendix, Section 5.1 is much more readable and understandable in the new version.\n\nIn light of this paper probably being published, I share some typos/inconsistencies I still noticed:\n\np. 4: the solving for -> solving for the\np. 7: s_{h+1} -> s' (in Table 3) ?\np. 7: table -> Table; tables -> Tables\np. 7: (Fix position of K:) \\pi_h(s)^K -> \\pi_h^K(s) ((also in Appendix))\np. 9: (2x) soft-Q learning -> soft Q-learning; Q Networks -> Q-Networks; Soft Q -> soft Q-learning\n\n\n", "belong_id": "S1xitgHtvS"}, {"uid": "ryez-G1Lqr", "paper_title": "Making Sense of Reinforcement Learning and Probabilistic Inference", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors develop a criticism of the 'RL as inference' standard approximations and propose a simple modification that solves its main issues while keeping hold of its advantage. Even though this modification ends up relating to a previously published algorithm, I judge the submission to be worthwhile publishing for the following contributions:\n- clarity/didacticism of the exposition, the minimal problem, the positioning,\n- the theorem,\n- the (hopefully to be completed) experiments\n\nThe experiments are my main criticism of the paper, in particular the bsuite ones that was absolutely impenetrable for me: not only the experiments but also the results. I hope this will be completed in the final version. It was also a bit unclear to me the advantage of K-learning over Thomson sampling methods.\n\nMinor remarks and typos:\n- famliy => family\n- I would not say that frequentist RL is the worst-case, but more high-probability (it's the worst case within the concentration bounds).\n- the agent in then => the agent is then\n- KL has 2 meanings in the notations: K-learning and KL divergence. For clarity, I suggest to use only K for K-learning (for instance).", "belong_id": "S1xitgHtvS"}, {"uid": "Hyg3vLW9Fr", "paper_title": "A multi-task U-net for segmentation with lazy labels", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The submission presents a neural network for multi-task learning on sets of labeled data that are largely weakly supervised (in this case, partially segmented instances), augmented by comparatively fewer fully supervised annotations.\nThe multiple tasks are designed to make use of both the weak as well as as full (strong) labels, such that performance on fully annotated machine-generated output is improved.\n\nAs noted in the related work section (Section 2), multi-task methods aim to use benefits from underlying common information that may be ignored in a single-task setting. The network presented here is quite similar to most of these multi-task approaches: a common feature encoder, and partially distinct feature decoding and classification parts.\nThe (minor) novelty mainly comes from the distinct types of weak/strong annotation data fed here: instance scribbles, boundary scribbles, and (some or few) full segmentations. \n\nThe submission is overall well written and provides sufficient clarity and a good overview of the approach.\nSection 3 presents a probabilistic decomposition of the proposed architecture. With some fairly standard assumptions and simplifications, the loss in Eq. 3 becomes rather straightforward (weighted cross entropy)\nThe actual network architecture described in Section 3.2 takes a standard U-Net as a starting point and modifies it in a fairly targeted way for the different expected types of annotations. These annotations (Section 3.3) are cheaper than full labels on a same-size dataset; it is not completely clear, however, if the mentioned scribbles need to capture each instance in the training set, or if some can also be left out. Without this being explicitly mentioned, I will assume the former.\n\nThe experimental evaluation is done reasonably well, although I am not familiar with any of the presented data sets. The SES set seems to be specific to the submission, while the H&E data set has been used at least one other relevant publication (Zhang et al.). My main issue here is that at least on the SES set, which does not seem to be that large, the score difference is not that big, so dataset bias could play some part (which is unproven, but so is the opposite).\nExperimental evaluation does not leave the low-number-of-classes regime, and Im left wondering how the method might compare on a semantically much richer data set, e.g. Cityscapes. Finally, unmodified U-Net is by now a rather venerable baseline, so Im also wondering how the proposed multi-task learning could be used in other (more recent) architectures, i.e. whether the idea can be generalized sufficiently.\n\nWhile I think the ideas per se have relatively minor novelty, the combination seems novel to me, and that might warrant publication.", "belong_id": "r1glygHtDB"}, {"uid": "HkgaxjChYH", "paper_title": "A multi-task U-net for segmentation with lazy labels", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method for semantic segmentation using 'lazy' segmentation labels. Lazy labels are defined as coarse labels of the segmented objects. The proposed method is a UNET trained in a multitask fashion whit 3 tasks: object detection, object separation, and object segmentation. The method is trained on 2 datasets: air bubbles, and ice crystals. The proposed method performs better than the same method using only the weakly supervised labels and the one that only uses the sparse labels.\n\nThe novelty of the method is very limited. It is a multitask UNET. The method is compared with one method using pseudo labels. However, this method is not SOTA. Authors should compare with current methods such as:\n - Where are the Masks: Instance Segmentation with Image-level Supervision\n - Instance Segmentation with Point Supervision\n - Object counting and instance segmentation with image-level supervision\n - Weakly supervised instance segmentation using class peak response\n - Soft proposal networks for weakly supervised object localization\n - Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation\nThese methods can use much less supervision (point-level, count-level or image-level) and may work even better.\n\nThe method should be compared on standard and challenging datasets like Cityscapes, PASCAL VOC 2012, COCO, KITTI...\n\n", "belong_id": "r1glygHtDB"}, {"uid": "B1g4ENhV9r", "paper_title": "A multi-task U-net for segmentation with lazy labels", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThis paper addresses the problem of learning segmentation models in the presence of weak and strong labels of different types. This is an important problem that arises while learning in data-scarce settings. The presented approach optimizes jointly over the various types of labels by treating each one as a task. They extend the U-net architecture to incorporate the different tasks.\n\nPrior work:\nThere has been other work on incorporating multi-resolution or different types of labels. Here is one that can be cited:\nLabel super-resolution networks (https://openreview.net/forum?id=rkxwShA9Ym)\n\nMajor comments:\n- The motivation for the specific structure of the multi-task blocks is not clear\n- The object boundaries labels can be noisy (i.e s(2) can have noise). How does model deal with this?\n- Is it the case that every image in I_3 is completely labeled - i.e all segments/classes marked?\n- The assumption that s(3) is independent of s(1) and s(2) is not true. Instead of constraining the model to learn masks that respect the various types of labels, it seems they learn from each source independently. It is not clear how the sharing of parameters in the multitask block helps.\n- Can they comment on the applicability of the prior work suggested above?\n\nMinor comments:\n- How do the rough labeling tools work on biomedical data where the objects are more heterogenous patterns where different labels can have very different distribution of pixels. How well will their method generalize in such settings?\n- Can this work be used for segmentation and prediction on crop data?\n\nResults:\n- It seems as if the improvement over the PL baseline (pseudo labels) is incremental? Can the authors provide error bars so the reader knows what the significance of the results is?\n- Can they give a more thorough comparison in terms of human effort? It is interesting to note that only 2 images give 0.82. Would 3 images give 0.94? They need to show the trade-off between additional effort vs gains in performance.\n- What is the performance of MT U-net without the SL images (i.e without task-3)? Table-2 does give some intuition, but authors should add another row with multitask WL\n- Table-3: How well does MDUnet do with 9.4% SL data?\n\n\n", "belong_id": "r1glygHtDB"}, {"uid": "rkeeRAvCtB", "paper_title": "Visual Interpretability Alone Helps Adversarial Robustness", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Interesting work and good contribution\n#Summary:\nThe paper demonstrated that by having an l1-norm based 2-class interpretability discrepancy measure, it can be shown both empirically and theoretically that it is actually difficult to hide adversarial examples. Furthermore, the authors propose an interpretability-aware robust training method and show it can be used to successfully defend adversarially attacks and can result in comparable performance compared to adversarial training.\n\n#Strength\nThe paper is well written and structured, with a clear demonstration of technical details. Compared with other works that tried to use model interpretation to help improve the models robustness, the authors not only consider the saliency map computed for the actual target label but also the label that corresponds to the adversarial example. The proposed interpretability discrepancy measure is novel and has been proven effective to defend interpretability sneaking attacks that aiming to fool both classifiers and detectors and against interpretability-only attacks. Furthermore, extensive experiments have been done to prove the effectiveness of interpretability-aware training, which strengthens the claims of the entire paper.\n\n#Presentation\nGood coverage of the literature in both adversarial robustness and model interpretation.\nSome minor typos need to be fixed. For example, in the second last line of the caption of Figure. 2, one L(x,i) should be L(x,i) if I understand correctly. ", "belong_id": "Hyes70EYDB"}, {"uid": "S1eSBR_CtH", "paper_title": "Visual Interpretability Alone Helps Adversarial Robustness", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The present work considers adversarial attacks that also yield similar outputs for 'interpretability methods', which are methods that output some vector corresponding to a given classification (usually the vector is e.g. an image or a similar object). It also shows that by regularizing nearby inputs to have similar interpetations (instead of similar classifications), robustness can be achieved similar to adversarial training. \n\nI did not understand the motivation of the paper. Why is it important for adversarial attacks to yield similar interpretations? A human would need to assess the interpretations to detect the attack, but it would already be trivial for an attack to be detected given human oversight (just check whether the classification of the image matches the human-assigned label). It also wasn't clear how this was related to the other observation that regularizing based on interpretability yields robustness; these seem like two fairly separate results.\n\nFinally, I found the claim that 'interpretability alone helps robustness' to be misleading and not substantiated by the paper. The purported justification is that regularizing nearby inputs to have the same interpretation yields robustness. But a better summary of this observation is that 'robustness of interpretability implies robustness of classification', which is not surprising, and is in fact a trivial corollary of the fact that the metric on interpretations dominates the classification error metric (an observation which is made in the paper).\n\nMore minor, but I found it hard to follow the writing in the paper (this is related to the motivation being unclear). This is exacerbated by the paper being longer than unusual (10 pages instead of 8).", "belong_id": "Hyes70EYDB"}, {"uid": "rJlQeu8GiB", "paper_title": "Visual Interpretability Alone Helps Adversarial Robustness", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In summary, this paper studies if interpretation robustness (i.e., similar examples should have similar interpretation) can help enhance the robustness of the model, especially in terms of adversarial attacks. The study direction itself is interesting and very useful for the interpretation and adversarial attack community. Moreover, some promising results can be observed in part of the empirical study. However, this paper can be improved a lot as follows.\n\n1. This paper states several times that 'adversarial examples can be hidden from neural network interpretability'. It is not clear on the definition of 'hidden' in terms of  'interpretability'. Therefore, how this 'hidden' is related and why this 'hidden' is important are unclear too.\n\n2. Many details are missing, which makes the proposal suspicious. For example, the proposed method has a tradeoff parameter \\lambda. However, the settings and affects are not discussed at all. Without a clear setup, the reproducibility and applicability is in doubt.\n\n3. Some empirical results are overstated. For example, why 0.790 vs 0.890 and 0.270 vs 0.170  are comparable results? These results show the weakness of the proposed method. Further explanations can be provided. From the reported results, it could be useful to see results when the perturbation is even higher to check the limitation of the proposed method.\n\n4. Besides the clarification in the writing mentioned above, some typos or errors should be fixed, e.g., f_t'(x') - - f_t(x') >=0 in the proof of proposition 1.", "belong_id": "Hyes70EYDB"}, {"uid": "H1xiUtOFFS", "paper_title": "Discourse-Based Evaluation of Language Understanding", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper presents a new GLUE-like dataset collection called DiscEval, which focuses on discourse and pragmatics. Like GLUE and SuperGLUE, datasets from existing works are collated and formated. The tasks are all classification tasks.\n\nThe paper also evaluates several baselines including bag-of-words, BiLSTM encodings, and BERT fine-tuned on different types of data, which have shown success in GLUE tasks.\n\nLike other NLP benchmarks, the DiscEval benchmark would be a good resource for other researchers to hill-climb their systems on, provided that the data format is standardized and the submission system is easy to use like GLUE.\n\nThat said, the paper has some rooms for improvement:\n\n- With the information in Table 2, it is hard to judge the difficulty and headroom for each task. Only a few tasks have human evaluation scores were estimated from the inter-annotator agreement. Contrast this to the GLUE and SuperGLUE papers which provide human baselines from actual humans. Without these anchoring numbers, it is hard to see if the remaining gap is due to the model's inability to model discourse, or due to noise in the dataset.\n\n- Providing the best single-task result from previous work would also help give a more complete picture.\n\n- With the result of fine-tuned BERT almost matching the human performance in several tasks, the argument that BERT is not a universal representation (abstract + introduction) is weakened somewhat.\n\nAs a valuable resource for other researchers, I am still leaning toward acceptance despite the issues above.\n\nOther comments:\n\n- The bullet points on Page 5 could be clarified. Currently, the first bullet seems to contain multiple groups, for example.\n\n- Sentiment analysis (in GLUE) could be viewed as a discourse task. It would be nice to be a bit more upfront about it.", "belong_id": "B1em8TVtPr"}, {"uid": "SyxE6vl0tS", "paper_title": "Discourse-Based Evaluation of Language Understanding", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "- Overview: This work presents a benchmark for language understanding centered on understanding pragmatics and discourse, in contrast to existing NLP benchmarks which focus on mostly semantic understanding. The stated contributions are\n    - The DiscEval benchmark, with some estimates of human performance\n    - Baseline performance with state of the art models\n    - Comparisons between commonly used supervised training objectives (NLI) and discourse-oriented objectives\n\n- Review: This work offers a complementary evaluation benchmark for NLU systems to what currently exists in the field, and also offers compelling evidence that the current methods used do not provide good signal for learning the types of phenomena assessed in this benchmark. I recommend (weak) accept.\n    - The paper would benefit a lot from having more in-depth explanation of the tasks, what the classes are, and what the classes mean. It's not always clear from the examples or the labels in Table 1 what exactly is being tested.\n        - Additionally, it'd be nice to get the source of the data for each task.\n    - The rough grouping of tasks in nice.\n    - I understand that estimates of human performance, especially for tasks with a high number of classes, can be tricky to obtain. I do feel that they are especially important to have for this dataset. As you said, some of these tasks rely on context that isn't explicitly provided in the utterance, so I wonder if for some of the tasks, there may be insufficient context in just the provided utterances for humans to perform well.\n        - Additionally, some tasks seem to have quite subjective label definitions. For example, 'eloquence' and 'specificity' in Persuasion; 'formality' and 'informativeness' in Squinky; etc. Validating that humans are consistent on these tasks seems crucial.\n        - For the estimates you do have, are these expert or lay annotators? For some of these annotations, it seems like you'd need trained annotators.\n    - Have you given any thought to exploitable data artifacts that may occur in these datasets that might be driving the fairly high performance on some of these tasks (e.g. Verifiability)?\n    - I think it's quite interesting that fine-tuning on MNLI doesn't lead to good performance on DiscEval, as MNLI, as the authors point out, is commonly taken to be a useful pretraining task. This discrepancy gives practical weight to the authors' claim that discourse and pragmatic phenomena are not being sufficiently studied or evaluated for in current NLP research, despite the fact that these handling these phenomena will be crucial for NLP systems.\n\n- Things to Improve + Questions + Comments\n    - I would hope that most people in the NLP community would not say that language understanding is a solved problem, but I agree that putting 'universal' in model names and paper titles is a reach-y thing to do.\n    - Tasks\n        - What is the original citation for STAC?\n    - S4.3, Table 2: \n        - Any reason not to try combining all four pretraining/intermediate training tasks in a single model, or at least more combinations of DiscEval with other things?\n        - Could you comment on the standard deviation of the scores (per task) given that you're averaging 6 runs?\n    - Table 3\n        - 'The best overall unsupervised result is achieved with Discovery STILT': what does unsupervised mean here? It also doesn't look like BERT+Discovery is the best in any column here.\n    - Tables 4 and 5 feel like a bit of a dump of numbers. It'd be more useful to the readers to extract trends (probably using the groupings and theoretical frameworks introduced earlier). The noting of BERT+MNLI being good at predicting absence of relations is nice.\n    - Typos: There are a noticeable number of typos. Here are some I noted:\n        - The formatting of the task descriptions in Section 3 is a bit inconsistent and awkward.\n        - Table 1: 'exemple'\n        - P4: 'We use a custom train/dev validation split'\n        - P5: 'that help *realize* speech acts' intentions.'\n        - P6: 'Prediction of discourse markers based *off* the context clauses...'\n", "belong_id": "B1em8TVtPr"}, {"uid": "SkeKZ0WjqH", "paper_title": "Discourse-Based Evaluation of Language Understanding", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a new benchmark for natural language understanding called DiscEval. The benchmark focuses on datasets that more directly measure a model's understanding of the discourse structure and relations in the text. The paper makes direct comparisons to GLUE and especially studies the previous claims of MNLI being a generically good pre-training task. They find that MNLI pretraining, using STILTS, does not help BERT's performance on DiscEval.\n\nSome comments and recommendations,\n- You say that for PDTB you 'select the level 2 relations as categories,' I believe these are the class level relations. Maybe add in a brief explanation in the paper?\n- Was discarding all but the Strength subclass from the Persuasion dataset and empirically motivated decision or just something you did a-priori?\n- The human results are already hedged, but maybe the grain of salt should be bigger: it occurs to me that the comparison of the model performance and human performance could be unfair since the human performance is reported on the original tasks, before the filtering of data and changing label distribution like with PDTB and GUM.\n- Particularly given the data filtering and restructuring of some of the tasks, getting a rough estimate of human performance for all the datasets would be quite valuable\n- I think saying the MNLI does not help model performance on DiscEval is completely valid but claiming it hurts performance could be a stretch given the margins of error.\n- I know tasks like PDTB and GUM have quite a few classes. Please include the number of classes for each of the datasets in the benchmark. Also including information on the size of the dev and test sets could be helpful.\n\nOverall, I agree with the author's claim that we need to have a broader evaluation suite for NLU, and this benchmark is a step in the right direction. Some of the DiscEval datasets measure NLU information that is somewhat orthogonal to what datasets in GLUE/SuperGLUE measure. I think this will be a useful resource to the research community\n\n\nMinor things,\n- Page 5, last line the CoLA citation is wrong, should be Warstadt et al. (2019)", "belong_id": "B1em8TVtPr"}, {"uid": "rklBAZ8AYH", "paper_title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces a model trained for video prediction hierarchically: a series of significant frames called keyframes in the paper are first predicted and then intermediate frames between keyframes couples are generated. The training criterion is maximum likelihood with a variational approximation. Experiments are performed on 3 different video datasets and the evaluation is performed for 3 tasks: keyframe detection, frame prediction and planning in robot videos.\nThe idea of generating an abstraction or a summary of a video via a sequence of important frames is attractive and could probably be used in different contexts. The proposed model is new and the authors introduce some clever ideas in order to train it. The evaluation work is important and the authors propose different settings for this evaluation. \nThe paper also present weaknesses. First the motivation for keyframes generation should be better developed: the model does not perform better than baselines for video frames prediction so that keyframes generation should be motivated by other applications. Planning as proposed by the authors could be one, but in this case it should be more developed. The main weakness is however the technical presentation which is painful to follow. When it is possible to get a general picture of what is done, it is quite difficult to figure out exactly how the model works. A global rewriting and maybe a better focus are required for publication. The probabilistic model (section 3.1) is relatively clear, even if it could be improved. It seems that the generation of a keyframe and the prediction of the corresponding time (tau^n)  are independent (eq. 3). This could be commented. Also it seems that in eq. 3 the log(K|z..) term should be inside an expectation. Section 4 was difficult to decipher for me. My understanding is that instead of sampling from a multinomial during training, you bypass this non differentiable operation by using what you call soft targets thus obtaining a differentiable objective (eq. 4). Is that true? In any case, the procedure should be made a lot clearer. The intermediate frame passage also remained confuse for me.\nConsidering the experiments, the authors make an important effort in order to evaluate different aspects of their model. In a fisrt step, they evaluate the ability of the model to generate significant keyframes using a detection setting.  It is not clear how they define ground truth frames for this evaluation. Those ground truth frames are defined as the frames where the movement in the image changes, which is easy on the Brownian movement dataset but what about the others? Also the baselines used in this comparison are weak. In the paper of Denton, they suggest some way to detect surprise and apparently this is not what you used. This should be justified/ commented. For keyframe modeling the proposed model behaves similarly to the baselines and even performs worse than the simpler jumpy model. Concerni g the paragraph about the selection of the number of predicted keyframes, it is not clear what is the reference (ground truth) number of target keyframes.\n The planning experiments are interesting, but difficult to follow at least from the main text.\nOverall, I think that there are several interesting ideas and realizations. They should be better put in perspective and explained.\n\n----- post rebuttal -----------\n\nThanks for the detailed answer. The paper is largely improved both for the style and the comparisons. But still requires further improvements. I will keep my score.\n", "belong_id": "BklfR3EYDH"}, {"uid": "SklIUCrW5B", "paper_title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a variational objective to train a model which can jointly select keyframes of a video and generate the intervening frames to produce a resultant video. The model is provided an initial set of frames as context. At training time the model always learns to produce N*J frames, where N is the number of keyframes and J is a fixed number of frames to generate for each keyframe. The authors compare their method for selecting informative keyframes on a number of baselines and show an improvement over these baselines. The problem is interesting and well-motivated, but I have some concerns with the proposed approach and experiments. As such, I am a weak reject.\n\ncomments / questions:\n- Equation 3 lacks context. Initially, when looking at the authors' objective it seems that the inner expectation should be taken with respect to the joint time indices for the current and next keyframe. Only later after equation 4 do they mention that they always predict a fixed number of frames J. \n- The need for normalizing over the first T timesteps in equation 4 seems quite messy. Is it guaranteed that all of the needed keyframes will actually be within the first T timesteps? How does this work in practice?\n- Many important details of the inference procedure are relegated to the appendix. For example, there are no details for extracting which of the 60 keyframes that were trained for a sequence (due to the fixed length sequences) should be selected at test time. Looking at the appendix, it is clear that the approach requires an extensive planning algorithm at inference time, which seems like an important component.\n- The authors prominently highlight that their method is fully differentiable, yet they train in two stages while freezing weights. Why isn't the model trained end-to-end? The stated reason for doing so is that this 'simple' two-stage procedure improves optimization. What exactly happens if you don't do this two stage training process? Does it fail to learn? Some experimental numbers would be nice to see. \n- The authors do not compare their method to any strong keyframe prediction baselines. Considering there is existing work in keyframe prediction, it seems important to highlight the difference between other competing models, rather than relying on simple baselines. Why don't they use self-information/surprisal as a baseline i.e., by training an autoregressive model on the frames and then picking the N frames with the largest -log(p)? This is a metric that has been investigated frequently and has better interpretability than defining a new measure of surprise. Note that Kipf et al. (2019) uses this notion of surprisal as well.\n- Sauer et al. (BMVC 2019) should likely be cited as it does very similar keyframe analysis. Also, as the ICML 2019 conference had already concluded by the ICLR submission deadline, is it really fair to state the work with Kipf et al. (2019) was conducted in parallel?\n- Why does the model trained to learn a fixed number of timesteps for the intermediate frames? Did they investigate jointly predicting the indices for the current and next timesteps? It seems like it would greatly simplify their inference scheme if they did this. If they tried that approach and it failed, maybe that should be mentioned in the paper (with an explanation as to why it fails).\n- In the literature review, when discussing hierarchical temporal structure, the authors state: 'However, these models rely on autoregressive techniques for text generation and are not applicable to structured data, such as videos.' Autoregressive techniques have been investigated in relation to videos; in fact, the authors later describe papers that have used autoregressive techniques for modeling videos.\n", "belong_id": "BklfR3EYDH"}, {"uid": "SJl6wpNVcS", "paper_title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors address the problem of discovering and predicting with hierarchical structure in data sequences of relevance to planning. Starting with the kinds of data that have been used recently in video prediction, the authors aim at learning a sequence of keyframes (i.e., subsets of frames forming the overall sequence) that in a suitable sense 'summarize' the overall trace. As they rightly note, many alternate models struggle with making good long term predictions in part because they focus on all levels of prediction equally.\n\nThe technical approach is to pose the problem as one of inferring the temporal location of each of these key frames and then to interpolate with a model to generate intermediate frames. One could try to make either step sophisticated - the authors choose to make the keyframe selection more sophisticated and interpolation simpler. The paper first described the KeyIn model in terms of a probabilistic model of jointly finding the Ks and then the inpainted Is. This can become delicate, so the authors propose a relaxation that is more forgiving when the keyframe locations are being searched for. Learning is driven by a reconstruction loss of finding the approximate location, locally interpolating and then seeing if this accords with the training data. This is all implemented with an LSTM based NN architecture which seems sound to me.\n\nI feel the paper is taking on the right kinds of questions, looking for ways to inject the right kind of structure. I do have some concerns about the overall formulation:\n\n1. Much of the paper is focussed on rather clean images where nothing extraneous is happening. In reality, the backgrounds of real images is not so benign and other extraneous dynamics might interfere. While I understand this is a step towards the long term goal, I wonder if the end result is a bit too incremental in the absence of some attempt to explore this source of (lack of) robustness.\n\n2. In 6.3, the authors try to demonstrate that the number of keyframes parameter can be wrong by a little bit but these are still small ranges. In realistic images it is likely that the total number of keyframes selected by such an algorithm is much larger due to extraneous events. This is why a proper robustness study is crucial on more realistic input. As it, in anything other than the trivial dot on black background, the precision-recall numbers are fairly modest. This will likely degenerate into noise in most camera-based images of the kind seen by a real robot. So, how much confidence should we expect to have in the approach's generality?\n\n3. For the baselines, the true good baseline might have been a human annotation that tells us how people really conceptualise the structure. With data such as pushing, this might not be so different from the simple visual inspection, but again with real data this will vary. The paper would really be much stronger if these were addressed.\n\n\n ", "belong_id": "BklfR3EYDH"}, {"uid": "Skxw--ijYB", "paper_title": "Attack-Resistant Federated Learning with Residual-based Reweighting", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an algorithm for mitigating poisoning attacks in federated learning settings and compares it , on four different datasets, against state-of-the-art baselines.\n\nExcept for some minor issues (see the list below), the paper is well-written and -organized. The description of the proposed algorithm (in pseudo code and using the illustration in Figure 2) is very clear. Overall, the experiments are carefully described.\n\nMy main concern is that many choices in the design of the proposed algorithm lack context/discussion and thus appear rather ad-hoc. For instance,\n- Why is the repeated median estimator used for estimating the linear regression? Could other robust estimators have been used?\n- Similarly, could alternative weighting schemes be used in equations (3)-(5)?\nI think it's important to provide more context and discuss possible alternatives. An important element is the exact threat model that the authors are considering. E.g., in the last paragraph on page 4, the authors mention specific attack strategies like altering only 10% of the model parameters. It appears that the design of the model weighting scheme aims at defending against these specific types of attacks. It will be good to either discuss or evaluate empirically how this scheme performs against other strategies.\n\nThe theoretical guarantee in Section 3.2 is a bit sketchy in my opinion. In what sense is $\\mu$ the 'expected value of the global model'? I.e. what is the expectation over? Consequently, I could not follow the statement in equation (14). Some explanation in plain text is needed here, too: in what sense does this equation provide a guarantee?\n\nIn the experiments, several aspects deserve further discussion: (1) the poor performance of FoolsGold almost across the entire board (except for the Gaussian noise attacks), which may indicate that this method was applied outside the threat model it was designed for; (2) the failure of all the baselines on CIFAR-10 for the naive attacking approach, while they perform fairly well on MNIST; (3) why does the attack success rate starts increasing in Figure 4 for the baseline methods only after ~25 iterations? (4) why do the baseline methods perform so poorly against label-flipping against on MNIST (Figure 3) while performing fairly well on CIFAR-10 and Amazon reviews (Table 1/2)? - I think that answering those questions may shed insights into the type of attacks that the different defences can / cannot withstand. I'd also like to challenge the authors to address whether they expect their defence to match or outperform the baselines on *any* attack strategy, or whether they can come up with scenarios where some of the baselines perform better? I would expect that the latter should be possible; it would not diminish the value of the proposed defence but shed more clarity on its possible limitations.\n\nList of minor issues:\n- in the abstract: 'aggression' -> 'aggregation'\n- p.1: I would omit the statement in brackets 'less than 100 lines'. \n- p.2: some of the related work discussion repeats content from the introduction\n- p.3: 'summaries' -> 'summarizes'\n- p.3: what does that mean: 'has a high breakdown point of 50%'? Please explain/clarify.\n- p.4: 'is the k-the diagonal of matrix in Hn' -> 'is the k-th diagonal element of the matrix Hn'\n- p.4: my pdf reader couldn't render the binary operator on the right hand side of equation (8)\n- p.5: 'the details of the proof is presented' -> 'are'\n- p.8: 'that of which' -> 'whose'\n- p.8: upper case 'We' after comma\n- p.8: first column 'Acc' in Table 3: FedAvg has the highest accuracy. Generally, bold numbers in tables do not always mark the best-performing method. Sometimes, bold numbers are entirely missing. In cases where the difference is insignificant (which often appears to be the case) I would mark multiple numbers in bold, as appropriate.\n\n", "belong_id": "HkgAJxrYwr"}, {"uid": "HJxmkyoaFr", "paper_title": "Attack-Resistant Federated Learning with Residual-based Reweighting", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes an aggregation algorithm, based on repeated median regression and residual-based weighting to defend federated learning from adversarial attacks. Experiments are shown to demonstrate the method robustness against label-flipping, backdoor and Gaussian noise attacks.\n\nThe paper is interesting, the topic recent and the methodology quite new, but a number of comments arise: \n\n1) the methodology seems to rely on a number of ad-hoc steps, hyper parameter-dependent, that might hinder reproducibility and generalization: i) residual normalization via IRLS; ii) confidence assignment; iii) extreme value correction. Could be interesting to show analysis how varying such hyper-parameters affects the results, or otherwise to add further explanation at the comments in Appendix A.3 as to why the model seems to be insensitive to \\lambda, or why \\delta is significantly affected by data distribution.\n\n2) could the repeated median estimator still be affected by the federated size i.e. the number of models involved in the federated learning? Is there any bound on the number of participants, below which the estimator would perform poorly?\n\n3) proof of eq (14) could be more readable if\n\t\tfull passages were shown (for instance for the reviewer the first passage was not immediate and took adding and subtracting \\sum_i z^(i)\\mu/\\sum_i z^(i), and further simplification to be addressed), and\n\t\tii) reference to the exact point in which previous results are used were made explicit (i.e. where in (Yin et al 2018) the bounds are proven).\n\n4) proof of eq (14), when the attacker a\\in B is fixed, then |\\hat{y}^(i) - \\tilde{y}| should be replaced by  |\\hat{y}^(a) - \\tilde{y}|\n\n5) A last question concerns the aspect of 'fairness' of this learning strategy. By removing aberrant updates there is still a chance of excluding from the learning process nodes that are intrinsically different form the average ones. In this sense, it is not clear from the paper how the reweighing strategy can mitigate this aspect, as there is no certainty that underrepresented data samples would not be rejected with the proposed scheme. Still aspect could have been better investigated in controlled scenarios.\n", "belong_id": "HkgAJxrYwr"}, {"uid": "HkxDJvg1cS", "paper_title": "Attack-Resistant Federated Learning with Residual-based Reweighting", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an approach to robust federated learning that uses robust regression to weigh all the model parameter coefficients in order to achieve the robustness. \n\nSpecifically, for each coefficient in the model, a repeated median estimator is used to compute a linear regression fit, and the residual of each individual model's coefficient is normalized and used to compute a confidence score for that coefficient. Coefficients which have too large a confidence have their confidence reset to zero, to avoid the influence of outliers. The local model's coefficients are now aggregated using a weighted average with weights given by these confidence scores.\n\nThey compare their algorithm experimentally to reasonable baselines of model aggregation algorithms: the FedAvg algorithm, 3 recent robust FL algorithm, and an approach based on a standard robust regression estimator, using experiments on 4 different datasets and 4 different neural net architectures. They test the robustness of these algorithms to label flipping (MNIST, CIFAR-10), backdoor attacks, and multiplicative gaussian noise corruption of the model coefficients.  \n\nOverall the paper presents an interesting and novel approach to robustness in FL, using a robust regression estimator to aggregate the model coefficients. The motivation of the algorithmic design is for the most part clear, but the rationale behind the particular choice of the parameter confidence score is unclear, and should be clarified. The theory in support of the method seems reasonablish, but key definitions and steps in the proof are not explained in detail, referring instead of an earlier paper. In particular, it is not clear how the smoothness of the loss function and subexponentiality of its derivatives enter into the analysis of the method, nor do these parameters enter into the final error bounds. Also, how is mu defined in the error bound: what does it mean that it is the expected global model --- does this mean this would be the model if all the participating workers were non-corrupted, honest, and had iid data? This theory is hard to parse: more effort should be spent in clarifying the assumptions and definitions and showing how the claimed result follows. The specific result referenced from earlier work should be stated unambiguously as a proposition so the reader sees how it applies where it is used.\n\nThe experimental results show that the algorithm performs slightly better than the considered baselines in most situations considered, but the important question of the impact of hyperparameter selection for the method (e.g. the clipping at which the weights of 'outlier' parameters are set to zero) and the competing methods (e.g. the clipping in the trimmed mean estimator) is not addressed-- the authors indicate that the method is robust to some choices and fixes them in the appendix. This makes it difficult to tell whether the method performs better due to careful or lucky hyperparameter selection. \n\nAlthough the method is interesting and novel, and seems principled, the theoretical claims are unclear, and the experimental evaluation is not sufficiently informative about the impact of hyperparameter selection to draw conclusions about the effectiveness of this method of model aggregation as opposed to the baselines considered. In particular because of the latter issue, I'm leaning towards reject, but would be willing to change my score if this were addressed.", "belong_id": "HkgAJxrYwr"}, {"uid": "Byg-szbhtH", "paper_title": "Robust Local Features for Improving the Generalization of Adversarial Training", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors proposed a new approach to improve the robustness of CNNs against adversarial examples.\nThe recent studies show that CNNs capture local features, which can be easily affected by the adversarial perturbations.\nThus, in the paper, the authors proposed to train CNNs so that they can capture local features that are robust against the adversarial perturbations.\nThe difficulty here is that existing adversarial training algorithms tend to bias CNNs to ignore local features and to capture only global features.\nTo avoid this unfavorable property of the adversarial training, the authors proposed the random block shuffle (RBS) that intensionally destroys the global feature of the images.\nThe authors demonstrated that combining RBS with the existing adversarial training algorithms can lead to robust CNNs.\n\nI found the paper well-written and the idea is easy to follow.\nEspecially, the use of RBS seems to be an interesting idea.\nAs a small downside, the proposed approach looks rather straightforward, and I expect to see any theoretical foundations if possible.\n\n### Updated after author response ###\nIn summary, the contribution of this study is in twofolds.\n1. Proposed an algorithm for learning robust local features.\n2. Demonstrated that learning robust local features is effective to improve the robustness of the model.\nThe possible downside is\n3. The proposed approach looks straightforward.\nOverall, I like the paper (especially for the reason 2 above), and therefore keep my score.", "belong_id": "H1lZJpVFvr"}, {"uid": "rkxiT9QatH", "paper_title": "Robust Local Features for Improving the Generalization of Adversarial Training", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The work suggests reshuffle images blocks of adversarial examples during adversarial training, in order to improve the generalization performance on benign and adversarial test samples.  The main method is based on the hypothesis in [Zhang et al 2019], [Ilyas et al 2019].  The assumption claims that robust models rely on global structural features, and non-robust models rely on local features. Thus, the work tries to learn local robust features, by cutting and reshuffling the image blocks. Overall the idea is interesting and the paper is well written .\n\nHowever, there are some concerns about the presentation and the main methodology:\n1.\tCan the paper give more explanation on the purpose of inserting the feature transfer term in the objective function? What is the difference of the proposed one with directly minimizing the loss on both original PGD image and reshuffled image?\n2.\tFor CIFAR10, TRADEs and PGDATs performance in the result is not as good as the ones shown in their original works, which is comparable to the performance of the proposed RLFAT method. More discussions are needed, otherwise the experimental results are not convincing. \n3.\tMore intuitions are needed  on what local and global features are, and why training on the reshuffled images can help learn generalizable robust local features. \n\nOverall the paper is easy to understand, but we suggest that more insight should be given on the success of the proposed method.\n", "belong_id": "H1lZJpVFvr"}, {"uid": "H1lZuixYcS", "paper_title": "Robust Local Features for Improving the Generalization of Adversarial Training", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper is interested in robustness w.r.t. adversarial exemples. \n\nThe authors note that:\n* features reflecting the global structure are more robust wrt adversarial perturbations, but generalize less;\n* features reflecting the local structure generalize well, but are less robust wrt adversarial perturbations. \nIn hindsight, these claims are intuitive: adversarial perturbations and unseen shape variations are of the same flavor; one should resist to both or handle both, with the difference that the latter is bound to occur (and should be handled) and the former is undesired (and should be resisted). \n\nThe goal thus becomes to define local features that are robust. \n\nThe proposed approach is based on \n* enforcing the invariance of the intermediate representation through shuffling the blocks of the training images; \n* building normal adversarial images x' and deriving the block shuffling RBS such that the x' and RBS(x') are most similar w.r.t. the logit layer\n* adding these RBS(x') to the training set;\n\nThe idea is nice; the experiments are well conducted and convincing (except for the addition of uniform noise, which is unrealistic; you might consider instead systematic noise mimicking a change of light);\nI'd like more details about:\n* The computational cost of line 7 in algo (deriving the best RBS).\n\nYou might want to discuss the relationship between the proposed approach and the multiple instance setting (as if the image was a bunch of patches). ", "belong_id": "H1lZJpVFvr"}, {"uid": "SkxVEsCjKB", "paper_title": "MANIFOLD FORESTS: CLOSING THE GAP ON NEURAL NETWORKS", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposed a new method called manifold forest to improve decision forest (DF) classification results. It is motivated by that, natural data is often in some manifold but not randomly distributed. It showed how to use the 2D spatial structures of natural images by constructing structured atoms. Results on 3 toy examples and MNIST showed the better performance than standard RF and SPORF.\n\nOverall, the paper is easy to follow and well written. The idea is intuitive: using structured 2D information to improve the classification results. But there are some issues with the implemetation.\n\n1. In image classification, we definitely need 2D structure information. This is normally extracted by the descriptors such as SIFT, GIST, where the 2D information has been included. It is rare to use the pixel values as the features directly for classification.   In this case, the benefit of the proposed method is very weak. This is the main issue of the paper. No results on real features.\n\n2. The results are weak too. The real data is MNIST, which is also a very toy dataset. It would be good to include some real world image dataset, such as CIFAR, ImageNet etc.\n\n3. The algorithm is somehow incremental compared with SPORF. \n", "belong_id": "B1xewR4KvH"}, {"uid": "HyeFQ5Kb9B", "paper_title": "MANIFOLD FORESTS: CLOSING THE GAP ON NEURAL NETWORKS", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The goal of the paper is clear. However, the proposed method has only a very incremental novelty compared to SPORF and the previous approaches in computer vision (e.g., Shotton et al., 2011). Although the authors claim the method can take advantage of structure in all kinds of data, the only conducted experiment is on the image data which is fairly limited.\n\nIn Fig. 1, they claim MORF outperforms other methods given fewer training data. However, for classifying Orthogonal Bars, CNN still outperforms MORF when few training data are given. The results on MNIST is also not impressive. CNN is still much better compared to other tree-based methods. Moreover, no other real-world dataset has been conducted experiments on.\n\nIn general, the paper's goal is clear and interesting. But the authors failed to propose a novel method and the results are not convincing. Hence, I recommend rejection.\n", "belong_id": "B1xewR4KvH"}, {"uid": "SyeMINIucB", "paper_title": "MANIFOLD FORESTS: CLOSING THE GAP ON NEURAL NETWORKS", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n\n====== Post Rebuttal ======\n\nNo rebuttal was provided and all reviewers have raised issues. Therefore, I will maintain the original rating.\n\n====== Summary====== \n\nThe paper puts forward a potential issue with the standard decision tree and tries to remedy it. The issue is that standard decision trees, by independently picking the split feature(s), virtually discard the structure of the input features (if any). In this work, structure essentially refers to ordered input features such as sequences, signals, and images. The work is motivated by the fact that convolutional networks (ConvNet) exploit this structure thanks to its local convolution operation. The idea of this work is to bridge this gap by constraining the split feature selection to follow the local structures of the input where each split feature would correspond to a local bounding box of the input feature. Applying this idea to random forests, it demonstrates significant improvements over unstructured split features, on a few synthesized datasets as well as MNIST dataset. The performance is also compared to one ConvNet architecture which demonstrates similar performance on synthesized datasets while being considerably slower than the proposed random forests.\n\n\n====== Strengths and Weaknesses ====== \n\n+ The motivation to make random forests respect the input structure similar to ConvNets is well-grounded and important since random forests are still being used in certain applications where computational complexity and/or interpretability are crucial factors.\n+ It proposes a simple technique to add locality to the split criterion which brings significant improvements over the standard random forest.\n\n- I believe the papers title should be closing the gap to *convolutional* neural networks since fully-connected networks are not local in the first place. \n- continuing on the previous point, the proposed method pushes the decision forests closer to locally-connected networks where, although the features are local, they are not shared across different locations in the input. The additional sharing property which takes locally-connected networks to convolutional networks is an important property of ConvNets since it brings translation equivariance for the representations. The proposed random forest method is not translation-equivariant by design and is only local. \n- regarding the previous point, a larger difference between ConvNet and the proposed method is more imminent if one goes to datasets with non-aligned observations. This already becomes more evident in the MNIST (which contains mostly aligned digits) where ConvNet clearly outperform MORF but would likely become more significant when going to real-world datasets, e.g. CIFAR.\n\n- the proposed method is very similar to patch-based random forest image recognition methods. For instance, several variants exist that are used for object or part detection in a given image where a patch is selected from the image for the split criterion. This patch will respect locality in a similar way to the proposed MORFs bounding boxes. For instance, see tomography scans example of Criminisi et al. 2012 (section 4.5).\n\n- The paper is missing to provide many important details\n- what are the actual hyperparameter (hp) values used for the different methods in figure 1,2, and 3. This includes the hp relevant for the random forests including the number of decision trees, the stopping criterion, the random data partitioning method, number of random projections, as well as h_min, h_max, w_min, and w_max. It also does not discuss the hyper parameters of the baseline methods including k for KNN, distance measure for KNN, penalty cost for SVM, variance for the RBF kernel, ConvNet architecture, etc.\n- more importantly, it is not mentioned how these hp are optimized for the different baselines as well as the proposed method. What algorithm has been used (e.g. grid search)? How much hp optimization budget is used for different baselines? Is there a validation set put aside for hp optimization?\n- from the description in the start of page 4, it seems that the atoms for the proposed MORF are vectors of binary dimensions while for the general SPORF each atoms element can be -1 as well (page 3). Why is this choice made despite the fact that it reduces the capacity of the model?\n\n- the bounding box sampling seems biased as presented. That is, bounding boxes closer (than h_max and/or w_max) to the right and/or lower borders are more likely since the number of valid boxes will be lower. \n\n\n====== Final Decision ====== \n\nI think it will be very interesting to bridge the gap between ConvNets and random forests since the latter comes with attractive properties. While I find all the concerns that are listed above important, my rating is mainly due to the novelty of this work compared to the prior patch-based random forest techniques for image analysis.\n\n\n====== Points of improvements ====== \n\nI believe its important to disentangle the two main properties of convolution operation in ConvNets being shared and local parameters. Then, accordingly propose strategies to bring these properties to a decision tree.\n\n", "belong_id": "B1xewR4KvH"}, {"uid": "BkxPOKMe9r", "paper_title": "Amharic Text Normalization with Sequence-to-Sequence Models", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper addresses the text normalization problem, where  the special processing is required for different kinds of non-standard words (NSWs).\n\nDataset:  a new dataset is collected, including many types of non-standard words from different Amharic news Media\nand websites, FBC more than eighty percent, VOA and BBC. \n\nModel: Bidirectional GRU with the size of 250 hidden units both are used for encoding and decoding layers.\n\nThis paper is not ready to publish. Please consider to complete the project, polish the writing, and submit to a different venue.\n\n", "belong_id": "SJe-HkBKDS"}, {"uid": "HkgF8KJT9r", "paper_title": "Amharic Text Normalization with Sequence-to-Sequence Models", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Text normalization or the transformation of words from the written to the spoken form is an important and realistic question in natural language processing. This paper aims to use sequence-to-sequence models to perform text normalization.\n\nHowever, this paper does not use the official template and the content is too short to be a conference paper.\nI suggested resubmitting to another (NLP) conference after extending the content with detailed description for the model and the method, and conducting more experiments on public acceptable benchmarks.  ", "belong_id": "SJe-HkBKDS"}, {"uid": "Skg1Jt4msB", "paper_title": "Amharic Text Normalization with Sequence-to-Sequence Models", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper describes a method for word normalization of Amharic text using a word classification system followed by a character-based GRU attentive encoder-decoder model.\n\nThe paper is very short and lacks many important details, such as where the data is collected from, how it is processed and split into training and evaluation sets, and how the initial token classification is performed. The paper also doesn't adhere to the conference paper template, which is grounds for desk rejection.\n\nThe authors should revise the paper with this information and consider submitting to a different venue, as the task considered, while interesting, seems far from the core focus of ICLR.\n", "belong_id": "SJe-HkBKDS"}, {"uid": "BJl5fZS6YS", "paper_title": "DBA: Distributed Backdoor Attacks against Federated Learning", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies backdoor attacks under federated learning setting. To inject a certain backdoor pattern, existing work generate poisoning samples by blending the same pattern with different input samples. Even for federated learning where the adversary can control multiple parties, such as [1], all parties still use the same global backdoor pattern to generate poisoning samples locally. On the contrary, in this work, they decompose the global pattern into several small local patterns, and each adversarial party only uses a local pattern to generate poisoning samples. In their evaluation, they show that the backdoor attacks generated in this way are more effective, resilient to benign model parameter updates, and also survive better against existing defense algorithms against attacks in federated learning settings.\n\nI think the topic studied in this paper is very important and meaningful, and I am convinced that by decomposing a global pattern into several smaller local pieces, the model parameter updates computed by each party should be more similar to benign updates and thus can better bypass the defense algorithms. Meanwhile, the evaluation is pretty comprehensive and it is good to see that the conducted backdoor attacks are effective. However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance. Thus, I would like to see more possible explanation on it. Specifically, I have the following questions for clarification:\n\n1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?\n\n2. To evaluate A-S setting, I understand that it may be tricky to enable a fair comparison between the centralized attack and DBA. However, one explanation of why DBA is more persistent in this case is because the adversarial parameter updates happen 4x times compared to the centralized attack. Therefore, another baseline to check is to conduct centralized attacks with the same number of times as DBA, but each update includes 1/4 number of poisoning samples, so that the total number of poisoning samples included to compute the gradient update still stays the same.\n\n3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes? For backdoor attacks, a line of work studies physical triggers, e.g., glasses in [2]. It is not natural to decompose such kind of patterns into several smaller pieces, unless the performance is significantly boosted.\n\n4. Can the authors show concrete examples on how the attacks are generated? The details are especially unclear on LOAN. Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?\n\n[1]  Bagdasaryan et al., How to backdoor federated learning.\n[2] Chen et al., Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning.\n\n------------\nPost-rebuttal comments\n\nI appreciate the authors' great effort to address my concerns! I think the evaluation in the current version of the paper is pretty comprehensive and provides a valuable study, and I am happy to raise my score accordingly.\n-------------", "belong_id": "rkgyS0VFvr"}, {"uid": "rkeucgq15r", "paper_title": "DBA: Distributed Backdoor Attacks against Federated Learning", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors introduce the idea of distributed backdoor attacks in the FL framework, in which the dishonest participants in FL add local triggers to their training data to influence the global model to classify triggered images in a desired way. They show empirically that the learned models then are more likely to be successfully forced to misclassified images in which all the local triggers are present at test time, than are models learned using centralized backdoor attacks, where all attackers use the same trigger pattern (one of the same size as the concatenation of the local triggers, to be fair in the comparison). They then demonstrate that because the local triggers cause smaller corruptions in the model coefficients, these distributed attacks survive robust FL training algorithms (namely FoolsGold, and a recent robust regression based method) more often than centralized attacks. Similar experiments are conducted on the Loan text dataset, using appropriate analogs of local triggers, with similar results.\n\nThe paper contributes a novel model for conducting backdoor attacks in the FL setup, and shows that this model is more successful at attacking when training using robust FL algorithms than the standard centralized backdoor attack model. I lean towards accept, as this is a realistic attack model, and as such can further stimulate research into the robustification of FL model aggregation algorithms.", "belong_id": "rkgyS0VFvr"}, {"uid": "SJe4yM7lcr", "paper_title": "DBA: Distributed Backdoor Attacks against Federated Learning", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a distributed backdoor attack strategy, framed differently from the previous two main approches (1) the centralised backdoor approach and (2) the (less discussed in the paper) distributed fault tolerance approach (often named 'Byzantine').\n\nThe authors show through experiments how their attack is more persistent than centralised backdoor attack.\nThe authors also compare two aggregation rules for federated learning schemes, (Fung et al 2018 & Pillutla et al 2019), suggesting that both rules are bypassed by the proposed distributed backdoor attack.\n\nStrength:\n\nwhat I found most interesting in the paper is Section 3.4, presenting an appreciable attempt to 'interpret' poisoning. Together with Section 4. \nThis kind of fine-grained analysis of poisoning is highly needed.\n\nWeakness: \n\nin section 3.3, the authors compare against RFA and take what is claimed in Pillulata et al as granted (that RFA detects more nuanced outliers than the wort-case of the Byzantine setting (Blanchard et al 2017) ). In fact, there is more to the Byzantine setting than that, see e.g. Draco (Chen et al 2018 SysML), Bulyan (El Mhamdi et al 2018 ICML) and SignSGD (Bernstein et al 2019 ICLR) which have proposed more sophisticated approches to distributed robustness.\nSince this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.\n\npost rebuttal: thank your for your detailed reply, I acknowledge your new comparisons with the distributed robustness mechanisms of Krum and Bulyan, too bad time was short to compare with the other measures such as Draco and SignSGD.", "belong_id": "rkgyS0VFvr"}, {"uid": "r1l45lqTKH", "paper_title": "RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper addresses the problem of intrinsically motivating in DRL. In particular, it focuses on exploration of procedurally generated environments where many states are novel compared to training experiences. It offers an intrinsic reward based on large movement in a state embedding space where this state embedding representation is co-trained on the same data already collected for learning. The paper claims to overcome shortcomings of specific past approaches (e.g. count-based / curiosity).\n\nThe need for intrinsic motivation in exploration is well motivated, and the approach for training a state embedding is anchored in multiple past works. The use of movement in this state embedding as an intrinsic reward is importantly novel and valuable. The problematic propensity for RL researchers to train on the test environments or design agents that are confused by proverbial noisy TVs and/or sacrifice extrinsic rewards in favor of intrinsic rewards is satisfyingly discussed and addressed through detailed experiments.\n\nThis reviewer moves to accept the paper for its contributions to intrinsically motivated exploration with thorough discussion of how the technique addresses shortcomings of past methods. This reviewer is thankful that the authors do not overinterpret the MiniGrid results and that they provide intuition for why the state embedding functions capture what we want them to capture. The fact that this approach makes joint use of the whole (s,a,r,s') tuple feels significant, as does the fact that this approach does not require any changes to the policy network (e.g. presuming that features useful for computing intrinsic rewards are also going to be useful for directly acting to optimize extrinsic rewards).\n \nQuestion:\n- In partially observable environments that require agents to wait for something, should a RIDE-motivated agent consider changes in its own internal clocks (part of the recurrent state) impactful moves? If an environment might require a recurrent / history-aware action policy, should RIDE also be made history aware? Might a history-aware RIDE reward sufficiently motivate a stateless/reactive policy?", "belong_id": "rkg-TJBFPB"}, {"uid": "HkxHBiPAKr", "paper_title": "RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new intrinsic reward method for model-free reinforcement learning agents in environments with sparse reward. The method, Impact-Driven Exploration, learns a state representation of the environment separate from the agent to be trained, based on a combied forward and inverse dynamics loss. The agent is then separately trained with a reward encouraging sequences of actions that maximally change the learned state.\n\nLike other latent state transition models (Pathak et al. 2017), RIDE learns a state representation based on a combined forward and inverse dynamics loss. However, Pathak et al. rewards the agent for taking actions that lead to large difference between the actual next state and the predicted next state. RIDE instead rewards the agent for taking actions that lead to a large difference between the actual next state and the current state. However, because rewarding one-step state differences may cause an agent to loop between two maximally-different states, the RIDE loss term is augmented with a state visitation count term, which decreases intrinsic reward for a state based on the number of times that state has been visited in the current episode.\n\nThe experiments compare RIDE to a selection of other intrinsic reward methods in the MiniGrid, Mario, and VizDoom environments. RIDE provides improved performance on a number of tasks, and solves challenging versions of the MiniGrid tasks that are not solved by other algorithms.\n\nDecision: Weak Accept.\n\nThe main weakness of the paper seems to be a limitation in novelty.\nPrevious papers such as (Pathak et al. 2017) have trained RL policies using an implicit reward based on learned latent states. Previous papers such as (Marino et al. 2019) have used difference between subsequent states as an implicit reward for training an RL policy. It is not a large leap to combine these two ideas by training with difference between subsequent learned states. However, this paper seems to be the first to do so.\n\nStrengths:\nThe experiments section is very thorough, and the visualizations of state counts and intrinsic reward returns are insightful.\nThe results appear to be state of the art for RL agents on the larger MiniGridWorld tasks.\nThe paper is clearly-written and easy to follow.\nThe Mario environment result discussed in section 6.2 is interesting in its own right, and provides some insight into previous work.\n\nDespite the limited novelty of the IDE reward term, the experiments and analysis provide insight into the behavior of trained agents and the results seem to improve on existing methods.\nOverall, the paper seems like a worthwhile contribution.\n\nNotes:\nIn section 2 paragraph 4, 'sintrinsic' should be 'intrinsic'.\nIn section 3, at 'minimizes its discounted expected return,' seems like it should be 'maximizes'.\nThe explanation of IMPALA (Espeholt et al., 2018) should occur before the references to IMPALA on page 5.\nLabels for the axes in figures 4 and 6 would be helpful for readability.\n\nThe motivation for augmenting the RIDE reward with an episodic count term is that the IDE loss alone would cause an agent to loop between two maximally different states.\nIt would be interesting to know whether this suspected behavior actually occurs in practice, and how much the episodic count term changes this behavior.\nIt is surprising that in the ablation in section A.5, removing the state count term does not lead to the expected behavior of looping between two states, but instead the agent converges to the same behavior as without the state count term.\n\nAlso, in Figure 9, was the OnlyEpisodicCounts ablation model subjected to the same grid search described in A.2, or was it trained with the same intrinsic reward coefficient as the other models?\nBased on the values in Table 4, it seems like replacing the L2 term with 1 without changing the reward coefficient would multiply the intrinsic reward by a large value.\n", "belong_id": "rkg-TJBFPB"}, {"uid": "S1xw9yhZcH", "paper_title": "RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary\nThis paper proposes a Rewarding Impact-Driven Exploration (RIDE), which is an intrinsic exploration bonus for procedurally-generated environments. RIDE is built upon the ICM architecture (Pathak et al. 2017), which learns a state feature representation by minimizing the L2 distance between the actual next state feature and the predicted next state feature while minimizing the cross-entropy loss between the true action and the estimated action from the consecutive state features. Finally, RIDE's intrinsic reward bonus is computed by L2 norm of the difference between the current state feature and the next state feature, divided by the square root of the visitation count of the next state within the episode. Experimental results show that RIDE outperforms the existing exploration methods in the procedurally-generated environments (MiniGrd), and is competitive in singleton environments.\n\n\nComments and questions:\n- In reinforcement learning, the agent should explore the experiment due to uncertainty. If everything in the environment is certain to the agent, then it does not have to explore and just exploiting the past experience would be the best. My major concern about the paper is 'impact-driven' reward bonus may not account for the uncertainty. Constantly encouraging the states that have a high impact would not always good, and it may interfere to converge to an optimal policy.\n- It seems that RIDE assumes that 'high-impact' states are always good, thus rewarded. It could be true on the conducted MiniGrid domains, but this assumption may not hold in general. Could 'impact-driven' exploration be realistic and be applied to more general problems?\n- Similarly, in the problems where high-impact states have to be avoided, can RIDE still work effectively? For example, how about 'Dynamic-Obstacles' domains implemented in MiniGrid? In this task, RIDE may promote to chase obstacles that have to be avoided, interfering with learning optimal policy. It would be great to show the effectiveness of RIDE in such environments.\n- In MiniGrid problems, if the colors of walls and the goal are changed at every episode, does RIDE work well?\n- In Figure 4, why the intrinsic reward heatmaps are drawn only on the straight paths?\n- Minor: In the last sentence of Section 3, 'the current state and the next state predicted by the forward model' -> 'the actual next state and the next state predicted by the forward model'\n\n\n---\nafter rebuttal:\n\nThank the authors for clarifying my questions and concerns. Most of my concerns are addressed, and I raise my score accordingly.", "belong_id": "rkg-TJBFPB"}, {"uid": "HJe-PntOtS", "paper_title": "Rethinking deep active learning: Using unlabeled data at model training", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper argues that active learning (AL) methods shold combine unsupervised and semi-supervised learning during the iterative training process. Combining these complementary is indeed sensible, and this work is therefore a welcome effort. However, the results are quite mixed, and in fact seem to suggest that AL is rather ineffective. Therefore, what one might take from these results is that unsupervised and semi-supervised learning methods can boost predictive performance; but I think this is widely appreciated already. Perhaps a better framing for this work is: AL using standard metrics seems to be comparatively ineffective, especially when one uses pre-training/semi-supervised learning. \n\nSome specific comments and questions:\n\n- The authors have decided to frame this paper in terms of improving AL using un/semi-supervised learning. But given that, by the authors' own admission, the 'random baseline may actually outperform all other acquisition strategies by a large margin', what is the motivation for adopting 'AL' at all? I mean, if we are performing random (iid) sampling, this just reduces to vanilla learning with pre-training and semi-supervision; the 'active' component becomes irrelevant.\n\n- I think the characterization of AL is not quite right on page 2. The authors write that AL is focuses on the 'least certain' instances. This is often true -- namely under the popular uncertainty sampling regime -- but not all acquisition strategies use this heuristic. Indeed, even the geometry method the authors use explicitly ignores classifier confidence. \n\n- The use of sampling in the SSL component is interesting, although an ablation here investigating this specific choice (as opposed to, say, naive sampling with uniform probability over unlabeled instances).\n\n- I would not characterize the gains brought by unlabeled data here as 'spectacular'.\n\n- As is often the case in work on AL, there is no real notion of a 'test set' here; instead the authors repeat experiments using different seed label sets. It is not entirely clear how much hyperparameter/architecture fine tuning was performed informally, but there is a lot going on here, so I would assume at least some. Therefore there is a risk that all results reported are in some sense optimistic, potentially being 'overfit' to these datasets. It would be best to provide additional comparisons of approaches on completely unseen datasets. ", "belong_id": "rJehllrtDS"}, {"uid": "HJg_1902tB", "paper_title": "Rethinking deep active learning: Using unlabeled data at model training", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper explores the setting where unsupervised/semi-supervised learning is combined with active learning. The results are that active learning doesn't really help. This paper is interesting in that it provides additional experiments for the intersection of active learning and unsupervised/semi-supervised learning. However, I don't really see the point of this paper. Active learning and unsupervised/semi-supervised learning have been combined before and there are other papers submitted to ICLR this year that combine these. The paper does not claim to provide anything new algorithmically (other than jLP which appears to work no better than random and isn't really advertised as the point of this paper). The only conclusion that I can draw is that sometimes unsupervised/semi-supervised learning works better than active learning, but no understanding of when and why this is the case (from other papers, it is not always the case).\n\n\nComments:\n\n - Although the paper claims to yield a general framework, it only does so partially. For instance, the framework in this work is restricted to semi-supervised methods that use pseudo-labels. \n\n - It may be the case that active learning doesn't help or even hurts because the batch size is too large and/or the initial seed set size is too small. Although this paper varies the acquisition strategies, these other hyper-parameters are equally, if not more, important.", "belong_id": "rJehllrtDS"}, {"uid": "SJlVOqRh9B", "paper_title": "Rethinking deep active learning: Using unlabeled data at model training", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors study the problem of incorporating unsupervised (representation pre-training) learning and semi-supervised learning into active learning for image classification; specifically, performing pre-training before active learning starts [Caron, et al., 2018] and then applying inductive label propagation [Issen, et al., 2019] (slightly modification in the cost function to look more like importance sampling) before active learning querying occurs for each round (Algorithm 1).  The most novel technical innovation of this submission is the joint label propagation (jLP) querying function (which is a method of spanning the learned manifold space).  Experiments are conducted on four (multi-class) image classification datasets (MNIST, SVHN, CIFAR-10, CIFAR-100), showing that unsupervised learning and semi-supervised learning can improve active learning on these datasets  although random selection often works better (as best as I can tell) implying that negative results are also a contribution of this paper. Finally, some active learning experiments are conducted using a per-round label budget of one example per class  also demonstrating mixed results with random sampling performing better in general. \n\nIn my mind, this paper has two primary components: (1) taking the position that semi-supervised and unsupervised learning can improve overall performance and, in principle, help with active learning and (2) propose jLP, which is a learning algorithm agnostic approach to spanning the manifold space. However, jLP doesnt really seem to work in general. Thus, the main result is the first point  updating previous (pre-deep learning) results on SS/US AL to deep learning. Honestly, I think the primary conclusion is that semi-supervised and unsupervised learning has improved over the past decade (especially semi-supervised learning for image classification). The second result is that active learning in deep learning (at least for this application) hasnt kept up. Wrt to (1), as the authors have pointed out, many others have applied semi-supervised learning to AL (including more that the authors didnt include). Additionally, many have used unsupervised learning for AL (which the authors seem less aware of) from pre-clustering (e.g., [Nguyen & Smeulders, Active Learning using Pre-clustering; ICML04]) to one/few-shot learning (e.g., [Woodward & Finn, Active One-Shot Learning; NeurIPS16 workshop]) to using pre-trained embeddings for many real-world tasks (e.g., NER [Shen, et al., Deep Active Learning for Named Entity Recognition; ICLR18] using word2vec). Thus, the interesting question would be to compare multiple pre-training techniques and ideally the relative effect on the active learning component (assuming this is the focus of the paper). With respect to semi-supervised learning, they have validated that inductive label propagation [Issen, et al., 2019] works for this task, but havent shown that this helps with active learning. Since this is a negative results without a theoretical contribution, I would again expect trying several semi-supervised algorithm and evaluating their relative performance in general and wrt the active learning querying strategy. Accordingly, I dont think the contribution of this work in its current state is sufficiently well-developed  and would lean toward rejecting in its current form.\n\nBelow are some additional detailed comments (some also covered above): \n Given that this points toward a negative result, a more convincing direction to take would be to consider more combinations of unsupervised and semi-supervised approaches  specifically emphasizing how they affect the active learning component. This might point to more general findings and maybe toward a theory (maybe even consider a second application).\n The empirical emphasis is more around overall performance rather than the interaction between unsupervised representation learning and active learning, which is more toward the stated goal of the paper.\n Wouldnt the right way to do (deep) representation learning in multiple rounds be to fine-tune at least some fraction of the time?  If the only claim is pre-training or pre-clustering, people certainly do this  just often not as a point of emphasis.\n The first semi-supervised claim really only holds in the context of deep learning; however, scope is really more like semi-supervised applied to image classification, which would be a pretty narrow contribution in scope.\n Overall, there is a general overstatement of contributions and results: this is certainly not the first SSAL or USAL and the statement relative to deep learning is subtle; some of the empirical results are interesting, but I am not sure about spectacular gains (and these gains arent seemingly due to the contribution of the paper).\n I dont understand the ensemble model analogy in the abstract; is it because it is a meta-algorithm?\n\nSome more positive notes: \n+ It is interesting that there is some contradictory evidence relative to [Wang, et al., 2017; Gal, et al., 2017]; this is probably worth digging into a bit deeper.\n+ The experimental details well-described given space constraints.\n\nIn summary, there are some interesting observations that are probably worth pursuing. However, the current contribution is basically that: (1) active learning doesnt seem to really help, (2) semi-supervised learning and unsupervised learning improve performance for this task. Since (1) was really the point of the paper (as stated) in the title, I dont think there is enough here to accept in its current form.", "belong_id": "rJehllrtDS"}, {"uid": "rkgUHZwTKS", "paper_title": "Unsupervised Meta-Learning for Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper develops a meta-learning approach for improving sample efficiency of learning different tasks in the same environment. The author formulates the meta goal as minimizing the expected regret under the worst case, which happens when all the tasks are uniformly distributed. The paper introduces two types of tasks: goal-reaching task and a more general trajectory matching task. Then the author introduces a meta-learning algorithm to minimize the regret by learning the reward function under different sampled tasks. The paper is interesting. Below are my questions/concerns.\n \n1. Why trajectory matching is considered as more general? Intuitively, trajectory matching is more restricted in that whenever an agent can match the optimal trajectory, it should also reach the goal state. \n\n2. The theoretical results (lemma 2, 3) actually indicates that the previous work universal value function approximator can optimize the proposed meta learning objective with theoretical convergence guarantee in tabular case by learning the value function Q(s, g, a) where s is a state, g is goal state, a is an action (as long as s and g are visited infinitely often) . As a result, why is it necessary to introduce meta-learning approach? Why not simply learn universal value functions? \n\n3. The experimental results are not very persuasive. What is the VPG algorithm used? And if you run the algorithm longer, is it finally worse than learning from scratch? Option learning methods/universal value function can be added as baselines. ", "belong_id": "S1et1lrtwr"}, {"uid": "S1lqr4oRFr", "paper_title": "Unsupervised Meta-Learning for Reinforcement Learning", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: this paper claims to design an unsupervised meta-learning algorithm that does automatically design a task distribution for the target task. The conceptual idea is to propose a task based on mutual information and to train the optimal meta-learner. They also use experiments to show the effectiveness of the proposed approach. \n\nOverall Comments:\n\nI would think this paper requires a major revision. It is written in a very confusing way. Many terms are directly used without a definition. The problem is also not clearly defined. I have tried to understand everything, but I have to give up in Section 3. Overall, I do not think this paper is ready for publication.\n\nDetailed comments:\n\t It would benefit a lot if you can clearly define the original meta-learning procedure and then compare that with the one proposed in this paper.\n\t Define hand-specified distribution. This word does not make sense if you claim this is the difference between the meta-learning procedure proposed in this paper and the original meta-learning algorithm. In this paper, you used p(z) to specify a task. I would think p(z) is also hand-specified.\n\t I am not very sure by what you mean for task-proposal procedure, goal-proposal procedure\n\t In the first paragraph of the intro: what do you mean by specifying a task distribution is tedious, is specifying p(z) also tedious\n\t 2nd paragraph of intro: automate the meta-training process by removing the need for hand-designed meta-training tasks. Again, why p(z) is not hand-designed\n\t Why compare with the original meta-RL algorithm on p(z) is not fair? \n\t What do you mean by acquire reinforcement learning procedures?\n\t Environment, task are not clear when they first appear\n\t The word learn is used everywhere, and is confusing. E.g. what do you mean by learn new tasks, learn a learning algorithm f, learn an optimal policy, learn a task distribution ...\n\t Reward functions induced by p(z) and r_z(s,a): isnt r_z(s,a) already a reward function? What is induced?\n\t What is meta-training time?\n\t What is no free lunch theorem?\n\t The controlled-MDP setting is actually much easier: perhaps you just need to learn the probability distribution. Then for every r_z, we just solve it. Why not compare with this simple algorithm?\n\t Regret is not defined when it first appears\n\t The task distribution is defined by a latent variable z and a reward function r_z: why distribution is defined by an r.v.?\n\t In (2), regret should be the (cost of the algorithm) - (the total cost of an optimal policy)  it is not hitting time\n\t (3) is confusing, no derivation is given\n\t Based on the usual definition of regret, how can a policy have low regret? Any fixed policy would have linear regret ...", "belong_id": "S1et1lrtwr"}, {"uid": "rkeDSjvI5r", "paper_title": "Unsupervised Meta-Learning for Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "# Summary of the paper:\n\nThis paper formulates conceptually the unsupervised meta-RL problem (to learn a policy without access to any reward function) as a minimization of the expected regret over tasks, and instantiate an algorithm based on DIAYN (Eysenbach et al., 2018) and MAML Finn et al. (2017a). \n\n# Brief explanation of my rating:\n\n1. *Novelty*: Mutual information based unsupervised RL was proposed by DIAYN (Eysenbach et al., 2018). Meta-model was also considered by DIAYN (Eysenbach et al., 2018), in which they call it 'skill'. \n2. *Technical contributions*: Sec 3.1-3.4 try to justify DIAYN. However, the reasoning is not sufficiently rigorous and the proposed Algorithm 1 is inconsistent with the theory built up in these sections. \n3. The *writing* can be improved a lot -- it's not easy to guess what the author was trying to say until I read DIAYN (Eysenbach et al., 2018). \n4. The key ingredient is missing -- the learning procedure f, which was mentioned in eq.(1) and Algorithm 1, but the details are never specified. It is impossible to reproduce the algorithm based on the description in the paper. \n4. The same *experiments* are conducted in DIAYN (Eysenbach et al., 2018). I am still confused on why we suddenly should use meta-RL. \n\n# Comments:\n\n1. Why we should consider regret? What is the relation between (1) & (4)? It's quite strange you start with (1) but turn to something else, i.e., (4), quickly. \n2. 'This policy induces a distribution over terminal states, p(s_T | z)' Why? \n3. What are you optimizing over in (5)?  The statement in Lemma 2 says 'I(s_T; z) maximized by a task distribution p(s_g)'. However, you are only able to control p(s_T | z), not the marginal distribution p(s_T). The statement of Lemma should be made more clear.\n4. The definition of the reward function: r_z(s_T, a_T) = log p(S_T | z), which is independent of the action a_T? \n5. In Algorithm 1, the reward reuse the definition of DIAYN -- log D(z | s),  but which is different from log p(S_T | z). Could you elaborate this? \n6. What is the definition of Markovian reward? Why does the inequality on page 6 hold? ", "belong_id": "S1et1lrtwr"}, {"uid": "Hkx1u0nj_H", "paper_title": "Collaborative Inter-agent Knowledge Distillation for Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThis paper proposed an ensemble method (CIKD) that train multiple agents and\nuse knowledge distillation to transfer knowledge from the current best agent to\nsub-optimal agents periodically.  According to the reported results, CIKD is a\nsimple yet effective approach to improve sample-efficiency and final performance.  \nThe experimental results are sufficient, and the ablation studies are conducted thoroughly. It is shown that both selecting the best agent and using KD to\ntransfer knowledge are effective comparing to other naive alternatives. \n\n\nI recommend the acceptance of this paper. \n\nThe paper proposed a novel approach (CIKD) to improve the sample-efficiency of the state-of-the-art. The proposed ensemble approach is aligned with our intuition, and it is effective. The authors proposed to train several agents at the same time and randomly select one of\nthe agents as a behavior policy during each rollout. Then the collected trajectory is used to update the policy of all agents. Meanwhile,\nthey keep tracking the performance of each agent and use the current best agent to conduct knowledge distillation to other agents periodically. \n\nThis paper first conducts experiments to show when consolidating\nthe SAC with CIKD, both of the final performance and sample-efficiency can be improved. Then a set of ablation studies verified the best agent selection strategy, and the knowledge distillation\nstrategy is necessary for the ensemble method. \n\n\nInvestigation on the reasons for improvement:\nThough extensive ablation studies have shown the effectiveness\nof each component of CIKD. It is still not clear why this approach\ncan be effective. \nIntuitively, it is possible that the exploration from a set of agents would outperform\na single agent. The measure of exploration efficiency could help in explaining the results. Furthermore, better exploration not necessarily\nleads to better performance and sample-efficiency. Does knowledge distillation serve as a better alternative to exploit existing data? \n\nModel/algorithm agnostic\nThe proposed method is more convenient to be applied with off-policy approach when the policy is in the form of softmax. Is it also applicable\nto other approaches? \n\nExperiments:\nHow do you determine when to stop the KD process? As mentioned in section 5.5, if we conduce KD fully, all students would be just imitating\nthe teacher's behavior. It seems the key is to tune a good termination\nthreshold for each task? Are there any guidelines to set up this threshold?\nDo you have some automatic way to terminate the KD procedure?\n\n\nMinor:\nL1, P5, 'how to CIKD improves the sample efficiency' \n\n", "belong_id": "BkeYSlrYwH"}, {"uid": "HygygT3M9H", "paper_title": "Collaborative Inter-agent Knowledge Distillation for Reinforcement Learning", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an RL training procedure that maintains an ensemble of k policies and periodically pushes all the policies to be closer to the best performing one. The formulation, experiments and analysis are very clear and show a mild improvement over using the same underlying RL algorithm without the imitation part. The idea is close to many other proposed in the literature, but to my knowledge it is the first time this exact procedure is studied in detail.\n\nThe first piece of their approach is an off-policy RL algorithm. In their case, they use SAC. The second piece is adding an ensemble of policies (3 in their case), and randomly selecting one of them every time a rollout is collected, and using the collected rollout to update all the policies. This effectively implies 3 times more overall gradient updates compared to SAC. They call this ablation SAC-ensemble. Interestingly they only use the most recently collected trajectories to update all policies, and despite storing the rollouts in a replay buffer, they seem to only use the stored transitions for the imitation part described below. Some of their experimental results uses extra gradient steps, although its not clear if those gradient steps are also only on the last rollout collected, or on transitions sampled from the replay buffer as it is typical in off-policy RL methods. In general, I think the work could improve with more details about how much the policy training could improve by increasing the number of gradient steps on the full replay buffer.\n\nThe final piece of their method is selecting the best performing policy (or teacher) of the ensemble based on the recent experience, and update all other policies by executing some gradient steps on the KL divergence between them and the current teacher. They also try an experiment where the teacher is selected randomly, and it does surprisingly well in my opinion (specially realizing that the HalfCheetah experiments seem to not have all seeds run to convergence, please report the full results). I suspect that most of the benefit of their method comes from randomly perturbing the parameters of the policies in the ensemble. More thorough and careful experimentation needs to be carried out to investigate this direction. This is in fact not very surprising given the results of Evolutionary Strategy methods, or Population-based training (even if usually used for hyper-parameters adaptation).\n\nFurthermore, the authors only run the environments for 1M steps, whereas in previous works some environments are shown to get higher return after more training steps. I would also encourage the authors to report the results in all the standard MuJoCo benchmarks for the ablations (even if its in the appendix) to better asses their claims.\n\nOverall, this is a very well presented work, although it lacks some novelty and a few more thorough experiments to fully understand the improvements they show. I think this idea is worth sharing with the community, and I recommend a weak accept.", "belong_id": "BkeYSlrYwH"}, {"uid": "Bkg-TbEmor", "paper_title": "Collaborative Inter-agent Knowledge Distillation for Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces a method for using an ensemble of deep reinforcement learning policies, where members of the ensemble are periodically updated to imitate the most promising member of the ensemble. Thus learning proceeds by performing off policy reinforcement learning updates for each individual policy, as well as some supervised learning for inter-policy imitation learning.\n\nI start by what I view as the positive aspects about the paper:\n1- The algorithm is quite simple (to understand and to implement).\n2- Experimental results are performed on a variety of domains, and more importantly, each experiment is motivated by a question.\n\nThat said, I have some concerns about this paper which I list below:\n\n1- Perhaps my biggest concern is that the approach is not motivated from a theory stand point. There has been interesting results in Osband's work [Osband, 2016] (and references therein) for randomized value functions which can serve as a foundation for this work. That said, a) Osband's results, at least immediately, are related to value-function based methods, as opposed to policy gradient b) the KL update which one could argue is the main and only significant contribution of the paper, is not justified by Osband or any other prior work c) there is not anything that this paper adds to the literature to better justify diversity through randomization and/or imitation learning based on the best member of the ensemble.\n\n2- I have found various claims in the paper which are unclear, scientifically not true, or sometimes even contradicting. In Introduction, for example, the authors mention that the agent sometimes gets into a sub-optimal policy and may require a large number of interactions before escaping the sub optimal policy. How does gathering more data help to improve the policy? Either we are in a local maximum, which if we are doing gradient ascent, there is really not much we could do, or that we are in a saddle point, which we can escape by adding some noise to the gradient. [Jin,2017]\n\n3- In section 4.3 the authors talk about on-policy methods requiring importance sampling (IS) ratios. To the best of my knowledge, IS is only used for off-policy learning. Can the authors provide a link to an on-policy method that does IS?\n\n4- Again in section 4.3 authors claim and I quote 'Using off-policy methods, all the policies in the ensemble can easily be updated, since off-policy update methods can perform updates from any \\tau'. But later on in Section 5.3 authors claim that 'off-policy actor-critic methods (e.g. SAC) cannot fully utilize the other agent's or past experience.' So which statement is true?\n\n5- Again, the KL update is interesting, but is it even surprising that the KL update is necessary for an ensemble of policies updates using policy gradients? In the absence of this KL update, which the authors characterize as the method that Osband proposed, the policies could generally be arbitrarily far from one another. This means that each policy needs to perform policy evaluation using trajectories that are coming from other policies who in principle can be radically different than the policy we want to update. This means that updates will be quite 'off-policy' which we know can really degrade the quality of the estimated gradient. This is perhaps why even choosing a random policy to update towards is providing 'some' improvement. I think this is the real insight, but it is not really discussed at all in the paper.\n\n6- On the same note, I do not think that one can say Osband's method is the same as CIKD but only without the KL update. Most notably, Osband's work was presented for value-function-based methods like DQN. These methods work fundamentally different than policy gradient methods, which rely on (near) on-policy updates to perform good policy improvements. In that sense, the presented results make sense, but I disagree with the framing of the results and how they are presented here.\n\n7- In section 5.3, when the authors utilize more policy updates to have a fair comparison, are they retuning hyper parameters? Surely they need to do that, at least for hyper-parameters that are known to be super important such as the step size.\n\n8- Overall I liked section 5.5 that is trying to dissect causes for improvement. However, it seems like that the 'dominant agent' hypothesis has been rejected hastily, unless I misunderstood the experiment. The authors show that the notion of best is spread across different agents. But of course this will be the case in light of the KL update, since the policies are getting closer to one another. Can you redo the experiment in the absence of the KL update?\n\n9- Have the authors thought about any connection between this and genetic algorithms? In genetic algorithms, the idea is the next set of candidates are chosen based on the most promising candidates in the current iteration. CIKD seems like a soft implementation of this idea.\n\nIn light of the comments above, I am voting for weak rejection, though as I said before, I do see some interesting things in this paper. I encourage the authors to think about CIKD from a theoretical lens in the future.", "belong_id": "BkeYSlrYwH"}, {"uid": "rklkefNcFr", "paper_title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper studies the adversarial detection problem within the robust optimization framework. They propose an adversarial detection and a generative modeling technique called asymmetrical adversarial training (AAT). With one detector for each class discriminating natural data from adversarially perturbed data, AAT can learn class-conditional distributions, which further result in generative detection/classification methods with competitive performance. Experimental results are provided on MNIST, CIFAR10 and Restricted ImageNet, compared with CW method as baseline.\n\nThe paper is well written with detailed experimental results. I'd suggest accepting the paper.\n\nTo my understanding, the objective function of AAT is similar to GAN's, while there is a detector for each class discriminating natural data from adversarially perturbed data instead of generated data. They incorporate the attack into the training objective with three attacking scenarios: classifier attack, detectors attack, and combined attack. They also introduce integrated classification of the classifier and detectors with the reject option. Further, they demonstrate ATT promotes the learning of class-conditional distributions and leads to generative classifiers. They claim in addition to more robust classification, ATT also gives rise to improved interpretability, which I'm not convinced of with given experimental results.", "belong_id": "SJeQEp4YDH"}, {"uid": "B1xdR_S6YH", "paper_title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Review: The paper addresses the adversarial example detection problem. The framework proposed in the paper divides the input space into subspaces based on a classifiers output and trains detectors on these subspaces to classify a natural sample (classified as that class) from an adversarial one fooling the network. The goal is to use a robust optimization approach to enable detection methods to withstand adaptive/dynamic attacks. Hence, an asymmetrical adversarial training (AAT) regime is employed which presents solving a min-max problem. AAT supports the detectors to learn class conditional distributions, motivating generative detection/classification approaches. There are three different attacking scenarios and evaluation shows that the combined attack turns out to be most effective (as it fools both the classifier and detectors) against integrated detection. The paper also demonstrates empirical improvements over state of the art detection techniques with higher L2 distortion of perturbed samples.\n\nPros:\n- With the vulnerabilities associated with neural networks, the motivation behind building defense mechanisms against adversarial attacks has been well-justified.\n- Most of the evaluation metrics look appropriate and well-defined.\n- It was interesting to observe the perturbed samples produced by attacking generative classifier. While they exhibited visible features of the target class, these perturbations had to be different on a sematic level to be distinguished from the natural samples. \n\n\nCons:\n- While the idea to partition into subspaces and learn a different detection for each of them is novel, it involves training multiple detectors one by one that can be computationally expensive. The loss function for different attacking scenarios uses the outputs from all the detectors, which can also be expensive, especially when there are a lot of classes.\n- To deal with extremely unbalanced data sets when training the detector, the solutions used in the paper resamples to balanced the positive and negative class data. This would mean throwing off most of the data, I would see how it affects the training. \n", "belong_id": "SJeQEp4YDH"}, {"uid": "SklzDtn-9H", "paper_title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method for adaptive adversarial example detection. The authors propose to construct the adversarial subspace detector based on Asymmetrical Adversarial Training (AAT). The proposed model is composed of both classifier and adversarial detector, where the classifier makes the classification prediction and the adversarial detector evaluate if the input sample is natural of adversarial. The goal of the objective function is to minimize the adversarial detector error given large enough perturbation budget.\n\nThe authors provide extensive experimental results showing the promising performance of the model in detecting various types of adversarial attack. I have several concerns regarding the model and experiments:\n\n1) Since D^{'^{f}}_k \\subset D^f_k, would the model minimize w.r.t. both the loss of L(h(x, \\theta), 0) and L(h(x, \\theta), 1)? Would this cause unstable training?\n\n2) Maybe I missed it, but it seems that the objective function in Eq. (5) is based on the adversarial detector. How could the classification performance of classifier f be guaranteed in training?\n\n3) What does the cross mark mean in Fig. 2(b) and 4(b)?", "belong_id": "SJeQEp4YDH"}, {"uid": "SygNWhNWtr", "paper_title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Contributions: This paper builds an ad-hoc connection between the Transformer and the numerical ODE solver (the Lie-Trotter splitting scheme and the Euler's method) for a convection-diffusion equation in a multi-particle dynamic system. Then, the author(s) developed an ad-hoc Strang-Marchuk splitting style architecture, named Macaron Net. Finally, this paper provides some experiments to verify the performance of the proposed architecture. However, the comparisons with the benchmark results are questionable. I have listed my concerns in the Experiment section.\n\n\nMotivation: This paper developed the Macaron Net based on a locally third-order operator splitting scheme for the convection-diffusion equation. However, there is no theoretical interpretation of why third-order splitting corresponding to better architecture. Theorem 1 in the paper is a known result, and it is irrelevant to the paper, I highly recommend the author to remove it from the main text. I also suggest the author explore more operator splitting schemes and do a systematic comparison between them. Moreover, I think it will be a real contribution if the author can analyze the error between the numerical scheme and architectures.\n\n\nReformulate Transformer Layers ans an ODE solver for Multi-Particle Dynamic System: There is a big gap between Eqns 3, 4 and 5. Why F represents a diffusion term, why G represents a convection term? From a statistical mechanics point of view, this comparison does not make sense. I do not buy this model.\n\n\nRelated Work: There is no related-work section that discusses the related work, and all the referenced papers are generic. For instance, the efforts in developing language models, the application of convection-diffusion equation, and String-Marchuck and other operator splitting schemes in machine learning. The author should better position the paper to exist work.\n\n\nExperiments: This section is extremely questionable. My initial thought after reading the reported results is that the architecture proposed in this paper easily outperforms the existing work. However, after I do a cross-check with the existing work, I found the author did not compare with the best results reported in the benchmark work and hide much information. After simply checking two existing papers, I found that the author ignored the comparison with BERT large. Also, the author ignored the most important result reported by Wu et al. 2019b.  To be fair, the author should perform an apple-to-apple comparison with the existing work and report the uncertainties in their results. Moreover, the author should report the parameters used in all their experiments.\n\n\nI think this heuristic study might be a contribution to ICLR if all my concerns are addressed, and I am willing to raise my rating to accept.", "belong_id": "SJl1o2NFwS"}, {"uid": "S1eKfLh4KH", "paper_title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work, the authors show that the sequence of self-attention and feed-forward layers within a Transformer can be interpreted as an approximate numerical solution to a set of coupled ODEs. Based on this insight, the authors propose to replace the first-order Lie-Trotter splitting scheme by the more accurate, second-order Strang splitting scheme. They then present experimental results that indicate an improved performance of their Macaron Net compared to the Transformer and argue that this is due to the former being a more accurate numerical solution to the underlying set of ODEs.\n\nThe authors highlight an interesting connection between the Transformer architecture and ODEs. In particular, they derive a set of ODEs that is solved numerically by the Transformer and borrow from the body of literature on numerical ODE solvers to improve the architecture. I find that this is a very elegant and promising approach for finding better architectures. \n\nHowever, I also identified two major and a couple of minor shortcomings of the paper that are explained in detail below. Based on these shortcomings, I recommend rejecting the paper but I would be willing to increase the score if these points were addressed in sufficient detail.\n\nMajor points:\n\n1) Replacing the first-order operator splitting scheme by a second-order scheme only guarantees a lower overall truncation error if the split ODEs are solved with sufficiently high accuracy. In particular, the overall accuracy of the numerical solution to the original ODE depends on the accuracy of the operator splitting and the accuracy of the integration scheme used to solve the split ODEs (e.g. Eulers method). The authors improve the operator splitting, i.e. they make it second-order, but they keep Eulers method to integrate the individual ODEs. Because of that, they actually do not get rid of the lowest-order error term of the overall scheme and therefore do not obtain a more accurate ODE solver. I think this is a crucial point that invalidates the authors claim that the Macaron Net employs a higher-order integration scheme. As far as I am aware, this is not commented on in the paper at all. To address this shortcoming, the authors could replace Eulers method by a second-order integrator. \n\n2) The experiments considered in this paper are interesting and show competitive performance but, in my opinion, they do not sufficiently support the claim that the Macaron Net yields a more accurate solution to the underlying set of ODEs compared to a Transformer. For a more convincing support of this claim, the authors could consider a toy problem, i.e. a simple set of ODEs with known analytical solution, and actually show that the Macaron Net is more accurate. The accuracy of a numerical ODE solver is commonly assessed by plotting the absolute difference between the exact solution (or a high-resolution numerical approximation to it) and the numerical solution vs the timestep (here \\gamma). I suspect that such an analysis would support my previous comment and show that the proposed new architecture is not more accurate ODE solver than the original one.  \n\nMinor points and questions:\n\ni) Eqs. (17-19) suggest that you apply two different FFN layers (doubling the number of parameters) instead of applying the same FFN layer twice. You comment on this in Sec. 4.1 when you say ..., we set the dimensionality of the inner-layer of the two FFN sub-layers in the Macaron layers to two times of the dimensionality of the hidden states. It is not clear to me why consistency with Strang splitting requires two different layers rather than applying the same FFN layer twice. Is the reason for having a separate, trainable layer to account for the explicit time dependence of G in Eq. (16)? I think that this is a very important point that should be clarified.\n\nii) I think that this type of system is usually referred to as dynamical system and not dynamic system. Please check that and, if applicable, update the title.  \n\niii) The authors say that Eq. (5) is a 'convection-diffusion equation. As far as I am aware, the diffusion equation is a partial differential equation (PDE). Perhaps there is a different notion 'diffusion equation in ODE theory. If thats the case, could the authors please clarify this point to avoid confusion, e.g. by adding a suitable reference in which this type of ODE is classified as a convection-diffusion equation? \n\niv) In Sec. 2 (2nd paragraph),  you cite Vaswani et al. (2017) but in that work the quantity under the square-root in the denominator of Attention(Q, K, V) is actually d_k, the dimension of the key, and not d_model.\n\nv) Figure 1 is a very vague illustration of the connection to ODEs and provides almost no explanation in the caption. I dont think there is much value in having this figure there.\n\nvi) There are a couple of mistakes in the paper (grammar and expressions) that should be fixed. For example, the Eulers method instead of Eulers method, movement in the space instead of movement in space, dynamic system instead of dynamical system, project parameter matrices instead of parameter matrices or projections, specially instead of specifically, etc. Please take a look at the relevant sections in the paper and revise them accordingly.\n\nvii) You explain multiple times why the proposed architecture is called a Macaron Net (Abstract, Sec 1, Sec. 3). To avoid repetition, I would only explain it once.\n", "belong_id": "SJl1o2NFwS"}, {"uid": "HyexZfMiKB", "paper_title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper points out a formal analogy between transformers and an ODE modelling multi-particle convection (the feed-forward network) and diffusion (the self-attention head). The paper then adapts the Strang-Marchuk splitting scheme for solving ODEs to construct a slightly different transformer architecture: FFN of Attention of FFN, instead of FFN of Attention. The new architecture, refered to as a Macaron-Net, yields better performance in a variety of experiments.\n\nPROs\n1. The proposed new architecture is fairly simple.\n2. The experimental results are fairly good.\n\nCONs\n1. Introducing two feedforward layers with *different* parameters W^{down} and W^{up} is a significant deviation from Strang-Marchuk splitting. I expected the two FFNs inside the Macaron to have the same weights. As I understand it, the motivation for the splitting is to improve the numerical performance of the update scheme for the ODE. In contrast, allowing different weights for the FFNs means the physical process is now a lot more free. Is there any physical motivation for the different parameters? (Beyond the fact that it improves performance). How much worse is empirical performance when the parameters are the same?\n\n2. Following on from the above point, the analogy between the multi-particle system and the transformer is quite weak. The equations look similar when you squint the right way. But thats as far as it goes. Fig 1 is a nice visualization, but it doesnt provide insight into the dynamics of transformers. What does Particles move in the space along time (Semantics encoded in stacked neural network layers) mean? How do the particles connect to the semantics? \n3. The proof of Bobylev & Ohwadas theorem is included in the paper. Is there any connection between the theorem (or the techniques used in its proof) and transformers? I suspect the answer is no.\n\nSUMMARY\nIn short, the paper (i) proposes two FFN layers instead of one in each block of the transformer and (ii) shows it performs slightly better than before. This is decent, but in my opinion not enough the clear the bar for ICLR.\n\nThe connection to multi-particle ODEs is genuinely interesting. However, it is not sufficiently fleshed out to count as a contribution (yet). Its possible the authors have discovered something deep. Its also possible they got lucky with a physically motivated modification of transformers that actually has nothing to do with the dynamics of multi-particle systems. Im not sure what further experiments would be needed to make the case. But I recommend the authors dig into the equations and the dynamics to see what it really going on under the hood. Just showing improved performance on a few benchmarks is not enough to convince the connection is solid. \n", "belong_id": "SJl1o2NFwS"}, {"uid": "rJxOXFAoFS", "paper_title": "Global Adversarial Robustness Guarantees for Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper seeks to analyze the global robustness of neural networks, a concept defined in the paper. The authors show using concentration inequalities that the empirical local robustness approximates the global robustness. The authors investigate various other issues in the robustness literature, including the robustness/accuracy tradeoff, whether iterative pruning increases robustness, and the robustness of Bayesian networks.\n\nI would vote for rejecting this paper for two key reasons. First, the notion of global robustness is not well-motivated (why do we want to compute this metric? What does it tell us that local robustness does not?). Second, the paper tries to do too many different things, and as a result does not give enough attention to any particular topic.\n\nFirst, I believe it is up to the authors to motivate their study of global robustness further. While I acknowledge that a few prior works exists along these lines, I do not feel that this work provides much new insight into why global robustness is interesting to examine.\n\nThe authors go on to prove results showing that an empirical estimator of the local robustness will converge to the global robustness. The bounds require that the dataset size scales with eps^-2, where eps is the error. This is not terrible but also not great; for example, achieving 1% error requires a dataset size of 10^4 (realistically, even larger datasets would be required to achieve results with high probability).\n\nNext, I would suggest that the authors avoid using the word guarantees if they are estimating empirical local robustness in an approximate (rather than exact) manner. Guarantees implies strict results, but the authors use a weak attack (FGSM) to approximate empirical local robustness. The results from FGSM could be far from optimal; the authors could use a stronger attack (e.g. PGD) in addition to changing the wording, or they could find provable guarantees using alternate methods.\n\nLastly, the authors try to tackle 3 extra questions beyond global robustness toward the end of the paper, and the last two questions are not properly fleshed out.\n\nI like section 4.2, where the authors empirically show that networks that have better hyperparameters (for regular accuracy) tend to be less robust. This is a confirmation of a previously studied phenomena in the literature. Ideally, I would also appreciate it if the authors found the line of best fit to the dataset in addition to the plots provided. I would like a clarification on whether any of these networks were trained to be robust, although it appears that they were all trained normally. I would also like to see plot 2c (for the standard case of robustness of C(x_tilde) = C(x)), except for MNIST and CIFAR10 as well. I feel that the last-layer representation metric the authors analyze (f(x) is close to f(x_tilde)) could be misleading, as robustness on the last layer does not necessarily imply standard adversarial robustness.\n\nSection 4.3 explores iterative pruning, but that seems fairly unrelated to the rest of the paper. Finally, Section 4.4 tries to show the opposite trend for Bayesian Neural Networks, but unfortunately the results for such networks do not yet scale beyond MNIST.\n\nAdditional Feedback:\n\n- Why did you use R^emp and D^emp as opposed to just R^emp(g) and R^emp(g_bar)?\n- In Figure 1, what is the dataset size |S|?\n- In the last sentence of Section 4.3, I didnt understand what you meant about the relationship between weight pruning and network regularization. Do you mean that weight regularization has no effect on robustness, just like iterative weight pruning?\n", "belong_id": "BJgyn1BFwS"}, {"uid": "SkxcESapFS", "paper_title": "Global Adversarial Robustness Guarantees for Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe paper formally defines local and global adversarial robustness. Following that, the paper investigates how to estimate local and global adversarial robustness using an estimator based on evaluating these quantities on the empirical distribution. Using a Chernoff bound, the papers evaluate probabilistic bounds on the deviation of the estimated quantities from the true quantities.  Finally, simulations are provided to evaluate these bounds for examples.\n\nComments:\n\nThe authors' insistence on their contribution being proving measurability does not make sense -- of course everything is measurable! Furthermore, the formal definitions or local and global robustness are well-known, the bounds in Theorems 1 and 2 are not novel and highly unlikely to be tight. The redeeming aspect of the paper is the experiments, where the authors show that these bounds can actually be (approximately) calculated. However, I feel that merely experimental results with correct but not significant theoretical contributions does not meet the bar for acceptance. ", "belong_id": "BJgyn1BFwS"}, {"uid": "Skl1SoDAFH", "paper_title": "Global Adversarial Robustness Guarantees for Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the adversarial robustness of neural networks by giving theoretical guarantees, providing statistical estimators and running experiments. It is a lot of work and it is reasonably written. The problem is that a fair bit of it is quite basic: for example the measurability property is very much expected -- noone was doubting it, and the proof is more of a formality than a contribution. Similarly with the statistical sampling: the method seems to rely on i.i.d. sampling -- has this reviewer missed any important details? If not, then it's only the bounds that are a contribution, but the method is not. We would appreciate more specific description of the main contribution, without it we cannot recommend the acceptance of this paper.\n\nI am very grateful to the authors for their response. I feel now that a main weakness of this paper may be that it puts too many results in one place. I would strongly suggest re-writing it, possibly into separate papers, to make the things pointed out in the response more clear and self-standing.", "belong_id": "BJgyn1BFwS"}, {"uid": "rJe7ZT5l9S", "paper_title": "Task-Based Top-Down Modulation Network for Multi-Task-Learning Applications", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper tackles the multi-task setting by using modulation connections between three network pipelines, i.e., one bottom-up network to contextualize the problem, one top-bottom network that is conditioned on the tasks, on a last bottom-up network that solves the task. \nThe key contribution of the paper is to introduce a feature-wise and spatial-wise tensor to modulate the different neural pipelines better. Finally, they assess the proposed method on three datasets: Multi-MNIST, a yes/no CLEVR, and CUB-200.\n\nThe abstract, introduction, and related works are pretty clear. Figure 1 is also a nice summary that puts the paper architecture in perspective with another approach, and it is a very insightful sketch. I appreciate the effort of the authors to release the code with several baselines. I also acknowledge the diversity of tasks that are studied.\n\nHowever, I have three concerns that I am willing with the authors.\n\nMy first concern deals with the method description, which I found a bit misleading. Thus, I not sure that I fully got all the subtleties of the proposed method. The mathematical notation is misleading: Are Y, and X function of (x,y,ch) or are tensors over x, y, z. Later in the text, W is defined over (ch, t), but it is also mentioned that t is an input. Thus, is W \\in R^(CxT) or W(t) \\in R^(C). Besides, the implicit tilling with * makes things even harder to follow. On a different side, what do you mean by training the convolution network instead of optimizing W. Is W fixed? Do you use simply the feature map after 1x1 conv as mentioned in 4.2? How do you embed the task t in general, how do you append it to the feature map of BU1.\nIn the current paper state, I would not be able to reproduce the experiments.\n \nThe second concern relies on the results. The gap between the methods is tiny, e.g. max 0.5 in 2-MNIST, and may fluctuate a lot from one experiment to another, e.g., it is weird that 3-MNIST is harder than 4-MNIST. Note that the same observations can be applied to the CLEVR. Therefore, it is hard to assess the method without the std over at least 5 seeds. The result only convinces me regarding 4-MNIST without such std.\n\nMy third concern is about CUB200 experiments. The authors used an auxiliary loss on top of TD to help to visualize the network decision. As such auxiliary losses provide additional information, I have the following question: did you add the same losses to other baselines? Did you use a stop-gradient before decoding the feature-maps? Otherwise, the comparison between methods may not be fair\n\n\nRemarks:\n - I am missing some results from external literature. For instance, even if I prefer your setting CUB300  over 312 questions, it would have be nice to add such experiments in the appendix. (or literature baselines on N-MNIST)\n - Please report the original MOO too results in addition to your experiments\n - Why ch-mod is missing in 4-CLEVR?\n - Can you describe how did you pick the CLEVR questions (before/after computing the results)? It would have been nice to have experiments that randomly pick K questions (and repeat the process N time + report std), or even dynamically condition on the question at hand.\n - it took me quite some time to understand the meaning of #P, please make it explicit from the beginning, or add in the caption!\n - Although releasing the code is good, I also encourage you to put a table in the appendix with the hyper-parameters. The paper should be as much self-content as possible. It is also hard to evaluate the quality of the training time during the review\n - typo: extra parenthesis in Eq 4\n\nIn conclusion, the authors give some good intuition about promising methods, but I had some difficulties in understanding all the details of their approaches. Besides, I am missing both std and external references to assess the quality of the methods. In this current state, I cannot recommend paper acceptance even if I acknowledge several qualities of the paper, but I am open to discussion.\n\n", "belong_id": "BklBp6EYvB"}, {"uid": "BkgA0-5RqH", "paper_title": "Task-Based Top-Down Modulation Network for Multi-Task-Learning Applications", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a new light-weight framework for multi-task learning. In this method, the combination of extracted image features and task are fed into a top-down network which is responsible for generating a task-specific weight matrix. The weights are next convolved with the input image as an input to the task-agnostic bottom-up network that generates the labels.\n\nThe idea of the paper interesting. The main shortcoming of the paper in my point of view is that all the numbers are reported as a single number, so they are prone to be changed by using different initial networks or optimizers. Here are some more comments:\n\n1) One limitation of the result section is that all the numbers are reported as static numbers. I am interested to see the training curves, either in using the wall-clock time or iteration in the x-axis and testing accuracy in y.\n\n2) Sections 3.1 and  3.2 as the main parts are not well-written. The shapes of the tensors are vague. What is the y,x in the parentheses? What does ch stand for? (defined?) I think that this part of the paper requires significant improvement.\n\n3) One valid question is how the proposed method is scalable. For example, can a model trained for 3 tasks used for 4 tasks? How hard is adding a new task? Also, worths comparing it with the learning from scratch. \n\n4) In Section 3, the discussion about the loss function is missing. I believe that the explanation of how to choose a loss function as well as auxiliary losses should be move there. Also, I didn't find the current explanation of BU1 and TD auxiliary loss for Multi-MNIST very clear.\n\n5) Why the results of your method is better than the single model? This behavior should be justified. My impression is that each task trained independently should outperform any multi-training method. Your results seem counter-intuitive in this respect.\n\n6) I am not able to make any strong conclusions from Section 4.3.2. It is really hard to tell which connection is better based on a single number. I would suggest providing confidence intervals for making such kind of arguments. For example, you may train from 10 different network initializations and use them to construct more reliable estimates. I also believe that more reliable estimations are required for Table 3. \n\n\nMinor:\n* In paragraph 2 of pages 2, you mention 'as illustrated in Figure 2a'. I do not see the attention to a part of the image. Am I missing something? A similar issue exists in the next sentence: I don't see any content-related modulization in Figures 2b and 2c. Please clarify.\n* use comma after equations if the equations are not ending the sentences. For example, add a comma after eq (1), (2) and (3). Also on page 4, 'Where $W$' -> 'where $W$'.\n* Page 4, 'Our method address' -> 'Our model addresses'\n* Where the third column of Table 1 is defined? On page 8. Move it to earlier sections.\n* In Table 2b, you have used +x, but the notation for gated modulation is something else in the text.\n* Are LL and RU used in Table 1 defined in the text?\n* The bold numbers in Figure 2b seem wrong. If you are bolding the large accuracies, be consistent in all tables.\n* Font of table 4 can be larger", "belong_id": "BklBp6EYvB"}, {"uid": "BkgBBW0RcH", "paper_title": "Task-Based Top-Down Modulation Network for Multi-Task-Learning Applications", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "> What is the specific question/problem tackled by the paper?\nThis paper tackles a restricted multi-task setting where the task is known. The main contribution is a new architecture for training a task conditional model. The new architecture is reminiscent of an encoder-decoder-classifier with ladder (latent) connections, the decoder is conditioned on task ID. The claim is this is a type of modulation, it is unclear. Results on three multi-task datasets show that the proposed method is slightly better than compared methods and single task learning. There is no theory or loss function to analyze.\n\n> Is the approach well motivated, including being well-placed in the literature?\nIn my opinion, this is lacking. The assumption that task ID is known is fairly severe. Unfortunately the prior works cited also have this restriction, whereas few papers under the topic of continual learning have removed this limitation. This assumption/drawback needs to be clearly mentioned in the paper and discussed if it is realistic? A related shortcoming is that the training data simultaneously comes from all the tasks, whereas prior work has looked at the more interesting setup where tasks arrive sequentially and incrementally. \n- Reference [1] seems relevant and should be cited as it shows context dependent gating of tasks / modulation as well. Other missing references e.g. learning without forgetting (LwF) [2] and [3]. \n- There is not a clear explanation to think that this is modulation since the result is only passed through a residual connection. More importantly there is no discussion on these important issues. I found the writing to be brief and sketched. \n- in the introduction, it would be good to define multi-task learning with the assumption made clear. It would be good to introduce what you mean by TD and BU clearly\n- Another drawback is assuming the tasks being encoded as integers, whereas there might be a continuous task space with interpolation, or hierarchical task structure.\n- 'However, all of these works modulate the recognition network\nchannel-wise, using the same modulation vector for all the spatial dimension of the feature-maps.' - why is this not enough? A nontrivial explanation or discussion is needed. Simply extending to W(t, y, x, ch) would increase performance by a little.\n- how is the proposed model different from a conditional model like a task conditional classifier? Also in experiments.\n- How is the proposed model different from an encoder-decoder? The impact of 'modulation' is not clear.\n- 'We can scale the number of tasks with no additional layers.' - task conditional classifier can also scale in this way to the number of tasks. This claim is not valid.\n- Page 3: 'uncorrelated gradients from the different tasks' - need not be uncorrelated, but still can be interfering\n- next about Kendall (2018) and Sener (2018) - need to compare and contrast to them.\n- Last para on page 3 seems not relevant.\n- Modulation equations: this seems specific to CNNs. How would you extend this technique to beyond CNNs to recurrent units or even simpler MLPs? Modulation as a technique has been successfully applied in these architectures as well.\n- 'added to the input tensor X through a residual connection' - this is not clear at all. Are the residual connections not shown in Fig 1(d)? \n- 'it to be unfeasible due to their large dimensions' - can you explain please? later you say 'To avoid the unfeasible computation burden of directly optimizing W'\n- Fig 1d, would be good to mark the modulation arrows in a different color\n\n> Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\nHaving said that, I like the experimental section even in the restricted setup. But a lot of details are missing. It is not surprising that there is slight increase in performance over channel modulation due to the increase expressivity. \n- table 1: why is there degradation in the LL task across all methods? the introduction of an additional task seems to bring the performance back up. It seems to be a weakness of your method. Please improve the discussion. I'm inclined to think that the tasks are not uncorrelated, as claimed by the authors. \n- table 1: how did you arrive at the number of parameters like 1.12x? Doesn't the separate BU and TD nets mean you have at least 2x parameters compared to single task? It seems the larger number of params in single task is mainly coming from the hidden layers?\n- table 1: it would be fair for the comparison methods to have equal number of parameters as the proposed method.\n- Missing experimental comparison to Kendall et. al. 2018\n- Missing details about reproduction of results from Sener (2018)\n- An important baseline would be to show image sensitive full tensor modulation without the new architecture. Similar to XdG.\n- Another baseline should be a task-conditional classifier that takes task as input along with the image. \n- ablation study: what are the auxiliary losses? I could not find any details.\n- The third experiment with CUB seems to use a different loss function that the other methods. This is somewhat hard to evaluate.\n- number of parameters are not reported for the CUB experiment\n- 'where only a single pixel is labeled as foreground, blurred by a Gaussian kernel' needs more details about the smoothing\n- In the CUB experiment, due to lateral connections, the top-down result 224x224 image is not the only input to the BU2 classifier, the interpretability argument is weak.\n- Why did you choose these 4 questions from CLEVR? There are many interesting types of questions that can be handled.\n\n", "belong_id": "BklBp6EYvB"}, {"uid": "r1eYj0gftS", "paper_title": "Disentangling Improves VAEs' Robustness to Adversarial Attacks", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I am a bit confused about the model. While i understand the generative model in Fig 2a, i am a bit puzzled about the choice of proposal distribution eq(7)/Fig 2b for the Seatbelt-VAE. The key claim of the paper is that an attacker has to attack all layers at the same time to attack the reconstruction in eq (12). However, Figure 2b and eq(7) claim that all deeper layers only depend on the previous layer in the approximate posterior. Since in (12) we rely on the posterior for the attack, a successful attack on layer m < L should immediately generate the correct values from q_phi(z_{i+1}|z_i) i=m,...L-1.  So from that point of view it is not a seatbelt, as since in Fig 2b if the attacker manages to control z^1, he has immediate control of z_2 and therefore the now  attacker controlled z_2 will directly feed into the generated target. What might be is that the optimization problem (12) becomes harder to solve because of the increased model-complexity. \n\nI am not too impressed by the attacks presented in Fig1 as well as the appendix. One of the key points of the old adversarial attacks was that the attack-image was indistinguishable from the true image by a human.  However, the adversarial inputs, even for VAEs are clearly not part of the distribution and the errors reported for eq (12) are very large to the point where attacking via the target image would probably be harder to spot. If we for example look at page 24, second row: there is no way, that the adversarial image has a likelihood similar to the target. This looks more like the algorithm did not manage to find a suitable direction.\n\nI am therefore not sure whether the evaluation is correct: if we did not manage to find an attack image, does it proof there is none? And is it meaningful to report their error values if we did not manage an attack?\nBtw: did the optimization of (12) begin with d=x_t-x or d=0? maybe starting with d=x_t-x would be more meaningful because it would make it easiest for an attacker to ensure the correct reconstruction.\n\nGiven the quality of the attack images, the error of eq(12) should be reported when choosing d=x_t-x as a baseline in Fig 5. It would also be good to see the actual VAE values.\n\nUnfortunately, the reconstructions on dsprites are bad. But an important experiment could be to check whether you can attack the orientation of an object. Orientation is difficult to regularize via TC, since the parameterisation is inherently circular. Thus TC might make it difficult to encode orientation in higher layers and it should be easier to attack.\n", "belong_id": "rkeZ9a4Fwr"}, {"uid": "SJxmteaQtS", "paper_title": "Disentangling Improves VAEs' Robustness to Adversarial Attacks", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors of this paper propose a new VAE model called seatbelt-VAE and investigate its robustness to output and latent adversarial attacks. Inspired by beta-TCVAE, DLGM, and BIVA, seatbelt-VAE allows multiple latent layers, enforce disentanglement via weighted total correlation on the top latent layer, and conditions the likelihood on all latent layers. Robustness to adversarial attacks is the focus in experiments. Visual and quantitative comparisons show that seatbelt-VAE is more robust for latent attack than benchmarks. Specifically I have the following three concerns:\n\n1. Defining ELBO using samples from the entire dataset may bring in some benefits, but it complicates the calculation of ELBO and related distributions when minibatch or single sample are used in learning and inference. Please explicitly discuss this issue.\n\n2. I would like to see how seatbelt-VAE performs in sampling and generating new samples, instead of just reconstruction. \n\n3. Similarly, it would be beneficial to investigate disentanglement, that is the interpretation of the top latent factors in seatbelt-VAE. \n\nMinors: \nFactor analysis -> factor analysis\nsection X -> Section X\nfigure X -> Figure X\n", "belong_id": "rkeZ9a4Fwr"}, {"uid": "BkelA7lkqr", "paper_title": "Disentangling Improves VAEs' Robustness to Adversarial Attacks", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper examines adversarial attacks to a VAE. It is known that by small norm perturbations on the conditioning input x of a VAE can dramatically change the generated output. This paper\nempirically illustrates that alternative objectives can improve robustness, in the sense of previously proposed adversarial attacks such as (Tabacof et al. (2016); Gondim-Ribeiro et al. (2018); Kos et al.(2018)). This paper is concerned with the stability of reconstructions and their quality, and proposes Seatbelt-VAE to remedy some of the shortcomings in the original VAE objective. \t\t\n\nThe key idea of the Seatbelt-VAE is introducing a conditionally Gaussian chains (of length L) in the encoder and decoder distributions of a VAE. This is a plausible and sensible idea. Then the authors evaluate the robustness of reconstructions under various output attacks.\n\nThe methodological part of the paper is quite well written and easy to follow, despite the fact that it is somewhat overloaded with too many abbreviations. The experimental section is harder to read as the motivations and its organization is not clearly stated. Overall, this section feels as if it is too hastily written, many results put into appendix without much discussion. The organization can be much more improved.\n\nThe disentanglement achieved by this novel representation is characterized only anecdotally and by contrasting the resulting objectives to a beta-VAE. It would have been much more informative to illustrate and discuss further the representations learned by such a conditionally Gaussian architecture. Figure 6 and 7 partially try to achieve this by showing the interplay of depth L and the inverse-dispersion parameter beta but I found it hard to interpret this results, for which an entire page is devoted.\n\nIn the experiments, the ELBO is reported for various methods. I would argue that the ELBO is not a very representative proxy for robustness. For example VAE ELBO and beta-VAE ELBO are both lower bounds of the true marginal likelihood and it is possible that beta-VAE is much lower while attaining a higher robustness in the sense of being resilient to suitably defined attacks.\n\nThe authors claim that there are no clear classification tasks for the datasets -- but this is not accurate as both celeb-a has clear classification tasks in the form of predicting attributes. It would have been really quite informative if adversarial accuracy on downstream tasks would have been reported. Relying on qualitative results in Figure 1 is only providing partial evidence about the approach.\n\nRobustness to independent noise, as the authors have, is a good experiment to have -- however typical adversarial examples may be quite structured and such a randomized strategy may not give an accurate indication about the nature of the representation.\n\nOverall, the paper is quite promising but I feel that one more iteration maybe needed.", "belong_id": "rkeZ9a4Fwr"}, {"uid": "ryxSuiIjFB", "paper_title": "AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing     ", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a method for point-based learning that is inspired by a hybrid Eulerian-Lagrangian fluid simulation method. The work first explains how the simulation algorithm is mapped to the learning problem: MLPs are employed to learn sets of particle based features which are mapped to a Eulerian grid. A second MLP infers a particle based velocity, which is likewise mapped to the grid and used to advect the grid quantities. This is repeated for a certain number of steps to obtain final positions. The 'warped' features are then projected back onto the particles to solve, e.g., a classification task. In contrast to a typical flow solver, the motion can be divergent, i.e., not necessarily conserves volume.\n\nThe paper presents a brief ablation study for number of iterated steps, grid size and point count, before presentation two comparisons with existing baselines.\n\nOverall, I found the idea to employ FLIP for Lagrangian learning tasks novel and very interesting. Unfortunately, the paper (as mentioned in the text) only contains only a somewhat preliminary study. The method does not yield clear gains over previous work, but rather a similar performance for classification and segmentation of ShapeNet and S3DIS data is shown. Given the fairly complicated construction, I think it would be important to actually show improvements at least for specific learning tasks. Several of the deformations shown in figure 5 and 6 are also not really intuitive\n\nAlso, on second sight, I don't fully understand the motivation for employing and learning a grid based deformation. The grids seem to inherently limit the spatial extent of the point clouds, and the features that can be resolved. Features smaller than a grid cell will essentially 'stick together', and can't be separated. It's also not obvious how to choose parameters such as the number of time steps. Intuitively, I'd expect the method to 'converge' to a position for a larger number of steps.\n\nTo conclude, the direction this paper takes is certaily new and interesting, but the preliminary results in combination with the complexity and limitations introduced by the grid-based representation make me hesitant to recommend accepting this paper in its current form. (The nine pages also contribute to this assessment.)\n", "belong_id": "H1eqQeHFDS"}, {"uid": "SJghlklntH", "paper_title": "AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing     ", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper is about using classical PIC/FLIP scheme in Computational Fluid Dynamics for solving the learning problem of 3D object detection and segmentation. In general, there are extrinsic CNNs like the Vox net etc. which look for global features which the authors refer to as Eulerian formulation of the data representation, and there are intrinsic CNNs like the GCN(graph convolutions), Point nets etc. which look for localized neighborhood information which the authors refer to as Lagrangian formulation. The authors acknowledge that hybridizing the extrinsic CNNs and intrinsic CNNs is not new and several works are cited. The key contribution is to look at this problem from the perspective of PIC/FLIP scheme which has been used in CFD for decades. \n\nThe idea is very nice, well describes and quite novel in my opinion. I really liked the adoption of classical CFD approaches in learning. This provides a very interesting perspective to 3D deep learning. \n\nHowever, the papers struggles to demonstrate why the 3D deep learning community would adopt this approach. The results are not that conclusive. The algorithm works (understood well from the ablation study). However, the performance of the proposed approach is at best comparable to some of the state-of-the-art methods such as PointCNN or SE-Net. The authors need to clarify what potential advantages could there be other than accuracy (if any) such that the community uses the proposed method. \n\nAlso, the grids used in the study are too low to make any conclusive remarks on what happens when dealing with higher resolutions of grid. The authors themselves acknowledge the limitation of not being able to go higher in resolution of grid. Interestingly, such limitations of CFD has recently motivated the community to explore deep learning based fast and agile surrogates for computationally tractable approaches.  \n\nSome of the new works in 3D object recognition and segmentation such as  Deep SDF(https://arxiv.org/abs/1901.05103), AtlasNet(https://arxiv.org/abs/1802.05384),  Deep Level Sets (https://arxiv.org/abs/1901.06802), occupancy networks (https://arxiv.org/pdf/1812.03828v1.pdf), http://openaccess.thecvf.com/content_cvpr_2018/html/Yu_PU-Net_Point_Cloud_CVPR_2018_paper.html, Adaptive O-CNN (https://dl.acm.org/citation.cfm?id=3275050), https://arxiv.org/abs/1805.12254, 3D Point Capsule Networks, http://t.cvlibs.net/publications/Niemeyer2019ICCV.pdf etc. can be compared with or at least contrasted in the related works.\n\nIn summary, I really liked the algorithmic idea, but skeptical about its practical relevance from the results.", "belong_id": "H1eqQeHFDS"}, {"uid": "HJgxfRhEqS", "paper_title": "AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing     ", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper addresses the task of learning with point clouds for semantic labeling (classification and segmentation). The authors propose a novel point-based architecture based on viewing the learning process as an advection in the 3D space. This formulation aims at an explicit connection the two formulations for learning with point clouds, the first being focused on points (Lagrangian formulation), the second on the regular spatial grid not necessarily coinciding with points (Eulerian formulation). While the connection between the two formulation is known in the literature, the paper does a good overview of the relevant work and highlights the interplay between the two treatments for learning, which is valuable to the reader. The proposed view of learning with point clouds is, as far as I know, novel.\n\nWith the proposed learnable operations, the authors are able to efficiently learn the functions defined in 3D space, such as the semantic class labels. The operations include transferring the features between the grid and the point cloud, advection, and interpolation, all implemented in a unified learnable model. \n\nThe architecture is evaluated on classification and segmentation tasks with common datasets, where it performs on par with existing methods. While the experimental evaluation does not indicate that the proposed method is a new state-of-the-art, it convincingly validates that the proposed method is capable of learning powerful enough representations. \n\nI believe the paper should be accepted for publication, as (1) the proposed method is generally novel while it bases on solid and well-known foundations, (2) the experimental validation is sufficient to demonstrate the capabilities of the approach. ", "belong_id": "H1eqQeHFDS"}, {"uid": "BkgSu9lhYS", "paper_title": "Adaptive Adversarial Imitation Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper describes an approach that combines domain adversarial neural network with generative adversarial imitation learning. In the setup, each environment is dependent on some latent context variable (e.g. physics parameters) through which latent variable dependent policy and latent variable independent discriminators are learned. \n\nI don't think the exact same idea has appeared in existing literature. The authors justifies its connections and differences between third person imitation learning, but it seems that the proposed method bear some similarities to the NeurIPS19 papers below. \n\nhttps://arxiv.org/pdf/1909.09314.pdf\nhttps://drive.google.com/file/d/1urPE7J5tT8dzoBHSFvZKwNLsQieU706o/view\n\nThe following papers also assumed that GAIL-like methods in a meta learning setup, where environments depend on context. Perhaps the difference here is that the discriminator is also trained with a gradient reversal layer, so it encourages the discriminator to not use redundant state information. Also in this paper the source domain only contains demos from one env, which might highlight the importance of the gradient reversal layer.\n\n\nQuestions:\n\nIn 'dynamics learning', it seems that the inference network learns is mostly the context variable c, wonder if it is better to use terms like 'latent variable inference' to avoid confusion.\n\nWhat does the standard deviation mean in Figure 6? It seems a lot of them are even larger than the mean.\n\nThere is little explanation to the VAE-ADAIL experiments -- is it safe to assume most of the experiments require certain knowledge of the latent variable in order to be successful? Maybe some of the arguments about VAE can go to the appendix.\n\nWhy would in some cases UP-True performs much worse than ADAIL? In Ant it is even worse than PPO expert.\n\nHow would you adapt to unseen environments in ADAIL-pred? I don't see explanations in the text about how this is done. Essentially, how are samples obtained from the environment in order to perform posterior inference?\n", "belong_id": "HklvMJSYPB"}, {"uid": "SkxqREopYS", "paper_title": "Adaptive Adversarial Imitation Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe submission considers the problem of imitation learning when the dynamics of the expert are not known to the agent and the dynamics of the agent may change frequently. It is however assumed that the agent has access to a parameterized simulator that can simulate the expert dynamics. The parameters for the simulator are not known but are assumed to be drawn from a known distribution.\nThe proposed method is based on GAIL but uses several modifications:\n- A contextual policy is trained that also takes the dynamics-parameters as additional input. At each iteration, a new environment is sampled for performing the policy-rollout.\n- A 'posterior' prediction network is trained to maximize the likelihood of the parameters that were used for the different roll-outs. This network is used for the test case, where the true dynamics of the agent are not known.\n- The discriminator might use features of the state-action input that correlate with the corresponding dynamics. Classifying based on such features may be undesirable because the discriminator might no longer produce useful rewards. In order to address this problem, an additional head is added to the discriminator that outputs a prediction of the dynamic parameters. The prediction error is trained by backpropagation, however by flipping the sign of the gradient at the last shared layer, the features of the discriminator are optimized to be unsuited for predicting the dynamic parameters (the technique is known as Gradient Reversal Layer).\n- A VAE-based method for learning latent dynamic parameters is proposed, by training a conditional VAE to reconstruct the next state, where the current state and action are provided as context to the encoder and decoder.\n\nContribution:\nOne of the strong-points of the submission is the fact that it features several different, orthogonal contributions. I also think that the considered problem setting is relatively interesting. However, also when considering real applications such as robotics, I am not convinced that explicitly modeling the dynamic changes is necessary. Some existing imitation learning methods focus on a setting where the dynamics of the expert may differ from the agent, but the dynamics of agents do not change. This setting does not require dynamic-contextual policies and seems to be applicable to typical robot applications.\n\nSoundness:\nThe different components of the proposed methods seem reasonable to me. They do not come with (and arguably do not require) new derivations but seem rather like pragmatic solutions for the encountered problem.\nThe optimization problem (Eq.2) seems to be formulated slightly inaccurate, because the last term should in my opinion not depend on theta. If I understand correctly, the policy should not maximize the likelihood of the dynamics posterior.\nThe contrastive regularization loss needs to be better motivated. A high KL in the first term may not necessary be bad, for example, if the confidence in the prediction of (s_0, a_0, s'_0) is lower compared to (s_1, a_1, s'_1). If a similar regularizer has been used in prior work, such work should be referenced. Otherwise, it needs to be motivated.\n\nPresentation/Clarity:\nThe presentation of the work is arguably the main weakness of the paper.\nThe submission does not seem polished. It contains a large amount of typos. Figure 2 is confusing and adds little compared to the text description. Also the structure could be improved. For example, the submission introduces the posterior loss and outlines the algorithm before describing the individual components.\nThe paper uses some techniques such as conditional VAEs [1] or contextual policy search [2]\nare used but not described / referenced.\n\nEvaluation:\nI like that the different aspects of the proposed method are also evaluated individually. The ablations with respect to the adaptability and GRL are crucial. The evaluation of the performance could be improved. PPO and UP-True use the true reward function, so the only real competitor is a naive GAIL-baseline that uses randomized dynamics during training. I'm not aware of prior work that considers the exact same setting as the manuscript. However, one of the main arguments for inverse reinforcement learning is the claimed generalizability of a reward function as opposed to a policy. I see that learning a new policy after each change in the dynamics may be too costly in some settings. However, comparisons to methods such as AIRL that aim to learn reward functions that are robust to changes in the dynamics would be highly interesting.\n\n[1] Sohn Kihyuk, Honglak Lee, and Xinchen Yan. Learning Structured Output Representation using Deep Conditional Generative Models. Advances in Neural Information Processing Systems. 2015.\n[2] Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters.  A survey on policy search forrobotics.Foundations and Trends in Robotics, 2(12):328373, 2013.\n\nComparison: How about IRL, e.g. AIRL?\nWhat about state-only GAIL?\n\nTypos:\nEquations are not properly integrated into the sentences (missing punctuations)\n'domains, as oppose to one domain.'\n'Inspired by to GANs'\n'and generates a environment'\nFigure 5a (legend): 'dyanmics'\n'that can generalized across'\nAlgorithmbox: 'A environment', 'and Generate environment'\n'is achieved through 1) allowing'\n'the policy is mainly concerned with the end-effector of the latent parameters'\n\n\nQuestion: \nWhat are the network architectures?\n\nAccording to line 10 of the algorithmbox only the current trajectory is used for updating the dynamics posterior. Why?", "belong_id": "HklvMJSYPB"}, {"uid": "BJlqXqoptB", "paper_title": "Adaptive Adversarial Imitation Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an algorithm for imitation of expert demonstrations, in situations where the imitator is acting under a different environment (different dynamics, for instance) than the one used to collect expert demonstrations. The algorithm builds on GAIL with the following modifications  the discriminator is made dynamics-invariant by adding a domain-adversarial loss, and the policy is made to condition on a dynamics context. A separate dynamics posterior network is trained (either supervised or unsupervised) to predict this context at test-time. \n\n\nI have the following concerns about the paper:\n\n1.\tLack of novelty: \n         a.\tLearning a dynamics-invariant discriminator with the gradient-reversal-layer was proposed in Stadie et. al (2017). How is this approach different? In particular, what is the new element in Figure 1. and complete Section 3.4?  \n         b.\tLearning a posterior network to predict context codes, and conditioning the policy on those was explored in papers such DIAYN and InfoGAIL. \n\n2.\tThe proposed algorithm is reliant on the possibility of being able to sample from a distribution of environments (with varying dynamics), and then collect many trajectories in that environment (Line 6-7 in Algorithm 1). This is a severe requirement for real-word scenarios, and somewhat antithetic to the robotics learning motivation given by the authors in the introduction. Moreover, Figure 7 seems to imply that the method doesnt generalize well to unseen environments, if enough environments cant be sampled during training time.\n\n\nMinor comment:\n\nFigure 8. Friction value should not go from -3 to 3. Also, this single result inspires no confidence in the benefit or general applicability of the VAE-based context prediction.\n", "belong_id": "HklvMJSYPB"}, {"uid": "H1x4jhziFS", "paper_title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors describe a method for training plug and play language models, a way to incorporate control elements into pre-trained LMs. In contrast to existing work, which often trains conditioned upon the control element, the authors emphasize that their method does not require re-training the initial LM. This is exciting and a great research direction. It is evaluated in a number of different settings.\n\n1. The authors claim that this method is a baseline for controlled text generation (see e.g. the title). However, there does not appear to be any evaluation with any existing work that performs controlled text generation. I don't see how this can be proposed as a baseline for controlled text generation is there is no comparison to other methods. I imagine the authors will emphasize that that's not fair - because their method doesn't require retraining the language model - but it is relevant to demonstrate if there is a gap in performance or not. As is, there is only one baseline- unconditional language model - and to me this is mostly a way to calibrate the evaluators and not a way to compare their model against other models. \n\n2. Can the authors make a point or discuss the relationship of this work to neural style transfer? Compared to unsupervised style transfer approaches, which also use lists of words or attributes to learn to dis-entangle content and style, what are the benefits of the proposed approach and how would it compare?\n\n3. Can the authors discuss the effectiveness of their control mechanism for less logical control settings? For example, what if there was 'religion' for 'the potato' prompt? Does the model still respect these settings, or no? \n\n4. Can the authors add analysis on how much the model respects the control variables? This is quite common in existing controlled generation papers. If the model is updated to have the control variables and then is not provided with one at test time, what happens? Can you also control very easy to measure attributes, such as length?\n\nThis question ties in with a general point I am ambivalent to in this paper- that it is very long, but there is very little analysis done on what makes the method work, why it is better than other control methods or control baselines, where the proposed control mechanism is not effective, how the model scales if there are large quantities of topics rather than just a few of them, if the BoW and discriminator attribute models work well together or if certain attributes are easier to learn than others, so the model focuses more on those when there are conflicts, etc\n\n5. Missing citations: \n\nPrevious work has investigated controlling various attributes of text generation. Several of these works have also controlled multiple attributes simultaneously. For example, here's a list of a few of the works that were missed:\n\nKikuchi et al 2016\nFicler and Goldberg, 2017\nWang et al, 2017\nFan et al, 2018\nBaheti et al, 2018\nSee et al, 2019\nMartin et al, 2019\n\nThe related work section only focuses on very recent work, e.g. only one paper is discussed amongst a large body of existing work. I feel this is not an accurate reflection of how much previous work has investigated these techniques and analyzed how models deal with control variables. \n\nPlease also cite:\n- which dataset was used for story generation, appears to be missing\n- top-k sampling \n\n\nI have read the author response. Thanks for the details and additional analysis in the paper. ", "belong_id": "H1edEyBKDS"}, {"uid": "HkegjQxaFH", "paper_title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a Plug and Play LM model for controlled natural language generation. Similar to the idea of the Plug and Play Generative Networks for vision, the model plugs in a discriminator, which is either a bag-of-words model or a single layer classifier. The added simple discriminator is then coupled with a pre-trained generative language model such as GPT-2, to obtain a conditional probability for generating controllable text. The authors evaluate the proposed model using human evaluation studies and quantitative perplexity metrics, aiming at measuring the relevance and fluency of the generated text. Their experimental results show that the text generated is fluent and aligned with the desired attributes.  \n\nThe proposed method is simple and makes sense to me. The idea of how one can make good use of large, pre-trained  generative language models is very neat here. However, I have two main concerns, as follows.\n\n1. The main focuses of the generated text seem to be dramatically changed in an unpredictable way while tailoring the control attributes. In this sense, how useful these kinds of text generation techniques are not clear to me. For example, the first two rows in Table 3 contain two paragraphs with very different main ideas to be conveyed. Similarly for sentences in Table 1. It seems that those sentences talk about very different topics/things to me, although they may reflect the desired control attributes.  Is there an automatic evaluation metric to subjectively evaluate the change of the focuses/ideas of two pieces of text?\n\n2. The model is a straightforward adaption of the Plug and Play Generative Networks from the vision community. \n\nIn short, the idea in the paper is simple and seems effective. On the other hand, the lack of a good evaluation metric makes me a bit uncertain about the contribution of the paper. I am willing to increase my evaluation score if I will be convinced by other reviews and comments.  \n", "belong_id": "H1edEyBKDS"}, {"uid": "rklLgxopKB", "paper_title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper introduces an approach to the conditional generation of text, relying on pre-trained decoders, without fine-tuning and, in certain cases, without any training at all. The approach they introduce is following the framework known in NLP as noisy-channel modeling, previously standard in machine translation (in its SMT days), but undergoing certain revival recently (https://arxiv.org/abs/1611.02554, https://arxiv.org/abs/1910.00553,https://arxiv.org/abs/1908.05731,https://arxiv.org/abs/1907.06616). The authors do not mention this connection (they should!).\nVery differently from these previous approaches attempting to integrate the two factors in the search process (e.g., using reranking), the authors instead rely on gradient descent in the latent space of their model (Transformer), similarly to plug-n-play generative networks in image generation.  \n\nI find this approach interesting and like the paper overall. However, I do not see why authors do not compare to more direct ways of integrating the conditional component into the model. This would have been tricky in the NMT papers mentioned above, as the entire source sentences need to be reconstructred, however, it should be quite straightforward in this work, with conditioning on single categorical control variables (or maybe a couple in the additional experiments in sect 4.4). Especially, given that the authors already make the predictions of the control variable independently per prediction (e.g., see eq. (5) in section 4.2) / greedily per prefix (bottom lines, page 7). I would actually expect the proposed approach to work better (or at least differently) but it would be interesting to see it confirmed. E.g., for the experiments defining topics as sets of seed words (section 4.2), when integrating factors directly (unlike the proposed approach, Table 3), there will be no increase in the probability of generating relevant words before the first seed word is generated. \n\nAnother limitation is the lack of comparison to standard controlled generation work, i.e. those requiring training a model or/and fine-tuning pretrained decoder. I understand that the proposed approach falls in a different category and, of course, do not expect it to beat a fine-tuned model, but I'd like to get some feel for how much one loses by using this simpler method. There has been a lot of work on controlled generation in recent ~3 years, and they can also be combined with intializing and fine-tuning off-the-shelf pretrained decoders.\n\nThere is an interesting relation to the NIPS 2019 paper: https://arxiv.org/abs/1907.04944  They also rely on gradient descent to steer a pretrained language model. Their goal is to assess the degree of 'steerability' rather than building a controlled-generation model.\n\nGiven that style-controlled but otherwise unconditional generation may not have that many applications, I am curious how far you can push this approach. E.g., can you make it scale to more complicated data-to-text generation tasks (https://www.aclweb.org/anthology/D17-1239/)? Or, will the only application in this context be integrating new conditioning variables into pretrained conditional LMs? \n\nMinor: I am confused with the notation in 'Post-norm Geometric Mean Fusion' section.  It says that softmax is applied to the product of probabilities. Maybe to a linear interpolation of log-probs? Or maybe that's not softmax at all? Something seems off here.\n", "belong_id": "H1edEyBKDS"}, {"uid": "HygO3_32YH", "paper_title": "Fairness with Wasserstein Adversarial Networks", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors proposed a fairness-aware learning method.\nIn particular, the authors considered two kinds of fairness problem and designed two regularizers accordingly.\nEssentially, both of these two strategies learn classifiers and calibrate the distributions conditioned on protected variables jointly. \nThe calibration of the distribution is achieved in the framework of optimal transport.\n\nThis work is a natural extension of the optimal transport-based method shown in (Barrio et al, 2019a,b). The main differences include 1) instead of calibrating distributions after learning classifiers, the proposed method achieves calibration and learning jointly, replacing the primal Wasserstein barycenter problem with the dual form of Wasserstein distance (Arjovsky et al. 2017); 2) the proposed method considers two types of fairness problem. \n\nCompared with vanilla GAN, the potential advantage of WGAN on distribution matching is well-known. It seems unfair that the authors compared the vanilla GAN-based regularizer with the proposed WGAN-based regularizer just on EMD because EMD corresponds to the proposed regularizer directly. In Table 1, although the DI of vanilla GAN is higher than that of WGAN, its ACC is also higher than that of WGAN as well. In Figure 5 (a, b), if we set lambda=0.6 for WGAN and lambda=1 for vanilla GAN, both of them can achieve ~0.838 ACC and ~0.100 DI. In Figure 5(c), what do the points represent? Why not use DI as the x-axis? Because of the issues in experiments, it is hard to evaluate the improvements of the proposed method.\n\nAdditionally, the proposed method always causes the degradation of ACC when improving DI. However, the method in (Barrio et al, 2019a) just applies a Wasserstein barycenter-based post-processing but can suppress the degradation on ACC greatly. Could the authors discuss the differences and the advantages of the proposed method in detail?  Could the authors consider more recent work as their baselines?\n\nIn summary, the method makes sense, but its novelty is limited and the improvements are incremental.\n\nMinors:\nPage 6, Line 3: Figure 3 > Figure 2.\nI suggest swapping Figure 2 and Figure 3.", "belong_id": "BkeGPJrtwB"}, {"uid": "BJerxTNAYr", "paper_title": "Fairness with Wasserstein Adversarial Networks", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a method for adding an approximate disparate impact loss to a classification objective, and show that optimizing a classifier for this loss leads to 'fairer' predictions with little or no accuracy loss.\n\nThe authors first formulate two notions of fairness in terms of earth mover distance between the distribution of scores conditioned on either value of a protected variable. They then show that the dual formulation of the earth mover distance can be approximated (specifically, lower-bounded) by optimizing the parameters of a neural network under spectral norm constraints. This leads to a min-max global optimization scheme to learn a fair classifier.\n\nStrengths: The proposed method does better on the considered fairness metric than a GAN model with similar accuracy.\n\nWeaknesses: The paper is difficult to read, glosses over some important details, and contains some inaccuracies.\n\n-- Clarity: \n--- The authors need to better describe the assumptions (or lack thereof) made on the joint distribution of X, S, and Y. \n--- Measures such as the quantiles or probability laws need to be formally defined before they are used in definitions. \n--- The \\mathcal{L} notation is overloaded (it is used for probability laws, conditional and unconditional, as well as marginals, with \\mathcal{L}_1 referring to both!), leading to potential confusion.\n--- The domain of X and Y in equation (2) is not defined anywhere, neither is the distance.\n--- Similar lack of consistency with the use of F / \\mathcal{F} / \\hat{f}, without any explicit parameterization\n--- Figure 2 needs to be in Section 4, and Table 1 needs to be trimmed to size\n\n-- Overlooked problems:\n--- In the dual formulation, the optimization is done over a sub-set of Lipschitz function, hence approximation of \\mathcal{W} is a lower bound at every step. Minimizing a lower bound on a loss can be justified, but requires more discussion\n--- The trade-off inherent in the choice of n_w in algorithm 1 needs to be further discussed, especially in the case of large datasets where a full epoch of SGD in the inner loop of the optimization process is impractical\n\n-- Inaccuracies:\nThe graphical models in Figures 1 and 2 and conditional independences written in the text are not consistent:\n--- In Figure 1,  X is NOT independent of S given Y (neither is Y*) (see: V structures in a directed graphical model)\n--- In Figure 2, X* is independent of S regardless of conditioning on Y\n\nConsidering all of the above issues, the paper is not currently ready for publication", "belong_id": "BkeGPJrtwB"}, {"uid": "rkgtvbuH9S", "paper_title": "Fairness with Wasserstein Adversarial Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a variant of adversarial learning to achieve some of the popular group fairness definitions. The main novelty is the idea of minimizing Wasserstein distance between the conditional distributions of classifier predictions given different values of the protected attribute.\n\nMy main concern is the approximation of a simple 1d Wasserstein distance with a neural network. Wasserstein distance between two discrete distributions in 1d can be computed in closed form (simple function of order statistics). That is, eq. (1) is simple to evaluate for two empirical distributions. There is no need to use a neural network for approximation, and even if authors choose to do so, some discussion on how well it approximates actual Wasserstein distance is needed. I think the proposed algorithm could be more interesting if authors can work out the optimization problem with the actual Wasserstein distance.\n\nOn the theoretical/motivation side, it is not enough to say that demographic parity is achieved when the corresponding Wasserstein distance is 0. What is needed is that demographic parity difference is bounded from above by the corresponding Wasserstein distance (I don't know if it is true or not, but would like to know). Then minimizing Wasserstein distance to achieve demographic parity could be justified.\n\nFinally, the paper is quite poorly written. The description of fairness in the introduction is very vague. Authors essentially describe demographic parity as fairness, while it is simply one of the several definitions of group fairness. There is also individual fairness (the paper by Dwork et al. is cited, but not properly discussed) and prior work emphasizing certain deficiencies of group fairness [1] along with several recent papers studying individual fairness [2,3], some also utilizing Wasserstein distance [4].\nAuthors also provided incorrect definition of disparate impact. Equation in the bottom of page 2 corresponds to statistical parity difference, while disparate impact is the ratio.\n'Equality of opportunity' on the top of page 3 seems to be a typo\n'the mathematical properties of the disparate impact measure are not favorable, in particular it lacks robustness and smoothness features which would be necessary to blend algorithmic practice and mathematical theory' - I don't think this claim makes sense. There are many prior works studying disparate impact and proposing algorithms to achieve it, e.g. the cited work of Feldman et al. Authors should be more specific regarding what mathematical properties they consider not favorable.\n\nThere are a lot of typos and grammatical mistakes, e.g.\nin the 1st paragraph of section 2.2, the sentence 'Hence the aim in this case is to is unfinished.\nin the 1st paragraph of section 3, the first sentence seems to be unfinished.\n\n[1] Kleinberg, J., Mullainathan, S., & Raghavan, M. (2016). Inherent trade-offs in the fair determination of risk scores.\n[2] Kearns, M., Roth, A., & Sharifi-Malvajerdi, S. (2019). Average Individual Fairness: Algorithms, Generalization and Experiments.\n[3] Jung, C., Kearns, M., Neel, S., Roth, A., Stapleton, L., & Wu, Z. S. (2019). Eliciting and Enforcing Subjective Individual Fairness.\n[4] Yurochkin, M., Bower, A., & Sun, Y. (2019). Learning fair predictors with Sensitive Subspace Robustness.", "belong_id": "BkeGPJrtwB"}, {"uid": "HkljIvR3tr", "paper_title": "Last-iterate convergence rates for min-max optimization", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper shows that Hamiltonian gradient descent (HGD), which is gradient descent on the norm of the squared norm of the vector field, achieves linear convergence for a broader range of problems than bilinear and convex-strongly concave formulations. In particular, the authors show the result for convex-concave problems satisfying a sufficiently bilinear condition that is related to the PL conditions. Finally, the authors argue that consensus optimization (CO) can be viewed as a perturbation of HGD when the parameter choice is big enough. From this viewpoint they derive convergence rates for CO on the broader set of problems. This provides some further theoretical justification of the success of CO on large scale GAN problems.\n\nThe paper is presented in a clear manner, with the objectives and analysis techniques delineated in the main paper. This was helpful to get a sense of the main points before going through the appendix. The objective of the paper is to extend the problem settings for which there is last iterate min-max convergence rates, which now exist for bilinear, strongly convex-strongly concave, and convex-strongly concave problems. The authors achieve this by analyzing HGD and giving convergence rates for when a sufficiently bilinear condition is satisfied. The primary idea behind the proof techniques is to show that the objective (Hamiltonian) satisfies the PL condition. I found this to be an interesting approach.\n\nAs a result, the main question in evaluating this paper is on the significance of the result and the generality of the sufficiently bilinear condition. I tend to lean toward the result carrying some significance since it does extend the class of problems for which the convergence rates exists. However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to. I do acknowledge that the authors did a reasonable job of trying to clear this up in section 3.2 and section G of the appendix. It did still leave me wanting more with respect to the practical significance though. \n\nFinally, I found the connection to CO valuable. In particular, since this paper does not show large-scale experiments, the connection serves to provide some more theoretical evidence for they CO performs well in practice.\n\nPost Author Response: Thanks for the response. I agree with your perspective and think this paper should be accepted.", "belong_id": "SylGpT4FPS"}, {"uid": "H1evEoR6YH", "paper_title": "Last-iterate convergence rates for min-max optimization", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nSummary:\nThe paper, considers methods for solving smooth unconstrained min-max optimization problems.  In particular, the authors prove that the Hamiltonian Gradient Descent (HGD) algorithm converges with linear convergence rate to the min-max solution. One of the main contributions of this work is that the proposed analysis is focusing on last iterate convergence guarantees for the HGD. This result, as the authors claim can be particularly useful in the future for analyzing more general settings (nonconvex-nonconcave min-max problems).\nIn addition, two preliminary convergence theorems were provided for two extensions of HGD: (i) a stochastic variant of HGD and (ii)  Consensus Optimization Algorithm (CO) (by establishing connections of CO and HGD).\n\nMain Comments:\nThe paper is well written and the main contributions are clear. I believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding  the presentation and the combination of different assumptions used in the theory. \n\n1) I think definition 2.5 of Higher order Lipschitz is very strong assumption to have. What exactly means? Essentially the authors upper bounded any difficult term appear in the theorems. Is it possible to avoid having something so strong? Please elaborate.\n\n2) In assumption 3.1 is not clear what $L_H$ is. This quantity never mentioned before. Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).\n\n3) What is the main difference on the combination of assumptions on Theorems 3.2, 3.2 and 3.4. Which one is stronger. Is there a reason for the existence of Theorem 3.3?\n\n4) All the results heavily depend on the PL condition. I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial. In particular, one can propose several combinations of assumptions in order for the function H to satisfy the PL condition. Can we avoid having the PL condition? The authors need to elaborate more on this.\n\n5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.\n\nMinor Suggestions:\nIn first paragraph of page 5 where the authors divide the existing literature into the three particular cases, I am suggesting to add the refereed papers inside each one of this cases (which papers assumed function g bilinear , which papers strongly convex-concave etc.)\n\nI understand that the main contribution of the work is the theoretical analysis of the proposed method but would like to see some numerical evaluation in the main paper. There are some preliminary results in the appendix but it will be useful for the reader if there are are some plots showing the benefit of the method in comparison with existing methods that guarantee convergence (which method is faster?). In the current experiments there is a comparison only with CO algorithm and SGDA.\n\nIn general i find the paper interesting, with nice ideas and I believe that will be appreciated from researchers that are interested on smooth games and their connections to machine learning applications. \n\nI suggest weak accept but I am open to reconsider in case that my above concerns are answered.\n\n**********after rebuttal********\nI would like to thank the authors for their reply and for the further clarification. \nI will keep my score the same but I highly encourage the authors to add some clarification related to my last comment on the globally bounded gradient. \nIn their response they mentioned that the analysis only requires that  H is smooth and that $\\|\\xi(x^{(0)})\\|$ is sufficient bound. This needs to be clear in the paper (add clear arguments and related references). \nIn addition, in their response they highlight the non-increasing nature of function H over the course of the algorithm which is important for their argument. Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong. In SGD,  function H does not necessarily decrease over the course of the algorithm. The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.", "belong_id": "SylGpT4FPS"}, {"uid": "BkgIGYrAKS", "paper_title": "Last-iterate convergence rates for min-max optimization", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "*Summary* \n\nThis paper study the convergence of Hamiltonian gradient descent (HGD) on minmax games. The paper show that under some assumption on the cost function of the min max that are (in some sense) weaker than strong convex-concavity. More precisely, they use the bilinearity of the objective (due to the interaction between the players) to prove that the squared norm of the vector field of the game follows some Polyak Lojasiewicz condition. Thus the proof is concluded by the linear (resp. sublinear) convergence of gradient descent (resp. stochastic GD) under PL assumption.\n\n*Decision*\n\nI think that is work is clearly very interesting. The fact to prove linear convergence rate without strong-convex-concavity is quite surprising. And this paper brings nice tools to analyse HGD. Also the result on Stochastic HGD is very interesting.\n\nHowever, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).\nOne way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper. \n\nRegarding the practical limitation of this work: \n- the sufficient bilinearity condition are hard to meet in practice. (even for convex-concave problems)\n- In a non-convex-concave setting, Hamiltonian gradient descent is attracted the any stationary point, even local maxima (or the equivalent in the minmax setting). Making this algorithm not very practical. (However, CO is)\n\nHowever, I really think that the community is currently lacking of understanding on minmax optimization and that we need better training method in many practical emergent frameworks that are minmax (such as GANs or multi agent learning). That is why, I would vote for a weak accept.  \n\n*Questions* \n- What are the practical implication of your work ? for instance does it say anything on how to tune $\\gamma$ for CO ?\n\n*Remarks*\n- It is claimed that Theorem 3.4 gives the first linear convergence rate for minmax that does not require strong-convex or linearity. Note that, recently [1] seem to propose a result on extragradient in the same vein (i.e. without strong convexity or linearity).\n- (Minor) $\\alpha$ not alway have the same unit: Thm 3.2 it is proportional to a strong convexity and in Lemma 4.7 it is proportional to a strong convexity squared (actually the PL of the squared norm of the gradient). For clarity it might be interesting to use the notation $\\alpha^2$ in Lemma 4.7. The same way for unit consistency I would use $L_H^2$ instead of $L_H$\n\n[1] Azizian, Waiss, et al. 'A Tight and Unified Analysis of Extragradient for a Whole Spectrum of Differentiable Games.' arXiv preprint arXiv:1906.05945 (2019).  \n\n\n=== After rebuttal === \nI've read the authors's response. \nThe concern raised by reviewer 3 is very important. The descent lemma used by the author is not valid for the stochastic result. The authors should address that in their revision.  \nI however maintain my grade. \n\n ", "belong_id": "SylGpT4FPS"}, {"uid": "HJxKkZIRtB", "paper_title": "End-to-end named entity recognition and relation extraction using pre-trained language models", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents an end-to-end methods for jointly training named entity recognition (NER) and relation extraction (RE). The model leverage pre-trained BERT language models, making it very fast to train. The methods is evaluated on 5 standard NER+RE datasets with good performances.\n\nPros:\n\n- the paper is well written and very clear\n- the proposed model has two main advantages: (1) it is very fast to train due to the use of pre-trained BERT representations and (2) it does not depends on any external NLP tool (such as dependency parser)\n\nCons: \n\n- I think the main source of improvement comes from the BERT representations used as input. As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.\n- Without this ablation study, the contributions of this paper are to show that using BERT representations as input (1) leads to better performances for NER+RE  and (2) makes the model faster to train. This is not really surprising...", "belong_id": "rkgqm0VKwB"}, {"uid": "BJeklIo0tr", "paper_title": "End-to-end named entity recognition and relation extraction using pre-trained language models", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a new joint learning algorithm that works for two tasks, NER and RE. The model is based on a pre-trained BERT model, which provides the word vectors of the input word sequence. Then it solves two tasks with two network branches: the first branch minimizes the loss for NER, and the second branch minimizes the loss for RE. The second branch uses entity labels predicted by the first branch, so joint learning may benefit both tasks. \n\nThe design of the architecture is novel, but it is also not groundbreaking. Each network branch is from known structures, but the combination is not proposed before. \n\nThe submission has evaluated the proposed algorithms on four datasets and improved SOTA performances. The ablation study justifies the design details. \n\nThe writing is generally clear. \n\nNow critics: \n\nAblation study: \n1. As pointed by one public comment, the ablation study should show how much improvement is from BERT vectors. \n\n2. I'd like to see another ablation study of whether RE helps NER. If you remove the RE component, does the NER performance suffer? \n\n\nWriting: \n3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?\n", "belong_id": "rkgqm0VKwB"}, {"uid": "SJlvvnJkcB", "paper_title": "End-to-end named entity recognition and relation extraction using pre-trained language models", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an end-to-end joint model for named entity recognition (NER) and relation extraction (RE), using pre-trained language models. The model is very simple, with the key is to use BERT and take NER output as input to RE. The experimental results show the model, without the need for handcrafted features, get state-of-the-art results on five datasets. \n\nAlthough the paper is well written and shows good results, I would reject the paper because: \n- the idea is trivial and simple. I don't think there's significant novelty here: all the components are existing and combining them seems very trivial to me. \n- the good performance seems to be from BERT rather than the model's structure (table 2 suggests that). I thus think the contribution of the paper is pretty not significant. \n\nI think the paper does not fit this conference. It is better to be presented in a Demonstration section at a *ACL conference.", "belong_id": "rkgqm0VKwB"}, {"uid": "SJg1jSWvtH", "paper_title": "Optimal Unsupervised Domain Translation", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an analysis of CycleGAN methods. Observing that the class of solutions (mappings) covered by those methods is very large, they propose to restrict the set of solutions by looking at low energy mappings (energy being defined wrt. a dedicated cost). A natural formulation of the associated problem is found in optimal transport (OT) theory. They examine the underlying problem in its dynamical formulation, for which a direct connection can be made with ResNet architecture that are commonly used in cycleGANs. They illustrate these results on simple examples, involving pairing swapped digits from MNIST and celebA male to female examples. As a matter of facts, results presented with the OT formulation are more constant. The main proposition of the paper is that the task at hand can be efficiently coded through the distance (cost) function of OT.\n\nOverall the paper is well written and the proposition is reasonable. Yet some parts seem unnecessary long to me, or bring little information, notably the formalisation of 2.1 and 2.2. The fact that cycleGANs are severely ill-posed problems is well known from the computer vision community. Variants that can include a few paired samples can be found (not exhaustive): \nTripathy, S., Kannala, J., & Rahtu, E. (2018, December). Learning image-to-image translation using paired and unpaired training samples. In Asian Conference on Computer Vision (pp. 51-66). Springer, Cham.\nOe that try to regularize the associated flow:\nDLOW: Domain Flow for Adaptation and Generalization Rui Gong, Wen Li, Yuhua Chen, Luc Van Gool; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 2477-2486\n\nIn this spirit, I wonder if a comparison with only the vanilla cycleGAN is sufficient to really assess the interest of using the OT formulation of the problem. Notably, in the example of digit swaps, a cost is learnt by finding a representation of the digits that eliminates the importance of position in the representation. Training such a classifier assumes having some labelled data, that could theoretically be paired, and thus making amenable variants of cycleGans that use a few paired samples. In this sense, I think that the paper fails in giving convincing arguments that advocate the use of OT here. As the dynamical formulation is known and already used to learn mappings (  see Trigila, G., & Tabak, E. G. (2016). Datadriven optimal transport. Communications on Pure and Applied Mathematics, 69(4), 613-648. For instance). Also variants of OT that estimate a Monge mapping could have been included (e.g. V. Seguy, B. B. Damodaran, R. Flamary, N. Courty, A. Rolet, M. Blondel, Large-Scale Optimal Transport and Mapping Estimation, International Conference on Learning Representations (ICLR), 2018.)\n\nAs a summary:\nPros:\nA nice interpretation of CycleGAN with OT\nThe paper is fairly well written\nCons: \nOverall the quantity of novelties is, in the eyes of the reviewer, somehow limited. At least the contributions should be clarified;\nThe experimental section is not convincing in explaining why the OT formulation is better than variants of cycleGAN or also other schemes for computing OT than the dynamical formulation\n\nMinor remark:\nA reference to Benamou, Brenier 2000 could have been given regarding section 3.2 and the dynamical formulation of OT. \n", "belong_id": "H1eRYxHYPB"}, {"uid": "rkgyzs7iKB", "paper_title": "Optimal Unsupervised Domain Translation", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper revisits unsupervised domain translation (UDT) in light of optimal transport. The paper shows that CycleGAN-like models are ill-posed. It redefines UDT tasks using an additional set of suitable mappings. Then the paper redefines UDT problems in the optimal transport framework. Last it proposes an approach to solve UDT problems based on the dynamical formulation of optimal transport. Experiments support the proposed approach.\n\nUDT is a relevant and up-to date problem. The paper helps to clarify some shortcomings of previous approaches and proposes a new solution. The paper is well written. Therefore, in my opinion, the paper should be accepted to ICLR. But, as I am not expert in optimal transport, I would like to have the exact reference of Theorem 1 because I would like to be sure that, in the proof of Proposition 3, the optimum of (A_c) is unique and therefore also satisfies the first item of Theorem 1.\n\nDetailed comments.\n* It should be fair to say somewhere that in Zhu et al. (2017a) limits of the approach were already mentioned\n* As said before, you should give the exact reference of Theorem 1: which Theorem in Santambrogio (2015). In the proof of proposition 3, you should explain why the minimum of (A_c) is unique and thus corresponds to the minimum in Theorem 1.\n* End of Section 3.1. The design of the cost function is left open. This should be made explicit and be discussed somewhere, perhaps in the conclusion. \n* I think that there should be a paragraph for the computation of the inverse. This question is considered in different parts of the paper. See for instance the caption of Figure 5. What is the meaning of 'inverting the forward network' and to which part of the text does it refer?\n* End of Section 4.1. As said before, the design of the cost function is sensitive. Did you have any idea of other cost who would allow to learn the targeted translation without using internal representations?\n\nTypos. \n* The notation $T^{\\alpha-a.s.} is difficult to read and should be explained\n* Beginning of Section 3. 'based the dynamical formulation of PT'\n* Please check references in texts.  such as 'from OT theory Santambrogio (2015)'\n* Beginning of Section 3.2., 'calculate the retrieve the OT mappings'\n* Please give a reference for the dynamical formulation of OT. ", "belong_id": "H1eRYxHYPB"}, {"uid": "r1xMmonntS", "paper_title": "Optimal Unsupervised Domain Translation", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: \nThe paper addresses the ill-posedness of the unsupervised domain translation (UDT) problem. It provides a more structured and rigorous problem definition than previous works (mainly CycleGAN-based), and proposes the theory of optimal transport (OT) as a better framework for solving UDT. The paper provides an interesting link between a dynamical formulation of OT and residual networks, which leads to a practical algorithm for solving OT/UDT. Experiments highlight two main points: 1) CycleGAN are biased towards learning nearly identity mappings, and 2) the OT formulation allows for modelling explicit biases in the learned solution through the design of the cost function.\n\nStrengths & Weaknesses:\n  +  The paper addresses an important problem, which as far as I know, is widely known but not properly or explicitly addressed in prior work. \n  -  While most definitions are rather intuitive, some are still vague so they cannot be constructive. For example, a UDT task is a subset of all possible mappings which are *desirable* for the given task, but it is not clear how we can exactly define *desirable* mappings.\n  -  In addition, it is not clear why the set of all mappings X_{alpha,beta} needs to be constrained to invertible mappings. I see invertibility as only a constraint added by CycleGAN to limit the set of possible learned mappings.\n\n  +  The paper makes an interesting observation that CycleGAN is biased towards simple, and nearly identity mappings (which I believe is the main consequence of small initialization values), which could explain its practical success.\n  -  However, the paper needs to emphasize that this is particularly tied to the choice of resnet architectures that is commonly used.\n\n  +  I like the proposed dynamical formulation for solving OT and the link to resnets, which provides an interesting practical algorithm.\n  -  The main problem that remains unsolved is how to choose the cost function $c$. The paper acknowledges that, and proposes a specific cost functions for the specific tasks of the experimental section.\n\n  -  While experiments support the main claims of the paper, they are still quite limited and do not really have a clear practical significance. The paper would have been much stronger if the proposed approach solves a more practical problem. \n\nIn conclusion, while I think that the practical significance of the proposed approach is rather limited, I think that overall it makes an interesting contribution to the domain of UDT which can be useful for future work.\n", "belong_id": "H1eRYxHYPB"}, {"uid": "Hye3HcXoFB", "paper_title": "Improved Training of Certifiably Robust Models", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Strengths:\nThis work proposed two regularizers that can be used to train neural networks that yield convex relaxations with tighter bounds.\nThe experiments display that the proposed regularizations result in tighter certification bounds than non-regularized baselines.\nThe problem is interesting, and this work seems to be useful for many NLP pair-wise works.\nweaknesses:\nSome presentation issues.\nThe dataset, MNIST, is not good enough for a serious research. \nMore datasets need to be added to the experiments in this paper.\n\n\nComments:\nThis paper proposes two regularizers to train neural networks that yield convex relaxations with tighter bounds. \n\nOverall, the paper solves an interesting problem. Though I did not check complete technical details, the extensive evaluation results seem promising. \n\n1. There are some presentation issues that can be addressed. For example, on page 8, the sentence of the family of 10small misses a blank space.\n\n2. In the experiments, the dataset is not a good one for evaluating the performance of the proposed idea.\n\nIn conclusion,  at this stage, my opinion on this paper is Weak Accept. ", "belong_id": "HygqFlBtPS"}, {"uid": "HyeOL_lGcB", "paper_title": "Improved Training of Certifiably Robust Models", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summary:\nThe paper proposes two new regularizers for adversarial robustness inspired by literature on verification of ReLU neural networks for resilience to epsilon perturbations using convex relaxations. The paper shows empirically that the proposed method leads to better robustness than previous works.\n\nStrengths:\n+ The paper seems to have an interesting perspective (with the proposed looser relaxation) of the convex relaxation of an adversary adding noise at every layer in the network\n\nWeaknesses:\n\n*Sec. 4.1: Eqn. (O) does not have a convex relaxation, it is the exact problem which is intractable. Why are we comparing the optimal values of p*(O) and p*(C)? The paper from Salman et.al. already shows that there is a convex relaxation barrier, which essentially corresponds to this difference. In general, in Sec. 4, it is often unclear whether when we talk about p(O) if we are referring to the unrelaxed original problem or the tightest convex relaxation. For example, at the start of Sec. 4.1, it seems like we are talking about the convex relaxation and then in Sec. 4.3 it seems like we are talking about the unrelaxed problem.\n\n*It is not clear how/ why the proposed method of relaxing (which by the way seems identical to Fast-Lin (Weng et.al.) is better than the optimal convex relaxation. Would this not lead to looser bounds? Is that the thing we are looking to investigate? Making that more clear would be useful. Perhaps it would be good to argue the proposed regularizer in this work cannot be constructed with the optimal convex relaxation. Is that true? A discussion on this would be helpful.\n\n* The crux of the contribution seems to rest on the premise that identifying the optimal perturbation in the input space with the relaxed model, and then computing the activations with respect to that and forcing the forward pass to saturate near the margins of the relu polytope (relaxation) is a good idea. In general, it seems very unclear why this should work based on the evidence presented in the paper. Specifically with the relaxation, it might not even be guaranteed (as far as I understand) that the value of \\delta_0^* that is found from problem C is even going to lie inside the L\\inf norm ball around the point x, for example. Thus it is not clear to me if this is an approach for verification or a regularizer based on verification.\n\n* Ultimately, the value of the approach in this context (as per my understanding) comes from the experiments and the results which show that there is increased robustness. It would be great to clarify a couple of details in the experiments:\n1. Is the method of Wong et.al. using the looser convex relaxation (used here) or the tight convex relaxation when reporting the numbers in Table. 1? \n2. If the optimal convex relaxation can be used to construct the same regularizer as the one proposed here, it would be good to evaluate how well that does.\n\nOverall, I am not an expert in the area but a lot of details from the writing (such as point 1 under weakness) and the theoretical justification of the regularizer are unclear to me. Thus given these (perceived) weaknesses I would lean towards weak rejection. Clarifications on these points would help me revise my score.", "belong_id": "HygqFlBtPS"}, {"uid": "HyxG4FXV5B", "paper_title": "Improved Training of Certifiably Robust Models", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThe aim of the paper is to improve verified training. One of the problem with verified training is the looseness of the bounds employed so the authors suggest incorporating a measure of that looseness into the training loss. It is based on a reformulation of the relaxation of Weng et al.\n\n\nComments:\nPage 2: 'it can certify a broader class of adversaries that IBP cannot certify, like the `2 adversaries.'. You can definitely use IBP to very properties against L2-adversaries. It is simply a matter of changing the way the bound is propagated through the first layer.\nPage 3: It's a bit pedantic, but the convex relation of Ehlers (middle of figure 1) is not the optimal convex relaxation. It is optimal only if you assume that all ReLU non linearities are relaxed independently. See the work by Anderson et al. for some examples \nPage 5, section 4: 'We investigate the gap between the optimal convex relaxationin Eq. O' There is a bit of confusion in this section. Eq O is not the optimal convex relaxation, it's the hard non-convex problem.\nSection 4.1 bothers me. Equation C is the relaxed version of equation O, so they are only going to be equal if there is essentially no relaxation going on. Saying that it's possible to check whether the equivalence exists is a bit weird. The only case where this can happen is if all the terms in the sum over I_i are zero, which is essentially going to mean that no ReLU is ambiguous. (or if the c W are all positives, but that would be problematic during the optimization of the intermediate bounds given that c would make them both signs then)\nPage 5, section 4.2: The authors suggest minimizing d, the gap between the value of the bound obtained, and the value of forwarding the solution of the relaxation through the actual network. Essentially, this would amount to maximizing the lower bound (which all verified training already does), at the same time as minimizing the value of the margin (p_O) on a point of the neighborhood for which we want robustness (x + delta_0). Minimizing the value of the margin is the opposite of what we would want to do, so I'm not surprised by the observation of the author that this doesn't work well.\nThe conclusion of the section that d can not be optimized to 0 also seems quite obvious if you think about what the problem is.\n\nSection 4.3:\n'the optimal solution of C can only be on the boundary of the feasible set.' -> There is a subtlety here that I think the authors don't address. The three points they identify are the only feasible optimal solutions for solving a linear program over the feasible domain given by the relaxation of one ReLU but, when solving over the whole of C, the solution needs to be on the boundary of the feasible domain of C, which is larger than those three points.\n\nThe whole section is quite convoluted and makes very strong assumption. For Proposition 1, the condition x \\in S(\\delta) means that all the intermediate bound in the network must have been tight (so that the actual forwarding of an x can match the upper or lower bound used in the relaxation), and that the optimal solution of the relaxation requires all intermediate points to be at either at their maximum or their minimum. The only case I can visualise for this is essentially once again the case where there are no ambiguous ReLU and the full thing is linear.\n\nRegarding the experiments section, it would be benefical to include in table 1 the results of Gowal et al. (On the Effectiveness of Interval Bound propagation for Training Verifiably Robust Models) for better context. The paper is already cited so it should have been possible to include those numbers, which are often better than the ones reported here.\nThe comparison is included in table 2, when the baseline is beaten, but this is with using the training method of CROWN-IBP and it seems like most of the improvements is due to CROWN-IBP.\n\nTypos/minor details:\nPage 2: ' In addition, a bound based on semi-definite programming (SDP) relaxation was developed and minimized as the objective Raghunathan et al. (2018). (Wong & Kolter, 2017) presents an upper bound' -> citation format\nPage 8: 'CORWN-IBP '\n\nOpinion:\nI think that the analysis section is pretty confusing and needs to be re-thought. It provides a lot of complex discussion of when the relaxation will be exact, without really identifying that it will be when you have very few ambiguous ReLU. I think that there might be a few parallels to identify between the regularizer proposed and the ReLU stability one of Xiao et al. (ICLR2019) from that aspect. The experimental results are not entirely convicing due to the lack of certain baselines.", "belong_id": "HygqFlBtPS"}, {"uid": "rJgQN2PcFB", "paper_title": "DeepSFM: Structure From Motion Via Deep Bundle Adjustment", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper tackles Structure from Motion, one of the canonical problems in computer vision, and proposes an approach that brings together geometry and physics on one hand and deep networks on the other hand. Camera unprojection and warping (of depth maps and features) are used to build a cost volume onto hypothetical planes perpendicular to the camera axis. Similarly, various camera poses are sampled around an initial guess. A deep network regresses form the cost volume to a camera pose and a depth map. The method can be applied iteratively, using the outputs of the current stage as the initial guess of the next one. Training is supervised, and the the results are evaluated on multiple datasets.\n\nI am inclined to recommend accepting the paper for publication, because it addresses a canonical problem, outperforms the state of the art on multiple datasets and brings together geometry / physics and deep learning, which is IMO very a promising and underexplored direction.\n\nI found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.\n\n1. In Sec. 3 the Authors write 'We then sample the solution space for depth and pose respectively around their initialization'. However in Sec 3.2 they write 'we uniformly sample a set of L virtual planes {dl} Ll=1 in the inverse-depth space'. In what way are the planes 'around their initialization'? If the initial depth map spans over multiple orders of magnitude, will the planes be uniformly sampled between the minimum and maximum disparity of the initial map? If yes, it seems that the initial depth map is not really needed, just its minimum and maximum value is needed, but then how come the method can be applied iteratively with respect to depth?\n\n2. The Authors mention that depth maps are warped onto the virtual planes using differentiable bilinear interpolation. Is there a mechanism to protect from interpolating across discontinuities? If no, were bleeding edge artifacts observed?\n\n3. In the introduction, the Authors point that prior methods have trouble dealing with textureless, reflective or transparent approaches, but it's not clear form the paper where it addresses these cases, and if yes, what is the mechanism for that.\n\nLastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique. For example, 'our network learns a cost volume of size L  W  H using several 3D convolutional layers with kernel size 3  3  3'  - more details about this network are needed, as well as the others in the paper.\n\n", "belong_id": "SyeD0RVtvS"}, {"uid": "Syx181Q7cB", "paper_title": "DeepSFM: Structure From Motion Via Deep Bundle Adjustment", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a physical driven architecture of DeepSFM to infer the structures from motion. Extensive experiments on various datasets show that the model achieves the state-of-the-art performance on both depth and pose estimation. In general, the paper is clearly written but I still have several concerns. \n1.\tThe paper is easy to follow but the authors are expected to clarify the rationality in integration of the loss function. How the parameter of \\lambda_r, \\lambda_t, and \\lambda_r influence the performance. It would be better if the authors could present some analysis. \n2.\tThe experiments are rather insufficient. The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II. \n3.\tThe experiments in section 4.3 are also expected to be improved. It is difficult to draw a conclusion that the method is better than other ones based on such limited experiments.  \n", "belong_id": "SyeD0RVtvS"}, {"uid": "S1e7O15zjr", "paper_title": "DeepSFM: Structure From Motion Via Deep Bundle Adjustment", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe authors propose a SfM model which integrates geometric consistency with a learned pose and depth network. An initial estimate of depth and pose are used to construct pose and depth cost volumes, which are then fed into a pose regression and depth refinement network, to produce a new set of cost volumes, and so on. In this manner, the pose and depth estimation are improved iteratively.\n\nStrengths:\n\nThe proposed model is well motivated and shows strong performance and generalization ability on several datasets. There are convincing experiments to show the importance of the P-CV network.\n\nWeaknesses:\n\nThe authors claim that the LM optimization in BA-Net is memory inefficient and may lead to non-optimal solutions. Its not clear to me that the proposed method can guarantee optimality any better. Its also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.\n\nOther comments:\n\nIt would be very interesting to see the test time behavior of the network when it is run with more iterations than it is trained with (say 10 or 20), especially since the depth error does not seem to have stopped decreasing at only 4 iterations.\n\nIts not made entirely clear whether the training backpropagates through the update/construction of the pose and depth cost volumes. \n\nIn equation 5, x should be i.\n", "belong_id": "SyeD0RVtvS"}, {"uid": "ryejz8JhFr", "paper_title": "Classification Attention for Chinese NER", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\n      This paper discussed an approach to do named entity resolution (NER, the paper focuses only on Chinese NER but I think it could generalize to other languages as well). The idea is based on smart integration and extension of multiple existing building blocks: 1) BERT pre-trained model 2) a previous work to get document embedding by doing weighted average of word embedding (https://openreview.net/pdf?id=SyK00v5xx) and 3) Scaled dot-product attention mechanism applied directly to multi-label classification. The 'Introduction', 'Related work', and 'Experiment Settings' sections are well written and covers many details and decent references. Especially, the 'experiments' section is described in a great amount of details, which should be very helpful for reproducibility. \n      \nContributions:\n      * The author found an interesting application of the original algorithm (https://openreview.net/pdf?id=SyK00v5xx) to represent the entity class embedding based on averaging 'BERT' embeddings of all the component words. This could be implemented as a pre-processing step against any training dataset to derive 'pre-learned' entity class embedding.\n      * Instead of the common approach of connecting the BERT sequence outputs directly to CRF layer, the author added an intermediate layer to calculate the classification attention between a sentence (sequence of token embedding) and any entity class (based on the above pre-learned entity embedding). This result plus the original sentence embedding are concatenated.  The concatenation is further fed into a few additional layers to produce the final inputs into CRF layer.\n\nWeakness:\n     * The paper lacks novelty. As pointed above, I did not see that the contribution from the paper is sufficiently original. It is a good application of various existing methods though.\n\nI also have a few suggestions/questions below:\n\n* The ERNIE paper (https://arxiv.org/abs/1907.12412v1) is mentioned in the related work. Since ERNIE can potentially learn a good vocab for Chinese, did you ever compare your approach vs ERNIE+CRF? \n* There is one paper that I know which is pretty relevant to what you are doing here, which is probably worth a reference. https://arxiv.org/abs/1805.04174.  In that paper, the idea is to co-learn a class embedding and perform text classification. Their class attention is performed through dot-production attention though.\n* The Table index seems wrong in your paper. (I think Table 2 is not mentioned in your paper, but all tables (3-6) is offset by 1). \n* There are some minor typos or places that need some clarifications.\n   - in the abstract: 'character-based' model. This is a little confusing. Because BERT is a word-piece based model. word-piece could across multiple characters for English. IIUC, You probably want to say 'Chinese-character' instead of character.\n   - in 'Introduction', 'providing greater weight to characters identical to each entity class', you might want to revise this sentence to clarify its meaning further.\n   - In section 3.2, you might want to give some explanation to some notations (the first time you refer to it). For example, what is $L$, what is $m$ and $n$.  What is $S$?  Also why the denominator of Emb(Word) is not $n-m+1$? \n\n   - The last paragraph in section 3.3 needs more clarification as well. How do you merge the three tensors after attention stage? (a concatenation ?) . The last sentence mentioned 'residential', I guess instead you want to say 'residual'.  You might also want to clarify where the '3 layers' of residual appear in your network.\n\n  - In your experiment, (if I did not miss), did you freeze the BERT parameters and entity embeddings when finetuning your NER model?\n\n  - in Table 2 and Table 3, why the 13-layer BERT + CRF performs significantly worse on Recall (Table 2) and significantly better on Recall (Table 3)? \n\n\n  ", "belong_id": "B1gUn24tPr"}, {"uid": "ByeUmO-R5H", "paper_title": "Classification Attention for Chinese NER", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper tries to improve the performance of Chinese NER by developing a novel attention mechanism that leverages BERT pre-trained model which considers bi-directional context. Experiments on a number of tasks show that the proposed approach is effective.\n\nComments:\n[1] A bunch of experiments are conducted\n[2] Chinese NER is a hard problem, but it would be great to see the proposed approach generalizable to other tasks. So, the contribution of this paper is limited\n[3] The proposed algorithm is simple and effective, but the novelty is a bit low\n", "belong_id": "B1gUn24tPr"}, {"uid": "rylcHyzA9r", "paper_title": "Classification Attention for Chinese NER", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Comments by sections : \n\nSummary : \n\nThe use of entity is not clear. Are you referring to named-entities ?\n\n1 INTRODUCTION\n\n'Due to the differences in language structure' : this paragraph is not clear. It should say explicitly that in Chinese word can be composed of one or a couple of characters and that word boundaries can not detected graphically. \n\n 'A common mitigation is to add external lexicon as a reference'   references are needed on how to includes words in embedding (except just training word embeddings)\n \n ' an entity classification-assisted model' : not very clear : the classification is assisted ?\n \n ' The first step is to form word embeddings of entities appearing in the trainning sentences through character embeddings and the second step is to aggregate entity embeddings by category and generate classification embeddings' : is it a multi-task training ?\n \n 'After that, we designed a novel Attention mechanism to integrate entity ' : is it the same model or two different propositions of the paper ? 'After that' is not very clear as a transition.\n \n Section 2 :\n \n 'more works Yang et al. (2016) Ruder12 et al. (2017) ' : strange formulation\n \n 'Whats more, the attention mechanism...' : odd expression.\n \n 'In this paper, we revise the Scaled Dot-Product Attention to Classification Attention which would give a weighted representation of the input sentences through a series of entity classes.' : maybe it should be moved to the introduction as a novelty proposed by the paper.\n \n \n 3.2  EMBEDDING EXTRACTION FOR ENTITY CLASS\n \n ' the smooth inverse frequency is abandoned' : why ? please explain this choice.\n 'the weighted projection of the word embeddings on their first singular vector is removed.' : explain. If it is a common practice, give a citation otherwise justify this choice.\n \n 3.3 CLASSIFICATION ATTENTION \n \n Here again, the proposed attention system is described but not justified : why would a class specific attention system be better ? What are the expected advantages ?\n \n 4.1.3 EXPERIMENTAL RESULTS\n Experiments are conducted on 4 dataset and the proposed model is compared to a 'standard' BERT-based model and several results form the litterature. The proposed model outperforms sometimes the other models, often by a small margin as it is usually the case in NER experiments. \n But more insight on the strengths of the models should be given by conducting an ablation study. \n  \n  \n In conclusion ,this paper present an incremental improvement over BERT-based NER for Chinese. The proposed approach is not sufficiently justified and experiments, even if showing improvements over state-of-the-art models or published results, does not sufficiently explore the benefits of the proposed model (with ablation study for example).  ", "belong_id": "B1gUn24tPr"}, {"uid": "Byxju1vRFr", "paper_title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper considers personalization federated learning problem in which the goal is to personalize the global model on a given client/device based on available data on that device/client.  The paper claims not only their proposed method can lead to fast convergence time but also provide a solid initial model per device/client and results in a better-personalized model. To evaluate the performance of their method, EMNIST-62 and Shakespeare data are used.\n\nEven though personalization in federated learning is very interesting and challenging, I am not sure about the contribution of this paper and what is exactly proposed in this paper: \n\n1)  Section 2: this paper shows the relationship between FedAvg and MAML. In my view, the connection is very straight forward and can be shown in a couple of sentences. I might be missing something here, but it is not obvious to me what this paper adds to the connection between MAML and FedAvg.  \n\n2)  Personalized FedAvg Section: The same is about section 3. In my view, Algorithm 2 doesn't say anything new rather than to use Adam in local machine and SGD on global models and to optimize for 'E' steps. But what if we use other datasets rather than EMNIST-62 and Shakespeare? will these recommendations still hold, i.e. using SGD on server and Adam on the devices? Per section 3 of this paper, Algorithm 2 indeed is the result of the experimental adaptation of the FedAvg algorithm so generalization to other datasets won't be obvious and it is a big question to me.\n \n3) Also, the paper mentioned that this method can work even if there is no local data available on some of the devices/clients. I wasn't able to understand how personalization possible if there is no data to personalize. Wouldn't a device/client just use the global model?\n\nIn summary, I find the contribution and novelty of this paper limited and the empirical findings of this paper can't be always applicable to other datasets and scenarios. Plus, I am not convinced this paper shows anything different than FedAvg rather than some recommendations about local and global optimizer selections.", "belong_id": "BkeaEyBYDB"}, {"uid": "r1xmrgpU9H", "paper_title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Update: I thank authors for the rebuttal. I agree that direction of exploring personalization in FL is interesting. With a stronger methodological contribution, this could become a good paper.\n\n----------------------------------------------------------------------------------------------------------------\nThe main contribution of this paper is to notice the connection between Federated Averaging (FedAvg) and Model Agnostic Meta Learning algorithms (MAML). Authors also consider an algorithm that first trains with FedAvg and then continues training using Reptile.\n\nPros:\n\nInterpretation of FedAvg as a meta-learning algorithm is interesting.\n\nCons:\n\nVery limited methodological contribution. Proposed algorithm is essentially two existing algorithms applied one after another.\n\nExperiments are not conducted rigorously enough. There are many arbitrary hyperparameter choices which may bias the conclusions made in the paper. Statement 'We tried SGD with a range of other learning rates, and in all cases we observed this value to work the best.' is alarming suggesting that authors tried a variation of settings observing test data performance and reported a few selected runs. Although 'each experiment was repeated 9 times with random initialization', the train/test split of the clients was fixed. Randomizing over client train/test split could help to improve the reliability of the results.\n\nEMNIST-62 is the only dataset analyzed in some detail. This dataset has drastically varying P(y|x) across clients, i.e. some people write zeros as some others write 'o's. This suggests that it is very hard to train a good global model and personalization is necessary. However this doesn't mean that Shakespeare dataset 'does not provide any interesting insights'. Perhaps, it is indeed more interesting and challenging, demanding more advanced methodology.\n\nIn Figure 1, number of communication rounds may be impractical for FL (considering also addition 200 Reptile rounds). On Shakespeare, FedAvg paper reports 54% accuracy achieved in under 50 communication rounds in one of the settings. There are also recent works on improving communication efficiency that were not discussed or studied for personalization quality, e.g. FedProx from 'Federated Optimization in Heterogeneous Networks' and PFNM from 'Bayesian Nonparametric Federated Learning of Neural Networks'.\n\nQuestions about Figure 2 experiments:\n1. Fine-tuning requires 200 extra epochs over the initially trained model. What's the initial model accuracy when FedAvg is further trained with Adam optimizer for 200 extra communication rounds?\n2. The personalized test accuracy with FedAvg and Reptile fine-tuning reaches the same value in 10 update epochs, even when Reptile fine-tuning gets 200 extra initial training epochs. Does Reptile fine-tuning provide additional benefits to the initial model as compared to running FedAvg for more number of epochs?", "belong_id": "BkeaEyBYDB"}, {"uid": "HkxMzGThcS", "paper_title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies the application of techniques from meta-learning (a method\nto train a single model which can then be easily adjusted to perform well on\nmultiple tasks) to federated learning (the task of distributed training of\nmodels on distributed datasets).  The paper notes that standard meta-learning\nalgorithms are similar to standard federated learning algorithms, and uses\nthis perspective to produce a merged method and evaluate it empirically.\n\nPros.\n+ The motivation of the paper is clear and indeed these methods seems similar,\n and meta-learning can help with federated learning.\n\nCons.\n- The resulting method appears somewhat underdeveloped; it is simply to run\n some amount of federated learning and then some amount of meta-learning,\n whereas the first parts of the paper led me to believe that a single\n simultaneous merge of the methods is the way to go.  The paper does not\n report any fine-grained evaluation of various such choices, thus I don't know\n why they did that they did, and thus do not find their choices compelling.\n- The Reptile method is already presented in the original paper with\n a distributed counterpart, so why not just run that?  I am not convinced that\n some more minor modification of Reptile could not already do well on this\n paper.\n- The empirical evaluation is not very extensive, so I am also not convinced\n there, and in particular I need convincing of this type to believe that\n regular reptile is beaten by FedAvg+reptile.\n\nMinor comments.\nPage 1, second paragraph, the word 'outperform'.  I'm not sure what the\nperformance measure is; in federated learning, we care about many things, for\ninstance privacy, keeping the work on the distributed clients low, etc.\nPage 2, the 'three objectives'.  I feel meta-learning is doing all three too.\nPage 3, Algorithm 1.  I realize space is a concern, but this was hard to read.\nPage 4, Algorithm 2.  'relatively larger' is vague.", "belong_id": "BkeaEyBYDB"}, {"uid": "S1g9D58Rtr", "paper_title": "Learning with Social Influence through  Interior Policy Differentiation", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new method for learning diverse policies in RL environments, with the ultimate goal of increasing reward. The paper develops a novel method, called interior policy differentiation (IPD), that constrains trained policy to be sufficiently different from one another. They test on 3 Mujoco domains, showing improved diversity in all of them and improved performance in 2 of them.\n\nOverall, this paper is very well executed. The explanation of the method is thorough, and the paper is well-written and polished. I like the idea of enforcing a constraint on the policy diversity via manipulating the transitions that the agents can learn from.  The experiments section compares to two other methods of increasing policy diversity and IPD outperforms both of them. I think this is a solid contribution to the literature on improving policy diversity.\n\nThat being said, I have some concerns about the paper:\n1) The motivation for explicitly encouraging diverse policies is a bit confusing, and isnt very convincing. The paper draws inspiration from social influence in animal society, and say they formulate social influence in RL. First, the term social influence already has an established meaning in RL (see e.g. Jaques et al. (2018)) and refers to agents explicitly influencing others in a causal way in a multi-agent environment. Second, I think calling policy diversity a form of social influence is a bit of a stretch (and anthropomorphizes the agents unnecessarily). I think the paper should scrap the social influence angle and instead frame it as increasing policy diversity. \n\nThe paper also motivates itself in comparison to Heess et al. (2017), which uses a set of environments to get diverse policies. However, the goal of these works are different: in Heess et al., the goal is to train agents that can exhibit complex behaviours in relatively simple environments (the focus is more on complexity of behaviours vs. the fact that agents in the same environment learn diverse policies). In this work, the goal is not to develop any more complex policies, but to have different agents on the same task learn diverse policies (and since the experiments are in Mujoco, the degree of diversity is limited). Thus, while the works are related, I dont think the Heess et al. paper is good motivation for this work. \n\nI think the primary motivation that makes sense for explicitly encouraging diversity is to improve final performance on the task. Thus, I think it would be best for the paper to clarify the introduction by focusing on this. The paper could also give some reasons why having diverse policies is inherently a good thing (maybe for some applications with humans-in-the-loop it could be helpful?), but currently this is absent. \n\n2) Given that improving the final reward of an RL agent is the main goal, its not clear that the experiments (in 3 simple Mujoco settings) are enough to show this reliably. Specifically, its unclear whether encouraging diversity in this way will generalize to more complex tasks or domains (e.g. tasks in Mujoco with sparser reward, or environments with larger state spaces). It is possible that the success of the technique is most prevalent when there is only a small observation space. \n\n3) Id like to see more discussion / analysis of *why* wed expect diverse policies to lead to better rewards. In work on intrinsic motivation / curiosity for better exploration, its clear that encouraging agents to visit unseen states will lead to a better exploration of the state space, and thus will make them more likely to stumble upon rare rewards. But is this also true for policy diversity? Currently, the paper speculates that encouraging diversity could help agents not all fall into the same failure mode. But I could also imagine that it could lead agents to avoid a successful strategy that another agent learned. For example, if a certain sequence of moves is necessary at the beginning to avoid termination, the first agent could find this sequence of moves, but the other agents might avoid this sequence for the sake of diversity (depending on the threshold). Does something like this happen in practice? In my opinion, the environments considered arent rich enough to know. \n\n4) There are also some inconsistencies in results section. Specifically:\n- There seems to be a disagreement between the results in Table 1 (which uses 10 peers) and the ablation over number of peers in Figure 4, which shows that the performance with 10 agents is roughly the same as it is with 1 agent (and overall shows little positive trend between the number of peers and performance). \n- If success rate means percentage of time beating average PPO policy, why does PPO sometimes get 100% in Table 1? \n\nGiven the concerns above, Id assess the paper as being borderline for accept. Im currently erring on the side of rejection, but Id consider changing my score if some of the above points are addressed. \n\n\nSmaller concerns and questions:\n- There are a couple of instances where I found the claims of the paper with respect to related work to be over-stated. For example:\nYet designing a complex environment requires a huge amount of manual efforts -> not necessarily. There is an initial engineering overhead, but its possible to generate environments programmatically with different properties, resulting in different agent behaviours. \nAlso:\nOn the Task-Novelty Bisector method of (Zhang et al., 2019): the foundation of such joint optimization is not solid. This is given without any explanation --- how is it not solid?\n\n- In the TNB and WSR implementation, what metric is being used? Is it the same as is defined in Section 3?\n\n- It would be nice to have some videos of the agents behavior to be able to more easily assess the learned policy diversity. \n\n\nSmall fixes:\nand similar results can be get -> and get similar results. \n", "belong_id": "SJeQi1HKDH"}, {"uid": "r1lkFUpCKr", "paper_title": "Learning with Social Influence through  Interior Policy Differentiation", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a new way to incentivize diverse policy learning in RL agents: the key idea is that each agent receives an implicit negative reward (in the form of an early episode termination signal) from previous agents when an episode begins to resemble prior agents too much (as measured by the total variational distance measure between the two policy outputs). \n\nResults on three Mujoco tasks are mixed: when PPO is combined with the proposed objective for training diverse policies, it results in very strong performance boosts on Hopper and HalfCheetah, but falls significantly short of standard PPO on Walker 2D. I would have liked to see a deeper analysis of what makes the approach work in some environments and not in others. \n\nExperimental comparisons in the paper are only against alternative approaches to optimize the same diversity objective as the proposed approach (with weighted sum of rewards (WSR) or task novel bisection(TNB)). Given that this notion of diversity is itself being claimed as a contribution, I would expect to see comparisons against prior methods, such as in DIAYN. There are other methods that have been proposed before in similar spirit to induce diversity in the policies learned. Aside from the evolutionary approaches covered in related work, within RL too, there have been methods such as the max-entropy method proposed in Eysenbach et al, 'Diversity is All You Need...'. These methods, evolutionary and RL, could be compared against to make a more convincing experimental case for the proposed approach.\n\nThe experimental setting is also not fully clear to me: throughout experiments, are the diversity methods being evaluated for the average performance over all the policies learned in sequence to be different from prior policies? Or only the performance of the last policy? Related, I would be curious to know, if K policies are trained, the reward vs the training order k of the K policies. This is close to, but not identical to the study in Fig 4, to my understanding.\n\nAside from the above points being unclear, the paper in general could overall be better presented. While I am not an expert in this area, I would still expect to be able to understand and evaluate the paper better than I did. \n- Sec 3.1 makes a big deal of metric distance, but never quite explains how this is key to the method.\n- The exact baselines used in experiments are unhelpfully labeled 'TNB' (with no nearby expansion) and 'weighted sum of rewards (WSR)', with further description moved to appendix. In general, there are a few too many references to appendices.\n- The results in Fig 2 are difficult to assess for diversity, and this is also true for the video in the authors' comment.\n- There is an odd leap in the paper above Eq 7, where it claims that 'social uniqueness motivates people in passive ways', which therefore suggests that 'it plays more like a constraint than an additional target'. \n- Sec 5.1 at one point points to Table 1 for 'detailed comparison on task related rewards' but says nothing about any important conclusions from the table.\n- There are grammar errors throughout. ", "belong_id": "SJeQi1HKDH"}, {"uid": "BklTKgR-qB", "paper_title": "Learning with Social Influence through  Interior Policy Differentiation", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper presents a new algorithm for maximizing the diversity of different policies learned for a given task. The diversity is quantified using a metric, where in this case the total variation is used. A policy is different from a set of other policy if its minimum distance to all the other policies is high. The authors formulate a new constraint optimization problem where the diversity to previous policies is lower bounded in order to avoid a tedious search for combining task reward and diversity reward. The algorithm is evaluated on different Mojoco locomotion tasks. \n\nPositive Points:\n- The idea of maximizing the minimum total variation is novel and interesting\n- The approach seems to work better than current SOTA approaches for generating diverse behavior\n\nNegative Points:\n- The paper needs to be improved in terms of writing as in particular some of the main parts of the algorithm are unclear\n- The definition of Eq 7 does not make too much sense to me (see below)\n- The results have high variance and some conclusion drawn from it are hard to verify given the plots\n\nMore comments:\n- Eq 7 does not seem to be a very good choice to me. Why does the total variation needs to be different at *every* time step? We can certainly generate very diverse behavior even if the policy is exactly the same for some states. It could even be the case that for some states, only one action does not lead to a failure. In this case, Eq 7 would completely fail to produce any valid policy (?)\n- In general the writing is clear, it gets however quite unclear for the main part of the algorithm (after Eq. 7). It is unclear how equation 8 is obtained and why the limit of alpha going to 0 should lead to the same solution as Eq 7 (if alpha is 0 than it should be the same as optimizing just the task reward??). While this might be obvious for experts of the interior point method, it needs to be explained in much more detail in this paper. I think it is always a good strategy to make a paper self-contained, in particular for the main parts of the algorithm.\n- Also the termination mechanism needs to be much better explained. What reward is given in this case? The current formulation sounds quite heuristic to me, but maybe a better explanation can fix that.\n- while Fig 3 shows a clear advantage of the method, the section about better policy discovery would need better data to verify their claims. Fig 4 shows very noisy results and while for the hopper there might be a clear improvement of performance for number of policies > 2, this does not seem to be very significant for half cheetah. Given the amount of noise in the results many more trials would be needed to really make such statements.\n", "belong_id": "SJeQi1HKDH"}, {"uid": "BkxAlRIRYr", "paper_title": "Convolutional Tensor-Train LSTM for Long-Term Video Prediction", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper proposes a method that saves memory and computation in the task of video prediction by low-rank tensor representations via tensor decomposition. The method is able to outperform standard convolutional lstm and other methods by using less parameters when testing it in the Moving MNIST and KTH datasets. The authors also present a proof to validate their method.\n\n\nPros:\n+ Interesting method for decomposing tensors operations in convolutional architectures\n+ Outperforms immediate baseline (Convolutional LSTM)\n\nWeaknesses / comments:\n- Weak experimental section\nThe authors mainly compare against Convolutional LSTM. The performance increase is there but the difference in parameters is not that significant in comparison to the performance. Needing fewer parameters is one of the claims in this paper and I am not fully convinced of the trade-off between the complexity of the model and the gain in parameter reduction / performance. In addition, the show videos do not look that much improved. The paper is also missing baselines from Villegas et al., 2017 and Denton et al., 2017 which both have available models for the KTH dataset.\n\n- No videos provided\nThe paper does not provide any videos which is a must for video prediction papers. Judging the video quality from images in the paper is not easy, and also the used metrics have been shown to not be very objective in terms of video prediction quality or image generation in general.\n\n\nConclusion:\nThe proposed decomposition method is interesting, but the experimental section fails to convince me as to whether the methods performance validates the complicated formulations. My current score is between weak reject and reject so I will give a weak reject.", "belong_id": "Hkee1JBKwB"}, {"uid": "rkgc4-Cy5S", "paper_title": "Convolutional Tensor-Train LSTM for Long-Term Video Prediction", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a convolutional tensor-train (CTT) format based high-order and convolutional LSTM approach for long-term video prediction. This paper is well-motivated. Video data usually have high dimensional input, and the proposed method aims to explicitly take into account more than one hidden representation of previous frames - both lead to a huge number of parameters. Therefore, some sort of parameter reduction is needed. This paper considers two different types of operations - convolution and tensor-train (TT) decomposition - in an interleaved way. The basic model considered in this paper is a high-order variant of convolutional LSTM (convLSTM).\n \nThere exist several works using tensor decomposition methods including TT to compress a fully connected layer or a convolutional layer in neural nets, to break the memory bottleneck and accelerate computation. This paper takes a different direction - it further embeds convolution into the TT decomposition and thus defines a new type of tensor decomposition, termed convolutional tensor train (CTT) decomposition. CTT is used to represent the huge weight matrices arisen in the high-order convLSTM. To my best knowledge, this combination of convolution and TT decomposition is new.\n \nThe paper is well-written as the literature review is well done. Experimental results demonstrate improved performance over the convolutional LSTM baseline, a fewer number of parameters, and the qualitative results show sharp and clean digits. This improvement could be attributed to multiple causes: the high-order, the tensor decomposition-based compression, or the CTT. The authors also provide an ablation study, but it mainly concerns comparisons with ConvLSTM.   \n \nDespite the promising results, this paper is not ready for ICLR yet. Below is a list of suggested points needed to address:\n(1) Yang et al 2017 claim that TT-RNN without convolution can also capture spatial and temporal dependence patterns in video modeling. This is an important baseline but missing in the current version of the paper. \n(2) The justification of high-order modeling in long-term prediction. The first-order model also implicitly aggregates multiple past steps. It would be good to add more experimental evidence to support the necessity of the high-order.\n(3) There exists some unjustified complexity for the CTT approach. How does it compare to TT for high-order ConvLSTM?\n \nPerhaps, a more complete ablation study should include:\n(1) LSTM with TT but without high-order and convolution\n(2) LSTM with high-order and TT but without convolution\n(3) ConvLSTM with TT\n(4) ConvLSTM with CTT\n(5) ConvLSTM with high-order and TT\n(6) ConvLSTM with high-order and CTT\n \nQuestion:\n How is the backpropagation done for the CTT core tensors? \n What is the error propagation issue of first-order methods and how does the high-order one not prone to it? ", "belong_id": "Hkee1JBKwB"}, {"uid": "HyegM4fo9H", "paper_title": "Convolutional Tensor-Train LSTM for Long-Term Video Prediction", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper build a higher-order spatio-temporal model by means of combining Convolutional Tensor-Train Decomposition(CTTD) and ConvLSTM, and utilize the combination method to solve long-term video prediction problems. The CTTD factorizes a large convolutional kernel into a chain of smaller tensors, so as to relieve the difficult convergence and overfitting problems caused by too much model params. \nExperiments on Moving-MNIST and KTH datasets show that the proposed method achieved better results than standard ConvLSTM, and in some way comparable with SOTA model. Ablation Studies are also provided.\n\nAlthough it seems novel combing CTTD with ConvLSTM, the idea of CTTD and the combination mainly comes from [Yu et al.,2017] and [Yang et al.,2017],this paper use the method in a new problem of video prediction,  I think the theoretical innovation is not enough for ICLR.\nAlthough the experimental results were better than ConvLSTM(2015), but not as good as PredRNN++(2018), especially in terms of the MSE metrics. Since the prediction accuracy has not yet achieved, I don't think the reduction of model params is a matter of primary importance.  Whats more, Moving-MNIST and KTH are relatively simple datasets, video prediction on a more complicated datasets such as UCF101 will be more convincing.\nConclusion:\nThis paper is in some way novel, but not enough for ICLR, and the experiment results seems not enough convincing, so I will give a weak reject.\n", "belong_id": "Hkee1JBKwB"}, {"uid": "Sye2f6xAur", "paper_title": "Hamiltonian Generative Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes two ideas: 1) Hamiltonian Generative Networks (HGN) and 2) Neural Hamiltonian Flow (NHF). \n\nHamiltonian Generative Networks are generative models of high-dimensional timeseries which use hamiltonian differential equations to evolve latent variables (~position and ~momentum vectors) through time (using any differentiable integration scheme). Given the ~position vector a decoder network generate predictions. The initial latent variables are inferred using a VAE style inference network that takes a sequence of images as the input. The decoder, inference network and crucially the hamiltonian energy function is learned by minimizing a VAE style ELBO on observed sequences. The model induces a strong hamiltonian physics prior and is quite elegant all in all. The model is evaluated on 4 simulated physics tasks, and beats the only baseline the Hamiltonian Neural Network (HNN).\n\nNeural Hamiltonian Flow notes that hamiltonian dynamics are invertible and volume preserving, which is the properties you need for neural flow models. As such it propose to use a series of hamiltonian update steps with multiple learned energy functions as a flexible density estimator. The resulting density estimator is subjectively evaluated on three 2d toy density estimation tasks.\n\nI propose a weak accept as I think the paper is interesting and well written, but could be much better. The paper explains how both HGN and NHF work, but not much more. The HGN is only compared to a single other method (the closely related HNN), on four toy benchmarks. The NHF is barely evaluated, and not compared to anything.\n\nDoes the authors actually care about modelling physics and think their method is superior at this? If so, they should compare and contrast to some of the many, many papers on modelling physics, e.g. [1,2,3,4] and references herein. If not, what do they care about? Where do they think this model can be useful? Why should anyone use this model over some of the many, many other models one could use?\n\nSimilarly for the NHF, if I only read this paper I have no idea whether it's better than any of the other flow based models. Is it faster (to sample? to eval likelihood?) is it a better estimator? Why should I use it?\n\nI think the paper would benefit from being split into two papers, each thoroughly examining one idea.\n\nA few questions and minor comments\n\n - While the hamiltonian dynamics expect position and momentum vectors, the neural network is free to use those however it sees fit. Actually, if I understand correctly, the position vector must also encode the color of the objects for the 2 and 3 body problem. Is that correct? It would be interesting if you could examine how predictive the q and p vectors were of the true position and momentum vectors. \n - Successful experiments with n-body problems with n randomly sampled during training and unseen n used in testing would be very powerful in showing generalization. I'm afraid that the current setup doesn't generalize well.\n - I'm surprised that the generated images start showing artifacts after some time, e.g. pendulum sample 4 and 6 in https://docs.google.com/presentation/d/e/2PACX-1vRD2FnKgymgR2lU8lE6-XM8Cz-UWLTI6n_Uht3v6Gu4hIyMHmOcNL5D-0eG6Z4WHDAWS4qFosU-lxXP/pub?start=false&loop=false&delayms=3000&slide=id.g61bbdf339d_0_426. How can those appear if the hamiltonian dynamics preserve energy?\n - Equation 3 is given as self evident. It's not clear to me why 1) det(I+dt*A) = 1+dt*Tr(A)+O(dt^2). Can the authors give a reference? Also, doesn't the O(dt^2) term accumulate over multiple timesteps or longer rollouts? if so, how can the multiple steps proposed be said to be volume preserving? \n\n[1] - Battaglia, Peter, et al. 'Interaction networks for learning about objects, relations and physics.' Advances in neural information processing systems. 2016.\n[2] - de Avila Belbute-Peres, Filipe, et al. 'End-to-end differentiable physics for learning and control.' Advances in Neural Information Processing Systems. 2018.\n[3] - Santoro, Adam, et al. 'A simple neural network module for relational reasoning.' Advances in neural information processing systems. 2017.\n[4] - Fraccaro, Marco, et al. 'A disentangled recognition and nonlinear dynamics model for unsupervised learning.' Advances in Neural Information Processing Systems. 2017.", "belong_id": "HJenn6VFvB"}, {"uid": "HJgFZBKTKB", "paper_title": "Hamiltonian Generative Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: The authors present a method for learning Hamiltonian functions that govern a dynamical directly from observational data.  The basic approach uses three networks: 1) an inference network (I'm not clear why this is not just called an encoder), that maps past observations to a latent p,q space in a VAE-like fashion; 2) a Hamiltonian network that governs the time-evolution of the system over this latent state; and 3) a decoder network that outputs the observation from the latent state.  In addition to introducing this basic formalism, the \n\nComments: I have mixed opinions on this paper, though am leaning slightly toward acceptance.  The overall notion of learning a Hamiltonian network directly is a great one, though really this is due to the Hamiltonian Neural Networks paper of Greydanus et al., 2019.  Although the focus in that work is on applying learned Hamiltonian networks directly to physics-based data, they also have an encoder-decoder network just using a classical autoencoder instead of a VAE.  So my first impression is that the benefits of the proposed HGN over HNNs in Figure 6 is really just an artific of this replacement.\n\nPerhaps because the authors also felt this was a marginal contribution, the paper's ultimate value may prove to be in the consideration of such networks for the purposes of normalizing flow models.  This portion seemed a little bit underdeveloped in the paper, to be honest, but overall the idea of parameterizing a normalizing flow with a Hamiltonian dynamical system seems like a good one (e.g., allowing for easier large-timestep inference).  But on the flipside, it does seem like the presentation here is rather brief, i.e., just defining the ELBO without much context or detail, etc.\n\nThus, while I'm very much on the fence on this paper, I think the marginal improvement over HNNs via a better encoder/decoder model, plus the realization that these methods are a good fit for normalizing flow models, altogether put the paper slightly above bar for me.", "belong_id": "HJenn6VFvB"}, {"uid": "BkgU-6KRKS", "paper_title": "Hamiltonian Generative Networks", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces a novel way of learning Hamiltonian dynamics with a generative network. The Hamiltonian generative network (HGN) learns the dynamics directly from data by embedding observations in a latent space, which is then transformed into a phase space describing the system's initial (abstract) position and momentum. Using a second network, the Hamiltonian network, the position and momentum are reduced to a scalar, interpreted as the Hamiltonian of the system, which can then be used to do rollouts in the phase space using techniques known from, e.g., Hamiltonian Monte Carlo sampling. Finally, a decoder network can use the system's phase space location at any rollout step to generate images of the system. The HGN can further be modified, leading to a flow-based model, the Neural Hamiltonian Flow (NHF).\nThe authors evaluate the HGN on four simulated physical systems, showing substantial improvements over the competing Hamiltonian Neural Network (HNN). Lastly, the NHF is shown to be able to model complex densities, which can further be interpreted in terms of the kinetic and potential energy.\n\nThis paper is a rare treasure. It tackles a well-motivated problem and introduces a, to my knowledge, completely new framework for embedding Hamiltonian dynamics in a generative model. This is hugely inspiring! The paper is a joy to read and includes very informative figures providing a high-level understanding of the proposed models. Accept is a no-brainer.\n\nThat being said, I have a few questions and suggestions for improvements. My biggest complaint is the evaluation of the NHF model. I would have liked to see a comparison to a state-of-the-art flow-based model in terms of density modelling. The authors state that the NHF offers more expressiveness and computational benefits over standard flow-based models, but this is never shown. While I am willing to believe the claim, it is not intuitive to me, and I would have liked to see experimental verification of it.\n\nFigure 6 needs a bit of love. It is quite challenging to read. Larger font sizes, conversion to vector format, and more distinguishable colours will help a lot.\nAdditionally, I think it would be helpful to have the derivation of the ELBO in Eq. (4) written out, e.g. in the supplementary material.\n\nAdditional questions:\n- In the experimental section, I am not sure what is meant by the deterministic version of HGN. Which part if the model is deterministic?\n- On p 6, it is mentioned that the Euler integrator results in an increased variance of the learnt Hamiltonian and that this can be seen in Fig. 6. How exactly is this seen in the figure?\n- How many epochs were HNN and HGN trained for to produce table 1? How do the convergence rates look, and how long time did they take to train?\n\nMinor comments:\n- p 5: Reference to 'Salimans et al.' is missing the year.\n- p 6: There is a hanging ')' after 'as shown in Fig. 6).'\n- p 6: 'reversed it time' -> 'reversed in time'\n- In the reference for Glow, 'Durk P Kingma' should be 'Diederik P. Kingma'.\n\n", "belong_id": "HJenn6VFvB"}, {"uid": "rkeMVzZJ5r", "paper_title": "Progressive Compressed Records: Taking a Byte Out of Deep Learning Data", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: This paper introduces a new storage format for image datasets for machine learning training. The core idea is to use progressive JPEG to create sequential scans of the input image, from lower resolution to higher resolution. The authors found that on some datasets, using half of the scans is already enough to reach similar accuracy but speeded up the convergence by a factor of 2.\n\nDetailed feedbacks:\n-\tThe paper presents a simple idea that directly uses the nature of JPEG compression. The paper shows that it can work well and can be potentially integrated into real machine learning dataset storage applications.\n-\tRelated work section is thorough.\n-\tThe experiments are limited to image classifications, and some of the datasets are subsampled (e.g. ImageNet and CelebA). This may not well represent real machine learning tasks, and practitioners may be unsure about the reliability of the compression. The Cars dataset contains fine-grained classification, in which the proposed method is\n-\tFigure 1 is not very clear what is the key advantage of the proposed method, and what are the different mechanisms.\n-\tAlternatively, one can subsample the pixels and store incremental subsets of those pixels. It would be good if the paper can discuss about this baseline.\n-\tThe data storage format is only loosely related to the main goal of the paper, which is to show that network can still train very well and even faster when receiving partial input data. Once they figured out the number of scans needed for this application, they dont necessarily need to keep a full lossless version and can just go for a lossy version. In other words, the experiment section can be replaced by any other lossy compression by varying the compression ratio.\n-\tIn my opinion, there could be two reasons for faster convergence. 1) lowered image quality makes the data easier to learn and 2) the smaller data size allows faster reading of data from disk. The paper only shows wall-clock speed-up, but it is unclear which factor is bigger. 2) can be potentially addressed by faster disk reading such as SSD or in-memory datasets. One of the motivations is to help parallel training of dataset and it is also mentioned how non-random sampling of data can hurt training performance. It would be good to showcase how the proposed method can help in those parallel training settings. \n\nConclusion: This paper presents a simple and effective idea and can be potentially beneficial. However, my main concern is whether the experiments can be representative enough for large scale experiments (e.g. using non-subsampled ImageNet dataset with parallel training using SSD storage). Therefore, my overall rating is weak accept.", "belong_id": "S1e0ZlHYDB"}, {"uid": "SJluQZ2kor", "paper_title": "Progressive Compressed Records: Taking a Byte Out of Deep Learning Data", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes using progressive encoding of images and re-arrange of data blocks in images to improve reading speed and therefore training speed.\n\nTo fully analyze the maximum possible speed of training, it would be great to the measure upper bound of images/sec, when avoiding reading from disk and just using images from memory. \n\nDecoding a typical progressive JPEG image usually takes about 2-3 times as much time as decoding a non-progressive JPEG, for full resolution, analyzing the time to read vs time to decode the images would be great. It is not clear how changing the number of total groups would affect the image size and the reading speed.\n\nBased on the current experiments it is not clear what is the impact of the batch size when creating PCRs and when reading the image blocks, or the impact of the batch size on the training speed.\n\nFigure 3 is really hard to read and compare times to convergence, authors should provide a table with times to X% accuracy.  Although time to convergence is the key metric, it would be great to know the difference in images/sec of different settings.\n\nUsing ImageNet 100 classes (not clear how the 100 classes were chosen) instead of the usual 1000 classes, can distort the results, since it is not clear if higher resolution would be needed to distinguish more classes or not.\n\nHave the authors considered other image compression formats like WebP? How tie is the proposed record encoding with the image compression?  ", "belong_id": "S1e0ZlHYDB"}, {"uid": "BkeXwO--sB", "paper_title": "Progressive Compressed Records: Taking a Byte Out of Deep Learning Data", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces Progressive Compressed Records (PCR) which is an on-disk format for fetching and transporting training data in an attempt to reduce the overhead storage bandwidth for training large scale deep neural networks. This is a well written paper that includes all the required background and related works, as well as an easy-to-understand example that runs through the manuscript, explaining what the reader needs to know in order to appreciate the work. The empirical results of several experiments show that the PCR requires up to two times less storage bandwidth while retaining model accuracy.\n\nMy only concern is that although the related work section provides a thorough survey of the current methods in the literature, the authors did not demonstrate the performance of state-of-the-art and compare their performance with them. I believe this is necessary to truly validate the superiority of their method over state-of-the-art.\n", "belong_id": "S1e0ZlHYDB"}, {"uid": "r1ehxrXZoB", "paper_title": "Progressive Compressed Records: Taking a Byte Out of Deep Learning Data", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper demonstrates an interesting application of progressive compression to reduce the disk I/O overhead of training deep neural networks. The format encodes the trade-off between data fidelity and I/O bandwidth demand naturally, which could be useful when I/O is the bottleneck.\n\nMy major concern is that the paper should be clearer about the setting.\n* Does your work target the case where data cannot fit in RAM and should be fetched from local disk or through network? However, the datasets used in the evaluation look small and could fit in RAM.\n* How are mini-batches created? You mentioned in the related work that previous work (Kurth et al., 2018) lets each worker sample from a local subset instead of performing a true sampling of the whole dataset. Does your work perform true sampling? How much benefit does it give?\n* Is disk I/O really a bottleneck in training? There are many evidence [1][2][3] of almost linear scalability in training ResNet on *full* imagenet across hundreds or even thousands of GPUs. These work focus heavily on network communication rather than disk I/O. Does your setting differ from theirs? How does your approach compare with their techniques for optimizing disk I/O?\n\nThat being said, I think this approach should be appealing when the I/O bandwidth is limited and dynamic. Examples include training on edge devices, or federated training where data needs be fetched via ad-hoc network.\n\nOther detailed comments:\n\n* Figure 1 is not very informative and quite puzzling. There is no definition of quality at that point.\n* Sec 2 paragraph 3. What is the issue of data augmentation with the standard JPEG compression? Does your compression ease data augmentation?\n* Sec 3.1 paragraph 1. 'This is turn enables ...' -> 'This in turn enables ...'\n* How to decide the number of scans? Does it have impact on the I/O efficiency?\n* Evaluation\n  - I'm not familiar with Ceph. Why choose this particular environment? Does it bring in extra overhead (e.g., communicating with metadata server). What does the network topology look like? Is the data loading stall (figure 7) due to network congestion?\n - It worth evaluating more tasks such as detection and segmentation to measure the impact of compression.\n\n\n[1] Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour, Goyal et al.\n[2] Massively Distributed SGD: ImageNet/ResNet-50 Training in a Flash, Mikami et al.\n[3] Image Classification at Supercomputer Scale, Ying et al.", "belong_id": "S1e0ZlHYDB"}, {"uid": "rkg56JppKr", "paper_title": "Neural Architecture Search in Embedding Space", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work searches neural architectures in an embedding space, which is continuous. Overall, it lacks of innovation and experimental results are not strong enough. \n\n\t1. The innovation and technical contribution are weak. The key idea of searching in embedding space has already been proposed and applied in  Luo et al. 2018. The authors do not differentiate this work from Luo et al. 2018, and I didn't find any essential differences between them.\n\n\t2. The proposed method does work well. It is not compared with latest algorithms including NAO and DARTS. The reported numbers in Table 2 are poor, far from SOTA numbers.\n\nBesides, there are many writing issues and typos.", "belong_id": "HylWahVtwB"}, {"uid": "Hkl24SyG9B", "paper_title": "Neural Architecture Search in Embedding Space", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper borrows the idea of word-to-vector from NLP and applies it in reinforcement learning based Neural Architecture Search (NAS). It suggests a pretrained encoder to transform the search space to a dense and continuous architecture-embedding space. First it trains the architecture-embedding encoder and decoder with self-supervision learning like Auto-Encoder.  Then it performs reinforcement learning based Neural Architecture Search(NAS) in the architecture-embedding space.\n\nStrength:\nThere is no architecture prior, such as cell, in the searching process. Thus it's more general and can explore more architectures possibilities.\nBecause it performs architecture search in a continuous space, a CNN based controller is used instead of a RNN controller. \nThe result of the proposed method on CIFAR-10 is comparable with other popular NAS approaches.\nIt reduces the number of searching architectures to <100 in <12 GPU hours without using tricks such as cell or parameter sharing.\n\nWeakness:\nThe evaluations are highly insufficient. It only performs experiment on CIFAR-10, and the generalization ability on other datasets is unclear. In many NAS works. CIFAR-100 and ImageNet are commonly used to evaluate the performance. \nBesides, there is no comparison with more recent and related important methods such as DARTS and the method proposed by Luo et al. (2018). Actually its performance is not as good as Darts or the best performance reported in ENAS.\n", "belong_id": "HylWahVtwB"}, {"uid": "BJeoxR3Q5H", "paper_title": "Neural Architecture Search in Embedding Space", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes an interesting idea to perform Neural Architecture Search: first, an auto-encoder is pre-trained to encode/decode an neural architecture to/from a continuous low-dimensional embedding space; then the decoder is fixed but the encoder is copied as an agent controller for reinforcement learning. The controller is optimized by taking actions in the embedding space. The reward is also different from previous works which usually only considered validation accuracy but this work also considers the generalization gap.\n\nThe idea is interesting, but there are some problems on both the method's and the experimental sides:\n1. NAO [1] also embeds neural architectures to a continuous space. Different from NAO which applies gradient descent in the embedded space, this paper uses RL. I double that RL can work better than gradient descent in a continuous space. The paper should compare with NAO. Ideally, this paper might work better than NAO if the accuracy predictor in the NAO is not accurate, while this paper uses real accuracy as a reward for search. However, this is not soundly compared.\n2. It is unreasonable to discretize continuous actions to a Bernoulli distribution. Many RL methods, such as DDPG, can handle continuous actions;\n3. The paper uses Eq. 1 as a reward. It's interesting, but it's unclear why the generalization error is needed. Ablation study is required.\n4. As the community makes more progresses in AutoML, a better and better (smaller and smaller) search space is used. It doesn't make much sense to compare the search time under different search spaces. Comparison under the same setting (e.g. NASBench-101) is required. \n\n\nMinors:\n1. missing a number in 'and T0 = epochs'\n2. missing 'x' in '32 32 RGB in 100 classes', and '100' should be '10'\n\n[1] Luo, Renqian, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. 'Neural architecture optimization.' In Advances in neural information processing systems, pp. 7816-7827. 2018.", "belong_id": "HylWahVtwB"}, {"uid": "r1gG4uu3FB", "paper_title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes stable GradientLess Descent (GLD) algorithms that do not rely on gradient estimate. Based on the low-rank assumption on P_A, the iteration complexity is poly-logarithmically dependent on dimensionality. The theoretical analysis of the main results is based on a geometric perspective, which is interesting. The experimental results on synthetic and MuJoCo datasets validate the effectiveness of the proposed algorithms.\n\nThe theoretical contribution of this paper is nice and valuable. My main concern is the structure f(x) = g(P_A x) + h(x) looks somewhat limited. A more natural form is moving the perturbation into g, i.e, f(x) = g(P_A x + h(x)). \n\nThe experiments on Mujoco do not satisfy the assumption previous. Is there any real-world application which matches the theoretical analysis?\n\nIn summary, I think this is a good paper and tend to accept it.\n", "belong_id": "Skep6TVYDB"}, {"uid": "H1ePXMhaYS", "paper_title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "** Summary\nThe paper proposes a novel zeroth-order algorithm for high-dimensional optimization. In particular, the algorithm as an instance of direct search algorithms where no attempt is made to estimate the gradient of the function during the optimization process. The authors study the optimization of monotone transformations of strongly-convex and smooth functions and they prove complexity bounds as a function of the condition number, the dimensionality and the desired accuracy. These results are also extended to the case where the function actually depends on a lower-dimensional input. Without any knowledge of the actual subspace of interest, the algorithm is able to adapt to the (lower) dimensionality of the problem. The proposed algorithms are tested on synthetic optimization problems and in a few Mujoco environments for policy optimization.\n\n** Overall evaluation\nThe paper is a solid theoretical and algorithmic contribution to the zeroth-gradient optimization literature. The positive aspects of the paper are:\n- Novel algorithm with strong theoretical guarantees improving or generalizing previous state-of-the-art methods.\n- Ability to adapt to low-dimensional problems and more in general to monotone transformations of convex functions.\n- Efficient version.\n\nNegative aspects of the paper that the authors may address are:\n- The empirical validation is rather weak at the moment. It provides some evidence of the effectiveness of the proposed method but it uses only one baseline and a very few type of optimization problems. Although in my opinion the main contribution is on the theoretical side, a more thorough empirical validation would be welcome.\n- Some theorem statements can be made clearer and some comparisons should be more explicit (see detailed comments later).\n\nDetailed comments:\n1- The authors explicitly mentioned in the introduction that they do not compare/discuss alternative approaches such as Bayesian optimization (BO). Although I agree the approaches may be different, BO is probably the most popular type of black-box optimization. Furthermore, many methods (e.g., GP-UCB https://arxiv.org/abs/0912.3995) come with strong theoretical guarantees on the regret and so optimization performance both under the Bayesian assumption (i.e., the function is generated from a prior) and the 'frequentist' case (i.e., the function is an arbitrary element of a bounded RKHS). Furthermore, there are also adaptive BO methods that adapt to the actual dimensionality of the problem, in a similar spirit as the low-dimensional case studied in this paper. See e.g., https://arxiv.org/abs/1903.05594 and http://papers.nips.cc/paper/8115-efficient-high-dimensional-bayesian-optimization-with-additivity-and-quadrature-fourier-features. I would appreciate if the authors would at least provide a high-level discussion on similarities and differences between these type of approaches.\n2- Thm7: r = 2^k1 C_1 and r = 2^-k2 C_2 are the only two possible radii? Is the statement valid for any choice in the range?\n3- Thm13: Unlike the statements in Sect.3.2 and 3.3, here the result is reported in terms of x_T (instead of f(x_T)). This is perfectly fine, but the guarantee you obtain is not an epsilon accuracy, but Q^{3/2}epsilon. If we want to obtain an epsilon accuracy, how much is the number of evaluation going to change? It seems like it would just make an additional Q appear in the log, but I would like the authors to confirm.\n4- Thm13: 'High-probability': could you make this more explicit? Can I make the probability arbitrarily close to 1? How would it appear in the number of iterations? As just a log(1/delta) term?\n5- Thm14 is reported for 'suitable parameters'. Although this choice of parameter actually appears in Alg.2, it would be more complete to report it in the statement as well.\n6- Fig1 bottom line, first two charts display a weird behavior for GLD-Fast, where the error seems to plateau and spot decreasing. Can you explain why this is happening? Is it due to wrong parameters \\hat alpha and \\hat beta?\n\nMinor comments:\n- In the proof of Lem.8, it would be helpful to have a graphical representation of the spheres and the hyperspherial caps.\n- In the proof of Lem.15, you mention 'strong smoothness assumption', it should be just smoothness.\n- It would be helpful to have more intuition on the why the algorithm is able to adapt to the actual dimensionality of the problem. My understanding is that the probability to pick a point of lower value is increased and since the algorithm is testing different radii and pick the best point, it successfully adapt to this better situation.", "belong_id": "Skep6TVYDB"}, {"uid": "B1geUE_Hcr", "paper_title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nUpdate after rebuttal: I found the rebuttal convincing and I liked the fact that concerns regarding empirical justification were addressed. Consequently, I increase my score from 'Weak Reject' to 'Weak Accept'.\n--------------------------\nThis paper focuses on derivative-free, or zero-th order, optimization. That is the setting where a function may be continuous and/or smooth and/or (quasi)-convex, however, we do not have access to the gradients. As such, it is not possible to apply standard gradient descent methods. The paper proposes an algorithm for 'gradientless' descend. The basic intuition is that by careful randomly sampling it is quite probable that a lower objective value will be attained. Doing so recursively can then lead to the optimum with high probability. Clearly, in such a setting it is quite important to clarify what is 'careful random sampling'. To this end the paper casts this as sampling from a Gaussian ball of specific radius, chosen such that the samples are with high probality below the current level set (the hyperplance of equivalent solutions f(x) as our current solution f(x_t)). The paper derives and proves various theorems on how to select the optimal radius and how to perform the sampling. Specifically, the case of strongly convex and smooth functions is analyzed, however, the paper also shows how this generalizes to functions after a monotone transformation (thus leading to quasi-convex functions) and with extra error perturbations. The proposed algorithm is compared on a synthetic experiment, and a selection of MuJoCo benchmarks.\n\nStrengths:\n+ The derivations and the theorems are non-trivial. There is some serious analysis regarding the selection of radius of Gaussian balls. I would like to congratulate the authors for this. I particularly like the extension to having a perturbing function h(x), leading to a more realistic setup.\n+ I also particularly like that the algorithm is able to recover subspaces automatically. This definitely makes the algorithm much more practical and more efficient.\n+ The writing and the presentation are rather clear and well taken care of. Although there are several theorems, it was not too hard to follow the flow of the paper. The algorithm boxes are also concise and clear, helping with understanding the final result.\n\nWeaknesses:\n+ Although the contributions of the work are mostly on the theoretical side, I have a hard time grasping how useful is the algorithm in practice. For one, there is the assumption of strongly convex and smooth function. Granted, there is the relaxed case of having the perturbing function h(x), howevrer, in that case it seems that the algorithm becomes a slower by an order of 60  k Q_g(A). How fast or slow is this in practice? Even with applying the monotone function, the algorithm becomes more practical by being applicable to quasi-convex setups. However, how realistic is that a function will in practice be strictly monotone? While most of this may be hard to be theoretically proven, they can be experimentally tested.\n\n+ Also, given that the paper is interested in black box functions, we cannot have much information regarding the function. So, what happens when the function is not strongly convex or not always smooth? Furthermore, how realistic is to know the condition number, that is the maximum derivative (or an upper bound of it) since we do not have access to the gradients in the first place?\n\n+ I would say that the paper could benefit from a more extensive experimental section. Currently only a single synthetic function is analyzed, also under a single monotone exponential transformation. From a more practical point of view, MuJoCo environments are also examined. However, there exist no comparisons with other methods in the literature, including ARS. Another relevant algorithm to compare with would be the stochastic tree points (Bergou et al., 2019), if not experimentally at least theoretically. In the end, it quite unclear whether the algorithm work well in practice. Some experiments that could shed light would relate to how sensitive the algorithm is to the convexity/smoothness assumptions, how sensitive the algorithm is to the perturbing function h(x), how sensitive is the algorithm to the present of a lower-dimensional subspace that needs to be discovered. And for the MuJoCo experiments, the algorithm can compare at least with ARS.\n\n+ In the experiments it seems the paper is particularly good in high dimensions. Can this be more precisely connected to the derived theory in the discussion of the experiments? Does this relate to the better subspaces k that are discovered automatically by the algorithm?\n\n+ It is unclear how many evaluations are needed per step, that is what is the K value in the algorithm box? Also, there are at least two balls to sample from, so twice as many evaluations, correct?\n\n+ It is unclear why Bayesian Optimization is not considered for at least comparing experimentally. Currently, the paper discards them on the grounds that they do not provide strong theoretically guarantees. However, it would be interesting to examine at least in practice how good/bad are these algorithms in comparison to the proposed one. Two recent bayesian optimization papers that can be considered for continuous and discrete inputs are\n\n[1] BOCK: Bayesian Optimization with Cylindrical Kernels, C. Oh, E. Gavves, M. Welling, ICML 2018\n[2] BOCS: Bayesian Optimization of Combinatorial Structures, R. Baptista, M. Poloczek, ICML 2018\n\n+ The paper does not have a conclusion. That shows great sloppiness. Also, i/n the abstract, do you mean to say k>=n or k<=n?\n\nTo conclude, I recommend weak rejection only because I am not completely convinced by the experiments and do not know if the proposed algorithm is competitive against reasonable baselines and in more complex setups. I am more than happy to upgrade my score if experiments become more clear.", "belong_id": "Skep6TVYDB"}, {"uid": "HkxgREa4Kr", "paper_title": "Dual-module Inference for Efficient Recurrent Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper attempts to compress the networks so as to accelerate the running procedure as well as save the storage. The authors propose a dual-module that is composed of a little module and big module. The big module use the full original data and parameters whereas the little module use small data and parameters by random projecting on the original ones. Through a statistical investigation, the authors provide a method to choose the little or big module dynamically. By applying this method on LSTM and GRU, the authors make them more efficient. Experimental results validate this point.\n\nOverall, I think this paper is well written and easy to follow. The idea of dual-module is interesting and wise. The experiments is valid. However, I would like the authors to answer me two questions. \n1 How do you draw Figure 2? \n2 Why is the distribution of the outputs of sigmoid not bipolar? Intuitively, the distribution should be similar to that of tanh since their functions are similar.\n", "belong_id": "SJe3KCNKPr"}, {"uid": "B1e26lgRYr", "paper_title": "Dual-module Inference for Efficient Recurrent Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This manuscript proposes an approach to reduce memory access and computation in Recurrent Neural Networks.  Specifically, they train a second 'little' neural network to approximate a pre-trained 'big' network and use simple rules to switch between the little and the big network.  The approach can provide some speedups while reducing the total number of memory accesses and the computational cost in exchange for a mild decrease in predictive performance.\n\nWhile this manuscript proposes a reasonable contribution, it lacks real comparisons to many of the common competing methods that hinder the interpretation.\n\nWeight sparsity/pruning is a very common approach that has shown the ability for larger speedups than what is shown here.  I disagree with the assessment that 'those methods require extensive retraining via regularization.' Realistically, you can take a pretrained model and add the penalties with mild re-training and extensive reuse of code.  The result is also simpler to implement.  I would argue that this is less work than the proposed approach, which requires switching rules and a second trained network.  I don't know which is better, but the authors should actually evaluate whether their approach improves over the more popular approach.\n\nThe authors should give better discussion and motivation on the random projections.  This is an area with very deep theory, yet the rules are provided without a rationale.  Realistically, where does the sparsity level in the random sparse matrix come from?  Why use the rule for k in (3)?  The authors should motivate and discuss this section more.\n\nAlso note that there is existing literature on learning multiple models and switching between them, for example:\nBolukbasi, Tolga, et al. 'Adaptive neural networks for efficient inference.' Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\nAs there is a lot of similarity in the motivations, you should discuss that line of research in your related work.", "belong_id": "SJe3KCNKPr"}, {"uid": "H1xpu-VWcB", "paper_title": "Dual-module Inference for Efficient Recurrent Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors design a big-little dual-module inference to dynamically skip unnecessary memory access and computation to speedup RNN inference.\nIt cannot only mitigate the memory-bound problem to speedup RNN inference but also leverage the error resilience of nonlinear activation functions by using the lightweight little module to compute for the insensitive region and using the big module with skipped memory access and computation. They also conduct several experiments to evaluate their approaches.\n\n\nStrength:\n(1)\tWell written in general.\n(2)\tContributions clearly stated and justified\n\nA couple of minor questions:\n\n(1)\tThe organization in Section 3 can to be improved. It is better if the author can give a brief overview of their method first and then go into details.\n(2)\tSome of the technical details necessary for understanding the soundness of the techniques are either missing or are poorly explained. For example, in Section 3\na.\tthe authors did not mention how to construct the HH module\nb.\tthe authors did not provide detailed information of how to conduct dimension reduction since this will affect the performance\nc.\tmany mathematical notations and equations need to be revised to increase the readability. For instance, there is no information in the paper that explain why the authors design functions in such certain way (such as equation (2) and (3))\nd.\tthe authors did not provide enough detailed information about how to select the quantization methods since there are lots of approaches such as static (uniform) or dynamic quantization, where different methods may have different impacts on the final performance \ne.\tthe authors mentioned that they have tried both sigmoid and tanh activation function to find the sensitive region. However, they do not provide enough reason to do so, how about using other non-linear activation functions\n\n(3)\tThe organization in Section 4 can to be improved. It is better if the author can introduce the motivation of each experiment.\n\n(4)\tParameters of the evaluation are unclear or missing. For example, \na.\twhat is the data size, what is the dropout, learning rate, how many time stamps for the RNN modules\nb.\twhy the authors only use single-layer LSTM and why they select 750 and 1500 hidden units in the experiments\n\n(5)\tWhile the authors have applied their models on other existing method, they do not provide good discussion of results and such model seems old (released in 2016). It would be great if this approach can also be applied on other newly released models\n(6)\tSome tables need to be reorganized. For instance, for table 6, there needs some space between the title and the table.\n(7)\tWhile the paper has good coverage of the prior work, I do suggest the authors can also cite or discuss some newly designed models (in 2019).\n", "belong_id": "SJe3KCNKPr"}, {"uid": "BygUatwYOB", "paper_title": "Stagnant zone segmentation with U-net", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "- This paper simply proposes to use UNet for the segmentation of stagnant zones in X-ray CTs. While the applicability of this model may represent an advance in the particular field of the authors, the technical contribution of this paper is far from the level expected in this conference. \n\n- As the paper reads, the main contribution of the paper is the modified version of UNet 'proposed' by the authors, which major modification consists on replacing SGD by Adam. Nevertheless, this cannot be considered a contribution, as changing the optimizer in a deep model is a marginal change, from a methodological point of view. \n\n- Overall, the quality of the paper is below the standards of ICLR (content, technical contribution, length).\n\n- The submission is not anonymized (authors included their names and affiliations). ", "belong_id": "H1eH9hNtwr"}, {"uid": "Hyxln7Sotr", "paper_title": "Stagnant zone segmentation with U-net", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "I support desk rejection since violating double blind rule, wrong format and insufficient length.\n\n                                                                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                              \n", "belong_id": "H1eH9hNtwr"}, {"uid": "SklcjZbP5r", "paper_title": "Stagnant zone segmentation with U-net", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper propose a modified U-net architecture to segment the stagnant zone during silo discharging process. It lacks novelty and the improvement is marginal.  More importantly than all of that, this paper violates the double blind review rule and is same with [1]. So I think this paper is not suitable for acception.\n\n[1]Waktola S, Grudzien K, Babout L. Stagnant zone segmentation with U-net[C]//2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering (AIKE). IEEE, 2019: 277-280.\n", "belong_id": "H1eH9hNtwr"}, {"uid": "S1l6oaWstr", "paper_title": "Accelerated Variance Reduced Stochastic Extragradient Method for Sparse Machine Learning Problems", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes an optimization method for solving unconstrained convex optimization problems where the objective function consists of a sum of several smooth components f_i and a (not necessarily smooth) convex function R. The proposed method AVR-SExtraGD is a stochastic descent method building on the previous algorithms Prox-SVRG (Lin 2014) and Katyusha (Zeyuan 2017). The previous Prox-SVRG method using a proximal operator is explained to converge fast but leads to inaccurate final solutions, while the Katyusha method is an algorithm based on  momentum acceleration. The current paper builds on these two approaches and applies the momentum acceleration technique in a stochastic extragradient descent framework to achieve fast convergence.\n\nI am not working in the field of optimization, therefore, unfortunately I am not in a position to give detailed technical comments for the authors. However, as far as I could follow the paper, it seemed sound and well-written to me in general. I hope the following minor comments may be useful for improving the paper:\n\n- The paper gives detailed explanations about previous work. However, the proposed AVR-SExtraGD algorithm is only presented in the form of a pseudocode in Algorithm 1 and it is not explained in much detail. It would be good to explain and discuss intuitively the steps of the proposed algorithm in the main body of the paper as well, so that it is well understood.\n\n- Algorithm 1 has a set K as input, according to which the solution is updated. How should this set be chosen in practice?", "belong_id": "BklDO1HYPS"}, {"uid": "B1x_ANL1cS", "paper_title": "Accelerated Variance Reduced Stochastic Extragradient Method for Sparse Machine Learning Problems", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a new stochastic algorithm: AVR-ExtraGD. AVR-ExtraGD combines the extended extragradient method proposed in [3] and the accelerated SVRG method in [1][2]. In their experiments, AVR-ExtraGD outperforms [2] in running time for sparse linear regression.\n\nThis paper presents their convergence analysis using results in [1][2]. They showed that the proposed algorithm can achieve O(sqrt{kappa n} log (1 / \\epsilon)) complexity for strongly convex problem and O(1/sqrt{epsilon}) for convex problem, which are the best results for both cases.\n\nThe idea of an accelerated version of variance reduced stochastic extragradient method is novel. However, there are some issues that the authors should address in order for the paper to match the quality of ICLR.\n\nEach step of extragradient approximates the proximal operator x_{k+1} = argmin_x P(x) + 1/(2 eta_k)\\|x  x_k\\|_2^2, therefore we would expect a faster and more stable convergence from this method. This paper claims that extragradient reduces the gap between the obtained optimal value and the real optimal value, which is confusing. The update of extragradient is actually biased towards x_k. The claim is then discussed in section 3.1 and 3.2 but is not clearly explained. Besides, how this claim is reflected in the convergence result is not discussed. I encourage the authors to clearly elaborate this claim and make relevant remarks after the main theorems.\n\nTo better understand the convergence result, it is important to know how extragradient affects the complexity and choice of hyperparameters such as K, eta_1, eta_2, and beta. Such discussion is not in this paper. I suggest the authors to make these aspects clear.\n\nThe experiments compare the proposed algorithm with other algorithms by their running time for lasso and elastic-net. The comparisons show the efficiency of the proposed algorithm. However, a more careful experimental design is required to better demonstrate the performance:\n1.\tFor the choice of inner iterations, choosing m=2n for Katyusha actually requires calculating 5n stochastic gradient because each iteration of Katyusha does gradient updates twice.\n2.\tThis paper only presents comparisons of running time. I encourage the authors to also plots comparisons on number of iterations, which will help revealing where the speed up of AVR-ExtraG comes from.\n3.\tIt is also preferable that the author compare with MiG [1], since the proposed algorithm is an extragradient version of [1].\n4.\tPlease at least solve two different optimization programs (e.g. logistic regression, neural network) so any conclusions are not specific to the oddities of a particular program.\n\nThe presentation and structure of this paper need to be improved. Here are some suggestions:\n1.\t In Section 1, only provide a high-level literature review and then motivate the work. A comprehensive review can come after the introduction.\n2.\tIn Section 4, put all the lemmas into the appendix while giving more intuitions and remarks. \n3.\tIssues including notions without pre-definition or reference, typos, and incorrect gramma need to be fixed.\n\n\nDetailed comments:\n1.\tFrom the title, the main application of this work is sparse learning problem. However, how the proposed algorithm benefits sparsity is not discussed. Besides, I suggest the authors to move the asynchronous algorithm in the appendix to the main paper.\n2.\tThe paragraph before section 1.1: lasso and elastic-net are used without citation.\n3.\tSection 1.1: PGD and SGD are used without citation.\n4.\tBeginning of page 2: And should be Besides\n5.\tBesides, for accelerating the algorithm and ...: for accelerating should be to accelerate\n6.\tSection 1.2: Nguyen et al. (2017) proposed the idea of extragradient which can be seen as a guide during the process, and introduced it into the optimization problems. What does the process and it refers to is unclear.\n7.\tSection 1.2: the claim extragradient examines the geometry and curvature of the problem is confusing. The geometry of the problem is inspected through a line search step in [3]. However, line search is not discussed in this paper.\n8.\tSection 1.2: reduce the gap between the optimal value we get and the real optimal value, these two kinds optimal values are important notions of this paper but they are not defined.\n9.\tIn Assumption 2, you can refer to Part 2, Section 7 of [5] for the definition of semi-continuity.\n10.\tdw is the gradient of the function at w, what does the function refers to?\n11.\tAPG and Acc-Prox-SVRG needs citation.\n12.\twas proposed to simply the structure of Katyusha, simply should be simplify\n13.\tSection 3.1: updated with the update rules of MiG: with should be by\n14.\tIn the equations of Section 3.2, the equivalent of gradient norm square and function f is incorrect, and the purpose of this equation is unclear.\n15.\tSection 4.1 Theorem 1: The inequality in theorem 1 is not intuitively related to the convergence rate. I suggest the author to simplify the inequality (For example, Theorem 2.1 in [2]).\n16.\tThe references are not in a uniform format. Conference/Journal names are missing for some references.\n17.\tOne useful reference for this paper is [4], it discussed extragradient for online convex learning.\n\nAdditional question:\n[5] update the extragradient step by sampling a new stochastic gradient while in this paper the same sample is used twice. How you compare these two approaches in terms of their performance and convergence?\n\n[1] A simple stochastic variance reduced algorithm with fast convergence rates, Zhou et al., 2018.\n[2] Katyusha: the first direct acceleration of stochastic gradient methods, Z. Allen-Zhu, 2017\n[3] Extragradient method in optimization: Convergence and complexity, T. Nguyen et al., 2017\n[4] Online Optimization with Gradual Variations, Chiang et al., 2012\n[5] Convex Analysis, R. Rockafella, 1970\n[6] Reducing Noise in GAN Training with Variance Reduced Extragradient, Chavdarova et al.  2019\n", "belong_id": "BklDO1HYPS"}, {"uid": "B1eqF1Ykqr", "paper_title": "Accelerated Variance Reduced Stochastic Extragradient Method for Sparse Machine Learning Problems", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This is an optimization algorithm paper, using the idea of 'extragradient' and proposing to combine acceleration with proximal gradient descent-type algorithms (Prox-SVRG). Their proposed algorithm, i.e., accelerated variance reduced stochastic extra gradient descent, combines the advantages of Prox-SVRG and momentum acceleration techniques. The authors prove the convergence rate and oracle complexity of their algorithm for strongly convex and non-strongly convex problems. Their experiments on face recognition show improvement on top of Prox-SVRG as well Katyusha. They also propose an asynchronous variant of their algorithm and show that it outperforms other asynchronous baselines.\n\n- technically sound, seems like a nice addition to the variance reduced gradient-type methods. Combines the nice properties of proximal methods with variance reduced gradient-descent.\n- Nice summary of recent progress in this research area. \n- How does it compare to SVRG++? How about the Proximal Proximal Gradient? \n- algorithm suitable for non-smooth optimization problems\n- their experimental results look convincing.\n\nHaving said that, it seems to me that combining momentum with an existing algorithm is not extremely novel -- I would defer to reviewers who are experts in the optimization area to fully assess the novelty and technical difficulty of the proposed solution.\n\n ", "belong_id": "BklDO1HYPS"}, {"uid": "SyeESNS6YH", "paper_title": "NESTED LEARNING FOR MULTI-GRANULAR TASKS", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Nested learning for multi-granular tasks\n\n1. Summary\nThe paper considers a framework for classification with a hierarchy of labels [from coarse to fine]. The paper proposes a network architecture with multiple bottleneck layers, one for each label level, and skip connections. The objective function is the standard classification loss. The experiments show that coarse labels help learning and can improve label efficiency, i.e. dont need all fine labels to get good classification performance.\n\n2. Opinion and rationales\n\nWhilst I think the execution of ideas is good and the motivation is very practical, Im leaning towards reject for this paper due to the reasons below. I welcome the authors clarification and am willing to reconsider my view.\n\ni. The paper justifies the proposed architecture as successive information compression into label embeddings using some entropy based criteria (as in information bottleneck literature). This ensures the relationship of the entropies between the label layers. However, Im not sure this justification is necessary (albeit being a valid one) given that the training/later sections do not come back to this justification.\n\nii. The novelty of the proposed architecture and training approach is low. The network is a nested structure of successive classifiers. The proposed calibration using rejection class and temperature scaling is not new.\n\niii. it would be better if the proposed architecture + method are validated on more real-world datasets with a more natural label grouping scheme, and compared to alternative architectures, e.g. multi-task learning with a shared network except the output layer.\n\n3. Minor details\n\nSome citations should be enclosed in brackets", "belong_id": "Byxl-04KvH"}, {"uid": "B1giGrB0FS", "paper_title": "NESTED LEARNING FOR MULTI-GRANULAR TASKS", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I read the author response, thank you for responding to my questions.\n\nOriginal review:\n\nThis paper presents a hierarchical learning approach that trains neural network classifiers on a known hierarchy of labels.  The experiments show that the approach makes a network more robust to distortion compared to standard end-to-end learning.  The approach in this paper and its formalization of the task are interesting, but the significance of the techniques is somewhat unclear and the experiments could be more thorough in terms of the baselines and data sets considered.\n\nFrom the related work section, the relationship between this paper and the previous work is somewhat complicated -- it is hard for a reader not deeply familiar with these previous works to understand the unique contribution made in the submission, and assess why it is significant.  For example, in the last paragraph of that section, many of the distinctions drawn between the submitted work and previous methods seem minor (including a confidence measure for a certain prediction, for example) or seemingly subjective (about whether an operation with a previous method was natural or could be done transparently).  Making crisper, less ambiguous distinctions between this work and previous work would help.\n\nLikewise, the experimental results here do not compare against any of the hierarchical learning approaches discussed in the related work section.  The results show that the papers approach is more robust to distortion compared to standard end-to-end learning.  However, I was unclear on why it was not appropriate to compare against the other hierarchical learning methods from previous work.  Also, if the claim is improved robustness, I feel that evaluating against adversarial training (Madry et al., ICLR 2017) or similar approaches is necessary to understand the practical relevance of these improvements.\n\nFinally, experiments that consider larger hierarchies (here, the number of target classes tends to be small, CIFAR-10 and MNIST each have ten classes, meaning the hierarchies are not very rich) would help illustrate the potential power of the techniques.\n\nMinor\nI didnt understand the following statement, and given that its a fairly bold claim I would rephrase it or explain it better in the paper body rather than referring the reader to the appendix:\nA standard DNN unknowingly uses low quality data also to train higher layers, even if there is no high level information in the data.\n\nI dont understand what the right arrow operator on the top of page 5 means.  From earlier statements it seems that I(f_i(X), Y_i) -> I(X, Y_i) means that the left quantity is approximately equal to the right.  But Im not sure why to use an arrow for that rather than an \\approx symbol.  The arrow would seem to imply that the left approaches the right in the limit, but if that is what you mean you should tell us in the limit of what.  Later the arrow is used to represent links in a Markov chain, adding further confusion for me.\n\nI think it would be helpful if before Equation 1, you mentioned this holds for strictly nested Y_is (since earlier in the paper, Y_i referred to more general things).\n \nI assume the ECE is computed over held-out validation data (i.e., not training data)?  The paper should say this.\n\nPage 7: lineal -> linear", "belong_id": "Byxl-04KvH"}, {"uid": "HygywE9K5B", "paper_title": "NESTED LEARNING FOR MULTI-GRANULAR TASKS", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: The problem addressed is how to train a DNN to learn a hierarchical representation of the input that has heterogeneously annotated labels at various levels of granularity. After training the networks goal is to emit sequentially finer grained labels with corresponding confidence. The authors use information theory to propose a network typology of information bottlenecks with skip connections to achieve this nested learning problem.  \n\nDecision: weak reject. \n\nReason: The proposal of learning a hierarchical representation is not new. Nor is the architecture. I think the interesting points (that are not really flesh out as much, but appear to be in the auxiliary material - section B.2) is the training regime. I wouldve liked to have seen more of what the role of the training regime is on the outcomes and how the networks gradients behave in different regimes. But in general, the paper is well organized and argued, although there is a little belaboring of ideas of entropy and mutual information only to use it to buttress a point that is made and left hanging. The paper also defines too many concepts for every proposition it wants to support, making it cognitively costly to follow. \n\nFeedback:\nPleasure reading the paper. Few points of feedback:\n\n- Perhaps show the network gradients to help understand the dynamics of the learning\n- What do you think is the role of the training regime (section B.2) on the outcomes you are observing? Do you think it would be worth observing the effect of changing the learning regime (and the accompanying gradient) on the outcomes?\n- Generalization to any number of nested labels is not demonstrated\n- The empirical demonstration of contribution of skip connections is not too  \n- Corollary to above, what do you think would be the role of attention in the hierarchical representation learning?\n\nQuestions:\n1- Not clear why complementarity is a necessary condition of learning (section 3, before definition 1). Take for example the vehicle, wheels, truck example in figure 1. Learning F2 (wheels) features isnt conditioned on correctly learning F1 (vehicle). In fact, could it be satisfactory, to first order approximation, to assume that learning finer-grained features first (roundness of wheels) and then combining lower level features (in what you refer to as Markov chain) may result in equal if not better accuracies? Is it possible to test this?\n\n2- Not clear why calibration of outputs (section combination of nested outputs in p 5) is an approximation of P(Y_i=q).\n\n3- Its not clear to me why the proposed rejection calibration method as a way of handling overconfidence is the right approach. Why this solution? Why not use, for example, regularization instead?\n\n4- Table 1 of results. As the distortions increases the relative improvements of the nested learner appears to increase in CIFAR-10 results. Can you demonstrate this is not an artifact of the distortion generation strategy and is indeed a stable observation?\n\n5- why is the marginal accuracy improvements so much larger for going from a budget of 1 to 2 than 2 to 5 in all cost functions? Does this not refute the claim that the nested model gradually breaks?", "belong_id": "Byxl-04KvH"}, {"uid": "H1l-cYdUFS", "paper_title": "Attention Interpretability Across NLP Tasks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Motivated by an existing paper, the paper analyzes the interpretability of attention mechanism over three NLP tasks. The previous paper claims that attention mechanism is not interpratable. The paper makes incremental contribution by showing that attentions are not interpratable when they mimic gating units (single sequence task) but are interpretable for two sequence and generation tasks. Experimental results are given to support the claims, which can also help readers to gain insights into the attention mechanism. The paper is well written and all claims are supported. I also have some questions below for clarification. If I get reasonable answers to these questions, I tend to accept the paper. \n\n1. Jain & Wallace (2019) show that for the SNLI problem, attentions weakly correlate with the output based on Kendall's correlation and JSD which contradicts to your observation. Could you explain how this happens? are there any model settings different?\n2. in figure 5, for the single sequence (original), most of attentions leading to correct predictions are labeled meaningful. Does this mean that even the attention doesn't necessarily contribute too much to the prediction correctness but they are also interpretable if they are allowed to be trained. Also, in Table 1, with modified attentions, all scores go down a little. This means that attention still can contribute to the final prediction but not significant enough (some like yelp are significant). I am gussing that the sequence encoding already present useful features for the final prediction. Did you check the distance of different word encodings? Are these encodings all very similar?", "belong_id": "BJe-_CNKPH"}, {"uid": "r1xry0J6tB", "paper_title": "Attention Interpretability Across NLP Tasks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigates the degree to which we might view attention weights as explanatory across NLP tasks and architectures. Notably, the authors distinguish between single and 'pair' sequence tasks, the latter including NLI, and generation tasks (e.g., translation). The argument here is that attention weights do not provide explanatory power for single sequence tasks like classification, but do for NLI and generation. Another notable distinction from most (although not all; see the references below) prior work on the explainability of attention mechanisms in NLP is the inclusion of transformer/self-attentive architectures. \n\nOverall, this is a nice contribution that unifies some parallel lines of inquiry concerning explainability, and attention mechanism for different NLP tasks. The contrast in findings between single sequence (classification) and other NLP tasks and the implications for explainability is interesting, and provides a piece missing from the analyses conducted so far. \n\nThe main issue I have with the paper is that I'm not sure I follow the rationale in Section 4 arguing that because attention 'works as a gating unit', it follows that we cannot view single sequence tasks 'as attention'. Can the authors elaborate on why the conclusion follows from the premise? In other words, why is attention inherently *not gating*? This seems like an interesting connection, but again I do not see why attention acting as a gate implies that we should not view it as attention at all. Perhaps the authors could elaborate on their reasoning. \n\nSome additional references the authors might consider. Note that the latter two papers, I think, would seem to broadly disagree regarding self-attention and tasks aside from single sequence (classification): \n- 'Fine-grained Sentiment Analysis with Faithful Attention': https://arxiv.org/abs/1908.06870\n- 'Interrogating the Explanatory Power of Attention in Neural Machine Translation': https://arxiv.org/abs/1910.00139\n- 'On Identifiability in Transformers': https://arxiv.org/abs/1908.04211", "belong_id": "BJe-_CNKPH"}, {"uid": "Hke1Vxpg9r", "paper_title": "Attention Interpretability Across NLP Tasks", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "CONTRIBUTIONS:\nI use (unqualified) self-attention to refer to attention of tokens in a sequence to other tokens in the same sequence, as described by [some corrected version of] Eq (1) and the paragraph following it (citing Bahdanau et al. 2015). This contrasts with Transformer self-attention and cross-sequence attention.\nC1. Self-attention is gating. (Prop. 4.1, Sec. 4)\nC2. Gating cannot provide explanations in the way that attention is alleged to do.\nC3. Single input sequence models deploy only self-attention, so by C1-C2, attention cannot be used for explanation of these models; attention in two input sequence models is not limited to self-attention and is not equivalent to gating, so attention can provide explanations in these models.\nC4. Outputs are sensitive to alteration of attention weights for two-sequence but not one-sequence models. (Tables 2-3 vs. 1; Figs. 2-3) \nC5. Human judgments (manual evaluation) of the intuitive importance, for the output, of items with highest attention weights show that such intuitive importance is found for both one- and two-sequence models. (Fig. 5)\nRATING: Reject\nREASONS FOR RATING (SUMMARY). Because the modeling experiments closely follow previous work, the primary contribution rests on the account provided of why explanation-by-attention works only sometimes --- on the basis of the Proposition identifying attention with gating. But the reasoning and the math as presented are problematic. There is a worthwhile contribution from the human judgment experiment, but this is not sufficient to overcome the weakness with the main argument of the paper. (I also have reasons to question whether one- vs two-sequence inputs is the right distinction that needs to be accounted for.) \nREVIEW\nC1. If this point were made in plain English, attention weights and gating units both multiply activation values, I would say, yes, obviously. But the point is stated as a Proposition, with a Proof, so the math given should be correct. I dont see a way to make it correct, and that shouldnt be the job of the reader anyway. There are several errors in the dimensions of the matrices which make the equations incoherent. Eq. (1) contains W*h, a matrix product, where the dimensions stated are W in R^{d x d} and h in R{T x m}; these cannot be multiplied. This unclarity about matrix dimensions propagates into Prop. 4.1. In the definition of g(X), we have WX + b, where b is presumably a vector. Addition then requires that WX also be a vector, but X is stated to be in R^{n x d}, so WX cannot be a vector. Whether WX + b is actually a vector or a matrix, g(X) = c^T tanh(WX + b) is not a matrix: it is either a scalar or a vector. But this cant be. The definition of h uses the elementwise product, which requires that both arguments have the same dimensions, so g(X) must have the same dimensions as f(X). Were told f is the identity function, so f(X), like X, must be a matrix. Furthermore, the statement of Prop. 4.1 says that self-attention can be interpreted as *a* gating unit. By the standard definition of unit, this should mean that self-attention is a scalar.\nThroughout the paper, we are never told what kind of RNN is being assumed. If the RNN unit contains gates, as in an LSTM or a GRU, I can imagine that the intention is for Prop. 4.1 to say that (*) the effect of self-attention can be reproduced without attention by adjusting the weights in the gating mechanism already present in the RNN, so that attention doesnt increase the capacity of the model. But what I see in the paper does not convince me that (*) is true. (Because of the kind of global normalization required by the softmax, I actually suspect it is not.)\nC2. I dont see why (formally or intuitively) gating is not a legitimate candidate for explaining the behavior of networks containing gates; I would assume just the opposite, actually. How can it *not* be part of a satisfactory explanation? And why should changing a name from attention to gating have any bearing on whether it (whatever it is called) can potentially serve for explanation of network behavior?\nC3. Leaving the formalism aside, I dont see intuitively why, whatever an analysis of self-attention might entail about explanation, the same implication shouldnt apply to straightforward (not Rocktaeschel) attention when two sequences are present. Why cant we just treat the concatenation of the input sequences as a single input sequence, as standardly done for example for the Transformer? If the formal content of Prop. 4.1 were clear, perhaps this could be justified, but it is simply asserted without justification in the proof that the same reduction does not hold in the case of pair sequence.\nC4. Claims C1-C3 attempt to give an account for why various tests of the explanatory role of attention turn out positive for two-sequence but not one-sequence tasks, a pattern previously reported and verified with new results in the present paper. I fear however that one- vs two- is not the correct empirical generalization about attention that one should try to account for. Messing about with attention weights would not be expected to alter outputs if the output is determined by the input considered as a bag of words. And there is a troubling confound in the tasks compared: the one-sequence tasks are sentiment and topic classification, where a BOW model should do very well  and I suspect that is the real reason why these tasks dont show strong sensitivity to attention weight distribution. But the two-sequence tasks are NLI and QA, where (ideally) BOW models should not do nearly so well: paying attention to the right tokens should be important. The same is true of translation. So the confound in the tasks examined leaves it undetermined whether the crucial factor to account for is one- vs two-sequence or BOW-friendly vs BOW-unfriendly tasks.\nC5. Put together, the human judgments and the sensitivity-to-altering-attention-weights seem to indicate that attention tends always to be allocated to intuitively important tokens, and that matters for the output of the two-sequence models but not for the one-sequence models. This is what wed expect if attention is always being allocated appropriately, but for BOW-friendly tasks that doesnt make much difference.\n", "belong_id": "BJe-_CNKPH"}, {"uid": "r1xcYOqhKS", "paper_title": "Scalable Model Compression by Entropy Penalized Reparameterization", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a novel, information theoretic approach to learning compressed neural networks, whereby model parameters $\\theta$ are expressed in an (approximately discrete) latent space as $\\theta = f_{\\psi}(\\phi)$. During training, 'reparameterizations' $\\Phi$ are treated as draws from a generative distribution $q(\\phi)$. As per the source coding theorem, penalizing the entropy of $q$ (more accurately, of $\\hat{q}$) directly promotes compressible choices of $\\Phi$ and, therefore, of $\\Theta$. After training, arithmetic coding of $q$ is used to compress $\\Phi$. Empirical results on a suite of common benchmarks demonstrates the proposed algorithm gracefully trades off between model performance and size.\n\n\nFrequency Domain Interpretation:\n  Here, I am operating off of the assumptions that, in Sect 3.2 paragraph 3, $\\phi_{w}$ should be $\\psi_{w}$. If so, can 'Our Method (DFT)' also be interpreted as frequency domain SQ (of convolutional filters)? When trained on natural images, CNN filters are typically 'smooth' (see any visualization thereof) and this smoothness translates as a prior of sorts on $\\Phi_{w}$. This insight was previously explored in [1, 2] for purposes of compressing CNNs. Does evidence of this manifest in your experiments? For example, do the empirical distributions of high- and low-frequency components of $\\phi_{w}$ differ?\n\n\nFeedback:\n  The authors propose a general framework where in (seemingly) arbitrary decoders $f$ can be used for purposes of 'reparameterization'. It is therefore somewhat disappointing that, in practice, $f$ is restricted to affine transformations and, in some cases, even fixed. Memory issues notwithstanding, it is unclear how well complex decoders $f$ could be jointly learned with (surrogate) encodings $\\tilde{\\Phi}$ and density models $\\tilde{q}$.\n\n  The lack of comparison between the proposed method's test-time throughput and that of its baselines leaves open questions regarding potential real-world use cases. Much of the time, methods for compressing neural networks divide their attention between both memory footprint and prediction speed. Outright ignoring this aspect of the problem seems odd. As a reviewer, I would much rather see the authors take a proactive approach in addressing this issue.\n\n\nQuestions:\n  - What do results look like if convolutional decoders $f_{\\text{conv}}$ are jointly learned during training?\n  - How sensitive is the algorithm to different groups assignments? Can assignments be reliably made by simply looking at the architecture?\n\n\nMinor Comments:\n  - The general notation of the paper is cumbersome at times, can $\\phi$ to be changed to, e.g., $\\tilde{\\theta}$? Similarly, can notation such as $\\theta_{k, W}$ be simplified as $W^{k}$?\n\n  - Section 2 jumps around a bit. Consider trying to order things 'chronologically' such that training comes before compression? By the same token, details regarding 'Model Partitioning' could be move to the end of the section such that the preceding material just refers to $\\theta$ more abstractly?\n\n  - Some additional details regarding training (esp. discretization and model distributions $\\tilde{q}$) would be appreciated.\n\n\n[1] 'Compressing convolutional neural networks in the frequency domain', Chen et al. 2016\n[2] 'CNNpack: Packing convolutional neural networks in the frequency domain', Wang et al. 2016", "belong_id": "HkgxW0EYDS"}, {"uid": "ryeXVdBRKB", "paper_title": "Scalable Model Compression by Entropy Penalized Reparameterization", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a compression algorithm for neural networks. It uses a linear projection to map the weights and biases to a latent space where they are quantized using a learnt coding distribution. The method is simple yet it shows strong performance of a variety of compression benchmarks.\n\nOriginality: The core idea is related to (Balle et al. 2018), but the paper puts its own twist to it with the projections and applies it to model compression. It is certainly an interesting direction to explore.\n\nPresentation: The paper is well written. It is easy to read and understand. It properly cites prior works and contains all the technical details. I appreciate that the authors fit the paper into the recommended 8 pages.\n\nImpact: The method shows very strong performance in terms of compression ratio, but its unclear whether the compressed model can be used to speed up inference. Currently the main use case would be saving storage/bandwidth.\n\nQuestions:\n- In section 2.2, the paper talks about the form of the coding distribution. It has d components and l dimensions. How is d determined?\n- Section 2.1, How are the weights grouped for coding? is every filter its own group? If the experiments use different groups for different models, how is it decided which model uses which approach?\n- \\phi was fixed for the CIFAR10/ImageNet experiments. Could you provide further insight into why this choice was made?\n\nAssessment:\nWhile the idea is not groundbreaking, it is very well presented and evaluated and shows strong performance.", "belong_id": "HkgxW0EYDS"}, {"uid": "H1eIk0dc9B", "paper_title": "Scalable Model Compression by Entropy Penalized Reparameterization", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces an end-to-end method for the neural network compression, which is based on compressing reparametrized forms of model parameters (weights and biases). Shannon-style entropy coder is applied to the reparametrizations.\n\nReparameterized forms are tensors stored in the compressed format. During inference, they are uncompressed into integer tensors and transformed via parameter decoders into weights/biases of convolutional and dense layers.\n\nDuring training, model parameters are manually partitioned into groups, and within a group they are considered as samples from the same learned distribution. Similarly, parameter sharing is introduced among the corresponding parameter decoders.\nLoss function, which is minimized during training is a sum of rate loss (self-information of all reparametrizations) and cross-entropy classification loss under reparametrization. A trade-off between compressed model size and model accuracy can be explored by varying a constant before the rate loss.\n\nDuring optimization, several tricks are applied. Straight-through gradient estimator is used to optimize the loss function over discrete-valued reparametrizations by means of stochastic gradient descent. Relaxation is used to obtain good estimates for both the rate term and its gradient during training.\n\nThe proposed idea is well-founded and intuitive. The proposed method is extensively evaluated on different classification network architectures and datasets. It provides good compression while retaining a significant portion of accuracy.\n\nIn the experiments, It'd be interesting to see a comparison on more efficient networks like MobileNets, ShuffleNets on ImageNet dataset. Also, I wonder whether under the same compression rate the proposed method outperforms DeepCompression (Han et al., 2015) in terms of accuracy? (for example, for LeNets and VGG-16)\n", "belong_id": "HkgxW0EYDS"}, {"uid": "rygpYvGTKr", "paper_title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper proposes Qgraph, an algorithm that  addresses the problem of extrapolation error that appear in RL tasks with continuous action spaces. The authors describe a method to construct a graph from transitions generated by some policy. In this graph nodes correspond to states and (s, a, r, s, t) define transitions between these nodes. Then this representation is simplified and  used to compute Q-values using methods for tabular MDPs.\n\nThe related work section is missing several methods that attempt to address the same problem. Batch Constrained Q-learning (Fujimoto etal, 2018) introduces a formulation of Q-learning that constrains action selection with a generative model trained on a replay buffer in order to omit unseen actions. BEAR-QL (Kumar etal, 2019) describes a similar approach that uses a hard constraint based on MMD. It would be interesting to discuss connections with the recent work on off-policy batch RL.\n\nThe clarity of the paper can be improved. In particular, I have several questions regarding the method:\n1) How are node of the graph are constructed? Does one state correspond to a single node or several states are merged into a different node?\n2) How the actions are selected?\n3) What are the assumption regarding the initial state distribution? Does the set of initial states have to be finite?\n4) If two similar states appear in different branches of the graphs, are the corresponding nodes merged or not?\n5) If the considered environments are deterministic, what is the motivation for stochastic approximation of dynamic programming?\n\nThe approach has several major limitations. One of the main limitations of the approach is that it can be applied only to deterministic tasks. Although it is not stated clearly in the paper, it seems also requires to have a finite set of initial states. \n\nThe experimental evaluation is performed on a limited set of tasks and it is rather unclear whether the method can be scaled to higher dimensional control problems.\n\nOverall, I feel that the paper needs to be significantly improved. \n", "belong_id": "HygTUxHKwH"}, {"uid": "HJgus3ATFH", "paper_title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is trying to tackle the soft divergence issue in deep RL when algorithms combine function approximation, off-policy learning and bootstrapping, which is also called deadly triad by Sutton & Barto (2018). The paper proposes a way to represent the transitions in the replay memory as a data graph, then construct a simple MDP from it. Much more accurate Q values could be computed from the simple MDP and it provides a lower bound for the Q-values in the original problem. In this way, the method becomes less prone to soft divergence.\n\nThe idea of constructing a smaller MDP whose Q-values can be computed exactly by dynamic programming on tabular states, then use these Q-values to help dealing with the instability issues in deep RL is very interesting. In the rebuttal, I'd like the authors to address my major concern of the paper, where the proposed method seems to assume that the finite number of transitions could form a graph, which might not be always true. In typical continuous state spaces, the same state might not appear twice in the sampled transitions. In these cases, the graph becomes a number of disconnected chains and the Q-values from this MDP might not be accurate. Maybe I'm missing something, it's not very clear to me how the proposed method could be applied in the common case in deep RL where there's seldom a loop and the states are rarely visited twice. ", "belong_id": "HygTUxHKwH"}, {"uid": "HJg27xDRtH", "paper_title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper aims to build an understanding of deep RL. Because RL remains under-investigated from a theoretical point of view. Many algorithms use function approximation, off-policy learning and bootstrapping together--This is an unstable combination of techniques. In this paper, the authors propose a graph-perspective on the replay memory which allows to analyze the structure of deep RL.\n\nThe paper aims at an important issue in deep RL. The motivation of the paper is meaningful.\nThe paper gives a good summary of the previous related works in Section2.\n\nThe paper in the current form needs to be polished again. To obtain a better score, I suggest the authors to modify this paper in these ways: First, the introduction section needs to provide more details, including the pros and cons of previous related works on the research problem of this paper, the challenges you face when dealing with this issue and the contributions; Second, there were more than a few spelling and grammatical errors, please proofread the work and improve the writing; Third, the paper lacks logic in writing. The writing from Section.2 to Section.6 needs to be organized better. It is difficult for readers to grasp the key ideas of the paper through a quick assessment.\nThe paper focuses on the understanding of RL when deep Q-learning diverges, however, most of the conclusions in the paper are not based on the necessary theoretical proof, but the observations on the experiments.\n\nIt would be better if this paper can provide a clear illustration for the proposed method as well as the experiments section.", "belong_id": "HygTUxHKwH"}, {"uid": "HygAVxO3OH", "paper_title": "NAS evaluation is frustratingly hard", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this submission, the authors conduct a series of experiments on five image classification datasets to compare several existing NAS methods. Based on these experimental results, they point out: 1) how a network is trained (i.e., training protocols/tricks such as DropPath, Cutout) plays an important role for the final accuracy; 2) within the search space, the existing NAS methods perform close to or slightly better than a random sampling baseline; 3) hyperparameters of NAS methods also have significant effect on the performance. \n\nWith these interesting findings, I suggest rejecting this submission. The reasons are as follows:\n1) For the first finding of training protocol, several existing papers and books already discussed it, such as Li & Talwalkar (2019) and the book chapter {Neural Architecture Search} by Thomas, Jan Hendrik and Frank.\n\n2) For the second finding of the search space and the performance of a randomly sampled architecture, existing work from Facebook AI Research group has studied this. And the existing work gives more experiments and discussion than this submission (from my own perspective). https://arxiv.org/pdf/1904.01569.pdf\n\n3) The conducted experiments in this submission also have certain risks to support the claims/conclusions of it. For example, only datasets of image classification are adopted. Another factor is the hyper-parameter tuning (actually, the authors also mention this in the last paragraph on Page 4). All the compared methods, either NAS methods or random sample baseline, should receive the same training procedure to get a fair experimental comparison. \n\nThe above mentioned existing work makes the contributions of this submission less, and the experimental results may not be convincing enough. These lead to a reject.\n\nHowever, a great point is made by the authors in the last paragraph of Section 6: hyperparameter of NAS methods should be either stable enough or counted toward the cost.", "belong_id": "HygrdpVKvr"}, {"uid": "H1xfPBr3FH", "paper_title": "NAS evaluation is frustratingly hard", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThe paper scrutinizes commonly used evaluation strategies for neural architecture search.\nThe first contribution is to compare architectures found by 5 different search strategies from the literature against randomly sampled architectures from the same underlying search space.\nThe paper shows that across different image classification datasets, the most neural architecture search methods are not consistently finding solutions that achieve a better performance than random architectures.\nThe second major contribution of the paper is to show that the state-of-the-art performance achieved by the many neural architecture search methods can be largely attributed to advanced regularization tricks.\n\n\nIn general the paper is well written and easy to follow.\nThe paper sheds a rather grim light on the current state of neural architecture search, but I think it could raise awareness of common pitfalls and help to make future work more rigorous.\nWhile poorly designed search spaces is maybe a problem that many people in the community are aware of, this is, to the best of my knowledge, the first paper that systematically shows that for several commonly used search spaces there is not much to be optimized.\nBesides that, the paper shows that, maybe not surprisingly, the training protocol seems to be more important than the actual architecture search, which I also found rather worrisome.\nIt would be nice, if Figure 3 could include another bar that shows the performance of a manually designed architecture, for example a residual network, trained with the same regularization techniques.\n\nA caveat of the paper is that mostly local methods with weight-sharing are considered, instead of more global methods, such as evolutionary algorithms or Bayesian optimization, which also showed strong performance on neural architecture search problems in the past.\nFurthermore, the paper doesn't mention some recent work in neural architecture search that present more thoroughly designed search spaces, e.g Ying et al.\nIt would also be helpful if the paper could elaborate, on how better search spaces could be designed.\n\nNas-bench-101: Towards reproducible neural architecture search\nC Ying, A Klein, E Real, E Christiansen, K Murphy, F Hutter\nICML 2019\n\n\nMinor comments:\n\n- Section 4.1 How are the hyperparameters of drop path probability, cutout length and auxiliary tower weight chosen?\n\n\n\npost rebuttal\n------------------\n\nI thank the authors for taking the time to answer my questions and performing the additional experiments. The paper highlights two important issues in the current NAS literature: evaluating methods with different search spaces and non-standardised training protocols. I do think that the paper makes an important contribution which hopefully helps future work to over come these flaws.\n\n", "belong_id": "HygrdpVKvr"}, {"uid": "Hyezhj16YS", "paper_title": "NAS evaluation is frustratingly hard", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper provides a benchmark of 8 the-state-of-the-art NAS methods (DARTS, StacNAS, PDARTS, MANAS, CNAS, NSGANET, ENAS, NAO) on 5 datasets (CIFAR10, CIFAR100, SPORT8, MIT67, FLOWERS102). The paper first points out how fair comparisons of NAS methods is difficult, especially those with different search spaces. The paper proposes making relative comparisons to random 'samples' of architectures in search space to remove advantages of expertly engineered search spaces or training protocols. Also, the paper further investigates the case of commonly used the DARTS search space through ablation studies. The results suggest that many sensitive factors such as tricks in evaluation protocols, random seeds, hand-designed macro structures, and depth gaps can have predominant impacts rather than primary factors of NAS such as search strategy. The paper concludes with best practices to mitigate these disturbing factors to design NAS with reproducibility.\n\nThis is a very nice paper with extensive empirical evaluations. The topic of empirical comparisons of NAS algorithms is already very difficult to tackle in a fair way, but it gives thought-provoking strategies to evaluate the target NAS algorithms. Also, the fact that even random sampling (without search) provides an incredibly competitive baseline is quite informative and gives a very important recognition on how to design and evaluate NAS. \n\nThe paper is well written and well organized, and I have no problems to report, but would like to make sure one thing. In section 4.1, the same 8 initial random architectures from DARTS search space are used for all of the variants? Since the non-negligible impact of random seeds is reported, I just wonder how seeds are controlled in individual experiments.", "belong_id": "HygrdpVKvr"}, {"uid": "HJg-1E3aFH", "paper_title": "COMBINED FLEXIBLE ACTIVATION FUNCTIONS FOR DEEP NEURAL NETWORKS", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a family of parameterized composite activation functions, and a regularization technique for parameterized activation functions in general. While the three sets of experiments show potentially promising results, they aren't able to disambiguate clearly between the effect of the activation function you introduce and the effect of additional parameters in general, or between the effect of the regularization technique you introduce and the effect of regularization in general. I would really like to see the kind of carefully baselined, ablated, and hyperparameter-tuned results that would justify adding the techniques introduced to the toolbox of a typical deep learning practitioner.\n\nSome feedback:\n\nTypos:\n- The formatting is off a bit (shifted horizontally?)\n- In the abstract: RPeLu -> PReLU\n- p. 5: 'basement settings'->baseline, 'logarithmic sale'->scale\n- p. 6: basement->baseline again\n- p. 7 etc.: trail->trial\n\nFeedback:\n- Intro:\n  - Explain why maxout is similar to training piecewise activation functions?\n  - It's unclear what 'making the most of the non-linear properties by introducing adaptation policy on the input' means\n  - An 'autoencoder' is not an architecture as much as a broad family of architectures coupled with training approaches (I'm guessing you mean a fully-connected autoencoder)\n- Methodology:\n  - consider using 'f' instead of 'a' so it's easier to tell apart from alpha?\n- Experiments:\n  - I'm not sure I'm convinced by the statistical tests used on the LSTM results; they demonstrate that your approach, with a few specific sets of hyperparameter settings, does better than the baseline, but not that this represents a valid claim about your activation function's effect on this LSTM model in general.\n  - The autoencoder experiments are even less convincing, in that they represent two seemingly arbitrary network architectures, with equally arbitrary placement of the activation function in one of them.\n  - The regularization experiments use LeNet-5, which is not a compelling benchmark architecture with respect to contemporary practice. The effects of regularization techniques can be very different in different regimes of dataset and network size.\n- Code:\n  - I'm not sure why you performed backprop by hand for your activation functions and used torch.Function, rather than writing them directly as nn.Modules that make use of PyTorch autograd\n  - There are lots of details in the code that aren't in the paper; in general, papers should aim to be relatively self-contained (although I'm very glad to see your code, and it's pretty simple to follow)", "belong_id": "r1lh6C4FDr"}, {"uid": "r1xVKGHAYH", "paper_title": "COMBINED FLEXIBLE ACTIVATION FUNCTIONS FOR DEEP NEURAL NETWORKS", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors introduce a parameterized activation function to learn activation functions that have sigmoidal shapes that can be used in LSTMs. The authors apply their method to a dataset of forecasting stocks as well as to CIFAR-10. They also propose a method to regularize the activation function parameters.\n\nThe authors propose an activation function that helps very little in certain cases. I am not familar with this stock prediction dataset, but the differences shown are often less than 1%. I am familiar with CIFAR-10 and the architecture they use give bad baseline results and their method of regularizing a previous learned activation function (PReLU) gives marginally better results in half the cases.\n\nThere have been many learned activation functions proposed that give significantly better results than these. This paper is simply proposing another one and showing little to no improvement.\n\n**After reading author feedback**\nMy score stays the same.\n\nThere were some issues I had with the response:\n'For stock return forecasting, 1% improvement with statistical significance is sufficiently import...As we know, most newly proposed unbounded activation functions (flexible or not) cannot outperform ReLu by more than 0.5% in terms of test accuracy in a large range of image classification tasks.'\nThe 1% improvement was on error, not accuracy, so the two are not related. If referring to error rate, there are many that can make improvements greater than 10% (APLs, SReLUs, Swish, etc.)\nIf the authors would like to claim superiority to other activation functions, they should compare to them directly.\n\nIn addition, my concern about the poor CIFAR-10 baseline was not addressed. The results from the 2014 dropout paper gives significantly better results (Dropout:  A simple way to prevent neural networks from overfitting) than the authors' baseline.\n", "belong_id": "r1lh6C4FDr"}, {"uid": "SJgnV5C_sB", "paper_title": "COMBINED FLEXIBLE ACTIVATION FUNCTIONS FOR DEEP NEURAL NETWORKS", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed combined form of flexible activation functions with carefully designed principle of choosing activation functions. It shows some gains on stock price and one standard image task over the baseline.\n\nI'm leaning to reject or give borderline for this paper because (1) the paper don't have comparison with neural architecture search. For example, https://arxiv.org/abs/1710.05941 (Searching for Activation Functions). I don't know what the advantage of this approach compared to searched activation. I guess it's less computation heavy and maybe better motivated. But at least the author should give some pros/cons. (2) the paper has two benchmark, stock price prediction and CIFAR-10. I don't understand why as arch paper, it use such non standard benchmark (stock) and non standard arch (LeNet?). I don't think based these benchmark we can make solid conclusion. (3) These model seems introduce quite a bit more hyper-parameters. It's unclear if it is better than tuning other architecture e.g., batch norm/layer norm/dropout or even just optimizer. For example, 'flexible is 0.032' does this parameter generalize to other dataset? Like if the gain is really significant, like resnet over AlexNet, hyperparam doesn't matter. But if it's marginal win over a weak baseline, the how to get the results is important.\n\nSome comments:\nIn section 2, ' back propagation of these activation parameters by stochastic gradient descent can be\ndone as follows'\nWhy we need to list the detail backprop formulation here? Are these special? Isn't just autograd?\n\nCan the author explain more for principle 1? What is the 'same domain' means here?", "belong_id": "r1lh6C4FDr"}, {"uid": "HklZG5TYFB", "paper_title": "Adversarial training with perturbation generator networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces a new adversarial training approach, where a generator is used to generate the most challenging adversarial examples and the classifier is trained to correctly classify the generated adversarial examples. In this way, the robustness of the classifier is expected to be improved. For the generator, the input includes the original data sample, the gradient of the classifier, and the true label of the data sample. The paper is clearly presented and easy to follow. The experimental results relatively support the main claims of the paper.\n\nThe reasons that I go towards mild rejection for this paper are as follows:\n\n1. The general idea of improving the robustness of a classifier to by feeding adversarial examples to it is not a new idea. As shown in the paper, the idea has been adopted in Goodfellow et al. (2015) and Madry et al. (2018). In addition, [1] comprehensively studied how to ensemble different adversarial attacks as 'training data' of a classifier to improve its robustness. The paper generates adversarial examples with a generator that takes various things as inputs, which can be viewed as an extension to the method in [2]. [2] introduced a generator that only takes the data sample as inputs and the difference to the paper is that the one in this paper additionally takes gradients and label as inputs. With Goodfellow et al. (2015), Madry et al. (2018), and [1], it may not be too hard to apply the method in [2] to improve classifier robustness. Therefore, it is questionable whether the additional information used in the generator of this paper helps and how it helps. I did not see any comparison in the experiments.\n\n2. Although the experiments look to be well conducted, it is not comprehensive enough. For the defence methods in comparison, only two approaches that fall into the exact same line of the proposed method are included. In this line, I think it is necessary to compare with [1], which combines various attacks. Moreover, I would also expect a comparison with other closely related methods such as [3] and [4], to further demonstrate the effectiveness of the proposed one.\n\n3. It is also suggested to do ablation tests on the inputs of the generator to see how each part of the inputs helps. If gradients and true label are removed from the generator, it reduces to [2]. Therefore, those ablation tests also help the comparison with [2].\n\nMinor comments:\n\n1. It could be interesting to visualise the perturbations that are generated from the generator.\n\n2. Some of the figures and captions are too small to see in a printout.\n\n\n[1] Tramer, Florian, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. 'Ensemble Adversarial Training: Attacks and Defenses.' (2018).\n\n[2] Xiao, Chaowei, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. 'Generating adversarial examples with adversarial networks.' arXiv preprint arXiv:1801.02610 (2018).\n\n[3] Samangouei, Pouya, Maya Kabkab, and Rama Chellappa. 'Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models.' (2018).\n\n[4] Matyasko, Alexander, and Lap-Pui Chau. 'Improved network robustness with adversary critic.' In Advances in Neural Information Processing Systems, pp. 10578-10587. 2018.", "belong_id": "S1xXiREKDB"}, {"uid": "HkxX68w6FH", "paper_title": "Adversarial training with perturbation generator networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use the GAN framework for adversarial training. The proposed algorithm is mini-max loss plus L2-regularization on the perturbations generated by a generator network. Additionally, the paper shows a mathematical interpretation of the L2-term. Experiments on CIFAR-10 and CIFAR-100 shows that the algorithm achieved higher clean/adversarial accuracy in the settings.\n\nI vote for rejection. A major difference between the proposed method and adversarial training variants is whether we use neural networks or gradient-based algorithms to calculate the internal maximum in the mini-max formulation of the robust training. The focus of discussions in this paper should be why the former performs better. However, this paper does not provide enough justifications. Experiments are not convincing to support its advantage.\n\nMajor comments:\n(1) On page 2, this paper claims, 'The gradient descent based adversarial examples for robust optimization is not adaptive. Therefore, those neural networks are vulnerable to other types of adversarial attacks (Athalye et al., 2018).' Please expand these two sentences.\n(2) It is weird to observe that adversarial training with PGD performs worse than FGM. It is possible that the baselines are weaker than they should be. For example, if evaluations use L2-based adversarial training as baselines, it can be improper. Please refer to this comment on OpenReview: https://openreview.net/forum?id=Hk6kPgZA-&noteId=BJVnpJPXM .\n\n======= Update =======\n\nThank you for the replies and updates. After reading the comments and the updated draft, I am still not convinced that the proposed method should be superior to existing attacks. Even though I agree that the proposed method might outperform PGDs or other attacks in some settings, the paper needs to confirm it either theoretically or empirically. However, it was doubtful whether the baseline methods are strong enough. The concern remained after authors' responses. Hence I keep my score.", "belong_id": "S1xXiREKDB"}, {"uid": "H1xDmCCJ5S", "paper_title": "Adversarial training with perturbation generator networks", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes to construct adversarial attacks by training a neural network to produce a distortion, rather than by constructing the distortion directly via PGD. It then uses this during adversarial training to produce networks that it purports to be more robust.\n\nPros:\n-Faster speed of adversarial training could substantially democratize ability to work on adversarial ML\n-Neural net attack could overcome gradient masking in some situations\n\nCons:\n-Some of the main arguments seem incorrect, although the paper may still be strong if these incorrect arguments are removed and the experiments are further substantiated.\n\nDetails:\nI think there are some interesting ideas here, which if further substantiated could be the base of a strong paper. However, I found the overall argument in the current paper to be misleading or at least mistaken. A core claim is that the neural-net generated attack is better than PGD because PGD only generates a 'single attack' while the neural network generates many different attakcs. But PGD is an adaptive attack method that depends on the current model parameters, so it changes over the course of training just like the neural net attacker. Furthermore, both models approximate the same maximization over the L2 ball (in the neural net's case, rather than maximizing over the L2 ball we penalize the L2 norm, but a similar modification can easily be made to PGD). So it is not clear why we should expect the neural net to perform better, except perhaps that it is a different type of way of approximating the same objective that PGD approximates, and perhaps this approximation has some favorable properties? But this is never justified.\n\nHere is one attempt to justify why the neural net attack should be better: perhaps while PGD often works well, it gets stuck on certain examples, e.g. due to gradient masking issues. While the neural net might initially get stuck on those examples (it also has to work by taking gradients), since it can share knowledge across examples it initially learns a good direction for the 'stuck' examples by generalizing from the 'unstuck' examples. This overall allows it to generate a more consistently good attack even in the presence of (partial) gradient masking. If we believe this story, then actually the neural net attacker should already work well just as an attack (i.e. just train it to generate good attacks against a fixed model for some set of examples). I would find this quite interesting if true but unfortunately it wasn't explored in the paper. One way to do it would be to take models that are believed to be robust based on attacking them with PGD, and then show that the neural net attacker is substantially more accurate.\n\nI do find the point about faster speed interesting. This is given as a minor comment but it may be the biggest benefit of the current method. Adversarial training on large-scale datasets like ImageNet is computationally infeasible, and your method could potentially address this and allow people who don't have $100k+ compute budgets to actually work on adversarial ML.\n\nI also have some comments on the evaluation:\n\n-Reporting average accuracy across attacks in the evaluation is an inappropriate summary statistics. Min accuracy would be better, which in this case is zero for all methods. This suggests that you either did not adversarially train appropriately, or used too large of a norm (what norm did you use anyways? Or is this allowing the norm to be unbounded until you fool the model? In that case all the numbers should be zero for any reasonable attack.)\n\n-For evaluation I would rather see the full curve of accuracy vs. allowed attack norm, rather than just a single summary metric. It's too hard to tell what is going on from a single number. It is also hard to compare against adversarial training methods that penalize the L2 norm versus constrain the L2 norm, as the former might do better than the latter simply due to more closely conforming to your evaluation of average distortion. Having the full curve helps better assess this. Average distortion is also only meaningful if the method is finding the minimum-norm attack point that changes the label, which most of the methods you consider do not do. You do include the full curve in Figure 4 (please make the font bigger though). Under the full curve it seems the improvement over other methods is minimal, and could possibly be due to hyperparameter tuning.\n\nFinally, more minor but the Gaussian derivation does not make sense. It is unlikely that your norms actually follow a Gaussian distribution. A more appropriate claim would be that you are constraining or penalizing the expected norm of the perturbations. I would remove the math part as it does not add anything to the paper (and is also wrong as per preceding comment) and focus more on how you actually construct the generator network (this is only briefly discussed in the appendix and not at all in the main text, even though it is a key point to getting the method to work). You could also use the space for more detailed experiments, following best practices as in https://arxiv.org/abs/1902.06705 to ensure that your evaluation is sensible.", "belong_id": "S1xXiREKDB"}, {"uid": "r1xGNNd2FS", "paper_title": "Confidence Scores Make Instance-dependent Label-noise Learning Possible", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper focuses on instance-dependent label noise problem, which is a new and important area in learning with noisy labels. The authors propose confidence-scored instance-dependent noise (CSIDN) to overcome strong assumptions on noise models. They clearly define confidence scores and justify their availability. To solve CSIDN model, they propose instance-level forward correction with theoretical guarantees. Their experiments on both synthetic and real-world datasets show the advantage of this algorithm.\n\nPros:\n\n1. This paper is clearly written and well-structured in logic. For example, in Section 2, they introduce from class-conditional noise to instance-dependent noise first, which paves the way for confidence-scored instance-dependent noise. This make readers easy to follow the main contribution, namely the new noise model.\n\n2. This paper pushes the knowledge boundary of learning with noisy labels, since it focuses on more realistic and challenge topic 'instance-dependent label noise'. The authors leverage the idea of confidence scores, and propose confidence-scored instance-dependent noise (CSIDN). Compared to previous solutions, CSIDN is a tractable instance-dependent noise model, which enjoys several benefits, such as multi-class classification, rate-identifiability and unbound-noise.\n\n3. This paper proposes an algorithm to solve CSIDN inspired by forward correction called instance-level forward correction (ILFC). Their algorithm has been verified in both synthetic datasets and real-world datasets. The empirical results show the advantage of ILFC.\n\n(Minor) cons:\n\n1. Section 3 is a bit dense in understanding the estimation of transition matrix. The authors are encouraged to polish this section. \n\n2. Although ILFC outperform CT and LQ in real-world datasets, the authors need to add the reasults of MAE and FC to more thoroughly verify the performance of ILFC.", "belong_id": "SyevDaVYwr"}, {"uid": "Bye6SmyRYS", "paper_title": "Confidence Scores Make Instance-dependent Label-noise Learning Possible", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "--- Overall ---\n\nI think the problem of incorporating information about labeling uncertainty (when such information is available) is an interesting and important problem; however, I think this paper contains a crucial misunderstanding of the definition of calibration (as defined in Guo et al. 2017), is missing some important parts of the literature, and does not provide adequate convergence guarantees for the proposed method.\n\n--- Major comments ---\n\n1. In section 3.1, the authors substitute the confidence score r_x for the conditional probability P(Y=y|\\bar{Y}=y,X=x); however, even if the confidence scores are perfectly calibrated, these quantities are not necessarily equal (or even particularly close). As defined in Guo et al. (2017), a confidence score is well calibrated if the probability that a prediction is correct given the confidence score is equal to the confidence score. Using their notation: P(Y=\\hat{Y}|\\hat{P}=p) = p. Importantly, this is not conditioned on \\hat{Y} or X. One way to see that these two quantities are not equal is to observe that there are many possible confidence score functions that satisfy the above definition whereas the conditional probability defined above is a unique function. In particular, if the confidence score function is a constant equal to the accuracy of the model, then the confidence score is well calibrated, but clearly not equal to the conditional probability above.\n\n2. The other scenario considered by the authors is when there are multiple annotators; however, there is substantial literature on learning from multiple noisy annotations which the authors do not review. I suggest the authors start with 'Modeling annotator expertise: Learning when everybody knows a bit of something' by Yan et al. (2010)  (and the citations therein) which also considers the instance dependent noise case.\n\n3. Under what conditions does the proposed algorithm converge and to what does it converge to?\n\n4. The proposed method relies on several assumptions that are scattered throughout the description of the algorithm. I highly recommend making these assumptions clear near the beginning of the paper. Specifically, my understanding is that the main assumptions are:\n\ni. Confidence scores r_x are available for each instance and r_x \\approx P(Y=y|\\bar{Y}=y,X=x).\nii. Y _|_ X | Y\\neq\\bar{Y}, \\bar{Y} = y\niii. Anchor points are available on some portion of the data.\n\nBeyond the presentation, I find this to be a fairly strong set of assumptions, particularly the first assumption. \n\n--- Minor comments ---\n\n1. I really appreciated the synthetic example demonstrating the potential pitfalls of the small loss approach; however, I would spend a bit more time clearly explaining the small loss approach so that readers understand why it fails and how you are solving it's problems.\n\n2. Also in the synthetic example, the authors state that covariate shift leads poor accuracy, however, I think this point would be stronger if demonstrated instead of just asserted.", "belong_id": "SyevDaVYwr"}, {"uid": "BJxXPiqHqB", "paper_title": "Confidence Scores Make Instance-dependent Label-noise Learning Possible", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Learning with noise labels is a hot topic now due to the reason that deep learning algorithms often require large-scale supervised training samples and labelling a large amount of data is costly. However, almost all of the existing methods assume that the label noise is instance-independent. It either depends on the clean classes or is completely random. This paper studies the instance-dependent label noise, which is more realistic and applicable, but difficulty to address. The authors target to solve this problem. A feasible solution would contribute to the community a lot.\n\nThe challenging part for dealing with instance-dependent label noise is to learn the instance-dependent label flip function, which is hard to learn by only exploiting noisy data without any assumption. Few existing papers proposed assumptions to make the flip function learnable. This paper also introduces an assumption that the confidence score for each data is given. To me, this is also a strong assumption. Although the authors have provided examples of how to collect confidence score, collecting confidence scores may not be easy for many specific problems. \n\nGiven the confidence score, the authors proposed to learn the flip function. The clean class posterior can be inferred by using the noisy class posterior and the label flip function. So, how to learn the instance-dependent flip function is an essential step. Some technical contribution has been made to learn the flip function. In this section, the authors further assume that the off-diagonal entries of the flip matrix are independent of instance. After seeing the explanation, I personally agree that the assumption is reasonable for some cases. However, I found that in the experiments, the authors synthesized label noise where the off-diagonal entries depend on the instance. The proposed method also shows its advantages on this case. Can you explain this?\n\nThe experiment parts show the effectiveness of the proposed method both on synthetic data and real-world data. Overall, the paper is well-motivated and well-written. \n\nI have another concern that the obtained confidence score may not accurate. Is the proposed method sensitive to the confidence scores?\n", "belong_id": "SyevDaVYwr"}, {"uid": "rJxN9w_TYr", "paper_title": "Neural networks are a priori biased towards Boolean functions with low entropy", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the a-priori bias of a feed-forward neural network when the weights are initialised uniformly at random and independent of the network architecture. The paper claims that this initialisation leads to biases towards low entropy functions when the input and output are binary values. \n\nThe paper starts with a single layer perceptron without the bias term, and generalises the analysis to networks with multiple hidden layers and ReLU activations. The proposed approach seems rigorous, but I have a hard time to follow the paper as many of the important results are presented in appendix. In addition, the analysis is based on a feed-forward neural network with binary inputs and a single binary output, it is not clear whether these results can be generalised to architectures of practical importance such as convolutional/recurrent neural networks. Overall an interesting piece of work that contributes to the understanding of deep neural networks.\n\n\nMinor comments:\n\nSection 4.2: \n'as as predicted by Eq. 1' -> remove duplicated 'as'\n\nSection 5.2: \n'some interesting recent work' -> 'Some ...'\n'produce.At first sight' -> add a space before 'At'\n'If there is not bias' -> 'If there is no bias'", "belong_id": "Skgeip4FPr"}, {"uid": "HyxddGNAtH", "paper_title": "Neural networks are a priori biased towards Boolean functions with low entropy", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors study the behavior of simple neural networks at initializations. Particularly, the authors show that at initializations, neural networks tend to be functions with high class imbalance.\nFurther, the authors show that how such conclusion would be reached with or without a bias term, with different number of hidden layers, with change of activation functions. \nThe work is fairly interesting, yet the motivation is less clear.\nThe conclusion that study of such initializations can help understand the generalization power is not convincing.\nDespite that neural networks at initializations are biased towards low entropy functions, its not clear with training on a dataset with an optimizer, how much we can conclude about the generalization power.\nOverall the paper is well written.\n\nBelow are some more detailed comments:\n1) In the Introduction and the first paragraph of Section 2, the authors motivate by describing how important it is to understand the inductive biases. Yet the study is about the behavior of a random initialization. It would be nice to tie these two better; or motivate from another angle, other than the inductive bias. To my reading, the sentence what the inductive biases ... are, and how they arise is not well supported because I still dont follow what the inductive biases are from reading the paper except that a random initialization likely be low entropy functions.\n\n2) In Figure 3(a), 4(b,c,d), there is a spike at the mid-point of t. Though not as high as the extreme points, this is contradictory to the main conclusion. It would nice to add discussions.\n\n3) It would be nice to add experiments to study how such bias at initializations would impact the model training.\n", "belong_id": "Skgeip4FPr"}, {"uid": "BJevQEK7jr", "paper_title": "Neural networks are a priori biased towards Boolean functions with low entropy", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The topic of the paper is the inductive bias of neural networks. The authors study a simple model, namely a perceptron with no bias term viewed as a mapping from {0,1}^n->{0,1}. They show that initializing weights with a distribution that is symmetric under coordinate sign flips corresponds to an initialization in function space that is biased towards low-entropy functions. They also exhibit empirical evidence that by adding a bias term, or by using multiple layers, this tendency towards low entropy appears to increase. They also prove a bound on the minimal size of a network in order for it to represent all boolean functions. Finally, they prove a result that suggests that for ReLU networks with infinite widths the bias towards low-entropy function does indeed increase with depth. \n\nMy main concern regarding this paper is that the claim in the title and the statement of Theorem 4.1 seem to rely crucially on the fact that the functions are viewed with input as {0,1}^n. The origin of the 'simplicity', in the basic case that the authors address (perceptrons with no bias) appears to be a consequence that hyperplanes through the origin are quite likely to classify input points in {0,1}^n similarly. If one switches to a symmetric domain, for example {-1,1}, the effect in this setting completely disappears. The authors actually mention this in Section 5, noting that the expressivity of the perceptron is much lower for centered inputs. However, this to me suggests that Theorem 4.1 is not capturing any significant aspect of neural networks (in fact, the statement is a property of how linear hyperplanes to separate {0,1}^n, not neural networks). I may be mistaken, but I would like the authors to clarify this point.\n\nAnother concern is related to Theorem 5.5. This might be a more substantial result, but it is difficult to interpret and its implications are not discussed. Understanding the effect of depth on the 'simplicity bias' seems to me an important problem, but for some reason Theorem 5.5 (which deals with deep neural networks rather than linear perceptrons) is emphasized much less than Theorem 4.1. Why is this the case?\n\nThe paper is well-written but not always very clear. In particular, notation is not always defined and the authors use on notions from complexity theory that are not introduced (e.g., Lepel-Ziv complexity).\n\nOther comments: \n\n* Is the set F_t is defined as set of all functions with assigned \\mathcal T, but later this seems to be restricted to the functions expressible by a network/perceptron.\n* Definition 3.5: this defined the entropy H(f) of a function but then write H(p). It should probably be H(f) = -plog p - (1-p)log(1-p) where p = \\mathcal T(f), right?\n* Definition 3.6: some context or references for this definition could be useful.\n* Regarding the fact that functions in F_t are not uniform, shouldn't the distribution be the same for isotropic weight distributions? Assuming the distribution of w/|w| is uniform on the sphere, a more precise description of  P(f) seems possible\n* Section 4.3: what is the 'rank' in this setting? Isn't the parameter a vector w in R^n?\n* Some typos: Definition 3.1 w_l \\in R^{n_{l+1}}, ',.' in the beginning of Section 5, several in the last paragraph of Section 5.", "belong_id": "Skgeip4FPr"}, {"uid": "BkxkiR7o_r", "paper_title": "OPTIMAL BINARY QUANTIZATION FOR DEEP NEURAL NETWORKS", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "this paper looks at different quantization schemes (replace values of vector in \\R to, basically, plus or minus v, for well chosen v).\n\nDifferent schemes are considered, namely the low rank binary quantization (a matrix is approximated by the componentwise product of a low rank matrix and a +/-1 matrix) and k-bit binary  quantization.\nFinding the best quantization ends up in solving a program, which is convex and quite straightforward fo k=1 and 2, and unfortunately (apparently) non-convex for k>2. So the authors suggest a greedy approach for k>2.\n\nThe motivation of this work is DNN, arguing that quantized vectors should improve computations cost, hence some experiments are provide on DNN, yet they only illustrate the fact that quantization does not deteriorate too much the learning & test error.\n\nThe main criticisms is therefore the lack of concrete evidences that those schemes are actually helpful and, on the other hand, the relative simplicity (so the theoretical part of the paper are not sufficient by itself)", "belong_id": "S1gTwJSKvr"}, {"uid": "SklAmT2cKS", "paper_title": "OPTIMAL BINARY QUANTIZATION FOR DEEP NEURAL NETWORKS", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new family of quantized neural networks, where the weights are quantized based on fixed precisions. The authors proposed the optimal 1-bit/2-bit/ternary quantization schemes based on minimizing the L_2 loss. The authors proposed a greedy algorithm to approximate the optimal k-bit quantization.  The authors showed through extensive experiments on real DNNs that the proposed optimal quantization (Opt in the tables) can give better generalization as compared to several state-of-the-art alternative methods.\n\nThe reviewer appreciates that this quantized DNN-paper has a theory. This theory is based on minimizing the L_2 loss between the original data and quantized data. It unifies several quantization schemes into one framework. This is in contract with technical papers in the area which propose a single quantization method. The proposed quantization can further be implemented efficiently using XNOR operations and bit-counting.\n\nThe writing is of good quality. The length is a bit larger than the recommended length of 8 pages.\n\nThe reviewer has the following concerns,\n\n- This paper motivates from rank-k quantization but implemented as a scaled quantization. At the end of page 3, the authors showed that the scaled quantization can somehow approximate the rank-k quantization. However, the approximation is loose without any guarantee. Overall, I don't understand how the low-rank quantization in section 2 is related to the proposed method and fits in the overall picture. Ideally, in section 2, the authors can have some theoretical statements to state the optimality of the rank-k quantization. Then, the authors can say that, because of practical difficulties to implement the SVD, the use the scaled binary quantization instead. Or, the authors can simply remove section 2, and add a paragraph to introduce rank-k quantization, which is in contract to the scaled quantization they used in the paper.\n\nSomewhere in the text, the authors have to explain the optimality is with respect to the L_2 loss. There can be alternative quantization based on different losses.\n\nFigure 4, could you explain why the angle trends up as the layer index increases?\n\nAfter equation(4): mention the choice of p(x)\n\nIn conclusion, analyz -> analyze, introduc->introduce", "belong_id": "S1gTwJSKvr"}, {"uid": "SygD-H4pKH", "paper_title": "OPTIMAL BINARY QUANTIZATION FOR DEEP NEURAL NETWORKS", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proves seval binary quantization guarantee for the 1 or 2 bit cases, and shows some empirical error analysis. \n\nI am not an expert on neural network compression so I am not quite sure how the proposed method compares with the state-of-the-art algorithms. On the other hand, I checked several proofs provided by the authors for the 1-bit and 2-bit quantization cases. The proofs look good to me.\n\nSome minor comments:\n1. For the definition (9) can the authors make it clear that it is for all v_j s.t. v_1>= v_2>=.... instead of \\exits v_j?\n2. Can authors provide some explanation why in (8) we want to have v1>=v2>=vk in the constraint? I understand we need that in the proof, but is there any reason this is also the case in empirical evaluation? To me we may also have cases such that v1 < v2, is there any guarantee for those cases?\n3. Feels like the draft can be compressed into 8 pages, even if the work looks nice.\n", "belong_id": "S1gTwJSKvr"}, {"uid": "SkgXuMtDnB", "paper_title": "OPTIMAL BINARY QUANTIZATION FOR DEEP NEURAL NETWORKS", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to quantize the weights of neural networks that can minimize the L_2 loss between the quantized values and the full-precision ones. The authors propose solutions for optimal 1-bit/ternary/2-bit quantization, as well as a greedy algorithm to approximate the optimal k-bit quantization.  Experiments are performed on image classification data set ImageNet using ResNet-18.\n\nFirst of all, the authors should explicitly state in the paper that the optimality is in terms of what?  Indeed the authors obtain the solution of (1) quantization with a scaling parameter, and (2) in terms of minimizing quantization error (L_2 loss between the quantized value and the full-precision one), which is quite restricted to be a universally optimal one.\n\nSince the number of weights and activations are limited in the network, it is not appropriate to formulate quantization error the weights and activations in the format of continuous distribution in (4) and (8). \n\nIt is also not clear to me why this paper begins with rank-1 quantization but ends up with scaled quantization. What kind of assumption are used in this approximation, and can the optimality still be guaranteed?\n\nOne of my main concerns is the novelty of this paper. Many of the solutions in the paper have already been discovered in literature. For instance, the optimal 1-bit solution in (5) was already obtained in Binary-Weight-Network [1] in 2016. The optimal ternary solution (i.e., the ternarization threshold should be 1/2 of the scaling parameter) in (12)  was also already obtained in Corollary 3.1 in  [2] in 2018, as a special case when curvature information is dropped.\n\nYet another concern is about the experiments. Since the proposed optimal binarization has the same solution as BWN, where does the performance gain in Table 2 come from? Moreover, some of the recent quantization methods are not compared. For instance, in PACT [3], 2-bit weight&acitvation quantization already achieves 64.4% top-1 accuracy of Resnet18 on ImageNet, while the proposed method achieves the same accuracy with full-precision activation and 2-bit weight (Table 2). In addition, according to Table 2, the proposed method also can not beat LQ-Net. \n\n\n[1] Rastegari, Mohammad, et al. 'Xnor-net: Imagenet classification using binary convolutional neural networks.' ECCV, 2016.\n[2] Hou, Lu, and James T. Kwok. 'Loss-aware weight quantization of deep networks.' ICLR 2018.\n[3] Choi, Jungwook, et al. 'Pact: Parameterized clipping activation for quantized neural networks.' arXiv preprint arXiv:1805.06085 (2018).", "belong_id": "S1gTwJSKvr"}, {"uid": "BJxsoyDqYB", "paper_title": "Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1. Summary: This paper proposes Capacitron, a conditional variational latent variable model for TTS which allow for controllable latent variable capacity. They optimize the Lagrangian dual of the ELBO and restrict the capacity of the rate-term through a learnable, non-negative multiplier. They demonstrate the effectiveness of their approach on a range of TTS tasks such as same-text prosody transfer and inter-text style transfer, and provide extensive analyses on their latent variable capacity (in addition to comparisons to non-variational approaches based on Tacotron).\n\n2. Decision: Weak accept. I recommend this paper for acceptance due to its strong empirical results and clear presentation of the approach/unification of existing methods.\n\n3. Supporting arguments: The extension of Capacitron to existing methods such as [Hsu et al., 2019 and Zhang et al., 2019] is simple (basically adopting the beta-VAE approach), as the conditional generative model in both the vanilla and hierarchical forms exist already. But the authors do a thorough job evaluating the strengths and weaknesses of their method through ablation studies on latent variable capacity and comparing to existing baselines in their experiments. The results are also convincing, as demonstrated through human listening tests and the samples provided in the supplement. \n\n4. Feedback:\n- The authors mention in Section 3.1 that in previous work, the variational posterior has the form q(z|x). While this is true for [Zhang et. al, 2019], I believe that [Hsu et. al, 2019] also uses the conditional generative model as described in Figure 3 -- it would be helpful to provide a more clear distinction between the two works in the exposition. From what I understood, this works contribution is not so much the introduction of the conditional generative model but identifying the effects of controlling the rate term in the ELBO decomposition.\n- In Section 3.2, there is a lot of notation and several terms that make it difficult to parse Eq. 14 at first glance. For example, (1) R_L is never explicitly written out, (2) and is R == R_avg? It would be helpful to clean up this section so that eyeballing Eq. 14 would be easier for the reader (e.g. having all the relevant terms organized).\n\n5. Questions:\n- In the Single speaker section of Section 4.3, you mentioned that the model has to divide the latent space into regions that correspond to different utterance lengths. Im curious -- is this something that was observed empirically?", "belong_id": "SJgBQaVKwH"}, {"uid": "HJgtYejpYr", "paper_title": "Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper introduces a new embedding method in expressive TTS by focusing on the capacity of hidden variables introduced in variational autoencoder. Such a method is supported by the KL divergence and lower bound theory and the paper well formulates/describes the capacity concept. In the experiments, the proposed method is compared with other conventional methods with well-designed subjective and objective metrics and shows the effectiveness in terms of the style transfer. The paper is well written in general.\n\nOne of the difficulties in this paper is reproducibility. The paper does not seem to provide source code and part of the data used in this paper also does not seem to be public data like libriTTS (I would be wrong but I could read that the multispeaker training data are not public), although I appreciate the authors' efforts to provide the implementation and evaluation details as much as possible in the appendix.\n\nAnother discussion of this paper is that it is not explicit to provide the meaning of the latent variables (this is a general issue in VAE) and I could not be fully convinced by the discussion and analysis based on the latent variable. There would be several semi-supervised studies to explicitly connect some (elements) of latent variables with actual attributes and I'd like to ask the authors to consider such direction to make the latent variable discussion in the paper more plausible.\n", "belong_id": "SJgBQaVKwH"}, {"uid": "Hkg7mpxQ9S", "paper_title": "Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work authors present a regularized, variational autoencoder method for speech synthesis. To endow the latent space with more capacity, the authors employ a modified variational autoencoder objective, which uses a learnable Lagrange multiplier to impose a capacity limit on KL divergence between latent posterior and prior. The authors furthermore propose to decompose the latent embedding space into a two-level hierarchical representation to give generative process more control over style transfer and sample-to-sample variance. They extend earlier theoretical results providing upper bounds on the mutual information between data and its latent embedding to their hierarchical latent representation. In numerical experiments the authors evaluate their approach on a number of speech synthesis tasks involving same-text prosody transfer, inter-text style transfer, inter-speaker prosody transfer. They also analyze speech samples generated from latent samples drawn from the prior.\n\nThe paper is well-written and easy to follow, but the significance of the main contribution of the paper remains unclear to me. The authors propose to use an augmented standard VAE loss with a capacity hyperparameter and a Lagrange multiplier, but such modifications have been used before and it is not clear to me where is the novelty in there?\n\nMoreover, the authors treat the Lagrange multiplier as a learnable parameter, which brings up the question how does it effect the learning dynamics. For instance if the KL-divergence reaches its capacity, the Lagrange multiplier may be pushed down to zero, which in turn can allow the posterior to diverge unchecked to possibly a point mass distribution? The authors provide no details on how beta (the Lagrange multiplier) evolves in their experience and how it effects the stochastic nature of the model. \n\nI am not sure why the authors call non-stochastic latent variable models heuristic (instead of deterministic) methods?\n\nIn Figure 1, how are  the loss values computed for variational methods? Can we see how the error bars look for different C values and embedding dimensions?\n\nCan the authors be more clear about why for the tasks they consider, a standard (deep) VAE architecture with non-hierarchical latents does not sufficiently capture variations in the data? Can the authors quantify the differences? \n\nI am unfamiliar with prior work in this application area, but maybe the work is novel with respect to the application of the regularized VAE framework to speech synthesis. However, the application alone is in my opinion not a contribution that is significant enough for publication.\n", "belong_id": "SJgBQaVKwH"}, {"uid": "H1x72FypKB", "paper_title": "Global Concavity and Optimization in a Class of Dynamic Discrete Choice Models", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers reinforcement learning for discrete choice models with unobserved heterogeneity, which is useful for analyzing dynamic Economic behavior.  Random choice-specific shocks in reward is accommodated, which are only observed by the agent but not recorded in the data. Existing optimization approaches rely on finding a functional fixed point, which is computationally expensive.  The main contribution of the paper lies in formulating discrete choice models into an MDP, and showing that the value function is concave with respect to the policy (represented by conditional choice probability).  So policy gradient algorithm can provably converge to the global optimal.  Conditions on the parameters for global concavity are identified and rates of convergences are established.  Finally, significant advantages in computation were demonstrated on the data from Rust (1987), compared with nested fixed point algorithms that is commonly used in Econometrics.\n\nThis paper is well written.  The most important and novel result is the concavity of the value function with respect to the policy.  My major concerns are:\n\n1. How restrictive the assumptions are in Definition 1.4?  In particular, R_min is defined from Assumption 2.2 as the immediate reward ... is bounded between [R_min, R_max].  So if we set R_min to negative infinity, the right-hand side of Eq 6 will be infinity, and so the condition is always met.  Is this really true?  At least, does the experiment satisfy Definition 1.4?\n\n2. The experiment is on a relatively small problem.  Solving value/policy iteration with 2571 states and 2 actions is really not so hard, and many efficient algorithms exist other than value/policy iteration.  For example, a variant of policy iteration where policy evaluation is not solved exactly, but instead approximated by applying a small number of Bellman iterations. Or directly optimize the Bellman residual by, e.g., LBFGS, which also guarantees global optimality and is often very fast.  See http://www.leemon.com/papers/1995b.pdf .  An empirical comparison is necessary.", "belong_id": "H1efEp4Yvr"}, {"uid": "ryg-vTdpKB", "paper_title": "Global Concavity and Optimization in a Class of Dynamic Discrete Choice Models", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper is consider dynamic discrete choice models. It shows in an important class of discrete choice models the value function is globally concave in the policy, implying that for example policy gradients are globally convergent and are likely to converge faster in practice compared to fix-point approaches. \n\nThe paper is very well written and structured. It present convergence results together with a sanity check implementation. However, as an informed outsider, I am also a little bit confused. As far as I understand, Rust (1987) is also using gradient descent. Indeed the problem might not be convex/concave and hence this might get trapped in local minima. Moreover, Ermon et al. (AAAI 2015) have already shown that Dynamic Discrete Choice models are equivalent to Maximum Entropy IRL models under some conditions. Then they provide an algorithm that is kind of close (at least in spirit) to policy gradient. The propose to 'simultaneously update the current parameter estimate  (Learning) while we iterate over time steps t to fill columns of the DP table (Planning)'. Indeed, this is still not giving guarantees, but the together with Ho et al. (ICML 2016) it suggests that the take-away message 'use police gradients' for dynamic discrete choice models is actually known in the literature. This should bee clarified. Generally, the paper is providing a lot of focus on the economics literature. While this is of course fine, the authors should clarify what is already known in the AI and ML literature (including the work described above). \n\nNevertheless, the proof that there are convergent policy gradients for some dynamic discrete choice models appears interesting, at least to an informed outsider. However, this results heavily hinges on e.g. (Pirotta 2015). So the main novelty seems to be in Sections 4.3 and 4.4. Here is where they make use of their assumption. So, the only point, in my opinion, that should be clarified is the usefulness of the considered class. For an informed outsider, this is not easy to see. ", "belong_id": "H1efEp4Yvr"}, {"uid": "r1lqaMWaqB", "paper_title": "Global Concavity and Optimization in a Class of Dynamic Discrete Choice Models", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper deals with a certain class of models, known as discrete choice models. These models are popular in econometrics, and aim at modelling the complex behavioural patterns of individuals or firms. Entities in these models are typically modelled as rational agents, that behave optimally for reaching their goal of maximizing a certain objective function such as maximizing expected cumulative discounted payoff over a fixed period.\n\nThis class of models, modelled as a MDP with choice-specific heterogeneity, is challenging as not all the payoffs received by the agents is externally observable. One solution in this case is finding a balance condition and a functional fixed point to find the optimal policy (closely related to value function iteration), and this is apparently the key idea behind nested fixed point methods used in Econometrics.\n\nThe paper proposes an alternative. First it identifies a subclass of discrete choice models (essentially MDP with stochastic rewards) where the value function is globally concave in the policy. The consequence of this observation is that a direct method, such as the policy gradient that circumvents explicitly estimating the value function, can (at least in principle) converge to the optimal policy without calculating a fixed point. The authors illustrate computational advantages of this direct approach. Moreover, the generality of the policy gradient method enables the relaxation of extra assumptions regarding the behaviour of agents while facilitating a wider applicability/econometric analysis. \n\nThe key contribution claimed by the paper is the observation that in the class of dynamic discrete choice models with unobserved heterogeneity, the value function is globally concave in the policy. This enables using computationally efficient policy gradient algorithms with convergence guarantees for this class of problems. The authors also claim that the simplicity of policy gradient makes it also a viable model for understanding economic behaviour in econometric analysis, and more broadly for social sciences.\n\nThe paper deals with Discrete choice models with unobserved heterogeneity as a special class of MDPs and is relevant to ICLR. However, the writing style is quite technical and terse -- while I could appreciate the rigour, the authors develop the basic material until page 5 -- Notation is also somewhat non-standard at places (alternating using delta or sigma for a policy and pi for thresholds of an exponential softmax distribution) and makes it harder to see the additional structure from generic MDPs more familiar in RL. I suspect that a reader more familiar with the relevant econometric, marketing and finance literature could follow the model description more easily. \n\nThere are a number of assumptions in the paper, especially 2.4 and 2.5 that relate to the monotonicity and ordering of the states. These assumptions seem to be important in subsequent developments for showing the concavity but they seem to be coming from out of the blue. Unfortunately the authors do not provide any intuition/discussion -- an example problem with these properties would make these assumptions more concrete. I was hoping to find such an example in the empirical application however this section does not make the necessary connections with the theoretical development. There are not even references to basic claims \ndone in the abstract, for example, I am not able to find an illustration of significant computational advantages in using a simple implementation policy gradient algorithm over existing nested fixed point algorithms used in Econometrics. The lack of any conclusions makes it also hard for me to appreciate the contributions.\n\n\nMinor:\n\nAbstract:  \n.. Existing work in Econometrics assumes that optimizing agents are fully rational and requires finding a functional fixed point to find the optimal policy. ...\n\nAmbiguous sentence: Existing work in Econometrics [...] requires finding a functional fixed point to find the optimal policy.\n\nTheorem 3.1 and elsewhere  Freechet =>  Frechet\n", "belong_id": "H1efEp4Yvr"}, {"uid": "rJgl-c_P_B", "paper_title": "Contrastive Learning of Structured World Models", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The construction and learning of structured world models is an interesting area of research that could in principle enable better generalisation and interpretability for predictive models. The authors overcome the problem of using pixel-based losses (a common issue being reconstruction of small but potentially important objects) by using a contrastive latent space. The model otherwise makes use of a fixed number of object slots and a GNN transition model, similarly to prior approaches. The authors back up their method with nice results on 3D cubes and 3-body physics domains, and reasonable initial results on two Atari games, with ablations on the different components showing their contributions, so I would give this paper an accept.\n\nThe comparisons to existing literature and related areas is very extensive, with interesting pointers to potential future work - particularly on the transition model and graph embeddings. As expected, the object-factorized action space appears to work well for generalisation, and could be extended/adapted, but setting a fixed number of objects K is a clearly fundamentally limiting hyperparameter, and so showing how the model performs under misspecification of this hyperparameter is useful to know for settings where this is known (2D shapes, 3D blocks, 3-body physics). The fact that K=1 is the best for Pong but K=5 is the best for Space Invaders raises at least two questions: can scaling K > 5 further improve performance on Space Invaders, and is it possible to make the model more robust to a greater-than-needed number of object slots? On a similar note, the data collection procedure for the Atari games seems to indicate that the model is quite sensitive to domains where actions rarely have an impact on the transition dynamics, or the interaction is more complex (e.g. other agents exist in the world) - coming up with a synthetic dataset where the importance of this can be quantified would again aid understanding of the authors' proposed method.", "belong_id": "H1gax6VtDB"}, {"uid": "H1l81UwCYS", "paper_title": "Contrastive Learning of Structured World Models", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper tackles the problem of learning an encoder and transition model of an environment, such that the representation learnt uses an object-centric representation which could favor compositionality and generalisation. This is trained using a contrastive max-margin loss, instead of a generative loss as previously explored. They do not consider RL or follow-up tasks leveraging these representations and transition models yet.\nThey perform an extensive assessment of their model, with many ablations, on 2 gridworld environments, one physical domain, and on Atari.\n\nThe paper is very well motivated, easy to follow, and most of its assumptions and decisions are sensible and well supported. They also provide interesting assessments and insights into the evaluation scheme of such transition models, which would be of interest to many practitioners of this field.\n\nApart from some issues presented below, I feel that this work is of good quality and would recommend it for acceptance.\n\n1.\tThe model is introduced in a very clear way, and most decisions seem particularly fair. I found the presentation of the contrastive loss with margin to be clear, and the GraphNet is also well supported (although see question below). However, two choices are surprising to me and would deserve some clarification and more space in the main text, instead of the Appendix:\n\ta.\tWhy does the object extractor only output a scalar mask? This was not extremely clear from reading the main text (and confused me when I first saw Figure 1 and 3a), but as explained in the Appendix, the CNN is forced to output a sigmoid logit between [0, 1] per object channel.\n\tThis seems overly constraining to me, as this restricts the network to only output 1 bit of information per object.\n\tHowever, maybe being able to represent other factors of these objects might be necessary to make better predictions? \n\tThis also requires the user to select the number of output channels precisely, or the model might fail. This is visible in the Atari results, where the objectness is much less clear.\n\tDid you try allowing the encoder to output more features per objects? \n\tObviously this would be more complicated and would place you closer to a setting similar to MONet (Burgess et al. 2019) or IODINE (Greff et al. 2019), but this might help a lot.\n\tb.\tIt was hard to find the dimensionality D of the abstract representation $z_t$. It is only reported in the Appendix, and is set to $D=2$ for the 2D gridworld tasks and $D=4$ for Atari and the physics environments. These are quite small, and the fact that they exactly coincide with your assumed sufficient statistics is a bit unfortunate. \n\tWhat happens if D is larger? Could you find the optimal D by some means?\n2.\tThe GraphNet makes sense to me, but I wondered why you did not provide $a_t^j$ to $e_t^{(i, j)}$ as well? I could imagine situations where one would need the action to know if an interaction between two slots is required.\n3.\tSimilarly, the fact that the action was directly partitioned per object (except in Atari where it was replicated), seemed slightly odd. Would it still work if it was not directly pre-aligned for the network? I.e. provide $a_t$ as conditioning for the global() module of the GraphNet, and let the network learn which nodes/edges it actually affects.\n4.\tIn your multi-object contrastive loss, how is the mapping between slot k in $z_t$ and $\\tilde{z}_t$ performed? Do you assume that a given object (say the red cube) is placed in the same $k$ slot across different scenes/timesteps?This may actually be harder to enforce by the network than expected (e.g. with MONet, there is no such slot stability, see [1] for a discussion).\n5.\tIt was unclear to me if the grid shown in Figure 3 (b) and 5 is real? I.e. are you exactly plotting your $z_t$ embeddings, and they happen to lie precisely along this grid? If yes, I feel this is a slightly stronger result as you currently present, given this means that the latent space has mirrored the transition dynamics in a rather impressive fashion.\n6.\tRelated to that point, I found Figure 3 b) to be slightly too hard to understand and parse. The mapping of the colours of the arrows is not provided, and the correspondence between what 3D object is actually moving where and which of the coloured circles correspond to which other cubes in the image is hard to do (especially given the arbitrary rotation). Could you add arrows/annotations to make this clearer? Alternatively, presenting this as a sequence might help: e.g. show the sequence of real 3D images, along with the trajectory it traces on the 2D grid.\n7.\tFigure 4 a) was also hard to interpret. Seeing these learnt filters did not tell much, and I felt that you were trying too hard to impose meaning on these, or at least it wasnt clear to me what to take of them directly. I would have left this in the Appendix. Figure 4 b) on the other hand was great, and I would put more emphasis on it.\n8.\tThere are no details on how the actual test data used to generate Table 1 was created, and what unseen environment instances would correspond to. It would be good to add this to the Appendix, and point forward to it at the end of the first paragraph of Section 4.6, as if you are claiming that combinatorial generalization is being tested this should be made explicit. I found Table 1 to be great, complete, and easy to parse.\n9.\tIt would be quite interesting to discuss how your work relates to [1], as the principles and goals are quite similar. On a similar note, if you wanted to extend your 2D shape environment from a gridworld to a continuous one with more factors of variations, their Spriteworld environment [2] might be a good candidate.\n\n\nReferences:\n[1] Nicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P. Burgess, Alexander Lerchner, COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration, 2019, https://arxiv.org/abs/1905.09275\n[2] Nicholas Watters, Loic Matthey, Sebastian Borgeaud, Rishabh Kabra, Alexander Lerchner, Spriteworld: A Flexible, Configurable Reinforcement Learning Environment, https://github.com/deepmind/spriteworld/ \n\n", "belong_id": "H1gax6VtDB"}, {"uid": "S1gKxxeZ5B", "paper_title": "Contrastive Learning of Structured World Models", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper aims to learn a structured latent space for images, which is made up of objects and their relations. The method works by (1) extracting object masks via a CNN, (2) turning those masks into feature vectors via an MLP, (3) estimating an action-conditioned delta for each feature via a GNN. Learning happens with contrastive losses, which ask that each feature+delta is close to the true next feature, and far away from other random possibilities. Experiments in simple synthetic environments (e.g., 2D geometric shapes moving on a black background) show encouraging results. \n\nThis paper has a simple, well-motivated method. It is clearly written, and easy to understand. The evaluation is straightforward also: the paper merely shows that this model's nearest neighbors in featurespace are better than the nearest neighbors of World Model (2018) and PAIG (2019). Also, some visualizations indicate that for these simple directional manipulations (up/down/left/right motion), PCA compressions of the model's states have a clean lattice-like structure.\n\nIt is impressive that the model discovers and segments objects so accurately. Perhaps this could actually be evaluated. However, I do not understand why results are so sensitive to the number of object slots (K). This seems like a severe limitation of the model, since in general we have no idea what value to set for this. \n\nAlthough I like the paper, I am not sure that there is sufficient evidence for the method being something useful. Yes, H@1 and MRR are high, but as the paper itself implies, the real goal is to improve performance (or, e.g., sample efficiency) in some downstream task. Given how simple these domains are, and the fact that data is collected with purely random exploration, it is difficult to imagine that there is any significant difference between the training set and the test set. For example, if you make 1000 episodes of 10 steps each in Space Invaders, you practically get 1000 copies of the same 10 frames. I worry that all the evaluation has shown so far is that this model can efficiently represent the state transitions that it has observed.\n\nThe authors note that it was beneficial to only use the hinge on the negative energy term. This seems unusual, since a hinge on the positive term allows some slack, which intuitively makes the objective better-formulated. Can the authors please clarify this result, at least empirically? \n", "belong_id": "H1gax6VtDB"}, {"uid": "BJg0mfZ2tr", "paper_title": "Oblique Decision Trees from Derivatives of ReLU Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this paper, the authors proposed an approach to fit locally constant functions using deep neural networks (DNNs).\nThe idea is based on the fact that DNN consisting of only linear transformations and ReLU activations is piecewise linear.\nThus, the derivative of such a network with respect to the input is locally constant.\n\nIn the paper, the authors focused on connecting the locally constant network with oblique decision trees.\nSpecifically, they proved that these two models are in some sense equivalent, and one can transform one model to another.\nThis connection enables us to train the oblique decision trees by training the locally constant network instead.\nBecause the locally constant network can be trained using the gradient-based methods, it would be much easier to train than the oblique decision trees.\n\nI think the paper is well-written and the idea is clear.\nConnecting the locally constant network with oblique decision trees looks interesting.\n\nI have one concern, however.\nThe authors mention that the training of oblique decision trees is difficult, and the use of the locally constant network is helpful.\nIf I understand correctly, oblique decision tree is one specific instance of the hierarchical mixtures of experts.\nAnd, [Ref1] pointed out that the hierarchical mixtures of experts can be trained using EM algorithm, which is another type of the gradient-based training.\nThe current paper misses such a prior study.\nI am interested in to see if the use of locally constant network is truly effective for training oblique decision trees over the algorithms considered in the literatures of hierarchical mixtures of experts.\n\n[Ref1] Hierarchical mixtures of experts and the EM algorithm\n\n\n### Updated after author response ###\nThe authors have successfully demonstrated that the proposed approach is better than the EM-like classical approaches. I therefore keep my score.", "belong_id": "Bke8UR4FPB"}, {"uid": "rkgFDc0HcB", "paper_title": "Oblique Decision Trees from Derivatives of ReLU Networks", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes locally constant network (LCN), which is implemented via the gradient of piece-wise linear networks such as ReLU networks. The authors built the equivalence between LCN and decision trees, and also demonstrated that LCN with M neurons has the same representation capability as decision trees with 2^M leaf nodes. The experiments conducted in the paper disclose that training LCN outperforms other methods using decision trees. \n\nThe detailed comments are as follows:\n\n1) The idea of LCN is very interesting, and the equivalence to decision trees is also very valuable, as it provides interpretability and shines light on new training algorithms. \n\n2) The derivation of LCN and the equivalence is clear. The analysis based on the shared parameterization in Section 3.5 is helpful to understand why LCN with M neurons could be of equal capability to decision trees with 2^M leaf nodes. \n\n3) One weakness is that the performance of ELCN seems to be very close to RF, as shown in Table 2. \n\nI am not sure whether some similar ideas to LCN have been explored in the literature. But the topic studied in this work is very valuable, which connects deep neural networks and decision trees.\n", "belong_id": "Bke8UR4FPB"}, {"uid": "HJlLZg0dqH", "paper_title": "Oblique Decision Trees from Derivatives of ReLU Networks", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "*Summary*\nThis paper leverages the piecewise linearity of predictions in ReLU neural networks to encode and learn piecewise constant predictors akin to oblique decision trees (trees with splits made on linear combinations of features instead of axis-aligned splits). The core observation is that the Jacobian of a ReLU network is piecewise constant w.r.t to the input. This Jacobian is chosen to encode the hard splits of a decision tree. The paper establishes an exact equivalence between decision trees and a slightly modified form of the locally constant networks (LCN). The LCN used for experiments is slightly relaxed to allow for training, including 'annealing' from a the softplus nonlinearity to ReLU during training, adding one or more output layers to perform the final prediction, and training with connection dropout. Experiments show LCN models outperform existing methods for oblique decision trees, but ensembles are often matched or outperformed by random forests.\n\n*Rating*\nPerhaps the greatest attribute of decision trees is utter simplicity. (The second best attribute the out-of-the-box competitive accuracy of tree ensembles on a wide variety of problems.) An argument to be made for this paper is that it leverages the machinery of learning DNNs to learn more powerful, oblique tree-like models. The counterpoint is that despite the added complication, it's still often beaten by ensembles of CART trees. Overall, the idea is clever, the presentation could be improved slightly, and the experiments raise existential questions for this kind of work. My current rating is weak reject.\n\n(1) It's difficult to know how LCNs should be compared to traditional decision trees, with accuracy, number of parameters, prediction speed, and training time/parallelism as viable components. The paper focuses almost exclusively on accuracy, while cross-validating over model sizes and other hyperparameters. This is a reasonable choice, though a discussion of model size and prediction speed would be welcome. I do have two significant questions about the experiments:\n\n(2) It seems unfair that LCN has access to one or more hidden layers between the splits and the final output, denoted g_\\phi. Would competing decision tree models improve with such a layer learned and appended to the final tree? Would LCN suffer from using a tabular representation like the others?\n\n(3) Despite the assertion that these are datasets that necessitate tree-like predictors, the LLN method outperforms LCN and the trees on 4/5 datasets and is competitive with ensemble methods. While not explicitly stated, am I correct that LLN is essentially a traditional ReLU-network? If high accuracy is the goal, then why should I go to the trouble of training LCN when a traditional DNN is better. And if a tree is needed, then LCNs should be evaluated on more than just accuracy.\n\n(4) LCNs seem to present a less bulky alternative to e.g. Deep Neural Decision Trees (https://arxiv.org/abs/1806.06988), but that work should be cited and discussed\n\n(5) The proof sketch in Section 3.6 of the equivalence between the 'standard architecture' and decision trees is difficult to understand and not convincing. (On second reading I noticed the subtle vector '\\mathbf 0' indicating that all entries of '\\grad_x a^i_1' are zero. Some further exposition and enumeration of steps would clear up confusion.)\n\n(6) Overall the presentation is reasonable, other than the notes below. I did find myself searching back over the (dense) notation section and following sections looking for definitions of variables and terms used later. Consider better formatting (e.g. more definitions in standalone equations), strategic pruning of some material to make it less dense, and repeating some definitions in line (e.g. see below for 'p7:... remind the reader').\n\n*Notes*\n(Spelling typos throughout; most are noted below)\np3: clarify in 3.1/3.3 that L is the number of outputs\np4: 'interpred'\np5: 'aother'\np5: Theorem 2 proof: note that the T/T' notation is capturing left/right splits\np5: 'netwoworks'\np5: 'Remark 5 is important for learning shallow...': should 'shallow' be 'narrow' instead?\np7: in the first paragraph, remind the reader of the definitions of o^M and J_x a^M\np7: 'Here we provide a sketch of [the] proof'\np7: 'unconstraint' should be 'unconstrained'\np7: '...can construct a [sufficiently expressive] network g_\\theta'\np7: 'simlify'\np9: Table 2: instead of '2nd row', ..., use '1st section', ...; also consider noting which methods are introduced in this paper\np9: Figure 2: text is too small\n", "belong_id": "Bke8UR4FPB"}, {"uid": "HyeYxYAs_H", "paper_title": "Decoupling Hierarchical Recurrent Neural Networks With Locally Computable Losses", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe paper introduces a hierarchical RNN architecture that could be trained more (memory) efficiently. The difference in the architecture seems to be an auxiliary loss that decodes k step inputs and some perturbation of TBPTT.\n\nComments on the paper\n\n1. The paper seems to be have been written in a rush. The language could be improved, the format is not always consistent and in general the paper could be much better written. There are quite some typos as well in the paper, for example , Trinh et al.  is not a proper citation.\n\n2. The authors mentioned that TBPTT is not memory efficient, this is not very clear to me, as it only needs to keep the number of truncation steps that it backprops through and hence much more memory efficient compared to full BPTT.\n\n3. It is not clear to me what is the benefit of gr-HMRNN. It is not clear why cutting of the gradients from the higher level to the lower level would help.\n\n4. It is surprising to me that HMRNN could only solve the copy task upto a length of 108. \n\n5. I would also suggest another copy task from Hochreiter, Sepp and Schmidhuber, Jurgen. Long short-term memory. Neural computation, 9(8): 17351780, 1997.\n\nIn general, the paper seems to have been written in a rush. I would recommend the papers to be revised.", "belong_id": "S1lNWertDr"}, {"uid": "SJxfjMU9YB", "paper_title": "Decoupling Hierarchical Recurrent Neural Networks With Locally Computable Losses", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The proposed work investigates the problem of learning hierarchy in RNNs. Authors note that different layers of the hierarchy are trained in 'sync'. The proposed paper suggests to decouple the different layers of hierarchy using auxiliary losses.  The form of auxiliary losses used in the paper are of the form of local losses, where there is a decoder, which is used to decode past inputs to each level from the hidden state that is sent up the hierarchy, therebyforcing this hidden state to contain all relevant information. \n\nClarity of the paper: The paper is clearly written.\n\nMethod: The proposed method  ignores the gradients from higher to lower levelsin the backward pass,  (because of this, the authors can also save some memory). In order to compensate for the lost gradients, authors propose to use local losses, and we introduce an auxiliary loss term to force this hidden state to contain all information aboutthe last k inputs. The authors note that the hidden state from the lower level (to the higher level) should contain the summary of the past, and hence use a decoder network (which is simply parameterized) as a feedforward network which is used to decoder a 'past' hidden state. \n\nRelated Work Section: The related work section is nicely written. The authors have covered mostly everything. These 3 papers may still be relevant. (a), (b), (c).   (b) could be relevant for mitigating the parameter update lock problem as mentioned by authors in the introduction of the paper. (c) is also relevant as authors in (c) also consider using auxiliary losses for learning long term dependencies. \n(a) SkipRNN: https://arxiv.org/abs/1708.06834(b) Sparse Attentive Backtracking: http://papers.nips.cc/paper/7991-sparse-attentive-backtracking-temporal-credit-assignment-through-reminding\n(c)  Learning long term dependencies in RNNs using auxiliary losses https://arxiv.org/abs/1803.00144\nExperiment Section: In order to validate the proposed method, authors evaluate it on copying task, pixel MNIST classification, permutedpixel MNIST classification, and character-level language modeling. \na) Copying results show that the decoder network are essential to achieve decent results. This task though does not show the strength of the proposed method though as baseline also solves the problem completely. It might be interesting to actually scale the 'gap' time in copying time step to something larger like T = 1000 or something.\nb) PIXEL MNIST classification: Authors use the pixel by pixel classification task to test the proposed method. Here, the proposed method performs comparable to the hierarchical RNN (but without using too much memory). \nc) Character level modelling: Authors demonstrate the performance of the proposed method on language modelling task (PTB). These results are particularly not interesting, as the performance gain is very marginal. Also, may be using other language modelling datasets like Wikitest103 or Text8 might be more useful.  As for the results, even unregularized LSTM performs better than the baseline in this paper. (For reference, see https://arxiv.org/abs/1606.01305) \n\nWhat authors can do to improve paper:\n- The problem considered in the proposed paper is very interesting to me. Though, the results are not (yet) convincing. It might be interesting to think about a task, where there are really long term dependencies like reading CIFAR10 digit pixel by pixel and then doing classification, where the authors can actually show the promise of the proposed method. \n- It might also be interesting to know how are the original training cost objective is weighed against the auxiliary loss. Have authors tried any search over what kind of auxiliary loss performs well ? ", "belong_id": "S1lNWertDr"}, {"uid": "BJge242NjH", "paper_title": "Decoupling Hierarchical Recurrent Neural Networks With Locally Computable Losses", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Claim: Backpropagation of gradients from a higher to lower level in a HRNN can be removed and replaced with auxiliary losses predicting input tokens at the lower level without affecting performance. \n\nSignificance: The significance of the claim hinges on whether HRNNs are more effective than other methods designed to help RNNs capture long-term dependencies (e.g. stacking RNNs or using different architectures). I think the authors could make a more substantive argument why this would be the case in the introduction, but they do a nice job of situating their work in the context of the present literature.\n\nNovelty: The proposed method is not very original, since augmenting RNNs with auxiliary losses in order to better capture long-term dependencies has been used in many previous papers. The authors mention some of these papers in the related work section.\n\nClarity: The paper's description of the proposed method is well-written. Some parts of the experiment section could be made clearer. \n--  I encourage the authors to invent a new acronym to refer to 'our model' (perhaps aux-HRNN?). In the description of the mr-HRNN (pg. 5), I find the sentence 'trained using only as much memory as our model requires for training' confusing.  I initially thought our model referred to the mr-HRNN in the setence.\n-- Training settings (e.g. the number of ticks of the upper RNN) should be described at the beginning of each section. \n-- A seeming contradiction is made when discussing the results in 4.3. First, it said that because short term dependencies dominate long term dependencies it is expected that the proposed method will suffer greatly (pg. 6, bottom). In the next paragraph, it is claimed that all three models perform similarly due to the same reason. Which is it?\n\nSupporting evidence: The claim is empirical and the supporting evidence is experimental. As such, I find the comprehensiveness of the experiments wanting. There are several ways the experiments could be improved. \n-- Results for each \\beta value should be included, to see how placing increasing significance on the auxiliary loss impacts the results.\n-- Include all relevant details necessary to reproduce the results, such as the length of training or stopping criterion used. \n-- Additional results when varying the number of ticks.\n-- More results with deeper hierarchies, since the ability to capture salient information at different levels of coarseness is a key selling point of HRNNs. \n-- Results on larger scale tasks besides character level language modelling on Penn TreeBank.\n\nOther comments:\n-- In the intro, I think some mention of parallel architectures such as transformers or convolutional architectures is warranted here, since parallelizability of training is a significant reason why these architectures are becoming preferred over RNNs.\n-- Citations are mishandled throughout the paper. Citations should be enclosed in parentheses unless used as a subject in the sentence (e.g. 'Sordoni et al. make the case that...'). There is no need to refer to a citation twice in a sentence, like you do in 'More recently, Koutnik et al. introduced the Clockwork RNN Koutnik et al. (2014)...'\n-- I don't understand why the permuted accuracy of the gr-HRNN is so much higher than the non-permuted accuracy. One possible explanation is that the important pixels ended up at the end in each of the three trials, hence the gr-HRNN did not have to remember much information from the past. This should be addressed in the paper. \n-- I would welcome some theoretical analysis as to why replacing the gradient path with this particular auxiliary loss does not impact results. I also think some discussion of what this means HRNNs are actually doing might be nice as well.", "belong_id": "S1lNWertDr"}, {"uid": "Skgbh4Untr", "paper_title": "Guiding Program Synthesis by Learning to Generate Examples", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "= Summary\nA method for a refinement loop for program synthesizers operating on input/ouput specifications is presented. The core idea is to generate several candidate solutions, execute them on several inputs, and then use a learned component to judge which of the resulting input/output pairs are most likely to be correct. This avoids having to judge the correctness of the generated programs and instead focuses on the easier task of judging the correctness of outputs. An implementation of the idea in a tool for synthesizing programs generating UIs is evaluated, showing impressive improvements over the baseline.\n\n= Strong/Weak Points\n+ The idea is surprisingly simple and applies to an important problem in program synthesis.\n+ The experiments show that the method works very well in UI-generation domain\n- The paper repeatedly claims general applicability to program synthesizers, but is only evaluated in the specific domain of UI-generating programs. I have substantial doubts that the approach would work as well in the domains of, e.g., string manipulation, Karel, or data structure transformations. My doubts are based on the fact that there are easily generalizable rules for UIs (no overlaps, symmetry, ...), whereas other domains are less easily described. This creates a substantial gap between paper claims and empirical results.\n- The writting is somewhat sloppy (see below), which makes it sometimes hard to understand. Names such as 'views' are used without explanation, and it's not explained how a device is an input to a program (yes, I get what this means, but it makes in unnecessarily hard to follow the paper)\n\n= Recommendation\nI would ask the authors to rewrite their paper to make less general claims, but believe that the general idea of judging the correctness of a program (or policy) by evaluating it on different inputs is a powerful concept that would be of substantial value to the wider ICLR audience. Improving the readability of the paper would make me improve my rating to a full accept.\n\n= Minor Comments\n* page 1, par 'Generalization challenge': The second sentence here is 4 lines long and very hard to follow. Please rephrase.\n* page 2, par 2: 'no large real-word datasets exists' -> exist\n* page 2, par 3: 'even when both optimizations of InferUI are disabled': at this point, the reader doesn't know about any optimizations of InferUI.\n* page 4, par 1: 'i.e., $\\exists p \\in \\mathcal{L}$' - $\\mathcal{L}$ is undefined here (will be defined later on the page)\n* page 4, par 2: 'Generate a candidate program $p_1 \\models \\mathcal{I}$' - in step 2, there are suddenly also $p_2 \\ldots p_n$, which are never explicitly generated. Either adapt this step, or explicitly generate them in step 2 based on the distinguishing input\n* page 7 par 2: 'We use one screen dimension as the input specification $\\mathcal{I}$, the second as the distinguishing input' - this confused me, as the paper discussed discovering the distinguishing input (page 4, paragraph 'Finding a distinguishing input'), whereas it sounds here like that input is manually selected.", "belong_id": "BJl07ySKvS"}, {"uid": "SJlNo_lk5B", "paper_title": "Guiding Program Synthesis by Learning to Generate Examples", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "[Summary]\nThis paper aims to improve the generalization of program synthesis, ensuring that the synthesized programs not only work on observed input/output (I/O) examples but also generalize well to assessment examples (i.e. model the real intent of the end-user). To this end, the paper proposes a framework that iteratively alternates between producing programs with an existing program synthesizer and augmenting examples to disambiguate possible programs with a neural oracle that learns to select correct outputs. Several architectures and the design of input of the neural oracle have been investigated. The experiments on Andriod layout program synthesis with an InferUI synthesizer show that the proposed framework can improve the generalization of synthesized programs. However, I find it is difficult to evaluate the effectiveness without sufficient qualitative results and the intermediate outputs (e.g. a distinguishing input and candidate outputs) of the proposed framework (see details below).\n\nSignificance: are the results significant? 4/5\nNovelty: are the problems or approaches novel? 4/5\nEvaluation: are claims well-supported by theoretical analysis or experimental results? 3/5\nClarity: is the paper well-organized and clearly written? 4/5\n\n[Strengths]\n\n*motivation*\n- The motivation for improving the generalization of program synthesis by augmenting examples is convincing.\n\n*novelty*\n- The idea of utilizing a neural network to select correct outputs to augment examples for disambiguating the possible programs is intuitive and convincing. This paper presents an effective way to implement this idea.\n\n*technical contribution*\n- The paper investigates a set of network architectures and ways to specify the network input for learning the neural oracle. The RNN+CNN model that leverages both rendered views and features seems effective.\n\n*clarity*\n- The overall writing is clear. The authors utilize figures well to illustrate the ideas. Figure 1 clearly shows the proposed framework.\n\n*experimental results*\n- The presentations of the results are clear. The results demonstrate that the proposed framework can improve generalization accuracy.\n\n*reproducibility*\n- Given the clear description in the main paper and the details provided in the appendix, I believe reproducing the results is possible if the dataset is available. \n\n[Weaknesses]\n\n*related work*\nThe descriptions of the related work are not comprehensive. Some neural program synthesis works explore a variety of mechanisms to encode examples and fuse their features, which are not mentioned in the paper. [Devlin et al. in ICML 2017] investigates different attention mechanisms to sequentially encode a set of I/O examples and performs pooling to merge them. [Sun et al. in ICML 2018] proposes a doubly encoding method to capture more details of examples and merge the features using a relation network. I believe it would be interesting to see if these methods could further improve the performance of the neural oracle.\n\n*experiment setup*\n- The experiments are not sufficient. While the claims look promising, the proposed method is only evaluated in only one dataset, which is not sufficiently convincing. I suggest the authors to also experiment the FlashFillTest dataset where string transformation programs are synthesized. \n- A more comprehensive description of the dataset is lacking. \n\n*experiment results*\n- I find it hard to judge the effectiveness of the proposed framework without seeing sufficient qualitative results. I suggest the authors randomly sample some synthesized programs (both success and failure) and present them in the paper.\n- I believe it is important to present some examples of the given I/O pairs, initially synthesized programs (p_1), found distinguishing input (x*), candidate outputs (y), the prediction of the neural oracle (i.e. selected outputs), the augmented examples (I \\cup {(x*, y*)}), and finally the next synthesized program. Without this, it is very difficult to understand the performance of the proposed framework and what could go wrong. \n\n*ablation study: the neural oracle*\nOnly the final performance (i.e. the program synthesis performance) is shown in the paper. I believe it would be helpful if the performance of the neural oracle was also presented. As the whole framework depends on how accurate the neural oracle can select the correct output, it is important to evaluate this. One way to show this is to simply show the performance of all the neural oracles (with different architectures) trained on D_S (the positive samples and the incorrect samples) or even D_{S+}.\n\nDevlin et al. 'RobustFill: Neural Program Learning under Noisy I/O' in ICML 2017\nSun et al. 'Neural Program Synthesis from Diverse Demonstration Videos' in ICML 2018", "belong_id": "BJl07ySKvS"}, {"uid": "SJegExwycB", "paper_title": "Guiding Program Synthesis by Learning to Generate Examples", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper handles the challenge of generating generalizable programs from input-output specifications when the size of the specification can be quite limited and therefore ambiguous. When proposed candidate programs lead to divergent outputs on a new input, the paper proposes to use a learned neural oracle that can evaluate which of the outputs are most likely. The paper applies their technique to the task of synthesizing Android UI layout code from labels of components and their positions.\n\nThe experiments compare the method against the baseline InferUI. To summarize the results, we can see that the proposed method in the paper can perform about as well as existing hand-crafted constraints that guide the search process of the previous work, when training an oracle on the dataset with negative examples created by noising the positive examples.\n\nOne limitation of the method is that it would works best when there is a clear latent structure behind the outputs produced by the correct program, such as in the paper's target domain of generating UIs where there are clear aesthetic rules and design guidelines that make it possible to evaluate which output is most preferred. For other domains, it may be more important to evaluate the candidate program together with its output, which would make it similar to a re-ranking approach.\n\nI believe this paper presents a novel and insightful approach to creating programs from imprecise specifications. Therefore, I vote to accept the paper.\n\nSome questions for the authors:\n- How big was $\\mathcal{I}_i$ in the supervised dataset $\\mathcal{D}_{S+}$? Was it always 3?\n- I wasn't able to find any evaluation of a model trained on $\\mathcal{D}_U$, did I miss it in the paper?", "belong_id": "BJl07ySKvS"}, {"uid": "SyeYAMQIKS", "paper_title": "Learning Boolean Circuits with Neural Networks", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nSummary: \nThe paper proposes a layer-wise method for training the weights of a binary-tree-structured neural network such that it correctly reproduces certain classes of Boolean functions defined by binary-tree-structured Boolean circuits. Specifically, this paper shows analytically that if a circuit satisfies a property termed local correlation where there is sufficient correlation between every gate in the circuit and the true output label of the circuit, then this circuit can be learned by a neural network with the same structure as the circuit by training it one layer at a time from the input to the output. The paper motivates this by showing empirically that the k-parity problem with some bias to the labels can be learned by a neural network, but that this does not work when there is no bias in the labels, implying that this bias is necessary for successful learning. The paper shows formally that instances of the k-parity problem satisfy the local correlation assumption and can thus be learned, and also shows that there exists at least one distribution given by a simple generative model that satisfies this assumption and is thus also learnable in this manner. \n\nOverall:\nWeak reject. My main concerns are as follows.\n\nFirst, if you define a circuit such that each gates output is correlated with the actual label, and then simply copy that structure and learn each gating function independently to predict the value that it is correlated with, then it seems likely that each additional gate should improve the representation. And because the network mimics exactly the circuit structure, the capacity of the network will not be a problem either. Thus, while its not trivial to say that the function can be recovered exactly, it is not exactly surprising either, and I dont see what this offers in terms of novel intuition.\n\nSecond, the definitions and assumptions are not very reflective of what is actually done in deep learning, and I do not see a clear connection that would make this result useful in the field. It doesnt connect well enough to actual datasets, problems, models, or learning algorithms that are used, so I do not see what insights can be taken from it.\n\nHowever, I do not have a problem with the quality of the paper, and think it would find a more appropriate audience at a different venue.\n\nClarity: The paper is quite well written and intelligible, although the notation is fairly dense and not always intuitive (e.g., numbering layers of the network from output to input).\nSignificance: I do not think the results are particularly significant.\n\nDetailed comments:\nSection 1.\n- The claims made in this section come off overly strong for the remainder of the paper. For example, training a deep neural network is not necessarily computationally hard because training is almost always taken to mean using SGD to locally minimize some loss function, and not taken to mean global optimization. This should be made more clear. Further, in my opinion, the holy grail of theoretical deep learning right now is the work on understanding why these networks generalize, not on global optimization or exact function learning (although there is some overlap between these two goals).\n- Can you better explain why correlation between the input bits and the label being necessary is not obviously necessary for all standard correlation-based learning methods? I agree that it would be nice if our methods were not so limited but this seems to simply reiterate the fact that they are. (e.g., I would not expect a deep network to learn on ImageNet if there was no correlation between some pixels and the output).\n- The ImageNet example is not particularly clear. First, the appendix implies you are taking center crops from each image, not random crops as stated in the intro. Second, I assume that you take a center crop per image, but the text is written to sound like only a single crop for all of ImageNet is used, which doesnt make sense. \n- I do not think that the ImageNet experiment implies that local correlation exists in ImageNet any more than is already well known. It simply shows that an even smaller center crop from ImageNet images is still mildly predictive of the output classes; however, since ImageNet images are generally object-centered already, this is not particularly surprising.\n\nSection 2.\n- This is a well-written section that does a good job of situating this work.\n\nSection 3.\n- The assumption that the circuit must be a binary tree is quite restrictive and excludes pretty much all common deep learning architectures. Could this be relaxed empirically, even if not analytically?\n- In neural networks, it is common to define layer 1 as the input and layer d as the output, and it wasnt immediately clear from the notation here that the opposite was being done here. Please make this more clear early on (or flip the ordering).\n- In the neural-gate definition, should v_i be v_l? Otherwise what is i indexing? Further, since v is not actually learned later, can it just be removed? This would simplify the notation somewhat.\n- How are v and w initialized?\n\nSection 4.\n- While it is strong to assume that every gates output should correlate with the label, its natural to think that many of each layers output in an actual neural network will correlate with the output, since earlier layers can be thought of as an input corresponding to a transformed representation of the actual input. This is also implied by the Belilovsky et al. layer-wise training work. Making these points more clearly and providing some empirical evidence could strengthen this papers claims. Perhaps measure the correlation between outputs in different layers and the labels?\n- Assumption 2 is at odds with the fact that datasets are typically designed to be as balanced as possible. Can you reconcile these?\n\nAppendix.\n- It would be nice if this read as well as the main paper.\n", "belong_id": "BkxtNaEYDr"}, {"uid": "B1g4Qu4ctB", "paper_title": "Learning Boolean Circuits with Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper aims to study the correlation between the neural network's input and output by abstracting the network as a binary tree Boolean circuit problem. The paper is well-written, motivations are clearly presented, and literature reviews are well placed. The contributions are mainly theoretical, and the experimental plots are simply used for concept illustrations, therefore the correctness of the theoretical analysis has no empirical evaluations. \n\nDue to the rareness of the study regarding deep learning theory, this manuscript makes one step further towards understanding the training hardness on a few certain types of neural networks with different underlying distributions. Multiple strong assumptions have made to favor the analysis, however, due to the lack of expertise, I can understand parts of the intuitions behinds them. To mimic the Boolean circuits network, the authors have focused the analysis on a layerwise gradient-based training, which might be a potential drawback because modern deep models are much more complex (e.g., the Residual network architecture shares connections between layers) and this over-simplified analysis may be too restricted. My understanding is that this paper could become a very solid work if a few rules of thumb or intuitions can be proposed to guide finding the correlation property in realistic neural network architectures such that we can verify these proposed theories with some experiments. \n\nOverall, I believe this is a fine theoretical foundation paper that should attract the attention of the researchers in deep learning community.", "belong_id": "BkxtNaEYDr"}, {"uid": "rJlPEXk3Yr", "paper_title": "Learning Boolean Circuits with Neural Networks", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Thank the authors for their rebuttal. It resolves all my previous concerns.\n##############################\n\nThis paper proposes to use neural networks for learning binary tree structured boolean circuits. For the boolean circuits problem, the authors notice two importance factors influencing why the target circuit is easy to learn or not. The two factors include 'local correlation' and 'label bias'. On the one hand, 'local correlation' requires every influential node in the circuit have strong correlations with the target label, which makes the network trainable to exploit this correlation for minimizing losses. On the other hand,  'label bias' requires that there are not the same amount of positive and negative examples. The paper proves that the proposed algorithm can faithfully learn the target circuit  and presents multiple examples for the scenario of learning binary tree structured boolean circuits.\n\nStrengths,\n1, This paper points out the two key factors 'local correlation' and 'label bias' for the learnability of a boolean circuit. Empirically in Figure1, they also validates the finding by demonstrating problems with balanced labels are more difficult to train.\n2, The paper puts their theoretical findings to the setup of k-parity problem, proving that their proposed algorithm can faithfully tackle the k-parity problem.\n\nWeakness,\n1, The main theorem proves that the target networks can be approximated using O(n^logn) examples. However, it it apparent that binary tree boolean circuits cannot represent all 2^n n-ary boolean functions. Actually, I GUESS all functions a binary tree can represent MIGHT also be in the scale of O(n^logn). Then the result is not surprising, as the training set probably covers all training examples. I think it is necessary that the authors give some estimates on the total number of representable functions and compare it with their |S|.\n2, It is not clear to me why the the 'label bias' is important for learning a boolean circuit. Could the authors clarify to me ? \n3, It is also helpful to empirically compare the cases when there are local correlations and there aren't. \n4, Sec 5.1 shows that the proposed algorithm can solve the k-parity problem, however they assumed that all parity nodes locates together. In practice, these parity nodes might scatter everywhere, which could incur big problems for a binary-tree  to approximate.", "belong_id": "BkxtNaEYDr"}, {"uid": "ryeNngAwYr", "paper_title": "Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Paper Summary\n\nThis paper examines reinforcement learning in the context of blood glucose control to help individuals with type 1 diabetes. The authors show that their methods lead to strong algorithms that can improve artificial pancreas systems. Their results are promising, and, very importantly, do not require meal announcements. The importance of their application is self evident.\n\nDecision\n\nShould their claim to novelty hold up, then the authors have provided evidence that RL can be useful for this important application of glucose control. Overall, the paper is very well written with clear arguments, and the impact of their study for type 1 diabetes is high. However, there are novelty concerns with the proposed methods. The paper needs some work before acceptance.\n\nAdditional Feedback\n\nOne caveat is that a small search of previous RL methods in blood glucose control did yield some similarly titled papers (please discuss 'Reinforcement Learning Algorithm for Blood Glucose Control in Diabetic Patients' by Javad et al), and they were not addressed or compared in this paper. To push this review over the edge, the authors should address these papers in the related work, and discuss how this paper's method compares.\n\nAdditionally, the novelty of the actual RL methods is not entirely clear. The authors should very clearly point out their contributions within the methods sections, differentiating between past methods and the proposed one. Most importantly, the authors should write a paragraph-length section at the end of the introduction detailing their proposed methods, with a bullet-point layout of every novel detail. This will help future readers get the gist of the paper more accurately.\n\nLastly, though the authors addressed the limitations of their dataset in terms of it being a simulation, they should also discuss the sample size being only 10 patients in different age groups. It would be helpful for readers to know how the method will generalize to new patients.\n", "belong_id": "ryeN5aEYDH"}, {"uid": "SkeuvSR2FH", "paper_title": "Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper describes an RL based approach to administer insulin for blood glucose control among  type-1 diabetic patients.  The paper formulates this blood glucose control problem as a closed-loop reinforcement learning problem and demonstrates its effectiveness on data generated from an FDA-approved simulator of glucoregulatory system. Compared to existing approaches, the proposed method can operate without meal announcement by potentially making use of latent meal intake patterns. The authors also demonstrate how a learned policy for one particular subject can be used as initialization to train/fine-tuned the policy of another subject so as to combat the issue of high sample complexity.\n\nOverall, the reviewer finds that the authors provide a reasonable approach to model the blood glucose management problem for type-1 diabetes. Each component of the paper is well-explained and the paper is easy to follow. The algorithmic design adopted in this paper, such as the representation of the problem, the choice of the reward function, and the choice of RL controller, are reasonably justified. While the experiments conducted in this paper are not based on real-world data, the reviewer finds the use of data from an FDA-approved simulator of glucoregulatory system sufficiently convincing for this type of problem. The limitation of the proposed method is also well discussed.\n\nThe reviewer has the following concerns:\n\n1. The reviewer views the major contribution of this paper as formulating and solving the glucose management problem as an RL problem.  From a machine learning perspective, the reviewer finds the contribution made in this paper to advance ML methodology very limited as the components used in the algorithmic design of the proposed method are already available in the existing literature. Therefore, the reviewer finds that the paper could be of limited interest to the audience in ICLR while it might be more suitable to the audience of diabetes management.\n\n2. Compared to competing methods reported in this paper, a major characteristic of the proposed method is that it can operate without meal announcements. Methods with meal announcements seem to operate reasonably well compared to the proposed method. Therefore, the authors can consider further justifying why meal announcement is an important bottleneck to alleviate in blood glucose management, which is currently not well explained in the paper. Related to this question, the authors may also consider justifying why making meal announcements more convenient/automated is not a good alternative to handle the blood glucose management problem. It will also be interesting to see how the proposed method will behave when augmented with meal announcements since in reality, the proposed solution might not always be reliable and intervention options like meal announcements could potentially improve the robustness of the solution.\n\n\nMiscellaneous:\nin Section 3.1 of the glucoregulatory system model G, the carbohydrate input $c_t$ seems to be considered as an aspect of the action. Based on the understanding of the reviewer, such an action is a proxy to the meal announcement and is not considered as an input from the user for the deployed policy. For better clarity, the authors can consider reporting the formula of the deployed policy and explain how the quantity $c_t$ is related to this policy.", "belong_id": "ryeN5aEYDH"}, {"uid": "rygXNl-qtr", "paper_title": "Ridge Regression: Structure, Cross-Validation, and Sketching", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a theoretical study of ridge regression, focusing on the practical problems of correcting for the bias of the cross-validation based estimate of the optimal regularisation parameter, and quantification of the asymptotic risk of sketching algorithms for ridge regression, both in the p / n -> gamma in (0, 1) regime (n = # data points, p = # dimensions). The authors derive most of their results exploiting their (AFAICT) new asymptotic characterisation of the ridge regression estimator which may be of independent interest. The whole study is complemented by a series of numerical experiments.\n\nI am recommending this paper to be accepted for publication at ICLR. The paper is clearly written, makes several solid theoretical contributions, and recommends a simple and practical bias correction for CV-based estimates of the optimal ridge regulariser. While ICLR is biased towards deep learning focused publications, the work of Belkin, Hsu, Ma, Bartlett, Hastie, Montanari, Rakhlin, Liang and others (apologies to everyone whose name was omitted---it is for the sole reason of brevity) has recently shown that we can learn non-negligible amount about neural networks from study of linear models.\n\n\nComments:\n\n- Most of the examples in the paper focus on the regime gamma < 1. Would you expect the observed behaviours to be significantly different when gamma > 1? I am asking specifically because of [1] which has found that the bias of the risk estimate obtained via cross-validation is often most extreme when p >> n, which makes me wonder about what would an experiment like those in fig.2 look like in the p >> n regime?\n\n- I was somewhat confused when I first read the statement of thm.2.1. In particular, the definition of asymptotic equivalence requires (roughly speaking) that any series of projections of the difference between the two random vectors converges to zero a.s. However, within the theorem, you introduce Z without much explanation which confused me because not every Z ~ standard normal would be asymptotically equivalent. I needed to look at the proof to understand how Z is coupled with hat(beta), which I think should not be necessary. If possible, I would either say that there exists a (series of) Z (all standard normal) s.t. the asymptotic equivalence holds, or add some other clarification (possibly in the form of a footnote).\n\n- When you are citing a book, please consider citing exact pages or at least chapters/sections (e.g., when citing the exact shortcut from Hastie et al. (2019)).\n\n- In the definition of asymptotic equivalence (starting at the bottom of p.2), did you mean to assume limsup ||w|| < \\infty a.s. (or is limsup not needed here)?\n\n\nReferences:\n\n[1] Tibshirani, R. J., & Tibshirani, R. (2009). A bias correction for the minimum error rate in cross-validation. The Annals of Applied Statistics, 822-829.", "belong_id": "HklRwaEKwB"}, {"uid": "BkgrXByTYB", "paper_title": "Ridge Regression: Structure, Cross-Validation, and Sketching", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper deals with 3 theoretical properties of ridge regression. First, it proves that the ridge regression estimator is equivalent to a specific representation which is useful as for instance it can be used to derive the training error of the ridge estimator. Second, it provides a bias correction mechanism for ridge regression and finally it provides proofs regarding the accuracy of several sketching algorithms for ridge regression.\n \nThe paper addresses an important problem and puts itself nicely in context of previous work. However, it comes across as three papers stapled together, that were submitted to some journal and have now been put into ICLR format. Most of the important results are in the appendix. The main body of the paper is just a smattering of theorems with some text flowing around them and too much notation. There is little or no intuition provided for the proofs in the paper. Moreover, the connection between the 3 theoretical properties of ridge regression studied is also unclear. There could very well be 2 or 3 conference papers written out of this one paper. \n\nAs far as the technical merit is concerned, I checked some theory and it appears correct, however some things were unclear. For example, the Theorem 2.1 is proven under a random design setting; how would the proven ridge representation look under a fixed design setting? Or can we even prove something in that case?\n\nI think the paper definitely has some merit but the presentation makes it hard to assess it. \n", "belong_id": "HklRwaEKwB"}, {"uid": "Skg97KzhKB", "paper_title": "Rigging the Lottery: Making All Tickets Winners", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Overview:\n\nThe paper is dedicated to developing a more efficient and powerful dense-to-sparse training method. In order to break the limits of the size of the largest trainable sparse model to that of the largest trainable dense model, the author proposes a dynamic method that updates the network topology via parameter magnitudes and infrequent gradient calculation. In the experiments parts, they conduct extensive studies to show the proposed approach can surpass the previous sota with ResNet-50, MobileNet v1 and v2 on the imagenet 2012. What's more, the author also provides some intuitive explanation about why allowing topology change during the optimization is beneficial. \n\nStrength Bullets:\n\n1. Due to the dynamic network topology, the paper's methods exactly achieve the memory and computation efficient. i) Required memory is only proportional to the size of the sparse model. ii) The amount of computation is proportional to the number of nonzero parameters in the model.\n2. The author performs detailed comparison experiments among different sparsity distribution and different pruning methods. And the results overcome the previous state-of-the-art results.\n3. Fig 5 shows some interesting insight. It suggests that static sparse training may be stuck at some local minima which are isolated from improved solutions. However, the dynamic update has a big chance to avoid this problem.\n\nWeakness Bullets:\n\n1. The author claims that the ticket in the paper does not rely on a 'lucky' initialization. But it doesn't exclude the possibility that starting from the original initial conditions may give a better performance. Even if the connection is dynamic, we can still record the initial point for each weight. It will be better the author can provide related analysis.\n2. In my opinion, in order to prove dynamic pruning is better than static methods, the author needs to provide a comparison with the previous sota method in it's own setting. i.e. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n\nRecommendation:\n\nI think this paper is a novel work. Although it has some flaws in the experiment design, the motivation and experiment results are conniving enough. So, this is a weak accept.", "belong_id": "ryg7vA4tPB"}, {"uid": "H1gDIdo2FS", "paper_title": "Rigging the Lottery: Making All Tickets Winners", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method for training sparse network without first training a dense network (e.g. the lottery ticket hypothesis or distillation). The method involves a combination of dynamic pruning of weights coupled with a dynamic 'growing' of new weights given a novel criterion based on the magnitude of the gradient of the loss. As a result, networks can stay sparse throughout training and testing, leading to a large reduction in computational cost.\n\nThe paper's approach of dynamically changing the topology of networks is an interesting and motivated idea that seems to work rather well. I also appreciate the experiments on MobileNet, a setting where one expects investigations into sparse network architectures to have significant application. Relatedly, I appreciate the importance of the fact that the computational cost of training and evaluating the network is proportional to the sparse model size, which is not normally true for masked dense models. Overall, I found the paper to be very clear and of high quality, and thus I find this to be an interest addition to investigations into the lottery ticket hypothesis.\n\nAs the authors state, the novelty of their method is that they use the gradients with the highest magnitudes to grow connections. This is somewhat intuitive given the role gradients play in gradient descent based optimization, but I was wondering if they had any further intuition as to why this is the right criterion?\n\nIn section 4.3, I was a bit confused by Figure 5. My understanding is that many paths between loss landscape minima follow nonlinear paths -- why is it at all significant that there's a linear barrier? Why are only quadric and cubic Bezier curves used, rather than a more general path finding algorithm?\n\nOverall, this is a nice paper that should be accepted to ICLR.", "belong_id": "ryg7vA4tPB"}, {"uid": "Bye-WT3W9S", "paper_title": "Rigging the Lottery: Making All Tickets Winners", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a pruning technique called RigL, which performs sparse initialization of the network weights, and allows the network to grow weights during training. The sparse initialization makes the pruning algorithm to be memory- and computation- efficient, unlike existing models that starts with dense network weights. The authors train RigL with three different sparsity distributions that considers the number of input and output nodes, and validate them on various deep convnets for training on ImageNet, on which the model outperforms several existing sparsification methods and even dense counterparts. \n\nPros\n- The proposed model, RigL, is memory- and computation- efficient, and thus allows to train a large network in an efficient manner.\n \n- RigL obtains impressive sparsification performance, even yielding sparse networks that outperform their dense counterparts.\n\nCons\n- The idea of starting from a small, sparse network and expanding it is not novel. DEN [Yoon et al. 18] proposed the same bottom-up approach with sparsely initialized networks, while they allowed to increase the number of neurons at each layer and focused more on continual learning. The authors should compare the two methods both conceptually and experimentally. \n\n- The method is more like a set of heuristics rather than a principled approach, which makes it less appealing. This is not really an issue if the paper includes extensive experimental validation and in-depth analysis, but this is not the case. \n \n- The experimental validation is largely lacking, as the authors only perform experiments on ImageNet and do not compare against recent state-of-the-art Bayesian sparsification methods (SBP, VIB, L0-regularization). Without such extensive experimental validation, it is uncertain whether the result will generalize, given the highly empirical nature of the work. \n\nIn sum, although I believe that the paper proposes a very practical method that is easy to implement and is promising, due to lack of experimental validation against a similar approach, state-of-the-art sparsification methods, and results on more datasets, I temporarily provide the rating of weak reject. I may change my opinion if the authors provide those results during the rebuttal period.\n\n[Yoon et al. 18] Lifelong learning with dynamically expandable networks, ICLR 2018", "belong_id": "ryg7vA4tPB"}, {"uid": "HJe8zpxjYr", "paper_title": "VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a stochastic model based on Glow for conditional video generation. The major novelty of this work is to introduce the flow-based models to video modeling and learn the video dynamics via the dependencies of the latent variables. The general idea is reasonable and the proposed model is technically correct, but I have the following concerns mainly about the originality and the experiments.  \n\n**Above all, most of the text in Section 3 is very similar (or exactly the same) to the text of the Glow paper (the background section).**\n\n Significance and originality \na.1) In Section 1, the authors discussed some possible application scenarios of video prediction models, e.g. learning from unlabeled data and being used for downstream tasks. However, all models mentioned here, including [Mathieu et al. 2016] and [Finn et al. 2016], are deterministic models. Thus, in what way can the stochastic model proposed in this paper be used in real applications?\n\na.2) VideoFlow can be viewed as an extension of the Glow model. There are two problems. First, the originality is limited. I dont think modeling the temporal dependencies of the latent variables with a convolutional network is a significant contribution to the conditional flow-based methods. Second, this paper is not self-contained. After reading Section 4.2, I have to check the previous literature to find the objective function, the network details, or the training procedure.\n\n Experiments \nb.1) Throughout the experiments, the VideoFlow model is mainly compared with two stochastic video prediction models that were probably proposed by the same research group. If it is possible, the authors might include other stochastic models such as the SVG-LP [Denton & Fergus 2018], and at least one deterministic model such as the E3D-LSTM [Wang et al. 2019] as well.\n[Wang et al. 2019] E3D-LSTM: A Model for Video Prediction and Beyond.\n\nb.2) The evaluation metric bits-per-pixel was not directly optimized by the previous video generation/prediction models. Thus, the comparisons in Table 2 might be unfair.\n\nb.3) Since training the Glow model requires a huge computational cost, how is the training efficiency of the VideoFlow model compared with other stochastic video generation models?\n\n Other \nc.1) In Section 4, it is not clear what the temporal border effect means?\n\nAFTER REBUTTAL:\nThough the overall novelty is still not fully convincing, this paper may shed some insights into video generation by introducing flow-based models to this topic. I have increased my score from 3 to 6.", "belong_id": "rJgUfTEYvH"}, {"uid": "SygDedWRtH", "paper_title": "VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper 'VideoFlow: A Conditional Flow-Based Model for Stochastic Video ' proposes a new model for video prediction from a starting sequence of conditionning frames. It is based on a state-space model that encodes successive frames in a continuous hierarchical state, with contraints on trajectories of the codes in this state. \n\nI like the invertible NN framework the model relies on. It allows to avoid variational autoencoding of frames via invertible deterministic transforms. Learning the dynamics of the video is therefore easier, since there is no need of any stochastic inference process.    However, is there no risk of high latent vacancy in the representation space? Uncertainty of stochastic inference usually helps filling the space by considering larger areas of codes than deterministic process. Also, since at each step, the next code is conditionned by the whole past sequence of codes, besides the increasing complexity induced, I am wondering if such a model is able to efficiently encode the dynamics and the stochasticity of the video. In fact, a given z_t does not encode any dynamics nor uncertainty at that point, only the image (it cannot since it is fully determined via the invertible function from the image). Imagine that at a given point, two very different scenarios can follow, with very different following frames. In that case, how could the next state could encode these two different futures with a simple gaussian in the space ? Also,  it would be useful to compare the model with a version where the invertible frame encoder and the sequential model would be learned separately, to better understand what the model really does during training. A study of the impact of the hierarchy depth would also be useful.  \n\nAlso, an additional real-world dataset would be useful for really assessing the performance of the model, since BAIR is known to be fully random and the past does not highly impact the future. A possible dataset would be KTH. Other baselines could also be considered, notably the famous approach from  [Denton et al., 2017]. \n\nAt last, the clarity of some parts could be improved. Notably the description of the sequential model in the space, whih is succintly given in the appendix.   \n\n", "belong_id": "rJgUfTEYvH"}, {"uid": "HJetRuM0KH", "paper_title": "VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper extended the flow-based generative model for stochastic video prediction. The proposed model takes an advantage of the flow-based models which provide exact latent-variable inference, exact log-likelihood evaluation, and efficiency. The paper used the autoregressive model and the multi-scale Glow architecture. The experiments on the stochastic movement dataset (synthetic) and the BAIR Robot push dataset show the performance improvement against other state-of-the-art stochastic video generation models (SV2P and SAVP-VAE). \n\nThe main contribution in this paper is the use of flow-based models for video prediction, and it is the first work in this direction. The major idea sounds and the paper is clearly written. \n\nBelow is my concerns and the feedback. \n\nIt looks like the low-temperature sampling is important to achieve the better scores for prediction. Can the low-temperature sampling trick be applied for SV2P and SAVP-VAE as well? If then, how is the performance difference compare to the proposed model?\n\nThe authors reported the best possible values of PSNR, SSIM and VGG perceptual metrics by choosing the video closest to the ground-truth. However, I believe this evaluation does not present the benefit of the stochastic models. The better comparison I believe is to report the median/mean with the range between best and worst values. \n\nThe BAIR robot push dataset is with a pretty limited setting: a small robot and/or object motion between frames and a small variation of the background between videos. It would be interesting to see more dynamic scenarios such as driving or human motion scenes. ", "belong_id": "rJgUfTEYvH"}, {"uid": "B1eAkDUuKH", "paper_title": "Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #564", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The main problem that is tackled here are tasks that have a main goal that can only be reached by solving prerequisite tasks. They test their method on a simple game and a very complex one. \n\nMethodology and novelty\nThe authors combine various techniques (subtask graph inference, gradient based meta-learning and inductive logic programming). It is not clearly stated if the authors combined techniques and/or if they invented a new one. What is the big difference from the work by Sohn et al. (2018)?\n\nExperiments\nThe authors evaluated one agent. It would have been better if they trained multiple agents and showed a performance distribution, so it is clear that the performance is not just achieved by luck (Fig 5.). \nThe video material showed clearly how the complex game (StarCraft II) was solved much quicker than a baseline model. \n\nPresentation\nFigure 3 does not give a description of the subtask graph (middle) and the StarCraft II. The video material clearly shows the performance of their method. Section 5.1.2 does not clearly explain the different datasets D1-D5 of Playground. ", "belong_id": "HkgsWxrtPB"}, {"uid": "S1lrmJG2tH", "paper_title": "Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a new meta-reinforcement learning algorithm, MSGI, which focuses on the problem of adapting to unseen hierarchical tasks through interaction with the environment where the external reward is sparse. The authors make use of subtask graph inference to infer the latent subtask representation of a task through interacting with the environment using an adaptation policy and then optimize the adaptation policy based on the inferred latent subtask structure. Each task in the paper is represented as a tuple of subtask precondition and subtask reward, which are inferred via logic induction and MLE of Gaussians respectively. At meta-test time, MSGI rollouts a subtask graph execution (SGE) policy based on the graph inferred from the interactions between the environment and the adaptation policy. The authors also propose a UCB-inspired intrinsic reward to encourage exploration when optimizing the adaptation policy. Experiments are conducted on two grid-world domains as well as StarCraft II.\n\nOverall, this paper is mainly an extension of the prior work [1], which uses a subtask graph for tackling hierarchical RL problems. This work builds upon [1] by extending to meta-learning domains and studying generalization to new hierarchical tasks. While the contribution seems a bit incremental and the experimental setting is a bit unclear and limited to low-dimensional state space, the inference of task-specific subtask graphs based on past experiences and the proposal of a UCB-inspired reward shed some interesting insights on how to approach meta-hierarchical RL where long-horizon tasks and sparse rewards have been major challenges. Given some clarification on the experimental setup and additional results on more challenging domains in the author's response, I would be willing to improve my score.\n\nRegarding the experimental setup, the set of subtasks is a Cartesian product of the set of primitive actions and a set of all types of interactive objects in the domain, while the state is represented as a binary 3-dimensional tensor indicating the position of each type of objects. Such a setup seems a bit contrived and is limited to low-dimensional state space and discrete action space, which makes me doubt its scalability to high-dimensional continuous control tasks. It would be interesting to see how/if MSGI can perform in widely used meta-RL benchmarks in Mujoco. I also wonder how MSGI can be compared to newly proposed context-based meta-RL methods such as PEARL.\n\nAs for the results, the authors don't provide an ablation study on the UCB exploration bonus though they claim they would show it in the paper. Moreover, the result of GRProp+Oracle is also missing in the comparison. I also don't understand why MSGI-Meta and RL2 would overfit in the SC2LE case and are unable to adapt to new tasks. Is that a limitation of the method? The authors also introduce MSGI-GRProp in this setting, which is never discussed before, and claim that MSGI-GRProp can successfully generalize to new tasks. It seems that the authors don't use a meta-RL agent in order to get this domain to work. I believe more discussion on this part is needed.\n\n[1] Sungryull Sohn, Junhyuk Oh, and Honglak Lee. Hierarchical reinforcement learning for zero-shot\ngeneralization with subtask dependencies. In NeurIPS, pp. 71567166, 2018.", "belong_id": "HkgsWxrtPB"}, {"uid": "r1l0Qu3nKS", "paper_title": "Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n-------------\nThe authors propose a novel meta-rl problem where hierarchical tasks are characterized by a graph describing all sub-tasks and their dependencies. They propose a meta-rl approach to meta-train a policy that quickly infers the subtask graph from new task data. The approach is compared to relevant baselines from both the meta-rl and hierarchical rl literature on complex domains. In particular, the authors consider a large-scale Startcraft II experiment which proves the efficiency and scalability of the proposed methodology.\n\nMajor Comments\n--------------\n\nMeta-rl is a relevant direction for reducing the sample-complexity of rl agents and scaling them to large domains. This work presents interesting and novel ideas in these settings. In particular, the few-shot rl problem with subtask dependencies seems quite interesting for both encoding and solving large hierarchical rl problems. The proposed meta-rl algorithm is sound and simple to understand. The paper is well-organized, though sometimes it is difficult to follow the formalisms due to a large number of different symbols introduced. The experiments are quite interesting and convincing. In particular, the Starcraft domain should address all concerns about the scalability and efficiency of the proposed approach. Some comments/questions follow.\n\n1. The state available to the agent includes the number of remaining time-steps and episodes. When/how are they used?\n\n2. The paper requires the reader to be quite familiar with some previous works (e.g., Section 3.2 requires to know Song et al. 2018 to understand the test phase). It would be good to add more background/details about these works (at least in the supplementary), so that the paper is more self-contained.\n\n3. In the Starcraft experiment, what is the difference between MSGI-meta and MSGI-GRProp? Furthermore, where is the 'oracle' baseline (introduced in sec. 5) used in the experiments? I did not find any plot reporting it.\n\n4. The main limitation is that this approach requires options for each subtask to be provided before-hand. Do the authors think that the method is easily generalizable to learn such options as well? Furthermore, I realized this limitation only after reading the very last lines of the paper. Since this is of major importance, I believe it should be clearly stated much earlier.\n\nMinor Comments\n--------------\n1. First line of sec. 2.1: R_\\tau should be R_G\n2. I did not find a definition of o_t and d_t which appear, e.g., in Algorithm 1.", "belong_id": "HkgsWxrtPB"}, {"uid": "SJxD4x_TYH", "paper_title": "Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper develops a cyclical stepsize schedule for choosing stepsize for Langevin dynamics. \nThe authors prove the non-asymptotic convergence theory of the proposed algorithm. Many experimental results, including ImageNet, are given to demonstrate the effectiveness of the proposed method. \n\nHere I suggest that authors also need to point out that the continuous-time MCMC is the Wasserstein gradient flow of KL divergence. The bound derived in this paper focus on the step size choice of gradient flows. This could be a good direction for combining gradient flows studies in optimal transport and MCMC convergence bound for the choice of step size. \n\nOverall, I think that the paper is well written with clear derivations. I strongly suggest the publication of this paper. ", "belong_id": "rkeS1RVtPS"}, {"uid": "S1eVwgy15B", "paper_title": "Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper propose a new MCMC scheme which is demonstrated to perform well for estimating Bayesian neural networks. The key idea is to not keep lowering the step sizes, but -- at pre-specified times -- go back to large step sizes.\n\nThe paper is timely, the proposed algorithm is novel, and the theoretical analysis also seem quite novel.\n\nMy key concern is that with MCMC sampling it is often quite difficult to tune parameters, and by introducing more parameters to tune when step sizes should increase, I fear that we end up in a 'tuning nightmare'. How sensitive is the algorithm to choice of parameters?\n\nI would expect that the proposed algorithm is quite similar to just running several MCMCs in parallel. The authors does a comparison to this and show that their approach is significantly faster due to 'warm restarts'. Here I wonder how sensitive this conclusion is to choice of parameters (see nightmare above) ? I would guess that opposite conclusions could be reached by tuning the algorithms differently -- is that a reasonable suspicion ?\n\nIt is argued that the cyclic nature of the algorithms gives a form of 'warm start' that is beneficial for MCMC. My intuition dictate that this is only true of the modes of the posterior are reasonable close to each other; otherwise I do not see how this warm starting is helpful. I would appreciate learning more about why this intuition is apparently incorrect.\n\nMinor comments:\n* on page 4 it is stated that the proposed algorithm 'automatically' provide the warm restarts -- but is it really automatic? Isn't this a priori determined by choice of parameters for the algorithm?\n\n* It would be good to use \\citet instead of \\cite at places, e.g. 'discussed in (Smith & Topin, 2017)' should be 'discussed by Smith & Topin (2017)'. This would improve readability (which is generally very good).\n\n* For the empirical studies I think it would be good to report state-of-the-art results as well. I expect that the Bayesian nets still are subpar to non-Bayesian methods, and I think the paper should report this.", "belong_id": "rkeS1RVtPS"}, {"uid": "HJlFucOe5S", "paper_title": "Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This article presents cyclical stochastic gradient MCMC for Bayesian deep learning for inference in posterior distributions of network weights of Bayesian NNs. The posteriors of Bayesian NN weights are highly multi-modal and present difficulty for standard stochastic gradient MCMC methods. The proposed cyclical version periodically warm start the SG-MCMC process such that it can explore the multimodal space more efficiently.\n\nThe proposed method as well as the empirical results intuitively make sense. The standard SG-MCMC basically has one longer stepsize schedule and is exploring the weight space more patiently, but only converges to one local mode. The cyclical SG-MCMC uses multiple shorter stepsize schedules, so each one is similar to a (stochastic) greedy search. Consequently, the cSG-MCMC can collect more diverse samples across the weight space, while the samples of SG-MCMC are more concentrated, but likely with better quality (as shown in Figure 3). \n\nPersonally I would like to see how Bayesian deep learning can be applied to real large-scale applications. Probabilistic inference is expensive; Bayesian model averaging is even more expensive. That's probably why recent literature focuses on variational inference or expectation propagation-based approaches. ", "belong_id": "rkeS1RVtPS"}, {"uid": "ryg4ZdOhtH", "paper_title": "Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper uses the continuous Game of Life as a testing ground for algorithms that discover diverse behaviors. The problem is interesting, under-explored, and rich. The  combines a variety of interesting ideas including compositional pattern producing networks (CPPNs) to learn structured primitives. Although the authors do propose formal measures of behavioral diversity and so show performance improvements, at the end of the day this work, like much empirical work on generative adversarial networks, is drifting towards art -- where performance is ultimately judged by human eyes rather than quantiative metrics. \n\nComments:\nThe paper refers to hand-designed goal spaces and talks, on p28, about the statistical measures used to define the goal space. At the same time, the analytic behavior space is also defined in terms of statistical measures, but it is *not* referred to as hand-designed. At this point, the profusion of spaces and measures means that I am no longer sure what counts as hand-crafted or not. Please clarify.\nThe hypothesis on p34, sec E.4.2 that the VAEs 8-dim bottleneck helps focus on animals rather than non-animals (which are differentiated more in terms of textures and details) is important and should be checked. \nSome of the decisions about what to check and vary are unclear. For example, section E.1 considers the effect of different initializations (pytorch, xavier and kaiming). The choice of initialization is important mostly to do with improving gradients to improve the rate of convergence (or convergence at all) in deep nets. Its not clear why initializations are an parameter to vary when considering diversity of solutions. Or, rather, why initializations are more interesting to consider various other architectural considerations. More broadly, looking at Fig 17, the x-axis doesnt make much sense. The experiments along the x-axis vary according to initialization, but also according to the nature of the goal space and other features. It seems a bit incoherent. \n\nOverall I think this is a good paper. The results are novel and even better, they are fun. However, the paper is extremely long, and it feels as though the authors have to some extent lost control of the material. I could add more comments but TL;DR it needs a lot of editing and pruning. \n", "belong_id": "rkg6sJHYDr"}, {"uid": "BJxu9RJAYH", "paper_title": "Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The focus of the presented paper is on formulating the automated discovery of self-organized patterns in high-dimensional dynamic systems. The introduced framework uses cellular automata (game of life) as a testbed for experimentation and evaluation and existing machine learning algorithms (POP-IMGEPs). The goal of the paper is to show that these algorithms can be used to discover and represent features of patterns. Moreover, an extension of SOTA algorithms is introduced and several approaches to define goal space representations are compared. \n\nOverall, I have the impression this is an interesting paper that could be accepted to ICLR. The idea of applying IMGEPs to explore parameters of a dynamic system is novel and interesting, which could also simulate further research in this field. Furthermore, the paper well-written, technically sound, and the results are interesting. The overall contribution of the paper is in applying IMGEP algorithms to exploring parameters of dynamic systems and in comparing different algorithms along with an extensive set of experiments. As a point of criticism, a lot of (interesting) material was pushed to the Appendix. Resolving the references makes reading the paper harder. Moreover, given that this paper has more than 35 pages appendix material, it seems this work would better be suited for a journal as for a conference. There is a reason for papers to have a page limit and this work circumvents this limit by presenting a lot of additional material. Therefore, I am not willing to strongly support this work. \n\nSpecific Comments: \n\n- Section 3.1: It is not clear how the initial system state is established. In Section 3.1. the text states that 'parameters are randomly sampled and explored' before the process starts, but it is not clear why a random sampling is used and what this means for the subsequent sampling. Later in the text (3.3) it becomes more clear, but here this appears too unclear.\n-  Section 3.1: 'distribution over a hypercube in \\mathcal{T} chosen to be large enough to bias exploration towards the frontiers of known goals to incentivize diversity.' This sentence is not clear and needs more details. How is the distribution chosen exactly?\n- Section 3.2 appears a bit repetitive and could be more concise. I don't think it is necessary here to contrast manual vs learned features of the goal space.\n- Section 3.2 (P3): the last sentence of this paragraph reads as if there exists no approaches for VEAs in online settings. This should be toned down or backed up by a reference.\n- Section 3.2: (last sentence): it is not clear how the history is used exactly to train the network. Which strategy is used to sample from the history of observations?\n- Section 3.3: What is meant by 'The CPPNs are used of the parameters \\{theta}'? The details provided after this sentence are not clear and need more details. \n- Section 4.2: Please provide more details what 'very large' dataset means.\n- Section 4.2: 'HGS algorithm' is not defined.\n- Section 5: It seems unnecessary to explain what t-SNE does as a method. ", "belong_id": "rkg6sJHYDr"}, {"uid": "Hyez9WiNcr", "paper_title": "Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper describes an algorithm to find diverse patterns in Lenia (a continuous CA system) by using a CPPN to generate initial states, and a stochastic exploration algorithm to mutate parameters of the CPPN + CA parameters. The fitness is the closeness of a generated set of latents to a set of latents produced through one of several possible processes; hand-design, pretraining, or online training on previously generated CA settings. \n\nThe core results are in Figures 27 to 31 in an appendix. Initial inspection reveals that handdesigned goal states produce the most interesting non-animal patterns. With regard to animal forms, it appears to me that Online goal learning harms the diversity of animal forms considerably compared to PGL and perhaps HGS. High frequency spatial structure seems to be lost there. \n\nI would like to see a further analysis of maybe 10000s of such images generated, and an understanding of exactly why RGS produces the same kind of red linear patterns, and why HGS produces the distribution of pattern types in Figure 29, and why non-animal types differ in PGL vs HGS, and why high frequency spatial structure is lost in OGL. How robust are these over many runs? The results should NOT be shown just for the first repetition of the experiment but for all independent runs of the experiments, e.g averaged over 30 independent CPPN evolutions, for PGL, OGL, Random, and HGS! \n\n", "belong_id": "rkg6sJHYDr"}, {"uid": "ByllmnE6tH", "paper_title": "Filling the Soap Bubbles: Efficient Black-Box Adversarial Certification with Non-Gaussian Smoothing", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces an improvement to the randomized smoothing analysis in Cohen et al. (2019), using Lagrangian relaxation to achieve a more general lower bound. Using this, it considers different adversarial smoothing distributions that yield some increase in certified adversarial accuracy.\n\nOverall assessment: While the Lagrangian relaxation idea is interesting and could yield interesting follow-up work, the paper is sloppy in several respects and needs to be tightened before it can be considered for publication.\n\nKey issues:\n1. Proof of main theorem (strong duality) is incorrect. Likely the statement itself is also incorrect. Fortunately the most important direction (lower bound) is still true, so this isn't a fatal flaw to the approach.\n2. The paper makes several references (in italics) to a 'fundamental trade-off between accuracy and robustness'. But a fundamental trade-off means that *any* method that attains good accuracy must sacrifice robustness and vice versa; this requires a 'for all' statement, i.e. a lower bound. All the paper shows is that the *particular upper bound* exhibits a trade-off (and even then, the notions of 'accuracy' and 'robustness' are merely interpretations of quantities in the bound; it's not clear why the robustness term in particular is tied to more standard notions of robustness).\n3. The justification for why the particular smoothing distributions are good ideas is sketchy.\n\nI elaborate on 1 and 3 below. Addressing 1-3 effectively will improve my score.\n\n#1 (main theorem is incorrect): Claim 3 in the appendix is wrong. The fact that (delta', f') outperforms (delta-bar, f-bar) with respect to lambda* does not imply that (delta', f', lambda*) is a better solution to the primal problem, because we must take max over lambda and the maximizing lambda need not be lambda*. In particular if f' doesn't satisfy the constraint we would instead take lambda to infinity.\n\n#3 (sketchy justification): The paper justifies a smoothing distribution that concentrates more mass around the center as follows: 'This phenomenon makes it problematic to use standard Gaussian distribution for adversarial certification, because one would expect that the smoothing distribution should concentrate around the center (the original image) in order to make the smoothed classifier close to the original classifier (and hence accurate).' I don't see why we should want more mass near the center---in the limit as we move all the mass towards the center and get the original classifier, our certified bound will be terrible, so it's not clear why moving in that direction should be expected to help. Indeed, the experimental gains are minimal (1 to 3 percentage points) and on methods that were not carefully tuned, so one could imagine that the baseline method could be improved by that much just with careful tuning.\n\nI similarly didn't understand the justification for the mixed L-inf / L-2 distribution for L-infinity verification. The main justification was 'The motivation is that this allows us to allocate more probability mass along the pointy directions with larger`norm, and hence decrease the maximum distance term max B`,rDF(0).' This is at the very least too brief for justifying the main experimental innovation in the paper (here at least the empirical improvements are bigger, although still not huge).\n\nMinor but related: Why is the x-axis in Figure 4 so compressed? This is also in a regime where all 3 methods fail to certify so not clear it's meaningful.\n\nWriting comment: Change some of the Theorems to Propositions. Theorems should be for key claims in paper (there shouldn't be 4 of them in one 8-page paper).", "belong_id": "Skg8gJBFvr"}, {"uid": "rJejRI6aKS", "paper_title": "Filling the Soap Bubbles: Efficient Black-Box Adversarial Certification with Non-Gaussian Smoothing", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThis paper investigates the choice of noise distributions for smoothing an arbitrary classifier for defending against adversarial attacks.  The paper focuses on the two major adversaries: \\ell_2 adversaries and \\ell_\\infty adversaries. Theorem 1 quantifies the tradeoff between the choice of smoothing distribution which (1) has clean accuracy close to the original classifier and (2) promotes the smoothness of smoothed classifier (and hence adversarial accuracy).  For the \\ell_2 adversary, the paper argues that Gaussian distribution is not the right choice, because the distribution is concentrated on the spherical shell around the x. Instead, the authors propose using a new family of distributions, with the norm square  (p_{|z|_2^2}) following the scaled \\chi^2 distribution with degree d-k (Eq. 8). This allows an extra degree of freedom, and setting k=0 recovers the Gaussian distribution. For \\ell_\\infty perturbations, the paper suggests another family of distributions combining the \\ell_2 and \\ell_\\infty norm (Eq. 9), and argues that it outperforms the natural choice of \\ell_\\infty norm-based distributions (Eq. 10).\n\nI think the paper should be rejected because (1) For \\ell_2 perturbations, there is no major difference between this new family of distributions (d-k \\chi^2) and a Gaussian with different variance. (2) For \\ell_\\infty distributions, the motivation of mixed norm distributions (Eq. 9) over \\ell_\\infty based distributions (Eq. 10) is not very clear. (3) The experimental evidence is also weak (see below).\n\nMain arguments:\n\n1. The distribution of the norm \\|z\\|_2 in Eq. (8) would be concentrated on a thin spherical shell of radius about \\sqrt{d-k}\\sigma. As the Gaussian distribution with standard deviation \\sigma' is supported on a shell of radius about \\sqrt{d} \\sigma', for each (k,\\sigma) in the family of Eq. 8, there is an equivalent Gaussian with appropriate \\sigma' (Theorem 3 now just compares the radius of the spherical shell). Therefore, I don't see the benefit of this extra degree of freedom of k: the noise distribution is again a 'soap bubble' of a different radius. Thus, a grid search over \\sigma' for a Gaussian should be the same as a grid search over (k,\\sigma) in Eq. 8.\n\nEven the experimental experiments are a marginal improvement over Cohen et al.  I don't see why the value of (k,\\sigma) was not provided in Table 1 and only \\sigma was provided. Also, the table of Cohen et al. was only calculated for specific values of \\sigma for Gaussian distributions (0.12, 0.25, 0.5, 1.00). For a fair comparison, comparable values of \\sigma's must be calculated, and then the best choice should be selected. \n\n2. In the light of previous arguments, I don't think the choice of Eq. (9) or Eq. (10) is well motivated. \nWhy not smooth it with a cube of appropriate radius? Also, not enough experimental details are provided for Table 3.  Salman et al. (2019) reports the accuracy of 68.2% for \\ell_infty perturbations (Table 3, Salman et al. (2019)), whereas the value reported in your Table 3 for at the same radius is 58%. Is it a typo? In any case, the values reported for the proposed model in Table 3 are only a marginal improvement over Figure 1 (left) in Salman et al. (2019), just going by the trivial \\ell_2 to \\ell_\\infty certificate.\n\nOther areas for improvement:\n\n1. The paper contains numerous grammatical errors, confusing statements, and nonstandard phrases.  For example: (i) more less robust, (ii) black start, (iii) pointy points, etc.  I suggest that the authors spend more time clarifying their manuscript.\n\n2. The paragraph starting with 'Trade-off between Accuracy and Robustness': I think this paragraph should be reworded for clarification.  It is not robustness but rather the lack thereof -- say, sensitivity.\n\n3. On p.5, why was the toy classifier sphere-based? The toughest classifier for Gaussian smoothing (the one achieving the lower bound for Gaussian smoothing) is actually a linear classifier.\n\n\n", "belong_id": "Skg8gJBFvr"}, {"uid": "B1lWEtgb9B", "paper_title": "Filling the Soap Bubbles: Efficient Black-Box Adversarial Certification with Non-Gaussian Smoothing", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a new method for adversarial certification using non-Gaussian noise. A new framework for certification is proposed, which allows to use different distributions compared to previous work based on Gaussian noise. From this framework, a trade-off between accuracy and robustness is identified and new distributions are proposed to obtain a better trade-off than with Gaussian noise. Using these new distributions, they re-certify models obtained in previous work.\n\nI am hesitating between a weak reject and a weak accept. The theoretical results are interesting, showing a clear trade-off between robustness and accuracy with a new lower bound and deriving better smoothing distributions. However, the experimental results are lacking, and do not support much the proposed method. Training with this new distribution would have been a natural experiment given the argument. Moreover, the results for L_inf are partial and it would be expected to have some results for ImageNet as claimed in the introduction. I would have given an accept if the previous points had been addressed and I feel that with some more work on it, it would become an excellent paper.\n\nMain arguments:\nMy main concern is about the experiments: Why were Cohen et al.s models used instead of Salman et al.s? Salman et al.s have achieved better certified accuracy under the L_2 norm so it would only seem natural to use their model.\nAbout the main results: there seems to be a discrepancy between the results reported for Cohen et al. and the original paper for both CIFAR-10 and ImageNet L2 certification. Also, the reported certified accuracy for Salman et al.s model for L_inf on CIFAR-10 reported in the original paper is 68.2 at 2/255, which is very far from the 58 in Table 3. What is the reason for these differences?\n\nMinor comments:\nIn the third paragraph, it is claimed that L_inf attacks are a stronger and more relevant type of attacks than L_2 attacks. These two different objectives cannot be compared in those terms.\nDefenses such as adversarial training have not been broken as claimed in section 2 in the sense that the claims made in the original paper still hold true. The term broken is used for defenses in which the claimed accuracy against stronger attacks were found to be much lower than what was claimed in the original paper.\nIt is claimed that if ||z||_inf is too large to exceed the region of natural images, the accuracy will be obviously rather poor; however, the common practice is to clip to the input space bounds. How would that affect the method?\n\nThings to improve the paper that did not impact the score:\nIn the first paragraph, Goodfellow et al., 2015 is cited, however, papers on adversarial attacks were published earlier than that such as Szegedy et al., 2014 or Biggio et al., 2013.\nVershynin, 2018 is cited about the distribution of a gaussian in high-dimensional spaces. However, this is a very well known result and does not need any citation (or if any, Bellman, 1961).\nTypo after equation 4: ||f||_{L_p}\nTypo in Black-box Certification with Randomness paragraph: by convovling\nTypos in Table 2.: the columns 2.0 to 3.5 are mislabeled\n", "belong_id": "Skg8gJBFvr"}, {"uid": "SkxALfA6YS", "paper_title": "Stochastic Conditional Generative Networks with Basis Decomposition", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors introduce BasisGAN, a novel method for introducing stochasticity in conditional GANs, i.e., a way of conducting one-to-many mappings. This is a good addition in the literature as: (a) most of the widely-used conditional GANs such as pix2pix (Isola et al., 2016) or pix2pixHD (Wang et al., 2018) are deterministic (i.e., for a specific input a single output is always generated), (b) it improves upon the current SOTA in one-to-many mappings, (c) it is very useful application-wise. As also stated in the paper, there is a number of applications where this method is handy (e.g., converting a sketch to images varying in colors, etc.).\n\nI am leaning towards accepting this paper as this work is well-motivated and found the idea of using the basis generator to learn the bases for the generation of the parameters quite interesting. This is the main contribution and difference of this paper in comparison to DCFNet (Qiu et al., ICML 2018), where the bases are not learned. \n\nNevertheless, I have the following questions/requests:\n\n- How can we tell that the generated bases are indeed bases (e.g., are they orthogonal?)\n- Please report the number of parameters used in your implementation in comparison to the rest of the methods. \n- Please provide qualitative results against the compared methods and especially against DSGAN (Qin et al., 2018).", "belong_id": "S1lSapVtwS"}, {"uid": "rJed5l9tqH", "paper_title": "Stochastic Conditional Generative Networks with Basis Decomposition", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new conditional GAN architecture. In particular, in order to allow for further diversity in conditional signal generation, the BasiGAN proposes to model the convolutional layers as a combination of basis which is stochastically sampled. The idea of the paper is interesting and some interesting experiments are presented. Nevertheless, I do not quite get why a set of predefined random basis would enforce more variability than the non-parametric way of training which is currently applied for conditional-GANs. If I get a convincing answer from the authors, I would definitely accept the paper (which otherwise is well-written and quite interesting to read). ", "belong_id": "S1lSapVtwS"}, {"uid": "BkgrY8ih9S", "paper_title": "Stochastic Conditional Generative Networks with Basis Decomposition", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a model for stochasticity for conditional image generation, building upon the previously available (DCFNet) results on composition of  convolutional filters out of the elements of the filter basis. \n\nThe idea of introducing stochasticity by convolutional filters into the conditional generative models seems to be novel and the reviewer thinks it could be of interest for the community.\n\nThe following remarks could be given to improve the presentation:\n1) Theorem 1 is an existence theorem, so it does not give the procedure for construction of the basis. Does the construction procedure for the basis, described under the theorem formulation, meet the conditions of Theorem 1? \n2) The Theorem 1 formulation states that  If there exists a set of deterministic linear transforms. Should the linear independence be stated as well as one of the theorem conditions ( so that the space dimensionality would indeed be K)? \n3) The reviewer finds the structure of Section 4 confusing: it starts from the problem statement (first paragraph 'Using the method above, filters of each stochastic layer...), then provides the description of the approach and only then outlines Theorem 1. It might be that stating Theorem 1 and then defining the method for generation of the basis (how exactly could we get to the basis? ) could improve readability of the paper. Essentially, the question is: is there any way to emphasise the procedure for filter generation and inform the reader in which circumstances these filters would be the basis (e.g. why it wouldn't be prone to the analogue of mode collapse when the filters do not effectively have enough diversity for linear independence)? \n***\nIn addition to this list, it might be useful to provide some evidence on whether there is any inherent mechanism to regulate the diversity of filters and therefore of samples (so that to change the variability of the conditional samples from the model with the impact analogous to the one of temperature in Glow (Kingma et al, 2018)). If there is one, further experimental evidence, which shows the impact on diversity of filters, would contribute to improvement of the paper.  \n", "belong_id": "S1lSapVtwS"}, {"uid": "rJewS5lLtB", "paper_title": "XD: Cross-lingual Knowledge Distillation for Polyglot Sentence Embeddings", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes two improved strategies for fine-tuning XLM (a multilingual variant of BERT) for cross-lingual NLI. First of all, it shows that fine-tuning a single model on the combination of all languages (the original English data from MultiNLI and their MT translation into the rest of languages) performs better than fine-tuning a separate model for each language. Furthermore, they show that minimizing the L2 distance between the English training sentences and their MT translation into the rest of languages, which does not explicitly use any labels in the foreign languages and is presented as a way of performing cross-lingual knowledge distillation, also performs better than zero-shot transferring a regular model fine-tuned in English.\n\nI think that the paper makes some interesting contributions and, in particular, I think that the finding that multilingual fine-tuning performs better than the standard approach of fine-tuning a separate model for each language is important. Nevertheless, I am not convinced that there is enough novelty and substance on this, I have some concerns on the evaluation, and I think that the overall presentation should also be improved:\n\n- I am not convinced by the 'knowledge distillation' approach. First, although I see the connection, I do not think that presenting this as 'knowledge distillation' is consistent with the common use of this term in the literature. More importantly, I do not see what is the value of this approach considering that multilingual fine-tuning performs better, and combining them both does not bring any clear improvement. The authors motivate it as a form of performing zero-shot cross-lingual transfer as, unlike the multilingual fine-tuning, this approach does not use any label in the foreign languages. However, I am not convinced at all by this reasoning, as it still relies on the translation of the English labeled data into the other languages. So, from a practical perspective, it requires the exact same resources as the other approach, as you would always be able to use the English labels for the rest of the languages, while being more complex and worse.\n\n- It looks like the IndT results, which is the real baseline, are taken from the original XLM paper, while the rest of the results come from the authors' own runs, who use a different implementation. I think that you should also report IndT results from your own runs to make sure that your improvements come from the actual method, and not from small implementation details.\n\n- You are trying small variations of your method (e.g. removing a particular language from the multilingual training) to support your claims, and it is not clear if the (rather small) differences in the results are significant. It would be good if you at least run the baseline multiple times and show the variance.\n\n- It is unfair to try so many variants of your method in the test set, and then pick the best and claim SOTA as done in Table 6. Your final system looks rather ad-hoc and arbitrary: it is doing multilingual fine-tuning in all languages but Urdu, and cross-lingual knowledge distillation in a subset of 4 languages out of 15. It might get SOTA results in this particular scenario, but what if we move to a different set of languages, a different task, or even a different test set?\n\n- The authors claim that 'Urdu (ur) is an unrelated language' and 'Swahili (sw) is loosely between French and Urdu in terms of relatedness to English', which they use to justify why Urdu behaves differently in their experiments. I do not speak neither Swahili nor Urdu, but from what I know this statement looks wrong. Swahili and English belong to completely different language families, and from what I know their grammar is very different. In contrast, Urdu at least belongs to the Indoeuropean language family.\n\n- This is not relevant at all, but I would suggest the authors to find a different acronym instead of XD, which happens to be a widely used emoticon. I assume that the authors deliberately made this choice thinking that it would be funny, but I just find it confusing to see XD all over the place in a formal paper.", "belong_id": "BkePneStwH"}, {"uid": "BygAwA40FH", "paper_title": "XD: Cross-lingual Knowledge Distillation for Polyglot Sentence Embeddings", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "First, the authors propose to train a model for natural language inference (NLI) on multiple languages simultaneously. In particular, they translate English examples into all target languages and fine-tune a pretrained language model on all thereby obtained data at once. This is different from the previous state-of-the-art approach which consisted of, after translating from English into target languages, fine-tuning one NLI model for each language individually. The authors show that their approach is superior to training individual models for each language. For evaluation, XNLI is used.\n\nSecond, they introduce cross-lingual knowledge distillation (XD), where the same polyglot model is used both as teacher and student across languages to improve its sentence representations without using the target task labels. The main idea is that the same sentence in all languages should receive output representations as similar as possible.\n\nThe paper seems okay to me and the experiments seem solid. However, the results are not particularly surprising and the methods are not very innovative. The writing could be improved. \n\nThis paper could further be improved in the following ways:\n- A more detailed investigation which combination of languages improve performance (and why?).\n- Similarly: A combination of MTL and XD doesn't seem straightforward. Why? What is learned?\n\nSmaller comments:\n- Articles are missing frequently (e.g., 'we substitute the word prediction head with classification layer' -> 'we substitute the word prediction head with a classification layer')\n- Table 5: 'w/0' -> 'w/o'?\n- Have you run any significance tests?", "belong_id": "BkePneStwH"}, {"uid": "S1e0ZJj35B", "paper_title": "XD: Cross-lingual Knowledge Distillation for Polyglot Sentence Embeddings", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "What is the task?\nMultilingual natural language inference (NLI)\n\nWhat has been done before?\nCurrent state-of-the-art results in multilingual natural language inference (NLI) are based on tuning XLM (a pre-trained polyglot language model) separately for each language involved, resulting in multiple models.\n\nWhat are the main contributions of the paper?\n[Not novel] Significantly higher average XNLI accuracy with a single model for all 15 languages.\n[Moderately novel] Cross-lingual knowledge distillation approach that uses one and the same XLM model to serve both as teacher (for English sentences) and student (for their translations into other languages). The approach does not require end-task labels and can be applied in an unsupervised setting\n\nWhat are the main results? \n A single model trained for all 15 languages in the XNLI dataset can achieve better results than 15 individually trained models, and get even better when unrelated poorly-translated languages are removed from the multilingual tuning scheme.\n Using XD they outperformed the previous methods that also do not use target languages labels. \n\nWeaknesses :\n1. Combining XD with multilingual tuning is not effective in improving average results or even in case of target languages\n2. Final system is adhoc as experiments on a particular set of languages have been used to support claims. For example, Urdu was excluded to get the best MLT model. Only 4 languages were used while combining XD and MLT\n3. Findings, methods and experiments are not strongly novel.\n4. Paper was not an easy read.\n\nStrengths:\n Using XD they outperformed the previous methods that also do not use target languages labels. \n\n", "belong_id": "BkePneStwH"}, {"uid": "rJgnL8waqB", "paper_title": "XD: Cross-lingual Knowledge Distillation for Polyglot Sentence Embeddings", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper addressed multilingual natural language inference. The motivations of the authors are two folds: 1/ to have only one model for all the languages instead of one model per language and 2/ achieving good results in a zero shot setup where only the english labels are available. \n\nPrevious work proposed first to learn multilingual language model in a self-supervised manner for the initialization. Then, fine-tuning it on the final task, separately for each different language. It results in multiple models, one for each language.\n\nThe authors proposed to fine-tune the model with all the data from the different languages simultaneously, resulting in only one model with comparable results.\n\nIn addition, they also proposed a method based on distillation where only the english targets are used. While the model doesn't require the label data for the other languages, the scores remained similar to the previous experiments.\n\nFinally the authors proposed to combine both methods. It compares favorably, obtaining 4 points of improvement over SOTA in the zeroshot setup.\n\nPros:\n-the motivation for having only one model is interesting\n-the results are promising\n\nCons:\n-one of main motivation of the paper is to achieve zeroshot as opposed to previous work. To that purpose, the authors chose to keep only the translated non-en input and not the non-en target. In XNLI, all the non-en training data, including target, are automatically translated from the English data. Therefore, the authors did not use less human annotated data and their approche still requires automatic translation. Hence, the motivation to perform the task in a zero-shot scheme, as opposed to previous work, doesn't seem correct. \n-the paper was not always easy to follow and would benefit from more clarity.\n'large margin of 5.9 and 4.2 points' in 3.2.1, please add the reference to table 6.\n-the method 'significantly' improved results. I didn't see any significance measurements and it would be important to add them.\n\nOverall, using all the data together seems like a natural and effective approach to and achieves good results through only one model. \nHowever, the motivation behind 'distillation' is to perform in a zeroshot scheme. It seems abusive to me since it actually requires the exact same amount of human labels than the previous works. ", "belong_id": "BkePneStwH"}, {"uid": "ryg6Ve_tuB", "paper_title": "Power up! Robust Graph Convolutional Network based on Graph Powering", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a new architecture for graph convolutional network based on graph powering operation which generates a new graph based on the shortest distance between pair of nodes.  Its main motivation is to overcome the dominance of the first eigenvector in the existing GCN architectures based on the graph Laplacian operator. The theoretical evidence for the robustness is provided based on the signal-to-noise (SNR) ratio of the simplified stochastic block model (SBM). Two versions of the algorithms are proposed, namely the robust graph convolutional network (r-GCN) and variable power network (VPN). First, r-GCN is based on augmenting the graphs with graph powering operation. Next, VPN replaces the adjacency matrix of the graph convolutional operator by the newly proposed variable power operator. An additional sparsification scheme is proposed since the graph powering operation densifies the original graph.  \n\nOverall, I like how the paper addresses the weakness of the existing graph Laplacian operators (dominance of the first eigenvector) and proposed a new method with theoretical justifications. Experiments were conducted thoroughly and results look great in the presented datasets. However, I also have concerns about the paper that I feel necessary to be resolved. \n\nMost importantly, the concept of 'robustness' in GCN seems to be inconsistent throughout the paper.  Namely, the meaning of robustness in the neural network (adversarial robustness) and the SBM literature (spectral robustness) are different. This point is crucial since the paper use the spectral robustness for justification of the method, yet experiments are done on the adversarial attacks. More specifically, adversarial training methods for neural networks, e.g., adversarial attack methods [1] considered in the paper, typically make the loss function (or output of network) more persistent against the small perturbation of inputs. On the other side, the robustness for SBM models, e.g., Theorem 3 in the paper, cares more about the preservation of the original input characteristics. For illustration, an invertible neural network [2] is not necessarily robust to adversarial attacks (the first meaning of robustness) but preserves all the input characteristics (the second meaning of robustness). \n\nI also hope the paper could have done the experiments on more datasets since there exists some evidence on the unreliability of evaluations on citation networks [3].  However, I do not think this point is critical since the paper did a great job of evaluating the robustness in various aspects and they all show consistent improvement.  \n\nMinor questions and suggestions: \n- The acronyms are slightly confusing to understand at first sight, since they first appear at the equations without any information on what the letters stand for.  Something like a 'variable power network (VPN)' would make the paper more pleasant to read.\n- In the r-GCN framework, there might be an edge case where the powered graph is almost identical to another graph. Would there be any justification for avoiding this?\n- In the r-GCN framework, the terminology distillation is slightly confusing. Was this choice of word used for making a connection to the knowledge distillation [4]? How is the knowledge distilled between graphs? \n\nReferences\n[1] Bojchevski and Gunnemann. Adversarial attacks on node embeddings via graph poisoning. ICML 2019\n[2] Jacobsen et al., i-RevNet: Deep Invertible Networks. ICLR 2018 \n[3] Shchur et al., Pitfalls of Graph Neural Network Evaluation, Arxiv 2018\n[4] Hinton et al., Distilling the Knowledge in a Neural Network, Arxiv 2015", "belong_id": "BkxDxJHFDr"}, {"uid": "H1gRE3E6tH", "paper_title": "Power up! Robust Graph Convolutional Network based on Graph Powering", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors study the classic GCN and proposed the new convolution operator with wider spatial scope and robust properties. The proposed models could improve the accuracy in both benign and evasion setting on synthetic, ie., SBM dataset and real world benchmark graphs. However, I have the following questions for the authors:\n\n1. In the paper, compared to the classic GCN, the authors replace the adjacent matrix $A$ with the proposed variable power operator. However, the proposed variable power operator is very similar to k-th order polynomials of the Laplacian, which has been fully discussed in [1]. Could you distinguish the differences between the proposed variable power operator and k-th order polynomials of the Laplacian? And as the authors proposed to use high-order matrix  some recent models which also explores high-order matrix such as [2, 3] may also need to be selected as baseline methods. \n\n2. As it is clearly defined in [4,5], all the five attack methods adopted in the paper are poisoning (training time) attacks methods. However, the proposed models are claimed to defense against evasion (testing) attacks. Why choose the poisoning attacks methods here? The experiment with the evasion attack method [6] is suggested to be added. \n\n3. When the proposed models are applied to other kind of graph data, ie., social network, according to the small world theory, the 'variable power adjacency matrix' would be very dense when $r>2$ with 2-layer GCN. The efficiency of the proposed might be an issue. Is it possible to add one experiment with demonstrating the running time on the real world social network?\n\n[1] Defferrard, Michael, Xavier Bresson, and Pierre Vandergheynst. 'Convolutional neural networks on graphs with fast localized spectral filtering.' Advances in neural information processing systems. 2016.\n[2]Wu, Felix, et al. 'Simplifying graph convolutional networks.' International Conference on Machine Learning. 2019.\n[3] Abu-El-Haija, Sami, et al. 'Mixhop: Higher-order graph convolution architectures via sparsified neighborhood mixing.'  International Conference on Machine Learning. 2019.\n[4]Zugner, Daniel, and Stephan Gunnemann. 'Adversarial attacks on graph neural networks via meta learning.' In ICLR 2019.\n[5]Bojchevski, Aleksandar, and Stephan Gunnemann. 'Adversarial Attacks on Node Embeddings via Graph Poisoning.' International Conference on Machine Learning. 2019.\n[6] Dai, Hanjun, et al. 'Adversarial attack on graph structured data.' International Conference on Machine Learning. 2018.\n\n\n", "belong_id": "BkxDxJHFDr"}, {"uid": "BkxQEAZ0YB", "paper_title": "Power up! Robust Graph Convolutional Network based on Graph Powering", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a graph convolutional operator based on graph powering and applies it to GCN architecture to improve the performance and robustness. This work is mainly motivated by the paper (Graph powering and spectral robustness, Abbe et al., 2018). The authors introduce the graph powering to graph convolution neural network domain to replace the original Laplacian operator. They further propose a graph sparsification/pruning strategy on the powered adjacency matrices in order to reduce the complexity and increase the robustness against adversarial attacks. They also provide theoretical analysis to prove that the proposed powering operator and subsequent methods have some spectral properties and theoretical feasibility. However, some conclusions are limited to the ideal situations or seem subjective. Extensive experiments are conducted to show better or comparable performance in both benign and adversarial situations. \n\nThere are some concerns that need to be addressed or clarified:\nA major concern is that the theoretical analyses in this paper are limited to graphs sampled from the SBM model. It is unclear how these analyses can be generalized to real graphs. Furthermore, the theorem 3 and proposition 5 are even limited to SBM model with 2 communities, which makes the analyses less convincing. \n\nSome of the arguments in the paper might be imprecise. For example, in Section 1.1, when discuss why not graph Laplacian?, a small spatial scope is claimed to be problematic. Although, it is correct for the GCN (Kipf & Welling, 2017), the powered Laplacian (mentioned earlier in the same section) does have a broad spatial scope.      \n\nIt would be better if the authors could provide more details about the sparsification. Specifically, how to choose the threshold (adaptively). \n\nIn the performance part of Section 4.2, the improvement of the performance by replacing Laplacian with VPN is marginal (compared with the original GCN). Furthermore, the performance of VPN is close to or sometimes worse than the baseline RGCN. \n\nSuggestions:\nIn the Informative and robust low-frequency spectral signal part of Section 4.3, it would be better if the authors can clarify the experiments setting. Is it using the low-frequency part (first few eigenvectors) to recover the signal and then using the recovered signal to perform the classification task? The titles of Figure 7 and Figure 8 are a little bit confusing. \n\n\nSome minor problems:\nThere are many typos such as: with the presence of absence of edges, normalizating, asymptotoic, benigh, ajdacency, sensitve, adajacent, one of the network, etc.", "belong_id": "BkxDxJHFDr"}, {"uid": "ryed1wPvKH", "paper_title": "Neural networks with motivation", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper builds a model of motivation-dependent learning.  A motivation channel is provided as an additional input to and RL-based learning system (essentially concatenated to state information), similar to goal-conditioned approaches (as the authors mention).  The motivational variables evolve according to their own rules, and are designed/interpreted as biological motivations such as water, food, sleep and work.  While the narrative is interesting, I lean towards reject as I believe it failed to deliver on what it promised.\n\nIn the first experiment, the satisfaction of these motivations are mapped onto a 4-room setting, where being in each room satisfies a motivation.  The choice to map the four rooms to biological drives is cute, but possibly confusing/misleading since this navigation problem really has nothing to do with these biological drives. A claim is that by providing the motivation as input to the policy, it is more robustly (across seeds) able to learn the 'migration' (i.e. cycling) behavior among the rooms.  In a second example, a similar problem is solved involving navigation on a graph.\n\nThe final, most substantial example, is a policy trained to solve a simple, abstract version of a behavioral task. In this setting, a motivation channel was again used.  However, the motivation channel value is now fixed to one of two discrete values, essentially meaning it is simply a task-label variable, a paradigm that has already been applied in the context of simple models of neuroscience tasks, e.g. see Song et al. 2017 'Reward-based training of recurrent neural networks for cognitive and value-based tasks'.  \n\nThere is a bit of a mixed framing overall as to whether it is being claimed that the 'motivation' being passed as an input is a fundamental contribution to AI/RL (I think it is not), versus the computational modeling of biological motivation.  I think the people qualified to judge whether the computational model is a worthwhile model of motivation specifically are probably a narrower set of computational neuroscientists.  I do think there is value in the kind of computational modeling performed, involving establishing a relationship between training a neural network to solve a behavioral task and comparing this with real neural data.  This paradigm already becoming increasingly popular within computational neuroscience.  However, while I find the results slightly interesting, but not very significant, as someone interested in the biology of motivation, I question whether the nature of these contributions would be of broad interest at this venue.  \n\nMore fundamentally, I don't believe there is a meaningful ML/AI/RL contribution, and I have some issues with the presentation of the first two examples.  While I do like the narrative inspiring these problems, I find the implementations of the problems too simplified to really be meaningfully related to their inspiration (in terms of motivated behaviors).  Rather than really model motivation as part of the policy architecture, the authors have proposed a solution to modeling motivation that makes motivation a feature of the environment.  Essentially, the reward provided by the environment depends on an extra latent variable and by hiding this (in the cases where the policy does not see motivation inputs), it is quite likely that it becomes too difficult for the value function to predict what is happening (the environment has become partially observed).  This seems less a setting where motivation channels solve a problem, and more just an example of an environment that has more complex rules for generating rewards being more challenging to learn about, especially if latent variables are not available to the value function.  Critically, it has not been shown that motivational systems are useful for artificial agents, rather the tasks themselves have been designed to attempt to be models of biological motivation.  \n\nPersonally, I am interested in motivated behaviors and think that future AI developments should take note of this field, but again, the present work does not provide actionable insights into implementing an artificial motivation system.  At the same time, this work does not provide interesting enough neurobiological results for those to stand on their own either.\n\nMinor clarification:\n\n'trained to perform in realistic tasks' -- the task is very simple.  I would consider this a fairly abstract model of the task.  \n", "belong_id": "BJlJVCEYDB"}, {"uid": "B1gtEtq2YH", "paper_title": "Neural networks with motivation", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors investigate mechanisms underlying action selection in artificial agents and mice. To achieve this goal, they use RL to train neural networks to choose actions that maximize their temporally discounted sum of future rewards. Importantly, these rewards depend on a motivation factor that is itself a function of time and action; this motivation factor is the key difference between the authors' approach and 'vanilla' RL. In simple tasks, the RL agent learns effective strategies (i.e., migrating between rooms in Fig. 1, and minimizing path lengths for the vehicle routing problem in Fig. 5). \n\nThe authors then apply their model to a task in which the agent is presented with sound cues. Depending on the trial block, the reward for the given cue is either zero, positive, or negative; the authors suggest that these varying reward values correspond to varying motivational states. In this setting, the model learns to have two populations of units; each selective to either positive or negative rewards. Recurrent excitation within populations and mutual inhibition between populations define the learned dynamics.\n\nFinally, the authors train mice on this same task, and record from neurons in area VP. Those neurons show a similar structure to the RNN: subpopulations of neurons respond to either positive or negative rewards.\n\nFirst, I'd like to thank the authors for the excellent clarity of this paper. It was very clear, and interesting to read.  \n\nI have some suggestions for how to deepen the connection between the model and the experiment, and some concerns about the necessity of the motivation framework to the Pavlovian task:\n\n1) The authors make the prediction that neurons in VP should show (functional) connectivity matching that learned by their model. This could be tested in their data. If that prediction is true, then one should see positive noise correlations for neuron pairs of the same preference (i.e., within the same pool, defined by spiking more for positive, or for negative rewards), and negative noise correlations for pairs of neurons with different preferences (i.e., one neuron in each pool).\n\n2) A recent preprint by Sederberg and Nemenman (doi: https://doi.org/10.1101/779223) argued against over-interpreting the stimulus selectivity of neurons in recurrent circuits. They showed that, even in randomly connected (untrained) networks: a) neurons show either positive or negative selectivity; and b) neuron pairs with selectivity for the same stimulus (or task) feature tend to excite each other, and neuron pairs with opposite selectivity tend to inhibit each other.  Given that finding, I wonder how compelling is the match between the mouse data and the RL agent (Figs. 6 and 7): could randomly-connected untrained networks show similar phenomena as in the mouse (Fig. 6)?\n\nI'm not asking if the untrained network can duplicate all the details of the trained one in Fig. 7. Just whether the mouse data could be recapitulated by a simpler (no training) model.\n\n3) For the Pavlovian conditioning in the RL agent, I'm not sure I'd describe this as changing motivation. It seems instead that the (external) reward contingency really changes between states. So the fact that the same network can make predictions in both cases seems more like metalearning than motivation-based action selection. For this reason, it's hard for me to connect the two halves of the paper: the first half has nice ideas on motivation-based action selection, while the second one has no apparent action selection, and hence no mechanism for the agent's motivation to matter.\n", "belong_id": "BJlJVCEYDB"}, {"uid": "rJgOwFa1oB", "paper_title": "Neural networks with motivation", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a computational model of motivation for Q learning and relates it to biological models of motivation. Motivation is presented to the agent as a component of its inputs, and is encoded in a vectorised reward function where each component of the reward is weighted. This approach is explored in three domains: a modified four-room domain where each room represents a different reward in the reward vector, a route planning problem, and a pavlovian conditioning example where neuronal activations are compared to mice undergoing a similar conditioning.\n\nReview Summary:\nI am uncertain of the neuroscientific contributions of this paper. From a machine learning perspective, this paper has insufficient details to assess both the experimental contributions and proposed formulation of motivation. It is unclear from the discussion of biological forms of motivation, and from the experimental elaboration of these ideas, that the proposed model of motivation is a novel contribution. For these reasons, I suggest a reject.\n\nThe Four Rooms Experiment:\n\nIn the four-rooms problem, the agent is provided with a one-hot encoding representing which cell it the agent is located in within the grid-world. The reward given to the agent is a combination of the reward signal from the environment (a one-hot vector where the activation is dependent on the room occupied by the agent) and the motivation vector, which is a weighting of the rooms. One agent is given access to the weighting vector mu in its state vector: the motivation is concatenated to the position, encoding the weighting of the rooms at any given time-step. The non-motivated agent does not have access to mu in its state, although its reward is weighted as the motivated agents is. The issue with this example is that the non-motivated agent does not have access to the information required to learn a value-function suitable to solve this problem. By not giving the motivation vector to non-motivated agent, the problem has become a partially observable problem, and the comparison is now between a partially observable and fully observable setting, rather than a commentary on the difference between learning with and without motivation.\n\nIn places, the claims made go beyond the results presented. How do we know that the non-motivated network is engaging in a 'non-motivated delay binge'? We certainly can see that the agent acquires an average reward of 1, but it is not evident from this detail alone that the agent is engaging in the behaviour that the paper claims. \n\nMoreover, the network was trained 41 times for different values of the motivation parameter theta. Counting out the points in figure 2, it would suggest that the sweep was over 41 values of theta, which leaves me wondering if the results represent a single independent trial, or whether the results are averaged over multiple trials. Looking at the top-right hand corner I see a single yellow dot (non-motivated agent) presented in line with blue (motivated agent) suggesting that the point is possibly an outlier. Given this outlier, Im led believe that the graph represents a single independent trial. A single trial is insufficient to draw conclusions about the behaviour of an agent. \n\nThe Path Routing Experiment:\n\nIn the second experiment, where a population of agents is presented in fig 5, it is claimed that on 82% of the trials, the agent was able to find the shortest path. Looking at the figure itself, at the final depicted iteration, all of the points are presented in a different colour and labelled shortest path. The graph suggests that 100% of the agents found the shortest path. The claim is made that for the remaining 18% of the agents, the agents found close to the shortest patha point not evident in the figures presented.\n\n\nPavlovian Conditioning Experiment:\n\nIn the third experiment, shouldnt Q(s) be V(s)? In this setting, the agent is not learning the value of a state action pair, but rather the value of a state. Moreover, the value is described as Q(t), where t is the time-step in the trial; however, elsewhere in the text it is mentioned that the state is not simply t, but contains also the motivation value mu.  \n\nThe third experiment does not have enough detail to interpret the results. It is unclear how many trials there were for both of the prediction settings. It is unclear whether the problem described is a continuing problem or a terminating prediction problemi.e., whether after the conditioned stimulus and unconditioned stimulus are presented to the agent, does the time-step (and thus the state) reset to 0, or does time continue incrementing? If it is a terminating prediction problem, it is unclear whether the conditioned stimulus and unconditioned stimulus were delivered on the same time-steps for each independent trial. If I am interpreting the state-construction correctly, the state is incrementing by one on each time-step; this problem is effectively a Markov Reward Process where the agent transitions from one state to the next until time stops with no ability to transition to previous states.\n\nIn both the terminating and continuing cases, the choice of inputs is unusual. What was the motivation for using the time-step as part of the state construction?\n\nHow is the conditioned stimulus formulated in this setting? It is mentioned that it is a function of time, but there are no additional details.\n\nFrom reading the text, it is unclear whether fig 7b/c presents activations over multiple independent trials or a single trial.\n\nGeneral Thoughts on Framing:\n\nThis paper introduces non-standard terms without defining them first. For example, TD error is introduced as Reward Prediction Error, or RPE: a term that is not typically used in the Reinforcement Learning literature. To my understanding, there is a hypothesis about RPE in the brain in the cognitive science community; however, the connection between this idea in the cognitive science literature and its relation to RL methods is not immediately clear.\n\nTemporal Difference learning is incorrectly referred to as 'Time Difference' learning (pg 2). \n\nNotes on technical details:\n\n- The discounting function gamma should be 0<= gamma <=1, rather than just <=1.\n\n- discounting not only prevents the sum of future rewards from diverging, but also plays an important role in determining the behaviour of an agent---i.e., the preference for short-term versus long-term rewards.\n\n- pg 2 'the motivation is a slowly changing variable, that is not affected substantially by an average action' -- it is not clear from the context what an average action is. \n\n- Why is the reward r(s|a), as opposed to r(s,a)?\n\nNotes on paper structure:\n\n- There are some odd choices in the structure of this paper. For instance, the second section---before the mathematical framing of the paper has been presented---is the results section. \n\n- In some sentences, citations are added where no claim is being made; it is not clear what the relevance of the citation is, or what the citation is supporting. E.g., We chose to use a recurrent neural network (RNN) as a basis for our model following with a citation for Sutton & Barto, 1987.\n\n- In some sentences, citations are not added where substantial claims are being made. E.g, The recurrent network structure in this Pavlovian conditioning is compatible with the conventional models of working memory. This claim is made, but it is never made clear what the conventional computational models of working memory are, or how they fit into the computational approaches proposed.\n\n- Unfortunately, a number of readers in the machine learning community might be unfamiliar with pavlovian conditioning and classical conditioning. Taking the time to unpack these ideas and contextualise them for the audience might help readers understand the paper and its relevance.\n\n- Figure 7B may benefit from displaying not just the predicted values V(s), but a plot of the prediction over time in comparison to the true expected return.\n", "belong_id": "BJlJVCEYDB"}, {"uid": "rJx9Am1OYS", "paper_title": "Closed loop deep Bayesian inversion:  Uncertainty driven acquisition for fast MRI", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an active data acquisition framework for magnectic resonance imaging. A generative adversarial network is used to estimate the posterior distribution of the latent MRI image in a closed-loop and greedy manner where the uncertainty of the posterior image is used to guide the process.\n\nThe paper is well written and easy to follow. The main contribution seems to be the combination of deep Bayesian inversion in Adler & Oktem, 2018 with an uncertainty driven sampling framework. Using uncertainty to drive data acquisition and exploration is not a new idea; the concept has been applied to reinforcement learning, active learning, Bayesian optimisation, as instantiations of a broad class of methods in experimental design. The experimental results suggest that the technique can reduce the amount of time required to obtain good quality images from MRI scans which can potentially have a big financial impact. The technique is compared to several variants of compressed sensing approaches demonstrating superior performance. \n\nMy main concerns with the paper are:\n\n1. The key idea of using uncertainty to guide sampling was also the main concept in Zhang et al. 2019. This submitted paper highlights differences in the models but does not provide an experimental comparison. Since both papers share the same concepts, this reviewer considers that a comparison is critical.\n\n2. Deep Bayesian inversion approximates the posterior distribution by minimising the Wasserstein distance between the posterior  and a parametrised generator. I find the idea potentially powerful, with the advantage of learning a generative model as well, but wonder how this compares in theory and in practice to simpler stochastic variational inference and modern Hamiltonian MCMC. The min-max formulation is notoriously difficult to optmise and might lead to many local optima and instabilities.\n\n3. Given the complexity of learning GANs and the sensitivity to initialization, results should contain more information such as the std of the MSE for several runs of the algorithm.    \n", "belong_id": "BJlPOlBKDB"}, {"uid": "HkxcSGnntB", "paper_title": "Closed loop deep Bayesian inversion:  Uncertainty driven acquisition for fast MRI", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an uncertainty driven acquisition for MRI reconstruction. Contrary to most previous approaches (which try to get best reconstruction for a fixed sampling pattern) the method incorporates an adaptive, on-the-fly masking building (which is similar in spirit to Zhang at al. 2019). The measurements to acquire are selected based on variance/uncertainty estimates coming from a conditional GAN model. This is mostly an 'application' paper that is evaluated on one dataset.\n\nStrengths:\n- The paper studies an interesting problem of adaptive MRI reconstruction\n- The review of MRI reconstruction techniques is well scoped\n\nWeaknesses:\n- The evaluation is rather limited and performed on one, proprietary, relatively small sized dataset\n- Some simple baselines might be missing\n\n\nI like the idea of adaptive sampling in MRI. However, I'd slightly lean towards rejection of the paper. My main concerns are as follows:\n\nThe presentation of the paper could be improved. At the moment, the Theory section describes background information, related work and problem definition as well as the contribution of the paper. Maybe braking the section into related work, background and methodology (where the main contribution is presented) sections would improve the paper readability. \n\nThe paper uses a conditional GAN model (with a discriminator from Adler & Oktem, 2018 and a generator that is based on Schlemper et al. 2018). Making the methodological contribution to be rather limited.  The main difference w.r.t. the previous papers seem to be the last paragraph of  section 2.2 - the empirical variance estimation is performed in Fourier space. \n\nA simple baseline to compare might be to train a Unet-like model (e. g. Schlemper et al 2018) with a Gaussian observation model (outputting a mean and a variance per each pixel) and train it to minimize Gaussian NLL. At the test time, one could simply sample from the Gaussian model instead of taking just the argmax of the output. It might be the case that the assumption of gaussian image might be too simplistic, however, it would be interesting to show it experimentally. Note that when sampling from such model the empirical variance estimation could be performed is the Fourier space too.\n\nThe experimental evaluation is rather limited and the dataset used in the experimental section is small. Adding another dataset would make the paper stronger.\n\nOther comments:\n\nThere is a mention on training dataset and testing dataset -- there is no mention on validation set. How were the hyperparamenters of the conditional GAN selected?\n\nAs acknowledged by the authors, this paper bears several similarities with the work of Zhang at al. 2019. However, the approach is not compared to Zhang et al. Including this comparison would make the paper stronger.\n\nIt is interesting to see that CLUDAS outperforms CLOMDAS in terms of SSIM. If I understand this part properly, CLOMDAS uses ground truth image to estimate MSE. Is it expected that CLUDAS would outperform CLOMDAS? \n\nSection 5, Adaptive vs. fixed mask: 'We also have a simple generalization bound of the obtained mask, relaying on a simple application of Hoeffding's inequality.' Could the authors add a citation or explain this part in more detail?\n\n\nSome typos:\n'...we aim make a series...'\n'.. define an closed-loop...'\n'We choose adopt a greedy'\n'... we we found that...' \n", "belong_id": "BJlPOlBKDB"}, {"uid": "BylJ7Uoh9r", "paper_title": "Closed loop deep Bayesian inversion:  Uncertainty driven acquisition for fast MRI", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper is quite well written and the idea is novel. However, the results are rather weak. The authors present a method to perform adaptive MR compressed sensing, i.e. decide online which readout to sample next. They compare it to an offline learning method where one sampling pattern is optimized for a whole training set, then applied to test data. The offline method performs better in terms of MSE, which is the loss it was trained for, meaning that the authors have not demonstrated a gain in adapting the sampling pattern to individual scans.\n\nThe primary concern with the paper is not with the authors contribution, but with serious flaws in [Adler 2018] that unfortunately snowball into this one. While the authors in [Adler 2018] do acknowledge issues with learning a variance, they misdiagnose the problem as mode collapse. Mode collapse is an optimization problem, where the training set contains variability but the generator fails to learn it due to the lack of an encoder. That is not the case here: all the variability of the training set is encapsulated in y, and for each y the target empirical posterior distribution is a Dirac. This is very similar to the calibration problem in classification [1], where classifiers become overconfident because they are trained to always output 0 or 1. If the generator does not learn a Dirac, it can only be because of regularization (either explicit or implicit in the model architecture) or optimization failure (either involuntary or voluntary with early stopping.) Tweaking the loss as advocated in [Adler 2018] does not fundamentally change the problem as long as the loss is minimal at the target empirical posterior. It may change the dynamic behavior and result in posteriors with more variance when combined with early stopping, but those variances are not calibrated, i.e. they have not been trained to match the variances of the true continuous posteriors. In order to learn the variances, one would have to either provide multiple posterior samples for each y during training (not practical in this case,) or perform some kind of calibration on the validation dataset as in [1], i.e. learn the mean and variance from different data, which effectively uses the networks interpolation properties as a proxy for true random sampling.\n\nHowever this flaw does not invalidate the practical approach developed in the paper, but it seriously undermines its qualifications as a rigorous, principled, Bayesian approach. It also makes the reporting of posterior variances as final quality metrics pretty much useless since they are not interpretable: does lower variance mean that the generator got better at estimating the missing information, or that it got worse at estimating the true posterior variance? I would suggest to at least remove the variances highlights from Table 1 and Table 4, and maybe scrap the data altogether. The paragraph on posterior estimation should also be updated to represent the whole scope of the problem. \n\nSection 2: Theory: suggest to remove Without loss of generality. Due to the known issues with variance estimation, having p(y | x) as a density instead of a Dirac could very well change the behavior of the generator.\n\nSection 2.1 Adaptive masks. The whole first paragraph is somewhat misleading and should be revised. Real-time reconstruction is indeed possible without deep learning, see [2] for example. Furthermore, real-time reconstruction is not nearly fast enough for adaptive sampling. Real-time reconstruction means that reconstructing an image is at least as fast as scanning the whole image, i.e. in the order of 0.1 to 1s., but for adaptive sampling one must reconstruct at least as fast as the time between two successive readouts, i.e. in the order of 1 to 10 ms. Both [Jin 2019] and [Zhang 2019] only showed single-coil offline simulations with no indications of the reconstruction time and so do the authors.\n\nFigure 1: I must be missing something here. How can the image-domain and Fourier-domain figures be different? The Fourier transform being orthogonal, norms and variances should be the same in both domains.\n\n[1] C. Guo, G. Pleiss, Y. Sun and K. Q. Weinberger, On Calibration of Modern Neural Networks, ICML 2017 70:1321-1330.\n[2] M. Uecker, S. Zhang, and J. Frahm, Nonlinear Inverse Reconstruction for Real-Time MRI of the Human Heart Using Undersampled Radial FLASH, MRM 63:1456-1462 (2010).\n", "belong_id": "BJlPOlBKDB"}, {"uid": "SJxsl2aa9r", "paper_title": "Closed loop deep Bayesian inversion:  Uncertainty driven acquisition for fast MRI", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper describes a method for accelerating MRI scans by proposing lines in k-space to acquire next. The proposals are based on posterior uncertainty estimates obtained from GAN-based reconstructions from parts of the k-space acquired thus far. The authors address an interesting and important problem of speeding up MRI scans and thus improving the subject's experience. The proposed method achieves better posterior uncertainty and SSIM scores than competing methods.\n\n\n\nWhile the paper considers an important problem and takes a novel approach to solve it (using a GAN generative model to estimate uncertainty), I found that it may be particularly inaccessible to non-experts in the field of MRI image processing. Furthermore, several important methodology-related questions remain unanswered in the paper; and the experiments offered in the paper are insufficient for convincingly arguing the authors claims.\n\nSpecifically, it is unclear whether GANs with their mode dropping behaviour are the right model choice for proposing reconstructions - they are likely to drop modes and by extension - yield overconfident uncertainty estimates. This can be expected to be particularly problematic for scans of images that differ from the training distribution, and is exacerbated by the fact that authors train the model only on scans from healthy subjects. Furthermore, because of the the GANs propensity to drop modes, it is also unclear whether the posterior variance numbers reported in the paper are directly comparable between the methods.\n\nThe method used for obtaining the uncertainty estimates from GAN samples implicitly makes the assumption that reconstructions follow a Guassian distribution with a diagonal covariance. This assumption is also made in a competing method of Zhang et. al (2019) that the authors do not compare against, and claim to improve upon methodologically (i.e. the authors state that the method of Zhang et. al (2019) cannot be used to produce uncertainty estimates in Fourier space). I am not convinced that the authors claims about the method differences are sufficiently substantiated (see more under major comments). And because the methods bear significant similarity to each other, an experimental comparison - which is currently missing - should be carried out (on open datasets that ideally include non-healthy subjects).\n\nFinally, the paper teases fast(er) MRI in the title, but doesnt touch on this topic in the text. This aspect should of the authors contribution be discussed at length, in particular comparing the Cartesian sampling strategy adopted by the authors to other strategies, as well as evaluating the feasibility of implementing the adaptive sampling strategy in an actual scanner (e.g. can the network be ran fast enough)?\n\n\n==================\nMajor comments:\n==================\n\n1) In the proposed method the authors employ a procedure in which the currently sampled parts y of the k-space are fed to a generator network to obtain n_s reconstructions. These n_s sampled reconstructions are then averaged to obtain the empirical mean and variance, with the latter being used for estimating uncertainty. This procedure is potentially problematic for several reasons:\n\n  a) First, taking the empirical mean and variance of the samples is in fact equivalent to assuming that the reconstructed image follows a Guassian distribution with a diagonal covariance. This is the same assumption the authors argue is not realistic when discussing the work of Zhang et al. (2019) in the end of Section 1. \n\n  b) In case of GANs, which can model multi-modal distributions this uncertainty estimation is even more problematic in cases when the samples originate from different modes. What do the mean and variance represent then?\n\n  c) As the authors highlight in the discussion section of the paper in the paper, GANs are prone to mode collapse. This is also potentially problematic for their estimator - in case of mode collapse their method would underestimate uncertainty. The fact that authors are able to use only two sampled reconstructions to estimate the mean and variance with acceptable accuracy is consistent with the occurrence of mode collapse in their generator. Furthermore, because mode collapse may occur in the authors model, it is unsurprising that their method yields the smallest posterior variances in Table 1. The authors should provide evidence that mode collapse either does not occur or would not affect these numbers.\n\n  d) Finally, the authors use the empirical mean and sampled reconstructions to obtain the empirical mean and variance in Fourier (k-) space. Specifically, they argue that This feature is specific to generative models, as getting samples from P_{X|y_\\omega} allows to transform these to a different domain [...] this is not possible with methods that only provide point-wise estimates of the mean and the variance in image space, such as the one used by Zhang et. al (2019). I dont think this is true - Fourier transform is a linear transformation, thus given a mean and a variance in image space it is possible to deduce analytically what the mean and covariance in Fourier space would be. This should be elaborated in the text, and the comparison to Zhang et. al (2019) should thus be extended further.\n\n\n2) The proposed method, CLUDAS, was evaluated against existing methods on a single proprietary dataset consisting of only 100 images from healthy individuals. This is potentially problematic, for several reasons:\n\n  a) Using a proprietary dataset doesnt allow follow-up works to compare against the authors method; or for comparing CLUDAS to \n existing methods not considered in the paper; the methods should additionally be compared on a public dataset  and\n\n  b) Applying the method only to a single (small) dataset does not allow for reasoning on how the method behaves in different data regimes. I strongly encourage the authors to apply their method on public datasets, for example on data used in Zhang et. al (2019) - this would then allow for comparing the two methods despite not having access to an implementation of Zhang et. al (2019).\n\n  c) Since the data is used to train the GAN model is obtained from healthy individuals, its unclear whether it can be used to acquire data from subject that may potentially have aberrations in their scans - the GAN model would be expected to produce low uncertainty estimates for regions where these aberrations would lie and not propose acquiring parts of the k-space that could be used to resolve these aberrations. For similar reasons using a GAN that potentially drops modes (e.g. scans of unhealthy individuals, for example because they are less common in the training data) is also problematic. The authors should consider evaluating their method on a dataset that contains non-healthy subjects, and investigate the performance of the method when its evaluated on healthy subjects, but tested on unhealthy ones.\n\n3) The title of the paper ([...] for fast MRI) suggests that the authors aim to accelerate the MRI data acquisition process.\n\n  a) Yet they chose the work with the Carterian sampling (i.e. sampling lines in the k-space parallel to the x-axis), which arguably requires larger sections of the k-space to be sampled before a high-quality reconstruction can be obtained (e.g. see http://mriquestions.com/k-space-trajectories.html). Providing information on how this choice influences the speed of data acquisition (and thus subjects comfort) is important in order to assess the applicability of the authors method to real world scenarios. This information should be provided.\n\n  b) The paper does not actually describe how MRI is sped up. Given that the acquisition budget (number of lines acquired in k-space) is fixed, and assuming that time per line is constant, it is unclear where the speed up comes in. More generally, the speed claims / aspect of the proposed method should be discussed in more detail - how is speed measured and evaluated? How does it compare to non-Cartesian sampling approaches.\n\n  c) Finally, its unclear whether neural network inference can be made fast enough to allow for a real world application of the adaptive CLUDAS sampling - can the data be transferred fast enough from the scanner to do inference and propose the next line to scan without causing delays in the scanning process? This should be discussed in the paper.\n\n4) Currently, the paper places a strong expectation of knowing about MRI and being familiar with MRI-specific terminology on the reader. This makes the work substantially less accessible to a wide audience with machine learning expertise as the common denominator. The authors should take steps towards making the text more accessible to non-(MRI) expert audience, for example by introducing some of the basic knowledge (e.g. the data acquisition and reconstruction processes in MRI) early on, departing from MRI-specific jargon (e.g. k-space, lines in k-space) in favour of ML terminology whenever possible, and taking care to define and possibly illustrate (strongly encouraged) the MRI-specific concepts (e.g. the k-space, lines in k-space, sampling masks). Some specific examples the authors should address follow.\n\n  a) The k-space is defined only in passing as being the frequency domain. Its unclear whether these lines (which correspond to sampling masks) in this domain are parallel to the x-axis. The math (e.g. Equation 1) and Figure 2 suggest that, but its not obvious.\nx is referred to both as 'model parameter' in Section 2 (and Equation 1) as well as the 'ground truth image' later in the same section. This is confusing, especially because in case of GAN model parameter would typically refer parameters of the generator and discriminator.\n\n  b) SSIM is not defined anywhere, but already used in the abstract.\n\n  c) It could be made more clearly why undersampling is required in case of MRI. E.g. how/why does it correlation with patient comfort.\n\n  d) K-space sampling is used already in the introduction, but not really defined.\n\n  e) Its unclear whether sampling, subsampling and undersampling all refer to the same concept or not.\n\n  f) The term sampling mask is already used in the introduction, but not clear what it refers to.\n\n  g) Use of the term innovation to refer v_t, which appears to be a one-hot vector marking the newly added line in k-space.\n\n  h) The use of term sampling decision to refer to v_i.\n\n  i) Unclear what eta in Z ~ eta in Equation 5 refers to - it is not defined. Later in Section 2.2 it is stated that z_i are independent samples form Z. Is this the same as z_i ~ eta? If so, why the second layer of notation?\n\n  k) In Introduction [...] which is not feasible on a real problem without the ground truth available. Unclear what the ground truth refers to, I assume its the ground truth image.\n\n  l) In Section 2.1 data refers to x_i, with i=1,...,m; which I assume are images and thus contain real numbers. Yet Section 3 states As our data are complex [...]. This is confusing - is that different data?\n\n  m) Sampling is used ambiguously in the paper - to refer to sampling in the k-space (e.g. sampling masks, sampling decisions v_1,...,v_n) and to refer to sampling reconstructions from the generator. This should be resolved to improve readability of the paper.\n\n  n) Not entirely clear what k-space pixel-wise variances are. And what is the difference between spatial and pixels-wise variances is (Section 2.2).\n\n\n\n==================\nMinor comments:\n==================\n\n1. In the Introduction the authors argue that metrics such as MSE and SSIM [..] do not align with what clinicians see as valuable., yet use these throughout the paper for evaluating and comparing methods. This decision should be explained.\n\n2. In Section 2 (and beyond) images x are described as belonging to subspace C^p of complex numbers. While this is technically true, them to actually belong to the space of R^p. If so, this should be reflected in the text for the benefit of the readers. Furthermore, I dont think dimensionality p is actually defined anywhere.\n\n3. Compressive / compressed sensing abbreviations CS is defined twice in Section 1.\n\n4. Its not entirely clear why The CS-inspired methods shift the burden from acquisition to reconstruction [...]\n\n5. Incorrect double quotes are used throughout the text (both are right quotes).\n\n6.  In Introduction [...] yielding an estimator which can be used to drive back the whole sampling process in a closed-loop fashion. \nit is unclear what it means to drive back a sampling process. Perhaps back should not be there?\n\n7. In some cases it is unclear what the use of double quotes conveys, e.g.\n\n  a) In Section 2.1 [...] being the full mask [...]\n\n  b) In Section 2 [...] ground truth complete images [...]\n\n  c) In Section 5 [...] these inverse problems depend from each other [...]\n\n  d) Multiple places in Appendix B.\n\n8. The convention of using small letters x, y for data samples / instances and capital letters X, Y for random variables could be made explicit.\n\n9. Section mostly provides background (rather than theory) and could be named accordingly.\n\n10. In Section 2.1, t refers to time. It may be clear if it were instead referred to as the step of the sampling process or something similar - the use of time to refer to some discrete set of actions can be a little confusing.\n\n11. Section 2.1 refers to [...] the online reconstruction speed of DL [...]. This should be explained further - why are deep learning based approaches to reconstruction faster? Does this depend only on having the right hardware accelerators? Also, I dont think the abbreviation DL was introduced.\n\n12. Equation 4 is referred to both as an Equation and as a Problem.\n\n13. In Equation 6 there is a summation over j from v_i. It is my understanding that v_i is a one-hot vector and its unclear what this summation means. Presumably it is the summation over pixels covered by line v_i, but the notation doesnt convey this. It could be nice to also explain the 1D superscript in this equation.\n\n14. In Section 2.2 [...] this is why the approach of (Adler & Oktem, 2018) minimizes over distance for observation in Equation 4 [...]. I couldnt follow the part about minimizing over the distance for an observation. Please consider making this more clear.\n\n15. In Equation 8, what does index i run over?\n\n16. Minor typos / textual issues:\n\n  a) In Introduction [...] of the our estimator [...]\n\n  b) In Introduction [...] and show that even using a few samples [...] -> even when using?\n\n  c) In Section 2.2, after Equation 5 [...] where t After finding the optimal.\n\n  d) Inconsistent use of closed-loop and closed loop.\n\n  e) In multiple places throughout the paper a double space appears to be used instead of a single one.\n\n  f) Section 4.1 as can be in Figure 1\n\n  g) Section 4.3 samples art random\n\n  h) In Section 5 [...] depend from each other should be depend on each other?\n\n  i) In Section 5 [...] we we found that [...]\n\n17. Unclear whats meant by [...] using the aggregated variance as a loss function in Section 2.2\n\n18. Section to refers to i* (integer scalar) as a line. Previously it was v_i (vector).\n\n19. In Section 2.2 the authors state Once the generator has been trained until convergence [...]. The authors optimize a generator in an adversarial fashion. To my knowledge, this training procedure is not guaranteed to converge and would typically oscillate around a stable solution. Could the authors please comment on what they mean by convergence in this case and how they guarantee that the generators they train converge.\n\n20. For the posterior variance results in Table 1 it should be discussed whether all the methods obtain / compute the variances the same way.\n\n21. The data consistency layer (Section 3) should be explained briefly. How does it enforce perfect consistency and whats meant by consistency here?\n\n22. It should be made clear what MSE is calculated between in Figure 1 and Table 1. I assume its between ground truth (all frequencies sampled) and reconstructed (only some frequencies samples) images.\n\n23. Its not entirely clear whats meant by consistency in Section 4.1\n\n24. What do yellow arrows signify in Figure 2?\n\n25. I dont think the abbreviation UQ was defined in Figure 2.\n\n26. The masks in Figure 2 are such that is a certain line was chosen at lower sampling rate (left on the x-axis), it would also be chosen at higher sampling rate (right on the x-axis). This is somewhat unexpected since the budget of lines v_i to be acquired differs between sampling rates. Why is there such consistency?\n\n27. In Table 1, in the leftmost column, the number in brackets (number of posterior samples?) should be defined.\n\n28. In Table 1 and Section 4.3: LBC-M and LBC-U - the -M and -U suffixes should be explained. What are the differences between the methods?\n\n29. In Section 4.3: what are the FE methods?\n\n30. Appendix A.3 is mostly a copy-paste from the main text. Unnecessary duplication?\n\n31. Axes in Figure 4 (Appendix A.2) are not labeled or described.\n\n32. Loss in A.4 (Equation) does not match Equation 5. In the former the discriminator takes three arguments instead of two, and the arguments z1 and z2 are not described.", "belong_id": "BJlPOlBKDB"}, {"uid": "SkgtJzcntH", "paper_title": "Deep Network Classification by Scattering and Homotopy Dictionary Learning", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The work considers a new architecture for artificial neural networks to be used in image classification tasks. The network combines several promising previous research directions into an interesting approach (to the best of my knowledge, the architecture is novel). \n\nIn the first step, the input representation (i.e., image) is processed using deep scattering spectrum. This is an operator proposed by Mallat (2012) and it is known for extracting robust features with properties such as translation invariance and Lipschitz continuity of the operator mapping. As a result, such a representation is likely to be more stable to perturbations and noise in the inputs.\nThe scattering operator of the second and higher orders tends to produce a large number of coefficients. Such a representation is, in general, sparse with many of the higher order coefficients equal to zero. To lower the dimension of the scattering representation, the architecture employs a linear projection operator. For example, one can take principal component analysis and project to a sub-space retaining most of the variation present in the data.\nThe first two blocks (scattering and linear projection) are unsupervised and, thus, kept fixed during learning. \n\nThe third block in the architecture aims at finding a sparse coding dictionary that will take into account instances and labels. It builds a dictionary using a convolutional neural network and finds sparse coding using previous work on dictionary learning (e.g., Donoho & Elad, 2006: Marial et al., 2011; Jiao et al., 2017 etc). The algorithm works by computing the sparse encoding vector (see Eq. 1) in the forward pass and updating the convolutional parameters as well as sparsity controlling hyperparameter in the backward step. To speed up the convergence, the authors rely on homotopy iterated thresholding illustrated in Figure 2.\n\nThe approach is evaluated on ImageNet and empirical results demonstrate that the removal of the sparse encoding block amounts to a significant performance degradation. The results also establish a minor improvement in the accuracy as a result of adding a linear projection matrix (i.e., principal component analysis applied to scattering coefficients). Overall, the network shows promising performance on ImageNet by doing better than AlexNet. The result is not yet 'competitive' with ResNets but it might be worth to pursue this direction of research in the future.\n\nThe work is properly structured with well organized materials from previous work. The clarity, however, could be improved in several places. In particular, the last paragraph in Section 3.1 does not explain the algorithm clearly. The confusing part is how to update the dictionary (i.e., convolutional network) parameters. One might infer from the current materials that first the problem in Eq. (1) is solved to find \\alpha and that solution is fixed. Then, for that setting of the sparse encoding vector one will train the network parameters. Section 3.2 then givens an iterative procedure in Eq. (3) and Figure 2, which suggest that in a forward pass the dictionary representation is computed using some setting of parameters and \\alpha is updated per Eq. (3). Following this, the gradient of the convolutional parameters is computed (\\alpha_{n + 1} is differentiated, which means that the gradient depends on other \\alpha iterates). I believe that this is the crux of the methodological/conceptual contribution and requires proper explanation with proper illustration of the forward-backward pass.\n\nIn Section 3.2 (homotopy iterated thresholding and ALISTA), there is a matrix W which comes ad-hoc. There should be some motivation and gentle introduction. At the moment, it is completely justified to ask why one needs this matrix and why the approach would not work without Proposition 3.1. Please add some discussion and make sure things are properly motivated and gently introduced. This will also place the theoretical contribution in the proper context and strengthen the work.\n\nJust below Figure 2, I fail to follow how the number of layers N relates to the iterative algorithm? Does it mean that you would actually have blocks per each \\alpha_n for some N indices (this again refers to previous comment on clarity)?\n\nCan you please use \\ell_1 or l_1 notation for sparse dictionary coding. The current symbol reads as 1 to the power of 1 and it is very confusing (never seen it before in the context of sparse encodings).", "belong_id": "SJxWS64FwH"}, {"uid": "HJl5HgCpFB", "paper_title": "Deep Network Classification by Scattering and Homotopy Dictionary Learning", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n## Summary\n\nThe paper proposes an interpretable architecture for image classification based on a scattering transform and sparse dictionary learning approach. The scattering transform acts as a pre-trained interpretable feature extractor that does not require data. A sparse dictionary on top of this representation (the scattering coefficients) is learnt to minimize the classification error. The authors cast the dictionary learning as a classical CNN learning approach and implement an efficient solution via homotopy learning (given that some assumptions are fulfilled). The scattering transform approach is not new (as the authors mention in the paper, it was published in Oyallon et al., 2019). The main novelty comes from applying a previously published dictionary learning approach (as the authors mention in the paper, it was published in Jiao et al., 2017) on top to boost the performance. As a second contribution, the authors extend the exponential convergence proof of ISTC (Jiao et al., 2017) and ALISTA (Liu et al., 2019). In the experiments, they show that the proposed architecture, despite its simplicity, outperform AlexNet in the ImageNet classification problem.\n \n## Comments\n\nThe paper is well written and the exposition is clear. The main motivation of the paper is to propose an interpretable architecture with similar performance to black box deep learning architectures. To do so, the authors put together:\n\n- A scattering transform feature extractor: Unlike I am missing something, this is exactly what was previously proposed in (Oyallon et al., 2019). \n- A dictionary learning on top: This seems to be the biggest novelty of the paper. This component allows to boost the performance of the previously proposed architecture. However, this approach has been previously explored in the literature (Mahdizadehaghdam et al. 2018), the authors just apply it on top the extracted features. The justification of the paper lies in that previous dictionary learning approaches did not scale (convergence too slow), and so the authors use a different method recently published in (Jiao et al., 2017). \n\nThis allow the authors to apply the method to bigger datasets ImageNet, and keep the performance above AlexNet. \n\nGeneralizing the convergence results of ALISTA and ISTC is a nice contribution. However, my main concern is with respect the novelty of the rest of the paper. The authors do not propose a substantially different approach, rather they apply the same approach (an scalable  dictionary learning method already published in Jiao et al., 2017) on top of some extracted features (scattering coefficients)  to a different datasets. The problem with accepting the paper is that changing the dataset/dictionary learning method/features to compare with, you get a different paper, and so, in my opinion, the impact of this publication is limited.\n \nAlso, given that the paper main point is the interpretability of the proposed method wrt to black-box deep learning methods, I think the authors should include recent references to the active field of interpretability in the deep neural network community.", "belong_id": "SJxWS64FwH"}, {"uid": "SkeHSjoe9H", "paper_title": "Deep Network Classification by Scattering and Homotopy Dictionary Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a network architecture composed of three interpretable components followed by a simple MLP classifier. It first applies a scattering transform followed by a learned linear projection (to reduce dimensionality). A sparse representation of these coefficients is then obtained using dictionary learning. The projection, dictionary and MLP classifier are jointly trained to minimize the classification loss. Results show that the model outperforms AlexNet on the Imagenet benchmark.\n\nThe paper is well written. The main contribution of the work is to present an architecture with composed of mathematically interpretable components achieving very high empirical performance. I find that these results are very significant. \n\nThe second contribution is to propose a dictionary learning algorithm that uses ISTC and can be trained with gradient descent.  I think that it would be interesting to add an ablation showing the change in performance by changing N (the number of iterations in the ISTC net). Also, it would make sense to run FISTA or ISTA unrolls to see if the benefits of the faster convergence also affect classification performance.\n\nIt would be good to add to Table 1 the number of trainable parameters of each variant.\n\nI find it a bit confusing to refer to the setting in which W is learned as ALISTA, as to me ALISTA implies using analytical W. This is clear later in the text (and makes sense from a computational standpoint). Would be good to clarify it early in the text.\n\nFinally the paper presents a proof of exponential convergence for ALISTA in the noisy case. While this is an interesting result, it is not very closely linked to the main focus of the work. \n", "belong_id": "SJxWS64FwH"}, {"uid": "SkxNbzojYB", "paper_title": "``\"Best-of-Many-Samples\" Distribution Matching", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Best of Many Samples Distribution matching\n\nSummary:\n\nThis paper proposes to a novel VAE-GAN hybrid which, during training, draws multiple samples from the reparameterized latent distribution for each inferred q(z|x), and only backpropagates reconstruction error for the resulting G(z) which has the lowest reconstruction. The authors appear to use the AAE method instead of analytically matching KL(p||q) for enforcing that the latents q(z|x) match the prior. The authors present results on MoG toy datasets, CIFAR-10, and CelebA, and compare  against several other models.\n\nMy take:\n\nThe idea in this paper is moderately interesting, well-founded, has plenty of precedent in the literature (while still being reasonably novel),  but the results present only a minimal improvement (a 5% relative improvement in FID over the baseline model from Rosca et al on CIFAR, especially when including SN [which is not a contribution of this paper]) and come at a substantial compute cost, requiring up to 30 extra samples per batch in order to attain this minimal increase. While I think the idea is interesting, the change in results over Rosca et. al does not seem to justify its increased computational expense (which is also not characterized in sufficient thoroughness). I am pretty borderline on this paper ( about a 5/10) but under the 1-3-6-8 scoring constraint I tend to lean reject because while I like the idea, I do not think the results are significant enough to support its adoption; I think the relative compute and implementation cost limit this methods potential impact. I am keen to discuss this paper with the other reviewers.\n\nNotes:\n\n-The results on the 2D MoG toy datasets are good but are also suspectthe authors state that they use a 32-dimensional latent space, but the original code provided for VEEGAN uses a 2-dimensional latent space. The authors should re-run the experiment for BMS-VAE-GAN  using a 2D latent space (this should be very easy and take less than an hour on a GPU to get several runs in).\n\n-again outperforming by a significant margin (21.8 vs 22.9 FID) This is not a significant margin; this is less than a 5% margin and, at those FID scores, represents an imperceptible change in sample quality.\n\n-The authors seem to suggest that applying spectral norm to the GAN of Rosca et. al. is somehow a contribution (e.g. having ours next to this model in the tables); I would advise against even appearing to suggest this as it is clearly not a contribution.\n\n-Characterize the increase in compute cost. . We use as many samples during training as would fit in GPU memory so that we make the same number of forward/backward passes as other approaches and minimize the computational overhead of sampling multiple samples is a qualitative description; I would like to see this quantitatively described. How do the runtimes differ between your baseline and the T=10 and T=30 runs? If they dont differ, why? Are the authors e.g. reducing the batch size by a factor of 10 or 30 to make this computationally tractable?\n\n-The latent space discriminator D_L should be referred to in section 3; its formal introduction is deferred to later in the paper, hampering the presentation and flow. \n\n-CelebA is not multimodal; it is in fact, highly constrained, and primarily only has textural variation (virtually no pose variation).\n\n-ALI and BiGAN are listed under Hybrid VAE-GANs. These models are not VAE-GAN hybrids. Additionally, this section states that BiGAN builds upon ALI. This is not true, these papers are in fact proposing the same thing and were released at nearly the exact same time.  Do not mischaracterize or incorrectly summarize papers. Please re-read both papers and refer to them correctly.\n\n-Mode collapse (when many points in z map to an unexpectedly small region in G(z)) is a different phenomenon from mode dropping (when many points in x are not represented in G(z), i.e. no point in z maps to a cluster of xs, as is the case if e.g. a celebA model generates frowning and neutral faces but no smiling faces). While these phenomena often co-occur (especially during complete training collapse), they are not the same thing, and this paper conflates them in several places.\n\nMinor:\n\nSection 3, paragraph 2: The GAN (G,DI... Theres a close parenthesis missing here. \n\nSection 3.3: The network is traiend... \n\nPlease thoroughly proofread your paper for typos and grammatical mistakes.\n\n", "belong_id": "S1lk61BtvB"}, {"uid": "B1lD7rTsFB", "paper_title": "``\"Best-of-Many-Samples\" Distribution Matching", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1.  Using Discriminator to estimate the likelihood ratio is a commonly used approach, which was first proposed in [1]. This is also generalized as a reversed KL based GAN in [2] [3]. The authors failed to discuss this with these previous works in Section 3.3 and in Related works.\n\n2. How is the best of many comparing with importance sampling method? I think using importance sampling is the most intuitive baseline.\n\n3. this paper is not well written. L_1/L_2 has never explained throughout this paper, also has typos such as 'taiend'.\n\n\n[1] Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks\n[2] Variational Annealing of GANs: A Langevin Perspective\n[3] Symmetric variational autoencoder and connections to adversarial learning", "belong_id": "S1lk61BtvB"}, {"uid": "r1ejzBAX5r", "paper_title": "``\"Best-of-Many-Samples\" Distribution Matching", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper presents a new objective function for hybrid VAE-GANs. To overcome a number of known issues with VAE-GANs, this work uses multiple samples from the generator network to achieve a high data log-likelihood and low divergence to the latent prior.\nIn the experimental section, the ``'Best-of-Many-Samples' approach is shown to outperform other state-of-the-art methods on CIFAR-10 and a synthetic dataset.\n\nThanks for submitting code with your submission!\n\nCaveat: I'm not an expert in this domain and did my best to review this paper.\n\nQuestions:\n- Considering the smaller gap between -GAN+SN and BMS-VAE-GAN, I was wondering how much of the improvement is due to spectral normalization vs using multiple samples. Did you do an ablation study of BMS-VAE-GAN without SN?\n- I noticed some minor typos in the text. Please fix (3.2 'constrains' -> 'constraints', 3.3 'traiend', 3.3 'unsure' -> 'ensure').", "belong_id": "S1lk61BtvB"}, {"uid": "HJemqGSatB", "paper_title": "Towards Certified Defense for Unrestricted Adversarial Attacks", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a new certified defense strategy that considers unrestricted black box attacks.  The paper provides bounds for the minimum number of queries needed for the attacker to attack the classifier successfully and then the authors prove that they can devise a defender to be robust against that attack. Here are a few points to consider:\n1.\tThe paper is a bit difficult to understand (also not well-structured). The specific contributions are not quite clear with respect to the existing literature (which is reviewed in a sparse manner in the paper). Especially, the novelty of the theory and analysis presented here is a bit difficult to assess. \n2.\t The results are not really validating the points they make in the analysis (for example in part 4.2 they talk about upper and lower bounds for number of queries as a function of , , etc. but they never provide some plots or tables regarding that in the results section).\n4.\tAlso, in Fig 2, it is hard to grasp the performance of the defended vs undefended classifiers with respect to the lower and upper bound that they have computed theoretically in the previous section.\n5.\tThey emphasize on the defensibility and query privacy in the analysis but they do not provide anything in the results section considering them.\n6.\tAs this is primarily a theoretical paper, focusing on simple classifiers is probably okay, but some sort of empirical comparison with other certified defense strategies is necessary. Just claiming that none of the existing methods would work for unrestricted attacks will not work is not sufficient. Some empirical results to show the specific advantages (e.g., at what budget the existing methods start to fail and the proposed method continues to perform well). ", "belong_id": "S1lBVgHYvr"}, {"uid": "r1guL36CFS", "paper_title": "Towards Certified Defense for Unrestricted Adversarial Attacks", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Although this paper's title contains 'certified defense' and 'unrestricted adversarial attack',  what I believe this paper is doing is analyzing the query complexity of query-based black-box attacks under simple linear models such as logistic regressions (or kernelized versions). The authors considered a binary classifier with the additional capability of giving 'no response' when the confidence is low.  In addition, the output of the classifier has to be perturbed by a random Gaussian vector. The authors then define several metrics including defensibility and query privacy to develop the query complexity on the considered model. The authors tested the query performance on two attacks: (1) the sign attack proposed by the authors and (2) the simba attack proposed by Guo et al. \n\nI have several concerns regarding this paper:\n\n1. In my perspective, the title is very misleading and does not properly justify the claims made in this paper. 'Certified defense' usually refers to consistent top-1 prediction of a perturbed data sample under a defined threat model. The paper reads like the authors are actually certifying the defined defensibility metric but without a threat model to certify. In addition, the attack setting is limited to black-box attacks (i.e. zero-order adversary), whereas in certified defense the attack assumption is white-box. \n\n2. It is also very unclear how unrestricted attack plays a role in the studied problem.  In the introduction, the authors' definition of adversarial examples is 'any input is considered a valid adversarial example as long as it induces the classifier to predict a different label than an oracle classifier.' But what is the oracle classifier? How do we justify the credibility of the 'adversarial examples' in the experiments?\n\n3. Only two black-box attacks were compared in this paper, one is the sign attack proposed by the authors, the other is the simba attack proposed by Guo et al. To my knowledge, simba attack paper has not been published at any peer-reviewed venue. In other words, both attacks are not widely recognized attacks or methods from published papers. Therefore, the performance evaluation is not fully justified. Since there are many black-box attacks from published papers, why not do performance analysis on those attacks?\n\n4. Similar to 3, the classifier setting is also uncommon. Although I am happy to see classifiers have the ability to give no-response,  admittedly this type of classifier is rarely used in practice, not to mention the analysis is tied with Gaussian perturbation on the output. The technical contributions can be limited if the main contribution of this paper is characterizing the query complexity (or defensibility) of an uncommon classifier with Gaussian perturbation on the output. I believe providing more insights on how the analysis can be useful to mainstream classifiers are critical and necessary.\n\n***Post-rebuttal comments\nI thank the authors for the response. I hope the comments areuseful for preparing a future version of this work.\n***", "belong_id": "S1lBVgHYvr"}, {"uid": "BJxBz-t7cH", "paper_title": "Towards Certified Defense for Unrestricted Adversarial Attacks", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes adding noise to the output of scoring function to defend from black-box attacks. This topic is actually very interesting so I enjoyed reading the paper, although it is currently only working for logistic regression and Naive Bayes and there are several unclear parts. I have several concerns about this paper, especially the claim of robust towards arbitrary perturbation. \n\n- My main concern is about the assumption of the attacker. Based on the discussions in Section 3, it seems the authors assume that the query complexity of attacker relies on how many queries the attacker needs to recover a w that is close enough to w*. I don't think this is the correct assumption for the current attacks --- given an example, black box attacks are trying to find some x' for each x without trying to recover  or even estimate w. Therefore I wonder why the query complexity can be linked to the complexity of estimating w and is there any further assumption you need to make? \n\nIf the goal is to protect w, then this has been studied in several privacy/security papers and it's a different topic from adversarial attack. So the connection here is important but somehow unclear in the current draft. \n\n- For the experiments, to justify it is robust to attack I think it's important to try on various black-box attacks, including ZOO (Chen et al., 2017), Natural evolution strategy (Ilyas et al., 2018), Nattack (Li et al, 2019). For decision-based black box settings Boundary attack (Brendel et al., 2018) and OPT-attack (Cheng et al., 2019). \n(Not saying you should try all of them, but I feel more than 1 attack is needed to justify the claim). \n\n- Some unclear points that need further clarification: \n\nI feel assuming there's an optimal w* that correctly classifies data is unrealistic. Is is possible to relax this? \n\nCondition 1: I fail to understand how is this related to q (attacker)? This seems only guaranteeing there's a majority mass of w centered at w*. \n\nCondition 2: What is I ? (I didn't see the definition). \n\n- Some related work: \nIn DNN defense there are some related work on adding random noise. In [1], I think they only require adding a random layer which can be in the final layer of network, corresponding to adding random to the scoring function. In [2], they assume adding randomness to each layer so only adding random to final layer is a special case of that. I know the guarantees here are very different from those papers, but it will be nice to have some discussions. \n\n[1] 'Certified Robustness to Adversarial Examples with Differential Privacy' Lecuyer et al., (S&P'19)\n[2] 'Towards Robust Neural Networks via Random Self-ensemble' Liu et al., (ECCV '18)\n\n======\n\nThank you for the response and the additional experiments. I feel the paper has some interesting ideas and could be improved by a more careful writing and slightly adjusting the claim. I will rate the current draft borderline but slightly leaning to reject. ", "belong_id": "S1lBVgHYvr"}, {"uid": "Skx3jJ6qFB", "paper_title": "X-Forest: Approximate Random Projection Trees for Similarity Measurement", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper considers random projection forests for similarity measurements (which have been proposed earlier) and proposes to accelerate them by reusing projections. Tree levels up-to level-X use distinct random transformations, and subsequent levels cycle through existing projections (X of them). As this kind of reuse reduces the quality of trees, the paper proposes to (greatly) increase the number of trees in the forest. The paper also introduces a sensible 'beta-similarity' which is based on average tree-distance between leafs into which the two data-points fall, rather than fraction of trees in which they fall into the same leaf-node. \n\nI recommend to reject the paper -- as the contribution in my opinion is incremental, not principled (more of an engineering trick) and not very convincing. Furthermore, the paper has a number of issues with presentation, language, and experimental results. RP trees have been introduced over a decade ago (Dasgupta and Freund, 2008), and the authors cite a reference to RP forests from (Yan et al, 2019, IEEE Big data). Level-wise use of the same projection within a tree (that the authors call 'layer-by-layer RP Trees') has been discussed (and shown to be effective) in the original Dasgupta and Freund paper.  The paper makes it very hard to understand what is a contribution, and what is borrowed from existing papers -- as the authors say 'we introduce' both for well-known concepts (like RP trees), and for what I understand to be their contribution. I would ask to clearly state what is existing work, and what is new, and what are the key contributions.\n\nOther comments. \n1. Experiments:  you cite a paper on RP-forests (Yan et al, 2019) -- so ensembles of RP trees have already been proposed.  Why in the experiments you still compare only to a single RP-tree?  You report speed gains ~ 2x or 3x  w.r.t standard RP Trees -- even when you use very many (hundreds or thousands of trees in an X-forest).  Are these layer-by-layer RP trees, or do you use new random projection for each node? Are these your implementations of RP trees, or did you use existing code? The experiments do not provide enough details on the implementation to judge their significance -- e.g. 2x gain of speed could be achieved by better software implementation of the same algorithm.  Is it standard to measure clustering quality by  taking a classification problem and assuming that clusters should correspond to class labels? There are various other measures of cluster quality that do not rely on labels (e.g. rand-index, homogeneity, ratio of within-cluster to inter-cluster distances e.t.c.).\n\n2. In figure 8, (a),(b),(c) -- it looks like choice of beta can have a dramatic impact on accuracy, and can either be monotone, or peak at intermediate values.  How do you propose to chose beta in practice -- don't you need supervision?  What is the cost to compute beta-similarity compared to naive 0-1 similarity?\n\n3. The claim that similarity measures should be independent from prior knowledge -- can be controversial -- and is not a widely accepted truth. The field of metric learning tries to find better similarity matrices based on additional prior information. It's hard to imagine that a single similarity matrix will be suitable for all problem domains. I agree that it's useful to have a generic default similarity measure to try first, but clearly if prior data is available it should be used. \n\n4. In addition to 'mathematical distance-based similarity' and tree-based similarities -- there is also considerable work on similarity based on non-linear embeddings -- e.g. T-SNE or UMAP, provide an embedding, and then a simple cosine similarity can be used after the embedding. Overall -- this is a complex nonlinear similarity measure, not captured by your two classes. \n\n5. A well known paper 'Extremely randomized trees' also proposes kernels (similarity measurements) based on a forest created with (nearly) random splits, and should be cited, and used in comparisons. There is even a scikit-learn implementation -- called random trees embedding.\n\n6. The introduction of the paper:  'Similarity measurement is to measure similarity...' conveys no information. The entire first paragraph does not have much content, and should be rewritten or skipped.  Bad word choice:  Unsupervised clustering is 'to classify', 'Exalted speed', dimension of original dataset is 'degraded'...   If you say 'we introduce RP Trees' -- it sounds like you propose them in this paper -- which is clearly not the case, as you cite Dasgupta and Freund's paper.   You can instead say -- we consider RP trees introduced by (ABC)... \n", "belong_id": "S1lAOhEKPS"}, {"uid": "rkeLgBkTtS", "paper_title": "X-Forest: Approximate Random Projection Trees for Similarity Measurement", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes the similarity measure called 'beta-similarity' generated by an ensemble of Random projection trees (RP trees) by Dasgupta & Freund (2008). To reduce the computational costs for building many RP trees, the paper develops an efficient approximate version called X-Projection trees by first generating X independent random projection directions, and then by sharing them at layers in turns. X-Forest is a set of X-Projection trees with X different random projections, and the proposed 'beta similarity' between x and y is defined by distances between a leaf region having x and one having y in PR trees. Experimental evaluations demonstrated that the use of beta similarity improves the clustering accuracy using it within three types of methods (kernel k-means, DBSCAN, Spectral clustering). \n\nThe paper's idea of defining a similarity using many RP Trees with different randomization is quite interesting and sounds promising given that the recently reported performance of tree ensembles such as XGBoost and LightGBM is very good. Partition-based trees plus randomizations are known to have very nice properties particularly in high dimensions, theoretically speaking.\n\nHowever, this paper also has several problems 1) novelty and 2) confusing and imprecise statements.\n\n1) novelty\n\nThe novelty of the paper is basically (a) a simple computation savings of X-forest and (b) a definition of beta similarity.\n\nFor (a), the novelty is rather small, and how much this computation savings have a practical impact is questionable since reported computation timings in Figure.9 are in ms. Furthermore, trivial parallelization would be possible because individual computations of RP trees in the ensemble are independent. Also, there exists a highly cited paper by Yan et al KDD'09 proposed a fast clustering method based on RP trees as 'fast approximate spectral clustering' in their title.\n\nOn the other hand, (b) would be novel but any consideration about alternatives is not given, and the definition sounds quite heuristic and less convincing.\n\nRP trees are existing spatial structures (proposed by Dasgupta & Freund) extending widely used k-d trees. It naturally defines the spatial closeness of data points, and thus the use of RP trees to define the similarity, and applying them to clustering (kernel k-means, DBSCAN, spectral) is not new. RP trees were proposed as an alternative of k-d trees, and the primary applications would be for nearest neighbor search or data compression like vector quantization. \n\n2) confusing and imprecise statements\n\nMany confusing and imprecise statements exist. \n\n2a) The proposed 'beta similarity' eq (2) seems to lack the definition of DIS_i(X, Y). It would be something like the path length between node X and Y, or steps to the LCA (the lowest common ancestor). Also, the number m (the number of trees?) is also undefined.\n\n2b) The three goals are set: accuracy, efficiency, and independence from prior knowledge. But when we use RP trees, 1st and 3rd are considered as resolved and sounds like the only remaining problem is 'efficiency' for their computations. Also, the third goal 'independence from prior knowledge' is quite vaguely explained, and hard to understand. For example, the affinity matrix in spectral clustering or kernel matrix with an RBF kernel is the case? It would be a kind of hyperparameters but not like 'dependence on prior knowledge'.\n\n2c) Also, 'accuracy' of 'similarity measurements' is quite ambiguous. The use of kernel distance with RBF kernel is less accurate than the use of RP trees?? The 'similarity measurement' sounds like the problem of definition, and it cannot be accurate or inaccurate. The distance-based similarities themselves have no problems, and even when RP trees are used, we need some distance metric (i.e. Euclidean distance) in a space.\n", "belong_id": "S1lAOhEKPS"}, {"uid": "SkgRmZra5r", "paper_title": "X-Forest: Approximate Random Projection Trees for Similarity Measurement", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a new method for measuring pairwise similarity between data points. The idea is to define the similarity between two data points to be the probability (over the randomness in constructing the trees) that they are close in an RP tree. More concretely, the proposed method constructs a collection of RP trees (albeit with some modifications), and takes the similarity to be the average over different RP trees of a strictly decreasing function of the distance between the leaf nodes containing the data points in each RP tree. The key modification to the RP tree is to limit the number of projection vectors used in an RP tree and re-use previous projection vectors. \n\nI believe the method is more or less equivalent to Euclidean distance, for the following reason. Two data points would have the highest similarity under the proposed similarity measure if they are in the same leaf node. For this to happen, both points must be on the same side of the dividing hyperplane corresponding to each of the ancestor nodes. Because the threshold along the projection vector is chosen randomly at uniform, this means that for the two data points to have high similarity consistently, the distance between them along the projection vector must be small (so that the probability of splitting them is small). Because the projection vectors themselves are chosen randomly on the unit sphere, this implies that this must hold along most projection vectors for similarity to be high consistently, which means that the Euclidean distance between the two points must be low. \n\nIf true, this raises several questions:\n\n1) Why does the proposed method work better than distance similarity (which I assume means Euclidean distance) in the experiments? Are there situations when the proposed method would yield a high similarity consistently whereas Euclidean distance wouldnt? Are the results in Fig. 6 just for a single run of the proposed method? If so, many more runs need to be performed since the decisions of the RP tree should vary significantly depending on the projection vectors and thresholds. Both the mean and standard deviation should be reported. \n2) In Sect. 1.2, the authors critiqued distance-based similarity because it often does not correspond to intuitive notions of similarity/perceptual similarity. However, it does not appear that the proposed method would correspond to perceptual similarity either. For example, consider a dataset where some coordinates are more perceptually important than others (this is the case for example for the wavelet coefficients of a natural image - the lower frequencies are typically more perceptually important than higher frequencies). A more perceptually meaningful distance than Euclidean distance would be a Mahalanobis distance (which can essentially weight different coordinates differently), but the random projections use standard inner products and so are unable to capture the appropriate weighting of the different coordinates. So, why would one expect the proposed method to be more perceptually meaningful?\n3) In Sect. 1.2, the authors critiqued multi-partition-based similarity because it does depend on the data distribution and cited the elimination of prior knowledge dependence in Sect. 1.3 as one of the benefits of the proposed method. This appears to be at odds with the goal of devising a similarity measure that is perceptually aligned, because such a similarity measure must depend on the representation of the data (e.g.: if the data is represented in the wavelet domain, one needs to know which order the different dimensions are arranged, i.e. from lowest frequency to highest frequency or the other way around). \n\nOverall, it is unclear if the desiderata makes sense, and if the proposed method achieves the objectives. \n\nOther questions:\n\n4) For the X-Projection tree (which re-uses projection vectors), it seems to be equivalent to a layer-by-layer RP tree with larger branching factor. If so, the presentation of the method should be changed to this, because a layer-by-layer RP tree with larger branching factor is both conceptually clearer and simpler to analyze. If not, the proposed method should be compared to a layer-by-layer RP tree with larger branching factor, to justify the increased conceptual complexity of the X-Projection tree. \n5) For the experiments, comparisons should also be made to multi-partition-based similarity, like Multiple RP+EM and RF similarity. \n6) In addition, the proposed method should be compared to two simpler baselines that computes the average and the minimum distance of the two points along multiple random projection vectors, in order to justify the increased conceptual complexity of RP trees. \n7) In Sect. 1.3, the paper claims that it is well known that in an RP Tree, data points that are closely distributed, indicating their high level of similarity in space, are always partitioned into the same subset. This is not true, since hyperplane could divide a cluster down the middle for example. \n8) One of the claimed contributions in the abstract that we introduce randomness into partition to eliminate its reliance on prior knowledge. Note that just by introducing randomness, prior knowledge is not necessarily eliminated. For example, the way in which random projection is performed (i.e. standard inner product vs. other inner products) assumes knowledge of the distance metric, which is induced from the inner product. \n9) In Fig. 6, what is the distance metric used for the baseline?\n\nMinor issues:\n\npg. 2: project all data points into one random vector -> project all data points along one random vector\npg. 3: leading to unsatisfied results -> leading to unsatisfactory results\npg. 3: nearest neighbours finding -> nearest neighbour search\npg. 3: pattern discovering -> pattern discovery\npg. 4: similarly data points -> similar data points\npg. 4: 01 matrix -> 0-1 matrix", "belong_id": "S1lAOhEKPS"}, {"uid": "Hkg1J89ptH", "paper_title": "On the Decision Boundaries of Deep Neural Networks: A Tropical Geometry Perspective", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "1. Summary of the paper\n\nThis paper describes the decision boundaries of a certain class of\nneural networks (piecewise linear, non-linear activation functions)\nthrough the lens of tropical geometry.\nAn earlier result by Zhang et al. (2018) is extended to multi-class\nclassification problems (technically, only the result for a binary\nclassification is given in the main text, though).\n\nSimilar to this earlier work, the network is shown to be represented as\na tropical rational function. The dual subdivision of this function is\nshown to be represented as the convex hull of two zonotopes.\n\nThis characterisation is used to explain different phenomena of neural\nnetwork training, viz. the 'lottery ticket hypothesis', network\npruning, and adversarial attacks.\n\n2. Summary of the review\n\nThis is a highly interesting paper with a very relevant subject. I think\nthat the perspective of tropical geometry leads to valuable insights. My\nbackground is *not* in tropical geometry, so this paper required several\npasses to fully grasp.\n\nI like the novel insights that this paper creates; it is very interesting\nto observe known phenomena via tropical geometry. I appreciate the\nthorough description of all concepts in this paper. This is to some\nextent both 'boon and bane': on the one hand, the paper contains a lot\nof information and concepts that need to be understood; on the other\nhand, the experiments go *wide* but not *deep*. I suggest to accept the\npaper, but to fully endorse it, I would recommend to work on the\nfollowing issues:\n\n- Clarity & exposition: In some places, the paper could build intuition\n  for non-experts (such as myself) better. This is closely tied to the\n  second point.\n\n- Focus: I would maybe pick *one* or *two* of the experimental areas and\n  use the remaining space to explain all concepts in more detail, build\n  some intuition, and provide a more in-depth setup. Nothing has to\n  removed of course; it could still be put in the appendix.\n\nThis paper has the potential to be a very strong insightful contribution\nto to our community; the authors are to be commended!\n\n3. Detailed comments (clarity)\n\nThe paper describes its concepts well and has a high information\ndensity. At times, there is the risk that readers are provided with too\nmuch information in the main text, leaving the necessary intuition\nsomewhat lacking (unless they are already experts in the subject matter,\nin which case a lot of the information can be skipped).\n\nI realise that writing a paper based on methods that are not yet\nwell-established is no small feat; the authors are to be commended for\nthat!\n\nHere are some suggestions from someone with a background in differential\ntopology:\n\n- The introduction and contributions are somewhat repetitive; I would\n  suggest merging the 'Contributions.' paragraph with the one preceding\n  it\n\n- Even though it *should* be a well-known definition, I would briefly\n  explain that the semiring lacks an additive inverse\n\n- Add an explanation of the tropical quotient to Definition 1; I find\n  the current phrasing of Definition 3 to be confusing at first glance\n\n- What are *upper faces*? Faces with a specific coordinate fixed?\n\n- A definition of $\\pi$ is required. It is my understanding that $\\pi$\n  is a projection function that 'drops' the last coordinate. Is this\n  correct? If so, it should be briefly mentioned on p. 3; else, the\n  discussion about the bias-free case on p. 4 cannot be understood.\n\n- Theorem 2 lacks an 'intuitive' formulation; the results are stated in\n  a terse mathematical fashion, but it would be helpful (also in light\n  of the subsequent discussion) to briefly comment on their *meaning*.\n\n  For example, the first result could be restated as 'the decision\n  boundary is a subset of a tropical hypersurface of the polynomial\n  $R(x)$'.\n\n  The paragraph 'Theorem 2 bridges the gap...' could maybe also be moved\n  to _precede_ the theorem statement.\n\n- The paper refers to $\\mathcal{T}(R(x))$ as a super-set, but in my\n  understanding, it is a *level set* because Definition 4 uses an\n  equality, not an inequality. Am I misunderstanding this? One potential\n  misinterpretation of my part could be that superset refers to the fact\n  that $\\mathcal{B} \\subseteq \\mathcal{T}(R(x))$; so not a superset\n  in the sense of level set analysis, but rather a superset in terms of\n  set theory. If this is the case, maybe rephrase the sentence above to\n  something like 'boundaries $\\mathcal{B}$ through their superset\n  $\\mathcal{T}(R(x))$ according to the first statement of Theorem 2'.\n\n- What does the colour map in Figure 2 depict? The number of iterations?\n  Moreover, I find the polytope though to understand at first glance;\n  how is it related to the decision boundaries that are shown in the\n  leftmost figure?\n\n- I would suggest to place Figure 1 after stating Theorem 2, since it is\n  only referenced later on. Furthermore, the red structures are somewhat\n  confusing. According to Theorem 2, the decision boundary is a subset\n  of the hypersurface, right? What is the relation of the red structures\n  in the convex hull visualisation? The caption states that they are\n  normals, but as far as I can tell, this has not been formalised\n  anywhere in the paper (it is used later on, though).\n\n- To what extent is the existence of the functions described by Theorem 2\n  unique? On p. 5, in Section 4, the paper alludes to *not* using the\n  functional form of the network directly because it does not seem to be\n  unique. I would like this to be explained a in more details, as\n  I found the justification of why the dual subdivision is used quite\n  hard to follow.\n\n- In Section 4, how many experiments of the sort were performed? I find\n  this a highly instructive view so I would love to see more experiments\n  of this sort. Do these claims hold over multiple repetitions and for\n  (slightly) larger architectures as well?\n\n  Please also see my comments on Figure 2 above.\n\n- The claim that orientations are preserved should be formalised.\n  I immediately understand the intuition behind this concept, but\n  if possible, I would like a quantification of this. Might it be\n  possible to *measure* changes in orientation with respect to an\n  original polytope? If so, it should be possible to provide more\n  experiments about these effects and summarise them accordingly.\n  Maybe it would also be interesting to investigate whether other\n  initialisations can be compared in terms of their orientations?\n\n- In Section 5, I would give a brief link to the appendix for the\n  definition of a Minkowski sum.\n\n- Section 5 has (in contrast to the other sections) a lot of details\n  containing the experimental setup, but it is missing a description of\n  the competitor methods. Adding to what I wrote above, I feel that the\n  paper should rather pick *one* area in which experiments are\n  performed; the pruning (together with the lottery ticket hypothesis\n  explanation, which could be seen as a motivating example) strikes me\n  as a good candidate for this.\n\n  I really like this concept of tropical pruning, by the way---it is an\n  elegant, principled description!\n\n- The plots in Figure 4 should summarise multiple pruning runs, if\n  possible. Why not show a standard deviation as well? Given the\n  stochasticity of training, I would think this highly necessary.\n\n- In Section 6, I find the comment on normals generating a superset to\n  the decision boundaries hard to understand.\n\n-  The perspective of perturbing the network such that the decision\n   boundaries change is really interesting, but I am missing\n   a 'take-away message' or a discussion of the insights. Currently,\n   this section seems more like a feasibility study: it appears to be\n   possible to use the tropical description to find new parameters that\n   misclassify a given input. I would propose a discussion of the\n   implications of these findings. \n\n- Concerning future extensions of this work, are there some promising\n  results or directions for CNNs or GCNs? If so, it would strengthen\n  the conclusion if they were mentioned.\n\n4. Minor style issues\n\nThe paper is well-written. I found some minor style issues:\n\n- 'piece-wise' --> 'piecewise' (occur multiple times)\n- 'recently demonstrated' --> 'demonstrated'\n- 'Thereafter, tropical hypersurfaces divide' --> 'Tropical hypersurfaces divide'\n- 'If set $B$ --> 'Letting B'\n- I am not sure if I would call adversarial attacks a 'nuisance'; maybe rather a 'problem'?\n- Use '\\operatorname' or '\\mathrm' to typeset the loss in Eq. 5\n\n5. Update after rebuttal\n\nThe authors addressed all my important comments; their efforts in rewriting and revising the paper in such a short time period are to be commended. I am very happy to raise my score.", "belong_id": "BylldnNFwS"}, {"uid": "rylL75J0FH", "paper_title": "On the Decision Boundaries of Deep Neural Networks: A Tropical Geometry Perspective", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method to characterize the decision boundary of deep neural networks. The authors take advantage of a new perspective on deep neural network i.e., tropical geometry. They present and prove a theorem connecting the decision boundary of deep neural networks to tropical hyperspheres.  Then, they use the theoretical results of the theorem for two applications i.e., network pruning and adversarial examples generation.  For the former, they show that using the introduced decision boundary characterization using the tropical geometry, one can dimish the number of parameters in a model with insignificant loss in performance. For the latter, they introduce a new method for adversarial examples generation by altering the decision boundary. \n\nOverall, the technical and theoretical contribution of the paper regarding the relation of decision boundary to the tropical geometry is significant and could be useful for further investigation of the decision boundary of deep neural networks. \nHowever, there are several caveats in this paper that need further clarification.\n\n1) This paper needs to be placed properly among several important missing references on the decision boundary of deep neural networks [1][2]. In particular, using introduced tropical geometry perspective, how we can obtain the complexity of the decision boundary of a deep neural network?\n\n2)  The second part of Theorem 2 should be explained straightforwardly and clearly as it plays an important role in the subsequent results and applications. \n\n3) In the tropical network pruning section, the authors mention that 'since fully connected layers in deep neural networks tend to have much higher memory complexity than convolutional layers, we restrict our focus to pruning fully connected layers'. However, convolutional layers of investigated architectures (i.e., VGG16 and AlexNet) have a large number of parameters as well. So, wouldn't it be necessary to investigate pruning convolutional layers as well if diminishing the number of parameters is the main purpose? Moreover, the size of the last fully connected layer is simply determined by the preceding convolutional architecture, which in fact extracts the salient features (at least for well-known image datasets), while the last fully connected layer just flattens the extracted features to be fed into the subsequent classifier. Hence, it would be interesting and, in my opinion, necessary to investigate pruning convolutional layers as well. \nFurthermore, the authors mention that a pruned subnetwork has a similar decision boundary to the original network. What does it mean exactly by 'similar'? What are the measures capturing this similarity, if any? Also, the authors imply that two networks performing similarly (in terms of the accuracy) on a particular dataset have similar decision boundaries. I am skeptical if this the case and it needs to be validated concretely. \n\n4) In adversarial examples generation, typically for a pre-trained deep neural network model one is interested in generating examples that are misclassified by the model while they resemble real instances. In this setting, we keep the model and thus its decision boundary intact. In this paper, nevertheless, aiming at generating adversarial examples, the decision boundary and thus the (pre-trained) model is altered. By chaining the decision boundary, however, the model's decisions for original real samples might change as well. Therefore, it is not clear to the reviewer how the introduced method is comparable to the well-established adversarial example generation setting.  \n\n5) Two previous papers investigated the decision boundary of the deep neural networks in the presence of adversarial examples [3][4]. Please discuss how the introduced method in this paper is placed among these methods. \n\nMinor comments:\nIn the abstract, 'We utilize this geometric characterization to shed light and new perspective on three tasks' --> unclear, needs to be revised.\n\nProposition 1, 'the zonotope formed be the line segments' --> 'the zonotope formed by the line segments'\n\nPage 7. Results. 'For VGG16, we perform similarly on both SVHN and CIFAR10 CIFAR100.' --> 'For VGG16, we perform similarly on both SVHN and CIFAR10.'\n\n\n\n\nReferences:\n[1] @article{li2018decision,\n  title={On the decision boundary of deep neural networks},\n  author={Li, Yu and Richtarik, Peter and Ding, Lizhong and Gao, Xin},\n  journal={arXiv preprint arXiv:1808.05385},\n  year={2018}\n}\n[2] @article{beise2018decision,\n  title={On decision regions of narrow deep neural networks},\n  author={Beise, Hans-Peter and Da Cruz, Steve Dias and Schr{\\'o}der, Udo},\n  journal={arXiv preprint arXiv:1807.01194},\n  year={2018}\n}\n\n[3] @article{khoury2018geometry,\n  title={On the geometry of adversarial examples},\n  author={Khoury, Marc and Hadfield-Menell, Dylan},\n  journal={arXiv preprint arXiv:1811.00525},\n  year={2018}\n}\n\n[4] @inproceedings{\nhe2018decision,\ntitle={Decision Boundary Analysis of Adversarial Examples},\nauthor={Warren He and Bo Li and Dawn Song},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BkpiPMbA-},\n}\n\n", "belong_id": "BylldnNFwS"}, {"uid": "Hyg__4H0KH", "paper_title": "On the Decision Boundaries of Deep Neural Networks: A Tropical Geometry Perspective", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposed a framework based on a mathematical tool of tropical geometry to characterize the decision boundary of neural networks. The analysis is applied to network pruning, lottery ticket hypothesis and adversarial attacks.\n\nI have some questions:\n\nQ1: What benefit does introducing tropical geometry brings in terms of theoretical analysis? Does using tropical geometry give us the theoretical results that traditional analysis can not give us? If so, what is it? I am trying to understand why the authors use this tool. The authors should be explicit in their motivation so that the readers are clear about the contribution of this paper. More specifically, from my perspective, tropical semiring, tropical polynomials and tropical rational functions all can be represented with the standard mathematical tools. Here they are just redefining several concepts.\n\nQ2: In Experiments on Tropical Pruning, the authors mentioned we compare our tropical pruning approach against Class Blind (CB), Class Uniform (CU), and Class Distribution (CD) methods Han et al. (2015). What is Class Blind, Class Uniform and Class Distribution? There seems to be an error here Figure 5 shows the pruning comparison between our tropical approach ..., i think Figure 5 should be Figure 4. \n\nQ3: In the adversarial attack part, is the authors proposing a new attack method? If so, then the authors should report the test accuracy under attack. Also, the experimental results should not be restricted to MNIST dataset. I am also not sure about the attack settings here, the authors said Instead of designing a sample noise  such that (x0 + ) belongs to a new decision region, one can instead fix x0 and perturb the network parameters to move the decision boundaries in a way that x0 appears in a new classification region.. Why use this setting? Are there any intuitions? Since this is different from traditional adversarial attack terminology, the authors should stop using adversarial attacks as in tropical adversarial attacks because it is really misleading.\n\n\n\n===================================================================\nThanks the authors for the response. I still have two questions:\n\nQ1: The authors say that this theory provides a deeper understanding to Lottery Ticket Hypothesis (LTH). Then another paper Rethinking the Value of Network Pruning [1] suggests something different than LTH. [1] suggests that we do not need the initialization of large networks to train the pruned network from scratch to achieve high accuracy. Since the authors claim that their theory is related to LTH, then what would the proposed theory say about [1]?\n\nQ2: Since you redesign the task of adversarial attacks, I am still not convinced why this setting is interesting? The reason why people are interested in adversarial attacks is because it could happen during test time. What is the application of this setting? Why this new setting is important and worth studying? They are not clear to me. \nAlso, as i wrote in my initial review, the authors should stop using adversarial attacks as in tropical adversarial attacks because it is really misleading.. I hope the authors can address this concern, or otherwise new readers may also find this part difficult to understand.\n\n[1] Rethinking the Value of Network Pruning. Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell. ICLR 2019.\n\n", "belong_id": "BylldnNFwS"}, {"uid": "rJgo2xHotB", "paper_title": "The Differentiable Cross-Entropy Method", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "*Summary*\nVarious optimization methods can be wrapped to form black-box differentiable deep learning modules. This allows end-to-end learning of energy functions that can be used, for example, in continuous control. There is a whole cottage industry of designing these modules. Advancements in this field are of broad interest to the ICLR community. This paper proposes to unroll the cross entropy method, which is very different than the standard practice of unrolling gradient descent. Experiments on continuous control benchmarks demonstrate that this can be used to learn a latent space in which test-time optimization is performed. By doing this optimization in latent space, it can be performed much faster than in the raw high-dimensional space.\n\n*Overall Assessment*\nThe paper is well written and the technical contribution is explained well. Both evolutionary search methods (e.g. CEM) and unrolled gradient-based optimizers are very popular in ML these days. This paper will be of interest to many readers, since it works at the interface between these.\n\nI do not have much background in continuous control, model-based RL, etc. Therefore, it is hard for me to assess whether the experiments compare to the right baselines, etc. It appears to me that the experiments on cheetah and walker do not compare a particularly broad set of methods. They only compare within the design space of DCEM. Furthermore, the key result in these experiments is that the DCEM policy results in more efficient inner optimization (such that running the policy is faster). The overall messaging of the paper was not about reducing the costs of executing the policy but in improving performance. Such a result is not provided in these experiments.\n\nI have worked extensively with unrolled optimizers and can speak to the correctness and usefulness of the paper's methodological contribution.and the experiments in sec 5.1. However, these are more for providing insight into the method, and are not large-scale experiments.\n\nMy evaluation is a weak reject, since the paper would be greatly improved by stronger empirical results for the large-scale continuous control benchmarks with comparison to a broader set of methods.\n\n*Comments*\n\nThe empirical advantage of DCEM vs. unrolled GD is clear, but it's not clear to me what the intuition behind this is. You write 'one potential advantage of DCEM is that the output is more likely to be near a local minimum of the energy surface so that, e.g., more test-time iterations can be used to refine the solution.' Why would GD not want the output to be be near a local minimum. Also, why is DCEM not also sensitive to the number of steps? The variance of the CEM distribution gives a natural lengthscale similar to the step size in GD. You discuss this further at the end of sec 5.1 Are there simple experiments you could do that compare the robustness of GD (such as with random restarts or unrolled Langevin dynamics) vs. DCEM?\n\nIn the standard CEM, doing weighted MLE with 0-1 weights coming from top-k is useful because the set of examples for MLE is size k, which yields computational savings. However, if you can tolerate doing weighted MLE on all available samples, then there may be better ways to set the weights than using a softened version of top-k. See, for example, the example weights in 'Design by Adaptive Sampling' (arxiv.org/abs/1810.03714). Can you comment on the suitability of other weighting schemes besides top-k? Also, will your relaxed top-k perform sensibly when there are many ties in the observed f(x) values?\n\nThe principal critique of the paper is the positive DCEM results on cheetah + walker are mostly about runtime, instead of performance. Can you speak more to why you don't think it is providing better performance as well? Perhaps the latent space is useful, for example, for transfer learning + adaptation? \n", "belong_id": "HJluEeHKwH"}, {"uid": "B1xaWlkCFS", "paper_title": "The Differentiable Cross-Entropy Method", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "After reading authors' response, I am sticking to my original decision. Authors addressed most of the issues I raised and I am happy with their response; however, I still believe the paper should not be accepted since it is not adding enough value. The problem is important and impactful. However, the algorithmic idea comes from LML (Amos 2019), and the impact on the real problems has not been demonstrated. Hence, it is adding no value algorithmically, and adding a very small value from application perspective. It is basically saying LML can be trivially applied to differentiate through CEM, and it works on some simple toy problems. To me this is mostly a sanity check. Hence, I am sticking to my weak-reject decision.\n-------\nThe manuscript is proposing a method to make cross-entropy method (CEM) differentiable. CEM is a widely used zeroth-order optimization method. The main idea in the paper is applying the recently proposed limited multi-label projection (LML) layer in a straight-forward manner to the CEM since the major computational tool in CEM iteration is top-k selection. The authors apply the proposed method to synthetic energy-based learning and continuous control problems. \n\nThe proposed method is definitely impactful. Considering the fact that CEM is a powerful and widely used tool, I believe the work will lead to many interesting follow-ups. In addition to these, the work is addressing computational scalability of model-based RL which is both under-explored and important problem. \n\nThe proposed model is novel from a modelling perspective since it makes CEM part of end-to-end learnable models. Whereas, it has no algorithmic novelty since it is a straightforward application of the LML layer to the CEM problem. Lack of algorithmic novelty is not an issue but the authors should at least discuss similarities to LML (Amos 2019) in a clear manner in related work. Not including it in the related work is somewhat surprising to me.\n\nThe exposition can clearly be improved. First of all, Proposition 1 is an existing result, hence authors should give a proper citation in its definition. Second of all, Proposition 3 does not include anything about asymptotic (tau -> 0) whereas the stated one-line proof is using asymptotic arguments. Finally, there are other minor issues like Lemma1 not having a proof, proposition 1 has no statement about its proof etc.  The manuscript would significantly benefit from a thorough proof reading for mathematical completeness and correctness.\n\nOne major issue with the manuscript is the experimental study. 1) The only additional algorithmic element introduced by the manuscript is the tau and it is not experimented. Is it crucial to use the temperature parameter? If yes, what is the effect of it? Manuscript needs a collection of ablation studies discussing the tau. 2) The main claim of the paper is '...make solving the control optimization process significantly less computationally and memory expensive.' This might be true but not really experimented. Authors do not report any quantitative computation time and/or memory requirement study. I believe the latent DCEM is more memory and computation efficient but quantifying this is important.\n\nI am curious on the choice of CEM. There are other methods which can be utilized since this is basically a bi-level optimization problem. One can use implicit gradients or similar methods (like: https://arxiv.org/abs/1602.02355, https://arxiv.org/abs/1809.01465, https://arxiv.org/abs/1909.04630, http://proceedings.mlr.press/v22/domke12/domke12.pdf).  Can these methods also be utilized instead of back-propagation through optimization procedure? If yes, you should compare with them or explain why you did not. If no, you should explain why.\n\nIn summary, the paper is very impactful. On the other hand, the proposed empirical study significantly lacks in many aspects. I would be happy to increase my score if authors can address these issues.", "belong_id": "HJluEeHKwH"}, {"uid": "HyxhrAGRtB", "paper_title": "The Differentiable Cross-Entropy Method", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a differentiable variant of the Cross-Entropy method and shows its use for a continuous control task.   \n- It introduces 4 hyper-parameters and it is not clear how robust the method is to these. \n- Although the idea is interesting, I think the paper needs a more rigorous experimental comparison with previous work and other methods.\nDetailed review below:\n- The abstract should mention clearly that the proposed method allows you to differentiate through argmin operation and can be used for end to end learning. Similarly, please reframe parts of the introduction to make it more accessible to a general reader. For example, in the introduction,  'approximation adds significant definition and structure to an otherwise...'. This statement requires more context to make it useful. Similarly, 'smooth top-k operation' is not clear. \n- Is there a way to guarantee that the solution found by (D)CEM is a reasonable approximation to the argmin. For unrolled gradient descent, this can be done by looking at the gradient wrt x. \n- It might be more useful to explain CEM before the related work section or just moving the related work to the end. \n- Section 3: If the paper is about CEM, please give some motivation and details rather than just citing De Boer, 2005. \n- There is a notation clash between \\pi for the sort and policy later in the paper. Similarly, 't' is for both for the iterations of CEM and the time-stamp in the control problem. \n- I don't understand how Proposition 1 adds to the paper. This is a standard thing. Similarly for Proposition 3. \n- Isn't there an easier way to make the top-k operation soft - by sampling without replacement proportional to the probabilities? Please justify this design decision. Similarly, how is the temperature \\tau chosen in practice?\n- Please explain the paragraph: 'Equation 4 is a convex optimization layer and... GPU-amenable..' Isn't this critical to the overall scalability of this method?\n- - How are the hyper-parameters for CEM  chosen - the function g(.), the value of k, \\tau, T chosen in practice. If the criticism of GD is that it overfits to the hyper-parameters - learning rate and the number of steps, why isn't this a problem with (D)CEM.  \n- Section 4: Since you're comparing against unrolled GD, please formally state what the method is. \n- Section 4.2: How is the structure of Z decided, that is how do you fix the space for searching for the policy in the Z space? \n- There are other methods that auto-encode the policy u_1:H to search the space. How does the proposed method compare to these methods? This is important to disentangle the effect of GD vs CEM and that of just searching in a more tractable space of policies. \n- Section 5.1: How is the number of optimizer steps (=10) decided? Also, how is the learning rate for GD picked. Is the performance of unrolled GD worse for all values of \\eta, even after a grid-search over the learning rates?\n- For Section 5.2, please compare to baselines mentioned in the paper. Also, there needs to be an ablation/robustness study for the DCEM method. \n\n\n\n\n", "belong_id": "HJluEeHKwH"}, {"uid": "HkgI8AGkYr", "paper_title": "A Base Model Selection Methodology for Efficient Fine-Tuning", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors tried several metrics, from which they pinpoint one as the indicator to select a pretrained model without practically fine-tuning. I appreciate that the authors addressed an important problem, while the experimental setup and results are less convincing. Therefore, the proposed metric(s) and settings in the current shape are not that practical. \n\nPros:\n-\tIn this work, the authors focused on an important problem  selecting the best pretrained model from a zoo of models without finetuning all of them in a brute-force manner. \n-\tThe idea of applying the metric(s) to select a layer from which the consecutive layers are truncated is intriguing, for the sake of model compression.\n\nCons:\n-\tThe work is more alike a course project than a novel scientific contribution, with all the metrics mainly inherited from the evaluation of clustering and other existing literatures. \n-\tThe experiments are not comprehensive, so that the conclusions drawn are weak and untenable.  \n-\tThe writing with many grammatical errors and typos definitely needs polishing.\n\nDetailed comments:\n1.\tThe authors calculated all the metrics in terms of feature maps in the last layer. Therefore, it is not valid to conclude that other metrics are less effective than sparsity. Some metrics, like fisher discriminator, are likely effective only in the low-level features. It is better for the authors to investigate all metrics in terms of all layers.\n2.\tSince all the pretrained models are pretrained from ImageNet, it is possible that they lose some diversity, which tends to deactivate metrics other than S5 and S6. Unless the authors trained several models from scratch on different datasets and achieved similar results, the conclusion that S5 and S6 are strong is not that convincing.\n3.\tWhy during fine-tuning, do you use SGD instead of Adam?\n4.\tWhat kind of correlation metrics do you use in Eqn. (4)? And will the correlation metric influence the effectiveness of S4?\n5.\tThe most confusing part lies in that the valid metric suggested by the author, i.e., C5,6(0.574), is only an empirical observation on a very limited number of datasets, without any principled methodology. Actually, it is highly possible that given a highly different dataset, the best metric changes. Should the practitioners determine which metric to use first, and even the coefficient to combine metrics?  This is paradox, in my opinion. \n6.\tGrammatical errors and missing details:\no\tAbstract: a variety of network structure -> a variety of network structures\no\tSection 3: The sentences for the three hypotheses, H1/2/3, do not even have verbs in the if part. \no\tSection 3.4: to estimates -> to estimate\no\tSection 3.6: which quantify -> which quantifies\no\t...", "belong_id": "BylT8RNKPH"}, {"uid": "H1xRBbkYtB", "paper_title": "A Base Model Selection Methodology for Efficient Fine-Tuning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper aims to speed up finetuning of pretrained deep image classification networks by predicting the success rate of the process without running it. The authors suggest running samples from the target task on the (trained) source model, and computing a few sensible measures from the final output layer which indicate how well the trained features are separating the target images. Many experiments are presented, and some of them seem promising. \n\nOverall the idea is, while simple, interesting and potentially promising: DL Training costs have been rising significantly in the past few years, and being able to make sensible predictions about which source dataset/task combination best fits a target task would be a great contribution. Nonetheless, there are some methodological concerns that cast doubt about the presented results, which would need to be addressed before making this paper ready for publication.\n\nComments:\n\n1. The authors present multiple experimental results, many of which indicate a somewhat noisy signal. The only method that works on most tasks, combining S5 and S6, is somewhat underreported. How many different \\alpha values did the authors consider? how were they selected? Was the same alpha values used for each all experiments? Given this large set of measures and combinations of measures experimented with in this paper, I am left wondering whether this approach would generalize to new target datasets.\n\n2. A major assumption the authors are making is that there is a single number that determines whether a given trained architecture would transfer well to a new target task. But the finetuning process is affected by many factors (e.g, hyperparameters, random seeds), and thus it might be that with a different hyperparameter selection, the observed correlations would look completely different. I would have liked to see at the very least an analysis of the correlation between multiple runs of finetuning that show that they are well correlated before computing correlations with external measures.\n\n3. The authors point out that transfer learning works poorly on medical imaging (Sato et al., 2018). It would be interesting to experiment with such datasets and observe whether the proposed method is able to make accurate predictions in this domain.\n\n4. The experiments regarding truncating CNNs seem interesting, and I was disappointed to find out they are not part of the main paper.\n\nOther comments:\n\n1. Moons et al. (2016) and Molchanov et al. (2017) speed up *inference* and not training.\n\n2. The authors argue that Mahajan et al. (2018) 'did not apply it (their method) for detection or segmentation tasks.'. However, neither did this paper.\n\n3. Writing: \n-- Several typos and grammatical errors across the paper. For instance:\n- '*In* many previous research efforts suggested' ('in' should be dropped)\n- Hypothesis H1 is ungrammatical \n\n-- Many of the citations were in the wrong format (intro: Canziani et al. 2016), repetitive (3.5: Yaguchi et al.), etc.\n\n", "belong_id": "BylT8RNKPH"}, {"uid": "r1lkxZb0FH", "paper_title": "A Base Model Selection Methodology for Efficient Fine-Tuning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose several metrics to evaluate the transferability of pretrained CNN models for a target task without actually fine-tuning the networks to accelerate fine-tuning. \n\nThe paper has done some interesting empirical studies on how to predict the transferability of a neural network. The motivation of performing model selection without actual finetuning has many benefits for practical applications and I believe this is the right direction. In this perspective, the paper is novel. However, my major concern is a lack of in-depth analysis and thorough verification of the proposed metrics.\n\nFirst, there is no clear relationship between the six evaluation metrics and the fine-tuning performance. Figure 1 depicts some correlation, however, it is not clear enough. The difference in ResNet18 is not further investigated. Given the dominant usage of ResNets and its variations in practice, it is important to provide analysis for the reasons behind.\n\nSecond, some of the proposed metrics (S1/S2) are actually closely related with weight norms.  The definition of high value elements in S6 is also based on the absolute difference with the maximum feature map values. Does the author apply weight decay during training? Normally the weight norm will becomes smaller and so could the featuremap values. The S1/S2/S6 metrics could therefore be influenced. I would like to see figures showing the relationship between the weight/feature norms and the metric during training. \n\nThird, it turns out that the only useful metrics are the combination of feature sparsity and feature steepness (i.e., C_56). The authors should make it more specific and clear for their contributions. Why only two combination of the metrics is considered? Does the best selected coefficient generalize to other models and datasets?\n\nFinally, according to https://arxiv.org/abs/1805.08974, better pretrained ImageNet model also generalize better. I would expect a deeper ResNets should outperform AlexNet and VGG16.  What is the relationship between the proposed metrics and the ImageNet performance? \n\nMinor:\n\nThe S3 metric is actually a ratio of inter-class variance to intra-class variance rather than  ratio of intra-class variance to inter-class variance.\n", "belong_id": "BylT8RNKPH"}, {"uid": "SyxkmrNAFH", "paper_title": "From English to Foreign Languages: Transferring Pre-trained Language Models", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method to adapt a pretrained BERT model from English to another languages with a limited time/GPU budget. Evaluation on 6 target languages shows good performance for natural language inference and dependency parsing. \n\nConcretely, the proposed approach consists of, starting from a pretrained English language model, first training language-specific embeddings and then fine-tuning the entire pretrained model on English *and* the target language, using those embeddings. The language-specific embeddings are initialized based on the English embeddings (the authors propose two different ways for doing that).\n\nI like about the paper that the approach is simple and fast. The experiments seem reasonable, too. The only minor negative point is that the approach is not particularly exciting.", "belong_id": "Bkle6T4YvB"}, {"uid": "r1lhNKE0Yr", "paper_title": "From English to Foreign Languages: Transferring Pre-trained Language Models", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a method to efficiently transfer pre-trained english language model to bilingual language model. The obtained representations are evaluated on downstream NLP task (natural language inference and dependency parsing) with state-of-the-art performances.\n\n\nPros:\n\n- Experiments clearly show that, using the proposed method, stronger pre-trained English embedding leads to stronger bilingual language model and thus to better performances for downstream foreign tasks.\n\nCons: \n\nWhile it is generally  intelligible, some structural modifications could be done to  improved the clarity of the paper. For instance, the method used to align foreign word vectors with English word vectors, when no aligned corpus is available, should appear sooner. It is described in 3.1 but should probably appear in 2.1 subsection Learning from Monolingual Corpus.\n\nMinor issues:\n\n- in section 3: RoBERA -> RoBERTa\n- in section 5.1: the third sentence is syntactically incorrect\n- in Conclusion: our approach produces better than -> our approach performs better than\n", "belong_id": "Bkle6T4YvB"}, {"uid": "rylWllu1qB", "paper_title": "From English to Foreign Languages: Transferring Pre-trained Language Models", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this work, the authors propose a way to transfer a pre-trained English BERT model to a new language within a short amount of time. The key insight is to map English embeddings to the foreign language and have separate embeddings for both English and the foreign language. The resulting bilingual LM is evaluated for zero-shot transfer learning on two tasks: XNLI and dependency parsing.\n\nPros:\n- The authors provide good details into their hyperparameter settings and about how the obtain the foreign language word embeddings.\n- By leveraging existing pre-trained models, theyre able to do pre-training for their bilingual LM within 2 days.\n\nCons:\nI find that a key comparison point in this paper is missing, which is Bilingual BERT trained on just the two languages that are being considered for their RAMEN system. This is not a fair comparison while mBERT which is trained on 100+ languages is not.\nAll comparisons are not fair since a simple baseline of just training mBERT on two languages with monolingual data and with a shared WPM is not evaluated here.\nThe proposed system has an unfair advantage over mBERT since its initialized from BERT/RoBERTA and fine-tuned only on two languages. Hence most of the parameters are used for just the two languages while mBERT uses the parameters for 104 languages.\nGiven this unfair comparison, Im not sure if we can draw a meaningful conclusion from all the experiments. \n\nRating justification:\nGiven the lack a fair comparison between the bilingual and multilingual BERT models, I don't think the conclusions are insightful.\n", "belong_id": "Bkle6T4YvB"}, {"uid": "HkeCv6fj_B", "paper_title": "Meta-Learning by Hallucinating Useful Examples", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors address few-shot learning via a precise collaborative hallucinator. In particular, they follow the framework of (Wang et al., 2018), and introduce two kinds of training regularization. The soft precision-inducing loss follows the spirit of adversarial learning, by using knowledge distillation. Additionally, a collaborative objective is introduced as middle supervision to enhance the learning capacity of hallucinator. \n\nHere are some comments:\n1. The novelty is relatively limited. The idea of hallucinating has been mainly introduced in (Wang et al., 2018). The soft precision-inducing loss is a straightforward extension of knowledge distillation (Hinton et al., 2015). \n\n2. It is not quite clear about how to use the collaborative objective on the hallucinator. Fig.2 and the text in ' Collaboration between hallucinator and learner ' (page 5) are not quite informative. Especially, how to perform the soft precision-inducing loss (l_hal^pre) for hallucinator?", "belong_id": "rJx8I1rFwr"}, {"uid": "rJl3L37nYB", "paper_title": "Meta-Learning by Hallucinating Useful Examples", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a general meta-learning with hallucination framework called PECAN. It is model-agnostic and can be combined with any meta-learning models to consistent boost their few-shot learning performance. \n\nThere are two key points for the proposed model. On the one hand, the authors introduce a novel precision-inducing loss which encourages the hallucinator to generate examples so that a classifier trained on them makes predictions similar to the one trained on a large amount of real examples. On the other hand, the authors introduce a collaborative objective for the hallucinator as early supervision, which directly facilitates the generation process and improves the cooperation between the hallucinator and the learner.\n\nOn the whole, the paper is well-written, and the proposed idea is novel and interesting.\n\nI have some following major concerns about the paper:\n(1) In Figure 2, the authors first sample the training set S^*_{train}, which contains n^* examples for each of the m classes, and then they randomly sample n examples per class, and obtain a subset S_{train}. Why not generate the S_{train} directly and then measure your precision-inducing loss over the real set S_{train} and S^G_{train}? I hope the authors explain it in their paper.\n(2) For Function 2 in the paper, why compute the cosine distance on the probability vectors that are obtained by removing the logit for ground-truth label in original probability distributions? Could we compute the distance on the probability vectors that contains the logit for ground-truth label? I hope the authors explain it in detail.\n(3) As far as I know, there are some latest work on few-shot learning in 2019, especially the work Few-shot Learning via Saliency-guided Hallucination of Samples and Edge-Labeling Graph Neural Network for Few-shot Learning. I hope the authors can compare with these two methods to further demonstrate the effectiveness of the proposed model.", "belong_id": "rJx8I1rFwr"}, {"uid": "SJlzWIKX5r", "paper_title": "Meta-Learning by Hallucinating Useful Examples", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper describes a method that builds upon the work of Wang et al. It meta-learns to hallucinate additional samples for few-shot learning for classification tasks. Their two main insights of this paper are to propose a soft-precision term which compares the classifiers' predictions for all classes other than the ground truth class for both a few-shot training set and the hallucinated set and b) to introduce the idea of applying direct early supervision in the feature space in which the hallucination is conducted in addition to in the classifier embedding space. This allows for stronger supervision and prevents the hallucinated samples from not being representative of the classes. The authors show small, but consistent improvement in performance on two benchmarks: ImageNet and miniImageNet with two different network architectures versus various state-of-the-art meta-learning algorithms with and without hallucination. The authors have adequately cited and reviewed the existing literature. They have also conducted many experiments (both in the main paper and in the supplementary material) to show the superior performance of their approach versus the existing ones. Furthermore their ablation studies both for the type of soft precision loss and for their various individual losses are quite nice and thorough. \n\nOverall the contribution of this paper is incremental over Wang et al and is mainly in the introduction of their new loss terms to regularize the hallucination process. This is clearly evident from Table 2 (comparing rows 1 and 3), where much of the performance gain is attained by including the l_learner^cls term versus the collaborative loss term (comparing row 1 and row 4).\n\nFurthermore, I would like the author to answer the following two questions:\n\n1. The authors claim that their method is general applicable to all meta-learning methods and can be combined with them. Yet, the meta learning methods that they apply it to: prototypical networks, prototypical matching networks and cosine classifiers are all metric-learning-based meta-learning techniques. I would like the authors to outline the procedure (and preferably also show experiments) for applying their proposed technique to meta-learning based techniques that do not involve learning a metric-embedding space and instead learn the learning procedure via nested optimization, e.g. MAML and its variants.\n\n2. In Table 4, I would like to see the results of PNM w/G or in other words the results of (Wang et al, 2018)'s method in comparison to the authors' proposed method.\n\n3. The authors make no attempt to solve the problem of hallucinating examples for regression tasks. This is fine as it is perhaps outside he scope of their current work. However, I would like the authors to fully qualify their claims everywhere in the paper and restrict the contribution of their work to classification tasks only.\n\n", "belong_id": "rJx8I1rFwr"}, {"uid": "SJeDZyV5tr", "paper_title": "Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigates the problem of predicting the truth of quantified boolean formulae using deep reinforcement learning. In this setting, the problem is formulated as a reinforcement learning task, in which the learner is interacting with a solver (CADET), and its goal is to find a sequence of actions (each associated with a choice of a variable and a value) in order to reach a terminal state as fast as possible. The neural architecture includes a GNN encoder for the input formula, a policy neural net for iteratively assessing the quality of literals, and a final softmax layer for choosing the final literal. Experiments, performed on various 2QBF instances, address several questions such as the ability to compete with existing heuristics (VSIDS) in CADET and to generalize predictions on long episodes or different formulae.\n\nOverall, the paper is well-motivated. The introduction is well-written and explains the interest of learning new heuristics for QBF problems. The learning framework is relatively simple and elegant. Unfortunately, the paper suffers from many clarity issues in the problem formulation, the neural-net architecture, and the experiments. So, it is quite difficult to accept the paper in its current state. \n\nSection 2: In this section, some background knowledge about boolean problems and solvers are provided. Although the second paragraph about CNF formulae and CDCL solvers is well-written, the third paragraph about QBF should be clarified. For QBF formulae, the authors write The algorithmic problem considered for QBF is to determine the truth of a quantified formula (TQBF). Well, this is not an algorithmic problem, but a decision problem. Furthermore, what is TQBF? I guess that the authors are talking about 2-QBF, for which the prenex is of the form $\\forall \\exists$. Here, the decision task should be explained in more detail using, for example, a game tree for explaining the distinct roles of for all variables and there is variables. As the decision problem for 2-QBF is more complex than the satisfiability problem for CNF, this point should be emphasized (i.e. the decision problem for 2-QBF is $\\Pi_2^P$ complete). \n\nSection 3: The problem of predicting the truth of 2-QBF is formulated as an MDP, where the environment is essentially controlling the input instances (of 2-QBF) and the states of the solver, and the learners actions are variable-value selections. First of all, I am not entirely convinced that an MDP is the right choice for specifying this problem. Basically, 2-QBF is a two-player game (for all vs there is), and the goal of the solver is to play there is by finding a satisfying assignment for each possible play (i.e. variable assignment) of the for all player. So, a natural framework here would be a stochastic game (SG) which generalizes the MDP framework. Actually, in the present MDP framework there are no distinctions between for all variables and there is variables. It seems that the learner can choose for all variables (which is wrong). Furthermore, the MDP is ill-defined. It is said that a policy is a mapping $\\pi: S \\times A$. This is ill-defined: what is the range of $\\pi$? Usually, a (mixed) policy is a mapping $\\pi$ from $S$ into the $|A|$-dimensional simplex. The reward function is also ambiguous: it is using a discount factor $\\gamma$ but, unless I missed something, this factor is not clarified in the rest of the paper. Finally, what is an episode? In the paper it is said An episode is the result of the interaction of the agent with the environment. Well, this is quite unclear. Usually, in an episodic-MDP the state space is partitioned into layers, i.e. $X = \\bigcup_{i = 0}^L X_i$, where $X_0$ is a singleton set (the initial state), and $X_L$ specifies the terminal states. Transitions are possible only between consecutive layers.  According to this usual framework, an episode is a sequence of actions made by the agent, starting from $X_0$ and moving forward across the consecutive layers until it reaches a state in $X_L$. For the 2-QBF decision problem, each terminal state in $X_L$ would naturally consists in the affectation of each variable in the prenex to a Boolean value. But this is not clear in the paper, because the authors are saying that We consider an episode to be complete, if the solver reaches a terminal state in the last step. This would mean that the number of actions per episodes is capped, and hence, the agent can reach a terminal, yet non-final, decision state.\n\nSection 4: The overall architecture (GNN encoder + Policy Network) is relatively standard, but the choice of the constants for dimension parameters $\\lambda_V$, $\\delta_L$ and $\\delta_C$ is a bit disconcerting. Notably, $\\lambda_V$ is used to capture the features of variables. Specifically, it is written that $\\lambda_V = 7$ indicates whether the variable is universally or existentially quantified, whether it currently has a value assigned and whether it was selected as a decision variable already on the current search branch. But what is the difference between the second feature and the third one? If a variable has already been branched, then it has been assigned, and conversely. Furthermore, if you have 3 boolean features $\\lambda_V$ should be fixed to 3. So, why choosing 7? For $\\delta_L$ and $\\delta_C$, it seems that their values correspond to the best model. But how this model is chosen? Did the author perform some grid search to find those values?  Finally, some comments about the last layer of the architecture (softmax function) would be welcome. In the end, we get a probability distribution over literals (agents available actions) after masking illegal actions (as written by the authors). But what is an illegal action? Is it a literal defined on a universally quantified variable? A literal defined on an already assigned existential variable? \n\nSection 5: In the experiments, the authors are examining four different questions, which are all interesting. But the experimental setup and the reported results are quite unclear. In fact, the experimental setup looks wrong, because if the Reductions dataset is taken from Jordan & Kaiser (SAT13), it consists of formulae for which the prenex is of the form $\\exists \\forall$. Unless I am wrong, this is the inverse of the 2QBF problem examined in the present paper  - Jordan and Kaiser were examining a $\\Sigma_2^P$-complete problem, while you are examining a $\\Pi_2^P$-complete problem. So, did you reverse the quantifiers for making experiments? This should be clarified in the paper. Furthermore, for this dataset which originally consists of 4500 instances, the authors say that We filtered out 2500 formulas that are solved without any heuristic decisions. What does this mean? Are all those 2500 formulas containing only universal quantifiers? In the remaining 2000 instances, what is the ratio between universally quantified variables and existentially quantified ones? By the way, how can we get 1835 training instances? 4500 - 2500 - 200 = 1800. \n\nIn the experimental results, which formulae have been used to compute the cactus plots? Training instances (1800)? Test instances (200)? Both of them? For training instances, the protocol reported in Section 5.2 is relatively clear. But for test instances, what is the protocol? Is CADET using the best policy trained on the 1800 instances for solving the remaining 200 instances? Furthermore, the notion of decision limit is quite confusing. According to Section 5.2, it seems that the decision limit is the horizon of each episode, i.e. the number of calls to the solver CADET using the latest policy estimated by the NN architecture. But this should be clarified unambiguously. \n", "belong_id": "BJluxREKDB"}, {"uid": "Hye1Ff45YH", "paper_title": "Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a GNN to improve an existing search based solver for 2-QBF solvers. The neural network is being used to predict the next steps in the search procedure, in this case, assignment to literals of the 2-QBF formula. Justifiably, a reinforcement learning formulation is used. I thought that the paper was very impressive. All necessary concepts were clearly introduced, the claims were very clear and thoroughly validated. Limitations of the current approach are also properly discussed. \n\nOnly comment I have is that using shallow networks with one iteration sounds not enough for a problem like 2-QBF solving. I noticed that you have this exploration in the appendix, would recommend moving it to the main paper. I would also have liked 2-QBF to be mentioned more explicitly in the abstract and early introduction.\n\nMinor errors:\n'and' --> 'an' in Sec 1\n'variale' --> 'variable' in Sec 4.2\nReference ? in Sec 6 ", "belong_id": "BJluxREKDB"}, {"uid": "Bkg4fpnatr", "paper_title": "Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis will be an uncharacteristically short review. The work poses an interesting idea: why not mix heuristics and learning. It reads as if the paper was written a while ago and the intro was not updated, since there is a lot of related work using the same concept. Please cite existing work in the introduction, it reflects negatively on the paper.\n\nThe elephant in the room is that I have read this paper before. The work has a number of citations and has sparked a good amount of follow-up work. I like the ideas and they have received enough scrutiny already. It was not the best decision for ICLR 2019 to have rejected this paper, honestly. I will argue to accept the paper if the references are updated, it deserves a wider audience.\n\nThat said, I don't like this GNN embedding of the QBF. The negation should have been defined as an edge attribute, not through nodes (as in (Yolcu and Poczos, 2019)). The representation also does not encode the quantifiers well but I feel this is a question for future work.\n\nMinor comments:\n- Please update your paper. It needs a good refresh with the recent literature that cites your work.\n\n- 'An intriguing question for artificial intelligence is: can (deep) learning be effectively used for symbolic reasoning?' => Can representation learning be effectively used for symbolic reasoning? This is one of the most intriguing question in artificial intelligence today.\n\n", "belong_id": "BJluxREKDB"}, {"uid": "H1euOzraKS", "paper_title": "The Frechet Distance of training and test distribution predicts the generalization gap", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors consider the relation between Frechet distance of training and test distribution and the generalization gap. The authors derive the lower bound for the difference of loss function w.r.t. training and test set by the Wasserstein distance between embedding training and test set distribution. Empirically, the authors illustrate a strong correlation between test performance and the distance in distributions between training and test set.\n\nThe motivation to find the relation between generalization gap and the Frechet distance of training and test distribution is sound. However, I am not sure that the lower bound as in Equation (1) is enough. I am curious that one can derive the upper bound for the relation or not. The finding about choosing a training data distribution should be close to the test data distribution seems quite trivial in some sense. I am not clear about its important since it is quite popular that the distribution shift affects the performance and many learning approach assumes same distribution for training and test data. Overall I feel that the contribution may be quite weak, and I lean on the negative side.\n\nBelow are some of my concerns:\n\n1) About the lower-bound in Equation (1), it seems unclear to me that when the W_2(p1, p2) = 0, we can inference any information about the test performance (It seems quite trivial for this case, the left hand side time is greater than or equal 0?) In my opinion, the upper-bound is more important which one can inference much information about the difference of generalization gap.\n\n2) In the proof of Theorem 1, it is quite hard to follow with the current notation, for the integral in (i), (ii) as well as in the proof using the intermediate value theorem, which variables are used? I am confused which one is variable, which one is constants in those integrals.\n\n3) In page 5, at the interpretation (1), for W2(p1, p2) = 0, the learned function fits training distribution perfectly and is not ill-conditioned ==> why one can deduce that the test distribution is fit perfectly? What we have in Theorem 1 is the lower-bound only?\n\n", "belong_id": "SJgSflHKDr"}, {"uid": "ryx1DuaTKr", "paper_title": "The Frechet Distance of training and test distribution predicts the generalization gap", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose to relate the performance of a classifier under distribution shift using a quantity called Frechet distance. It is common belief that the further apart the training and test distributions are, the more difficult it is to transfer a learned classifier. They give simple bounds via gradient norm/Lipschitz constants and distribution distance in Theorem 1. The authors try to capture it with Frechet distance, but I struggle to understand what is new in this work. \n\nFirst, there are a lot of assumptions in the computation of the Frechet distance: \n  1. The authors use the embeddings given by the neural networks instead of the raw data since density estimation is hard. This makes the distance model-dependent \n  2. The authors assume the embeddings are normally distributed in their computation, which have not been justified. \n\nMost importantly, they do not relate the Frechet distance to the lower bound in Theorem 1. There is no estimation on how the learned changes across distributions in the gradient norm term. This makes the evaluation nothing more than a confirmation of the general idea that the closer the distribution, the better the transfer. The lower bound is not used in any quantitative manner. \n\nThe authors should make the connection of the bound and its computation clear, with proper connections to the experiments. The current paper looks like separate theoretical and experimental results that do not tie together. \n\n", "belong_id": "SJgSflHKDr"}, {"uid": "B1goCQ9J9B", "paper_title": "The Frechet Distance of training and test distribution predicts the generalization gap", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the problem of how the mismatch between distributions of training data and test data would affect the generalization gap in machine learning tasks. This phenomenon has been observed many times in previous literature and has gathered significant attention in the machine learning community.\n\nThe paper took a step in relating the change in the performance of the learned function to the Frechet distance (FD), also known as 2-Wasserstein distance, between the input and output distributions and proved that the former is lower bounded by the latter multiplied by a term related to the sensitivity of learning algorithm to distribution shift. The paper also provides empirical evidence that the testing error is correlated with the FD between input and output distributions based on tasks including text classification, image classification, and speech separation.\n\nI find the idea of the paper interesting but the content not convincing enough. The theory proved in the paper does not provide additional quantitive insight beyond intuition. Specifically, the term about the sensitivity of the algorithm is not justified enough in the paper. The experiments provide some evidence but not convincing, especially for the part about image classification.\n\nI also find the statement about the generalization gap a bit misleading. Generally, the generalization gap refers to the gap between the expected error and the empirical error.  But the experiments are mostly presenting the performance on the test data. \n\nOverall, I don't think the paper meets the standard for publication at ICLR.", "belong_id": "SJgSflHKDr"}, {"uid": "Bkg1cfOhYr", "paper_title": "Unsupervised Learning of Node Embeddings by Detecting Communities", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is reporting an unsupervised approach to learn node embeddings and communities simultaneously by minimizing the mincut loss function. This approach programs the data through encoder to generate membership likehood matrix H, and then generates corresponding membership matrix P using node selection. The matrix P and adjacency matrix are coupled to generate community adjacency matrix to minimize the cutting. However, despite such attractive points, the novelty and strength of this study is not outstanding enough for publication in ICLR. Details comments are as follows,\n1. Similar work such as the methods of transferring matrix E to H then to P have already been published , which reduce the novelty of this study.\n2. Though there is a schematic, the learning embedding process is still not described clearly. More details of the algorithms need to be discussed to such as how to get the node embedding matrix E.", "belong_id": "Byl3K2VtwB"}, {"uid": "SylKtUbCFr", "paper_title": "Unsupervised Learning of Node Embeddings by Detecting Communities", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Although this paper seems to only combine existing techniques in community detection and node embedding into a co-train process. The idea is simple and easy understood and the paper is well-written. Theoretical analysis is provided for the approximation error for the sampling strategy. However, major concerns are:\n\n1. Experimental results show that co-training node embedding and community detection can improve the performance for node classification. The improvements may result from the assumption that papers with the same class label are associated with the same community in the citation graph. However, in the dataset, there are many cases that there are not dense connections among the same labeled papers. The authors should check the correlation between the detected communities and the original paper labels.\n\n2. No comparison with other community-preserving node embedding methods, such as 'Community Preserving Network Embedding' in AAAI17\n\n3. Since this paper aims to combine community detection and node embedding process, a set of baseline should be considered. For example, if considering the downstream node classification of node embedding as an evaluation task, then how about the performance of the following two-step method. We can first detect communities based on the node features then do graph node embedding by considering the communities' membership and node features together (e.g. simply concatenating both community membership features and node features).\n\n4. Efficiency and scalability evaluations are needed. Spectral clustering has a scalability issue when meeting big graphs. Since the spectral process is also applied in the proposed method, efficiency and scalability evaluations are encouraged to provide, especially for big graphs which are not covered in the selected datasets in this paper.\n\n5. In Sec 5.3 and Fig 2, it's mentioned that trends of the three datasets are different. For the increasing trend, how about the performance for an extreme case where all nodes are considered in one batch. On the other hand, adding more nodes in one minibatch could provide more information, but why there exists a decreasing trend? Though the authors provide a reason in Sec 5.3, it's better to analyze the reason directly from the datasets.", "belong_id": "Byl3K2VtwB"}, {"uid": "BklTIir0FB", "paper_title": "Unsupervised Learning of Node Embeddings by Detecting Communities", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This work proposes a neural netowrk approach to minimize mincutloss, thus achieving embedding nodes and find communities at the same time. However, it is difficult for me to understand the paper and I feel that it is not clearly written.\n\n1. The algorithm is not written in a box as in Algorithm 1. At first I thought algorithm 1 is the main method, but only after reading it I realized that it is one step of the algorithm. I would appreciate it if the complete algorithm (including input, output, parameters) can be summerized clearly.\n\n2. I am confused about the claim 'spectral approach underperforms significantly on the bibliographic datasets as it only uses the structure information in the graph.' I thought the input of all methods are the adjencency matrix A.\n\n3. In table 2 and table 4, why does the paper compare different methods with different measures? Is it possible to compare all methods using all measures?", "belong_id": "Byl3K2VtwB"}, {"uid": "rkxcmygpFr", "paper_title": "Emergent Tool Use From Multi-Agent Autocurricula", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "# Review ICLR20, Emergent Tool Use...\n\nThis review is for the originally uploaded version of this article. Comments from other reviewers and revisions have deliberately not been taken into account. After publishing this review, this reviewer will participate in the forum discussion and help the authors improve the paper.\n\nI apologize in advance for being reviewer 2.\n\n## Overall\n\n**Summary**\n\nThe article introduces a new multi-agent physics environment called 'hide-and-seek'. The authors trained agents in this environment and studied the emergence of and changes in strategies. The authors also study the performance of these same agents in new 'targeted intelligence tests' compared to training from scratch and compared to agents trained with curiosity.\n\n**Overall Opinion**\n\nI think the environment is very appealing and the paper is overall well-structured and demonstrates novel work. Therefore I'd recommend this paper to be accepted. That being said, there are glaring issues with some of the writing that need to be addressed before I think this work conforms to the standards of ICLR. However, if these issues are addressed, I have no issue increasing my review score.\n\nMain problems:\n\n- The majority of the paper presents essentially a case study of what happened during a single seed of policy training. For RL literature that's very uncommon and I think it's consensus that DRL is very sensitive to random seeds. I know that you do have additional seeds in the appendix, but why didn't you mention those in the main body of the paper? You seem to have found some robustness against multiple seeds, so why not show it? And also the fact that Figure 1 & 3 only apply to 1 seed is not mentioned. I think this is easy enough to fix - I suggest since you're already at 10 pages, to just bring in the additional seeds from the appendix and average over their performance in Fig.1&3.\n- The contributions section is overselling the work: (1) states that autocurricula lead to changes in agent strategy - Maybe I'm mistaken here but that sounds like a tautology. In other words, 'a self-generated sequence of challenges' ('Autocurricula', according to [Leibo et al., 2019][1]) lead to changes in strategy. And (3) advertises 'a proposed framework for evaluating agents in open-ended environments' and also 'a suite of targeted intelligence tests for our domain'. The former of those two is either not in the paper or you mean your section '6.2 Transfer and Fine-Tuning as Evaluation', which isn't novel (see e.g. [Alain & Bengio, 2016][2])\n- Your acknowledgments should be anonymized until publication. Otherwise, reviewers might draw conclusions which group published this work, thus violating the double-blind review procedure.\n\n[1]: https://arxiv.org/pdf/1903.00742.pdf\n[2]: https://arxiv.org/pdf/1610.01644.pdf\n\nLike I mentioned above, I think these are all easy to address, which should allow acceptance of this work. Here are some additional questions, comments, and nitpicks:\n\n## Specific comments and questions\n\n### Abstract\n\n- 'evidence that ... competition may scale better with increasing environment complexity' - that's only shown in the appendix\n\n### Intro\n\n- You mention TD-Gammon as a game, but I think it's an algorithm for the game Backgammon, similarly to how 'Go' is the game and 'AlphaGo' is an algorithm for playing.\n\n### Rel. Work\n\n- all good\n\n### Hide And Seek\n\n- Arena boundaries: What's the penalty and what's ' too far outside the play area'? And in all depictions, it looks like the geometry of the arena is elevated around the edges and the agents don't have a jump action, so how would they ever go out of borders? After watching the videos: Apparently, the jagged-looking arena boundary in the videos is purely cosmetic and agents can still access that space. This is unclear from just the paper and the renderings in Figure 1.\n\n### Policy Optimization\n\n- Policy network and fusion are underspecified: How do you deal with the varying number of agents, boxes, obstacles? Do you just set the x/v of the missing pieces to zero or is the observation actually of a different shape in case there are more/fewer objects or agents? How's the embedding done that is depicted in Figure 2? Also, I didn't see the embedding being mentioned in the text - any reason for that?\n- Figure 2 - This diagram is visually appealing but confusing and needs to be improved. Why does the agent's embedding say '1'? Why do the other agents' embeddings have a '-1' at the end of the orange box and the others don't? Is the agent's embedding concatenated with the other embeddings? If so, why and how (concat, sum, multiply, conditional batch norm, etc.)? In the center and on the right you use a blue block to indicate the agent's embedding and then at the bottom right you seem to use it as a network component or something (between the 'LSTM')? If you're trying to signal that this is the agent's perception at different stages in the network, I'd use a different color to separate it from the agent's lidar and pos/vel. You don't mention that 'x,v' stands for 'position, velocity'.\n \n### Auto-curriculum and Emergent Behavior\n\n- Figure 3: 'environment-specific' (add dash). Draw skill development boundaries like in Fig.1.\n- How exactly does the 'surfing' work? The seekers step (not jump, right, since there is no jumping?) onto the boxes and then what? The momentum propels the box forward? Do other seekers push the box? Their movement on top of the box somehow moves the box (this seems to be the case judging by the videos but this is the least physically plausible)? This is a super interesting adaptation but I'd suspect the physics simulation to have a bug/glitch that's being exploited here.\n- You mention in footnote 3 that the developmental stage and changes in reward aren't necessarily correlated. The same seems to be true for the metrics in Fig.3, which raises the question how did you come up with those boundaries for the different developmental stages in Fig.1? Did someone look at rollouts from the trained policy every couple of million steps? And do all agents learn new skills at the same time or is there a delay? From my understanding, they are all using the same policy and critic networks but maybe dependent on the proximity of an agent to an object/obstacle, it's easier or harder to execute.\n\n### Evaluation\n\n- clear and well-written, slightly too much content in the appendix and not enough in the main paper. Weird appendix numbering - A.6 appears in the main paper pages after A.7\n\n### Discussion and Future Work\n\n- all good\n\n### Appendix\n\n- I appreciate the TOC. I did not look into Appendix B-D because it's another 10 pages on top of the 10 pages of the article.\n\nAll in all an interesting work. Good luck with the rebuttal/discussion.", "belong_id": "SkxpxJBKwS"}, {"uid": "BJlxTZh6Yr", "paper_title": "Emergent Tool Use From Multi-Agent Autocurricula", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Authors in introduce a new competitive/cooperative physics-based environment in which different teams of agents compete in a visual concealment and search task with visibility-based team-based rewards (although There are no explicit incentives for agents to interact with objects in the environment). They show that, complex behaviour emerge as the episode progresses and agents are able to learn 6 emergent skills/(counter-)strategies (including tool use), where agents intentionally change their environment to suit their needs. Agents trained using self-play \n\nIn my opinion, this is an excellent paper which main contribution is to provide experimental evidence that relevant and complex skills and strategies can emerge from multi-agent RL competing scenarios.\n\nMinor comments:\n\n- Hide&seek rules and safety issues: is it not supposed that hiders and the seekers could not get together (i.e., hiders cannot push seekers or as we can see in some videos)? Furthermore, it is surprising (one would say worrying) that hiders identified the barriers as an impediment to the seeker (not only as a way to hide). I wouldnt say that this is a  human-relevant strategies and skills  as the authors claim. Hider agents even double walled seekers!  \n\n- Have the authors thought about joining the Animal-AI Olympics (http://animalaiolympics.com/) competition? It would be a great opportunity to to test the skills of your agents in a further general testing scenario. They provide an arena (test-bed) which contains 300 different intelligent tests for testing the cognitive abilities of RL agents (https://www.mdcrosby.com/blog/animalaiprizes1.html) which have to interact with the environment. \n", "belong_id": "SkxpxJBKwS"}, {"uid": "SkxqkHzRYH", "paper_title": "Emergent Tool Use From Multi-Agent Autocurricula", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "1. Summary\n\nThe authors report on an empirical study of emergent behavior of multiple RL agents learning to play hide-and-seek (a sparse reward task). The main point of this paper is that RL agents learning at scale (large number of samples, batch-size 64000). can learn to solve tasks with strategies that are human-interpretable (e.g., using ramps, boxes). Scale also requires various simplifications (e.g., keeping the learning setup as close as possible to a single-agent problem as possible).\n\nAgents are grouped in 2 teams (seekers, hiders). Each agent receives a team reward, e.g., it can be punished for events that it did not participate in, e.g., if a team-mate is seen by an opponent. If hiders are hidden, seekers also automatically see reward. The first 40% of the episode there is no reward to let hiders hide.\n\nThere is one actor model, all agents share weights. Hence this is self-play: hiders and seekers use the same agent model. Also, all agents use a central value function that can see the entire state (decentralized execution, centralized learning). This makes the setting basically a single-agent problem, with the only decentralized aspect being each actor model only receiving its own observation. Note that a large body of multi-agent RL work in fact uses agents that do not share weights, etc.\n\nOther features described:\n- Auto-curricula: e.g. agents find new strategies (using ramps, boxes) that other agents have to counteract.\n- Human-relevant skills: They report that the agent model learns multiple ways to interact with (objects in) the environment that are semantically interesting (resembles something humans might do).\n- Authors compare with policies learning via intrinsic motivation.\n- Evaluation through transfer learning shows some benefit of transfer of hide-seek agents to auxiliary tasks. However, it is not so clear how this evaluation informs future work on transfer learning (e.g., how would you pick evaluation tasks for a given train-task?) \n\n1. Decision (accept or reject) with one or two key reasons for this choice.\n\nReject.\n\nThe main point of the paper is empirical RL at scale. Although the learned behaviors are human-interpretable, this does not seem surprising given the fact that in many (large-scale) RL applications (Atari games, Go, DotA 2, Starcraft), it has been observed that RL agents can learn to manipulate and use their environment (which includes other agents!) in unexpected ways / find creative ways to exploit the reward function (see e.g. demos in https://www.alexirpan.com/2018/02/14/rl-hard.html). There has also been work on object-level RL [Agnew, Domingos 2018], which involves agents interacting with objects in the environment. Compared to this, the observation that RL agents learn human-interpretable uses of objects does not seem surprising.\n\nThe paper also does not give new insights in how to make large-scale RL ``'work'. For instance, there are no significant differences in algorithm / model structure from DotA / Starcraft agents that can inform future large-scale experiments.\n\nThe paper also does not introduce new concrete evaluation metrics that can apply to other tasks / RL problems, skill detection / segmentation methods to learn the structure of auto-curricula. Furthermore, the setup is very close to a single-agent problem (see above), and is far simpler in the multi-agent assumptions from other decentralized multi-agent work (Foerster 2018, Jacques 2019, etc).", "belong_id": "SkxpxJBKwS"}, {"uid": "HJxO0sg6YH", "paper_title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "UPDATE: I appreciated the authors' discussion. The authors addressed my questions satisfactorily, and I maintain my original rating of accept.\n\n----\nSummary: This papers tackles the question of building an object-centric latent variable generative model of scenes that can sample novel scenes with coherent objects and relationships. To do this, the authors define a generative model, GENESIS, that uses an autoregressive prior over mask variables. The component variables are generated conditioned on these mask variables. The visual appearance of the objects are generated conditioned on the component variables. Inference is done sequentially by inferring some later object varaibles conditioned on others. Results show that the model adopts a consistent strategy in generating and inferring the scene components, first considering the background then the foreground objects. The authors apply GENESIS to three datasets with monochromatic objects and show that GENESIS qualitatively generates coherent scenes and infers coherent scene components.\n\nDecision: Accept. This work clearly addresses a problem beyond current object-centric modeling approaches such as IODINE and MONet, which is the problem of generating novel scenes.\n\nStrengths:\n- The paper is well written and executed.\n- The evaluation is thorough\n- The problem and solution are well motivated\n\nWeaknesses: \n- While the authors demonstrates that GENESIS is able to model static scenes, it is not clear how straightforward it is to extend GENESIS to modeling dynamics for the purpose of robotics and reinforcement learning (as stated in the authors' motivation). Whereas approaches such as IODINE or RNEM (van Steenkiste et al. 2018) treat the object latent that can be propagated through time, maintatining that the same latent models the same object may not be a guarantee for autoregressive approaches such as MONet or GENESIS that re-parse the scene at every frame. Object temporal consistency is especially important when considering tasks that have occlusion, which are important problems in robotics.\n- The results in Appendix D seem to suggest that GENESIS decomposes a scene mostly via color segmentation, as IODINE and MONet do. One concern is that such models that rely mainly on color segmentation are not applicable for real world robotics with various lighting conditions and textures because segmenting based on color may not provide coherent object representations. Would the authors be able to provide an empirical analysis of how GENESIS models a real-world scene, analogous to Figure 11 in the IODINE paper?\n\nVan Steenkiste, S., Chang, M., Greff, K., & Schmidhuber, J. (2018). Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint arXiv:1802.10353.", "belong_id": "BkxfaTVFwH"}, {"uid": "BylK-ZHRYS", "paper_title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors propose a probabilistic generative latent variable model representing a 2D image as a mixture of latent components. It formulates the scene generation problem as a spatial Gaussian mixture model where each Gaussian component comes from the decoding of an object-centric latent variable. The contribution of the proposed method from previous works is the introduction of an autoregressive prior on the component latents. This allows the model to capture autoregressive dependencies among different components and thus help generate coherent scenes, which has not been shown in the previous works. In the experiments, the authors compare GENESIS with MONet and VAEs qualitatively and quantitatively and show that the model outperforms the baseline in terms of both scene decomposition and generation.\n\nThe proposed model seems like the right direction to improve upon MONet and it is nice to see the generation results. It is also nice to see the fully probabilistic modeling of the problem. Although it would improve over the MONet, I'm nevertheless not sure if the framework of sequential component generation (applying both MONet and GENESIS) can be a robust approach to more complex scenes, e.g., with a larger number of objects. Also, the mixture-based approach seems not guarantee the object-level decomposition. For example, if in the scene there are many objects of the same shape, size, and color, I think the proposed model may not properly distinguish them, as shown in some of the wall patterns in the experiments. But, I'm not sure what would happen if K is set to a large number to deal with this. Then, it would have the problem of long-term dependency in sequences.\n\nSome comments and questions:\n1. It would be good to show the qualitative result of the simplified Genesis-s and its qualitative result in the FID experiment.\n2. Is the VAE baselines (BD-VAE and DC-VAE) trained by maximizing the ELBO? If it is the case, will the result of the FID experiment section be different if we train the VAE baseline with GECO?\n3. Some detail of the implementation is missing, e.g. how do we model the posterior q_{\\phi}(z_{k}^{c} | x, z_{1: k}^{m}).\n", "belong_id": "BkxfaTVFwH"}, {"uid": "Bke06RuRYr", "paper_title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a generative model for images. There's a probability mask per-pixel per-component (which yields mixing probabilities), and then a set of latents per-component that yield an image. The system is tested on a set of scenes like the GQN dataset, stacks of blocks, and the multi-dsprites dataset. The system is better than MONet, although there are a few lingering questions.\n\nSummary of positives:\n+ The factoring of the image into various components is eminently sensible and more work should build in the notion of objects\n+ The method is well explained, and I found it easy to understand the entire process.\n+ The method does appear to perform better than MONet, and qualitatively produces good results.\n\nSummary of negatives:\n- There are a few overclaims that should be fixed: namely that the system is a 'generative model of 3D visual scenes' when in reality it is a generative model of images that has a latent space that is perhaps well-positioned to match images generated by 3D scenes composed of objects.\n- The method section sets up a few questions that are never answered in the paper: GENESIS--S gets introduced as a baseline to test a hypothesis, but this never gets quantitatively evaluated (indeed GENESIS-S beats GENESIS in 2/3 of the categories where they are compared head-to-head); similarly, the fact that GENESIS produces probabilities is repeatedly sold as an advantage over other methods, but it's never used.\n- The experiments are weak: the aforementioned questions aren't tackled, as well as a few other issues.\n\n\nOverall, I lean ever so slightly towards rejection. The results are good looking, and the method seems well-explained. However, in my view, the manuscript seems to fall short of the mark. I am not, however, strongly opposed to the paper's acceptance. If it is accepted, I would urge authors to address the issues to make their paper have maximum impact.\n\nBig picture things:\n-I'm baffled by the repeated claim that this is a generative model of 3D scenes. In the introduction 'first object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes'. I'm worried I'm just missing something profoundly obvious and critical -- since it seems obvious to me that it's not 3D. \n\nAs far as I can tell, x here is a HxWxC image (where C is the number of channels and probably 3); the mixing probabilities are over HxW images, so it's a 2D segmentation system. It may be applied to photos rendered with a perspective camera -- but if the requirement for the system is being applied to images that come via projection, then is faster RCNN a 3D object detector? Are normalized cuts, SLIC superpixels, or any of the other myriad unsupervised segmentation methods from vision then 3D scene segmentation approaches?\n\n-Similarly, it's not really a model for objects -- as can be seen in the multi-colored wall example in Fig 3, and Fig 8, 9. its notion of object is a region with uniform color (presumably due to the Gaussian p(x|z) ). So while it's great that it can model regions of uniform color, the system in large part seems to work (in its current form, reading the current manuscript) because the objects are all one color, and doesn't necessarily decompose the scene into objects.\n\nUnless I'm totally missing something, this sort of claim that should be corrected very quickly. I realize that my personal bias is towards more engineering and harder data and I appreciate that there should be different operating points along the spectrum of how much information is provided. However, I think that it's worth making claims and assumptions clear, rather than claiming to solve the general problem on a specific case because of peculiarities about the specific case.\n\n\nMethod:\n-The writing of the method is quite straightforward and written well. The system is clear to me. I largely have no complaints about the method. I do, however, have concerns about the experiments that are done to validate claims in the method section.\n-GENESIS-S appears as this baseline 'To investigate whether separate latents for masks and component appearances are necessary for decomposition', but then basically disappears. There are, as far as I can see, no qualitative results from it, and it basically appears once quantitatively, in S 4.3 / Table 1, where it appears to be as good if not better than GENESIS. In the appendix it's claimed that GENESIS trains faster than GENESIS-S, but if this is the selling point of GENESIS over GENESIS-S, it would be nice to have at least a little quantification of this.\n-There is considerable fuss made over the fact that the system produces probabilities and this is repeatedly mentioned as an advantage over existing systems, but it is never demonstrated that these probabilities are good or useful. This leaves the reader hanging a bit. \n\nExperiments:\n\nOverall the experiments are a little on the weak side.\n\n+The qualitative results are good, but the primary selling point it seems is that the system is better than MONET. This does seem clear, and GENESIS does appear quite a bit better.\n\n- While the results on factoring scenes do appear to be good, there's no quantitative evaluation of this (although the abstract promises it). Section 4.2 essentially says that ARI is bad, without demonstrating in a compelling way that it is. The paper says that this behavior can be seen in Appendix D, but this only shows the oversegmentation and not that this dramatically throws off performance -- I'd expect a figure showing a reversal of expectations -- a clearly worse result getting a clearly better ARI metric. \n\nAdditionally, computer vision has been evaluating unsupervised segmentation for close to two decades (BSDS came out in 2001). The authors should look at the metrics in e.g., 'Contour Detection andHierarchical Image Segmentation' Arbelaez et al. PAMI 2010. I realize that coming up with metrics is hard, but I think the burden is on the authors to find the metrics to show their conclusions quantitatively.\n\n- The tower stability/height/view experiments seem incomplete. I find them quite interesting, but I'm not entirely sure of what to draw from it. The paper obliquely comments that the Groth paper gets better results with a more complex backbone network and blames it on lack of augmentation, use of a subset of the data, and a reduced resolution. But why not just do straightforward like train a MLP on an even further spatially downsampled image? Random here isn't a particularly compelling baseline or reference point. \n\nSmall stuff that doesn't affect my review:\n1) It might be worth explicitly pointing out that p_\\theta(x_k,z_k^c) is just a Gaussian soon after Eqn 3. It's written after equation 5, but without making the p_\\theta explicit. This may help some readers.\n2) The tower stability is done with a one-hidden-layer (512) MLP. I'm of the personal opinion that readouts on latent variables are more informative if it's a linear transformation (since this can't do much heavy lifting).\n3) The abstract promises semi-supervised learning -- is this the tower experiment? This strikes me more as transfer learning.\n\n-----------------------------\n\nPost review update: I have read the authors' responses, and found them thoughtful and to have answered my questions. I'm happy to accept the paper and would encourage the AC to do so.", "belong_id": "BkxfaTVFwH"}, {"uid": "HJlbqymTYH", "paper_title": "Conditional generation of molecules from disentangled representations", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors introduce a variational autoencoder for conditional generation of molecules. The model is borrowed from text-based style transfer, applied here on sequence (SMILES) representation of molecules rather than viewing molecules as graphs (as more recent approaches). From a modeling point of view, the main new part is an additional regularizer whose role is to 1) ensure that the property used as input during generation matches the property derived from the generated molecule, and 2) to dissociate the latent molecule representation in the autoencoder (loosely speaking, its overall structure) from the property being controlled. This regularizer is just a squared difference between predicted and actual properties, averaged over independent samples from latent states and properties which are parametrically mapped to predicted properties via the decoder state (so as to be able to backprop).\n\nThe resulting ELBO criterion for the VAE, together with the regularizer, could be in principle directly used to train all the model parameters (encoder, decoder, property mapping) but apparently this criterion does not result in a 'stable' estimation procedure. The model is then amended in various ways. \n\nThe first change consists of introducing a property prediction part directly into ELBO so that the posterior over latent states will also be affected by property prediction (previously property prediction would only appear in the additional regularizer). Beyond stability that the authors cite as the reason, this is likely also needed because in eq (2) the posterior over latent states given the molecule does not depend on the property. The posterior is derived from a v-structure where latent states and properties appear as independent parents of the molecule. Since for the training cases the property is fixed, this would be fine with an arbitrarily complex posterior encoding. However, when the encoder is defined parametrically, this dependence may be needed, and may relate to the stability issues. It seems a bit roundabout way of putting the dependence back in, if so. The authors should comment on this further. \n\nThe other simplification that is introduced is that the prior over the latent states in the regularizer is modified to be the average posterior of training encodings, and later further collapsed (for computational reasons) to an adaptively estimated simple Gaussian with variance that matches the average posterior variance.\n\nIt's a bit unsatisfying that the authors only use and experiment with a simple logP as the property to control in the model. This is not really a particularly relevant property to control. The approach should in principle generalize to other properties but the many adjustments needed to make it work makes this a little questionable. Also, the results are a bit limited (only logP) and not encouraging (see comments below). \n\n- how is validity defined? SMILES syntax or validity as a molecule? \n\n- is table 1 reconstruction really autoencoder reproduction percentage? Is the correct property for the molecule being reconstructed offered as an additional input to the authors' model? isn't this a bit unfair?\n\n- the correlation between property given as input and the property derived from the generated molecule does not seem like a good metric. Wouldn't a model that overfits (essentially spitting out the nearest property neighbor from the training set) do particularly well according to this metric?\n\n- it seems Figure 6 can be interpreted as also indicating some degree of overfitting?\n\n- the stacked LSTM molecular generator baseline that takes the property as input seems to do much better in terms of maintaining the property in its generated molecules and it also realizes a more diverse collection of molecules. It's true that it doesn't support incremental molecular manipulation directly, eg, if one is keen on keeping the modified molecule close to a starting point. But it is so much better at maintaining the input property in its diverse realizations that perhaps one should instead invest in a tailored sampling procedure to obtain realizations close to a chosen reference.\n\nThe literature references seem to omit all molecular generation work since 2018 \n", "belong_id": "BkxthxHYvr"}, {"uid": "Byo_bZRtB", "paper_title": "Conditional generation of molecules from disentangled representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a VAE-based conditional molecular graph generation model. For that purpose, the disentanglement approach is adopted in this paper: learn to separate property information from the structure representation of a molecular graph. The authors use the supervised VAE objective since the KL regularizer in the objective has reportedly disentanglement-promoting effect. The final objective function is a standard VAE ELBO plus a penalty term of the property value prediction error. Non-differentiable property estimation is conducted via stochastic sampling expectation with the help of an external program (RDKit).  \n\nThe proposed model directly optimizes the generation model conditioned on the property values in a single objective function. This is preferable compared to many existing molecular graph generations, where property optimization is often carried out after the core generative models are trained and fixed. \nDerivation of a stable lowerbound (Eq. 11) is another plus. \n\nMy main concern about the paper is a weak survey for disentanglement researches. Since the core of graph generation model is not original of the authors (adopted from Dai+, 2018), the main technical advancement of the paper should be related to the disentangling VAE modeling. \nCurrent researches in disentangling VAEs go beyond the InfoGAN and beta-VAE. I present a part of the must-referred papers below:\n\n[Gao19] Gao+, Auto-Encoding Total Correlation Explanation, AISTATS, 2019. \n[Kim_Mnih18] Kim and Mnih, Disentangling by Factorising, ICML, 2018.  \n[Ryu19] Ryu+, Wyner VAE: Joint and Conditional Generation with Succinct Common Representation Learning, arxiv:1905.10945, 2019. \n\nAmong them, [Ryu19] is closely related to the proposed framework. In my understanding, the variable dependency structure studied in this paper (Fig. 1) is studied by [Ryu19]. The authors should clearly state the novelty of the proposed work in the literature of these disentanglement VAE works. \n\nI think the L_disent (Eq.5) is not a penalty for disentanglement: it enforces the model to correctly predict target property values, and do not say anything for factor disentanglement, correct? \n\nI have a few concerns about the experiment designs. \nIn the table 1, reconstruction performance evaluations, the authors chose string(SMILE)-based molecular graph generation models. However, it is largely admitted in the molecular graph generation studies that the graph-based generative models generally performed better. I want justifications for the choice of string-based generation models. Extending the table 1 with graph?base SotA generation models will strengthen the manuscript greatly. \nFor example:\n\n[Jin18] Jin+, Junction Tree Variational Autoencoder for Molecular Graph Generation, ICML, 2018. \n[You18] You+, Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation, NeurIPS, 2018. \n\nEspecially, [You18] directly optimize the property values of the generated graph. This is closely related to the goal of this paper, thus a proper reference and discussions are strongly expected. \n\n\nFigure 5. is an attractive visualization of the latent (z, y) vectors. However, the authors do not provide how to understand the figure. I GUESS that the authors want to show that learned z and y are somehow disentangled: the structural changes of molecular graphs are less sensitive to the vertical axis (property value) than the horizontal column (different). Please clearly state key messages of each figure and table. \n\nThere are several presentation issues. They are details, but fairly degrades the readability of the manuscript.  \n- Too small letters (alphabets) in Figure 3, 4, 6, it is simply unreadable so I cannot tell main messages of these figures (So I do not evaluate these figures positively). \n- In table 1, what are the significance figures of the presented scores? Some scores have 3 digits, others have 4 digits. Some are rounded at 0.01 precision but others are round at 0.1 precision. \n\nEvaluation summaries\n+ A graph generation model combining conditional property optimization in a single objective function.\n+ A new lowerbound for stable training (Eq.11)\n+ The property and the structure of the graph are somehow disentangled in the experiment (Fig. 5). \n+- Disentanglement effect is not modeled directly in the proposal. L_disent is not for disentanglement but for property regression.\n-- Survey for disentangling VAEs is not enough. This makes the proposal less convincing in terms of the novelty and the contribution compared to the recent dissent-VAEs. \n- Not compared with SotA graph-based molecular graph generation models. \n- Key messages of figures and tables are not clearly stated (e.g. Fig.5). \n-- Some figures are simply unreadable. The significance figure of the table 1 is unclear. These are formatting details but essential for readability. ", "belong_id": "BkxthxHYvr"}, {"uid": "HJeJEM-0tS", "paper_title": "Conditional generation of molecules from disentangled representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "# Summary\nThe paper considers the problem of generating molecules with desired properties using a variant of supervised variational auto-encoders. The key novelty is the idea to learn a representation with disentangled molecular properties which can be modified during generation of novel molecules.\n\nTo account for the diversity of molecules in a particular target class the conditional probability of a molecule x given a target property y is modelled with a latent variable z, i.e., p(x | y) = \\int p(x | z, y) p(z) dz.\n\nThe 'style transfer' is achieved by taking an initial molecule x and computing the posterior of novel molecule x' with modified property y' via p(x' | y', x) = \\int p(x' | y', z) p(z | x) dz.\n\nThe latent representation is learned using a supervised auto-encoder (Kingma et al., 2014), formally specified in Eq. (2). The optimization problem is then further extended by imposing a soft constraint that the molecules from a particular class exhibit a certain property (Eq. 3). The reason for this constraint is to enforce that the conditional probability p(x | y, z) takes into account the information in label y. This part might not be sufficiently well-motivated and would benefit from strengthening the argument for the property predictor and soft constraint (would an illustration be possible here?). In particular, why should it not be possible to encode (and keep fixed during training) the information present in y into the sufficient statistics of p(x | y, z)?\n\nThe soft constraint involves an oracle function (see Eq. 3), which evaluates the designed molecules. Section 3.2 deals with the approximation of the oracle via a property predictor function to side-step the fact that the oracle is not necessarily differentiable or CPU-bound. Section 3.3 and 3.4 deal with joint optimization of the supervised variational auto-encoder and gradient estimates.\n\nThe approach is evaluated on two datasets, relative to state-of-the-art baselines based on variational auto-encoders and deep learning. The main goals of the experiments are to establish that the approach can learn useful disentangled conditional distributions over molecules and to assess the extent of improvement in the data generation process as a result of using the soft constraint term. The focus of the experiment is on generating molecules with certain range of logP values. The data generating process is simulated/evaluated while controlling for the logP value range and molecular structure.\nThe experiments provide an objective assessment of the merits of the approach and indicate clear advantages over prior work. \n\n\n# Recommendation\nThe paper is very well written, easy to follow, and properly structured. The work is novel and the idea to disentangle molecular properties from the target property is quite interesting. The experiments are detailed and compare to baselines based on variational auto-encoders and deep learning. The part that could be improved is the choice of the target property. In particular, it would be great to evaluate the approach on the problem of finding molecules binding to a certain protein site. Such a problem is likely to exhibit scaffold hops and activity cliffs which make drug design problems very difficult (e.g., see [6-8]).\n\n\n# Related work (additional references)\n- While mapping of the discrete space of molecules to a continuous latent space (in active learning approaches combined with VAE) allows for gradient computation and the use of active learning/optimization techniques, the very difficult problem of finding latent space pre-images persists (in general, the problem should be in the NP class). The latter refers to finding a molecule that maps to a fixed point in the latent space. There have, however, been some efficient approximations in special cases [1-3].\n- In [4] and [5], an approach for generation of molecules with desired properties has been pursued with posterior sampler based on a conditional exponential family model. The representation is based on a tuple kernel mapping mapping (x, y) to some reproducing kernel Hilbert space (without disentanglement). The approach provides a consistent data generation process and a mean to deal with the fact that designs are not independent and identically distributed (e.g., Eq. 2 assumes that examples are IID).\n\n\n# Black-box target property\n- The logP property might not be a good proxy for the effectiveness in generating molecules with desired binding activities (to some protein site) because (to the best of my knowledge) it does not exhibit the 'activity cliff property' characteristic to drug design. This refers to the property that a small change in molecular structure results in a completely different activity level (e.g., see [6-8] and [4]). In this sense, the 'style transfer' might be beneficial for finding molecules binding to a certain protein site that are structurally similar to some available molecules with bad activity levels (for which a synthesis path might be known or easy to derive).\n- An actual black-box property (realized via a docking program) that is expensive to evaluate served as a target property in [5]. That docking program (supplied with suitable binding/docking constraints) can provide a nice proxy for the actual generation of molecules.\n\n\n# References\n[1] G. H. Bakir, J. Weston, and B. Schoelkopf. Learning to find pre-images, NIPS 2004.\n[2] C. Cortes, M. Mohri, and J. Weston. A general regression technique for learning transductions, ICML 2005.\n[3] S. Giguere, A. Rolland, F. Laviolette, and M. Marchand. On the string kernel pre-image problem with applications in drug discovery, ICML 2015.\n[4] D. Oglic, R. Garnett, and T. Gaertner. Active search in intensionally specified structured spaces, AAAI 2017.\n[5] D. Oglic, S. Oatley, S. Macdonald, T. Mcinally, R. Garnett, J. Hirst, and T. Gaertner. Active search for computer-aided drug design, Molecular Informatics 2018.\n[6] G. Schneider and U. Fechner. Computer-based de novo design of drug-like molecules, Nature Reviews Drug Discovery, 2005.\n[7] P. Schneider and G. Schneider. De novo design at the edge of chaos. Journal of Medicinal Chemistry, 2016.\n[8] J. Scannell, A. Blanckley, H. Boldon, and B. Warrington. Diagnosing the decline in pharmaceutical R&D efficiency. Nature Reviews Drug Discovery, 2012.", "belong_id": "BkxthxHYvr"}, {"uid": "H1gbR6xTtB", "paper_title": "Collaborative Training of Balanced Random Forests for Open Set Domain Adaptation", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper introduces a method for domain adaptation, where each domain has noisy examples. Their method is based on a decision tree in which the data at each node are split into equal sizes while maximizing the\ninformation gain.  They also proposed a way to reduce domain alignment. Their method is tested on several noisy domain adaptation settings and performs better than other baseline methods. \n\nPros:\nTheir idea to utilize a decision tree for domain adaptation sounds novel. \nExperiments indicate the effectiveness of their method.\n\nCons:\nThis paper is not well-written and has many unclear parts. \n1, The presentation of the problem set is unclear throughout this paper. In the abstract, they mentioned that they tackle the situation where both source and target domains contain noisy examples. However, they did not define the exact problem setting in any section. I could not understand what kind of problem setting motivated their method, which makes it hard to understand their method. \n2, How they actually optimized the model is also unclear. From Eq 1~4, it is hard to grasp how they trained the model. \n3, In open-set domain adaptation, simply minimizing domain-distance can harm the performance. How does the method avoid this issue? It was also unclear. \n4, Experimental setting seems to be wrong and unclear. In Openset1, they say that 'The labels from 1 to 10 of both source and target domains are marked as the known class, and all data with label 1120 in the source domain and label 2131 in the\ntarget domain are used as one unknown class'. However, Saito et al. (2018) used 21-31 classes in the target domain as one unknown class. In addition, 'According to Saito et al. (2018) the target data of the unknown class is not used in training, ', they used the 21-31 classes for training in an unsupervised way. How is this method used to detect unknown class? Is there any threshold value set for it?\n5, The experimental setting is unclear. In 4.4, ', we use only 10% of training samples', does it mean 10 % training source examples or target examples? This setting is also unclear. \n\nFrom the cons written above, this paper has too many unclear parts in the experiments and method section. I cannot say the result is reproducible given the content of the paper and the result is a reliable one. They need to present more carefully designed experiments. \n", "belong_id": "SkeJPertPS"}, {"uid": "ryg6fy_ptB", "paper_title": "Collaborative Training of Balanced Random Forests for Open Set Domain Adaptation", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new target objects for training random forests that has better generalizability across domains. The authors demonstrated that the proposed method outperforms existing adversarial learning based domain adaptation methods.\n\n\nStrength\n\nThe paper is clearly-written. The two objectives(balanced split and common split distribution between source and target domain) are well motivated and explained in the paper.\n\nThe authors show that empirically the proposed method outperform several existing adversarial learning based domain adaptation methods.\n\n\nWeakness\n\nOne of the main draw back of the method is that it relies on the features extracted from existing pre-trained neural networks, and cannot be used to update the representation of the neural networks. While the adversarial learning based method could do end to end training.\n\nIt would be great if the authors could clarify the setup of the baseline methods(e.g. Whether the baseline methods also take benefit of imagenet dataset, and is trained end to end).\n\nWhat will happen if you do not have the imagenet models and have to train all the models from scratch?\n\nOverall I think it is a borderline paper that might be interesting to some audiences in the conference.\n", "belong_id": "SkeJPertPS"}, {"uid": "S1l_sCFXcS", "paper_title": "Collaborative Training of Balanced Random Forests for Open Set Domain Adaptation", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an approach to building random forests that are\nbalanced in such a way as to facilitate domain adaptation. The authors\npropose to split nodes not only based on the Information Gain, but\nalso so that the sizes of each set passed to left and right children\nare equal. Another extension to the standard random forest training\nprocedure is the use of a collaborative term subtracted from the\ninformation gain over the source domain. This term encourages\nalignment of the source and target domains in the leaves of trees in\nthe forest. Experimental results are given on a range of standard\nand open-set domain adaptation datasets.\n\nThe paper has a number of issues:\n\n1. There are some problems with clarity, and the English is somewhat rough\n   throughout. These problems are not terribly distracting, but the\n   manuscript could use more polish.\n2. I don't see a detailed discussion anywhere about the\n   hyperparameters used for fitting the random forests. How many trees\n   are used? What is the max depth? These parameters should be\n   discussed and included in the ablations in order to appreciate the\n   complexity/performance tradeoffs.\n\nThis paper has some interesting ideas in it, and the experimental\nresults are excellent. I would encourage the authors to move salient\nmaterial from the supplementary material to the main article and to\nprovide a more thorough discussion of the complexity of the models\n(the structural parameters of the trees/forests).", "belong_id": "SkeJPertPS"}, {"uid": "HklcwLdRFr", "paper_title": "Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposed a model for extracting global feature interactions from the source model which was later being encoded in the target model to enhance its prediction performance.\n\nStrong points:\n1. The paper laid out the necessary background knowledge very clear even for the audiences outside this area.\n\n2. The paper performed reasonable amount of experiments to compare the proposed model with various baseline models with various data sets, which are strong enough to support the claims made in this paper.\n\nComments:\n1. The theoretical innovation of this paper is trivial. The local detection model for feature interactions, i.e. MADEX is simply employing the previous work, LIME and NID. Its global extension, i.e. GLIDER and the proposed encoding method are straightforward. \n\n2. The descriptions on the feature dimensions of the original data and the generated binary representation data x are rather confusing and inconsistent throughout the paper. If I understand it correctly, the data sample from the original feature space x \\in R^p, and a  binary representation x \\in R^d, where d <= p. However, when d first appears in the first paragraph of section 3, it is defined as the dimension of the original feature space and later in that paragraph the dimension became p in f(.): R^p -> R. There is no clarification on the differences between d and p till section 4.1 where it states x \\in R^p . However, the dimension of the data instance from the original feature space changed to d again in section 4.2 where it states x =[x_1, x_2, ..., x_d]. \n\n3. Whats more, the same approach to interpreting interactions can provide new insights into domains even beyond recommendation. should be Whats more, the same approach to interpret interactions can provide new insights into domains even beyond the recommendation.\n\n4. An interaction, I, is a subset of all input features..., but according to the following sections, I is the indices of a feature subset instead of the features themselves.\n \n5. ...by requiring the same cross feature ID to occur more that T times in a batch of samples,..., that should be than.\n\nOverall, although there is no significant theoretical innovation, it is a decent application paper.\n", "belong_id": "BkgnhTEtDS"}, {"uid": "rklPOdey9H", "paper_title": "Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a method to detect which features in the input of recommender systems are interacted each other, i.e., combining them behaves useful information, and examines to feed extracted interactions directly into the recommender systems to measure effects on actual recommendation.\n\nThe interaction detector consists of 1. perturbing the input vectors,  2. training NNs to utilize its internal non-linear representation as a signal of interaction, and 3. aggregating detected interactions over training set. 1. and 2. are consisting of known methods so that the proposed method is an application of them. For 3. authors introduced a simple heuristic (Algorithm 1).\n\nAs long as I heard how the NID work to detect interaction from this paper, it is sensitive not only the true interaction between features but also set of features holding similar signals (e.g., if a hidden unit behaves as a feature A, it may be natural to aggregate all features that implies A by itself, regardless of the meaning of their interaction). This is not a desired case as long as the paper specified the interactions in section 3. Maybe it requires more detailed explanation about how good applying NID for this task is.\n\nExperiments are conducted to show the behavior of the interaction detector and actual improvement of utilizing extracted interactions as additional features. Experiments look less informative for comparing the proposed method with other existing methods for similar motivations (e.g., some methods introduced in related work) since there is only a trivial baseline by the original LIME's method.\n\nIn the case-study of Figure 4(b), I thought that the proposed method is a bit biased for frequent but meaningless features, because it detected 'I' or 'a' that intuitively occur with any labels. It is maybe because the Algorithm 1. that simply aggregates all detected interactions regardless of their actual importance. For further improvement, it may be necessary to introduce some weighting strategy to detected interactions.\n", "belong_id": "BkgnhTEtDS"}, {"uid": "ryxiVxxs9H", "paper_title": "Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is focused on identifying/discovering feature interactions in blackbox models (with a focus on recommendations). Specifically the technique works by first corrupting datapoints and then using this 'local dataset' to find interacting features via lasso-regularized multi-level perceptron approach (NID from Tsang et al). Using an expanded feature space and repeated calls to the above steps this is then used to find the most commonly occurring patterns. \n\nThe empirical section is quite interesting, with some nice mix of qualitative and quantitative results. The quantitative results in particular are quite intriguing -- across recommendation and non-recommendation tasks.\n\nOverall the paper makes for an interesting read. On the whole I lean slightly positive -- largely due to the empirical section and the quantitative gains observed by adding the feature interactions as features to the model.\n\nThat said I had quite a few concerns about the work:\n\n- In general the exposition was quite lacking in the methodological section. I had to re-read the paper a few times to be able to make out to fully understand the underlying methodology. I would make the overall pipeline very clear and explain out the MADEX pipeline very clearly. The feature expansion section (4.3) in particular was not very clear -- as to how it fit in with the rest of the pipeline and something that could do with more work.  Nomenclature like source and target model are used somewhat arbitrarily and again can be clarified.\n\n- Another concern I had with the approach is how this would work if the underlying features itself were not known. Say for example you had a text understanding model -- the tokenization and vocabulary/OOV etc .. may all be components of the blackbox. Likewise for vision models, what features are being used may vary.\n\nBy assuming (effectively perfect) information about the features used, this is no longer really a 'blackbox' model. I would have wanted to see some deeper discussion on this topic.\n\nI also had some concerns about the scalability of dense features and their bucketization in this approach. I'm not entirely convinced the method would work as well and efficiently in such feature spaces. I would have wanted to see some more empirical evidence and understanding on this topic.\n\nIf possible it would have also been great to understand higher-order interactions and the ability of the model to find them empirically. \n\n- One thing I wasn't entirely certain of was the amount of novelty in the work since it leverages existing works like NID (Tsang et al) and LIME (Ribeiro et al) for some of the key aspects of the method. It would be nice to see some discussion on this front as well.\n\n- I also would have liked to see significance testing being performed in general across the experiments.\n\n- Another smaller concern stemmed from a lack of mention of the stopping criterion of the model. In particular I'm wondering how models were stopped further training / best checkpoint was picked. This would be good to elaborate on to make sure that the models were all compared fairly from a computational perspective.\n\n- I also had some concerns with the scalability of the approach in large features spaces but am willing to give the authors the benefit of doubt on this one based on the 3 hour number they quoted from their experiments (which indicates something that is not prohibitively slow)\n", "belong_id": "BkgnhTEtDS"}, {"uid": "r1xK6GH5YS", "paper_title": "Deep Imitative Models for Flexible Inference, Planning, and Control", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n- key problem: expert-like probabilistic online motion planning to reach arbitrary goals without reward shaping thanks to off-line learning from expert demonstrations;\n- contributions: 1) an imitative planning procedure via gradient-based log-likelihood maximization leveraging 'imitative models' q(future states | features), 2) multiple proposals to define flexible goals in this probabilistic framework, 3) a complete implementation for end-to-end navigation in CARLA, 4) an extensive experimental evaluation showcasing the performance, flexibility, interpretability, and robustness of the proposed approach w.r.t. the previous state of the art and several Imitation Learning (IL) and Model-Based Reinforcement Learning (MBRL) baselines.\n\nRecommendation: weak accept (leaning towards strong accept)\n\nKey reason 1: principled probabilistic framework bringing the best of both IL and MBRL worlds.\n- this planning as inference method is very succinctly and elegantly described in the paper with enough details in appendix (+ code) to suggest a high chance of reproducibility;\n- the flexibility of defining different interpretable goals (6 different types explored in the paper) highlights the versatility of the approach;\n- the additional benefits in terms of plan reliability estimation (Appendix E) are significant;\n- the paper showcases how powerful and useful a good 'imitative model' can be, therefore, reinforcing the interest of the research community in the important topic of off-line learning from large datasets of demonstrations (without requiring costly on-line data collection).\n\nKey reason 2: thorough experimental evaluation with convincing results.\n- the experimental protocol used is the standard one on CARLA and the results are state of the art;\n- the comparison with related works, including recent ones, is thorough and well explained;\n- the additional claims regarding robustness are substantiated by multiple experiments.\n\nSuggested improvements:\n- Not needing reward engineering is a major claim of this approach, but it seems that constructing goal likelihoods could be seen as a form of reward engineering, no? Table 3 indeed reports significant performance differences (absolute and relative) depending on how the goals are specified, especially in dynamic environments. As the experiments aim at maximizing the same performance metrics, is there a preferred goal type that works well across all experiments? If not, how is 'goal definition' different than 'reward engineering'?\n- Could the authors please include a variance analysis (using different seeds) in Tables 3 and 4? Previous papers have reported high variance in similar settings (cf., Codevilla et al 2019), and this is a common issue in IL/RL.\n- How important is knowing \\lambda (traffic light state) perfectly in practice? Can the robustness to noise in \\lambda be experimentally assessed? I would also clarify in section 4 and Table 2 that other methods do not use \\lambda (the traffic light state), which is a signal very strongly correlated with the 'ran red light' metric.\n- More generally, what is the robustness of this approach to uncertainty / noise in \\phi? Although it is typically available (as the authors mentioned) it is never perfect in practice. Can this be handled in a principled probabilistic way as an extension of the current formulation?\n- The current model does not factor the influence of the agent on its environment (\\phi := \\phi_{t=0}). Is this framework limited to open loop planning, or does this open interesting future research directions towards closing the loop? It seems to be a key open problem to at least discuss in Section 5.\n\nAdditional Feedback:\n- Figure 5 is confusing, not sure it adds much value to the paper;\n- typos in Appendix ('pesudocode', 'baselines that predicts', 'search search').\n\n ## Update post rebuttal\n\nThanks to the authors' excellent replies and my initial inclination towards strong accept, I am happy to bump my score to 8. The authors did an excellent job, their rebuttal is on point, not avoiding hard questions, running additional requested experiments (incl. in a clever way for the most computationally expensive ones), and showing clear insights in future steps. Great job!", "belong_id": "Skl4mRNYDr"}, {"uid": "r1xBD1MnKS", "paper_title": "Deep Imitative Models for Flexible Inference, Planning, and Control", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper propose imitative models that learns goal-based probabilistic models of expert demonstrations, and use this to perform test-time planning and control of certain goal-directed behavior. The paper demonstrates extensive and impressive experiments on the CARLA simulator and outperforms existing approaches in the success metric.\n\nThe idea is quite simple: given a set of states, learn a probabilistic model that assigns high probability likelihood to expert behavior. After training, inference is performed to optimize the likelihood of a goal according to an expert prior. The paper discusses extensively how goal-based likelihood functions would be designed for autonomous driving, and the architectures for good q(s|\\phi) in detail, which are of engineering importance in self-driving applications. While I believe the method described might not be significantly novel technically, I believe the paper made nice contributions in terms of the autonomous driving application.\n\nMinor comments:\n\t- It seems like the term 'state' is used to represent the agent's location on the ground plane, which is also not technically the entire state information?\n\t- Is the argmax in (1) a strict equality? I guess you would assume q(s|\\phi) = p(s|\\phi) for this to be always true?\n\t- How many iterations does it take to converge in Algorithm 2? It seems you would need to updates z{1:T} for all newly encountered goals? \n", "belong_id": "Skl4mRNYDr"}, {"uid": "S1x_4iWTYH", "paper_title": "Deep Imitative Models for Flexible Inference, Planning, and Control", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper takes a model-based approach to 'imitation learning'. It first learns a (flow) 'model' to assign likelihoods to trajectories being expert-like as well as sample expert-like trajectories. This is then combined with an estimate for trajectories being certain goal conditioned, where the goals come from a route planner. Results are shown in the context of autonomous driving in the CARLA simulator with a PID controller tracking the open-loop plan from the planner.\n\nOne major issue with the paper is that all the main contribution seem to be in the appendix. If I were to summarize it, the paper introduces an interesting way of integrating two different experts to perform learning from demonstration. On the one hand, you have the A* algorithm as an expert providing what waypoints to follow. On the other hand, you have the expert demonstrating how to drive around on the road. The question becomes, how do we reconcile these 'experts' acting at different abstraction levels so that our system works in a more general setting. This kind of a setting is definitely not as general as the paper tries to make it out to be, but nevertheless it's reasonably broad and useful.\nThe idea to of different subsystems for route planning and path planning are not new, but the way it's done here does seem interesting to me.\n\nNote that Sec 3 related work CILS description seems hard to understand. Moreover, the claim 'MBRL can also plan, but with a one-step predictive model of possible dynamics', seems incorrect. One can do multi-step predictions and use those to do model-based RL as well. So it's unfair to claim that this other approach is MBRL and yours is not on that basis. \n\nExperiments are interesting although quite dense. It's unclear what the initial state distribution look like for these experiments though. I'm guessing CARLA is otherwise a deterministic simulator?\n\nNow I am going to do something that you are not supposed to do as a reviewer. Tell the authors the paper they should have written, rather than the paper that was submitted. Apologies for that.\nThe terminology of Inverse RL, Imitation Learning and Apprenticeship Learning is a mess in the literature unfortunately and can't blame you not trying to fix it in this paper. However the community would benefit if don't overload these terms. Although all of these fall under learning from demonstrations, it's useful to restrict Inverse RL as trying to figure out the reward function being optimized by an expert, imitation learning as learning the exact behavior of an expert (and therefore not doing better than the expert which are by definition optimal or being applicable to doing something other than the expert) while apprenticeship learning as learning from demonstrations to perform even better at the task even generalizing to other things than the demonstrations. With these definitions under the belt the work in this paper is better positioned as about apprenticeship learning. Learning a model from the expert demonstration about how the world works and then using the same for accomplishing different tasks for which there are _no demonstrations_ doesn't sound like imitation?", "belong_id": "Skl4mRNYDr"}, {"uid": "Hyejd9jotH", "paper_title": "Gap-Aware Mitigation of Gradient Staleness", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a simple idea to mitigate gradient staleness in asynchronized distributed systems. Instead of scaling the staleness as in SA method, the authors propose to scale with the GAP, which is defined as the distance between current parameter and the staled parameter.\n\nThe paper is fairly well written, and the overall idea is interesting and simple in implementation. The authors also derive convergence bounds for the proposed algorithm and provide reasonable experimental results. I have the following comments:\n\n1. I think the theory, at the current stage, is not sufficient. The authors only provide a convergence bound for the proposed method, which does not tell anything about the advantages over other methods such as the ASGD and SA methods. I think a detailed comparison should be conducted. From the current presentation, it seems there is no theoretical advantages of the proposed method over existing methods, which seems to suggest that the empirical advantage might not come from the algorithm itself.\n\nI also notice that comparisons of convergence are illustrated in the appendix. However, it shows all SA and GA have similar convergence rates. This seems to indicate that SA and GA should performed similarly, which is not the case in the experiments.\n\n2. For experimental results, in Figure 2 (and others), how do you get the errors for different workers? I mean, how long do you need to run for each case? You should make sure that all the settings should use the same computational resources for fair comparisons, but this is not described in the text, making the results doubtable.\n", "belong_id": "B1lLw6EYwB"}, {"uid": "B1lQL-d6YH", "paper_title": "Gap-Aware Mitigation of Gradient Staleness", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces a new variant of asynchronous SGD, GA-ASGD, for distributed training. The goal is to mitigate the gradient staleness issue caused by asynchronously applying gradients to an old version of parameters. Prior work addresses this issue by penalizing the learning step of a worker linearly to its missed updates since getting a replica of parameters. However, that approach does not consider the differences between the old and new versions of parameters, which can cause over-penalization and under-penalization. The main contribution of this paper is to introduce a new way of measuring weight staleness and to explore the idea of penalizing the gradient itself to mitigate the staleness issue. The paper does a good job of discussing how GA can be applied to existing optimizers such as Adam. It performs empirical studies on ImageNet and Transformers-XL to conclude that GA-ASGD outperforms ASGD and the prior staleness aware approach. It also demonstrates the scalability of GA by scaling up to training with 128 asynchronous workers.\n\nStrengths:\n+ Introduced a novel approach to measure the parameter staleness, which helps penalize the gradients instead of the learning step. \n+ The approach seems to be easily applicable without much additional hyperparameter tuning. \n+ Evaluation on both image and NLP tasks demonstrate that GA leads to improvement over prior staleness aware approaches.\n\nWeaknesses:\n- Despite great improvements over prior ASGD based approaches, there is still a quite noticeable accuracy degradation compared to the SGD baseline. \n- The evaluation is done with simulation, and no report of the number of training steps to converge and end-to-end training time, making it difficult to compare the efficiency with SGD based approaches. \n\nOverall, I think this is good work. The idea of defining weights stableness as the minimal number of updates required to traverse the current distance between the master and worker weights seems to be reasonable. The comparison to prior ASGD based approaches are extensive, and the improvements seem decent. The fact that it does not require much or additional hyperparameter tuning also seems neat. \n\nMy major concern comes from results compared to SGD and motivation. The fact that almost all ASGD based workloads, even with the help of GA, cannot get on-par accuracy as the SGD baseline bothers me. In particular, the accuracy drops considerably as the number of asynchronous workers grows (Fig 1. and Fig. 2, and Table 3). For example,  the gap between the accuracy of GA vs. SGD can be as large as 3.46% when there are 128 workers. That gap might be closed with additional hyperparameter tuning, but it is unclear from the current draft and results. In the Transformer-XL example, GA has to limit the number of asynchronous workers to 4 in order to get close to baseline accuracy. It would have been better to show the trade-off between accuracy and performance in comparison with SGD. \n\nOn the other hand, the recent advance of SGD using large-batch training achieves great results on large model training such as BERT [1]. It helps to improve the compute/communication ratio, which seems to mitigate the straggler issue. Given that today's cloud service largely uses homogenous accelerators (TPU/GPU of the same SKU), it is less clear whether it is really beneficial to train with ASGD despite the improvements from GA.\n\nQuestion:\nOne question is about how the batch size should be changed as the number of workers increases. The convergence analysis indicates that by increasing the batch size, the convergence speed of GA-ASGD will decrease. Does that mean the batch size should remain relatively unchanged to avoid negatively impacting the end-to-end training?\n\nIs there any benefit to applying Gap to model parallelism paradigm such as pipeline parallelism?\n\n[1] 'Large Batch Optimization for Deep Learning: Training BERT in 76 minutes', by You et. al.", "belong_id": "B1lLw6EYwB"}, {"uid": "Sylx4uze9H", "paper_title": "Gap-Aware Mitigation of Gradient Staleness", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies training large machine learning models in a distributed setup. For such a setup, as the number of workers increases, employing synchronous stochastic gradient descent incurs a significant delay due to the presence of straggling workers. Using asynchronous methods should circumvent the issue of stragglers. However, these asynchronous methods suffer from the stale gradients where by the time a worker sends the gradients to the master server, the model parameters have changed based on the gradients received from other workers. This leads to severe performance degradation in the models trained by using asynchronous methods, especially where the number of workers scales.\n\nThis paper proposes a gap-aware method to reduce the adverse impact of stale gradient on the asynchronous distributed learning. In particular, when the master receives a gradient from a worker, it computes the norm of the difference between the current model parameter and the past model parameter associated with the gradient. The master then computes *gap* value based on this norm and the norm of the average gradient. Before employing the gradient, the master scales the gradient by the computed gap value. The paper establishes the convergence rate for the gap-aware asynchronous method which is similar to the convergence rate of SGD. The paper then performs an extensive experimental evaluation of the proposed method over different datasets and models. The empirical results demonstrate the advantage of the proposed gap-aware method over other baselines.\n\nPros\n\n- The extensive empirical evaluation shows that the proposed method is effective in preventing performance degradation in an asynchronous setup across tasks and models. \n\n- The method outperforms other solutions to combat stale gradient, such as the staleness-aware method by Zhang et al.\n\n- The paper shows that the proposed method can be combined with DANA (Hakimi et al.) and achieves the performance very close to the synchronous setting while realizing the speed up provided by the asynchronous methods.\n\nCons\n\n- It was not clear to the reviewer how the convergence analysis of the proposed method differs from the existing analysis in the literature and if any novel ideas were involved in obtaining the theoretical results presented in the paper.\n\n- The gap-aware method increases the overhead at the master as it needs to store the most recent model parameters sent to each of the workers. This is much higher than the staleness-aware method where the master stores a single scalar for each worker. The paper barely discusses such overheads associated with the proposed method.", "belong_id": "B1lLw6EYwB"}, {"uid": "Syxxveg0tH", "paper_title": "Learning robust visual representations using data augmentation invariance", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Motivated by biological visual systems, this paper investigates whether the representations of convolutional networks for visual recognition are invariant to identity preserving transformations. The results show that empirically they are not, and they further propose a data-augmentation approach to learn this invariance. Since transformations can be automatically generated, this does not require additional manual supervision.\n\nThe main weakness of this paper is that the approach is mostly data-augmentation, which is standard. Whereas standard data augmentation simply adds the transformed examples to the training set, this paper goes one step further and adds an additional regularization between two different views, so that they lie in the same space, such as using Euclidean or cosine distance. However, these loss functions are widely known and applied as a baseline in metric learning.\n\nThe problem itself is also well known in computer vision, and there are several empirical papers that demonstrate this. For example, the recent paper 'Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects' at CVPR 2019.\n\nTaking a step back, do we even want convolutional networks to be rotation invariant? The categorical label of some objects could change depending on the rotation. For example, a door is only a door if it is upright against a wall. If you rotate the door, it just becomes a board. \n\nIn my view, the novelty of this paper lies in the application of standard approaches to a standard vision problem. Due to this, I feel this contribution is not sufficient for ICLR. \n", "belong_id": "B1elqkrKPH"}, {"uid": "rylAHbq7cS", "paper_title": "Learning robust visual representations using data augmentation invariance", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes to explicitly improve the robustness of image-classification models to invariant transformations, via a secondary multi-task objective. The idea is that the secondary objective makes intermediate-layer representations invariant to transformations of the image that should lead to the same classification. The paper also establishes that the typical models do not actually learn such representations by themselves.\n\nThis is an interesting idea (I am not specialist enough on image classification to know whether this idea has been tried before.) and highly relevant to this conference. The paper is g. well-written, easy to read, and correct.\n\nI do, however, rate it as Weak Accept only for one reason: I would expect that making the model more robust should improve classification accuracy. But according to the paper, accuracy does not improve (and even degrades slightly). The paper does not experimentally demonstrate that the proposed methods objectively improves the model.\n\nIn a sense, it is to be expected, since the model is no longer optimized 100% for the classification task.\n\nI can think of three changes to the paper that would flip my review to a Strong Accept:\n\n* Try using the multi-task objective as a pre-training, followed by fine-tuning without multi-task objective. This should foster robust internal representations while allowing to fully optimize for the classification task. Alternatively, you could anneal alpha to 0. Try whether this alleviates the losses on the tests that got worse, and leads to higher gains on the others.\n* Maybe the robust models, while being worse on benchmarks, are better on real-life data, e.g. where training and test mismatches are higher. Can you find a test set that demonstrates a larger, reliable accuracy improvement from robustness?\n* 'However, note that the hyperparameters used in all cases were optimized to maximize performance in the original models, trained without data augmentation invariance. Therefore, it is reasonable to expect an improvement in the classification performance if e.g. the batch size or the learning rate schedule are better tuned for this new learning objective.' -- Then that should be done.\n\nBesides this, I have a few detailed feedback points:\n\nEq. (2): Using sigma for 'invariance', which is the opposite of the usual meaning of sigma... I wish you had used a different symbol. Not a dealbreaker, but if you have a chance, it would be great to change to a different one.\n\n'we normalize it by the average similarity with respect to the *other* images in the (test) set' -- If you use only *other* images, I think it is theoretically possible that sigma becomes negative. I think you should include the numerator image in the denominator as well. I understand that in practical terms, this is never going to be a problem, so it should be OK, no need to rerun anything.\n\n'we first propose to perform in-batch data augmentation' -- This increases correlation of samples within the batch, and may therefore affect convergence. To be sure that this is not the cause of the degradation, it would be good to rerun the baseline with the same in-batch augmentation (but without the additional loss). Was that already done?\n\nFigure 5: I was a little confused at first because I read this as the invariance score (like Figures 3 and 4), not the invariance loss. They seem to be opposite of each other. So I wondered why the right panel would show poorer invariance score as training progresses. Do you actually need the concept of 'invariance score' at all? Or can you redefine the invariance score as a non-invariance score (1- of it), so that its polarity is the same as the invariance loss? If not, an easy fix would be to add '(lower=better)' to the caption of Fig. 5, and likewise ('higher=better') to Fig 3 and 4.\n\n'DATA AUGMENTATION INVARIANCE' -- You really want more. You want to be robust to all valid transformations of the object in the images. Obviously it is not possible to augment data for that, but it would be good to state this somewhere as the idealized goal of this work, which you approximate by data augmentation.", "belong_id": "B1elqkrKPH"}, {"uid": "BklwIQg0cH", "paper_title": "Learning robust visual representations using data augmentation invariance", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces an unsupervised learning objective that attempts to improve the robustness of the learnt representations. This approach is empirically demonstrated on cifar10 and tiny imagenet with different network architectures including all convolutional net, wide residual net and dense net. \n\nThis paper is well written and organized. However, I have several concerns.  The novelty of the proposed method is limited. The unsupervised objective in eq. (3) is a good and straightforward  engineering trick, but it is of less scientific interest.  The empirical results are not encouraging. Table1 shows the comparison results between the proposed method and a baseline method, however, for ALL-CNN, the reported top1 in the original paper is 92.75%. The reviewer is aware that this paper mentions the proposed method doesn't apply regularization, but why not compare to the original results. Results shown in figs. 2,3,4 are obvious because the proposed objective eq.3 prefers a higher invariance score in eq(2) and the increasing alphas prefer increasing invariance score. \n\nIn my opinion this work is not sufficient for ICLR. ", "belong_id": "B1elqkrKPH"}, {"uid": "S1ewYgHaKr", "paper_title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "**** Post Rebuttal ****\n\nI have read the author's response and other reviewers' comments. In light of comments by other reviewers, I am increasing the score. The paper reports decent empirical results in some challenging settings which might be useful to the continual learning community. \n\n**** End ****\n\nThe paper presents a simple yet effective way to avoid catastrophic forgetting in a continual learning setting. The proposed approach is referred to as UCB - 'Uncertainty Guided Bayesian Neural Networks'. The main idea of the approach is to weight the learning rate of each parameter in the neural network by the standard deviation of its posterior distribution. This leads to regularizing parameters that are 'important' to tasks seen earlier and thus avoiding forgetting.  Results indicate an improvement over other baselines. However, I do not see any analysis of the method that explains this improvement. I do not recommend acceptance.\n\nCons:\n\n- My main concern with the paper is that it fails to justify the superiority of the method over other baselines. The numbers reported in the paper do seem good, but I don't see an explanation of why this is the case. What are the drawbacks of EWC, VCL or HAT that the proposed method solves? Why using uncertainty to define importance works better than using online VI in VCL or fisher information in EWC? There is no discussion in the paper about that. Without such a discussion it seems that the model was run a number of times and the best score was reported out of all those runs (especially because the improvement is only marginal). \n\n- I am not sure why weighting the learning rate would be a good idea? Having high uncertainty may increase the learning rate arbitrarily. Is there a constraint on the standard deviation? Does having a very high weight for learning rate not cause instability during optimization? I think the method would be very sensitive to the initialization of the standard deviation. \n\nOverall I think the idea of using uncertainties for continual learning is interesting. But from where it stands, I am not fully convinced that this method should do better than existing approaches.", "belong_id": "HklUCCVKDB"}, {"uid": "SJlbynqTYS", "paper_title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "** post rebuttal start **\n\nAfter reading reviews and authors' response, I decided not to change my score.\nI am happy with the author's response addressing my concerns (mainly about the fairness on the size of the model), so I recommend its acceptance. I believe it is a good addition to the community of continual learning.\n\n** post rebuttal end **\n\n\n- Summary:\nThis paper proposes to use a way to improve continual learning performance by taking 'Bayes-by-backprop' method. They claim that the uncertainty can naturally be measured by estimating (log of) the standard deviation, and it is indeed useful to judge the importance of each learnable parameter. Experimental results on several benchmarks show that their method outperforms few state-of-the-art methods.\n\n\n- Decision and supporting arguments:\nWeak accept.\n\n1. The proposed method is simple but effective. However, It is still questionable whether \\sigma is the best measure of the weight importance. An ablation study with different choices of the importance measure (maybe \\mu can also be incorporated as well as \\sigma?) would be good to see.\n\n2. Survey and comparison with memory-based methods are limited. Though memory-based methods require some memory to keep the experience, the proposed method also requires additional memory for \\sigma; it essentially doubles the model capacity, assuming that \\sigma is solely for measuring the weight importance. In particular, when it comes to large-scale models, memory for storing some important experiences would be small compared to the memory to store the model.\nHere are some papers about recently proposed memory-based methods, which are not cited:\n\nCastro et al. End-to-End Incremental Learning. In ECCV, 2018.\nWu et al. Large Scale Incremental Learning. In CVPR, 2019.\nLee et al. Overcoming Catastrophic Forgetting with Unlabeled Data in the Wild. In ICCV, 2019.\n\n3. Comparison should include the model capacity as in Table 1(b). Again, compared to the conventional non-Bayesian model, half of the model capacity is used for computing \\sigma (uncertainty), I wonder it causes a performance drop when the model capacity is the same over all compared methods. If they used the same model architecture and just doubled the number of learnable parameters for \\sigma, then it is obviously unfair.\n\n\n- Comments:\n1. Pruning is not beneficial in terms of the performance. I hope to see some quantitative benefits obtained by introducing pruning. In Table 1(b), why doesn't pruning reduce the number of parameters?\n", "belong_id": "HklUCCVKDB"}, {"uid": "Byl4mBDl9B", "paper_title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a novel method for continual learning with neural networks based on a Bayesian approach. The idea consists in working with Bayesian neural networks, using the Bayes by back-prop approach in which a factorized Gaussian variational distribution is used to approximate the true posterior. To address the continual learning setting, the authors propose to multiply the learning rate of the mean parameters in the posterior approximation by the corresponding standard deviation parameter in the posterior approximation, while the learning rate for the variance parameters in the posterior approximation is not changed. The authors also consider a version of his method which freezes the mean and variance variational parameters when the signal to noise ratio is high. The proposed method is evaluated in exhaustive experiments, showing state-of-the-art results.\n\nClarity:\n\nThe paper is clearly written and easy to read. The method proposed is well described and it would be easy to reproduce.\n\nQuality:\n\nThe proposed method is well justified and the experiments performed clearly illustrate the gains with respect to previous methods.\n\nNovelty:\n\nThe proposed method is novel up to my knowledge. The methodological contributions do not seem very sophisticated, but the experiments show that the proposed method, despite being very simple, works very well in practice.\n\nSignificance:\n\nThe experiments show that the proposed method achieves state of the art results when compared with a very large number of baselines. This indicates that the proposed method will be relevant to the community. In my opinion, this work is highly significant.", "belong_id": "HklUCCVKDB"}, {"uid": "SkgvG0dpFr", "paper_title": "Learning to Move with Affordance Maps", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes to learn affordance maps: a method to judge whether a certain location is accessible. This is done by distilling a series of 'trial and error' runs and the relation of a pixel in the image/depth plane to a corrdinate into a model.\n\nI like the idea and think the paper should be accepted. The idea to use trial and error (something I prefer to self-supervision, which is used differently in many contexts, I believe) to obtain a data set for learning a model is nice and very practical.\n\nSome concerns that I think should be adressed.\n\n- The term information gain is used wrongly. The entropy of class labels is not infogain. Infogain is the expected KL of the model posterior from the model prior. Please correct this. See [1, 2].\n- *Learning* a model of the environment and using it for navigation/exploratin has also been tackled recently by [1]. I think the authors should draw connetions to that work.\n- Self-supervision has recently been proposed by Lecun as a subsitute (of sorts) for unsupervised learning. What he means is that a part of the data is used to predict another part of the data. I have no hard feelings about the term, personally preferring unsupervised, but the authors should be aware of the name clash.\n\nI wonder how the authors envision to extend this method to real scenarios. The 'trial and error' method is clearly not viable for robotics setups, as hazards are costly. It would be nice if the authors could give there perspective on things.\n\n[1] Depeweg et al, 'Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning', Proceedings of the 35th International Conference on Machine Learning\n[2] Mirchev et al, 'Approximate Bayesian Inference in Spatial Environments' in proceedings of Robotics: Science and Systems XV.", "belong_id": "BJgMFxrYPB"}, {"uid": "BygsJ2jatS", "paper_title": "Learning to Move with Affordance Maps", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents an approach for navigating and exploring in environments with dynamic and environmental hazards that combines geometric and semantic affordance information in a map used for path planning.\n\nOverall this paper is fairly well written.  Results in a VizDoom testbed show favorable performance compared to both frontier and RL baselines, and the author's approach is more sample-efficient and generalizable than RL-based approaches.\n\nI wouldn't consider any particular aspect of this paper to be that novel, but it is a nice combination of leveraging active self-supervised learning to generate spatial affordance information for fusion with a geometric planner.\n\nAs humans show the best performance on the tasks, it might be worth considering learning a policy from human demonstrations through an imitation learning approach.", "belong_id": "BJgMFxrYPB"}, {"uid": "H1eF2cVy5r", "paper_title": "Learning to Move with Affordance Maps", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes an interesting, and to the best of my knowledge novel, pipeline for learning a semantic map of the environment with respect to navigability, and simultaneously uses it for further exploring the environment.\n\nThe pipeline can be summarized as follows: Navigate somewhere using some heuristic. When navigation 'works', as well as when encountering something 'negative', back-project that into past frames, and label the corresponding pixels as such: either positive or negative. This generates a collection of partially densely labelled images, on which a segmentation network can be learned that learns which part of the RGBD input are navigable and which should be avoided. For navigation, navigability of the current frame is predicted, and that prediction is down-projected into an 'affordance map' that is used for navigation. One experiment confirms the usefulness of such an affordance map.\n\n\nI am marking weak reject currently because of the following concerns, which might be me just missing something. On the one hand, I am glad to see something that is not just blind 'end to end RL with exploration bonus', sounds reasonable, and works well. On the other hand, I do have several major concerns about the method, outlined as follows:\n\n1. How can this approach work for moving obstacles? Let's say a monster walks from point A to point B, and collides with the agent at point B. Then, point B is marked as a hazard, but in the previous frames, the monster is not located at point B, and thus an image region that does not contain the monster is marked as hazard. Am I missing something here?\n2. The method does not seem practical for actual mobile robots, only for in-game or in-simulation agents. The reason being that in order to learn 'robot should not bump into baby', the robot actually needs to bump into multiple babies in order to collect data about that hazard. To be fair, blind 'PPO+exploration bonus' suffers from the same problem, but in this paper, the whole motivation is about mobile robots (at least that was my impression after reading it).\n\nFurthermore, I do not think I would be able to reproduce any of the experiments, as many details are missing. Will code be released?\n\n\n###### Post-rebuttal update\n\nI am happy with the author's response to my concerns, and they have included corresponding discussions in their paper. Thus, I am improving my rating to recommend acceptance of this paper to ICRL2020.", "belong_id": "BJgMFxrYPB"}, {"uid": "B1eJdibycS", "paper_title": "Accelerated Information Gradient flow", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper attempts to develop a counterpart of the well-known Nesterov accelerated gradient method for gradient flows on the space of probability measures equipped with an information metric. This is an important problem which is useful for optimization on probability spaces. The accelerated gradient flow is developed by leveraging a damping Hamiltonian flow. The paper focuses mainly on the case with the Wasserstein metric and provides a convergence analysis. Practical considerations such as discretizing the accelerated flow and bandwidth selection are developed for the use of the method in practical problems.\n\nAlthough the paper has some important merit, I find the paper extremely hard to follow, partly because of its writing style. There is not enough motivation and explanation for the ideas presented. Some discussions and sentences either do not make much sense to me or read badly. For example, this sentence 'For the Wasserstein gradient, many classical methods such as Markov Chain Monte Carlo .... are based on this framework...' doesn't make sense, as the development of MCMC is never based on Wasserstein gradient. Or the sentence right before it 'For the Fisher-Rao gradient, classical results including Adam ... and K-FAC .. demonstrate its effectiveness in ...': it's not clear what the authors are trying to say here. Adam is not relevant to the Fisher-Rao natural gradient while K-FAC is just an approximation method and isn't a good reference for demonstrating the effectiveness of the natural gradient. Also, there are many English typos and grammar errors.  \n\nI didn't read the proof carefully due to the time constraint, so I cannot judge on the theoretical part of the paper. The numerical experiment is quite limited as it considers very simple problems (a toy example, a single Gaussian distribution and a logistic regression problem). As such, I think there isn't enough evidence to judge the usefulness of the proposed method in practice. \n\nHaving said that, I believe this paper can be an important contribution if the authors invest more time on refining its presentation and if more thorough experimental studies are conducted.\n", "belong_id": "BJe-unNYPr"}, {"uid": "BklKUV1fcB", "paper_title": "Accelerated Information Gradient flow", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I acknowledge reading the rebuttal of the authors. Thank you for your clarifications and explanation. My point was this paper would make a good submission to ICLR if it was better motivated presented and explained to a wider audience. Unfortunately in its current form it can only reach a limited audience.\n\n####\nSummary of the paper: \n\nThe paper proposes accelerated information flows under Wasserstein or Fisher rao metric . i.e a method for solving minimization of probability functional , where the probability space is either endowed with the Wasserstein or the Fisher distance.\n\nGradient descent methods in euclidian spaces can be accelerated using a type of momentum , this paper extends this to gradient flows, using similar formalism of Hamiltonian that appeals to the dynamic of a the particles and the velocity (or momentum, $H(x,p)=\\frac{1}{2}||p||^2+ \\mathcal{E}(x)$).  Writing down the lagrangian one obtains two co-evolving PDE one of the dynamic of the density and one for the potential . The PDEs are specified for both Fisher Rao, and Wasserstein distance. The hamiltonian for example for the Wasserstein distance $H(\\rho_t,\\Phi_t)=\\frac{1}{2}\\int||\\nabla_x \\Phi_t||^2+ \\mathcal{E}(\\rho_t)$.  and PDEs amounts a continuity equation for the density  evolving with drift $\\nabla \\Phi_t$, the evolution of the momentum $\\Phi_t$ is also given by a PDE. \n\nProposition 2 of the paper gives the particles differential equation corresponding to the system of PDEs. For the energy being the KL divergence an explicit expression is given , this expression remains difficult in practice since it needs the knowledge of the density $\\rho_t$. Authors propose in the application section to use gaussian approximation , or using a kernel density estimators. The Bandwidth of the kernel is then choosen using a heuristic proposed in the paper.  \n\nThe paper then focuses on deriving expression for flows when the densities are centered gaussians, and this amounts to an ODE on the covariance , the ODE is discretized in Appendix D.2 to lead to  computational method. Then convergence of the flow is analyzed for the wasserstein accelerated flows, under '\\beta- convextiy' in the wasserstein sense of the functional. \n\nSome experiments of the particle based method are shown on synthetic experiments and in bayesian logistic regression. \n\nReview: \n\nContribution/ Clarity:\n\nThe main contribution of the paper is in deriving the accelerated gradient flow for the wasserstein distance this was also addressed in a recent paper [Taghevia and Mettha 2019]. \n\nThe technical contribution is interesting but given that this field of flows in probability space is still not very well spread in the ML community, I wonder if ICLR is the best fit for this type of work.  I support good theoretical work, but I think the authors could have done a better job in exposing the ideas how they extend form euclidean space, to manifolds, to probability spaces gradually. Simple derivations of euclidean space Hamiltonian will help the reader that is not exposed to such literature. I think the paper will benefit from a less technical writing in introducing the ideas coming from euclidean space and in conveying the intuitions. \n\nComments: \n\n- In the proof of Proposition 2 you give the expression of evolution of $dV_t$ by conservation of the momentum. Could you please elaborate more how you obtain this expression, and where you proved the conservation of momentum?\n\n- In term of damping if ones uses the Wasserstein Fisher Rao flows  , one obtain also accelerartion , maybe you can comment on that ? since you analyze both flows , would be interesting to discuss the relation to Global convergence of neuron birth-death dynamics, that shows that an acceleration is obtained via WFR flows, since it will introduce a damping as well. \n\n- since MCMC and BM method lead to similar result what is the advantage of the wasserstein accelerated flow? one could also implement also an accelerated langevin dynamic \n\n\n", "belong_id": "BJe-unNYPr"}, {"uid": "rkxAK85sKB", "paper_title": "Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper compares to approaches to bilingual lexicon induction, one which relies on joint training and the other which relies on projecting two languages' representations into a shared space. It also shows which method performs better on which of three tasks (lexicon induction, NER and MT). The paper includes an impressive number of comparisons and tasks which illuminate these two popular methods, making this a useful contribution. It also compares contextualized to non-contextualized embeddings.\n\nI have some questions:\n-can you goldilocks the amount of sharing in an alignment method? Put a different way, how much is performance affected if you perform alignment w/variously sized sub-partitions of the seed dictionary? Can you find a core lexicon (perhaps most common words shared cross-lingually) that will provide a good-enough alignment to joint-align with? If you were very ambitious, you could try artificially vary the amount of lexicon disjointness across languages (for camera-ready) and report how much your results are affected by incomplete overlap in translation variants.\n-for Table 1, do you have any ideas about why certain languages do better on eng--> them and others do better on them-->eng?\n-do you have any analysis exploring what is shared? wrt this sentence 'It also suggests detecting what to share is crucial to achieve better cross-lingual transfer.'\n\nPlease address:\n- I would like more intuitive motivation for (6).\n\nSmall notes: \n-Fig. 1 font is too small, you could also remove excess axes. There's also overlap between the labeled arrow and the y-axis label btw (a) and (b).\n-'oversharing' should be typographically delimited as a definition in 'We refer to this problem as oversharing'\n-typos: bottom of p4: 'Secition 2.3'\n-'e.g.' or 'i.e.' should be followed by a ','\n-list which languages you use on bottom of p5\n-Table 3 caption looks crazy, what happened to your spacing there?", "belong_id": "S1l-C0NtwS"}, {"uid": "HJeJ2SZ6FH", "paper_title": "Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper compares two approaches to learn cross-lingual word embeddings: cross-lingual alignment (where separately trained embeddings in different languages are mapped into a shared space) and joint training (which combines the training data in all languages and jointly learns a cross-lingual space). The authors argue that each approach has its own advantages, and propose a 'unified framework' that essentially applies them sequentially (first train jointly, and then further align them after the necessary vocabulary reallocation).\n\nI have generally positive feelings about the paper. To be honest, I do not like the way the authors frame their work (e.g. the way the method is motivated in Section 2.3 or calling it a 'unified framework'), but the actual method they propose does make sense, the experimental evaluation is solid, and the results are generally convincing.\n\n\nStrengths:\n\n- Good coverage of related work, including recent publications.\n\n- Thorough evaluation in 3 different tasks, much better than what is common in the area (usually limited to BLI).\n\n- The authors experiment with contextual embeddings in addition to conventional word embeddings.\n\n- Generally convincing results with relevant ablations and reasonable baselines.\n\n\nWeaknesses:\n\n- I feel that framing this as a 'unified framework' for cross-lingual alignment and joint training is going a bit too far. The proposed method is as simple as first doing the joint training and then the cross-lingual alignment (with a special treatment for vocabulary reallocation). This is just a sequential application of two class of methods, and not what I would understand as a 'unified framework' (which suggests some form of generalization or at least a closer interaction to me). At the same time, the only 'joint training' that the authors explore is training regular embeddings over concatenated monolingual corpora, which as far as I know has only been explored by Lample et al. (2018), and I would not consider as a representative example of this family of methods.\n\n- It is not clear how the different vocabulary spaces are handled in BLI. My understanding is that if the query is in the source vocabulary subset the retrieval is done over the target subset, and if it is in the shared subset it is the same query word that is returned, but this is not clear at all.\n\n- I think that the issue of 'oversharing' is magnified in the paper. I understand that this is directly connected to the reallocation step in the proposed method, and it of course makes sense to also map words that predominantly appear in a single language. However, in relation to the downstream tasks themselves, oversharing only seems relevant for BLI, where one needs to delimit the retrieval space and avoid returning the query word (which is of course its own nearest neighbor) unless it actually exists in the target language. This is connected to my previous point, and I think should be discussed in isolation.\n\n\nOther minor things to improve:\n\n- Please make Figure 2c an actual table instead of an image.", "belong_id": "S1l-C0NtwS"}, {"uid": "ByezsH96YH", "paper_title": "Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new and simple framework for learning cross-lingual embeddings that, based on well-supported insights, unifies two standard frameworks: joint learning and alignment-based approaches. While I like and acknowledge the fact that the paper examines these frameworks critically and has also some didactic value, I still have some concerns regarding the current experiments, and would like to see some additional analyses in the paper. Honestly, this type of work would work better as a short ACL/EMNLP paper, as the core methodological contribution is a very simple combination of the existing techniques.\n\nWith joint methods, it is true that shared words can be used as implicit bilingual learning signal which gets propagated further in the model. Even in the case of alignment-based methods, it was shown that this signal can assist in learning shared cross-lingual spaces, but: 1) such spaces are of lower-quality than the spaces learned by relying on seed dictionaries (see e.g., the work of Vulic and Korhonen (ACL 2016)), 2) this is useful only for similar language pairs which use the same script. The paper fails to provide an insightful discussion on how the scores differ when we move towards more distant languages. For instance, English-Chinese is included as a more distant language pair, and the combined method seems to work fine, but the reader is left wondering why that happens. The same is true for English-Russian. The paper should definitely provide more information and insights here. \n\nOne experiment that would be quite interesting imho is to take seed dictionaries to initialise the joint training method. It will not be unsupervised any more, but it would be very useful to report the differences in results between fully unsupervised joint initialisation (which, as mentioned should work only with more similar languages) and such weakly supervised init: e.g., the method could take all one-to-one translation pairs from the seed dictionary as 'shared words'. It would also be interesting to combine such 'shared words' and words that are really shared (identically spelled words) as initialisation and measure how it affects the results, also in light of differences in language distance. Adding one such experiment would make the paper more interesting.\n\nSome recent strong baselines are missing: e.g., I wonder how the model that does self-learning on top of seed dictionaries (similar to the work of Vulic et al., EMNLP 2019) compares to the proposed method. Also, can we use the same self-learning technique with the combined method here? Would that lead to improved results? Another work I would like to see comparisons to is the work of Zhang et al. (ACL 2019) and despite the fact that the authors explicitly stated that they decided not to compare to the work of Artetxe et al. (ACL 2019) as they see it as concurrent work, I would still like to see that comparison as the model of Artetxe et al. seems very relevant to the presented work.\n\nI do not see the extension to contextualized representations (described in Section 3.2) as a real contribution: this is a very straightforward method to apply an alignment-based method on multilingual BERT representations, and very similar techniques have been probed in previous work (e.g., by Aldarmaki & Diab), only the bilingual signal/dictionary came from parallel data instead.\n\nFinally, as mentioned before, one of the must-have experiments is including more (distant and diverse) language pairs and analysing the results across such language pairs, with an aim to further understand how the levels of sharing, over-sharing, and non-isomorphism affect performance. Also, while the authors state that reduced isomorphism is the key problem of alignment-based methods, I still fail to see how exactly the alignment refinement step does not suffer from that problem? More discussion is needed here.\n\nOther comments:\n- It would be useful to also point to the survey paper of Ruder et al. (JAIR 2019) where the difference between alignment-based and joint models is described in more detail and could inform an interested reader beyond the confines of the related work section in this paper. Similarly, differences between joint models and alignment-based models have also been analysed by Upadhyay et al. (ACL 2016); Vulic and Korhonen (ACL 2016).\n\n- I like how Figure 1 clearly visualizes the main intuitions behind the proposed framework, and how mitigating the oversharing problem leads to better alignments. This was very nice.\n\n- In light of the problems with silver standard MUSE datasets (see also the recent work of Kementchedjhieva, EMNLP 2019), I would suggest to run BLI experiments on more language pairs: e.g., there are BLI datasets of Glavas et al available for 28 language pairs.", "belong_id": "S1l-C0NtwS"}, {"uid": "BygceR5dFS", "paper_title": "An Empirical Study on Post-processing Methods for Word Embeddings", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a theoretical framework for post-processing methods of word embeddings. Given the framework, the authors derive their own methods and then a thorough experimental analysis follows.\n\nWhile the paper reflects thorough and substantial work - both in the theoretical framework and in the experimental part, I have serious concerns about its clarity and about the experimental results and also some concerns about comparison with previous work. All these, unfortunately, make me recommend a reject decision. Below, are more details:\n\n1. Clarity: There are multiple aspects of the paper that I needed to read a few times before I understood what the authors mean. \n\nFirst, the problem definition is not clear: Even when I reached the end of the introduction, it was not clear to me what is the problem that the authors are trying to solve and what they are trying to achieve. One example of this problem is the title of the paper, which suggests that the author performs an empirical evaluation, while in practice they also suggest their own method. But this is a much broader issue - the author suggest very little motivation to how the post-processing method should work, what improvements it should provide and why we should expect such improvements. At some points I felt that this is more of a clean mathematical exercise than a discussion of methods that should improve word representations in natural language.\n\nIn addition, the authors assume a strong background in a very specific post-processing  literature and do not provide any details about its fundamentals. Only on the beginning of section 3 I learned the fundamentals of that framework and basic concepts such as the Gram matrix. I believe a scientific paper should be self-contained, the motivations, goals and fundamental concepts form previous work should be clearly stated and explained. This is not done in this paper, unfortunately.\n\nFinally, it was hard for me to determine where the survey of previous work ends and the contribution of this work begins. Particularly, it seems that the authors propose a unified framework for the methods in previous work and it is not clear which parts of that framework were already discussed in previous work and which are original contributions of the authors. This makes it also hard to estimate how different the proposed method is form the previous ones.\n\n2. Experimental analysis:\n\nFirst, the authors describe their evaluation tasks very briefly and only in the appendix. This is just a list of tasks with no insight about natural language (please see a related comment in the clarity section of this review). Then, the results are reported as a macro-average over many tasks: Given the large number of tasks this is a very crude average, and there is no way that any real insight into the change/improvement of the vectors can be derived from this report. Finally, the reported numbers reflect very minor improvements, if at all, compared to previous post-processing methods and to the original vectors. Again, since these are macro-averages over many tasks, the conclusions that can be derived are very limited (e.g. it might be that the proposed method does improve on some of the tasks and harm the performance on others, or that it keeps the vectors very similar to the original ones - we have no way figuring out the actual picture).\n\n\n3. Comparison to previous work:\n\nAs said above, it seems that the authors view their work in a narrow context of a very specific literature. In fact, the NLP literature contains a large number of post-processing word embedding methods (often referred to as 'fitting' or 'specialization' methods). While these methods sometimes build on external linguistic knowledge (e.g. from wordNet or from other manually crafted lexicons), they have also shown useful with automatically constructed constraints, that are similar to the structural considerations mentioned in the paper in the sense that they do not require expert knowledge, they only build on common-sensical requirements from a good vector space for word meaning representation. Some relevant papers are:\n\nFaruqui, M., Dodge, J., Jauhar, S. K., Dyer, C., Hovy, E., & Smith, N. A. (2014). Retrofitting word vectors to semantic lexicons. arXiv preprint arXiv:1411.4166.\n\nMrksic, N., Seaghdha, D. O., Thomson, B., Gasic, M., Rojas-Barahona, L., Su, P. H., ... & Young, S. (2016). Counter-fitting word vectors to linguistic constraints. arXiv preprint arXiv:1603.00892.\n\nMrksic, N., Vulic, I., O Seaghdha, D., Leviant, I., Reichart, R., Gasic, M., ... & Young, S. (2017). Semantic specialization of distributional word vector spaces using monolingual and cross-lingual constraints. Transactions of the association for Computational Linguistics, 5, 309-324.", "belong_id": "Byla224KPr"}, {"uid": "rylgvrfAtB", "paper_title": "An Empirical Study on Post-processing Methods for Word Embeddings", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a novel word embedding post-processing method that maximizes the similarity between the estimated Gram matrix of word vectors and its oracle matrix. To find the optimal Gram matrix, they adopt the shrink method to make Gram matrix K' to target matrix T on semi-Riemannian space. The authors use the shrinkage method to find optimal K' and formulate the maximization on CKA problem as finding the optimal shrinkage parameter that maximizes the lower bound of the CKA between the estimated Gram matrix and oracle. By applying the proposed method to various word embedding methods, the authors show the performance of their post-processing method on word analogy/similarity task, word translation task, and sentence similarity task.\n\nStrengths\n* This paper provides a novel post-processing method that can relieve isotropy condition and shows experimental support that solving isotropy condition on word embedding vectors can improve its performance.\n* Large set of experiments on various word embedding benchmark tasks. \n\nWeaknesses\n* It would be nice if the authors show the performance of the post-processed word vectors on other NLP benchmark: text classification, NER, ... etc.\n\nQuestion\n* 'Neural Word Embedding as Implicit Matrix Factorization' and 'Analogies Explained: Towards Understanding Word Embeddings' show that PMI is the global optimum point of the previous word embedding model's problem space and prove word analogy can be explained from the PMI characteristics of word embedding models. How can this paper be related to the two papers mentioned above?", "belong_id": "Byla224KPr"}, {"uid": "r1g_kvLgqr", "paper_title": "An Empirical Study on Post-processing Methods for Word Embeddings", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a novel method for unsupervised post-processing of pretrained word embeddings that enforces the distributional word vector space to be more isotropic, which in turn improves the expressiveness and quality of the space in terms of similarity. The method is based on the shrinkage of the covariance/Gram matrix and its effects on the input space are evaluated across a range of intrinsic evaluation tasks. While I like the idea overall and this line of work in general, there are still some concerns with the current version of the paper:\n\n* Overall, although its design seems more principled, the proposed method does not seem significantly better than the previous (very similar) method of Mu et al. I would like to see more evidence in the favour of the proposed method, pointing that we should use that instead of Mu's method. Also, the gains over non-processed spaces often seem insignificant, and offer only small benefits.\n\n* The derivation of the method seems too verbose, especially in light of the fact that it is directly inspired by previous work on the shrinkage estimation of covariance matrices and CKA. I would suggest the authors to spend more time on linking their high-level hypotheses to the low-level mathematical implementations instead of flooding the paper with equations - for the interested reader a lot of the derivation process can be put into an appendix, the paper should focus on conveying the key principles instead. This would also offer additional space for more experiments.\n\n* One very relevant paper is not mentioned at all: https://arxiv.org/pdf/1809.02094.pdf (Artetxe et al., CoNLL 2018). I would suggest the authors to cite that work and ideally even compare to it on their set of intrinsic tasks (e.g., word similarity, word analogy), and then discuss the difference in results and their approach to unsupervised post-processing. This shouldn't be so difficult to do as the code from that paper is available online: https://github.com/artetxem/uncovec\n\n* The paper mixes true similarity datasets (such as SimLex) with broader semantic relatedness datasets (MEN, WordSim-353), even mixing true semantic similarity of the same dataset (WordSim353-SIM) with its relatedness subset (WordSim353-REL). In light of the known conceptual differences between the relations of similarity versus relatedness, I would suggest to report the results separately for the two tasks. For instance, another true similarity dataset, which is not used in the evaluation is SimVerb-3500. Along the same line, it is also not clear what type of similarity is meant when the authors state that through post-processing 'the similarity between words can be better expressed'. What does it mean to better express the similarity between words in the first place? Do we talk about true similarity or relatedness or both? However, the two relationships between words support different classes of downstream applications, so therefore it is even more problematic 1) not to distinguish between the two and 2) not to report any results in any downstream (extrinsic) tasks where the post-processed embeddings are used as features (STS is still a semi-intrinsic task imho; BLI is considered as an intrinsic task in cross-lingual settings).\n\n* One of the key reasons to apply post-processing is to mitigate the frequency artefacts: however, such an evaluation that goes towards that direction is never executed. For instance, I would like to see a focused experiment that measures how post-processing affects high-frequency versus mid-to-lower frequency or rare words. A recently developed CARD-660 dataset might be used to this end.\n\n* It is not clear how exactly the authors run Mu et al.'s method, that is, how many top principal components are removed for the input vectors? How is this selected? Are always the optimal results reported for the baseline Mu et al.'s method? Why not reporting the results with removing 2 and 3 at the same time to further prove the point that their method is non-automatic? This would also give the reader a hint how much the results with Mu's method actually differ/decrease if one just decides to make the method 'automatic' by just fixing the method to always remove the same number of top principal components.\n\n\nMinor remarks:\n* The title of the paper is a bit imprecise: in the word embedding literature, the term post-processing is often referred to the methods that fine-tune word embeddings using some external knowledge after (i.e., post) the initial distributional training (e.g., the so-called retrofitting methods). However, in the context of the paper post-processing actually refers to some unsupervised post-training steps on the input space without injecting any external information. This should be made clearer in the paper, and perhaps adding a paragraph which outlines the core difference to other work on retrofitting would be helpful as well.\n\n* I might be missing something while reading Section 3, but it is currently not clear to me how the oracle Gram matrix K is obtained in the first place. Perhaps it makes sense to briefly summarize this in a quite direct way to avoid the reader's confusion?\n\n* It is great to see a summary of the key post-processing steps at the very end of Section 3; this is really helpful for everyone who would like to try out the proposed method off-the-shelf. However, the summary is not self-contained as it is not clear what \\mathcal{L}'' refers to (and the reader must search through the derivations again to find its meaning).\n\n* I like the evaluation on word translation, and I believe that the proposed post-processing methods could actually improve word translation through some pre-alignment perturbations. It is a pity that the method is not evaluated on more distant language pairs, as I believe that the method might have much more effect there than on the already-saturated EN-to-ES/FR/IT bilingual lexicon induction tasks.\n\n* For de-en word translation Mu's method actually beats the proposed method (incorrect number in bold)\n\n* It is not clear why MUSE is used for word retrieval experiments, given the fact that it is known to be unstable (Sogaard et al., ACL 2019), and there are more robust and more effective methods available such as VecMap (Artetxe et al., ACL 2018) or RCSLS (Joulin et al., EMNLP 2019)", "belong_id": "Byla224KPr"}, {"uid": "Hyg-EusOKH", "paper_title": "Spectral Nonlocal Block for Neural Network", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, authors propose a spectral nonlocal block. First, they re-interpret the nonlocal blocks in a graph view and then use Chebyshev approximation to obtain the spectral nonlocal block which is quite simple by adding a ZW_1 term. Furthermore, they analyze the steady-state to build up a deeper nonlocal structure. Also, the gSNL is simple by adding a (2A-I)ZW_3 term.\n\nOverall, the paper is written well. I like the idea to interpret the nonlocal operation in the graph view. More important, the resulting formulation is quit concise for implementation. However, my main concern is the experiment, which should be further enhanced by perform large-scale video classification like Kinetics400.", "belong_id": "rkgb9kSKwS"}, {"uid": "rkeQXGZ_qS", "paper_title": "Spectral Nonlocal Block for Neural Network", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "SUMMARY: \n- Propose spectral non-local block\n- improvement on image and video classification tasks\n\nApologies, I am not at all familiar with the theory and math behind this proposal, I do not think I am in a position to review this paper. The experiments seem convincing enough that the authors made enough effort to prove their method might work.\n\n- Feature maps to show robustness of method is a good point\n- CIFAR-10 and CIFAR-100 are certainly a good start, but might not be the best datasets to test for image classification, in lieu of ImageNet and others.\n- Classification itself is a good start, it might be interesting to think about using this in a generative model such as GAN. The content reminds me of Self-Attention GAN which uses a similar non-local block (self-attention).", "belong_id": "rkgb9kSKwS"}, {"uid": "S1gW3qf29r", "paper_title": "Spectral Nonlocal Block for Neural Network", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a spectral non-local block, which is a generalized method of the non-local block and non-local stage in the literature. The proposed spectral non-local block can be plugged into a neural network to improve its effectiveness. The paper also provides theoretical analyses of the stability of the proposed method, and also extend the method by including more  Chebyshev polynomial terms. Experiments are conducted on image classification and action recognition tasks, and they valid the effectiveness of the proposed method. \n\nThe idea is well-motivated, and it is a generalization of existing works in the literature. I do like this idea. However, I am afraid that the idea is not well explained and supported, thus I gave a weak reject to encourage the authors to further improve the paper.\n\nThe major concern I have is the reasonability of the experiments.  The experiments in the paper show relative performance gain with respect to a baseline method. It seems that there is a lack of comparison with state-of-the-art methods in the literature. For example, in Table 8, a performance gain is observed when compared with I3D. However, the recent STOA models can achieve much higher accuracy than the baseline. and also the proposed method. Since the proposed method is generic to all neural nets, it makes more sense to compare with SOTA and make improvements based on SOTA.  What is the conclusion from Table 4? Are you trying to demonstrate that the best configuration is DP3, and increasing the number of consecutive non-local blocks (from SP3 to SP5) doesn't work? It is awkward since the paper gives a stable hypothesis for deeper nonlocal structure, but experimentally the deeper structure doesn't work well. Figure 4 is abrupt without much background descriptions. Are the images randomly chosen? Ours here means SNL or gSNL? Is the colored superimposition the attention map (I believe so but the paper doesn't indicate so) and how to interpret it? What is the relation of the coverage of the critical parts on birds and the long-range dependency? More background descriptions and interpretations of the results are needed. \n\nAnother concern I have is the clarity of the writing. There are quite a number of informal use of English, mismatched descriptions, undefined acronyms, etc. For example, in the caption of Fig. 1, it is said self-attention and self-preserving are taken effect by W1 and W2, which is contradictory to what is illustrated in the figure. Also, the terms self-attention and self-preserving, and other terms such as CGNL, A2, Hadama (Hadamard?) product, are not formally defined or described.  A lot of grammar errors and informal use of English are present, such as 'which lead to', 'the weight means', 'when using in the neural network', 'fig. 4', 'Figure. 2', 'more liberty for the parameter learning.', etc. ", "belong_id": "rkgb9kSKwS"}, {"uid": "rke6CgqpqH", "paper_title": "Spectral Nonlocal Block for Neural Network", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I have two general concerns, the first is related to the presentation and the second to the relevance of the results.\n\n(1) Presentation is confusing at many points, for instance:\n\n* It is unclear if theorem in Eq. 4 is original or belongs to Shuman et al 2013. (no proof is given)\n\n* Eq. 8 seems an arbitrary decomposition of the original NonLocal operator that could have been proposed without any reference to the Chebyshev expansion (which, on the other hand, is truncated to 1st order with no extra explanation).\n\n* The point of Fig. 1 and Fig 4 is not clear. Fig. 1 explains how SpectralNonLocal reduces to NonLocal and NonLocalStage, but we can see this from the formulas. I dont see how this discussion on the Ws relates to the regions highlighted in the bird.\nThe same applies to Fig. 4. What are we supposed to see in Fig. 4 (and, more importantly, why?).\n\n* What is the CFL condition? (is it the Courant-FriedrichsLewy sampling condition?). How is that related to the values of Ws. Can we take those arbitrarily small as suggested in that proof?\n\n* The upper limit in the sums after Eq. 10 is unclear.\n\n* First time table 4.2 is cited there is no context to understand it. (actually there is no table labeled as 'Table 4.2') Where do we see the different number of NonLocal units?. This is only clear when you arrive and read text of page 9 (but not when cited the first time from page 6).\n\n* Explanation of experiments is a little bit confusing (e.g. what does it mean top1 and top5 in tables?). The only explanation of 'top-something' I found in the text has to do with eigenvectors in fig. 5. This also apply to the 'topX' in the figures?\n\n(2) Nevertheless, the main concern is the scarce relevance of the results: differences of behavior in all tables are about 1%. Then, what is the real advantage of the proposed modification? \n", "belong_id": "rkgb9kSKwS"}, {"uid": "BJeOtaIFKr", "paper_title": "Towards Understanding the Regularization of Adversarial Robustness on Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summary:\nThis paper focuses on analyzing the regularization of adversarial robustness (AR) on neural networks (NNs). They establish a generalization error (GE) bound characterizing the regularization of AR, and identify two quantities: margin distributions and singular values of NNs' weight matrices. With empirical studies, they show that AR is achieved by regularizing NNs towards less confident solutions and making feature space changes smoother uniformly in all directions, which prevents sudden change wrt perturbations but leads to performance degradation.\n\nThe paper is well written with theoretically motivated experiments and detailed analysis. I'd suggest accepting the paper.\n\nWith proposed GE bound connecting 'margin' with AR radius via 'singular values of weight matrices of NNs', they present 3 key results with empirical experiments on CIFAR10/100 and Tiny-ImageNet.\n1) AR reduces the variance of outputs at most layers given perturbations.\n2) Empirically examples are concentrates around decision boundaries.\n3) The samples concentration around decision boundaries smooths sudden perturbation change, but also degrades model performance.\nThe paper only shed light on their conjecture that the performance degradation comes from the indistinguishable changes induced by adversarial noise and by inter-class difference. It'd nicer to further analyze on how to obtain AR without sacrificing performance on natural examples.\n\n\n", "belong_id": "BJlkgaNKvr"}, {"uid": "r1lOwA8TFS", "paper_title": "Towards Understanding the Regularization of Adversarial Robustness on Neural Networks", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "===== Summary ===== \nThe paper presents new theory to develop understanding about why adversarially robust neural networks show lower test performance compared to their standard counterparts despite being more robust to perturbations in the data. The main hypothesis is that the degradation in performance in adversarially robust networks is due to many samples being concentrated around the decision boundary, which makes the network less confident about its decisions. The paper studies this hypothesis by deriving a bound on the generalization error based on the margin between the samples in the training set and the decision boundary. The paper then presents empirical demonstrations that aim to illustrate the theoretical findings. \n\nContributions:\n1. Derive a generalization bound on the performance of adversarially robust networks  that depends on the margin between training examples and the decision boundary.\n2. Provide empirical evaluations that aim to illustrate the theoretical results.\n\n===== Review =====\nThe problem that the paper addresses is very significant to the robust optimization field and the study of adversarial robustness in neural networks. Thus, I believe that the results could represent a significant contribution. However, due to the way that the information is presented, it is difficult to validate the correctness of the theory and the insights from the paper. Consequently, I consider that the paper should be rejected.\n\n===== Detailed Comments =====\n- First, and foremost, the paper should be proof-read for English grammar and writing style. In its current form, it is difficult to follow the main argument of many of the paragraphs. This is exceedingly important because the main subject of the paper is already difficult to digest as is.\n\n- In the related work section, a lot of previous work is referenced without any context about what the contribution of each of those papers is. Each of these papers should be mentioned along with their corresponding contributions. Otherwise, it is difficult to frame the paper within the context of the current literature. Moreover, not providing context makes it difficult to determine which parts of the paper are original and which are the result from previous work. \n\n- The first item in Section 1.1 is difficult to follow because there are many gaps in logic that are left to the reader to fill in. It is reasonable to expect the reader to fill in some of the details, but since the sole purpose of this section is to build intuition about what is about to be presented in the paper, then each step in the explanation should follow as seamlessly as possible.\n\n- The motivation for studying the margins between the training set and the decision boundary is not clear until Section 5.2.1 where it is mentioned that this is a widely used tool in learning theory. This should be presented earlier since not every reader will be completely familiar with learning theory. Moreover, it would also be useful to provide some intuition about how margins relate to the confidence of a classifier.\n\n- After presenting the main result of the paper  Theorem 4.1  very little intuition is provided about each of the terms in the bounds. It would greatly increase the clarity of the result if each term was explained intuitively, so that the readers can gain the main insight of the paper before reading the proofs. This would also help motivate better the empirical evaluations in the following section. \n\n- The plots in FIgure 3 and Figure 4 are very difficult to understand and unintuitive. For Figure 3, the main reason the plots are difficult to read is because different colors are used for different networks. For Figure 4, it is difficult to understand what is happening because the tallest curves are plotted on top of the shortest ones. Hence, the information from the other curves is mostly lost. This seems like a very minor comment, but this graphs are not very complicated, so they should be easy to understand; yet it takes several minutes to take in what is happening in the graph.\n\n- For the proof of Lemma 4.1it is not clear how to get from Equation (7) to the main result of the lemma even after referencing Theorem 3 of Sokolic et. al. (2017) and Jia et. al. (2019). This could also be my lack of expertise on the topic; however, since the proof is already in the appendix, the proof should not be sparse in the amount of detail that it provides. \n\n===== References =====\nJure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust Large Margin Deep Neural Networks. IEEE Transactions on Signal Processing, 65(16):42654280, aug 2017. ISSN 1053-587X. doi: 10.1109/TSP.2017.2708039.\n\nKui Jia, Shuai Li, Yuxin Wen, Tongliang Liu, and Dacheng Tao. Orthogonal Deep Neural Networks. Technical report, 2019. URL http://arxiv.org/abs/1905.05929.\n\n", "belong_id": "BJlkgaNKvr"}, {"uid": "B1e_-c4kcH", "paper_title": "Towards Understanding the Regularization of Adversarial Robustness on Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes to explain a phenomenon that the increasing robustness for adversarial examples might lead to performance degradation on natural examples. The authors analyzed it from the following aspects: \n\n1)\tAdversarial robustness reduces the variance of output at most layers in terms of reducing the standard deviation (STD) of singular values associated with a layer of NN. The authors provide the experiment to show that the stronger robustness for adversarial examples leads to smaller STD of singular values of parameter of layers. \n\n2)\tThe reduced norm variance can cause the margins concentrated around zeros. Specifically, the authors provide the relevant lemma to show the relationship between margin and singular vectors. Moreover,the authors also conducted the experiment to show that stronger robustness over adversarial examples can lead to zero concentration of margin. The authors think a small margin might cause shrinking the hypothesis space which might cause low generalization.\n\n\n3)\tThe authors have derived a bound of generalization which is related to Instance-space Margin,and minimum singular values. This proved that strong robustness on adversarial examples might reduce the generalization. \n\n\nIn general, this paper seems technically sound. It is good that to some theoretic analysis can be derived in particular a bound of generalization can be given. Moreover, some experiments were made trying to verify the theory. Despite interesting, there are still some major concerns regarding the paper:\n\n1.\tThe authors mentioned that it is widely observed that such methods would lead to standard performance degradation, i.e., the degradation on natural examples. I am afraid that this may not be always the case. In some other related work (see C1), adversarial training can perhaps achieve the performance lift on both the adversarial examples and natural examples (if a trade-off parameter can be well specified). Some clarification or further discussion may be necessary regarding this point.\n\n2.\tOnly PGD was used as the adversarial perturbations in the experimental part of this paper. It would be more convincing if the authors could perform analysis on different adversarial training methods, e.g. FGSM and even the unified gradient perturbations developed in C2. There are also more adversarial attacks in the literature.\n\n3.\tThe authors stated that the sample concentration around decision boundaries smoothness sudden changes, which was verified by the accuracy degradation. This is ok but it would be better to visualize or quantifying directly whether this can indeed make the boundary smoother. One possible way is to plot the confidence when moving the points near the decision boundary to check whether the confidence changes smoothly. \n\n4.\tFinally, this paper seems to be written in a hurry. The paper may need substantial improvement on the English writing. There are still quite a few typos and grammar errors in the paper; this makes the paper less attractive though it contains some theoretic merits.\n\nIn summary, it is good that a theoretical bound can be derived from the paper, but this paper's quality may need more enhancement particularly on its writing and experimental parts. \n\nC1:  Virtual Adversarial Training: a Regularization Method for Supervised and Semi-Supervised, Learning' http://arxiv.org/abs/1704.03976\n\nC2: A Unified Gradient Regularization Family for Adversarial Examples C. Lyu, K. Huang, and H. Liang, ICDM 2015.\n\n\n============\nI have carefully read the response as  well as the revised paper. To me, the response has addressed those of my major concerns. I an inclined to increase my rating and would suggest to weak accept this paper.", "belong_id": "BJlkgaNKvr"}, {"uid": "SJx2t271cB", "paper_title": "Learning Disentangled Representations for CounterFactual Regression", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper proposes an algorithm that identifies disentangled representation to find out an individual treatment effect. A very specific model that tries to find out the underlying dynamics of such a problem is proposed and is learned by minimizing a suggested objective that takes the strengths of previous approaches. The method is demonstrated in a synthetic dataset and IHDP dataset and shown to outperform other previous methods by a large margin.\n\nMy initial review was negative, but I changed my mind after reading a few papers in this area. It seems that explicit learning of underlying factors that are described in (Hassanpour & Greiner, 2019) is a nice idea and works well. My only concern is that the paper has a lot of overlap with (Hassanpour & Greiner, 2019), even using identical figures. I am not sure whether it is OK.\n\n", "belong_id": "HkxBJT4YvB"}, {"uid": "SkeKSvde9r", "paper_title": "Learning Disentangled Representations for CounterFactual Regression", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new way of estimating treatment effects from observational data, that decouples (disentangles) the observed covariates X into three sets: covariates that contributed to the selection of the treatment T, covariates that cause the outcome Y and covariates that do both. The authors show that by leveraging this additional structure they can improve upon existing methods in both ITE and ATE\n\nThe main contributions of the paper are:\n* Highlighting the importance of differentiating between treatment and outcome inducing factors and proposing an algorithm to detect the two\n* Creating a joint optimisation model that contains the factual loss, the cross entropy (treatment) loss and the imbalance loss\n\nOverall, I like the paper quite a lot, I find it well-written and clearly motivated with a very nice experimental section that it is designed around understanding the behaviour of the proposed model.\n\nIn terms of suggestions, I think it will be very interesting to link the approaches using invariant causal representations with existing work in the Counterfactual Risk Minimization [1] literature and to mutualise the experimental setup. \n\n[1] Swaminathan, Adith, and Thorsten Joachims. 'Counterfactual risk minimization: Learning from logged bandit feedback.' International Conference on Machine Learning. 2015.", "belong_id": "HkxBJT4YvB"}, {"uid": "rJxIX7EWjS", "paper_title": "Learning Disentangled Representations for CounterFactual Regression", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\n   The authors consider the problem of estimating average treatment effects when observed X and treatment T causes Y. Observational data for X,T,Y is available and strong ignorability is assumed. Previous work (Shalit et al 2017) introduced learning a representation that is invariant in distribution across treatment and control groups and using that with treatment to estimate Y. However, authors point out that this representation being forced to be invariant still does not drive the selection bias to zero. A follow up work (Hassanpour and Greiner 2019) - corrects for this by using additional importance weighting that estimates the treatment selection bias given the learnt representation. However, the authors point out even this is not complete in general, as X could be determined by three latent factors, one that is the actual confounder between treatment and outcome and the other that affects only the outcome and the other that affects only the treatment. Therefore, the authors propose to have three representations and enforce independence between representation that solely determines outcome and the treatment and make other appropriate terms depend on the respective latent factors. This gives a modified objective with respect to these two prior works.\n\nThe authors implement optimize this joint system on synthetic and real world datasets. They show that they outperform all these previous works because of explicitly accounting for confounder, latent factors that solely control only outcome and treatment assignment respectively. \n\nPros:\n  This paper directly addresses the problems due to Shalit 2017 that are still left open. The experimental results seems convincing on standard benchmarks. \n\nI vote for accepting the paper. I don't have many concerns about this paper.\n\nCons:\n  - I have one question for the authors - if T and Y(0),Y(1) are independent given X is assumed, then how are we sure that the composite representations (of the three latent factors) are going to necessarily satisfy ignorability provably ?? I guess this cannot be formally established. It would be great for the authors to comment on this.\n\n\n", "belong_id": "HkxBJT4YvB"}, {"uid": "SygTdtzCKH", "paper_title": "A closer look at network resolution for efficient network design", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper explore how varying input image resolution can have a powerful effect in reducing computational complexity of Convolutional NN's to the point of outperforming state of the art efficient architectures.\nMore in detail, the paper proposes a method of joint training of multiple resolutions networks, leveraging student/teacher/distillation from scratch. This is based on training a high resolution teacher network and a low resolution student network, as well as a number of intermediate resolutions networks sampled randomly and jointly during training. Thanks to distillation well known regularization effects, the proposed method is achieving competitive results compared to existing state of the art efficient network architectures. The authors claim, and to some extent show, that this is due to the ability of the proposed method to take into account in a optimal way multi-resolution features available in the image. The paper is well written and presented with extensive results, comparing computational complexity/accuracy curves to existing state of the art architectures, as well as results on transfer learning to show that the feature learned do indeed generalize and don't necessarily overfit to imagenet. The idea is rather simple, but the results and the execution is inspiring.", "belong_id": "H1x-pANtDB"}, {"uid": "ryg1bSmy5H", "paper_title": "A closer look at network resolution for efficient network design", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a multi-resolution training scheme for a slimmable network. The proposed method provides a new regime leveraging diverse image resolutions for training sub-networks, which enables efficient network scaling. Throughout the experiments, the authors claim the proposed method shows better performance compared to US-net.\n\nPros)\n(+) The idea of multi-resolution training combining to a slimmable network looks good\n(+) Applying a slimmable network's technique to other tasks including detection and segmentation looks good\n(+) The combination of multi-resolution and the slimmable network seems to be reasonable.\n(+) The paper is well written and looks justified well.\n(+) The authors provided extensive experiments.\n\nCons)\n(-) There is no backups why the proposed method could outperform over US-net. \n(-) The proposed method is incremental and improvements are marginal.\n(-) Looks like there exists missing in details of the experiments.\n(-) The performance report of the compared methods is quite strange.\n\nComments)\n- The proposed method is too straightforward, so the authors should clarify why it works over US-net. Additionally, can the authors provide advantages using a different image-scale need for training a different sub-network? \n- The authors should clarify the training details of US-Net used in this paper. The performance of US-Net in Figure 4 (a) looks the same as the performance of US-Net trained with [0.05, 1]x scaling in the original paper. However, in the original paper, the authors of US-net reported [0.05, 1]x scaling as the worst performance setting in the original paper. Therefore, the authors should compare their method with the best performance setting of US-Net, which is [0.25, 1]x (because the proposed method looks being used [0.25, 1]x training setting, so the comparison should be done in fair).\n- The scaling parameters of US-Net used in the experiments should be specified. All the results of US-Net do not contain where they come from (i.e., the training width bound in US-net).\n- Can the authors report the results for 0.5-224 and 0.15-224 in Figure.4(a)? Why 0.7-160 and 0.25-160 were picked?\n- In Table 1, the performance of EfficientNet is weird. EfficientNet-B2 has 79.8% accuracy with 1.0B FLOPs, but the reported performance in this paper of EfficientNet has 75.6% accuracy with 2.3B FLOPs. Please clarify this.\n- How much does KLdiv contribute to the overall performance?\n- All the tables are not clearly shown. Please reattach all the tables for better readability.\n\nAbout rating)\nI think the idea looks novel, but the method is quite straightforward, and the paper does not incorporate any analysis as a backup for the proposed method. The initial rating is towards reject, but I would like to see the authors' response and the other reviewers' comments. After that, the final rating might be changed.", "belong_id": "H1x-pANtDB"}, {"uid": "S1evBrJ9sr", "paper_title": "A closer look at network resolution for efficient network design", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a new training regime for multi-resolution slimmable networks. Their approach is based on US-Net training technique but in addition to sampling from different network widths they also sample from different input resolutions and show that using multi-scale inputs improves the top-1 accuracy on ImageNet comparing to US-Net or MobileNet v1/v2 within the same resource constraints.\n\nPros:\n+ The authors correctly identify input resolution as one of the aspects of lightweight network design that is often overlooked\n+ They propose a practically viable training scheme that can be used to train & select networks given resource constraints\n+ The paper is well written and includes many insightful experimental findings\n\nCon:\n\nThe authors specify the mutual learning from width and resolution as their main contribution. They insist that treating input resolution independently from network structure is what distinguishes previous work from the newly suggested technique. But the paper doesn't include extensive experimental comparisons with the approaches that treat input resolution independently. Thus its claim that joint width/resolution sampling is beneficial comparing to independent approaches is somewhat unfounded. \n\nFor example, the authors show that MobileNet with 1.0-224 config (no sampling from widths nor from input resolutions during training) is outperformed by their network with 1.0-224 config (which effectively samples only from input resolutions during training). This is not surprising as one can view sampling from input resolutions as an equivalent to data augmentation. The importance of data augmentation is well known, so to prove the proposed mutual learning is beneficial the authors would need to compare against the networks that were trained using this multi-scale data augmentation. Figure 5 has a similar comparison but the only multi-resolution baseline there is US-Net+ which isn't using multi-resolution images in training. The paper would greatly benefit from adding such comparisons and proving they are not marginal.\n\nOn rating:\n\nI'd summarize the idea of this paper as A) US-Net + B) multi-scale data augmentation + C) selecting the best network based on both input resolution and width to achieve optimal performance within resource constraints. Although C is practical and novel contribution, it is also quite straightforward. I would like to see authors response on how their approach differs from US-Net + multi-scale data augmentation for training and how/why this works better.", "belong_id": "H1x-pANtDB"}, {"uid": "HyxmIYPEYH", "paper_title": "Gradient $\\ell_1$ Regularization for Quantization Robustness", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: the authors propose a regularization scheme that is applied during regular training to ease the pose-training quantization of a network. Modeling the quantization noise as an additive perturbation bounded in \\ell_\\inf norm, they bound from above the first-order term of the perturbations applied to the network by the \\ell_1 norm of the gradients. Their claims are also supported by experiments and qualitative illustrations.\n\nStrengths of the paper:\n- The paper is clearly written and easy to follow. In particular, section 2.1 clearly motivates the formulation of the regularization term from a theoretical point of view (reminiscent of the formulation of adversarial examples) and Figures 1 and 2 motivate the regularization term from a practical point of view. I found Figure 5 particularly enlightening (the regularization term 'expands' the decision cells). \n- The method is clearly positioned with respect to previous work (in particular using \\ell_2 regularization of the gradients) \n- Experiments demonstrate the effectiveness fo the method. \n\nWeaknesses of the paper:\n- The link between the proposed objective and the sparsity could be made clearer: does this objective enforce sparsity of the gradients, the weights, and how does this affect training?\n\n\nJustification of rating:\nThe paper clearly presents a regularization method to improve post-training quantization. The approach is motivated both from a theoretical point of view and from a practical point of view. The latter aspect is of particular interest for the community. The claims are validated by a limited set of experiments that are seem nonetheless well executed.\n\n", "belong_id": "ryxK0JBtPr"}, {"uid": "r1eFtky0tr", "paper_title": "Gradient $\\ell_1$ Regularization for Quantization Robustness", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper models the quantization errors of weights and activations as additive l_inf bounded perturbations and uses first-order approximation of loss function to derive a gradient norm penalty regularization that encourage the network's robustness to any bit-width quantization. The authors claim that this method is better than previous quantization-aware methods because those methods are dedicated to one specific quantization configuration.\n\nThe derivation of the proposed method is not complex but I like the idea that models quantization error as additive perturbation in this context and how it eventually connects with gradient penalty that's widely used in GAN training and adversarial robustness.\n\nQuestions:\n\n1. What is the capital N in the time complexity of gradient computation in Sec. 4.1? The authors should discuss in details the time complexity of the proposed regularization well because this is an essential problem of the regularization, which involves double back-propagation and should be computationally heavy. For the same reason, I'd like to see the training time comparison, and more results with deeper networks.\n\n2. Compared to STE, one of the quantization-aware methods, the proposed method is not very competitive even in the setting when a STE network, which is specially trained for 6,6 bits but quantized to 4,4 bits, can outperforms the proposed method. This contradicts with the claimed strength of the proposed method. Will it be better when we regularize more, if we want the model to perform well when quantized to 4,4 bits? It would be better if there is a set of experiments of different regularization hyperparameters.\n\n***********************\n\nUpdate: I'd like to keep my score after reading the authors' response to all reviewers. I think the authors do address some questions but the paper still has some weakness in terms of performance.", "belong_id": "ryxK0JBtPr"}, {"uid": "r1gvs0imcr", "paper_title": "Gradient $\\ell_1$ Regularization for Quantization Robustness", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper shows that if we add L1 regularization on gradient in the training phase, the obtained model can achieve better post-training quantization performance since it is more robust to Linf perturbation on the weights. I like the intuition of the paper but there are several weaknesses: \n\n1. The main concern is that the proposed method cannot outperform quantization-aware fine-tuning. This probably limits the application of the method --- it will only be used when there's not enough time budget for quantization-aware fine tuning for each specific choice of #bits. It will be good if the authors can discuss in what practical scenario their algorithm can be applied. \n\n2. The method is only tested under uniform symmetric quantization. I believe to demonstrate that the L1 regularized models are indeed easier to be quantized, we need to test it on several different kinds of quantizations. \n\n3. I have concerns about the hyper-parameter selection for lambda. The authors mentioned that lambda is chosen by grid-search, but what's the grid search criteria? In other words, are the hyper-parameters trying to minimize the validation error of the 'unquantized model', or they are minimizing the validation error of the 'post-quantized model'? \n\n4. Some minor suggestions: \n\n- The current paper uses boldfaced n as perturbation which is quite confusing (since small n is the dimension). I would suggest to replace it by something else, e.g, \\Delta. \n\n- Section 2.3 seems redundant. It's clearly that L1 regularization is better given it's the dual norm of Linf, so clearly it's better than L2 norm. You have proved L2 is not good anyway in experiments. \n\n===========\n\nAfter seeing the rebuttal, my concerns about the parameters have been well addressed. Also, I agree with the authors that there are use cases for post quantization, and personally I think post quantization is much easier to do in practice than quantization-aware training. However, this is quite subjective so the fact that the proposed method doesn't outperform quantization-aware training is still a weakness of the paper. \n\nI would like to slightly raise the score to borderline/weak-accept. I hope the authors can have some experiments on non-uniform quantization if the paper is being accepted; I really think that will demonstrate the strength of the method. People will likely to use this method if it can consistently improve many different kinds of post quantization. ", "belong_id": "ryxK0JBtPr"}, {"uid": "S1gzAdmp_r", "paper_title": "Robust Domain Randomization for Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper introduces the high variance policies challenge in domain randomization for reinforcement learning. The paper gives a new bound for the expected return of the policy when the policy is Lipschitz continuous. Then the paper proposes a new method to minimize the Lipschitz constant for policies of all randomization. Experiment results prove the efficacy of the proposed domain randomization method for various reinforcement learning approaches.\n\nThe paper mainly focuses on the problem of visual randomization, where the different randomized domains differ only in state space and the underlying rewards and dynamics are the same. The paper also assumes that there is a mapping from the states in one domain to another domain. Are there any constraints on the mapping? Will some randomization introduces a larger state space than others?\n\nThe paper demonstrates that the expected return of the policy is bounded by the largest difference in state space and the Lipschitz constant of the policies, which is a new perspective of domain randomization for reinforcement learning. \n\nThe proposed method minimizes the expected variations between states of two randomizations but the Lipschitz constant is by the largest difference of policy outputs of a state pair between domains. Should minimizing the maximum difference be more proper?\n\nThe center part of Figure 2 is confusing, could the authors clarify it?\n\nIn the Grid World environment, how does the random parameter influence the states?\n\nThe baselines are a little weak. The paper only compares the proposed with training reinforcement learning algorithm on randomized environments. Could the authors compare with other domain randomization methods in reinforcement learning or naively adapt domain randomization methods from other areas to reinforcement learning?\n\nOverall, the paper is well-written and the ideas are novel. However, some parts are not clearly clarified and the experiments are a little weak with too weak baselines. I will consider raising my score according to the rebuttal.\n\nPost-feedback:\nI have read the rebuttal. The authors have addressed some of my concerns but why minimizing the expected difference is not convincing. I think the paper should receive a borderline score between 3 and 6. ", "belong_id": "H1xSOTVtvH"}, {"uid": "H1gGDk4uKH", "paper_title": "Robust Domain Randomization for Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\n\nTo improve the generalization ability of deep RL agents across the tasks with different visual patterns, this paper proposed a simple regularization technique for domain randomization. By regularizing the outputs from normal and randomized states, the trained agents are forced to learn invariant representations. The authors showed that the proposed method can be useful to improve the generalization ability using CartPole and Car Racing environments. \n\nDetailed comments:\n\nI'd like to recommend 'weak reject' due to the following reasons:\n\n1. Lack of novelty: The main idea of this paper (i.e. regularizing the outputs from normal and randomized states) is not really new because it has been explored before [Aractingi' 19]. Even though this paper provides more justification and analysis for this part (Proposition 1 in the draft), the contributions are not enough as the ICLR publications.\n\n2. As shown in [Cobbe' 19], various regularization and data augmentation techniques have been studied for improving the generalization ability of deep RL agents. Therefore, the comparisons with such baselines are required to verify the effectiveness of the proposed methods.\n\n3. For domain randomization, it has been observed that finding a good distribution of simulation parameters is a key component [Ramos' 19, Mozifian' 19, Chebotar' 19], but the authors did not consider training the distribution of simulation parameters in the paper. \n\n[Ramos' 19] Ramos, F., Possas, R.C. and Fox, D., BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators. In RSS, 2019.\n\n[Cobbe' 19] Cobbe, K., Klimov, O., Hesse, C., Kim, T. and Schulman, J., Quantifying generalization in reinforcement learning. In ICML, 2019.\n\n[Mozifian' 19] Mozifian, M., Higuera, J.C.G., Meger, D. and Dudek, G., Learning Domain Randomization Distributions for Transfer of Locomotion Policies. arXiv preprint arXiv:1906.00410, 2019. \n\n[Chebotar' 19] Chebotar, Y., Handa, A., Makoviychuk, V., Macklin, M., Issac, J., Ratliff, N. and Fox, D., May. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In 2019 International Conference on Robotics and Automation (ICRA) (pp. 8973-8979). 2019\n\n[Aractingi' 19] Michel Aractingi, Christopher Dance,  Julien Perez, Tomi Silander,  Improving the Generalization of Visual Navigation Policies using Invariance Regularization, ICML workshop 2019.", "belong_id": "H1xSOTVtvH"}, {"uid": "Bklrq4s0KS", "paper_title": "Robust Domain Randomization for Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a regularization scheme for training vision-based control policies that are robust to variations of the visual input, which the paper classifies as 'visual randomization'. Visual randomization, as the paper notes, is one of the state-of-the-art techniques proposed from simulation to real robot transfer of vision-based policies trained on synthetic images. The regularization proposed in the paper aims to produce policies that rely on features that are invariant to the visual randomization, so that the resulting behaviour of the agent is consistent across different variations of its visual input for the same states. \n\nThe paper proposes that forcing a policy trained under randomization to have a Lipschitz constant of K, over the randomization parameters, causes the optimal policy to be similar across randomization parameters, with the difference in expected returns of two randomized environments being bounded by K times the maximum difference in their randomization parameters.\n\nI recommend this paper to not be accepted until the following issues are addressed. \n\n* There are missing details from the experimental setup, which makes the results hard to interpret (see, below). \n\n* There are  missing details on how vanilla domain randomization was implemented. Domain randomization aims to maximize the expected performance over the environment distribution. This can be implemented properly by computing the expected gradients with data from more than one environment. From the algorithm descriptions in the appendix, it is not clear that this is how vanilla domain randomization was implemented. \n\n* The title, introduction and conclusions do not reflect the scope of the paper. The paper only addresses situations where the same behaviour is achievable on all environments, an assumption (Mehta et al, 2019) also makes, and its proposed regularization is based on the assumption that the optimal behaviour is achievable with the same policy on all environments. But this is not true in general: for dynamics randomization, different environments may require different policies (e.g. driving a car on a road vs driving off-road). The regularization method may result in overly conservative policies i such situations.  \n\nQuestions about experimental details: \n\nWhat are the maximum returns  for Cartpole when trained until convergence without randomization? (175? 200? 1000?) If the maximum returns are higher than 175, how does Figure 4 look with more data? This is crucial to understand, for example, the results in Figure 11. That figure shows the proposed regularization slightly hinders the performance for the environments near l=1, g=50 (that region is a darker shade of green on the left subfigure). How do we know if the task has been successfully solved in the green vs purple regions? In all experiments, are the training curves showing the performance of the policies over the same environments (same seeds)? If not, how are the training curves comparable?\n\nOther things to improve:\n\nThe conclusions of this paper can be made stronger by adding a comparison with EpOpt-PPO (i.e. optimizing the worst case performance over a set of trajectories sampled from multiple environments)", "belong_id": "H1xSOTVtvH"}, {"uid": "Sygb7qk0tr", "paper_title": "Adversarially Robust Representations with Smooth Encoders", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the vulnerability of representations learned by variational auto-encoders (VAE). It first show that the learned representation of VAE is susceptible to small changes, similar to the adversarial examples in supervised learning setting. Then propose a regularization method, called smooth encoder, to improve the robustness of the representation. Experiments are conducted on several benchmark datasets to show the effectiveness of the method. \n\nOverall I find the idea interesting and the experimental results promising. The following are my detailed comments.\n\na About the theory\nThe illustration of the problem in VAE is interesting. However, one missing point is to theoretically quantify the effect of the proposed regularization (in some simple cases). In particular, it is claimed that the regularization could make the encoder smoother and the experimental results clearly justifies it. What would be better is to show in which sense/measure the encoder is smoother and provide some theoretical guarantee about it. (for instance smaller Lipschitz constant?)\n\nb About the Experiment\nThe experimental section is clear and promising. I just have one question about the evaluation on the robustness of the VAE representation. In particular, a linear classifier is concatenated right after the VAE representation and it is not clear to me where it is concatenated. Is it right after the layer of \\mu and \\Sigma or in later layers? If it is in the later layers, the VAE is outputting a distribution, then how does the accuracy measured?\n\nMinor comment:\nI think it is unnecessary to introduce the new term selection strategy because it is just an adversarial training with respect to a different loss. In particular, the loss is the Wasserstein distance between the latent space vectors instead of a supervised loss. For simplicity, it could be just named as latent space adversarial training. (this is just a suggestion, which will not change my decision)", "belong_id": "H1gfFaEYDS"}, {"uid": "HJg8AR0-9H", "paper_title": "Adversarially Robust Representations with Smooth Encoders", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper analyzes the shortcoming of VAE objective, and propose a regularization method based on a selection mechanism that creates a fictive data point by explicitly perturbing an observed true data point. It is lead to Wasserstein distance between representations. Experiments are made on three datasets; ColorMNIST, MNIST, and CelebA, which shows superior performance on adversarial accuracy while similar accuracy to VAE on nominal accuracy.\nThe paper is well-organized and well-written. The point is clear and the proposed algorithm is valid. The only problem of the paper is the improvement on the experiment is marginal. Although adversarial accuracy is far better (like 0% vs 50%), it is apparent that the vanilla VAE is fragile to the adversarial examples because the added noise is intended so. Thus I can not say this is a fair comparison and because the superiority of the proposed algorithm is shown in only this point, I am not sure the proposed algorithm is surely useful. \n", "belong_id": "H1gfFaEYDS"}, {"uid": "rJe5MJ5_9B", "paper_title": "Adversarially Robust Representations with Smooth Encoders", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This is a very interesting paper, I believe, a solid contribution to Variational Autoencoders. The basic argument is that encoders in VAEs are highly susceptible to noise in input data, whereas decoders are not. This argument is supported with a full fledged section 2.2, reformulating ELBO objective of VAEs, and introducing a VAE with discrete latent variables and discrete observations, so as to easily understand why and where VAEs fail.\n\nTo make encoders robust to noise in inputs, it is proposed to generate new fictive data points in the neighborhood of original data points so as to ensure that the latent representations of a data point and its fictive version are similar in 'some sense' as part of the proposed regularization term. The implementation of this idea is solid in the paper, relating it to theoretical concepts such as  'entropy regularized entropy transport problem', 'Wasserstein distance', etc. The most important point is that, it is easy to extend an encoder of an existing VAE with the proposed algorithm, while letting a decoder be untouched as the latter is shown to be robust/smooth anyways (in sec 2.2). It is also discussed on how to generate fictive samples, including but not restricted to approaches like projected gradient descent based adversarial attacks.\n\nSection 2.2 can be improved further, in terms of presentation. This is the most important section which can be of interest to the community to understand VAEs' limitations, a good contribution on its own. Though challenging, I encourage the authors to improve the exposition in this section as much as possible.\n\nIntroduction is written beautifully. Good job, done!\n\nFor instance, some explanation about variables, m_j, u_i, their distribution.\n\nHow do you relate the Eq. 1 with the standard ELBO. (some reference to derivation?)\n\nIs it not possible to explain limitations of present VAEs without introducing the particular von Mise like parameterization (last equation of page 3). I am not suggesting that you should remove it. The connections between the two could be more explicit, though I understand that it is already mentioned in the paper, 'parameterization emulates a high capacity network that can model any functional relationship between latent states and observations...'. \n\nIn this context, I found the explanation after Eq. 2 to be intuitive in regards to inefficiency of encoders. If I understand correctly, to put it in even simpler terms, the encoding neural network is overfitting mapping from input data points to the latent representations, not performing any learning for the unseen data points at all; on the other hand, decoder explores the space of latent variables well because it is modeled as a Gaussian?\n\nSome of the new equations should be numbered for easy reference. \n\nOn page 4, the flow is a bit abrupt. Right after Fig. 3, there are points 1 and 2 added without any note on what these two points (items in latex) are about.\n\nI found point 1 very confusing in page 4. On the other hand, point 2 is beautifully written. Though, it could be made explicit in the latter on why encoders found in VAE are not smooth, referring to Fig 2, 3.\n\nThere are minor grammar mistakes making some of sentences incoherent or confusing, in the paper. Something to do with style of language. I think, overall, language can be improved. Though, technical flow of the paper is great, and introduction is written very well, pointing out very important bold insights about the literature on unsupervised representation learning. I would say, it is a very well written paper, which is an enjoyable read, despite some of the grammar mistakes which can be easily fixed by proof reading.\n\nExperimental evaluation is sufficient. \n\nLast but not the least, one could argue that we are going to the literature of kernel function based methods, or markov random fields, to improve the neural network models. This is a general trend we are observing. It is interesting to see new models such as the proposed one, getting the best from both worlds. It may be worthwhile to point  out something along these lines in the paper so that other works like this can be accomplished which are bold, and advance representation learning, digging mathematical concepts from diverse domains. If I am mistaken, please feel free to point out. It is not going to be change the review. I am inspired from this work.\n \nOne practical challenge is to generate fictive data points which are not very near to existing data points. I am not sure if GANs can achieve that, either. Having such points is critical to deal with more structured noise. Any comments on this? ", "belong_id": "H1gfFaEYDS"}, {"uid": "r1eMQEs3FB", "paper_title": "Continuous Adaptation in Multi-agent Competitive Environments", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "UPDATE: Thank you for the detailed response. I think the changes you have made to the paper have improved it, but there remains significant work to complete before the paper reaches its full potential. For example, Figure 3 is a useful additional insight but it provides no quantification of the variation between repeated runs nor a comparison to a suitable baseline approach. Similarly, the additional hyperparameter details in Appendix A are helpful to enable reproducability but without explaining how these values were chosen it is impossible to assess the rigor of the empirical evaluation.\n\n--\nThis paper proposes modification to Deep CFR and introduce a simplified baseball environment to evaluate the modifications. The rigour of and detail in documenting the empirical evaluation is not currently of the standard I would expect for publication at ICLR. I will detail suggestions for improvement below, but my most pressing concern for discussion in the rebuttal is with regard to the 2nd conclusion - If the team has learnt a Nash equilibrium as initial strategy, then how can performance of either team be improved by only one team further adapting their policy as shown in Table 5 lower avg2 by the difference between No Adapt (0.61) and Guest Adapt (0.51)? If I have interpreted the authors results correctly, this demonstrates that the initial policy is not a Nash equilibrium.\n\nSuggested Improvements:\nIn the related work section, the narrative of the paper would be clearer if the authors introduced why the meta-learning literature is being reviewed. I would also recommend weakening the claim that 'instability of the training process' is 'the critical issue in MARL' to just noting it is 'an issue' and request the authors justify the claim that this 'is particularly severe in a competitive environment'. Why is this more of an issue in fully competitive environments than it is in general sum games?\n\nIn Section 3.1 the authors claim they need to switch to TD learning from tree search due to stochastic state transitions but there are forms of search that can accommodate stochastic transitions so this claim needs to be removed. Perhaps the authors can motivate this change in another way? This section also introduces 2 further simplifications of the domain (1) learning strategies for half-innings and applying them to the whole game; (2) agents knowing their opponents true action chosen and not the noisy observation (e.g. pitchers target location instead of actual pitched location) and (3) 'both agents know each other's strategies'  - these should be included in Section 2.1 when the domain is described.\n\nSection 3.1. closes by stating 'In principle, their average strategies will gradually converge to the Nash-equilibrium strategy.' This is very weakly argued, to make such a claim the authors should provide evidence that their environment and modifications to Deep CFR meet the requirements of the theory where this guarantee was proven. \n\nSection 3.2. notes that the learning rate is 'manually determined' but the precise methodology of tuning hyperparameters is not provided and no values of settings used for any hyperparameter are included in the paper. Without details of the methodolofy it is unclear if a rigorous empirical evaluation was performed and without the precise hyperparameter settings used the results are not reproducible.\n\nSection 4.1. notes 'It is very interesting to observe that the learned Nash-equilibrium strategy is actually quite similar to real-life baseball strategies.' Setting aside the issue regarding whether a Nash equilibrium has been learnt, this is a subjective opinion not an rigorous empirical observation. The same applies to the comment 'The simulation results are very similar to real-life baseball games' on page 9  Can you support these claims that the learnt strategy is similar to real-life by comparison to the data collected from the MLB Statcast?\n\nIn Section 4.2. there is further imprecision in the discussion of results. For example 'as the strategy adaption mechanism is employed, the WP of most teams ... actually decreases.' As this mechanism is a core contribution of the paper, this evaluation needs to be more rigorous. Is there a statistically significant difference caused by using the proposed mechanism? \n\nAll results presented should include quantification of variation as well as average values and the number of repeats these averages are taken from should be clearly documented. The inclusion of a limited subset of teams in empirical evaluations (e.g. Table 4 only including teams 0,1,2,3 and 12 and Table 5 only including teams 5 and 13) should be clearly justified.\n\nAll references to papers published in conference proceedings or journals should cite the published version of the paper and not the arxiv version. All references should include the full publication venue and not abbreviations (e.g. Zhang and Lesser, 2010 is currently listed as just AAAI). References to online resources should include a note of the date accessed.\n\nMinor Comments:\n1) The word 'purpose' is used frequently in place of 'propose' (e.g. line 7 of the abstract)\n2) Page 2: 'plays the-best-of-three game' -> plays the best-of-three games\n3) Page 2: 'that worth further study' -> that are worth further study\n4) Page 3: 'To simply the game' -> To simplify the game\n5) Page 3: 'listed in C' -> listed in Appendix C\n6) Page 8 (and recurring): Initial speech marks are the wrong way around, this looks like a Latex error.\n7) Page 10: 'we would relief' -> we would relax\n8) Page 10: 'in the real life' -> in real life\n", "belong_id": "BkllBpEKDH"}, {"uid": "BkxzF3NCtr", "paper_title": "Continuous Adaptation in Multi-agent Competitive Environments", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies adaptation of agent policies in a simplified baseball game, which is designed as a zero-sum two-agent game between a batter (B) and a pitcher (P), each of which has 5 discreet actions. The introduced game is fully observable but stochastic, which the authors argue is a challenging setup. The authors propose a Bayesian-style adaptation of the agent strategies (where each agent models the probability of the actions of the opponent by computing the posterior give a prior and evidence from the past observations), which seems to be computable analytically, from an initialization learned with counterfactual regret minimization (CFR) that approximates the Nash of the considered game.\n\nComments/Questions:\n\n1. Why do we need adaptation if CFR already learns Nash? It looks like one of the key differences between the proposed game/setup and many of the previous work is that it is a fully observable zero-sum game. The initialization learned by CFR already might come close to the Nash equilibrium. It's unclear why the agents need to play something different than Nash. Could the authors argue (preferably, formally theoretically or at least quantitatively) why adaptation is necessary?\n\n2. Related to point 1, in section 4.2, it looks like post-adaptation strategies turn out to be superior when playing against opponents that play Nash. I would like to understand why. Do opponents actually play Nash? Does the asymmetry of the game have to do something with this? There's virtually no analysis of the results in the paper, which significantly undermines any contribution.\n\n\nOther comments:\n\n1. I'm personally not familiar with baseball, and the paper doesn't really introduce the game. So, parts of the introduction and background that use baseball-specific terminology (paragraph 3) make no sense to readers unfamiliar with the game. It would be nice to have the game exemplified and explained along with the key simplifying assumptions.\n\n2. Writing can be significantly improved (and compressed!). There are typos throughout. Some phrases from the paper which meaning is really hard to parse (for example, 'To focus on the impact of strategy adaptation over the winning percentage <...>').\n\nAlso, the last paragraph of the intro that describes the organization of the paper is unnecessary for conference submissions (it just takes spaces and no one reads it because it's easy to scroll through 10 pages to get a sense of the organization).", "belong_id": "BkllBpEKDH"}, {"uid": "r1x-LlrnFH", "paper_title": "Disentangling Trainability and Generalization in Deep Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper studies the relation between trainability and generalization ability in deep neural networks. In the theoretical analysis, the authors used the Neural network Gaussian process (NNGP) kernel and Neural Tangent kernel (NTK). The paper clarified that the spectrum of the NTK and NNGP has an important role in investigating the generalization and trainability, i.e., the condition number of the NTK. Some numerical experiments showed an agreement of theory with the practical behavior of learning algorithms. \n\nIn this paper, some existing theoretical results on deep neural networks were combined to extract new insight. Thought the attempt of this paper is interesting, the readability of the paper is not necessarily high. \n\n- In equation (2), the operator T is defined as the kernel K(x,x'). However, the definition seems different from that in equation (8).  The authors need to make clear the definition of T.\n- What is the 'DC' mode in the sentence above the equation (15)? \n- Is the derivation of the left part in equation (9) straightforward? How was the second term, chi_1 q^* p^(ell), derived?  I'm not sure how the dot{T} was dealt with. The argument below equation (3) should be used? ", "belong_id": "Bkx1mxSKvB"}, {"uid": "SkgIDMbRYS", "paper_title": "Disentangling Trainability and Generalization in Deep Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the evolution of Neural Tangent Kernel (NTK) at large-depth regimes. By analyzing the conditional number and eigenvalues, they identify three phases of hyper-parameters; 1) In the chaotic phase NTK converges to an identity matrix, which is easy to train but hardly to generalize. 2) In the ordered phase NTK converges to an all-one matrix, which is hard to train but generalizes well. 3) In the critical phase the conditional number converges to a constant. Furthermore, they also analyze the influence of pooling and flattening in CNNs and identify potential regimes where pooling hurts the generalization. They conduct empirical experiments to supporting their theoretical analyses.\n\nHowever, I think this paper is worth of more revisions because many theoretical analyses are unjustified. And some potential typos makes the analyses even more difficult to understand.\n\n1) It looks to me that Eq(2) and Eq(6) are contradictory, where T already contains sigma_w and simga_b in Eq(2) but re-multiplied in eq(6).\n2) The paper analyzes the dynamics by assuming the variances of inputs are q*, which is debatable. The variance q^l also evolves with the depth increases. It is unclear whether the condition number will change if you takes the evolution of q^l into considered.\n3) It is unclear how Eq(9) comes directly from Eq(6), and there aren't any rigorous proofs in the Appendix. Similarly for eq(14).\n4) In the paragraph below Eq(11) the paper states that \\Theta* becomes an all one-matrix. However, Eq(11) states the diagonal converges to q*/(1-xi_1), but the paragraph below Eq(9) states the off-diagonal converges to q*_{ab}/(1- xi_c). Because q*=q*_{ab} as you stated nearby, do you mean xi_1 = xi_c ? \n5) In the first paragraph of Section 3.3, p^l = q* and p^l=l q*. \n6) In the 2nd contribution, you mentioned 'eigenvector correlation', while I cannot find anywhere else introducing this.\n7) The plots of Figure 1(b) should behave like convex if the kappa really evolves like x_1^l / l. However it is concave. \n8) In the first experiment, you state 'To confirm that the maximal feasible learning rate are ... 2/(lambda_max)'. However, learning rates are never discussed in this paper. It is confusing why this experiment is useful. \n\nGenerally speaking, I think the paper needs careful revisions to support its theoretical analyses. ", "belong_id": "Bkx1mxSKvB"}, {"uid": "rJgSrGqM5S", "paper_title": "Disentangling Trainability and Generalization in Deep Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the spectra of neural tangent kernels (NTKs) at large depth -- first let width go to infinity, and then let depth go to infinity. At infinite depth the kernel has the form a*identity+b*(all-one matrix), and the paper studies how the large-depth NTK converges to the limit in three cases: chaotic, ordered, and critical line. The paper draws connection between these behaviors with the trainability and generalization of corresponding neural networks. Furthermore, the difference between CNNs with and without global average pooling is studied.\n\nNTK has been a popular subject of research in deep learning theory, and it's an interesting direction to study the NTK in large depth. However, the exposition is confusing and I'm missing some key points of this paper. Therefore I cannot recommend acceptance at this time. See below for detailed comments.\n\n1. I don't really get how the spectrum of large-depth NTK is connected to generalization. At infinite depth, the NTK is just a trivial kernel Theta^*, as noted in the paper. It is claimed that a finite-depth correction Eqn. (7) 'captures the generalization.' How exactly does it capture the generalization? Generalization appears to be highly dependent on the data distribution. I don't understand how the paper arrives at its conclusions regarding generalization.\n\n2. The paper (esp. Section 3) is written in a way very unfriendly to someone who is not familiar with previous work, with notation, derivations and conclusions buried in paragraphs. I wish there were some theorems clearly and formally summarizing the conclusions.\n\n3. It's unclear whether the studied regime (large depth, probably even larger with) is relevant in practice. Although there are experimental results provided, the CNN experiments are for the infinite-width NTK. It's unclear how they look like for practical networks.\n\n4. There are numerous typos and grammar errors in the paper, even in abstract and introduction.\n\n\n------\nupdate:\nThanks to the authors for the response, especially the clarification about what they mean by generalization. Since the concern about the exposition is still present, I can only update my rating to 'weak reject.' I hope the authors could further improve the exposition of this paper.", "belong_id": "Bkx1mxSKvB"}, {"uid": "HJgxdIHqFB", "paper_title": "Compressive Recovery Defense: A Defense Framework for $\\ell_0, \\ell_2$ and $\\ell_\\infty$ norm attacks.", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies the problem of the robustness of the neural network-based classification models under adversarial attacks. The paper improves upon the known framework on defending against l_0, l_2 norm attackers. \n\nThe main idea of the algorithm is to use the 'compress sensing' framework to preprocess the image: Using F, the discrete Fourier transformation matrix, and the algorithm tries to reproduce on every given input x, a vector y with the smallest number of non-zero coordinate such that Fy approximates x. The main algorithms proposed in this paper are sparse iterative hard thresholding (IHT) or base pursuit (BP) which are all quite simple and standardized. \n\nThe intuition of the approach is that l_0, l_2 attackers on the original input x can not allude the sparse vector y by too much, thus the recovered vector Fy could have better robustness property comparing to the original input x. \n\n\nThe main concern for me is the experiment in this paper. The author does not provide enough details about how the attacker is trained in their task. It seems that the authors only use the attacker trained on a standard neural network. However, since the authors have a preprocessing algorithm (IHT, BP) on top of the given input, the attacker should in principle tries to attack this pre-processing process as well. Since the pre-processing process is not differentiable, it is, therefore, unclear to me how to define the true robustness of the approach of the authors. \n\nAn analog of my argument is if we create an artificial network that has a pre-processing layer that zeros out most of the input pixel, however, if we train an attacker without this knowledge (so it tries to attack a network without this pre-processing), the l_2, l_0 attacker might not be very good for the true network. \n\n\nAfter Rebuttal: I have read the authors' responses and acknowledge the sensibility of the statement. However, I still think the algorithm in this paper is merely a 'clever' version of gradient masking, which does not give the neural networks real robustness, it is just harder to design attacks on all these discrete operations.\n\n", "belong_id": "B1x9ITVYDr"}, {"uid": "SylPwe8AKB", "paper_title": "Compressive Recovery Defense: A Defense Framework for $\\ell_0, \\ell_2$ and $\\ell_\\infty$ norm attacks.", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper extends the compressive sensing framework introduced in Bafna et al. to handle l1 and l2 attacks. The authors provide theoretical analysis for several recovery algorithms (IHT, BP, DS) and provide experimental result on CIFAR-10, MNIST and Fashion-MNIST. \n\nMy major concern is how significant the provided results are. It is indeed interesting to extend the compressive sensing framework to handle l1 and l2 attacks. However, the proposed recovery algorithms are all classical ones, and it is unclear how novel the analysis is, since the authors do no discuss the technical challenges they overcome or the difference between their proof techniques and the previous ones. Also, it would be nice if the authors could discuss the theoretical results in more detail, e.g., how to interpret them and new insights it brings to us. \n\nMoreover, some experimental details are missing. In last paragraph in Section 3.1, the authors say ``We then use both x and x to train the network''. How do you do so? Just add both x and x' to the training set? ", "belong_id": "B1x9ITVYDr"}, {"uid": "H1xdXAoCYB", "paper_title": "Compressive Recovery Defense: A Defense Framework for $\\ell_0, \\ell_2$ and $\\ell_\\infty$ norm attacks.", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper extends the compressive recovery defense framework introduced by Bafna et al. (2018), which is mainly against l_0 attacks, to l_2 and l_ attacks. They provide guarantees for some recovery algorithms in the case of different kinds of norm bounded noises. The difference between their work and the previous work is clearly clarified. \n\nOverall, this paper is a follow-up work towards Bafna et al. (2018) but with better theoretical guarantees and ample experiment results to support their robustness against various popular attacks. Given their contribution and inspiration for future work, I think this paper could be accepted to the 2020 ICLR conference. \n\nIn Section 3.2 Recovery Algorithms, the author clearly states three algorithms including IHT, BP, DS, and their modification from the standard ones, but fails to compare the differences between these algorithms. It is not clear about the authors motivations to proposes these different recovery algorithms and whether their performance varies from each other also remains unknown. Maybe some analysis about their disadvantages and advantages in varied conditions of attacks could be necessary. \n\nIn the Section 3.4 Comparison to Related Work, the author mentions many works aiming at defending against adversarial inputs. However, Bafna et al. (2018) is the only work here that has something to do with compressive sensing. I think maybe the paper should involve some related work here regarding theory of compressive coding besides Bafna et al. (2018). And how they are combined to the defense against adversarial inputs. It would help the readers to have better understanding towards the novelty and breakthroughs in this aspect. \n\nFor the experiments, it would be better to have the comparisons between the proposed algorithm and related methods. Also, the proposed IHT and DS are modified versions. What are the differences in experiments? \n\nMinor comments:\n- Page 2: the line above the equation 1 meaning that x_t (k)....., it could be (x_t ) (k)\n- Page 6: in the explanation of figure 2, the adversarial inputs are second column and fifth column not fourth column. \n", "belong_id": "B1x9ITVYDr"}, {"uid": "Hygd6m3lKr", "paper_title": "Generalized Clustering by Learning to Optimize Expected Normalized Cuts", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new clustering method, called CNC, which is composed of two-step procedures.\nIt first embeds an input dataset into a d-dimensional space, followed by performing relaxed normalized cut to detect clusters.\nAlthough the contribution of introducing a new relaxed formulation of the normalized cut is interesting, I have the following concerns regarding with the clarity, significance, and evaluation of the proposed method.\n\n- The paper is not clearly written at many points and the quality of presentation is not high, which also deteriorates the significance of the paper.\n    In particular, the optimization process for clustering discussed in Section 4.1 is not clearly presented.\n    Although the objection function, which is the expectation of the Ncut, is introduced in Equation (6), how to solve it is not presented.\n    Since this is the key step for CNC, it should be carefully discussed.\n- In the embedding step, how to choose the dimensionality d?\n    This is not even reported in experiments.\n- Empirical evaluation is not thorough and important evaluation is missing.\n    * First, the contribution of embedding is not evaluated.\n      The performance between CNC with the proposed embedding and without it should be compared.\n      Moreover, the sensitivity of the performance with respect to changes in d should be examined.\n    * A number of resulting scores are missing; in particular, CNC is compared to only SpectralNet for CIFAR-10 and CIFAR-100 under NMI.\n      It would be more convincing if the NMI for other methods are also reported.\n- Parameter sensitivity is not evaluated, while there are a number of parameters in the proposed method as reported in Section 5.7.\n    Since parameter tuning is fundamentally difficult in the unsupervised setting, parameter sensitivity is crucial.\n    Also how to choose such parameters is not clear.\n\nMinor comments:\n- In Algorithm 1, line 1, 'X \\in R^n' -> 'X \\subseteq R^m'?\n- In Algorithm 1, the dimensionality 'm' of data points and the batch size 'm' are the same. Is it correct?\n- At the first line in Section 4.1: 'for each data point' -> 'For each data point'\n- P.4, L.-4: 'nreast-neighbor' -> 'nearest-neighbor'\n", "belong_id": "BklLVAEKvH"}, {"uid": "Hyx0Uc59tS", "paper_title": "Generalized Clustering by Learning to Optimize Expected Normalized Cuts", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an end-to-end approach for clustering. The proposed model is called CNC. It simultaneously learns a data embedding that preserve data affinity using Siamese networks, and clusters data in the embedding space. The model is trained by minimizing a differentiable loss function that is derived from normalized cuts. As such, the embedding phase renders the data point friendly to spectral clustering. \nThe paper follows the general setup of deep clustering: map data to a feature space while maintaining data distributional characteristic, and make data clustering-friendly in the feature space. The authors use Siamese networks for the first part and use a normalized-cut motivated loss for the second part.  The choices are reasonable and the loss is somewhat novel. \nCNC is evaluated on standard datasets, including MNIST, Reuters, CIFAR-10, and CIFAR-100. The results are impressive. However, deep clustering has been around for quite a few years. It might be time to move on to more challenging benchmarks. \n", "belong_id": "BklLVAEKvH"}, {"uid": "BkgdoUmCYr", "paper_title": "Generalized Clustering by Learning to Optimize Expected Normalized Cuts", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper suggests a differentiable objective that can be used to train a network to output cluster probabilities for a given datapoint, given a fixed number of clusters and embeddings of the data points to be clustered. In particular, this objective can be seen as a relaxation of the normalized cut objective, where indicator variables in the original formulation are replaced with their expectations under the trained model. The authors experiment with a number of clustering datasets where the number of cluster is known beforehand (and where, for evaluation purposes, the ground truth is known), and find that their method generally improves over the clustering performance of SpectralNet (Shaham et al., 2018) in terms of accuracy and normalized mutual information, and that it finds solutions with lower normalized cut values.\n\nThe method proposed in this paper is very simple and appears to work well, and so this paper represents an important contribution. However, there are some issues with the presentation that I think should be fixed before publication:\n- Equation (3): I'm not sure I understand the sum over z; don't we just want w_{ij} Y_{ik} (1 - Y_{jk})?\n- Equation (6): I don't think the final objective should be presented as an expectation. It is rather the quotient of two expectations. In general, it might be better to just present the objective as a relaxation of the normalized cut objective.\n\nA question regarding the results and parameterization: was using Gumbel-Softmax necessary to get good results? Did ordinary softmax not work?", "belong_id": "BklLVAEKvH"}, {"uid": "S1lOqdUCFS", "paper_title": "Multi-Task Learning via Scale Aware Feature Pyramid Networks and Effective Joint Head", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper works on the problem of improving object detection and instance segmentation. It is realized by two independent contributions: 1). adding a high-to-low-resolution connection in FPN and 2) adding more connecting layers between the mask and classification heads. Experiments show both contributions give a small improvement (~1AP) on both detection and instance segmentation task.\n\nOverall, the method seems reasonable and the improvements are healthy. The main concern is the technical novelty. It is not supervising that adding more connections inside the network can improve some performance. People have tried a lot of that (e.g. M2Det). These kinds of improvements come with a cost of slowdown and are usually not that appealing in practice.\n\nThe motivation for leveraging the relationship between segmentation and detection as a joint model is interesting and relatively new. However, the proposed method of feeding back the regressed bbox for segmentation seems straightforward and far from the full potential. It also requires a larger RoI feature map, which makes the contribution less clear. A fancier method or a more thorough analysis of how the information is shared between the two tasks is demanded for an ICLR publication.", "belong_id": "r1ezqaEFPr"}, {"uid": "SygMU1Uy5B", "paper_title": "Multi-Task Learning via Scale Aware Feature Pyramid Networks and Effective Joint Head", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes several modifications to the Mask R-CNN model:\n1. a variation of Feature Pyramid Networks (FPN): SA-FPN which merges features top-down and bottom-up.\n2. the 'effective joint head' (EJ-Head) which consists of\n    a) moving the segmentation head behind the detection one,\n    b) Doubling resolution of RoI crops, calling this 'enriched feature'.\n    c) adding Boundary Refinement (really just a residual block).\nExperiments show slightly improved scores on MS-COCO\n\n\nI vote to reject this paper.\n\nFirst of all, it is badly written. Not only are grandiose formulations of 'smart framework', 'innovative way', 'we slickly mix' not professional, but the whole write-up of the method is very confusing. After reading it multiple times, I am still not 100% sure I fully understood the SA-FPN.\n\nSecond, it is not clear to me that SA-FPN is really novel, the original FPN paper already compared top-down and bottom-up performances (Tables 1-3).\n\nThen, two modifications which simply increase capacity and could explain all improved scores are not ablated: increased resolution of RoI crops, and added 'boundary refinement', which is really just a residual block.\n\nFurthermore, moving the segmentation *after* the bounding-box prediction is *not* joint prediction. If predicting the box is called p(b) and predicting the mask is called p(m), the Original Mask R-CNN does p(b|features)p(m|features), the proposed model in this paper does p(b|features)p(m|b,features), and actual joint prediction would be p(b,m|features).\n\nFinally, I think the paper is much better suited for a conference like ICCV/ECCV or CVPR and will get better reviewers than me there.", "belong_id": "r1ezqaEFPr"}, {"uid": "HkeRF7LnKH", "paper_title": "Functional Regularisation for  Continual Learning with Gaussian Processes", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper develops a continual learning method based on Gaussian Processes (GPs) applied in the way introduced by prior work as Deep Kernel Learning (DKL). The proposed method summarizes tasks as sparse GPs and use them as regularizers for the subsequent tasks in order to avoid catastrophic forgetting. Salleviating the instability resulting from the representation drift.\n\nEmploying inducing point training for task memorization is a novel and interesting idea, which could be useful for the continual learning community. The fact that this approach also captures the uncertainty of the replays contributes fairly to robustness. Lastly, performing knowledge transfer by inheriting the KL term of the ELBO is also interesting, however, its theoretical implications deserve a close look. It would be enlightening to analyze which true posterior the learned model then corresponds to. Would not it be a slightly more principled Bayesian approach (i.e. one that has stronger grounds at first principles) to perform the knowledge transfer to assign the posterior of one task as the prior of the other, alternatively to keeping the entire KL term intact which employs the q(u_i) as the surrogate for q(u_j), i.e. the way introduced by Nguyen et al., 2017?\n\nThe presentation clarity of the paper is open for improvement. For instance, the abstract is written in a sort of convoluted way. I do not get how the KL divergence suddenly kicks in and for what exact purpose. Is it variational inference or a hand-designed regularization term?\n\nI find the argumentation from Eq. 1 downwards until the end of Sec 2.1 on BNNs with stochastic weights and their relation to GPs a bit unnecessary complication. These are very well known facts. It would suffice to state briefly that the task learner is a vanilla DKL used within a vanilla sparse GP.\n\nFigure 1 is also not so descriptive. I do not get what the GP here is exactly doing. What is input to and for which output modelity does it find a mapping? What is the calligraphic L in the figure? Is it a neural net loss or an ELBO? \n\nIn general I could not grasp why it makes sense to treat the the output layer params of a neural net treated for continual learning? They will not be sufficient to encode a task anyway, as an expressive enough neural net will leave only a linear mapping to the final layer. What happens if the intermediate representations of the input observations require a domain shift as the tasks evolve?\n\nOverall, the presented ideas are fairly interesting and the experimental results are good enough for proof-of-concept, though not groundbreaking (behind the close relative VCL on MNIST and no comparison against VCL on Omniglot). Hence, this is a decent piece of work that lies somewhere around the borderline. My major concern is that the proposed method is conceptually not novel enough compared to Nguyen et al., 2017. My secondary concern is that the presentation is very much open to improvement in points hinted above.\n\n--\nPost-rebuttal: Thanks to authors for their tremendous effort to alleviate my concerns. The fact is that the conceptual novelty of the paper is too slim compared to VCL. As mentioned above, I even find the VCL approach more principled. I could have viewed the outcome of the paper as a slightly bigger news for the community if there was something unexpectedly positive on the reported results. However, as it appears from the comment below, the authors propose a super close variant of VCL that combines a few well-known techniques in a rather straightforward way and achieves in the end a model that performs on par with it. Under these conditions, I have hard time to find a reason to champion this paper for acceptance. That being said, I view this paper a tight borderline case due to its technical depth, hence I will not object to a reject decision either.", "belong_id": "HkxCzeHFDB"}, {"uid": "B1xoID52tS", "paper_title": "Functional Regularisation for  Continual Learning with Gaussian Processes", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a function-space based approach to continual learning problems (CL), wherein a learned embedding\n\n    $\\hat{\\mathbf{x}} = \\text{NN}(\\mathbf{x}; \\theta)$\n\nis shared between task-specific GPs s.t.\n\n    $f_{i}(\\mathbf{X}) \\sim \\mathcal{N}(\\mathbf{0}, k_{i}(\\hat{\\mathbf{X}},\\hat{\\mathbf{X}}))$, \n\nwhere the $i$-th task's covariance $k_{i}$ is a defined via standard variational inducing points methods. CL manifests as KL divergences between tasks' variational posteriors $q_{i}$ and their respective priors $p_{i}$. Since the embedding helps define $p_{i}$, its parameters $\\theta$ are regularized to promote sharing.\n\nThe work investigates both practical and theoretical implications of this setup. On the practical side, the authors discuss enhanced 'on-task' inference via hybridization of function- and weight-space based approaches and, subsequently, strategies for optimizing inducing points. Additionally, a novel approach for automatically detect task switching is introduced that exploits the Bayesian aspects of the proposed framework.\n\nOn the theoretical side, points of (personal) interest revolved around differences between weight- and function-space approaches to CL. Here, I think that streamlining the presented argument would go a long ways. Paraphrasing, one of the authors' key insights is that:\n\n  1) CL in weight-spaces is hard, since weights' semantics are moving target that change along with shared parameters.\n  2) CL in function-space is easy, since the functions (i.e. tasks) themselves remain the same.\n\nThis information is provided in the introduction, but (as a relative newcomer to CL) I failed to connect regularization and rehearsal/replay based methods with the aforementioned spaces. It was only upon reading Sect 2.5 that this intuition 'clicked' for me. Hence, I suggest making this observation as obvious and intuitive as possible.\n\nThe provided experiments seem reasonable and do a good job highlighting different facets of the paper. Two additional results would be appreciated:\n\n  a) How well calibrated are FRCL-based classifiers?\n  b) How impactful is the hybrid representation (Sect 2.3) for test-time performance?\n\nGP approximations formulated solely in terms of weighted sums of (finitely many) basis functions typically suffer from degradation of predictive uncertainties. Since one often motivates use of GPs via a desire for well-calibrated uncertainty, (a) seem quite pertinent.\n\n\nNitpicks, Spelling, & Grammar:\n  - Lots of run-on sentences; consider breaking these up.\n  - Introductory modifying phrases are missing commas.\n  - Consider citing other recent works that use NN basis functions in conjunction with Bayesian Linear Regression.\n  - Various missing or superfluous words resulting in some garbled sentences, e.g.:\n      - '... our approach looks constraining.'\n      - 'The ability to detect changes based on the above procedure comes from that in'\n      - 'While the task boundary detection results for Omniglot are less strong, which may due to the smaller batch size (32 for Omniglot,  100 for the MNIST-versions), resulting a noisier test result.'\n", "belong_id": "HkxCzeHFDB"}, {"uid": "HJeWwjLpqH", "paper_title": "Functional Regularisation for  Continual Learning with Gaussian Processes", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThe authors propose a method to perform continual learning with neural networks by incorporating variational Gaussian Processes as a top layer (also called Deep Kernel Learning) and constructing an objective utilizing the inducing inputs and outputs to memorize across tasks.\nThey further study ways to approximate this behavior with weight space models and use their model for task boundary detection by utilizing statistical tests and Bayesian model selection.\nExperiments show good performance of their method.\n\nComments:\n1. The mathematical formulation of the basic model is very elegant. However, it is not immediately clear to me that the joint ELBO across successive tasks is still lower-bounding the actual objective.\n2. The paper is well written overall.\n3. To the best of my knowledge using such a model for task boundary detection is novel and quite interesting. There are obvious links to Bayesian changepoint detection in the timeseries setting. Possibly these links would be made more clear by a citation to a recent paper such as Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection by Knoblauch and Damoulas, or any other paper of similar content. The link is quite fascinating.\n4. Sections 2.3 and 2.4 of the paper are the weakest points and quite unsatisfactory as they forgo the elegance of the proposed approach to do 'something else' that Sec. D explains how to salvage with 'tricks'. Especially with regards to Sec. 2.4, why can't we just do inference on Z_i and have to pick datapoints via discrete optimization? That comparison would be useful in the experiments. Furthermore, recent papers utilizing GPytorch by Gardner et al have dramatically sped up GP inference. Could we aim to make the original idea fast enough to be used instead of resorting to an approximate model with weight spaces and corrections to extract Z and u per task?\n5. The experiments are good, but very focused on MNIST tasks. I would appreciate tasks of different structure given how well the method appears to work.\n\n\nDecision:\nI find the basic idea of the paper quite appealing as it leverages the elegance of the deep kernel learning formulation to yield an attempt at a principled Bayesian version of continual learning and demonstrates empirical value. \nSome discussion on the objective might be warranted to demonstrate that it actually lower bounds the true LLK.\nI am quite happy with the task boundary detection section and would encourage the authors to strengthen the link to changepoint detection.\nMy biggest qualms with the paper are that it departs from that strategy and performs weight space inference for training per task and then 'corrects' to move back to the GP representation. A more convincing discussion would be welcome here.\nThe experiments are functional and show good results, but I would appreciate more diversity in the tasks.\nAs the paper stands I learn towards recommending acceptance and would strongly encourage the authors to iron out the weaknesses of the paper.", "belong_id": "HkxCzeHFDB"}, {"uid": "rylO65p7Kr", "paper_title": "Estimating counterfactual treatment outcomes over time through adversarially balanced representations", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper adapts domain adversarial training to construct treatment invariant representation for adjusting time-varying confounding in counterfactual treatment outcome predictions over time. \n\nThe proposed method extends the recent static settings of balancing representation in counterfactual inference to longitudinal settings, and it also overcomes the problem of high-variance of IPTW weighting in MSN based methods such as the current state-of-art method RMSN.\n\nThe paper is very well written. Method is novel to me. Experiments are sufficient. However since I am not very familiar with this area, so there can be things I miss in the evaluation of this paper.\n\nThere is one problem with the illustrations in Figure 1: predictions on the potential outcomes before the first treatment was given should have stayed on the same path, and then depart when different treatment was initiated (at different time)", "belong_id": "BJg866NFvB"}, {"uid": "H1e1Ni-3FB", "paper_title": "Estimating counterfactual treatment outcomes over time through adversarially balanced representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper introduces Counterfactual Recurrent Network (CRN) that is able to estimate the effects of various treatments from longitudinal data. The claim is that the model can decide (i) treatment plan; (ii) optimal time of treatment; and (iii) when to stop treatment. The proposed method attempts to learn time-invariant representations that are not predictive of the next treatment by borrowing ideas from Ganin, et al. (2016)s work on for domain adversarial training. In fact, this paper is an extension of (Atan et al., 2018) to be applicable for longitudinal data. \n\nThis paper should be rejected due to the following arguments:\n\t- Page 1, par. 4, line 4-7: The example given in sentence For instance, if ... is true regardless of time. Therefore, it does not describe a situation where a time-variant confounder might make causal inference hard. In fact, nowhere in the paper there is a solid description of how time-variant confounders would break the current approaches for causal inference. \n\n\t- Page 3, par. 4, last sentence: I dont understand why sequential treatments and change of covariates through time can stop us from using the conventional methods for learning balanced representations. Why not consider each time point as an instance (thus representing each patient with multiple instances) and learn the representation from the collection of all these instances?\nAll in all, I dont understand why the authors assume that the world is non-Markovian and that they have to consider the entire \\bar{H}_t. Especially when in the their Experiments section the model used for tumor growth that generates the synthetic data is Markov, and consequently, does not check if the proposed method works in a non-Markovian world.\nIn summary, if the goal is to solve the non-Markov case, the presented experiments dont show that the proposed method is addressing that; and if the  goal is to solve the Markov case: (i) why insist on framing the problem as non-Markov; and (ii) (Atan et al., 2018) already have solved this (since we established that there are no time-dependent confounders and as the authors state on page 4, par. 3, line -4 to -3 the novelty here comes from the use of domain adversarial training to eliminate bias from the time-dependent confounders).\n\n\t- Page 6, Theorem 1 and 2 lines above it: This theorem does not prove that the confounding bias is completely removed because the objective function in Eq. (4) being optimized also includes a loss term for outcome prediction. In fact, IF there is a confounding bias (that [partially] determines both treatment and outcome), it would be wrong to remove it from the learned representation.\n\nThings to improve the paper that did not impact the score:\n\t- Page 2, par. 1, line -2: Use of adverb Moreover is wrong. Extreme weights are due to division by small Pr(t|x). The consequence of this numerical instability is high-variance estimates.\n\t- Page 2, par. 4, 1st sentence: incomprehensible\n\t- Page 2, par. 4, line -2: but also should come after a not only.\n\t- Page 3, par. 4, line -1: incorrect usage of citation; both author name(s) and publication year must be in parentheses.\n\t- Page 4, par. 5, last line: sentence For testing, the ...  needs elaboration.\n\t- Page 6, par. 1, line -3: start of should be start off.\n\nReferences:\n\t- Atan, O., Zame, W. R., & Van Der Schaar, M. (2018). Counterfactual Policy Optimization Using Domain-Adversarial Neural Networks. In ICML CausalML workshop.\n\t- Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., ... & Lempitsky, V. (2016). Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1), 2096-2030.\n\n\n********UPDATE after reading the rebuttal********\nThe authors have addressed my major concerns in their rebuttal and therefore I have increased my rating form reject to weak accept. \n\n", "belong_id": "BJg866NFvB"}, {"uid": "S1lBUXOaKr", "paper_title": "Estimating counterfactual treatment outcomes over time through adversarially balanced representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work addresses the problem of causal inference in time-dependent treatment regimes. To address the problem, the authors propose an extension of the balancing representation for causal inference framework that seeks render the current treatment independent from a representation of the history of treatment and confounders. This is sensibly actualized within an RNN. The authors provide empirical results that demonstrate the proposed method performing very well in comparison to prior art. \n\nThis paper is well written, proposes a sensible solution to a difficult problem, and performs well empirically. It would be nice to see theory that connects the imbalance to the expected error of the causal estimand, and a notion of hyperparameter tuning (the authors say they search over hyperparameters but do not specify how the parameters are ultimately selected. However, I don't feel that either of those points outweigh the benefit of the paper. ", "belong_id": "BJg866NFvB"}, {"uid": "SJepVJVitS", "paper_title": "Model Ensemble-Based Intrinsic Reward for Sparse Reward Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThis paper proposes a model-based method for intrinsic rewards based on probabilistic neural network ensembles. For a particular ensemble element, an intrinsic reward is defined as a log-term that measures the deviation in prediction between a Gaussian mixture over all ensemble elements and the particular ensemble element (at the previous update period). The intrinsic reward signal applied in practice is the minimum of the aforementioned quantity w.r.t. all ensemble elements. The authors provide some theoretical motivation for their approach and validate their method in sparse-reward continuous robotics domains (Mujoco), using PPO as reference algorithm. Experiments are averaged over 10 seeds and compared against various other intrinsic reward baselines including PPO without intrinsic motivation. In these experiments, the method provided by the authors seems to dominate over competing approaches.\n\nQuality\n\nI find the quality low. First, while I appreciate that the authors try to provide some theoretical motivation for their method, there is quite a gap from theory to practice. The theory assumes that the world model is known and considers a stationary reward signal. In practice, the world model is approximated with the current network ensemble, a quantity that is changing over time yielding a non-stationary signal. Second, the experiments are conducted in a sparse-reward modification of Mujoco-type environments that are non-sparse by construction. Sparsity is introduced by accumulating instantaneous rewards over some time window before 'releasing' them. This way of sparsifying rewards yields weird partial observability issues, e.g. the same state-action pair observed at the right moment in time yields significant reward whereas at the wrong moment in time yields no reward at all. Additionally, it is always guaranteed that there will be a reward signal at a certain frequency. There are probably better environments for studying the approach, like Atari for example. I do understand that the authors cite Oh et al. 2018 who apply the same technique of sparsification, but Oh et al. also conduct additional experiments in ALE.\n\nClarity\n\nThe paper is clearly written and easy to follow. On a side note, it could be stated more clearly that the presented approach is not model-based because no forward prediction is required for constructing intrinsic rewards (merely probability values are queried for observed transitions). There is one question I have though regarding the experiments where different intrinsic reward approaches are compared against each other. The paper states that all approaches normalize intrinsic rewards according to Equation (17)---why do all of them then need a different weighting factor \\beta as stated in the second paragraph of Section 3.3? Furthermore, since \\beta is fine-tuned for each intrinsic reward approach and each environment, can the authors please elaborate in detail how exactly this is accomplished (to ensure correct interpretability of the results)?\n\nOriginality\n\nThe originality is low. As stated by the authors, the proposed method is an incremental extension of Achiam and Sastry 2017 who proposed a similar method for non-ensemble methods.\n\nSignificance\n\nThe significance is low as well. The method's improvement over other intrinsic reward approaches is minor (the environments chosen by the authors are also not ideal). I feel the significance is reduced further by the fact that there are other model-based approaches that use models of the proposed kind for increasing sample-efficiency and performance considerably in non-sparse environments (e.g. Kurutach et al.2018).\n\nUpdate\n\nAfter reading the other reviews and the rebuttal, the concerns I have raised remain. I still feel this paper shouldn't be accepted to ICLR. However, given the extensive experimental analysis conducted by the authors, I feel a score of 1 from my side was too harsh. I therefore increase to 3.", "belong_id": "SyxJU64twr"}, {"uid": "rJeUWy3iYH", "paper_title": "Model Ensemble-Based Intrinsic Reward for Sparse Reward Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an approach for using an ensemble of learning dynamics models to generate an intrinsic reward for reinforcement learning in sparse reward environments. The paper's results demonstrate that this approach out-performs other benchmark approaches across a set of continuous control tasks.\n\nI'm curious about the motivation for taking the min surprise across the ensemble. Eqs 8-11 seem to motivate that this will make a tighter gap between the expected cumulative returns of the optimal policy and the policy with additional intrinsic rewards. This implies that this will converge to the same policy in the end, but I'm not sure that it implies that this is a good exploration strategy, an intrinsic reward of 0 would provide an even tighter bound. When you used the max or average return, was the problem that there was still too much intrinsic reward for the agent to converge? Or that it wasn't exploring enough or in the right places?\n\nFor the experiments, it would be very interesting to see the intrinsic rewards accumulated over time for each approach. That plot could be very enlightening as to what is going on. \n\nIt would also be great to see the results on these tasks when they're not modified to be sparse reward. Is your method a big hindrance in that case? Or does it still help?\n\nYour intro claims that typical model-free RL is about the circumstance where the agent receives non-zero reward for every time step. This is not true, many model-free RL papers look at tasks that have sparse reward on some or many steps.  I would agree that in many continuous control problems, shaping rewards are added to ease the exploration problem. \n\nFor Figure 3, how does the model-ensemble TRPO (Kurutach et al) fit in? Is that algorithm represented by one of the curves?\n\nHere's one more related work deriving intrinsic rewards from an ensemble of trained dynamics models:\nhttp://www.sciencedirect.com/science/article/pii/S0004370215000764", "belong_id": "SyxJU64twr"}, {"uid": "ryefwM-CtS", "paper_title": "Model Ensemble-Based Intrinsic Reward for Sparse Reward Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis paper proposes an auxiliary reward for model-based reinforcement learning. The proposed method uses ensembles to build a better model of environment dynamics and suggests some rules to optimize the new ensemble-based dynamics and to estimate the intrinsic reward.\n\nI am torn on this paper. I like the derivation of the method and the ideas behind it. I think it is an interesting direction of research. However, the experiments are limited to one domain and the paper needs proofreading. I will vote 'weak accept' for this paper, as I think it is incremental and the experiments are too limited.\n\nAs I said above, the paper could use some proofreading. Some sections (like pages 1-2) are well written, but others are full of grammatical mistakes. There is also a lot of redundant information.\n\nIt is often stated that the ensemble model has better capacity than the single model. Some experimental proof of this better modelling capacity could help convince a reader that the ensemble is indeed beneficial and warranted (e.g., show that P_K is better than the single model P).\n\nSome evaluation or discussion on the computational costs of the method would be beneficial. I assume the ensemble-based method is more computationally intensive. Would it perform better than single surprise if they were compared according to wall clock time?\n\n\nExamples of minor issues:\n\n- Page 3, after equation 7, sentence beginning with 'In addition to the advantage that the mixture model (4) has the increased model order for modelling' is confusing, contains redundant elements, and is not bringing useful information. It should be revised\n- page 4, in paragraph after eq. 11: the following sentence is grammatically incorrect, please revise: 'Propositions 1 and 2 suggest a way to intrinsic reward design under the mixture model for tight gap between ( ) and ()'\n- in the same sentence, revise: 'be close to the true () of the true optimal policy , as desired.',\n- page 6 text above Figure 1: 'single-modal' should be unimodal\n\n\nPOST REBUTTAL \n\nThanks for writing your rebuttal. I have read it, as well as the other reviews. I think reviewer 1 touches on some important points, especially regarding the engineered sparse rewards. It seems the method is not properly justified given the environments used for its evaluation. Based on this, and the fact that the method is rather incremental, I would like to change my score to a weak reject. The method should be evaluated in a setting with truly sparse rewards.", "belong_id": "SyxJU64twr"}, {"uid": "BygXZbAD_S", "paper_title": "Collaborative Generated Hashing for Market Analysis and Fast Cold-start Recommendation", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes new hashing schemes to learn hash codes to describe users/items for the purpose of recommendation.\n\nIt is claimed that doing so leads to three contributions\n(1) the new hash codes themselves, which can apply to various recommendation settings\n(2) the ability to better discover potential users in e-commerce settings\n(3) state-of-the-art performance on several datasets\n\nIn terms of (1), the paper contrasts with Collaborative Deep Learning and dropoutnet. The main difference compared to CDL is that the hashing-based method learns binary codes. Compared to DropoutNet the main difference is the use of a stacked autoencoder, rather than a different neural network architecture. This latter contribution is perhaps a little thin (really it's just a technical detail); the additional contribution of being able to be used as a marketing strategy I didn't really follow.\n\nThe 'mining potential users' contribution (contribution 2) seemed a little ad-hoc to me. It ultimately seems like a variant of KNN, and seems like something similar could be attempted for other methods.\n\nTraining etc. looks fine, though I didn't fully check the details.\n\nThe experiments seem not totally convincing. Most critically, the method does not seem to exhibit state-of-the-art performance as claimed, but is somewhat lower (in terms of accuracy) than other baselines. It might be better in terms of speed but this doesn't seem to be thoroughly evaluated. The choice of accuracy as the only evaluation metric also seems unusual.\n\nThe selection of datasets is also quite limited. It's claimed that these are the only datasets with user and item content, but why are both needed to run an experiment? Can't this method work with either (in which case many other datasets would be appropriate)?\n\nOverall the actual results seem mixed, and thus the paper hinges on its statement that it has the 'advantage of applications in marketing area'. However this latter contribution seems handwavy.\n\nIn order to be accepted, I'd need to see\n-- More clearly stated and demonstrable contributions\n-- More compelling experiments, in terms of datasets, evaluation measures, and actual performance", "belong_id": "HJel76NYPS"}, {"uid": "Bygqiv-3Fr", "paper_title": "Collaborative Generated Hashing for Market Analysis and Fast Cold-start Recommendation", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces a collaborative generated hashing (CGH) method to learn hash funcations of users and items from content data. The approach first provides a strategy to discover potential users by the generative step and inference through adding balanced and uncorrelated constraints. The experiments demonstrates some effectiveness on improving accuracy for both warm-start and cold-start recommendations.\n\nThis paper should be rejected because (1) this method only combines existing techs, such as Stochastic Generative Hashing (Eq.1 and Eq. 6), and lacks novelty; (2) lack of introduction to related work and baselines, (3) the experiments results can not support the claim, i.e. the effectiveness of CGH in marketing area, and (4) paper writing is awful and very hard to follow.\n\nMain argument\n\nAlmost every essential parts of the proposed method are from existing methods:\n(I) Eq. 1 and Eq. 6 are proposed by Stochastic Generative Hashing [1];\n(II) Eq. 2 and Eq. 5 are a multivariate Bernoulli distribution;\n(III) Eq. 3 is a normal distribution;\n(IV) Eq. 7 is proposed by [2];\n(V) loss function Eq. 9 is follow the Minimum Description Length principle [1];\nThe proposed method CGH is a combination of these techs and compared with these methods, there are few novel aspects.\n\nThis paper omits the related work part and does a rough introduction to two baselines (CDL and DropoutNet) in a confusing way in Section 2. A concise and precise introduction to other methods will help the reader to better understand the related works and the advantages and disadvantages of the proposed method.\n\nThe experiments do not provide convincing evidence of the corretness of the proposed method, especially in Section 3.3. In Section 3.3, Figure 3 shows the performance on Accyracy@k without any baseline. The results do not demonstrate the validity of the method and therefore cannot support the author's claim.\n\nThings to improve the paper that did not impact the score:\n1) page 1, 4th line in the 3rd paragraph, 'efficient' -> efficiently\n2) page 3, 1st sentence in Section 2.1\n3) page 3, hard to find the definition of the similarity function \n4) page 4, 2nd line 'similar for' -> 'similar to'\n5) page 6, 3rd paragraph 'From Fig. 2, we discosver the k nearest potential users for an item j'. What do you mean?\n\n\nReference\n[1] Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, and Le Song. Stochastic generative hashing. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 913922. JMLR. org, 2017.\n[2] Ke Zhou and Hongyuan Zha. Learning binary codes for collaborative filtering. In Proceedings of KDD12, pp. 498506. ACM, 2012.", "belong_id": "HJel76NYPS"}, {"uid": "BklWz3InFS", "paper_title": "Collaborative Generated Hashing for Market Analysis and Fast Cold-start Recommendation", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The work considers the problem of efficient user and item recommendations in the warm- and cold-start settings. It aims at improving computational efficiency of the best candidate selection in these settings by utilizing binary codes representation. The transformation from an actual to a binary code representation is learned in a hybrid manner using both collaborative and content information. In order to keep such representations compact yet expressive enough, the authors impose a set of constraints that ensure balanced and uncorrelated transformations. Once binary codes are learned, the inference can be made by virtue of efficient Hamming distance computations. Moreover, the search for candidate entities can be performed via the generative step that projects binary codes onto actual feature space, where kNN-based techniques can be further utilized.\n\nMajor drawback of the work is that it does not provide any quantitative evidence to support the main claim  that the proposed approach is at least more computationally efficient, since it underperforms competing methods in terms of accuracy. Essentially, the work answers the question whether it is possible to utilize hashing techniques based on binary codes; however, the question on the practicality and efficiency of this approach remains open. I would therefore suggest rejecting the work.\n\nOne of the weak points of other methods noted by the authors is the expensive similarity search in real latent space. The authors aim to resolve that problem by learning hashing functions based on compact and informative binary codes representation. However, while an overall problem formulation is clearly described and the learning objective is well explained, no further evidence supporting initial claims is provided. Moreover, an overall logic seems contradictory:\n1.\tBinary codes allow efficient preference estimation via XOR operation.\n2.\tLearning binary codes is a difficult discrete optimization task.\n3.\tHence, we employ special MDL principle for solving a constraint optimization problem and employ relaxation of hash codes to move away from discrete optimization.\nAfter relaxation, the hash codes are no longer binary. Do you still enforce binary representation by some thresholding or other method? If yes, more words explaining this should be added to the text. If no, then how is it different from classical learning of latent variables? Essentially, relaxed non-binary hash codes are similar to latent vectors.\nThe lack of description raises concerns in an overall efficiency and the authors, unfortunately, provide no evidence of improved computational performance. Given that the proposed approach underperforms competing methods in terms of accuracy of recommendations, more efforts should be made to demonstrate its competitive advantages in terms of time required for training and generating predictions.\n\nAnother contradictory part is the generative step for candidate selection. The idea of using inference to generate the most pertinent user vector for a selected item hash code is novel and interesting. However, it requires searching neighbors in the real feature space, which can be very inefficient, depending on the structure of features. Im not convinced that it is better than searching neighbors directly in the latent space, which can be done in the majority of hybrid models. Moreover, there exist various approximate nearest-neighbors search methods, e.g. Annoy, NMSLib, Faiss, etc., which allow trading-off accuracy and efficiency. Considering that hash codes also lose some information (which is observed in the results of experiments), it seems necessary to have a comparison with these approximate methods as well.\nIt should be also noted that in some cases you dont even have to run the similarity search. Many hybrid models learn latent representation of features directly and cold-start entities are straightforwardly described via combination of the corresponding latent vectors of their features (e.g, Factorization Machines [Rendle 2009]). Hence, affinity between a cold item and some user can be quickly estimated via inner product of their latent vectors. \n\nSuggestions on improving the work:\nWorth mentioning, that recent studies raise certain concerns about the superiority of modern neural network-based approaches over simpler (and properly tuned) linear baselines, see the work by [Dacrema, Cremonesi, Jannach 2019] on A Worrying Analysis of Recent Neural Recommendation Approaches. The DropoutNet method in your experiments is very similar to CDL in terms of accuracy. The latter, however, underperforms even simple knn models, as shown by the work mentioned above. The havent tested it in the cold-start regime, though. Still, Id strongly recommend adding to your experiments comparison with simpler hybrid models, e.g., Factorization Machines. Also note that there are even stronger baselines published recently, e.g. HybridSVD by [Frolov, Oseledets 2019].\n\nAdditional remarks:\n1)\tFigure 3 and the related text seem to focus on too obvious things. Indeed, by increasing the number of entities to compare against, you increase chances to have a hit. This part of the text, basically, states that the method works, which can already be seen from other results.\n2)\tA lot of attention is given to the marketing application. Its ok to have it in introduction and make a connection to the real-world problem; however, further mentions of it in the text feel excessive. In the experiment section you describe a standard evaluation procedure for the cold start, there is no need to refer to marketing application again as you do not provide any new metric. It would feel much more organic if you would have the results of A/B testing on real users. Otherwise, Id suggest to focus more on the problem that youre solving, not on possible application.\n3)\tThe text is in a very unpolished state. It reads more like a draft version. There are many typos and error both in text and in derivations.\n\nReferences:\nRendle, Steffen. 'Factorization machines.' In 2010 IEEE International Conference on Data Mining, pp. 995-1000. IEEE, 2010.\nFrolov, Evgeny, and Ivan Oseledets. 'HybridSVD: when collaborative information is not enough.' In Proceedings of the 13th ACM Conference on Recommender Systems, pp. 331-339. ACM, 2019.\nDacrema, Maurizio Ferrari, Paolo Cremonesi, and Dietmar Jannach. 'Are we really making much progress? A worrying analysis of recent neural recommendation approaches.' In Proceedings of the 13th ACM Conference on Recommender Systems, pp. 101-109. ACM, 2019.", "belong_id": "HJel76NYPS"}, {"uid": "H1g0adr3Kr", "paper_title": "Multichannel Generative Language Models", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "[Paper summary]\nThis work is an extension of KERMIT (Chan et al., 2019) to multiple languages and the proposed model is called multichannel generative language models. KERMIT is an extension of Insertion Transformer (Stern et. al, 2019), a non-autoregressive model that can jointly determine which word and which place the translated words should be inserted. KERMIT shares the encoder and decoder of insertion Transformer, and the source sentence and target sentence are concatenated to train a generative model (also, various loss functions are included). In this work, parallel sentences from more than two languages are concatenated together and fed into KERMIT. Each language is associated with a language embedding. This work demonstrates that a joint distribution p(x1, . . . , xk) over k channels/languages can be properly modeled through a single model. The authors carry out experiments on multi30k dataset.\n\n[Pros] Some discoveries of this work are interesting, including: (1) It is possible to use a single model to translate a sentence into different languages in a non-autoregressive way. (2) The unconditional multilingual generation in Section 4.5 is interesting, especially, the generation order is determined by the model rather than left-to-right.\n\n[Questions]\n1.\tThe authors work on multi30k dataset, which is not a typical dataset for machine translation. \n(A)\tThe dataset and the corresponding information is at https://github.com/multi30k/dataset. The number of words in a sentence is smaller than 15, which is too short for a machine translation. Also, the pattern of sentences is relatively simple.\n(B)\tFor real world application, I am not sure whether it is possible to collect a large amount of k-parallel data where $k>2$. Therefore, the application scenario is limited. What if we have a large amount of bilingual data instead of k-parallel data? How should we leverage the large amount of monolingual data?\n2.\tFor novelty, this is an extension of KERMIT to a multilingual version, which limits the novelty of this wok.\n3.\tThe best results on En->De in Table 1 are inconsistent. On tst16, bilingual en<->de is the best; on tst17, en<->{rest} is the best; on mscoco, any<->rest is the best. In Table 2, seems using bilingual data only is the best choice. This makes me confuse about how to use your proposed method. However, \n", "belong_id": "r1xQNlBYPS"}, {"uid": "S1gYvmxpFB", "paper_title": "Multichannel Generative Language Models", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a multichannel generative language model (MGLM), which models the joint distribution p(channel_1, ..., channel_k) over k channels. MGLM can be used for both conditional generation (e.g., machine translation) and unconditional sampling. In the experiments, MGLM uses the Multi30k dataset where multiple high quality channels are available, in the form of multilingual translations.\n\nI feel that this paper is not ready for publication at ICLR due to the following major issues:\n\n* Missing important related work: This paper seems unaware of an important related work 'Multi-Task Learning for Multiple Language Translation' by Dong et al, ACL 2015. In fact, Dong et al. investigated the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Although machine translation is just an example of MGLM, Dong et al. is highly relevant to the conditional generation with MGLM, needless to say that they share the same multi-language translation problem domain. Thus, this paper will be much stronger if comparison with important baseline methods is provided.\n\n* Limited novelty: This paper extends Chan et al.'s KERMIT by applying its objective on tasks with more than 2 sequences, in order to learn the joint distribution p(channel_1, ..., channel_k) over k channel sequences. Most of the math in this paper can be found in the original Chan et al.'s paper. The extension to the multichannel case is incremental as it is hard to justify the challenge of such extensions.\n\nBesides, as minor suggestions, it would help readers if more illustrations of Figure 1 (especially the inference part) can be provided.", "belong_id": "r1xQNlBYPS"}, {"uid": "SJlT6ASJcS", "paper_title": "Multichannel Generative Language Models", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This submission belongs to the area of multi-view modelling. In particular, the submission describes construction of multi-view language models that (i) can generate text simultaneously in multiple languages, (ii) can generate text in one or more languages conditioned on text from another language. This submission extends previously proposed KERMIT from two views to more than two views. I believe this paper could be of interest to multi-view modelling/learning community. \n\nThough the original KERMIT approach is very interesting and you application of it to more than two views is also interesting I find the presentation to be poor. In particular I find section 2 to be hard if not impossible to understand without referring to the original paper where the story, equations, nomenclature are much more clearly explained. Even though your extension from two views to multiple is simple I find reliance on a diagram to be a mistake as I find your description not to be very clear. Given that there are no equations to support the reader and that the original equations are not adequate I find it hard to understand Sections 2 and 3. The key experimental result in Table 1 is only briefly commented on despite featuring multiple models with different strength and weaknesses, multiple types of inference. If space is of concern I would suggest removing Figure 2 (or changing input from non-English to English and removing or removing another qualitative table).\n", "belong_id": "r1xQNlBYPS"}, {"uid": "rJemdEkV9H", "paper_title": "DDSP: Differentiable Digital Signal Processing", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper develops a framework for for audio generation using oscillators with differentiable neural network type learning. They showcase the usefulness and effectiveness of the approach with several examples such as timbre transfer, dereverberation, changing the room impulse response, pitch extrapolation and so on. I can imagine the proposed learnable oscillator based autoencoders in a variety of applications. \n\nI think that this suggested software library can be useful for a wide range of audio researchers, and I commend the authors for this contribution. It is very nice to see an example of research where we make use of our physical understanding of the sound medium rather than blindly throwing a neural network at the problem. \n\nI have one important question though: how susceptible do you think the system is robust with respect to f0 and loudness encoders? Have you experimented with situations where the f0 and the loudness encoders might fail (such as more non-periodic and noisy signals)? \n", "belong_id": "B1x1ma4tDr"}, {"uid": "rJlmneOVqr", "paper_title": "DDSP: Differentiable Digital Signal Processing", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a model for audio generation/synthesis  where the model is trained to output time-varying parameters of a vocoder/synthesiser rather than directly outputting audio samples/spectrogram frames. The model is trained by mining an L1 loss between the synthesised audio and the real training audio. Overall, I found the paper to be well written, I found the online supplementary material helpful in getting an intuition for the quality of audio samples generated and to understand the various parts of the proposed architecture. I also found the figures to be extremely  informative, especially figure 2. Although, I think the title of the paper could be updated to something more specific like Differentiable Vocoders or something similar, since the description and experiments very specifically deal with audio synthesis with vocoders, even though the components might be general DSP components. I think the paper presents a reasonable alternative to current autoregressive audio generation methods and should be accepted for publication. \n\nMinor Comments\n\n1. The reference provided for RNNs, Sutskever et. al. 2014, should be supplemented with older references from the 80s when RNNs were first trained with backdrop through time. \n2. The bias of the natural world is to vibrate. I am not sure what exactly is meant here. Is it really a bias if every object in the universe vibrates? I think the term bias has been overloaded in the paper and is confused with structural/inductive priors. Bias as used in the paper, seems to have several means. It would help the description if the authors cleared up this confusing use of the term. \n3. In Section 1.3, the authors claim that one of the strengths of the proposed method is that the models are relatively small, however all discussion of the sizes is relegated to the appendix. It would be helpful to the reader if some model complexity comparisons are presented in the results section and a high level summary is presented at the end of Section 1. \n4. In Section 3, some additional high-level description for the Harmonic plus Noise model (Serra & Smith, 1990) should be provided to motivate the discussion and experiments in the rest of the paper. \n5. Section 4.2 should provide some description of the parameters counts for each of the components considered and how this compares with existing auto-regressive generation algorithms. \n", "belong_id": "B1x1ma4tDr"}, {"uid": "SyeL1QN2qS", "paper_title": "DDSP: Differentiable Digital Signal Processing", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This very nice paper tackles the integration of of domain knowledge in signal processing within neural nets; the approach is illustrated in the domain of music. \n\nThe proof of concept (audio supplementary material) is very convincing. \n\nThe argument is that a 'natural' latent space for audio is the spectro-temporal domain. Approaches working purely in the waveform, or in the frequency domains must handle the phase issues. Approaches can learn to handle these issues, at the expense of more data. \nA key difficulty is that the L_2 loss does not match the perception. The authors present a perceptual loss that addresses the point (Eq. 4 - clarify the difference w.r.t. Wang et al.), . A natural question thus is whether applying this loss on e.g. Wavenet, Sample RNN or Wave RNN would solve the problem.\n\nIn short, the contribution is in designing a latent space that accounts for independent components of the music signal (pitch; loudness; reverberation and/or noise), using existing components (oscillators, envelopes and filters), and making them amenable to end-to-end optimization (noting that loudness can be extracted deterministically).\n\nI understand that the auto-encoder is enriched with a FIR filter at its core: the input is mapped into the time-frequency domain; convolved with the output of the neural net H_l, and the result is recovered from the time-frequency domain. \nExplain 'frames x_l to match the impulse responses h_l'.\n\nCare (domain knowledge and trials and errors, I suppose) is exercized in the conversion and recovery (shape and size of the window) to remove undesired effects. \n\nOverall, the approach works in two modes: one where the fundamental frequency is extracted, one where it is learned. I miss this comparison in the audio material, could you tell where to look / hear ? \n\n\nQuestions:\n* NN operate at a slower frame rate (sect. 3.2): how much slower ? How sensitive to this parameter ?\n\nDetail\n* were, end p. 5. A word missing ?  \n* are useful --> is useful ? \n* could produced, p.6 ", "belong_id": "B1x1ma4tDr"}, {"uid": "SkeVhAmjYH", "paper_title": "GQ-Net: Training Quantization-Friendly Deep Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work introduces GQ-Net, a novel technique that trains quantization friendly networks that facilitate for 4 bit weights and activations. This is achieved by introducing a loss function that consists of a linear combination of two components: one that aims to minimize the error of the network on the training labels of the dataset and one that aims to minimize the discrepancy of the model output with respect to the output of the model when the weights and activations are quantized. The authors argue that this has the effect of guiding the optimization procedure in finding networks that can be quantized without loss of performance. For the discrepancy metric the authors use the KL divergence from the predictive distribution of the floating point model to the one of the quantized model. The authors then propose several extra techniques that boost the performance of their method: 1. scheduling the weighting coefficients of the two loss terms (something which reminisces iterative pruning methods), 2. stopping the gradient of the floating point model w.r.t. the second loss term, 3. learning the parameters of the uniform quantizer, 4. alternating optimization between the weights and the parameters of the quantizers and 5. using separate batch normalization statistics for the floating point and quantized models. The authors then evaluate their method on Imagenet classification using ResNet-18 and Mobilenet v1 / v2, while also performing an ablation study about the extra tricks that they propose.\n\nThis work is well written and in general conveys the main idea in an effective manner. Quantization friendly neural networks in an important subject in order to make deep learning tractable for real world applications. The idea seems on a high level to be interesting and simple; train floating point models that can fit the data well while also encouraging them to be robust to quantization by enforcing the predictive distributions of the fixed and floating point models to be similar in the KL-divergence sense. Nevertheless, I do have some comments that would hopefully help in improving this work:\n\n- It does seem that GQ-Nets need extra tricks in order to perform well, and those tricks come with their own set of hyperparameters that need to be tuned. For example, at section 4.3 you mention that the top-1 accuracy of vanilla GQ-Nets is 60.95, which is lower than the RelaxedQuant baseline (that has 61.52). This raises the question whether the boost in performance is due to the several additional steps employed (which in general can be applied to other quantization techniques as well), and not due to the main idea itself. \n- Do you employ the straight-through estimator (STE) for the weights in the L_q objective? In the second paragraph of the second page you argue that due to the biased gradients of STE the performance is in general reduced, so I was wondering whether STE posed an issue there or whether you used an alternative estimator. \n- How is batch normalization handled? Do you absorb the scale and shifts in the weights / biases before you perform quantization or do you quantize the weights and then apply the BN scale and shift in full precision?\n- How do you ensure and ub > lb when you learn the quantizer? In general learning the quantizer can be also done with alternative techniques (e.g. simply learning the scale and offset) so I was wondering whether you noticed benefits from using the ub, lb parametrization compared to others.\n- Do you show the pre-quantization distributions at Figure 2b? In the caption you mention quantized but the resolution seems to be higher than the 16 values you should get with 4 bits. Furthermore, it should be noted that the discrepancy in BN in quantized models was, as far as I am aware, firstly noticed at [1] (and subsequently at RelaxedQuant) and both of these methods simply re-estimated the moving averages during the inference time.\n\nOverall, I am on the fence about this work and tend to reject. Having said that, I am of course willing to revise my score after the discussions with the authors / other reviewers.\n\n[1] Probabilistic Binary Neural Networks, Jorn W.T. Peters, Max Welling", "belong_id": "Hkx3ElHYwS"}, {"uid": "Hye6gdYCYr", "paper_title": "GQ-Net: Training Quantization-Friendly Deep Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper propose a new quantization-friendly network training algorithm called GQ (or DQ) net. I addresses the existing issues in the common paradigm, where a floating-point network is trained first, followed by a second-phase training step for the quantized version. It is a well-written paper. Concepts were clearly explained and easy to follow. Below I present my comments about some details in the paper that were not entirely clear for me. \n\n- The two loss terms conflict each other. If the training algorithm focuses too much on the first term, it will make the network less friendly to the quantization process. On the other hand, the second one is going to enforce too much emphasis on the accuracy from the quantized network. It is natural to involve some hyperparameter search to find the balance between the two blending parameters. The paper suggests a strategy as to how to handle this issue, but it is not comprehensive, and rather controversial. I think the paper will benefit from a more in-depth discussion and analysis on this regularization issue. \n\n- The schedule for the loss term blending parameters looks drastic to me. Its more like train the floating point net first, and then train the quantized one, and then revisit the floating point one, and so on. I know I simplified, because the floating point network never stops getting updated as its \\omega_f is always 1. However, it seems to me that this drastic scheduling strategy sounds like very similar to the traditional approach that trains the floating point network first and then finetune the quantized one, except for the fact that this proposed algorithm repeats this process a few times. Hence, I think the authors argument about the supremacy of the proposed method to the two-step finetuning approach is not clearly supported. \n\n- The exponentially decaying learning rate scheduling looks like the one from ResNet. Im wondering if it should be the best, especially with the drastic introduction and omission of the second loss. \n\n- In the ablation studies, it seems that some of the suggested training options are conflicting each other and the clear winner seems to be the multi-domain BN. I cannot conclude anything from this analysis as to which one is more important than the other one, except for the Alt{W,\\theta} case. \n\nSome minor things:\n\n- Whats the name of the proposed network? Is it GQ or DQ?\n ", "belong_id": "Hkx3ElHYwS"}, {"uid": "HJlVa_8WcB", "paper_title": "GQ-Net: Training Quantization-Friendly Deep Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors propose a framework towards 4-bit auantization of CNNs. Specifically, during training, the proposed method contains a full precision branch supervised by classification loss for accurate prediction and representation learning, as well as a parameterized quantization branch to approximate the full precision branch. A quantization loss between the full precision branch and the quantization branch is defined to minimize the difference between activation distributions. The authors proposed a series of improvements, including alternative optimization, dynamic scheduling, detach and batch normalization to help boosting the performance to SOTA under 4-bit quantization.\n\nStrengths:\n+ Well-written paper with good clarity and technical correctness.\n+ Proposed method seems light, sweet and technically correct.\n+ Good experimental performance and result on ImageNet.\n+ Good and clear ablation study.\n\nWeaknesses:\n- Major performance improvement comes from the combination of different incremental improvements.\n- Lack of evaluations with variety of datasets (CIFAR-10/MNIST)/configurations (other bitwidth)\n- Lack of the citation and comparison to many most recent works on binarized networks (except XNOR-Net)\n\nComments:\nI consider this a well-written paper with great clarity and good empirical performance. I enjoyed reading the paper. The proposed framework seems technically correct and effective. \n\nHowever, a major weakness of this work is that most of the performance improvement comes from a combination of add-on improvements, except that the authors put them together into a unified framework and explained elegantly. The vanilla architecture, which is a main contribution and described in Fig. 1, doesn't seem to give that significant improvement. To some extent, the real technical contributions of this work are partly weakened given the add-on combinations and the existence of similar methods. For example, the alternative optimization of W and \\theta is similar to alternative re-training in network pruning, although a unified loss/optimization framework is applicable in this case. Others such as dynamic scheduling and gradient detach are also heuristic-driven.\n\nThe results on ImageNet under 4-bit quantization are strong and convincing, but the paper could benefit from conducting additional experiments on different datasets and bitwidth configurations. A more comprehensive study similar to Louizos et al., 2019 will be great. Citations and comparisons to more recent binarized networks other than XNOR-Net will be appreciated too.", "belong_id": "Hkx3ElHYwS"}, {"uid": "B1eu4ZJAtr", "paper_title": "RL-LIM: Reinforcement Learning-based Locally Interpretable Modeling", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the problem of interpreting predictions of blackbox models. In particular, they study local interpretable models, which are used to study interpretability at the level of one or a few data points. A key challenge is that in order for local models to be interpretable, they need to be simple in form and therefore lower in capacity (i.e. linear); thus, if they are trained on entire datasets they will underfit.\n\nThe aim of this work is to address this issue by learning how to select representative samples from a dataset for training local linear models to reproduce predictions of black-boxes. In particular, they propose to use RL to learn to weight instances from datasets; RL is required as they make hard (i.e. non-differentiable) decisions to select a subset of the dataset. \n\nThis work is closely related to Ren et al. 2018 [1], which proposes to meta-learn how to weight samples in a batch so as to maximize performance on a validation set. By analogy, in this paper, the samples are taken from the entire dataset (i.e. the dataset is subsampled), while the validation loss is effectively an imitation loss formed by treating the black-box model predictions as a target. The data subsampling operation introduces the added complication of non-differentiability. The contribution of this paper is thus the use of RL for meta-learning how to subsample a larger dataset in order to maximize some validation loss. \n\t\t\n* Pros:\n\t* Considers an interesting dataset subsampling variant of the sample weighting meta-learning problem.\n\t* Novel application of meta-learning for improving locally-linear models.\n\t* Extensive quantitative evaluation shows that the method seems to perform better than baselines, though it might be that a differentiable approximation could do as well while being more sample efficient.\n\t* It is a nice result that the l1 penalty actually works well in reducing the number of samples chosen by the \n\t* I found the discussion and figures presented in 4.2 to be quite nice and informative.\n\n* Cons:\n\t* Given the lack of a differentiable approximation baseline, I am not entirely convinced that the use of RL is absolutely necessary/optimal.\n\t\t* I.e. if the weighting function is actually high-entropy, randomly sampling a (large) batch and weighting it might work just as well.  \n\t* Though there is discussion of the complexity of the overall method it would be nice to see a discussion and figures related to the sample efficiency of REINFORCE?\n\t\t* This would be strongest if given with a comparison to differentiable alternatives (mentioned above) as well.\n\t\t* This would help elucidate whether RL is optimal in this setting: fitting a linear model on more data might be cheaper learning to subsample with REINFORCE. \n\t* While the sample weighting function is fast at inference time, most of the overhead comes at training time. This function needs be updated in settings where the underlying dataset changes.\n\t* This is a minor issue, but this pushes the burden of interpretability further up to the black-box sample weighting function. While this interpretability problem is less critical, it still exists.\n\n* Other comments/requests:\n\t* While the use of RL is certainly motivated in order to solve the problem in an unbiased way, it would be nice to see a comparison to a differentiable approximation as a baseline? A few ideas:\n\t\t* Randomly sample a (possibly large batch) and learn to weight it (closely related to the straight through estimator)\n\t\t* Randomly sample a batch and apply [1]\n\t* Would be nice to show the sizes of datasets and how many samples end up being used for different values of lambda.\n\t* Would be nice to understand which samples are chosen and why. This is probably tricky to analyze, but it would be interesting to see if certain samples are often chosen, or if the weighting distribution has an interesting shape (i.e. is low or high-entropy).\n\nIve given a weak accept, conditioned on being provided more evidence regarding 1) comparisons to simple differentiable alternatives, 2) sample efficiency of the RL method, and 3) basic analysis of the weighting function. \n\n[1] Learning to Reweight Examples for Robust Deep Learning. Mengye Ren, Wenyuan Zeng, Bin Yang, Raquel Urtasun. https://arxiv.org/abs/1803.09050\n", "belong_id": "BJx8Fh4KPB"}, {"uid": "BJxA1JhCtS", "paper_title": "RL-LIM: Reinforcement Learning-based Locally Interpretable Modeling", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors aim to learn a locally interpretable model via the reinforcement learning approach, to address the fundamental challenge which is that the previous locally interpretable model has smaller representation capacity than black-box models, and causes under-fitting with conventional distillation techniques. Overall speaking, the paper is well organized, and the proposed approach is well tested, but in my opinion, there is a conceptual error.\n\nYou claimed your method is REINFORCEMENT LEARNING based, but the REINFORCEMENT LEARNING definition for your task is weird, or wrong. In section 3, you didn't give an explicit explanation for the state transition. With your given RL-like objective function, it seems that the state transition is from features to features. However, there is no specific correlated explanation in your paper on why you make such an assumption. Besides, the state transition in RL relies on decision making at each time step, while it has not reflected in your paper and code, namely, the state-transition independents on the decision making.\n\nTo sum up, I dont think the proposed method is RL-based, it would be more appropriate to define it as a MAB problem, and this paper should solve this problem before publishing.\n", "belong_id": "BJx8Fh4KPB"}, {"uid": "HyeCSCxhYr", "paper_title": "Thwarting finite difference adversarial attacks with output randomization", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present a randomization defense in the black-box threat model, where bounded l_2 norm perturbations are allowed. In the presented scheme, the defender adds Gaussian noise to every coordinate of the output probability vector before returning an inference result. \n\n\nThe paper suffers from an incomplete evaluation, and therefore I cannot recommend acceptance. \n\n\nDetails:\n* Completeness of evaluation. As shown by Ilyas et al [1] and Cheng et al [2] and Brendel et al [3], black-box attacks can succeed even when only information about the label is present. Thus, I suspect that an attacker can simply run any of these attack algorithms in order to fool the model. In addition, if we use enough samples in the NES-based Query-Limited algorithm of Ilyas et al (that is, with enough samples per step) then we should be able to perfectly mimic the white-box attack, which as shown in Figure 2 is effective. \n* Potential flaw in evaluation. BAND performs worse than QL on the undefended classifier in Table 1, which should not occur. I would check to make sure that the attacks are applied correctly here.\n* Lacking details in evaluation. I could not find how many samples are used to perform the attacks in Table 1. It is hard to evaluate the defense without knowing how many samples are used in each algorithm.\n\n\n[1] https://arxiv.org/abs/1804.08598\n[2] https://arxiv.org/abs/1807.04457\n[3] https://arxiv.org/abs/1712.04248\n", "belong_id": "S1lDkaEFwS"}, {"uid": "rklTFuDlcS", "paper_title": "Thwarting finite difference adversarial attacks with output randomization", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method for defending black box (in particular, finite difference based loss) adversary attacks by randomisation of the output of the network. \n\nThe idea appears to be somewhat novel considering that majority of existing methods consider randomize inputs or the model itself. \n\nA natural question that would be particularly interesting to me is how does such defence compare against the defence by  randomizing the input and the model. There is no such comparison in the paper, which, to me, is the main weakness of the paper. \n\nThe authors consider the randomization in terms of Gaussian distribution. How would this differ if other types of distributions are considered, e.g. non-Gaussian distributions?\n\nSection 4.2 considered the finite difference gradient error, and discussed the results for untargetted attacks. What bout targetted attacks?   The presentation of this section is also not very clear, e.g. the equation on page 6. \n\nThe citation style look odd to me, often you use either something like '[2,3]', or something like 'Dhillon et al. (2018)', but not '(2,3)'. In addition, the equations should be number for the convenience of references.\n\n'... in our code \\superscript {1}' - the links all point to other people's code, not your codes.\n\nThe notation for the output p is different from the notation in page 2 where you used y. Try to use consistent notations. \n\nRegarding the novelty of this paper, I was based on my judgement and experience of reading a few papers, not I never published papers on adversary attacks or defence. ", "belong_id": "S1lDkaEFwS"}, {"uid": "SkglhWCXcB", "paper_title": "Thwarting finite difference adversarial attacks with output randomization", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to introduce randomness in a classifiers predictions to mitigate black-box attacks that rely on gradient estimation through finite differences. The intuition behind the defense is correct: finite differences rely on the outputs of the neural network being non-deterministic and accurate to estimate gradients near the test points being attacked.\n\nHowever, the threat model chosen in this paper is not well justified: the adversary cannot be forced to use a particular strategy. Unlike what is suggested in Section 3, estimating gradients through finite differences is not the only strategy available in the black-box threat model (this is later mentioned in Section 6). In this case, the adversary could for instance decide to adapt by instead mounting a black-box attack that relies on transferability. Because Figure 2 shows that the defense does not provide robustness in the white-box setting, this suggests that other forms of black-box attacks that either (a) rely on transferability or (b) are label-based only would still evade the model. This limitation should be addressed to understand how applicable the defense strategy is in a realistic deployment.\n\nPutting this aside, it is not clear from Figure 3 that an adaptive strategy was evaluated in the limited black-box setting that is considered here (the caption of Figure 3.b only describes a white-box adaptive adversary), or that the defense is effective. The attack success rates are high for many graphs and increase as the adversary averages over more runs. Moving forward, increasing further the highest number of runs would help appreciate the limitations of the approach: it is currently set to at most 100, which is low. \n\nAs far as organization is concerned, a lot of real estate is spent on background material, and few experimental results are presented to support claims made in the introduction. Addressing the above comments would probably require compressing background material a bit. \n\nPage-by-page details:\n\n1/ An attack is always adversarial by definition, adversarial attack is a tautology. \n\n2 / What do you mean by successful attacks?\n\n2/ What do you mean by strongest loss?\n\n2/ Having a perturbation limited to be small does not guarantee it wont impact the semantics of the input, even in the vision domain. Have you verified that the perturbations that you chose left semantics unperturbed?\n\n2/ It is best to avoid making broad statements such as We use the l2 perturbation penalty as this type of attack results in the strongest attacks because they are unlikely to hold across datasets and models.\n\n7/ Figures are difficult to parse (e.g., does T and U stand for targeted and untargeted?)\n\n7/ Distillation was already shown to be vulnerable to black-box attacks in [7]\n\n8/ Gradient masking was introduced in [7] prior to [25].\n\n8/ It would be good to justify the following statement (see my comment above): Although this is a valid attack vector for even black box models, we do not consider this type of attack in this work", "belong_id": "S1lDkaEFwS"}, {"uid": "SkxkhdKrqS", "paper_title": "Thwarting finite difference adversarial attacks with output randomization", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes applying randomization to the output layer of a DNN to defend against query-based attacks based on finite difference estimates. Then some theoretical analysis is provided, showing that with perturbation of a suitable scale, the randomization layer will not affect the accuracy of the model, while causing a large estimation error of finite difference methods that prevents finite-difference based attacks. Empirical results verify that the proposed defense is still effective against adaptive attacks where the randomness is averaged.\n\nPros:\n\nThe proposed method is simple, straightforward, yet novel. Its working mechanism is easy to understand and analyze, so it should be useful against finite-difference based attacks.\n\nLimitation:\n\nThe proposed method is not useful to defense against white-box attacks and transfer-based attacks, since basically it does not change the predictive model. Some other randomization methods like [26], by contrast, change the predictive model, hence they may be useful against white-box attacks and transfer-based attacks.\n\nQuestions and suggestions:\n\nThis part is my main concern.\n\nIt seems that the experimental results are very good. For example, in Figure 3a, the defense is effective even if \\sigma^2<1e-6. However, by the analysis in Section 4.2 (the formula below Line 3, Page 6), when \\sigma^2=1e-6, |E[g_i-\\gamma_i]| should be rather small, hence it should not block finite-difference based attacks. I think more explanation is needed for the good performance in the experiments.\n\nFinite differences are extremely sensitive to small random perturbation of the function value when the spacing (step size) h is small. For example, g_i=\\frac{L(f(x+he_i))-L(f(x-he_i))}{2h}, when h is very small, f(x+he_i) and f(x-he_i) is very close, hence adding perturbation to them will change g_i a lot. To present stronger adaptive attacks to output randomization, my suggestion is that a larger h can be adopted. It will be better if the results are investigated against attacks with different values of h.\n\nA mistake:\n\nIn Section 4.1 on Page 4: 'we can express the probability that x is misclassified in the vector d(p) as: \\sum_{i=2}^C P(d(p_i)>d(p_m))'. I think this is wrong, since P(A or B happens)=P(A happens)+P(B happens) only when A and B are mutually exclusive. However, 'd(p_i)>d(p_m)' and 'd(p_j)>d(p_m)' are not mutually exclusive. Hence, the probability that x is misclassified in the vector should be less than or equal to that sum of probabilities.\n\nBy the way, the writing in Section 4.1 is not clear:\n\n- The overall misclassification probability is presented first, but after that K only represents the misclassification probability into a specific class. The connection between them is unclear.\n- At the beginning of Section 4.1, the distribution of \\epsilon is \\epsilon\\sim\\mathcal{N}(\\mu,\\sigma^2\\cdot\\mathbf{I}_C): a unique \\sigma is used. But after that, the variance of \\epsilon_i becomes \\sigma_i^2 instead of \\sigma^2.\n- In the second to the last line on Page 4, 'level of noise (\\sigma^2) can be set for each class separately', but the authors did not explain how to set them, and in the experiments \\sigma^2 is set as the same scalar.\n- In Figure 1b, the line style of 'K=5.0e-3' and 'K=1.0e-1' in the legend is very similar. The line style of 'K=2.0e-01' in the legend is not clear: I do not know whether it refers to '-.-.-.' or '------'.\n\nTypos:\n\nSection 6, Page 8: 'unintentionlly' => 'unintentionally'\nSome missing spaces after punctuation:\n- Section 4.2, Page 5, '... gradient estimate.When the ...' => should add a space before 'When'\n- Section 5, Page 7, 'In addition,input randomization ...' => should add a space before 'input'", "belong_id": "S1lDkaEFwS"}, {"uid": "rJglmTn5tS", "paper_title": "Selective sampling for accelerating  training of deep neural networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "### Summary of contributions\n\nThis paper aims to accelerate the training of deep networks using a selective sampling. \nThey adapt ideas from active learning (which use some form of uncertainty estimation about the class of the label) to selectively choose samples on which to perform the backward pass. Specifically, they use the minimal margin score (MMS). \nTheir algorithm works by computing the forward pass over a batch of size B (which is much larger than the regular batch of size b), compute the uncertainty measure for each sample, and only perform the backward pass over the b samples with the highest uncertainty. The motivation is that the backward pass is more expensive than the forward pass, and that by only performing this pass on a subset of samples, computations are saved. \n\n\n### Recommendation\n\nReject. The central premise of the paper is unclear, the writing/presentation needs improvement, and the experiments are not convincing. \n\n\n### Detailed comments/improvements: \n\n\nThere is a central premise of the paper that I don't understand: that the forward pass is much cheaper than the backward pass. \nThis is claimed in the intro by referring to charts that hardware manufacturers publish (but there are no references included), but I don't see why this should be the case. \nFor a linear network with weights W, the forward pass is given by the matrix-matrix product (rows of X are minibatch samples):\nY = XW^T\n\nand the backward pass is given by the two matrix-matrix products:\ndL/dX = dL/dY*dY/dX = dL/dY*W\ndL/dW = dL/dY*dY/dW = dL/dY*X^T\n\nSimilarly the two operations in the backward pass for convolutional layers are given by a convolution of the output gradients with the transposed weigtht kernels and the input image respectively. \n\nPoint being, I don't see why the backward pass should be more than 3x more expensive than the forward pass. A simple experiment in PyTorch confirms this: the code snippet pasted at the bottom shows that the backward pass takes only around 2.6x longer than the forward pass.  \n\nfprop: 0.009286s\nbprop: 0.0240s\nbprop/fprop: 2.5893x\n\nIn algorithm 1, it is assumed that b << B. For this to be effective the forward pass would have to be *much* faster than the backward pass for this method to yield an improvement in computation. Can the authors comment on where this justification comes from?\n\nI am unclear on what the purpose of Section 4.1 is. This shows that the MMS of the proposed method is lower than the other two, but this should be completely expected since that is exactly the quantity being minimized. \nThere are also several unsubstantiated claims: 'Lower MMS scores resemble a better...batch of samples', 'the batches selected by our method provide a higher value for the training procedure vs. the HNM samples.', 'Evidently, the mean MMS provides a clearer perspective...and usefulness of the selected samples'. What does higher value, usefulness, clearer perspective mean?\n\nMore generally, it is unclear if there is really any improvement in the final performance from using the proposed method.\nIn Figure 2, all methods seem to have similar final performance. \nIn Figure 5, is there a reason why the curve for MMS is cut off? How does its final performance compare to that of the baseline method in red? It looks like the baseline might be better, but it's hard to tell from the figure. \n\nWhy are the experiments with the entropy measure in a seperate section? Please include them along with the other methods in the same plot, i.e merge Figure 2 and Figure 4. \n\nMy suggestions for improving the experimental section are as follows:\n- include all methods together in all the plots/tables\n- repeat experiments multiple times with different seeds to get error bars. Include these both in the learning curves and in the tables. \n- It's hard to see small differences in the learning curves, so including tables as well is important. Include best performance for all the methods in the tables. \n\nFinally, in 2019 CIFAR alone is not longer a sufficient dataset to report experiments on. Please report results on ImageNet as well. \n\nOne of the central premises of the paper is acceleration in terms of compute/time. To make this point, there should also be results in terms of walltime and floating-point operations. Please include these results in the paper.  \n    \n\n\n\n### Code snippet timing forward/backward passes\n\n\nimport torch, torch.nn as nn, time\n\nmodel =\tnn.Sequential(nn.Linear(784, 1000),\n                      nn.ReLU(),\n                      nn.Linear(1000, 1000),\n                      nn.ReLU(),\n                      nn.Linear(1000, 10),\n                      nn.LogSoftmax())\n\ndata = torch.randn(128, 784)\nlabels = torch.ones(128).long()\nt = time.time()\npred = model.forward(data)\nloss = nn.functional.nll_loss(pred, labels)\nfprop_time = time.time() - t\nt = time.time()\nloss.backward()\nbprop_time = time.time() - t\nprint('fprop: {:.4}s'.format(fprop_time))\nprint('bprop: {:.4f}s'.format(bprop_time))\nprint('bprop/fprop: {:.4f}x'.format(bprop_time / fprop_time))\n", "belong_id": "SJxNzgSKvH"}, {"uid": "BJeWmNn3KS", "paper_title": "Selective sampling for accelerating  training of deep neural networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a minimal margin score (MMS) criterion to speed up the training of the deep networks.\n\nI would vote for a clear rejection of this paper. This submission is a clearly unfinished one. The two biggest problems are as follows\n\n1. Lack of a comprehensive discussion on rules for sampling section, please see 'Automated Curriculum Learning for Neural Networks'. Why previous methods are worse than the proposed one is not clear.\n\n2. All experiments are only compared with baseline approaches. In some experiments, the improvements are really marginal (e.g., Figure 2). In these cases, the STD of these curves is not shown, it is not clear whether the improvements are significant or not.", "belong_id": "SJxNzgSKvH"}, {"uid": "HkgDJXI-5B", "paper_title": "Selective sampling for accelerating  training of deep neural networks", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "A new approach is proposed to speed up training in deep models.\n\nThe idea is to select sample batches when back propagating the error based on the distance of the prediction foe the sample from the decision boundary. Specifically, we pick points closer to the boundary, i.e., ones that we are less confident about for backpropagation.\n\nExperiments are performed comparing the method with Hard negative sampling (HNM) , entropy-based sample selection as well as regular training. Experiments are performed on Cifar10 and Cifar100 datasets. \nWhy only two datasets, the method is general so there should be more datasets to verify its performance.\n\nThe results on Cifar100 in Fig 5 c seems to show that we cannot reach the training accuracy using the proposed method as compared to the other methods. What is the intuition here as to why it happens? In general though since the main goal is to speed up training I do not see very convincing evidence of this in the limited evaluation which seems to be the main weakness here.", "belong_id": "SJxNzgSKvH"}, {"uid": "BJgwdLAaKr", "paper_title": "Removing the Representation Error of GAN Image Priors Using the Deep Decoder", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "[Update after rebuttal period]\nI am sorry that the response cannot address my confusion. I still doubt the motivation of this paper and the actual experimental performance compared with state-of-the-art methods are still ignored. Thus I decrease my score.\n\n[Original reviews]\nThis paper proposed to modeling image as the combination of a GAN with a Deep Decoder, to remove the representation error of a GAN when used as a prior in inverse problems. The proposed methods are evaluated on two image restoration tasks, including compressive sensing and image super-resolution. The effectiveness of the combination is also presented.\n\nAuthors devote themselves to remove the representation error of the GAN image prior. Intuitively, the manner of the proposed linear combination model is rough and less reasonable. In Alg.1, the detailed algorithmic process is presented, it is clear that authors need to pre-train the used GAN and Deep Decoder, then combine them to train one network. If the motivation of this paper is to remove the representation error of GAN, GAN should be viewed as the main body. However, the authors view GAN and Deep Decoder as the same position against the original intention.\n\nAdditionally, in the experimental part, the ablation studies indeed reflect the effectiveness of the proposed algorithm, but it looks like the Deep Decoder plays a key role in all cases. In other words, the GAN image prior just plays a supporting role. This is also far away from motivation.\n\nMore importantly, I cannot see the surprising results because of this work only compare themselves with some basic version or naive methods. All state-of-the-art approaches to different tasks are ignored, which is the other big disadvantage. \n\nIn Table 2, what is the meaning of CSGM? The authors should describe it.\n\nIn a word, from the algorithmic and experimental perspective, this paper cannot achieve satisfying performance.\n", "belong_id": "rkegcC4YvS"}, {"uid": "SJlxZVMlqr", "paper_title": "Removing the Representation Error of GAN Image Priors Using the Deep Decoder", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: This paper proposes to use a combination of a pretrained GAN and an untrained deep decoder as the image prior for image restoration problem. The combined model jointly infers the latent code for the trained GAN and the parameters in the untrained deep decoder. It also jointly infers the mixing coefficient alpha and beta during test time for each image, thus learning how much we should rely on GAN. The proposed hybrid model is helpful on compressed sensing experiments on the CelebA dataset; however, it is only marginally better than deep decoder on image super resolution and out-of-distribution compressed sensing.\n\nDetailed comments:\n-\tThe writing is clear and I was able to understand the model part of the paper. The algorithm box is helpful. However, I would still appreciate if the authors can provide an overall model figure in the model section to help understanding. \n-\tJointly learning the mixing coefficient is an interesting part of the model.\n-\tThe motivation in the abstract and intro could be strengthened. A smaller version of Figure 1 can be probably moved to the beginning of the paper to illustrate the problem of GAN. But even with the help of Figure 1, it is still unclear what is the fundamental problem for GAN. Simply combining a GAN with an untrained decoder model doesnt help elucidate the source of the problem. \n-\tThe proposed Hybrid model seems to help on compressed sensing experiments on CelebA. However, it doesnt help much on out-of-distribution experiments. Moreover, in the super-resolution task, as shown in Figure 5, the improvement over deep decoder is also not significant.\n-\tThe out-of-distribution experiments seems lack of thorough study. In particular, the paper only studies the transfer between CelebA -> Caltech-UCSD Bird dataset. It would be better if the paper can study a variety of other image datasets as well. Also some visualization on the Bird dataset should also be included.\n-\tEffect of n_pre needs to be further investigated. Why not directly train both models together? It would be good if the authors could comment on how sensitive the n_pre is and what is the intuition.\n-\tFor figures, I would recommend rename Hybrid to Hybrid (Ours) to highlight the papers contribution, and use a brighter color.\n-\tFigure 5 should be renamed as a Table. \n-\tHyperparameter details should be moved to the Experiment section.\n\nConclusion:\nThe paper proposes a simple combination of a trained GAN and an untrained decoder model for the task of image restoration. Although the method is clear and straightforward, in the experiments, the influence of the new model component seems marginal. Moreover, the motivation is not strong enough. Therefore, I recommend weak reject.", "belong_id": "rkegcC4YvS"}, {"uid": "rkxEiTqbqB", "paper_title": "Removing the Representation Error of GAN Image Priors Using the Deep Decoder", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method for reducing the representation error generative convolutional neural networks by combining them with untrained deep decoder. The method is evaluated on compressive sensing and super-resolution, where a better performance than the isolated use of Deep Decoders and GAN priors. The main contribution of the paper is not the performance, but the simplicity of this approach.\n\n\n\nFor the title, I would suggest to replace the word Removing with Reducing.\nFurthermore, the clarification of 'GAN prior' is very nice in the introduction, maybe you could already clarify it in the abstract.\n\nYou should perform a critical grammar check. There are too many commas, for example:\n'At sufficiently difficult superresolution problems, the Hybrid model outperforms, the Deep\nDecoder, Bicubic upsampling, the BEGAN prior, and the BEGAN as DIP prior.' -> there should be no comma after 'outperforms'\nThe sentence from Page 3 to 4 reads strangely, probably a word is missing after 'For our GAN prior, we use the BEGAN architecture, and we demonstrate similar results'\n'Philosophically, they hybrid' -> 'Philosophically, the hybrid'\n\nFig 6 caption - shouldn't it be 49152 instead of 49512?\n\nYou perform various very good analysis experiments, which is well appreciated. Still, it would be good to think about some more experiments (and include at least one of them in the paper):\n1. You compare to IGAN and show that you achieve similar performance. You describe that a state-of-the-art approach are invertible generative models and that they are very time consuming (e.g., 15 minutes for a 64x64 image). How good would the invertible models be in terms of performance? Could you perform tests as well?\n2. It would be great if you report the runtime of all experiments as well - maybe also the memory usage.", "belong_id": "rkegcC4YvS"}, {"uid": "H1gCgZOrKB", "paper_title": "BasisVAE: Orthogonal Latent Space for Deep Disentangled Representation", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes BasisVAE for acquiring a disentangled representation of VAE. \nThough the topic is much of interest for this conference, I cannot support its acceptance because the paper leaves many aspects unexplained in the model design. \n\nIn particular, the following points need justified and clarified.\n1) Theorem 1 is difficult to follow. \nThe claim of the theorem is unclear. \nI suppose it says ELBO can be written as a sum with respect to z_i given p(z)=\\prod_i p(z_i), but the statement is not clear enough from the text. \nProof of Lemma 1 is logically incomplete. Discuss the cases n>2.\nDerivation of equation (6) from (5) seems erroneous: p(x|z_1, ..., z_n) = \\prod_{i=1}^n p(x|z_i) / p^{n-1}(x) does not hold in general even if z_i's are independent p(z_1, ..., z_n)=\\prod_{i=1}^n p(z_i).\n\n2) Connection between the objective function and Theorem 1 is unclear. \nBasisVAE uses a linear combination of Eqs. (9,10,11) as its objective function. \nHow Theorem 1 motivates this formulation?\n\n3) Reconstruction error (9). \nThe text says \\ell of Eq. (9) is the binary function and configured as in (Bojanowski et al. 2017). \nHowever, Bojanowski et al. used a weighted l1 error Laplacian Pyramid representation. \nFurthermore, the original VAE formulation uses a conditional log-likelihood log p(x|z) for the reconstruction term. \nHow is binary function \\ell related the likelihood?\n\n4) KL regularization term (10).\nFor computing this term, the output of encoder c=f(x) should be converted into z. \nNotation of N(f(x), \\Sigma) is confusing. \n\n5) Figure 6 shows diversity in many factors. \nFigure 6 is not as impressive for disentangled images since many factors change by varying a single basis. \nIs this an expected result?", "belong_id": "S1gEFkrtvH"}, {"uid": "B1xdl-vCFB", "paper_title": "BasisVAE: Orthogonal Latent Space for Deep Disentangled Representation", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "[updated rating due to supervision of $c_i$, which was not made clear enough and would require other baseline models]\n\nThis paper proposes a modification of the usual parameterization of the encoder in VAEs, to more allow representing an embedding $z$ through an explicit basis $M_B$, which will be pushed to be orthogonal (and hence could correspond to a fully factorised disentangled representation). It is however possible for different samples $x$ to use different dimensions in the basis if that is beneficial (i.e. x is mapped to $z = f(x) \\cdot M_B$, where f(x) = (c_1, ... , c_n) which sums to 1.). This stretches the usual definition of what a disentangled representation means, as this disentanglement is usually assumed to be globally consistent, but this is a fair extension.\nThey show that this formulation can be expressed as a different ELBO which can be maximized as for usual VAEs.\n\nI found this paper interesting, but I have one clarification that may modify my assessment quite strongly (hence I am tentatively putting it on the accept side). Some implementation details seem missing as well. Otherwise the presentation is fair, there are several results on different datasets which demonstrate the model's behaviour appropriately.\n\n1.\tThe main question I have, which may be rather trivial, is are the c_i supervised in any way?.When I first read the paper, and looking at the losses in equations 9-11, I thought that this wasnt the case (also considering this paper is about unsupervised representation learning), but some sentences and figures make this quite unclear:\n\ta.\tIn Section 3.2, you say We train the encoder so that c_i = 1 and c_j = 0 if the input data has i-feature and no j-feature. Do you?\n\tb.\tHow are the features in Figure 6 attached to each b_i?I.e. how was 5_o_clock_shadow attached to that particular image at the top-left?\n\tIf the c_i are supervised, this paper is about a completely different type of generative modeling than what it compares against (it would be more comparable to VQ-VAE or other nearest-neighbor conditional density models).\n2.\tThere is not enough details about the architecture, hyperparameter and baselines in the current version of the paper.\n\ta.\tWhat n_x (i.e. dimensionality of the basis) do you use? How does this affect the results?\n\tb.\tHow exactly are f(x), \\Sigma_f(x) parametrized? They mention the architecture of the encoder in Section 4.1, but this could be much clearer.\n\tc.\tHow do you train M_B? I assume they are just a fixed set of embeddings that are back-propagated through?\n\td.\tWhat are the details about the architecture of the baselines, and their hyperparameters? E.g. what is the beta you used for Beta-VAE?\n3.\tThe reconstructions seem only partially related to their target inputs (e.g. see Figure 4). This seems to indicate that instead of really reconstructing x, the model chooses to reconstruct a close-by related \\tilde{x}, or even perhaps a b_i. This would make it behave closer to VQ-VAE, which explicitly does that. How related are reconstructions/samples to the b_i?\n4.\tCould you show the distribution of c_i that the model learns, and how much they vary for several example images? How peaky is this distribution for a given image (this feeds into to the previous question as well)?The promise of the proposed model is that different images pick and choose different combinations of b_i, which hopefully one should see reflected in the distributions of c_i per sample, across clusters, or across the whole dataset.\n5.\tWhat happens when L_B is removed? I.e. what is the effect of removing the constraint on M_B being a basis, and instead allow it to be anything? This seems to make it closer to a continuous approximation to VQ-VAE?\n6.\tIs Equation 10 correct? Should the KL use N(f(x) \\cdot M_B, \\Sigma_f(x)), as in equation 9 above?\n7.\tSimilarly, in Section 4.2.3, did you mean c_i = 1 and c_j = 0 for i != j?\n\nIf the model happens to be fully unsupervised, I think that these results are quite interesting, and provide a good modification to the usual VAE framework, I find that having access to the M_B basis explicitly could be very valuable.\n\nThere is still an interesting philosophical discussion to be had about when one would like to obtain a global basis for the latent space (i.e. Figure 3 (b)), or when one would prefer more local ones. I can see clear advantages for a non-local basis, in terms of generalisation and compositionality, which your choice (i.e. Figure 3 (c) ) would prohibit.\n\nReferences:\n[1] VQ-VAE: Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu, Neural Discrete Representation Learning, https://arxiv.org/abs/1711.00937", "belong_id": "S1gEFkrtvH"}, {"uid": "HkgnZixS9S", "paper_title": "BasisVAE: Orthogonal Latent Space for Deep Disentangled Representation", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThis paper claims to achieve disentanglement by encouraging an orthogonal latent space.\n\nDecision: Reject. I found the paper difficult to read and the theoretical claims problematic. \n\nIssue 1: The Theorem\nCan the authors explain how they got from Eq 5 to Eq 6? It seems that the authors claim that:\np(x | z1 z2 ... zn) = p(x | z1) ... p(x | zn) / p(x)**(n - 1)\nI have difficulty understanding why this is true. It would suggest that\np(x | a b) = p(x | a) p(x | b) / p(x). \nSuppose a and b are fair coin flips and x = a XOR b. Then\np(x=1 | a=1 b=1) = 0\np(x=1 | a=1) = 0.5\np(x=1 | b=1) = 0.5\np(x=1) = 0.5\nCan the authors please address this issue?\n\nEven if Equation 8 is somehow correct, can the authors explain why BasisVAE provably maximizes the RHS expression in Eq 8? In particular the object p(x | z_i) is the integral of p(x, z_not_i | z_i) d z_not_i, which is quite non-trivial. \n\nIssue 2: The Model\nThe notation is a bit confusing, but it looks like the proposed model is basically a standard VAE, but where the last layer of the mean-encoder is an orthogonal matrix. I do not think the authors provided a sufficient justification for how this model relates back to Theorem 1. \n\nFurthermore, it is unclear to me why an orthogonal last-layer is of any significance theoretically. Suppose f is a highly expressive encoder. Let f(x) = M.T g(x) where g is itself a highly expressive neural network. Then M f(x) = g(x), which reduces to training a beta-VAE (if using Eq 12). From a theoretical standpoint, it is difficult to assess what last-layer orthogonality is really contributing.\n\nIssue 3: The Experiments\nExperimentally, the main question is whether the authors convincingly demonstrate that BasisVAE achieves better disentanglement (independent of whether BasisVAE is theoretically well-understood). \n\nThe only experiment that explicitly compares BasisVAE with previous models is Table 3. What strikes me as curious about the table is the standard deviation results. They are surprisingly small. Did the authors do multiple runs for each model? Furthermore, the classification result is not equivalent to measuring disentanglement. There exists examples of perfectly entangled representation spaces can still achieve perfect performance on the classification task (any rotation applied to the space is enough to break disentanglement if disentanglement is defined as each dimension corresponding to a single factor of variation).", "belong_id": "S1gEFkrtvH"}, {"uid": "S1gemwT6YH", "paper_title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "[Update after rebuttal period]\nIn response, the authors cannot clearly clarify the difference between this work with existing works integrating the physical model into the network. Thus I stay my original score.\n\n\n[Original reviews]\nThis paper proposed an unsupervised generative adversarial network for underwater generating realistic underwater images and haze removal, which can simultaneously deal with the color restoration and haze in the realistic underwater environment.\n\nFirstly, according to the widely used physical model in the image processing area, employed the UnderwaterGAN to trained parameters in advanced, and then use U-Net for color restoration and haze removal of underwater images. However, many existing works used the physical model to represent the imaging principles and using deep network to learn prior knowledge. Thus, I think the proposed idea is a little bit incremental.\n\nFor the experimental part, the experimental results fully demonstrate the effectiveness of the proposed method in comparison with state-of-the-art methods. Additionally, the ablation studies in the appendix also give us the intuition by using the different loss functions. Also, I suggest the authors demonstrate the proposed method on not only low-level, but also high-level vision tasks, e.g., underwater image target detection. \n\nFinally, the paper is well organized and sentence expression is also clear, but small errors that are correctable. \n", "belong_id": "HkgMxkHtPH"}, {"uid": "SyeZlLA6Yr", "paper_title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper uses U-net for underwater image restoration and enhancement. But, it is difficult to obtain realistic underwater images, thus this paper introduces a GAN-based method to generate realistic underwater images from in-air image and depth map pairs.\n\n- Although this paper points out that the previous work (i.e. WaterGAN) generates color noise and the camera model is not suitable, how does this proposed method overcome these points? Please make it clear.\n\n- The figures in this paper are too blurry to see them. To evaluate the effectiveness of the proposed method, the figures are important, thus, it would be better to make them clear.\n\n- The technical contribution of the proposed method is not clear. The proposed method seems to be just using the existing techniques.", "belong_id": "HkgMxkHtPH"}, {"uid": "Skl-8kPxjr", "paper_title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this article, the authors propose a generative adversarial network named UWGAN to generate realistic underwater images from the pairs of in-air images and depth images. Then, a U-Net was leveraged to enhance the results. \nHowever, the text suffers from too many language problems. The authors should consult professional proofreading services. As a courtesy towards referees, the quality of writing needs meticulous attention before a scientific paper should be submitted. \n\tOther comments:\n1.\tThe literature is limited. I found some novel works being done in the field that must be addressed and listed in the background and experiments.\n2.\tThe underwater imaging model presented in this paper derives from the Jaffe-McGlamery model, which is a common sense in this field. The authors use a generator to produce underwater images that only implements the common model by a neural network. Moreover, the statement of section 2.2 is not clear. Please rewrite this section.\n3.\tThe authors used U-Net without any improvement to enhance the results generated from UWGAN, which is the integration of existing models. \n4.\tThe authors claimed that their model is better than others, while there is no evidence to indicates that. For example, 1) in (page 5, line 4 from bottom), It can be seen that our proposed method has achieved a higher score., can we observe this from the Table 1 and 2? 2) The method we proposed has the fastest processing speed compared to other methods. Moreover, the method proposed in this paper has the fewest parameters compared to other deep-learning-based methods., it is suggested that a study about the parameters and FLOPs of the involved methods should be given.\n5.\tPlease carefully check the references. For example, Hummel R. Image enhancement by histogram transformation[J]. Unknown, 1975. lacks the journal name.\n6.\tHigh-resolution figures should be given in the manuscript.\n", "belong_id": "HkgMxkHtPH"}, {"uid": "Bye56P4fFH", "paper_title": "Hindsight Trust Region Policy Optimization", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an extension to TRPO that makes use of hindsight experience replay.  The technique follows previous work on HER for policy gradient methods, augmenting these previous approaches with a constraint based on the average squared difference of log-probabilities. Experiments show that the method can provide significant improvements over previous work.\n\nOverall, the paper provides a convincing demonstration of trust region principles to goal-conditioned HER learning, although I think the paper could be improved in the following ways:\n-- Bounding KL by the squared difference of log-probabilities seems loose.  The original TRPO objective is based on a TV-divergence (and before that, based on a state-occupancy divergence). Is it possible to directly bound the TV-divergence (either of actions or state occupancies) by a squared difference of log-probabilities? \n-- The use of WIS greatly biases the objective. Is there a way to quantify or motivate this bias?\n-- What is the method in which the alternative goal g' is proposed? I believe this can have a great effect on the bias and variance of the proposed method.", "belong_id": "rylCP6NFDB"}, {"uid": "rklIz1_4tH", "paper_title": "Hindsight Trust Region Policy Optimization", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "*Summary* : This paper augments the TRPO policy optimization objective with hindsight data, where the hindsight data is generated from goals based on trajectories. The key contribution of the paper is based on deriving an on-policy adaptation of hindsight based TRPO, that can be useful for sparse reward environments. The paper draws ideas from existing papers such as HPG and considers the IS based variant of HPG for on-policy, similar to TRPO, that can achieve monotonic performance improvements. Furthermore, the authors introduce a logarithmic form of constraint, by re-deriving the KL constraint and leading to a f-divergence based constraint, which is argued to have useful effects in terms of lowering the variance. Experimental results are compared with baselines including HPG and its variants on standard sparse reward benchmark tasks. \n\n*Comments* : \n\n\t- The core contribution of the paper is to introduce hindsight based TRPO where end states from the trajectory treated as goals can be useful for generating pseudo-rewards, such that existing TRPO based approaches can be better suited for sparse reward environments. The claim is that existing policy gradient methods cannot sufficiently explore in sparse reward environments. \n\t- Since incorporating hindsight data can make the approach off-policy in nature, leading to higher variance and instability, the authors propose to augment hindsight approach into on-policy based methods such as TRPO. The key contribution is to develop a theoretically sound approach for hindsight based policy optimization. \n\t- The hindsight TRPO objective is derived based on goal-conditioned policies with TRPO, based on existing recent work on Hindsight Policy Gradient (HPG) (Rauber et al., 2019). The key difficulty that needs to be tackled is when introducing hindsight experience makes the approach off-policy in nature. The authors derive the HTRPO objective based on an importance sampling approach that can also incorporate the hindsight data. \n\t- Equation 7 writes out the key quantity, an IS based approach considering the current goal and alternative goals to derive a similar TRPO objective based on IS based Monte-Carlo estimators, while maintaining the trust region guarantees. Equation 8 further shows the gradient of the objective. Theorem 3.1 and 3.2 follows naturally where the key trick in Thm 3.1 is going from equation 19 to 20 to derive the IS based estimator with goal conditioned trajectories. I am not sure why Thm 3.2 needs to be written out explicitly, given that it follows naturally for gradient of the expectation? Is there any key takeaway from it? \n\t- In TRPO, the expectation is based on states sampled from a state distribution. The authors argue that for hindsight based data, this state distribution in fact can change due to changes in the generated goals (ending states), and hence the KL needs to be in expectation w.r.t to the state occupancy measure. Furthermore, the authors change the KL based constraint into a logarithmic form of a constraint such as to reduce variance and introduce more stability. To achieve this, the paper uses an approximation to the logarithmic form of the constraint, by using an expectation of the square instead of plain differences between the log of policies. The key is that instead of using the KL divergence, the authors introduce f-divergence where the function is convex allowing smooth optimization. \n\t- The overall contribution of the paper can be summarized from equation 15 - introducing a IS based correction, while remaining on-policy for hindsight based TRPO objective. And since hindsight can change the underlying state distribution, leading to more instability, the paper introduces a different form of constraint (based on the f-divergence) which can have lower variance than the KL form of constraint. \n\t- Experimental results are demonstrated for few sparse reward benchmarks, comparing to the standard HPG, TRPO and several variants of the proposed HTRPO approach with weighted IS and with the KL constraint. The advantages of HTRPO on these tasks seems clear, mainly in the Fetch Reach and Fetch Push tasks, significantly outperforming the baselines. Even in the continuous tasks, HTRPO seems to outperform the baselines consistently. \n\n*Feedback/Questions* : \n\n\t- I am not sure of the significance of Theorem 3.2 - it seems obvious that the gradient of the objective spans out naturally from equation 7? \n\t- The authors mention about the state occupancy measure instead of considering the state distribution for the KL term. However, the discussion of state occupancy measure seemed to have faded away? What was the significance of mentioning it, or why should it be considered even? There are no assumptions being made on whether the state distribution should be the discounted state occupancy measure or the stationary distribution (if infinite horizon is considered). \n\t- The introduction of the logarithmic form of constraint, even though shows theoretically to reduce variance, is not well motivated or demonstrated from the suite of experimental results? From the results, it is not obvious whether this form of constraint is indeed having useful effects in terms of reduced variance? \n\t- The paper seems to adapt from the HPG objective, and does indeed a great job comparing to HPG throughout the suite of experiments. However, in the results, the paper mainly compares to other off-policy based methods including HPG and its variants (official and re-implemented one). I find the comparison of results a bit odd in that sense, since it is comparing the on-policy adaptation of HPG (ie, HTRPO) and the off-policy variants? If run for long enough, does all converge to the same results? If so, then the benefits is mainly in faster learning (assumably due to better exploration in the sparse reward tasks). But then again, these benefits may be because of the on-policy approach compared to the off-policy one? \n\t- I would encourage the authors to compare to more standard goal-conditioned or reward shaping based baselines for TRPO. For example, does the proposed HTRPO approach perform better compared to other goal-conditioned approaches of TRPO, or for example if a form of reward shaping (based on goals) are used in TRPO? It would then be a more likewise comparison? The current results seem to show benefits of HTRPO, but I think there is a need for stronger baselines where TRPO + exploraton (reward shaping or goal conditioning) performs better?\n\t- I am not convinced about the arguments with sensitivity of HTRPO with network architecture and parameters. How is this demonstrated from the suite of results?\n\u000b\u000b\n*Summary of Review and Score* : \n\nOverall, I think the paper has merits, mainly in terms of deriving the on-policy adaptation with hindsight data. The key objectives are derived nicely in the write-up and easy to follow, although there are some confusions that need to be clarified (example : the discussion on state occupancy measure and the significance of it). The paper motivates exploration for TRPO in sparse reward tasks, and considers the hindsight adaptation of existing TRPO. But related works such as HPG have already taken a similar approach for the off-policy case, and this paper's key contribution is in terms of theoretically deriving the objectives for on-policy adaptation. However, I am not convinced about the overall merits and contributions of the paper, especially in terms of demonstrated results and proper comparisons with baselines. I think while the objectives and derivations follow naturally, the contributions of the paper is somewhat marginal. \n\nI would therefore vote to marginally reject this paper - mainly in light of the core novel contribution and due to lack of sufficient results demonstrating the usefulness of the approach. The paper combines several things together - especially discussions of the logarithmic form of the constraint. I doubt whether all these introduced together led to the improvements shown in the experimental results or not. It would be useful to clarify the contributions from each of the components. \n\n", "belong_id": "rylCP6NFDB"}, {"uid": "B1xQBe0E5r", "paper_title": "Hindsight Trust Region Policy Optimization", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThe paper builds on top of prior work in hindsight policy gradients (Rauber et.al.) and trust region policy optimization (Schulman et.al.), proposing a hindsight trust region policy optimization. Conceptually this direction makes a lot of sense, since hindsight is in general shown to be useful when training goal conditioned policies. The formulation generally  appears to be sound and is a straightforward extension of the importance sampling techniques from Rabuer et.al. to the TRPO setting. Experimental results show that the proposed changes bring significant improvements over baselines on sparse reward settings.\n\nStrengths:\n+ The paper appears well formulated, and well motivated\n+ The experimental results appear quite strong.\n+ Description of the experimental details is quite clear.\n\nWeaknesses:\n- Most of the weaknesses that I can find are in terms of the presentation and writing which could be improved. Specifically, it would be good to clarify when writing the HTRPO equation (Eqn. 7) that this is still a constrained optimization problem with a KL divergence between polcies. Further, it should be better clarified and justified which KL divergence the constraint should be between, since there are two choices \\pi(a| s, g) or \\pi(a| s, g). \n- It would be make the results section flow much better if the paper were to adopt g as the original goal, and g as the alternative goal (which makes a much better flow from the non-hindsight case to the hindsight case).\n- It seems a bit wasteful to mention Theorem 4.1 as a theorem, since it does not feel like a major result and is a straightforward monte carlo estimation of the KL divergence. \n- Missing baseline: it would be nice to check if the method of estimating the KL divergence using difference of squares of log probs (Eqn. 12) improves TRPO (and a clarification on whether this is would be a novel application in the first place). This might be a result of independent interest outside of the hindsight context. \n", "belong_id": "rylCP6NFDB"}, {"uid": "H1xlCzUqtr", "paper_title": "A Simple Dynamic Learning Rate Tuning Algorithm For Automated Training of DNNs", "experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new way of scheduling the learning rate in optimization algorithms such as SGD. It is a stand-alone, parameter-free approach that optimistically doubles the learning rate at every loss improvement between two epochs, until the loss increases too much or diverges, in which case the learning rate is divided by two.\nThis approach is theoretically proven to converge and to follow an optimal scheduling strategy.\nIn addition, the authors experimentally tested their approach on two image classification tasks, showing that the proposed algorithm yields similar to baseline results.\n\nI am rejecting this paper because it seems to motivate things with non-related facts, experiments are not robust and thorough enough, and there is no conclusion (not even in the appendix).\n\n- The most important thing in this paper to me is the fact that 'adversarial training' is used to motivate this approach a lot. it is mentioned 14 times across the paper: 3 times in the abstract alone. Yet there is no explanation of what it is, and how is it different from 'natural training' as mentioned in the paper. I suggest the authors either to clearly explain the difference between the two and explain why their approach may help in one setting or the other; or to simply remove the mentions of 'adversarial training' if it is not important to the approach.\n- to better motivate the approach, I would suggest the authors include different tasks, rather than different training settings. For instance by having one image classification task (keep one of the two current ones) and one text classification or even generation task. This would show that the proposed approach generalizes well to other network architectures.\n\n- The second concern I have is about the experiments. If increasing the learning rate like the proposed approach is making training to convergence faster, then why are the experiments only measuring test set accuracy and not also runtime to convergence?\n- Overall, the experiments are not complete and thorough enough: some table values are missing, the set of adversarial training experiments on CIFAR100 are not reported, and some experiments diverged with the ADAM optimizer. Less than 20% accuracy on a 10-class image classification task seems very far from optimal.\n\n- Eventually, I strongly suggest the authors submit a better closing statement than 'We use cross entropy loss in all cases.' (especially after having read this same sentence earlier in section 5.1 of the paper). No conclusion is added to the paper, not even in the appendix.\n\nBelow are a few minor points not taken into account in the scoring but that could make the paper slightly better:\n- Section 1, paragraph #1, first sentence: a few citations here would be nice.\n- typo on the first line of page 3: '5The pseudocode ...'\n- typo in the 2sn paragraph of section 4: '... the convergence is the fast*est* since the step sizes ...'\n- typos in the first line of the second paragraph of section 4.1: 'Assuming that *the* loss surface is smooth, *the* loss will continue...'\n- page 6: 'At this time, p=2^{n-1} at this time.'\n- Section 5.1, paragraph 2, first sentence: '... with dropout and with both dropout and cutout, ...'\n", "belong_id": "rJxyqkSYDH"}, {"uid": "Hyx_lophKr", "paper_title": "A Simple Dynamic Learning Rate Tuning Algorithm For Automated Training of DNNs", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper considers the problem of automated adaptation of learning rate during (deep) neural network training. The use cases described are standard and adversarial training for image classification. Given the wide use of DNNs in computer vision (and other areas), learning rate tuning is clearly an important problem and is being actively researched.\n\nThe proposed learning rate adaptation procedure consists of a straightforward combination of learning rate halving/doubling and model checkpointing. Experimental results from implementing the adaptive learning rule for standard and adversarial training on CIFAR are provided. Multiple architectures are tested in each setting. The paper claims a primary advantage of the proposed learning rule to be that it requires no tuning as opposed to other rules such as SGD, Adam.\n\nMy decision is to reject the paper due to methodological issues with the experiments and lack of evidence wrt/ dataset variety. The paper should be considered a work-in-progress that may have potential in a more focused setting, e.g., adversarial training as described in the paper.\n\n***\n\nThe major claim of the proposed algorithm not requiring any manual tuning is technically true but misleading. The algorithm does have parameters (SGD momentum, batch size, initial learning rate, patience) with values that were set somehow. In fact, a major methodological issue with the experiments is that the reader does not know if the datasets were used to both set these values and to assess performance, i.e., there are no obvious 'held-out' datasets. Also, there is no rigorous or even informal justification of the settings. It could be that the paper is arguing that the specific values will result in competitive, if not better, performance than baselines across a variety of datasets - unfortunately, only two datasets are utilized in the experiments, and one, CIFAR10, is not considered challenging. This leads to the second issue with the paper: the experimental validation is not extensive wrt/ datasets which is significant given that the form of the evidence for the proposed method is almost entirely empirical.\n\nAdditionally, I don't agree that competitor algorithms should not be tuned b/c the proposed method does not require tuning. Even if the proposed method does not require tuning (as stated previously, I don't believe this to be accurate), that does not imply a fair comparison precludes tuning competitors via, e.g., cross validation. The only relevant quantities are final test-set performance and total training time/resources required. \n\nThe well-known interdependence between learning rate and batchsize as noted in e.g., Hoffer et al. (2018), is not addressed by the experiments. Batchsizes in the experiments vary, but no justification is provided for how these are selected.\n\nFinally, the paper is unfinished as some experimental runs were not complete at the time of submission.\n\nOn the positive side, the general point about the necessity of learning rate tuning for adversarial training (described in the fourth paragraph of the introduction) is a very good one, and there may be an opportunity for a more focused application of the proposed algorithm perhaps among further datasets and considering additional, alternative attacks.\n\n***\n\nSuggestions for improvement / questions (related to decision):\n\n* It should not be a challenge to find more image classification datasets to include in the experimental comparison: SVHN, Fashion MNIST, Imagenet, ... Using these, the paper can either follow the standard train/test methodology *across datasets*, i.e., split the meta-dataset into train/test, and/or provide a more compelling body of evidence for the proposed method. Also, the performance dependence on batchsize amongst the proposed algorithm and competitors should be investigated experimentally.\n\n* The Baydin et al. (2018) algorithm should be added to the set of competitors since it would provide a relatively easy* reference point wrt/ 'hypergradient' approaches. I don't agree with the statement in the related work section that this entails 'additional computation of gradients.' *In the sense that the rule should be straightforward to implement.\n\n* The convergence analysis assumption that the optimal oracle SGD follows typical learning rate regimes motivated by loss plateauing seems to be in direct contradiction to the sentiment expressed in the cited Hoffer et al. (2018) paper that such 'rules of thumb' may be misguided. Can the authors discuss the appropriateness of their assumption wrt/ this point? Also, in the convergence analysis, the phrase 'in expectation' is used twice. This has a specific probabilistic meaning, but appears to be used heuristically in this section. Can the authors clarify whether this usage is informal or formal? If the latter is true, it would be better to provide a more formal convergence argument that explicitly takes the inherent randomness into account.\n\n***\n\nEditorial comments (not related to decision):\n\n* Introduction: The first two sentences of the second paragraph, particularly the second, would do well to have an accompanying reference or references.\n\n* Proposed method: Even as an informal statement, the second sentence of the second paragraph under the Phase 2 sub-heading is problematic. The proposed method does not 'resist' lowering the learning rate 'for as long as possible' so much as it doesn't lower the learning rate for a fixed number of epochs (algorithm parameter).\n\n* 'Adversarial training' section (5.4): The paper assumes the reader is familiar with the terms 'FGSM', 'white box', and parameters \\epsilon and \\alpha since these are referred to w/o description. Perhaps a short (2-3 sentence) description of the adversarial scenario could be added?\n\n* Experiments: It would be good for the paper to include RMSProp and Adagrad results to the experimental tables as these rules are both readily available for use and widely used.\n\n* Experiments: Is the reporting of the peak accuracy standard in the literature?\n\n* Experiments: I want to give the paper credit for performance on CIFAR100, but this is difficult without explicit points of comparison. This can be easily remedied by including SOTA performance values (along with appropriate references) in the tables or text.\n\n* (Potential) Typos:\n\tProposed method algorithm description:\n\t\tRequirements has a weight decay parameter which seems strange given that the algorithm is performing automated learning rate adaptation...\n\t\tThe epoch counter is incremented in line 5, but not reset prior to Phase 2. Does this mean that Phase 1 training epochs are counted toward the total (T)?\n\t\tLine 7 should be \\eta_t <- \\eta_0 / 2.\n\t\tThe patience counter in line 15 is not utilized below.\n\t\tLine 23 could/should be an else statement.", "belong_id": "rJxyqkSYDH"}, {"uid": "S1xV-3gVqr", "paper_title": "A Simple Dynamic Learning Rate Tuning Algorithm For Automated Training of DNNs", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an algorithm for automatically tuning the learning rate of SGD while training deep neural networks. The proposed learning rate tuning algorithm is a finite state machine and consists of two phases: the first phase finds the largest learning rate that the network can begin training with for p = 10 epochs; the second phase is an optimistic binary exploration phase which increases or decreases the learning rate depending upon whether the loss is NaN, increasing or decreasing. Empirical results are shown on a few standard neural networks for image classification on CIFAR-10/100 datasets and for adversarial training on the CIFAR-10 dataset.\n\nI recommend rejecting this paper for the following reasons: (i) the algorithm developed here is extremely heuristic, no insight, theoretical or empirical, is provided as to why this could be a general algorithm, (ii) a major claim in the paper is that the automatic learning rate tuning does not have any hyper-parameters but the actual algorithm does have parameters such as patience and successive doubling of the learning rate although they are tuned adaptively using ad-hoc heuristics, (iii) the convergence analysis is not at all rigorous, in particular the optimal oracle for SGD  may not exist, and (iv) the baseline algorithms are not tuned and the minor improvements of the proposed algorithm over them is therefore not significant.\n\nSome questions that I would like the authors to answer:\n\n1. While the first phase of the algorithm seems a reasonable thing to do, the second phase is full of heuristics which I am not sure will work well for all problems. For instance, I do not see why the algorithm performs trains for p epochs twice even if the loss increased after the first stage, or why the learning rate should be increased if the loss decreased after the second stage.\n2. Section 4, bullet 3/4 in the definition are problematic: the loss in SGD is not monotonically decreasing with respect to time. What does divergence of training mean here? What does Any SGD algorithm mean? Do you instead mean any first-order stochastic gradient-based algorithm?\n3. If you imagine a double well potential with one wide minimum and one sharp minimum, both at the same training loss, if OPT starts in the sharp valley, it will not be able to go to the wide valley without the training loss increasing.\n4. Have you tried this algorithm on other problems which are sensitive to the values of learning rate, e.g., training optical flow or segmentation networks?\n5.  The wordy and heuristic argument in Section 4.1 rests on statements like AALR and OPT arrive at roughly the same location after so and so epochs and hence reaches similar generalization performance. This cannot happen in a non-convex landscape, the trajectory of SGD starting from the same initial condition can be very different across two independent runs. Therefore, I also dont see why the latter half of the statement about generalization should be true.\n6. Can you make the development in Section 4 rigorous?\n7. Why are some runs for SGDR stuck at 10% accuracy in Table 1-2?\n8.  FGSM is a very weak attack for measuring adversarial accuracy. Can you show results with a better attack, say a few steps of PGD?\n\n\nSome suggestions to improve the paper:\n\n1. A simple experiment to check the automatic tuning would be to increase the batch-size of the same network while maintaining the ratio of batch-size and learning constant (see https://arxiv.org/pdf/1706.02677.pdf, https://arxiv.org/abs/1710.11029, among others). It would be interesting to see whether the auto-tuner finds a learning rate that corresponds to stable learning without degradation in the generalization performance.", "belong_id": "rJxyqkSYDH"}, {"uid": "ryldwtXVKB", "paper_title": "Unsupervised Learning of Graph Hierarchical Abstractions with Differentiable Coarsening and Optimal Transport", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method to summarize a given graph based on the algebraic multigrid and optimal transport, which can be further used for the downstream ML tasks such as graph classification.\nAlthough the problem of graph summarization is a relevant task, there are a number of unclear points in this paper listed below:\n\n- In Section 3.1, the coarsening method has been proposed, which is said to be achieved by finding S such that A_C = S^T A S. \n    However, A_C is usually not binary for S \\in R^{n x m}, hence how to get the coarse graph G_C from A_C is not clear. Please carefully explain this point.\n- In the proposed method, coarse nodes should be selected beforehand. Is there any guideline of how to choose them?\n- In Section 3.2, optimal transport is introduced and the distance between G and G_C is measured via entropic optimal transport in Equation (4) or (7).\n    However, in Equation (4), a and b should come from the input G and G_C , and it is not clearly explained how to obtain them from the input.\n    Moreover, how to use the distance between G and G_C in the proposed coarsening method is also not clear. It seems that it is not used in Algorithm 1.\n- I do not understand why the k-step optimal transport distance is needed. Since it converges to the global optimum as k becomes large, it is usually enough to set k to be large enough.\n- In experiments, how is the proposed method used for graph classification?\n    Since the proposed method is for generating coarse graphs in an unsupervised manner, graph classification cannot be directly performed by itself.\n- In addition to the above issue, to assess the effectiveness of the the proposed method, the following experiment is recommended:\n    Fix some classifier and compare performance of graph classification for the original graphs and for the coarse graphs.\n- In the qualitative study in Section 4.4, while the authors discuss coarse nodes, they are just an input from the user and results are arbitrary. Hence such discussion is not informative.\n\nMinor comments:\n- What is 'X' in Equation (2)?\n- I recommend to write domain for matrices when they used at the first time.\n", "belong_id": "Bkf4XgrKvS"}, {"uid": "B1xsVn_aKH", "paper_title": "Unsupervised Learning of Graph Hierarchical Abstractions with Differentiable Coarsening and Optimal Transport", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper proposes a differentiable coarsening approach for graph neural network (GNNs).\nTo this end, it is motivated by algebraic multigrid and optimal transport methods. \n\nGNNs is indeed an interesting line of research. And introducing coarsening into them, it a highly relevant step. However, there are some major downsides. First, some of the statements are a little but too strong. The paper starts with claiming that GNNs are competitive to graph kernels. But then for instance\n\nChristopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, Martin Grohe:\nWeisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks. AAAI 2019: 4602-4609\n\nshow that many (if not all) GNNs are equivalently expressive as the Weisefeiler-Lehman (WL) graph kernel. Hence, the competitiveness has to be qualified. Moreover, since you also employ graph convolutional networks for coarsening, you are also in the regime of this paper. Consequently, one should actually compare to WL, at least one should mention this connection. Actually, given that the datasets are not that large, one should run some statistical significance test. Moreover, if you check the paper above, they report much better results for PatchySan on MUTAG, better results on Protein for graph kernels, better results on IMDB-B using a hierarchical GNN approach, based on ideas of higher-order WL. \n\nNevertheless, indeed, the present paper shows that a differentiable pooling using WL kind of ideas is competitive to existing pooling approach. This is nice, but in the light of the work above, the novelty is unclear. This has to be clarified before publication. ", "belong_id": "Bkf4XgrKvS"}, {"uid": "HygY5iIScS", "paper_title": "Unsupervised Learning of Graph Hierarchical Abstractions with Differentiable Coarsening and Optimal Transport", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an unsupervised hierarchical approach for learning graph representations. The proposed architecture is constructed by unrolling k-steps of a parametrized algebraic multigrid approach for minimizing the Wasserstein metric between the graph and its representation. The node distance (transport cost) used in the Wasserstein metric is also learned as an L2 distance between the embeddings of some graph embedding function. The approach is compared against 6 other state of the art approaches on 5 graph classification tasks, showing significant improvements 4 of them.\n\nThe paper is reasonably well written, however, I think some of the explanations can be tightened further. Especially a lot on the background of AMG is not really that relevant, since the authors are not transferring technical results from AMG. Also, it seems like a better flow for presenting this argument might be to switch the order of sections 3.2.1 and 3.1.2. It looks like the main point is  that this architecture is trying to emulate iterative coarsened residual optimization of the Wasserstein metric between a graph and its representation. How the coarsening matrix is derived is more of a technical point (it looks like the results would be much more sensitive to a switch of metric than to a switch of parametrization for S). \n\nThe empirical results are quite intriguing. There are, however, natural and important questions left unanswered. First and foremost, how does the amount of downsampling (compression) compare between methods. How many parameters do different methods require? It would also be good to see what the baseline performance would have been without any input compression as to understand how close these approaches are to the upper bound.\n\nFinally, I think the main issue of this paper, is left unresolved, namely, what is the point of not having supervision from the downstream task. As a user of graph representations trying to solve some problem, the only thing I would want from my representation is to capture some notion of sufficient statistics that are small enough to be efficient and allow me to solve my problem. I would not necessarily care about how well the learned representation resembles the original graph unless I believed that my downstream task was hard to evaluate  and that it was very smooth in the Wasserstein metric. I read the paper multiple times, trying to find any discussion on this, but it seems that the fact that an unsupervised representation is a good thing is taken for granted. A point could at least be made using the same representation for different tasks experimentally. Or, perhaps, literally doing an AMG-type unpacking of the downstream task itself as a comparison. This would shed light on the question of whether the iterated residuals or the choice of distance is what's driving the observed results.", "belong_id": "Bkf4XgrKvS"}, {"uid": "ByejMHG6FS", "paper_title": "Stochastic Gradient Descent with Biased but Consistent Gradient Estimators", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper studies stochastic optimization with consistent (may not be unbiased) estimators. This problem is well-motivated through the example of learning graph representations where consistent estimators are easier to obtain than unbiased one. Under the assumption that the estimate converges to the consistent gradient exponentially fast w.r.t. the sample size, the authors give convergence rates for convex, strongly-convex and non-convex optimization. The authors validate their theory through synthetic experiments.\n\nOverall, the paper is well-motivated and well-written however it lacks technically novelty. Under the assumption of exponentially fast convergence to small error, the setup is more like gradient descent (have access to approximate full gradient) than stochastic gradient descent as the paper supposes. The main convergence theorems seem to follow from standard techniques for inexact/noisy gradients. In [1], convergence rates for various first-order methods are proven under the assumption that the error is additive, that is, ||g - h|| <= \\delta. Since the authors implicitly convert the multiplicative error to an additive error in their analysis, their assumptions are comparable to [1]. Also, since the analysis is more like GD, in the strongly-convex setting one can actually get faster convergence rates (logarithmic) as long as \\delta is small (in comparison to the strong convexity parameter) unlike the O(1/T) ones mentioned in the paper.\n\nAdditional comments:\nAssumption - There should be an additive error along with the multiplicative error as in the current setup. If ||h|| is very small then according to the assumption, the estimates of the gradient are very tight; this may not be true. Also, this assumption seems to only be needed for sample complexity purposes, making the tails weaker would only give a larger sample complexity without affecting the convergence rates. Would be good to separate these.\n\nConvergence Analysis - As mentioned above, since the setting is like GD with noisy gradients, a more careful analysis in the strongly convex setting can improve the convergence result. Refer [2] for the standard analysis without noise. \n\nUpper bound on l -  In Thm 2, the authors assume l <= G/||w_1  w^*||. Why is this needed? Increasing l should make the problem more convex and easier.\n\n[1] Devolder, Olivier, Francois Glineur, and Yurii Nesterov. 'First-order methods of smooth convex optimization with inexact oracle.' Mathematical Programming 146, no. 1-2 (2014): 37-75.\n[2] Robert M. Gower. Convergence Theorems for Gradient Descent. https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf", "belong_id": "rygMWT4twS"}, {"uid": "S1gLjxYg9H", "paper_title": "Stochastic Gradient Descent with Biased but Consistent Gradient Estimators", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors study SGD algorithms for problems where obtaining unbiased gradients is potentially computationally expensive. In such cases while obtaining, unbiased gradients is expensive, it might be possible to establish consistent estimators of the gradient. The authors then establish that SGD algorithm when run with consistent gradient estimators (but not necessarily unbiased) have similar convergence properties as SGD algorithms when run with unbiased gradient estimators.  The example problem class considered is the problem of learning embeddings for graph problems, where the task is to get embeddings for nodes. Such embeddings can be used to do node classification or solve any other downstream task that involves the nodes of the graph. For such graph problems learning embeddings requires us  to look at the neighbours of a node, neighbours-of-neighbours and so on, which means that in the worst case calculating gradient w.r.t. a single node can be of time complexity O(N).  Consistent gradient estimators have been proposed for such graph problems in the past but this paper establishes theoretical properties of SGD with such estimators.  \n\nThe paper is well written and the results are convincing. I have a few questions/comments\n\n1. In all the experimental results the loss curves are shown w.r.t. the number of epochs. It is clear that using unbiased SGD, unbiased ADAM is better of than using biased SGD. However, these plots do not tell the complete story as the key point behind using consistent SGD is not achieving lower loss, but actually faster computation. I would suggest that the authors show run-time plots that show how the run-time scales with epochs.\n\n2.  I appreciate the authors efforts in explaining their assumptions and how different assumptions kick in. \n\n3. I wonder if a similar methodology can be applied even to the case of ranking problems (say rank net, see reference below). In ranknet training proceeds via  choosing a pair-of-documents and performing gradient updates w.r.t. the pair. However, if one were to pick a single document, the gradient update w.r.t. that document (d1) should involve all other documents (d2) that are less relevant than d1. My question is does applying the consistent gradient methodology in this paper reveal a new algorithm for training ranknets? ", "belong_id": "rygMWT4twS"}, {"uid": "ByelWUL8qS", "paper_title": "Stochastic Gradient Descent with Biased but Consistent Gradient Estimators", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The analysis of the convergence of SGD with biases gradient estimates dates back to Robins&Monroe, but the authors of this paper focused on a recent original algorithm that shows that once can estimate the approximate gradient of a large GNN network, simply by sampling nodes randomly.\n\nWhen I first read of the paper, I was enthusiastic because I did not know the FastGCN approach presented at ICLR the previous year, which showed that the gradient of a GCN could be efficiently approximated by sampling a subset of the nodes. After reading FastGCN, I was less enthusiastic as most of the originality relied on the consistent estimate of the gradient, when t (number of sampled in the neighbours of the output nodes) increases.\n\nThe main contribution of the paper is the proof that the algorithm converge, but there is no theoretical analysis of the key quantity 't', which is the number of sampled nodes in the neighbours of the output nodes. I would expect to see the number of sample grow as the algorithm converge to the optimal solution since the gradient needs less bias when the algorithm converges. However, the authors do not address this point.\n\nI did not into the details of the proofs, but it seems to me that they are quite loose and several details such as the functional spaces, and the boundedness assumptions, are not mentioned. Here are few examples:\n- In the first sentence: P(x, y) of data x and associated label y. The space of x, the space of y and the probability space are not defined. In fact, no set in which variables belong is defined in the paper.\n- The Theorem 1 is strange to me. I would assume that one needs some assumption of boundedness of Q and finite moments for P to avoid pathological examples where the integral (for the asymptotic expectation) is infinite, but the finite sum G_{st} is always finite, contradicting the limit in theorem 1. \n\nOverall, while proving that the FastGCN algorithm is consistent is important, it is hard to understand how useful the results are and how they can be useful in practice. For example, what can we interpret or what can we learn from the bounds given by Theorem 2?\n\nFinally, I might miss something, but the empirical results showed do not seem to show better gains than the Adam algorithm. The theory shows that the more bias we have, the less accurate we should be, why isn't it apparent in Table 1. Is there something such as the computational cost of Adam, that I'm missing, especially when looking at the graphs? I'm sorry if I did not get the main message of the experiments, but even after reading the paper 3 times, I did not understand what the authors wanted the reader to conclude with these experiments.\n", "belong_id": "rygMWT4twS"}, {"uid": "Bkx-GC8K5B", "paper_title": "Stochastic Gradient Descent with Biased but Consistent Gradient Estimators", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper aims to solve the stochastic optimization problems in machine learning where the unbiased gradient estimator is expensive to compute, and instead use a consistent gradient estimator. The main contributions are the convergence analyses of the consistent gradient estimator for different objectives (i.e., convex, strongly convex, and non-convex). Overall, it is interesting and important, but I still have some concerns.\n\nWhich objective function do the authors aim to minimize, the expected risk or empirical risk? I guess it's the empirical risk, right? If so, the sample size $N$ is constant. This may break the condition (sufficiently large sample sizes satisfying (6)) of theorems (i.e., Theorem 2, 3, 4, 5). Thus, it will narrow the application domains of the theorem.\n\nFor the proofs of theorems, the main difference between SGD and this paper is the involvement of Lemma 9, which is one part of the assumption. Besides, this assumption involves the failure probability $\\epsilon$. The convergence theorem (e.g., Theorem 2) has a probability condition to hold the Eq.(7) (or (8)). Maybe some comments below the theorem can be discussed to decrease $\\epsilon$, although authors discuss it in experiments ('This phenomenon qualitatively agrees with the theoretical results; namely, larger sample size improves the error bound.').\n\nFor the experiments, the authors focus on the training of GCN model. I think it can be considered a doubly stochastic sampling process, which is one for the sample and the other for its neighbor. Is that right? Besides, for Figure 1, can the 'SGD unbiased' be viewed as 'SGD consistent (sampl $n$)'? If not, I think it's important to compare these two because this will clearly show the performance difference between the unbiased and consistent estimator.\n", "belong_id": "rygMWT4twS"}, {"uid": "H1gz-n-oKr", "paper_title": "Amortized Nesterov's Momentum: Robust and Lightweight  Momentum for Deep Learning", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper provides a new simple method to incorporate Nesterov momentum into standard SGD for deep learning, with good empirical and theoretical results. Overall I think this paper should be accepted, some minor comments follow.\n\nAt no point does Polyak's heavy ball method get mentioned, even though the variant of Nesterov acceleration you are considering is very similar to it (since the momentum parameter is fixed, which is not the usual form of Nesterov except in the strongly convex case). It would be beneficial to delineate how this is or isn't related to heavy ball.\n\nThe experiments would benefit from a wall-clock time comparison too, rather than just epochs since these new methods would be slower (but presumably not by much).\n\nThe appendix is huge with most of the technical details relegated there which I did not read fully. I think this impacts the readability significantly, though not grounds for rejection. Perhaps it suggests that a conference with a small page limit is not the best venue?\n\nIt seems that SGD still has better convergence early on. The authors suggest their method fixes this (relative to standard nesterov SGD) but it doesn't seem to be quite as good as SGD. Can you explain or discuss why this is still the case?\n\nThe assumptions require some explanation, they are just listed with no context. What are they and why are they useful?\n\nStep size 'should be constrained to O(1/L)' is misleading, you should say explicitly that step-size <= 1/L (or whatever it is depending on the algorithm).\n\nSome of the writing is a bit strange / sloppy, e.g.:\n'AM2-SGD is a bit tricky in randomness'\n'However, full-batch loss is way too expensive to evaluate.'\n\nIn Algorithm 1 AM1-SGD:\n'xk+1  xk+1 +   ( x+  x )'\ndoesn't parse because x_{k+1} appears twice.\n\nMissing references:\n\nAccelerated proximal algorithms:\n\n*) Beck and Teboulle: A Fast Iterative Shrinkage-Thresholding Algorithm\nfor Linear Inverse Problems\n\n*) Nesterov: Gradient Methods for Minimizing Composite Objective Function,\n\nRestarting (slightly different to your approach but still relevant): \n\n*) O'Donoghue: Adaptive Restart for Accelerated Gradient Schemes ", "belong_id": "S1xJFREKvB"}, {"uid": "H1gUDg7atB", "paper_title": "Amortized Nesterov's Momentum: Robust and Lightweight  Momentum for Deep Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "%% Post Author Response comments %%\nThank you for your detailed response/revision. \n\n1 - Introducing m-times larger momentum: Somehow, this is not a particularly intuitive statement or one that reflects clearly in a theoretical bound. Since we are getting to issues surrounding the use of momentum with stochastic optimization, I would like to make a note that the performance of these algorithms more broadly aren't quite sketched out for their use in broader stochastic optimization. In particular, despite broad use in practice, it is unclear if standard variants of Nesterov acceleration/Heavy Ball method achieve 'acceleration' in stochastic optimization. See for e.g., the work of Kidambi et al ICLR 2018 (On the insufficiency of existing momentum schemes for stochastic optimization) - where, the argument was that these methods were designed for deterministic optimization (where we get exact gradients) - in fact, that paper empirically as well as theoretically shows that these schemes do not offer acceleration in a precise sense compared to specialized algorithms for stochastic optimization. It is unclear if the proposed algorithms can offer a similar improvement over SGD in a provable sense, even for the specific examples described in their paper.\n\n2 - The point about theory (just as you mention) is that it doesnt directly apply towards the simulations, nor, do they improve on already known algorithms - so I am unable to see the point that these results present broader implications that can guide practice.\n\n3 - The response doesnt address the fact that for the theory bounds presented in the paper to hold (even in the convex settings described), one requires knowledge of parameters that are not known a-priori, and are often fairly difficult to estimate. So the performance of the algorithm in practice may quite significantly be away from the bounds described in the paper.\n\nWhile I appreciate the points and revision made by the authors, I still believe the paper requires some rethinking to present their results (and this includes more detailed comparisons to existing works) in order to make a case towards broader practical applications.\n\n\n%%. %%\n\nThis paper considers robustness issues faced by Nesterovs Acceleration used with mini-batch stochastic gradients for training Deep Models. In particular, the paper proposes amortized momentum, an algorithm that offers a way to handle these issues. The paper in general is well written and easy to follow. \n\nThe paper proposes algorithms AM-SGD1 and AM-SGD2 and presents extensive results regarding their complexity analysis on convex problems and their performance when training neural networks. The algorithms require storing one more models worth of storage compared to standard momentum based methods (which can be viewed as a drawback in certain cases).\n\nComments:\n\n[1] I am concerned about the motivation behind this paper - which, according to the paper is that Nesterovs accelerated gradient method with stochastic gradients has huge initial fluctuations. The issue with regards to more fluctuations of the initial performance is natural given how aggressive these accelerated methods work. As long as this is not a reason/cause for worse terminal performance (which doesnt seem to be the case), I am unable to see why large initial fluctuations are concerning.\n\n[2] Theory: The theory bounds for this problem setting do not appear to improve over known bounds in the literature. As a side note, the work of Hu et al. Accelerated Gradient Methods for Stochastic Optimization and Online Learning is highly related to this papers theoretical aspects, setup and bounds. Furthermore, this bounded variance noise model for stochastic gradients, while being theoretically useful (and important), is often very detached from practice (as this implies that the domain is bounded and we perform projections of iterates whenever they go outside the set - such aspects hardly reflect on practical SGD implementations). Using this as a means to reason about robustness of the proposed algorithm (for e.g. remarks for theorem 1a. and in conclusions) appears to be a big leap that may lead to potentially misleading conclusions.\n\n[3] In order to run the algorithm to achieve the theoretical bounds claimed (in theorems 1 and 2), it appears that the stepsize \\alpha_s depends on unknown quantities such as initial distance to opt, noise variance etc.\n\n[4] The claim in page 2 about comparing SGD and M-SGD says that the stepsize in deterministic and stochastic optimization is constrained to be O(1/L) is rather misleading. In realistic practical implementation of SGD with a multiplicative noise oracle, one really has to use a much smaller stepsize than 1/L. This in a sense leads back to point[2] about the unrealistic nature of bounded variance assumptions for understanding SGD based methods used in the context of Machine Learning. They are better suited for understanding stochastic methods in black-box optimization (as opposed to considering Machine Learning problems).\n\nMy take is that even if the authors justify novelty in terms of theory results (which, to my knowledge is limited compared to existing literature), rewriting the paper by considering its theoretical merit and presenting empirical results (even as considered in this paper) of this algorithm (without attempting to make very strong connections to explain issues experienced in non-convex training of neural networks, since the theory works in vastly different settings under restrictive assumptions) can be appreciated by appropriate sections of audience (both in theory as well as optimization for deep learning communities).", "belong_id": "S1xJFREKvB"}, {"uid": "r1elVZBh5H", "paper_title": "Amortized Nesterov's Momentum: Robust and Lightweight  Momentum for Deep Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors proposed Amortized Nesterovs Momentum, a variant of Nesterovs momentum that utilizes several past iterates, instead of one iterate, to provide the momentum.  The goal is to have more robust iterates, faster convergence in the early stage and higher efficiency. The authors designed two different realizations, AM1-SGD and AM2-SGD.\n\nComments:\n\nMy major concern for this paper is its unconvincing motivation and experiment results, especially when the approach is designed for deep learning.\n\nThe motivation for the proposed approach is not quite convincing. The authors said that due to the large stochasticity, SGD with Nesterovs momentum is not robust...This increased uncertainty slows down its convergence especially in the early stage of training and makes its single run less trustworthy For image classification, Nesterov momentum is very popular and the final convergence values of different trails seems to be similar. It would be more convincing if the authors can provide practical evidence for supporting this claim.\n\nIt was claimed that Amortized Nesterovs Momentum has higher efficiency and faster convergence in the early stage without losing the good generalization performance. What is the benefit or advantage for having faster early convergence without improving the final generalization performance?\n\nThe authors claim that M-SGD2 is more robust and achieves slightly better performance, in Figure 1a, however, it is really hard to tell the difference between M-SGD2 and M-SGD from Figure 1a.\n\nThe efficiency improvement in page 4 is really hard to follow for comparison with Algorithm 1 in page 3. Though m > 2 could reduce the number of operations in step 5 and 6, I dont think this is a computational bottleneck. I believe these updates should be very fast in comparison with forward and gradient calculation. Making the 1% computation 50% faster does not mean more efficient. I would like to know how much computation cost can be saved with this modification. On the other hand, adding one more auxiliary buffer (scaled momentum) could significantly impact the training as the memory is often the limit. \n\n\nIn section 3.1, what is identical iteration? It is hard to compare AM2-SGD with AM1-SGD. It would be easier to follow if the side-by-side algorithm comparison can be shown early.\n\nThe section 4s theoretical analysis based on the convex composite problem is not quite convincing. I am not sure how these results are related with the deep learning applications.\n\nIn the experiment section, the comparison of AM1/2-SGD with other baselines seems not quite consistent. The authors first state that they use all 0.1 learning rate and 0.9 momentum for all methods, however, the setting for M-SGD is using 0.99 momentum and different learning rate schedule. This makes the comparison not very meaningful, while AM1-SGD and AM2-SGD do not use learning rate restart. With so many differences, the advantage of AM1-SGD and AM2-SGD are not that different with M-SGD.  In the task of ImageNet-152, M-SGD even is better than AM1-SGD. This makes the conclusion that AM1-SGD has a lightweight design, which can serve as an excellent replacement for M-SGD in large-scale deep learning tasks not quite valid.\n\nMinor: The author may assume readers maybe familiar Katyusha momentum, I think there may need more background about it. \n", "belong_id": "S1xJFREKvB"}, {"uid": "BJg7oswhqS", "paper_title": "Amortized Nesterov's Momentum: Robust and Lightweight  Momentum for Deep Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n\nThis paper proposes two variants of Nesterov momentum that maintains a buffer of recent updates. The paper proves optimal convergence in the convex setting and makes nice connections to mirror descent and Katyusha.\n\nI vote to reject the submission, with my main concerns being with the experimental results. I would consider raising my score if my concerns are addressed.\n\nConcerns\n-The learning rate schedule on the CIFAR experiments is unconventional. The original ResNet paper trains for 64k iterations (roughly 160 epochs). Its standard to train for at least 200 epochs (see schedule from Smith et al.). Do the results hold under the standard schedule with careful tuning for the baselines?  \n-The discussion on Train-batch loss vs. Full-batch loss in Section 2 is unclear. On smaller datasets, it is feasible to perform evaluation at the end of the epoch on the entire batch. Furthermore, reporting test accuracy couples optimization and generalization, and I am not sure what is meant by the statement ``test accuracy is too informative.\n-Reporting the peak test accuracy is strange. In general, we do not have access to the test set. Its more natural to report the final test accuracy or have a holdout set to determine an iteration for evaluation.\n-Its not clear how significant the results on ImageNet and PTB are. Namely, a comparison to AggMo and/or QHM would be good, since the flavor of these algorithms is quite similar. Experiments in the AggMo paper suggest that AggMo performs better on PTB. In the comparison given in Appendix A3, it seems like AggMo performs slightly better throughout training. SGD should also attain better performance with learning rate tuning on ImageNet.\n-Im not sure how useful Test Accuracy STD% is useful as a metric since it is influenced heavily by the learning rate and its schedule. Tail averaging schemes in general seem like they would increase robustness. In addition, there seem to be situations where a higher final variance is beneficial (just run the method for longer and you can find a better solution). It would be nice to expand the discussion on the notion of robustness defined in the paper.\n\nMinor Comments\n-The dashed line in figure 1b) is hard to read. I would recommend removing the grid lines and make the colors more differentiable.\n-Algorithm 1: use of both assignment and equality operator? Whereas other boxes use equality\n-Spacing looks a bit off in parts of the paper. 1) after the first sentence in the introduction 2) generic optimization layer (Defazio, 2018) . However)\n-M-SGD and M-SGD2 can be potentially confusing and are not too informative as acronyms.\n-Remark on Theorem 1b: depicts does not seem like the right word\n\nSmith, S. L., Kindermans, P. J., Ying, C., & Le, Q. V. (2017). Don't decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489.", "belong_id": "S1xJFREKvB"}, {"uid": "S1xi_7koYB", "paper_title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper provides a way to represent symbolic KBs called sparse matrix reified. Relations and entities' types are modelled using sparse matrices. This modelization allows distributing the computation on many GPUs. A neural model is used to manage these matrices. The trained model can be used to perform multi-hop queries. The proposed system has been compared with related systems. The results show that it achieves hits@1 that are comparable with the others.\n\nThe proposed approach seems promising, however, I feel that the paper is not ready for publication. The experiments lack a scalability test with related systems, the scalability test included in the paper only takes into account other definitions of the system. Also, comparison on the run time should be performed to see if the lower performance in terms of hits@1 (which are good anyway) is balanced by a better run time.\n\nTherefore, it is difficult to see whether the proposed system is good or not.\n\nAs for the description of the system, it seems to me to be quite foggy. In my opinion, the neural model should be better described to show how the sparse matrices are mapped into the model.\nAlso, how are the training and testing sentences created? How would the output of a query look?\n\nAfter reading the paper I have the feeling that it was written in a bit of a hurry, without working on the details. There are several typos, parentheses not correctly opened or closed, out of place commas that make some sentences seem unrelated to the rest of the paragraph. However, here last problems are easily fixable.\n\nTo sum up, I am conflicted about the choice of the final score. On the one hand, the approach is interesting, on the other hand the article seems to me not mature enough and strong enough from the point of view of organization, contents and experimental results shown.\n\nMinor problems\nOn page 4, the size of the matrices XM_k uses the factor b that is introduced later.\nIn the introduction, next to footnote 1, I suggest specifying that A is the first query, the one about Tarantino's movies.\nAlso, his surname is misspelt throughout the paper. The correct one is Tarantino.", "belong_id": "BJlguT4YPr"}, {"uid": "BklxFnq2YS", "paper_title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes sparse-matrix KB representation for end-to-end KB reasoning tasks. They demonstrate that their algorithm is scalable to large knowledge graphs which is the central contribution of the paper.\nThey apply this to a bunch of tasks such as KB Completion and KBQA. This is done by mapping the query to a set (weights) of relations over which reasoning is performed. \n \nI would first like to comment that I found the paper very hard to read. Thus due to my difficulty in understanding the paper, it is possible that I might have misunderstood parts of the paper. \n \nThe notations are overly complex. In my opinion, the notations can be simplified to a considerable extent. I would suggest a Table of notations or a small figure explaining the model. The paper, in my view, requires  considerable rewriting.\n \nThe paper states that the proposed approach encodes three floating point values and 6 integers for each triple. Is this true for KBC task? Because I am quite surprised that ReifKB approaches SOTA which use hundreds of floats for representing each entity and relation (e.g. DistMult/ComplEx).  \n \nHere are somethings which are not very clear to me KBQA tasks: It is not fully clear how the query is mapped to r. In my understanding r is a set of relations. Is the output of linear function taken as weights? \nKBC Task: How do you ensure that the N chains are distinct? Do you have N different linear functions (f_i)? Also why is x_i^t added to follow in KBC task?\n \nThe paper needs considerable rewriting and therefore I cannot recommend this paper for acceptance at this stage.", "belong_id": "BJlguT4YPr"}, {"uid": "S1lWRZMTYS", "paper_title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an efficient sparse-matrix based representation for symbolic knowledge bases. This representation enables fully differentiable neural modules to model multi-hop inferences, which is designed to be scalable to handle realistically large knowledge bases. Experiments using the proposed method with end-to-end architectures on downstream KB tasks demonstrate its effectiveness and efficiency. Overall, this paper makes clear contributions and can inspire other researchers in the community to apply this KB representation for various learning / reasoning tasks on knowledge bases. However, considering the readability, I would like to recommend a weak accept for this paper.\n\nAdvantages of this paper: 1) The proposed method employs three sparse matrices to represent all KB relations, which is more efficient than existing work such as TensorLog and can support relation sets; 2) The reified KB representation is scalable, which can be distributed across multiple GPUs, making it much faster than the naive implementation; 3) The proposed method can be naturally used in end-to-end neural models and efficiently trained with gradient-based approaches.\n\nDisadvantages of this paper: 1) In the introduction part, the authors mention that existing neural KB reasoning methods generally require some non-differentiable mechanism to retrieve small question-dependent subset of the KB, but there exist some existing methods such as memory networks that are fully differentiable for the KBQA task. It would be better to cover and discuss more existing methods when summarizing their properties; 2) In the methodology part (Section 2), the description is clear but lacks some guidance for the readers to understand the motivation behind the scene. For example, it would be helpful to discuss why representing relations as sparse matrices is necessary, whether there is any other choices, and the benefit of making the current choice. This might seem obvious for the authors, but can help readers that are not familiar with the context access the methodology more easily and understand the motivation better.", "belong_id": "BJlguT4YPr"}, {"uid": "r1gCIKb0KB", "paper_title": "Rotation-invariant clustering of neuronal responses in primary visual cortex", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present a rotation-invariant representation of a CNN modeling the V1 neurons and a pipeline to cluster these neurons to find cell types that are rotation-invariant. Experimental validation is performed on a 6K neuron dataset with promising results. \nThe paper is well postulated.\n\nBelow are comments about the work:\n\n1. In Figure 2, what does 1 x feature + 2 x another_feature mean?\n2. In Equation 3, why was the square of error differences not used? \n3. In the clustering approach, how is the number of mixtures set for the GMM? How stable is the model to different number of mixtures?\n4. In Figure 6: are Blocks 5 and 13 the same clusters (since they are of the same color) or is it that the colourmap use did not have 100 colors? \n5. In the network learned redundant features, Sentence 1: why do the authors say similar MEIs. The 16 neurons rendered in both blocks look different. \n6. It will be informative to know how the number of clusters vary based on the correlation threshold used to collapse 100 clusters to a lower number. Are the clusters still functionally distinct for varying thresholds? Further why is MEI confusion matrix only shown for 13 groups?", "belong_id": "rklr9kHFDB"}, {"uid": "B1lNHjJJ5B", "paper_title": "Rotation-invariant clustering of neuronal responses in primary visual cortex", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this study, the authors develop a method to cluster cells in primary visual cortex (V1) based on the cells' responses to natural images. The method consists in three steps: \n- fit a rotation-equivariant convolutional neural network model to V1 cells (previously described in Ecker et al. 2019)\n- align all cells by choosing the rotation for each cell that minimizes overall distance between cells in feature space, so that the clustering is mostly blind to the orientation of the filters.\n- cluster the cells using a Gaussian mixture model (GMM).\n\nAlthough I find this article mostly well-written and the topic important, I cannot recommend acceptance because (1) the study does not make a significant contribution to our understanding of V1, (2) the main innovation in ML presented (alignment method) is quite specific and will thus not likely be of interest for the general audience of ICLR:\n\n(1) An important question in visual neuroscience is whether V1 cells form discrete functional clusters as opposed to a continuum. Another related question is whether these functional clusters correspond to distinct cell types characterized by specific wiring patterns, gene expression and/or morphology.\nThe analyses performed do not answer any of these two questions:\n- the clustering model (GMM model) is not compared statistically to other models that would assume a continuous structure in the data (e.g. cells form a sparse continuous manifold in feature space). Although clusters do appear in the t-SNE visualization, this visualization does not provide statistical evidence that cells indeed form distinct clusters.\n- The correspondence of the proposed clusters to cell types with specific wiring patterns, gene expression and/or morphology is not established. To establish this correspondence would require further experiments, as acknowledged by the authors: 'To systematically classify the V1 functional cell types, these proposals need to be subsequently examined based on a variety of biological criteria reflecting the different properties of the neurons and the prior knowledge about the experiment'.\n\n(2) The alignment method, which consists in rotating the cells in feature space so that orientation is not a factor for subsequent clustering, is quite specific to the problem studied and likely not of interest for the general ICLR audience.\n\n\nAdditional feedback:\n\n- Title: ROTATION-INVARIANT CLUSTERING OF FUNCTIONAL CELL TYPES IN PRIMARY VISUAL CORTEX \n=> 'functional cell types' is not adequate here, since the article does not establish the existence of functional cell types. Could be replaced with 'cell responses'.\n\n- Abstract: We apply this method to a dataset of 6000 neurons and provide evidence that discrete functional cell types may exist in V1.\n=> this sentence is misleading, since no evidence for functional clusters is provided.\n\n- 'Thus, the network has learned an internal representation that allows constructing very similar functions in multiple ways'\n=> To avoid the caveat of redundant features, the authors could try to add a dimensionality bottleneck on feature space before readout.\n\n- 'Small values of  incur a small cost for poor reconstructions resulting in small optimised values of T and over-smoothed aligned readouts.'\n=> A simulated annealing procedure (progressive increase of T during learning) could potentially allow the use of larger  values here (i.e. less distortion of the filter).\n\n- The alignment procedure could lead to the emergence of spurious structure in the clustering. It would be important to control for this potential artifact by running the procedure on an unstructured synthetic dataset.\n\n- It is possible that the MEIs within clusters look more similar than they actually are, since the cells are fitted from the same common bank of features. It would be useful but maybe difficult to control for this.\n\n- It would be interesting to test the clustering procedure on a shuffled version of the readout weights (shuffle across features and V1 cells), so as to keep sparsity but not any other structure. Does the t-SNE map look less clustered? Is the GMM fit qualitatively different?\n\n- Fig1(2): add legend/caption. what are the ellipses?\n\n\n\n\n\n\n\n\n\n\n\n", "belong_id": "rklr9kHFDB"}, {"uid": "S1eKwBtIcr", "paper_title": "Rotation-invariant clustering of neuronal responses in primary visual cortex", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1876", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an original approach to predict the function of groups of neurons in the V1 cortex based on their invariance to well designed rotation invariant CNN filters. The design of these features is funded by the observation that specific ganglion cell types have rotation and scale invariant responses to visual stimuli. \nThe method is very clearly explained and the evaluation on an publicly available dataset looks promising. The clustering Figure 6 in particular is very insightful. \nThe paper could have been more impactful if a comparison with a ground truth was built. The issue is clearly that ground truth is hard to establish for this type of problems but biological observations and annotations of cell types can be available (unfortunately not public as far as I know). \nI would also be curious to know how such a method can be applied to a blind patient whose retina does not react to visual stimuli. Is there a biological function that will still preserve such invariance properties which allow to find structure in the data?", "belong_id": "rklr9kHFDB"}, {"uid": "Skgso79atr", "paper_title": "Learning in Confusion: Batch Active Learning with Noisy Oracle", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper provides a solution for batch active learning with noisy oracles in deep neural networks. Their algorithm suffers less from the well-known cold-start issue in active learning. They also improve the robustness by adding an extra denoising layer to the network. \n\nThe main concern is that the two contributions are rather orthogonal to each other and each of them is not that significant. \nThe first contribution, which alleviates the cold-start problem, is not very surprising, since it is a soft version of previous method BALD. \nThe second contribution, a de-noising layer, is relatively orthogonal to batch active learning. \n\nIn the experiments, the authors compared Proposed+noise with Proposed, Random, BALD, Coreset, and Entropy, but I think the only fair comparison here is between Proposed+noise and Proposed. \n\n", "belong_id": "SJxIkkSKwB"}, {"uid": "rJlOwYVRtB", "paper_title": "Learning in Confusion: Batch Active Learning with Noisy Oracle", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The topic handled in this paper is very important (hot topic), in my opinion. The authors tackled is the problem of training machine learning models incrementally using active learning with the oracle is noisy. Multiple samples are selected instead of a unique  sample as in the classical framework. The paper seems technically sound.\n I have some suggestions for improving the quality of the paper. See below.\n\n- Improve the captions of Figures 1 and 2 (more explanation, more clarity).\n\n- Use bigger parentheses in Eq. (3).\n\n- In other to increase the impact of your work, consider in your introduction (or in the 'related works' Section) this kind of approaches that are also active learning algorithms:\n\nD. Busby, Hierarchical adaptive experimental design for Gaussian process emulators, Reliability Engineering and System Safety, vol. 94, pp. 11831193, 2009.\n\nL. Martino, J. Vicent, G. Camps-Valls, 'Automatic Emulator and Optimized Look-up Table Generation for Radiative Transfer Models', IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2017\n\nThis discussion can increase the number of interested readers.\n\n- Upload the final version of your work in Research Gate and ArXiv (to increase the impact of your work).", "belong_id": "SJxIkkSKwB"}, {"uid": "H1lbEu1e5r", "paper_title": "Learning in Confusion: Batch Active Learning with Noisy Oracle", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: The paper proposes an uncertainty-based method for batch-mode active learning with/without noisy oracles which uses importance sampling scores of clusters as the querying strategy. Authors evaluate their method on MNIST, CIFAR10, and SVHN against approaches such as Core-set, BALD, entropy, and random sampling and show superior performance.\n\nPros:\n(+): The paper is well-written and well-motivated.\n(+): The problem is timely and has direct real world applications.\n(+): Applying the denoising layer is an interesting and viable idea to overcome noise effects.\n \nCons that significantly affected my score and resulted in rejecting the paper are as follows:\n \n1 - Experimental setting and evaluations:\nThe biggest drawback in this paper is the experimental setting which is not rigorous enough for the following reasons:\n\n(a) Weak datasets: Authors have chosen some standard benchmarks but they do not seem to be convincing as the datasets are too easy. Based on my experience, the behavior of an active learning agent trained on small number of classes does not necessarily generalize to cases where the number of classes is large. So Id like to ask authors to try to evaluate on datasets with more number of classes as well as more realistic images (as opposed to thumbnail images). \n(b) Comparison to state of the art: More importantly, authors are missing out on an important baseline which is a recent ICCV paper [1] on task-agnostic pool-based batch-mode active learning that has explored both noisy and perfect oracles and to the best of my knowledge is the current state of the art. Authors can extend their experimental setting to the datasets used in [1] including CIFAR100 and ImageNet and provide comparison. The reason that it is important to compare is that the method in [1] is task-agnostic and does not explicitly use uncertainty hence it is interesting to see how this method performs against it. \n(c) More on baselines and related work: In addition to [1], different variation of ensemble methods have been serving as active baselines in this field and I recommend adding one as a baseline. For a recent work in this line you can see this paper from CVPR 2018 [2]. Moreover, the authors seem to be missing on a long-standing line of active learning research known as Query-by-Committee (QBC) began in 1997 [3] in the related work section which should be cited as well. \n(d) Hyper parameter tuning: Last but not least about the experiments is the hyper parameter tuning which is not addressed. It is important to not use the well-known hyper parameters for these benchmarks that have been obtained using validation set from the entire dataset. Authors should explain how they have performed this. \n \n2 - Report sampling time:\nAnother important factor missing in the evaluations is reporting time complexity or wall-clock time that it takes to query samples. Authors should measure this precisely and make sure it is being reported similarly across all the methods. I am asking this because random selection is still an effective baseline in the field and it only takes a few milliseconds. Therefore, the sampling time of a new algorithm should be gauged based on that while performing better than random. Given the multiple steps in this algorithm I am skeptical that the sampling time would be proportional to the gain obtained in accuracy versus labeling ratio over random selection baseline. \n \n3: Section 5.2 is not informative:\n(a) My last major concern is section 5.2 where the discussion on results is given along with supporting figures. \nLack of quantitative results: First of all, no quantitative results are given for the values plotted in figure 3 and 4 (neither in the main text nor in the supplement) and different methods happen to be too close to each other, making it hard to see the right color for standard deviations. Also, in the discussion corresponding to those figures no information is provided in this regard. It is important to report how much labeling effort this algorithm is saving by comparing number of samples needed by each method to achieve the same accuracy because that is the main goal in AL. Lack of numbers also makes it hard for this work to be used by others.\n(b) Figure legends: The way authors have labeled their method in Figure 3 is confusing as the Proposed+noise happens to achieve better performance over Proposed. I think by noise authors meant denoising layer was being used (please correct me if I am wrong) but this is not what the legends imply. \n(c) X axis label: It is common to report accuracy versus percentage of labeled data making it more understandable of how far each experiment has been through each dataset. Additionally, I recommend reporting the maximum achievable accuracy for each dataset assuming that all the data was labeled. This serves as an upper bound.\n(d) Font sizes in figures: It will be helpful to make them larger.\n  \n4. I also have a more general concern about uncertainty-based methods. I know that they have been around for a long time but given the fact that predictive uncertainty is still an open problem and there is still no concrete method to measure calibrated confidence scores for outputs of a deep network (Dropout and BN given in this paper have been already outperformed by ensembles (see [4])), hence relying on uncertainty is not the best direction to go. It is literally chicken and egg problem to try to rely on confidence scores of the main-stream task while it is being trained itself. This issue has been raised in this paper but I am still not convinced that the paper has fully addressed it. I think the community needs to explore task-agnostic methods more deeply. [1] is a good start on this path but there is always more to do. This concern is not necessarily a major part of my decision assessment and I only want the authors to state their opinion on this and explain how accurately they think this issue is being addressed.\n \nThe following issues are less major and are given only to help, and not part of my decision assessment:\n\n1- In Figure 3(c), it appears that the accuracy for Proposed + noise when \\epsilon=0.1 is higher than when it is noise-free. It might be a miss-reading as the figure is coarse and it is hard to compare but if that is the case, can authors explain it?\n\n2- The Abstract does not read well and does not state the main contribution. It has put too emphasize on batch-mode active learning which has become an intuitive approach since deep networks have become popular. Also the wording Our approach bridges between uniform randomness and score based importance sampling of clusters should be changed as all other active learning algorithms are trying to do that. \n\n3 - In section 5.1 please state that you used VGG 16 (I assume so since it is what was used in the cited reference (Gal et al. 2017) but authors need to verify that. Also, the other citation given for this (Fchollet, 2015) is confusing as it is Keras package documentation while in the next sentence authors state that they have implemented their algorithm in PyTorch. So please shed some light on this.\n\n*******************************************************************\nAs a final note, I would be willing to raise my score if authors make the experimental setting stronger (see suggestions above).\n\n[1] Sinha, Samarth, Sayna Ebrahimi, and Trevor Darrell. 'Variational Adversarial Active Learning.' arXiv preprint arXiv:1904.00370 (2019). \n[2] Beluch, William H., et al. 'The power of ensembles for active learning in image classification.' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n[3] Freund, Yoav, et al. 'Selective sampling using the query by committee algorithm.' Machine learning 28.2-3 (1997): 133-168.\n[4] Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 'Simple and scalable predictive uncertainty estimation using deep ensembles.' Advances in Neural Information Processing Systems. 2017.\n\n*******************************************************************\n*******************************************************************\n*******************************************************************\nPOST-REBUTTAL:\n\nIn the revised version, there are new tables (Table 1-4) provided in the appendix which I found too different than results reported for previous baselines by more than 6%. For example, according to Core-set paper (Sener, 2018), Figure 4, they achieve near 80% using 40% of the data (20000 samples), and according to VAAL paper (Sinha et al. 2019 github page: https://github.com/sinhasam/vaal/blob/master/plots/plots.ipynb), they achieve 80.90+-0.2. However, the current paper reports 71.99  0.55 for Core-set, and 74.06  0.47 for VAAL which is a large mismatch.\nMore importantly, looking at the results provided in VAAL paper (Sinha et al. 2019 or Core-set paper (Sener, 2018) they show their performance as well as most of their baselines is superior to random selection by a large gap, but in this paper results shown in Table 1 to 4 in almost all of them random is superior (or on-par) to all baselines and the proposed method is the only method that outperforms baseline which is clearly a wrong claim. Therefore, I decrease my score from weak reject to reject.", "belong_id": "SJxIkkSKwB"}, {"uid": "B1lPAuj2tS", "paper_title": "Learning Good Policies By Learning Good Perceptual Models", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an empirical study of using error reduction as a curiosity measure. The authors consider an auto-encoder model, a colorization model and RND as intrinsic motivation signals. I find the write up very unclear and have trouble understanding what the claims are and how they are backed up. \n\nMajor points:\n* About the claims as stated on page 2: 1) The first claim I don't understand, I think what is meant is on navigation tasks they find 'their measure of representation learning' (proposed in kolesnikov 2019) seems to correlate well with reward optimization. In 3.1 to back this claim they claim to test disentanglement but seem to test classification. I see no reason to not put that part in 4.1. 2) The authors claim to propose a new method: it can't be a separate representation network to derive rewards because that is what burda et al. and many many others do, it can't be the minimax formulation because that has been known for a while (e.g. predictability minimization schmidhuber) so I am not sure what the claim is about. What exactly is novel about the model. 3) CRL seems at best to outperform baselines on beamrider, qbert and riverraid but the results are impossible to assess. We don't know what the x axis is in figure 7 (is it frames, with or without repeats, is updates etc). Pong -12 is far from learnt for instance and its one of the easiest games. \n* suprisal objective and modeling objectives are very high level concepts that we can talk about in the introduction and conclusion but much more precise terms need to be employed in the model exposition. The readers need to know what sort of properties they should have ideally easily identify examples (without having to read 2 papers and a large survey). I would start with the minimax formula and then explain what is considered, how the different intrinsic rewards are added, if they are normalized, how they are weighted etc. etc.\n* the details about failed experiments, environments and architecture should IMO be relegated to experiments\n* It should be very clear early on that the model is separate from the representation. \n* auto-encoders are a large family of models and it is not clear from the paper which exact model is meant by the authors. Also, citing Bengio 2013 is NOT a valid citation for auto-encoders. The right citation depends on the model you apply please use that! \n", "belong_id": "HkgYEyrFDr"}, {"uid": "Hyg8m3oTtr", "paper_title": "Learning Good Policies By Learning Good Perceptual Models", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper formulates curiosity based RL training as learning a visual representation model, where the policy tries to maximise the loss of a shared visual model minimising an auxiliary task (such as autoencoding the input).\n\nCuriosity is an important topic in the RL field and this paper is well motivated. I also like the approach taken as it looks into this problem through the lens of better representation learning (LR) and arguing that with focusing on better LR and maximising model loss for novel scenes, we are going to get also better overall performance.\n\nHowever, there are a few key question marks that are still open and I would suggest them to be answered explicitly in the paper:\n\n1) What is the relationship with methods that use auxiliary tasks for unsupervised training in RL (e.g. Jaderberg et al, ICLR 2017)? It's clear that this method doesn't use any extrinsic reward function but the underlying architecture is similar.\n\n2) Similar to above, the comparisons and contrasts to Burda et al, ICLR 2019 could be made more explicit as the objective functions such as autoencoding which seems to be working well in this paper has also been studied in that work.\n\n3) Continuing with comparisons, it's not clear if this method delivers better performance compared to other curiosity based methods. For examples, the top scores in Fig 7 are considerably lower than those achieved in Burda et al, ICLR 2019 (Fig 2). Similarly, we don't know how the method compares to state-of-the-art on other tasks considered in the paper. As a result, the paper lacks good benchmarking against state-of-the-art in this space and discussion on pros and cons.\n\n4) It seems that in Tab 1 the correlation collapses for the last row, any reason why this is happening?\n\n5) It would be good to add both a system diagram as well as a network architecture to clarify how everything is wired.\n\n6) The training details are missing, both in terms of hyperparameters as well as optimisation strategy for solving minmax.\n\n7) Minor: RND is used in the experimental section to refer to both random feature prediction and random network distillation, so would be better to use different references.", "belong_id": "HkgYEyrFDr"}, {"uid": "SkepPxT0YB", "paper_title": "Learning Good Policies By Learning Good Perceptual Models", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a framework called curious representation learning (CRL) which uses a better visual representation in RL. They show that better visual representation helps reward maximization.\n\nI have to recommend rejection for this paper. It appears 1) the idea of using curiosity is not originated from this paper; 2) I do not see what is a 'better visual representation'; 3) the comparison with baselines does not show that the new method is consistently better.\n\nThe paper is also very hard to read. I would think the name 'curious representation learning' means 'representation learning is curious'. There are many inaccurate languages used in the paper. To list a few: 'complex behavior', 'in curiosity', 'disentanglement'... I do not really understand what does it mean.", "belong_id": "HkgYEyrFDr"}, {"uid": "rJltgrq2FS", "paper_title": "Generative Latent Flow", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe authors propose a model that combines a simple Auto-Encoder (AE) together with a Normalizing Flow (NF) model, such that to derive a generative model. In particular, the AE is used to learn a low-dimensional representation of the given data in a latent space. Then, a NF model learns under a maximum likelihood principle, the distribution of these latent codes by applying an invertible transformation on a easy to sample distribution.\n\nGeneral Comments:\n\nThe paper is ok written, but some parts probably can be improved (see comment #5). As regards the proposed idea of the paper, I think that it is closer to an engineering practical approach than a consistent modeling choice. In particular:\n\n1. With the current modeling the likelihood p(x|z) is not defined, which means that the density of the p(x) can not be evaluated. So this should be considered as an explicit or implicit generative model? In that case the comparison with explicit density models where we can evaluate the p(x) is a bit unfair. Otherwise, the test log-likelihood should be provided.\n\n2. Learning the prior is already a debatable choice. However, in the proposed idea I have the feeling that there is a strong overfiting issue. Since the NF model is trained in a maximum likelihood principle, it will try to put all the mass from the simple distribution p(e) on the training latent codes z_i. Consequently, I am a bit sceptical as regards the generalization power of the generative model. Does the latent distribution learn something meaningful (an illustration could have been informative) or just how to re-generate the training data? \n\n3. The authors claim that the training is end-to-end. However, I think that the model which performs better is actually a 2 stage training model. More specifically, due the sg[.], the L_NLL term does not have any influence in the L_recon term. Therefore, the AE is trained independently, and simply the NF at every step 'follows' and tries to capture the latent (empirical) distribution of the encoded training data.\n\n4. From the experiments is argued that the proposed model provides better samples than the competitive methods. I have the feeling that the generated samples are basically very similar to the training samples. Because the learned prior essentially learns to generate the latent codes of the training data. Does the model generalize i.e. can it generate samples that have not be seen during training?\n\n5. I think that the first paragraph of Sec. 2 and some parts of the next paragraph need improvement. Also, how the decoder implies the distribution \\tilde{p}(z) in the latent space? I would expect the encoder to be responsible for the latent distribution. Moreover, in the classic VAE the KL divergence is used between the approximate posterior q(z|x) and p(z), while the KL between the aggregated posterior q(z) and p(z) is not the default choice, and usually, is not straightforward to optimize.\n\nIn general, I think that the proposed model is a rather good practical approach, but probably not a very well defined modeling choice. As a practical approach, the experiments is most of the times the only way to support the argued behavior. The conducted experiments mainly focus on the FID score. However, I think that it would have been interesting to include examples that show the latent distribution and why is this better from other models, for example less regularized from the p(z) in VAE. Also, another crucial issue is the level of overfiting the current approach might have, because learning the prior implies this behaviour. Does the generated samples cover only the training distribution or can it generalizes to unseen test samples?", "belong_id": "Syg7VaNYPB"}, {"uid": "BkxSFyR6Fr", "paper_title": "Generative Latent Flow", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a deterministic Auto-Encoder with a trained marginal distribution over latent variables, p(z), to be able to sample from the model. For this purpose, the authors propose to use a flow-based model for p(z), and regularize the AE objective (i.e., MSE) with a cross-entropy between q(z) = 1/N \\sum_n E(x_n) and p(z). In general, I find the idea quite interesting. The construction of the objective and motivation behind is rather clear. The experiments are rather convincing (however, the FID metric is subjective, but it's impossible to calculate the log-likelihood scores). The main disadvantage of the paper is its language. There are many typos and difficult to follow sentences. But besides that, the main ideas are well explained. Please find more specific remarks below. \n\nRemarks\n- The language in the paper is a bit off. There are sentences that sound very peculiar, e.g., 'Deep generative models can be roughly classified into explicit and implicit models. The former class assumes explicit parametric specification of the distribution, whereas the latter does not.' \nThe introduction should be re-written. Similarly, Section 2 is hard to follow.\nThere are places where a word or a punctuation mark is missing, for instance:\n* The first line misses a word: '(...) on deep learning.'\n* First paragraph, Section 2: 'This distribution is unknown and possibly The model also has a predefined prior distribution p(z) on Z.'\n* Section 3.2, below Eq. 2: '(...) assumptio (...)'.\n* Section 3.4, last paragraph: 'Our work is differs in two ways (...)'.\n\n- The authors should refer to the following very relevant paper:\n* Xu, H., Chen, W., Lai, J., Li, Z., Zhao, Y., & Pei, D. (2019). On the Necessity and Effectiveness of Learning the Prior of Variational Auto-Encoder. arXiv preprint arXiv:1905.13452.\n\n- It is unclear how the authors dequantize image data that are typically represent as integers from 0 to 255 (or 0 and 1 in the binary case). The authors mention in Section 3.2 that they use the MSE loss for \\mathcal{L}_{recon}. This corresponds to taking a Gaussian decoder in the VAE framework. However, the manner how the images are dequantized is extremely important to properly evaluate all results. For instance, if the authors add a uniform noise, then it highly influences the final quality of a model.\n\n- I do not fully follow why the authors call the objective part for learning the marginal distribution over latents 'the NLL loss'. It is rather the cross entropy between q(z) = 1/N \\sum_n E(x_n) and p(z). This follows from the notation and the description that we stop the gradient for E(.). I find it confusing.\n\n- Figure 4 is extremely important for understanding why we should stop gradient. However, jumping between Section 4.2 and E is extremely annoying. The authors can use up to 10 pages, so adding a half of a page would not make a difference, but it would help a reader a lot.\n\n- The main difference between the objective in Eq. 2 and the ELBO lies in considering a deterministic encoder and entails skipping the entropy term for the variational posterior. Could the authors comment why this is so important to choose a deterministic encoder? I can easily imagine taking q(z|x) = Normal(z | mu(z), a1), i.e., fixing the variance to some value a (e.g., a=1), that would result in Entropy[q] = const. Hence, we can skip entropy from the objective, but still we use a stochastic encoder. \n\n===== AFTER REBUTTAL =====\nI would like to thank the authors for their response and new version of the paper. I must admit that I had a very hard nut to crack. From one side, I really appreciate all the effort the authors put to improve the paper. However, on the other hand, I tend to agree with the reviewer #3 that the paper is interesting from the engineering perspective and it lacks novelty. Eventually, I decided to sustain my score. There are two reasons for that. First, more analysis of the model would be helpful. For instance, analyzing the latent space would be interesting (a suggestion from the reviewer #3). Second, considering a pure AE with a trainable flow-based prior is interesting, but it is also very limiting in the sense of obtaining nicely looking pictures instead of being able to provide (approximate) probability of an image. In my opinion, from the decision making perspective, having a probability of an object is much more important  than being able to generate a crisp image.", "belong_id": "Syg7VaNYPB"}, {"uid": "S1xybyqCKr", "paper_title": "Generative Latent Flow", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new model combining an auto-encoder (AE) and a normalising flow (NF). The model, Generative Latent Flow (GLF), uses the AE to map the inputs to a latent space, which is then transformed using the NF. The approach is intuitively beneficial in that the AE can reduce the dimensionality of the inputs such that the NF mapping becomes much faster, computationally. The proposed method is compared to related methods that use a variational AE (VAE) in combination with an NF, and the similarities are pointed out and studied empirically.\nThe authors compare the performance of GLF to a large number of competing methods, showing very competitive results. In particular, for $\\beta = 1$, GLF significantly outperforms its VAE+flow prior sibling in terms of the Frechet Inception Distance (FID) measuring the quality of the generated images.\n\nThe paper presents a well-motivated method, which is extensively and thoroughly evaluated. The experimental part is the paper's main strength, as the method itself is quite incremental (replacing a VAE with an AE). The authors do, however, spend considerable effort comparing the two versions of the method (using VAE and AE, respectively) both mathematically and experimentally. This is very well done and provides the reader with a good understanding of the behaviour of both models. My main concern is the lack of novelty in the proposed method.\n\nI am also slightly concerned that the paper tries to oversell the method a bit. Among the claimed benefits of the method are 1) better density mapping without over-regularised latent variables, 2) single-stage training, 3) minimal reconstruction trade-off, and 4) faster convergence. As far as I understand, benefits 1, 2, and 3 are shared with similar models (VAE+flow) and are as such not unique to GLF. I am not convinced that benefit 4 is true either, as from figure 3, the convergence rates seem to be similar. GLF clearly reaches a better FID, but this seems to happen before epoch 100, which is the earliest shown. It is not clear from the plot if GLF simply starts out being better or when it gains the advantage. Furthermore, in the discussion just below figure 3, the authors note that 'even with large $\\beta$, GLF still slightly outperforms VAE+flow prior'. I find this to be a stretch - had the training stopped at epoch 400, the conclusion would have been that the methods perform identically.\n\nWhile the authors have clearly put a lot of effort into the paper, they seem to have been rushing for the deadline. There are numerous typos and half-missing sentences (too many to list all, but the worst are pointed out below), so the text needs some polishing before publication. I think it would also make sense to rework section 1 and 2 as, currently, they both present introduction, motivation, and related works.\n\nQuestions:\n- Just above section 3.2, you say that you add a random permutation after each coupling layer. This is not shown in figure 1(b) if I understand it correctly. Here, only a permutation after the entire block is shown. Did I misunderstand the model?\n- Were the model runs in figure 3 also repeated as in table 1? If so, are the standard deviations just too small to be seen or nor shown at all?\n\nMinor comments:\n- 'Auto-encoder' has inconsistent capitalisation throughout the paper.\n- The very first sentence of the introduction misses an ending.\n- Fourth sentence of section 2 misses an ending.\n- Fifth sentence of section 2 changes the notation - I believe it should be z ~ p(z) here to be consistent.\n- There are many examples of a whitespace missing between a reference and the preceding word.\n- p 2: 'detremine' -> 'determine'\n- p 4: 'assumptio' -> 'assumption'\n- p 6: 'Frchet' -> 'Frechet'\n\n", "belong_id": "Syg7VaNYPB"}, {"uid": "H1eGv9N6Fr", "paper_title": "Few-Shot One-Class Classification via Meta-Learning", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\n\n This paper tackles an interesting problem, one-class classification or anomaly detection, using a meta-learning approach. The main contribution is to introduce a parameter such that the inner-loop of the meta-learning algorithm better reflects the imbalance which occurs during meta-testing. Results are shown comparing a few simple baselines to both MAML and the modified variant, on a few datasets such as image-based ones (MNIST, miniImageNet), a synthetic dataset, and a real-world time-series example from CNC milling machines.\n\nOverall, the paper presents an interesting problem and awareness that meta-learning might be general enough to solve it well, but provides no real novelty in the approach. The datasets and comparison to other state of art methods (including both other anomaly detection methods and out of distribution methods) is lacking. I suggest the authors perform more rigorous experimentation and focus the paper to be a paper about an understudied problem with rigorous experiments/findings, or improve their method beond the small modification made. Due to these weaknesses, I vote for rejection at this time. Detailed comments are below. \n\nStrengths\n \n  - The problem is interesting and under-studied in the context of deep learning and transferable methods from similar ML problems (e.g. few-shot learning)\n\n  - The method is simple and adapts a state of art in few-shot learning (meta-learning, and specifically MAML)\n\nWeaknesses\n\n  - While I enjoyed reading the paper since it tackles an under-explored problem, it is hard to justify publishing the method/approach at a top machine learning conference. Changing the balance in meta-learning is a relatively obvious modification that one would do to better reflect the problem; I don't think it results in general scientific/ML principles that can be used elsewhere. \n\n  - The relationship to out-of-distribution detection (which some of the experiments, e.g. Multi-task MNIST and miniImagenet essentially test) is not discussed or compared to. How are anomalies defined and is it really different than just being out-of-distribution? \n\n  - The datasets are limited. The MNIST dataset seems to choose a fixed two specific categories for meta-validation and meta-testing, as opposed to doing cross-validation. Results on just one meta-testing seems limited in this case with just one class. In terms of time-series, anomaly detection has been studied for a long time; is there a reason that the authors create a new synthetic dataset? For the milling example, how were anomalies provoked?\n\n  - The baselines do not represent any state of art anomaly detection (e.g. density based, isolation forests, etc.) nor out of distribution detection; the latter especially would likely do extremely well for the simple image examples. \n\n  - There is no analysis of what the difference is in representation (initialization) learning due to the differences between the OCC and FS setup. What are the characteristics of the improved initialization?\n\n\nOne minor comment not reflecting the decision:\n  - Exposition: Define the one-class classification problem; it's not common so it would be good to define in the abstract, or mention anomaly detection which is a better-known term. \n\n", "belong_id": "B1ltfgSYwS"}, {"uid": "r1xMn6u6tB", "paper_title": "Few-Shot One-Class Classification via Meta-Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors have investigated the few shot one classification problem. They have presented a meta-learning approach that requires only few data examples from only one class to adapt to unseen tasks. The proposed method builds upon the model-agnostic meta-learning (MAML) algorithm. I think the topic itself is interesting and I have the following concerns.\n(1) The first is about the real requirement of this learning scenario. Although the authors have pointed out some real applications, I think they have been introduced separated. In other words, since this setting is the combination of two previous areas, i.e., one class classification and few-shot learning, I fell that the authors have introduced it by just a combination. What are the unique challenges of this problem? I think these problems should be clarified at first.\n(2) The second one is the algorithms itself. Although I have not checked the details, I fell that the authors have prepared this paper in a rough way. The authors have only described the method, without deep analyses answering the question why. For example, the method seems heuristic, without theoretical analysis. In summary, I think this paper likes a technical report, not a research paper.\n(3) Although I can catch the main meaning of this paper, it seems that the writing style is not so fluently. I suggested the authors to recognize the presentation.\n", "belong_id": "B1ltfgSYwS"}, {"uid": "S1lWpY-x5H", "paper_title": "Few-Shot One-Class Classification via Meta-Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "One of promising approach to tackle the few-shot problems is to use meta-learning so that the learner can quickly generalize to an unseen task. One-class classification requires only a set of positive examples to discriminate negative examples from positive examples. The current paper addresses a method of meta-training one-class classifiers in the MAML framework when only a handful of positive examples are available. \n\n---Strength---\n- Few-shot one-class classification is a timely subject, which has not be studied yet.\n- Meta-training one-class classifiers in the MAML framework seems to be sound.\n\n---Weakness---\n- MAML is quite a general meta-training framework, which can be used when parameterized base-learners are updated using gradient methods.  Thus, when parameterized models for one-class classification are used, it is rather easy to meta-train one-class classifiers in the MAML framework.\n- Regarding episodic training, in contrast to few-shot classification problems, support sets in episodes have similar positive examples. Thus, fine-tuning baseline method could work well, even without using MAML. Please compare it with the fine-tuning method.\n\n---Comments---\n- I assume that the query set in each episode include negative examples, while support sets have only positive examples. Right? What is the value of c (class imbalance rate) in the query set?\n- Wouldn't it be  better to focus on experiments with c=0% since one-class classification requires the training with only positive examples?\n- What was the baseline one-class classifier? One-class SVM?\n- It was mentioned that CLEAR was an earlier work. Then, the empirical comparison with CLEAR should be included when image data is considered.", "belong_id": "B1ltfgSYwS"}, {"uid": "SkgFH5o3FS", "paper_title": "The Sooner The Better: Investigating Structure of Early Winning Lottery Tickets", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper attempts an in depth study of the lottery ticket hypothesis. The lottery ticket hypothesis holds that sparse sub-networks exist inside dense large models and that the sparse sub-networks achieve at least as good an accuracy as the underlying large model. These sub-networks are discovered by training and iteratively pruning the dense model. This paper investigates the epoch at which pruning should occur as well as the epoch at which weights should be rewound when retraining. Then, the authors conduct experiments with different pruning strategies (one-shot vs. gradual) in an attempt to find such sparse models (or 'winning tickets') earlier than they otherwise would have been found.\n\nThe experiments conducted by the authors seem to be very extensive, and I think the paper contains useful data to have for researchers interested in better understanding the lottery ticket hypothesis. However, my main issue is with both the originality and significance of this work. This paper gives evidence that winning tickets may be found 'early,' although their notion of early still involves quite a lot of training. \n\nAlthough the paper is interested in addressing the structure of the winning tickets, I really didn't find any of the discussion of structure to give much insight into the lottery ticket hypothesis. Most of the section focuses on analyzing weight magnitude, though I was hoping for something more about the actual structure of the sparse subnetwork -- especially given the title of the paper. Figure 3 is notable, showing that different winning tickets (parameterized by different prune and rewind epochs) can have a large Hamming distance between them. This is very interesting, and I wish the authors had more to say. How is this affected by different initializations? Are these solutions connected on a loss landscape? Is there something invariant about the sparse architecture after symmetries are taken into account? It's not clear to me that Hamming distance alone is enough.\n\nIn conclusion, the paper presents a set of nice experiments, but doesn't really shed too much additional light on the scientific nature of the lottery ticket hypothesis.", "belong_id": "BJlNs0VYPB"}, {"uid": "SkgLQMNaFr", "paper_title": "The Sooner The Better: Investigating Structure of Early Winning Lottery Tickets", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Overview:\n\nThe paper is dedicated to conducting an in-depth investigation of the structure of winning lottery tickets. The author provides supporting evidence for the structure of the early winning tickets: 1) lottery tickets emerge when the weight magnitude of a model saturates with SGD optimization. 2) pruning before model saturation may result in accuracy degradation. In the experiment part, they employ the memorization capacity analysis and discover the early wining tickets without expensive iterative pruning. The author also conducts extensive experiments with various ResNet architectures on both CIFAR 10 and ImageNet, achieving state-of-the-art results with only 1/5 of the total epochs for iterative pruning.\n\nStrength Bullets:\n\n1. The experiment organization is complete and convincing. Especially for the figure, it not only clearly shows that lottery tickets emerge much earlier before full training ends, but also shows the effect of rewinding. \n2. The author not reveal interesting observations, but also provides useful guidance. It is a complete logic chain that is also aligned with my intuition. For example, first, the author discusses the memorization capacity of different pruned models at different epochs. Then, they introduce a reasonable gradual pruning technique. Finally, they conduct experiments to confirm it.\n3.  The early winning tickets in this paper achieve state-of-the-art results with only 1/5 of the total epochs for iterative pruning.\n\nWeakness Bullets:\n\n1. For lottery tickets, especially for early winning tickets, I think there is a lot of randomnesses. Thus, for the plot like figure 2, figure 5, they need to contain an error bar and the curve should be the average of tens of experiments. It will be more convincing if it decouples the randomness from the real patterns.\n2. The description and organization of section 4 need to be more clear. For example, an algorithm pseudo code will definitely give readers a much more clear understanding of the early winning tickets finding strategy.\n\n\n\nQuestion the authors don't answer which confuses me more:\nNeed more convincing analysis about the indicator - Hamming Distance \n\nJust as the comment I posted after Review 1, we would like to see more analysis about Hamming Distance between different winning tickets. The author mentioned in the following way:\n\n''However, as we demonstrate in Fig. 3 of our paper, the mask-distance does not well characterize the winning tickets. E.g., the lottery tickets drawn at Epoch 120 and 200 have a mask distance of 0.082 in Fig. 3, which is much larger than the mask distance between Epoch 180 and 200. Whereas, all three tickets achieve comparably high accuracy as shown in Fig. 1, implying a shallow correlation between the accuracy and the mask distance''\n\nAs far as I know, this observation only tells us that the structure of the lottery tickets changes, which are drawn from 120 epochs to 180 epochs (although the maintain a similar accuracy). However, we can not conclude that the mask distance is not a reliable measure. As mention by the [EB] paper provided by the authors, you can use the rate of the distance change to indicate the early winning tickets. In this way, we can find winning tickets much earlier than authors' work.\n\nTo better address this question, I suppose the authors need to provide more analysis of the mask distance indicator. I think all three reviewers would like to see the results. A good indicator for early winning tickets is very important, otherwise authors' notion of early still involves quite a lot of training.\n\nRecommendation:\n\nDue to the unsolved important question, here is a weak reject.", "belong_id": "BJlNs0VYPB"}, {"uid": "B1xmfZs6Yr", "paper_title": "The Sooner The Better: Investigating Structure of Early Winning Lottery Tickets", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper carefully observes the behavior of weight magnitudes during training, finding the is a stage of saturation that is closely related to the winning lottery tickets drawing. Based on this observation the authors hypothesize that we can draw lottery tickets early but too early pruning can irreversibly hurt the learning capability for complex pattern. To remedy this and draw the tickets as soon as possible, the authors propose to adopt gradual pruning, which 1) can start early without hurting the learning capability too much; 2) avoid computation-heavy iterative pruning in previous works.\n\nQuestions:\n\n1. Overall I am very happy with the interesting observations and analysis of the dynamics of weight magnitudes and how it can be related to the early winning lottery tickets drawing. But how valuable is it for practical use? In practice, we cannot know in advance when to start (gradual) pruning.\n\n2. In Fig.1, what does it mean if we perform weight-magnitude based pruning at 10th epoch but rewind the weight to the 20th epoch? Is there a baseline network that is normally trained straight to the end and to which we rewind all pruned models?\n\n3. I am not quite convinced by the experiment of Fig. 4 and argument at the bottom half of page 5. I buy the intuition that pruning too early might irreversibly hurt the capability of learning complex pattern. But I have trouble understanding how the experiment of Fig. 4 supports this intuition. The curve of retraining with smaller LR (0.01) has the save trend as the baseline and retraining with larger LR (0.1). Retraining only one epoch can hardly convince me of its relationship with learning capability. Also, for the experiment in Fig. 5, to validate the proposed hypothesis, it's more valuable to provide results around the claimed turning point, i.e. around 100 epoch instead of suddenly jumping from 20 epoch to 120 epoch.\n\n4. In Tab. 2, the results of ResNet56 with gradual pruning is not presented. In Tab. 3, the results of ResNet50 with one-shot pruning is not presented. It would be better to have these results for clear comparison.\n\nOverall, I love the empirical observation of weight magnitudes and think it would help the community to understand lottery tickets and training process of deep models.\n\nUpdate:\nThe response from the authors addressed some of my questions and more experiments were added per my suggestions. However, also considering the authors' response to R#1 and R#3, I don't think it's strong enough for me to raise my score. Therefore I will keep my current score.", "belong_id": "BJlNs0VYPB"}, {"uid": "H1lX6OD6Kr", "paper_title": "CAQL: Continuous Action Q-Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes a novel value-based continuous control algorithm by formulating the problem as mixed-integer programming. With this formulation, the optimal action (corresponding to the maximum action value) can be found by solving the optimization problem at each time step. To reduce the time complexity of the optimization, the author proposes several variants to approximately solve the problem. Results on robotics control are presented. The proposed looks interesting and could be useful in practice. \n\n1. Section 4 of the paper can be improved. Although the author proposes several methods for approximating the optimal solution, it is unclear what message the author wants to convey. How to decide which approximation to use? Is there any situation where one of the approximations should be preferred? \n\n2. In the experiments, the standard deviation is very large, so it is hard to claim the proposed method is better.", "belong_id": "BkxXe0Etwr"}, {"uid": "S1lSZ2z0FH", "paper_title": "CAQL: Continuous Action Q-Learning", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: \nThis paper targets the maximization issue in continuous value based methods, especially Q Learning. The idea is to use Mixed Integer Programming (MIP) to solve the Q maximization step by formulating the neural network structure of the Q function as a constrained mixed integer program. Further improvements are made by approximating the MIP solution in order to make training/inference faster. The method is tested on tasks from the Mujoco domain and compared with other value based methods for continuous control. I found the paper simple to follow and well structured. The problem is well motivated too and the empirical analysis is quite rigorous.    \n\nThe obvious concerns are regarding scalability of the method; both in terms of 1) using other forms of neural network components (ex. Other activation functions, Convolutional networks in the case of vision based problems such as robotic manipulation) and 2) problems that are inherently less sample efficient, i.e. cases where shortening the sampling horizon is not a feasible option for learning meaningful policies. \n\nOverall, I feel the positive aspects more or less outweigh the drawbacks and therefore my vote is for a weak accept.\n\n\nComments/Questions: \n- Table 8 description says hyper-parameter sweeps were done for temperature and exploration noise decay values but the table is missing their values.\n- What happens when action range is increased from default? One of the reasons mentioned for constraining the action space is to validate how well policy-based methods work. To really take this point home, I feel it might be good to check with an increased action range.\n- Controlling w.r.t symmetry of the env (esp. in Mujoco domains), thus reducing the number of actions by half, might help faster MIP computation times. \n- Figure 3 x-axis runs till different values, but the description says training steps are 1000. How long are the experiments run for?\n- Can the authors elaborate on why the episode length is decreased from 1000 to 200?\n", "belong_id": "BkxXe0Etwr"}, {"uid": "S1lRpmI9Kr", "paper_title": "Switched linear projections and inactive state sensitivity for deep neural network interpretability", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Notes: \n\n  -Goal is to study the 'active subnetwork' of Relu based networks for interpretability.  \n\n  -The question of interpretation seems rather thorny.  \n\n  -In Figure 4, the result for Insens seem alright, although it's weird that the data is just mnist digit / noise.  I feel like something with multiple objects would make it much clearer if there is an actual improvement?  For example on Figure 5 I'm not really sure if Insens is better.  The results often look worse to me than 'DeepTaylor', especially on CIFAR10.  \n\nReview: This paper proposes to improve the interpretation of relu based networks by considering the 'inactive network' which could potentially become activated by local perturbations instead of just considering the active part of the network (which is locally linear).  I think this is a step in the right direction for the interpretation of relu based networks, although the results are somewhat borderline.  \n\nAdditionally the tasks could be much better, to show situations where an object is present but which is not related to the labels.  This would provide a much clearer test of the model's capabilities.  \n\n", "belong_id": "SyxjVRVKDB"}, {"uid": "Syl-oOHTYB", "paper_title": "Switched linear projections and inactive state sensitivity for deep neural network interpretability", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The work basically introduced a new way of looking at interpretability; instead of focusing on the source of activations in the network for a given input image, focus on the source of stability (non-active) neurons (in a ReLU network). The work starts by proving (although it is trivial) that in a ReLU (more generally any piece-wise linear) network, for a given input image, there is a locally linear relationship between a given neuron's activation and the image: v= w^T x + b. As the authors correctly mention, focusing on 'w' as the sensitivity analysis is basically the vanilla gradient method. The contribution, however, is focusing on the projection of bias and the introduced notion of 'centre'. With this provided notion, one can focus on the deactivated neurons in the network and how each input pixel is responsible for it. In other words, unlike previous work that focuses on the activation map, the authors correctly refer to the deactivated neurons as another source of the network's prediction.\n\nI'm have reasons for both accepting and rejecting this work.  The work provides a new perspective and asks a very interesting question. The introduced method, although quite simple and trivial, is useful and the authors do a very good job of making valid and reasonable claims about their work's contribution and how it connects to the existing literature. The main drawback of the paper, however, is whether the contributions are enough for this venue. The paper does not convince me that the introduced method would result in better interpretability of deep networks compared to what is already there. Another minor (or for some people in the field major) issue is the experimental setup.  All of the experiments are focused on subjective examples and no objective measure of the introduced method is provided (and the field has many of those objective measures). Providing a few examples of the method in comparison with other methods is not sufficient. Anyhow, the experiment where they prove the usefulness of the method by adding background noise is interesting. I would personally suggest the authors to expand this experiment to testing the method's sanity using the sanity measures provided in previous work: https://arxiv.org/abs/1810.03292 The claims made about the results on smallNORB can be controversial as the authors interpret their method's flipping of importance to be the reality of what's happening in the network and the other method's focus on the edges as false; this is not clear to be true. My score would be subject to change if better experimental results are provided (and the other way round).\n\nA few suggestions and questions:\n* One very important issue with the method is that it considers all of the inactive neurons. We know that a substantial percentage of inactive neurons are just dead neurons the stability of which does not matter. How would the method address the issue?\n* There definitely needs to be an objective measure of the introduced method's performance compared to previous work.\n* The work seems very related to DeepLIFT while there is no mentioning of it.\n* I'm not a fan, but adding results on a SOTA ImageNET paper always helps with making the experiments section crisper.\n* The authors claim that even small perturbations will change the activation pattern. This is not a small claim and is definitely in need of more evidence.", "belong_id": "SyxjVRVKDB"}, {"uid": "rklJ1VSbjH", "paper_title": "Switched linear projections and inactive state sensitivity for deep neural network interpretability", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This manuscript introduces a novel method to explain activities of ReLU-based deep networks by constructing a linear subnetwork which only contains neurons activated by the input. The status of each neuron can be obtained given any input sample. Moreover, the author applies the notion of neurons center, which is a neutral data point that is similar to actual input x, but with differences in particular objects to cause f(x) be positive. The activity of each neuron can be decomposed into the attribution of each input pixel, and this decomposition can also be used to measure the contribution of each pixel to the network stability. Overall, the proposed methodology is intuitive and distinctive to the state-of-the-art interpretability methods.\n\nHowever, the application constraint on the ReLU-based deep neural network prevents this method from being a model agnostic approach: the problem formulation would be much different if other non-linear activation functions are used. Although the experiment part visualizes the superiority of switched linear projections over other prevalent approaches, the evaluations contain mostly subjective assessment and the arguments are monotonous. I would suggest adding more experiments with quantitative analysis, or mathematically demonstrate why the proposed method is better than, say purely gradient-based method, in the linear case. In addition, additional experiments on a broader set of input data (e.g., tabular, text) could avoid the evaluations look cherry-pick. \n\nMinor issues: \n\n1. In figure 2, I think it would be better to write down explicitly the connections between v, \\hat{b} and \\hat{w} for each neuron given any input. Just seeing v and \\hat{b} on top of each subfigure is a bit confusing. \n2. I spent a long time to understand the 'neurons center' concept, it might be better to add some background or mathematical formulation.\n3. In figure 4, when the digits get misclassified, the Insens explanation should highlight the patterns of wrongly predicted digits, but the patterns of neurons' inactive state sensitivity still look like the correct digits.\n4. It would also be interesting to show how the Insens explanation would change when the input is under various kinds of adversarial attacks rather than adding simple Gaussian noise.\n", "belong_id": "SyxjVRVKDB"}, {"uid": "SkxlgZMOjH", "paper_title": "Switched linear projections and inactive state sensitivity for deep neural network interpretability", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #7", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a method to capture patterns of off neurons using a newly proposed metric. While the authors have considered only linear networks, the setup is still relevant because how often these networks can give meaningful results, and can possibly pave the way for future research into more general networks. \n\nPros: The idea itself is interesting, the related works are discussed well, and MNIST experiments are very interesting. \n\nCons/comments : The writing needs a lot of improvement if to be considered for a top venue like iclr.  Other than the MNIST experiments, which show and indicate the importance of relative contrast and boundary, I am not sure how other experiments are meaningful. CIFAR and smallnorb experiments are merely presented, without discussions on what the interpretation shows or helps over the existing methods. Infact, the other methods seem to capture a lot more information than the proposed method. I would suggest adding more discussions and more experiments that show interpretation that this method/metric helps with to make this work stronger.\n\nHave the authors considered  the metric to consider on neurons instead of offneurons ? Is it possible to have a general metric that combines the two in some way ? Intuitively, its unclear to me why only the off patterns can help (except in specific cases as shown in MNIST experiments). \n\n and thus is responsible for the activity vi  This is unclear to me. I understand the projection part though, but I cant make sense of this statement.\n\ninterpretation and interpretability  in the introduction  the writing is too informal. Making use of italicized phrases like switched linear projection does not help with the understanding at all, especially because switched is defined after using the term atleast thrice. \n\nConfirmation bias <-> information we want to get. \n\nThe same issue right after eq 6. Reference subtracted from .... where the first word is italicized to probably imply some intuitive explanation, but for someone not familiar with what reference is just tends to confuse the reader.  \nPlease fix missing references.\n\nEq 7 seems written incorrectly, with the where v= .... Please fix. \n\nWhat is the variance of saliency checks ? In other words, if the experiment of 100 random samples is repeated (say) 20 times, how different are the corresponding coefficients across these repeated runs ?\n\nFigure 3 is waste of space (move to appendix?)\n\nI might be splitting the hairs but Theorem 1 does not warrant a theorem. The result/proof is too straightforward to be a theorem and is already known in some form in the folklore. \n\n", "belong_id": "SyxjVRVKDB"}, {"uid": "r1eoUaJ0KB", "paper_title": "InfoCNF: Efficient Conditional Continuous Normalizing Flow Using Adaptive Solvers", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper examines the problem of extending continuous normalizing flows (CNFs) to conditional modeling. The authors propose a model, InfoCNF, which models a latent code split into two partitions: one that is unique to a class, and another that is more general. InfoCNF relies on the accuracy of ODE solvers, so the paper also proposes a method that learns optimal error tolerances of these solvers. They perform experiments on CIFAR10, showing that InfoCNF outperforms a baseline in accuracy and negative log-likelihood. They also ablate through experiments that demonstrate the utility of learning the ODE error tolerances. \n\nCNFs are an exciting tool, and it's an important problem to devise methodology that applies CNFs to conditional modeling tasks. The idea of splitting latent codes into two separate components -- one supervised, the other more general -- is interesting. And the approach to learning error tolerances is a good idea.\n\nThe main drawback of this paper is the lack of clarity. It is poorly written and the presented model is not clearly motivated. Even after reading through the paper multiple times, I find it difficult to understand various aspects of InfoCNF. Below are some examples of this lack of clarity:\n\n- When motivating Conditional CNF (CCNF), the details for training the model are unclear. What is the loss function, and how does it balance between modeling x and learning the auxiliary distribution? Although these may seem like small details, since InfoCNF builds off on CCNF, it is crucial to solidify an understanding of the training procedure and how the auxiliary task relates to modeling x. Moreover, the cited reference (Kingma and Dhariwal, 2018) does not contain these details (and there is no mention of object classification in the paper, contrary to the claim on page 3). It would be helpful to cite other references when mentioning that this approach is widely used. \n\n- The definition of InfoCNF is unclear. The variable y has been used to denote the image label, so why are there now L latent variables y_1, ..., y_L? The following terms of equation 4 are undefined: L_{NLL}, L_{Xent}, and y_hat. Although some readers would be able to understand these definitions from context (flow-based NLL, cross entropy loss, and the logits provided by the auxiliary distribution q), they are never explicitly defined and result in confusion and ambiguity. Most importantly, p(x|z,y) is never made explicit; although one can infer from context that  is transformed to x using CNFs, it is never made explicit in the definition (and that these flows only appear in L_{NLL}). Overall, the motivation for splitting the latent code into two pieces is not clearly explained, and the paper should spend more time arguing for this.\n\nThe paper compares InfoCNF to a single baseline (CCNF). I understand that the paper is proposing the first method that combines CNFs with conditional modeling, but there are plenty of non-CNF flow-based conditional approaches that could've been compared. The paper goes over these approaches in Section 5 and discusses their differences with InfoCNF, but these are never experimented with. It seems possible that these models could be extended in a straightforward manner to use CNFs instead of other normalizing flows. Even comparing with these models without CNFs would have been interesting. I think that using a single baseline, instead of applying a more complete set of comparisons, hurts the persuasiveness of their method.\n\nIn addition to comparing to a single baseline, the paper only compares for a single dataset (CIFAR10). A more convincing experiments section would compare against more models on more than a single dataset.\n\nThe paper proposes a promising idea to an important problem. Due to the lack of clarity throughout the paper and incomplete comparisons, I would argue to (weakly) reject.  ", "belong_id": "SJgvl6EFwH"}, {"uid": "SJgxY5YRFB", "paper_title": "InfoCNF: Efficient Conditional Continuous Normalizing Flow Using Adaptive Solvers", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a conditional CNF based on a similar intuition of the InfoGAN that partitions the latent space into a class-specific supervised code and an unsupervised code shared among all classes. To improve speed, the paper further proposed to employ gating networks to learn the error tolerance of its ODE solver. The experiments are performed on the CIFAR-10 dataset and synthetic time-series data.\n\nThe paper has addressed an important issue of investigating efficient conditional CNF. The general idea of the paper is clear, but I found certain parts can be improved, such as the formulation of InfoCNF. It seems the authors assume readers know InfoGAN well enough, which might not be the case. \n\nMy main concern is the limited evaluation as all the experiments are performed on the CIFAR-10 and synthetic data. Since the paper address efficient conditional CNF, it would make the claim much stronger if more experiments could be performed on larger images: if not the original imagenet, maybe imagenet-64 or imagenet-128 or image benchmarks with higher resolutions.\n\nWhy does InfoCNF achieve slightly worse NLL in small batch training, while it outperforms CCNF in all the other metrics? Do you have any explanations?\n", "belong_id": "SJgvl6EFwH"}, {"uid": "HyxVhmb6cH", "paper_title": "InfoCNF: Efficient Conditional Continuous Normalizing Flow Using Adaptive Solvers", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper suggests a methodology of partitioning latent code to a set of class-specific codes, to solve the inefficiency from the large size of the latent code in conditional normalizing flow. This inefficiency problem has been a weak point in the existing conditional normalizing flow models. Also, this work addresses the increase of the number of function evaluations which is caused by code partition, by optimizing the error tolerance with a reinforcement learning approach.\n\nThis paper has a novel contribution, outperforms the baseline (CCNF: CNF (Chen et al.) + GMM / auxiliary classifier (Kingma & Dhariwal, 2018)) on several evaluation metrics. I agree to accept this paper, but I vote for weak accept because of the following weaknesses:\n\n1. The (three) contributions described by the authors seem to be somewhat exaggerated.\n- For the second contribution, the authors way is quite similar to Wang et al. (SkipNet). For both studies, the purpose is the efficiency of certain neural architectures and training scheme is hybrid reinforcement learning with REINFORCE (but with different reward design).\n- For the last contribution, I think it is a minor contribution comparing to two first bullets and overlapping with the second one.\n\n2. There is a lack of explanation to support the efficiency of the proposed model.\n- The authors claim that InfoCNF needs fewer parameters than the baseline. Then, why didn't you show the actual number of parameters? The only line that I found was ... InfoCNF requires 4% less parameters than CCNF. (Section 2).\n- Also, it would be better if there were a direct computation and comparison between the size of InfoCNF and CCNF.\n- Finally, is there any constraint on the length of merged latent code z? Since InfoCNF is also an invertible model, it should have the same size as the input, but I cannot find descriptions about it.\n\n3. It is skeptical that automatic tuning of error tolerance has achieved its original purpose.\n- For me, it is questionable whether automatic tuning has achieved its original purpose: reducing NFEs for better speed (and performance).\n- In figure 3c and 3f, we can find NFE of InfoCNF learned tolerances eventually exceeds the NFE of InfoCNF fixed tolerance. It looks like learning tolerance increases NFE in large epochs, and the timing seems to depend on the batch size. If so, the batch size is a really important hyper-parameter in this framework, how can we determine the appropriate size?\n- In section 4.4 (Automatic Tuning vs. Manual Tuning), the authors state that our automatic approach learns the tolerances which outperform the manually-tuned ones in both classification and density estimation while being only slightly worse in term of NFEs.. But as abstract says, is reducing NFEs the original goal of automatic tuning?\n- Lastly, how can we know/confirm our automatic approach via reinforcement learning requires much less time and computational budget. I cannot see any explanation about this claim.\n\n4. Minor things.\n- What is the stage stated in Figure 1?\n- The abbreviation of the CCNF first appears in section 1 but its full name first appears in section 2.2.\n- In figure 3a and 3b (and table 1), why is test NLLs of CCNF lower than InfoCNFs where test error of CCNF is larger than InfoCNFs with high margin? Is there any possible explanation?", "belong_id": "SJgvl6EFwH"}, {"uid": "BJxHzbIT5S", "paper_title": "InfoCNF: Efficient Conditional Continuous Normalizing Flow Using Adaptive Solvers", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a conditioning approach for CNF and explore speed-up by tuning error tolerance of the ODE solvers.\n \nOverall, this is a mediocre paper which directly use a similar structure introduced in InfoGAN but absolutely lose beautiful insights in the construction of the loss function in InfoGAN, i.e., the mutual information between the generated image and codes. In other way, this paper is just an incremental paper with even less insights than the seminal paper. \nAt least in the loss function Eq. (4), I didn't see any mutual information regularization used here. Instead, the authors use a GMM but I am totally not sure why a GMM is better than the mutual information regularization. At the same time, in equation (4), I didn't see the specific definition of L_xent and L_NLL and thus I am not even able to verify that the use of the loss function is correct. For the current version, I am not be able to gain any insight from the loss function. It seems to be an ensemble of several existing works and definitely not innovative. \n\nFor the tuning error of the ODE solvers, I didn't even see the problem statement.  Is it possible to make the problem more clear? It seems that the first time when the authors mentioned error tolerance is in contribution 2, but I didn't see the definition of the error tolerance. I am not sure whether it is a good idea to introduce two problems in one paper. At the same time, I do not know why the problem should be formulated in the form of the reinforcement learning problem. I didn't see any advantage.  Intuitively, learning a generative model can be time consuming  and solving a reinforcement learning problem is also hard. I do not understand why combining them together would be beneficial and even time efficient? For me, it seems that authors are just trying to make the problem unnecessarily more complicated and thus they can use fancy tools to solve it.  \n\n", "belong_id": "SJgvl6EFwH"}, {"uid": "rkl4PWCsKB", "paper_title": "On the Convergence of FedAvg on Non-IID Data", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents convergence rates for straggler-aware averaged SGD for non-identically but independent distributed data. The paper is well-written and motivated with good discussions of the algorithm and the related works. The proof techniques involve bounding how much worse can the algorithm do because of non-identical distribution and introduction of stragglers into the standard analysis of SGD-like algorithms. The presented theory is useful, and also provides new insights such as a new sampling scheme and an inherent bias for the case of non-decaying step size. The empirical evaluation is adequate and well-presented. I think this paper is a strong contribution and should spark further discussions in the community. ", "belong_id": "HJxNAnVtDS"}, {"uid": "Bkxft2PptH", "paper_title": "On the Convergence of FedAvg on Non-IID Data", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper analyzes the convergence of FedAvg, the most popular algorithm for federated learning. The highlight of the paper is removing the following two assumptions: (i) the data are iid across devices, and (ii) all the devices are active. For smooth and strongly convex problems, the paper proves an O(1/T) convergence rate to global optimum for learning rate decaying like 1/t with time. It is also shown that with constant learning rate eta, the solution found can be necessarily Omega(eta) away from the optimum (for a specific problem instance), thus justifying the decaying learning rate used in the positive result.\n\nFederated learning has been an important and popular research area since it models a highly distributed and heterogeneous learning system in real world. Previous theoretical analysis of FedAvg was quite scarce and either made the iid data assumption or required averaging all the devices. This work is the first to prove a convergence guarantee without these two assumptions. In particular, it only requires averaging a (random) subset of devices each round, which is much more realistic than averaging all.\n\nI don't quite have an intuition for why you need strong convexity. I hope the authors could explain this in words and maybe comment on what are the challenges of removing this assumption.\n\n\n------\nThanks to the authors for their response.", "belong_id": "HJxNAnVtDS"}, {"uid": "SyeCOwkk9r", "paper_title": "On the Convergence of FedAvg on Non-IID Data", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Federated learning is distinguished from the standard distributed learning in the following sense: \n1) training is distributed over a huge number (say N) of devices and communication between the central server and devices are slow.\n2) The central server has no control of individual devices, and there are inactive devices that does not respond to the server; full participation of all devices is unrealistic.\n3) The local data distribution at each device is different from each other; i.e., the data is non-iid.\n\nDue to property 1), communication-efficient algorithms such as Federated Averaging (FedAvg) have been proposed and studied. FedAvg runs SGD in parallel on K (N) local devices using their local datasets, and updates the global parameter after E local iterations by aggregating the updates from the local devices.\n\nProperties 2) and 3) makes analysis of FedAvg difficult, and previous results have proven convergence of FedAvg assuming that the data is iid and/or all devices are active. In contrast, this paper studies FedAvg on the non-iid data and inactive devices setting and shows that, with adequately chosen aggregation schemes and decaying learning rate, FedAvg on strongly convex and smooth functions converges with a rate of O(1/T). \n\nOverall, I enjoyed reading this paper and I would like to recommend acceptance. This is the first result showing convergence rate analysis of FedAvg under presence of properties 2) and 3), which is a nontrivial, important, and timely problem. The paper is well-written and reads smoothly, except for some minor typos. The convergence bounds provide insights of practical relevance, e.g., the optimal choice of E, the effect of K in convergence rate, etc. The authors also provide empirical results supporting their theoretical analysis.\n\nSome questions I have in mind:\n- What is 'transformed Scheme II'? Is it the scaling trick described at the end of Section 3.3? The name appears in the experiment section before being defined.\n- What happens if we choose \\eta_t that is decaying but slower than O(1/t), say O(1/\\sqrt t)? Can convergence be proved? If so, in what rate?\n\nMinor typos:\n- Footnote 3: know -> known\n- Assumptions 1 & 2: f in $f(w)$ is math-bold\n- Choice of sampling schemes: 'If the system can choose to active...' -> activate\n- mnist balanced and mnist unbalanced: the description after them suggests they should be switched\n- Apdx D.1: widely -> wide, summary -> summarize", "belong_id": "HJxNAnVtDS"}, {"uid": "BkliLUPDqS", "paper_title": "Cyclic Graph Dynamic Multilayer Perceptron for Periodic Signals", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a novel architecture for extracting features for periodic signals that is sample efficient and has superior performance than previous approaches. The proposed method is based on a graph architecture that takes into account the ordering of the vertices, contrary to standard GNNs. In order to extract periodic signals, they prove that if two data points are phased shifted, then there exists a subgraph for each data point such that one is a cyclic permutation of the other. To that end, the authors present an architecture that is cyclic permutation invariant through a pooling operation on all the cycles. The proposed method is evaluated on simulated and real data.\n\nThe paper is well written and well motivated. The authors however ignore most of previous related work, there is a huge bulk of work on graph neural networks and on modelling time-series. It is important and interesting that the authors compare how the presented architecture differs from previous work.\n\nRegarding the method, the entire approach is based on two assumptions: 1) the data points are phase shifted with a period that is a multiple of T, and 2) that you know that windows size and slide size such that makes one graph a cyclic permutation of the other. How often does (1) happen in practice? How sensitive it is to the failure of such assumptions? \n\nThe results section is the weakest part of this paper. The comparison between other approaches not presented by this method is essentially just the MLP, which is the most naive baseline. The author should compare to 1Dconvs, RNNs, MLPs with fourier features, and state-of-the-art approaches tackling time-series and/or periodic signals. As I mentioned previously, it would also be important to analyze the sensitivity of the method with respect to the assumptions build upon.\n\nAt this stage, I do not think that the paper is ready for acceptance.", "belong_id": "S1xSzyrYDB"}, {"uid": "S1g1uWIO9r", "paper_title": "Cyclic Graph Dynamic Multilayer Perceptron for Periodic Signals", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a machine learning framework for periodic data. The authors note that representing input data in vector form does not encode input coordinates relationship to one. Capturing this structure can be especially important for periodic signals. The authors address this by adding graph structure to each data point to encode relative structure about each coordinate. They then apply a graph neural network to the resulting structured data. They evaluate the method on an anomaly detection in a low data setting.  \n\nThis paper would be greatly improved by an addition of a related work section. It is unclear where the novelty comes in precisely in this work because it is not very well situated within previous work on (i) anomaly detection w/ periodic signals, (ii) temporal and periodic signal processing and (iii) graph neural network approaches. Contextualized this work within these related areas would improve the clarity and readability of the work and also help frame the results. \n\nThe method is evaluated on synthetic and real datasets, comparing a couple variants of the model (one that can deal with phase shifts). Results support the main claims of the paper.\n\nIt is hard for me to assess the significance of this work since this is a very specific application of known techniques. I think the application is an important one, and also one that requires some domain knowledge, so there does appear to be a useful contribution in terms of adapting graph-based methodologies here. However, the methodology and application is outside my area of expertise.\n \nDetailed suggestions / questions / comments:\n- the ws-dimensional  vector v_i is defined as v_i = (x_{(i1)ss+1}, x_{i+1}, . . . , x_{(i1)ss+ws}). Is there a typo here? It's not obvious to me how the second index relates to the sequence or how this sequence is specified? Perhaps it should say  v_i = (x_{(i1)ss}, x_{(i-1)*ss+1}, . . . , x_{(i1)ss+ws})?\n - The authors mention alternative methods of capturing structured temporal information in the input features. For example, they suggest concatenating the original signal with the cross correlated signal. They also suggest time-frequency analysis methods (such as a Fourier transform and the wavelet transform) and applying a CNN to the time-frequency signal. They authors mention that these methods would require much more data than their graph convolution approach. I agree this is probably the case, but this would still be a useful empirical result to show the degree of data required for these alternative. \n- What are previous approaches to detecting the properties explored in this work? In addition to discussing previous approaches in a related work section, some empirical analysis comparison would help contextualize this work as well.  \n\nOverall, I think this paper is a useful application of graph-based methods. The claims are verified empirically on real and synthetic data. I think it could be significantly improved with a discussion of related work and better situating of the methods / more comparisons in the results. As a result of these significant weaknesses I'm really on the fence with my recommendation -- the work is sensible but the paper has a lot of room for improvement and I'm not quite sure its ready for publication. However, it is possible underestimated the significance/impact of this work because I am not very familiar with the topic.", "belong_id": "S1xSzyrYDB"}, {"uid": "HklFYLqOqr", "paper_title": "Cyclic Graph Dynamic Multilayer Perceptron for Periodic Signals", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In the paper, the authors proposed a novel method adding graph architecture for collected data points to utilize not only features but also relative information. This helps reduce time and costs to collect a huge amount of data from industrial machines and improve accuracy. Although the paper idea is very interesting when presenting a new learning model for graph data, explanations and experiments are not convincing. \n\nFor explanations, the authors did not provide sufficient related works or references to prove that the problem the paper wants to solve is important. Also, for some approaches using deep learning mentioned there are no references.\n\nFor experiments, the results presented in Table 1 are good but there are no official baselines (e.g. from some prior works) to make the comparison more reliable.\n\nBase on the arguments mentioned above, the paper is not convincing and reliable.\n\nSmall suggestion revision: \nMore analysis of prior works to show that the problem is important and need-to-solve\nThe introduction section should more references.\nThe experiments should be rigorous cause it lacks reliable baselines for comparison.  \n", "belong_id": "S1xSzyrYDB"}, {"uid": "SkeTY92ocH", "paper_title": "Cyclic Graph Dynamic Multilayer Perceptron for Periodic Signals", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The goal of this work is to explore multi-sensor data modelling, with a focus on anomaly detection in machinery containing rotating shafts. Multi-sensor recordings from machinery can be phase-shifted, due to errors in the relative timing of sensors. The authors develop a method for modelling such phase-shift data. They augment phase-shift data with a graph structure which represents the relationship between sensors and they use a graph neural network with a cyclic permutation structure to enforce phase-shift invariance. Model performance is evaluated with real-world data from machinery containing a rotating shaft. Their model may be useful for other domains with phase-shifted data, such as multi-sensor medical data.\n\nAlthough this is an interesting application domain and model, I have selected weak reject. \n\nThe primary reason for this decision is that the authors do not provide sufficient comparisons to related work and models, either in the form of a literature review, or in the form of model benchmarking. This is especially problematic for a domain which will be unfamiliar to much of the machine learning community.\n\nNone of the six references in the paper address anomaly detection for temporal data (e.g. Ahrens et al. A machine-learning phase classification scheme for anomaly detection in signals with periodic characteristics 2019) or the extensive related literature of time series models (e.g Pope et al. Learning phase-invariant dictionaries 2013, Edwards and Lee, Using Convolutional Neural Networks to Extract Shift-Invariant Features from Unlabeled Data, 2019), or more closely related work on shift invariant graph neural networks (e.g. Gama et al, Convolutional neural network architectures for signals supported on graphs, 2018). \n\nTo address this, I feel that the authors need to provide a related work discussion. I would also like to see some experiments comparing their model to benchmark models that have a greater chance of being competitive such as a variant of models introduced by Pope et al. 2013 or Edwards and Lee 2019 for example.\n\nMinor note: there is a typo in the definition of v_i\n\nThank you for the submission.\n", "belong_id": "S1xSzyrYDB"}, {"uid": "rJgofPeTKr", "paper_title": "Geom-GCN: Geometric Graph Convolutional Networks", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis work proposes geometric aggregation scheme for GCNs, which aims to overcome the limitations in traditional GCNs; those are lacking long distance dependencies and structure information in nodes. In particular, each node is transformed into a latent space. To overcome the first limitation, some nodes that are not directly connected but fall in a near range are also used in aggregation. A relational operator is used to provide position information for each pair of nodes. In this way, the structure information in graph can be used.\n\nThe method proposed in this work is novel and interesting. However, I am confused how this method can overcome the two limitations faced by previous GCNs. To my understanding, GEOM-GCN maps all node in to a 2D latent space. This can be treated as a lower-dimension representation for each node. Based on this, some similar nodes are clustered together. The relational operator is a kind of ranking operator that can rank two nodes based on latent space representations. If my understanding is wrong, please correct me.\n\nBased on this understanding, I didn't find this method can solve the two limitations.\n \n1. To overcome the long-term dependency limitation, GEOM-GCN selects some nodes that are close but not directly connected for aggregation. However, the selected nodes in this way may not connect to the center node. This is a issue that if two nodes that are not connected should be aggregated. The authors should clarify this.\n\n2. The relational operator is used to provide a ranking between two nodes. However, how such kind of operators can be used to aggregate the structure information as described in GIN. For example, how to distinguish those example graphs using this work. I think it would be a plus if authors can make this clear in the paper.\n\n3. The experimental studies are quite weak. Some ablation studies should be done to evaluate the contribution of each proposed methods. For example, how N_{s}(v) contributes to the performance. This is very important for fully evaluating your methods.\n\n4. More tasks and datasets can be added such as graph classification and social networks.\n\n5. Some notations are quite confusing. Like in eq.(1), why m is bold but W is not.", "belong_id": "S1e2agrFvS"}, {"uid": "SJluLVxXqr", "paper_title": "Geom-GCN: Geometric Graph Convolutional Networks", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The work is based on the premise that existing MPNNs have two main weaknesses: (i) the loss function doesn't properly capture the spatial information during graph convolution, and (ii) the difficulty to manage the information encoded in the long range connections.\n\nThe main contribution of this work is a novel method called geometric aggregation. The proposed method is based on two elements: (i) a latent space mapping to capture spatial information using a new bi-level operator, (ii) an integration of geometric aggregation inside GCN, namely geom-GCN. More in detail the idea reported in this work is to map the input graph into an embedding where the geometric relations between nodes are preserved. The graph is embedded using a usual embedding function that guarantees to preserve some graph property of interest like the hierarchy of nodes. After the node embedding, the authors propose to create a structural neighbourhood both (i) in the latent space, taking the nodes within an arbitrary radius, and (ii) in the original space, by taking the adjacent nodes. The expectation here is that with the proper embedding the latent space can catch connections, which are long range in the original space. The message passing is actuated exploiting first the structural neighbourhood to do low-level aggregation, which aggregates nodes that have the same geometric relationship using permutation invariant operators, and then the result of these aggregations, which are virtual nodes, are aggregated again through high-level aggregation making use of operators like concatenation.       \n\nThe goal of this work is clearly formulated by posing the proper research questions. The topic is relevant and it is part of the research agenda of ICLR. A key point of the proposed method is the ortogonalithy of geometric aggregation with respect to other aggregators like GAT. The design of the structural neighboorhood allows the network to choose which neighbors are the most important for the learning task.\n\nSome minor comments.\nThe strong dependency from the embedding fuction does not guarantee the discovery of long range connections. It may happen that the proposed embedding does not catch the relevant information for the task; in these cases the a-priori knowledge on the task becomes crucial. This potential issue is partially supported by the results presented in the mauscript, where there is a gain only when the correct embedding is chosen. \nA further critical point is the choice of the radius. Such a choice can be operated only with an empirical assessment. It is not clear whether it migth be meaningful to choose a radius thatwould encode the same neighbourhood as in the original sapce of data.\nThe authors claim that even if there are more hops between two nodes the relevant information would arrive from the far node to the target node. Nevertheless we may conceive a situation where the relevant information is washed out during the hops. It may happen when the information of the far node is relevant for the target node, but it is not relevant for the target neighbour nodes.\nThe use of concatenation as high level operator is critically dependent from the radius and from the number of edges in the graph. In cases of large values for radius or very dense graphs, the concatenation may increase the spatial complexity of the networks.\nConcerning the Section on empirical analysis, it might be of interest to investigate whether with a proper number of layers a GCN would emulate a geom-GCN.", "belong_id": "S1e2agrFvS"}, {"uid": "rkeT4UtIcS", "paper_title": "Geom-GCN: Geometric Graph Convolutional Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "GEOM-GCN: GEOMETRIC GRAPH CONVOLUTIONAL NETWORKS \n\nThe paper introduces a novel GCN framework, whose purpose is to overcome weaknesses of existing GCN approaches, namely loss of structural neighbor information and failure to capture important dependencies between distant nodes. The paper uses a mapping from nodes to an embedded space and introduces a second type of a neighborhood: a proximity in the embedded space. In the embedded space, a set of relations of nodes is defined. For each node v, the paper uses a 2-stage convolution scheme: 1) for each neighborhood type, the nodes in the same relation with v are combined; 2) the resulting nodes are again combined into a new feature vector. This approach allows one to overcome the issues described above. The experiments show that in most cases the approach outperforms the existing GCN solutions, sometimes with a large gap.\n\nI have the following concerns about the paper:\n-- My main concern is the learning time, which is an issue for a straightforward GCN implementation. There were multiple attempts to decrease it (GraphSAGE, FastGCN, etc.). Therefore, I would like to see running times on the presented graphs as well as on relatively large graphs (see e.g. https://arxiv.org/pdf/1902.07153.pdf for candidates). If some techniques were used to make the implementation faster, I would be good to include them in the paper (or, if they are standard, they should be referenced). At the very least, I believe it should be prioritized as a future direction.\n-- Its unclear why we should use the same latent space and the same  for both N_g and N_s. I would expect that mapping into different spaces could provide better results: the two neighborhood types seem very different, and I dont see why the neighbors should be aggregated in the same way. If using different spaces doesnt provide an improvement, an explanation for this would be very useful.\n--  and  are defined and shown in Table 2, but they are never used (as it stands now,  and  can simply be removed). If the results in Table 3 correlate with them, then this dependence should be highlighted. In such case, it would also be better to move  and  to Table 3.\n-- The paper uses 3 different node embedding strategies. These strategies can be combined in q with different weights (which can be learned as hyperparameters). Will it produce the best of 3 (or better) result?\n-- We use an embedding space of dimension 2 for ease of explanation But what  is used in the real implementation?\n-- There are various GCN implementations; however, the comparison is performed with only 2 of them. I would like to see either comparison with more implementations, or the explanation why the comparison with the given two suffices.\n-- Is it possible to make the implementation available?\n\nWhile there are a lot of possible improvements, I believe that some of them can be addressed in a future research, and the papers novel approach is noteworthy in itself. My current verdict is 5/10, and Ill be happy to improve it if the above issues are fixed.\n\nPresentation issues:\n-- The notation used in definition of m_v^l is unclear.\n-- Why  is a part of each nodes structural neighborhood? Its a global function, isnt it?\n-- Introduction: I believe that the exact problems which GCNs solve (e.g. node classification) should be mentioned.\n-- The flow in Section 2.1 is a bit weird. Namely, it says To overcome the first weakness, but the first wickness wasnt stated in the previous paragraph (of course, one can deduce it, and it also was defined long ago, but its disturbing for a reader).\n-- Figure 1B is confusing: it looks like the nodes from N_g(v) lie in a small region around v.\n-- I think that splitting Figure 1C into 2 figures would make it clearer.\n", "belong_id": "S1e2agrFvS"}, {"uid": "BylUf0l6YB", "paper_title": "Mogrifier LSTM", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe paper proposes a novel LSTM architecture that adds several gating mechanism that gates the hidden state and inputs in between the LSTM update. The proposed model shows superior performance on smallish datasets including PTB,  Enwick8 and NWC.\n\nComments on the paper:\n\n1. The paper proposes an interesting architecture and it seems to show significant improvement in terms of performance for some language datasets. \n\n2. The paper is very well written, the motivation and formulation is clear. There are many analysis to understand the model (the strength and weaknesses).\n\n3.. One thing is that since this could take into account more context,  it seems that this model could potentially generate language / tokens with longer time dependencies. I wonder if the authors have performed any experiments on this and if they have seen any improvements on that front.\n\n4. Also, I am curious about the generalization ability of the model. Could the authors train the model on shorter sequences and test for generation with longer sequences and see how this compares with baseline models.\n\n5. The model seems to be related to Adaptive Computation Time (ACT) from Gaves et al. it would be nice to compare to the ACT.\n\n6. Another slight improvement in writing could be to hightlight the intuition (conclusion in page 8) at the beginning of the paper, this could help in better understanding the motivation of the paper.\n\n\nMinor comments on the paper,\n\n1. The link and the self-citations on page 4 are does not seem to be valid links and citations.\n\nOverall, a well-written paper, extensive analysis and good experimental result.", "belong_id": "SJe5P6EYvS"}, {"uid": "HklaAK10FS", "paper_title": "Mogrifier LSTM", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThis paper tackles the problem of context modelling within recurrent neural networks (RNNs). The authors propose an interdependent gating mechanism that enriches the coupling between inputs and hidden states. For an input x_0 and hidden state h_0; h_0 gates x_0 to create x_1; x_1 then gates h_0 to create h_1; this cyclical gating operation is applied for several rounds and it's output is fed into a recurrent neural network. For the next time-step, this process is repeated, with h_0 as the final h obtained after the final round of gating in the previous time-step. This results in the RNN processing a more contextualized version of the input tokens x.\n\nMain Contributions:\n1. A simple pre-processing step that contextualizes inputs for recurrent neural networks and significantly improves performance.\n2. An extensive evaluation of the proposed technique against previous works and on all relevant datasets.\n\nPros:\nThe paper is very well-written and clear. It motivates and explores the questions and issues surrounding this topic very well.\n\nCons:\nIt would be good to see how this performance translates to other RNN architectures such as GRUs. \n\n\nFinal notes:\nThis paper raises many interesting question:\n- What is the really going on with the gating mechanism? \nThe authors explore this question but the jury is still out on exactly what is going on here.\n- 'Mogrification' as a general preprocessing step: could it also improve performance for transformer models?\n- Are there better ways to preprocess and gate the RNN inputs?\n\n--------\n\nReview Decision:\nIt is clear, well motivated, well written and represents a concrete contribution to the language modelling literature. Furthermore, most claims made are substantiated via thorough experimentation. Lastly, this work demonstrates that rather than relying on data and model scaling to improve performance; there is alot left to be done in tackling language modelling on smaller scale datasets. ", "belong_id": "SJe5P6EYvS"}, {"uid": "rJe0f83x9B", "paper_title": "Mogrifier LSTM", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I have read the authors' response. Their points regarding baseline comparisons are sensible in that there isn't a reason to expect the observations to *not* generalization to other datasets. It is odd that mLSTM is outperformed by LSTM in Table 3, but as the authors note in section 4.2 this may be due to instability of mLSTM during training. The results in the paper demonstrate significant improvement over LSTM, and while there are not as many baseline comparison to similar models as I would have liked to see, the quality of this work is sufficiently high that this is not a fatal flaw. In light of the author response and other reviews, I am revising my rating to 6: Weak Accept.\n\n=====\n\nThis paper proposes a modification of LSTM networks in the context of language modeling called Mogrifier LSTM. Ordinary LSTMs are defined as recurrent operations on the current input, previous hidden state, and previous cell state. The proposed Mogrifier LSTM utilizes the same recurrent unit as the LSTM, but the input and previous hidden state are updated with several rounds of mutual gating. In each round, the input  is multiplied elementwise by a gate computed as a function of the hidden state (or vice versa). The authors experiment on word-level and character-level modeling and compare their Mogrifier LSTMs to several state-of-the-art approaches. They also conduct an ablation study to show the effect of various design choices and hyperparameters and experiments on a reverse copy task.\n\nSpecific contributions include:\n* Proposal of a novel approach for modulating inputs to a recurrent unit by mutual gating.\n* Experiments demonstrating strong performance on a number of language modeling tasks.\n  \nThe paper in its current state is borderline, leaning towards weak reject. Points in favor of acceptance include the high clarity of writing, good experiments of the proposed model, and a discussion of possible reasons for why the mogrification operation works well. The main shortcoming of the paper is experimental comparison to baselines.\n\nThe authors were able to train baseline LSTMs to high levels of performance (presumably due to tuning of hyperparameters) and then demonstrate that Mogrifier LSTMs improve upon LSTMs significantly. This is perhaps not entirely surprising, because the hyperparameter range of the Mogrifier LSTM includes zero rounds of updates, which would render it identical to the baseline LSTM. Therefore, if the hyperparameters are tuned sufficiently well, the performance of the Mogrifier LSTM should be at least as good as the LSTM. What the experiments do not show is that the proposed mogrification outperforms other forms of multiplicative interaction and/or gating. The closest that the authors come to this is the single validation perplexity of the Multiplicative LSTM in Table 3. If thorough hyperparameter tuning is applied to the Multiplicative LSTM or the approaches of Wu et al. (2016) and/or Sutskever et al. (2011), does the Mogrifier LSTM still outperform them?\n\nOther than this critical issue of baseline comparison, the experiments are quite informative. The ablation study showing the effect of different design decisions and the hyperparameter visualiztion in Appendix B are particularly useful. The mogrification operation is described precisely enough for other researchers to implement and the arguments made in 4.4 are compelling.\n\nQuestion for the authors:\n* Some qualitative analysis of the learned mogrification operation would be helpful for understanding the nature of the modulation. For example, how do the predictions change depending on the modulation? If x is modulated by different hidden states, is there a noticeable effect on the output?\n* Did you experiment with other forms of modulation before arriving upon the mogrification formulation? There are some naive approaches such as concatenating the hidden state to the input and applying a nonlinear layer, or predicting affine parameters for the input as a function of the hidden state in the style of FiLM [1]. Are there obvious shortcomings in these naive approaches that mogrification handles gracefully?\n\n[1] Perez, E., Strub, F., De Vries, H., Dumoulin, V. and Courville, A., 2018, April. Film: Visual reasoning with a general conditioning layer. In Thirty-Second AAAI Conference on Artificial Intelligence.", "belong_id": "SJe5P6EYvS"}, {"uid": "rygrOcyJ5r", "paper_title": "Tensor Graph Convolutional Networks for Prediction on Dynamic Graphs", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: this work uses tensor methods to improve graph convolution for dynamic graph, where the nodes are fixed and the edges are changing. Specifically, it uses the M-product technique to develop the operations of sequence of matrices that analog to these operations of matrices. In the M-product notations, everything seems to be as neat as matrix operations. The works also shows decent supremacy on edge classification tasks.\n\n\nComments: this paper is mathematically interesting. It is well-written in general, but the definitions are dense and hard to follow.\n\nIt will be better to give some examples of M-product. For example, what these operations will be if we choose M to be the identity matrix?\n\nM-transfer is a tensor contraction, right?\n\nIt seems if you do the operations of the sequence of matrix, there is no need to do iterations like RNN. I am interested in how this will influence the runtime and memory cost.\n\nThe M matrix is defined as a lower triangle matrix such as (A \\times M)_::t depends on A^(1:t). Is it possible to formulate M such that (A \\times M)_::t will depend heavily on A^(t), and less on the farther matices? such that we encode some Markov property?\n\nDoes there exist some condition when this method will be equivalent to RNN?\n\n\nDecision: I feel this work novel and interesting in general. I would like to weakly accept it.\n", "belong_id": "rylVTTVtvH"}, {"uid": "Bkx1Ww41qH", "paper_title": "Tensor Graph Convolutional Networks for Prediction on Dynamic Graphs", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a M-product based temporal GCNs to handle dynamic graphs. Experiments on four real datasets are performed to verify the effectiveness of the proposed model.\n\nOverall, I think this paper make a few contributions to advocate tensor M-product. However, there are several big issues as listed below. Given the current status, I could not accept the paper.\n\nPros:\n\n1, The generalization brought by M-product seems to be general as it includes quite a few graph convolution elements for 3D tensors in a natural way.\n\n2, The experimental setup is reasonable. Datasets are collected from practical problems and of moderately large scale.\n\n3, The paper is clearly written and easy to follow.\n\nCons & Questions: \n\n1, My first concern is that M-product formulation does not bring any new insights as people have already used some of the key elements in practice for a long time. For example, the M-transform is just applying 1x1 convolution to multi-channel image. Slice-wise matrix multiplication is also common in practice.\n\n2, Moreover, I think there are several challenges in the M-product formulation which prevent the technique from being practical.\n\n(1) Sharing M such that frontal slices of the transformed signal are the same, i.e., each row of M share the same vector, limits the model capacity significantly. If there is no sharing mechanism, then the model learned on one sequence of graphs could not be applied to another sequence of graphs given two sequences have different lengths. \n\n(2) If you learn M from data, how could you ensure that M is invertible? In the paragraph before section 4.1, an edge classification formulation is proposed where the inverse M-transform is abandoned. However, if in practice, you do not need the inverse transform, then do those theoretical properties still hold and what is the meaning of introducing such M-product formulation?\n\n3, A few temporal GCN baselines are neither compared or discussed, e.g., [1]. \n\n4, Could you explain why all the other GCN variants performs significantly worse with a symmetrized adjacency matrix compared to using the asymmetric one? \n\n[1] Li, Y., Yu, R., Shahabi, C. and Liu, Y., 2017. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926.\n\n======================================================================================================\n\nAfter I read authors' reply and other reviewers' comments, I would like to keep my original rating as the issues have not been properly addressed. I agree with the Reviewer #4 that the theoretical results are a bit artificial and trivial. ", "belong_id": "rylVTTVtvH"}, {"uid": "B1eIYqtL9B", "paper_title": "Tensor Graph Convolutional Networks for Prediction on Dynamic Graphs", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposed a new type of graph embedding technique for dynamic graphs based on tensor representation (node x feature x time).  Experiments on edge classification demonstrate improved prediction accuracy. \n\n+ Clear writing with tensor notations and explanation is well-structured \n+ Improved prediction results on 3/4 real-world dynamic graph datasets\n\n- The theoretical results are a bit artificial. The tensor eigendecomposition used in this paper and [Kilmer and Martin] is for slices of the tensor, similarly for FFT and convolution. The technique is a trivial generalization from matrix results. \n- The paper is missing a large body of baselines, both from the network science community (non-deep learning methods) and from this community (diffusion convolutional RNNs, graph attention networks, etc). \n- The method doesn't scale well, especially for graphs with long-term dynamics. It would be good to show the scaling behavior of the proposed model. ", "belong_id": "rylVTTVtvH"}, {"uid": "ryldo2qvYr", "paper_title": "Evaluating Lossy Compression Rates of Deep Generative Models", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe paper proposes a new way to evaluate generative models that don't have tractable likelihoods, such as VAEs or GANs. Such generative models are composed of a prior over latent variables and a decoder that maps latent variables to data. The idea is to evaluate a trained model in terms of the best (lossy) compression rate that can be achieved by encoding a datapoint (e.g. an image) into the latent space, as a function of a permitted distortion between the datapoint and its reconstruction after decoding. The paper describes a method that estimates an upper bound on this rate-distortion curve using annealed importance sampling. The method is applied in evaluating and comparing a few VAE, GAN and AAE architectures on images (MNIST and CIFAR-10).\n\nOverall evaluation:\n\nThis is a very good paper, and I'm happy to recommend it for acceptance.\n\nThe problem considered (evaluating generative models with intractable likelihoods) is an interesting and important one. In general, such models are hard to evaluate and compare with each other. The paper proposes a new method for evaluating them, which can also improve our understanding of these models and potentially diagnose them in practice.\n\nThe method is well-motivated and backed by theoretical results. One clever aspect of the method is the way annealed importance sampling is used to approximate the rate-distortion curve: instead of sampling separately the rate for each distortion level with a different AIS run, a single AIS run is used to approximate the whole curve. This is done by taking the various points on the curve to correspond to intermediate distributions in AIS, which is quite clever.\n\nThe paper is well written, precise, and contains sufficient theoretical background to motivate the method.\n\nThe experiments are done carefully, and the results are interesting. I found particularly interesting the fact that VAEs behave differently to GANs (in terms of their rate-distortion tradeoff) when the dimensionality of the latent space is increased.\n\nSome discussion and critical feedback:\n\nI found the paper too long (10 full pages). I appreciate the detail, precision and depth of explanation, but I think it would be good to reduce the amount of text if possible.\n\nI though that the introduction was too specific to VAEs/GANs and to image modelling, which may give the impression that these are the main models/tasks of interest. I understand that these are the models and tasks that the paper is interested in, but I think it would be better if the introduction acknowledged the existence of other types of generative models (e.g. likelihood-based models such as autoregressive models and normalizing flows) and were less specific to image applications.\n\nProposition 3 is true almost by definition, since R_p is defined to be the minimum of all rate-distortion curves. I wonder if something more informative can be shown here. For example, my understanding is that the reason R^{AIS}_p is not optimal is due to the bias of the importance-sampling estimator in eq. (12). Since this bias asymptotically goes to zero, I suspect that R^{AIS}_p may become equal to R_p for M -> infinity, and perhaps the bound improves monotonically as M increases.\n\nIf my understanding is correct, the reason for the inequality in proposition 4 is that log\\hat{Z} is a biased estimate of logZ (due to Jensen's inequality) despite \\hat{Z} being an unbiased estimate of Z. If that's all there is to it, the proof in the appendix, although precise, is a bit of an overkill. It also seems to me that the log\\hat{Z} bias also approaches zero as M -> infinity, so this inequality maybe also becomes an equality asymptotically.\n\nIn future work, it would be interesting to also evaluate flow-based models (such as Glow). Since these models give exact likelihoods, it may be good to observe how the evaluation based on rate-distortion curves compares with a likelihood-based evaluation.\n\nSection 6 attributes the performance drop of VAEs in the low-rate regime to the 'holes problem'. If that's true, then I would expect the situation to be improved with more flexible prior / posterior models. What prior / posterior models were used in the experiments? If only diagonal Gaussians were used, then it would be interesting to see whether more flexible priors / posteriors such as normalizing flows would change the results.\n\nMinor corrections and suggestions for improvement:\n\n'For continuous inputs, the metric is often dominated by the fine-grained distribution over pixels rather than the high-level structure'\nThis statement is specific to images, not continuous inputs in general.\n\nOn the quantitative analysis of deep belief networks, https://dl.acm.org/citation.cfm?id=1390266\nis an older example of AIS used to evaluate generative models that could be cited.\n\nI think it would be better to drop z_0 and T_0 in equations (1) and (2), and have z_1 sampled from p_0 directly, to make the equations consistent with the equations that follow afterwards.\n\n'using a latent variable z with a fixed prior distribution'\nIn VAEs the prior can also be learned, it doesn't have to be fixed (this may in fact help alleviate the holes problem).\n\nIt could be mentioned that the objective in Eq. (9) has the same form as a generalized VAE objective, such as the one used by beta-VAE, https://openreview.net/forum?id=Sy2fzU9gl\n\nAt the bottom of page 4 and page 5, R_p(D) uses a different font for R than the rest of the paper.\n\nFig. 1 is too small to read on paper, I had to zoom in using the pdf in order to see it properly.\n\nLast paragraph of section 4 specifically mentions images, even though it doesn't need to (datapoints don't have to be images).\n\nThe last two paragraphs of page 7 contain a few grammatical mistakes:\n- fixing the architecture of neural network --> fixing the architecture of the neural network\n- as the result --> as a result\n- there exist a rate distortion code for any rate distortion pairs  -->  there exists a rate distortion code for any rate distortion pair\n- While in our definition of rate distortion -->Whereas, in out definition of rate distortion\n\nTop of page 8, use \\citet instead of \\citep where appropriate.\n\nCapitalize names and acronyms in references, such as ELBO, GAN, MMD, VAE, Bayes, Monte Carlo, etc.", "belong_id": "ryga2CNKDH"}, {"uid": "BygiiXj6KS", "paper_title": "Evaluating Lossy Compression Rates of Deep Generative Models", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the rate-distortion tradeoffs of deep generative models such as variational autoencoders (VAEs) and generative adversarial networks (GANs).\nThe authors propose an annealed importance sampling (AIS) method to compute the rate-distortion curve efficiently. In experiments, the authors compare the rate-distortion curves for VAEs and GANs and discuss the properties of rate-distortion curves.\n\nThe method for computing the rate-distortion curves of deep generative models is interesting and the rate-prior distortion curve is promising as a performance measure. However, the main technical contribution of this work is the estimated AIS rate-prior distortion curve and it is based on a straight-forward application of AIS. \n\nIn fact, Sections 2 and 3 discuss already known result in literature although summarizing them in a paper is nice for readers.\n\nAlthough the findings in the experiments are interesting and insightful, they are still preliminary and further investigations are desirable.\n\nIn Section 5, the authors mention the consistency of their framework with Shannons rate distortion theorem. This seems to be a little overstatement because the authors discuss little about the optimization of the prior p(z).\n\n", "belong_id": "ryga2CNKDH"}, {"uid": "Bkeatl8PcB", "paper_title": "Evaluating Lossy Compression Rates of Deep Generative Models", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a method for evaluating latent-variable generative models in terms of the rate-distortion curve that compares the number of bits needed to encode the representation with how well you can reconstruct an input under some distortion measure. To estimate this curve, the authors use AIS and show how intermediate distributions in AIS can be used to bound and estimate rate and distortion. They apply their evaluation to GANs, VAEs, and AAEs trained on MNIST and CIFAR-10.\n\nI found this paper well written, with a number of interesting technical contributions, particularly how to leverage AIS to compute R-D curves for an individual model. However, the utility and interpretation of these R-D curves for single models remains confusing to me, and there is insufficient discussion and comparison to other joint diversity/sample quality metrics proposed in the GAN literature. The compute time required for evaluation may also limit the applicability: 4-7 hours for 50 images on MNIST, and 7 days for CIFAR-10. \n \nMajor comments:\n* How should we interpret rate-prior distortion for an individual model vs. rate-distortion where models are optimized for each rate? Past work in learned compression and generative models (Theis et al. 2016, Balle et al. 2016, Alemi et al., 2018) show that models must adjust their decoder (and prior) as a function of rate to be optimal in terms of distortion. For a fixed decoder, optimizing the prior may still be required to achieve low rate. Given that many of the models you compare are trained to do well at one point on the R-D curve, why does it make sense to evaluate them at other points? Additionally, you only evaluate models with deterministic decoders and many of the experimental conclusions are highly specific to this setting but not noted. \n* As you focus on general distortion metrics instead of NLL alone, it'd be interesting to compare curves under different distortion measures, e.g. MS-SSIM for images or L1 vs. L2. Right now there's not much experimental novelty vs. prior work that looked at rate-distortion curves with NLL distortion and Gaussian observation models.\n* Itd be useful to include experiments comparing Rate-Prior distortion curves and Rate-distortion curves where you a) optimize over the prior, b) optimize the decoder, fixing the prior, and c) optimize both the prior and decoder. \n* Theres no comparison of other approaches to generate the rate-prior distortion curve. For example, you could just use an amortized inference network like in VAE w/ a flexible variational family and anneal beta over time.\n* There are several related papers which should be discussed and contrasted, in particular https://arxiv.org/abs/1901.07821 which looks at rate-distortion-perception tradeoffs, and https://arxiv.org/abs/1806.00035 which presents precision-recall curves for diversity/quality metrics applied to implicit models. How do the insights gained from the rate-distortion curves relate to precision/recall and why should one be preferred over the other? https://arxiv.org/abs/1611.02163 also looked at distortion as a metric for GANs (equivalent to beta -> infinity in your framework).\n\nMinor comments:\n* Missing discussion of several related works: that presents a complexity measure for the latent space of GANs: https://arxiv.org/abs/1802.04874\n* Wasserstein distance remains difficult to approximate... - see https://openreview.net/forum?id=HkxKH2AcFm that advocates for evaluation with Wasserstein\n* Tightness of bound on simulated data (what BDMC provides) may not correspond to tightness of bound on real data (what you care about in practice). \n* The treatment of VAEs as implicit models only makes sense with location scale family p(x|z), thus the entire framework proposed here doesnt make sense with e.g. autoregressive p(x|z), as used in PixelVAE and others.\n* Why focus on fixed prior p(z)? An alternative would be to optimize p(z), q(z|x) and fix p(x|z). How would this change the resulting rate-prior distortion curves?\n* We can compute the R-D curve by sweeping over \\beta rather than by sweeping over D - this is not the case when the R-D curve has linear segments, see e.g. Rezende & Viola 2018\n* Many of the properties and discussion around rate-prior distortion functions (especially w/NLL distortion) are also in Alemi et al. 2018 as their definition of rate is identical to your definition of rate-prior. Also many of these properties are specific to continuous latents which isnt noted.\n* Should clarify that q_k(z|x) correspond to points along R_p(D)\n* The results in Eqn 14-17 showing you can tractably estimate distortion and get an upper bound on rate using the AIS-derived distributions are very cool!\n* AIS variance is proportional to 1/MK - this is for variance in the partition function? How does this translate to variance in estimates of rate/distortion?\n* In the case of probabilistic decoders, ... -> need the caveat this is with NLL distortion\n* Validation on the linear VAE is great!It looks like some of the points for AIS at low distortion are below the analytic rate, but the proofs indicate the estimated rate should be greater than the analytic rate. Is this just noise?\n* Fig 4 and 5: hard to see difference between the dashed lines\n* VAE results would change drastically if you targeted them to different regimes (e.g. beta-VAE or constrained optimization like GECO)\n* Statements like VAE is trained with the ELBO objective, which encourages good reconstructions only make sense when the decoder is location-scale. VAEs w/rich autoregressive decoders typically do a horrible job reconstructing.\n* How robust are model differences across random initialization? Itd be great to add error bars to all these plots, especially given that GAN training can stochastically succeed.\n* Eqn 18/Fig6a: depending on the dataset, you could easily notice the difference of 4.6 nats and log-likelihood could still tell these two models apart. Itd be useful to add a line at beta=1 to show that the likelihood would be the same but the R-D curves are different.\n\n========================\nUpdate after rebuttal\n\nThank you to the authors for addressing a number of my concerns, adding additional text and experiments. However, my main concern remains: if rate-distortion in an individual model is a useful method for evaluating generative models, you should compare it empirically with other metrics that have been proposed for this purpose (e.g. precision-recall). Additionally, while the theoretical novelty of getting the full R-D curve from a single AIS run is very cool, I'm skeptical of the practical utility as a metric for generative models due to the computational costs of AIS (4-7 hours for 50 images on MNIST). The simple baseline I suggested of training an amortized inference network with beta annealed over time would not require training a separate encoder for each point in R-D, you could just start beta large, and anneal beta in steps to 0 over time, tracing out an R-D curve. Given the current experiments, it's not obvious if the win of AIS in terms of accurate posterior inference is worth the increased computational cost over a simple VI baseline.", "belong_id": "ryga2CNKDH"}, {"uid": "Syer6jjDYH", "paper_title": "Learning Explainable Models Using Attribution Priors", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors proposed the attribution priors framework to incorporate human domain knowledge as constraints when training deep neural networks. This is a general framework that the users can define different attribution priors for different tasks. For example, in this work, the authors proposed three reasonable priors for image input, graph data, and clinical medical data. For the image and the graph data, the prior is basically to have smoother attributions for nearby features; while for the clinical medical data, the authors used the Gini coefficient formula to encourage sparsity, which is of several practical benefits clinical practice. Moreover, the authors proposed the expected gradients algorithm which is a nice extension of the integrated gradients algorithm. The benefit of expected gradients is that it does not need a baseline input, which is usually arbitrary decided by the designer. The expected gradient method does indeed also performed better than the integrated gradient method in the benchmark (see Table 1.) The results in all three experiments are impressive. In the image domain, the model does generate models that paying more attention on the foreground objects, and is more tolerant to the Gaussian noise perturbation (though it does perform less well than a non-regularized baseline model in the no-noise test image, which is understandable.) More impressively, the model does outperform all other controls with a good margin in the anti-cancer drug prediction experiment, which is a nice demonstration of that domain knowledge could be incorporated in a neural network training to achieve better performance. Same to the healthcare mortality prediction data. The authors showed with a very limited amount of data, they can use sparsity prior constraints to get a model with good feature sparsity (Gini coef), and good performance (measured by ROC-AUC). Overall, I found the paper clearly written and the results are impressive. I am not super familiar with the field, and I am not sure how much progress is this paper compared to 'Axiomatic Attribution for Deep Networks' (Sundararajan et. al. 2017),where integrated gradients is proposed. The experiments conducted in that paper seems to be similar to the ones that are done in this paper.\n\nMinor point:\n1. Even though the authors has shown in Table 1 benchmark that expected gradient is performing better than integrated gradient. Also, in Figure 5 showing that integrated gradient cannot highlight black pixels. It would be nice to see how integrated gradient method perform in the three experiments (image, drug data, mortality prediction), does the expected gradient method always outperform?\n2. When the authors refer to Figure 2 and Figure 3 multiple times in the main text, they are referring to either left or right panel. Would be nicer to do for example '... as measured by R^2 (Figure 2 Left).  ", "belong_id": "rygPm64tDH"}, {"uid": "H1x8LhQRKr", "paper_title": "Learning Explainable Models Using Attribution Priors", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents expected gradients which is a method which looks at a difference from a baseline defined by the training data. \n\nThe structure of the paper is strange because it discusses attribution priors but then they are not used for the method. The paper should have a single focus.\n\nAttribution priors as you formalize it in section 2 (which seems like the core contribution of the paper) was introduced in 2017 https://arxiv.org/abs/1703.03717 where they use a mask on a saliency map to regularize the representation learned. \n\nIn section 2.2. I think a few papers to have a look at are a survey article about graph based biasing http://www.nature.com/articles/s41698-017-0029-7 as well as methods for using graph convolutions with biases based on graphs: https://arxiv.org/abs/1711.05859 and https://arxiv.org/abs/1806.06975 . Some of these should serve as baselines. It is not clear which model is used in Figure 2. It is also not clear from the literature if these models are really working so I think these results should be presented in a more detail. As I understand it, real improvements in predicting clinical variables has not been shown to be reproducible so this would be a significant claim of this paper.\n\nIt is not clear if the paper is presenting 'expected gradients' or existing attribution priors. Most of the experiments revolve around existing attribution prior methods. So with that the paper positions itself not as a survey but as a method paper but lacks evidence that the method expected gradients performs better.\n\nI am also not clear on where the image attribution prior comes from for the image task. Where is this extra information? Is it just smoothing?\n", "belong_id": "rygPm64tDH"}, {"uid": "BJx3xBU0tr", "paper_title": "Learning Explainable Models Using Attribution Priors", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary.\nThe paper improves the existing feature attribution method by adding regularizers to enforce (human) expectations about a models behavior. Three different datasets (i.e. image, gene expression, health-care) are chosen to evaluate the proposed models effectiveness, while different regularizers (i.e. image prior, graph prior, and sparsity prior) are explored for the respective task. \n\nStrengths.\n1. Incorporating human knowledge into the model has a growing interest in ML / CV communities.\n2. Three datasets from different domains (i.e. image classification data, gene expression data, and health care data) are used to evaluate the effectiveness of the proposed approach. Data shows that the proposed approach shows better generalization performance (i.e. better performance in test dataset) than baselines.\n3. The paper provides well-documented supplemental materials that contain details of the experimental setting and additional supporting figures.\n\nWeaknesses.\n1. Task-specific heuristic human prior\nI agree (and personally like) the motivation that a method is needed to align a models behavior with human knowledge or intuition -- models behavior may be explained by feature attribution methods while making models accept human knowledge is challenging. However, such an ability is achieved by simply adding task-specific heuristic functions as a penalty or a regularizer. Also, the introduced human priors are similar to general regularization conventions, i.e. a penalty of smoothness over adjacent pixels is commonly used in the CV community. I am concerned that only a limited set of expert-invented human priors can be used in this approach.\n\nFurther, feature attribution methods aim to develop a richer notion of the contribution of a pixel to the output. However, the difficulty would be the lack of formal measures of how the network output is affected by spatially-extended features (rather than pixels). The explored priors (e.g. a total variation loss to make neighboring pixels have a similar impact on the final verdict) actually relieve this issue.\n\n2. Incorporating humans into the modeling process?\nA key motivation behind this work is incorporating humans into the modeling process. This would imply that (human-understandable) information needs first to be transferred from a model to humans. However, I am concerned about what information end-users are expected to obtain from the model. For example, Figure 1 (left) shows an attribution map that highlights multiple intermittent regions from which I cannot understand its behavior. Unless end-users cannot understand the models behavior, how can we expect humans can provide knowledge to model? A user study would be needed to support that the proposed method can really provide a way to incorporate humans into the modeling process.\n\nMinor comments.\n1. Plots in Figure 3 are not intuitively understandable.\n2. There is no section Conclusion.\n3. A template for the reference section looks different from other ICLR papers.", "belong_id": "rygPm64tDH"}, {"uid": "Sye_AQX0FH", "paper_title": "Asymptotic learning curves of kernel methods: empirical data v.s. Teacher-Student paradigm", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper experimentally investigates how fast the generalization error decreases when some specific kernel functions are used in real datasets. This paper conducted numerical experiments on several datasets to investigate the decreasing rate of the generalization error, and the rate is determined for such datasets. This decreasing rate is theoretically analyzed by using the approximation theory of RKHS in the teacher-student setting. It is shown that the rate is determined with the smoothness and effective dimensionality of input. Then, the smoothness of the teacher function is also derived through this analysis.\n\nOverall, the paper is well written. I could easily follow the line. The pros and cons of the paper are summarized as follows.\n\nPros:\nThe numerical experimetns conducted in this paper are thorough, and they show interesting observations on the real datasets. This paper gives a practical information on the theoretical analysis as an empirical study.\n\nCons:\n- The approximation theory shown in this paper (Theorem 1) is closely related to well-known results on kernel interpolation. However, this paper misses several related work in the literature. The result should be properly put in the literature. See, for example, [R1].\n\n[R1] H. Wendland. Scattered Data Approximation. Cambridge University Press, Cambridge, UK, 2005.\n\n- It is mentioned that this paper investigates the 'generalization error.' However, what is acutally done is more like 'approximation error' analysis (about linear interpolation in RKHS). In reality, there are observation noises and thus we typically consider the generalization error. But, the teacher-student setting does not assume the existence of noise. Under existence of noise, generalization error analysis seems more appropriate as performed in [R2].\n\n[R2] I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.\n\nMinor comment:\n- In the introduction, it is mentioned that the assumption that the target function is included in RKHS is strong. However, the teacher-student setting considered in Theorem 1 assumes this assumption. The introduction requires some modification to make the message consistent.\n\n---Update---\nThank you for your reply.\nI understand the RKHS for teacher and that for student are different. But, in the introduction, you stated as 'Yet, RKHS is a very strong assumption which requires the smoothness of the target function to increase with d (Bach, 2017) (see more on this point below), which may not be realistic in large dimensions.', which sounds like that an assumption that the target function is included in 'some' RKHS corresponding to a smooth kernel is a strong assumption. At least, this sentence is not saying anything about difference between teacher and student, but is just saying assuming smoothness on the target is unrealistic. For me, this sounds inconsistent to your analysis. (This is just a minor concern. I wanted to clarify my understanding of your problem setting.) \n\nI think the setting where the teacher is not included in the student RKHS is also analyzed, for example, in the following papers (there are also several related papers):\nF.J. Narcowich, J.D. Ward, and H. Wendland. Sobolev Error Estimates and a Bernstein\nInequality for Scattered Data Interpolation via Radial Basis Functions. Constr. Approx.,\n24:175186, 2006.\nSCHEUERER, M., SCHABACK, R., & SCHLATHER, M. (2013). Interpolation of spatial data  A stochastic or a deterministic problem? European Journal of Applied Mathematics, 24(4), 601-629. \n\nTherefore, I still feel that the paper requires more expositions about the relation to the literature.", "belong_id": "r1enqkBtwr"}, {"uid": "HylkirUAYS", "paper_title": "Asymptotic learning curves of kernel methods: empirical data v.s. Teacher-Student paradigm", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies, empirically and theoretically, the learning rates of (shift-invariant) kernel learners in a misspecified setting. In the well-specified setting, the rate of kernel learners is at least $n^{-1/2}$, and in a misspecified setting assuming only Lipschitz targets, the rate is $n^{-1/d}$. Neither seems to match the experimental rate on MNIST and CIFAR-10; this paper proposes a theoretical model that can more-or-less match the experimental rate with essentially-reasonable assumptions.\n\nMy main complaint is on the basic setting of the work: in your motivation, you say 'it is nowadays part of the lore that there exist kernels whose performance is nearly comparable to deep networks.' The main such kernel, though, is the (convolutional) neural tangent kernel of Arora et al. (2019), which unlike the kernels you study here is not shift-invariant, and your theorems do not at all apply to this kernel. This is fine, but should probably be clearer in the description.\n\nA related comment on your main theorem: your target function evaluated at every conceivable point (not just on a grid) is a sample from a Gaussian process. Samples from GPs with mean zero and covariance kernel $K_T$ almost surely are not in the RKHS $\\mathcal H_T$, but they *are* almost surely in the RKHS of any kernel $K_R$ which nuclearly dominates $K_T$ (see Lukic and Beder, 'Stochastic Processes with Sample Paths in Reproducing Kernel Hilbert Spaces', Trans. AMS 2001). If such a kernel exists, using it as the 'student' kernel should give us a rate of at least $n^{-1/2}$ with standard results (with some slight details still to be worked out, but should be true). Thus, it seems that your theorem implies that for $\\alpha_T < \\frac32 d$, no such translation-invariant kernel $R$ exists. This might be already easy to see from a Fourier definition of nuclear dominance, I'm not sure, but if not it is something that seems of somewhat independent interest.\n\nIt is also notable that both your practical results and your theorem are for algorithms essentially without any regularization other than the choice of kernel: the regression setting is exact interpolation, and your soft-margin uses $C = 10^4$ so is 'almost' a hard-margin SVM. This is also fine  interpolation methods have seen a lot of interest of late, and certainly can perform well. But it's not the typical setting, and it would be interesting to see if the curves of Figure 1 look different when using e.g. a cross-validated setting for the amount of regularization.\n\nAnother complaint: you argue that applying Theorem 1 with this particular notion of effective dimension seems to give good results, but at least as it's stated, Theorem 1 doesn't actually apply with effective dimension, only ambient dimension. Is it possible to prove Theorem 1 with an appropriate version of effective dimension? I didn't carefully check the proof, but from your outlined sketch it seems like it might be only a small change.\n\nEmpirically, your investigations are nice, but it would be good to consider some other shift-invariant kernels as well: inverse multiquadric, Matern, or spline RBF kernels would be prominent options.\n\nOverall: I think this is a worthwhile study with interesting results. The theoretical setting, though, is somewhat limited by its fundamental approach, and the experiments aren't as thorough as they could be. Also, honestly, I'm not sure ICLR is the best venue for it (if I had written this paper around this time, I probably would have submitted it to AISTATS; it's certainly not *off* topic for ICLR, but fairly distant from most work at it).\n\nSome typos:\n- Under (2): 'man-square error.'\n- Under (25): 'where where.'", "belong_id": "r1enqkBtwr"}, {"uid": "HklSLYXrcr", "paper_title": "Asymptotic learning curves of kernel methods: empirical data v.s. Teacher-Student paradigm", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In order to rationalize the existence of non-trivial exponents that can be independent of the specific kernel\nused, this paper introduces the Teacher-Student framework for kernels. In this scheme, a Teacher generates data according to a Gaussian random field, and a Student learns them via kernel regression. Theresults quantify how smooth Gaussian data should be to avoid the curse of dimensionality, and indicate that for kernel learning the relevant dimension of the data should be defined in terms of how the distance between nearest data points depends on sample numbers.\nThe paper is well written, tghe major issue of this paper is the lack of comparison with other previous methods. Therefore, the efficacy of the proposed model can not be well demontrated.", "belong_id": "r1enqkBtwr"}, {"uid": "rklBR5qaKr", "paper_title": "MLModelScope: A Distributed Platform for ML Model Evaluation and Benchmarking at Scale", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a unified approach to specify, evaluate and benchmark different ML methods.\nWith the main goal of enforcing repeatability and faireness when testing different methods, authors propose\nan open source runtime on which 1) specify the model, 2) describe the workflow and 3) evaluate the benchmark \nof several ML algorithms and frameworks.\nThe core of the work is the definition of the so-called 'model evaluation manifest' which consists of a formatted\ncollection of descriptive information where both hardware/software and framework versions are specified, along with\nthe set of tasks to be carried on as well as the data sources to test the methods against.\nOnce the manifest has been created, the desired hw/sw configuration is deployed on Amazon and the specified models are benchmarked.\nThis benchmarking offers several insights on the evaluation of a given ML model, by stressing out the importance of aspects that can severely bias  the final outcome of the model (e.g., pre-processing tasks, different hardware configurations or normalization of the data).\nTo describe the workflow, authors use an image classifier on a given hardware as a running example, and play with different  preprocessing methods to measure their impact on the final accuracy of the model.\n\nSome details are not well specified/clear in the work:\n1) Data exploitation. There is the possibility of testing different methods on own datasets. Given that the deployment is run on Amazon instances, what are the requirements (e.g., data must be on S3 and so on). \n2) The manifest can be injected with python scripts that, running in a container, perform the desired operations (preprocessing). It is stated that 'parameters are passed by reference'. So if you pass a 'mutable' object ('env', I guess) you need to bind it to the outher scope. How this is accomplished? (globals?)\nInstead, if you pass an 'immutable' object ('data', I guess), you cannot rebind the outer reference nor mutate the object. So, what's the meaning of 'passing by reference'?\n3) Privacy and anonymity. When performing debugging, system, framework and model level profiling information are collected on a tracing server. Is this server  part of the platform?", "belong_id": "ryx2wp4tvS"}, {"uid": "rJggCzLbcB", "paper_title": "MLModelScope: A Distributed Platform for ML Model Evaluation and Benchmarking at Scale", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies the question of model evaluation and reproducibility in machine learning research, specifically deep learning research, and designs and tests an extensive system for evaluating and comparing models. Users specify their evaluation parameters through a text file, and this is used by the runtime, which can be interfaced through a UI or the command line, in order to carry out the evaluation. Several experiments shed interesting light on various aspects of model, framework, and hardware performance.\n\nHypothetically, if I were to design a new model and wish to evaluate its performance relative to existing SotA models, I would potentially use this system to run all of the models, including my own. That would mean that I need to 'upload' or otherwise integrate my model into this system, and it was unclear from my reading of the paper how easy such a process would be. Similarly, I would wish to maintain similar training and evaluation conditions for my model, e.g., the same pre and post-processing, and that would involve 'extracting' those steps from the system for use during training. I would also like to understand whether or not this is feasible and easy given the system's design.\n\nIn section 3.1, the authors write 'The hardware details are not present in the manifest, but are user-provided options when performing the evaluation.' An example of how this operates would be useful in the paper.\n\nAs far as experiments go, I'm not sure what the main takeaway is from section 4.1. To me, the takeaway that pre-processing is important and existing models are sensitive to pre-processing is not a new finding. The results from Table 1 could certainly be obtained without the use of the proposed system, and though there would be some scaffolding involved, I don't think that the coding would not be particularly difficult. Is the takeaway that the proposed system makes it easier or faster to evaluate the effects of different types of pre-processing? Wouldn't this be most interesting at training time?\n\nI find the experiments in sections 4.2 and 4.3 interesting. In section 4.2, I'm not sure if figure 9 includes enough information or description to conclude that 'GPU instances in general are more cost-efficient than CPU instances for batched inference', and some more detail here would be useful.\n\nGenerally, I believe that the work is well-motivated and timely, the authors seem to have done a good job in citing related work (though admittedly I don't know much about this area), and the results are supportive of the claims of the system's usefulness.", "belong_id": "ryx2wp4tvS"}, {"uid": "r1xlT9d5oH", "paper_title": "MLModelScope: A Distributed Platform for ML Model Evaluation and Benchmarking at Scale", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "* Note: I highlight I did not assess the model design, which is the main contribution of the paper, and  did not know the background of prior work of system design to really assess the novelty of the work, my score is solely based on the experiments. \n I am an expert in machine learning/computer vision, so I could assess the experiments in terms of their validity and relevance from the machine learning/vision perspective, however, I may not be the best person to evaluate the design choices of the system. Therefore I choose the low experience for my background.\n\n* Paper summary: The paper proposes a framework to evaluate machine learning models in a hardware-agnostic way.\nTo evaluate the models using this framework, the user needs to specify the pre-processing, inference, and post-processing steps and the required software/hardware stack. The authors argue that this is important to consider the  HW/SW stack to allow a fair evaluation and reproducibility. Models are specified using a model specification called manifest.\n\n* The authors assume that SW/HW stack change the results of deep learning models a lot, and this is the main assumption in this work, however, normally in practice HW/SW stack wont change the results.\n\n* I found the experiments either not related to the point of the paper or being very trivial not helping to backing up the arguments of the paper. \n\n\n* In section 4.1, the authors consider different preprocessing operations and study their impact on the model performance, however, the fact that preprocessing impact the results is trivial in machine learning. In the same section, color layout and data layout, cropping and resizing, where the authors discussed about for instance how changing the data representation from NCHW or NHWC change the results, this is also trivial, because if you change the dimensions, you need to also change the model in a way that handles this change of dimension, therefore, this is clear that the results will change accordingly as well. Such experiments does not back up the main argument of the paper, which argues for fair evaluation between neural models, nor provides informative information to the reader.\n\nOn section 4.1, the experiment of type conversion and normalization, again this is mathematically clear that the order would change the results,  let's call imgByte=x, then by substituting given\nvalues for mean and standard evaluation, equation (b) is simplified to (b) = (x-127.5)/((127.5)*(255)) \nhowever simplification of (c) results in (x/255-0.5)/0.5 = (x-0.5*255)/(0.5*255)=(x-127.5)/(0.5*255) \nthe dominator of (b) and (c) are not equal, therefore, this is trivial that the results of these two \nthe expression would not be the same. The author posed it as a new finding, but this is trivial that mathematically\nthese two equations would not be equal. Again, this experiment does not add any value to the paper.\n\nIn section 4.2, in Figure 9, the authors show a plot of the CPU latency for different batch sizes and instances,\ntogether with GPU throughput for different batch sizes, i.e., images/seconds. The authors show latency for CPU\ninstances, versus throughput for GPU instance, since these two measures are not shown for both instances, this is \nnot supported from the text, how actually authors compare this two instance and draw the conclusion that which instance is more efficient since there is no value shown for CPU throughput. Apart from that, I don't see how this section and determining if GPU or CPU instances of  Amazon compute cloud is more cost-efficient is related to the point of this paper which is on reproducibility. Also please have a look at Amazon webpage:\nhttps://docs.aws.amazon.com/dlami/latest/devguide/gpu.html\nHere, they explicitly mention that 'A GPU instance is recommended for most deep learning purposes. Training new models will be faster on a GPU instance than a CPU instance. You can scale sub-linearly when you have multi-GPU instances or if you use distributed training across many instances with GPUs.',  so having this experiment again neither back up the arguments in the paper, nor add value to the paper.\n\n* The major issue with this submission is that the experiments are not related to the arguments of the paper, and are not conveying any message towards backing up the arguments of the paper.\n\n* Another crucial problem is that to allow a fair comparison especially in neural models, as shown in several studies(see [1] as a sample), this is important to account for random seeds and study how it impacts the model performance, to allow a fair evaluation of the models this is important to consider this factor, fair evaluation of models is argued to be the main point of this paper, however, the authors does not consider this factor in the paper, nor study it in the experiments.\n\n[1] Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks, Nils Reimers and Iryna Gurevych\n\n\n\n", "belong_id": "ryx2wp4tvS"}, {"uid": "rJxofaHa_r", "paper_title": "Batch Normalization is a Cause of Adversarial Vulnerability", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper identifies an important weakness of batch normalization: it increases adversarial vulnerability. It is very well written and the claims are theoretically sound. In the experiments, the authors demonstrated a significant difference in robustness between networks with or without batch normalization layers, in varies settings against both random input noise and adversarial noise. This weakness of batch norm was explained due to the 'decision boundary tilting' effect caused by the normalization. Overall, this paper has done solid work to reveal an interesting phenomenon. If it is true, this finding will impact almost all DNN models. \n\nMy concern is that this phenomenon is just another effect of 'gradient masking ' (as pointed out by Athalye, et al.). Batch norm is a well-known technique to avoid overfitting, without batch norm the network can be easily trained to be saturated with almost zero gradients, demonstrating a false signal of 'robustness' to noise. The random noise and real-world corruption experiments are definitely helpful to clear this doubt, but only partially. My concern remains because of two obvious signs of  gradient masking: \n1. The accuracy on PGD-li (epsilon=0.031) attacks are suspiciously too high (20% - 40% Table 3/4). For this level of attack, the acc should be nearly zero. This is likely caused by the gradient masking effect, considering the cifar-10 networks were trained for longer time with larger learning rate (150 epochs, fixed lr 0.01). Training on MNIST is much easier to get zero gradients.  \n2. The weight decay discussion is not helpful at all, on the contrary, it confirms my concern on the gradient masking effect. In Table 8, the robustness was increased ~40% by just using large weight decay. This is not the 'real robustness', and can be easily evaded by adaptive attack (see Athalye's paper).\nWith the above two concerns in mind, I doubt the phenomenon revealed in this paper is just 'one can easily train a saturated model without batch norm' or equivalently 'it's hard to train a saturated model with batch norm'. It is hard to say if this is a bad thing for batch norm.\n\nI am quite surprised that the authors ignore this completely. Here are a few things that can be done to rule out the possibility of gradient masking. The masked gradient can be identified by: 1) One-step attacks perform better than iterative attacks; 2) Unbounded attacks do not reach 100% success., etc (see Section 3.1 of Athalye's paper).\n1. Including FGSM in the experiments and show the same trends as PGD-li. \n2. Show two networks have similar gradient norms.\n3. Apply cw-l2 attack, and show batch norm has forced large perturbation.\n\nTwo other suggestions:\n1. Summarize the different angles/steps taken to verify the phenomenon, somewhere before the experiments.\n2. Cannot see why the input dimension discussion contribute to explanations of the batch norm weakness.\n\n============\nMy rating stays the same after rebuttal. \n\nMy original concerns are like the other reviewers: why BN, not other techniques such as structure of DNNs MLP vs CNN vs ResNet, activation functions, weight decay, learning rates, softmax etc. My initial suspect was that it is caused by gradient masking likely caused by the l2 weight regularization, so asked the authors to look at the gradient norms and run some testes to rule this out. Yes, the weight norm is directly related to the Lipschitz continuity of the function represented by the network, but it often becomes more complicated on complex nonlinear neural networks. \n\nAccording to the new experiment results, the vulnerability is indeed not an effect of gradient masking, thanks for the clarification. However, the new results also indicate that the finding is susceptible to both weight decay and learning rate: in Figure 16 (a): 'Un PGD' < 'BN PGD' before learning rate decay, andFigure 17 (a) vs (b), doubling the weight decay penalty to 1e-3 also increases the vulnerability of BN. Overall, I believe the phenomenon exists, but the reasons behind requires more explanations, at least not just the batch norm.", "belong_id": "H1x-3xSKDr"}, {"uid": "rJg7I5MztH", "paper_title": "Batch Normalization is a Cause of Adversarial Vulnerability", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nIn this empirical study, the authors identify that batch normalization -- a common technique for accelerating training -- leads to brittle representations that exhibit a lack of robustness and are more susceptible to adversarial attacks. The authors demonstrate their results on SVHN, CIFAR-10, CIFAR-100 CIFAR-10.1 using a variety of network architectures including VGG, BagNet, WideResNet, AlexNet, etc.\n\nMajor Concerns:\n\n1. As presented, the experiments are not convincing.\n\nI do not know how much of the changes in adversarial vulnerability are due to batch normalization as opposed to other facets of the training procedure that may have changed in their BN vs no-BN experiments.\n\nFor instance, batch normalization usually accommodates higher learning rates and it is not clear if the authors adjusted the initial learning rate, learning rate schedule or training schedule accordingly. If so, it would be important to run a set of experiments with these parameters fixed as per the baseline no-BN models. \n\nThat said, even if the authors did run these experiments, it is still not clear if the cause of adversarial vulnerability is due to BN. Consider that what is truly important in model training is not the learning rate (i.e. step size), but rather the magnitude of the changes in each weight (or the ratio of weight change to the weight). By swapping in batch normalization, the authors may just be altering the norm of the weight change in the (re-parameterized) weights. In this scenario, the gains of removing batch normalization could just as well be explained by the effective change in the learning rate, and not about batch normalization itself, c.f.\n  Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks\n  Yuanzhi Li, Colin Wei, Tengyu Ma\n  https://arxiv.org/abs/1907.04595\nIf the differences in the adversarial vulnerability could be ascribed to effective changes in gradient updates, then this would change the interpretation of these results notably.\n\n2. The underlying hypothesis is specious.\n\nI have several reservations about the underlying hypothesis that requires stronger evidence to overcome. In particular, I have reservations in believing that BN itself is a cause of adversarial vulnerability because BN is just a factorization of a network's weights. That is, there is nothing 'special' nor unique about BN-networks; instead, the BN factorization merely permits accelerated training efficiency.\n\nConsider the fact that a BN model may be re-expressed by merely folding in the parameters (i.e. applying the matrix multiplications) into the MLP weights or CNN filters. Thus, the numerical function approximated by the BN and the 'folded' non-BN model is identical. What would it mean to say that the BN is 'causing' adversarial vulnerability in the BN model given that both the BN and non-BN model perform the identical function?\n\nAnother way to say this is to pretend we train a non-BN MLP or CNN model. After training the model, we could apply a BN factorization of the weights. Thus, the non-BN model may be factorized into a BN model. If the resulting BN model were adversarial vulnerable (which I suspect is the case), it would seem very hard to believe that BN was the cause of the vulnerability given it was a post-hoc factorization of the weights.\n\nThat said, I could definitely imagine that the training procedure itself could lead to adversarial vulnerability (e.g. citation above) and by employing a BN factorization, one may be encouraged to use a training procedure which leads to increased vulnerability. I would encourage the authors to consider this line of attack and thus, re-orient their analysis and discussion accordingly.\n\n3. The title is poorly worded.\n\nNot withstanding the point above, adversarial vulnerability predates BN. Likewise, non-BN models exhibit adversarial vulnerability. Thus, this title is not a great reflection of the findings of the paper. I would strongly suggest replacing 'is a cause' with 'increases' or 'exacerbates'.", "belong_id": "H1x-3xSKDr"}, {"uid": "S1lNkzJ2YB", "paper_title": "Batch Normalization is a Cause of Adversarial Vulnerability", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Overview:\nThis is an interesting work. The paper is dedicated to studying the effect of BN to network robustness. The author shows that BN can reduce network robustness to small adversarial input perturbations and common corruptions by double-digit percentages. Then, they use a linear 'toy model' to explain the mechanism that the actual cause is the tilting of the decision boundary. Moreover, the author conducts extensive experiments on popular datasets to show the robustness margin with or without the BN module. Finally, the author finds that substituting weight decay for BN is good enough to nullify a relationship between adversarial vulnerability and the input resolution.\n\nStrength Bullets:\n1. I like the linear toy example. For that binary classification example, the author explicitly explains the boundary tilting, which increases the adversarial vulnerability of the model. It is clear.\n2. The paper conducts extensive experiment on SVHN, MNIST, CIFAR10 (C) datasets. And they show performance margin with or without the BN module. And for the attacker setting, they do use the popular setting (i.e. Mardy's PGD setting) in this field which makes the results more convincing.\n\nWeakness Bullets:\n1. Why do not visualize the decision boundary of networks (used in this work) to valid the boundary tilting 'theory'. The toy example is clear but not convincing enough. There exist several techniques may be helpful to the visualization. (i.e. Robustness via curvature regularization, and vice versa). I think it is one of the important parts of this work. The observation of BN causes adversarial vulnerability is interesting but the main focus should be offering more convincing explanations.\n2. I do run experiments for VGG11,13,16,19 on cifar10 with PGD 3 attack (Mardy's setting). There exist ~20 ATA performance gaps between networks with BN and BN networks without BN. But for adversarial trained models, the gaps don't exist anymore, at least for VGG11,13,16,19 on cifar10 with PGD 3 attack. (The performance gap is less than 0.5). In other words, without the BN layers, the robustness of adversarially trained models will not increase in my experiments. I see you report some results in Appendix C. But it is not enough to convince me. Could you provide more implementation details about the adversarial training and attacker? And more experiment results about this point are needed. If adversarial training can fix the vulnerability by BN and BN can give a TA boost, there is no reason we need to remove BN in our adversarial training setting. I see there are similar concerns in the OpenReview.\n3. [Minior] The experiments need to be organized better. Especially for section 3, it will be better to divide different experiments or observations into the different subsections.\n\n\nRecommendations:\nFor the above weakness bullets, this is a week reject.\n\nSuggestions:\n1. To solve the weakness bullets;\n2. minor suggestion: add the reference mention in the OpenReview, they are related to this work.\n\nQuestions:\n1. You mention that you run PGD for 20-40 iterations in the experiment at the bottom of page three. But at each table, you only report one number. So my question is for that accuracy number, you run how many iterations for PGD?", "belong_id": "H1x-3xSKDr"}, {"uid": "ryxyH98XYr", "paper_title": "Disentangling Factors of Variations Using Few Labels", "experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "After rebuttal edit:\nNo clarifications were made, so I keep my score as is.\n\n------------------------------------------------------\nClaims: Explicitly requiring a small number of labels allows for successful learning of disentangled features by either using them as a validation set for hyper parameter tuning, or using them as a supervised loss. \n\nDecision: Reject. This paper needs a substantial rewrite to make clear what specific contributions are from the multitude of experiments run in this study. As is, the two contributions stated in the introduction are both obvious and not particularly significant -- that having some labels of the type of disentanglement desired helps when used as a validation set and as a small number of labels for learning a disentangled representation space. There are no obviously stated conclusions about which types of labels are better than others (4.2). Section 3.2 seems to have some interesting findings that small scale supervision can help significantly and fine-grained labeling is not necessarily needed, but I don't understand why that finding is presented there when Fig. 4 seems to perform a similar experiment on types of labels with no conclusion based on its results. Conclusion sentence of 4.3 is hard to decipher, but I assume is just saying S^2/S beats U/S even when S^2/S is subject to noisy labels. Overall, I find it very difficult to absorb the huge amount of results and find the analysis not well presented.\n\n", "belong_id": "SygagpEKwB"}, {"uid": "SJeN77VTYH", "paper_title": "Disentangling Factors of Variations Using Few Labels", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the challenge of learning disentangled representations---i.e. learning representations of data points x, r(x), that capture the factors of variation in an underlying latent variable z that controls the generating process of x---and studies two approaches for integrated a small number of data points manually labeled with z (or noisier variants thereof): one using these to perform model selection, and another incorporating them into unsupervised representation learning via an additional supervised loss term.  This investigation is motivated by recent results concluding that inductive biases are needed otherwise learning disentangled representations in an unsupervised fashion is impossible.  The paper poses its overall goal as making this injection of inductive biases explicit via a small number (~100 even) of (potentially noisy) labels, and reports on exhaustive experiments on four datasets.\n\nI think that this paper merits acceptance because (a) the motivation of taking a necessity in practice (somehow selecting models / injecting inductive biases) and making it more explicit in the approach is a good one, and because the thorough empirical survey (and simple, but novel, contribution of a new semi-supervised representation learning objective) are likely valuable contributions to this community.\n\nOne negative comment overall would be that the results are not that surprising: that is, the fact that using labels either (a) to do model validation or (b) in a semi-supervised fashion would help is not too surprising.  However, I believe in the context of (a) making more explicit a practical (and theoretically) necessary step in the pipeline of learning representations, and (b) contributing a comprehensive empirical study, this is a worthwhile contribution.\n\nMinor notes:\n- Fig. 1 isn't the most intuitive- ideally would be better explained for the headlining figure\n- 8.57 P100 GPU years is ~= $75k based on a cursory glance at cloud instance pricing at monthly rates... this is a lot to reproduce these experiments...", "belong_id": "SygagpEKwB"}, {"uid": "r1x0CupCYH", "paper_title": "Disentangling Factors of Variations Using Few Labels", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents evidence that even a tiny bit of supervision over the factors of variation in a dataset presented in the form of semi-supervised training labels or for unsupervised model selection, can result in models that learn disentangled representations. The authors perform a thorough sweep over multiple datasets, different models classes and ways to provide labeled information. Overall, this work is a well executed and rigorous empirical study on the state of disentangled representation learning. I think experimental protocol and models trained models will prove extremely useful for future work and advocate for accepting this paper.\n\nComments\n\n1) Would it be possible to use the few labeled factors of variation in a meta-learning setup rather than as a regularizer?\n\n2) The paper provides high level conclusions about the impact of having supervised model selection or semi-supervised learning in models in general, but doesnt offer much discussion into their behavior under specific settings (i.e.) it seems to be hard to pick a winner amongst presented model. Some are better with 100 labeled examples but dont scale as well as others when an order of magnitude more labeled data is available. It is certainly hard to discuss all the thousands of experimental observations, but the paper can benefit from some more fine-grained analysis.\n\nMinor\nFigure 4 is hard to comprehend without a model to index mapping similar to Figure 3", "belong_id": "SygagpEKwB"}, {"uid": "rJgEXQv6tS", "paper_title": "A Gradient-Based Approach to Neural Networks Structure Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a new neural network architecture, in which all neurons (called 'floating neurons') are essentially endowed with 'input' and 'output' embedding vectors, the product of which defines the weight of the connection between any two neurons. The authors discuss two network architectures employing floating neurons: (a) multi-layer floating neural networks and (b) farfalle neural network (FNN), in which there is one hidden layer, but additional recurrent connections are introduced between the hidden neurons.\n\nAs mentioned by the authors, the proposed architecture is similar to architectures employing low-rank weight matrix factorization. In my opinion, the main novelty lies in: (a) 'floating neuron' interpretation, (b) additional weight matrix normalization, and (c) FNN architecture similar to that of a 'floating neuron' RNN network with additional restrictions.\n\nI find the proposed idea to be promising and quite intriguing, but I think that the paper has some room for improvement and provided empirical evidence might be insufficient (including for understanding the importance of individual model components), which in turn makes the claims of potential practical attractiveness less justified. I will be happy to update the final score provided with more compelling arguments or empirical analysis of the proposed architecture.\n\nAddressing the following issues might greatly improve the quality of the paper:\n\n1. In Section 4.1, the authors compare FNN and DNN on MNIST and CIFAR10 datasets. My concern is that the authors pick a seemingly arbitrary DNN architecture (just a single one) and restrict comparison to it. One issue is that ~50% accuracy on CIFAR10 can be easily demonstrated by a variety of 5-layer DNN architectures including those much smaller, with just ~600k parameters (!) and possibly even lower. This makes the 90% parameter reduction claim not particularly meaningful. And why were models matched based on the total number of neurons, but not, say, the total number of parameters, or other measures? I believe that these questions require additional discussion and empirical evidence. Just as an example, if it was possible to sample (potentially randomly) different DNN architectures (with a reasonable parameter prior) and compare them with FNNs on a 2D accuracy-parameters plot (or using other important metrics), it would provide much more information to the reader.\n\n2. Another important point that I would like to make is that there is much more that can be done to explore the hyper-parameter space of FNN to isolate which particular factors play a decisive role in its superior accuracy. The authors present us with a specific choice of the normalization function, and values of k and d, but it would be very informative to study how results change when different choices are considered. FNNs differ from DNNs in at least three aspects: usage of low-rank factorization, weight normalization and recurrent structure. How important are these individual aspects? Are some of them redundant, or almost redundant, or do FNNs require all of these components to achieve their peak performance? In other words, I believe that a careful ablation study would greatly improve this publication.\n\n3. As a minor note, I think that the statement that FNNs 'are more general' than floating neural networks is only partially correct. If I am not mistaken, FNN can also be 'unrolled' and represented as a multi-layer floating neural network with additional parameter sharing. Also, the computational complexity of the constructed FNN (in Theorem 1) appears to be significantly higher than that of the floating neural network (especially for high l). This would imply that FNNs do not necessarily supersede multi-layer floating neural networks, at least when the computational complexity is of importance.\n\n4. There are a few minor misprints throughout the text. For example, in '0<j<=j' in the proof of Theorem 1, or in 'R output floating neurons for the final deduction from hidden neuron' (output should be S). Also, I could not find information about the value of d used in the described experiments (which I estimated to be 256; is this correct?).\n\n5. In Section 4.3, the authors propose to use FNNs for the final layers of conventional CNN architectures. The issue is that the VGG16 network chosen for experiments was probably picked because it uses several large fully-connected (FC) layers in its tail whereas all more recent and efficient CNN architectures actually gravitate towards a smaller single FC layer. It is possible that FNNs could still be used in FC layers of these modern networks as well (especially with a large number of classes). But additional empirical results for these architectures would, in my opinion, be much more convincing.\n\nUpdated: The authors updated the text and addressed many of my questions. In my opinion, this improved the paper and made some of its claims much better justified. I change the rating to 'Weak Accept'.", "belong_id": "Bye-sxHFwB"}, {"uid": "HklEgucTtH", "paper_title": "A Gradient-Based Approach to Neural Networks Structure Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Overall the paper is easy to read and I welcome that.\n\nI like the idea of using node-level embedding instead of pairwise weights to learn a low-rank weight representation. However, I am more skeptical about using this in a recurrent architecture and claiming that this is structure learning. The empirical results do not provide sufficient evidence that this performs structure learning.\n\n1. Theorem 1 seems rather straightforward because the FNN has much more representational power in the sense that its number of parameters is O(Nld) whereas the multi-layer version has O(Nd/l) parameters (in the uniform-width case). \n\n -- A more interesting question when it comes to structure learning is this: Suppose the best architecture for task A is shallow-and-wide while for task B is deep-and-narrow, each requiring roughly the same number of parameters. Can I use the proposed FNN with a similar number of parameters to learn the corresponding architecture for A and B respectively, without the need to figure out which is which? There is no evidence, analytical nor empirical, in this work, that suggests that this is the case.\n\n2. Section 4. It would be interesting to try baselines that have roughly the same number of parameters as the proposed FNN. Also, the choice of d (embedding size) and the number of iterations can be viewed as making architectural decisions. How were they chosen? Assuming that the same amount of computational resource is spent on searching through baseline architectures as well, could the results have been different from those in Table 1?\n\nThere are interesting ideas in this work but in its present form I cannot yet recommend acceptance.\n", "belong_id": "Bye-sxHFwB"}, {"uid": "rylhQa4CKB", "paper_title": "A Gradient-Based Approach to Neural Networks Structure Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new architecture based on attention model to replace the fully-connected layers. In this architecture, each neuron is associated with an embedding vector, based on which the attention scores (between two consecutive layers) are calculated, and the computational flow through the layers are derived based on these attention scores. The experiments on MNIST and CIFAR demonstrate some degree of superiority over plan FC layers.\n\nPros:\n\n1. The idea is indeed interesting and AFAIK, there is no prior works trying to derive embedding for each neuron. The embedding based connection might encourage other follow-up works.\n\nCons:\n\n1. The writing sometimes seems unnecessarily complicated. For example, the iteration in section 3.3 is actually layer, right?  I furthermore see no motivation of listing the four items in this section, even the whole section 3.3: they are just re-stating the feedforward process of FNN. \n2. I donot believe FC is essential in modern computer vision (CV) tasks, so the better performance over a plain FFN on CV tasks are not that convincing (especially the two datasets are typically regarded as debugging dataset nowadays). I suggest the authors conduct more experiments on Transformer based tasks (e.g., machine translation), since in Transformer, the FFN is quite important. If the replace of FFN using the proposed FNN is successful for Transformer on some large scale task (e.g., WMT14 En-De Translation), this work will be much stronger in terms of empirical performance.\n\nQuestion:\n\n1. What is the embedding size d in the experiments? If d is large, the complexity comparison in the last paragraph of section 3.2 will not make too much sense.\n\n", "belong_id": "Bye-sxHFwB"}, {"uid": "Byg8y2bUKr", "paper_title": "Model Based Reinforcement Learning for Atari", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThis paper proposes a model-based reinforcement learning algorithm suitable for high-dimensional visual environments like Atari. The algorithmic loop is conceptually simple and comprises 1) collecting real data with the current policy 2) updating an environment model with old and newly acquired data and 3) updating the policy 'virtually' inside of the environment model using PPO. The approach is evaluated on 26 games from the Atari benchmark and compared against the model-free baselines Rainbow DQN and PPO. The newly proposed model-based method clearly outperforms both model-free baselines in low training regimes (100,000 steps). Further ablation studies are provided, e.g. similar results are obtained in a more stochastic setting of ALE with sticky actions.\n\nQuality\n\nThis paper has a strong applied focus and needs to be judged based on its experiments. The quality of those are high. The method is evaluated on a suite of 26 games, compared to strong model-free baselines and results are averaged over 5 seeds. One concern I have is that the method is only evaluated in low training regimes. While I do understand that increasing the training horizon is computationally demanding, results in the appendix (Figure 11a) indicate that the proposed model-based method has worse asymptotic performance compared to the model-free baselines. After 500,000 training steps the effect of sample efficiency vanishes and the final performance results are far away from the final performance results of the model-free baselines after 50,000,000 training steps. Also, a plot similar to Figure 11a) from the appendix for Rainbow DQN would be good (but I do understand this might be difficult to obtain in the course of the review period should this require more experiments).\n\nClarity\n\nThe paper is clearly written and easy to follow. However, the authors could state in the main paper more clearly that their method excels in low training regimes and that the sample efficiency effect seems to vanish when increasing training iterations from 100,000 to 500,000 steps. In fact, Figure 11a) from the appendix should go into the main paper, and it should be also mentioned that there is a huge discrepancy between the maximum performance achieved by the proposed model-based method and the maximum performance achieved by the model-free baselines when training for 50,000,000 steps. Based on the experiments, it is not clear at all if the new method will eventually catch up with best model-free results from the literature when training time is increased, or stall in low-performance regimes indefinitely.\n\nOriginality\n\nThe originality of this paper is not very high since the proposed algorithm and its components are not novel (there might be some minor novelty in the environment model architecture). However, this paper should not be judged based on its originality but based on its significance. \n\nSignificance\n\nA working model-based RL algorithm for Atari is clearly a huge gap in the current literature and this paper takes an important step towards this direction. Demonstrating improved sample efficiency compared to strong model-free baselines in low training regimes is a significant result. The significance is however decreased by the fact that the paper does not answer the question how to obtain good asymptotic performance that matches (or comes close to) model-free state-of-the-art results. I therefore vote for weak accept at this stage.\n\nMinor details\n\nOn a side note, there are two citations missing related to model-based RL in visual domains:\n- S. Alaniz. Deep Reinforcement Learning with Model Learning and Monte Carlo Tree Search in Minecraft. In the 3rd Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM), 2017.\n- F. Leibfried and P. Vrancx. Model-Based Regularization for Deep Reinforcement Learning with Transcoder Networks. In NIPS Deep Reinforcement Learning Workshop, 2018.\n\nUpdate\n\nAfter the author response, my review remains the same. I think this paper is worthwhile publishing at ICLR.", "belong_id": "S1xCPJHtDB"}, {"uid": "SJg3esx3tH", "paper_title": "Model Based Reinforcement Learning for Atari", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper covers the authors approach to learning a model of a game, which can then be used to train a reinforcement learning agent. The major benefit of the approach is that instead of requiring millions of training steps in the game, the model can be constructed with only 1-2 hours of footage, and then train in the simulated game for millions of training steps. \n\nThis is a well-written paper, and the results are very impressive. The approach builds upon prior work with the same general thrust, but broadly makes clear that it stands above these existing approaches. However, I would have appreciated some clarity in the comparison made to the work of Ha and Schmidhuber (2018). It is unclear if the difference given is just because of the environments employed by Ha and Schmidhuber or if the authors see the approach presented in this paper as fundamentally different or improved in some way. \n\nMy one major technical concern comes down to how this work is framed and what that implies about appropriate baselines. The authors state clearly that the benefit of this work is that it can learn a sufficient model of a game in only 1-2 hours of gameplay footage. As I said above that is very impressive. However, the agents then requires 15.2 million interactions in this environment to learn to play the game. I would have appreciated some clarity then in the computational resource cost in this approach as opposed to just training say Rainbow in the actual game environments with 15.2 million interactions. Its also not clear if optimizing Rainbows performance on 1M steps is a fair comparison. Ideally I would have liked to have seen some variance in the amount of time Rainbow was trained for compared to the associated computational costs. Clarity on this especially in sections like 6.1 would help readers better grasp the tradeoffs of the approach.\n\nThe authors could have also included reference to non-DNN work on learning forward/engine/world models that were then used to play or reason about the game. For example Guzdial and Riedls 2017 Game Engine Learning from Gameplay Video on Super Mario Bros. or Ersen and Sariels 2015 Learning behaviors of and interactions among objects through spatiotemporal reasoning on a novel game.\n", "belong_id": "S1xCPJHtDB"}, {"uid": "H1lwnmopFH", "paper_title": "Model Based Reinforcement Learning for Atari", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper addresses sample-efficient learning (~2 hours of gameplay equivalent) for Atari (ALE) games. Building on the idea of training in a learned world model and the use of a u-net next-frame predictor, the approach is claimed to yield almost comparable performance to other models with only a fraction of the true-environment experience.\n\nSample efficiency is a major concern for DRL, particularly with an eye towards robotics and other physical domains. Although the approach is rather specific to the shapes and qualities of data in the ALE setting, the work is motivated at a high level, and the specific techniques for predicting the next frame explained in the past are explained.\n\nThis reviewer moves for a weak accept on account that the paper is well written (with quite thorough experiments explaining improvements in sample efficiency and possible limits in final task performance) but specifically targets ALE where execution is so cheap. The total number of PPO updates made in the new approach is not much reduced from before even if the number of trajectories evaluated in the true environment is very much reduced. On the problem of how much RL itself is sample efficient, not much progress is made.\n\nQuestion:\n- What is the impact on total wall-clock training time when using this approach? Given that the technique is centered on ALE, the characteristics of ALE compared to the learned world model are relevant (ALE executes very quickly and easily parallelizes whereas the learned world model presumably only runs where you have a GPU).\n- Can this approach be stacked to benefit from training in a lighter-weight approximate model (env'') of the world model (env')?", "belong_id": "S1xCPJHtDB"}, {"uid": "HJew1MGhFr", "paper_title": "INSTANCE CROSS ENTROPY FOR DEEP METRIC LEARNING", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a method inspired by the categorical cross entropy (CCE). In a similar way as many metric learning approaches, a softmax approach based on cross entropy is used to enforce similar examples to have smaller distances than dissimilar distances. \nNonetheless, instead of considering a centroid-based deep metric learning approach (e.g. like Snell et al. (2017)), only one anchor is considered for each training example, and the distance of an anchor with one of its positive is learned that its distance is smaller than the distance with examples belonging to different categories.\n\nI vote for reject for the following reasons:\n- The paper is too similar to NCA and S-NCA (introduced in Section 2.2). In the same way as ICE, S-NCA also considers learning l2-regularized representations. The main difference with S-NCA is the way negative examples are sampled. \nS-NCA proposes a framework based on augmented memory, ICE proposes a sampling strategy similar to [A] and used in Nickel et al. (2018) to subsample negative pairs.\n- Another contribution of ICE is the use of some hyperparameter s in Equations (11) and (12). This hyperparameter s plays the role as the inverse of the temperature and is in fact learned in ICE. S-NCA also plays with the temperature but does not learn it. \nOn the other hand, learning the temperature in a similar way has been proposed in the deep metric learning literature (e.g. TADAM).\n\nOn the other hand, the paper has some nice contributions:\n- The main 'novelty' of ICE seems to be the analysis in Section 3.4 of the partial derivatives and their impact on the sample reweighting. Although the explanation is simple, it helps understand what's happening during optimization.\n- The experimental results on different transfer learning benchmarks seem convincing.\n\n\n[A] Jean et al. On usingvery large target vocabulary for neural machine translation. ACL 2015", "belong_id": "BJeguTEKDB"}, {"uid": "ryxSBCNAFr", "paper_title": "INSTANCE CROSS ENTROPY FOR DEEP METRIC LEARNING", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a method to measure the difference between an\nestimated instance-level matching distribution and its ground-truth\none, based on instance cross entropy (ICE).\nThe goal is to learn an embedding that captures the semantic\nsimilarities among samples. \nIn particular, with ICE they try to maximize the matching probability\nof an instance with similar instances (same class).\nThe authors also use sample re-weighting into ICE and show the benefits\nof the approach against other state-of-the-art methods on three\ndatasets. The authors performed several experiments with convincing\nresults.\n\nThe positive aspect of the paper is introducing this instance-based\nmeasure which is shown to perform well on 3 challenging datasets.\n\nIt is not clear how is the scaling parameter (s) determined. Are there\nany guidelines for fixing its value?\nThe authors should explain in more detail how is the non-linear\ntransformation achieved.\n\nThe algorithm goes through all the examples of a class on each\nmini-batch, which seems a computational expensive procedure. The paper\nwill benefit if time results are reported for different approaches.\n\nThe paper is sometimes difficult to follow and needs a careful\nrevision. \n", "belong_id": "BJeguTEKDB"}, {"uid": "Hkg88mzdsr", "paper_title": "INSTANCE CROSS ENTROPY FOR DEEP METRIC LEARNING", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Overview  \n\nThis paper proposes a new objective function called Instance Cross Entropy (ICE) for metric learning. Compared to the triplet loss (or contrastive loss) and its many variants, the distance between points in feature space is defined to be the dot/inner product, rather than computing the euclidian distance between feature vectors. Since the L2 norm of the final features is constrained  to be 1, the dot product represents the cosine distance between 2 feature vectors. Compared to the softmax/categorical loss, this objective has the advantage that there is no need to learn a per-class weight vector in the output softmax layer and therefore this method can be used even when the number of classes during training is unbounded. This is a very useful design feature, since the output softmax layer grows linearly with the size of the classes and can quickly become prohibitively large. Furthermore, mini batches for training can be randomly sampled without requiring expensive negative data mining strategies which are necessary when using the triplet loss. This is another very useful property. \n\nOverall, I think the ideas presented are interesting and the paper provides a useful summary of existing approaches for metric learning. However, I think the technical presentation needs to be improved significantly before  the paper can be accepted for publication. I found the paper quite difficult to follow. I had to re-read the motivation and the description of the loss function many times before being able to understand what is going on and how the proposed loss compares and contrasts with existing approaches. I also think some of the terminology and notation used in the paper is very confusing and should be updated to help the reader get to the main argument quickly. \n\nComments\n\n\n1. Title: What exactly does instance cross entropy (ICE) mean? And how is it different from categorical cross entropy (CCE)? Isnt categorical cross-entropy also calculating the entropy between the predicted distribution for *each instance* and the ground truth? I think the authors should reconsider the name for the proposed loss and choose a more descriptive name for the algorithm.\n\n2.  I also found the use of the term matching distribution a bit confusing since it has a well-defined meaning in statistics. However ML is full of overloaded terms and I understand if the authors want to keep this description. \n\n3. None of the references in the main text have brackets around them. I think this makes the paper appear very cluttered and makes parsing each paragraph quite difficult. I would highly recommend the paper be reformatted and the authors use brackets around references. \n\n4. In the abstract, the authors mention that the proposed method has a clear probabilistic interpretation. From my understanding, I see ICE as a blend of the categorical cross-entropy loss and the triplet loss, retaining useful properties of each without increasing the complexity of the loss computation. However, I do not see a clear probabilistic interpretation of the loss function. The softmax computation yields a probability distribution like output given a query and an anchor point, which is hand-designed in the objective function. I  think the authors should more clearly motivate what the probabilistic angle for the loss function is. \n\n5. I found Figure 1 to be very difficult to interpret. This is a missed opportunity, since this figure alone could communicate some of the key ideas in the paper to the reader. However, there are many details that have not been explicitly mentioned. What do the colours (white, blue, yellow) mean? The figure shapes probably mean classes. What do the terms p_* mean? What do i,j mean in Figure 1 e and how are they different from the previous figures. These details should all be present in the title for the figure. Even with these details, the figure could still be a bit more explicit. I found the yellow arrow with cross entropy and the notion of ground truth in the figure difficult to follow. \n\n6. How was the constrained optimisation performed? In Equation 6, we see that there is a constraint associated with each example in the mini batch which says the norm of the feature vector should be equal to 1. However I could not find any implementation details of how this constraint was satisfied. Did the authors use a Lagrangian formulation? I think the paper is irreproducible without this detail.  \n\nSummary\n\nI think this is an interesting proposal to combine the useful features of the softmax and triplet losses. However, I think the technical presentation needs to be improved significantly in order for the paper to be accepted for publication. \n", "belong_id": "BJeguTEKDB"}, {"uid": "SkgJL1KqKr", "paper_title": "Finding Deep Local Optima Using Network Pruning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a few sets of experiments related to deep local optima. The first set of experiments are to study the local minima of two layer neural networks with ReLU activation for the hidden layer and specialized for XOR problem. The paper claims that it is quite easy to find a deep local minimum with good generalization when the number of irrelevant features is small, and it becomes harder to find a deep minimum with good generation as the number of irrelevant features increases. It also claims there is a large difference between the test AUC of the best local minimum and the worse one if the training data is difficult. \n\nThe second experiment set is about pruning fully connected neural networks to find deeper and better optima. The proposed pruning method employs a annealing schedule and iteratively pruning connections to reduce features and nodes. For XOR datasets, the pruning seems be effective. For several real datasets, pruned models are better than original or equivalent models. \n\nOne thing concerns me is that there are a lot of experiment settings seem to be arbitrary. For instance, why use 500 hidden nodes, p is 4, 16, then 100, ...It will be better to explain why those setting are representative so the statements derived from those are valid. \n\nFor Figure 1 and 2, why switch sequence? The top 3 subfigures in Figure 1 is AUC, but the top 3 subfigures of Figure 2 is Loss. It is a bit confusing. \n\nThe paper is interesting and the experiments are comprehensive. I think the results and conclusion are specific for FC networks. It will be more interesting to study on CNN, etc.  Overall, I am a bit concerned with the significance of this paper. \n\n\n", "belong_id": "SyeHPgHFDr"}, {"uid": "H1exfgYCYB", "paper_title": "Finding Deep Local Optima Using Network Pruning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This submission studies losses at local minima of a set of neural networks trained on an XOR-like synthetic dataset, finds that local minima are of varying quality, and proposes a network pruning method to find better local minima. The pruning method is evaluated on XOR-like datasets as well as real-world datasets. \n\nThe use of an XOR-like dataset to study loss landscapes is interesting, making for a controlled and analyzable setting to carry out the study. The way the authors set it up, the XOR-like problem involves nuisance variables that naturally introduce suboptimal local minima into the loss landscape (this is my observation as a reviewer -- I am not sure if the authors were aware of this). I am unsure if Section 2 of the paper was intended as a core contribution or as a motivation for the pruning algorithm proposed in Section 3. Given the set-ups simplicity, a short theoretical argument (maybe even a theorem) about the quality and number of local minima one would expect to find could have been more concise and compelling than the empirical analysis from the paper. The findings from Section 2 may not be surprising enough to warrant two full pages. \n\nSection 3 proposes a network pruning method to find better local minima. The authors cite a paper by Adrian Barbu as the inspiration for their pruning algorithm with annealing, and use it to improve the capability of NNs to find a deep local minimum even when there are irrelevant variables. The cited paper by Barbu as well as https://arxiv.org/pdf/1805.01930.pdf (also by Adrian Barbu, not cited, maybe because it appeared) explore feature selection and regularization with (nearly) the same annealed pruning algorithm in some detail. I would be grateful if the authors could highlight the differences between their work and Barbus. \n\nI vote to weak reject this paper. The paper discusses interesting ideas, but other ICLR submissions present deeper and more novel material, and there appears to be some (unintentional, I believe) overlap with already-published work. I recommend that the authors cite and discuss https://arxiv.org/pdf/1805.01930.pdf , and possibly submit the paper at a less competitive conference. \n\n\nFurther comments / questions / advice\n=================================\n\n- It would be helpful if the authors made more clear what they consider the key contributions of their paper. If contributions build directly on earlier work, its helpful to highlight the differences. \n\n- Section 4.2 states that datasets were carefully selected in what sounds like a case-by-case basis, probably with the goal of finding data sets on which CPNA outperforms networks trained with vanilla gradient descent methods. This process would have selection bias and surface data sets on which CPNA outperforms. I could be grateful if the authors could clarify if this was indeed the process, or if a less biased criterion was used. For example, one could have chosen data sets on which a 1-layer fully connected neural network achieves between 50% and 90% F-1. \n\n- A reader of the paper might wonder for what data sets they should use CPNA in order to train network that achieves low out-of-sample loss. I could be grateful if the authors could comment on this. Following up on the previous point: it would be great the authors could include data sets where CPNA does not outperform. \n\n- Could the authors include information on how long training takes for the experiments from Table 3? \n\n- https://openreview.net/pdf?id=HkghWScuoQ should probably be cited\n\n- https://arxiv.org/pdf/1805.01930.pdf should definitely be cited", "belong_id": "SyeHPgHFDr"}, {"uid": "H1lOu8zkcS", "paper_title": "Finding Deep Local Optima Using Network Pruning", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper addresses the very important topic of local minima in Deep Learning. This is one of the central questions in the theory of Deep Learning for the last years, and despite many interesting results the main questions remain wide open.\nThe reviewer really likes the approach proposed in the paper, to use a simple model and an artificially generated data to study a certain phenomenon. The reviewer represents the opinion that more focus on such setups would greatly benefit the community in terms of progressing the theoretical understanding.\nThe claim made in the paper that there is a relationship between the number/suboptimality of local minima  and the scarcity of the data is both convincing and interesting. The result is well motivated and explained.\nWhat the reviewer thinks the paper would greatly benefit from would be improving the Related Work section. There was a lot of valuable work in the field done in the past years that ids very relevant to the results presented that is not mentioned.", "belong_id": "SyeHPgHFDr"}, {"uid": "B1eyRil5tr", "paper_title": "Model-free Learning Control of Nonlinear Stochastic Systems with Stability Guarantee", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n\n######## Rebuttal Response:\n\nThanks for the thorough response.\n\nQ2: The title still hasnt changed on the current draft\nQ4: To be more precise: \na novel data-based approach for analyzing the stability of the closed-loop system is proposed by constructing a Lyapunov function parameterized by deep neural network - this alone is not novel, you would need to specify how your method of doing this is new \na practical learning algorithm is designed to search the stability guaranteed controller - this is a natural consequence of contribution 1, some further justification is needed as to why this could be viewed as an interesting contribution (i.e. the Lagrangian approach, if this is novel)\n the learned controller is able to stabilize the system when interfered by uncertainties such as unseen disturbance and system parameters variations of certain extent - this is not a contribution, but an experimental result.\n\nQ5: The review believes that model-free control and stability-guarantees are fundamentally orthogonal ideas, rather than just under-studied work as the authors have been suggesting in the script and rebuttal. Given that discrete-time Lyapunov stability is defined through expressions along the lines of L(f(x)) - L(x) < 0, for Lyapunov function L and closed-loop dynamics f, claiming that stability is being analyzed without f is disingenuous. Instead, by making the value function a Lyapunov function, the goal is that the *converged* value function should produce a stable policy, and still, this is surely only assured within the space of samples. Moreover, the use of the discount factor \\gamma, popular in MFRL, essentially acts as a time horizon, so Im not convinced a Lyapunov function learned with a \\gamma < 1 can be called stable in the pure infinite-horizon sense.\nWith this in mind, I think the work would benefit from a revised central claim: that the use of Lyapunov value functions (as an inductive bias) provides more *robust* model-free controllers. I believe this message highlights the value of this work for MFRL, without making false assertions. This, in particular, would highlight the fact that many MFRL algorithms are benchmarked on deterministic environments, and therefore incredible brittle as the experimental results suggest.\n\nQ9: This remark was aimed at earlier in the paper, either the introduction or main section, rather than the experimental section. The fact that a value function can be viewed as a Lyapunov function makes sense but Im not sure it is a well-known fact in the wider community. Basically, an introduction to the intersection of Lyapunov stability and optimal control would improve the paper.\n\nQ:10 The fact that the clipping of the multiplier corresponds to unstable policies during learning demonstrates that this pitfall needs to be expressed explicitly. Whether the stability guarantees apply to the converged policy or also intermediate policies is not clear on the initial reading of the paper.\nFor me, this highlights another weakness in the paper. This initial theorems talk of L(s), which relates to the critic L_c by L(s) = E_{u\\sim\\pi(s)} [L_c(s,u)], however in the subsequent objectives (eg Eq 2), this marginalization never occurs, therefore I dont feel like you can say Theorem 2 applies to your resultant algorithm.  Moreover, with the \\alpha_3 c term in Equation 3, c should be c(s, a) with a marginalized, which it doesnt appear to be, and the hyperparameter alpha_3 is never discussed nor tuning explained. Assuming this not done in the code, the experiments need to be re-evaluated with Theorem 2 properly enforced through a sample approximation of the marginalization.\n\nQ11/Q12: Thank you for the Markov jump experiments. Im not sure I understand why LAC is able to learn the task while SAC cannot. To me, this suggests perhaps a lack of hyperparameter tuning for SAC or further investigation. Moreover, there are typos in captions Fig 1, e and f.\nA note of figures: Please ensure all axes should be labeled and should be of sufficient size. Many are too small and unreadable. Figure 2 looks like it could be 1 plot (though perhaps requires normalization).\nGiven that the strength of this method is the added robustness upon convergence, I think it would be valuable to focus less on time-domain results (Figure 3) (these can be added to the appendix for clarity), but instead show how each parameter/noise variation affects the mean and variance of the episodic return. I would expect that, while LAC provides significant robustness, it is still limited. The results dont demonstrate this. It would also be interesting to know which hyperparameter controls this limit. I imagine there is a robustness/performance tradeoff.  \n\nIn conclusion, while I appreciate the efforts the authors put into the rebuttal, the extended discussions made me rethink my rating and I have decreased my rating to reject. I believe fixing the issues highlighted above and redrafting the central message must be done before this paper is ready for publication. \n\n\n######## Review:\nThis paper investigates the use of Lyanpunov theory as an inductive bias for improving the stability / robustness of policies in a model-free actor-critic reinforcement learning setting. Through viewing the Critic as a Lyapunov function, optimizing the policy with a Lyapunov-based constraint is meant to ensure the stability of the policy through a cost stability metric.. Experimental results show that Lyapunov-based Soft-Actor Critic (LAC) is more robust than SAC on some linear and nonlinear environments.\n\nThe reviewer believes that the study of intersections between Control Theory and RL to be immensely valuable and the authors outline a principled formulation. However,  the implementation, experiments and general manuscript suggest that paper requires further work before it is conference-ready. \n\nAs the author understands it, the current state of the literature of Lyapunov methods for Deep Reinforcement Learning can be summarized as:\nRichards et al, 2018, Classify stable region and learn neural Lyapunov function for a safe exploration strategy\nBerkenkamp et al, 2018: Classify the stable region via GP, move there for exploration\nChow et al 2018 Constrained MDPs for discrete gridworld environments\nChow et al 2019 Constrained MDPs for continuous environments through a projection on the policy\nThis work: Actor-Critic constrained policy optimization with a lyapunov-based value function critic\n\nIn the introduction and the related work, too much emphasis is put on explaining stability and discussing methods like Model Predictive Control (MPC) which do not benefit the rest of the paper. Additionally, the three contributions listed do not seem particularly novel given the past literature. \n\nThe premise of the formulation also presents several unquestioned assumptions and design decisions:\nWhy model-free RL, as the authors also state that many samples are required to validate stability?\nHow does the requirement of stability inform the search strategy in this work? Especially as SAC uses a maximum entropy stochastic policy to aid exploration. \nDo you really get guarantees with sample-based methods? I would expect bounds based on the number of samples\nThe cost-based measure of stability seems open to abuse - i.e. for the half-cheetah environment only the centre-of-mass horizontal velocity in covered in the cost function, the stability of the embodiment (joint angles and velocities) are ignored. For the Fetch Reacher, a cost function in cartesian space ignores instabilities from kinematic singularities in joint space. I would image the cost function needs to be a measure on the entire dynamic state. \n\nThe notion of a Value function as a Lyapunov function is very interesting, and since it was the basis of the work, would have benefitted from more discussion, i.e. for which cost/reward function families the equivalence is valid for, and how it compares to other Lyapunov candidate functions.\n\nWith the RL formulation, the requirement of clipping with the lagrangians is suspicious, as it suggests the objective and/or its numerics are not well posed.\n\nWith the choice of experiments, they do not seem to question the central problem outlined by the paper. Rather than show environments SAC returns unstable trajectories during learning, the experiments aim to demonstrate instead a general robustness. The reviewer appreciates that stability is difficult to assess; however, while stability is heavily linked to robustness, a paper title promising stability guarantees should demonstrate some strong empirical evidence stability.\nAdditionally, the choice of environments do not seem to be ideal test beds for stability - i.e, the half-cheetah is stabilized via interactions with the ground. The reviewer would prefer to see simpler nonlinear environments, such as Markov Jump Processes / Switching Linear Dynamics, where SAC clearly demonstrates instability during learning which LAC is sufficiently regularized against. Additionally, while the `repressilator is an interesting application to the domain of bioengineering, its addition does not seem to be especially motivated by the central goal of the paper, so just adds to confuse the reader with unnecessary theoretical content.\n\nMoreover, a brief literature review uncovered some relevant earlier work which was not cited: \nConstruction of neural network based Lyapunov functions, Petridis et al, 2006\nGeneration of Lyapunov functions by neural networks, Noroozi et al, 2008\nLyapunov Design for Safe Reinforcement Learning, Perkins et al, 2002\nSome of the references also appear incorrectly formatted or incorrect, i.e. the reference for Spencer et al, 2018 should be the CoRL 2018 version rather than arxiv. \n\nAlso, the general use of grammar in the manuscript would benefit from another draft. In particular, the title could be improved, i.e.\n    Model-free Control of Nonlinear Stochastic Systems with Stability Guarantees\n", "belong_id": "SkeWc2EKPH"}, {"uid": "B1e1O5yJ9H", "paper_title": "Model-free Learning Control of Nonlinear Stochastic Systems with Stability Guarantee", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work the authors studied the model-free RL approach for learning a policy with stability guarantees. Leveraging the Lyapunov stochastic stability criterion, instead if minimizing the cumulative cost (plus a soft entropy), they propose optimizing an objective function with a specific Lyapunov critic, which is a specific critic function that satisfies the Lyapunov criterion to guarantee stability. They also show in several Cartpole, Mujoco, and Repressilator experiments that this approach is more robust to perturbations (such as sinusoids), where the agent are more robust to dynamic uncertainties and disturbances. \n\nIn general, the topic of guaranteeing stability is a topic in safe RL, and I find this work of enforcing stability in model-free RL interesting. Through the specific parameterization of quadratic Lyapunov function (in the latent space), the authors proposed learning a new critic function that is a value function but at the same time (almost) satisfies the Lyapunov constraints. While this is an interesting idea, and the experimental results look promising, I do have several questions.  \n\nFirst, regarding the learning problem of Lyapunov function, how does the proposed way of learning L differ from the one in Richard'18: The lyapunov neural network: Adaptive stability certification for safe learning of dynamic systems, where the problem is formulated as a classification (while in here it is a regression problem)? \nSecond, while this approach is intuitive, since the approach is penalty-based (Lagrangian based), I do not see how the Lyapunov criteria in Theorem 1 is guaranteed, in this case is stability guaranteed by the policy learning  algorithm? If not, what do the authors do to enforce that? \nThird, if one formulates the immediate constraint cost of the CMDP to be the distance of the state to the equilibrium point,  then the (undiscounted, shortest-path type)  CMDP total cost constraint should guarantee stability (because the total distance cumulative cost is bounded, meaning that the distance cost converges to zero). Then, one can use the Lyapunov approach by Chow'19 (in modulo to their setting in discounted MDPs) to enforce stability (which is a specific notion of safety in this case).  How does the proposed method compare with this approach? Can the authors provide numerical comparisons with the method proposed by Chow'19 as well?", "belong_id": "SkeWc2EKPH"}, {"uid": "SyeJTy7ccr", "paper_title": "Model-free Learning Control of Nonlinear Stochastic Systems with Stability Guarantee", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors introduce an algorithm to learn a stable controller using deep NN actor-critic method. They define the stability in the mean cost criteria,  which is used to constrain the critic network as a Lyapunov function. In addition, the semi-positive definiteness of the Lyapunov function is enforced by constructing the critic.\nThe problem is important to control with deep RL. The paper is written clearly. The reviewer has the following questions regarding the stability of the learned policy.\n\n- How is the stability in the mean cost related to the stability of stochastic systems? See, for example, the Lyapunov stability of stochastic systems (survey in [1])?\n- The authors enforce semi-positive definiteness using the construction of the value function approximation as the quadratic function bases. Then the semi-negative definiteness is enforced using penalty on the Lyapunov stability of the critic. Then the target network is trained to minimize the difference between the target and the critic. The question is, how is the stability of the target ensured by minimizing the difference with a Lyapunov critic? Is it possible to have an unstable target function that happens to minimize the distance? \n\n[1] H. J. Kushner, A partial history of the early development of continuous-time nonlinear stochastic systems theory, Automatica, vol. 50, no. 2, pp. 303334, 2014.", "belong_id": "SkeWc2EKPH"}, {"uid": "BJeDrU0bKH", "paper_title": "WORD SEQUENCE PREDICTION FOR AMHARIC LANGUAGE", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "I would not like to sound offensive, but this paper is clearly below the standards of the conference, and outside any academic orthodoxy for the matter:\n- It is only 3 pages long including references, and does not even follow the conference template.\n- It has only 3 sections ('introduction', 'methodology' and 'conclusions') and 2 subsections ('background of study' and 'limitation'), and all of them are only one paragraph long.\n- The work done is not adequately described, so it is not possible to say much about it, but it seems clear that there is no novelty nor sufficient rigor in it. The problem tackled is defined as 'word prediction', which seems to be some form of language modeling. The proposed method combines HMMs with n-grams and morphological and POS features, which are all well established and should be considered more of a (nowadays outdated) baseline. There is no proper evaluation: the experimental settings are not described, and only one number is reported, with nothing to compare to.\n\nTo the authors: Please do not feel discouraged by my review. Your motivation (helping people with dyslexia in Ethiopia) is certainly laudable, but neither the work carried out nor its presentation meets the standards of our research community. I would suggest that you check some of the accepted papers in last year's conference to get a sense of what is expected. I assume that you are new to the field. Don't feel the need to publish your own research from the first day, and take your time to become familiar with the field and study the basic concepts. It takes time, but we all had to go through it :)", "belong_id": "HkgqmyrYDH"}, {"uid": "HJxiqVt3tr", "paper_title": "WORD SEQUENCE PREDICTION FOR AMHARIC LANGUAGE", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\n\nThis paper proposes to predict word sequences for Amharic language-- a language spoken in Eastern Africa. It proposes to use HMMs with POS tags and morphological features to perform this prediction task.\n\n\nThe paper is just 3 pages, contains 1 paragraph of methodology, and no experiments section. It is clearly a very early stage work and not in the scope of ICLR. This paper should have been desk-rejected as it needs more work before it is fit for publication. There is lot of work on word sequence prediction and HMMs are no longer the state-of-the-art. The authors should consider looking at RNN-based methods such as LSTMs for this task.", "belong_id": "HkgqmyrYDH"}, {"uid": "rygH6_Mh5r", "paper_title": "WORD SEQUENCE PREDICTION FOR AMHARIC LANGUAGE", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "*What is this paper about?*\n\nThe authors propose a method to incorporate POS tags into a language model to improve its performance in Amharic language.\n\nShort review:\n\nThe authors tackle an interesting task which deserves more attention. Nonetheless, they do not fully describe their models or results with enough detail, so it is hard to evaluate this work.\n\n\nContributions:\n\nThis work tackles a relevant problem that seriously impacts speakers of low resource languages.\n\n*What strengths does this paper have?*\n\nIt tackles and interesting problem.\n\n*What weaknesses does this paper have?*\n\nThe authors do not present their models in enough details so that the reader fully understands it.\nThey also only gloss over the results, not presenting them in any concrete form, stating: We believe the results obtained were effective in reflecting bet-ter speed, correctness of suggestions (grammatical), and search space since these are the basic issues in word sequence prediction and in assistive technology. This referred results are not shown in the manuscript though.\n\n\n*Detailed comments:*\n\nThe paper does not use the official conference template. It is also very short, not going in details about the used techniques. Finally, references are not correctly formatted.\n\nSection 1.\n", "belong_id": "HkgqmyrYDH"}, {"uid": "Ske7FfJTFB", "paper_title": "Unsupervised domain adaptation with imputation", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The submission describes an approach for unsupervised domain adaptation in a setting where some parts of the target data are missing.\n\nBoth UDA approaches as well as data completion approaches have a sizable research history, as laid out in the related work section (Section 5). The novelty here comes from the properties that a) domain adaptation and data imputation are handled in a joint manner, b) the missing data in the target domain is non-stochastic, and c) imputation is performed in a latent space. This maps to a fairly specific, but realistic enough set of real-world problems; the authors give an image recognition as well as an advertising prediction related problem as experimental examples.\n\nThe submission is overall well written and easy to understand. I'd rate the novelty as medium (smart combination of existing methods), but the exemplary experimental evaluation elevates it to more than a systems paper.\n\nThe method is described clearly in Section 3, and the joint training makes sense. I notice that not all hyperparameters ({lambda_adv, lambda_mse}, {lambda_1, lambda_2, lambda_3}) are truly needed. lambda_adv and lambda_1 could be canonically set to 1 for such a loss minimization problem, so why are the extraneous parameters included?\n\nIn addition to Section 3, the experimental evaluation on two very different data sets in Section 4 is highly detailed and describes the insights clearly, both qualitatively and quantitatively. I'm happy that mean standard deviations are reported on an acceptable experiment sample set size.\nRegarding the different approaches: I'm wondering whether the higher performance of the ADV approach over OT (or the parameter hunger of OT over ADV) is only due to the tuning of the network architectures, or whether this is due to the approximations described in B.1.\nThe ablation study in Section 4.4 is interesting w.r.t. the trade-off it shows between stable, consistent, 'average' results from an MSE loss term, vs. high-variance (and on average better) results when a choice of mode is forced using an adversarial loss term.\n\nMinor comments:\n- In Table 2, I am not sure what the first row ('Naive') refers to. As far as I can tell, it is not referenced in the text.\n- I would move Section 5 (related work) to right after the introduction, as is common in conference papers and makes for smoother reading.\n- Section 5.2: type 'impainting' -> 'inpainting'\n- Appendix, section 'Pre-processing': It seems to me that there is a clear assumption made that the target set is balanced, since training happens with a balanced source set. Is this realistic in practical scenarios? There is work on DA with unequal class distributions between domains.\n\nIn summary, I can clearly recommend this submission for publication.", "belong_id": "B1lgUkBFwr"}, {"uid": "ryxFdPLRYH", "paper_title": "Unsupervised domain adaptation with imputation", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "*Summary.* The paper presents and addresses the problem of performing domain adaptation when the target domain is systematically (i.e., not the result of a stochastic process) missing subsets of the data. The issue is motivated by applications where one modality of data becomes unavailable in the target domain (e.g., when deciding which ads to serve to new users, the predictor may have access to behavior across other websites but not on a specific merchant's website). The proposed method learns to map source and target data to a latent space where the representations for the source and target are aligned, the missing components of the target can be inferred, and classification can be performed successfully. These are achieved by adversarial/optimal transport loss on source and target features, a mean-squared error and adversarial loss on latent generation/imputation, and a cross entropy loss on source label prediction, respectively. Experiments are performed on digits and click-through rate (CTR) prediction and include a thorough set of baselines/oracles for comparison.\n\n*Review.* While the problem statement is novel, I am unconvinced that the advertising experiment includes both a domain adaptation and imputation problem. I describe this in detail below. For this reason, I am giving the paper a weak reject.\n\n*Questions that impacted rating.*\n1. Ads experiment: From my understanding, the source domain is the traffic of users who have interacted with (clicked through to?) a specific partner and the target domain is the traffic of the users who have not interacted with that specific partner. The data that needs to be imputed is the click through rate for target users with that specific partner. In this case, it is not obvious to me why there is a domain shift between these two groups of users. This would imply that the traffic of source users and target users is different for other partners. I don't see why this would need to be true. Could the authors provide an explanation as to why this is the case (e.g., by showing that CTRs differ with other (partner, publisher) pairs between source and target). From my understanding, Table 5 only shows CTR averaged across all users in each domain, but does not show that the CTRs differ between source and target users for contexts/(partner, publisher) pairs (i.e., the results in table 5 could be due to the fact that the prior distribution over context is different for source and target users).\n\n*Additional notes. Immaterial to rating.*\n1. I personally felt that the motivation for UDA vs imputation in the first paragraph was a bit muddled. I think sticking to one example would make the motivation more clear to the reader. E.g., explain the prediction problem for medical imaging (which I assume is disease diagnosis, but it is not stated explicitly), describe how some medical imaging may be missing for certain patients (imputation), then explain that there may be noise across different medical imaging systems (UDA), then list the other applications where this arises with citations (e.g., These phenomena have also been documented in advertising applications [1], ...).\n2. I was surprised by the difference between Adaptation-Partial and the other two train/test conditions in Figure 2 when p=30%. Out of curiosity, do the authors have an explanation for this discrepancy? I would have predicted that, if most of the information necessary for prediction was available in the remaining 70% of the image that the performance of these cases would be very similar.  I think it would be helpful to see the accuracy on the source domain and the labeled target domain to better understand that result.", "belong_id": "B1lgUkBFwr"}, {"uid": "HJlx9WDWcS", "paper_title": "Unsupervised domain adaptation with imputation", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed to address a compound problem where missing data and distribution shift are both at play.  The paper goes on to describe some heuristic methods that resemble the gradient reversal methods due to Ganin et al for handling both problems. \n\nThe novel part of the paper over DANNs is the joint, end-to-end training of latent representations for missing data,  While it is sloppy with terminology, the paper is overall reasonably easy to follow although it might mislea a novice reader and sufficient details are provided to replicate their results.  \n\nThe major problem here is the problem appears to be underspecified, and its not clear under what conditions if any the proposed methods are valid. Moreover its not clear to what extent the experimental results should ameliorate these concerns. \n\nIf the data is not missing at random then there is presumably confounding. The authors dance around this topic, just asserting that they are handling non-stochastic missing data but do not say precisely what is assumed about the relationship between the observed and missing data. \n\nIn short the paper addresses an under-specified problem with a heuristic technique based upon domain-adversarial nets which have recently been shown have a number of fundamental flaws. It's never made clear under what assumptions this proposed procedure is valid and the paper misrepresents the prior work on lable shift, including the theoretically sound work, e.g.:\n\n'we assume covariate shift as in most UDA papers e.g. Ben-David et al. (2010); Ganin & Lempitsky (2015).\n>>>  Ben-David 2010 is not about covariate shift ....\n\nSome minor thoughts:\n\nsome components of the target data are systematically absent\n>>> \tNot clear what component means at this point\n\nWe propose a way to impute non-stochastic missing data\n>>> \tWhat does this mean? Is non-stochastic, not missing at random? What is the pattern of missing-ness conditioned on? What assumption, if any, is made? \n\nThis key property allows us to handle non-stochastic missing data, \n>>> \tagain what precisely does this mean?\n\nConsider that x has two components (x_1, x_2)...\n>>>\tsloppy  notation:\n\tSource features x_s = (x_S1, x_S2) are always available \n\n\nI read the author's reply but do not believe that the responses are satisfactory. The authors do not address the primary concerns clearly and do not point to specific improvements in the draft that might cause me to change my mind.\n\n", "belong_id": "B1lgUkBFwr"}, {"uid": "B1xbXQwnKB", "paper_title": "Training Neural Networks for and by Interpolation", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new gradient descent methods for training deep neural network which can take the adaptive step size with only one hyper-parameter to tune -- the maximum learning rate -- and achieve comparable results to stochastic gradient descent (SGD) on various tasks and models. In order to achieve that, they develop a stochastic extension of the Polyak step-size for the non-convex setting, namely the adaptive learning-rates for interpolation with gradients (ALI-G), in which the minimal value of the objective loss is set to 0 due to interpolation in neural networks and the learning rates are clipped by a chosen maximal value. The problem is formulated clearly, and the review on the Polyak step-size and related works are well done. Another main contribution of the paper is to provide the convergence guarantees for ALI-G in the convex setting where the objective loss is Lipschitz-continuous (Theorem 1 in the paper). Their theorem also takes into account the error in the estimate of the minimal value of the objective loss. In addition, they derive the connections between  ALI-G and SGD and show that compared to SGD, ALI-G take into consideration that the objective loss is non-negative and set the loss to 0 when it is negative. They perform empirical study to compare their algorithm with other methods including Adagrad, Adam, DFW, L4Adam and SGD on learning a differentiable neural computer, object recognition, and a natural language processing task. Their experimental results show that ALI-G performance is comparable with that of SGD with schedule learning rate.\n\nOverall, this paper could be an interesting contribution. However, some points in the theory and experiments need to be verified. I weakly reject this paper, but given these clarifications in an author response, I would be willing to increase the score. \n\nFor the algorithm and theory, there are some points that need to be verified and further clarification on novelty:\n\n1. When there are regularization such as the weight decay regularization, the minimal objective loss will not be 0. In such cases, Theorem 1 in the paper only guarantees that ALI-G reaches an objective loss less than or equal to a multiple of the estimate error \\epsilon of the true minimal objective loss. It cannot guarantee that the objective loss reached by ALI-G can converge to the true minimall objective loss. Furthermore, when training neural networks with, for example, the weight decay regularization, often times the value of the regularization loss, i.e.  the estimate error \\epsilon, is not small. Therefore, the upper bound given by Theorem 1 is rather loose. \n\n2.  The paper mentions that when no regularization is used, ALI-G and Deep Frank-Wolfe (DFW) are identical algorithms. The difference between the two algorithms are when regularization is used. However, given my concern for Theorem 1 above, the convergence of ALI-G and advantage of ALI-G over DFW in this setting is questionable, and the claim that ALI-G can handle arbitrary (lower-bounded) loss functions also needs to be verified. \n\nFor the experiments, the following should be addressed:\n1. In the experiment with the differentiable neural computers, even though ALI-G obtains better performance for a large range of \\eta, its best objective loss is still worse than RMSProp, L4Adam, and L4Mom.\n\n2. Given the merit of Theorem 1 is that the convergence guarantee takes into account the estimate error of the minimal objective loss, an ablation study that compares ALI-G with other methods in the same setting with and without regularization are needed. For example, it would be more convincing if similar results to those in Table 2 or 3 but without regularization are provided and discussed.\n\n3. In Section 5.5, given that ALI-G and DFW are related, why is there no result for DFW in Figure \n4.?\n\n4.  As the paper mentions, AProx algorithm and ALI-G are related, why is there no comparison with AProx in the experiments?\nThings to improve the paper that did not impact the score:\n1. In all experiments, the performance differences between ALI-G and competitive methods are small. Thus, error bars are needed for these results.\n\n2. In Table 3, the gap between SGD and ALI-G can be significantly different on different architectures. For example, in CIFAR-100 experiments, while ALI-G achieves the same result as SGD with the DenseNet, ALI-Gs performance on Wide ResNet is much worse than SGD. Do you have any explanation for this?\n\n3. How is ALI-G compared with the methods proposed in the paper Stochastic Gradient Descent with Polyak's Learning Rate (https://arxiv.org/abs/1903.08688)\n\n\nThe following paper also proved the convergence of adaptive gradient methods for nonconvex optimization:\nDongruo Zhou*, Yiqi Tang*, Ziyan Yang*, Yuan Cao, Quanquan Gu. On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization.\n\n-------------------------------------------------------------\nAfter rebuttal:\nI noticed that the authors:\n\n1. did not compare with SGD or Adam with good hyperparameters (https://arxiv.org/abs/1907.08610)\n\n2. did not test on the large scale datasets, e.g., imagenet\n\n3. the proposed algorithm is not as good as SGD on most numerical experiments.\n\nFor the above reasons, I think this paper should be rejected.\n", "belong_id": "BJevJCVYvB"}, {"uid": "Syxp4JxAKH", "paper_title": "Training Neural Networks for and by Interpolation", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a new adaptive learning rate method which is tailored to the optimization of deep neural networks. The motivating observation is that over-parameterized DNNs are able to interpolate the training data (i.e. they are able to reach near-zero training error). This enables application of the Polyak update rule to stochastic updates and a simplification by assuming a zero minimal training loss. A number of proofs for convergence in various convex settings are provided, and empirical evaluation on several benchmarks demonstrates (a) ability to optimize complex architectures, (b) performance improvements over, and (c) performance close to manually tuned SGD learning rates.\n\nI vote for accepting this paper. The approach is well-motivated, the method is described clearly and detail, and the experiments support the paper's claims well. What I would still like to see are a few additional details regarding the experimental protocol. In particular, did you train a single or multiple models for each result that is reported? Do different runs start from the exact same weight initialization? What condition was used to stop the training? The results in section 5.2. are all very close to each other, and it would be helpful to have a sense of the variability of the different methods. The graphs in Figure 4 do look like the models did not converge yet.\n\nGenerally, it would be nice to examine the behavior of the method in cases where the neural network is underparameterized or is otherwise unable to effectively interpolate the training data. Does the method lead to divergence in this case, or is it subpar to other methods? I think section 2.2. could benefit from a short motivational introduction; on the first read, I was not clear about the purpose of introducing the Polyak step size as it is not mentioned explicitly in the text leading to it.", "belong_id": "BJevJCVYvB"}, {"uid": "BJgm9PbTqH", "paper_title": "Training Neural Networks for and by Interpolation", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper addresses designing and analyzing an optimization algorithm. Like SGD, it maintains a low computational cost per optimization iteration, but unlike SGD, it does not require manually tuning a decay schedule. This work uses the interpolation property (that the empirical loss can be driven to near zero on all samples simultaneously in a neural network) to compute an adaptive learning rate in closed form at each optimization iteration, and results show that this method produces state-of-the-art results among adaptive methods. I can say the paper was very well written and easy to follow along/understand. Prior work seems comprehensive, and the intuitive comparisons to the prior methods were also useful for the reader. \n\nMy current decision is a weak accept, for a well-written paper, thorough results including meaningful baselines and numerous hyperparameter searches, and a seemingly high-impact tool. Some concerns are listed as follows:\n1-\tConvergence was only discussed in the stochastic convex setting, which seems limiting because we rarely deal with convex problems in problems requiring neural networks.\n2-\tRegularization of the weights during the optimization is dealt with by projecting onto the feasible set of weights, but it seems like there are other types of losses that dont necessarily to go 0. For example, terms in the objective such as entropy seem worrisome.\n3-\tOne detail that I did not fully follow along with is section 3.1. How does Theorem 1 (Regarding convexity) related to the each training sample can use its own learning rate without harming progress on the other ones and/or allow the updates to rely on the stochastic estimate rather than the exact? \n4-\tUnfortunately, I am not an expert in this particular area, so Im not confident about the novelty. For example, the difference between L4 and this is stated to be the utilization of the interpolation policy (which just sets f*=0) and the maximal learning rate, and the stated benefit of convergence guarantees in stochastic convex settings seems poor since most problems will not be convex anyway. More generally, it seems like all details of the algorithm came from elsewhere, although the presented synthesis of ideas does have clear benefits.\n\nAfter reading the author response: \n-I'm fine with points 4 and 3. \n-My feelings about 1 are still the same.\n-My comment on 2 wasn't about cross-entopy loss, but rather other types of objectives that people are often interested in optimizing (such as max-ent RL, where we aim for maximizing rewards as well as maximizing entropy of the policy), in which case, it's not clear to me how we could apply this optimizer. \n-My decision stays as a weak accept", "belong_id": "BJevJCVYvB"}, {"uid": "rJg9gcITcB", "paper_title": "Training Neural Networks for and by Interpolation", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Thanks for the responses and my concerns seem to be addressed. But since I do not know much about this area, I would like to stick to my initial rating 6.\n==================================================================================================\nThis work designs a new optimization SGD algorithm named ALI-G for deep neural network with interpolation property. \nThis algorithm only has a single hyper-parameter and doesnt have a decay schedule. The authors provide the convergence guarantees of ALI-G in the stochastic convex setting as well as the experiment results on four tasks.This paper shows state-of-the-art results but I still have have two concerns.\n\nMy main concern is that the performances of other SGD algorithms may be potentially better than the results showed in section 5 because it is not easy to tune the parameter. It would be better if the authors can tune the hyper-parameter more carefully. Take figure 3 as an example. The settings where step size bigger than 1e+1 can hardly shows something, because the step-size is too big for SGD to converge. The settings where step size smaller 1e-2 can also hardly shows something, because the step-size is too small and the experiments only runs 10k steps. It would be better if authors can do more experiments in the settings where step size is from 1e-3 to 1e+0. Moreover, the optimal step size of different optimization algorithms may differ a lot. It would be much more fair if the authors can compare the best performance of different algorithms.\n\nAnother concern is that the authors only give the convergence rate of ALI-G in section 3 but havent make any comparisons. For example, it would be better if the authors can show that ALI-G has better convergence result than vanilla SGD without decay schedule.\n", "belong_id": "BJevJCVYvB"}, {"uid": "BJesxs8dYS", "paper_title": "CLEVRER: Collision Events for Video Representation and Reasoning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "A new benchmark is presented, which requires to reason on spatio-temporal data (videos) and incorporates physics (videos of physical dynamics), language (questions are formulated through language) and causality. The benchmark builds on the well-known CLEVR benchmark and adds several interesting contributions, in particular reasoning over time.\n\nCompared to other benchmarks proposed on reasoning on physical dynamics, this benchmark adds to the language component (which is presented in classical VQA benchmarks), and an interesting counterfactual component known from the causal inference literature. This is fairly new for the computer vision and statistical machine learning literature (a few papers on counterfactual reasoning exist), but in contrast to the causal inference literature, here the do-operator is observable during training: we do have access to do-interventions and even the outcome during training and therefore we can even learn counterfactuals using supervised learning. This statement is not meant as criticism, as learning counterfactuals without supervision from high-dimensional input like images seems to be currently out of a reach, or at least has not yet been demonstrated up to my knowledge.\n\nOn the downside, compared to other physical reasoning benchmarks, the answers here are multiple choice instead of regressions of the future motion, which requires more fine-grained reasoning. This will guide solutions to a certain type and will also favor solutions, requiring the network to detect certain binary concepts and combine them instead of regressing complex functions. It also favors solutions of the type presented in the paper.\n\nAs for other benchmarks, the dataset is quite large (20 000 videos) and, given its synthetic nature, is accompanied by functional programs. As for CLEVR, the simulations have been performed by a physics engine and then separately rendered with Blender to maximize visual quality. The result is a very interesting benchmark, and I have no doubt that it will be very useful for us (the learning reasoning community).\n\nAnother positive point is the number of baselines tested on the benchmark, among which we can find strong papers on VQA/VQA2\n\nI have a couple of questions:\n\nHow is the causal graph created? Are the experiments rendered and outcomes examined, creating the causal graph for the counterfactual answers, or are the experiments selected with a given outcome already decided?\n\nThe distribution of question types has been provided, but how about the biases? Distributions of the answers would have been helpful. How did the authors avoid biases during construction of the dataset, in particular in the counterfactual case (see remarks on the causal graph).\n\nThe paper also comes with a method for solving, which is very similar to existing methods on learning through functional programs. The method itself is unfortunately described only very briefly and the reader needs to look it up almost entirely in the appendix of the paper, which is surprising, as the paper is only 8 pages long instead of 10.\n\nAs for CLEVR, one of the downsides of this type of benchmark is the synthetic nature of the images and the limited range of different objects in the scene. Of course this comes with the advantage of being able to study compositional reasoning in detail, as a scene graph can be calculated easily (and is available during simulation). However, it also makes reasoning through functional programs much easier, as the proposed filters are limited in number and can strong respond to the small number of shapes and colors available in the data. I have strong doubts that this kind of approach extends to real life scenarios.\n\nFor VQA type of scenarios, GQA is a nice compromise between natural looking images and the availability of scene graphs and the restriction of questions to compositional reasoning. The optimal choice would be a similar compromise for spatio-temporal data, but of course this would be a huge effort and it would be up to impossible to have access to counterfactuals.\n\nLast point, and this question is not restricted to this paper, as the name came up elsewhere, why is the model called neuro-symbolic reasoning? While it could be argued that the questions require a sort of symbolic reasoning, I am not sure that the reasoning method itself is symbolic even partially. Other than selecting functional programs out of a discrete set, the reasoning itself is connectionist and performed with graph networks.\n", "belong_id": "HkxYzANYDB"}, {"uid": "H1lez5_nFr", "paper_title": "CLEVRER: Collision Events for Video Representation and Reasoning", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper studies the temporal and causal structures in videos. Specifically, the authors first introduce a new dataset called CLEVRER drawing motivation from CLEVR, a well-known visual reasoning dataset. They further evaluate a set of state-of-the-art methods on the newly introduced dataset to confirm their initial beliefs of the challenges posed by causal reasoning. Based on empirical clues, they also suggest neural-symbolic based framework for causal reasoning.\nOn the whole, I think this is a good paper and it addresses one of the most challenging and exciting tasks in visual reasoning. The introduction of the CLEVRER dataset is likely valuable for the community to facilitate research in this area. I have some concerns as follows:\n(1) As the fact that questions are generated algorithmically, I believe many of them are either ill-posed or degenerate similar to what described in the CLEVR. Did you manage to filter out those questions? Please provide more details on the question generation process.\n(2) I have a doubt on the reported results in table 2 as you extract visual features not fairly between all methods. Outputs of pool5 feature basically kill all spatial information while 14x14 feature map used as input of MAC, IEP and TbD-net keep the spatial information. I also not sure if the temporal attention is a better option than vectorizing the whole video features (Some pooling layers might be helpful) before feeding into those methods. It would be fairer to evaluate the state of the art methods on the CLEVRER more carefully.\n(3) As the number of objects in CLEVRER is limited, using class labels from Mask R-CNN may introduce noises to the model due to incorrect detections. The authors may need to explain more clearly at this point in the implementation details.\n(4) Have you tried to incorporate flow features, for example, C3D/I3D feature?\n\nMinor comments:\nThere are some typos in the paper in both main paper and supplementary document -causal vs. casual. Please fix these.", "belong_id": "HkxYzANYDB"}, {"uid": "SJl5uAax5B", "paper_title": "CLEVRER: Collision Events for Video Representation and Reasoning", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Authors propose a new Dataset CLEVRER, a simulated video dataset involving interaction between objects. It is discussed, the existing state-of-the-art models for visual question answering, doesnt capture the causal structure between the objects and their claim is supported by their experiments.  Authors also proposed a model which captures the dynamics of the objects involved in the video, through experiments they have shown their model performs better than the existing models. \n\nThe CLEVRER dataset is designed to test a model capability to answer the queries involving causal relationship between the objects involved in the video. \n\nDetails.\nThis work begins with a well motivated problem by pointing out the drawback in existing VQA(Visual Question Answering)  models, that existing works focus on visual and input language patterns to answer the queries and doesnt tackle the task involving causal structure.  It  explores the current literature around the problem related to visual question answering(both real world data and simulated data). Through experiments they have shown the existing state-of-the-art work on visual question answering doesnt perform well on the dataset CLEVRER. \n\nTo prove the incompetence of existing models to capture causal structure, authors designed an artificial dataset with questions which can be answered only when the model is capable of capturing the causal structure between the objects.  \n\nThe process involving the creation of CLEVRER dataset is well explained. But, it is unclear how the questions are generated. \n\nThrough experiments authors revealed the drawbacks of existing models on capturing the dynamics between the objects and  proposed a model which is said to be inspired by previous VQA[1] model. An important modification by incorporating neural dynamics predictor module to the existing model is key, and also achieves good performance on the dataset. \n\nComments:\n\n- The paper is well written, but it is unclear how the questions are generated during dataset creation process.\n- The main contribution of the paper is to show the incompetence of existing models to capture dynamics. Which is a form of analysis.\n- It is shown that,  learning dynamics of the objects the model can achieve better performance.\n- This dataset is created in more restricted environment like height of the objects should be same. How can this be generalized to a more real world setting ?\n\nSome minor issues:\nIn few places causal is misspelled as casual(page 2,8).\nEquation 2, in the appendix the subscripts are not proper. \n\n[1] Yi, Kexin, et al. 'Neural-symbolic vqa: Disentangling reasoning from vision and language understanding.' Advances in Neural Information Processing Systems. 2018.", "belong_id": "HkxYzANYDB"}, {"uid": "B1laVbdjYH", "paper_title": "Adversarial Training Generalizes Data-dependent Spectral Norm Regularization", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies the link between adversarial training and the proposed data-dependent operator norm regularization for ReLU network. Under specific conditions, in theory, the authors show the equivalence between the l2 PGD training and the regularization method. Empirical experiments are conducted to support the theory.\n\nWhile this paper gives interesting observation on both theory and empirical study, I think this paper is not qualified for publishing in ICLR due to the following reasons: (1) limited theoretical results; (2) No significant improvement for practical algorithm;\n\nMain argument:\n\nThe main theorem 1 seems to be weak as it is only valid for small perturbation region \\epsilon and it is unclear how this assumption is consistent to the practice. It is also unclear how the assumption that \\alpha \\to \\infty influences the practical algorithm.\n\nIt would be better to generalize the theorem to other \\ell_p attack, instead of just \\ell_2. \n\nDiscussion of computational complexity of the proposed regularization method compared with PGD is missed.\n\nThe adversarial robustness is related to the (local) Lipschitz continuity and many other types of regularization decreases the (local) Lipschitz constant. Could you further give result that distinguishes the proposed norm?\n\nIt would be better to give improved algorithm for adversarial training based on the current result. The current contribution for further theoretical is too weak as the main theorem requires strong assumption. And I dont see significant contribution to empirical algorithm.\n\n\nMinor\nEqu (10) seems not the typical one used and seems not the one studied later.\n", "belong_id": "S1ervgHFwS"}, {"uid": "SyeD7eqHcH", "paper_title": "Adversarial Training Generalizes Data-dependent Spectral Norm Regularization", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This largely theretical paper establishes a theoretical link between adversarial training and operator norm regularization\nfor DNNs. It is well written and structured, and it falls squarely within the the remit of the conference. The experimental apparatus is thorough and the derivations, proofs and the maths at large seem sound to me, even if I have not checked them in full detail. The study delivers a data-dependent variant of spectral norm regularization affecting large singular values of the DNN. It is proved to be equivalent to adversarial training based on a type of norm-constrained projected gradient ascent attack.\nResults are novel and relevant and, in my opinion, they merit acceptance.", "belong_id": "S1ervgHFwS"}, {"uid": "B1xIuisysH", "paper_title": "Adversarial Training Generalizes Data-dependent Spectral Norm Regularization", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Adversarial training generalizes data-dependent spectral norm regularization\n\nThis paper shows that, projected gradient descent based adversarial training is similar to the data-dependent spectral norm regularization, and under very restrictive condition, the authors show that this two methods are the same. Some experiments are conducted to support the theory.\n\nOverall, I think this paper is marginal, while the experiments are not convincing. First, the relation between spectral normalization and adversarial training have been investigated by [1], while the fast computational of maximum singular value with power methods have also been proposed in [1]. The authors only give a data-dependent version of the spectral normalization based on the Jacobian of the neural networks, which I think is somewhat weak. The experiments are limited with specific settings that are not generally used in practice, which alleviate my confidence on this papers results. Also, the experiment section contain several not so important information. I think the authors should do far more experiments to support the main claim, while move these additional justification to the appendix.\n\nDetailed comments:\n1. I think the claim of theorem 1 is somewhat ambiguous. How to guarantee there exists such epsilon satisfies this condition? Is this the case we face in the real world? What will happen if \\alpha is not sufficiently large? If we dont use logits pairing and \\ell_2 norm constraint, will the claim hold? I think the correlation behinds the spectral norm and adversarial training is well investigated and use this correlation as the intuition behinds work is enough. This theorem cannot convince me that the proposed methods have a strong theoretical basis.\n2. Generally, the neural networks have a large number of parameters (~ millions) for image classification task. The global spectral norm regularization only needs to calculate the spectral norm of each layers weight matrix, whose computational cost is acceptable. However, to calculate the Jacobian and use the power methods, we will additionally do several forward pass and backward pass just as adversarial training. As a regularization technique, is this calculation tolerable? If this is some variant of the adversarial training, I dont find the experiment results support the claim that it will outperform the adversarial training consistantly.\n3. Why dont use some standard neural network architecture like ResNet? As this results is not comparable to other existing work, Im not sure if this result is meaningful. Also, are the comparisons fair? For example, the regularization coefficient of global spectral norm regularization and data-dependent spectral norm regularization are far more different. And the authors use only 1 iteration to calculate the singular value in global spectral norm regularization, why to do that? Also, whats the result compared with \\ell_p norm constraint adversarial training?\n4. The evaluation of some assumption on the network is better moved to appendix, as this is only some sanity check, not the core contribution. More experiments with ResNet, WideResNet, MobileNet etc. on CIFAR100 and ImageNet are more convincing.\n5. Whats the attack method in the main context?\n6. I think the discussion in Appendix A.5 is somewhat confusing. If the authors want to argue that the network is locally linear so that we can approximate with linear regression, why should we use the power methods?\n\nStill, I feel the contribution of this paper is somewhat weak. I dont see any improvements of the proposed algorithms compared with the standard adversarial training, as well as the theoretical contribution like adversarial generalization. The experiments are not convincing, as the setting is different from the general setting the community used in adversarial training. Im not familiar with the results in global spectral normalization and its possible that the global spectral normalization may have little gain in adversarial robustness, but in my opinion, the main contribution [1] is the generalization analysis of spectral normalized adversarial trained neural networks, which this paper lacks. On the empirical side, the computation efficiency and performance of the proposed algorithms dont outperform adversarial training much. So I tend to reject this paper.\n\n\n[1] Farnia, Farzan, Jesse Zhang, and David Tse. 'Generalizable Adversarial Training via Spectral Normalization.' International Conference on Learning Representations, 2019.", "belong_id": "S1ervgHFwS"}, {"uid": "Skl8XqOndH", "paper_title": "HighRes-net: Multi-Frame Super-Resolution by Recursive Fusion", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an end-to-end multi-frame super-resolution algorithm, that relies on a pair-wise co-registrations and fusing blocks (convolutional residual blocks), embedded in a encoder-decoder network 'HighRes-net' that estimates the super-resolution image. Because the ground truth SR image is typically misaligned with the estimation SR image, the authors proposed to learn the shift with a neural network 'ShiftNet' in a cooperative setting with HighRes-net. The experiments were performed on the ESA challenge on satellite images, showing good results.\n\nOverall, I found this paper interesting, and the method described is both clever and efficient. While some points need to be clarified, I am in favor of accepting this paper to ICLR.\n\nPositive aspects:\n- the paper is very clear and easy to read, with nice figures.\n- a sensitivity analysis on many different parameters or types of inputs are made, which makes this paper an interesting research paper. For example, the tests on the type of reference image to stack at the beginning are very interesting.\n- While I am not an expert on super-resolution, I do see a clever algorithm, that can be for example used with different number of input views. \n- The end-to-end framework is also quite interesting as it allows to be spread easily across the very large satellite images users, with the code aldready publicly available.\n- Lastly, the results are good wrt to the state-of-the-art, as the algorithm was proposed during a 2019 challenge and was in the top ones on the private leaderboard.\n\nRemarks and clarifications:\n- After looking at the challenge website, it is quite strange to find that the results values in Table 1, in terms of public and private scores, differ from the actual leaderboards, even if the metric is still the same. Thus, I was not able to understand (i) if the authors really participate to the challenge (ii) what method they presented during the challenge as 3 or 4 are shown here (iii) what was their ranking in the real leaderboard. It is important to be precise. From what I see, they might be second on the private score (the one that matters), if so it can be clarified also in the abstract where the word 'topped' was used.\n- cPSNR metric : (i) can you please explain the acronym; (ii) it was the competition metric, so it means it is not 'we use' but 'the challenge used' - which will also show that it was not a choice to satisfy your results, but a standard metric.\n- Lanczos interpolation: can you please explain in a small sentence in what this interpolation differs from the others?\n- ShiftNet: what is this network? We only know tha it is adapted from HomographyNet, but we don't have any information about how it is composed. We just know from App. 1 that it has a very large number of parameters (34M). Why is that? Why a rigid registration needs such a large number of parameters?\n- median anchor image: this is one of the interesting points of the paper. Can you please just clarify that you take a naive median image? Such as: for pixel (i,j), med(i,j)= med( LR1(i,j), LR2(i,j) ,...) .\n- You are saying 'each imageset is padded to 32 views'. What do you mean? I thought your network was able to be used with different numbers of views. How did you pad your imageset?\n- you list 'several baselines': for me, a baseline is something to compare to - usually, it is even something easy, such as the ESA baseline. But in your list, you are mixing baselines, other state-of-the-art methods (some of which you don't beat, so it's not a 'baseline'...), and your methods. It is very difficult to know which ones are yours. Please make different lists and/or identify yours in the table 1.\n- What is the Ensemble method? It appears that in the paper, you explain the 'HighRes-net + shiftNet' method, but actually, the 'Ensemble' is the better one, while not clearly described. I don't understand what outputs are averaged - I thought HighRes-net and shiftNet were performed simultaneously.\n- How did you select the hyperparameters of your model?\n\n\nScientific questions: \n- Is it possible to have views of different sizes as inputs? Or views with missing parts?\n- Usually, we have a high resolution image that is different in terms of type of acquisition (like a different type of satellite, but also true in medical images for ex.). Is your network ready for that? I mean: can we constrain the method even if it is not the same type of acquisition as ground truth? In that case, is that possible to have a super-resolution of the type of the input LR images?\n\nTypos: \n- is comprised of -> is composed of\n- in Table 7, the bold number should be the beta=infinity as it is the best one. It will be clearer, even if of course, a good train score does not mean a good method because of overfitting.\n", "belong_id": "HJxJ2h4tPr"}, {"uid": "rygMx9RTYS", "paper_title": "HighRes-net: Multi-Frame Super-Resolution by Recursive Fusion", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a framework including recursive fusion to co-registration and registration loss to solve the problem that the super-resolution results and the high-resolution labels are not pixel aligned.  Besides, the method is able to achieve good performance in the Proba-V Kelvin dataset. However, I have some concerns about this paper:\n\n1) This paper lacks many references. Recently, many works focus on multi-frame super-resolution containing video super-resolution and stereo image super-resolution via deep learning.  They are using multiple low-resolution image to construct high-resolution image. For example:\n\nStereo super-resolution:\n\nJeon, Daniel S., et al. 'Enhancing the spatial resolution of stereo images using a parallax prior.' *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2018.\n\nWang, Longguang, et al. 'Learning parallax attention for stereo image super-resolution.' *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2019.\n\nVideo super-resolution:\n\nTao, Xin, et al. 'Detail-revealing deep video super-resolution.' *Proceedings of the IEEE International Conference on Computer Vision*. 2017. \n\nFRVSR: Sajjadi, Mehdi SM, Raviteja Vemulapalli, and Matthew Brown. 'Frame-recurrent video super-resolution.' *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2018.\n\nFFCVSR: Yan, Bo, Chuming Lin, and Weimin Tan. 'Frame and Feature-Context Video Super-Resolution.' *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 33. 2019.\n\nEDVR: Wang, Xintao, et al. 'Edvr: Video restoration with enhanced deformable convolutional networks.' *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops*. 2019.\n\n2) Recursive fusion is aimed to fuse multiple low-resolution image information. Recently, more and more work utilize different methods to fuse multiple low-resolution image. For example, Tao et al proposes SPMC (Sub-pixel Motion Compensation) to align image, FRVSR uses unsupervised flow network that predicts optical flow to warp image, FFCVSR directly concatenate low-resolution image as the input of 2D convolutional network to fuse the information, and EDVR fuses multiple image features via utilizing deformable convolution. Thus, what is the advantage of recursive fusion compared to the above methods? This paper should discuss the difference between recursive fusion and the above methods.\n\n3) Registration loss is important in this paper and it can solve the problem the output SR is not pixel-wise aligned to the HR ground truth. Registration loss utilizes ShiftNet that is adapted from HomographyNet. Thus, what is the difference between ShiftNet and HomographyNet? This paper should add some details about ShiftNet and Lanczos interpolation. \n\n4) It is better to test more datasets and compare with more state-of-the-art methods. This paper only tests in a satellite image dataset. Some datasets can be considered such as VID4 dataset in video super-resolution.", "belong_id": "HJxJ2h4tPr"}, {"uid": "SJx-WbR0KB", "paper_title": "HighRes-net: Multi-Frame Super-Resolution by Recursive Fusion", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a multi-frame super-resolution method applied to satellite imagery. It first estimates a reference image for the multiple input LR images by median filtering. Then it pairwise encodes the reference image and each of the multiple images in a recursive fashion then fuses the corresponding feature maps with residual blocks and bottleneck layers until only one feature maps for the entire multiple images obtained. In other words, LR images are fused into a single global encoding. Then, it applies a standard upsampling network to obtain the super-resolved image this image is fed into a network that estimates only the translational shift, and the shifted image with the estimated translation parameters finally resampled. \n\nA major concern is the estimation of a single translational motion for the SR image at the end of the network after all multiple images are already fused. The fusion strategy disregards the underlying spatially varying motion. This explicitly assumes the images are on a flat surface, which perhaps an acceptable assumption for high-orbit satellite imagery where the ground surface depth variances might be negligible. Still, this is a very critical limitation of the method. Besides, I am not convinced that pair-wise fusion can handle significant translational fusion as the filters have shared parameters. How a single convolutional layer accomplishes a global encoding and compensates for any translation between any LR image pair is neither articulated nor convincing discussion and evaluations are provided. Of course, such a problematic approach needs at least some kind of motion compensation, which may explain the need for the ShiftNet layer at the end. Nevertheless, this seems quite problematic. \n\nEven assuming the method only applies to satellite imagery, it lacks mechanisms to compensate/distinguish cloud coverage and atmospheric distortions. Characterization of satellite imagery noise models (Weibull, etc.) common in such imagery as a prior also completely disregarded. For these reasons, the proposed method fails to be considered as a comprehensive approach for multi-image super-resolution of satellite imagery. \n\nNovelty-wise, there is very little as all modules have been commonly used for SR tasks. ", "belong_id": "HJxJ2h4tPr"}, {"uid": "BkgbSpbaKH", "paper_title": "Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper tries to ask if there is a good neural net architecture that works as effectively as gradient boosting decision trees on tabular data. The authors propose an architecture (NODE) that satisfies this conditions. NODE is an architecture consisting of differentiable oblivious decision trees that can be trained end to end via back propagation. The paper is readable and the experiments are well presented. They make use of an alpha-entmax transformation to obtain a differentiable architecture. The approach seems well motivated in the literature. It is unclear how novel the contribution is. It is unclear if in the experimental section the datasets used are standard for this classes of tasks. Would be good to mention if it is the case. ", "belong_id": "r1eiu2VtwH"}, {"uid": "SJepxs90tS", "paper_title": "Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Paper Summary:\n\nThe paper considers training oblivious trees ensemble with gradient descent by introducing a relaxation for feature selection and node thresholding. The relaxation is based on the recently introduced EntMax. The approach is compared with standard gradient boosting tree learning on benchmark datasets.\n\nReview Summary:\n\nThe paper reads well, is technically sound. The approach is novel and relevant to ICLR. Reference to related work are appropriate. Experimental comparison with CatBoost, neural nets could be more rigorous, more ablations could give a complete picture. Overall this is a good paper that gives an extra tool applicable to many practical settings.\n\nDetailed Review:\n\nThe introduction needs to define 'tabular data'. In your case, it seems that you mean mostly numerical heterogeneous features. Could you comment on using categorical features as well? \n\nThe method is clearly explained and references are appropriate, so most of my questions relate to the empirical setup and results.\n\nFirst, it seems to me that the paper would be much stronger if you were to reproduce the results from an established paper. If you take the catboost paper (arXiv:1706.09516v5 [cs.LG] 20 Jan 2019), the error on epsilon dataset is 10.9 which is better than the number your report, similarly click reports 15.6 error rate. To me, the paper would be much better if you simply added an FCNN and a NODE column to Table 2 and 3 of the catboost paper. It does not mean that your approach has to be better in all cases, but it will give a clear picture of when it is useful and it would clear any doubt on the tuning of the catboost baseline.\n\nSecond, the model you propose builds upon the densenet idea while the FCNN you compare with has no densenet connections. It would be fairer to consider neural net with this kind of residual.\n\nThird, I feel you need to report results over CPU as well. Boosted trees primary advantage is their low cost on regular CPU, the entmax formulation requires integrating over more leaves \nthan typical thresholded trees and it would be interesting to compare the effect on CPU. Reporting timing with batch and individual sample evaluation would make sense as well.\n\n As a side note, I would advise to define entmax with its equation. It is too recent to consider it should be known by the reader.\n\nOverall, this is a good paper than reads well. The method is novel, interesting and practical. With the extra experiments, it would make an excellent ICLR paper.", "belong_id": "r1eiu2VtwH"}, {"uid": "rkeiA3I19H", "paper_title": "Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a new method to make ensembles of decision trees differentiable, and trainable with (stochastic) gradient descent. The proposed technique relies on the concept of 'oblivious decision trees', which are a kind of decision trees that use the same classifier (i.e. a feature and threshold) for all the nodes that have the same depth. This means that for an oblivious decision tree of depth d, only d classifiers are learned. Said otherwise, an oblivious decision tree is a classifier that split the data using d splitting features, giving a decision table of size 2^d. To make oblivious decision trees differentiable, the authors propose to learn linear classifiers using all the features, but add a sparsity inducing operator on the weights of the classifiers (the entmax transformation). Similarly, the step function used to split the data is replaced by a continuous version (here a binary entmax transformation). Finally, the decision function is obtained by taking the outer product of all the scores of the classifiers: [c_1(x), 1-c_1(x)] o [c_2(x), 1-c_2(x)] ... This 'choice' operator transforms the d dimensional vectors of the classifier scores to a 2^d dimensional vector. Another interpretation of the proposed 'differentiable oblivious decision trees' is a two layer neural network, with sparsity on the weights of the first layer,\nand an activation function combining the entmax transformation and the outer product operator. The authors then propose to combine multiple differentiable decision trees in one layer, giving the neural decision oblivious ensemble (NODE). Finally, several NODE layers can be combined in a dense net fashion, to obtain a deep decision tree model. The proposed method is evaluated on 6 datasets (half classification, half regression), and compared to existing decision tree methods such as XGBoost or CatBoost, as well as feed forward neural networks.\n\nThe paper is clearly written, ideas are well presented, and it is easy to follow the derivation of the method. As a minor comment, I would suggest to the authors to give more details on the EntMax method, as it is quite important for the method, but not really introduced in the paper. The proposed algorithm is sound, and a nice way to make decision trees differentiable. One concern that I have though, is that it seems that NODE are close to fully connected neural networks, with sparsity on the weights. Indeed, I think that there are two ingredients in the paper to derive the method: adding sparsity to the weights and the outer product operator (as described in the previous paragraph). In particular, the improvement over vanilla feed forward neural networks seem small in the experimental section. I thus believe that it would be interesting to study if both two differences with feed forward networks are important, or if only is enough to get better results.\n\nTo conclude, I believe that this is a well written paper, proposing a differentiable version of decision trees which is interesting. However, the proposed method relies on existing techniques, such as EntMax, and I wonder if the (relatively small) improvement compared to feed forward network comes from these. I believe that it would thus be interesting to compare the method with feed forward network with sparsity on the weights. For now, I am putting a weak reject decision, but I am willing to reconsider my rating based on the author response.\n\nQuestions to the authors:\n(1) do you use the same data preprocessing for all methods (quantile transform)?\n(2) would it make sense to evaluate the effects of each the entmax and the outer product operator separately in the context of fully connected networks?\n", "belong_id": "r1eiu2VtwH"}, {"uid": "Syee2fmjYH", "paper_title": "Higher-Order Function Networks for Learning Composable 3D Object Representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a method for single image 3D reconstruction. It is inspired by implicit shape models, like presented in Park et al. and Mescheder et al., that given a latent code project 3D positions to signed distance, or occupancy values, respectively. However, instead of a latent vector, the proposed method directly outputs the network parameters of a second (mapping) network that displaces 3D points from a given canonical object, i.e., a unit sphere. As the second network maps 3D points to 3D points it is composable, which can be used to interpolate between different shapes. Evaluations are conducted on the standard ShapeNet dataset and the yields results close to the state-of-the-art, but using significantly less parameters.\n\nOverall, I am in favour of accepting this paper given some clarifications and improving the evaluations.\n\nThe core contribution of the paper is to estimate the network parameters conditioned on the input (i.e., the RGB image). As noted in the related work section this is not a completely new idea (cf. Schmidhuber, Ha et al.). There are a few more references that had similar ideas and might be worth adding: Brabandere et al. 'Dynamic Filter Networks', Klein et al. 'A dynamic convolutional layer for short range weather prediction', Riegler et al. 'Conditioned regression models for non-blind single image super-resolution', and maybe newer works along the line of Su et al. 'Pixel-Adaptive Convolutional Neural Networks'.\n\nThe input 3D points are sampled from a unit sphere. Does this imply any topological constraints? Is this the most suitable shape to sample from? How do you draw samples from the sphere (Similarly, how are the points sampled for the training objects)? What happens if you instead densely sample from a 3D box (similar to the implicit shape models)?\n\nOn page 4 the mapping network is described as a function that maps c-dimensional points to 3D points. What is c? Isn't it always 3, or how else is it possible to composite the mapping network?\n\nRegarding the main evaluation: The paper follows the 'standard' protocol on ShapeNet. Recently, Tatarchenko et al. showed in 'What Do Single-view 3D Reconstruction Networks Learn?' shortcomings of this evaluation scheme and proposed alternatives. It would be great if this paper could follow those recommendations to get better insights in the results.\nFurther, I could not find what k was set to in the evaluation of Tab. 1. It did also not match any numbers in Tab. 4 of the appendix. Tab. 4 shows to some extend the influence of k, but I would like to see a more extensive evaluation. How does performance change for larger k, and what happens if k is larger at testing then on at training, etc.?\n\nThings to improve the paper that did not impact the score:\n- The tables will look a lot nicer if booktab is used in LaTeX\n", "belong_id": "HJgfDREKDB"}, {"uid": "HyxflLtAYB", "paper_title": "Higher-Order Function Networks for Learning Composable 3D Object Representations", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThis paper describes a contextual encoding scheme for reconstruction of 3D pointclouds from 2D images. An encoder outputs the parameters of a hierarchy of reconstruction networks that can be applied in succession to map random samples on a unit sphere to the surface of the reconstructed shape. \n\nStrengths:\nThe author's model was quite novel in my opinion. Deep 2D->3D is becoming a crowded space and there are many other models that encode image inputs, and many others that perform recursive or composition-based decoding. However, the particular link here was interesting, and I appreciate the small number of parameters resulting in solid reconstruction performance. While most related work was covered well, I believe the authors could have a more up-to-date list of recent work that reconstructs triangle-mesh representations from images [A-C] (especially since several of these methods has an architecture that involves encoding and subsequent compositional refinement). \n\nSome of the reconstructions shown in this paper are quite impressive, and the quantitative results show outperforming 2 recent methods. I did appreciate also the novel path-based evaluation of shape accuracy in the Appendix, although it would have been helpful to see more discussion of this in the main paper. \n\nAreas for improvement:\nI found that the core technical description was quite brief and would have benefited from simply more detail and space. You have argued that your method is sensible to try (cog. sci motivations), and shown that one instance works, but what can we expect in a more mathematical or general sense? Can any sizes of encoder and mapping network fit together? How does the number of mapping layers effect performance? Won't we eventually expect vanishing/exploding gradients with particular activation and can one address this in some way? \n\nI note that recent papers in this field tend to perform significantly more extensive experimental evaluation, typically selecting a wider range of competitors and using a number of more standardized metrics including IOU, F1 score and CD and typically repeating these at a variety of resolutions or on additional datasets or category splits etc. \n\nDecision: \nWeak reject because the idea is quite interesting, but I believe a more thorough explanation and expanded experimental comparison would be of great help to ensure the community can appreciate this work. \n\nAdditional citations suggested: \n\n[A] Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. Wang, Zhang, Li, Fu, Liu and Jiang. ECCV 2018. \n[B] MeshCNN: A Network with an Edge. Hanocka, Hertz, Fish, Giryes, Fleishman and Cohen-Or. SIGGRAPH 2019.  \n[C] GEOMetrics: Exploiting Structure for Graph-Encoded Objects. Smith, Fujimoto, Romero and Meger. ICML 2019. ", "belong_id": "HJgfDREKDB"}, {"uid": "SklBkB-f9r", "paper_title": "Higher-Order Function Networks for Learning Composable 3D Object Representations", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work is focused on learning 3D object representations (decoders) that can be computed more efficiently than existing methods.  The computational inefficiency of these methods is that you learn a (big) fixed decoder for all objects (all z latents), and then need to apply it individually on either each point cloud point you want to produce, or each voxel in the output (this problem exists for both the class of methods that deform a uniform distribution R^3 -> R^3 a la FoldingNet, or directly predict the 3D function R^3 -> R e.g. DeepSDF). The authors propose that the encoder directly predict the weights and biases of a decoder network that, since it is specific to the particular object being reconstructed, can be much smaller and thus much cheaper to compute.\n\nThe authors then note the fact that their method lacks a continuous latent space that allows for interpolation, as provided by existing (VAE-like) methods. They propose to solve this by learning an MLP that produces the output by recurrent application, and then composing subapplications of different networks as a type of interpolation.\n\n-------------------\n\nI like this work, it addresses a real problem in a number of models for 3D representation learning (similar models are also used for e.g. cryo-EM reconstruction). While the fast weights approach is not totally original, its application to this problem is novel and very well-suited to it. I was a bit surprised by just how much the decoder network could be shrunk by using fast weights. \n\nThe paper is also quite well written. I especially like how Section 2 synthesizes existing work into model categories which make it easier to think about their relationships. I also think the explanation in Sec. 3.2, while kind of obvious, is a nice way think about decoder vs. fast weights.\n\nI like that the authors are straightforward about the deficiency of the method (i.e. that you can't interpolate in latent space). Their proposed solution of functional composition is exceedingly clever but in my opinion too impractical to really be useful. It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end? The purpose of generative models is not to interpolate per se; the interpolation is really a sanity check that the model is capturing the underlying distribution rather than just memorizing training examples. The function composition doesn't capture that. I think the authors should just acknowledge that you can't soundly *sample* from their generative model the way e.g. VAE or GAN allows (their function composition is not a sampling method). But I think there are lots of useful things you can do without that capability, e.g. do 3D point cloud completion, go image -> structure, etc. I think this function composition angle should be deemphasized in the title/abstract, but I think the paper stands  reasonably on its own without that.\n\nNits:\n- In Figure 2 it's pretty hard to see the differences between the methods. What exactly is being visualized here? DeepSDF shold be visualizing surface normals vs. HOF which is point clouds, right?\n- For predicting a deformation R^3 -> R^3 function composition sort of makes sense, but how generalizable is this approach e.g. to directly predicting a function R^3 -> R (a la DeepSDF)? I think there are ways this function composition approach could generalize, e.g. using skip connections and layer dropout (which encourages layers to be composable).\n", "belong_id": "HJgfDREKDB"}, {"uid": "HJex9qu3tH", "paper_title": "Variational Autoencoders with Normalizing Flow Decoders", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a combination of a conditioned flow-based model with a VAE. The main contribution of the paper is a two-phase training that allows to train the model. Unfortunately, a joint training of the model failed. In general, combining VAEs with flow-based models is an important research direction. Unfortunately, the paper is not clearly written. A lot of details are missing that makes the paper impossible to reproduce and fully understand. Moreover, the most interesting part about training procedure, is discussed superficially. Finally, I find the comparison to SOTA methods disappointing. The authors skipped many recent papers. I do not expect to see SOTA results among all models, but at least comparable results within a group of approaches. However, the authors provide only a small subset of models that makes me wonder whether they are aware of other papers.\n\nRemarks:\n- The following statement is not fully correct:\n'In our implementation we use a normalizing flow similar in structure to Real NVP Dinh et al. (2016) (which is a special case of autoregressive normalizing flows Papamakarios et al. (2017)), as it allows both efficient training and sampling'\nReal NVP is a bipartite flow, while Masked Autoregressive Flow is an autoregressive flow. In special cases, these two flows coincide, but their original implementations are different.\n\n- The paper misses a lot of important details. For instance, a reader needs to figure out what is the objective function. Further, the authors do not mention how they deal with images represented by integers. Do they use dequantization as in other papers (e.g., Theis, L., van den Oord, A., and Bethge, M.  A note on the evaluation of generative models. In Workshop Track,ICLR, 2016)?  These are very important details to fully understand the proposed approach. Providing a very general diagram (Figure 1) and generic equations (2-4) are not sufficient. Currently, there are different implementations of flow components, and describing them would be definitely beneficial. Moreover, it is important to show how conditioning is used in the flow model.\n\n- Section 3.4 is incredibly interesting part of the paper and it should be further analyzed. The proposed two-phase is reasonable, but it leaves an open question why a joint training fails. I agree with the authors that a possible explanation is training instability. Nevertheless, I would be more than interested in seeing a more thorough analysis of this phenomenon.\n\n- I do not understand why the authors skipped reporting bpd for CelebA. \n\n- In general, the comparison in Table 2 is far from being complete. For instance, please see the following paper:\nHo, J., Chen, X., Srinivas, A., Duan, Y., & Abbeel, P. (2019). Flow++: Improving flow-based generative models with variational dequantization and architecture design. arXiv preprint arXiv:1902.00275.\nOn Cifar10, current non-autoregressive models are able to achieve 3.08 bpd (Flow++)  and 3.11 bpd (IAF-VAE). This is much better than the reported 3.17 bpd.\n\n- In Section 4.2.2, first line, a number to a figure is missing.\n\n======== AFTER REBUTTAL ========\nI would like to thank the authors for their rebuttal. However, I am not fully satisfied with some answers. First of all, the paper should be clearly written so that a reader could be able to find all necessary details and seamlessly implement the presented idea. If some details do not fit the main text, then they should be included in an appendix. Second, the objective function is an important component of any ML problem and, therefore, it should be included in the paper. Sometimes a verbal description is not sufficient. Third, by conditioning a flow I meant whether the based distribution was conditional or/and a single flow layer was conditioned on z. Currently, there are multiple ways to condition and I was curious which of them was used by the authors. Fourth, I do not believe that achieving nicely looking images is the ultimate task for generative models. However, this is an open discussion. Fifth, I agree, the discussion on why the joint training failed is an incredibly interesting question.\nAgain, I really appreciate all the answers, and I believe that the authors did their best to improve the paper. However, as a reviewer I must look for novelty and evaluate how the paper is readable for others. In my opinion, combining VAE with GLOW is not sufficiently novel idea. If the paper was very clearly written, I would think about rising the paper. However, right now, I decide to keep my original score.", "belong_id": "r1eh30NFwB"}, {"uid": "B1e3Kw7atH", "paper_title": "Variational Autoencoders with Normalizing Flow Decoders", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a variant of the variational autoencoder, specifically by using a simplified normalizing flow model as the decoder. When compared with Glow, the proposed method is simpler and more efficient to train. The authors applied their algorithm on a few datasets, and showed better or competitive performance, both qualitatively and quantitatively.\n\nThis paper is in general well written. I think the idea looks interesting, although the novelty is a bit incremental, as it basically combined the two well-known models (VAE and Glow). The experimental results showed the promise of the new method, which could be more convincing if applied on larger scale datasets. My detailed comments and questions are as follows.\n1. Regarding training, the authors decomposed it into two phases, i.e., training VAE first and then Glow. The authors also mentioned that jointly training resulted in images with poor qualities. I am curious about how the authors designed the Glow model: Intuitively a larger model may have more modeling capacity, but at the cost of computational cost. Some ablation studies or explanation could be helpful.\n2. The authors claimed at the beginning of Section 3 that the their normalizing flow 'should not need to do as much work as a full marginal normalizing flow model such as Glow'. I am wondering how the performance will be for the Glow used, if without the VAE part? \n3. For the bits/dim results for Glow in Table 2, was it computed by yourself or just from the Glow paper? I saw the FID score was obtained by yourself.\n4. For the Glow used in experiments, how does its architecture compare with the one used in the original paper?\n5. I am a bit surprised to see the results in Table 3 and Figure 3, as Glow has a better FID score but the overall image quality is worse. Is it related to the size of the Glow used?", "belong_id": "r1eh30NFwB"}, {"uid": "rJgx7oKM5B", "paper_title": "Variational Autoencoders with Normalizing Flow Decoders", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes adding additional flow layers on the decoder of VAEs. The authors make two claims\n1. The proposed model achieves better image quality than a standalone Glow.\n2. The proposed model is faster to train than Glows.\nThe intuition is a VAE can learn a distribution close enough to be target distribution, and the Glow only needs to do much less work than standalone Glow, hence faster. Some positive results are reported in the experiments, including better image quality, faster training time, and the Glow indeed sharpens the output of VAEs. \n\nThe paper indeed has some good results, particularly they can achieve it only with single-scale Glows with additive coupling layers. However, I think the claims are not sufficiently supported. Taking point 1 as an example, it is not clear to me why VAE+Glow is better than a standalone Glow. Imagine two models\n\nM1: VAE+Glow (proposed in the paper)\nM2: Glow0+Glow (standalone Glow)\n\nsharing the last 'Glow' part. M1 is better than M2 implies 'VAE' is more powerful than 'Glow0', which I doubt. Similarly, for point 2, it is not clear to me why 'VAE' is faster than 'Glow0'. I think comparing the proposed model with IAF make more sense, because the proposed model just adds flows to the decoder and the prior. But the relationship with Glows needs to be considered more thoroughly.\n\nAnother confusing detail for me is the two-stage training in Sec. 3.4. The explanation 'likely because the Glow layer is unable to train efficiently with a changing base distribution' doesn't make sense. Because IAF does successfully train their q-net without 2-stage training. There might be other reasons?\n\nThe baselines are not strong enough. Most importantly, Flow++ [1] reports a likelihood 3.08 on Cifar10 with standalone flows, which should also be a part of the baseline. I also wonders whether the proposed model benefits from deeper model, like standalone flows do. Will standalone flows surpasses the proposed model as the number of layers goes to infinity?\n\n[1] Ho, Jonathan, et al. 'Flow++: Improving flow-based generative models with variational dequantization and architecture design.' arXiv preprint arXiv:1902.00275 (2019).\n\nFinally, the paper is somewhat incremental. Particularly comparing with VAE-IAF, where this paper just adds flow layers to not only q but also p.\n\nUpdate\n=====\n\nAfter reading the rebuttal I found some of my concerns are unaddressed. (regarding to the two-stage training, and the novelty)\n\nPoint 1 is still not answered. The answer I expect to have is how 'The interaction between two models when they are being stacked may affect the overall performance in such a way that it is more than just the sum of its parts.' Why are these two models perform particularly well when combined? The purpose of my initial question is for some in-depth analysis and intuition / theory.\n\nTherefore I will keep my score unchanged.", "belong_id": "r1eh30NFwB"}, {"uid": "H1lojMTjKH", "paper_title": "Blending Diverse Physical Priors with Neural Networks", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "=== Overall comments ===\nThis paper proposes to generalize approaches to physics-based learning (PBL) by performing network architecture search (NAS) over elements from PBL models found in the literature. This entails including physical inputs to the network and the incorporation of new operations to the NAS. I think the idea has merit and rather like it. However, there are several aspects of the work that could be improved. The technical novelty is small, as the extension of the  existing NAS models to handle physical inputs and a few new operators is relatively straightforward. The experiments, while well designed, only explore uninteresting toy problems. While I appreciate the necessity to explore the methods performance in a more controlled setting, a more impactful testbed would be more convincing. Another drawback of the evaluation is the lack of a proper statistical analysis of the results, given the small data and model sizes.\n\n\n=== Relevance & Prior Work ===\n+ The related work gives a good summary and categorization of prior work in physics-based learning\n+ The problem (physics-based learning) is interesting and relevant to the community\n\n\n=== Novelty & Approach===\n+ application of NAS to physics based learning\n+ incorporation of physics solutions as inputs into differentiable NAS\n+ creation of physics-informed operation sets to merge physical models into network\n- technical steps to merge NAS and PBL are relatively straightforward\n\n\n=== Evaluation ===\nTwo representative physical simulations were chosen for evaluation, where elements of the physics model are intentionally omitted,  1) estimating trajectory of a ball in presence of wind and air resistance, and 2) a collision speed simulation where two objects collide, where sliding friction is not accounted for in the physics model.\n\nThe baselines consist of: a 3-layer MLP (data-driven), a 3-layer MLP with Physical Regularization, a 3-layer MLP with residual connection to the physics prediction, an MLP with two input branches, on for the data and one for the physics predictions (Physical Fusion), and the Embedded Physics model which estimates parameters for the physics modelu using a 3-layer MLP.\n\nPhysicsNAS can combine elements of the baseline models, but the total number of nodes is limited to 5.\n\n+ Experiments testing the dependence of the model on the numbers of samples and the strength of the physical inconsistencies were conducted. In both cases, PhysicsNAS outperformed the best specialized physics models.\n\n- The chosen testbed tasks are toy problems. While these types of experiments are necessary to understand the performance of the model, it would have been interesting to see PhysicsNAS applied to a more impactful task\n\n- Given the size of the networks and the training data, there is no reason why a more sophisticated statistical analysis of the results wasnt performed (confidence intervals, t-test, p-value). Similarly, a more complete set of experiments with more sample amounts could be provided with little effort.\n\n\n=== Clarity & Other Comments ===\n- precious nodes -> previous nodes\n", "belong_id": "HkeQ6ANYDB"}, {"uid": "HkxVmBAy9S", "paper_title": "Blending Diverse Physical Priors with Neural Networks", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes PhysicsNAS, which proposes a method to automatically design architectures that\nincorporate domain expertise from phyiscs-based models while also accounting for potential\nmismatch between the model and real world due to unaccountable factors. While existing work seem\nto incorporate such information via one of 4 standard ways (given on page 2), the proposed work\nattempts to meld them so as to find the optimal combination for the problem at hand and the data\navailable.\n\nWhile I don't see anything fundamentally wrong with the paper, I do not feel that the technical\ncontributions are substantial enough to warrant acceptance at ICLR.\nMore specifically, the methodological novelty is limited and the experimental evaluation only\nevaluates the method on two fairly simple problems. \n\nOn a positive note, the authors have done a good job of illustrating the idea and have compared it\nto most natural baselines. I also thought that the illustrations of the architectures found\nfor different sample sizes (in the Appendix) quite insightful.\n\nI encourage the authors to pursue this line of work, but test this on more complex prediction tasks\nwhere entirely model-based approaches are unreliable, and entirely black-box estimators are sample\ninefficient. It also seems that the approach need not be confined to physics per se - in many\nproblems in chemistry, materials science etc. scientists are looking for ways to incorporate domain\nexpertise while accounting for model-mismatch. \n", "belong_id": "HkeQ6ANYDB"}, {"uid": "rklrUUyI5B", "paper_title": "Blending Diverse Physical Priors with Neural Networks", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors apply neural architecture search techniques to the problem of physics based learning. It is interesting because it cleverly tackles the challenge of manually designing priors and network architectures. The results are also impressive as the proposed method surpasses all the considered baselines. Despite of the above upsides, I have the following questions/concerns.\n1. There is limited technical novelty as the entire method is mainly based on previous work on neural architecture search. Nevertheless, it might be helpful to have some ablation study to show the improvement of the task-specific adaptations presented in the paper, with which I believe this could be a good paper on the application side.\n2. I'm curious about the performance of the baseline methods given the same amount of computation. For example, is it possible to perform intensive hyperparameter tuning for the baselines to also obtain improvement. It seems that the authors did not discuss the computational costs and whether different methods are compared given the same cost.", "belong_id": "HkeQ6ANYDB"}, {"uid": "HJengQ5IYH", "paper_title": "Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the topic of learning with noisy labels, in particular, classification problem where the labels are randomly flipped with some probability. The main technical contributions of this paper are two folds: 1) proof of generalization bounds for kernel ridge regression solutions that depends on the clean labels only. 2) two regularization techniques that are shown to be equivalent to the kernel ridge regression when the neural networks approach the neural tangent kernel regime.\n\nNumerical experiments are performed to verify that the proposed methods indeed helps over the baseline at fighting noisy labels. One weak point of the paper is that there is no comparison to any other methods designed to learn under noisy labels. Even though the paper states that the primary advantage of the proposed methods is simplicity, it would still be good to have some empirical comparison for reference.\n\nI like that the paper has a section to explicit check whether the neural networks used in the experiments are in neural tangent kernel regime. However, I'm not very sure how to interpret the scale of the Frobenius norm in the change of the weights. Maybe one alternative approach is to compare the difference between the two neural tagent kernel --- one defined by theta(0), and one defined by theta(t) after training. Alternatively, maybe the experiments can be carried out with recent techniques to perform learning directly on infinite width networks (e.g. https://arxiv.org/abs/1904.11955 ).", "belong_id": "Hke3gyHYwH"}, {"uid": "S1xDkcCjtB", "paper_title": "Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, based on the effectiveness of early stopping in the training of noisily labeled samples, the authors proposed two intuitive (and novel) regularization methods: (1) regularizing using distance to initialization (2) adding an auxiliary variable b_i for every input x_i during training. In terms of theory, the authors showed that in the NTK regime, both regularization methods trained with gradient descent are equivalent to kernel ridge regression.  Moreover, the authors also provided a generalization bound of the solution on the clean data distribution when trained with noisy label, which was not addressed in previous research.\n\nOverall, the paper is very well-organized and well-written. The contribution of the paper is significant, and numerical results also vindicate the theory developed in the manuscript. I recommend accepting the paper.\n\nTwo minor questions that are not going to affect my rating:\n1. The theory developed in the paper depends highly on NTK. What if the network is not sufficiently wide (which is usually the case in practice), and the loss function is not L2?\n2. In the second method using the auxiliary variable, it seems that every training sample x_i needs a variable b_i. Is this going to cause any problem in practice if data augmentation is used?\n", "belong_id": "Hke3gyHYwH"}, {"uid": "S1gq-N_pYH", "paper_title": "Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes two regularization methods for learning on noisily labeled data: the first penalizes the distance w.r.t. Euclidean norm from an initial point and the second uses an additional auxiliary variable for each example to learn a noise. In the theoretical part, the paper shows that an original clean dataset can be learned from a noisily labeled dataset based on NTK-theory. Finally, the effectiveness of proposed regularization methods is well verified empirically for 2-layer NN, CNN, and ResNet on image classification tasks (MNIST, CIFAR-10).\n\nContributions:\n- Propose two simple regularization techniques for learning from a noisily labeled dataset.\n- Give generalization guarantees for these methods\n\nClarity:\nThe paper is well organized and easy to read.\n\nQuality:\nThe work is of good quality and is technically sound.\n\nSignificance:\nSince proposed methods are in some sense related to the early-stopping for the (stochastic) gradient descent, the developed theory is useful in understanding the generalization ability of over-parameterized neural networks falling into NTK-regime. Although an additive noise setting for the regression problem is rather common in statistical learning theory, an artificially flipped label setting for the classification problem may be new except for a few recent studies [Li+(2019)]. A result (Theorem 5.1) for the regression problem with the squared loss is not so surprising because the generalization error of gradient descent in a high-dimensional space (e.g., over-parameterized NNs) or an RKHS (i.e., infinite-dimensional model) has been well studied and the generalization error is composed of the (constant) variance and the distance between the model output and the true label. Thus, existing results of generalization error analyses for the regression are directly applied to bound the prediction error for clean labels. However, I basically like this paper and I think this paper makes a certain contribution to understanding the effect of over-parameterization.\n\nA few questions:\n- Usually, the regularization parameter goes to zero as the number of examples increases. Conversely, the regularization parameter in the proposed methods also increases. Could you explain why this difference happens?\n- In classification tasks, the original problem setting is recovered by setting $p=lamba=0$. However, the generalization bound by Theorem 5.3 is still affected by $\\lambda$. Is this theorem tight?\n\n-----\nUpdate: \nI thank the authors for the response. I have read the revised version of the paper and have confirmed an improvement of Theorem 5.2. I will keep my score. This paper is of good quality.", "belong_id": "Hke3gyHYwH"}, {"uid": "SkxYYEW4qr", "paper_title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I am not from this area and don't know much about reinforcement learning.\n\nThe paper discusses zero shot generalization (adaptation) into new environments. The authors propose an approach and then show results on Grid-World, Super Mario Bros, and 3D Robotics. \n\nIn the training environment E1 = (S, A, p) the algorithm sees a bank of exploratory trajectories \\tau_i = {(s_t, a_t)}_{t=1}^{T} but not rewards. The authors  then say that algorithm is tested on the test environment E2. They ' propose to only inform the new task per trajectory terminal evaluation r( ) in E1' to give the training signal (where r is the reward).\n\nI am a bit confused by this setting. The model never sees any rewards for E1 but it does see rewards for E2? How is this zero shot?\n\nThe authors then propose their approach, I wish some of it had been described more rigorously with math (e.g. the loss etc.) so it was easier to understand for people not in the domain and familiar with some of the terminology. \n\nEmpirically the authors show results for 3 datasets and this seems thorough. ", "belong_id": "HyeuP2EtDB"}, {"uid": "HyghHQBr9S", "paper_title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper describes a method that aims to learn task-agnostic priors for zero-shot generalization. The main idea is to employ the following modeling approach on top of the model-based RL framework: a local convolution network is used to compute a score for each local state action pair, and then another network is used to aggregate all the scores. While the problem being studied is important and the experimental results seem positive, there are a few concerns.\n\nFirst, the baselines presented in the experiments are relatively weak. In Related Work, the authors discuss the differences between the proposed method and the related methods, but few of the related methods are used as baselines for comparison with the proposed method. Moreover, the experiments are quite insufficient in terms of ablating different components of the proposed methods.\n\nSecond, essentially the proposed method is trying to solve the zero-shot generalization by parameter initialization; a model is pretrained on related tasks and used as initializations for target tasks. The authors claim that it is different from prior work mainly because of the neural architecture that deals with sparse rewards via score aggregation. While the proposed architecture might be more suitable for solving tasks with sparse rewards, it is not intuitive whether it has something to do with learning zero-shot generalization. And apparently, the method will also rely on the similarity between the pretrained task and the target task, and such a scope constraint is not discussed in the paper. In other words, I'm not quite sure a better architecture is fundamental progress towards zero-shot RL.", "belong_id": "HyeuP2EtDB"}, {"uid": "r1lfJzG3qS", "paper_title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a framework (Scoring-Aggregating-Planning (SAP)) for learning task-agnostic priors that allow generalization to new tasks without finetuning. The motivation for this is very clear - humans can perform much better than machines in zero-shot conditions because humans have learned priors about objects, semantics, physics, etc. This is achieved by learning a scoring function based on the final reward and a self-supervised learned dynamics model.\n\nOverall, the paper is very clear and easy to follow.\nThe presented task is realistic and important, and the paper seems to address it in a reasonable approach. \nHowever, the evaluation seems lacking to me - the evaluation convinced me that SAP works, but I am not convinced that it works better than existing approaches (see below), and especially did not convince me that it is better in the zero-shot test environment.\nThe (anonymized) website contains nice videos that support the submission.\n\nQuestions for the authors:\n\n1. Page 3, 3rd paragraph of Section 3: the paper says that 'The proposed formulation requires much less information and thus more realistic and feasible' - I agree that this is more realistic, but is it really more feasible? The requirement of much less information makes the proposed formulation much more sparse.\n\n2. A basic assumption in the SAP framework is that a local region score is a sum of all the sub-regions. As phrased in the paper: 'in the physical world, there is usually some level of rotational or transnational invariance'. I'm not sure that this assumption makes sense neither in the Mario case or in other tasks, e.g., robotics. Doesn't it matter if you have a 'turtle' right in front of you (which means that the turtle is going to hit you), or below you (which means that you are going hit the turtle)?\n\n3. A question about the planning phase - page 5 says: 'We select the action sequence that gives us the best-aggregated score and execute the first action'. Do you select the entire sequence of actions in the new environment in advance? Can the agent observe the new state after every action, and decide on the next action based on the actual step that the action has reached, rather than on the state that was approximated in advance?\nIn other words - what happens if the first action in the new test environment yields an unexpected state, that was not predicted well by the dynamics model; does the agent continue on the initial planned trajectory (that ignores the 'surprise'), or does it compute its next action based on the unexpected state?\n \n4. Experiments: in Gridworld and Mario - are there any stronger baselines in the literature, or reductions of known baselines to the zero-shot scenario? Are the chosen 'Human Priors', BC-random and BC-SAP just strawmen? \nSince the main goal of this paper is the zero-shot task, what would convince me is a state-of-the-art model that does possibly *better than SAP on the training level*, but *worse than SAP in generalizing to the new level*. Additionally, are there other baselines that specifically address the zero-shot task in the literature?\n\nMinor (did not impact score):\nPage 2, 1st paragraph: '... we show that how an intelligent agent'...\nPage 3, 3rd paragraph: '... in model-free RL problem' - missing an 'a' or 'problem*s*'?\nPage 3, 3rd paragraph: '. Model based method ...' - missing an 'a' as well?\nPage 4, 1st paragraph:: '... utilizing the to get the ...'\nPage 4, last row: missing a dot after the loss equation, before the word 'In'.\nPage 7, Table 1: 'BC-random' is called 'BC-data' in the text. Aren't they the same thing?\n", "belong_id": "HyeuP2EtDB"}, {"uid": "SJl91FP2tH", "paper_title": "INFERENCE, PREDICTION, AND ENTROPY RATE OF CONTINUOUS-TIME, DISCRETE-EVENT PROCESSES", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present a model for time series which are represented as discrete events in continuous time and describe methods for doing parameter inference, future event prediction and entropy rate estimation for such processes. Their model is based on models for Bayesian Structure prediction where they add the temporal dimension in a rigorous way while solving several technical challenges in the interim. The writing style is lucid and the illustrations are helpful and high quality. \n\nHowever, the paper and the technical challenges which the authors faced have been extensively studied in the closely related field of (marked) temporal point process modeling, which also models processes with discrete events in continuous time. Hence, the claim 'When it comes to continuous time, discrete-event predictors, far less has been done' is mildly erroneous. The literature in that field has provided several ways of handling the technical challenges which authors have tried to overcome. For example, excellent predictor for the next event as well as time has been proposed by Du. et. al. (2016), Mei and Eisner (2017) and, very recently, by Turkmen et. al (2019), among many many others. Also, probability model (i.e. \\phi_s(t)) of the next event time can be represented using intensity functions \\lambda(t). However, if the authors require an explicit likelihood function, Normalized Flows with re-parametrization, can provide that too (see Kobyzev (2019) for a review of recent work in the area). This while being a Neural Network based approach well-weathered models and does not require numerical integration. Without placing this work in context of these works, it is very difficult to judge the contributions of the paper well.\n\nThen there are a few unanswered questions about the model:\n\n 1. In the Background, where the authors describe the generative process, does the emitted symbol also depend on the dwell time \\tau (like Mei and Eisner, 2017), or is the distribution of the dwell time and the next symbol independent given the state (like Du et. al)?\n 2. Similarly, during entropy calculation, while breaking the entropy term from (9) to (10), shouldn't there be a dependence of the discrete symbols on the continuous dwell times of the prior step?\n 3. We need significantly more information about the training used for the LSTM/RNN models to be able to judge whether the performance of the methods is comparable or not. These could be provided in a supplementary material, if they do not fit within the main text.\n\n\nSome other ways of improving the paper:\n - The reference style changes frequently in the paper and is mutually inconsistent.\n - The \\pi in Eqn. (3) is not formally defined. If it is the initial state probability, then should it be dependent on the model \\Mcal and parameters \\theta?\n - The illustrations, though useful, are too large and use up space.\n - Though the task of determining the entropy is indeed intellectually satisfying, it would very much help motivate the reader if there were some applications which the authors could allude to.\n\n\nCitations:\n - Aalen, Odd, Ornulf Borgan, and Hakon Gjessing. Survival and event history analysis: a process point of view. Springer Science & Business Media, 2008.\n - Du, Nan, et al. 'Recurrent marked temporal point processes: Embedding event history to vector.' Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.\n - Mei, Hongyuan, and Jason M. Eisner. 'The neural hawkes process: A neurally self-modulating multivariate point process.' Advances in Neural Information Processing Systems. 2017.\n - Turkmen, Ali Caner, Yuyang Wang, and Alexander J. Smola. 'FastPoint: Scalable Deep Point Processes.'\n - Kobyzev, Ivan, Simon Prince, and Marcus A. Brubaker. 'Normalizing flows: Introduction and ideas.' arXiv preprint arXiv:1908.09257 (2019).\n", "belong_id": "B1gn-pEKwH"}, {"uid": "S1xR2dmTFr", "paper_title": "INFERENCE, PREDICTION, AND ENTROPY RATE OF CONTINUOUS-TIME, DISCRETE-EVENT PROCESSES", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposed a model for continuous-time, discrete events prediction and entropy rate estimation by combining unifilar hidden semi-Markov model and neural networks where the dwell time distribution is represented by a shallow neural network. \n\nComments:\nThe literature review on previous work for continuous-time, discrete events prediction is not thorough enough. For this problem, there are continuous-time Markov networks [El-Hay et al. 2006], continuous-time Bayesian networks [Nodelman et al. 2002] and its counterpart in relational learning domain, i.e. relational continuous-time Bayesian networks [Yang et al. 2016]. The authors should have learned their work and addressed the difference between the proposed model and these work in the related work section.\n\nTal El-Hay, Nir Friedman, Daphne Koller, and Raz Kupferman. Continuous Time Markov Networks. In UAI, 2006.\nNodelman, U.; Shelton, C.; and Koller, D. Continuous Time Bayesian Networks. In Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence (UAI), pages 378387, 2002.\nShuo Yang, Tushar Khot, Kristian Kersting, and Sriraam Natarajan. Learning continuous-time Bayesian networks in relational domains: A non-parametric approach. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pages 22652271, 2016. \n\nThe approximations made to get equation (3) and equation (4) lacks theoretical proof for the up-bound of its influence on the model performance. If these assumptions have been made before, please cite the references; if not, please illustrate why it makes sense and how the approximation will influence the value of the objective function.\n\nPlease explicitly state the meaning of all the symbols used in the paper even it can be inferred by the readers. E.g. n, which first appears in Equation (5) and is never being explained.\n\nThe experiments are rather simple both in terms of the model used to generate the data and the number of different data sets being used. Hence, the experimental results are not strong enough to support the claims made by this paper. \nSpecifically, it claims With very little data, a two-state model shown in Fig. 3 is deemed most likely; but as the amount of data increases, the correct four-state model eventually takes precedence, but in Figure 3, the plot with the highest BIC score when the training sample is less is the green curve which stands for the six-state model according to the legend. \nThe corresponding mean-squared errors for the three methods are shown in Fig. 3(bottom) for two different dataset sizes. I could not find it in Figure 3.\n", "belong_id": "B1gn-pEKwH"}, {"uid": "B1ljx_SEcH", "paper_title": "INFERENCE, PREDICTION, AND ENTROPY RATE OF CONTINUOUS-TIME, DISCRETE-EVENT PROCESSES", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper focuses on the problem of modeling, predicting and estimating entropy information over continuous-time discrete event processes. Specifically, the paper leverages unifilar HSMM's for model inference and then uses the inferred states to make future predictions. The authors also use inferred model with previously developed techniques for estimating entropy rate. The authors describe the methods and provide the evidence of the effectiveness of their method with experiments on a synthetic dataset.\n\nThe paper should be rejected as it doesn't appear to be fit for this conference. While the paper does build techniques that leverages classical statistical learning and combine them with deep learning, it falls short on various aspects. Specifically, the paper neither provides any rigorous analysis of the developed methods so as to be useful to theoretical learning community nor does it support its approaches with strong empirical evidence so as to be of practical importance. \n\n- While the authors cite lack of work in such data setting (which is actually incorrect as described below), they do not motivate or justify why such an approach of using either unifilar HSMM or Bayesian inference techniques are good starting point to model such information. The authors claim that unifilar HSMM is more of a restriction to make the three proposed tasks easier. But such a model assumption is very strict and will likely lead to large model mis-specification issues when used in real-world. The author does not describe what can be done in this scenario.\n\n- Page2, Section 3: the statistics of {(x_i, t_i)} are unchanging in time. What does this mean?\n- As stated by authors, to make the approach tractable, they have to adopt a slew of approximations which seems to put severe restriction on what kind of data can be modeled and inferred on using these approaches.\n- It is not clear why one wants to estimate the entropy rate in such settings and authors avoid discussing it in Section 4.2\nsimilarly the authors explicitly avoid discussing bias-variance tradeoff of such method, however, this makes the paper\ndevoid of any insights into the proposed approach.\n- How was k chosen to be 3 in Eq 10? \n- Another major miss for the paper is that there has been lot of work at the intersection of stochastic processes (e.g. temporal point processes) and machine learning techniques that aim to model exactly the continuous-time discrete event information and the paper does neither cites any of them nor discusses them or compare with them. We encourage the authors to look at  the relevant literature and position their work in comparison to these existing approaches. I provide a few references here as starting point, however, lot of follow-up work are available:\n\nPoint Processes and Machine Learning:\n------------------------------------\nModeling the Dynamics of Learning Activity on the Web\nSmart Broadcasting: Do You Want to be Seen?, Karimi et. al.\nUncovering the Temporal Dynamics of Diffusion Networks, Gomez-Rodriguez et. al.\n\nHidden semi-markov models:\n-------------------------\nRecurrent Hidden Semi-Markov Model, Dai et. al. \n\nDeep learning and temporal modeling:\n-----------------------------------\nRecurrent Marked Temporal Point Processes: Embedding Event History to Vector, Du et. al.\nThe Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process, Mei et. al.\n\n- Finally, the authors show only one synthetic experiment. This is far from providing convincing evidence of the efficacy of the method and certainly needs a lot more work for acceptance in any machine learning conference.  The authors, at the least, must use their approaches on couple of real-world dataset (e.g. earthquake information, animal behavior as they mention or others) and also compare with the above stochastic processes techniques to test how their methods fare. \n\n- Also, the analysis of efficiency of their method is required. It appears that their methods may not be scalable for instance to a multi-dimensional discrete label case.\n\n- Figure 4 is not correctly referenced in the paragraph right above Section 4.3. ", "belong_id": "B1gn-pEKwH"}, {"uid": "rkgnCl8MKS", "paper_title": "Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary and Decision \n\nThe authors of this paper studied the optimization and generalization properties of shallow ReLU networks. In particular, the authors were able to show a width dependence that is polylogarithmic on the number of samples, probability or failure, error tolerance, and a margin parameter. This work is unique in that the authors showed how to bound many key quantities in terms of the margin parameter, and drew a connection with the neural tangent kernel's maximum margin. Furthermore, the overall reading experience was very smooth, although I do have some minor comments later. \n\nThe main concern from me is on the implicit dependence of the margin parameter \\gamma, most notably this can lead to the width dependence to be polynomial in terms of the number of samples and the minimum separation distance. While this concern warrants a careful discussion (below), I believe the paper still offers a nice analysis of shallow networks. \n\nOverall, I would recommend accept for this paper. \n\n\nBackground \n\nThere has been a large number of works studying very wide networks, showing both optimization and generalization results. While there has been great progress, most of these existing results require the width of both deep (and shallow) networks to be very large. For example, even by being polynomial in the number of samples, the networks are already unrealizable in practice. Therefore, guarantees with much better dependence is highly desirable. \n\n\nDiscussion of Contributions\n\nAs the title may suggest, it is a bit surprising that we can show that polylog width is sufficient. Intuitively, we can imagine that the classification margin can grow exponentially more complex as the number of samples increase. I believe the nice result can be attributed to a careful analysis of key quantities such as \\| W_t - W_0 \\|_F in terms of the margin parameter from Assumption 2.1. \n\nSome nice examples of the analysis in this paper include the introduction of the weight matrix \\overline{U} and \\overline{W} as an intermediate between W_0 and W_t, and observing that the activations of the ReLU \\xi_{i,t} do not change very much during training. The tricks together led to a very tight bound on the change in weights \\| W_t - W_0 \\|_F in terms of the margin parameter \\gamma. As the authors mentioned on page 6, this tight control was used to bound the Rademacher complexity later. \n\nThe connection drawn between the margin assumption and neural tangent kernel (Proposition 5.1) is also interesting on its own. The authors intended this result to serve as a justification of the margin assumption (2.1). \n\n\nDiscussion of the Margin Parameter\n\nLet me start by saying I'm not completely certain on how to interpret this margin parameter \\gamma in Assumption 2.1. Perhaps I'm missing some obvious ideas here, but I would still like the authors respond with some more details. At the same time, I don't believe this is a sufficient criticism to reject this paper, as I believe the analysis in terms of \\gamma is still valuable. \n\nOn one hand, if we were to assume the margin condition holds for all possible data points (i.e. Assumption 3.1), then there is no concern about polynomial dependence on the number of samples, and this is certainly a reasonable assumption in some applications. \n\nOn the other hand, many of the previous analysis on wide networks were in terms of a minimum separation distance, i.e. assume there exists a \\delta > 0 such that for all i \\neq j, we have \n\t\\| x_i - x_j \\| \\geq \\delta . \nThe authors have provided a discussion in section 5, including both a worst case bound of \n\t\\gamma \\geq \\delta / (100 n^2),\nby Oymak and Soltanokotabi (2019) and an example where the margin is O( n^{-1/2} ) with high probability. \n\nUsing either bounds on \\gamma, we will have a width with polynomial dependence in terms of the number of samples and minimum separation distance. Therefore if we were to compare against previous works in the same benchmark, i.e. using a minimum separation assumption instead, then arguably this work did not achieve a width that is only polylog in terms of the number of samples. \n\nThat being said, I don't believe the authors were intentionally trying to hide sample dependence inside an assumption. The paper is presented in a very transparent way, and the authors were being honest in chapter 5 about the worst case dependence on the number of samples. \n\nTo summarize, it is unclear to me whether the paper truly achieved a width dependence that is polylog in terms of the number of samples, but the analysis in terms of \\gamma remains a valuable contribution. I welcome the authors and the other reviewers to provide additional comments on whether the title and claims of this paper is appropriate. \n\n\nMinor Comments \n\nFor the sake of improving readability, I also have some minor comments that do not contribute towards the decision. \n\n - On page 5, the first observation bullet point in section 2.2, it is written here that by triangle inequality \n     \\| \\nabla \\widehat{R}(W) \\|_F \\leq \\widehat{Q}(W) , \n    I thought you needed an absolute value on the right hand side, perhaps you should mention that \\ell is strictly decreasing. \n\n - On page 6, in the statement of Corollary 3.3, you are missing \\eta \\leq 1, and perhaps the \\tilde on \\Omega and O should be defined. \n - On the same note, while it is obvious that Theorem 3.2 implies this corollary, it is still worth writing up a proof to compute the constants. \n\n - On page 7, the notation \\Delta_n and \\odot are not defined, I had to infer definition from the proof. \n\n - On page 12, just below the second equation, it wasn't immediately clear how \\widehat{R}( \\overline{W} ) \\leq \\epsilon / 4. I believe it's worth expanding the definitions a bit, and explicitly plug in Lemma 2.5. \n\n - On page 12, in the second last equation, I'm actually not sure where this inequality comes from \n      \\| W_t - \\overline{W} \\|_F \\geq \\langle W_t - \\overline {W} , \\overline{U} \\rangle\n    Perhaps it's obvious, but I currently don't see it. \n", "belong_id": "HygegyrYwH"}, {"uid": "HyeBXPOpFr", "paper_title": "Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the author shows that for a two-layer ReLU network,  it only requires a network width that is poly-logarithmic in the sample size n to get good optimization and generalization error bounds, which is better than prior results. \n\nOverall, the paper is well written and easy to follow. However I still have some questions about this paper.\n\nOne of my major concerns is that there might be an important error in the proof of the main theorem. Specifically, in the proof of Theorem 2.2 (page 12), it says that due to lemma 2.5, $\\hat R^{(t)}(\\bar W)\\leq \\varepsilon$. However, Lemma 2.5 only shows that $|f(x_i,W_0,a)|$ is small, and the reason $\\hat R^{(t)}(\\bar W)$ can also be small is not explained in this paper at all. Based on Lemma 2.5, I can roughly get that $\\hat R^{(0)}(\\bar W) $ can be small, but the reason why $\\hat R^{(0)}(\\bar W) $ is small is unclear to me, especially when the network width m is only polylogarithmic in n and \\varepsilon. Without a clear explanation on this issue, the theoretical results in this paper might be flawed, and the polylog claim might not be correct.\n\nMoreover, this paper does not provide sufficient comparison with existing work. For example, Assumption 2.1 looks very similar to the assumption made in Cao & Gu (2019a). The definition of $\\hat Q(W)$ has also been introduced in Cao & Gu (2019a). However these similarities are not mentioned in the paper at all. Moreover, the result of Lemma 2.6, which is also one of the selling points of this paper, is actually very similar to Fact D.4 and Claim D.6 in the following paper:\nAllen-Zhu, Zeyuan, and Yuanzhi Li. 'What Can ResNet Learn Efficiently, Going Beyond Kernels?.' arXiv preprint arXiv:1905.10337 (2019).\n\nFinally, the authors claim in the title that the width of the network is poly-logarithmic with the sample size n might be misleading. In fact, in Section 5, it has been discussed that in certain settings about the data distribution, $\\frac 1\\gamma$ is polynomial of n. However, the width is polynomial with $\\frac 1\\gamma$, which means the width is poly of n in these settings. ", "belong_id": "HygegyrYwH"}, {"uid": "Bkgha_aM9r", "paper_title": "Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary :\n1. Classification task with shallow Relu and logistic loss.\n2. Showing fast global convergence rate with polylog width for both training and generalization error under appropriate assumptions\n\nOverall, this paper is very exciting and surprising. At some point, I was trying to prove such results, but couldnt get it. This paper should be accepted as an oral presentation in ICLR. If the authors can address some of the questions in the comments, I will be happy to increase the score.\n\nAdvantages:\n1. Better results for classification task with shallow Relu in terms of global convergence rate and network width\n2. Showing the essence of the power of over-parameterization that the weights dont change much\n3. Clear logic and proof\n4. Also discuss the stochastic GD/generalization error? (I didnt read that part)\n\n\n\nDisadvantages:\n1.Bug in proving Theorem 2.2, a larger lambda is needed (but wont influence the polylog result). See the below for a fix.\n2.In the proving sketch, why ||W_t-W_0||_F=O(ln t)? In the proof, it looks like ||W_t-W_0||^2_F=O(t), as in the third equation on page 12. More explanation about proof sketch is needed.\n3.Typo\na.The \\odot operation in Equation 5.1 is not defined. According to later computation, this operation seems to be the hadamard product between two vectors. But this notation is not widely used and some brief introduction will be benefinitial.\nb.On page 1, ... and standard Rademacher tools but exploiting how little the...., but seems to be a typo.\nc.On page 2, also suffices via a smoothness-based generalization bound, suffices should be suffice.\nd.Last formula in page 11 missing a >0 in the indicator function.\n4.(optional) why using ||W_t-W_0||_F instead of ||w_r(t)-w_r(0)||_2, which used in previous work for square loss, for the analysis? Is there any benefit or restriction here? \n5.(optional) Give more insights about intermediate quantities such as \\hat R^(t), \\bar{W}, etc.\n\nComments:\n1. More arguments for polylog width in last section needed. E.g., give a specific case where the gamma in Assumption 2.1 is constant, or comparable to the smallest eigenvalue of NTK; otherwise in the worst case, gamma can be as bad as  the smallest eigenvalue of NTK over n, which ruins the polylog results. To be more specific, we can always set q to be the uniform distribution over [n], then \\|q\\odot y\\|_2 is indeed 1/\\sqrt{n}, hence \\gamma_1\\leq \\sqrt{\\lambda_{max}(K_1)/n}. If K_1 has constant spectral norm(which is the case if all the data points are orthogonal to each other), then \\gamma_1 will depend on 1/n.\n2. For the over-parameterization theory, more references are needed. https://arxiv.org/abs/1902.01028 [Allen-Zhu, Li] is about generalization bound for the over-parametrized networks, https://arxiv.org/abs/1810.12065 [Allen-Zhu, Li, Song] and https://arxiv.org/abs/1905.10337 [Allen-Zhu, Li] are about the over-parameterization bound for more than two-layer networks. https://arxiv.org/abs/1906.03593 [Song, Yang] obtains a better width bound for two-layer neural networks under the framework of https://arxiv.org/abs/1810.02054 [Du, Zhai, Poczos, Singh].\n3. Theorem 2.2 shows that the average loss converges. Does this imply after training for T steps, we obtain good weights with small logistic loss? Can you get results showing the loss is decaying, like Theorem 4.1 in https://arxiv.org/abs/1810.02054 [Du, Zhai, Poczos, Singh]?\n4. On page 8, the lower bound of \\lambda_0 is given as \\delta/n^2. Is this bound tight? Is this lower bound achievable? \n5. Under what assumptions can we prove o(log n), say poly(log log n) width?\n6. What is the role of logistic loss in the proof? In general, if we replace logistic loss with square loss, will this make it harder to train neural networks?\n\n\nThe original analysis might has some flaw/bug:\n\nIn the proof of Theorem 2.2, top of page 12, to show \\hat R^{(t)}(\\bar W)<= \\epsilon/4, the term y_i <\\nabla f_i (W_t) - \\nabla f_i (W_0) , W_0> seems to be forgotten to consider.\n\nThis could be a fix.\n\nNote that above term equals y_i/m \\sum_{r=1}^m ( 1_{[< w_{r, t}, x_i> >= 0]} - 1_{[< w_{r, 0}, x_i> >= 0]} ) < w_{r, 0}, x_i >. We can use concentration to bound <w_{r, 0}, x_i>, such that with high probability, it will be no larger than polylog(n). Correspondingly, we know this term wont be too small. Adding this extra polylog factor into lambda, we can fix the proof.\n", "belong_id": "HygegyrYwH"}, {"uid": "SkeyGWrHKr", "paper_title": "Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new solution called CHAMELEON for deep learning code optimization, which accelerates the process of compiling codes and achieves faster training and inference of deep networks. The proposed method can be used to compile various deep network architectures. Experimental results show that the proposed method outperforms the previous method with large margin.\n\nThe paper exploits reinforcement learning to address code compilation, which is novel for me. \n\nThe experimental results are convincing, the paper evaluates the proposed evaluation in multiple aspects.\n\nI am totally new in the area. I will refer to the other reviews' review and the authors' rebuttal to have my final decision.\n\n\nPost-rebuttal\nThank the other two reviewers and the authors to help me better understand the paper. I think I have no concern on the paper so I still give 6.", "belong_id": "rygG4AVFvH"}, {"uid": "BylKi-56tH", "paper_title": "Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors proposed a method for code optimization for deploying neural networks. The main idea is to formulate it as a search task over tuning knobs in the code template, and to apply reinforcement learning to optimize the configurations for the tuning knobs with respect to a cost model. The cost model is trained based on a subset of representative samples from the RL controller and their corresponding hardware cost measurements.\n\nThe paper is very well written and the proposed method seems technically sound. The authors did a good job combining existing techniques in a complementary manner. For example, the usage of RL for efficient search space exploration and the usage of clustering for selecting representative samples for on-device measurements.\n\nSome concerns:\n* The authors are referring to the usage of reinforcement learning as Adaptive Exploration and the usage of clustering as Adaptive Sampling. While combining them to tackle the task of neural network compilation is interesting, these techniques themselves are very standard and hence come with limited technical novelty.\n* The proposed workflow seems to involve a nontrivial amount of additional hyperparameters, e.g., those in the RL controller as well as those for clustering. It might be useful to discuss about the overhead caused by hyperparameter tuning, as otherwise numbers reported in Table 2 (based on a single trial) could be misleading.\n", "belong_id": "rygG4AVFvH"}, {"uid": "HkxnE68RYH", "paper_title": "Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an optimizing compiler  for DNN's based on adaptive sampling and reinforcement learning, to drive the search of optimal code in order to reduce compilation time as well as potentially improve the efficiency of the code produced. In particular, the paper proposes to use PPO to optimize a code optimization 'search' policy, and then use K-mean clustering over a set of different proposed compilation proposals, from which to perform adaptive sampling to reduce compilation time while still keeping a high diversity of the proposed solution pools during exploration. At the same time the authors claim that using RL will learn a better search strategy compared to random search - such as simulated annealing which is used by competing methods - thus producing faster and better solutions.\nThe paper show results of up to 4x speedup in compilation time (autotuning time) while obtaining a slightly better or similar efficiency of the generated code (in term of execution time). This is a well written extensive research with good results. The authors mention it is (will be) integrated in the open source code of TVM. However I could find no mention in the paper of whether the code will be released with this publication, and I would like to solicit the authors to clarify their code release strategy and timing.\nMy other question pertains to whether or not  compilation time is an key metric to target. It is important to some extent, but I would say that aside from exponential / super-polynomial behaviour of auto-tuning algorithms, a multiple hours / days process to create the best optimized code for a certain network / hardware platform might not be such a big hurdle for a community already used to multiple days / weeks / months to train the same models. I believe that focusing on the efficiency of the optimized code produced would probably be a better metric of success.", "belong_id": "rygG4AVFvH"}, {"uid": "HJgrncv9KB", "paper_title": "DiffTaichi: Differentiable Programming for Physical Simulation", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces DiffSim, a programming language for high-performance differentiable physics simulations. The paper demonstrates 10 different simulations with controller optimization. It shows that the proposed language is easier to use and faster than the other alternatives, such as CUDA and TensorFlow. At the end, the paper provides insightful discussions why the gradient of the simulation could be wrong.\n\nDifferentiable physics simulation is an important research area, especially for optimal control and reinforcement learning. While I am impressed by the large variety of examples demonstrated in the paper, I am leaning towards rejecting the paper because of its poor presentation. The paper only gives a simple and high-level example of the language (optimizing the rest length of springs that form a triangle), very brief descriptions of 10 examples and some discussions about the difficulty of computing useful gradients, but without any in-depth discussion how everything is implemented. This is not enough for an ICLR paper. For example, the paper does not answer some of the fundamental problems of differentiable physics. For example, collision and contact are inherently non-differentiable. How does the paper handle it in the examples of locomotion and billiards (Figure 4)? In addition, how does the paper back-propagate the gradient through the incompressibility conditions (Poisson solve) of fluid simulation? \n\nHere is my suggestions how to improve the writing. There are several ways to write the paper, with different emphasis. If this paper is more about introducing a new programming language, Appendix B Compiler Design and Implementation would be important and should be moved to main text. If the paper want to emphasize how to handle the non-differentiable cases of the simulation, then detailed derivations of contact, collision, and linear/nonlinear solving (due to incompressibility conditions or implicit integrators) should be presented. If the paper would like to demonstrate how differentiable physics simulation can help with controller optimization, then two to three examples, such as the locomotion control for soft bodies or rigid bodies, should be analyzed in far more details, and compared with traditional method without differentiable simulation. It is good to focus on one of the above points, based on the venue that this paper is submitted to. Currently, the paper is trying to touch all three. But due to the page limit, it is not thorough, or detailed in any one of them.\n\n-------------------------Update after rebuttal------------------------------\nThank you for the revision of the paper and the additional comparisons with Jax. The revised version reads much better. The response and the revision addressed most of my concerns. Thus, I raised my rating to weak accept.", "belong_id": "B1eB5xSFvr"}, {"uid": "ryeQ4y6htH", "paper_title": "DiffTaichi: Differentiable Programming for Physical Simulation", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a programming language for building differentiable physics simulators. This is a very interesting goal, as differentiable systems are a crucial building block for many deep learning methods and similar optimization techniques. \n\nThe system presented by the authors is certainly impressive. Unfortunately, the paper itself covers a wide range of topics, and consists of an overview of the language with a programming tutorial, a collection of ten results, and a brief discussion of problems when computing gradients. \n\nThe core of the proposed work, the programming language seems to be quite powerful. However, it seems to be built on an existing system, which was published as a programming language for simulation in this years siggraph asia conference (Taichi: A language for high-performance computation on spatially sparse data structures. In SIGGRAPH Asia 2019 Technical Papers, pp. 201. ACM, 2019a). This ICLR submission seems to extend this system to build and provide gradient information automatically along with the simulation itself. There seem to be few technical challenges here, and many aspect discussed in section 2 are shared with the original simulation language.\n\nThe examples cover a nice range of cases, from simple mass spring systems and a rendering case to complex 3d simulations. Here, I was a bit surprised that the paper only compares to autograd, which has been succeeded by jax. The latter also provides a compiler backend to produce GPU code with gradients, and as such seems very closely related to the proposed language. From the submission, it's hard to say which version has advantages. The examples seem to be a sequence of demos of the language, rather than illustrating different technical challenges or improvements for a scientific conference. Or at least a discussion of these differences is currently missing in the text.\n\nSection four also mostly gives the impression of a loose discussion. The gradients for rigid body impacts are interesting, but seem relevant only for a subset of 2D examples shown in the paper. The discussion of gradient explosions is quite ad-hoc, and would be stronger with a more detailed analysis.\n\nThe submission as a whole aims for a very interesting direction, but I think the paper would benefit from focusing on a certain range of problems, such as the rigid body control cases, in conjunction with topics such as the improved gradients. Instead, the current version tries to combine this topic with a systems overview, a tutorial and loosely related discussions. Combined with the length of 10 pages, I think the work could use a revision rather than being accepted in its current form.", "belong_id": "B1eB5xSFvr"}, {"uid": "H1loP67jqH", "paper_title": "DiffTaichi: Differentiable Programming for Physical Simulation", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "*Summary*\nThis paper describes DiffSim, a differentiable programming system for learning with physical simulation. The system (built on the Taichi system) allows users to specify a forward simulation in a Python-like syntax, after which the program is compiled and iteratively run in both forward-mode and gradients computed for system parameters and controllers, as desired. A variety of simple simulations are included, demonstrating that the automatically generated CUDA code runs as fast as hand-written CUDA code (and noticeably faster than TensorFlow or PyTorch implementations), while requiring far fewer lines of code. The final section details two issues--time of  impact errors due to discrete time intervals and gradient explosions with long time horizons--and some potential solutions.\n\n*Rating*\nThe paper is interesting and easy to read. While some part of the underlying functionality of DiffSim is directly derived from previous work (Taichi), the paper does describe a non-trivial contribution.\n\nI lack the background to comment constructively about expectations for these simulations or the fidelity of the methods in this paper. What evidence can you offer regarding the physical fidelity achievable and how that relates to issues of scalability, gradient behavior, size of time steps, code complexity, etc.? For a sense of context, what might be needed to simulate a 7 DoF robotic arm and learn a controller that would reasonably transfer to a real robot?\n\nOverall, I'm optimistic about this paper, and would tend to vote for acceptance.\n\n*Notes*\npg3: define k (spring stiffness?)\npg4: what is the value of 'mass' for this simulation?\nFig 8: what is the x-axis in the two right plots? initial height?\nFig 10: right plot title should probably be 'Gradient Explosion with Damping'", "belong_id": "B1eB5xSFvr"}, {"uid": "Skl5hr_6FS", "paper_title": "Unaligned Image-to-Sequence Transformation with Loop Consistency", "experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces the LoopGan, which aims to enforce consistency in a sequential set of images but without aligned pairs.  The prototypical example used in the paper is a network to transform seasonal images, where Summer -> Fall -> Winter -> Spring -> Summer -> etc.\n\nI suggest rejecting the paper.\n\nWhile the idea has some merit, it is an interesting premise to train a recurrent generator to create images in multiple domains, I feel as though the experiments are quite lacking.  The primary evidence the method works is Figure 3, for which we see only 4 examples of the method on two tasks.  Looking at the results they don't look all that great and the proposed method is hard to identify as the best. \n\nThe idea doesn't strike me as particular innovative, feeling like a natural extension of the prior work listed.\n\nTable 1 shows the results of a user study with 20 users.  With only 20 users rating 4 examples of each method we get very little power to resolve the best method, and the papers proposed method is selected in aggregate only 26% of the time out of 6 methods, this is hardly a clear winner.  \n\nFigure 4 showing the imputed ages for the generated images doesn't seem that strong of evidence the proposed method is that much better either.  The histograms are difficult to compare by eye, perhaps the paper should report computed estimates of the kl divergence between the two?  \n\nThe ablation studies of section 6 are out of place.  The paper does not compare against the incremental removal of its' proposed loss, it simply reports results of alternative architectures.  ", "belong_id": "H1ebc0VYvH"}, {"uid": "BJlpRXDAtB", "paper_title": "Unaligned Image-to-Sequence Transformation with Loop Consistency", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes to extend CycleGAN by replacing a bidirectional cycle with a 'loop' or a sequence of transformations across gradually changing image domains (sequential transformation). This is achieved with a simple modification to CycleGAN training where the training involves additional steps to create a loop. A number of experimental results are shown on specific sequential datasets, comparing to a number of other baselines. On the chosen datasets and tasks, the proposed architecture provides good results over the baselines. However, I would have liked to see ablation studies showing that they achieve the same results as CycleGAN when training on 2 domains. The paper's technical novelty is limited to this simple loop extension and generator sharing. As such, while the experimental results are reasonable, I find this lack of any technical novelty a negative factor. ", "belong_id": "H1ebc0VYvH"}, {"uid": "SyxKW99S5B", "paper_title": "Unaligned Image-to-Sequence Transformation with Loop Consistency", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes loop consistency on the base of cycle consistency and alleviate difficulties associated with learning in the resulting long chains of computation. Like CycleGAN, the method proposed in this paper does not require data to be specifically matched.\n\nThis paper still has the following problems:\n1)For the time series images, the reason for processing with a single generator is not clearly stated. In CycleGAN, the two transformations use two different generators, respectively, and use the cycle consistency loss to force the results to converge. In the examples presented in this paper, such as Face Aging, it isn't a process that can be reversed. This paper does not explain why the use of a single generator has been effective.\n2)To be more convincing, this article needs to be tested on more baselines. The indicators provided in this article are not objective enough.\n3)This article also does not give the training computational complexity and testing time cost of the proposed method.\n", "belong_id": "H1ebc0VYvH"}, {"uid": "SJgfNIk3YB", "paper_title": "RNNs Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors present a novel work to address the problem of signal propagation in the recurrent neural networks. The idea is to build a attractor system for the signal transition from state h_{k-1} to h_k. If the attractor system converges to a equilibrium, then the hidden to hidden gradient is an identity matrix. This idea is elegant. The authors verify the performance of Increment RNN on long-term-dependency tasks and non-long-term-dependency tasks.\n\nThe work successfully constructs a negative identity hidden to hidden gradient matrix but I still have a few concerns about the theory and the experiments. If the authors can address my concerns in the rebuttal, I am willing to increase my score. \n\nTheoretical concerns:\nEven in the limit sense, the inner ordinary differential equation of g will converge to the equilibrium, it may not converge in the finite steps. Thus, the state-to-state gradient may be slightly away from the identity. And we know that (0.99)^T goes to infinity when T goes to infinity. In practice, the long-term-gradient problem may still exist in the incremental RNN.\n\nExperimental concerns:\nThe theorem requires the norm of U to be bounded. But I cannot see how the authors bound the norm of U in the experiment section.\n\nClarity of writing:\nThere are a bunch of typos in the papers, especially the ones in the proof of Theorem 1. The proof of theorem 1 can be polished. The author swap the use of phi and psi several times in the proof. And gradient calculation on equation (5) should be expanded into more details. The authors also missed one U in the equation in the second last line of the proof. \n\nOverall, I think the paper is an interesting contribution to the community.\n", "belong_id": "HylpqA4FwS"}, {"uid": "SyeP2jSaYH", "paper_title": "RNNs Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose the incremental RNN (iRNN), which is inspired by the continuous-time RNN (CTRNN). Theoretically, the equilibrium point of iRNN exists and is unique. Furthermore, the norm of the Jacobian between two hidden states is always one, provided that the Euler iterations converge. The authors proved this property as well as the exponential convergence rate of the Euler iteration. These properties avoid the vanishing/exploding gradient problem typical in RNN with long sequences in theory. Empirically, the proposed method is compared with multiple RNN architecture on various tasks. \n\nThe paper is clearly written and well organized. It starts with the existence and uniqueness of a fixed point; then provides the converge rate, and finally the main theorem of identity gradient. The main theorem provides a good theoretically guaranteed solution. The authors have done extensive experiments on several datasets compared against various popular and recently developed RNN-based models. The authors also provided some empirical experiments on the converge rate and the non-singularity of grad \\psi.\n\nThe motivation of the paper could be improved. For example, despite the appealing theoretical properties. I did not fully understand the motivation in Equation 3. It would be great if the authors could further elaborate on it.\n\nTypos:\nPage 4 at Upon computation, we see that. The gradient on the left-hand side should be grad \\psi? \nPage 4 at at the equilibrium points we have, the gradient of \\psi should be the gradient of \\phi?\n", "belong_id": "HylpqA4FwS"}, {"uid": "S1l7cek0FS", "paper_title": "RNNs Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Update: the authors have addressed my issues below and I have raised my score to 8.\n\n***\n\nThis paper on recurrent neural networks goes back to Rosenblatt's continuous time dynamics model and uses a discretised version of that equation (equation (1) in the paper) to build an incremental version of the RNN, called incremental RNN, where the transition from hidden state h_{k-1} at step k-1 to next step's h_k is done using small incremental updates until the system achieves equilibrium. It claims that it manages to solve the vanishing gradient problem by keeping all gradients \\frac{\\partial h_k}{\\partial h_{k-1}} equal to minus identity matrix. The algorithm is then extensively evaluated on a large number of tasks and compared with plain RNNs, LSTMs, and two recently published papers on antisymmetric RNN and FastRNN.\n\nI need to admit that after reading the paper twice, I am not sure I understand how the method works exactly (how does inserting intermediary steps and variables g_0, g_1, ... g_T enable the system to reach equilibrium: is there an iterative evaluation until convergence?) and more worringly, how the single-step SiRNN differs from a normal RNN with an extra residual connection?\n\nAccording to equation (5), for T=1 and g_0=0, we have:\ng_T = g_1 = \\eta_k^1 ( \\phi (U (h_{k-1}) + W x_k + b) - \\alpha h_{k-1} ).\nIf the gradients are vanishing in the normal RNN, why would they not vanish here for T=1?\n\nPropositions 1 and 2 are for the case K=\\infinity, and I could not understand the proof of theorem 1 that shows why \\frac{\\partial h_k}{\\partial h_{k-1}} = - I. This seems to be the major contribution of the paper and should be given prominence.\n\nWhat is missing is clear explanation, like (Bengio et al, 1994), of the identity gradient and of how the algorithm works. These questions could be solved by including code or pseudo-code explaining how to actually implement incremental RNNs.\n\nThere are also several important papers recently published that have approached the problems of continous-time dynamics and relaxation of hidden state to equilibria.\n* The paper does not mention at all Neural ODEs [2] [3] where the state flows in a continuously differentiable way thanks to the continuous-time residual network ODE formulation. Moreover, isn't the idea of inserting a relaxation to equilibrium using ODEs already implemented in the ODE-RNNs [3]?\n* How do incremental updates related to Adaptive Computation Time [4]?\n\nFor this reason, I am currently tending to reject the paper, but am open to change my score upon clarifications and links to other similar work.\n\nAdditional remarks:\nThe first paragraph of the paper explains the Elman RNN, not RNNs in general.\nPlease cite [1] alongside Bengio et al (1994) for the problem of the vanishing gradient.\nDefine alpha in equation (1)\nNotation k and K is very confusing\nBlue vs. green on Figure 2 is hard to read, and where is the new initialisation?\nWhy do you add h_k^K to h_{k-1} in equation (8)? I thought it was g_k^K?\nKeep the same colours for all experiments in figure 4.\n\n[1] Hochreiter (1991) 'Untersuchungen zu dynamischen neuronalen Netzen'\n[2] Chen, Rubanova, Bettencourt & Duvenaud (2018) 'Neural Ordinary Differential Equations'\n[3] Rubanova, Chen & Duvenaud (2019) 'Latent odes for irregularly-sampled time series'\n[4] Graves (2016) 'Adaptive computation time for recurrent neural networks'\n", "belong_id": "HylpqA4FwS"}, {"uid": "H1gBBwcGKr", "paper_title": "Temporal-difference learning for nonlinear value function approximation in the lazy training regime", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper discussed the nonlinear value function approximation for temporal different learning on on-policy policy evaluation tasks in the lazy training regime. The authors proved that for the both over-parameterized case (state number is fixed finite number) and the under-parameterized case (state number is sufficiently large or infinite), the value function can converge to some stationary point with exponential convergence rate. Moreover, the authors also characterized the error when the value function is under-parameterized.\n\nOverall, this is a good paper. But the paper organization is awful. There are many places that are ambiguous or with notations that not formally defined. It may not due to the page limit as the authors currently use only 8 pages. I think the authors should polish the whole paper and make it more readable. Below are some main clarity issues I find, but the authors should not only solve the issues I mentioned.\n\n1. For better presentation, I suggest the authors include a notation paragraph in the main text, which will be very helpful for the readers.\n2. I think it would be better to mention (7) returns w that V_w = V^* / \\alpha in Sec. 2 for better reading.\n3. In Theorem 3.5, it is unclear that the estimation \\tilde{V}^* is from \\alpha V_{w}.\n4. The WP1 in the paragraph after Theorem 3.5 means with probability 1?\n5. In Equation (11), what is the definition of the measure \\pi? If I understand correctly it is still \\mu as the invariant measure should be fixed for a given policy?\n6. The last paragraph in the proof of Theorem 3.5 is hard to follow. It can be better to introduce the result from the textbook and list the condition that need to verify, then give the lemmas show that the conditions can be verified.\nWhat is the functional X in (12)? Should mention it in the main text, not in the appendix.\n7. Figure 2 is somehow hard to understand. Maybe better show how the projection of TD error vector field outwards along the spiral in (a) and inwards in (b) in the figure.\n\nFrom my point of view, the proof is almost correct and most of the lemmas are standard. This result is meaningful as it shows how and when the nonlinear function approximation will converge in temporal-difference learning (under the context of lazy training, and I think it is also correct under the context of NTK). The perspective on viewing the TD learning as linear dynamic system on functional space can inspire several new research in this field. My main concern is about the paper organization. I feel it can be hard for the potential readers to go through the whole paper. If the authors improve the quality of writing during the rebuttal period, I will consider improve my score.", "belong_id": "HJghoa4YDB"}, {"uid": "SyltdOWAYB", "paper_title": "Temporal-difference learning for nonlinear value function approximation in the lazy training regime", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper considers TD learning with function approximation, and establishes convergence results for over- and under-parameterized models in the lazy regime, and illustrates the theory on simple numerical examples.\n\nAlthough the obtained results are interesting and the paper is well written, the contribution is quite incremental, in that it simply combines prior work on TD learning with linear function approximation with lazy training in order to show that models with a certain scaling can lead to convergence. This scaling makes the models essentially linear in the parameters (with a non-linear feature map given by initialization), so that it is not surprising that convergence can be reached, given the prior work on linear function approximation. I encourage the authors to further explain their motivation in studying such a setting.\n\nIt is claimed that the over-parameterized regime is only useful for finite state spaces, which seems quite limiting, since one cannot obtain global convergence in the under-parameterized case. When considering neural networks at infinite width, would the results be applicable if one assumes that V* belongs to the RKHS of the corresponding neural tangent kernel?", "belong_id": "HJghoa4YDB"}, {"uid": "HkgtJlIJ5B", "paper_title": "Temporal-difference learning for nonlinear value function approximation in the lazy training regime", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper analyses the convergence of on-policy TD-learning for policy evaluation with non-linear function approximation (deep nets) in the lazy regime. Similar to deep learning theory, the key idea is that in the lazy regime, for an overparameterized network, if initialized in a certain manner, weights do not change significantly during training. The paper heavily draws upon techniques from n Chizat & Bach (2018) and adapts them to the setting with value functions, and policy evaluation. In order to get a strongly convex objective in function space, they consider a strongly convex Lyapunov function for the analysis. In the under-parameterized regime, the paper shows convergence to local optimum, by showing that convergence is exponentially fast to a local minimum where the key insight is that standard differential geometry can be applied to analyze the behavior of the projection on top of TD-lambda operator using past existing analyses. Finally, Section 4 shows numerical examples -- a divergent function approximator and some empirical results on a single-layer neural net.\n\nOverall, I lean in favor of rejection for this paper. I am mainly concerned with the significance of the content in the paper and positioning with respect to past theoretical/empirical work. My specific concerns are:\n\n1. Assumption 1 is strong, it assumes full support over the state-space, however, in any situation of practical relevance, this is not the case. There are many papers at this point (for example, [1], [2]) which show empirically that even with Q-learning (Fitted Q), divergence is not common if the function approximator is large/wide enough, and the support of the state space is full, however, these methods can diverge if the support is not full/ skewed. I am not sure if this assumption is very realistic then.  (The results are for specific function approximation in this paper, and hence it is unclear if that is the case we use in practice)\n\n2. I am not sure if the techniques used in the paper are relatively novel (from a theoretical point of view), and I would appreciate if the authors can elaborate a bit on this. It seems like most of the proof is drawn from past work (although past work has theoretical results in a different problem setting -- supervised learning). While the setting of policy evaluation is novel, I am concerned about how many new techniques are to be gained from a theoretical point of view here.\n\n3. What assumptions are needed for Theorem 3.1 and Theorem 3.2, from a deep-learning standpoint? What recommendations does the theory give to practitioners? I find a discussion on both of these points missing from the paper. It would be appreciated if the authors can elaborate on these points.\n\nReferences:\n[1] Deep Reinforcement Learning and the Deadly Triad, van Hasselt et.al.\n[2] Diagnosing Bottlenecks in Deep Q-learning Algorithms, Fu et.al.", "belong_id": "HJghoa4YDB"}, {"uid": "BygGVkvh3B", "paper_title": "Temporal-difference learning for nonlinear value function approximation in the lazy training regime", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper discusses the policy evaluation problem using temporal-difference (TD) learning with nonlinear function approximation. The authors show that in the lazy training regime both over- and under-parametrized approximators converge exponentially fast, the former to the global minimum of the projected TD error and the latter to a local minimizer of the same error surface. Simply put, the lazy regime refers to the approximator behaving as if it had been linearized around its initialization point. This can happen if the approximation is rescaled, but can also occur as a side-effect of its initialization. The authors present simple numerical examples illustrating their claims.\n\nAlthough I did not carefully check the math, this seems like a solid contribution on the technical side. My main concern about the paper is that it falls short in providing intuition and contextualizing its technical content. Regarding the presentation, I believe it is possible to have a less dry prose without sacrificing mathematical rigor. If some of the technical material is moved to the appendix --like auxiliary results, discussion on proof techniques, etc--, the additional space could be used to discuss the implications of the theoretical results in more accessible terms.\n\nFor example, a subject that ought to be discussed more clearly is the nature of the approximation induced by the lazy training regime. As far as I understand, this regime can be thought of as a sort of regularization that severely limits the capacity of the approximator.  Although the authors mention in the conclusion that ...convergence of lazy models may come at the expense of their expressivity, after reading the paper I do not have a clear sense of how expressive such models actually are. In their experiments, Chizat et al. (2018) observed that the performance of commonly used neural networks degrades when trained in the lazy regime --to a point that they consider it unlikely that the successes of deep learning can be credited to this regime of training. It seems to me that this subject should be more explicitly discussed in a paper that sets out to provide theoretical support for deep reinforcement learning.\n\nStill regarding the behavior of lazy approximators, my intuitive understanding is that they work as a linear model using random features. If this interpretation is correct, this makes the theoretical results a bit less surprising. They are still interesting, though, for they can be seen as relying on a smoother version of the linearity assumption often made in the related literature. Maybe this is also something worth discussing? Still on this subject, it seems to me that one potential disadvantage of lazy models with respect to their linear counterparts is that it is less clear how to enforce the lazy regime in practice. In Section 4.2 the authors discuss how this can naturally happen as a side-effect of the initialization, but it is unclear how applicable the particular strategy used to illustrate this phenomenon, with the doubling trick, is in practice. This is another example of a less technical discussion that would make the paper a stronger contribution.", "belong_id": "HJghoa4YDB"}, {"uid": "H1xNJi32KH", "paper_title": "Fast Linear Interpolation for Piecewise-Linear Functions, GAMs, and Deep Lattice Networks", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposed several adjustments to speed up the existing implementations of piece-wise linear functions based on Look-up tables (LUT), including an auxiliary LUT through optimisation and sacrificing negligible precision in the standard IEEE754 protocol to reduce the time complexity.  \n\nGiven my limited experience in the field, I have several concerns and questions after reading this paper several times, which eventually leads to my decision (hopefully it was an educated one):\n\n(1) The paper assumes that the input distribution is known to the system, but it is only true in the testing mode where we generally assume that samples in the test set come from the same distribution which is used to generate training examples. In the case where training is required or finetuning is needed to adapt the system to another distribution, the the proposed auxiliary LUT could have a hard time mapping input data properly.\n\n(2) In my understanding, the transformation, T: supp(P) -> R, is assumed to be known whilst IMO it is crucial to the proposed method. A non-trivial effort is expected in finding T, and a whole area of research is dedicated to this which is Optimal Transport. Currently, plausible and computationally efficient method is through Sinkhorn algorithm. I don't think the paper should downplay the part of finding T as it is crucial.\n\n(3) In the shared Index PWLs, the paper seems to have ignored the fact that different functions still have different output values, and different key-value pairs are indeed needed. For example, log(x) and x^2 (as used in the paper) have different input domains and output domains, so sharing indices between two functions is impossible. Could the authors elaborate on why it is okay to share indices?\n\n(4) A necessary process of LUT from piece-wise linear functions of approximating the ones that we care about is to conduct 1-D density estimation on the input domain, since more fine-grained splits are required in the dense area and less splits elsewhere. The proposed methods in this paper seem to have largely ignored this effect. For example, log(x) needs splits with smaller intervals when the x is around 1. \n\n(5) The presented results only include the speed of the forward computation of several models. It is okay if there is a performance (accuracy or likelihood) and speed trade-off, but the paper didn't present any results on this. Could the authors present some results for this?\n\n", "belong_id": "H1e-X64FDB"}, {"uid": "BJlQ28YpFB", "paper_title": "Fast Linear Interpolation for Piecewise-Linear Functions, GAMs, and Deep Lattice Networks", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes several low-level code optimizations aimed at speeding up evaluation time for linearly interpolated look-up tables, a method often used in situations where fast evaluation times are required. The paper is very practically oriented in the sense that it does not aim for improving asymptotic speed-ups, but rather real speed-ups expressible in terms of CPU cycles that are achievable by exploiting compiler optimizations such as branch prediction and loop unrolling. The focus is on unbatched computations which can arise in many real-time situations. The proposed implementation techniques are as follows:\n- A method for fast index mapping, which first transforms inputs using a monotonic function such as log_2 or 2^x, and the applying a branch-free linear search implementation on a uniformly-spaced auxiliary LUT.\n- A memory-efficient bit-packing technique to store both integer and floating point representations together.\n- Speed-up for multilinear interpolation using latency hiding\n- Branch-free implementation of sorting permutation, needed for simplex interpolation\nThe proposed implementation is then evaluated on 4 different benchmarks, with considerable speed gains over interpreter-based baselines, while batching and single-vs-double precision do not have major impact on speed.\n\nWhile the paper is relevant and interesting, and the proposed techniques are reasonable and probably result of a considerable amount of work, more effort is needed to improve clarity and preciseness of the explanations, and (most importantly) the experimental evaluation. Detailed strengths and weaknesses are outlined below.\n\nStrengths\n- The paper is well-motivated and relevant to the ML community\n- Low-level speed optimizations are needed but overlooked in the community\n- Reasonable choice of experimental conditions (focus on unbatched CPU evaluation, testing on a selection of 4 different tasks)\n- Proposed techniques are sensible\n\nWeaknesses (roughly in order of decreasing significance)\n- Gains over Tensorflow performance is advertised in the intro, but only mentioned anecdotally in the experiments. Also, the Tensorflow implementation should be briefly explained to make clear where these gains come from.\n- The experiments put much focus on speed performance over different batch sizes, but this is (1) not the focus of the paper (unbatched CPU operation is the focus), and (2) is little informative because the introduced methods (which do not benefit much from batching) are not compared against Tensorflow (which does benefit from batching).\n- No ablation study is presented. The method description mentions informally how much speed-up is to be expected from different parts of the proposed method, but do not clarify how much this contributes to overall speed-gains.\n- The description in 4.1 is rather hard to follow, even though the ideas behind it are relatively simple. An illustrative figure might be of help for readers.\n- The method description in 4.1 lacks formal preciseness. For example, in 4.1. alpha, P, supp, are not defined (or in some cases introduced much after they are used first), and the +4 in the beginning of page 5 appears out of nowhere.\n- The proposed bit-packing is not well motivated. It promises to save half of the memory at a minor loss in precision (at least for the double-precision case), but it is unclear how much of a bottleneck this memory consumption is in the first place. In addition, it remains unclear to me why this is relevant particularly in the shared index case.\n- While the topic is relevant for this conference, readers are likely not familiar with some of the concepts used in the paper, and a bit more explanation is needed. An example for this is branch prediction, which is a key concept; readers unfamiliar with this compiler concept will likely not understand the paper, and a brief explanation is needed. Another example is loop-carry dependency, a term that could be explained in a short footnote. A third example is FPGA, which is mentioned in the very last sentence without further explanation/justification.\n- The introduction could be a bit more concrete on describing tasks where PWLs are relevant", "belong_id": "H1e-X64FDB"}, {"uid": "SkghvLwZ9H", "paper_title": "Fast Linear Interpolation for Piecewise-Linear Functions, GAMs, and Deep Lattice Networks", "experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes two techniques for fast linear interpolation on CPUs. They achieved speedups by reducing 1) fixed overhead cost and 2) per example computation (linear interpolation operation level optimization).\nAuthors consider this problem for small operation models like linear interpolation rather than the models requiring large operations such as ResNet. In this case, dispatch overhead cannot be ignored and so they use the MLIR frameworks to optimize trained model into the C++ code (reducing fixed overhead cost). This results in 2-3x speed up. Secondly, they propose the way to construct auxiliary index-mapping function by considering spacing of the key points rather just using for example evenly spaced index-mapping.\nThey compare proposed method to C++ interpreter implementation on two-layer and deep lattice networks and achieve 5-10x speed improvements.\n\nIt seems the topic of this paper does not fit ICLR and most machine learning researchers are unlikely to be interested in and even understand this paper. This reviewer also does not have enough knowledge and background to judge this paper. But my impression is that achieving speed up using existing MLIR framework has no surprising novelty. \nMoreover, the experimental results seems quite limited in the sense that they only experiment with trained 2 and 4-layer calibrated lattice models which are kind of small.  \n\nIt would be better to highlight why the proposed method is meaningful and provide more background knowledge to understand this paper. \n\nThis is only consider optimization on CPUs. What about the case of using GPUs?\n\nIs branch free assumption for functions Adjust & T is valid? (I dont have much knowledge on compiler..)", "belong_id": "H1e-X64FDB"}, {"uid": "r1xFi7sFFS", "paper_title": "End to End Trainable Active Contours via Differentiable Rendering", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "EDIT: The rating changed from '1: Reject' to '6: Weak accept' after the rebuttal. See below for my reasoning.\n\nThe submission considers two-class image segmentation problems, where a closed-contour image region is to be specified as the 'object'/region of interest, vs. 'no-object'/background. The approach taken here is end-to-end learning with an active-contour type approach. The main loss, in contrast to other active contour approaches, contains a direct difference of the estimated polygon area vs. ground truth polygon area.\n\nThe applied method seems conceptually quite simple (as admitted by the authors in Section 5), and the neural rendering approach seems quite neat, but both method presentation (Section 3) and evaluation (Section 4) seem incomplete and leave significant open questions.\n\nOne of my main concerns is related to the fact that the displacement field is static and, according to Figure 1 and Algorithm 1, is evaluated only once per image.\nIf the displacement field J is not conditioned on the current polygon shape (and this does not seem to be the case), then I am wondering why T iterations in the sampling/rendering part are necessary at all. When only considering L_seg, the optimal solution should be found within one iteration, since the displacement field will be able to provide the optimal answer. So maybe these iterations are only necessary when L_B and L_K are incorporated?\nIn any case, it is unclear why even L_seg is accumulated (using unweighted mean) over all T iterations before being backpropagated. Does this mean that these iterations are not meant to yield shape improvements? Why is ||M^t-M|| not evaluated per iteration, for the purpose of minimization?\nIt is also not sufficiently clear whether M^t in Equation 4 is a filled polygon mask, or if the mask is just related to the boundary (with a certain width). In absence of explanatory image material, I am assuming the former.\nOverall the method description remains weak, since obvious questions/concerns such as the above are not addressed.\n\nThe experimental results look good from a quantitative point of view, and indeed, the strongest baselines, e.g. DARNet, are outperformed significantly in many cases.\nSection 4 mostly focuses on quantitative evaluation and lots of picture examples, but fails to give insight into particular behaviors, failure cases, etc.\nThe evaluation procedure is cast a bit into doubt by two things: 1) In Figure 4, the initializations (blue circles) between the DARNet method and the proposed method are very different in size. I am wondering if this then still constitutes a fair comparison, and I have some doubts there. 2) In Figure 6, the proposed method consistently looks much worse than the DARNet baseline (and, in contrast to the baseline, completely fails for 4 vertices), unless the colors were swapped in the description.\n\nOverall, I do not think the submission is in a good enough shape for acceptance.\n\nMinor remarks:\n- The values for lambda_1 and lambda_2 seem to come out of thin air, and they also seem quite small. It needs to be mentioned how they were determined.\n- Data augmentation by rotation seems to be missing several values (between 270 and 260 degrees) and also not evenly spaced. Is this a typo or on purpose? In the latter case, an explanation is needed, since this seems weird.\n- Section 4.3: There is no 'Figure 4.2', I assume you mean Figure 6, which otherwise remains unreferenced.\n- Section 4.3, Ablation Study: Don't use the word 'derivatives' when you're talking about variations.\n- Section 4.3, Ablation Study: 'even without no auxiliary loss' -> remove 'no' or change 'without' -> 'with'\n\n-------------\nPost-rebuttal comments:\n\nI have read the revised version, as well as the other reviews and all authors' comments. The inclusion of an evaluation on a larger-size data set is highly appreciated, and seems to indeed validate the robustness of the method. Typos were fixed, including the switched color descriptions in Figure 7 (which should not have passed initial submission in the first place, if the text had been proofread properly).\n\nSeveral of the open questions (e.g. 'Why is L_seg accumulated before backpropagation?', 'Why is the algorithm iterative if the displacement map is computed only once, if not for the other loss terms?', 'Choice of values for lambda_1, lambda_2', Initial diameter of initialization') have been somewhat addressed by the authors in the rebuttal comment, though not in great detail.\n\nBased on the quality of the results across data sets, and because I believe that the timely publication of this rather simple method can benefit further research in this area, I have adjusted my score to a 'Weak accept'. That said, I still do not think it is a good manuscript, and my score should be seen as a massive benefit of the doubt toward the authors.\n\nMost importantly, above questions have NOT been adequately addressed in the actual revised text. The authors claim they have 'improved the manuscript considerably', but yet I see more reasoning for certain choices described in the comment here than in the actual manuscript. Most of the changes are in Section 2 and the new Section 4.3, but not much relevant to my comments changed in Section 3.\n\nFor example, balloon and curvature losses aside, it is still not clear why an iterative approach would be helpful past the first iteration. An ideal displacement map that is not conditioned on the polygon should point, for each pixel, straight to the closest contour pixel. It is clear to me that this may not be what is being learned when multiple iterations are forced, yet it is not addressed why multiple iterations should be beneficial. (I could see why they could be beneficial if the approach was conditioned on the polygon vertices, to avoid vertex collapsing, but it's not.)\n\nA good submission preempts these kinds of questions by addressing them carefully. What seems crystal clear to the authors will not be crystal clear to every reader. The authors should be more careful to include their reasoning in the actual text, which I believe this is essential for proper, easy understanding of the paper.", "belong_id": "rkxawlHKDr"}, {"uid": "Syl8NXg6tH", "paper_title": "End to End Trainable Active Contours via Differentiable Rendering", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper investigates an image segmentation technique that learns to evolve an active contour, constraining the segmentation prediction to be a polygon (with a predetermined number of vertices).  The advantage of active contour methods is that some shapes (such as buildings) can naturally be represented as closed polygons, and learning to predict this representation can improve over pixelwise segmentation.\n\nThe authors propose to learn an image-level displacement field to evolve the contour, and a neural mesh renderer to render the resulting mask for comparison with the ground truth mask.  The performance compared to prior learning-based active contour methods is impressive.\n\nIn section 4.3, theres a reference to a gap in performance between the proposed method and DARNet and a reference to a 'low number of vertices,' but a comparison between the two methods as the numbers of vertices is varied seems to only be present in Fig. 6 -- it would be interesting to see an explanation of the discrepancy for the lower number of vertices seen in this figure.\n\nOverall, due to the relative simplicity of the approach and impressive performance compared to prior learning-based approaches I recommend to accept.\n\nPost-rebuttal:  I maintain my recommendation.", "belong_id": "rkxawlHKDr"}, {"uid": "H1xhlGTQqH", "paper_title": "End to End Trainable Active Contours via Differentiable Rendering", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a straightforward method for end-to-end learning of active contours, based on predicting a dense field of 2D offsets, and then iteratively evolving the contour based on these offsets. A differentiable rendering formulation by Kato et al is employed to make the process of aligning a contour to a GT mask differentiable. \n\nThe model shows rather compelling results on small datasets, and is very simple, with very strong parallels to active contours, which is a strength. The results improve those of DARNet, which to the best of my knowledge is the main published work in the space other than Curve-GCN. One thing that would be helpful, is  to have an experiment on a large dataset, such as Cityscapes -- right now all the datasets are testing the model in only the small-data regime. Perhaps in a supplement, it would also help to do ablation of how input image / dense deformation resolution affects the result quality -- the input can be subsampled by powers of 2 for the experiment. \n\nAs Amlan Kar helpfully points out, the work heavily overlaps with his approach 'Fast Interactive Object Annotation with Curve-GCN', CVPR 2019, which is not cited or compared to. Curve-GCN similarly utilizes differential rendering (only a different variant) to match the GT masks. To me, the main difference wrt Curve-GCN is that explicit dense displacement fields are generated by the net and used directly for the iterative refinement steps, while Curve-GCN leverages implicit feature embeddings and uses GCN layers for their iterative updates. A second main difference is that Curve-GCN supports splines and interactive editing, while the proposed approach does not. Beyond these, there are multiple other differences that the authors point out, but those are more of a technical nature. Unfortunately, without a more direct comparison, it is very difficult to evaluate the design choices in the two approaches, which I feel is necessary for proper understanding of the paper. \n\nAFTER REBUTTAL: The authors made additions that covered my concerns, so I have switched my recommendation. \n\nA few more minor clarity / presentation issues. \n-- The recent learning-based approaches are either non-competitive or proven to be effective in the specific settings of building segmentation'. It's not exactly clear what the point is in the context. Which 'learning-based approaches'? \n-- Typo 'backpropogation'. \n-- A little better explanation of how a differentiable renderer of Kato works would have been helpful. \n-- Figure 3 is not referenced in the text, takes a little bit of thought why it is relevant (helps explain Fig 1, but maybe better to show it prior to Fig 1). \n-- In Eq 4 its not clear what F is.  (I see it is explained in Algorithm box, but that's much later)\n\n\n\n\n\n", "belong_id": "rkxawlHKDr"}, {"uid": "rkg7VVswKB", "paper_title": "PROVABLY BENEFITS OF DEEP HIERARCHICAL RL", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper performs a regret analysis for a new hierarchical reinforcement learning (HRL) algorithm that claims an exponential improvement over applying a naive RL approach to the same problem. The proposed algorithm and the regret analysis performed seem rigorous and well-thought out.\n\nHowever, I think that this paper should be rejected because (1) the algorithm does not appear to be a substantial improvement over existing algorithms, (2) the paper makes strong claims about an exponential improvement over standard RL, but doesn't provide a strong benchmark to compare to, and (3) the paper is imprecise and unpolished, with many grammatical errors.\n\nI would be open to reconsidering my score if a) the authors submit a revised version with significantly cleaned up text, and b) if the authors could provide more information about how their contribution compares to the existing literature.\n\nMain argument \n\nThe paper would benefit from establishing stronger context for the central contributions of their paper. For instance, the paper begins by contrasting HRL approaches with a number of standard RL algorithms, saying that approaches such as AlphaGo do not require high-level planning. This seems surprising; many RL researchers would describe MCTS (the base of the AlphaGo algorithm) as performing planning. It would be great if the authors could go into more detail as to what they view as planning, and why AlphaGo does not do so.\n\nAdditionally, the main comparison the authors seem to make is between HRL and naive RL, which does not provide sufficient context to properly analyse their algorithm. Many algorithms are better than applying a classical RL algorithm naively. As such, it is not sufficient to show that the algorithm proposed by the authors is stronger than a naive approach; it would be better to compare the algorithm to either a) the state of the art (SOTA) approach, or b) a more credible approach than the naive one. Experimental evidence would help.\n\nOne point of comparison is Fruit et al. (2017), which is mentioned as another paper which carries out a regret analysis in a HRL setting. Fruit et al. (2017) contains a number of simple numerical simulations; a similar effort here would help.\n\nAnother issue is that the paper is confusing, with systematic grammar errors and typos. The paper would benefit significantly with some copy-editing/proofreading by a native English speaker. For instance, the title should (presumably) read 'Provable Benefits of Deep Hierarchical RL.' Such errors appear throughout the paper. Fixing them would make the paper much easier to understand.\n\nFinally, although this did not factor into the score I awarded the paper, the terminology used by the authors is confusing, referring to their setting as 'Deep Hierarchical Reinforcement Learning.' 'Deep Reinforcement Learning' is a widely used term in industry, referring to algorithms that apply Deep Learning to RL problems, such as AlphaGo or DeepStack. I would encourage the authors to use a different term to describe the setting.\n\nQuestions to the authors: \n\n1) In what way is AlphaGo not doing planning? What is an example of an algorithm that does planning in a standard RL setting? e.g. what would planning look like in Go?\n2) Did you run any experiments/simulations of your work? If not, why not? \n3) Can you elaborate on what a classical RL algorithm would look like that would serve as a proper benchmark to this algorithm?\n4) In your mind, what is the SOTA algorithm for your setting?\n5) What are some simple domains that your algorithm would apply to?\n\n[0]: Moravcik, Matej & Schmid, Martin & Burch, Neil & Lisy, Viliam & Morrill, Dustin & Bard, Nolan & Davis, Trevor & Waugh, Kevin & Johanson, Michael & Bowling, Michael. (2017). DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker. Science. 356. 10.1126/science.aam6960. ", "belong_id": "ByxloeHFPS"}, {"uid": "BJxd6v70YS", "paper_title": "PROVABLY BENEFITS OF DEEP HIERARCHICAL RL", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new kind of episodic finite MDPs called 'deep hierarchical MDP' (hMDP). An L-layer hMDP can be *roughly* thought of as L episodic finite MDPs stacked together. A variant of UCRL2 [JOA10] is proposed to solve these hMDPs and some results from its regret analysis are provided. \n\nPros:\n\n1. The essential result (Theorem 4.1) on the regret bound of the proposed algorithm seems correct. I have not checked the proofs in detail but in part because it does not seem surprising and that a precise assessment is hindered by many typos (see Min2 and Con2).\n\nCons (in descending order of their weights in my decisions):\n\n1. The proposed hMDPs do _not_ seem to capture important features or challenges in hierarchical RL. My understanding is that the transitions in hMDPs work _like_ a clockwork (more on this in Mis6), the algorithm interacts with the sub-MDPs at each layer in turns according to their fixed horizons H_l's. This structure is very rigid temporally and seems to exclude the mentioned example of autonomous driving: the number of decision steps between intersections would be fixed.\n\n2. There are many (typographical) errors in both the text and mathematical expressions. Some of them are more severe than others hindering understanding. \n\n3. Possible as a consequence of Con2, some quantities defined seem unclear or incorrect at worst. For example, the 'standard regret' defined in (2) is an expectation, not a random variable as in convention.\n\n4. There are some notable deviations from similar settings in prior works. They might be worthwhile innovations but their significance or motivations is omitted. For example, the rewards in hMDPs are defined as a function of the full state, i.e. in general not decomposable to rewards on the states of each layer, yet the analogy for hMDP is 'L levels of episodic MDPs.'\n\nA non-exhaustive list of obvious mistakes/typos:\n1. In the title, 'Provably' -> Provable.\n2. In the abstract, 'often both' -> often requires both.\n3. In Organization, 'theoremm' -> theorems.\n4. In Section 2, between exploration -> between exploration and exploitation.\n5. Above Section 3, 'carried' -> carried out.\n6. Below (1), 'amount reward' -> amount of reward.\n7. The definition of horizon H is incorrect. Consider H_1 = 2 and H_2 = 3, the algorithm will interact with the sub-MDPs in the following order within one episode: 1, 1, 2, 1, 1, 2, 1, 1, 2. There are 9 steps not 6 = 2 * 3 as defined.\n8. Section 3.3, 'able accumulate' -> able to accumulate.\n9. Section 3.3, the definition of V_h^\\pi, there should be not \\max.\n10. (5), 'H' -> H - h.\n11. Section 6, 'tabular R' -> tabular RL.\n12. In References, 'Posterior sampling for reinforcement learning: worst-case regret bounds' -> Optimistic posterior sampling for reinforcement learning: worst-case regret bounds.\n13. In References, 'Temporal abstraction in reinforcement learning' should be cited as a PhD thesis.\n\nSome other possible errors/inconsistencies:\n1. Related work listed regret bounds from prior works (the presentation closely mirrors that of [JABJ18]) assume an episodic MDP with non-stationary transitions, i.e. P_t = P_{t'} in general. However, in 3.1 the transitions are stationary. Relatedly, regardless of the stationarity of the transitions, there may not be an optimal _stationary_ policy in an episodic MDP contrary to the claim in the paper.\n2. Indexing seems inconsistent near the top of page 3. The initial state is s_0 but the trajectory starts with s_1. \n3. Near the top of page 3, V_h^\\pi and Q_h^\\pi should sum from h'=h, not h'=1. I assume that the authors intend to define h-step values (to appear in the Bellman equations).\n4. Section 3.3, what are the k's in the equations? \n5. (6), what is n(k-1, e)?\n\nMinor (factored little to none in my decision):\n1. The claim in Introduction that some games 'do not require high-level planning' while others do is highly speculative and vague. Note that any policy can be written a function with codomain in the primitive actions. In fact, many people thought to solve a game like chess or Go requires some temporal hierarchy (opening, mid-game, and end-game).\n2. The comparison to running UCRL2 on hMDP ignoring the given structure seems weak. Given the knowledge of the particular clockwork-like structure of hMDP at each layer (horizons, states, actions), the natural attempt would be run O(L) copies of UCRL2, one for each sub-MDP (under different terminating states of the immediately lower sub-MDP). Frankly, in my understanding, that seems to be roughly what the authors propose as the solution (thus the results unsurprising). Moreover, it is not immediately clear that UCRL2 can apply to the proposed setting of hMDP without checking regular conditions like communicating (diameter being finite).\n3. The claim that RL with options can be viewed as a two-layer HRL needs much elaboration if not correction. Note that in the former, primitive actions are always taken in the original MDP at consecutive steps. \n4. There is a limited relevance to deep learning or deep RL central to the themes at ICLR, i.e. the general issue of representation. This work may be more suitable for other general ML venues.\n\nSome suggestions\n\nI agree with the authors' sentiment that our theoretical understanding of hierarchical RL is relatively limited. I applaud the authors' effort to address this limitation. But judging from this aim of advancing our theoretical understanding, I think the paper may be improved by \n\n1. better articulating the motivations for hMDPs (concrete examples would help)\n\n2. contextualizing hMDPs with respect to other well-known models such as semi-MDPs (technical and precise comparison would help).\n\nTo put it in a different way, it is unclear to the readers why we want to solve this special class of hMDPs and what does hMDPs have to do with the general issues in hierarchical RL. Technically, I feel that assuming episodicity seems against the spirit of hierarchical RL where subtasks are often delimited by their subgoals instead of durations.\n\nIn conclusion, I cannot recommend accepting the current article. \n\n(To authors and other reviewers) Please do not hesitate to directly point out my misunderstandings if there is any. I am open to acknowledging mistakes and revising my assessment accordingly.\n\n\nPost-rebuttal update:\n\nThank you for replying to my review and incorporating some of my suggestions into your revision. However, I found many concerns (and mistakes) unaddressed, such as Mis7. The use of driving in Manhattan as an example troubles me because even stopping for a traffic light seems to disrupt the fixed temporal hierarchy of decisions. In conclusion, I will maintain my recommendation.", "belong_id": "ByxloeHFPS"}, {"uid": "SJgUpKLeqH", "paper_title": "PROVABLY BENEFITS OF DEEP HIERARCHICAL RL", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the theoretical aspects of HRL. It provides theoretical analysis for the complexity of Deep HRL. The idea is to exploit a given action hierarchy, and known state decomposition, the fact that the high-level state space shares similar low-level structures. The final result is an exponential improvement of HRL to flat RL. \n\nOverall, the paper pursues an ambitious goal that analyses the complexity of Deep HRL. The writing is not easy to follow. I some questions and concerns as follows\n\n- I wonder why the state space must be defined in a product form? If a standard RL is used, then it could be applied directly to the state space ($S_L$) on that primitive actions operate. Hence L-1 state spaces will be discarded? I don't see why a flat RL must estimate policies for states at all levels. It looks like many later derivations based on the assumption of factored state spaces and factored transitions on different levels. In the case of factored representation, the authors should make clear assumptions and find a better way to describe the overall algorithm.\n\n- Section 3.2: the authors use time index for Q and V, does that mean all analysis is for non-stationary MDPs? This is not the assumption in Jaksch et al. (2010) and this paper. The description in this section is very confusing and contains a lot of imprecise definitions\ne.g. should H = \\prod {i=1} H_i?? is h =(h_1,...h_L) not in [1,H]? what is the definition of the immediate next lexicographical tuple? etc. The definition of \\sigma is also unclear and hard to understand.\n\n- The analysis in Section 4. and Algorithm 1 are not for Deep HRL as said in Abstract and Introduction. The analysis is based on PAC-MDP learning for models at each action level. This paper's contributions might be clearer if the authors made clearer assumptions, e.g. on action hierarchy, abstract state space structures etc..\n\n", "belong_id": "ByxloeHFPS"}, {"uid": "SJxM3ExoFS", "paper_title": "Unsupervised Representation Learning by Predicting Random Distances", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors discuss the a novel technique, called Random Distance Prediction, to learn rich features from domains where massive data are hard to produce; in particular, they focus on the two tasks of anomaly detection and clustering.\nThe paper is well written and understandable by a non-specialistic audience; introduction and references are adequate, and the theoretical analysis is reasonably explained, although the two optional losses should have been discussed more deeply.\nResults are fairly supporting the authors' claim: however, the improvement in performances w.r.t. alternative approaches are limited in most cases, and the contribute of the optional losses is somehow unclear and inconsistent across different datasets. It would be interesting also to check how stable is the method (i.e. the losses) for different underlying network architectures, and also how RDP compares to very basic approaches employing dimensionality reduction algorithms such as t-SNE or UMAP.", "belong_id": "rkgIW1HKPB"}, {"uid": "SyeWNBsjFr", "paper_title": "Unsupervised Representation Learning by Predicting Random Distances", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a method of unsupervised representation by transforming a set of data points into another space while maintaining the pairwise distance as good as possible. The paper is well structured with background literatures, formula, as well as experiments to show the advantage of the proposed method. I find it generally interesting, with the following major concerns.\n\n1. Representation or dimension reduction? If the original space is a structured space like Euclidean space, then effectively this paper's method coincides with regular distance preserving method in dimension reduction, and Johnson-Lindenstrauss theories. If the original space is not structured or doesn't naturally have a good distance measure, then the proposed method cannot work. For example, if the original dataset is a set of documents, and the task is to do representation learning to convert each document into a compact vector. However, there's no good distance metric for the document space. If TF-IDF is used, then the representation space also inherits TF-IDF type features which is not desired. If more advanced similarity is used for the document space, then the role of representation learning is not essential anymore as that similarity measure can already help the downstream tasks.\n\n2. Section 3 the theoretical analysis. This part seems like a collection of previous works and contains minimal information about the proposed method.\n\n3. Some writing issues, like page 4 line 7 about the equation numbering.", "belong_id": "rkgIW1HKPB"}, {"uid": "rkxisntMor", "paper_title": "Unsupervised Representation Learning by Predicting Random Distances", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n###### Overall Recommendation\nI vote for the Weak Accept decision for this paper. \n\n### Summary\nThis paper introduces a novel model termed Random Distance Prediction model, it can predict data distances in randomly projected space. The distance in projected space is used as the supervisory signal to learn representations without any manually labeled data, avoiding the concentration and inefficiency problem when dealing with high-dimensional tabular data. Their main contribution is extending the random distance in projected spaces to approximate the original distance information of the hight-dimensional tabular data effectively. Overall, the idea in this paper is interesting and effective, the experiment results in two typical unsupervised tasks (anomaly detection and clustering) also look very promising. However, the writing sometimes has unclear descriptions, given these clarifications in an author's response, I would be willing to increase the score.\n\n### Strengths\n1. The illustration of the authors' idea is clear and concise.\n2. The theoretical analysis of the proposed method is solid and systematic, the validity of the subparts have been proven previously.\n3. The experiment part is well organized. The RDP model are compared with several state-of-the-art unsupervised learning methods in 19 real-world datasets of various domains. The experimental setup is solid with realistic considerations, the results are very convincing and promising.\n4. This paper provides sufficient detail for reproducing the results.\n\n### Weaknesses\nLack of systematic description of the authors major contribution. The proposed model looks more like a combination of previous conclusions, which makes readers feel the core parts of this paper build heavily on previous work.\n\n### Questions\n1. What is the relation between random distance prediction loss and task-dependent auxiliary loss?\n2. Are there any solutions and references about how to choose the task-dependent loss L_aux? \n3. Why you shade the second part of the loss function in Figure 1; \n4. How long is it for training the proposed model and getting the experiment results? Does the RDP model still outperform the other algorithms?\n\n### Suggestions to improve the paper\n1.  It would be better to reorganize Section 1 and Section 2, please describe the contribution in a more systematic way.\n2. Add details for the architecture of the model, please give more descriptions about Figure 1.\n3. It might also help to add an algorithm comparison box for the test time for the proposed method.\n\n### Minor Edit Suggestions\n1. It would be better to give more descriptions about Figure 1; the lower right part in Figure 1 is not explained in the caption; the shadow part in Figure 1 is not precise.\n2. Figure 1 was bad organized, please make the legend readable size. \n3. I don't think there exist the proofs of Eqns. (2)-(4) in the reference paper (Vempala, 1998), which was written in Page 4. The number should be revised.", "belong_id": "rkgIW1HKPB"}, {"uid": "BklPSoBsFB", "paper_title": "Collapsed amortized variational inference for switching nonlinear dynamical systems", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Thank you for an interesting read.\n\nAs far as I understand, the paper claims two contributions:\n1. A combination of collapsed variational inference and amortised inference for SNLDS, to make the training pipeline fully differentiable;\n2. An improved loss function upon the variational lowerbound (ELBO) to force the model to use the discrete states.\n\n======= novelty =======\nThe 1st idea is combinatorial: \n1. The forward-backward algorithm is a standard inference method for HMM-like sequence models; collapsed variational inference has been investigated extensively in 2000s when hierarchical Bayes models were actively developed; amortised inference is widely used in variational auto-encoders. \n2. The combination of the above two inference methods on S(N)LDS is new to the best of my knowledge. However, this combination has been proposed on a similar model called Kalman VAE ( Fraccaro et al. 2017), where the sequence model can be viewed as a 'soft' version of SLDS. \n\nThe 2nd idea is interesting but not very well explained to some extent:\n1. The goal of the modified objective function is to encourage the model to use the discrete states (instead of pushing all useful information to the continuous states). It is interesting as it regularises the *exact posterior* of the discrete states conditioned on the *approximately inferred* continuous states. \n2. I believe the entropy regulariser is non-differentiable as it is based on a *histogram* estimate of the temporally averaged discrete state distribution. How exactly is this regulariser implemented? \n3. I agree adding the KL regulariser can avoid the iterating assignment pathology, however, is random assignment of the regime preferred in any case? From the introductory example, I think contiguous segments are preferred.\n\n======= significance =======\nExperiments consider 3 synthetic examples for sequence segmentation (so that ground truth is available). The proposed approach performs significantly better which is a good sign. The paper also provides useful analysis on the effects of balancing parameter tempering which is always welcome.\n\nHowever, two baselines are missing:\n1. To claim the significance of the collapsed variational inference approach, a non-collapsed inference version of the proposed SNLDS model needs to be compared. The authors did discuss this and mentioned possible workarounds (e.g. using the Gumbel-softmax trick for discrete state inference), but the comparison is not reported. If compared, this will serve well as an ablation study for the inference method.\n2. The paper also provides comparisons across models, but I do think the Kalman VAE model needs to be compared to SNLDS. Both models are more flexible than the original SLDS, but the complexity is added in different ways. Since I think the inference mechanisms are similar (both using forward-backward inference for top-level latents and amortised inference for bottom-level latents), this comparison would provide a better ablation study on the modelling side.\n\n======= clarity =======\n1. The paper presentation is overall clear to me, although I found the many sentences in parenthesis a bit distracted, so I would suggest maybe using footnotes for them instead. \n2. For readers who are less familiar with HMMs/forward-backward algorithms, the papers can be difficult to understand, as it skips all the detailed computation of the gamma terms. I would suggest adding the details in the appendix, and/or visualise the intuition using e.g. message passing on factor graphs.\n3. I found the related work well presented with most relevant papers, although I do think the Kalman VAE approach is highly relevant which needs to be cited and discussed.\n\n======== references ========\nFraccaro et al. (2017). A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning. NeurIPS 2017", "belong_id": "BkxdqA4tvB"}, {"uid": "HyedYpoTKr", "paper_title": "Collapsed amortized variational inference for switching nonlinear dynamical systems", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors consider the problem of learning model parameters of a switching nonlinear dynamical system from a dataset. They propose a new variational inference algorithm for this model-learning problem that marginalizes all discrete random variables in the model using the forward-backward algorithm and, in so doing, converts the model to one with a differentiable density, so that the gradient of the variational objective can be estimated with the low-variance reparameterization estimator. The authors also point out an issue in choosing a variational objective; the standard ELBO objective is not suitable for their learning problem, because it leads to a model that does not use discrete random variables meaningfully. To overcome this issue, they suggest a new improved objective and a learning procedure, which encourage the learned model to use discrete variables for capturing different modes of dynamics. The proposed variational inference algorithm was applied to three datasets, and in all these cases, it showed promising results.\n\nI found the main idea and technique of the paper simple and nice. I am reasonably positive about the paper. The main text of the paper is written well, but the experimental result section seems to be rushed and needs to be polished slightly. I gave weak accept, but if the authors give a convincing answer for my question below, I may raise my score.\n\nI presume that the objective L(theta,phi) in (11) is optimized by a version of gradient ascent. Here is my question related to this:\n\n[Q] Why is H(O) in p5 differentiable with respect to theta and phi? \n\nI am asking this question because the distribution O is defined in terms of arg max, which is not a differentiable operator. Furthermore, the definition of O uses p(s_t|z,x), which uses the model parameters theta. Oh, by the way, I think that the definitions of H and L_CE should include the expectation with respect to q_theta(z|x). \n\nSome minor comments are added below.\n\n* formula (1), p2: p(x1|s1) should be replaced by p(x1|z1)p(z1|s1)\n\n* p3: There are no sub-figures labeled with (a) and (b) in Figure 2. I suggest to put (a) and (b) in front of the captions of the two diagrams in Figure 2. A similar comment applies to Figure 3, because the main text refers to something called Figure 3(a) and Figure 3(b). Also, the paper uses fig. 2(b) sometimes, and Figure 2(b) in other times. Using one convention consistently might help some readers.\n\n* p3: Cat(s_t | S(f_s(...)) ===> Cat(s_t | S(f_s(...)))\n\n* p3: SDLS ===> SLDS\n\n* p3: log p(x) <= L(...) ===> log p(x) >= L(...) \n\n* p4: I found the phrase 'so they need to perform multiple forward-backward (FB) passes' vague. The algorithm in the paper uses FB twice, and 'multiple' in the quoted phrase might mean 2, 3 or more. This makes it less clear whether the algorithm has any benefit over the existing approaches.\n\n* p6: This measures compliments ===> This measure complements\n\n* p6: within some small temporal around ... where noted ===> within some range around ... as noted\n\n* p6: are is constant ===> are constant\n\n* p6: The ground truth discrete states ===> The ground truth discrete state\n\n* p7: The resulting of ===> The result of ", "belong_id": "BkxdqA4tvB"}, {"uid": "rylQ61g-cB", "paper_title": "Collapsed amortized variational inference for switching nonlinear dynamical systems", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "SUMMARY:\nThis paper proposes a method to segment time series into discrete intervals in an unsupervised way. The data is modeled using a state space model where each state consists of a discrete and a continuous part. The discrete state denotes the segment the system is currently in and the continuous state which is conditioned on the discrete one denotes an uninterpretable feature vector. The transition distributions are non-linear. The observation at each time step is high-dimensional and produced by an emission distribution whose parameters are given by a neural network which takes in the continuous state. Learning and inference is done by maximizing the evidence lower bound (ELBO). Problems with the discreteness of latent variables is circumvented by marginalizing (collapsing) them out using the forward-backward algorithm. Problems with making discrete states meaningful when there are non-linear transitions/emissions is addressed by annealing. This annealing scheme forces the conditional distributions on the discrete state to have high entropy (be close to a uniform distribution) at the start by adding a term to the ELBO objective and the multiplier of this term is decreased as the training progresses. There are actually two terms to do this since one alone didn't work.\n\nSTRUCTURE:\nThe paper is well-written and easy to understand.\n\nNOVELTY:\nI found the technique of estimating gradients using forward-backward to be interesting and potentially useful in other domains when parts of generative models can be marginalized out using belief propagation.\nWhile the problem of unsupervised time-series segmentation is an important one, I'm not sure the proposed technique addresses it completely.\nThe main thing that seems to be doing the work is not the marginalization using forward-backward, but rather the annealing scheme which itself seems ad-hoc and it is not clear whether this is generalizable to other domains or it just happened to work on the problems in the paper.\nIt is not clear why maximizing the entropy of the variational transition should encourage meaningful clustering.\nWould this work even if the emission distribution is made much more powerful?\n\nEXPERIMENTS:\nThere are experiments on three synthetic datasets. While the proposed method beats the competing methods, it is unclear that collapsing is helpful. Also, no annealing was used in the baseline methods (like increasing K in SVAE or multi-step training).\n\nCONCLUSION:\nWhile the problem this paper is tackling is significant, it isn't clear that the proposed method tackles it. I would consider bumping up my score if \n- this method is demonstrated to work on a real dataset and/or\n- there is a better understanding of the principles behind why this annealing scheme helps.\nAlso, proper tweaking of the competing algorithms (similar to annealing) is needed to compare the proposed method fairly.", "belong_id": "BkxdqA4tvB"}, {"uid": "S1l96w7HKH", "paper_title": "NoiGAN: NOISE AWARE KNOWLEDGE GRAPH EMBEDDING WITH GAN", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to provide a novel noise-aware knowledge graph embedding (NoiGAN) by combining KG completion and noise detection through the GANs framework. More specifically, NoiGAN repeatedly utilizes a GAN model to 1) approximate the confidence score for facts identifying reliable data (discriminator) and 2) generate more challenging negative samples (generator). Then, it uses this confidence score and negative samples to train a more accurate link prediction model. The authors validate the proposed model through several experiments.\n\nThis paper reads well and the results appear sound. I personally find the idea of incorporating confidence score into a link prediction model to achieve a more accurate model very interesting. Furthermore, the provided experiments support their intuition and arguments outperforming considered baselines.\n\nAs for the drawbacks, I find the baselines considered in this work outdated missing many SOTA and related works in link prediction and noise detection [1,2, 3, 4, 5]. Further, I believe this work needs more experimental results and an ablation study capturing different aspects of the presented method. My concerns are as follows:\n\n\tConsidering the existing reverse relation issue in FB15K and WN18, I suggest conducting the experiments on the FB15K-237 and WN18RR from [6] instead. \n\tI suggest considering more recent link prediction models as baselines.\n\tI am wondering if the only difference between NoiGAN and KBGAN [7] is incorporating the confidence score in the link prediction loss?\n\tConsidering the fact that NoiGAN repeatedly retrains GAN and link prediction model, I suggest providing a comparison of computational complexity.\n\tI am wondering if NoiGAN can only work with pre-knowledge of noisy triples in KG? If not, why didnt you report NoiGAN performance with 0% noise in Table 3?\n\tI find utilizing few examples to evaluate the power of discriminator in distinguishing noisy triples (Table 4) not satisfactory at all. I suggest experimenting with more data and providing the per-relation breakdown performance of the discriminator.\n\nOn overall, although I find the proposed model quite novel and interesting, the paper needs more experimental results to validate the idea.\n \n[1] Pinter, Yuval, and Jacob Eisenstein. 'Predicting Semantic Relations using Global Graph Properties'.\n[2] Nathani, Deepak, et al. 'Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs'. \n[3] Balazevic, Ivana, Carl Allen, and Timothy M. Hospedales. 'TuckER: Tensor Factorization for Knowledge Graph Completion'.\n[4] Sun, Zhiqing, et al. 'Rotate: Knowledge graph embedding by relational rotation in complex space'.\n[5] Pezeshkpour, Pouya, Yifan Tian, and Sameer Singh. 'Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications'.\n[6] Dettmers, Tim, et al. 'Convolutional 2d knowledge graph embeddings.', AAAI-18.\n[7] Liwei Cai and William Yang Wang. Kbgan: Adversarial learning for knowledge graph embeddings. \n", "belong_id": "rkgTdkrtPH"}, {"uid": "ryl29S9x5H", "paper_title": "NoiGAN: NOISE AWARE KNOWLEDGE GRAPH EMBEDDING WITH GAN", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a GAN-oriented framework for training robust-to-noise neural link predictors. My main concern is that CKRL is the only baseline -- ignoring years of prior works in this space (see e.g. [1, 2]).\nFurthermore, [2] shows that two of the three datasets used by the authors suffer from test triple leakage in the training set.\n\nFinally, the considered datasets do not really test for the presence of noise - authors may want to check out e.g. https://arxiv.org/abs/1812.00279 (there are several works in this space, all of which were systematically ignored by this paper).\n\nFinally, authors claim neural link predictors were never used for denoising, but actually [3] use them to learn a prior distribution over triples in a probabilistic DB setting.\n\n\n[1] https://arxiv.org/abs/1806.07297\n[2] https://arxiv.org/abs/1707.01476\n[3] https://ai.google/research/pubs/pub45634", "belong_id": "rkgTdkrtPH"}, {"uid": "HklhuRpxiH", "paper_title": "NoiGAN: NOISE AWARE KNOWLEDGE GRAPH EMBEDDING WITH GAN", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presented a jointly learning framework based on GAN for tackling both knowledge graph completion and noise detection simultaneously. Existing works only deal with each of task independently and did not investigate the benefits of coping with both tasks together. The paper is well motivated. In order to achieve them, the paper presented a GAN framework in order to train a noising KG embedding as well the generator and discriminator. The key connections between two parts are through the confidence of a noise triple and generation of the negative sample triples. The whole framework looks quite interesting and promising. The experimental results are provided to validate the effectiveness of the proposed model. \n\nThere are two key concerns about this paper:\n\n1) It is well known that both GAN and RL are hard to train, not to mention combining them together to joint train in order to deal with data indifferenceability issue of discrete triple generation. Are the results easy to reproduce?\n\n2) Choosing 10% triples as positive training examples seems very ad-hoc. Have you studied the sensitivity of the number of percentage of triples as positive training examples on the system performance?\n\n3) I don't know too much about the methods from knowledge graph noise detection so maybe one baseline - CKRL is enough for representing state-of-the-arts. However, for knowledge graph completion task, TransE is most simple baseline and they are rich state-of-the-art methods in this line such as [1]. It is not convincing to show the advantages of the proposed NoiGAN without such comparisons. \n\n[1]  RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space. ICLR'19. \n\n\n", "belong_id": "rkgTdkrtPH"}, {"uid": "Byx6q3uqtS", "paper_title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper is written very nicely and the experiments are convincing (though you can always show more)\n\nIn terms of novelty, I shamelessly can say the idea is very simple and the basic form, the Hoyer regularization, was known. That said, I am all in for simple and efficient solutions so I am giving this paper a weak accept for now.\n\nThere is not much to ask here (unless I say I want to see how this would work on other backbones and problems). nevertheless, to improve this work, I think the authors need to compare which solution (referring to algorithms in Table1, 2 etc.) is faster/more well-behaved given the combo explained at the bottom of page 5 . This is basically my question/request for the rebuttal.  ", "belong_id": "rylBK34FDS"}, {"uid": "HygbT3sqKH", "paper_title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "To enforce sparsity in neural networks, the paper proposes a scale-invariant regularizer (DeepHoyer) inspired by the Hoyer measure. It is simply the ratio between l1 and l2 norm, which is almost everywhere differentiable, and enforces element-wise sparsity. It further proposes the Hoyer measure to quantify sparsity and applies the DeepHoyer in DNN training to train pruned models. The extension of Hoyer-Square is also straightforward. \n\nI generally enjoy simple yet effective ideas. The idea is very straightforward and well intuitive. The paper is well written and easy the follow. The discussion on the Hoyer measure is inspiring and the empirical studies on various different network architecture/datasets compared to several competitive baselines verify the effectiveness of the DeepHoyer model. \n\nTherefore I'm leaning to accept it. ", "belong_id": "rylBK34FDS"}, {"uid": "HkllzwGI5B", "paper_title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper focuses on sparse neural networks. Typically, l1 regularization is the go-to strategy, however, it is not scale invariant. That is, all weights are affected by the regularization, not only those that are being driven to 0. l0 regularization is theoretically optimal, however, it is not smooth and has no gradients almost everywhere, so it cannot be used for training. As a compromise the paper proposes Hoyer regularization, that is the l1/l2 ratio. The Hoyer regularization has the same minima structure and leads to sparse solutions while being scale invariant, that is it does not affect all weights in the process. Additionally, the paper proposes structured Hoyer regularization. Last, it employs the said regularizations in deep networks: LeNet, AlexNet and ResNet on several datasets: MNIST, CIFAR, ImageNet.\n\nStrengths:\n+ The described method is simple, intuitive and straightforward. By applying the said regularization (~  |w_i|/sqrt( w_i^2)), one arrives at seemingly sparser solutions, which is verified in practice.\n\n+ The experiments are extensive and convincing. I particularly like that the authors have used their method with complex and deep models like ResNets, on large scale datasets like ImageNet.\n\n+ The presentation is generally clear and one can understand the paper straightaway.\n\nWeaknesses:\n+ The contributions of the paper are rather on the thin side. At the end of the day, Hoyer regularization is taken from another field (compressed sensing) and applied on deep networks. This is also witnessed by some moderate repetition in the writing, e.g., between the introduction and the related work.\n\n+ There are some points where the paper becomes unclear. For instance, in Figure 3 what are the 'other methods'?\n\n+ In Figure 1 it is explained that the Hoyer regularization leads to minima along the axis. The gradients then push the models 'rotationally'. Could this lead to bad multiple local optimal problems? Is there any guarantee that any particular axis will generate better solutions than the other?\n\nAll in all, I would recommend for now weak accept. I find the work interesting and solid, although not that exciting.", "belong_id": "rylBK34FDS"}, {"uid": "rkxD5oj2YH", "paper_title": "Neural-Guided Symbolic Regression with Asymptotic Constraints", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper considers a task of symbolic regression (SR) when additional information called 'asymptotic constraints' on the target expression is given. For example, when the groundtruth expression is 3 x^2 + 5 x, it behaves like x^2 in the limit x -> infinity, thus the power '2' is given as additional information 'asymptotic constraint'. In the paper's setting, for SR with univariate groundtruth functions f(x), 'asymptotic constraints' for x-> infinity and x -> 0 are given. For this situation, the paper proposes a method called NG-MCTS with an RNN-based generator and MCTS guided by it to consider asymptotic constraints. In SR, a learner is asked to acquire an explicit symbolic expression \\hat{f}(x) from a given set of datapoints {x_i, f(x_i)}, but unlike the parameter optimization aspect of a standard supervised ML setting, the problem is essentially combinatorial optimization over exponentially large space of symbolic expressions of a given context-free grammar (CFG). For a given symbolic space with a prespecified CFG, extensive experimental evaluations are performed and demonstrated significant performance gain over existing alternatives based on EA. Also, quantitative evaluations about extrapolative performance and detailed evaluation of the RNN generator are also reported.\n\nThough it includes a lot of quite interesting methods and results, the paper has two major issues: \n\n(1) the proposed method NG-MCTS explicitly uses the fact that the target expressions are generated from a CFG, but this assumption sounds unnatural if the target problem is SR (unlike the case of GVAE). Apparently, all results except a toy case study in 5.2 depend on artificial datasets from a CFG, and any practical advantages or impacts are still unclear because the experimental settings are very artificial. \n\n(2) the most important claim of this paper would be the proposal to use 'asymptotic constraints', but the availability of this information sounds too strong and again artificial in practical situations. A one-line motivation saying that 'this property is known a priori for some physical systems before the detailed law is derived' is not convincing enough.", "belong_id": "S1gTAp4FDB"}, {"uid": "HklhRjcJcH", "paper_title": "Neural-Guided Symbolic Regression with Asymptotic Constraints", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThe authors of this paper propose a novel approach for symbolic regression. The simulation results demonstrate that the proposed approach can find a better function g producing similar results as desired function f by providing the additional information  the leading power of function f.\n\nPaper strength:\n1.\tThe proposed NG-MCTS is elegant to solve the problem of symbolic regression.\n2.\tExperiment results illustrate the superiority of the proposed approach\n\nPaper weakness:\n\n1.\tI can follow most of the mathematics in the paper. But the most confusing part for me is why you feed the random partial sequence for training. Besides, how you do inference to generate a sequence of the production rules.\n2.\tWhat is the final objective function? If I do not misunderstand, it could be the cross-entropy loss between the output of GRU and the next target production rule, RMSE and the error on leading power. Then how you optimize it? Please describe more details about this.\n3.\tThe authors of this paper introduce more information  leading power of desired function for symbolic regression but they incorporate the additional information by introducing a simple loss function term. How about the performance of baseline approaches with those kinds of information?\n4.\tThe whole systems seem like very complicate and it would be more interesting if the authors provide sufficient ablations to decompose the complex algorithm.\n", "belong_id": "S1gTAp4FDB"}, {"uid": "SyeeGb2gcH", "paper_title": "Neural-Guided Symbolic Regression with Asymptotic Constraints", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\n\nThis paper introduces the use of asymptotic constraints to find\nprune the search space of mathematical expressions for symbolic\nregression.  This is done by training a neural network to\ngenerate production rules conditioned on being given the\nasymptotic constraints and previously generated production\nrules. This neural network is then itself also used to guide a\nMCTS to generate mathematical expressions.\n\nThe algorithms is compared against a reasonable set of baselines\nand related algorithms.\n\nFeedback:\n\nThis is a very clear and well-written paper. It was\nstraightforward to understand and very easy to see how it fits in\nwith the broader literature. The insight about using asymptotic\nconstraints makes the result a bit limited to only generating\nmathematical expressions, and it would have been a bit nicer if\nthere was something more generically applicable to program\nsynthesis in general. It's not really clear to me how the\nexisting work extends to programs.\n\nThe evaluation was very thorough and the appropriate algorithms\nwere compared against the work. I came away with a good\nunderstanding of how well the model generalizes to larger\nexpressions compared to existing work.\n\nMinor notes:\n\nThe abstract is a bit inaccurate as the NN generates production rules\nand not expressions.", "belong_id": "S1gTAp4FDB"}, {"uid": "rJgavbZ2tr", "paper_title": "City Metro Network Expansion with Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces a method for solving the problem of network expansion, in particular, considers the city metro network and its expansion within the new metro line. Since this problem was previously represented as the non-linear integer problem, having an exponential number of constraints or requiring expert knowledge, authors had an initial motivation to appeal to learnable algorithms from the reinforcement learning paradigm. Authors proposed to consider the problem as Markov Decision Process, so that the environment is represented as the city grid, with constructed metro network, before some timestamp conditioned by additional transportation constraints, while each new state in the expansion process of construction one separate metro line is the state of the metro network as graph with new station added to the metro line, considered as the agents action. The custom reward function was represented based on metrics like OD trips used in the traditional approaches and the first time used in this problem specific social equity score. The permissibility constraints of action, i.e. adding a new station, were incorporated into policy function, while the attention mechanism was used here as in other seq2seq models, to query the most suitable station among candidates. Authors use the actor-critic approach, widely exploited in reinforcement learning problems, and with all the above-mentioned modification reached the baseline results on the real-world data, obtained from Xian city metro network, Shaanxi province, China.\n\nMain arguments \n\nApart from the interesting approach, this paper should be rejected due to several reasons: (1) the specific application field, considered in this paper, have to be generalized to more general cases, in order to be valuable for Machine Learning community, (2) from RL algorithms perspective, the novelty of proposed method is questionable due to lack of literature review and advanced approach to deep reinforcement learning, (3) requirement of explainable AI is essential for deployment and in spite of the quite sufficient explanations of the algorithms works, this paper does not well justify its superiority over the traditional methods either by theory or practice, due to experiments suffering from the lack of variability and missing the main point of improvement, which leads to generally insufficient contribution (4) the paper is unpolished and lacks specific formulations from the existing literature on related subjects.\n \nIn detail\n\nThe paper does a great job of justifying neither the novelty of its method nor the guarantees of increased performance or efficiency compared to existing methods. Although the proposed method discusses the specific problem and there could be a lack of existing sufficiently good methods in this specific topic, the reinforcement learning paradigm is utilized by authors in a very superficial and general manner. \n\nFirstly, the paper excludes any comparison to similar problems and already existing methods of RL used by similar fields, which can be expressed as planning with constraints (for example, path planning in mobile robot navigation), thus will be more valuable to ML community than specific application of graph expansion problem, limited to line-by-line addition. \n\nSecondly, except general notice of combinatorial problems solved by RL algorithms, the literature review lacks mentioning graph representation methods or any seq2seq algorithms (which efficiently uses attention), which should serve as an initial guess for the well-performing model in this particular problem. Regarding the RL approach, there are plenty of already solved planning problems, papers comparing RL algorithms and their performances on grid-based, discrete environment problems in order to mention here and ideally, comparing to proposed one within the same specific problem formulation. Because of this reason, there is no justification of concrete architecture construction and choice of actor-critic model for learning this policy in this specific example and so there is no intuition, why this model will work better than existing methods. Besides, this leads to the doubtful novelty of the proposed algorithm, although used first time in this specific field, but composed totally from the same components and sometimes in the same combination as in other works related to RL methods in planning.\n\nMoreover, the explanation of the methods architecture and basic components influence on the system is insufficient but might serve as an initial explanation of how this algorithm works. However, the number of experiments and their variations leaves much to be desired to justify its practical advantages and its utilization purposes as explainable AI. (1) there is no ablation study conducted to justify the choice of the proposed model over existing reinforcement learning methods, like actor-critic methods and their several variations with baseline, including modifications like attention model, graph representation, encoder and decoder architecture. The mentioning of these methods in the literature review is enough to answer this issue. (2) Using only one, hardly tuned baseline method on one dataset (without any reference to existing benchmark results on it) does not prove the efficiency and good performance of the model. Additional experiments can on different environments structure (city grids, metro network) of complexity of the environments (the width of the cell in the grid) to measure the performance of the algorithm based on hyperparameters (3) Real-world data brings more application-based matter into the subject, but requires more thorough investigation on the optimality of the solutions, proposed by RL method. This means that the comparison with the groundtruth metro network expansion (built after October 15) cannot be used as it is without using the expert evaluation. As a solution and habit in the machine learning literature, without the expert knowledge on the dataset, authors may propose a comparison of the real-world solution with the algorithms results, based on the optimized metrics (OD trips score and social equity) included into reward function. \n \nFinally, the proposed paper is imprecise in several ways. In terms of RL formulation and usage of RL terms (commonly introduced in literature), it reveals inappropriate usage of MDP terms and insufficient description policy-gradient approach in actor-critic training algorithm. While not appealing to this issue, authors missed the opportunity to formulate this problem as discrete MDP with high-dimensional action space and sparse rewards, thus had less opportunity to research different model-based reinforcement learning problems in discrete environments to conduct better model selection and experiments. Besides, the paper lacks details in some parts, like the usage of 3G cellular mobile data, which was not mentioned in this work, although it is emphasized as an essential part if not a significant contribution of this research.  \n \n\n \nQuestions to answer\nIntroduction\n\tWhat is the reason by constructing the city metro networks line by line, not iteratively adding stations to the existing network on the grid? This would make your problem closer to general grid-based planning problems. Do traditional methods expand metro networks only line-by-line and can this be there a limitation?\n\tHow does a good solution is represented in the literature with traditional approaches? What are the general measures and constraints in practice?\n\tIs incorporating social equity concern your contribution? Then why does the maximization of OD trips is not enough (there is no mention of the preferability of social equity metric based on the results of experiments)?\n\tThe effectiveness of the method due to experimental results is still questionable.\nRelated work\n\tIf the main concern of the paper is still the limitation of other methods which use expert knowledge then it is better to state the usage of additional data (development index) in social equity metric as a reward design part in Appendix to justify this reward engineering\n\tWe believe that RL has the ability to solve the metro expansion problem is the statement, which should be substituted by extensive literature review on RL methods used for planning with constraints or specifically graph-based expansion methods. \nProblem formulation\n\tIs the network graph undirected or directed (imprecise inclusion of direct links)? How does this the OD trips are measured (no information here or in Appendix A).\n\tWhat is the reason behind 4 constraints provided in problem formulation? Are they used or defined in the literature review papers?\n \nMethod\n\tRL and deep learning methods included in the literature review should be the reference of choosing one or another component of the algorithm, including the formula for policy and critic update.\n\tIt is essential to use notation based on RL formulation, for example, use r(s_t,a_t) = 0 if s_t  is not terminal state and r(s_t,a_t) = w(Z|G) in order to state the sparsity of the problem. S_t here is Z_t sequence of chosen stations during the episode (line expansion process) and so on.\n\tUse precise formulation, for example, the proposed P(Z|G) probability distribution is the trajectory distribution and not policy function, which could be denoted as (z_t | S,G,M(Z_t)) \n\tHow does the choice of filter design is justified, based on similar works in the transportation field?\n\tWhat is the reason by choosing the 1-dimensional CNN layer to represent candidate stations and what is the concrete input information? There is no formal specification of the input data, regarding the existing graph structure, only a short mention of it.\n\tIs the attention mechanism architecture, used here, similar to other models used in seq2seq problems? (Answer: Yes) What is the motivation to use this concrete architecture? \n\tWhy does the permissibility mapping using filter is used in policy function? Are there reasons to use it in policy function, rather than include in reward design? For example, the commonly used approach is to penalize unfeasible actions (stations) with a very low reward, during the episode learning?\n\tTraining: \n\tHow does the sparsity of reward influence the performance? Is there a way to better design reward, so that there will not be a necessity to update actor and critic only after the end of the episode (termination step)?\n\tWhat is the reason for learning critic as V(Z) = w(Z|G)? Is there a need for baseline if the state and the sequence of actions have bijective mapping, meaning that one sequence of stations can generate the unique state of the environment  line expansion, and vice versa? The intuition of the baseline is to measure the value of the state as the average among all possible actions, which lead to this state.\n\tWhat are the batches B used in the actor-critic training procedure?\n\tHow do we generalize the environment training? Do we need to retrain the reinforcement learning algorithm for each different initial metro city network configuration, or it is generalizable to other grids, using the same weights?\nExperiments\n\tPlease, include the training time of the RL algorithm and its inference time, and compare it to the performance of the baseline algorithm, which should be one of the most essential contributions, due to the high time-complexity of traditional methods.\n\tHow does the choice of corridor width influence the performance of the baseline method? \n\tIs there the baseline performance on the same city metro network (Xian city metro) mentioned in other literature, to directly compare with? This is necessary to fill the gap in justification of proposed results optimality (for the baseline case).\n\tHow does the partial similarity of the 2 times line expansion by RL method and 6 real-world lines of the city metro network can justify the optimality of the proposed method? Can you provide the measurement based on OD trips and social equity? Can you provide a truly optimal solution based on the grid granularity, initial network graph, and constraints, to compare with as an optimal solution?\n", "belong_id": "SJxAlgrYDr"}, {"uid": "Hyxa9ARAKr", "paper_title": "City Metro Network Expansion with Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Review of City Metro Network Expansion with Reinforcement Learning\n\nIn this work, they investigate the use of RL (actor-critic) for planning the expansion of a metro subway network in a City. They formulate the problem of expanding the metro system as sequence of actions, hence a seq2seq+attn type architecture is used for the actor. The reward calculation is based on a combination of origin-to-destination and social equity metrics. They apply their method to plan the expansion of the current metro lines in a large city in China where they can leverage the mobility information information gathered from ~ 25M mobile phones in the city.\n\nI think this work has great potential, as they identify a data-driven approach that can have a high impact (i.e. design subway lines affecting 25M+ people in a real city). That being said, there are some issues that I feel needs to be addressed before the work can be published at a high quality conference, so I want to help the authors improve the work by highlighting important points that will make the work better:\n\nThe related work section on metro network design is only a paragraph. I think both the related work and experiments sections are missing many substantial contributions, as there is vast literature of work from Operations Research area about solving such problems through constraint optimization. Even a simple google scholar search brings many examples [1]. This is before discussing about existing ML and Genetic Algorithm approaches [2], or with Monte Carlo Tree Search [3].\n\nWithout discussing existing work and offering detailed comparisons and experiments, this paper essentially just shows that RL can be applied to such problems, but the reader wouldn't know whether it is the best tool, or simply if RL is the hammer that is used to treat every problem like a nail. The only baseline the paper compared against is another paper published in 2019 which IMO is not satisfactory.\n\nOn a related note, there are also a few projects doing similar network optimizations with slime mold (Physarum) and different variations using it for shortest path finding in mazes and all sorts of interesting problems [4].\n\nI'm reminded of a nice work called Neural Combinatorial Optimization with Reinforcement Learning [5] that proposed the use of neural nets to solve TSP problem, but ultimately needed to put in the work to compare with traditional approaches. I'm including the reference here so the authors can learn from that paper's experiences to help improve the work.\n\nRegarding the dataset: One of the most impressive points is that the work utilized a giant dataset of ~ 25M mobile phones. For an important dataset like this that is central to an impactful application of ML, would be nice to have a discussion (even in the Appendix) to describe how the data is collected, and what regulations / user privacy issues the research team might have to overcome, as these types of issues are becoming very important to the wider research community. Would also like to see a discussion about whether the large amount of data points can be reduced to a simpler 2D density map and achieve similar performance? Would there be any plans to release an anonymized version of the dataset for demonstration purpose?\n\n[1] https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=transportation+network+operations+research+constraint+optimization&btnG=\n\n[2] https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=transportation+network+design+machine+learning&btnG=\n\n[3] i.e. Link Prediction with Monte Carlo Tree Search (https://paperswithcode.com/paper/m-walk-learning-to-walk-over-graphs-using)\n\n[4] https://www.researchgate.net/publication/324791496_Physarum-Inspired_Solutions_to_Network_Optimization_Problems\n\n[5] https://openreview.net/forum?id=rJY3vK9eg", "belong_id": "SJxAlgrYDr"}, {"uid": "rJl7sAFy5H", "paper_title": "City Metro Network Expansion with Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper the authors train a seq2seq model through reinforcement learning to iteratively expand a city metro network. The authors show that different objectives can be satisfied with this approach, such as the accessibility to different areas (something the authors call social equity indicator) or maximising origin-destination trips. \n\nThe paper is interesting but could use a more extensive comparison to alternative approaches or ablated  version of the same approach. For example, what if the approach would only take into account the last metro station instead of the complete previous sequence? Would it work less well? Additionally, the baseline method the approach is compared against is not explained in enough detail. In addition to RL methods, method such as genetic algorithm have shown great promise in producing layouts, such as for wind turbines (e.g. Grady et al. Placement of wind turbines using genetic algorithms). I wonder if such an approach would work equally well for designing metro lines and if RL is really the best technique here (which it might be but Im not convinced yet). Because of the mentioned shortcomings, I believe the paper should be improved before publication. \n\nAdditionally, the paper would benefit from a careful spell and grammar check. I found multiple typos, especially in the introduction.  ", "belong_id": "SJxAlgrYDr"}, {"uid": "Hket-APeFB", "paper_title": "VIMPNN: A physics informed neural network for estimating potential energies of out-of-equilibrium systems", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a number of new / extended datasets for the evaluation of ML-based prediction of energies of unstable systems, as well as a network (VIMPNN) that includes a new and better way of including bond-type information. It is also proposed to use auxiliary losses (predicting other chemical properties).\n\nAlthough I am not an expert in chemistry, the new datasets seem fairly well thought out and their utility is well motivated. The proposed change to the MPNN network architecture is rather simple and hardly physics inspired, but the empirical improvement seems substantial, so this too is a nice contribution. So I have decided to give the weak accept rating.\n\nIn 4.2 it is explained how different ways of incorporating bond information were evaluated, and it is stated that best results were obtained in the case a.ii). However, no results are presented to support this claim, leaving the reader to wonder how rigorous this exploration was. I would suggest systematically evaluating the different options and including the results in an appendix. \n\nIn table 1 results are shown for the MPNN baseline, baseline with specialised node updates a.ii, and with auxiliary estimates. However, combinations of these are not evaluated. Nevertheless, if I understood correctly, the VIMPNN method tested later includes all of the separate improvements. It would be good to include experimental results to motivate this.\n\nAs acknowledged in the paper, the idea of using bond-type information was already in Gilmer et al. Also, I think the different ways of including bond-type explored in this paper are not really informed by physics. The choice for method a.ii is made based on empirical results. This is not a problem in itself, but I would suggest that the authors change the wording to not over-promise on the physics-inspiredness. E.g. the abstract says VIMPNN integrates prior knowledge such as the existence of different interatomic bonds, suggesting that there is more prior knowledge being exploited than just bonds.\n\n\nComments:\n\nIt produces comparable accuracy to that of DFT while also improving computation time by 5 orders of magnitude. \nI assume this speedup is relative to DFT. It would be good to be explicit about that, and also discuss the speed relative to the MPNN baseline (I suppose MPNN and VIMPNN are similar).\n\nThe change of atomic distances are performed isomorphically - I would say isometrically. \n", "belong_id": "HJl8SgHtwr"}, {"uid": "S1lomgQcYS", "paper_title": "VIMPNN: A physics informed neural network for estimating potential energies of out-of-equilibrium systems", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper tackles the problem of estimating the electronic structure's ground state energy of a given atomic system by means of supervised machine learning, as a fast alternative to conventional explicit methods (DFT).\nThis is done by improving on a previous method, MPNN, Message Passing Neural Networks, and in particular by including information on bond type as input, so that the NN can learn the appropriate weight for messages going through bonds of that type.  In addition, training on several target labels (multi-regression) is attempted, with the idea that more physical outputs may help building better hidden representations (on this, there are mixed results).  Separately, training sets are enriched with non-equilibrium structures, so as to confront the NN with more diverse data.\n\nThe method is tested on 3 kind of training sets. The first is a simple but time-consuming augmentation of QM9, with inter-atomic distances varied, so as to increase the training set's size (and in particular, including non equilibrium configurations). The second consists in a periodic and thus infinite simple crystal structure (with, again, variations in inter-atomic distances, enriching the dataset). The third is a pseudo-cristalline structure with atoms randomly placed on a regular grid, forming a somewhat random structure, also named crystal (this is not a very good name).\n\nThe paper is overall rather well written, sometimes being a bit cumbersome (long sentences), but mostly it is stating clearly what is done or discovered.  The work is situated within the existing literature (that I am not familiar with at all).  The idea of using physics to guide architecture choices is gaining a lot of attention recently and seems to be well-suited to this particular problem, and well applied.  Several ways to use the bond type information have been attempted in this work, and several of them are reported and compared (a couple of them are discarded).  The results convincingly show that using bond type information indeed increases performance, both for small systems and for regular crystals.  The impact of performing multi-regression is less important, but still positive.  For large ''random crystals'', the method does not perform very well, and this represents a challenge for future work. Such a confession on the method's limitations is welcomed.\n\nGiven the idea (using physical information as bond type) is clearly and honestly presented, produces significant improvement compared with previous works, and has perspective for multiple future developments, I recommend acceptation of this paper.\n\n\nThere are however a number of points that could be improved.\n\n1. There is a physical mistake that is not crucial but should be corrected, when talking about the ground state, and in particular before this sentence: ''accurate ground-state energy estimation of out-of-equilibrium molecule''. Ground state means minimal, T=0K energy level, so by definition it is at equilibrium. Thus, the sentence seems quite contradictory to a physicist.\nWhat DFT and VIMPNN actually compute is the electronic structure's ground state's energy (at fixed positions of the atom kernels). I think this distinction should be mentioned just once, and then you could proceed with saying ground state energy. \nBecause of this, I would recommend to edit the title so as to suppress ''out of equilibrium'' from it.  Otherwise readers may think the method deals with non-equilibrium electronic structures (non ground states), which clearly it does not at all, or they may think that it is especially good at estimating energies for out of equilibrium systems, which is not its primary goal.\n\n2. I do not understand very well the training procedure.  Also there are some tests that seem to be interesting and that are not performed (as far as I understood).\nDoes each training set contains the 90%-150% data augmenations, for each non-augmented training configuration ?\nWhy is training performed separately for each kind of data set ? Wouldn't the ultimate goal be to transfer learning from a type to another, e.g. from QM9-style to crystal style, etc ?  (as far as I understood, this was not done)\nIsn't it interesting to see how much training on an augmented data set (let's say QM9) improves performance on the non-augmented data (the ''true data'' in a sense) ? Although the augmented data is obtained by DFT, and comparing models trained on different data sets is unfair, I think it may be interesting to see if VIMPNN benefits more than MPNN from this strategy (so compare the performance gains of both algorithms obtained by augmenting a data set). This kind of comparison may also be done for the ''augmentation'' of a training set by the concatenation of it with another one (although in that case it may be detrimental to the test accuracy?).\nIf you actually did some of this, then I misunderstood and I am sorry, but then this also means you should clarify.\n\n3. Section 3.3:\nI would not call this a crystal, but more something like ''random finite structure''. This should be done everywhere in the paper.\n\n4. Section 4.1 is a bit too short for the inexperienced reader. I suggest to be a bit more explicit on what is learned\n\n5. Section 4.2: It is nice to say you tried other ways, keep that.\nHowever, try to be more explicit on what is shared and what isn't, in the architecture you finally pick. Is lambda(v,w) a common value for all bonds of the same type, like C-C ? Maybe you could provide an example or some more detailed notation to make your choice fully explicit (after all this is the core of the paper).\n\n6. Section 4.3: could you quickly comment on why you don't use more of the 13 physical observables available in QM9 ?\n\n7. Table 1: do the three last lines correspond to ''no BT information + a single auxiliary estimate'' ?  It seems to be the case, but then you say you will continue with the auxiliaries, in addition to the BT information.  Why don't you display the result of using BT information AND the 3 auxiliary estimates ?  If you did, then I misunderstood, but it would also mean you did not explain well enough a counter-intuitive result (which would be that adding auxiliary information actually hurts the performance of the VIMPNN).\n\n8. Please include a couple of explicit numbers of your training/test/validation sets sizes.\n\n9. section 5.2 could be made more concise. In particular, there is no need in repeating what can be seen directly in the figures (stating numbers). It is useful to comment on the meaning of the results however (as you currently do).\n\n10. section 5.4  is a good idea, promising, however it does not really conclude into a very strong statement, and takes essentially 1 full page, which could be used to better clarify the architecture and/or the training procedure (or to reduce towards the ideal page length).\n\n\n\n", "belong_id": "HJl8SgHtwr"}, {"uid": "rkeUP-b8oB", "paper_title": "VIMPNN: A physics informed neural network for estimating potential energies of out-of-equilibrium systems", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies approximation of the potential energy of molecules by a message passing architecture. The work builds upon [Gilmer et al., 2017] and the contributions are two-fold:\n1) The creation of new datasets to learn and test such architectures on and the augmentation of an existing dataset in order to account for energies at non-equilibrium states.\n2) A proposed modification to the MPNN architecture proposed in [Gilmer et al., 2017], in order to account for physical properties in the message-passing procedure.\nThe performances of the architectures are studied with numerous numerical experiments.\nThe paper is overall well-written and clear.\n\nThe new dataset utility is sound and well-motivated. Unfortunately I can not further motivate upon this, as I am not familiar with this area.\n\nFrom the point of view of the proposed architecture, the work is quite incremental. The bond type information, previously included as feature, is now transferred to an architectural modification. On the other hand, many different modifications (although no substantially different from each other) are proposed and tested (although no results about the different modifications are reported - it could be nice to have them in an appendix). This motivates the weakly accepted.\n\nThe authors also considered the idea of adding additional learning modules (and a related loss) to help the model learn more 'physics interpretable' hidden states. While it does not seem to give notable gains here, it is an interesting idea and I believe deserves further experimentations in the future.\n\nThe experiments are numerous and various, and they offer a very good overview on the goodness of the model (and its limitations). They first compare with the baseline on the augmented dataset, and they show notable gains on the MPNN baseline. The ability of the network to reproduce the energy curve at different interatomic distances is then studied on the different dataset and in different settings, showing gains over the baseline. The authors also report some negative results and experimental interpretation of the model hidden states, which are also an important contribution in my opinion. \n\nFurther comments:\n\n1. No details are given about the training of the models. I think a small paragraph (or larger and reported in the appendix) should be added.\n\n2. Even if it builds upon previous work, the (VI)MPNN model may be further explained. For example, what type of functions are M_t and R? The explanation on the considered modifications of MPNN may be clearer (maybe with the introduction of a more mathematical notation). \n\n3. How long is the message diffusion (T)? What is the effect of larger / smaller Ts?\n\n4. Whats the point of equations (5) (6) (7)? They are exactly the same and they do not add any information. It would be more useful to explain what type of function R is in my opinion.\n\n5. Table 1, Auxiliary estimates: Are these the results obtained by the model a.ii trained to jointly learn the energy and the properties i) ii) iii) ? In what sense they improve on the baseline? This part was not clear to me.\n\n6. Section 5.2: [...] we combine our proposed physics integration strategies, namely bond type\nspecialised\nnode updates (case a.ii) of Section 4.2) and auxiliary estimations of physical properties,\ninto the VIMPNN model [...]. I do not understand this sentence. Isnt in fact the VIMPNN architecture the same as MPNN with the modification a.ii? In what sense do you integrate a.ii in it?\n\n7. If I understood correctly, the main final objective is to be able to characterize the minima of the energy. In this case, could you asses the performances of the two methods (MPNN) and (VIMPNN) by measuring some kind of distance from the approximated minima from the actual one?\n\nTypos:\n\nSection 5.3 first line: We investigate the VIMPNNs the ability ... -> We investigate the VIMPNNs ability\n\nSection 5.2 Fourth sentence: The first seeks to demonstrates a the models ... -> The first seeks to demonstrates the models...", "belong_id": "HJl8SgHtwr"}, {"uid": "HklhT8o3KH", "paper_title": "Deep Evidential Uncertainty", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a novel approach to estimate the confidence of predictions in a regression setting. The approach starts from the standard modelling assuming iid samples from a Gaussian distribution with unknown mean and variances and places evidential priors (relying on the Dempster-Shafer Theory of Evidence [1] /subjective logic [2]) on those quantities to model uncertainty in a deterministic fashion, i.e. without relying on sampling as most previous approaches. This opens the door to online applications with fully integrated uncertainty estimates. \nThis is a very relevant topic in deep learning, as deep learning methods are increasingly deployed in safety-critical domains, and I think that this works deserves its place at ICLR.\n\nPros:\n1.\tNovel approach to regression (a similar work has been published at NeurIPS last year for classification [3]), but the extension of the work to regression is important.\n2.\tThe experimental results show consistent improvement in performance over a wide base of benchmarks, scales to large vision problems and behaves robustly against adversarial examples.\n3.\tThe presentation of the paper is overall nice, and the Figures are very useful to the general comprehension of the article.\nCons:\n1.\tThe theory of evidence, which is not widely known in the ML community, is not clearly introduced. \nI think that the authors should consider adding a section similar to Section 3 of Sensoy et al. [3] should be considered. Currently, the only step explaining the evidential approach that I found was in section 3.1, in a very small paragraph (between the mean of [...] to \\lambda + 2\\alpha.). I believe that the article would greatly benefit from a more thorough introduction of concepts linked to the theory of evidence.\n2.\tThe authors briefly mention that KL is not well defined between some NIG distributions (p.5) and propose a custom evidence regularizer, but theres very little insight given on how this connects to/departs from the ELBO approach. \n\nOther comments/questions:\n1.\t(p.1)  Im not sure to fully understand whats meant by higher-order/lower-order distributions, could you clarify?\n2.\t(p.3) In section 3.1, the term in the total evidence \\phi_j is not defined.\n3.\t(p.3) Could you comment on the implications of assuming that the estimated distribution can be factorized? \n4.\t(p.4) Could you comment on the difference that there is between NLL_ML and NLL_SOS from a modelling perspective?\n5.\t(p.4) The ELBO loss (6) is unclearly defined, and not connected to the direct context. I would suggest moving this to the section 3.3, where the prior p(\\theta) used in eq. (6) is actually defined.\n6.\t(p.4) In equation (6), p_m(y|\\theta) isnt defined, and q(\\theta|y) is already parameterized on y if I understand that q(\\theta)=p(t\\heta|y1,...,yN). Making the conditioning explicit in equation (6) might make the connection to the ELBO clearer. \n7.\t(p.7) Im not sure to understand how the calibration of the predictive uncertainty can be tested by the ROC curves if both the uncertainty and estimates error are normalized. Could you also define more clearly what you mean by an error at a given pixel? \n8.\tSpelling & typos:\n-\t(p.4) There are several typos in equation (8), where tau should be replaced with 1/\\sigma^2. \n-\t(p.8) In the last sentence, there is ntwork instead of network.\n-\t(p.9) There is a typo in the name of Jsang in the references. \n-\t(p.10) In equation (13), due to the change of variable, there should be a \n-(1/\\tau^2) added;  \n-\t(p.10) In equation (14), the \\exp(-\\lambda*\\pi*(...)) should be replaced with \\exp(-\\lambda*\\tau*(...)). \n\n[1] Bahador Khaleghi, Alaa Khamis, Fakhreddine O Karray, and Saiedeh N Razavi. Multisensor data fusion: A review of the state-of-the-art. Information fusion, 14(1):2844, 2013. \n[2] Audun Jsang. Subjective Logic: A formalism for reasoning under uncertainty. Springer Publishing Company, Incorporated, 2018. \n[3] Sensoy, Murat, Lance Kaplan, and Melih Kandemir. 'Evidential deep learning to quantify classification uncertainty.' Advances in Neural Information Processing Systems. 2018.\n", "belong_id": "S1eSoeSYwr"}, {"uid": "ryxl-cgTYB", "paper_title": "Deep Evidential Uncertainty", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposed deep evidential regression, a method for training neural networks to not only estimate the output but also the associated evidence in support of that output. The main idea follows the evidential deep learning work proposed in (Sensoy et al., 2018) extending it from the classification regime to the regression regime, by placing evidential priors over the Gaussian likelihood function and performing the type-II maximum likelihood estimation similar to the empirical Bayes method [1,2]. The authors demonstrated that the both the epistemic and aleatoric uncertainties could be estimated in one forward pass under the proposed framework without resorting to multiple passes and showed favorable uncertainty comparing to existing methods. Robustness against out of distribution and adversarially perturbed data is illustrated as well.\n\nOn the technical side, the novelty is incremental. The extension from the classification regime to the regression regime, from the conjugate Dirichet prior to the conjugate Normal-Inverse-Gamma prior, is quite straightforward. Besides, the presentation of the paper could be largely improved. It is not easy to follow the derivation in Section 3. The discussion of concepts and problem definitions look fragmented and incoherent. Even though the presentation largely follows (Sensoy et al., 2018) and uses terms from theory of evidence, the derivation actually is more aligned with the prior network [3] under the Bayesian framework which is missing from the references. It is really confusing that the authors talked about the variational inference when conjugate prior is used, and it is unclear how the variational distributions are used in Section 3.2 or how the 'I don't know' loss term relates to the KL-divergence between the variational distribution and the prior in Section 3.3. This term was manually added as additional regularization to 'prefer the evidence to shrink to zero for a sample if it cannot be correctly classified' in (Sensoy et al., 2018), and a different regularization was used to encourage distributional uncertainty in [3]. I hope that the authors could spend more efforts clarifying their ideas, especially the derivations in Section 3.2 and 3.3.\n\nOn the other hand, there is no referring to the input x in the entire derivation and problem formulation in Section 3. It took me a while to realize that the formulation in (4) actually defines the generation for a particular input, not for all the inputs. That is, the model is trying to model heteroscedastic uncertainty, not the homoscedastic counterpart. It could be better to call out the dependence on the input explicitly. \n\nOn the quantitative side, the baseline models considered in Section 4 are mainly concerned with epistemic uncertainty estimation. So it would be good to explicitly discuss which uncertainty estimation was compared with. This work estimates both aleatoric and epistemic uncertainties, so a better comparison is to models that estimate both quantities (Kendall & Gal, 2017)[4] which has been shown to give better output estimation comparing to epistemic uncertainty estimation only (Kendall & Gal, 2017).\n\nOther comments:\n- What is the \\pi in equation (8)?\n- The 'I don't know' loss introduced in Section 3.3 used L-p norm. What is the originality of the L-p norm here? In practice, which p value should be used? In the experiments, which p value was used?\n- The RMSE results of the depth estimation presented in Table 2 are orders of magnitude smaller than those from existing work, for example Table 2(b) in (Kendall & Gal, 2017). Was a different RMSE computation used in this work?\n- From the caption in Table 2, it seems that only 5 samples were used in MC-dropout, which is considerably smaller than those used in existing work (Kendall & Gal, 2017).\n\n[1] D.J.C. MacKay. Hyperparameters: optimize, or integrate out? Maximum Entropy and Bayesian Methods, Springer 1996.\n[2] B. Efron. Two modeling strategies for empirical Bayes estimation. Statistical Science, 2014.\n[3] A. Malinin and M. Gales. Predictive uncertainty estimation via prior networks. NeurIPS 2018.\n[4] Y. Kwon, J.-H. Won, B.J. Kim, and M.C. Paik. Uncertainty quantification using Bayesian neural networks in classification: application to ischemic stroke lesion segmentation. MIDL 2018.", "belong_id": "S1eSoeSYwr"}, {"uid": "HJlBIefWcB", "paper_title": "Deep Evidential Uncertainty", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigates the aleatoric uncertainty and epistemic uncertainty in machine learning. The evaluation was performed on benchmark regression tasks. The comparison with other state-of-the-art methods was provided. The evaluation of the robustness against out of distribution and adversarially perturbed test data was performed.\n\nStrength:\n1. Experiments were complete. Analyses were provided with useful information.\n2. A model with smaller number of parameters was proposed.\n3. Computation efficiency was improved.\n\nWeakness:\n1. Total evidence and model evidence were defined. The derivation of these evidences should be clarified.\n2. Theoretical justification for related methods could be improved.", "belong_id": "S1eSoeSYwr"}, {"uid": "SygDufkFKB", "paper_title": "Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\n##### Rebuttal Response:\nThe other reviewers seem to have understood more than me. Their opinion and the rebuttal did not convince me to update my score. In my opinion the writing must be adapted to be interesting to the ICLR community and the bigger picture should be highlighted more, as the bigger picture is remains quite unclear at the current state.\n\n\n##### Review:\nSummary: \n[...]\n\n\nConclusion:\nI have read the paper multiple times and I still have a problem summarizing the paper with my own words. The contributions summarize the most fundamental works of RL but do not really relate these methods to the proposed approach. Therefore, I am still uncertain about the general motivation and intention of the work as well as the evaluation. Currently I vote for borderline reject as I am familiar with RL & PDE'S but do not understand the motivation and intention. I am leaning towards rejection as the paper is a resubmission from Neurips and has not been substantially improved. However, I am not certain about my evaluation. I am happy to adapt my vote based on the other reviewers and a clarified and better structured paper, which can be submitted during the rebuttal.", "belong_id": "rygBVTVFPB"}, {"uid": "r1lYBi9cKH", "paper_title": "Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the author maps the problem of time series PDE into a naive reinforcement learning problem. Under the MDP assumption, the author sets the initial state of the particles as the current state, the flux at all spaces as the possible actions, and map the state-action pair deterministically to the next state of the particle diffusion. The reward is defined as the two norms between the prediction and the Burgers equation. The naiveness comes from the fact that the typical reinforcement learning problem, the agent needs to decide how to choose an action. In this paper, it is formulated as an intrinsic proper that follows Burgers equation instead. \n\nWhile the motivation is interesting, the author argues this work is novel due to it does not fall under supervised learning, but rather reinforcement learning. This perspective is not completely correct. The correct category for this work would be more similar to imitation learning using WANOs algorithm as the expert label. This is a field of supervised reinforcement learning.\n\nThe authors work has brought the possibility of using neural network architecture in the field of particle diffusion. The benefit is the improved estimation of how particles diffuse in long-horizon conditions. The author has shown in their paper their simple fully connected network has already performed better prediction than the current state of the art non-neural network model: WENO.\n\nWhile the framing of the problem is perhaps novel in the space of PDE, algorithmically there needs to have a breakthrough or new invention. The lack of comparison with other neural-network-based models also hurts the credibility of the model. Therefore, I reject this paper under the ICLR conference. I would suggest that this paper would be better suited as a paper submission under the perspective science field conference instead.\n\nSome suggestions to further improve this paper: The author could add CNN and RNN structure to the prediction model. These structures would further expand other possibilities in the solution space. CNN would help turn the limited 1D problem to a higher-dimensional, a more real-world like problem space. RNN is known for its ability to model long horizon problems, perhaps even better breakthrough would happen with these architectures.\n\nAs a whole, the paper is written very well such that even nonexpert can grab onto the logic flow of this paper. The weaknesses of the paper are the lack of diversity in comparison with other models and the paper needs some level of novel breakthrough in an algorithmic sense.\n", "belong_id": "rygBVTVFPB"}, {"uid": "rJekPAOaFr", "paper_title": "Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use reinforcement learning for constructing discretziation stencils of numerical schemes. More specifically, the method focuses on the widely used WENO schemes, which are an established class of finite difference schemes. Within this context, the method aims for training models to infer the weighting for a specific stencil with eight flux terms.\n\nFor RL this task requires a continuous action space, and the DDPG algorithm is used for training the policy. The network itself is an MLP with 6 layers, and ca. 20000 weights in total. This is a significant number, given the focus on 1D problems.\n\nThe tests are quite thorough and interesting, while at the same time being limited in scope. The paper targets 1D cases, which make the problem very low-dimensional. Despite the simplicity, only a single data set (Burgers) is used, and a single modified target function with a u^4 term. Targeting 1D casesl, I would have expected a broader range of tests and model equations.\n\nDespite the limited scope of the models, table 1 and 2 assess a nice range of different timestep and discretization parameters. I found it very interesting to see that the method consistently outperforms the regular WENO scheme. The gains are relatively small, with 4-5%, but WENO already represents a quite accurate scheme, so it's surely not easy to outperform it.\n\nWhile reading the paper, I was wondering about the bigger picture, i.e. using RL in the context of discretization stencils. We have model equations, and discretized versions of all operators involved in training. Why employ a 'brute force' approach like RL here? Wouldn't it be better in terms of efficiency and potentially also accuracy to train the stencils in a supervised manner, e.g., with a more accurate discretization as reference? One could argue that it would be expensive to pre-compute such data, but I think RL scales even worse to higher dimensional problems.\n\nWhat's also missing in the current version is a more thorough discussion of inference and training performance. I guess that despite the small model problems, the training takes a substantial amount of time. And due to the large size of the trained model, which has to be evaluated for every single node in the 1D mesh, it's probably also quite slow. I think this is worth a discussion in the text. One could even estimate the number of operations necessary to evaluate the model, and run a higher-order WENO scheme for a 'fair' comparison.\n\nMinor, but in equation (1), I guess the t subscript should indicate a material derivative, and just just a time derivative, right? This could be clarified in the text (or written out).\n\nI am somewhat on the edge with this paper - the 1D case for the two equations is carefully evaluated in the submission, and it's great to see the trained model can improve the accuracy across a fairly wide range of settings. As such, it's definitely a good and interesting first step. On the other hand, there are a range of open questions, as outlined above, and it's not clear whether the approach could be easily translated to higher dimensions. I hope the authors can clarify some of these points in the rebuttal, right now I'm leaning towards the positive side.\n", "belong_id": "rygBVTVFPB"}, {"uid": "H1lAbHqTYH", "paper_title": "Symplectic Recurrent Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces SRNN to model the hamiltonian of a dynamical system. Authors break down the design choices made in the algorithm and validate each one through experiments. \n\nI like the motivation of the paper of solving general physics problems using neural networks. The paper is well-written and the ideas are communicated properly.\n\nHowever, I have some concerns regarding the experimental results:\n\nIn Figure 1, even though the fifth mass seems to follow the exact pattern of observations for single-step L-L H-NET, the L2 error is increasing after each time-step suggesting that the fifth mass might not be the best one to consider for comparison between different algorithms here. Would be nice if a comparison between all trajectories was presented and discussed (perhaps in the Appendix)\n\nThe idea tested in section 4.2 seems not novel since previous works have already used recurrence to mitigate problems faced using one-step training. (https://arxiv.org/abs/1902.09689) \n\nI like the optimization method proposed in Section 4.3 and the results in Table 1 and 2 seems to justify its effect. Does this approach also handle degenerate cases in which different initial states can lead to the same trajectories after some amount of time? To me, it seems like a boundary on the noise variance should be assumed. Otherwise, no amount of optimization would be actually able to retrieve the correct initial state. Is this true or am I missing something?\n\n\n\n-minor comments:\n\n1. The conclusion section is missing from the paper. It would be nice to recap your findings there.\n2.  In section 2, the 'universality property' of Niu's recurrent model should be explained or referenced. \n3. In Figure 1, the second 'Left:' should be changed to 'Right:'.\n4. In Table 2, the Error std. for O-NET (E-L) should be bold not H-NET (L-L) unless it was intended to highlight the values for the model with the lowest mean error. (please clarify this in the paper.)\n5. Section 6: 'We focus on this section' should be changed to 'We focus in this section'.\n\n \nOverall I think the work is interesting but it lacks some justifications regarding the claims made (as mentioned above), and although it is generalizable to other tasks and systems, it does not have sufficient novelty in its algorithm and approach.\n\nAs of now, I am recommending a rejection, but I am willing to reconsider my score should the authors address the above concerns.\n\n\n----------------------------------------------------------------------------------------------------------------------------\nUpdates:\n\nThank you for your response and addressing my concerns in the revised version of the paper. I also see new updates to the text which has improved the readability considerably. Thank you for your work.\nI have updated my score accordingly. \n\n\n", "belong_id": "BkgYPREtPr"}, {"uid": "BJlTuVBRFr", "paper_title": "Symplectic Recurrent Neural Networks", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to represent a Hamiltonian model of a physical system by a neural network. The parameters are then adjusted, so that the observations are considered maximally likely under a probabilistic model. The novelty is to consider a symplectic Leapfrog integration scheme for the Hamiltonian system, which is known to conserve important quantities such as volume in the state space. The proposed approach is shown to outperform the recent work 'Hamiltonian Neural Networks' by a large margin on mass-spring chain dynamics and three body systems. The approach can even handle stiff dynamical systems such as bouncing billiards. \n\nOverall, the work is solid contribution and a reasonable improvement over the recent work on HNNs is demonstrated. Therefore I recommend acceptance of the paper. However, I have some fundamental doubts on the motivation on this line of works. This might be, because I'm not too familiar with the subject, and I'm willing to increase my score if the doubts are cleared.\n\nIn the shown examples, to best of my knowledge, the 'exact' Hamiltonians describing the physics of the system are well-known. Therefore, I'm unsure what is the advantage of trying to learn physical laws, that are already well understood. The paper argues that the learned Hamiltonian will correct for errors in the discretization, but one could instead use a better integration scheme or a finer time-discretization, based on classical theory which has been developed over the last 50 years which comes with strong convergence guarantees and error bounds. I would have liked to see a stronger motivation, why it is interesting to learn an Hamiltonian of a system, where the exact Hamiltonian is already known. It would also be enlightening to see some plots, which illustrate how 'far' the learned Hamiltonian is from the analytical one.\n\nOf course, one might argue that the ultimate goal is to have a learning based approach discover physical laws so far unknown to humans,  just from observations. But it is unclear why the inductive bias that the observations are generated by a Hamiltonian might be reasonable. It could very well be, that the law cannot be described by a Hamiltonian system. \n\nFrom a high-level point of view, one might even argue that it is not too surprising that one can fit a parametrized Hamiltonian to observations generated by a Hamiltonian system better than a general purpose function approximator without such an inductive bias or better than a system based on a naive/unsuitable non-conservative integrator.\n\nAs a remark, often the exact Hamiltonian is known to be (strictly) convex. I'm wondering whether convex function approximators such as convex neural networks could provide an even stronger inductive bias. But it might be that a general purpose RNN can account better for the discretization errors. ", "belong_id": "BkgYPREtPr"}, {"uid": "Hyg1X9VTqS", "paper_title": "Symplectic Recurrent Neural Networks", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe paper improves upon Hamiltonian Neural Networks to model physical systems from observed trajectories. Specifically, the authors propose to use (i) better integrator, (ii) multi-step learning, and (iii) initial state optimization. Authors experimentally show that all these improvements are beneficial in modelling complex and noisy Hamiltonian systems.\n\n\nMy Comments:\n\nI come from RNNs background and I am not an expert on physical systems. But the paper is extremely well written that I can easily follow. Experiments are convincing.\n\nI do not have any issues with the claims.\n\nAre the authors willing to release their code to reproduce the results?\n\n========================\n\nPost rebuttal: I stand by my decision.\n", "belong_id": "BkgYPREtPr"}, {"uid": "rJlMUMIoFH", "paper_title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present ALBERT, a modification of the BERT architecture with substantially fewer parameters. They show that despite being much smaller, the performance is very strong and achieves state of the art on a variety of different tasks. There are several ideas proposed here: embedding factorization, sharing layers, and sentence ordering as a training objective. \n\n1. The point that naively increasing the size of the BERT architecture does not work is a good one, but the authors don't acknowledge that this is tied up in the effect of regularization. Cross layer parameter sharing has a regularization effect that simply scaling up BERT large to x-large or such sizes does not have. This is also an issue with the authors making the statement that they are the first to show that dropout is harmful for Transformers. This is a large generalization that seems to be a special case of not only the regularized architecture they propose but also the large quantity of data that the model still underfits to.\n\n2. The authors propose embedding factorization to reduce the number of parameters in the embedding dimension. This is very intuitive, but the authors do not cite or compare to related approaches. I understand these models are computationally intensive and thus do not expect large quantities of detailed ablations. However, this kind of dimensionality reduction has been explored with other techniques, for example for knowledge distillation, quantization, or even adaptive input/softmax (and with subword as well, not just whole word modeling). These techniques have also been applied to machine translation models, which do not use them to learn rare words. I believe a better discussion of these methods should be added to the paper, as this is not a novel proposition.\n\n3. A large takeaway I have from this paper is that parameter size is not a good metric. While ALBERT is substantially smaller, the authors do not make it clear that this model is very slow at inference time due to the large size. This raises several questions: is it better to have models that are deeper or more wide? Can the authors actually report the latency in a comparative table next to BERT? Can the authors provide a sense of how large this model is in MB - e.g. presumably a goal of less parameters would be to have a model with less memory, but then the decision between memory and latency that different models make should be made more clear.\n\n4. Section 4.8 is not clear. Exactly how much data, in terms of GB of uncompressed text, is used here? Is it the data of XLNet and RoBERTa, so larger than both of those settings individually? Further, the authors train for 1 million steps. This is larger than both XLNet and RoBERTa, is that correct? Or there is some detail about the size of the batch that actually makes it comparable? The many small tables where the changes are not clearly delineated makes it difficult to compare results. \n", "belong_id": "H1eA7AEtvS"}, {"uid": "SygC0QIhFB", "paper_title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper investigates improving upon BERT by reducing complexity in terms of free parameters and memory footprint as well as computation steps. They propose 2 strategies for doing this: 1) Splitting the embedding matrix into two smaller matrices (going from V x A to V x B + B x A where B <<<< A); 2) layer-wise parameter sharing. They also utilize sentence order prediction to help with training. These coupled with a bunch of other choices such as using the lamb optimizer, certain hyperparameters etc help show dramatic empirical gains across the board on a wide variety of NLP/NLU tasks.\n\nPositives: This paper has a dramatic, seemingly statistically significant reduction in error across a wide-variety of tasks. It provides a thorough experimental plan and approaches the few addendums to training (splitting the embedding matrix, the layer-wise parameter sharing, and the sentence order prediction).\n\nConcerns & Questions: There's a lot of experimentation here and a lot of seemingly deliberate choices after seeing empirical results during the research phase. How crucial are the choices of optimizer and other specific hyperparameters? Were there ones you observed that were more brittle than others? Any specific 'reasonable' configurations/settings that caused degenerate solutions?", "belong_id": "H1eA7AEtvS"}, {"uid": "Hkl2NgFecr", "paper_title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a new pre-trained BERT-like model called ALBERT. The contributions are mainly 3-fold: factorized embedding parameterization, cross-layer parameter sharing, and intern-sentence coherence loss. The first two address the issue of model size and memory consumption in BERT; the third corresponds to a new auxiliary task in pre-train, sentence-order prediction (SOP), replacing the next sentence prediction (NSP) task in BERT. These modifications lead to a much leaner model and improved performance. As a result, ALBERT pushes the state of the art on GLUE, RACE, and SQuAD while having fewer parameters than BERT-large. \n\nThis is a well-written paper which is easy to follow even for readers without deep background knowledge. The proposed method is meaningful and effective. Its empirical results are impressive. \n\nOther comments:\n\n- Section 4.9. Why use the all-share condition for state-of-the-art ALBERT results (as indicated in Table 2)? Judging from Table 4 and 5, shouldn't the non-shared condition give better results? The number of parameters would be larger, of course. \n\n- I like the justification/motivation given for replacing NSP with SOP. I wonder if the authors have tried other objectives (but didn't work out). Such negative results are valuable to practitioners.\n\n- Typo in Sec. 4.1: x1,1, x1,2 should be x2,1, x2,2. \n", "belong_id": "H1eA7AEtvS"}, {"uid": "H1xjTu5iYH", "paper_title": "Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\n\nThis paper proposes using a black-box estimation method from Liu and Lee 2017 to estimate the propensity score for off-policy reinforcement learning. They suggest that, even when the full proposal distribution is available (as is the case in much of the off-policy RL literature), that estimating the propensity score can allow for a much lower variance off-policy correction term. This reduced variance allows the proposed method to 'break the curse of horizon' as termed by Liu 2018.\n\nReview\n\nThe proposed fixed point formulation for learning a parameterized off-policy state distribution allows for lower variance off-policy corrections decreasing the impact of the curse of horizon. This approach appears both theoretically sound and empirically well-supported.\n\nI am concerned with the consistency argument made in section 4.3. It appears from equation (10) and the final statement of theorem 4.1 that it is necessary to have two independent samples of x' given a single state x. In the general case, without access to the environment model, it is not possible to obtain two samples of x'. If the environment is continuing, then the probability of returning to state x to obtain a second sample is 0. Am I misunderstanding the requirements specified by the objective function in equation (10)? An additional concern with the consistency argument is that it appears to assume that the approximator for d_w can achieve 0 error according to the MMD. If d_pi is not representable by the approximator, which could reasonably be the case in more challenging domains, it is unclear from this analysis or empirical results what the behavior of the system will be. It is difficult to assess how large of an assumption this is for consistency claim; for difficult problems will d_pi never be representable, or is this a fairly low concern?\n\nThe empirical results look to have high enough variance in the final outcomes that it is difficult to consistently assess the performance of each algorithm (looking specifically at figure 3). However, the primary competitor algorithm is IPS which the proposed algorithm handily beats in three of the four problems. In the fourth problem, it is unclear that the competitor algorithm is winning, and could in fact be better only due to chance given the size of the respective error bars. It is worth noting that, because the parameter settings were tuned only for 50 trajectories, it is important to primarily assess performance based only on that point. It is likely that, given more trajectories to learn from, each algorithm would have chosen a smaller stepsize and effectively performed similarly.\n\nAdditional comments (did not affect score)\n\nI would be interested in seeing the scalability of this approach empirically. Given the additional parameterized function to learn, I am unsure if this method would scale reasonably to much larger problems. However, I recognize that the scalability question is largely outside the scope of this paper.", "belong_id": "S1ltg1rFDS"}, {"uid": "rklAXTxaKr", "paper_title": "Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary : \n\nThis paper proposes an approach for long horizon off-policy evaluation, for the case where the behaviour policy may not be known. The main idea of the paper is to propose an estimator for off-policy evaluation, that computes the ratio of the stationary state distributions (similar to recent previous work Liu et al.,) but when the behaviour policy may be unknown. This is an important contribution since in most practical off-policy settings, the behaviour policy used to collect the data may not necessarily be known. \n\n\nComments and Questions : \n\n\t- This is an important and novel contribution where the authors propose an approach for OPE that comes down to solving a fixed point operator. This is simlar to solving for fixed point Bellman operators, but where their problem can be formulated as a backward recursive problem. \n\t- The operator defined as in equation 3 is in terms of the backward view of time - this allows the operator to capture the visitation from the previous s,a to the current s. This is the backward flow operator with which a fixed point equation can be described. Although the authors note that similar operators have appeared in the literature before - their main contribution is in terms of using such operators for the OPE problem, which seems novel to me and is an interesting approach with potential benefits as demonstrated in this paper. \n\t- The core idea comes from equation 9 which tries to minimize the discrepancy between the empirical distribution and the stationary distribution. This can be formulated as an optimization problem, and the paper uses blackbox estimators, as described in section 4.2 for solving this problem.\n\t- The next interesting part of the paper comes from solving the optimization problem in equation 9 with Maximum Mean Discrepancy (MMD) - this is a popular approach that has recently become well-known, and the authors make use of it minimize the differences between the empirical and the stationary state distribution. \n\t- Section 4.2 appears a bit confusiing to me with some details missing - it would perhaps be useful if the authors could include more details for 4.2, especially with some explanations of how they arrived at the expression in equation 10. This would also make the paper more self-contained, for readers in the RL community perhaps not so well-read with literature on MMD.  Appendix C contains the detailed derivation, but more intuitive explanations might be useful for the context of the paper. \n\t- The proposed black box estimator seems quite useful as demonstrated in figures 2 and 3. Although the authors evaluate their approach of few simple domains - it would have been useful if there were more extensive experiments performed for OPE problems. This would be useful since from fig 2, it appears that the proposed method only outperforms in 3 out of 4 evaluated problems. \n\t- For experiments, it would also be useful to demonstrate the significance of not knowing the behaviour policy and what are the usefulness of it. The paper is motived in terms of unknown behaviour policies that generated the data - so few experiments that explicitly shows the benefit of it would perhaps strengthen the paper more. \n\t- I am curious to know more about the bias-variance trade-off of the proposed OPE estimator as well. Ie, does the proposed method introduce any bias, or has significance in terms of lower variance for the long horizon problem? Experimentally, would it be possible to demosntrate whether the approach has lower variance compared to existing baselines?\n\nScore : \n\n- Overall, I think the paper has useful contributions. It is a well written paper, but some additonal details in section 4.2 might be useful, especially on the appriach with MMD. Experimentally, I think there are some experiments missing and doing those can significantly strengthen the paper as well. The proposed method seems simple and elegant, and I would recommend for a weak acceptance of the paper.  \n", "belong_id": "S1ltg1rFDS"}, {"uid": "rkxhdccTFB", "paper_title": "Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new algorithm to the off-policy evaluation problem in reinforcement learning, based on stationary state visitation. The proposed algorithm does not need to know the behavior policy probabilities, which provide a broader application scenario compared with previous work. It is based on directly solving the fixed point problem of the resulting target stationary distribution, instead of solving a fixed point problem about the stationary ratios.\n\nIn general, this paper is looking at an interesting problem with a lot of increasing focus these days. I think this paper contributes great idea and good theoretical analysis. I like the idea of matching the resulting target distribution instead of minimizing a loss over the ratio. However, several unclear places in the current version hurt the presentation of results. I would like to see them get improved and will increase my score accordingly if so.\n\nDetailed comments:\n\n1) The algorithm part could be presented more clearly. So far I did not see where the empirical operator \\hat{B} is formally defined. The word *empirical* is also confusing to me in 'B is approximated by empirical data' because B is not an expectation, but an *integral* which has no *empirical* opposite of it. For the equations on top of page 5, shouldn't they be k[, ] about empirical operator instead of the expected operator since the RHS is already in sample-based form?\n\n2) Related with the last one, B has an integral. To approximate the integral, we only have one sample from the transition probability actually, and the sample state is not uniformly distributed. It needs some explanation of why that would not cause a problem to approximate the integral.\n\n3) The current loss function is invariant to the scale of w at all. Since the w is normalized, this is not a problem for the resulting estimator, ideally. However, that can be a numerical issue for float numbers. It's possible that the output from function approximator w goes to 0 or \\infty. Both cases can lead to NaN of the output/function approximator update eventually. I'd like to hear if the author has met this problem in the experiment or not, and how can that be fixed. \n\n4) I have to point out, as just a slight con of this paper, the technique used in this paper is not that much different from Liu et al 2018. Since it minimizes a loss function which is a supremum over an RKHS, and the resulting empirical loss also has a similar form. It's nice to see the author provide some details of making it work with mini-batch. These details are important for function approximator as NN.\n\nMinor point:\n - On page 12 the equations after 'We have by the definition of D_k', I did not follow the second step of the equations.\n\nSuggestion:\nThis paper study the similar settings (behavior-agnostic OPE), using similar method (on the stationary distribution) came out several months ago: https://arxiv.org/pdf/1906.04733.pdf. I knew it's unfair to ask the author to compare it with a very recent prior/parallel work. However since they are in such a similar case, and they have code available, is it possible to directly compare with the result from their code? https://github.com/google-research/google-research/tree/master/dual_dice\n\n\n======= After rebuttal =======\nThe author's feedback clarified some of my concerns in the initial review. After reading the author's feedback and other reviews, I think this paper has enough show contribution to the related work. Some of my previous concerns (point 2 and 3) seems true for many related works in this area in general. I partly agree that it is not very fair to ask this paper to fix them. The updated version also presents the theory section in a more clear way. So I'd like to raise my score. I've no problem with acceptance, but I won't fully heart argue for acceptance.\n\n\n", "belong_id": "S1ltg1rFDS"}, {"uid": "rylzNrlaYB", "paper_title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a new metric to evaluate both the amount of pruning and quantization. This metric is agnostic to the hardware architecture and is simply obtained by computing the Frobenius norm of some point-wise transformation of the quantized weights. They first show empirically that this Evaluation metric is correlated with the validation accuracy. Then use this metric to provide some general rules for pruning/quantizing to preserve the highest validation accuracy. Finally, they derive a strategy to perform pruning by monitory the signal to noise ratio during training and show experimentally that such method performs better than competing ones.\n\nPros: - Extensive experiments were performed to test the methods, and the results seem promising.\n\nCons: - The paper is not very clear, and the structure is somehow confusing.\n- It is not easy at first to understand the experimental setup and requires to make a lot of guesses in my opinion. \n- The paper didn't motivate properly the use of a hardware-agnostic metric in the context of the quantization and pruning. Isn't the ultimate goal of pruning/quantization is to optimize the run time/energy consumption of the specific device with the least compromise on the accuracy?\n\nI feel that the paper currently jumps between very different ideas: \n\t- Evolution of the proposed metric during training: 2.3 and 2.5. While the 2.3, the take-home message is relatively clear:  the ESN is correlated with the validation accuracy, I don't fully get the point of section 2.5: It suggests that the optimizer does some sort of pruning just by choosing a higher learning rate.   \n\t- Finding an optimal strategy for pruning/quantizing a network: 2.4 and 2.6. Those two sections are relatively clear, although I have some questions about the experiments.\n\t- Developing a new strategy based on the proposed metric to quantize and prune a network in a Pareto optimal sense: This is briefly and not very well explained in section 2.7, which sends back to 2.3, but it is hard to understand how it is exactly done. It seems that section 3 provides some empirical evidence supporting this strategy, but the description of the method is hidden in the experimental details.\n\n\nSome questions: \n- In figure 3, the blue dots represent validation vs ESN at each training iteration? What about the red plot, is it obtained by quantization of the parameters at different stages of training, or is it using the final parameters? Which equation was used to compute the red curve (2) or (4)?  How much quantization was performed? If the quantization was chosen to match the level of noise then it seems natural to expect such behavior in figure 3.  \n\n- In figure 4, how much pruning was performed for each network and was it the same quantization? In other words how each point in the plot was obtained?  The authors come to the conclusion that one should 'prune until the limit of the desired accuracy and then quantize', but it is hard for me to reach the same conclusions as I don't see the separate effect of pruning and quantization in those figures. Or maybe pruning is implicitly done by choosing a small network? In this case, it makes more sense, but still, some clarifications are needed.\n\n- Which equation for the ESN was used to produce figure 5? Equation (2) or (4)? \n\n- What is the Pareto frontier? I think it is worth first introducing this concept and describing more precisely how those curves are obtained. For someone who is not very familiar with these concepts, which is my case, it makes the reading very hard. \n\n- How was the number of pruned filters computed in figure 5 (right)? I don't expect the solutions to be sparse during training, especially that no sparsity constraint was imposed, or was it?\n\t\n\n\n-------------------------------------------------------------------------------\nRevision:\n\nThank you for all the clarifications and the effort to make provide a clearer version of the paper. \n\nRegarding section 2.7: ESNa FOR QUANTIZATION: Would it make sense to include the paragraph 2.7 at the end of 2.3, since it related to it and doesn't seem to require any of the intermediate subsections.\n\nresponse to Comment 3: Unfortunately, I'm not convinced by the explanation about the effect of the lr on sparsity. The decay coefficient controls the saparsity indeed, but not the lr. That is because unlike the lr, the decay coefficient defines the cost functions to be optimized:  L+dc ||W||^2, while the lr corresponds simply to the discretization of some gradient flow.   For instance, in a deterministic and convex setting, the solution that is obtained would be the same, regardless of the chosen lr  ( provided the lr is smal enough so that the algorithm converges) see for instance [1].  In a non-convex and stochastic setting do the authors have a particular reference in mind? I'm not aware of such behavior. I would expect a similar sparsity if dc is kept fixed and only lr changed. Is it likely that with smaller lr, the algorithm just didn't have time to converge? This would explain why the obtained solutions were less sparse.\nresponse to answer 5:  it is indeed well known that L1 norm induces sparsity, however l2 doesn't, it just encourages the weights to be smaller. In the optimization litterature sparsity of x% means x% of the parameters  are exactly 0. This is achieved with l1 norm, however l2 norm would only enforce that the coefficients are small but not necessarily 0 (see [1])\n[1]:  ROBERT TIBSHIRANI, Regression Shrinkage and Selection via the Lasso.\n\nAlthough the paper improved in terms of clarifications and experimental details, I still think it will benefit from additional work on careful interpretation of the results.\n\n\n\n\n\n\n\n\n\n\n", "belong_id": "HkxAS6VFDB"}, {"uid": "rylETCDaFS", "paper_title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an effective signal norm metric that measures the cost of the neural networks under both compute(ESN_a) or memory(ESN_d) in an ideal hardware setting. The authors then show that the slimmer models with fewer parameters are better than fatter models.\n\nStrength\n\nFirst of all, I like the discussion about different hardware(CPU) setups and corresponding cost. I also like the usage of pareto frontier to compare experimental methods. The proposed metric was simple but the author justifies it with respect to possible hardware setups in an ideal setting.\n\nWeakness\n\nThe main drawback of the paper is the lack of novelty in proposed method. The metric itself appeared to be the main contribution, but as the author said, the metric was based on an ideal hardware setup that ignores the memory hierarchy and data transfer cost, which could be the bottleneck in reality.\n\nMore importantly, it is hard to use the empirical evaluation to justify the conclusion fatter models are worse. As we know that extremely low bit quantized models are very hard to train, and it is not surprising -- by setting the number of bits to extremely low bits, the models cannot recover in a few rounds of re-training. \n\nThis being said, I cannot think of a better alternative. While I do not think the experimental results offers new insights(other than it is hard to re-train lower bits models). How to quickly train low bits models remains an open question.\n\nFinally given that there are already quite a few methods that prunes model based on real hardware evaluations, it would be great to compare to these methods as well.\n\nBecause the above weakness of the paper, I think this paper should not be accepted to the program.\n\n", "belong_id": "HkxAS6VFDB"}, {"uid": "S1lnx65qqS", "paper_title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors propose a hardware-agnostic metric called effective signal norm (ESN) to measure the computational cost of convolutional neural networks. This metric aims to fairly measure the effects of pruning and quantization. Whats more, based on the metric, the authors demonstrate that models with fewer parameters achieve far better accuracy after quantization. A large number of experiments are carried out to prove the effects of the metric and related conclusions, however, several experiments and arguments are confusing. \n\nPlease see my detailed comments below.\n\nPositive Points:\n1. The authors propose a hardware-agnostic metric named effective signal norm (ESN), aiming to measure the computational cost of the pruned and quantized models.\n\n2. Based on the proposed metric, the authors conclude that a moderately quantized slim model with fewer weight parameters achieves better performance rather than an extremely quantized fat model with huge number of parameters.\n\n3. This paper presents many interesting assumptions and possibilities, which can be further researched and explored in the future.\n\n\nNegative Points:\n1. The authors argue that ESN is a hardware-agnostic metric. However, ESN is based on ideal hardware, where the energy consumption is linearly proportional to the number of non-zero weight parameters and monotonically depends on the bit-width of weight parameters. Therefore, ESN is not suitable for existing hardware.\n\n2. Both of ESN_a and ESN_d are based on the assumptions instead of the fact, which is not convincing. Whats more, the assumptions are hard to be proved.\n\n3. The experiments are not strong enough to support the author's conclusions. For example, the authors argue that models with fewer parameters achieve far better accuracy in low computational cost region after quantization. However, this conclusion is only based on the ESN metric. Since ESN is based on some assumptions, this conclusion is not convincing.\n\n4. Please give more details about the quantitative relation between the quantization noise and the perturbation of weight parameters during SGD training.\n\n5. In terms of Figure 2, the author's description is not objective. The similarity between the ESN_a curves and the validation accuracy curves is not very high. Besides, the authors mention that after steep rise at 80 epoch, both ESN_a and validation accuracy decrease gradually. However, the performance degradation in Figure 2 is not obvious. Therefore, the conclusions drew by the observation are also unreliable.\n\n6. The ESN can not make an accurate evaluation of a model. The trade-off between accuracy, speed, model size is often required in model compression. However, the ESN can not make the trade-off between them. Moreover, the ESN does not have a clear application scenario.\n\nMinor issues:\n1.\tThere are lots of spelling and grammar mistakes in the paper, such as an ideal hardware , a extremely quantized fat model and so on.\n", "belong_id": "HkxAS6VFDB"}, {"uid": "Bkgp_NaFYB", "paper_title": "Option Discovery using Deep Skill Chaining", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the problem of learning suitable action abstractions (i.e., options or skills) that can be composed hierarchically to solve control tasks. The starting point for the paper is the (classic) observation that one skill should end where another can start. The paper then proposes a recursive algorithm for learning skills that obey this property. After finding some number of trajectories that reach the goal, the last few states of these trajectories are taken to define the initiation set for the ultimate skill and the termination set for the penultimate skill. The procedure is repeated, yielding a sequence (a 'chain') of skills that extends from the initial state distribution to the terminal state. The fact that the number of skills is not defined apriori seems to be a strength, and the extension to trees of skills is neat.\n\nThe paper compares the proposed algorithm against state-of-the-art hierarchical baselines on five continuous control tasks of varying complexity; the proposed method outperforms baselines on 2 - 4 of the 5 tasks (depending on the setting considered).\n\nPerhaps the biggest limitation of the paper is that it ignores the exploration problem. As noted in 'Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?', exploration is empirically one of the main benefits of HRL, yet the proposed method, by construction, cannot take advantage of this strength.\n\nI am leaning towards accepting this paper. While the idea is quite simple, it seems to work well empirically and, to the best of my knowledge, is novel. The simplicity should also make it a good launching point for future work in HRL. My main concern with the paper is that the poor performance of baselines on some of the tasks. For example, HIRO [Nachum 18] quickly solves AntMaze, while the baseline used in Fig 1 (HAC, which the paper claims is stronger than HIRO) gails to solve this task. I would be more confident about this paper if it added a comparison with HIRO.\n\nMinor comments:\n* 'fixed and costly hyperparameter' -- Why is the number of skills 'costly'?\n* 'Learning backward from the goal' -- Two other closely related papers are Policy Search by Dynamic Programming [Bagnell 04] and 'Forward-Backward Reinforcement Learning' [Edwards 18]\n* 'it may choose to ignore learned options' -- Why should we expect that the optimal policy will use any skills, if it can be represented entirely using primitive actions?\n* [Jinnai 2019] -- This citation is repeated in the references.\n\n----------------- UPDATE AFTER AUTHOR RESPONSE --------------------------\nThanks for the discussion. I think this paper makes a useful contribution to the field, and would be a good ICLR paper. My reason for not voting for 'accept' (rather than 'weak accept') is that the experiments are not the most complex, and are all navigation-esque (reacher is effectively a point mass). Could we use this method to solve tasks of significantly greater complexity than those tackled in prior work?", "belong_id": "B1gqipNYwH"}, {"uid": "SJxLXHtaFS", "paper_title": "Option Discovery using Deep Skill Chaining", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary of the paper\nThe authors tackle hierarchical RL and build upon the work of (Konidaris & Barto, 2009b) to derive a skill chaining algorithm with non-linear function approximations.\nDuring execution, skill chaining is an option-based algorithm, ie., a master policy (the option policy in the paper) selects one of O options and then delegates the control the corresponding sub-policy that executes primitive actions until their termination condition.\nThe learning consists of learning the master policy, here a DQN algorithm, as well as the sub-policies, each of which is a separate instance of DDPG (meaning, they do not share weights).\nThe gist of the learning algorithm, DSC, is to proceed backward. Initially the goal state is is the default termination condition.  As new options are added to the pool, their terminal condition simply correspond to the initial condition of their subsequent option, which consist in a learnt classifier that acts as indicator function during the learning phase.\n\nGeneral remarks\nThe paper is overall clear and well written. The background and related work section is informative and well structured. The experiments are sound (and all averaged over 20 different seeds-except for one). \n\nFrom a performance perspective, DSC is comparable to the state of the art most of the time, and even improving upon it on some experiments. However, I am not totally convinced that the learnt skills can be interpretable as clearly indicating different regions of the environment.\nLooking at Fig 3 and the attached video for instance, the skills oscillate and seem unstable overall.\n\nAlso, the choice of N (the number of the rollouts during the gestation period) and K (the segmentation threshold) seem to be crucial and very much application dependent. It would have been useful to plot the performance of DSC for different values of of these hyperparameters in order to show the potential flatness of the error surface.\n\nFinally, when the learning starts, the (only) option o_G must reach the goal state N times. Which means, the initial DDPG agent has to gather a certain amount of successful rollouts. \n\n- Isnt that a strong condition, given that initially, hierarchical RL is meant to overcome long sequences issues in flat RL? \n- Could you please elaborate on the training time of DSC?\n- Given that DDPG fails at the Point E-Maze environment, how could DSC still learn new options?\n\nSuggestions for improvement\n- Page 4, in learning the option-value function: it is mentioned that the master policy can choose primitive actions. This part only becomes clear in Page 5 when it is mentioned that it happens through the global option.\n- Page 4, in adding new options..., could you please clarify how the max of the returns is set as the initial Q-value. Since its not tabular, how can it be assigned? If I understood correctly, it is regressed towards this max return value, but I cant find it clearly expressed in the text. But then, how does it affect the rest of the Q values? \n\nMinor\n1. T represents the transition function page 1 and is then overloaded to represent time steps (page 3) without notice. \n2. page 5, paragraph 3.5: redundancy in is is", "belong_id": "B1gqipNYwH"}, {"uid": "Syx0cm0pKB", "paper_title": "Option Discovery using Deep Skill Chaining", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\nThe authors tackle the problem of skill discovery by skill chaining. In particular, the authors claim two key contributions over the state of the art in option discovery 1) learn initiation sets 2) do not need to specify the number of options and this is also learned. Skill discovery is formalized by skill chaining; wherein the skills are chained backward from a goal state, and in a way, such that termination of an option is the initiation of the option that precedes in its chain. \n\nInteresting and useful aspects of this work are goal-based learning of where options should initiate (although clarification on goals would be crucial), the discussion of the optimality of the discovered solutions, scales up various ideas already proposed in Konidaris & Barto (2009b). \n\nSkill discovery through skill chaining, in particular, has not been extensively explored in a deep RL setting and serves as a useful contribution.\n\nDetailed comments:\nThe authors introduce DSC with an intuition based on a goal. However, it is never mentioned where this goal comes from. In sec 3, the authors describe an algorithm based on the presumed goal. It is not clear to me: the goal is at one instance defined as \\beta_{o_i} := \\mathcal{I}_{o_i - 1}, and then in the next few lines, it is said goal state of the MDP or the initiation condition of a previously learned option. Is it correct to say this is an algorithm (sec 3) for goal-based option discovery using DSC, where the goal is specified in the MDP? \n\nBefore going into the details of the architecture, it would be useful for the reader to have a formalism or a clear algorithm where at least the definition of what a goal constitutes here is clearly stated. \n\nIntra-option policy: It is nice to see that the options internal policy is not driven by the task-specific reward and has its internal reward. In the sparse reward setting: how is the subgoal reward chosen? In dense reward: what kind of distance metric is used? Please provide details.\n\nPolicy over options: The foremost option constructions seems a bit weird: is there a single goal specified which helps determine the termination condition of the global option? What would happen if there are multiple goals in the MDP? \n\nInitiation set classifier: This is an interesting approach. Although, it raises a natural question: \t\nWe continue adding to our chain of options in this fashion until a learned initiation set classifier contains the start state of the MDP.  What happens if this never happens, or is this process is guaranteed to converge?\n\nExperiments: Initiation set visuals are nice and interpretable. Experiments are not very convincing: authors mostly demonstrate results on control tasks that are specific to navigation and could do a more rigorous analysis by considering different tasks such as visual domains, robot manipulation tasks.  In particular, authors should compare their method and discuss other skill chaining approaches such as [1,2,3].\n\n[1] Shoeleh, Farzaneh, and Masoud Asadpour. 'Graph-based skill acquisition and transfer learning for continuous reinforcement learning domains.' Pattern Recognition Letters 87 (2017): 104-116.\n[2] Metzen, Jan Hendrik, and Frank Kirchner. 'Incremental learning of skill collections based on intrinsic motivation.' Frontiers in neurorobotics 7 (2013): 11.\n[3] Konidaris, George, et al. 'Robot learning from demonstration by constructing skill trees.' The International Journal of Robotics Research 31.3 (2012): 360-375.\n\nMy primary concern is the amount of engineering that is needed to get this to work. There are multiple steps which do not seem to be sequentially dependent on the success of the previous steps (eg: global option construction needs a goal, the next options are only discovered once the global actions has been constructed, there is an initiation period and internal components of an option are all learned through multiple algorithms DQN, SMDP Q learning, DDPG). \n\nOverall:\t\t\n+Break the assumption that all options are everywhere.\n+Number of options per task is also learned and does not need to be specified a priori.\n+Chains skills in a smart way - which could be very useful for lifelong learning, but the approach, as it is, is limited by the goal of the MDP (whatever that means in this context, state or a Reward). \n+Options are driven by internal rewards\n-The heavy machinery used in the core algorithm seems to be inspired by Konandiais 2009b. It would be very useful to discuss, clarify and contrast what is novel and what is borrowed from Konandiais 2009b as is.", "belong_id": "B1gqipNYwH"}, {"uid": "BygZ_Hp9FS", "paper_title": "Neural Arithmetic Unit by reusing many small pre-trained networks", "experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use neural networks to evaluate the mathematical expressions by designing 8 small building blocks for 8 fundamental operations, e.g., addition, subtraction, etc. They then design multi-digit multiplication and division using these small blocks. \n\nThe motivation of this paper is not very clear to me, i.e., why do you want to mimic the arithmetic operations using the logic networks, what is the real use case here. In the introduction, the paper motivates by pointing out the limitation of neural networks, which is memorization based and they want to generalize by understanding the inherent rules. However, if you look at the way the fundamental building blocks are designed, and how the multiplication model works, the rules are injected based on human knowledge, e.g., the way signal digit multiplication extends to multi-digit multiplication, there is simply no understanding by the model itself. Besides, the whole process has no training, i.e., the weights of the small networks are fixed, and what is the trainable parts? \n\nThe whole paper has many spelling and grammar errors, which hinders the reading. And the writing needs to be significantly improved. ", "belong_id": "HyerxgHYvH"}, {"uid": "H1luNTN3tS", "paper_title": "Neural Arithmetic Unit by reusing many small pre-trained networks", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a method to design a NN based mathematical expression evaluation engine. I find that the paper could benefit a lot from some rewriting as it is not very clear and over claiming at points.\nThe introduction states that almost all ANNs lack generalization, this is in my opinion an overstatement. Domain shift and adaptation are techniques to cope with situations where the test data distribution is not coherent with the training data distribution. If this would be true in general we would have not seen such a resurgence and widespread use of ANN in the past years.\n\nThe paper lacks also proper citations to previous work and I find the background section and motivation rather weak. \n\nThe fundamental operations presented in section 3 do not involve any learning at all, contrary to the referenced work of Trask et al where parameters are actually learned (see relaxation of sign function with tanh etc.). I therefore find the use of ANN as basic constituent of the block to be wrong, each network has fixed hand-crafted weights. If I were to replace ANN with the ordinary corresponding function nothing would in the presented framework.\n\nMultiplication and division as explained in the algorithms do not require learning at all. I am afraid the ML contribution of this work is in my opinion almost non existent. I see every component as being scripted rather than learned from the data, which would be of course much more interesting.\n\nExperiments are not clear at all, setup and explanation of results are not sufficient and I find them not thoroughly executed. Table 1 mentioned classification but the task is never clearly explained. Experiment 2 compares to NALU but in the proposed work nothing is learned, unless I misunderstood the work entirely.\n\n", "belong_id": "HyerxgHYvH"}, {"uid": "H1gznbUMqr", "paper_title": "Neural Arithmetic Unit by reusing many small pre-trained networks", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Writing\nOverall, the readability of this paper is far from the acceptance criteria of ICLR. there are just way too many grammatical errors or typos throughout the entire paper that prevent me from understanding this paper, such as\nSec.1\nAlthough..., however...\nIn neural network -> neural networks\nThey lack understanding -> it lacks understanding\nNumerical and quantitative reasoning is their r fundamental capability -> are .. capabilities...\n...\nJust too many to print all of them here. Please proofread your paper before submission.\n\nThe introduction is poorly written that I cannot get a full picture of what goals this paper tries/has achieved after reading it.\n\nAlgorithm 1 and 2 seem to be very poorly formatted but only illustrate minimal useful information.\n\n\nMotivation\nI dont see clear usage nor convincing results based on the current shape of this paper -- what application could this work enable or what theoretical insights it reveals?\n\nMethod\nThis paper proposes to use neural nets to do arithmetic operations (though I dont see convincing motivations to do so). A new idea the paper proposes is to train a few networks to first learn/fit basic operations, and then use these trained NNs to assemble large NNs which are supposed to form more complex arithmetic operations. Unfortunately, the writing of this paper prevents me from fully understanding the technical details of this paper.\n\nResults\nThe results in this paper are currently minimal. Many details about the experiment setup or how different methods are compared are not clear.\n", "belong_id": "HyerxgHYvH"}, {"uid": "Byxm0iP5KH", "paper_title": "Reducing Computation in Recurrent Networks by Selectively Updating State Neurons", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper proposes selective activation RNN (SA-RNN), by using an update coordinator to determine which subset of the RNNs hidden state dimensions should be updated at a given timestep. The proposed loss term is then a sum of the original objective (e.g. classification) and a weighted sum of the probability that each dimension will be updated for each timestep. The method is evaluated on 3 time series datasets: Seizures, TwitterBuzz, Yahoo. \n\nDecision: Weak Reject. Although the authors tackle a challenging problem, their empirical results are lacking to provably demonstrate that their approach outperforms existing baselines.\n\nSupporting Arguments/Feedback: The authors compare SA-RNN to 5 baselines: random updates, clockwork RNN, phased LSTM, Skip RNN, and VC-GRU. Although I appreciated the authors comparison across the suite of methods with respect to various metrics (e.g. # FLOPS, proportion of neurons that werent updated, etc.), the experiments were conducted on datasets that were relatively simple. For example, in prior work, the empirical evaluations were on much larger-scale datasets such as Wikipedia [Shen et. al 2019], real clinical data sources [Liu et. al 2018], and Charades videos [Campos et. al 2018], among others. I would be very interested to see how this training procedure fairs when evaluated on much more complex tasks, and would make the results about computational speedups at train/test time much more convincing.\n\nQuestions:\n- Im curious if you tried different types of gradient estimators to get around the non-differentiability rather than the straight-through estimator. Also how was the slope-annealing conducted (e.g. annealing schedule)?\n", "belong_id": "SkeP3yBFDS"}, {"uid": "BJecY0BnFr", "paper_title": "Reducing Computation in Recurrent Networks by Selectively Updating State Neurons", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper attempts to reduce computation in recurrent neural networks. Instead of artificially determining the update pattern for updating the states, the authors propose SA-RNN to predict discrete update patterns automatically through optimization driven entirely by the input data. Experiments on publicly-available datasets show that the proposed method has competitive performance with even fewer updates.\n\nPros:\nOverall, I think the idea of this paper is clear and the whole paper is easy to follow. The experiments clearly show the advantage of the proposed method claimed by the authors.\n\nCons:\n1.\tSome expressions need to be improved. For example, in This way, representations can be learned while solving a sequential learning task while minimizing the number of updates, subsequently reducing compute time. two whiles are not elegant and there should be an In before this way. In We augment an RNN with an update coordinator that adaptively controls the coordinate directions in which to update the hidden state on the fly, the usage of in which to is not right. I suggest the authors to thoroughly proofread the whole paper and improve the presentation.\n2.\tSince this paper focuses on the efficiency of RNN, I suggest the authors could provide the time complexity comparisons. Merely the comparisons on skip of neurons cannot show the advantage on the efficiency.\n", "belong_id": "SkeP3yBFDS"}, {"uid": "S1xB6zspFr", "paper_title": "Reducing Computation in Recurrent Networks by Selectively Updating State Neurons", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "A main problem with RNN is to update all hidden dimensions in each time step. The authors proposed selective-activation RNN (SA-RNN), which modifies each state of RNN by adding an update coordinator which is modeled as a lightweight neural network. The coordinator, based on the incoming data, makes a discrete decision to update or not update each individual hidden dimension. A multi-objective optimization problem is defined to both solving a sequential learning task and minimizing the number of updates in each time step. The authors evaluated their networks on three public benchmark datasets and achieved good results compared to the state-of-the-art ones.\nThe papers is well-written. The idea proposed in this paper is interesting and it is presented very well. There is also an extensive evaluation.\n", "belong_id": "SkeP3yBFDS"}, {"uid": "r1gnXjEatr", "paper_title": "A NEW POINTWISE CONVOLUTION IN DEEP NEURAL NETWORKS THROUGH EXTREMELY FAST AND NON PARAMETRIC TRANSFORMS", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new pointwise convolution layer, which is non-parametric and can be efficient thanks to the fast conventional transforms. Specifically, it could use either DCT or DHWT to do the transforming job and explores the optimal block structure to use this new kind of PC layer. Extensive experimental studies are provided to verify the new PC layer and experimental results show that the new layer could reduce the parameters and FLOPs while not loosing accuracy.\n\nOverall, this paper provides a promising PC option for the CV community and the experiments seems solid. Although the novelty is limited, which is just to combines the DCT/DHWT with NN, the experiments are sufficient and I am glad to see that this simple idea works in practice. \n\nConcerns: This paper is based on the practical validation. Is there any theory to support that using DCT/DHWT could achieve better performance. In Image Processing, DCT/DHWT could be used to compress images or videos, but what is the benefit to use them in computer vision?\n\n---post comments after rebuttal---\nThanks for the response. \n\nHowever, the rebuttal does not address my concerns. The logic behind the authors response is that since there is much work that uses DCT / DHWT-like features in computer vision tasks, we can also use them to replace the conventional PC layers. Unfortunately, I do not think that is the theory to support the idea described in this paper. Actually, the success of CNN architecture has already confirmed the advantage of conventional PC layer, it seems it is a backward step for the community. Moreover, I agree with Reviewer#4 that ImageNet should be used as benchmark to so the advantage of the paper. \n\nHence, after reading the response and the reviews from other reviewers, I support to reject the work.", "belong_id": "H1l0O6EYDH"}, {"uid": "Bkx7z0H6KH", "paper_title": "A NEW POINTWISE CONVOLUTION IN DEEP NEURAL NETWORKS THROUGH EXTREMELY FAST AND NON PARAMETRIC TRANSFORMS", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: \n\nThis paper proposes using non-parametric filters like Discrete Cosine Transform (DCT) and Discrete Walsh-Hadamard Transform (DWHT) which have been widely used as feature extractors in vision and image processing before deep learning became prevalent as layers especially to replace pointwise convolution (PC) layers in deep network architectures like ShuffleNet-v2 and MobileNet-V1. \n\nThe motivation is that using such layers can capture cross-channel correlations without addition of extra parameters that need to be learnt and by replacing PC layers which tend to make up the bulk of the parameters in such settings large reduction in number of weights and flops can be achieved with little drop (or even increase) in accuracy.\n\nThey show experiments on cifar100 datasets.\n\nComments:\n\n- The paper is overall easy to read although the writing and presentation can use some work. \n\n- I really enjoyed reading the paper though because it seems in the deep learning era we have a tendency to re-learn what we already know. This paper shows that combining our knowledge of image processing and compressed sensing with good function approximation leads to better and more compact representation learning. I think these aspects are being generally overlooked currently but wont be surprised to see more of these papers. \n\n- While reading section 3.2 it occurred to me that it might be interesting to consider throwing these layers in to a neural architecture search (NAS) algorithm and let it figure out the right architecture. (Just a suggestion for future work not asking for this in the rebuttal.)\n\n- Overall I am positive about the paper and have no major asks but if time and resources permit perhaps trying out the same experiments on ImageNet to see if the trend holds.", "belong_id": "H1l0O6EYDH"}, {"uid": "r1e_MoDtqH", "paper_title": "A NEW POINTWISE CONVOLUTION IN DEEP NEURAL NETWORKS THROUGH EXTREMELY FAST AND NON PARAMETRIC TRANSFORMS", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a new pointwise convolution (PC) method which applies conventional transforms such as DWHT and DCT. The proposed method aims to reduce the computational complexity of CNNs without degrading the performance. Compared with the original PC layer, the DWHT/DCT-based methods do not require any learnable parameters and reduce the floating-point operations. The paper also empirically optimizes the networks by removing ReLU after the proposed PC layers and using conventional transforms for high-level features extraction. Experiments on CIFAR100 show that the DWHT-based model improves the accuracy and reduces parameters and FLOPs compared with MobileNet-V1.\n\nAlthough this paper is well organized and easy to follow, the novelty of the proposal seems limited and the performance improvement claimed by the author(s) is not very convincing due to the insufficiency of experiments. Firstly, the proposed method is just a manually designed and fixed 1*1 convolutional kernel, and its superiority over random initialization seems very limited as shown in Table 1. Also, the proposed method makes accuracy degrade when applied to low- and middle-level features. I wonder whether there is a more theoretical explanation for that. Moreover, the experiments are performed only on a small dataset CIFAR100. According to my own experience, the artificial convolutional kernels with some prior knowledge may work well on small datasets but tend to fail on larger ones. More experiments on larger-scale datasets like ImageNet are recommended to make results more convincing.\n\nTherefore, my decision leans to a rejection.\n\nSome questions:\n1. How is the performance when applying RCPC only to low-/middle-/high-level features? I suppose it should be proved that the proposed method is definitely better than random initialization.\n2. Why is only applying the proposed block to high-level layers working? How is the trade-off between parameters and accuracy different for each level of features?\n\nSpelling mistake:\nPage 6: in the second last paragraph, 'non-parameteric' should be 'non-parametric'.", "belong_id": "H1l0O6EYDH"}, {"uid": "Bkgb0FksYS", "paper_title": "Semi-Supervised Generative Modeling for Controllable Speech Synthesis", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overview:\n\nThis paper proposes to use semi-supervised learning to enforce interpretability on latent variables corresponding to properties like affect and speaking rate for text-to-speech synthesis. During training, only a few training items are annotated for these types of properties; for items where these labels are not given, the variables are marginalised out. TTS experiments are performed and the approach is evaluated objectively by training classifiers on top of the synthesised speech and subjectively in terms of mean opinion score.\n\nI should note that, although I am a speech researchers, I am not a TTS expert, and my review can be weighed accordingly.\n\nStrengths:\n\nThe proposed approach is interesting. I think it differs from standard semi-supervised training in that at test time we aren't explicitly interested in predicting labels from the semi-supervised labelled classes; rather, we feed in these labels as input to affect the generated model output. I agree that this is a principled way to impart interpretability on latent spaces which are obtained through unsupervised modelling aiming to disentangle properties like affect and speaking rate.\n\nWeaknesses:\n\nThis work misses some essential baselines, specifically a baseline that only makes use of the (small number of) labelled instances. In the experiments, the best performance is achieved when gamma is set very high, which (I think) correspond to the purely supervised case (I might be wrong). Nevertheless, I think a model that uses only the small amount of labelled data (i.e. without semi-supervised learning incorporating unlabelled data) should also be considered.\n\nAs a minor weakness, the evaluation seems lacking in that human evaluations are only performed on the audio quality, not any of the target properties that are being changed. For affect specifically, it would be helpful to know whether the changes can be perceived by humans. As a second minor weakness, some aspects of the paper's presentation can be improved (see below).\n\nOverall assessment:\n\nThe paper currently does not contain some very relevant baselines, and I therefore assign a 'weak reject'.\n\nQuestions, suggestions, typos, grammar and style:\n\n- p. 1: 'control high level attributes *of of* speech'\n- p. 2: It would be more helpful to state the absolute amount of labelled data (since 1% is somewhat meaningless).\n- p. 2: I am not a TTS expert, but I believe the last of your contributions have already been achieved in other work.\n- Figure 2: It would be helpful if these figures are vectorised.\n- p. 4: '*where* summation would again ...'\n- Figure 4: Is there a reason for the gamma=1000 experiment, which performs best in (a), not to be included in (b) to (d)?\n- Section 5: Table 1 is not references in the text.\n- Section 5.1: 'P(x|y,z_s,z_u)' -> 'p(x|y,z_s,z_u)'\n- In a number of places, I think the paper meant to cite [1] but instead cited the older Kingma & Welling (2013) paper; for instance before equation (6) (this additional loss did not appear in the original VAE paper).\n\nReferences:\n\n[1] https://arxiv.org/abs/1406.5298\n\nEdit: Based on the author's response, I am changing my rating from a 'weak reject' to a 'weak accept'.", "belong_id": "rJeqeCEtvH"}, {"uid": "BygnsoO3Kr", "paper_title": "Semi-Supervised Generative Modeling for Controllable Speech Synthesis", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use a semi-supervised VAE based text-to-speech (TTS) for expressive speech synthesis. The main contribution of this paper is that it can provide more explicit interpretation for the latent variable with the help of the supervised learning component. The formulations and implementations in the main body are quite high-level and we may not easily understand the technical/implementation details only with the main body but, in other words, the paper is well written to convey their main messages and of course some details are described in the appendix. The experiments show the effectiveness in terms of subjective (MOS) and objective (cepstral distance etc.) with a lot of audio examples on the demo page.\n\nMy concern for this paper is a lack of reproducibility. The paper uses the in-house data to perform their experiments and the code does not seem to be publically available. Also, the paper misses several detailed information (e.g., detailed configurations of the Wave RNN vocoder, what kind of neural network toolkits and libraries). The high computational cost ('distributed across 32 Google Cloud TPU chips') would also make the reproducibility difficult. I also would like to see whether this method can have some experimental comparisons with (Hsu et al., 2018) with their postprocessing to show the distinction in terms of the performance in addition to the functional difference. \n\nComments:\n- In general, the font size in the figures is too small\n- Figure 1: it's better to have an explanation of 'CBHG'. People outside the end-to-end TTS community cannot understand it.\n- Can you also control the noise level as shown in (Hsu et al., 2018) but more explicitly within this framework? Controlling the noise level is quite important for end-to-end TTS, and I think this method can fit this direction because we can easily obtain the noise attribute (supervision) by data simulation or annotate the noise. \n- Section 2, second paragraph y_{1...t} --> y_{1...k} (?)\n- equation (6), classification loss: I think this part requires more clarifications in this timing, e.g., by giving an example of classification tasks.\n- I think it's better to add what kind of (neural) vocoder is used in the main body (not in the appendix) to asses the sound quality for their experiments.\n\n", "belong_id": "rJeqeCEtvH"}, {"uid": "BygP7mHAtr", "paper_title": "Semi-Supervised Generative Modeling for Controllable Speech Synthesis", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose to do neural text to speech, conditioned on attributes such as valence and arousal and speech rate. A seq-to-seq network is trained using stochastic gradient variational Bayes.        \nThe idea is interesting and new.\n\nThe method section could be made clearer by giving first some intuition, explaining the formulas in the prose and introducing the terms used.\n\nRegarding the crowd sourced MOS: how were the ratings obtained? how many subjects were used for the rating? How were they selected? Was each rater presented with samples from each method? Or was each method assessed by different groups of raters?\n\nThe experimental setting is a little weak, relying mostly on the Mean Opinion Score. It would be useful to include more evalution, for instance:\n* run a speech recognition method on the generated speech and measure the error rate\n\n* examples could be included in the supplementary  (e.g. spectrograms)\n\n* For the evaluation of emotional speech to be meaningful, the proposed classifier should be tested, and more detailed given (hyper-parameters, training setting, validation/test splits?, etc). In particular, it would be useful to compare the proposed method for affect classification to an existing (state-of-the-art) methodology. A state-of-the-art method should be ideally be directly used to classify the generated sequences into emotional classes.\n\nThe authors collected data in studio conditions, as such it is hard to compare. Will the data be released? How much hdata was collected?\nIt seems that what the authors effectively do is condition on discrete emotion classes, not valence and arousal, which are continuous measures of affect.", "belong_id": "rJeqeCEtvH"}, {"uid": "rJe9E2ITYr", "paper_title": "Neural Machine Translation with Universal Visual Representation", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose to augment NMT with a grounded inventory of images.  The intuition is clear and the premise is very tempting.  The key architectural choice is to allow the transformer to use language embeddings to attend into a topic-image lookup table.  The proportion is learned to balance how much signal comes from each source.    Figure 4, attempts to investigate the importance of this sharing and its effects on performance.\n\nWhile reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.  For example, 'The old system of private arbitration courts is off the table' from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture.  It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets.  I trust that the authors did in fact achieve these results but I cannot figure out how or why.  This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.  \n\nIn contrast, it does make sense that Multi30K would benefit from this architecture.  As a minor note, were different feature extractors compared? The recent flurry of papers on multimodal transformers indicate that deeper resnet stacks correspond to improved downstream performance.  Is that also true in this domain?", "belong_id": "Byl8hhNYPS"}, {"uid": "B1gYdNvCFS", "paper_title": "Neural Machine Translation with Universal Visual Representation", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper provides an approach to use visual information to improve text only neural machine translation systems. The approach creates a 'topic word to images' map using an existing image aligned translation corpora. Given a source sentence, the model extracts relevant images, extracts their Resnet features and fuses them with the features generated from the word sequence. The decoder uses these fused representation to generate the target sentence. Overall, I like the approach, seems like it can be easily augmented to existing NMT systems. \n\nOne of the claims of the paper was to be able to use monolingual image aligned data. However image captioning datasets are not mentioned. It would make sense to use image captioning data to create the image lookup. Also, what will be the performance of a standard image captioning system on the task ? I believe it will not be great, but I think for completeness, you should add such a baseline.\n\nMinor comments: \n1. What is M in Algorithm 1 ? \n2. First paragraph in related work is very unrelated to the current subject, please remove.\n", "belong_id": "Byl8hhNYPS"}, {"uid": "ryxpUP6RKr", "paper_title": "Neural Machine Translation with Universal Visual Representation", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: This paper uses visual representation learned over monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs for multimodal NMT. Their approach enables visual information to be integrated into large-scale text-only NMT. Experiments on four widely used translation datasets show that the proposed approach achieves significant improvements over strong baselines.\n\nStrengths:\n- This paper is well motivated and well written. I especially like how they use external paired sentence-image data from Multi30k to learn weak pairs for sentences in machine translation.\n- Experimental results are convincing. I like how low-resource translation is included as a priority in their experiments.\n\nWeaknesses:\n- Do you have any explanations as to why the number of images, if too large, actually hurts translation performance? Is it because more images also leads to a higher chance of noisy images?\n- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.\n- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.\n- Why are there missing BLEU scores and the number of parameters in Table 1?\n\n### Post rebuttal ###\nThank you for your detailed answers to my questions.", "belong_id": "Byl8hhNYPS"}, {"uid": "SklQj-86tr", "paper_title": "Entropy Penalty: Towards Generalization Beyond the IID Assumption", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this work, the author(s) presented a regularization scheme that intends to suppress the identification of spurious features when learning deep representations. Their construction was inspired by the information bottleneck framework. By making Gaussian assumptions on the form of label conditioned feature distribution, the entropy penalty can be efficiently computed in the form of L2 loss, which is easy to implement. My major concerns for this submission are its clarity, novelty, and theoretical depth. The arguments provided are not very convincing and reported experimental results are based on toy-scale datasets. I recommend rejection for this submission, with more detailed comments attached below. \n\nStrength\n+ The author(s) are trying to resolve the issue of learning spurious discriminant features for predictive models, which is a trendy topic with a potential impact on the field. \n+ There are some interesting discussions in the related work section. \n\nWeakness\n- The presentation needs to be much improved. In its current form, the lack of clarity leads to serious confusion. Examples include but not limited to the following:\n\t- 'violates the IID assumption which is the foundation of existing generalization theory'. Not sure what this IID assumption means, please briefly/intuitively describe these classical generalization theories. \n\t- 'all the correlations btw inputs and targets.'\n\t- 'throws away maximum possible information about the input distribution'\n- The author(s) have made a strong statement, quote 'it is the second term that regularizes the model representations to become invariant to non-robust features'\n- Eqn (1) and Eqn (3) is equivalent, what's the point??? There is no novelty here. \n- Prop 1. Modeling the conditional entropy H(f_{\\theta}(X)|Y) nonparametrically is not any easier than modeling the marginal entropy H(f_{\\theta}(X)). The assumption of a parametric form of f_{\\theta}(X) given Y is very strong and needs to be justified (at least experimentally). Although the author(s) are honest about this limitation in the discussion. \n- The concept of distribution shift is not formally introduced in the manuscript. \n- Eqn (7) implicitly makes a strong prior assumption that the feature distribution condition on the label is isotropic Gaussian. This reminds me of Linear Discriminant Analysis (LDA), which followed from a similar heuristic, and might partly explain the empirical success of this practice (the model is forced to be LDA like, which combats the overfitting via appealing to simpler models). However, I have not found any discussion related to this, which evidence that the author(s) might lack a proper understanding of classical treatments.  \n- Theoretical analyses of synthetic examples do not lend strong support to this paper. \n\nQuestions\n# What is the fundamental difference btw the proposed work differs and domain adaption?\n\nMinors:\n% Conditional entropy H(f_{\\theta}(X)|X) is zero. \n% I do not see the point of referencing adversarial robustness literature. ", "belong_id": "Byes0TNFDS"}, {"uid": "BJghkMLaYS", "paper_title": "Entropy Penalty: Towards Generalization Beyond the IID Assumption", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes to combat domain shift via the information bottleneck principle, where the representations are encouraged to learn features that have high mutual information with the labels while having low mutual information with the original data. The paper proposes a specific approach to enforcing this information bottleneck, called 'entropy penalty', which tries to minimize the L2 distance between the representation at the first layer and the mean representation of the corresponding class. \n\nThe paper proceeds to demonstrate theoretically that some variant of the proposed entropy regularization works for synthetic, binary cases in the sense that it decreases the effect of features that are not highly correlated with the label.\n\nThe paper finally demonstrates superior performance on synthetic experiments and classifying digits in an out-of-distribution setting where the OOD distribution is changing foreground and background color.\n\nMy greatest concern of the proposed approach is the generality of the method. While the information bottleneck is a very general principle, the way it is implemented in this paper is very specific, which only works for the first hidden layer representation of an input x (even before non-linearity). Combining this with the experiment in which the domain shift is changing colors of in the images (which applies linear transforms to background and foreground pixels), it is hard for me to believe that the same procedure is not tailored for this specific type of domain shift (although this is still an interesting type of domain shift that is important in sim-to-real applications in robotics). \n\nIn fact, based on the information bottleneck principle, one could essentially learn that the color is also a feature that also has high MI with label and low MI with inputs, thus the top-left pixel would be indicative of the label. If I use this as the feature for entropy penalty (assuming that there is 1 color per class), this gives a R_{EP} loss of zero. I wonder if this is the reason why the experiments specifically asked for two foreground and two background colors for each class.\n\nBased on there concerns, it might seem more helpful to consider other types of 'unknown' domain shifts, such as CIFAR vs. CINIC, in which the method proposed might introduce fewer inductive biases than in the case of being invariant to color. \n\nMinor comments:\n\t- It would seem like L1 regularization would achieve a similar effect to what is shown in the theory here? It would be interesting to see how sensitivity changes with some existing regularization methods, because they can be easily implemented.\n\t- Entropy is smaller for lower layers?\n\t- Minimizing entropy for higher layers at least minimizes an upper bound to the entropy of lower layers (if we consider discrete RVs).\n\t- Eq 6 is false for continuous RV and differential entropy. One could simply have h_2 = 2 * h_1 to get larger entropy. Eq 5 is true from data processing but the conditional entropy H(h_l | x) is not fixed.\n\t- It seems that from the discussion about entropy / MI of different layers does not justify the EP approach for the first layer. Even if the final layer retains all the information, you can still technically apply EP to remove redundant information? You can use leaky relu activation to make sure information is not lost in activation functions.\n\t- Typo for definition of \\mu_k\n\t- The first layer is a convolution layer, so maybe it could be helpful to visualize the learned features with or without EP?\n", "belong_id": "Byes0TNFDS"}, {"uid": "HJeSUXxRFH", "paper_title": "Entropy Penalty: Towards Generalization Beyond the IID Assumption", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed Entropy Penalty (EP) training, based on Information Bottleneck (IB) to make the trained model generalize beyond the IID assumption that is satisfied by usual training and testing datasets.\n\nFirst, a loss function derived from IB is provided Prop. 1. Then they argue that minimizing this loss over lower layers is better Eq. (5) and (6). They further assume the hidden layer is Gaussian, and use squared l2 loss as entropy penalty.\n\nThey theoretically analyze a different loss function Eq. (7) under two simple cases, where optimal solutions have closed forms, and find that the optimal solutions contain smaller weights for non-discriminative features (p_i there).\n\nExperiments on coloured versions of MNIST are conducted to show that the proposed Entropy Penalty achieves better results than several baselines.\n\n1. There is no definition of a robust or non-robust feature, but just a vague description by a simple example (camel appearance). I suggest before presenting the method, providing examples like p_i the synthetic cases, to give a sense of what kind of features are (non-)robust.\n\n2. The Gaussian assumption of the hidden layer is quite restricted as mentioned. And why only apply EP on the first layer rather than all the hidden layers?\n\n3. The objective in analysis Eq. (8) looks different from Eq.(7). Why not using the original objective?\n\n4. The synthetic examples are not convincing enough. For example, what are the solutions of other methods, and how can we see the EP solution is better than others for these examples. More comparisons are needed to give a sense of the advantage of EP.\n\n5. Why is randomly assigning colours considered as a distribution shift? It is not clear to connect what the synthetic examples try to deliver and the coloured version of MNIST here. \n\nThis paper shows that EP based on IB, together with the Gaussian assumption of the hidden layer can learn robust features as shown in synthetic case analyses and coloured MNIST experiments, comparing with several baselines. However, there are several gaps,\n\n1) what exactly is a non-robust feature, in synthetic examples, these are p_i and k, however, in experiments, it seems to colour. There is no definition and thus it fails to show what exact kinds of features the proposed EP actually can capture (p_i, k, colour, or something more general). In this sense, I have the impression that the presentation is not very clear (EP can learn something, but what it is?).\n\n2) The claim is that EP learns robust features for deep learning methods, but both the analyses and experiments are not enough to show that. First, several restricted assumptions are made as mentioned, learn the first layer only, Gaussian assumption, and the examples are too simple, and lack of calculation for other methods and comparisons. Second, just coloured version of MNIST is not convincing to show that EP captures many/most robust features, even though the proposed EP significantly outperforms the other baselines here because of there probably (must) have many different kinds of other features. More experiments are needed to support the efficacity of EP.", "belong_id": "Byes0TNFDS"}, {"uid": "B1gLTsN2FB", "paper_title": "AlignNet: Self-supervised Alignment Module", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThis paper proposes AlignNet, a bipartite graph network that learns to match to sets of objects. AlignNet has a slot-wise object-based memory that associates an index with each unique object and can discover new and re-appearing objects. Experiments are conducted on a symbolic dataset.\n\nI do not think the paper meets the acceptance threshold, and recommend for weak rejection. While the paper proposes an interesting architecture to address the alignment problem, it has noticeable flaws in its experimental designs.\n\nFirst, all the experiments are conducted on toy symbolic datasets, where the alignment problem is rather easy to solve. On the other hand, real-world scenarios can be far more complicated. For example, the appearance of the same object can change due to lighting and distance, and it is unreasonable to assume that their features would remain static (apart from simple uniform noises). In addition, the paper only compares against hand-crafted similarity measures (MSE and cosine). It is unfair to compare learned methods only to hand-crafted methods. As a reasonable and fair comparison, the paper should also compare AlignNet against learned similarity measures (such as a neural network supervised with ground-truth labels for alignments).\n\nThe toy dataset and simple baselines in this paper raise doubts on whether the proposed method is applicable to more complex scenarios (such as aligning two sets of objects in natural images through their appearance features).", "belong_id": "H1gcw1HYPr"}, {"uid": "ByxlfiUntH", "paper_title": "AlignNet: Self-supervised Alignment Module", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "With the assumption of object persistence and inspired by the sticky indices, this paper proposed a novel object alignment method for matching arbitrary entities in different sets. The motivation is clear and easy to follow. The simulation experiment with the symbolic dataset gives impressive results. \n\nI like this idea and have one concern about the experiment. To be specific, all the reported results are obtained in the symbolic dataset to simulate the real-world case. Have you performed the entity alignment in the real-world data, such as the cross-lingual knowledge graphs, or cross-modal analysis? It's expected to experiment with more challenging datasets. By the way, have you released your code? This is important to an ICLR submission. \n", "belong_id": "H1gcw1HYPr"}, {"uid": "B1x_gbizcr", "paper_title": "AlignNet: Self-supervised Alignment Module", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a model that is able to associate objects seen in a new time step with the objects seen in previous frames and therefore consider the cases of adding new objects, dropping object no longer visible, but keeping them in memory in case they reappear in the future. The model assumes that the location of the objects is given and the only task to perform is the correct association of instances. Results shows that the proposed approach is better than hand-crafted features on different simulated tasks. Additionally the propose model is shown to help to reduce the computational cost for question answering task.\n\nI lean to reject this paper because in my opinion the proposed method is just a set of predefined rules to train a network to be able to perform object association between a new time step and a memory. Additionally the experimental evaluation is very weak in several points (see below). \n\n- Contribution: this approach proposes a set of rules to train a network to be able to learn the correct association  between two set of features. Additionally, the network is used together with a memory to keep track of previously seen objects. In my understanding the proposed work presents a set of hard-coded rules to train a network for tracking. However, the connection with tracking is presented only in related works (at the end of the paper) and no comparison with other tracking approach is presented. Additionally, the network model that is actually used for the experiments (graph network) is not presented in the main paper, which makes more difficult to understand how the network works for the given task.\n\n- Experimental evaluation: Experiments are performed on a very simple, simulated environment. Objects are very simple to recognize and their location (which is often  a difficult component of the problem) is given. As the task is quite straight forward, the model obtains almost 100% on all tests. Comparisons are made only with hand-crafted features. It is quite evident that a learned similarity measure will be better than hand-crafted distances. \n\nAuthors should explain more clearly that the proposed contribution is a set of rules used to learn a distance between features in order to associate object instances. \nA cleared connection with tracking should be provided from the introduction. In the evaluation there should be a comparison on multiple and more challenging tracking datasets. In this way we can compare the proposed training technique with other tracking approaches that also learn to a distance in order to associate object at different time steps. \n\n", "belong_id": "H1gcw1HYPr"}, {"uid": "rkxbq22pYH", "paper_title": "What graph neural networks cannot learn: depth vs width", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThis paper studied the expressive power of graph NNs, specifically, their universality and limitations under the non-anonymous setting, via the theory of distributed computations. For the universality, it proved the Turing completeness of graph NNs if messaging and aggregation functions are sufficiently strong. For the limitation, it characterized the lower bound of width for solving graph-theoretic tasks (such as subgraph detection, subgraph verification, approximate, and exact optimization problems) using graph NNs. The key idea is to reduce the computation model of graph NNs to LOCAL (for Turing completeness) or CONGEST (for limitations), which are well-studied in the literature of distributed computations and use the known results for these models.\n\n\nDecision\nThis paper gave us a new approach to analyzing the expressive power of graph NNs. Not only does this paper give new theoretical results, but also it opens the door to a new research direction by bridging the theories of graph NNs and distributed computations. However, I cannot confirm the correctness of the proof of Theorem 3.1 (see Suggestions section). For now, I am tending to accept the paper. But I want to determine the final decision after I am certain that the proof of the theorem is correct.\nWe can roughly divide existing approaches for studying the expressive power of graph NNs into two. One is to compare the power of discriminating non-isomorphic graph pairs with isomorphism tests such as the WL isomorphism test (Xu et al., 2019). The other one is to theoretically justify the oversmoothing phenomena (Li et al., 2018). The proof techniques the authors used are different from both of the two. It related a graph NN to the computational models LOCAL and CONGEST, and enabled to incorporate the theory of distributed computations. By doing so, the authors successfully derived many lower bounds in a systematic way, proving the effectiveness of their strategy. I think we can expect that a more refined analysis inspired by this approach will appear in the future.\n\nRegarding the Experience Assessment: I have published several papers in graph NNs (4). But I do not know much about the area of the theory of distributed algorithms (1--3).\n\n\nSuggestions\n\n- Section 3.2\n\t- Theorem 3.1 proves the equivalence of GNN_n and LOCAL. However, the definition of equivalence is missing. Please write it in the main part, since this theorem is the key result of this paper.\n\t- I could not find any reference for the Turing completeness of the LOCAL model. Could you add the reference for it?\n\t- The description of the CONGEST model is only available in the appendix informally (Appendix B.3). Could you write it in the main part?\n\t- The authors emphasized the importance of the universality and limitation results in the introduction and paragraph after Corollary 3.1. In my opinion, the importance of such tasks is in machine learning community (Cybenko's paper on the universality of MLPs (Cybenko, 1989) is one of the most cited papers in the community). Rather, I think many graph NN researchers who are expected to read this paper are not familiar with the theory of distributed computations. Therefore, I would recommend to use page resources to explain the basic concepts of the distributed computation theory.\n\n\nQuestions\n\n- Is there any existing work which tries to solve the graph theoretical tasks using graph NNs? If there is, can the theorems in this paper give explanations for the results?\n\n\n[Cybenko, 1989] Cybenko, George. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems 2.4 (1989): 303-314.\n[Li et al., 2018] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. In Proceedings of the 32nd AAAI Conference on Artificial Intelli- gence, pp. 35383545, 2018.\n[Xu et al., 2019] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019.\n", "belong_id": "B1l2bp4YwS"}, {"uid": "BJxkAi7AFS", "paper_title": "What graph neural networks cannot learn: depth vs width", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies theoretical properties of GNN in particular their expressive power. There are many recent works on this topic and the 2019 ICLR paper 'How Powerful are Graph Neural Networks?' is the closes related to this paper. In the 2019 paper connects GNN with the  Weisfeiler-Lehman graph isomorphism test in theoretical computer science. This paper makes a connection between GNN and the locality notion developed in distributed computing.\nThis connection is rather obvious and GNN being particular local algorithms, their expressive power is at least as limited as the expressive power of local algorithms. In this paper, results in distributed computing are reformulated in a GNN framework mapping the number of rounds required by a local algorithm to the depth of the GNN in order to solve a given graph problem in a worst case scenario.\nIn my opinion, this paper is rather incremental. In order to improve it, it would be nice to see experiments supporting the theoretical results in section 4. Here it is not clear at all if the bounds given are tight in practice.\n\n###\nThe authors added experiments supporting their theoretical results. I am upgrading my rating to weak accept.", "belong_id": "B1l2bp4YwS"}, {"uid": "BJgvajA15r", "paper_title": "What graph neural networks cannot learn: depth vs width", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies impossibility results of GNN in the worst-case sense. In particular, it reduces GNN to a distributed computing model CONGEST and adapt the impossibility result from distributed computing to GNNs. The impossibility results show that for certain problems, e.g., subgraph detection, there exists a graph such that GNN can not solve the problem unless the product of a GNNs depth and width exceeds (a function of) the graph size.\n\nI am not an expert of distributed computing and I did not check all the proofs thoroughly. But I do think this paper provides a solid contribution to broaden the communitys understanding about what the limitations of GNNs are. Overall, I tend to accept it and would like to increase the score based on authors feedback.\n\nPros:\n\n1, I like the contribution of the paper which tries to build connections between GNNs and distributed computing models. From the perspective of computation, GNNs and distributed algorithms do share a lot of similarities. Therefore, some algorithm design choices in distributed computing would shed some light on designing novel GNNs. This may open a new direction for the community.\n\n2, The depth and width dependency results are novel in the context of GNNs.\n\nCons & Questions & Suggestions:\n\n1, Since these impossibility results for a certain subclass of GNNs are in the worst-case sense, it is not clear how it would be useful for practical machine learning problems. Some discussion along this line would be very helpful.\n\n2, It would be great to discuss the relationship between the Turing universality and the universality of function approximation studied in [1]. \n\n3, For people who have no background of distributed computing, it would be great to describe CONGEST before going to the impossibility results reduced from CONGEST to GNNs.\n\n4, I do not recommend authors to refer to the computation model 1 as GNN. You could name it as MPNN in order to make the claim more accurate. GNN in general has a few variants which does not fall into this category and could have higher capacity than MPNN. For example, the authors claim that graph neural networks always sum received messages before any local computation. However, this is not true in GraphSAGE [2] where the aggregation is a LSTM rather than a simple sum. It makes the model resemble more to the computational model 2. Recent spectral graph convolutional networks [3,4] leverages Krylov subspace methods to compute approximated eigenvalues and eigenvectors of the graph Laplacian which are further used to compute long-range propagation / high-power Laplacian to improve representation power. The results on depth may not hold for these models any more since one layer graph convolution could aggregate multi-hop information. Therefore, being more specific on the model class would make the conclusion more accurate. It would be great to discuss these models separately from the computation model 1.\n\n[1] Chen, Z., Villar, S., Chen, L. and Bruna, J., 2019. On the equivalence between graph isomorphism testing and function approximation with GNNs. arXiv preprint arXiv:1905.12560.\n[2] Hamilton, W., Ying, Z. and Leskovec, J., 2017. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems (pp. 1024-1034).\n[3] Liao, R., Zhao, Z., Urtasun, R. and Zemel, R.S., 2019. Lanczosnet: Multi-scale deep graph convolutional networks. arXiv preprint arXiv:1901.01484.\n[4] Luan, S., Zhao, M., Chang, X.W. and Precup, D., 2019. Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks. arXiv preprint arXiv:1906.02174.\n\n======================================================================================================\n\nThe response from authors address most of my concerns. I improved the score.\n", "belong_id": "B1l2bp4YwS"}, {"uid": "rkgFHuWGtH", "paper_title": "Analysis and Interpretation of Deep CNN Representations as Perceptual Quality Features", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In the article the authors propose to measure quality of CNN-features by quantifying the orientation tuning and spatial frequency sensitivity of the features. The underlying hypothesis is that properties of features in the human visual cortex are also indicators for quality in CNNs. The authors devise an experiment similar to experiments performed on mammals to check which features are active under which types of basic patterns. Afterwards, a loss-function is devised that uses proportions of the best or worst features according to the metrics and it is shown that features that have high values on the metrics also lead to good performance.\n-----------------------------------------------------------------------------------------\nI have a problem understanding some of the metrics used. In the introduction, the following claim is made:\n'The first attribute is sensitivity to spatial frequencies at which there is minimal contrast masking in human visual perception'. To my understanding, the metric to measure this is (2) in 4.2. a_m^k is to my understanding the average response of the feature when given an image with orientation-frequency f. therefore, the derivative should be 'the change of activation under change of frequency'. I feel unable to connect this with the initial hypothesis, as it does not mention change of frequencies. i would have expected the correlation between CSF(f) and a^k_m(f), \ndid you have a reason why you did not chose correlation?\n\nSimilarly, for mu_2 in (3) you are using the maximum value. Is there a reason not to use the mean? in that case mu_2 would be the variance, which would be a natural measure for orientation selectivity.\n\n------------------\nIs there a way to make Table 1 more pleasing for the human eye wrt the discussion of the results?", "belong_id": "BJlLvnEtDB"}, {"uid": "rJltQvNAFH", "paper_title": "Analysis and Interpretation of Deep CNN Representations as Perceptual Quality Features", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I thank the authors for their detailed response. Some of my questions have been addressed in the rebuttal, but as far as I can tell very few modifications have been made to the paper: mainly an additional super-resolution experiment in the appendix, which goes in a good direction, but currently comes across as quite preliminary. So I think the paper still has most of the weaknesses I mentioned and thus it is not quite fit for publication. But I do encourage the authors to strengthen the experiments (for instance by addressing the points I raised, or by some other means) and resubmit elsewhere.\n\n---\n\nThe paper proposes an approach to analyzing the properties of 'perceptual metrics' used in deep learning image generation methods. 'Perceptual metrics' are computed by measuring distances between images not in the pixel space, but i na feature space of a pre-trained cNN. The proposed analysis method, inspired by studies of human perception, is based on measuring the response of these features to sinusoidal gratings of varying frequency or orientation. Based on these responses, the paper proposes a 'Perceptual Efficacy Score' that should measure the importance of certain feature in the feature maps for the performance of a perceptual metric. Experiments show that indeed distances measured between features with high score better correlate with human judgement of image similarity than distances between features with a lower score.\n\nI find the paper quite interesting, but lean towards rejection at this point. This i smainly because the experiments seem somewhat anecdotal and incomplete, see below for further details.\n\nPros:\n1) Application of methods from psychology/neuroscience to artificial neural networks is an interesting avenue of work. Moreover, better unsderstanding of 'perceptual metrics' is of wide interest for various image processing applications.\n2) The proposed score seems to indeed correlate quite well with the importance of features for human judgement of image similarity.\n3) Presentation is mainly clear.\n\nCons:\n1) Experiments are not very exhaustive and at times a bit confusing. For instance:\n1a) Results are sometimes presented in a confusing way. In Figure 4 first of all it is not quite clear what points correspond to I guess each point is an image) and, second, it is not very obvious that the correlation is higher i none of the plots. In tables 1 and 2 it is confusing that different percentiles for H and L are used for different networks/layers. Is this based on some tuning? Then the tuning process should be clearly explained. Moreover, it might be useful to report the full curves of performance as a function of the percentage of features used.\n1b) There are no baselines and there is not much justification of computing the 'Perceptual Efficacy' score the way it is computed. What if one uses only the orientation-based score? Or only the frequency-based? What if one selects the most relevant features in a data-driven way (based on correlation on a training set)? What if one selects subsets of features randomly? \n1c) While the method is inspired by methods used for studying natural vision systems, there is no connection to human experiments. It would be interesting to see a comparison of frequency and orientation tuning of features in a CNN to human cells (as I understand, the latter should be available in prior works?).\n1d) It would be great to see the selected features be used not only for offline image similarity assessment, but also for training image processing models - in the end, this has been the main use of 'perceptual metrics'. Do they lead to improved results?\n1e) Since the paper is about (subjective) image quality, it might be useful to show some qualitative results, potentially in the appendix if space is an issue.\n\n2) There are some issues with the presentation:\n2a) I had a hard time understanding what exactly 'Contrast Sensitivity Function' and 'contrast masking' are.\n2b) Minor issues:\n- In the abstract: 'trained object detection deep CNNs' - I guess image classification is meant\n- Beginning of Section 2: 'Section. 2', 'convolution layer as collection channels'\n- Section 4: 'corresponds the the peak:\n- Section 5.2 'Berkeley-Adobpe'", "belong_id": "BJlLvnEtDB"}, {"uid": "HylO0Xs75H", "paper_title": "Analysis and Interpretation of Deep CNN Representations as Perceptual Quality Features", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an analysis of convolutional neural networks (CNNs) features the basis for making perceptual quality comparisons. The analysis is based on the proposed Perceptual Efficacy (PE) Score that measures spatial frequency and orientation selectivity of CNN features. The hypothesis put forward by the authors is that a CNN features with high PE score can be used to formulate a perceptual loss (Eq. 1) that correlates well with human image quality judgement. The authors use a dataset of human image quality judgements to assess their hypothesis. \n\nOne issue I see with the hypothesis as stated is that in the definition of frequency selective features, the authors make use of\nthe Contrast Sensitivity Function (CSF) which quantifies the dependency of human perceptual characteristics on frequency. So in the definition of the PE score we have embedded knowledge of human perceptual sensitivity. Is it therefore surprising  that we see correlation between the high PE features and human judgements of quality? \n\nExperimental results: The scatterplot presented in Figure 4 does not say to me what the authors claim it should. I do not see a significant difference between the low-PE features and the high-PE features in terms of their correlation with human image quality judgement (as measures in this case by the DMOS). I also find the large table of number in Table 1 to be rather\nimpenetrable. I would recommend an alternative method of presentation to make the desired point.  \n\nClarity: There are many undefined terms and acronyms (eg. SISR, HVS are not defined, while DMOS and SROCC are not described). Also, the description of visual masking in Sec. 4.3 was confusing and difficult to follow. Otherwise the writing was reasonably clear.\n\nImpact and significance: Overall, the findings of the paper are not terribly surprising and as discuss above, given the use of the CSF (quantifying human perceptual characteristics) in the definition of the CNN Perceptual Efficacy (PE) score, it would seem rather surprising that a correlations would not be found. As a result of this as well as the rather narrow nature of the study involved, I am inclined to think that the impact potential of this paper would be rather low. ", "belong_id": "BJlLvnEtDB"}, {"uid": "HkxhCSGC9B", "paper_title": "Analysis and Interpretation of Deep CNN Representations as Perceptual Quality Features", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The submission aims to analyze deep neural network (DNN) features in terms of how well they measure the perceptual severity of image distortions. It proposes to characterize each DNN feature in terms of two well known properties of the human visual system: a) sensitivity to changes in visual frequency and b) orientation selectivity. Both properties are evaluated with respect to the known human Contrast Sensitivity Function (CSF) and measured empirically from the features response to (oriented) sinusoidal gratings. The results are quantified by a composite score termed Perceptual Efficacy (PE).\nIn a set of comprehensive experiments (several pre-trained DNNs, several layers per DNN and two different datasets of distorted images with human perceptual quality annotations) it is demonstrated that feature representation consisting of a layers features with high PE better agree with human perceptual quality judgments than low PE feature representations from the same layer.\n\nI believe the submission convincingly demonstrates a statistical association between the proposed PE score of a DNN feature and human perceptual quality assessments.\nThough, it remains unclear whether the characteristics captured by the PE score are necessary or sufficient to explain the success of DNNs to guide image generation tasks by providing a perceptual loss function.\nFor that I believe it is necessary to demonstrate that the present empirical results can be used to improve results of an image generation task, e.g. super-resolution. \nFurthermore the limits of the PE score could be explored by hand-crafting image representations with maximal PE score and comparing their usefulness in guiding e.g. a super-resolution task compared to a pre-trained DNN.\n\nThus, overall I believe the submission reports interesting initial results but falls short of showing that they capture general properties that can be transferred to improving perceptual loss functions. ", "belong_id": "BJlLvnEtDB"}, {"uid": "SJlaTfYptS", "paper_title": "Neural Linear Bandits: Overcoming Catastrophic Forgetting through Likelihood Matching", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\n\nThis work provides a memory-efficient nonlinear bandit algorithm based on deep neural networks. More specifically, the algorithm in this work only uses part of history information to save the memory usage. To overcome the catastrophic forgetting problem,  the authors provided novel covariance matrix approximation method. Experiment results also suggest that \n\nPros:\n\nThe writing of this paper is very well. It provides enough introduction of the background of nonlinear bandit problems. The experiment settings and results are convincible. \n\nCons:\n- The core idea lacks solid theoretical supports. There is no regret bound result in this paper. The reason why I think the authors should add such theoretical proof is that it seems that the idea to construct new prior matrix instead of old one to avoid the catastrophic forgetting is not related to deep neural network at all. Thus, given existing regret analysis for Thompson sampling on linear bandit problems, the authors should also provide a simple analysis on linear case to show that the construction of prior matrix is indeed meaningful. \n\n- The experiment part does not show the accuracy the SDP solve needs. As the authors mentioned in Discussion part, below equation 6, it is very crucial to decide the accuracy the SDP solver needs. I suggest the authors add more details about the SDP solver in the experiment part.\n\n\nMinor comments:\n\n- The authors used DNN to minimize equation 3. Have the authors tried  a regularized MSE instead of (3)? I think to add a regularizer can further improve the results. \n- At page 13, below equation 8: why the first equality lacks?", "belong_id": "r1gzdhEKvH"}, {"uid": "HJlg_dy0KB", "paper_title": "Neural Linear Bandits: Overcoming Catastrophic Forgetting through Likelihood Matching", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a neural linear bandits algorithm that is resilient to catastrophic forgetting when using limited memory.\n\nThe proposed algorithm Alg. 2 is similar to Thompson sampling for linear contextual bandits, Alg. 1, but using the last layer activation vectors as a linear feature, and also a different way of updating noise parameter prior and posterior is used based on Bayesian linear regression Eq. (2).\n\nAlg. 2 also works with limited memory of history data, therefore after every time, the memory is refreshed, likelihood matching is used to calculate new Phi to make the likelihood (mean and variance) of reward estimation the same as it for the old feature. For mean matching, minimizing MSE Eq. (3) is used and for variance, solving PSD problem Eq. (6) is used.\n\nThe complexity of this algorithm is analyzed. And experiments are conducted to show that the proposed method is resilient to catastrophic forgetting and can achieve good cumulative reward results.\n\nThe proposed method is reasonable and the results look promising. However, I found several weak points as follows.\n\n1. As mentioned Bayesian linear regression Eq. (2) is used to update noise prior and posterior, but this update has no theoretical guarantees as mentioned. As an algorithm mainly works under bandit settings, this is kind of undesirable.\n\n2. This algorithm works with neural network-based features, but it is in nature not scalable as shown in the complexity analysis (linear dependence on action number). The linear feature is just replaced by the last layer activation of NNs. From this perspective, the experimental results just justify again that the NN feature is somehow powerful, which is as expected.\n\n3. The likelihood matching can deal with catastrophic forgetting with limited history memory, which looks good. But the fact that it actually works for linear feature (last layer activation) together with realization assumption weakens this contribution a lot. The authors find using Eq. (3) is better than the exact mean matching Eq. (5), and there is no explanation for this, which kind of shows the proposed likelihood matching probably is not a good way when using full NNs rather than just linear features (last layer). On the other hand, the SDP seems also can only work under linear feature settings, and is not promising to be generalized to fully update for NNs.\n\nOverall, this is a reasonable paper. However, on the one hand, as an algorithm mainly works under bandit settings, it is a lack of theoretical support. On the other hand, the linear feature setting weakens the contribution of likelihood matching to deal with catastrophic forgetting with limited memory. There are some questions of the proposed mean matching, and the matching is not able to generalize. ", "belong_id": "r1gzdhEKvH"}, {"uid": "rklDDmY0tB", "paper_title": "Neural Linear Bandits: Overcoming Catastrophic Forgetting through Likelihood Matching", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper adapts Bayesian linear regression to the setting of a limited memory replay buffer. The idea is to calibrate the prior mean and variance when the neural representation of context is updated. Overall the paper is well written and explained clearly. Some experiments are provided to show that the proposed method is able to achieve a performance competitive to Bayesian linear regression with infinite memory.\n\nThe result of this paper is interesting. But I am not sure if the current experimental results are convincing enough to justify the significance of the proposed method. \n1. The results in Section 4.2 seem to be following the setting in Riquelme 2018. These datasets are all in a supervised learning setting. It is a bit disappointing that the proposed method is not tested on RL datasets. \n2. No other baseline is provided in the experiments for comparison. \n    a. There are other methods in the literature to overcome catastrophic forgetting of neural networks, e.g regularizing the update of the network. How would that be compared to the proposed method?\n    b. What about other methods, like [1]?\n3. Most of the experiment details are missing. For example, how is the reward defined in section 4.3? What is the overhead in computation in practice, especially for the SDP?\n\n\nOther comments:\n1. Why would solving a SDP require only O(g^{0.5}) in section 3.1?\n2. In the discussion in section 3, even if equation (5) and (6) can be exactly solved, how does the heavy tailed problem mentioned in section 2 been solved?\n\n\n[1] Elmachtoub, Adam N., et al. 'A practical method for solving contextual bandit problems using decision trees.' arXiv preprint arXiv:1706.04687 (2017).", "belong_id": "r1gzdhEKvH"}, {"uid": "rkxncuZQYH", "paper_title": "BOSH: An Efficient Meta Algorithm for Decision-based Attacks", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1547", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors study the adversarial example generation problem, in the difficult case where the attacked model is a black box. Since the model is unknown, the approaches based on the minimization of a loss function with a gradient based optimizer do not apply. The current alternatives, known as decision-based attack, use iterative local updates from a starting point to a local minimum, where the class of the adversarial example is different from the initial example while its distance stays close to the initial one.\n\nFor handling the sensibility to starting points, the authors propose a meta-algorithm, which uses any iterative local update based attacks, and which maintains a set of solutions corresponding to different starting points. The proposed algorithm uses successive halving for iteratively maintaining empirical good solutions by discarding the worst half of solutions at each step, and uses Tree Parzen Estimator to explore by resampling promising area.\n\nIn the experiments, the meta-algorithm uses SignOPT attack. It is compared with three decision based attacks, including SignOPT. Three image datasets are used. The attacked models are neural networks and gradient boosting tree in the last experiment.\n\nPros:\n- The paper is well-written and easy to follow.\n- Generic algorithm.\n\n\nCons:\n- No analysis is provided.  \n- The proposed algorithm has a lot of parameters: $k, M, s, m, T, inf, \\alpha$. \n- The $\\epsilon$ are not the same for each attacked model (table 1 and 3). Is it the result of a post-optimization? Could you plot the curves ASR versus $\\epsilon$ ?\n- In algorithm 2, $min_score=inf$,  whatever $t$, so why using two variables ?\n\n___________________________________________________________________________________________________________________________________\nI read the rebuttal.\nThanks you for answering my concerns.\n\nI think that it is possible to provide some theoretical guanrantees. For instance, may be one could show that the quality of the attacks is increasing when Algorithm 1 is run. Finding the highest increasing rate could be useful for tuning the parameters of the algorithm.\nHowever, I understand that this could be tricky.\n\nI took a look to Figure 6. Good point: BOSH Sign-OPT attack outperforms Sign-OPT attack whatever $\\epsilon$.\n", "belong_id": "ryxPbkrtvr"}, {"uid": "HylNib0yqB", "paper_title": "BOSH: An Efficient Meta Algorithm for Decision-based Attacks", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a meta-algorithm for the so-called 'decision-based attack' problem, where a model that can be accessed only via label queries for a given input is attacked by a minimal perturbation to the input that changes the predicted label. The algorithm, BOSH, augments any iterative algorithm for this problem with a diversification strategy based on bayesian optimization and throwing away bad solutions. Empirically, it is shown that BOSH can improve the performance of recently developed algorithms for this problem, by exploring more solutions and refining them intelligently.\n\nOverall, the decision-based attack problem is very practically relevant as it assumes minimal access to the classifier. I also really like that the authors looked into tree-based models in addition to neural networks. The algorithmic ideas that are proposed are simple and effective, as supported by the experimental results.\n\nHowever, I have some serious comments about the experimental evaluation that I believe can substantially improve the quality of the paper, if addressed. Whether I raise my score or not will depend on how well the authors address these questions. I also have concerns about related work in heuristic algorithms.\n\nQuestions:\n- Related work: the ideas of diversifying solution paths and throwing away bad solutions are very popular in combinatorial heuristics. You should do a thorough review of work in that area and in Genetic Algorithms (GA). You could look into the following classical/survey papers as starting points, in particular the first survey's chapter 4.\n\nGlover, Fred, and Manuel Laguna. 'Tabu search.' Handbook of combinatorial optimization. Springer, Boston, MA, 1998. 2093-2229.\nFeo, Thomas A., and Mauricio GC Resende. 'Greedy randomized adaptive search procedures.' Journal of global optimization 6.2 (1995): 109-133.\n\n- Other Black-Box algorithms: you should compare against well-established black-box optimization algorithms such as NOMAD and RBF-OPT. Both are based on very solid mathematical foundations and have high-quality open source implementations:\nNOMAD: https://www.gerad.ca/nomad/\nrbf-opt: https://github.com/coin-or/rbfopt\n\n- Runtime comparison: your analysis with respect to number of queries is very good and insightful. In addition, we should get a sense of the runtime performance. If you run each of the approaches with the same time limit, how do they fare?\n\n- Comparing to 'optimal' attacks: we need to know how well the solutions are compared to the best possible, or a close-enough approximation. You could run white-box attacks and compare the relative error to the quality of the white-box attack. Otherwise, it is hard to tell what gap remains to be closed algorithmically and it is difficult for other researchers to know whether it's worth trying to improve what you propose here in the future.\n\n- Time complexity: please give a time complexity analysis of BOSH as a function of all its hyperparameters.\n\nClarity:\n- Figure 1: I don't understand what this figure shows. Where are the 2 minima? Please clarify further.\n- You minimize l(.) / g(.) in Algorithm 2, but maximize it in Appendix B.2. Maximizing makes more sense. Which one is it?\n- Theorem 1: Is that your result or Bergstra et al.'s?\n\nMinor comments:\n- 'Adversarial example generation becomes a viable method for evaluating the robustness of a machine learning model.' --> 'Adversarial example generation has become a viable method for evaluating the robustness of a machine learning model.'\n- 'when searching an adversarial' --> 'when searching for an adversarial'\n- 'Distortion' is used in the literature much less than 'Perturbation'; consider switching them.\n- 'our mega algorithm' --> 'our meta-algorithm'\n- Please use consistent notation: SignOPT or Sign-OPT.\n- Appendix B.1: 'undifferentiable' --> 'non-differentiable'\n- 'are the t  x1 samples' --> 'are the t  1 samples'", "belong_id": "ryxPbkrtvr"}, {"uid": "Hylq3SQaYr", "paper_title": "Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes an approach to learn how to plan in continuous spaces using neural\nnets to learn a value function and a policy for scoring and sampling next-step candidates\nin a stochastic tree search.  The networks are updated as more planning tasks\nare executed, producing more data for the policy and value function, leading to gradually\nbetter plans compared to a number of baselines on benchmarks introduced by the authors.\n\nThis is a very interesting paper, although I did not always found it easy to read,\nmaybe too densely packed for comfort. My main concerns are clarity of the exposition (especially\nof the neural net architecture (sec 4.2) and that the comparisons are exclusively done\non benchmarks introduced by the authors rather than on benchmarks on which the baseline\nmethods had been previously been optimized, which may introduce a bias in favour of the\nproposed approach.\n\nClarifications\n\nBefore eqn 2, I don't understand why U includes both S_free and map, although the map specifies the free space and thus S_free seems redundant.\n\nIn sec 3 (page 3), the authors introduce a new notation s_init which seems to be the same as s_0 in the previous sections (or is it?).\n\nSection 4.2 was really difficult for me to parse and is too compressed (so is the rest of the paper but this one was worse).\n\nFigures were too small (esp. fig 4 and fig 7) for me to read from the printed paper.\n\nThe term 'meta self-improving learning' seems inappropriate. I did not see  how this was a form of meta-learning. Unless I missed something I suggest to change the terminology.\n\nOther Concerns\n\nI have a concern regarding the way r_t(s) is estimated (page 4) by kernel interpolation of the rewards. I fear that it will not generalize properly when trying to extrapolate, especially in high dimensions (since the claim of the paper is that the proposed algorithms is meant for 'high dimensional' states).\n\nIn addition, the experiments are actually performed in  rather low-dimensional settings (compared to working on problems with perceptual inputs, for example).\n\n", "belong_id": "rJgJDAVKvB"}, {"uid": "SkewetU15S", "paper_title": "Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\n\nMotion-planning in high dimensional spaces is challenging due to the curse of dimensionality. Sampling-based motion planners like PRM, PRM*, RRT, RRT*, BIT* etc have been the go-to solution family. But often these algorithms solve every planning problem tabula rasa. This work combines learning with sampling-based planning such that the parent-sampling and expansion steps instead of being done by common heuristics are learnt in an online manner. Also the resulting exploration-exploitation problem is naturally dealt via using a UCB-style contextual bandit algorithm. Since the number of parents are always varying the common trick of 'describe the action choices with respect to the environment' is adopted so that varying number of actions (states to be sampled from) can be naturally incorporated. \n\nThe other significant aspect of this paper is that there is a self-improving component (Algorithm 3) where a dataset is built up every time step, of environments where either an expansion with RRT or the learnt expansion policy is attempted with the policy being invoked more as time goes on and it trains more. If the process succeeds in finding a path to the goal then this example is added to a dataset and the dataset used to update the policy and associated value function to guide it towards the feasible paths found in the tree. \n\nComments:\n\n\n- Algorithm 3: 'Reconstruct optimal path'. These paths are not really optimal for the problem. They are optimal in the tree T that is built so far for example U. But for the problem they are feasible and if RRT* were to be run asymptotically then perhaps near-optimal. The accompanying text should be updated accordingly so that there isn't confusion.\n\n- Here is my main concern with Algorithm 3: For equation 6  where the policy and value functions are updated, the policy is inevitably going to suffer from covariate shift. This is because the algorithm is essentially doing behavior cloning (BC) with respect to the feasible paths found on the planning examples. Since we are inherently in a sequential setting (non-iid) where the states visited by the policy are a direct result of its own decisions the error bound will be quadratic in the horizon (path-length) for equation 6. This phenomenon has been well-understood in imitation learning literature and algorithms like DAgger, AggreVate or online versions like AggreVateD, LOLS already address these problems in a principled manner. Equation 6 should ideally be replaced with an inner DAgger/AggreVateD like loop (with an RRT* dynamic oracle) for stable learning of policy and value function. I am happy to be convinced that covariate shift and resulting quadratic mistake-bound problems are not present here.\n\n- Application of imitation learning to both self-improvement style path planning and leveraging experience in planning has been done before: See 'Learning to Search via Retrospective Imitation\nJialin Song, Ravi Lanka, Albert Zhao, Aadyot Bhatnagar, Yisong Yue, Masahiro Ono, 2018' (this is unpublished it seems so it is unfair of me to mention this perhaps but I wanted to give an example of how to use dynamic oracles for stable imitation in planning.) and 'Data-driven Planning via Imitation Learning\nSanjiban Choudhury, Mohak Bhardwaj, Sankalp Arora, Ashish Kapoor, Gireeja Ranade, Sebastian Scherer and Debadeepta Dey', IJRR 2018. At least the last paper should be cited and discussed in related work.\n\n- Also would be curious how the authors would situate methods which are non-learning based but leverage experience in planning (example E-Graphs: Bootstrapping Planning with Experience Graphs, Phillips et al, RSS 2012) via graphs discovered in other problems directly. Perhaps a discussion in related work is warranted?\n\nUpdate: After rebuttal updating to Accept.\n", "belong_id": "rJgJDAVKvB"}, {"uid": "Bkey_3u7qS", "paper_title": "Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces a novel  meta path planning algorithm  that utilizes neural network module that improves the data-efficiency for iterated path planning problems.\n\nThe authors address a relevant issue and the experiments make sense given the research question.  I particular like the 3 ablation studies that the authors include, which makes the empirical analysis very thorough.\n\nWriting and Clarity:\nThe introduction is written quite well. Section II&III is written quite technical and dense. This can be very hard to understand for non-experts. However these section are  important to understand the  rest paper. Finally, these two sections should be integrated (preliminaries, quite literally, should be at the beginning). Section 5 \n\n\nAdditional Questions:\n1. Philosophically, how does the self-improvement for iterative planning problems not contradict the no-free lunch theorem? What kind of repeated structure do we assume here (because it seems as in Fig. 1 both the obstacles as well as the goal state change randomly)\n2. As you employ a neural network to do value iteration how does the wall-clock time compare to the baselines?  I do not mean the environment time-ticks (that you checked for using the number of collision checks), but actual compute time. \n3. How sensitive is the proposed solution to parameter initialization?  Did you find much variation in changing hyper-parameters, such as network topology, learning rate et cetera?\n", "belong_id": "rJgJDAVKvB"}, {"uid": "BkluC-v3FS", "paper_title": "Weight-space symmetry in neural network loss landscapes revisited", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper presented a method for studying the landscape of the loss function w.r.t. parameters in a neural network from the perspective of weight-space symmetry. The detailed method includes constructing/optimising a low-loss path in between two parameter vectors (incoming connections from the previous layer) of two neurons respectively, and then set the output weight vectors (outgoing connections to the next layer) to be the same without changing the output of the current layer. \n\nThe empirical results show that the proposed optimisation scheme for finding the path is indeed low-loss, which implies that there exists numerous critical points in between two equivalent local minima (which are global minima in over-parametrised models).\n\nMy major concern is that scope of the study is very limited, since only permutation is considered here. I am not an expert in this field, however, I could come up with examples which could easily generalise the method of study to rotations since rotation matrices include permutation matrices. \n\nWe can look into the loss landscape of 2D matrix factorisation in this form UV^T=W, and it is obvious that any rotation of U on the RHS of it is an optimal solution as long as the same rotation is applied to V. The perspective from this optimisation problem is that it gives us a continuous plateau and it includes permutation of the dimensions. \n\nFor neural networks with only one hidden layer. For example, consider a neural network in this form y=Uf(Vx) where U and V are parameter matrices and f() is a monotonic squashing function. It is easy to show that, when U is timed with a matrix R, where RR^T= I, as in U'= UR, there exist a V' and \\alpha that gives R^Tf(Vx) = \\alpha f(V'x) so that Uf(Vx) = URR^Tf(Vx) = U'f(V'x) for the hyperbolic tangent function. In the case where ReLU activation function is used as f(), then as long as the rotation doesn't produce negative entries, V' exists. In addition, when f is ReLU, isotropic scaling of the outputs from the first layer also gives rise to equivalent optima.  \n\nCompared to the proposed study, the aforementioned way of studying the neural networks naturally gives continuous plateaus w.r.t. U in the loss landscape, and, by studying the discontinuity of the landscape w.r.t. V, more understanding could be unveiled. ", "belong_id": "rkxmPgrKwB"}, {"uid": "BJxkDMC6YS", "paper_title": "Weight-space symmetry in neural network loss landscapes revisited", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the permutation symmetry of deep neural networks. It was known that by reordering neurons and their connections in each layer, the input -> output map the neural network represents can be preserved. This corresponded to a set of unconnected equivalent points in the weight space. The authors study the weight-space connectivity of these points through midpoints they call permutation points. They demonstrate that such points are members of high-dimensional manifolds of equivalent points. After that, they look at empirical experiments and explicitly construct a path between two equivalent weight-space points on a toy task and MNIST.\n\nI generally like the paper and its geometrical lens on the problem. I find the figures very helpful in understanding what is going on. However, there are a few points that I wasnt entirely clear on. I will detail them below.\n\n-- Point 1 --\nConnecting equivalent minima vs connecting SGD-found minima. \n\nIf I understood the paper correctly, the derivation connects two weight space points A and B whose weights and biases, once loaded to the neural network, would have the exact same answers on all inputs X i.e. f_A(X) == f_B(X), i.e. they are a pair of equivalent points. I understand that those are the ones we obtain by using the permutation symmetries.\n\nHowever, some of the papers cited look at the low-loss paths between pairs of optima found by training with SGD from independent initializations, which in turn represent different functions. I.e. for two such optima C and D, the predictions on the val/test set are (sometimes) different, showing that the functions are not the same. I found the initial evidence in:\n\nLarge Scale Structure of Neural Network Loss Landscapes. Stanislav Fort and Stanislaw Jastrzebski. NeurIPS 2019. (https://arxiv.org/abs/1906.04724)\n\nAnd also in much more detail in another OpenReview submission:\n\nDeep Ensembles: A Loss Landscape Perspective. (https://openreview.net/forum?id=r1xZAkrFPr)\n\nI found your results very compelling, however, the two problems seem to be quite different -- on one hand you are connecting a pair of minima that are in fact *identical* by construction. On the other hand the empirical work in literature (especially in the two papers I provided above) deals with pairs of minima that in fact do differ on the test set (at least).\n\nWould you mind commenting on how the two approaches relate to each other? \n\n-- Point 2 --\nHigher order connectivity\n\nIn Large Scale Structure of Neural Network Loss Landscapes. Stanislav Fort and Stanislaw Jastrzebski. NeurIPS 2019. (https://arxiv.org/abs/1906.04724), the authors look at higher-order connectivity between SGD-found optima (e.g. connecting 3 optima on a 2-manifold, 4 optima on a 3-manifold etc.). They also have a particularly simple path-finding algorithm. This seems relevant to the approach you are presenting, although the points in Point 1 still stand.\n\n-- Point 3 --\nPrevious work on connecting two optima using layer-wise weight merging\n\nExplaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets. Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Sanjeev Arora, Rong Ge. (https://arxiv.org/abs/1906.06247)\n\nThey prove that a low-loss path between 2 optima exists provided you can apply a p_keep = 0.5 dropout on each of the optima without incurring a significant loss punishment for it. This paper seems very related to your approach. Would you mind commenting on the differences?\n\n-- Conclusion -- \nI like the paper and the idea in general. I appreciate the geometrical lens the authors took. My main point of confusion relates to the connection between this work and the low-loss connectivity between inequivalent optima found in literature, which (at least to me) seems to be the more interesting of the two connectivities.\n", "belong_id": "rkxmPgrKwB"}, {"uid": "HklMZ14z5H", "paper_title": "Weight-space symmetry in neural network loss landscapes revisited", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies a special type of weight symmetry in neural networks. I think studying the geometry of neural-nets is an interesting and important direction for understanding neural-nets, and along this direction, weight-space symmetry is an important subject. However, it seems to me this paper does not make enough contributions. Details are given below.\n \nList of contributions of the paper:\n \n1.\tPropose an algorithm to find a (low-loss) path connecting arbitrary two partner local minima and passing through a permutation point, where a permutation point is defined as a weight setting where a pair of neurons (in the same layer) have the same fan-in and fan-out weights. \n\n2.\tTheoretically prove that some permutation points are connected via paths with equal loss. (Proposition 1 and 2)\n\n3.\tProvide a lower bound for the number of permutation points and high-order permutation points (Proposition 3).\n\nCons: \n\n1.\tThe theory of this paper is a bit weak. There are three propositions.  Proposition 1 and Proposition 2 are about the equal-loss surface and theoretical existence of an equal-value path. They are kind of straightforward to prove. Prop. 3 is about counting the number of permutation points. It is a rather simple combinatorial problem, and the lower bound of the expression (an exponential bound) seems standard. \n\n2.\tSimple proofs can sometimes provide nice insight, but it is not clear how the study of partner global minima can help improve the understanding of DNN. \n    --First, partner global minima are just a special case of critical points created by 'neuron splitting', which has been comprehensively studied in  [FA2000] (Fukumizu and Amari, 2000). Note that Theorem 1 of this paper is also directly borrowed from [FA2000]. To me, this paper does not provide much additional theoretical insight on neuron splitting. \n    --Second, what is the significance of the simulation results? Prior works have shown the existence of low-cost path; this paper shows the existence of a low-cost path containing a permutation point. The major difference is that the new low-cost path is more special. Why is this finding interesting and useful? (noting that the proof of the existence of such a path seems to be much easier than proving the existence of such a path for two general global minima).\n    \n\n3.\tIt is not clear how the path-finding algorithm helps in practice.\na)\tThe algorithm is computationally expensive. It is a double-loop algorithm: in the outer loop, d is reduced by a tiny amount at each time; in the inner loop, for a fixed d, gradient descent is run for the entire DNN (with only one parameter excluded) until convergence. The total time is (# of d) * (original running time). Here, # of ds depends on the grid: if the initial d is 10 and the grid size is 0.1, then (# of d) = 100. \n     Compared to the path-finding algorithm in Garipov et al., which only runs GD for one time, this algorithm is much more expensive, yet the benefit is unclear. \nb)\tThe motivation of the algorithm is not clear. Why choose to monotonically decrease the difference between the fan-in weights (of the chosen pair of neurons), but let all other weights freely optimized during the pathfinding algorithm? The paper does not provide a detailed explanation of this. \n\n4.\tMinor issues\n  -- The weight parameters created by Algorithm 1 may not be a critical point of the loss function, and thus not necessarily a saddle point. I think the claim since the path connects two partner minima, there must be at least one saddle point on the path is incorrect without extra assumptions. \n  --The paper mentioned 'global minima' in the introduction; but in practical training, one does not always find global minima. Is 'global minima' crucial for the theory and for the algorithm? ", "belong_id": "rkxmPgrKwB"}, {"uid": "S1xI2WlfYH", "paper_title": "Fast Neural Network Adaptation via Parameter Remapping and Architecture Search", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors take a MobileNet v2 trained for ImageNet classification, and adapt it either (i) semantic segmentation on Cityscapes, or (ii) object detection on COCO. They do this by first expanding the network into a 'supernet' and copy weights  in an ad-hoc manner, then, they perform DARTS-style architecture search before fine-tuning for the task at hand.\n\nThere is no TLDR for this paper, and I must admit, on reading the abstract and introduction I wasn't entirely sure what this paper was doing at first. Perhaps I was being slow.\n\nFrom a narrative perspective, one of the main selling points is not needing to perform any expensive ImageNet pre-training; however, a pre-trained MobileNetv2 is being utilised. While this was off-the-shelf, it still incurred an initial training cost, so it isn't really fair in e.g. Table 4 to put pre-training cost as zero. On a related note, the authors write that this network is used for its 'generality'. I'd argue that MobileNetv2 is a highly engineered network specialised for mobile computation; a standard ResNet-50 would be more general really.\n\nI would like to see a comparison to a random search, as there are several papers (https://arxiv.org/abs/1902.07638, https://arxiv.org/abs/1902.08142) indicating that this is a very strong baseline. \n\nAs mentioned earlier, the choices for remapping weights seem very ad-hoc. I can't really tell what's going on in Table 5 (why is PR in the NE and PA row?) so the ablation study of how effective this weight mapping is lost on me.  The stuff in Table 6 is pretty interesting however, if convoluted.\n\nI find the odd choices of hyperparameters (tau as 45, gamma as 10, lambda as 9e-3) rather alarming. How important are these? Would this technique work under any other circumstances?\n\nError bars would be a welcome inclusion, particularly in Table 3 where you have 0.1% separating FNA and MNasnet-92. I appreciate that this can be expensive however.\n\nPros:\n- Some promising results\n- Good figures\n\nCons:\n- Ad-hoc design choices\n- Not a fair comparison regarding pre-training. \n- Very specific to one network choice\n- Lack of error bars or comparison to random search.\n\nI am giving this paper a weak reject, as there is insufficient experimental evidence that the technique works, or generalises beyond Mobilenetv2. I am also concerned about the ad-hoc hyperparmaters or weight-mapping. A comprehensive ablation study, along with error bars, and another choice of seed network would do much to strengthen this paper.\n\n\n\n\n", "belong_id": "rklTmyBKPH"}, {"uid": "r1x08lBvYB", "paper_title": "Fast Neural Network Adaptation via Parameter Remapping and Architecture Search", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper provides a new technique to adapt a source neural network performed well on classification task to image segmentation and objective detection tasks via the author called parameter-remapping trick. The parameter remapping uses weights from the source neural network to the two-stages: architecture adaption phase and parameter adaption phase. The technique results in improvements in both performance and training time.\n\nI like the direction this paper takes, NAS is too expensive and we need faster methods through meta learning/transfer learning. The paper is also clearly organized and written.  To the best of my knowledge, the experiments setting is sensible and the results are good. But I am not in the Computer Vision field and I am not so familiar with NAS, I may missed something. Thus, I am less confident about my rating.", "belong_id": "rklTmyBKPH"}, {"uid": "SkekZOLsKB", "paper_title": "Fast Neural Network Adaptation via Parameter Remapping and Architecture Search", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThe paper proposes a method called FNA (fast network adaptation), which takes a pretrained image classification network, and produces a network for the task of object detection/semantic segmentation. The process consists of three phases: Network Expansion, Architecture Adaptation and Parameters Adaptation, and uses the developed parameter remapping scheme twice. Experiments show that it outperforms recent other NAS methods for these two tasks with same or less computation.\n\nConcrete comments\n1. The paper's overall method is a novel one, unifying NAS on det/seg tasks, while prior works mostly only focus on one task. It also 'eliminates' the need for pretraining each instance of the subnetwork. But no one ever pretrain every classification network for searching on det/seg tasks right? It's an insane amount of computation after all. I'm afraid the emphasis of advantage over prior method here is not very accurate.\n\n2. The concrete parameter remapping scheme is not entirely novel. It is similar to the Net2Net method, while seems more naive than that. It does not preserve the mapping function like Net2Net. It seems like a very coarse effort, since mostly what you do is to copy weights, remove weights or fill in zeros. But it is also interesting to see that this naive method works, and actually beat some of the more advanced alternatives in Section 4. \n\n3. The results are quite impressive. On segmentation, the adapted model achieves ~1% mIOU improvement using  similar or less iterations and similar size of model with the methods it compared to, and GPU hours' saving is more significant. If the authors faithfully compared with state-of-the-art methods in search det/seg architectures, but I'm not super familiar with this literature. On object detection the method does not improve the model size or accuracy, but reduces the search time a lot compared with DetNAS. Could the authors clarify that you compared with every recent high-performance NAS method on seg/det tasks?\n\n4. Though the improvement over prior methods is good, the experiments lack an apple-to-apple comparison. For example, using exactly the same NAS search method and supernet, and comparing the FNA method with that not using a pretrained model (i.e., directly search on det/seg) could be a good experiment to showcase the importance of adaptation.\n\nOverall I find the method is effective and experiments convincing and I recommend weak accept in my rating. I hope authors can address my concerns in the rebuttal. \n", "belong_id": "rklTmyBKPH"}, {"uid": "BJgeyM2iYH", "paper_title": "Scale-Equivariant Steerable Networks", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed scale-equivariant steerable convolutional neural networks that is able to preserve both the translation and scaling symmetry of the data in the representation. To achieve this, the authors developed the scale-convolution blocks in the network, and generalized other common blocks, such as pooling and nonlinearity, to remain scale-equivariant. Extensive experiments have been conducted to show that the proposed scale-equivariant network\n(a) is indeed scale-equivariant even with numerical discretization\n(b) achieves better classification performance when compared to non-scale-equivariant networks as well as previously proposed locally-scale invariant networks.\n\nOverall, this is a very good paper. The paper is well-written and well-organized. The newly proposed scale-convolution is the most general way of achieving scale-equivariant representations. Experiments are convincing and justifies the usage of the proposed architecture in dealing with multi-scale inputs.\n\nOne question to ask that does not effect my rating:\nThere is a very similar paper submitted to this conference:\nhttps://openreview.net/forum?id=rkgCJ64tDB\nWould you care to make a comparison between these two manuscript?", "belong_id": "HJgpugrKPS"}, {"uid": "H1g-eN-3KS", "paper_title": "Scale-Equivariant Steerable Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper describes a method for integrating scale equivariance into convolutional networks using steerable filters.  After developing the theory using continuous scale and translation space, a discretized implementation using a fixed set of steerable basis elements is described.  Experiments are performed measuring the error from true equivariance, varying number of layers, image scale and scales in scale interactions.  The method is evaluated using MNIST-scale and STL-10, with convincing results on MNIST-scale and bit less convincing but still good results on STL-10.\n\nOverall, I think this is a nice paper with generally good explanations and experiments probing the behavior.  I would have liked to see more probing into the effects of number and distance between scales.  Table 1 and corresponding text say that a significant advantage of the approach is that it can handle arbitrary scale values, but there was no explicit exploration of the effects of using this beyond one set of scales per experiment/dataset.  What scale values can be sampled, which work best, and why?\n\nAlso, while the MNIST-scale experiment seems convincing, I think the STL-10 is a bit less (but still OK):  Although the method outperforms other methods and appropriate baseline models, it's a little disappointing that pooling over scales (which I would would convert the equivariance to invariance) is best, and inter-scale interactions increase error.  (Perhaps this is not too surprising in retrospect, as images may have limited scale variation from camera position in this dataset, but significant within-class viewpoint variation.)\n\nEven so, I still find the method concise and of interest, with the basics evaluated, even if some of its unique advantages may have been better explored.\n\n\nAdditional Questions:\n\n* Inter-scale interaction could be elaborated a bit more.  End of sec 4 says, 'use convHH for each scale sequentially and .. sum'.  I believe this is sequencing over scales in the kernel; explaining a bit better how this is implemented, including the shape of w in this case, would be helpful.\n\n* Which scales were chosen for the fixed basis?  How large in spatial extent are the kernels in the basis elements, at each scale?\n\n* In the implementation, what is the value of V (sampled 2d conv kernel size)?\n", "belong_id": "HJgpugrKPS"}, {"uid": "SkgxHUDycS", "paper_title": "Scale-Equivariant Steerable Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a framework (SESN) for learning deep networks that possess scale equivariance in addition to translation invariance. The formulation is based on group convolution on the scale-translation group. Filters are represented as the coefficients of a set of continuous basis functions, which are sampled (once) at a discrete set of scales. The theoretical formulatioin is clear and interesting. The approach is evaluated in terms of image classification accuracy. The set of baselines is quite exhaustive, including recent papers and papers that are not widely-known.\n\nThe most significant improvement for the STL-10 dataset was obtained by the SESN-B variant. This is interesting, because it applies the same operation independently at multiple scales and periodically performs global pooling over scale.\n\nThe effectiveness of the approach was demonstrated in the low-data regime, where the inductive bias of scale equivariance is more likely to help.\n\nOverall I found the paper to be thought-provoking and well-executed. There are a number of questions that I would still like to see investigated, but nevertheless I feel that this paper already represents a worthwhile contribution.\n\nMost important issues and questions:\n\n(1.1) The SESN-B architecture resembles quite closely the SI-ConvNet architecture of Kanazawa et al. (except that that paper resized the images instead of the filters). While your approach may be more computationally efficient, it's not clear what leads to the improvement in accuracy here? Can you explain the difference?\n\n(1.2) I would have preferred to see the approach demonstrated on a task which possesses scale equivariance, such as semantic segmentation.\n\n(1.3) To argue in favour of the continuous basis, it would have been more convincing to compare against directly learning the filters at the highest resolution and obtaining the other filters by downsampling. This would not represent a runtime cost during inference.\n\n(1.4a) It seems that SESN-C should contain SESN-A as a special case. However, SESN-C is worse than SESN-A. Do you have any idea whether this is due to optimization difficulty or over-fitting? Could you compare the training objectives?\n(1.4b) It is stated that the scale equivariance of SESN-C is worse than SESN-A and -B. However, it should still be scale equivariant, except for boundary effects in scale? What is the parameter N_S in this experiment compared to the number of scales S? And the same question for the plot on the right in Figure 2.\n(1.4c) How does SESN-C have the same number of parameters as SESN-A and SESN-B? I thought that more parameters would be required to compute interscale interaction. Was the number of channels reduced?\n\nIssues with clarity:\n\n(2.1) The explanation of equation 10 is not clear. In particular, the diagonal structure in Figure 1 is not stated anywhere in the text, it is simply explained as an expansion from [C_out, C_in, S, V, V] to [C_out S, C_in S, V, V].\n\n(2.2) It's not immediately apparent how multiple applications of convHH are used to provide interscale interaction. I assume it is achieved by shifting f or psi in the scale dimension for each application of convHH, or equivalently by modifying the base-scale in the basis?\n\n(2.3) The explanation of 'scalar' and 'vector' variants in the experimental section was not perfectly clear. It is stated that 'all the layers have now scalar input instead of vector input.' However, I understood that the max-reduction was only over the scale dimension, not the channel dimension, so that the inputs are still vectors? This is confusing as a reader.\n\n(2.4) The expansion of the filters to a diagonal structure is described in the 'implementation' section. However, it seems that this would entail wasteful multiplications by zero. Nevertheless, SESN is shown to be highly efficient in the appendix. Do you avoid these pointless operations in the actual implementation?\n\n(2.5) It was not immediately obvious how $\\psi_{\\sigma}(s, t)$ was related to $\\psi_{\\sigma}(x)$.\n\nOther details:\n\n(3.1) In Figure 2, how many layers does the network have which was used to construct the middle plot?\n\n(3.2) It would have been useful to include a study of the effect of the range and resolution of the scale space.", "belong_id": "HJgpugrKPS"}, {"uid": "S1lPeozCYH", "paper_title": "Axial Attention in Multidimensional Transformers", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes axial attention as an alternative of self-attention for data arranged as large multidimensional tensors, which costs too much computational resource since the complexity of traditional self-attention is quadratic in order to capture long-range dependencies for full receptive fields. The axial attention is applied within each axis of the data separately while keeping information along other axes independent. Therefore, for a d-dimensional tensor with N = S^d, axial attention saves O(N^{(d1)/d}) computation over standard attention. The proposed axial attention can be used within standard Transformer layers in a straightforward manner to produce Axial Transformer layers, without changing the basic building blocks of traditional Transformer architecture.  The authors did experiments on two standard datasets for generative image and video models: down-sampled ImageNet and BAIR Robot Pushing, and they claim that their proposed method matches or outperforms the state-of-the-art on ImageNet-32 and ImageNet-64 image benchmarks and sets a significant new state-of-the-art on the BAIR Robot Pushing video benchmark. \n\nReasons to accept:\n\n1.\tSimple, easy-to-implement yet effective approach to adapt self-attention to large multidimensional data, which can save considerable computation for efficiency, while still have competitive performance.\n2.\tClear writing, with sufficient but not redundant introduction of background knowledge and explanation of both the advantages and drawbacks of existing models (too large computational complexity on high-dimensional data).\n\nSuggestions for improvement:\n\n1.\tIt would be better if the authors can provide more analysis or case study to show the reason why Axial attention (Axial Transformer) can reach good performance even if it omits considerable operations compared to traditional Transformers, or to show why the attention operations within axis are important instead of attention operations between axis. \n2.\tDefinition of axis should be more clear in section 3 (there could be some ambiguities of axis).\n", "belong_id": "H1e5GJBtDr"}, {"uid": "ryeuXv839B", "paper_title": "Axial Attention in Multidimensional Transformers", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "It is known that the standard self-attention method is computationally expensive and cost a significantly large amount of storage when the number of points to be attended is large.\n\nThis paper attempts to solve this problem and proposed the Axial Attention method. It is claimed to be able to save an O(N^(d-1)/d) factor of resources over standard self-attention.\n\nThe proposed method looks novel to me, but some of the related works are missing and the experiment session is insufficient. \n\n1)  The author should at least include the following works which also aim to reduce the cost of self-attention. Since the author did not mention these works which also focus on solving the same problem, It is hard for me to judge if the proposed method is better than existing works.\n[a] CCNet: Criss-Cross Attention for Semantic Segmentation\n[b] A^2-Nets: Double Attention Networks\n\n2) self-attention has shown its effectiveness on a broad range of computer vision tasks, including image generation, detection, segmentation, and classification. I do not get why the proposed method is only benchmarked for generative models. Is it because the proposed method cannot be adopted on other popular CV tasks, such as detection, segmentation, and classification? Extra experiments should be included if the proposed method is not only designed for generative models.\n\n3) The ablation study is missing. The author directly compared its own method with other existing methods that are implemented and trained with different hyperparameters. It is hard to know which indeed benefits the accuracy gain and how significant is the proposed method. \n\n4) In table 2 and 3, I do not see a clear advantage of the proposed method over the SOTA methods.", "belong_id": "H1e5GJBtDr"}, {"uid": "BylznUt3qr", "paper_title": "Axial Attention in Multidimensional Transformers", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper claims to propose a new approach to solve the computational problems of self-attention. However, the paper mainly focuses on adapting Transformer for image generation, which has far less applications. The whole paper needs to be rewritten to make their target and contribution clearer.\n\n1. The authors overclaim that they provide a new approach for accelerating self-attention. However, they only adapted Transformer for image generation. In fact, Transformer does not equal to self-attention. Currently, two directional self-attention like Bert has much wider applications compared with Transformer like sequential self-attention. \n\n2. For a paper claim to improve self-attention, they should show its effectiveness on a broad range of tasks, with comprehensive experimental evaluation. However, authors mainly reported the image generation on several datasets. \n\nOverall, the authors need to rewrite the paper. They should either show more applications with the proposed self-attention approach or treat it as a new approach for image generation.", "belong_id": "H1e5GJBtDr"}, {"uid": "Bkp7J9Mor", "paper_title": "Axial Attention in Multidimensional Transformers", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a novel approach to deal with the computational problems of self-attention without introducing independence assumptions. The proposed approach is simple, easy to understand, and easy to implement.\n\nHowever, evaluation for this paper is severely lacking. As it is, there is not enough information provided to adequately assess the proposed method's strengths in practice. The following should be added:\n\nEvaluation on a variety of different tasks, such as image segmentation, temporally consistent object detection, object tracking, etc. Why are the evaluations limited to generative modeling? To prove the generality of the method (as claimed), it needs to be applied to various tasks.\nRuntime (in inference) comparisons for each of the datasets and for each of the baselines. Additionally, a theoretical analysis for runtime in terms of the size of the input should be given (the column in Table 1 should have runtimes for each method clearly specified, and this should be done for each dataset and baseline)\nAblation study. What is the baseline architecture used without axial attention? There is only comparison to previous work which may have used a different architecture.\n\nIf these concerns are thoroughly addressed, I would be happy to increase my score.", "belong_id": "H1e5GJBtDr"}, {"uid": "B1lK0Nv6KS", "paper_title": "Learning to Guide Random Search", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Contributions: \n\t-Authors have proposed a methodology to optimise high dimensional functions in a derivative-free setup by reducing the sample complexity by simultaneously learning and optimising the low dimensional manifolds for the given high dimensional problem. \n\nAlthough, performing dimensionality reduction to learn the low dimensional manifolds is popular in the research community, the extensions made and the approach authors have considered seems to be novel.\n\nComments:\n\t\n\t- Authors have talked about the utilization of domain knowledge on the geometry of the problem. How feasible is to expect the availability of the domain knowledge? Authors have not discussed the downsides of the proposed method if the domain knowledge is not available, and a possible strategy to overcome the same.\n\t- Authors have said that they are specifically interested in random search methods. Is there any motivating reason to stick to the random search methods? Why not consider other sample efficient search methods? \n\t.......random search scale linearly with the dimensions, why one should not consider other sample efficient methods that grow sub-linearly as against random search? \nSrinivas, N., Krause, A., Kakade, S. M., and Seeger, M. Gaussian process optimization in the bandit setting: No regret and experimental design. International Conference on Machine Learning, 2010\n\t- Please derive Lemma 1 in the appendix for the sake of completeness.\n\t- I am missing a discussion about manifold parameters like  in the important equations. \n\t- Authors have made a strong claim that neural networks can easily fit any training data, but it may be not be true for many datasets.\n\t - Authors have claimed that they have fast and no-regret learning by selecting mixing weight =1/d. Author might want to discuss more on this as this is an important metric.\n\t -  .... total time spent on learning the manifold is negligible....   any supporting results for this claim.\n\t - .......communication cost from d+2k to d+2k+kd...   curious to know if there is any metric like wall-clock time to talk about the optimisation time.\n\n\t- Authors have restricted the comparisons to only three important methods, but it is always comparing with other baselines in the same line. Authors should consider Bayesian optimisation as it is a good candidate for the performance comparison, even though the researchers are interested only in random search methods (just like CMAES).\nKirschner, Johannes, Mojmir Mutny, Nicole Hiller, Rasmus Ischebeck, and Andreas Krause. 'Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces.' arXiv preprint arXiv:1902.03229 (2019).\n\n\t- It is seen from the results that the proposed method is not performing better for low dimensional problem like Swimmer function. But according to the initial claim, method was supposed to work better in low dimensional problems. Is it because of the fact that the problem space is not drawn from high dimensional data distributions? \n\t- .....improvement is significant for high dimensional problems  It will be better if the authors compare their proposed method with some more derivative-free optimisers that are proven to be good in high dimensions (like high dimension Bayesian optimisation).\n\t-  The no-learning baseline outperforms random search ..........  this statement is not very clear, does it mean like the proposed method works only when the problem is reduced from higher dimensions to lower dimensions and not on the lower dimensional problem itself?\n\t- Performance profiles represent how frequently a method is within the distance T of optimality  Any thumb rule considered for the choice of T?. Can we think of any relation with standard metrics like simple regret or cumulative regret that are used to measure the optimisation performance?\n\t- Although BO methods typically do not scale......   Authors have made a strong assumption here. In the literature, we see active research in the context of high dimensional optimisation.\nRana, Santu, Cheng Li, Sunil Gupta, Vu Nguyen, and Svetha Venkatesh. 'High dimensional Bayesian optimization with elastic gaussian process.' In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2883-2891. JMLR. org, 2017.\n\t\n\n\nMinor Issues:\n\tImprove the sample complexity may not convey the meaning very clearly to the readers, something like Improve the sample efficiency or Reduce the sample complexity would add more clarity.\n\tInconsistency in the terms used in Proposition 1 and Proposition 2. What does I signify in the formula? Was that supposed to be t?\n\tEven though the constants are mentioned in the appendix, it is always better to mention the constants used in the algorithm like  as step size for quick understanding.\n\tFollow The Regularized Leader (FTRL) is more appropriate than follow the regularized leader (FTRL)\n\tnoise table in pre-processing  Should it mean something relevant to the paper? \n\tWe use widely used.....  may be consider rephrasing the sentence here\n\ttreshold  Typo in Table 1\n\tY  Axis in Figure 2 is missing\n\tAppendix B :\n\tWe also perform grid search ....  would look better\n\tMuJoCo Experiments  is the parameter space continuous and what is the search space considered for n,  and . Do we deal with smaller search spaces in every problem? Any other way of searching the parameter space to further improve the efficiency? \n", "belong_id": "B1gHokBKwS"}, {"uid": "rye3yb6ptr", "paper_title": "Learning to Guide Random Search", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The computation times for random search methods depend largely on the total dimension of the problem. The larger the problem, the longer it takes to perform a single iteration.  I believe the main reason why many people use deep reinforcement learning to solve their problems is due to its dimension-independence.  I am not aware of a paper that tries to minimize the sample complexity. Thus, I think the idea in this paper is novel and may have influence on the literature (maybe an encouragement for a shift from deep reinforcement learning to derivative-free optimization methods). \n\nIn terms of presented results I think that there is not much that they could do wrong. They show in Figure 1 that the reward they achieved with their method is only outperformed by Augmented Random Search (ARS) on the Ant task. On all other tasks, their method at least performs on par with ARS which is a good result.\n\nIn Table 1 they show the number of episodes that are needed to achieve the reward threshold. Their method required less episodes than all other methods, but I think this is not the only criteria they should have looked at. So, it might be the case that their iterations take longer to compute than the iterations of the ARS and thereby making it slower. \n\nThe authors have showed that their method has a lower sample complexity, which is their goal of the research (Our major objective is to improve the sample efficiency of random search.). However, I am not sure whether this means that it also has a lower computational complexity. They address this issue briefly by stating that Our method increases the amount of computation since we need to learn a model while performing the optimization. However, in DFO, the major computational bottleneck is typically the function evaluation. When efficiently implemented on a GPU, total time spent on learning the manifold is negligible in comparison to function evaluations. This would mean that their iterations are performed in less computation time than the ARS, but I would have personally liked to see a number attached to this. \nIf we thus assume that this is the case, then their results are sound. However, I do not see this reduced complexity reflected in the results. If I look at the ratios between the number of episodes it takes to solve the tasks, they seem to be similar to the ones from the ARS. The number of episodes reduces by roughly 50% for all tasks but this keeps the ratio between the different tasks identical. I would have assumed that the ratios would increase in the favor of the larger problems like the Humanoid task. In other words, I still see the influence of the larger dimension in the results. Maybe I am too critical, but to me if they would have just found a faster method without the reduced sample complexity, they would have achieved similar results. \n\nOf course, this problem would not be present if the computation time increases with the number of iterations. In that case, the computation time would not reduce by a fixed ratio and would therefore decrease relatively much on the tasks with a higher dimension. But that would require an exact comparison between the computation times for all tasks for both their method and the ARS which I do not see in their results. If all these things are common knowledge, then their results are sound and they have found a large improvement to the already well performing ARS. \n", "belong_id": "B1gHokBKwS"}, {"uid": "H1gLHy9L9B", "paper_title": "Learning to Guide Random Search", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nIn this paper, the authors first improve the gradient estimator in (Flaxman et al., 2004) zeroth-order optimization by exploiting low-rank structure. Then, the authors exploit machine learning to automatically discover the lower dimensional space in which the optimization is actually conducted. The authors justified the proposed algorithm both theoretically and empirically. The empirical performances of the proposed estimator outperforms the current derivative-free optimization algorithms on MuJoCo for policy optimization. \n\nThe paper is well-motivated and well-organized. I really like this paper, which provide an practical algorithm with theoretical guarantees (although under some mild conditions). The empirical comparison also looks promising, for both RL problems and zeroth-order optimization benchmark. \n\nI have roughly checked the proofs. The main body of the proof looks reasonable to me. However, I have some questions about one detail: In the proof of lemma 1, how the forth equation comes form third equation is not clear. Only manifold stokes' theorem might not enough since there is Us in side of f while U^*s outside of f. I think there should be one more bias term. \n\n\nFor the empirical experiment, it is a pity that the algorithm is not compared with Bayesian optimization, which is also an important baseline.  I am expecting to see the performances comparison between these two kinds of algorithms.\n\nMinor:\n\nThe 'unbiasedness' of the gradient should be more clear. It is NOT unbiased gradient w.r.t. the original function, but the smoothed version. \n\n=====================================================================\n\nThanks for the reply. The comparison between the proposed algorithm and BO looks promising. \n\nI will keep my score. \n\nI am still confused about the proof of lemma 1. Following the notations in the paper, I was wondering the unbiased gradient should be \n\n$E_{S}[f(x + \\delta U^*s)U^*s]$\n\nThen, the lemma should characterize the difference between \n\n$E_{S}[f(x + \\delta Us)Us]$ and $E_{S}[f(x + \\delta U^*s)U^*s]$.\n\nHowever, current lemma 1 is not bounding this error.\n", "belong_id": "B1gHokBKwS"}, {"uid": "Hyx0NWWCcB", "paper_title": "Learning to Guide Random Search", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper addresses the problem of optimizing high dimensional functions lying in low dimensional manifolds in a derivative-free setting. The authors develop an online learning framework which jointly learns the nonlinear manifold and solves the optimization. Moreover, the authors present a bound on the convergence rate of their algorithm which improves the sample complexity upon vanilla random search. The paper is overall well written and the core idea seems interesting. However, the reviewer has a few concerns which needs to be addressed. \n \n1) Methodology: This work depends on deep networks to learn the nonlinear manifolds which is justifiable by the power of deep nets. However, several issues may arise.\n\n1.1)  Globally optimizing the loss function of a deep network is no easy task and according to the authors, their theoretical results holds only if equation (6)--which includes the loss function of a deep net-- is globally optimized. \n\n1.2) Even if one could globally minimize the loss function up to a tolerance, this will require a large number of epochs resulting in a high overhead cost for each update of the algorithm. This cost should be considered during the evaluation of the performance of the algorithm. \n\n1.3) Finally, although the authors mention that : ' Experimental results suggest that neural networks can easily fit any training data', the success of neural networks highly depends on their architecture and carefully tuning their several hyperparameters including the number of hidden layers, the number of nodes in each such layer, the choice of activation function, the choice of optimization method, learning rate, momentum, dropout rate, data-augmentation parameters, etc. One evidence around the necessity of carefully tuning the neural networks lies in appendix B where the authors mention their specific choice of hyperparameters for each experiment as well as the cross validation range they have used. Again, the overhead cost of finding a good deep network through cross-validation or any other method of choice (such as Bayesian optimization or Hyperband) should be considered towards the total cost of the algorithm.\n\n* Note that complex nonlinear manifolds might be better captured by complex yet flexible architecture as the authors also state that: 'If the function of interest is known to be translation invariant, convolutional networks could be deployed to represent the underlying manifold structure'. Hence, a simple fully connected network with fixed hyperparameters is suboptimal in capturing the different manifolds over various problems. This highlights the importance of exploring the space of hyperparameters.\n\n2) Experiments: The results are reported solely over the number of episodes (function evaluations) while the cost of each episode might be significantly different among different methods. Thus, for a thorough examination, reporting the performance over wall-clock time is recommended and required, ideally in both serial and parallel settings . It does not matter whether the time is spent for a function evaluation or for reasoning about the manifold through training the deep network, it should be taken into account.\n\nMinor issues:\n\n1. On page 2, there is a typological error in the footnote in defining the L-Lipschitz concept (replace \\mu with L).\n\n2. On page 3, section 3.1, at the end of the second line, g should be a function of both \\mathbf{r} and psi. ", "belong_id": "B1gHokBKwS"}, {"uid": "Syx91gcotr", "paper_title": "Visual Explanation for Deep Metric Learning", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes a novel approach for visualizing the predictions of neural network models on pairwise tasks, e.g. predicting whether two images are similar. The authors show that to see similarity between two images down on the pixel/region level, it is not sufficient to apply methods like Grad-CAM which do not aim for decomposition. Instead, the authors' approach namely targets decomposition, and shows the benefit of decomposition through intuitive examples and qualitative results. The authors also quantitatively show the benefit of their method. First, they measure the performance of their method vs CAM and Grad-CAM, on the weakly supervised localization (WSL) task. They also show how their method reveals the disadvantages of standard triplet loss compared to a recent metric learning loss. \n\nMy concerns:\n1) While showing performance on WSL is appealing as it allows for a way to quantitatively evaluate the method, I wonder if there are other ways to evaluate, that explicitly measure the quality of the proposed technique for allowing interpretability and understanding of the base model's performance. \n2) The Triplet vs MS experiment is interesting, but I'm not sure this is the most convincing way to show that this proposed visualization technique is better than something else. Just because something shows one method is worse than another, doesn't mean that this better/worse assessment is accurate. Further, how would Grad-CAM do on the same task?\n3) The retrieval experiment only shows qualitative results, and again, no baseline is compared. \n4) Similarity is a relative judgement; it's hard to say if two items are similar, but easier to say if A and B are more similar than A and C. It seems the proposed method doesn't consider negatives, which is perhaps a limitation.", "belong_id": "S1xLuRVFvr"}, {"uid": "B1gxS8ARFS", "paper_title": "Visual Explanation for Deep Metric Learning", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a visualization method for deep metric learning, which derived by analyzing the inner product of two globally averaged activations. The proposed method can generate an overall activation map which highlights the regions contributing most to the similarity. Also, it can generate a partial activation map that lights the regions in one image that have significant activation responses on a specific potion in the other image. The authors also analyzed the linearly of the fully connected layers and global max pooling. These contributions make the applicability of CAM to many CNN architectures. Further, the metric learning architecture is extended to Grad-CAM map, and the problem of Grad-CAM map is pointed out. To the best of my knowledge, these contributions are novel, and derivations seem to be correct. \n\nExperiments on weakly-supervised localization, model diagnosis, and the applications of the proposed decomposition model in cross-view pattern discovery and interactive retrieval are promising. \n\nOverall, this paper is well written, and contributions are good. \n\nMinor problems.  \nIn Sec.1 and Sec.2.2, the authors wrote the Grad-CAM has been used for visualization of re-ID (Gordo & Larlus (2017)). However, this paper seems to be not the works of Grad-CAM nor re-ID. \n\nIn my understanding, Decomposition+Bias is a more accurate model than Decomposition. \nIn the experiments of the Sec5.1 and 5.2, the performances of Decompsotion+Bias are lower than Decomposition. However, there are no explanations for this reason. \n", "belong_id": "S1xLuRVFvr"}, {"uid": "SylvE-sx5r", "paper_title": "Visual Explanation for Deep Metric Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "= Summary \nThis paper presents a simple method that draws visual attention of deep embedding networks for metric learning. It basically follows the class attention mapping strategy based on global pooling operation [Zhou et al., CVPR 2016], but extends the original version to point-specific attention which is novel and enable interesting applications on image retrieval. In addition, the proposed method seems also independent of the loss function used for metric learning, thus it can be applied to most of existing deep embedding networks to understand their behaviors in a qualitative manner. \n\n\n= Decision\nMy current decision is officially 'weak reject' but 'borderline' in my mind. The major concern of mine is its weaknesses in clarity and technical novelty. However, I still believe this submission is valuable since it addresses a relatively new and timely topic, the proposed method is simple yet effective, and the applications of point-specific attention (i.e., 'cross-view pattern discovery' and 'interactive retrieval') are all interesting and practically useful. If the clarity issues are all clearly addressed, I would upgrade my rating. \n\n\n= Comments\n[Pros]\n1) The motivation and implementation of the point-specific attention are convincing. \n2) The point-specific attention enables not only image-to-image retrieval, but also more elaborate understanding about a pair of images and their similarity.\n3) The proposed technique is model-agnostic and loss-agnostic, thus can be applied to most of existing deep embedding networks for image retrieval. Also, the proposed technique does not degrade the retrieval performance.\n4) The proposed technique is simple yet effective in multiple applications, most of which are practically useful and have great impact. For example, the weakly supervised localization is an essential step towards many weakly supervised approaches for higher-level recognition tasks like semantic segmentation, and the interactive retrieval will allow us to build more realistic and useful image retrieval systems.\n\n[Cons]\n1) The proposed technique itself is not new but a straightforward extension of an existing work [Zhou et al., CVPR 2016].\n2) The manuscript is not crystal clear.\n- The name of the proposed point-specific attention (i.e., 'Partial attention') is misleading.\n- The way to compute the point-specific attention map is not clearly described in Section 3.\n- It is hard to understand the contents in Figure 6 as they are not clearly illustrated. \n- The experimental and implementation details of the last two applications are not given. For example: (cross-view pattern discovery) how to compute the angle error, and how much the proposed technique is sensitive to the position selected on the query image, (interactive retrieval) how to compute the similarity between images given a specific region of interest on query.\n3) More qualitative results on the last two applications should be presented, even in an appendix, to convince future readers. Especially, in the case of 'interactive retrieval', more results are demanded as quantitative performance analysis seems not straightforward.\n\n\n= Post-rebuttal review\nThe rebuttal resolves my major concerns and the manuscript has been carefully revised accordingly. So as I promised in my original review, I upgrade the score to weak accept. ", "belong_id": "S1xLuRVFvr"}, {"uid": "rkgGvZb3KS", "paper_title": "Model-Agnostic Feature Selection with Additional Mutual Information", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a practical improvement of the conditional randomization test (CRT) of (Candes et al., 2018).\nIn the study of (Candes et al., 2018), the choice of the test statistic as well as how one estimates conditional distributions were kept open.\nThe authors proposed 'proper test statistic' as a promising test statistic for CRT, and proved that f-divergence is one possible choice.\nThey further shown that KL-divergence has a nice property among possible f-divergences: KL-divergence cancels out some of the conditional distributions, and thus the users need to estimate only two conditional distributions to compute the test statistic.\nFor estimating those conditional distributions in the test statistic, the authors proposed fitting regression models.\n\nOverall, I think the paper is well-written and the idea is stated clearly.\nThe use of KL-divergence for CRT seems to be reasonable.\nThe proposed algorithms look simple and easy to implement.\n\nMy only concern is on the practical applicability of the proposed algorithms (which, however, may be not a unique problem for this paper, but for all the CRT methods).\nThey require fitting regression models for each feature xj.\nFor high-dimensional data with more than thousands of features, fitting regression models for all the features seem to be impractical.\nFor the imagenet data experiment, the authors successfully avoided this problem by using an inpainting model.\nHowever, this approach is apparently limited to image data.\nI am interested in seeing if there is any promising way to make the algorithms scalable to high-dimensional data.\n\n\n### Updated after author response ###\nThe authors have partially addressed my concern on the scalability of the proposed algorithm to high-dimensional data.\nI therefore keep my score unchanged.", "belong_id": "HJg_tkBtwS"}, {"uid": "H1xZGoOhFS", "paper_title": "Model-Agnostic Feature Selection with Additional Mutual Information", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "# Paper summary\n\nThis paper addresses supervised feature selection: given a D-dimensional input variable x = (x_1, ..., x_D), and a response variable y, the goal is to find a subset of 'useful' features in x. Here, a feature x_j is useful if it is dependent on y even when conditioning on all other input variables (denoted by x_{-j}, which is a set). A generic procedure that can produce a p-value for each feature (allowing on to test each feature whether it is useful) is the conditional randomization test (CRT) proposed in Candes et al., 2018.  For the CRT to produce a valid p-value for each feature (input dimension) x_j, one needs to specify a test statistic that measures conditional dependence between x_j and y given the rest of the features.\n\nThis paper contributes the following results:\n\n1. Propose using an estimate of the f-divergence for the conditional dependence measure and use it with the CRT (section 2.2).  \n\n2. Measuring the conditional dependence with an f-divergence requires estimating a few conditional density functions. The paper considers the KL divergence as a special of f-divergence. This particular choice turns out the reduce the number of conditional density functions that have to be estimated (section 2.3). The paper also shows that the resulting conditional measure coincides with what is known as the Additional Mutual Information (AMI) studied in Ranganath & Perotte, 2018.  \n\n3. The paper also studies instance-wise feature selection i.e., selecting a subset of input features which can explain the response specifically for one instance (example) x.  Yoon et al., 2019 proposed a criterion to decide the importance of a feature for instance-wise feature selection (definition 2). Briefly, a feature x_j is deemed important if q(y | full x) > q(y | x without the jth feature), where q is the conditional density function of y given x. _Contribution_: The paper notes that this criterion may fail and derive sufficient conditions (Definition 3) under which this approach will always work.  \n\nIn simulation on toy problems, the paper shows that the proposed method (KL divergence + CRT) has the highest mean under the ROC curve (Table 2), compared to competing methods. In real problems on images, the paper shows that the proposed instance-wise feature selection can be used to select relevant image patches (features) that explain the class of the input images. The paper also conducts experiments on hospital readmission data (Section 4.3), and genomics data (Section 4.2).\n\n\n\n# Review\n\nThe paper is overall well written with some parts that can be improved (details below). Introduction and related work in section 1 are easy to follow. The paper is also mostly self-contained and friendly to non-specialists who may not work on feature selection primarily. My concerns are\n\n1. I find that the amount of contribution is not sufficient. CRT is known from Candes et al., 2018. The present paper proposes using KL-divergence with it. This can be interesting if the combination gives some clear advantages.  Unfortunately I do not find that this is the case. It turns out that one still needs to learn two conditional density functions (see lines 3-4 in Algorithm 1). Further and even more concerning, one has to refit another conditional density function *for each draw from the null distribution* (see 'Fit regression' in the loop in Algorithm 1). As an intermediate step for solving the original feature selection problem, I find that learning conditional density functions is a much more difficult problem. All these limit the novelty of the idea. While the title of the paper contains 'model-agnostic', the idea of fitting conditional density functions seems to contradict it. The paper could have considered some nonparametric conditional dependence measures but did not. For instance, see \n\nKernel-based Conditional Independence Test and Application in Causal Discovery\nKun Zhang, Jonas Peters, Dominik Janzing, Bernhard Schoelkopf\n2012\n\nand other papers that extend this paper.\n\nWhy was the approach of fitting conditional density functions chosen?\n\n2. Related to the previous point, refitting a conditional density model for each draw from the null distribution must be very costly computationally. This point is never addressed in the paper.\n\n3. Lemma 1 states that the expected f-divergence is a 'proper statistic' (in the sense of Definition 1) i.e., p-value is uniformly distributed if the feature is not useful, and vanishes (asymptotically) if the feature is useful. This result unfortunately relies on a strong assumption that there is a consistent estimator for the f-divergence. In fact, the proof does not even rely on the fact that it is an f-divergence. It can be any divergence D(p,q) such that D(p,q) > 0 if  p!=q and D(p,p) = 0. In the proof in section C.2 in the appendix, existence of the quantile function $(F^{-1}_N)$ is never discussed. I can see the first part of the proof (under the alternative H1). But I do not see the second part (under H0). Since $\\hat{f}$ is a consistent estimator by assumption, as N goes to infinity, the two quantities in the indicator function (in expectation) should both go to the same constant. Isn't this the case? \n\n4. As a contribution, the paper states a sufficient condition in Definition 3 under which instance-wise feature selection with the approach in Definition 2 is *always* possible. When does the condition hold in practice? How do we know if it holds or not? If it does not, what can go wrong?\n\n5. Toy experiments: What is D in Xor and Orange? Where is the 'selector' problem in Table 2? In table 2, 'lime' and 'shap' also seem to perform well. The paper never explains why the proposed approach is better than other methods (only reporting higher mean are under the ROC curve). This should be possible for toy problems.\n\n\n\n# Minor but does affect the evaluation\n\n* Paragraph after Lemma 1: it is unclear why those conditional distributions are required instead of conditional distributions in Eq. 3.\n\n* Section E.1 (appendix), page 16: I think you should have $N( 0.5x_1 + 0.5x_2, \\sigma^2_\\epsilon)$ instead of \n$0.5N(x_1, \\sigma^2_\\epsilon) + 0.5N(x_2, \\sigma^2_\\epsilon)$.\n\n\n\n# Things that can be improved. Did not affect the score.\n\n* Section 1.1: the sentence about permutation tests is vague.\n\n* Page 2, our contributions: 'necessary' should be 'sufficient'?\n\n* Section 2, conditional randomization tests: This paragraph is unfortunately not well written even though it is a very important prerequisite of this work. For instance, at ' ... replaced by samples of $\\tilde{x}_j^{(i)}$ that is conditionally independent of the outcome...', at that point, it is unclear 'conditioning on what'. Following this sentence, one approach might be to replace $\\tilde{x}_j^{(i)}$ with a constant (which is independent of everything else). It is not until definition 1 that this becomes clearer. Also, the 'null hypothesis' (which is in the first line of equation 2) is never stated throughout the paper.\n\n* Eq 1: that (i) is unclear. Should state that for i=1,...,N.\n\n* Eq 2: rewrite the second line. The left hand side states that the p-value 'converges in distribution to'. The second line should be just 0.\n\n* After eq.6, how to choose T (the number of bins) in practice?\n\n* Definition 3 is actually a proposition? It is unclear what is being defined there.\n\n* The word 'complete conditional knockoffs (CCKs)' appears for the first time in Section 3.2 without any explanation.\n\n* Orange skin on page 8: what is '~ exp(...)'? An exponential distribution, or just exponential function?\n", "belong_id": "HJg_tkBtwS"}, {"uid": "r1eV9y2k5r", "paper_title": "Model-Agnostic Feature Selection with Additional Mutual Information", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method to provide some level of interpretation on the influence of input features on the response of a machine level model all the way down to the instance level. The proposed method is model agnostic. Quoting the authors, they advocate for methods that look at interpretability as understanding the population distribution through the lens of the model without restriction on the models fit. The problem is posed as a hypothesis testing problem. The paper proposes proper test statistics for model agnostic feature selection. It is argued that f-divergence tests are proper statistic tests, with the KL being particularly interesting as it provides computational advantages. \n\nI have found the paper interesting. The topic is relevant and the approach is interesting. However, I have two main reservations for this work. First, I have found the method difficult to follow and sometimes unclear. Important results are only explained in the appendix. For instance, the derivation of Equation 5 is important but only shown in the appendix. Furthermore, that derivation in the appendix needs to be clarified in my view. For instance, on page 15, for the derivation of $\\delta_I$, can you explain how you went from the second equality to the third equality where references to \\tilde{x}_j are removed from one line to another? It could be due to your definition for the term with a conditional independence\twith the outcome assumed but I suggest that you clarify this as it is important for the paper and for the use of the KL. Also, in this equation, should it be $q(x_j|x_{-j}) instead of $q(x_j,x_{-j})$?\n\nThe second issue that I have is with the experiments. Any reason why the key results on the interpretability of the approach are mostly shown in the appendix (e.g., table 4,5,6)?  Why does table 6 not show results for all the baselines? For the hospital readmission use case, were you able to also get percentages of important features and have it compared with the baselines and vetted for clinical significance? This is more minor but worth double checking in my opinion. For this experiment on re-admission, the paper claims to have data from 130 hospitals for 10 years. Yet the n numbers seems pretty small to me. Total number of events < 100 000 for 130 institutions over 10 years. That would mean that we are dealing with less than an average of 80 admissions per institutions per year. Please confirm or explain if any filtering was done beyond what is described in appendix I. ", "belong_id": "HJg_tkBtwS"}, {"uid": "rkeFqzHiKS", "paper_title": "Autoencoders and Generative Adversarial Networks for Imbalanced Sequence Classification", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper consider important and interesting problem: how to generate a sequence from minority class if we want to do oversampling with synthetic data in a way similar to SMOTE. \n\nNow from the paper, the exact used approach is not clear, as the details are too scarce (see some examples below). Experiments are not convincing, as the authors don't compare to the state of the art approaches. As the exact problem statement is hard to grasp, it is hard to identify the exact contribution of authors.\nNote, that the proposed approaches, in my opinion, are not that different from approaches from modern data generation for imbalanced classification [1, 2], as we propose some kind of GAN to generate new data and so have only two variables: how we select loss for this GAN and how we select the architecture.\n\nI assume, that to be accepted at a major venue a deeper investigation is required at the moment.\n\nSee also the following comments:\n1. Figure 4: title is not required, as we have a caption with the same information. Better to use confidence bars too.\n2. The figures will benefit from usage of vector format.\n3. Figure 3: tSNE can vary from one run to another. It is better to provide at least 3 random figures or even better train e.g. a simple classification model for t-SNE embedded model and present ROC AUC scores for identification synthetic/non-synthetic.\n4. Table 1&2 formatting is different from that usually used in Academy (see e.g. https://dl.sciencesocieties.org/files/publications/style/chapter-05.pdf)\n5. F1 score is often not the best metric for imbalanced problems. The paper will benefit from providing also PR AUC (average precision) scores.\n6. Figure 2: avoid confusion matrices presented in this form, as they take much space providing almost no information. Classic tables are better.\n7. From the problem statement at the very beginning of section 2 it is not clear what kind of labels do we expect (I suppose that for each sequence we have a specific label i.e. all y_i are 0 but one, that is 1)\n8. Sometimes bigger weights for minority objects or dropping significant part of majority sequences are enough, so results for these approaches also should be included\n9. How the hyperparameters mu and lambda were selected?\n\n[1.] Guo, Ting, et al. 'Discriminative Sample Generation for Deep Imbalanced Learning.' Proceedings of the 28th International Joint Conference on Artificial Intelligence. AAAI Press, 2019. IJCAI 2019, https://www.ijcai.org/proceedings/2019/0334.pdf\n[2.] Douzas, Georgios, and Fernando Bacao. 'Effective data generation for imbalanced learning using conditional generative adversarial networks.' Expert Systems with applications 91 (2018): 464-471.", "belong_id": "ryxtCpNtDS"}, {"uid": "HyeyClaKqH", "paper_title": "Autoencoders and Generative Adversarial Networks for Imbalanced Sequence Classification", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper is well-written. The idea is good, but it seems like GANs have been suggested for Imbalanced data sequences before. A quick search on google, I found this paper:\n'Multi-Task Generative Adversarial Network for Handling Imbalanced Clinical Data' by Mina Rezaei et al., arXiv:1811.10419v1\n\nMoreover, the paper doesn't seem to be comparing their results with other state of the art imbalanced sequence classification methods. The comparisons are all between different proposed GAN methods. \n\nFor these two reasons, I do not recommend this paper for publication at this point. ", "belong_id": "ryxtCpNtDS"}, {"uid": "r1ej5XlTqB", "paper_title": "Autoencoders and Generative Adversarial Networks for Imbalanced Sequence Classification", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #5", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1. For imbalanced learning problem, Precision cannot play a good role. Therefore, I recommend using the performance metrics F-value and G-mean to provide comprehensive assessments. \n2. In table 2 of the 5%  data imbalance, the proposed method is not as good as the baseline. Could you provide more results with different percentage of data imbalance, such as 2%, 3%, and 4%. \n2. The authors created their own baseline, and compared against it. There is plenty of baseline methods in literature to compare against such as:\n[1] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. SMOTE: synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16:321357, 2002. \n[2] Haibo He, Yang Bai, Edwardo A Garcia, and Shutao Li. ADASYN: Adaptive synthetic sampling approach for imbalanced learning. In 2008 IEEE International Joint Conference on Neural Networks, pp. 13221328. IEEE, 2008. \n[3] Han H, Wang W Y, Mao B H. Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning[C]//International conference on intelligent computing. Springer, Berlin, Heidelberg, 2005: 878-887.\n3. Figure 2 is hard to understand.\n4. What if projecting both original data and synthetic data into 2D space for visualization, as shown in Model-Based Oversampling for Imbalanced Sequence Classification. \n5. How robust is the proposed algorithm when facing different levels of noise?\n", "belong_id": "ryxtCpNtDS"}, {"uid": "rkxv0JQtYS", "paper_title": "Behavior-Guided Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThis paper proposes using Wasserstein Distances to measure the difference between higher-level functions of policies, which this paper terms as 'behaviors'. For example, one such behavior could be the distribution over final states given the policy, or the distribution over returns given policy. Through the lens of these behavioral embeddings, this paper recovers a few important special cases that are well-known in the literature including WD-based TRPO and distributional RL. This paper shows that the dual formulation of the Wasserstein Distance gives the ability to score individual policies based on a given 'behavioral mapping'.\n\nReview\n\nThe idea of generalizing the trust-region of TRPO by using the WD measure and behavior maps is intriguing. The paper introduces many choices of behavior maps that could lead towards interesting algorithms that merit additional study. I think the connections between the proposed family of algorithms and WD-based TRPO and distributional RL is highly motivating.\n\nThere are, however, a several key issues with the empirical study of the proposed methods that make it challenging to assess the value of introducing another partially understood deep policy gradient algorithm. The results show only behavioral studies of the algorithm, not investigating the effects of the WD based regularizer or the effects of the choice of behavioral map. The results are also significantly limited in their statistical significance, making distinguishing between algorithms difficult in most cases. And the comparison of wall-clock time is particularly difficult to assess due to the uncountably many possible sources of noise when comparing wall-clock time of highly complex algorithms. In the following paragraphs, I will expand upon each of these points.\n\nOne of the key contributions of this paper is the ability to define regularizers based on the definition of the behavior map. The paper introduces many such behavior maps, those over state visitation, actions, and returns; however, the paper does not investigate the effects this novel choice has on the results. It is unclear if introducing each of these behavior maps leads towards different results or if they each induce roughly the same final performance of the agent. It would be highly valuable for me to see a more careful study of the effect of choosing each of these behavior maps on a single simple environment, clarifying that this formulation leads to a family of useful algorithms. As it stands, BGPG may only beat TRPO on these domains because it had more meta-parameters to choose between (BGPG introduces many new meta-parameters: choice of behavior map, kernel for produce RKHS, the meta-parameters of that kernel, the meta-parameters of the behavior map like trajectory length, the entropy regularization term in the WD, etc.). Without a careful study, or intuitive explanation of any of these parameter choices, it is unclear if BGPG won simply through overfitting to the problem.\n\nThe statistical significance of the proposed algorithm is impossible to assess in the given form. The comparisons are made using only five random seeds and the standard error bars are frequently quite large. I refer to Henderson et al. 2017 for further explanation as to why five random seeds is simply too few to provide a meaningful comparison between algorithms. Instead, I'll discuss a few of the results in particular. In figure 3a, I notice that the proposed method has high variance until it plateaus around -300 reward; why? In each of the remaining plots of Figure 3, I notice that the propose method is significantly higher variance than any of its competitors. This greatly leads me to suspect that the proposed method would have much lower average performance if studied across a greater number of random seeds. In the walker domain (Figure 3d), why do BGPG and TRPO both plateau at the same point for many timesteps, then eventually BGPG starts improving again? From Figure 5, the paper somewhat misleadingly states that 'BGES is the only method that drives the agent to the goal in both settings.' However Figure 5 on the left clearly shows that BGES and NSR-ES have indistinguishable performance, and the error bars indicate that neither significantly outperform NoisyNet-TRPO. In fact, due to the high variance of BGES it is unclear if it would drive the agent towards the goal on average if run over more random seeds. On the right, NoisyNet-TRPO noticeably outperforms BGES and is significantly lower variance. The language used to describe Figure 6 is strong, stating that Figure 6 'proves that the benefits come here not just from introducing the regularizer, but from its particular form.' However, I tend to disagree that Figure 6 reliably proves anything. Although it appears that BGES is significantly outperforming other methods, the number of meta-parameters over which it gets to optimize makes it difficult to say if the performance gain is from overfitting to the problem or the form of the regularizer. Additionally, good performance demonstrated across a single problem is hardly proof especially using only five random seeds (Henderson et al. 2017 Figure 5).\n\nAlthough this plays comparatively little role in my scoring of the paper, I feel it is necessary to discuss briefly. Because it is impossible to determine the source of the speed differences between the particle approximator and BGPG, the results in Figure 4 are extremely difficult to interpret. It simply could be that BGPG uses slightly more optimized code than the particle approximator, or it could be that there is in fact a significant performance difference between the algorithms. The only true way to discuss performance differences between algorithms generally is through computational complexity for these reasons. In the case that these statistical algorithms share the same computational complexity, then further analysis including convergence rates would be able to shed light on the speed difference. However, wall-clock time vs. performance has so many confounding factors, it is a fairly meaningless unit of measure. For an empirical investigation of speed, I would suggest giving each algorithm similar number of learning steps or similar number of updates to their weights and compare performance in this way.\n\nAdditional Comments (do not affect score)\n\nI'm curious if this work could be extended to the off-policy case. It does seem like a minor disadvantage that distributional RL is well-defined in both the on-policy and off-policy cases, but the proposed family of methods is not. From my reading of the paper, it appears that there is little preventing this extension. Is this true?\n\n--------\nEdit after reading other reviews and rebuttals.\n\nI appreciate the response noting that the meta-parameters of this algorithm were not tuned. I still feel that understanding the sensitivity to these parameters is important to understand how the choice of these parameters impacts the performance. While it is certainly likely that extensive tuning of these parameters could yield even more improved results, it is also quite possible that the chosen parameters have biased the results. If a more intuitive discussion of the choice of meta-parameters could be included in the paper, I would be more willing to concede this point.\n\nI recognize that 3 or 5 random seeds are a standard in the deep RL community, but I think Henderson et al. 2017 make this point better than I could: this is not a standard that we should hold to. I think that using small domains and smaller networks would allow running across more random seeds and providing a much more careful scientific study. This paper does not convincingly show state of the art performance, a metric that is nearly impossibly to define in a field that moves as quickly as ours, so should not strive to follow the same demonstration study design that SOTA papers follow. A paper with strong theoretical motivations (such as this) should compliment with strong empirical understanding of how that theory translates to practice. Instead, having strong theoretical motivations backed by a demonstration that the algorithm still works is much less convincing (to me). I think a more convincing set of experiments that match well with the intended purpose of the paper (theoretical contribution of behavioral embeddings) would be an investigation to _how_ the optimization affects the learned policies or representations. Instead of performance benchmarks, this would provide understanding for the various effects of the new framework.\n\nAfter the rebuttal phase, I now have an additional new concern about the clarity of the writing and the completeness of the empirical details. Many of the results do not mention most of the important details necessary to replicate or even interpret the results. I had assumed that details were consistent across figures (e.g. figure 3 uses 5 random seeds, so I assumed the same to be true for all other figures that did not specify), but the author response made clear that this is not true. In additional, little to no details are provided in the paper referring to the choices of meta-parameters for any algorithm or the methodology used to chose these meta-parameters. After these details were given in the rebuttal, my concerns were greatly alleviated, but not completely removed. It is unclear if the authors intend to update the paper before the final version, but based on the language of the rebuttal (e.g. indicating it is purely a misunderstanding of the paper), I am forced to assume that the paper will not include these important details.\n\nTo summarize. I think the theoretical underpinnings of these paper are extremely motivating and interesting. I do not want to see these innovations lost. However, the current state of the empirical section of the paper leaves too much open for me to be able to recommend an accept at this time. I do not believe a paper should solve everything in one pass (e.g. careful parameter studies, SOTA demonstrations, real-world demonstrations, etc.), but I do not believe this paper demonstrates any one of these convincingly.", "belong_id": "Hklo5RNtwS"}, {"uid": "r1l0QRy0FH", "paper_title": "Behavior-Guided Reinforcement Learning", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work explores two uses of Wasserstein distances (WD) within reinforcement learning: the first is a variant of policy gradient, where WD is used to guide the policy search (instead of alternative such as Trust-region used in TRPO); the second is a variant of evolutionary search where WD is used again to guide the policy updates.\n\nOne of the strengths of the work is to clarify the notion of Behavior embeddings (Sec.3), which I expect can have several uses in RL.   In this paper, the behavioral embeddings are assumed to be given; it would be interesting to discuss/explore learning these embeddings.\n\nSection 4 of the paper reviews key concepts related to WD.  This is much harder to follow for an RL researcher, and would be improved by adding some intuition relating the material presented to the concepts of Sec.3.  Furthermore, this confusion carries out in Sec.5.  For example, what is the best way to think of \\lambda_1 and \\lambda_2?  And the maps s_1 and s_2?  What are necessary/desirable properties of P^\\phi_b?   There are also many steps packed in Alg.2 & Alg.3, which are difficult to unpack.  For example, what are the \\epsilon (step 1., Alg.3), scalars or vectors, how are they sampled?  It would be helpful to have a discussion of the complexity (both data & compute) of both algorithms.\n\nSection 6 presents empirical results for each proposed algorithm.  Corresponding baselines are presented, but I would be interested to see a wider set of baseline methods. The literature is rich with methods in these classes, both variants of TRPO and ES.  Its necessary to at least pick a representative sample to show and compare (e.g. GAE, SAC).  I am also puzzled by the actual results presented, for example the Hopper reward shown in Fig.3 seems much worse (by orders of magnitude) compared to that reported in the SAC paper (Haarnoja et al. 2018).\n\n\n", "belong_id": "Hklo5RNtwS"}, {"uid": "Bye86rHeqB", "paper_title": "Behavior-Guided Reinforcement Learning", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nSummary: this paper proposes a new regularized policy optimization (PO) method which is based on Wasserstein distances. Its idea is to use SGD to optimize the dual form of the WD, then used in two different policy search approaches TRPO and Evolution Strategies. The evaluations are carried out on a variety of control tasks from OpenAI Gym. \n\n\nOverall, the paper studies an interesting problem in RL. The idea is somewhat interesting. The experiment results also look promising. However, the writing sometimes has unclear descriptions, probably because there are many things packed in this paper. I have some following concerns about it. \n\n- The idea of using behavior embedding is new in PO. The description in section 3 is a bit unclear. There are lacks of motivation why in the paper there needs BES/BEM, the interplay of policies, trajectories, and embedding maps, and why non-surjective of BEM is still fine in this case, etc.. In addition, the definition of state-based, action-based, reward-based assume only discrete domains? Besides, they seem not to reappear in other places in the paper. \n\n- Does the choice of the functions \\lambda in 4.2 as a function in RKHS, especially when approximated as a linear function with random Gaussian features, limit the representation power of the embedding space? \n\n- What is the effect of \\beta when positive vs. negative?\n\n- Experiments: The proposed methods show a lot of potentials, but the description in this section is sometimes unclear. Is TRPO with KL divergence the standard algorithm defined without using BEM or with BEM? Then one can wonder how more ablations can be added, e.g. BGPG with KL divergence, TRPO with WD distances are compared with the proposed algorithm. In addition, how BGPG is compared with other related work that also uses skill/policy embedding, instead of a flat PO approach like TRPO. Similar questions are also applied to the experiments for BGES.\n\n- It would also be more self-contained if the paper includes experiment settings.\n\n- The paper also comes with theoretical results. The author could also consider mentioning one key result in the main paper, instead, everything is put in the appendix.\n\n\n* Minor comments:\n- The KL definition on page 4 use rho instead of \\xi\n\n\n- are u and v in Eq. 3 functions in \\cal C(X) and \\cal C(Y), respectively?", "belong_id": "Hklo5RNtwS"}, {"uid": "HJg3kV22FS", "paper_title": "Predictive Coding for Boosting Deep Reinforcement Learning with Sparse Rewards", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a reward shaping method which aim to tackle sparse reward tasks. The paper first trains a representation using contrastive predictive coding and then uses the learned representation to provide feedback to the control agent. The main difference from the previous work (i.e. CPC) is that the paper uses the learned representation for reward shaping, not for learning on top of these representation. This is an interesting research topic. \n\nOverall, I am leaning to reject this paper because (1) the main contribution of the paper is not clear (2) the experiments are missing some details and does not seem to support the claim that the proposed methods can tackle the sparse reward problem. \n\nFirst of all, it wouldve been better to have a conclusion section, so the readers can see the contributions of the paper. After reading the paper, I still do not understand what are the contributions of the paper and what're from the previous works. The paper does not provide well justification why CPC feature can provide useful information for reward shaping. The paper does not provide a new method to learn predictive coding. It does not provide a novel reward shaping method (the Optimizing on Negative Distance method is very similar to [1]). So, I am not sure whatre the contributions of this paper. \n\nMoreover, I am not convinced that the proposed method can tackle long horizon and sparse reward problems. As the paper discuss in introduction, learning in sparse reward environment is hard because it relies on the agent to enter the goal during exploration. However, the proposed approach seems only able to work in environments where exploration with random policy can generate trajectories that contain sufficient environment dynamics (e.g. dynamics near the goal states). How can the method learn that information without entering the goal? \n\nFurthermore, it seems that the proposed approach only works for goal-oriented tasks (since we need to know the goal state for reward-shaping). I think this should be clearly stated in the paper. \n\nThere are some missing details which makes it difficult to draw conclusions: \n1. How is the success rate computed (e.g. in figure 7 and table 1).\n2. How were the parameters selected (e.g. table 5 in the appendix). Why did you use the default the parameters?\n3. How many runs are the curve averaged over and whats the shaded region (e.g. one standard error)? Most of the results in the paper seem not statistically significant. \n4. In figure 4, five domains are mentioned but only three of them are tested in the section 5.\n5. Section 6.2 seems irrelevant to the paper. Whats the purpose of this section?\n6. Figure 10 shows the result of using CPC feature directly vs. reward shaping. Are both feature using the same NN architecture, same PPO parameters, and same control setting? Moreover, the reward shaping method assumes we know the goal state but using CPC feature does not. Is it a fair comparison?\n\nThe paper has some imprecise parts:\n1. The definition of MDPs (in section 2) is imprecise. For example, how is the expectation defined? how is the initial state sampled? What does $p\\in\\mathcal{P}$ (last line in the first paragraph) mean where $\\mathcal{P}$ is the state transition function? \n\nMinor comments which do not impact the score:\n1. Figure 1 should come before figure 2. \n2. It would have been better if there is a short description of how the hand-shaped rewards is designed for each domain in the main text. \n\n[1] The Laplacian in RL: Learning Representations with Efficient Approximations\n", "belong_id": "Hkxi2gHYvH"}, {"uid": "Byeo7VT3tS", "paper_title": "Predictive Coding for Boosting Deep Reinforcement Learning with Sparse Rewards", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a method to derive shaping rewards from a representation learnt with CPC. They propose learning a CPC representation from some random data and fix it. They assume exploration is not too difficult so that rewards are achievable without additional mechanisms. Once a reward is achieved they can compute the embedding of the corresponding state as a goal under the CPC learnt representation either directly ie. distance or via a clustering step. The paper is clearly written and presents a nice idea. The paper is correct and part of the approach seems novel. I find the analysis of the results well executed though I think that they should be improved for publication.\n\nMajor points: \n* I think this is a valid contribution and it seems like something the RL audience might be interested in. \n* I am very surprised that CPC does such a good job given that the main object for learning in CPC is the distribution of trajectories which should be quite different in random exploration and the optimal policy. I presume this is because the environments are quite simple. This issue is of interest to the readers so an example of a simple failure case should make that point. \n* Secondly, as nice as those embeddings in figure 5 look I wonder what happens in larger mazes with more structure i.e. somewhere where random walk will not be a uniform distribution and thus CPC will (most likely) not work as intended.\n* The clustering bonus how do you prevent it from staying at the edge of the goal region and deriving infinite rewards from that ?\n* Since these are not potential functions how do you prevent the rewards from biasing learning ? Ng et al. -- Policy invariance under reward transformations. This should be discussed in the paper because it is of practical importance.\n* Contrastive learning has been used to find goal embeddings before Warde-Farley et al. Unsupervised control through non-parametric discriminative rewards. In that paper they don't need the CPC future state predictors but instead contrast the goal and the final state of the trajectory. They use the resulting embedding to learn a reward function and ignore the extrinsic reward. Interestingly, they show that the rewards can be learnt online (maybe ideas from that paper can be applied here).\nMinor points:\n* please make legends on plots readable size.\n", "belong_id": "Hkxi2gHYvH"}, {"uid": "SkgcmjEaFH", "paper_title": "Predictive Coding for Boosting Deep Reinforcement Learning with Sparse Rewards", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "  *Synopsis*:\n  This paper proposes using the features learned through Contrastive Predictive Coding as a means for reward shaping. Specifically, they propose to cluster the embedding using the clusters to provide feedback to the agent by applying a positive reward when the agent enters the goal cluster. In more complex domains they add another negative distance term of the embedding of the current state and goal state. Finally, they provide empirical evidence of their algorithm working in toy domains (such as four rooms and U-maze) as well as a set of control environments including AntMaze and Pendulum.\n\n  Main Contributions:\n  - Using the embedding learned through Contrastive Predictive Coding for reward shaping.\n  - A reward shaping scheme that seems generally applicable to any embedding.\n\n\n  *Review*:\n\n  I think the paper provides a compelling motivation, and is well written. I think using the embedding learned through CPC could provide meaningful semantics for representation learning and for reward shaping (as done in the current paper), and encourage the authors to continue down this line of inquiry. Unfortunately, I have several concerns over the method as currently implemented and the empirical comparisons (specifically with the chosen competitors) which I detail below. Given these concerns I am unwilling to recommend accepting this paper, unless several of these concerns are addressed.\n\n  1. This algorithm, by nature, is purely offline as the CPC and clustering all are currently done offline. Furthermore, the clustering portion of this approach requires states to be randomly sampled from the environment to create a nice set of clusters which are representative of the environment's full state space. These two requirements significantly limit this approach, especially when looking at domains where simulation is not possible, or the underlying state distribution is unknown. By and all, I don't think this means we should completely discount this method entirely and the authors do mention this as a detriment to their algorithm in section 6.3. I'm wondering if this paper should look at implementing an online version before publication, but think this is less prescient to the other concerns.\n\n  2. I am concerned about the current policy learning scheme (i.e. tiered policy (1) go to the correct cluster (2) go to the goal) which seems to only be used by your approach. This invalidates the comparisons made with the other reward shaping schemes as this goes beyond reward shaping (you are learning two separate policies).\n\n  3. The current competitors are unsatisfactory as they don't include other reward shaping techniques from the literature. Also the related works section seems to completely disregard this part of the literature. I would recommend comparing your approach to (methods you even cite!):\n     - 'Reward Shaping via Meta Learning' by Zou et. al https://arxiv.org/pdf/1901.09330.pdf\n     - 'Potential based reward shaping' by Gao et. al. https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/viewPaper/10930\n     (I'm sure there are others beyond these)\n\n  4. I'm also concerned with the current significance of the results, particularly AntMaze, Half Cheetah, and Reacher. I'm especially concerned because I don't feel your competitors are not relevant/representative of the current state of reward shaping.\n\n  5. It would be worthwhile to try this method on several RL algorithms (i.e. Q-learning, TRPO, SAC, DDPG, etc...). This will help readers understand if this approach is a general method, or only applicable to PPO.\n\n  6. I quite like the idea of predictive coding (albeit the original scheme presented by Rao and Ballard 1999) as an unsupervised representation learning scheme, but am unsure this is critical for your method and the current approach is not really predictive coding in a sense (or at least the ideas I'm familiar with from cognitive computational neuroscience). I am concerned with the predictive coding idea being highlighted here as a key ingredient, but none of the papers containing the originating idea of predictive coding are mentioned. Rao and Ballard is one, but there is a rich literature following from this work into the free energy formulation (Friston) and active learning. These ideas should appear in your introduction, as you heavily rely on them. Also there are other predictive coding schemes for unsupervised representation learning (such as PredNets from David Cox https://coxlab.github.io/prednet/), which I believe should be mentioned. In light of these other methods, I'm not sure your discussion on only using predictive features for reward shaping is accurate, and instead these claims should be softened for only features learned through CPC.\n\n  Missing experimental settings:\n  - Number of runs tested\n  - What are the error bars in your plots?\n\n  I would also like to recommend 'Deep Reinforcement Learning that Matters' by Henderson for a reference on how to conduct meaningful Deep RL experiments using policy gradient methods (https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16669). I think this paper could benefit from the recommendations made there.\n\n\n  More questions/clarifications:\n  \n  Q1: What constitutes success in the grid world domains for table 1?\n\n  Q2: If you were to train a single policy using your reward shaping scheme (i.e. not the tiered approach currently employed) how would the new policy perform? Are there problems with the current scheme where you have to have the tiered policy?\n\n  Q3: Why CPC and not say an autoencoder or VAE? A comparison over many types of unsupervised embedding learning algorithms could be interesting and make your method more general than currently presented. It could also strengthen your argument for using CPC.\n\n  Q4: How long were the trajectories for training CPC?\n\n  *Other minor comments not taken into account in the review*\n  - I think your labels are backwards in table 1.", "belong_id": "Hkxi2gHYvH"}, {"uid": "H1gKSJxnKH", "paper_title": "Superbloom: Bloom filter meets Transformer", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work presents Superbloom, which applies the bloom filter to the Transformer learning to deal with large opaque ids. Quantitative results demonstrate it can be efficiently trained and outperforms non-hashed models of a similar size. The authors also highlight an important distinction from learning using the entire vocabulary: the depth is crucial to get good performance.\n\nThe size of the vocabulary could be largely reduced through hashing, which makes a larger embedding dimension and more complex internal transformation eligible and thus better performance. To make the story complete, it is good to have the results with the same embedding dimension and internal complexity as the baseline model to see the limitations. In Section 4, it is equally interesting to compare the performance between the bloom filter reduction and the data-driven word piece reduction. That is, models learned with the bloom filter applied to the whole vocabulary and models learned with the word pieces only.\n\nI think the empirical contribution is above the bar, but I do not think the authors gave enough credit to (Serra & Karatzoglou, 2017). The adoption of bloom filter on large opaque ids has already been proposed in (Serra & Karatzoglou, 2017) as bloom embeddings to deal with sparse high-dimensional binary-coded instances. It seems that the technical part of Superbloom, including the hashing and the inference, is the same as those in (Serra & Karatzoglou, 2017). I would appreciate a lot if the authors could revise the presentation to either emphasizing the adoption of the work, or highlighting the differences.\n\n", "belong_id": "SJxy5A4twS"}, {"uid": "rkeOdb0TFS", "paper_title": "Superbloom: Bloom filter meets Transformer", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose to learn a Transformer in embedded spaces defined via $m$ hash functions, with application to (fixed length) sequence-to-sequence classification for NLP. The method differentiates itself, in large part, by formulating model outputs in terms of $m$ distinct softmax layers, each corresponding to a hash of output space $\\mathcal{Y}$. The relative low dimensionality of each hashed output space $d \\ll \\vert\\mathcal{Y}\\vert$ helps to avoid computational bottlenecks associated with wide softmax layers.\n\nFor a given labelled pair $(\\mathbf{x}, y)$, the model outputs a matrix $\\mathbf{P} \\in \\mathbb{R}^{m \\times d}$, whose $i$-th row is trained using the $i$-th hash $h_{i}(y)$. At test-time, predictions are made by finding tokens $y^{*} \\in \\mathcal{Y}$ whose hashes best 'align' with model outputs $\\mathbf{P}$. By way of example, target alignment might be defined in terms of the aggregated log likelihood\n\n    $\\ell(y; \\mathbf{P}) = \\sum_{i=1}^{m} \\log(p_{i, h_{i}(y)})$,\n\nwhere $p_{i, h_{i}(y)}$ can be understood as the element of $\\mathbf{P}$ corresponding to the $i$-th hash of token $y$.\n\nAt this time, the work is significantly hindered by a lack of clarity. This issue begins with the chosen notation, which routinely obscures otherwise simple points. Similarly, the section on test-time predictions (Sect 2.5) is needlessly hard to follow. Given its pivotal role, this section feels strangely rushed. Some unanswered questions I had include:\n  a) What happens if two tokens' hashes collide all $m$ times?\n  b) How were hash functions inverted and what was the cost of doing so?\n  c) How does the test-time throughput of the proposed compare with that of alternatives?\n\n\nQuestions:\n  - Did you compare against different approaches to sparse softmax, such as LSH-based methods [1]?\n  - What was the impact of approximate vs. exact inference and which was used during experiments?\n  - How important were embedding matrices $E^{I}, E^{O}$? What happens if you directly feed hashes into the Transformer or use random projections for $E$?.\n\n\n[1] 'Deep Networks With Large Output Spaces', Vijayanarasimhan et al, 2015", "belong_id": "SJxy5A4twS"}, {"uid": "HJl9otEe5H", "paper_title": "Superbloom: Bloom filter meets Transformer", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes to use codes based on multiple hashing functions to reduce the input and output dimensions of transformer networks. The novelty of the idea is mostly to integrate these codes with transformers. The authors present experiments on two tasks showing that keeping overall model size fixed (i..e, number of parameters), transformers with shorter (but denser) binary codes achieve better performances than standard transformers using one-hot encodings. The gain mostly comes from using larger embedding sizes and larger intermediate layers. \n\nWhile the technical contribution is limited, because most of the principles are already known or straightforward, the main contribution of the paper is to show that random hash functions are sufficient to create significantly shorter codes and maintain good performances. Such codes provide more freedom in terms of where to put model capacity (larger embeddings, more transformer layers, etc.) which may be useful in applications where most of the model parameters are in embedding matrices. \n\nThe value of such a paper resides mostly in the experimental study. On the bright side, the experiments present in sufficient details the impact of the various hyper parameters and the new trade-offs model size/performance that can be achieved. On the other hand, the experiments are carried out on non-standard tasks without previously published baselines, and it is unclear why. Since the method is applicable to any problem involving natural language data (and more generally categorical values, such as knowledge base completion), I would have expected experiments on tasks with a well-defined state-of-the-art. This makes the experiments in the paper look more like 'proofs of concept', and they are less convincing than they should be.\n\ndetailed comments:\n- The paper really is *not* about bloom filters (Bloom filters are data structures that represent sets and efficiently answer membership queries). It is about using codes of identifiers of lower dimension than one-hot encoding. This idea has been used in multi class classification setting (i.e., for the output layer) since (at least) Dietterich & Bakiri (1995) 'Solving Multiclass Learning Problems via Error-Correcting Output Codes' (with more insights on what makes a good code for prediction problems). The authors borrow from Bloom filter the way to create the codes using random hash functions, but the analogy stops here. \n\n- following the comment above, and assuming I understood correctly: There is an originality on the paper compared to other works that use binary codes/bloom filters: In the current paper, the authors actually predict the result of individual hashing functions. This is different from predicting the binary encoding that results from using the 'or' of one-hot encodings generated by several hash functions, as would be done in approaches (truely) based on Bloom filters. For instance, if there are m hash functions taking values in {1, ..., P}, an approach based on Bloom filters would predict a binary output of dimension P, while here there are m multiclass problems with P classes (IIUC). This difference from previous work may be significant in practice.\n\n- I found the description of the link prediction task (section 3.1) rather cryptic: \n* 'from each page, we take a random contiguous segment of entities'. If I understand clearly, the text is filtered out and only links are kept (?). Links are replaced by the entity id they point to. What happens to the entity the page is about? Is it added at the start?\n* 'For evaluation, we hold out one random entity from a random segment on a test page. ': what does 'holding out' mean? From my understanding, it means replaced by the [MASK] token, but it could also mean removed altogether from the input sequence.\n* the task is to predict masked links in sequences of links with the surrounding text filtered out. Does that correspond to any real-life prediction problem (i don't see which one)? Is this 'task' intended to serve as unsupervised pre-training of embeddings? If yes, maybe the authors might say so and give example applications.\n\n- For the natural language task: \n* ', then apply a Bloom filter to reduce the vocabulary size.' -> From my understanding, there is no bloom filter here. If I understand, what is done is to represent unigrams and bigrams by their bloom digest to reduce the input dimension.\n\n- 'We hold out 10% random entity pages for testing. ' -> is there a validation set? How do you choose the hyper parameters?\n\n\nminor comments:\n- 'Since the Wikipedia site usually removes duplicates of links on each page, the distribution of pages is rather long tail.' -> I wouldn't be surprised if the distribution was long tailed even without this specific policy\n- I found the formalization/notation more confusing than helping because it is not really thorough (there is no distinction between sets and sequences, '1[\\eta_j(t_i)] ... where 1[.] is the indicator function' -> what is the 'indicator function' of a number?)", "belong_id": "SJxy5A4twS"}, {"uid": "SkeqZkz-5S", "paper_title": "Min-Max Optimization without Gradients: Convergence and Applications to Adversarial ML", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors aim to propose new algorithms for min-max optimization problem when the gradients are not available and establish sublinear convergence of the algorithm. I don't think this paper can be accepted for ICLR for the following reasons:\n\n1. For Setting (a) (One-sided black-box), the theory can be established by the same analysis for ZO optimization by optimizing y. Even by a proximal step for y, the analysis is essentially the same as ZO where an estimation of the gradient for x is conducted. \n\n2. The assumptions A1 and A2 are hardly satisfied in ML applications, where the objective is essentially smooth. The authors should at least analyze the case where a sub/super-gradients is available.\n\n3. Also, for most ML problems we have today, I don't find many applications where the gradients are not available, and I thus feel that it is not interesting to consider ZO optimizations.", "belong_id": "rylkma4twr"}, {"uid": "SylOV5CfqB", "paper_title": "Min-Max Optimization without Gradients: Convergence and Applications to Adversarial ML", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents an algorithm for performing min-max optimisation without gradients and analyses its convergence. The algorithm is evaluated for the min-max problems that arise in the context of adversarial attacks. The presented algorithm is a natural application of a zeroth-order gradient estimator and the authors also prove that the algorithm has a sublinear convergence rate (in a specific sense). \n\nConsidering that the algorithm merely applies the zeroth-order gradient estimator to min-max problems, the algorithm itself only makes up a somewhat novel contribution. However, to the best of my knowledge, it has not been used in this context before and personally I find the algorithm quite appealing. In fact, due to its simplicity it is essentially something that anyone could implement from scratch. \n\nPerhaps a more important contribution is that the authors provide a fairly extensive convergence analysis, which is an important tool in analysing the algorithm and its properties. Unfortunately, it is not trivial to understand the presented convergence results and their practical implications (if any). For instance, equation (10), which is arguably one of the key equations in the paper, contains variables zeta, nu and P1, all of which depend on a number of other variables in a fairly complicated manner. The expression in (10) also contains terms that do not depend on T and it is not obvious how large these terms might be in practice (in the event that the assumptions are at least approximately true in a local region). Even though I am somewhat sceptical to the practical relevance of this convergence analysis, I recognise that it is an interesting and fascinating achievement that the authors have managed to provide a convergence analysis of an algorithm which is based on black-box min-max optimisation. \n", "belong_id": "rylkma4twr"}, {"uid": "HkxtF2y29H", "paper_title": "Min-Max Optimization without Gradients: Convergence and Applications to Adversarial ML", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers zeroth-order method for min-max optimization (ZO-MIN-MAX) in two cases: one-sided black box (for outer minimization) and two-sided black box (for both inner maximization and outer minimization). Convergence analysis is carefully provided to show that ZO-MIN-MAX converges to a neighborhood of stationary points. Then, the authors empirically compare several methods on \n1) adversarial attack on ImageNet with deep networks, and \n2) black-box poisoning attack on logistic regression. The results show that ZO-MIN-MAX can provide satisfactory performance on these tasks.\n\nIn general a good paper with dense content, clear organization and writing. However, the experiment part does not seem truly convincing. \n1.\tWhat is the relationship between Eqn.(13) and the proposed ZO-MIN-MAX? It seem that in the experiment you compare using this loss ( Eqn.(13) ) against finite-sum loss, but both with ZO-MIN-MAX algorithm? In figure 1 and 2, I dont see a competing method. So the point here is that the loss Eqn.(13)  is better, but not the proposed algorithm? I think you should compare different optimization algorithm under same loss, e.g. something like Eqn.(13)+ZO-MIN-MAX vs. Eqn.(13)+FO-MIN-MAX. This is not evident to show that ZO-MIN-MAX is better than other zero-th order methods.\n2.\tI would suggest comparing to more zeroth-order methods in the experiment.\n\nFrom the experiments I cannot tell whether ZO-MIN-MAX is good enough compared with other methods", "belong_id": "rylkma4twr"}, {"uid": "r1xRDNSNKS", "paper_title": "Online Meta-Critic Learning for Off-Policy Actor-Critic Methods", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a meta reinforcement learning approach where a meta-critic is trained in addition to a conventional actor and critic, so that the meta-critic boosts the training of the actor over a single task. The approach is combined to DDPG, TD3 and SAC and is claimed to convey superior performance (or learning speed) over these state-of-the-art actor-critic algorithms.\n\nAt first glance, the paper looks convincing, but closer inspection reveals a potential issue that I would like the authors to discuss.\n\nThe first cue is that in Fig. 6, the meta-loss does not seem to converge to anything, as the authors say it just fluctuates. Shouldn't it converge once the actor converges to close to optimal performance? \n\nThe second cue is that appart from the rllab tasks (ant and half-cheetah), it is not clear that the meta-critic approach brings some gain in the end of training. Particularly in Reacher (Fig 7), the performance seems to collapse faster with DDPG-MC than with DDPG, and on the rest of curves of Figs 7 and 8 it is had to determine whether the MC approach brings something significant or not. And in Fig. 3, in Walker2D, TD3-MC looks rather unstable.\n\nThe third cue is that in Section 3.2, the paragraphs 'Updating MC parameters' and 'Designing MC' are rather unclear (I'll come back to that) and lack a theoretical justification.\n\nSo I'm wondering what exactly MC is doing and I would like to see a more detailled analysis. Couldn't a similar performance improvement be obtained by just increasing the actor learning rate and addiing some noise to the actor learning gradient? Isn't this more or less what the meta-loss does, when looking at Fig. 6?\n\nTo me, unless the authors give a clear answer to the points above, the paper should be rejected as it does not provide clear enough evidence and justification in favor of the proposed meta-learning approach.\n\nAs stated above, I found the paragraph 'Updating MC parameters' lacking a principled justification.\n\nAbout 'Designing MC', it could be much clearer. You first express two requirements (i) and (ii). Fine.\nThen you explain how to meet requirement (i), but you say this is not what you are doing (!), and then you try to explain what you are doing but without going back to the requirements. In particular, it is not clear at all why your design is 'permutation invariant'. You should be much more direct. Besides, the way to extract features and the relationship to batch-wise set-embedding should be made more explicit. Well, it is complicated and should be explained more clearly...\n\n'TD3 borrows the Double Q learning idea...': no, TD3 does more than this, as it uses the min between both critics, which double Q-learning and DDQN do not.\n\nYou use HalfCheetah-v2 from gym-mujoco and HalfCheetah from rllab. Why? One might suspect this is because the performance of the MC appraoch is only better with rllab...\n\nYou say you are using 10 million steps, but this is not always the case.\n\nIt is interesting to see that the performance of TD3 on Half-Cheetah collapses at around 4 million steps. Any idea why? It seems that in many papers experiments are stopped before performance collapses, and a strong study about this phenomenon is missing as the authors don't want to show this. This is an open call to readers: a paper on that would be great! :)\n\n'SAC-MC' gives a clear-boost for several tasks': well, looking at the figures it is not so clear. Do you mean faster learning, higher final performance or both? I would like to see this claim backed-up by some proper statistical significance test and a clear specification of the number of seeds, etc. Fig.5 conveys the adequate data for such test if your claim is that this is the final performance that matters (but beware that curves are crossing eachother, so the final performance depends a lot on where you stop...).\n\ntypos:\n\np1 'For example, ... (Zheng et al., 2018).' is not a sentence (no main verb).\np2 'the on-policy (approach?) needs to interact with (the) environment...'\np2 'is less effective than off-policy transitions.' => unclear statement\np3 to assist(s)\n\nEquations (2), (6) and (8) should finish with a dot as they close a sentence.\n\nEq X => Eq (X) : use \\eqref{label} instead of just \\ref{label}\n\np5 It's input => Its\np5 two key technologies: I would not call this a technology. Ingredients?\np5 'computational cost is ? by using': missong word\n\np6: asmyptotic\n\np9: we removing\n", "belong_id": "H1lKd6NYPS"}, {"uid": "Skgom5J6FS", "paper_title": "Online Meta-Critic Learning for Off-Policy Actor-Critic Methods", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In actor-critic algorithms, the policy tries to optimize cumulative discounted rewards by a loss formed with components from the critic. In this paper, the authors propose a novel way, namely, meta-critic, which utilizes meta-learning to learn an additional loss for the policy to accelerate the learning process of the agent. There are several advantages of the proposed method, it's learned online for a single task, it's trained with off-policy data, it provides sample efficiency and it's generally applicable to existing off-policy actor-critic methods. \n\nOverall the proposed method is novel and the research direction is a very interesting one to explore. Eqn (3) and Eqn (4) explains the key idea of the proposed method. Eqn (3) describes the meta-learning problem as a bi-level optimization problem where the agent is updated with the main loss L^main with data d_train, in addition, it's updated with L^aux where the loss is learned and parameterized by \\omega. After the agent being updated, it uses L^meta and data d_val to validate the performance of the updated agent. Eqn (4) describes an explicit way to formalize the usage of the auxiliary loss, which is to accelerate the learning process. Thus the meta loss is whether the L^aux helps the learning process or not. \n\nHope that the authors could address the following issues in the rebuttal:\n1) Investigate why DDPG with meta-critic gets much more improvements than TD3/SAC;\n2) Show SAC (or TD3) could get better performance on harder task. (I can understand that they are strong baselines and hard to improve on the current environments, however, for harder environments, there might be rooms for improvements.);\n3) Investigate different ways to parameterize the meta-critic other than simple MLP;\n\nA few minor points:\n1) Page 2 first paragraph, 'This is in stark contrast to the\nmainstream meta-learning research paradigm  where entire task families are required to provide\nenough data for meta-learning, and to provide new tasks to amortize the huge cost of meta-learning.'. Try to revise it and avoid the words like 'mainstream'. Think about the paper being read 5 years later or even more;\n2) Consider introducing a weighting hyperparam between L^main and L^aux in Eqn (3), these two losses might have different scales and it might be better to weigh them differently;\n3) Minor literature detail: Page 7 'Comparison vs PPO-LIRPG' mentioned that 'Intrinsic Reward Learning for PPO (Zheng et al., 2018) is the only existing online meta-learning method that we are aware of.', however, AFAIK, 'Meta-Gradient Reinforcement Learning' in NeurIPS 2018 and 'Discovery of Useful Questions as Auxiliary Tasks' NeurIPS 2019 are methods where meta-learning is applied online and for a single task;", "belong_id": "H1lKd6NYPS"}, {"uid": "rkeydHcRKH", "paper_title": "Online Meta-Critic Learning for Off-Policy Actor-Critic Methods", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I am very torn about this paper as the proposed approach is a fairly straightforward extension of past work on the meta-critic approach to meta-learning and the results are pretty good, but nothing amazing. I tend to accept this paper because I like their general direction and think what they are proposing is pretty simple with broad applicability. It should be fairly straightforward to append this idea to most new off policy methods as they come out, so I find their consistent gains across 3 different popular models pretty convincing that this could have value to the community.  \n\nThat being said, the gains are not huge, which does make me think about the potential computational overhead. How much more run time per step does the meta-critic add to the models in the paper? I am a bit worried that the comparisons are not apples to apples from the perspective of the amount of computation/update steps per environment interaction due to the use of the validation data. I wonder if it changes the conclusion at all if the other approaches are given the same amount of computation on their replay buffer between interactions with the environment. For example, more optimization steps on the buffer is another plausible explanation why the meta-critic does better at optimizing the loss. \n\nI also should note that this paper is not the first to propose conducting online meta-learning over a replay buffer. A paper at last year's ICLR [1] did so in the context of lifelong learning, but does not need task labels or tasks and was tested on single non-stationary environments as well. The Meta-Critic approach of this work, of course, still is cool as it does for learning with a Meta-Critic what Meta-Experience Replay does for optimization based meta-learning. However, I thought this should be pointed out as the novelty can be a bit overstated at times. Additionally, the paper would be significantly improved by fleshing out the theoretical motivation for the Meta-Critic approach in more detail. What are the underlying reasons why we would expect it to generically improve single task RL? \n \n[1] 'Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference'. Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. ICLR-19. \n\nThoughts After Author Feedback:\n\nI really appreciate the response of the authors to my review, which included some interesting new experiments and explanations addressing concerns I raised. I do, however, also see where reviewer 3 is coming from with both major comments. \n\nI don't think that R3A1 is particularly clear and it is an important concern. I think reviewer 3 is saying that Delta(t) in R3A1i will eventually converge to 0 when the policy stops changing while the author argue that it 'need not converge if there are fluctuations at each iteration'. So it not converging seems to be tied to the existence of some source of non-stationarity in the problem. This doesn't seem to be coming from the environment as they are considering single task settings. As a result, I believe the source of the non-stationarity in this case is the fluctuating parameters. Looking at figure 6a it does not seem obvious that the policy has really converged in the traditional sense as its score does seem to be changing to some degree throughout the chart. My best guess is that this is the reason why the meta-loss does not converge. However, I still totally agree that this is a major concern that is very much under addressed. \n\nI also agree that I found the comments about what the meta-critic is doing unconvincing. The authors provided a few different kinds of explanations of what the model could potentially be doing, but this approach to the answer really highlights  how the theoretical benefits of this approach remain unclear. I think it should be possible to directly verify some of these theories with well designed experiments. It feels like a better explanation is necessary in light of the often small margin of difference with baselines before publication. \n", "belong_id": "H1lKd6NYPS"}, {"uid": "SkltZkCOtr", "paper_title": "Faster Neural Network Training with Data Echoing", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper discusses the use of data echoing (re-passing data fetched from drive or cloud) to maximize GPU usage and reduce reliance on data transportation time. The schemes basically are: reusing data at the example level, after data augmentation, or after batching. The experiments measure how much fresh data is needed in order to reach the same level of validation accuracy, with significant speedup when echoing is used. \n\nI thought the paper is very nicely motivated, although this is out of my area so I cannot comment on how thoroughly the problem of data fetching is investigated in other works. The evaluations are also nice, and appropriately uses a large model (Resnet 50) and dataset (Imagenet). \n\nThe simplicity of the method is a plus, but I question a fundamental part, especially if batch echoing is used--isn't this just the same as running SGD twice, and therefore halving the stepsize and doubling the number of steps? From the optimization viewpoint, it seems that if less data was used and a good validation error level is reached, then how do we not know that less data wouldn't work well in the first place? I understand that all step sizes and decay rates were chosen independently per experiment; can those numbers be shared in a way to see if this is happening or not? Figure 8 also suggests that though it may take a long time for the baseline to reach the same level as that with batch echoing, everyone reaches a pretty low error rate at about the same point, and the difference may be in the 'slow converging' phase of the optimization; thus measuring how long it takes to reach a specific low error rate may be an exaggerated measure. \n\nBasically, what I am saying is that the idea is nice, but the results look a bit magical. I'm happy to increase my score if the authors can upload more intermediary results, like plots of form figure 8, decay rates and schedules, batch sizes, exact repetition schedules, etc. \n\nminor: page 3 end of paragraph 2: 'repeated data would NOT be more valuable than fresh data?'\n\nAfter rebuttal: The authors have addressed my concerns and I more-or-less believe the results. I see why the contribution can be viewed as minor, but it is well-motivated and looks like a nice set of experiments. I encourage the authors to make their code available so that it can be easily incorporated in applications. ", "belong_id": "rJeO3aVKPB"}, {"uid": "HklnaKqnYH", "paper_title": "Faster Neural Network Training with Data Echoing", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose a simple method for avoiding bottlenecks during NN training, whereby training examples are utilized multiple times per read. The work focuses on cases where the cost of preparing a minibatch exceeds that of a training step (both a forward pass and, subsequently, a parameter update). Working within said regime, the authors investigate different strategies for 'echoing' examples.\n\n\nFeedback:\n  The proposed method itself is very simple: that's fine. While some cursory analysis of data echoing's theoretical implications would be appreciated, I am fine with practically motivated solutions that address real issues. Simple 'tricks' that are easy to implement and widely applicable are often useful tools. Especially when little theoretical analysis is provided, introducing such a trick requires strong empirical evidence to validate its efficacy. As it stands, failure to provide crucial information forces the reader to suspend disbelief when evaluating the proposed method's impact.\n\n  The authors seem to tiptoe around the issue of the relative cost of prefetching a batch versus that of a combined forward pass and parameter update. The work is predicated upon the assumption that the ratio of said costs $R > 1$, but the authors state that an unspecified subset of their experiments violate this assumption. What's more, real-world values of $R$ are not reported (or even measured!). As per Amdahl's law, $R$ upper bounds the potential benefits for data echoing; hence, failing to report $R$ is more than a little bit concerning. By the same token, the appropriate statistic for various result figures would seemingly be time rather than, e.g., the number of fresh examples read. As a reviewer, I would rather see evidence that data echoing provides modest benefits in realistic scenarios than x3.25 speedup in self-described 'contrived' examples. Alternatively, consider providing real-world examples where $R > 1$ to help ground your arguments.\n\n\nQuestions:\n  - Why was extensive hyperparameter tuning necessary?\n  - Why are most results reported in terms of time/steps to achieve a target value? If nothing else, consider providing the corresponding learning curves (as in Figure 8) as an appendix (incl. means and standard errors).\n\n\nNitpicks, Spelling, & Grammar:\n  - Metaparameters -> hyperparameters\n  - Streamline list at end of introduction:\n    '''\n    In this paper, we demonstrate that data echoing:\n        1. reduces the...\n    '''", "belong_id": "rJeO3aVKPB"}, {"uid": "S1eXhCVAKH", "paper_title": "Faster Neural Network Training with Data Echoing", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method to compensate the high latency brought by data IO/processing in neural network training. Specifically the authors propose to repeat training on the same subset of data during waiting time for the new data.  In this way, the data efficiency is improved, as verified by thorough experiments on various of real-world tasks.\n\nAlthough the experiments look promising, I have to say the innovation of this paper is limited. The way of reusing the current data during waiting looks more like a straightforward trick, rather than a novel idea that deserved to be published at ICLR.  Furthermore, Im wondering that how general the scenarios of t_{downstream}>t_{upstream} will be. Even in industrial level applications (e.g., billon level recommendation or click prediction task), AFAIK the training (including feedforward/backprop/communication in the distributed setting) consumes most of the time, while the data reading/preprocessing is comparatively cheap. Last but not least, what will the final performance will be given the potentially harmful consecutive reuse of data? Will it be worse than baseline?\n", "belong_id": "rJeO3aVKPB"}, {"uid": "HylapLxoKH", "paper_title": "On Universal Equivariant Set Networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "*CAVEAT*\nI must caveat that this paper is out of my comfort zone in terms of topic, so my review below should only be taken lightly. It also explaina the brevity of my review. My apologies to the authors and other reviewers.\n\n*Paper summary*\n\nThe authors design a set architecture, which is equivariant to permutations on the input. They show the simplest such set architecture, which preserves equivariance, while being a universal approximator. Nicely this architecture relies on a correction to PointNet, called PointNetST, which they show is not equivariant universal. Furthermore, they run experiments on a few toy examples demonstrating that their system performs well.\n\n*Paper decision*\n\nI have decided to give this paper a weak accept, since it contains both theory and nice experiments. To change to a firm accept, I think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area, which can easily be implemented in the camera ready stage of paper preparation. For instance, the omission of a results discussion section or a conclusion are clearly not reader friendly.\n\n*Supporting arguments*\n\n- The paper is written clearly. This said, it requires a great deal of effort to follow the maths if you are not already fluent in a lot of the ideas used in the paper (this includes myself). \n- I think the structure of the paper is fine for this sort of work. Perhaps at the beginning it would be more useful to spend more time on a roadmap of the results presented in the paper and to explain the exact significance of why the reader should want to continue reading.\n- I think the selection of experiments is nice, containing both regression and classification. What would have been nicer would be to perform some sort of ablation study, where the authors studied how the representational capacity of the network changed as a result of them introducing the universal linear transmission layer.\n- A direct theoretical and experimental comparison between PointNet and PointNetST would have been useful for me to understand the impact of the change that the authors introduce.\n\n*Questions/notes for the authors*\n\n- Please answer my concerns in the support arguments\n- Where is the conclusion section?", "belong_id": "HkxTwkrKDB"}, {"uid": "SyxwgLc0tB", "paper_title": "On Universal Equivariant Set Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents proof that the DeepSets and a variant of PointNet are universal approximators for permutation equivariant functions. The proof uses an expression for equivariant polynomials and the universality of MLP. It then shows that the proposed expression in terms of power-sum polynomials can be constructed in PointNet using a minimal modification to the architecture, or using DeepSets, therefore proving the universality of such deep models. \n\nThe results of this paper are important. In terms of presentation, the notation and statement of theorems are precise, however, the presentation is rather dry, and I think the paper can be significantly more accessible. For example, here is an alternative and clearer route presenting the same result: one may study the simple case of having single input channel, for which the output at index 'i' of an equivariant polynomial is written as the sum of all powers of input multiplied by a polynomial function of the corresponding power-sum. This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible. Generalizing this to the multi-channel input as the next step could make the proof more accessible. \n\nThe second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model. Given the fact that it applies the same MLP independently to individual set members, it is obvious that it is not universal equivariant (for example, consider a function that performs a fixed permutation to its input), and I fail to see why the paper goes into the trouble of having theorems and experiments just to demonstrate this point. If there were any other objectives beyond this in the experiments could you please clarify? \n\nFinally, could you give a more accurate citation (chapter-page number) for the single-channel version of Theorem 2.? \n", "belong_id": "HkxTwkrKDB"}, {"uid": "rke3Frfx5S", "paper_title": "On Universal Equivariant Set Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "TLDR: The function these deep set networks can approximate is too limited to call these networks universal equivariant set networks. Authors should scope the paper to the specific function family these networks can approximate. No baseline comparison with GraphNets.\n\n\n\nThe paper proposes theoretical analysis on a set of networks that process features independently through MLPs + global aggregation operations. However, the function of interest is limited to a small family of affine equivariant transformations.\n\nA more general function is\n\n\\begin{equation}\nP(X)_i = Ax_i + \\sum_{j \\in N(x_i, X)} B_{(x_j, x_i)} x_j + c\n\\end{equation}\n\nwhere $N(x_i, X)$ is the set of index of neighbors within the set $X$. It is trivial to show that this function is permutation equivariant.\n\nThen, can the function family the authors used in the paper approximate this function? No.\nCan the proposed permutation equivariant function represent all function the authors used in the paper? Yes.\n\n1) If $B=0$, then the proposed function becomes MLP.\n2) If $A=0, N(x_i, X) = [n]$ and $B_{(x_j, x_i)} \\leftarrow B$, then this is $\\mathbf{1}\\mathbf{1}^TXB$, the global aggregation function.\n\nAlso, this is the actual function that a lot of people are interested in. Let me go over few more examples.\n\n3) If $N(x_i, X) = $adjacency on a graph and $B_{(x_j, x_i)} \\leftarrow B$, then this is a graph neural network 'convolution' (it is not a convolution)\nExample adjacency $N(x_i, X) = \\{j \\;| \\; \\|x_i - x_j\\|_p < \\delta, x_j \\in X\\}$.\n\\begin{equation}\n\\text{GraphOp}(X)_i = Ax_i + \\sum_{j \\in \\{j \\;| \\; \\|x_i - x_j\\|_p < \\delta, x_j \\in X\\}} Bx_j + c\n\\end{equation}\n\n4) If $x_i = [r,g,b,u,v]$ where $[r,g,b]$ is the color, $[u,v]$ is the pixel coordinate and $N(x_i, X) =$ pixel neighbors within some kernel size, $B(x_j, x_i)$ to be the block diagonal matrix only for the first three dimensions and 0 for the rest, then this is the 2D convolution.\n\n\nAgain, the above function is a more general permutation equivariant function that can represent: a graph neural network layer, a convolution, MLP, global pooling and is one of the most widely used functions in the ML community, not MLP + global aggregation.\n\n\nRegarding the experiment metrics and plots:\n\nOn the Knapsack test, the metric of interest is not the accuracy of individual prediction. Rather, whether the network has successfully predicted the optimal solution, or how close the prediction is to the solution.\nFor example: success rate within the epsilon radius of the optimal solution while satisfying all the constraints. Fail otherwise. If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.\n\nAlso, the authors should compare with few more graphnet + transmission layer (GraphNetST) baselines with the graph layers: $P(X)_i = Ax_i + \\sum_{j \\in N(x_i, X)} Bx_j + c$ and the same single transmission layer $\\mathbf{1}\\mathbf{1}^TXB$ in PointNetST.\nPointNet is a specialization of graphnets and GraphNetST should be added as a baseline with reasonable adjacency.\n\nAlso experiment figures are extremely compact. Try using log scale or other lines to make the gaps wider.\n\n\n\n\n\n\nMinor\n\nI am quite confused with the name PointNetST. Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer. The convention is B -> B', not A + B -> A'. In this case, A: PointNet, B: DeepSet\n\nLemma 3 is too trivial.\n\nThe paper is not very self contained. Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.\n\nP.2 power sum multi-symmetric polynomials. 'For a vector $x \\in R^K$ and a multi-index ...' I think it was moved out of the next paragraph since  the same $x$ is defined again as $x \\in R^n$ again in the next sentence.\nAlso, try using the consistent dimension for x throughout the paper, it confuses the reader.\n\n", "belong_id": "HkxTwkrKDB"}, {"uid": "rJx2scAsYr", "paper_title": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This is an important contribution to understand finite depth and width corrections to the NTK. The authors show that the diagonal terms of NTK remain stochastic when depth and width approach infinity at the same rate. \n\nNTK [1] is one of the most exciting discovers for extremely over-parameterized NNs in the last year. In the single limit setting, i.e. fixing depth and letting width -> infinity, the [1] showed that the NTK converges in distribution to a deterministic kernel and remained almost unchanged during gradient descent.  This regime is known as the kernel regime or linearized regime, where the training dynamics of the NN is well-approximated by its first order Taylor expansion. \n\nIn this paper, the authors show that in the double limit regime, i.e. depth/width = \\beta and depth, width -> infinity, the diagonal terms of the NTK, as well as the first gradient step, is NOT deterministic. More precisely, they upper and lower bound the second moment of the diagonal terms of NTK (and first gradient step) through the temperature \\beta. Their method builds on the `'sum-over-path approach' developed in [3], etc. \n\nOverall, this is a very interesting result, proposing a new scaling limit that gradient descent dynamics can be highly nontrivial (i.e. not in the kernel regime.) and NNs can possibly learn useful representation. \n\nOther comments: \n1. It will be very helpful to have some experiments to support the main theorems in the paper since the proof is quite involved. \n2. How difficult is it to compute the off-diagonals? Is it possible to obtain other statistics of of the NTK, trace, max eigenvalue, etc. \n3. Is it possible to extend the results to other non-linearities, e.g. Tanh? \n\n\n[1]Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-  \neralization in neural networks. In Advances in neural information processing systems, pp. 8571\n8580, 2018.\n[2] Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient\ndescent. arXiv preprint arXiv:1902.06720, 2019.\n[3]Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.\nIn Advances in Neural Information Processing Systems, pp. 571581, 2018", "belong_id": "SJgndT4KwB"}, {"uid": "BJlnf573Yr", "paper_title": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper investigates a novel infinite width limit taking depth to infinite at the same time. This is beyond conventional theoretical studies for infinite width networks where depth is kept finite when the width is taken to be infinite. The main object that paper studies is the neural tangent kernel which is of great interest to the theoretical deep learning community as it describes gradient descent dynamics in a tractable way.\n\nWhile standard NTK becomes deterministic in the infinite width limit, when both depth and width are simultaneously taken to infinity this paper shows that NTK is no longer deterministic. Moreover authors show that gradient descent update induce non-negligible change to the Kernel. \n\nThere are two main limitations of otherwise significant work. One is generality of sums over path method used here beyond ReLU/Fully connected/single input setting. It is a very powerful technique allowing tight upper and lower bound for variation of diagonal entry of NTK. I worry that the method may be too specific to the particular network setting. Still this does not eclipse the strong results it could say for ReLU networks. \n\nSecond limitation is lack of empirical check. I understand it may be non-trivial to simulate double scaling limit and theoretical contribution alone could be significant progress. I still believe that empirical support should be a strong foundation of science of neural networks and this paper would improve even with some toy model implication of simultaneous depth/width limit. One might particularly wonder, for sufficiently deep/wide network that we could train on our computer, can we observe effects of d/n ( or \\sum_i  1/n_i)?\n\nQuestion:\nI did not quite comprehend the alluded connection to double descent curve with data-dependent features in section 3.2. Could you elaborate?\n\nnit : Dyer&Gur-Aris workshop paper is from the 2019 ICML workshop instead of 2018. \np3 sentence below eq (5) unnecessary them in the end of line.    \n", "belong_id": "SJgndT4KwB"}, {"uid": "rJxfij46tS", "paper_title": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the finite depth and width corrections to the neural tangent kernel (NTK) in fully-connected ReLU networks. It gives sharp upper and lower bounds on the variance of NTK(x, x), which reveals an exponential dependence on a quantity beta=d/n, where d is depth, and n is hidden width. This implies that when beta is bounded away from 0, NTK(x, x) is not deterministic at initialization. The paper further analyzes the change of NTK(x, x) after one step of SGD on a single datapoint x, and shows that the change also depends exponentially on beta.\n\nNTK has been a popular subject of theoretical study in deep learning, and it's an important question to understand when and to what extent NTK can capture the behavior of real neural networks. This paper makes partial progress by analyzing the diagonal entries of the NTK in fully-connected ReLU networks. It concludes that NTK(x, x) at initialization is not deterministic, and can change significantly after doing one SGD step on x, when beta is bounded away from 0. While it's nice that the authors obtained precise answers to these two questions, there are some drawbacks that limit the significance of this paper:\n\n1. The entire paper only considers one single datapoint x, so it doesn't apply to the non-diagonal elements NTK(x, x') or the usual SGD with mini-batches containing multiple datapoints. Of course, it's already implied by the current paper that the NTK is not deterministic and can move a lot when beta is large, but it's unclear whether the reverse is true, i.e., what is the regime when the NTK becomes deterministic and frozen when an entire dataset is involved. An answer (even empirically) to this question would make the paper much more complete.\n\n2. The result is also not so surprising given [Hanin, 2018] (and possibly some other papers by the same author) which also obtained a similar exp(beta) dependence using a similar combinatorial approach.\n\n3. Minor issues regarding incorrect or missing references:\n-- 'Further, kernel method-based theorems show that even in this infinitely overparameterized regime neural networks will have non-vacuous guarantees on generalization (Wei et al., 2018).' The result of Wei et al. is not about kernel method and in particular not NTK.\n-- 'In fact, empirically, networks with finite but large width trained with initially large learning rates often outperform NTK predictions at infinite width.' The authors should refer to the work of [Arora et al., 2019] which AFAIK is the first paper that provides empirical study of the (convolutional) NTK predictor at infinite width on relatively large datasets like CIFAR-10.\n\n[Hanin, 2018] Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?\n[Arora et al., 2019] On Exact Computation with an Infinitely Wide Neural Net\n\n\n--------\nupdate: Thanks to the authors for the detailed reply, which answers most of my questions. I am updating my rating to weak accept in light of this.\n\nRegarding Wei et al.: Their result on NTK is negative, i.e., they aimed to show that NTK is inferior to regularized NN. I find the way you cited this paper a bit weird. ", "belong_id": "SJgndT4KwB"}, {"uid": "Syl2HTO2tB", "paper_title": "UNIVERSAL MODAL EMBEDDING OF DYNAMICS IN VIDEOS AND ITS APPLICATIONS", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper considers the problem of extracting the underlying dynamics of objects in video frames. The paper focuses on two major applications: background/foreground extraction and video classification. The paper proposes a method that first obtains latent vectors from a video sequence by training a neural network and then applies dynamic mode decomposition (by Schmidt, 2010). \n\nThe paper is not well written and even after reading it the second time I, unfortunately, have difficulties understanding the exact contributions and experiments.\nHere are concrete examples that lead to this critique:\n- Section 2: Letters are not defined (e.g., \\mathcal F is not defined when first used), and sentences are not finished and/or do not make sense.\n- Sec. 5.1: not clear whether the experiment is only done for one sequence or for many. If done on only one sequence, this is not sufficient to demonstrate that the method works well, if done on more than one then the results are not reported.\n- Conclusion: States that ``this method can be applied to any multivariate-time series data to extract complex and non-linear dynamics''. That statement sounds overlay general given the experimental evaluation.", "belong_id": "H1lkYkrKDB"}, {"uid": "rJxRE-91cS", "paper_title": "UNIVERSAL MODAL EMBEDDING OF DYNAMICS IN VIDEOS AND ITS APPLICATIONS", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I found the topic of this paper interesting and I believe I understand what the authors are trying to achieve but I'm afraid this was after several readings and I do think the paper could be presented differently that would make it more accessible. My suggestion would be to explain how the model will be applied first (identify the required properties) to motivate the need for the learned basis and then present the DMD as a method for providing a basis that meets the properties required. I acknowledge that different communities have different styles of presentation so apologies if this is just me.\n\nFirst I would just like to check that I have understood correctly so please could the authors point out if I have missed something or misunderstood in the following?\n\nOur goal is to establish a basis invariant to the video dynamics that can then be used, for example, to partition the video into parts with differing dynamics - e.g. foreground/background. To do this we need to identify such a basis from a specific video - we will use the collection of pairs of neighboring frames.\n\nThe Koopman operator acts on a differential system to identify a function space invariant to the dynamics. If we instantiate this with a finite number of dimensions we can essentially establish the invariance as an eigenvalue problem. From this and our pairs of successive videos we can establish a vector basis for the space and then project the video into this basis. The spectral properties of the coefficients of the projection will determine whether something is static (omega = 0) or transitory in the scene and these can be used to identify foreground and background.\n\nNext there is the issue that this method operates in a linear domain with something like Gaussian noise which is not a good fit for image space videos so the authors propose to identify the dynamics in a linear latent space determined by an autoencoder to handle the non-linear mapping to image space.\n\nI hope I have understood the main points?\n\nIf this is the case, I think that much more needs to be said about the second part, which is the essential novelty of the paper, with a discussion of the merits of different approaches and full details - at the moment there is just one small paragraph at the end of 4.2 which contains the majority of the contribution.\n\nMy main concern about the paper is that I find it very difficult to appreciate the efficacy of the method given the current presentation of the results. There are no error bars to ascertain significance for any of the results and the summarization of multiple experiments to a single percentage gives very little insight into where this method works and where it doesn't. There are a number of ways that a dynamic prior could be added to a latent space and it is unclear why we would expect this approach to be preferred given the evidence presented in the paper.\n\nOther Notes:\n\nI found that the notation is not always consistent and sometimes could be simplified - it is unclear whether some operators are convolutions or multiplications (vector or scalar). To me the asterisk does not represent straight forward multiplication but it might be being used for this?\n\nCould Table 1 be placed in the experiments section rather than in the middle of page 5?\n\nDo the authors mean half the number of pixels or half the edge size (e.g. a quarter of the area) in terms of the latent space?\n\nPlease can all equations be numbered so that they can be referred to - there are no equation numbers in all of section 2.", "belong_id": "H1lkYkrKDB"}, {"uid": "SyeTbYISqH", "paper_title": "UNIVERSAL MODAL EMBEDDING OF DYNAMICS IN VIDEOS AND ITS APPLICATIONS", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an application of convolutional autoencoder networks and a nonlinear dynamic systems analysis method known as extended dynamic mode analysis (EDMD) to a data-driven analysis of multivariate time series.  \nThe DMD method appears to be well-known in the physics community but is outside my area of expertise and unfortunately I have limited time to make a quick study of it.  However, from what I gather, it involves empirical approximation of a nonlinear dynamical system as a high-dimensional linear dynamical system, which in turn enables analysis in terms of eigendecomposition of the resulting linear operator, revealing basic modes of the dynamics.   In the proposed method, DMD is used in the latent representations of a convolutional autoencoder for image sequences.   The DMD objective is incorporated into the autoencoder training loss to minimize its reconstruction error.    The DMD is also used, by conditioning on the eigenvalues, to split the reconstruction into high frequency (quickly varying) foreground modes and low-frequency (slowly varying) background modes.   Although end-to-end training is mentioned, it is not made clear how the derivatives of the DMD decomposition are implemented, especially considering that the DMD involves an SVD, which can have unstable/ singular derivatives when two or more singular values are close to the same / exactly the same.   The resulting methods are applied to a foreground extraction and a classification tasks, and compared with numerous baselines.  It is not clear to me what the state of the art is on these tasks, but the proposed methods compare favorably to reported baselines, and the images of results look convincing.   However the experimental results seem a little thin and I would expect a more thorough study.  Overall the method looks very interesting.  \n\nSome complaints:\n -  the tables are a bit sloppy and should be formatted to fit in the document with normal sized fonts, \n - the images are too small to see well.\n", "belong_id": "H1lkYkrKDB"}, {"uid": "B1egi4UoYH", "paper_title": "Empirical confidence estimates for classification by deep neural networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: This paper proposes an uncertainty measure called an implied loss. The authors suggest that it is a simple way to quantify the uncertainty of the model. It is suggested that 'Low implied loss (uncertainty) means a high probability of correct classification on the test set.'. They suggest that the analysis of an implied loss justifies the maximum confidence value of softmax-cross entropy. They also extend to evaluate Top-k uncertainty (the uncertainty whether our prediction is in the Top-5 maximum values of our confidence score or not). \n\n========================================================\nClarity:\nI found that this paper content does not seem to be difficult mathematically, however, it is difficult to follow the paper and here I list several parts that can potentially be improved:\n\n1. INTRO: the sentence 'Our implied loss interpretation justifies both methods, since we demonstrate that 'both these quantities' are uncertainty measures.'. What is both quantities here, the maximum softmax probability and something? \n\n2. INTRO: first contribution, accurate estimates of the probability that the classification of the model on a test set is correct. What do you mean by this sentence? I couldn't see the accuracy of the estimation in Table 2.\n\n3. second contribution in INTRO: what is the meaning of 'a consistent manner' here. If it means 'successfully', is there a case that your method fail? It would be nice to be precise when describing the contribution. Figure 2 is more like a small example but can be considered difficult to convince to the readers about the capability of the proposed method.\n\n4. Figure 1 can be much improved. I found the caption is hard to understand. Loss (y-axis) may be written as Kullback-Leibler loss to be more precise in this context (if I understood correctly).\nminor comment: Figure 1(a) [colon missing], since Figure 1(b) has ':' right after. The authors made an effort to explain the color in the caption of Figure 1(b) but the explanation of (yellow) is missing. The explanation of how to train a network to get Figure 1(a) seems to be missing. It seems -log(f_(1)) should be -log(f^{sort}_(1)). Finally, I'm not sure what is P(Correct|x). Is this histogram suggested that when the U_1 is large and the P(Correct|x) is small, we get a correct prediction, or maybe this means the ratio of correct prediction under the value of loss (histogram)? I think it would help the reader to make it more concise. In the description in page 4 -\\log(p_{max}) seems never define before, was it a typo?\n\n5.1 First page, last sentence: what do you mean by the current setting. Before this point there is no explanation about the problem setting, only we are interested in quantifying an uncertainty measure of the networks.\n5.2 First page, last sentence:  I'm not familiar with Bayes factors, is the last sentence your contribution or it's the finding of the existing work, if it's the latter one, it would be nice to cite them. However, I found this sentence a bit vague: Bayes factor more informative (not sure what is the definition of informative here) better than Brier score under the situation where prob of correct classification is high (is this means high accuracy on the test set?).\n \n6. PRIOR-WORK: Although the authors suggested many existing works, it would be highly useful if the authors discuss the relationship between existing works and their proposed work, e.g., where to put your work in the literature. And since they proposed an uncertainty function, it would be nice to see a few definitions of uncertainty existing works described (doesn't need to be mathematical formulation, I think just an intuitive explanation is sufficient).\n\n7. I am confused with the definition of an implied loss. It is first defined in page 3 with a fixed k (as 1) as a loss where the prediction is correct but in Eq. (4), it looks like a set with one element where y is the maximum prediction score, not a correct label. Then there is U_k(x), if k!=1, is this an implied loss? Although in (5) it is a real number, not a set anymore, I read this paper and took it as a 'yes U_k(x) is also an implied loss'. Also it would be better to define Kullback-Leibler here to be concise and kind to the readers. Then in def 4.1, it's mentioned that the uncertainty measure U_k(x) is an implied loss if the event has high expected loss, does that mean if the event has low expected loss, it is not an implied loss? My opinion is that the authors may use a definition environment and define precisely what is an implied loss. For example, given '\\ell: X x Y \\to R, a correct label y \\in Y, and an integer k <= K (number of classes), an implied loss is defined as' to avoid confusion.\n\n8. Def 4.1: U(x) without subscript is undefined (perhaps U_k(x)?). What is an element of the set S^\\eps_k? If it is a set then what is the meaning of the event S has a high expected loss? Does the definition of implied loss after Eq. (6) and the definition of implied loss in Eq. (5) identical when it is Kullback-Leibler loss?\n\n9. Theorem 4.2: S^\\eps seems to be undefined without k. Moreover, how to interpret the bound in (7), it would be nicer to explain the bound after stating the theorem. It is only an inequality that says the left-hand side is smaller or equal to the right-hand side. And does (7) hold for any y? And how tight is the bound?\n\n10. Remark 4.3: what is e_k? what is k here if k in U is set to one? And the name of the remark, how to interpret (8) as 'Neural networks are always overconfident?' Is this about neural networks or this apply to any function?\n\n11. Sec 5: Figure 6 is not in the main body but the appendix... If this a mistake (and the paper is supposed to be 9 pages without ref.) or it is supposed to be in the appendix? If It's in the appendix, it would be better to mention that this figure is in the appendix.\n\n12. Sec5: since Bayes factor is highly used in this paper to motivate the use of the measure, I don't think it is a good idea to put the explanation of Bayes factor in the appendix, i.e., it is impossible to understand this paper without knowing Bayes factor. \n\n13: Fig 6: I think it is better and kinder to use U_1, U_5 in the legend of the plot instead of f. What is the model entropy?\n\n14. Table 1: why there is '-' in CIFAR-10, it is better to clarify it in the paper (or maybe I missed it). I am not sure how to interpret the result, if the higher the better, does that mean the Loss is great?, I'm confused with the experimental results.\n\n15. Before the beginning of 6.2: Tables 7 and 6 are in the appendix and we should state clearly it is in the appendix.\n\n========================================================\nComments:\nThis paper lacks of clarity and difficult to understand. Although it is claimed to be better than existing measure, I am not convinced about that despite many experiments were conducted unfortunately. \nFor the criticism of using the maximum confidence of the softmax score from the softmax-cross entropy loss may not quantify the uncertainty, it is known theoretically that the score of softmax-cross entropy corresponds to p(y|x) if our prediction function achieve the global minimizer this loss function and our function class to be considered is all measurable functions (Zhang, JMLR2004: Statistical Analysis of Some Multi-Category Large Margin Classification Method). For other losses, see Williamson+ JMLR2016: Composite Multiclass Losses. However, it may not be accurate empirically when we use a deep network as it is reported in Guo+, 2017. Thus, one direction is to do post-processing or finding a way to modify a network. For U_1(k), I feel that it should suffer from the same problem as using maximum confidence score of softmax. Extending to top-k may have a good point when discussing about uncertainty and I believe it is good to explore that direction. For experiments, I would like to know how many trials did the authors run the experiment? and it would be helpful to see the standard deviation of the reported value. I believe this paper can still be improved a lot. For these reasons, I vote to reject this paper this time.\n\n========================================================\nMinor comments:\nthere exists the writing convention of 'Top 5', 'top 5', 'top5'. It's better to pick one way to describe it if there is no reason to make it different.\n========================================================\nAfter the rebuttal:\nI have read the rebuttal.\n\nI appreciate the authors' effort to modify the paper. \nAlso, please let me state that I totally agree that the problem the authors try to solve is indeed important and relevant for using machine learning in the real-world. \n\nI feel that the structure of the modified version is better than the first version. It is a nice to include the explanation of the Bayes factor in the main body. Appendix C is also very useful for everyone to understand the Bayes factor. \n\nAs the author requested, I have read through the whole revised paper (including the appendix) carefully . I am aware of the positive sides of this paper. For example, it is interesting that we can find wrongly labeled data. Utilizing the uncertainty information for several applications. However, I found that the paper still requires a lot of modifications. The authors have modified many of my concerns, but still several of them were not addressed. I also emphasized the comments for parts that are unrelated to clarity (please see below). For these reasons, I decide not to change my score. \n\nBelow are my comments after reading the rebuttal (which some of them may be overlapped with the issues that were not addressed in the revised version).\n============================\nGeneral comments:\n\n1. Although the work is about tackling the empirical confidence estimation problem, Theorem 3.4 and Eq. (5) are providing insights about the population, not finite data points for empirical estimation. If we focus on the population case, it is known that the minimizer of the softmax cross-entropy loss must be a conditional probability $p(y|x)$. Thus, it is natural that as we minimize such a loss, the probability of correct classification must be high, since we can pick the best choice for classification, i.e., argmax of $p(y|x)$. But the most challenging part of the research in this direction is that, although the theory suggests we can get nice confidence information (in population), when we use the deep neural networks, the quality of confidence estimation can be very bad (overconfidence in empirical estimation) compared with the high accuracy we can achieve. As a result, a finer theory that can quantify the quality of confidence estimate for the finite sample case is highly needed, but I think Theorem 3.4 fails to capture this. I am aware that this theorem only concerns the KL loss. I believe that even only for KL-loss, the result can be significant if we discuss a finer theory for empirical estimation.\n\n2. Regarding the Remark 3.5 (Neural networks are always overconfident), in my understanding, the result has no relationship with neural networks at all since it is true regardless of the hypothesis class of interest (e.g., linear models, kernel models, deep networks). This is because it is simply the definition of a loss function (and the result in the paper focus on KL loss, but I believe we can derive for many other losses). We know that the quality of confidence estimation of simple models can be better although the accuracy is worse. Thus, if the objective is to visualize the problem of neural networks, Remark 3.5 does not seem to help and adding neural networks in the remark title can be misleading. Thus, the implication of Remark 3.5 is insufficient to state that Neural networks are always overconfident.\n\n3. What is the advantage of an implied loss? It seems the paper has two separate stories, the first one is implied loss (Sec. 3) and then move on to Bayes factor (Sec.4). Then, there is an adversarial detection problem using the gradient norm in the last experiment. From Sec. 4, the discussion about implied loss is very limited and if I understand correctly, the $-\\log p_\\max$ and $-log\\sum p_{1:5}$ (the latter seems to require a superscript $sort$) are the implied loss, which does not seem to have the clear advantages over other methods. My impression is that the contributions of this paper are unclear and I do not know what is the main point of this paper. While the abstract dedicates most space to highlight the implied loss (nothing about Bayes factor), the conclusion dedicates most space to highlight the Bayes factor. Improving the connection between two parts may help to signify the contributions. Despite all that, I do like the idea of introducing the Bayes factor in this paper. \n============================\nClarity:\n\n1. Most importantly, I think the clear and solid definition of implied loss is missing.\nAccording to Definition 3.1, 'The uncertainty measure $U_k(x)$ is an implied loss if the event $S^\\epsilon_k$ has high expected loss'. I believe the implied loss is one of the most important contributions of this paper. I am not sure what does it means by 'if the event $S^\\epsilon_k$ has high expected loss' How can we define 'high expected loss'?. I tried to read the paper many times to understand what exactly is the implied loss, and what is the scope of implied loss (what is and what is not an implied loss).\n\n2. Following my first issue on clarity, what is the definition of uncertainty measure in this paper? According to the paper, it is defined roughly as $U_k$, whose statistics allow us to better estimate $p_k$. And in the abstract, it is emphasized that if uncertainty measure is an implied loss, then low uncertainty means a high probability of correct classification. Is there any uncertainty measure that is not implied loss? Also in the introduction, it seems that both softmax and the model entropy are uncertainty measures, are they implied losses? Is MC-dropout an uncertainty measure or even an implied loss in this context?\n\n3. Eq. (11) left side: I think it is useful to the expectation with respect to which variable, I believe it is $B$. As a result, is it a typo to have $Y_i$ instead of $B_i$ on the left-hand side of (11)?\n\n4. Example 3.3: If we set $k=1$ according to the definition of the implied loss at the last sentence of this example. Will it contradict $U_1(x)$ defined just right before that sentence? Because $y_w$ will become the $2$-nd ranked label, which I feel it is different from $U_1(x)$ defined in Example 3.3\n\n5. I think it is better to clearly state that the figures/tables are in the appendix when referring to them from the main body. I saw the authors refer to Table 5 in Sec. 4.1, Figure 5 in Sec. 4.2 and Tables 6 and 7 in Sec. 4.3. All of them are in the appendix. And it seems some of them are highly needed. For example, the authors said that 'by fine graining the bins we can capture relatively small ... on the order of 20' (before Sec. 4.3), then suggested the reader to see Figure 5. I feel Figure 5 must be included in the main body because it is hard to understand that without seeing the figure. \n\n6. I think all figures that have $f_{(1)}$ (Figures 1a, 1b, and 5) must have a superscript $sort$ for all of them. Otherwise, it is wrong.\n\n7. Kullback-Leibler loss is used extensively here without definition. It is important to clarify the clear definition of it. $U_1(x)$ also used extensively for the KL loss case and non-KL loss cases. This can make the paper hard to read. I suggest using $U^{\\mathrm{KL}}_1(x)$ when referring to the uncertainty measure with respect to the KL-loss.\n\n8. I saw $-\\log(p_\\max), -\\log(f_{(1)}), -\\log(f_1^{sort}), U_1$. Are these all refer to the same thing? And I found that sometimes the argument $(x)$ is ignored in the paper sometimes it doesn't in a quite random way. If they are the same, it would be nice to unify them.\n\n9. The caption of figure 2(b) is uninformative. The authors may consider improving it.\n\n10. It would be very helpful to add the implication or interpretation of the theoretical results to help the readers understand the intuition of the proven results. For example, how does Eq. (3) implies that when small uncertainty implies high chance of correct classification.\n============================\nMinor typos:\nINTRO: 'uncertainly measures' -> 'uncertainty measures'\n5.2: 'on-distribution' -> 'in-distribution'\nConclusion: logpmax -> write clearly with $$ should be better\n\nMinor comments in the appendix:\n1. How does Appendix A related to implied loss or Bayes factor in this paper? Did I miss something?\n2. Figure 5:\n\t2.1 y-axis: is it Bayes ratio or Bayes factor? It seems the Bayes ratio and Bayes factor to be a different thing. Even if it is the same in some literature, I think it's better to use the Bayes factor here for the consistency of this paper. \n\t2.2 Caption: 'entropy' -> 'model entropy'. \n\t2.3 Caption: Is it mistakes or do you want to insist on using $U_1$ and $U_5$ in the caption but using different notions in the figure? (in figure, they are $-\\log(f_{(1)})$ and $-\\log(\\sum f_{(1:5)})$).\n3. Appendix C: Eq. 16 and Eq. 19: I believe there is a typo. I think it should be $BF(X|Y_1), BF(X|Y_2), BF(X|Y_3)$, respectively.\n4. Appendix C: Eq. 18-right: does it need to sum to 1 not 0.3+0.5+0.3 = 1.1?\n", "belong_id": "Hke0oa4KwS"}, {"uid": "Hkgur3Lg9B", "paper_title": "Empirical confidence estimates for classification by deep neural networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper provides a comparison of several uncertainty measures that have been proposed for neural networks. The goal of the work is to alleviate the lack of clear interpretability of softmax outputs as measures of uncertainty in neural networks for multi-class classification (in particular, here image classification). The authors propose the use of expected Bayes factors as aggregate measures of how much information is conveyed by a measure of uncertainty, and compares several proposed approaches based on this measure on CIFAR-10/100 and ImageNet-1k. The authors also discuss the application of uncertainty measures to detect mislabeled or ambiguous images, detecting out-of-distribution samples and adversarial examples. Passing by, the authors propose a measure based on the norm of the gradient to detect adversarial examples that seems to work well. \n\nThere are interesting ideas in the paper. The use of Bayes factors for measuring the quality of the uncertainty estimates, and the comparison of various existing methods on that measure seems useful. The experiments on various tasks also have their own merits.The extension to top-k uncertainty is also interesting.\n\nOn the other hand, I found the paper difficult to follow because the contributions are scattered over the paper and the appendices without clear link. A lot of space is devoted to the definition of the implied risk, which does not bring much to the overall interpretation of the results. The criterion for detecting adversarial examples based on the norm of the gradient appears at the end of the paper somewhat independently from what was before, and the definition and computation of the Expected Bayes factor is entirely deferred to Appendices, which makes the main paper not really self-contained.\n\nThe paper also lacks a conclusion. The various uncertainty measures seem to perform differently depending on the datasets, and it is unclear what the authors recommend in the end. \n\nOverall, it seems to me that with a bit of restructuration, the paper would be an interesting contribution -- for instance in terms of the assessment of dropout variance vs direct model entropy. It seems to me though that for now the paper lacks coherence and clarity in the message.\n\n\n====== after author rebuttal\n\nThe rebuttal answer most of my concerns, and I raised my score to weak accept. Overall, even if there is no clear evidence that one method to evaluate uncertainty is consistently better than the others, I feel that the idea of using Bayes factors to compare uncertainty measures is possibly impactful.\n", "belong_id": "Hke0oa4KwS"}, {"uid": "rkewbIiu9B", "paper_title": "Empirical confidence estimates for classification by deep neural networks", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper shows that the maximum softmax probability is useful for uncertainty estimation on in-class data and not just for detecting out-of-distribution data. They argue this with the Bayes Ratio, which here is an uncertainty estimation quality measure that seems worth exploring more for assessing the quality of uncertainty estimation techniques. The community is still in need of a good uncertainty estimation measure (Brier score is too tangled with accuracy, has low numerical resolution, and hardly penalizes consistent overconfidence; AUROC for error detection doesn't budge; the Soft-F1 score is without theoretical motivation; area under the response rate recall curve is too close to 1 when accuracy is high), and this could be it. They evaluate on ImageNet-1K, which most uncertainty estimation papers fail to consider. By considering CIFAR-10, CIFAR-100, and ImageNet, we get fuller picture of the ranking of the utility of many uncertainty estimators, which is somewhat important for the community. This paper is currently borderline, in that what it proposes is simple, but perhaps too simple. Its empirical contributions are fairly minimal, though its proposal of the Bayes Ratio for uncertainty estimation quality assessment could quite impactful.\n\nSmaller Notes:\nFigure 1 should be on page 3 not 2. \n\nSome content in Appendix B should be incorporated in Section 5.1\n\n> In the odds have increased\nIf\n\nB.3 should have a worked example in our setting as well\nEqn (19) should say > 65 not <65\n\nNotation of Y is confusing when it means a binning of U. Perhaps use 'B'?\n\nIs the histogram adaptively computed? Is that what chosen to have equal weight means?\n\nPerhaps compare the Bayes ratio with RMS Calibration Error, AURRA, or AUROC for error detection.\n\nUpdate: The other reviewers are concerned about lack of clarity which is separate from why I like the paper.", "belong_id": "Hke0oa4KwS"}, {"uid": "Hyeeki6nKB", "paper_title": "Multi-Agent Interactions Modeling with Correlated Policies", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this work, a multi-agent imitation learning algorithm with opponent modeling is proposed, where each agent considers other agents expected actions in advance and uses them to generate their own actions. Assuming each agent can observe other agents actions, which is a reasonable assumption in MARL problems, a decentralized algorithm called CoDAIL is proposed. For each iteration of CoDAIL, (1) each agent trains opponent models (other agents policies) by minimizing either MSE loss (continuous actions) or CE loss (discrete actions), (2) samples actions from those opponent models, (3) updates individual rewards (discriminators) and critics and (4) updates policies with multi-agent extention of ACKTR (which is used in MA-GAIL and MA-AIRL as well).\n\nThe experiments in the submission show that there is a significant gain relative to baselines (MA-GAIL and MA-AIRL) in OpenAI Multiagent Particle Environments (MPE) in terms of (true) reward differences and KL divergence between agents and experts state distributions.\n\nI think the empirical contribution of this work is clear to be accepted, but I give Weak Accept due to the following comments:\n\n- I think theres a similarity between Theorem 6 in MA-GAIL paper and Proposition 1 in the submission. I hope the difference between Proposition 1 and Theorem 6 to be clarified. \n\n- Proposition 2 seems to me redundant because its neither important for theoretical analysis in 3.3 nor for the experiments. I believe a few sentences are enough to describe why authors choose \\alpha=1 (or equivalent explanations).\n\n- The authors suppose fully observable Markov Games in the paper, but it makes me confused when I consider the experiments in the submission. For example in Cooperative Navigation, each agents observation includes (1) position vector relative to agents and landmarks and (2) their own velocities (which cannot be observed by other agents directly). Since authors argue CoDAIL is a decentralized algorithm, I think agents are not allowed to use others observation for opponent modeling, but it seems that agents fully utilize others observations. I hope it to be clarified and if thats the case, I wonder if we can regard CoDAIL as a decentralized method. \n\nIm willing to increase my score if my questions are clearly answered. ", "belong_id": "B1gZV1HYvS"}, {"uid": "B1gXtGEb5S", "paper_title": "Multi-Agent Interactions Modeling with Correlated Policies", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors propose a decentralized adversarial imitation learning algorithm with correlated policies, which recovers each agents policy through approximating opponents action using opponent modeling. Extensive experimental results showed that the proposed framework, CoDAIL, better fits scenarios with correlated multi-agent policies.\n\nGenerally, the paper follows the idea of GAIL and MAGAIL. Differing from the previous works, the paper introduces \\epsilon-Nash equilibrium as the solution to multi-agent imitation learning in Markov games. It shows that using the concept of \\epsilon-Nash equilibrium as constraints is consistent and equivalent to adding the difference of the causal entropy of the expert policy and the causal entropy of a possible policy in RL procedure. It makes sense. \n\nBelow, I have a few concerns to the current status of the paper.\n\n1.\tThe authors propose \\epsilon-Nash equilibrium to model the convergent state in multi-agent scenarios, however, in section 3.1 the objective function of MA-RL (Equation 5) is still the discounted causal entropy of policy, the same as that of MA-GAIL paper. It is unclear how the \\epsilon-NE is considered in modeling MA-RL problem.\n\n2.\tRather than assuming conditional independence of actions from different agents, the authors considered that the joint policy as a correlated policy conditioned on state and all opponents actions. With the new assumption, the paper re-defines the occupancy measure and introduces an approach to approximate the unobservable opponents policies, in order to access opponents actions. However, in the section 3.2 when discussing the opponents modeling, the paper did not clearly explain how the joint opponent function \\sigma^{(i)} is designed. The description \\sigma^{(i)} is confusing.\n\n3.\tTypos: in equation 14 i or -i; appendix algorithm 1 line 3 pi or \\pi. \n", "belong_id": "B1gZV1HYvS"}, {"uid": "rJezMXKS5r", "paper_title": "Multi-Agent Interactions Modeling with Correlated Policies", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to model interactions in a multi-agent system by considering correlated policies. In order to do so, the work modifies the GAIL framework to derive a learning objective. Similar to GAIL, the discriminator distinguishes between state, action, next state sequences but crucially the actions here are considered for all agents.\n\nThe paper is a natural extension of GAIL/MA-GAIL. I have two major points that need to be addressed.\n\n1. The exposition and significance of some of the theoretical results is unclear.\n- The non-correlated and correlated eqns in 2nd and 3rd line in eq. 8 are not equivalent in general, yet connected via an equality.\n In particular, Proposition 2 considers an importance weighting procedure to reweight state, action, next state triplets. It is unclear how this resolves the shortcomings of pi_E^{-1} being inaccessible. Prop 2 shifts from pi_E^{-1} to pi^{-1} and hence, the expectations in Prop 2 and Eq. 11 are not equivalent. \n- More importantly, how are the importance weights estimated in Eq. 12? The numerator requires pi_E^{-1}, which is not accessible. If the numerator and denominator are estimated separately, it becomes a chicken-and-egg problem since the denominator is itself intended to be an imitating the expert policy appearing in the numerator?\n\n2. Missing related work\nThere is a huge body of missing work in multi-agent interactions modeling and generative modeling. [1, 2] consider modeling of agent interactions via imitation learning and a principled evaluation framework of generalization in the Markov games setting. By sharing parameters, they are also able to model correlations across agent policies and have strong results on generalization to cooperation/competition with unseen agents with similar policies (which wouldn't have been possible if correlations were not modeled). Similarly, [3, 4] are other similar works which consider modeling of other agent interactions/diverse behaviors via imitation style approaches. Finally, the idea of correcting for the mismatch in state, action, next state triplets in Proposition 2 has been considered for model-based off-policy evaluation in [5]. They proposed a likelihood-free method to estimate importance weights, which seems might be necessary for this task as well (re: qs. on how are importance weights estimated?).\n\nRe:experiments. Results look good and convincing for most parts. I don't see much value of the qualitative evaluation in Figure 1. If the KL divergence is low, we can expect the marginals to be better estimated. Trying out various levels of generalization as proposed in [2] would significantly strengthen the paper.\n\nTypos\nsec 2.1 Transition dynamics should have range in R+\nProof of Prop 2. \\mu instead of u\n\nReferences:\n[1] Learning Policy Representations in Multiagent Systems. ICML 2018.\n[2] Evaluating Generalization in Multiagent Systems using Agent-Interaction Graphs. AAMAS 2018.\n[3] Machine Theory of Mind. ICML 2018.\n[4] Robust imitation of diverse behaviors. NeurIPS 2017.\n[5] Bias Correction of Learned Generative Models using Likelihood-free Importance Weighting. NeurIPS 2019.", "belong_id": "B1gZV1HYvS"}, {"uid": "S1geYsQnYH", "paper_title": "MMD GAN with Random-Forest Kernels", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overview: \nThe paper propose an MMD GAN extension via using Random forest Kernel.  Instead of using Gaussian kernel on the top of the learned embeddings from the discriminator, it combines existing deep forests kernels. The theory of being differentiable is carefully studied (to prove zero measure) and the experiments are well conducted.  \n\n1.  Some important  references are missing.  One very related paper is \n\n* Li et al., Implicit Kernel Learning,  AISTATS 2019. \n\nThat paper is using the same idea to learn to manipulate the random features on the top of the learned embedding.  The main difference between it and the proposed algorithm is they use MLP parameterization instead of the tree-based model.    Also, the deep forest model can be treated as a sparse neural network, does it have more advantage over Li et al., (2019)? given they use simple dense MLP.   Please at least discuss the similarity and difference in the rebuttal and update the draft correspondingly.  I would even encourage the author to empirically compare with it in the camera ready version.  It would be interesting to see which parameterization is better in this space. \n\nThere are also other recent MMD GAN extensions should be cited in the discussion, such as \n* On gradient regularizers for MMD GANs. \n\n2. For the theory part, based on Binkowski (2018), the gradients for the generator parameters should be biased. Could you discuss it with Theorem 2? \n\n3. For most MMD GAN results, one important property in Li et al., (2017),  Arbel et al., (2018) and Li et al., (2019) is weak* topology.  Does the proposed Random Forest MMD GAN also has that property? In Li et al., (2019), they need some condition to ensure that, how's case in the proposed algorithm?  ", "belong_id": "HJxhWa4KDr"}, {"uid": "rkxC0wKhFH", "paper_title": "MMD GAN with Random-Forest Kernels", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Theorem 2 and its proof are plagiarised: they are rephrased and reorganized formulation and proof of Theorem 1 of [1], while being presented as authors' own work. \n\nAlthough the assumptions are slightly different (random forest kernels vs general kernels), core of the proof is the same, including notation and its split into Lemmas and helper Theorems. In particular:\n- formulation is the same (even use of MMD_u is copied, while not being defined before),\n- main proof of Theorem 2 (p.25-26) is the proof of Corollary 3 of [1] followed by the proof of Theorem 5 of [1],\n- Lemma 13 is Lemma 3 of [1],\n- Definition 3 in Appendix B.2 is the same as Assumption D of [1] (Appendix C.2),\n- Proposition 4 and it's proof (p. 21) are the same as Lemma 2 of [1].\n\n[1] Mikoaj Binkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. International Conference on Learning Representations, 2018.", "belong_id": "HJxhWa4KDr"}, {"uid": "HJx3j8eaFr", "paper_title": "MMD GAN with Random-Forest Kernels", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed to utilize the random-forest kernel into MMD GAN.  The experiments are conducted on CIFAR-10, CelebA and LSUN datasets. \n\nThe method is not novel. Both MMD GAN and the random-forest kernel have been well explored. Combining them together is considered as an extension. For the theory, the paper only provides the unbiasedness analysis. It is not clear to me whether this kernel is better than other MMD GAN variations. It is not clear how the claimed flexibility comes from. \n\nRegarding the experiments, it only compares with very basic baselines and the results are not significantly better. It would be better to include stronger baselines (Wang et al., 2019, Binkowski et al., 2018).\n\nThe writing of the paper is poor. with several typos. Moreover, as mentioned by reviewer #1,  theorem 2 and its proof are plagiarised. \n\nOverall, I think the paper is a clear reject. ", "belong_id": "HJxhWa4KDr"}, {"uid": "SJlLFPEjFr", "paper_title": "SNOW: Subscribing to Knowledge via Channel Pooling for Transfer & Lifelong Learning of Convolutional Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper attempts to tackle transfer learning and lifelong learning problem by subscribing to knowledge via channel pooling. The channel pooling is actually selecting the subsect of the feature map according to the way that prediction accuracy from the delta model can be maximized. Experiments show effectiveness of the proposed method.\n\nPros:\nOverall, this paper is well written and easy to follow. The technique is sound and the problem studied in this paper is significant.\n\nCons:\n1.\tI do not think that the model proposed in this paper is able to tackle lifelong learning problem. The main reason is that lifelong learning basically requires only one model that will continue to learn from new tasks. After learning several new tasks, people hope this model can still perform well on the previous tasks as well as the current ones. However, in this paper, not only one model is learned. Instead, new models appear when new tasks are given, which does not meet the definition or requirement of lifelong learning. It only meets the requirement of transfer learning. The experimental results also validate my opinion since only one new task is given while in lifelong learning, continuous new tasks will come and the original model should perform well on all of them as well as on the old tasks.\n2.\tIn Figure 4, the legend in the first picture will confuse the readers. I suggest the authors put it outside all the figures. Besides, the proposed method in the last picture is not the best. What do the authors want to convey by this picture?\n\n", "belong_id": "rJxtgJBKDr"}, {"uid": "rygKCo52KH", "paper_title": "SNOW: Subscribing to Knowledge via Channel Pooling for Transfer & Lifelong Learning of Convolutional Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "*** Increased to Accept from Weak Accept after author rebuttal and changes to the paper ***\n\nThis paper proposes a method, SNOW, for improving the speed of training and inference for transfer and lifelong learning. SNOW starts with a pre-trained, frozen source model, and trains delta models for target tasks which, at each layer, concatenate a small number of task-specific features with the top-K most useful subset of features in the corresponding layer in the source model. As long as the target tasks are sufficiently related to the source task, it allows for small delta models and a small additional parameter overhead in the form of one weight per source model feature map. While there are (i) some issues with the presentation of results for training efficiency, (ii) some question marks over the sensitivity of the model to hyper-parameters, and (iii) several grammatical errors / typos in the manuscript, if these can be addressed I recommend the paper for acceptance because it seems to strike a superior balance of efficiency (regarding memory usage and inference speed) and accuracy when compared to a number of baselines, and to my knowledge it is a novel approach.\n\nDetailed comments:\n* Section 2.2 - How is sigma (the exploratory noise added for feature selection during training) chosen and how sensitive is the approach to its value? It seems like it was fine-tuned, given that a different sigma is chosen for the Action dataset (several orders of magnitude difference). In practice, tuning sigma could significantly increase training time. \n* It seems like the performance of only one run was plotted per hyperparameter setting - it would be informative to see a mean and standard deviation especially since the approach seems like it could be unstable for the wrong hyperparameter settings.\n* Related to the previous point, how much do the top-K feature selections change throughout training? One would have thought that this could cause instability during training for a high sigma. If sigma is too low, you could end up with suboptimal feature selection.\n* Figure 4 graphs are a bit misleading because the throughput on the x-axis is reported per GPU and the larger models all need 2 or more GPUs. While this is mentioned in the main text, it is still optically deceptive and the results are GPU-dependent - presumably if the GPUs had a larger memory, the larger models would not seems as slow. I think it would be clearer to plot images/sec on the x-axis or to rerun the experiments just using a single GPU.\n* It is stated that  [d]etermining K [...] has a critical impact on both size and target accuracy in the target models, where K is the number of feature maps in the source model that the delta model subscribes to in each layer. How sensitive is the accuracy exactly? Can this be quantified or discussed in more detail?\n* Furthermore, how sensitive is the performance to the number of target-model-specific features at each layer?\n* Different learning rate schedules were used for SNOW and baselines - initial lr for SNOW is 1.0, while for all other models it is 0.1. Was it checked whether the baselines improve when they are run with an initial lr of 1.0? Was this hyperparameter more heavily tuned for SNOW than for the baselines?\n* Since the source model is fixed, the applicability of the approach to lifelong learning is heavily dependent on the usefulness of the source model to subsequent tasks. If it is not, then one will have to incorporate large delta models. Furthermore, there can be no transfer between the tasks trained in the delta models.\n \nGrammatical errors / suggestions:\n* Page 1, first line: hallmark doesnt make sense in this context - maybe key objective or goal?\n* Page 1, 2nd paragraph, first line: wee -> we.\n* Page 1, 2nd paragraph, line 6: best top-K -> either K best or top K'\n* Page 2, last paragraph, first line: three folds -> threefold'\n* Section 2.1, line 2: pooing->pooling. Same typo on Page 4, last line.\n* Page 6, line 1: training from the scratch -> training from scratch'\n* Page 6, line 9: more 6x than -> 6x more than'\n* Overall, the manuscript needs to be proofread a few times.", "belong_id": "rJxtgJBKDr"}, {"uid": "SygBFYrscB", "paper_title": "SNOW: Subscribing to Knowledge via Channel Pooling for Transfer & Lifelong Learning of Convolutional Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "After rebuttal:\nAuthors have addressed all my doubts. I recommend accepting this paper.\n\n=============================\nBefore rebuttal:\nSummary:\n\nThis paper proposes a new way to do transfer learning. Specifically, authors first train a big source ConvNet and then for each task, they train a small ConvNet in which each layer subscribes to some k channels in the corresponding layer of the source ConvNet. Authors show that this model works better than methods that fine-tune the last few layers of the source network and performs close to costlier methods like progressive networks but with lesser parameters and higher throughput. Experiments on 5 tasks verify their claim.\n\n\nMy comments:\n\nOverall, this is a very interesting paper.\n\n1. This is an interesting model to do transfer or lifelong learning but only for ConvNet architectures with image data. To avoid overstating the results, I request the authors to highlight this limitation in both the title and the abstract.\n2. Page 3, para starting with In detail: Is the ResNet50 for delta model pre-trained or not? I know it is not pre-trained based on future paragraphs. But it is good to clarify it here.\n3. Sharing the same source network across multiple tasks during inference time is useful only when all the tasks take the same input. This is a very restricted application. This needs to be elaborated and highlighted in the paper.\n4. I would like to see the LF results included in the paper even though it has catastrophic forgetting issues.\n5. In Figure 4, the x-axis represents training throughput or inference throughput? I guess it is training throughput. Also, are the models trained for all the tasks in parallel (as described in serving all the tasks at once section) or separately? Even though I can guess answers for these, it is better to make these explicit in the paper for the benefit of the readers.\n6. It is never a good idea to show test curves for a task. Please remove the test curves from Figure 4. Instead, use a separate validation set and show validation curves.\n7. Are the authors willing to release the code to reproduce their results?\n\nMinor comments:\n\n1. Section 1, second para, 1st line: wee should be we\n2. Table 1: Fix grammar in MP description.\n", "belong_id": "rJxtgJBKDr"}, {"uid": "rJxeamAAKB", "paper_title": "Intriguing Properties of Adversarial Training at Scale", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper reveals some interesting properties of neural networks when trained adversarially at ImageNet scale. The total cost of the experiments is quite impressive, therefore the results are valuable references. With extensive experiments, the authors reveals two intriguing properties of neural networks when trained adversarially, and devotes most of the paper to studying the first one, i.e., why networks with Batch Normalization cannot achieve high robustness when both clean and adversarial images are used for training. I am not fully convinced by this point, but I do think the second point is very interesting. Different from previous work arguing that more data is needed for improving adversarially robust generalization, this paper shows that adversarial robustness can be improved consistently just by making ResNet deeper. \n\nI tend to accept this paper for the valuable results and the discovery of the positive correlation between network capacity and adversarial robustness, but I am not fully convinced by the explanations for the problems with BN. I hope the authors could address my concerns before I can be more confident about my decision.\n\nTo me, it seems the correct way to do adversarial training is to only use adversarial samples, since we are trying to minimize the maximum risk inside a norm ball around the clean sample (Madry et al. 2018). All the experiments with clean images in the objective are consistently worse than training only on adversarial samples under the same setting in this paper. It therefore becomes not that important to study the effect of Batch Normalization on training with both clean and adversarial images.\n\nAlso, I am a little bit unconvinced that the running mean and variance of BN is the cause of bad performance for the mixed training. First, I want to know the standard and adversarial accuracies of the network with GN in the '100% adv + 0% clean' setting. It seems missing in the paper. Despite being much better than BN in the '100% adv + 100% clean' setting, it is not sure whether it is caused by the improvement in some other property (e.g., capacity) from GN. If the robust accuracy with '100% adv + 0% clean' is also much higher with GN than BN (seems unlikely though), then replacing GN with BN does not solve the problem and it is still the objective to blame.\n\nI am also not fully convinced by the experiments with separated BN parameters for clean and adversarial samples. By doing so, the network is actually trained to approximate its behavior in the '100% adv + 0% clean' setting. Since the adversary is maximizing the loss, the gradient (of the conv layers) from adversarial samples will dominate the total gradient in the 'MBN 100% adv + 100% clean' setting, and the 'MBN_{adv}' network will be trained similarly as the network trained in the '100% adv + 0% clean' setting. This explains why the adversarial accuracy can be very close to '100% adv + 0% clean' but still lower.  Comparing results (under different ratios of clean samples) for networks without BN is very important and much more effective at explaining the phenomenon than trying to make BN work.\n\nIt would also be great if the authors could provide some curve showing the tendency of the running variance of BN. Sec 4.3 does make lots of sense from a practical aspect, i.e., fixing mean and var for training in the last 10 epochs could improve the results using same number of epochs, but what if we just train more epochs? Will the variance converge in just tens of epochs?\n\nFinally, though it seems that deeper networks are more robust,  the robustness might be a misconception caused by gradient vanishing. Could the authors provide the average gradient norms on the correctly-classified images (remaining correct after attack) in the first step of PGD attacks for the models in Figure 7? If deeper networks indeed have much smaller gradient norm, could the authors try scaling the loss by some factor to make the attacks stronger?\n\n\n===========================================================\nI am not fully convinced by your explanations. Could you give the results for the gradients, or use the loss function from CW attack + PGD to report the robustness of the deeper models? I am not sure which type of residual connection you are using, but if you are using the 'original' version as mentioned in this paper [1], then it does not necessarily avoid gradient vanishing, since if ReLU is deactivated, the gradient will be brought to zero. Another possible cause for the possible misconception of robustness might be caused by the saturated cross entropy loss, which could also give 0 gradients but could be verified by switching to the margin loss as in the CW attack. I have encountered such cases when I achieved non-zero robust accuracy (evaluated with PGD on cross entropy loss, and the robust accuracy is similar to your improvement from ResNet152 to ResNet638) on a naturally trained network as I increased the number of layers, but when I switched to CW loss (or margin loss), the robust accuracy goes to zero. Even for resnet, when you multiply the number of layers by ~4 as done in the paper, such phenomenon will actually happen.\n\n[1] He, Kaiming, et al. 'Identity mappings in deep residual networks.' European conference on computer vision. Springer, Cham, 2016.\n\n=====================================================================\nRaising my score. Found a recent good paper based on the findings of this paper. ", "belong_id": "HyxJhCEFDS"}, {"uid": "B1xyoFBGqH", "paper_title": "Intriguing Properties of Adversarial Training at Scale", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper studies adversarial training 'at scale', i.e., on the ImageNet dataset. The paper makes two main contributions in this context:\n- An in-depth investigation of the effect batch normalization (BN) has on adversarial robustness when the network is trained adversarially.\n- Training increasingly deeper residual networks (up to 600 layers) and demonstrating that adversarial robustness still increases in this regime (unlike standard accuracy).\n\nOverall I find the findings presented in the paper interesting and recommend accepting the paper. Experimenting with adversarial training on ImageNet is still hard for many academic groups due to the high computational cost. Hence the results of the paper may be useful for the wider robustness community. To achieve this goal, I strongly encourage the authors to release their models in a format that is easy to build on and experiment with for other researchers (e.g., PyTorch model checkpoints). Moreover, I find it interesting that very deep models on ImageNet can achieve increased adversarial robustness. To the best of my knowledge, these are the best robustness numbers published on ImageNet.\n\nFurther comments and questions:\n\n- It would be good to know if BN also affects adversarial robustness on CIFAR-10 or other datasets.\n\n- What happens when the width of the network is increased? Does this also help adversarial robustness?\n\n- Section 4.1 states that 'Adversarial training can be dated back to (Goodfellow et al., 2015), [...]'. While the specific form of adversarial training for adversarial robustness in CNNs is indeed recent, it may be helpful for readers to provide additional context, e.g., min-max formulations have a long history in robust optimization and statistics.\n\n- Section 4.4 states 'Interestingly, we find that the accuracy on clean images can be significantly boosted from 62.3% to 73%.'. It would be good to add context and state what accuracy the network achieves with standard training.", "belong_id": "HyxJhCEFDS"}, {"uid": "Skl9fUh4qB", "paper_title": "Intriguing Properties of Adversarial Training at Scale", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces two properties of adversarial training observed from abundant empirical results. Based on the discoveries, the authors propose plausible explanations as well as new methods to gain higher adversarial robustness. The two properties are as follows\n\n1. The batch normalization may negatively affect the adversarial robustness, where a training batch consists of a mixture of clean and adversarial examples. The authors observe that the parameters for batch normalization (BN) may be quite different between batches of clean and of adversarial examples, and conjecture the reason as that these two sets of examples are from two different domains. Therefore, the authors propose a few methods to boost the adversarial robustness: using different BN stats for clean and adversarial examples (but keeping the other parameters shared, which may seem useful only for fundamental study due to the requirement of clean/adversarial label for each example), using batch-unrelated normalization, and changing stat estimation for BN to reduce the difference between training and inference steps.\n\n2. The deep networks for general image classification may be too shallow for adversarial training. Since the mixture of clean and adversarial examples may form a two domain distribution that is challenging to model, typical neural networks (even for ResNet-152 that has high depth for general image classification) may have too low capacity. The authors show that increasing the depth of neural network (up to and possibly beyond ResNet-638) results in even higher accuracy.\n\nTo my best understanding, the results and analysis of this paper are valid, and the proposed methods have shown gains in abundant experiments. Due to the significance of adversarial training and the new discoveries of this paper, I think the contributions are sufficient and would lean towards the paper being published / weak accept. However, below are my questions and concerns that I would like the authors to address.\n\n1. For this paper, the adversarial examples are generated from a particular class of attacker, namely Projected Gradient Descent. I am curious how the conclusion could generalize and help the robustness if we have more than one adversarial attackers.\n\n2. The paper focuses on adversarial robustness and seems to deprioritize clean image accuracy, which could seem to limit the scope for application purposes. Frankly, I agree that a fundamental understanding of adversarial robustness would be significant, and the authors discuss the problem of relatively lower clean image accuracy in Section 4.4. However, I might feel that clean image accuracy should be as significant for practical purposes, and it would be great if we can balance the trade-off between clean image accuracy and the adversarial robustness. The authors should feel free to correct me if I do not understand correctly.\n\n3. There seems to be an interesting observation in Fig 1 for which I am curious. The accuracy for PGD-2000 in Fig 1 does not go monotonically with the ratio of clean images  the lowest accuracy is at 60 percent of clean images, which seems not to fully align with the argument that removing clean images will help robustness. Personally it would be great if the authors could share their insights of possible reasons.\n\n4. For the second discovery that deeper networks help adversarial robustness, the red line (for adversarial robustness) in Fig 7 seems not converged yet at ResNet-638. If the computation resources allow, I am curious on the depth of ResNet at which the red line becomes flat, and this could be useful for headroom analysis on how good accuracy we can reach.\n\n5. For the second discovery that deeper networks help adversarial robustness (Section 5), it seems Madry et al 2018 (Towards deep learning models resistant to adversarial attacks) also discusses model capacity vs the adversarial robustness. The mentioned paper does not seem to use deeper structure but uses other ways to increase capacity. The mentioned paper has been referred to in other sections of this work, however, it may be good to contrast in Section 5 on the conceptual novelty in this paper.\n\n6. Typos: The title of Table 1, MBN_{clean}/MBN_{clean} would be MBN_{clean}/MBN_{adv}. There are some , , (double commas) in the appendices. Comma after ( in appendix B3. Overall this paper is well written and easy to follow.", "belong_id": "HyxJhCEFDS"}, {"uid": "r1eDh8QAFB", "paper_title": "Poisoning Attacks with Generative Adversarial Nets", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nThis paper proposed a method pGAN based on Generative Adversarial Networks to generate poisoning examples in order to degrade the performance of classifiers when trained on the poisoned training data. The authors evaluated pGAN on both synthetic datasets and commonly used MNIST and Fashion MNIST datasets in machine learning.\n\nThe paper is self-contained and easy to read. My main concern is on the experiment results. The detailed questions are as follows:\n\nQ1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.\n\nQ2: Can the authors structure the experimental results with different sections? Currently it is just a single section which is difficult to read. \n\nQ3: The authors noticed that But, as we decrease the value of , the distribution of red points shifts towards the region where both green and blue distributions overlap. This observation is interesting as it finds that the poisoned input tends to lie on the overlap of two classes. But this can easily lead to a defense method: remove those training examples that are close to the other class. This defense mechanism can be used together with other sanitization approaches. So I would like to see how would pGAN perform in this case?\n\nQ4: The authors mentioned Comparison with existing poisoning attacks in the research literature is challenging: Optimal poisoning attacks as in Munoz-Gonzalez et al. (2017) are computationally very expensive for the size of the networks and datasets used in our experiments in Fig. 2.. \nHowever, I can not agree because you can simply generate poisoned data and train the neural networks on the poisoned data regardless of the underlying approach that is targeted in generating the poisoned data. This would be an effective baseline to compare. (Correct me if I am wrong here.)\n\nI will change my score if the authors can address my concerns here.\n\n================================================================\nThanks for the rebuttal. I am more convinced now.", "belong_id": "Bke-6pVKvB"}, {"uid": "Skgk1VhAYH", "paper_title": "Poisoning Attacks with Generative Adversarial Nets", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a new generative poisoning attack method against machine learning classifiers. The authors propose pGAN with three components to maximum the error of classification and guarantee undistinguished poisoning data for the discriminator. The experimental results show that the hyperparameter \\alpha significantly affects the poisoning data distribution and pGAN leads to specific error in a classification task.\n\nThis paper should be weekly accepted, considering the following aspects.\n\nPositive points: (1) The experiments seem solid. The overall performance with different parameters and the corresponding error type have been evaluated. (2) The error-specific and performance-control characteristics of pGAN seem to be interesting. (3) The paper is well organized.\n\nNegative points: (1) The authors should provide more justification on equation-3. Why do the authors directly average different loss for the discriminator and the classifer? (2) The function of the discriminator is not very clear, especially for the classification error test. Does the discriminator exclude the poisoning data according to certain rule? It would make more sense if the classification error measured from the data the discriminator selects. (3) pGAN can produce error-specific attack without sufficient justifications. Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency? (4) For the error-specific attack task, it would be better to provide an ablation experiment. For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \\alpha=0) or typical pGAN when they compare with the label-flip operation. Please explain which component contribute to the error-specific inclination.\n", "belong_id": "Bke-6pVKvB"}, {"uid": "ByeZZKSxqB", "paper_title": "Poisoning Attacks with Generative Adversarial Nets", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper tackles vulnerability to poisoning. An important subtopic of adversarial ML.\nThe authors propose using a GAN to generate poisoning data points, as an alternative to existing methods.\n\nWhile most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).\n\nThus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)", "belong_id": "Bke-6pVKvB"}, {"uid": "r1gRUl0TYB", "paper_title": "DyNet: Dynamic Convolution for Accelerating Convolution Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1718", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nMain contribution of the paper\n- The paper proposes a dynamic convolution selection method can be applied to arbitrary classification networks based on the global average pooled (GAP) feature map info.\n- The method obtained improvements over various networks (SuffleNet v2, MobileNet v2, ResNet 18) on ImageNet.\n\nMethods\n- Given the set of fixed convolutional filters, the method dynamically selects the (weighted sum) kernels by given a kind of channel attention.\n- The GAP of the features gives the channel attention on each stage, and the method applies the dynamic selection of the kernels.\n- The number of channels in skip-connection shluld be the same because it should be elementwise multiplied with the channel attention acquired from GAP.\n- The author slightly revises the baseline networks to set the networks integrated with the proposed method to have smaller Flops.\n\nQuestions\n- According to Figure 4, it seems that the proposed add-on requires many parameters because it would include a FC layer for each block. But we cannot find the number of parameters in this paper.\n- The parameter $g_t$ is defined as 6. The experiment shows the ablation to the case $g_t$ =1, but what if we set the parameters to other numbers?\n\nStrong points\n- The proposed model achieved improvement with fewer Flops on large scale image classification dataset.\n- The method shows effectiveness when it is attached to various classification networks.\n\nConcerns\n- The main concern of the reviewer is that the model shares the core contribution to the existing method; squeeze-and-excitation network (SEnet, Hu et.al.). The method also proposes the attention-based scaling of channels, where the attention comes from GAP, so the reviewer thinks that it is possible to explain this work as some variation of SEnet.\nThe author should clarify the difference and the strong points of the proposed block compared to SEnet.\n- Also, the reviewer cannot guarantee that the networks trained by the proposed method can transfer the knowledge to other tasks such as detection. \nThe reviewer thinks that it is a critical part because one of the primal reasons for training the network is to use them as the pre-trained backbone for the other tasks.\nRegarding this, the baseline methods (MobileNet V2, Shufflenet v2, ResNet)  are widely used as a pre-trained backbone for object detection, and the papers mention the CoCo object detection results using the pre-trained backbones from their method. The reviewer thinks that the experiment regarding this should be included.\n- The other thing is that the parameter increases. As in the question, the reviewer thinks that the number of parameters would be increased. The reviewer agrees that some recent works focus more on Flops, but the number of parameters is also discussed in general, when telling about the 'model size'.\n\nConclusion\n- The author proposed a dynamic kernel selection method (add-on), which can enhance the classification accuracy of the baseline network. \n- However, the reviewer cannot convince the novelty of the proposed approach and usefulness of the pre-trained backbone network from the proposed method when applying it to the other tasks (Object detection).\n\n\nInquiries\n- Clarifying the difference between SEnet.\n- Testing the ImageNet trained network of the proposed method into an object detection task (as the pre-trained backbone).\n- Discussing the number of the parameter as well.\n", "belong_id": "SyeZIkrKwS"}, {"uid": "B1lC6hMCYS", "paper_title": "DyNet: Dynamic Convolution for Accelerating Convolution Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "=== Summary ===\nThe authors propose to use dynamic convolutional kernels as a means to reduce the computation cost in static CNNs while maintaining their performance. The dynamic kernels are obtained by a linear combination of static kernels where the weights of the linear combination are input-dependent (they are obtained similarly to the coefficients in squeeze-and-excite). The authors also include a theoretical and experimental study of the correlation.\nThe authors conduct extensive experiments on image classification and segmentation and show that dynamic convolutional kernels with reduced number of channels lead to significant reduction in FLOPS and increase in inference speed (for batch size 1) compared to their static counterparts with higher number of channels.\n\n=== Recommendation ===\nThe experimental setup is rigorous but the current draft lacks some metrics that should be reported (as training times, parameter counts, memory requirements at training/inference) since the focus is on making CNNs more efficient.\n\nThe presented experimental results are satisfactory but the studied networks are not quite SOTA: they are much more competitive alternatives to ResNet and MobileNetv2. The correlation study is interesting.\n\nMy main issue with the paper is the lack of novelty. The use of dynamic convolutions is by no means a novel idea and has been studied in multiple previous works in vision (mixture of experts, soft conditional computation, pay less attention with dynamic convolutions, ...) which the authors fail to cite/compare against.\nHowever, most previous work focuses on leveraging dynamic kernels to use more parameters so the focus on accelerating CNNs is novel.\n\nOverall, I am on the fence with this paper but slightly leaning towards rejecting it for the above reasons.\n\n=== Questions/Comments ===\n- Figure 5: how are the models constrained to have same FLOPS? Is it by changing the number of channels?\n- Consider adding training times for more transparency\n- Consider adding parameter counts in experiment tables\n- The related work subsection 2.3 is rather poor compared to existing work.\n- 'While model compression based methods' -> 'On the other hand, model compression based methods'\n- 'computing efficient' -> 'compute efficient'\n- 'values distribute' -> 'values distributed'\n- 'DETAIL ANALYSIS OF OUR MOTIVATION' -> 'Detailed analysis of our motivation'", "belong_id": "SyeZIkrKwS"}, {"uid": "SJxn_9yy9H", "paper_title": "DyNet: Dynamic Convolution for Accelerating Convolution Neural Networks", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed dynamic convolution (DyNet) to accelerating convolution networks. The new method is tested on the ImageNet dataset with three different backbones. It reduces the computation flops by a large margin while keeps similar classification accuracy. The additional segmentation experiment on the Cityscapes dataset also shows the new module can save computation a lot while maintaining similar segmentation accuracy.\n\nClarity:\nThe novelty of the paper is limited and the experimental results are weird for me.\n1. The proposed module named dynamic convolution is detailed in Sec 3.2. As far as I can see, it is very similar to the former SENet especially in Figure (3) and Equation (2). The only difference is the introduction of g_t where the output dimension is much larger than SENet.\n\n2. As shown in Equation (2), the proposed method contains the normal computation of fixed kernels. How can this method save computations compared to classical convolution? Is the computation flops calculated in the right way?\n\n3.  The results in Table 5 are strange to me. Larger g_t will increase the flops absolutely according to Equation (2).\n\n4. The author may need to show the comparisons of the number of parameters. In my opinion, the new module will increase the parameters a lot (the output dimension of the fully connected layer is as large as C_cout*g_t).\n", "belong_id": "SyeZIkrKwS"}, {"uid": "HkxRhsD5YS", "paper_title": "When Robustness Doesn\u2019t Promote Robustness: Synthetic vs. Natural Distribution Shifts on ImageNet", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: This paper tries to evaluate whether robustness interventions such as robustness to adversarial examples and other synthetic distribution shifts on natural distribution shifts. \nOverall this paper is generally well written and focuses on an important direction of understanding when the toy/synthetic settings used for convenience actually transfer to the real world. However, I have some concerns with the current version of this work. \n\nDecision: I vote for rejecting this paper. My main concerns are regarding both the motivation of this work and the quality and soundness of the claims. \n\n Soundness: My major concern is with the datasets that are used for natural distribution shift. ImageNetV2 is essentially from the same distribution as the original Imagenet. Its unclear what the distribution shift is and whether there is a distribution shift. In fact, the paper that produces Imagenet V2 tries very hard to minimize the distribution shift. Hence it seems unreasonable to use this dataset for a shift. Similarly, the other dataset used is from Imagenet Videos. Here again, its unclear what the shift actually is, and hence using these datasets to measure performance to natural distribution shifts seems questionable. \n\n Motivation: I am a little confused about the motivation of this work as presented. It does not seem unreasonable that robustness to one kind of shift doesnt transfer to another. It seems like the right questions to ask would be the following: Do the synthetic robustness goals seem like they would occur in natural settings? Does robustness on synthetic datasets and synthetic generations of such perturbations transfer to perturbations (of the same kind) but occurring in natural world. \n\nFinally, it seems like the novelty/contribution of this work is very small. The paper uses existing datasets and existing robustness measures. ", "belong_id": "HyxPIyrFvH"}, {"uid": "Hke8aQMpKS", "paper_title": "When Robustness Doesn\u2019t Promote Robustness: Synthetic vs. Natural Distribution Shifts on ImageNet", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\n============================================ Update after rebuttal =========================================\n\nI thank the authors for their detailed rebuttal. I'm in general satisfied with the authors' response to my concerns, so as promised, I'm happy to increase my score.\n\n========================================================================================================\n\nThis paper considers the relationship between various measures of synthetic robustness and two distinct measures of natural robustness in large scale image classification models. The authors argue that the synthetic robustness measures considered in this paper are not predictive of natural robustness when the effect of baseline accuracy is subtracted. I think, if true, this is an important message that researchers in this area need to be aware of. However, I have a number of questions and concerns about the results. I would be happy to increase my score if the authors could address some of these issues:\n\n1) Another important recent benchmark not mentioned in the paper is ImageNet-A (https://github.com/hendrycks/natural-adv-examples). I would encourage the authors to include this benchmark among their natural robustness measures (in addition to ImageNetV2 and ImageNetVidRobust). The advantage of this dataset is that because it samples from the error distribution of a high-performing ImageNet model, to a large extent, it already comes with the baseline subtracted, so it essentially obviates the need for the indirect effective robustness measure introduced here. The raw accuracies on ImageNet-A would be directly interpretable and they would also answer the question why should we care? in a more visceral way, because even the Instagram trained state-of-the-art ImageNet models seem to achieve a mere 17% accuracy on this benchmark: https://arxiv.org/abs/1907.07640\n\n2) There seems to be a direct conflict between the main conclusion of this paper (that synthetic robustness measures do not predict natural robustness) and an opposite conclusion reached in an earlier paper (https://arxiv.org/abs/1904.10076) where the authors claim that robustness against synthetic perturbations like translation, hue, and saturation are actually highly predictive of video robustness (not sure if this would generalize to ImageNetV2). As far as I can see, these particular perturbation types are not included among the synthetic perturbations considered in this paper. Can you please clarify this discrepancy? \n\n3) Relatedly, looking at the scatter plots of effective robustness vs. robustness against individual perturbations in the appendix, especially for the video robustness measure, some of the correlations seem to be pretty significant (for example, video robustness vs. jpeg compression robustness, p. 19). So, I am wondering to what extent the main conclusion of this paper might just be driven by the averaging of a large number of non-predictive perturbations and a smaller number of more predictive perturbations. \n\n4) Also, ImageNet-P perturbations are not included in the paper. If the authors want to make their claims more reliable, I would encourage them to consider these among their synthetic perturbations as well. The translation perturbation in ImageNet-P, in particular, would be particularly important to consider for video robustness given the results from the arxiv pre-print mentioned in 2) above.", "belong_id": "HyxPIyrFvH"}, {"uid": "BkgJfUqRYH", "paper_title": "When Robustness Doesn\u2019t Promote Robustness: Synthetic vs. Natural Distribution Shifts on ImageNet", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studied an interesting question, whether the gain of robustness from synthetic distribution shifts can be transferred/generalized to the robustness under natural distribution shifts. It was shown that in the context of natural distribution shifts, no current robustness intervention can really outperform standard models without a robustness intervention. The main strength of this paper is its extensive experimental study. However, I still have concerns on this work.\n\n1)  The authors mentioned 'an implicit assumption underlying this research direction is that robustness to such synthetic distribution shifts will lead to models that also perform more reliably on natural distribution shifts.' However, I am uncertain about this point. Let us take adversarial robustness as an example, I am NOT surprising that the robustness on crafted adversarial examples cannot be generalized to the robustness over natural distribution shifts. Thus, I do not think that adversarial robustness obeys the 'implicit assumption'. In other words, the studied problem should be better connected to adversarial robustness.\n\n2) The significance of 'effective robustness' is not clear. It seems that most of insights were learnt from Figure 1, 3 and 4. Do they rely on the metric 'effective robustness'? \n\n3) In Figure 1, 'Trained with more' -> 'Trained with more data'\n\n############# Post-feedback ################\nThanks for the response. I am still not fully convinced by the significance of the findings in this work.\nIn the paper, the authors highlighted that 'our results show that current robustness gains on synthetic distribution shifts do not transfer to improved robustness on the natural distribution shifts presently available as test sets.'  I am not surprising at this point, since the synthetic shift, e.g., introduced from adversarial examples, may only characterize the short-cut shift for misclassification. Thus, robustness learnt from this synthetic distribution shift might not transfer to the natural distribution shift. \n\nI decide to keep my original score. \n", "belong_id": "HyxPIyrFvH"}, {"uid": "B1gTZuqaKB", "paper_title": "Projected Canonical Decomposition for Knowledge Base Completion", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, a tensor decomposition method is studied for link prediction problems. The model is based on Tucker decomposition but the core tensor is decomposed as CP decomposition so that it can be seen as an interpolation between Tucker and CP. The performance is evaluated with several NLP data sets (e.g., subject-verb-object triplets). \n\nAlthough the entire idea is interesting, the current form of the paper is not sufficient for acceptance. The main reasons are (A) the proposed model is not completely novel and (B) the empirical results are not significant. \n\n(A) The idea of combining CP and Tucker is not new. For example, Tomioka et al. (2010; Section 3.4) considered the Tucker-CP patterns (CP decomposition of the Tucker core). Although they used the Tucker-CP model to improve the interpretability rather than link prediction, the paper needs to make some attribution to the prior work. \n\n(B) By looking Figure 3, the proposed method, PComplEx, is not significantly better than the existing methods such as ComplEx. Except SVO data, PComplEx and ComplEx share almost the same performance curve. Also, other existing methods such as TuckER and MurP are evaluated only in a few points while (P)ComplEx is evaluated in many points. I feel this is unfair.\n\nTomioka, R., Hayashi, K., & Kashima, H. (2010). Estimation of low-rank tensors via convex optimization. arXiv preprint arXiv:1010.0789.", "belong_id": "ByeAK1BKPB"}, {"uid": "rkl0nrbb5S", "paper_title": "Projected Canonical Decomposition for Knowledge Base Completion", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present a new way of decomposing 3-order tensors which uses interpolation\nbetween the Tucker and CP decompositions, called CPT. The main idea is to present the components of the CP model\nwith an additional low-rank structure.\nThe authors also provide a new optimization algorithm called ADA-imp, for learning this decomposition,\nwhich is a variant of Adagrad adapted to their settings. \nThe paper is overall interesting, clearly written and well-motivated. \nThe mathematical derivations are, as far as I could follow, correct and non-trivial.  (I did not read all the details in the Appendix). \nThe authors also show favorable experimental results on two knowledge-base datasets, with improved loss vs. #parameter used tradeoff.\nA few unclear issues and suggestions for improvements are below.\n\nThe authors present the problem as completion of a binary 3-order tensor, i.e. predicting for triplets (subject, predicate, ?) if '?' refers to 0 or 1.\nBut they also write 'we formulate this problem as a multi-class classification problem, where the classes are the entities of the knowledge base'  - so this is not a binary problem? does this mean there is some structure that must be present in the tensor? (e.g. there is exactly one '1' in each column of length N? This should be clarified. \n\nIt would be good to make the description of Algorithms 1 and 2 more precise and detailed. \nFor example, the operation/algorithm AdaGrad(\\eta;w_k; g_k;G_k) is not defined. AdaGrad is described in the Appendix but it is hard to match it to get the precise operation used in Algorithm 1. \nAlgorithm 1 shows one step of PComplEx, and it would be good to add the entire PComplEx algorithm, with input,output&parameters. \n\nThe authors present their method in the context of knowledge base completion, thus for tensors of order 3, but it is not clear if any of the components they proposed indeed specialized for this problem, or is it a contribution to general tensor decomposition. Some remarks regarding the (in?)applicability of the method more generally would be helpful. \n\nFigure 3 describing the experimental results should be explained better. There are few methods shown only in some of the graphs and only for some parameter values - why?\nThe complexity measure 'parameters-per-entity' should be clearly defined (I didn't find it in the text). Similarly, the performance measures 'mean reciprocal rank' and 'hits at 5%' \nshould be defined in terms of the tensor. \nThe authors should also add running times of the different experiments and methods. \n\n\nMinor:\n--------\nIn the main paper, the authors define an (N,L,N) tensor, but in the appendix Section 9.9 they list N and P. Does P refer to L here? \n\nThe authors mention a few times usage of 'deep-learning techniques' - but I believe that in at least some of the contexts, they refer to optimization methods which are typically used in deep learning, and  are applied here to train other models presented in the text, and not to the usage of actual deep learning architectures - this is confusing and should be clarified. \n\nPage 7, top: what are the matrices M^(1), M^(2), M^(3)? they seem to be different for different decompositions  \n\n", "belong_id": "ByeAK1BKPB"}, {"uid": "SJeYwE_VqB", "paper_title": "Projected Canonical Decomposition for Knowledge Base Completion", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "* Summary:\nThe paper introduces a novel tensor decomposition that is reminiscent of canonical decomposition (CP) with low-rank factors, based on the observation that the core tensor in Tucker decomposition can be decomposed, resulting in a model interpolating between CP and Tucker. The authors argue that a straight application of AdaGrad on this decomposition is inadequate, and propose Ada^{imp} algorithm that enforces rotation invariance of the gradient update. The new decomposition is applied to ComplEx model (called PComplEx) that demonstrates better performance than the baseline.\n\n* Comments:\nAlthough the approach is well motivated, the paper has many ambiguities that need to better clarification.\n1. Tucker decomposition results in lower dimension factors, 'd' in the paper. So the resulting core tensor is of size (d \\times d \\times d). However, this core tensor is further decomposed with a rank-D CP as shown in Section 3, where D >= d. Basically, first the original tensor is factored into lower rank d, and the core tensor is then expanded into rank D >= d. The reader did not understand what is the justification for this approach? Please provide further explanation on this part.\n2. The confusion of P_2 and P_3 terms in the paper. At the beginning of Section 3, P_2 is assumed to be identity through out the paper. But P_2 is mentioned to have specific attributes in other parts of the paper, such as in the second paragraph from the bottom of page 4, the first paragraph and first equation on page 5. And P_2 does not appear in AdaGrad algorithm.\n3. The experiment is lacking. First, the paper does not explain the meaning of evaluation metrics. Second, the authors do not provide an insight, why PComplEx is better than the ComplEx baseline on SVO dataset, but performs similarly on other datasets. Which factors lead to such improvement?\n4. The comparison to other state-of-the-arts is inadequate, each compared method only has one or few configurations in terms of number of parameters.\n\nOverall the proposed decomposition method might have significant contribution to research progress in this field, but the paper fails to convince the reader of its significance. I feel the paper should be overhauled.", "belong_id": "ByeAK1BKPB"}, {"uid": "B1gnQjh5KH", "paper_title": "Efficient Training of Robust and Verifiable Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summary: This paper studies the problem of certified robustness to adversarial examples and proposes a new training method to obtain better certified robustness. This new method is based on using a double margin regularizer. \n\nDecision: I vot for rejecting this paper. I think the paper studies an important problem. However, I find some serious flaws in the experiments and also do not find the motivation and proposed idea convincing, Detailed under: \n\nExperimental concerns:\n Evaluating on 200 examples seems very small. Most papers consider at least 1000 examples. \n The paper compares all models based on *one* certified procedure. This is an incorrect comparison. Some certification procedures work better on some networks over another, and this does not reflect onthe actual robustness of the network. For example, ReLU stability paper reports 80% certified accuracy at \\eps = 0.3, but this paper reports 0% (possibly due to weaker attack). While some certification methods are generally weak, its not necessary that they are uniformly weaker by the same amount on all networks. Hence this is an incorrect comparison. \n Even if we use the results reported on the one certification method (ignoring the flaw above), the results only show in most settings that the double margin method ends up with a different point on the accuracy-robustness tradeoff. Its not clear why this tradeoff is better.\n\n\nConcerns with the motivation and paper overall: \n Paper seems to emphasize the unified perspective of certified training and regularization. I find this unification very natural and not particularly novel/insightful. Its just a restatement of the objective. \n I havent verified the correctness of the propositions inthe paper. However, just the statement confuses me. \\lambda -> 0 is the regime where we dont care about robustness and only care about test accuracy. Why is this even interesting or relevant?\n Finally, unfortunately while it looks interesting, I dont understand the motivation behind the double margin method. This doesnt seem to follow from rest of the paper. The authors mention below eq(9) that the gradient of the regularizer is the important term. Even if this is true, why does the double margin have a better gradient?\n\nIn light of the experimental concerns and generally weak motivation/description of the proposed regularizer, i vote for rejecting the current version. ", "belong_id": "Hke_f0EYPH"}, {"uid": "rygwJPQ0FH", "paper_title": "Efficient Training of Robust and Verifiable Neural Networks", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors first demonstrate that many existing approaches are special cases of regularized objectives, and then provide a theoretical analysis on the relationship between the local minima of the original loss and the corresponding regularized loss.  Afterwards, the authors propose a new regularizer inspired by the IBP regularizer by taking account of second order information. Through a large set of experiments the authors demonstrate that their approach achieves higher certified accuracy using CNN-Cert, compared to many previous approaches.\n\nI have some concerns on the writing and experiments of this paper. \n\n- The paper seems to have two parts that are isolated from each other. The first part of this paper discusses some theoretical analyses on the relationship of local minima for regularized and unregularized losses. The second part of this paper proposes the DoubleMargin regularizer. However, I can't see why the theoretical analysis motivates the DoubleMargin regularizer. The only statement that tries to relate theoretical analyses and the proposal is 'the gradient of a regularizer rather than its bound validity determines its certified test loss. Therefore ... using an upper bound on the adversarial loss is not necessary to train certifiable models'. This is a super general and vague motivation, and is not specialized to the DoubleMargin regularizer. The argument can actually be used for justifying arbitrary regularizers...\n\n- Since the advantage of DoubleMargin is not motivated theoretically, the empirical performance becomes critical. However, I don't think the experiments are rigorous and the comparisons are fair. In Table 2 only certified accuracies from CNN-Cert are reported. However, CNN-Cert does not work well for models trained by IBP. For fair comparison, the authors should report the best result from a group of certification methods. The certified results of IBP using CNN-Cert seem to be much worse than the results reported in the original IBP paper (Gowal et al., 2018) , which were verified using IBP. In fact, in both table 4 and table 12, the authors show results that the IBP method outperforms the DoubleMargin approach when results are verified by IBP. ", "belong_id": "Hke_f0EYPH"}, {"uid": "Hkx64BnJ5r", "paper_title": "Efficient Training of Robust and Verifiable Neural Networks", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "At a first glance, this paper proposed an interesting refinement of interval bound propagation (IBP). However, it has a major flaw in empirical evaluation, and the proposed 'theory' and 'bounds' are also questionable and have many issues.\n\nIn short, the main results of the paper in Figure 1 and Table 2 are problematic and not the right comparison, so they cannot justify the claim that the proposed method outperforms other state-of-the-art baselines like IBP.  Specifically, when comparing to IBP, the certified error should be computed by IBP; however the verification algorithm used in this paper performs extremely poor on IBP based models (giving vacuous bounds like 0%, and the authors are able to outperform this 0%).  Under fair comparison metrics (Table 12), the proposed method is worse than the IBP baseline in almost all settings. I will explain the reason the proposed method does not work in detail below.\n\nBesides the empirical results, the 'theory' developed in this paper also has several fundamental weakness (will discuss in detail below) and are lack of solid connections to robustness and the proposed 'bounds'; the proposed 'bounds' are questionable and are not sound bounds, and they can hardly be justified theoretically; so it is not surprising that they cannot outperform IBP, which is based on rigorous minimax robust optimization and sound over-approximations of neural networks.\n\nOn the positive side, the authors considered the ensemble of multiple IBP trained models, as well as extending IBP to L0 norm setting. Both of them are valid (but small) contributions, but they are not sufficient. Also, overall the writing of the paper is great and easy to follow and understand.\n\nI really do not want to make the authors of this paper upset, especially, the main author might be the first time submitting a paper to ICLR or a undergraduate student new to this field. However I have to say this paper has significant flaws and should not be published. Especially, the wrong evaluation methodology used in this paper can be very misleading for new comers to this field, and misguide future research.\n\nI encourage the authors to read my detailed comments below and learn from the failure of the proposed method. If the authors can rephrase this paper significantly (especially, removing the entire section 3), and emphasize on the contributions of ensemble or L0 perturbation, it might become a good paper for a next venue. My suggestions for improvements are below:\n\n1. Be honest with your findings and do not try to hide the weakness of your method, and do not overclaim. Especially, the authors are aware of the problem that IBP based models are better if evaluated under IBP bounds (in Table 12), but still make strong and wrong claims that the proposed method outperforms other state-of-the-art methods in certified accuracy in Introduction.\n\n2. For the ensemble part, consider more 'smart' ensembles rather than directly adding them together. For example, we can consider balancing the accuracy and certified error of each model and choose a blend of them. IBP is a strong method, and an ensemble of IBP can yield the best defense.\n\n3. For the L0 robustness with IBP, it is not a significant contribution alone since it only converts the L0 norm to interval bounds at the very first layer.  However the authors can consider more interesting settings, like adversarial patches or masks (https://arxiv.org/pdf/1712.09665.pdf), which can be dealt with similar techniques.\n\n4. When evaluating a certified defense method, it is also good to conduct PGD attacks to the networks, to see how tight the certified bounds are.  If the authors attack the models in Table 1, we can actually see that IBP based models can perform much better than the proposed method. From this, the authors should have realized that the verification method they used is not appropriate to evaluating IBP.\n\n5. Evaluation on only 200 test data points is not sufficient. Certified accuracy is computed on the entire test set (10,000 examples) in almost all previous certified defense papers (Wong et al., 2018; Mirman et al., 2018; Gowal et al., 2018; Wang et al., 2018). The authors should use a proper implementation of verification algorithm, like DiffAI (https://github.com/eth-sri/diffai), convex adversarial polytope (https://github.com/locuslab/convex_adversarial) or symbolic interval (https://github.com/tcwangshiqi-columbia/symbolic_interval). In my experience, on a single GPU they can verify small models over entire dataset (10,000 examples) within a few minutes; large models may take a few hours, but still quite reasonable. The verification method used in this paper is lesser-known and was probably implemented poorly and inefficiently. It is better to use a mature and well accepted library.\n\n6. A Minor issue: the first paper that proposed IBP training is Mirman et al., ICML 2018 (where the 'box' domain was used for training), not Gowal et al. 2018. So some sentences in Introduction and Related works are not accurate.\n\n\nNow let's discuss the issues in this paper in detail, and let's focus on the empirical comparisons to IBP first.\n\nThe authors made the main claim based on Table 2 and Figure 1, where the 'certified accuracy' (or most commonly referred to as 'verified accuracy') for models trained using the proposed method seems to be higher than other methods, especially IBP. 'Certified accuracy' is a lower bound of accuracy under any norm bounded perturbations (given a certain epsilon). Conversely, attack based methods like PGD give an upper bound, as there can be stronger attacks that further decrease accuracy.\n\nThere are many neural network verification methods to obtain certified accuracy; some of them can be particularly weak on certain models (giving vacuous lower bounds like 0%).  Generally, you choose the best possible (and computationally feasible) verification method to verify the robustness of a model. For example, if verification algorithm A gives a certified accuracy of 10%, but algorithm B gives 90% for the same model, we should use 90%. As an analogy in the adversarial attack setting, you pick the strongest possible attack to evaluate robustness: a model has high accuracy under weak FGSM attacks is not necessarily robust; conversely, a model has low certified accuracy (even 0%) does not necessarily mean it is vulnerable, as the verification method can be particularly weak on this model.\n\nIn the original IBP training paper (Gowal et al., 2018), the certified error is computed efficiently using IBP, and the error is about 8% for MNIST (epsilon=0.3), and 68% for CIFAR (epsilon=8/255).  My first hand experience on IBP can confirm that it is very easy (without too much tuning effort) to get 10% certified error for MNIST and 73-75% for CIFAR, even using small models.  These numbers translate to 90% certified accuracy on MNIST (eps=0.3) and 25-27% certified accuracy on CIFAR (eps=8/255). However, in Table 2 and Figure 1 of the paper the authors show 0% (!) certified accuracy for IBP trained models for both MNIST eps=0.3 and CIFAR eps=8/255, and their method outperforms this 0%.\n\nUnfortunately, the verification method ('cnncert') used in this paper performs extremely poor on IBP trained models (giving vacuous bounds like 0%); IBP trained models should be certified using IBP bounds to give non-vacuous results.  What Table 2 and Figure 1 really show is the weakness of their verification method used, rather than the true robustness of the model. What we really want to show here is how robust the models are, not how good a verification method is, so we need to use the best possible verification method; for IBP trained models, using IBP for verification is almost mandatory since it not only gives tight bounds but is also much more efficient.\n\nThe authors are aware of this problem - in appendix, Table 12 (a table never discussed anywhere), they listed IBP certified error for IBP trained models.  The MNIST numbers for IBP trained models are close to those on IBP paper (90% at eps=0.3), significantly better than their method in Table 2 (68%) or Table 12 (79%). The CIFAR numbers for IBP (22.5% certified accuracy at 8/255 in Table 12) are apparently de-tuned (in my experience IBP can easily do at least 25%, and Gowal et al. reported 32%), yet it is still better than the proposed method (less than 20% in Table 2 and 12).  So under the right metrics (IBP trained models certified using IBP), even if the IBP models are detuned, they can outperform the proposed method by a large margin. The proposed method only makes IBP worse under the right metrics.\n\n\n\nNow let's understand why the proposed method cannot improve IBP. The bounds themselves have a few issues:\n\n1. The 's' bound is not a sound bound for interval analysis anymore, because it uses the wrong center z_nom (the correct center is (l+u)/2 if you propagate the 'center' and 'difference' along the network, as an alternative implementation of IBP). The author claims that it is fine since we don't need sound bounds thanks to their 'theory', however the 'theory' itself is implausible, as will explained below. Although this tampered 's' bounds may empirically help to improve robustness, it is not theoretically sound; training a sound bound helps to obtain better certified accuracy.\n\n2. The 'v' bound is claimed to capture second derivative of activation function. However, first of all, for ReLU the second order derivative does not exist at all. The author also argue that 'v' is a finite difference based bound, however it is also not accurate since when the bounds propagate to later layers both 's' and 'v' can become large, and this can be a very bad 'finite difference'.\n\n3. I do agree 'v' somewhat regularizes linearity (assuming the 'finite difference' is partially working). However, linearity does not guaranteed to produces good robustness, nor it is necessary. In fact, we should not impose unnecessary regularization to neural networks, since any regularization restricts its learning power. In some papers on empirical defense, linearity sometimes can help to reduce PGD error; however in the certified setting, a direct surrogate to certifiable robustness like IBP usually produces the best results. The addition of unnecessary regularizations mostly makes results worse, unless you have a very good reason and demonstrate strong empirical evidence that it can significantly outperform the baseline. See https://arxiv.org/pdf/1807.09705.pdf for a case study on the failure of over-regularization.\n\nI think the reason the authors still get a somewhat verifiable model is that the 's' bound sort of propagates a bound that is not sound but carries some similarity to IBP. IBP is a strong method so even tampering it a little bit, you can still get something.  The 'v' bound implicitly regularizes the norm of weight matrices, which helps to gain better certified accuracy only under convex relaxation based verification methods.  I believe simply IBP+L1 regularization can achieve similar results as the proposed method, under the *wrong* evaluation metric in Table 2 and Figure 1. Under the correct evaluation metric, we shouldn't add this regularization term at all as it harms performance.\n\n\n\nThe 'theory' developed in section 3 is unconvincing and cannot support the 'bounds'. There a few problems:\n\n1. The 'theory' does not help us to find a good regularizer. When the authors argue that the gradient needs to be close to an 'optimal' regularizer, we don't have the optimal regularizer at hand and have no idea how to approach it. Also the inverse Hessian used for distance metric in (6) is never known, so it is impossible to say which gradient is good and which is bad.\n\n2. The assumption that lambda is close to zero is almost never true, yet Proposition 1 and 2 strongly depends on it. In the paper the authors use lambda=0.5 (and other similar numbers) and never decay it to zero. So the proposed training method cannot be supported by the 'theory'.\n\n3. The 'theory' makes weak or no connection to robustness guarantees; (4) is a classical results for the connection between test error and global Lipschitz constant, and the connection between this bound and our goal (robust classifier under adversaries) is too general and too weak. A more direct formulation, like minimax robust optimization will be a much better surrogate.\n\n4. The connection between the theory and the proposed bounds is vague; the authors claim 'the gradient of a regularizer rather than its bound validity determines certified test loss', and under this sense, I can use any arbitrarily loss function and call it a 'regularizer'. For example, I can use a 'regularizer' that encourages BAD robustness, and it still fits into the authors explanation. This is like someone publishes a proof showing that P=NP, yet you can use the same argument to show P != NP. This is embarrassing.\n\nThe bottom line: I am not saying the propositions in this paper are technically wrong (under the strong assumptions the authors proposed); at least their derivations are straightforward and simple enough to check within a few minutes. However, they are too weak to guide us to find a good training method, too far-fetched to our goal of obtaining good robustness guarantee, and too general that you can use them to prove both sides. So I don't think the 'theory' is useful, and the proposed 'bounds' guided by the theory has also failed to improve the baseline.\n\nSorry for the long comments and I hope they can be helpful for the authors.\n\n****** Reply to general author response:\n\nThe comparison in the 'certifier' table is misleading. 'CNN-Cert-Zero' seems to be a special case of CROWN, with a special setting of lower bounds for ReLU. CROWN allows any slope between 0 to 1 as the lower bound, and 'CNN-Cert-Zero' is just a special case of that.\n\nMost importantly, the main issue with the paper is not the verifier used; the main issue is that the proposed method performs worse than baseline under correct evaluation, and the proposed 'theory' is distracting or wrong.\n\nThe new empirical results still do not address any of my concerns - IBP still significantly outperforms the proposed method. For MNIST, Gowal et al. reported over 90% verified accuracy (the proposed method is 75%); the IBP results provided in author response has 0% verified accuracy at , which seems to be a problem or bug. For the CIFAR=8/255 case, the authors keep detuning IBP models and obtain an IBP baseline with less than 20% verified accuracy, yet the IBP model reported in literature (Gowal et al., 2018) can perform over 30%. The proposed method only performs around 20% and the performance gap is huge.\n\n\n****** Conclusions after author response\n\nAfter reading the author response, I am still keep my score of reject since the paper contains major technical errors. In a word, the theory is distracting or wrong, and the empirical results provided are intentionally misleading (the proposed method cannot outperform baseline under the right evaluation metrics).\n\nThe author response does not address any of my concerns raised, yet the authors insisted that their 'theory' is useful (which is apparently not true according to all reviewers) and provided more confusing and misleading results. I have written a long review with detailed reasons and hope the authors can understand why the proposed method fails, but it seems they completely ignored it and did not learn anything from it.\n\nThis is quite disappointing.\n\n\n ", "belong_id": "Hke_f0EYPH"}, {"uid": "H1lsO14PtB", "paper_title": "Equivariant neural networks and equivarification", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work, the authors employ concepts from group theory to turn an arbitrary feed forward neural network into an equivariant one, i.e. a network whose output transforms in a way that is consistent with the transformation of the input. To this end, the authors first introduce the basic concepts of group theory required to follow their work and provide a comprehensive definition of equivariance. They then explain how to equivarify (w.r.t. a finite group G) a given neural network, and present experimental results on rotated MNIST digits to support their approach.\n\nI find that the proposed approach is very elegant and addresses a highly important question, namely how to devise or modify an architecture such that it preserves symmetries. In my opinion, the current paper makes an interesting theoretical contribution towards achieving this goal, but it also has several shortcomings that are detailed below. Based on these shortcomings, I recommend rejecting the paper but I would be willing to increase the score if these points were addressed in sufficient detail.\n\nMajor comments:\n\n1) Scaling\nThe authors mention in the abstract that although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one in terms of the number of the parameters. Based on Eq. (3.0.2) and Fig. 3, it is my understanding that n evaluations of each data point (input to a layer) are required for a cyclic group of order n. If the outputs of a layer are then concatenated, the input dimension of the subsequent layer grows by a factor of n. I would therefore argue that out-the-box application of the proposed approach does increase the number of variables dramatically and that the abstract is misleading in this respect. The authors briefly comment on this point with one sentence in the third paragraph of the introduction. However, I would appreciate if this point was addressed in more detail, for example in a dedicated paragraph after the theory is introduced. Please also address the question of whether variable sharing is essential from an equivariance point of view, or whether its simply a necessity to prevent an explosion of the number of parameters. Furthermore, convolutions encode translational symmetry which may be beneficial for the current application but may not be desirable for other datasets. A few comments for clarification would be very helpful. Finally, the equivarified network seems to increase the required number of computations significantly compared to the original one, which I find worth a comment .\n\n2) Experiment\nThe authors only consider a convolutional architecture and say that in order to illustrate flexibility we choose not to simply equivarify the original network. However, to me one of the main advantages of the paper seems to be that you can take this approach to equivarify any FFN. It would therefore be interesting to see this approach be applied to different networks starting with a simpler one, e.g. a 2-layer MLP. The authors could then compare the original network to the equivarified one with and without variable sharing. That would not only help the reader understand the approach better but also be much more in line with the main motivation of the paper. Then adding a second experiment, e.g. a convolutional architecture, to demonstrate flexibility would be very interesting. With regard to Fig. 4, I think there may be better ways of summarising the results than dumping 160 numbers of which only 4 seem to be of interest. The message seems to be that the network yields identical probabilities irrespective of the degree of rotation. What I find surprising is that all numbers are actually identical (shifted by 10). Is this by construction?\n\n3) Limitations\nAs indicated in the second paragraph of Sec. 4, this approach is limited to finite groups and the authors only consider image rotations w.r.t. the cyclic group of degree 4. Although I appreciate that this is meant to serve as a toy problem to illustrate that the approach works, I do not think that rotations by a constant angle are very interesting. What would be really interesting is equivariance w.r.t continuous rotations (Lie Groups), e.g. the SO(2) in this particular case. I doubt that an extension to the SO(2) is straightforward within the current theoretical framework. However, even if that is the case, I would appreciate if the authors could comment on this in a paragraph.\n\nMinor comments:\n\ni) There are many typos and grammar mistakes in the paper:\nany feedforward networks -> any feedforward network.\nenables to design -> enables us to design\nour proposed equivarification method use -> our proposed equivarification method uses\ntraditional sense multiplication -> traditional sense of multiplication\na group G acts -> a group G that acts\nneural network that defined on the -> neural network that is defined on the\nwhich achieves promising performance -> which achieve promising performance\nsupplymentary material -> supplementary material \nEtc.\n\nii) I think there may be a mistake in the 3rd point of Definition 3.0.3: For consistency with the previous definitions and with Fig. 1, shouldnt F map from X to Z and \\hat F from X to \\hat Z? \n\niii) Last paragraph of Sec 3: then after the identification \\hat F becomes a map from Z to...'.  Should it be a map from X to ..?\n\niv) In Definition 3.0.3 you define the tuple (\\hat Z, T, p) to be a G-equivarification, but in the paragraph below you call the G-product itself a G-equivarification (without including T and p). \n\nv) Footnote 2: You could correct for that and present the theory shifting by g instead of g^-1 to make it easier for the reader to follow. Or, at least, give a reference to the footnote earlier on in Example 4.0.1 to avoid confusion.\n\nvi) Unless there is a special reason for this, I would suggest changing the enumeration of definitions, lemmas, examples and equations, i.e. (3.0.1) -> (3.1), etc...\n\n\n*********************************************************\nI increased my rating based on the authors addressing many of my comments. \n*********************************************************\n", "belong_id": "BkxDthVtvS"}, {"uid": "r1xeNwATKH", "paper_title": "Equivariant neural networks and equivarification", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this paper, the authors propose a method for making a neural network equivariant. Their method also can be applied to make each layer equivariant too. \n\nStrengths:\n-- The paper is very well written and easy to follow with clear notation. \n\n-- The derivations seem to be correct.\n\n\nWeaknesses:\n-- The experiment is nice but very limited and does not demonstrate the benefits of having an equivariant network. For example, the authors do not report the accuracy of recovering the original (0) rotation.\n\n-- The novelty of the work is questionable. While the development is different, the final example for equivarification of a neural network is very similar to the existing works by Cohen and Welling.\n\n-- There are other works on equivarification that are missed by this paper. For example, consider the following paper:\nLenssen, J. E., Fey, M., & Libuschewski, P. (2018). Group equivariant capsule networks. In NeurIPS.\n\n-- The layer-wise equivariant method does have extra computational overheads.\n\n-- The fact that we have to specify the groups that we want to make the network equivariant with respect to is a limitation. The promise of capsule networks, in contrast, is to 'ideally' learn the pose (variation) vectors in a data-driven way.\nSabour, S., Frosst, N., & Hinton, G. E. (2017). Dynamic routing between capsules. In NeurIPS.\n\n-- The following statements need more explanation:\n  * 'However, these may require extra training overhead as the augmented data increase.'", "belong_id": "BkxDthVtvS"}, {"uid": "B1xIzwVAcB", "paper_title": "Equivariant neural networks and equivarification", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Motivated by group action theory, this paper proposes a method to obtain equivariant neural nets given trained one, where equivariant refers to a network that gives identical output if certain symmetry of the dataset is performed on the input (for example, if we rotate a sample the predicted class should not change). \n\nAfter reading the paper, I dont understand the experiments section. In particular, it is not clear to me how the proposed method differs from regular data augmentation, as to my understanding, the input to conv1 is copied 4 times and performed rotation for 0, 90, 180 and 270 degrees and the 4 times increased number of parameters (in-depth) of conv1 are shared. Furthermore, the same rotations are performed to the input to the second layer-conv 2: as the augmentation is cyclic I dont understand why the authors perform this operation second time. Could the authors elaborate on this? After reading this section, I dont understand the proposed fine-tuning procedure (pre-train, finetune and test): (1) what is the accuracy of the pre-trained network that was started from? (2) how is the initial network fine-tuned and modified? (as the authors mention that during training the samples are not rotated). Also, I am confused with the first sentence on page 8: the complexity of the constructed network does not grow in terms of the number of parameters. It would be useful if the results in Fig. 4 are more clearly illustrated.\n\nIs the order or increased computation 4x4x4? It would be useful to compare the method (computation & performance) with a baseline where the dataset is enlarged with data augmentation. The authors mention in the introduction that this increases training overhead, whereas the proposed practical method increases the computation at inference as well as the memory footprint of the model and the forward pass. It would be useful if the authors compared empirically with baselines with (1) data augmentation (2) network with an increased number of parameters (same as the proposed one).\n\nIn summary, the idea of using group action theory seems interesting. However after reading the paper, it is not clear to me how the idea is carried out, and although the authors provide theoretical justification, it is not clear how this connects with the practical proposed method and whether it outperforms standard data augmentation (see above). Moreover, I find the writing of the paper quite unclear (see discussion above and examples below).\n\n- If digits 6 or 9 are rotated the label changes, how does the proposed method handle this?\n- page 8, conclusion: The authors claim that the proposed approach yields a significant reduction in the design and training complexity. I dont understand relative to what this comparison refers to, as the regular data augmentation approach is more straightforward in my opinion. Also, given that this is pointed as an important contribution, in my opinion, an empirical comparison must be done with such baselines (see above).\n- Page 1: it is mentioned that the number of parameters can be the same as the original network but the experiments do not include such architecture. After reading the paper I dont understand how such a network can be implemented and whether it works.\n\n Minor \n- Page 1 & 1par-Pg2: I dont understand what the authors mean by uniformly *across layers* of NN? \n- Page 2: In these existing works, ... I dont understand this sentence\n- Page 2: our .. method use -> uses\n- Page 2: map over the orbits. I dont understand this\n- Page 2: the first truly equivariant NN. After reading Sec 5 I dont understand this point.\n- Sec. 4: how to equivarifying -> equivarify\n- Page 4: pick a generator, would recommend elaborating this term or only mentioning g as an element of G for clarity for readers unfamiliar with group theory\n- What is the testing accuracy if rotated for different angles than trained (e.g. 45 degrees)?", "belong_id": "BkxDthVtvS"}, {"uid": "Bkgdz79Mor", "paper_title": "Equivariant neural networks and equivarification", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper adds an interesting new perspective to equivariant neural nets. However, the actual construction looks equivalent to steerable neural nets to me (see the papers by Cohen and Welling). The generalization of steerable nets has been published under the name 'gauge equivariant neural nets', it would be very interesting to chart out the exact connections between these concepts. \n\nThe authors mention that Z^{\\times G} is not the only possible lifting space. I believe that the general case would be Z^V where V is a representation of G. \n\nMany of the earlier papers on equivariant nets were written in the language of representation theory. It is interesting that similar nets can be constructed by purely group theoretic methods, but I really think that ultimately they are same thing. Consequently, I would expect all the experimental results to be identical.\n\nWhat would make this paper really valuable for didactic purposes is if these connections were carefully mapped out and presented with intuitive diagrams and examples.", "belong_id": "BkxDthVtvS"}, {"uid": "HyxQAQLatr", "paper_title": "Unsupervised Universal Self-Attention Network for Graph Classification", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors developed a graph embedding method called U2GAN based on self-attention mechanism. Similar to many existing graph neural network, U2GAN samples and aggregate neighboring features for each node in a graph. The aggregation function is similar to GAT, i.e., a query based attention layer. The difference is an incorporation of a transition function followed the attention layer. By minimizing Eq. 7, node embeddings can be inferred, which are summed up to obtain a graph embedding, for the downstream graph classification task.\n\nThere are several points that are unclear in the paper.\n1.  The major argument of the advantage of using self-attention for neighborhood aggregation is to facilitate memorizing the dependencies between nodes and explore the graph structure similarities locally and globally. This argument, however, was not clearly discussed in the paper. First, it is not clear on why existing GNN methods, such as GCN, GraphSage, and GAT, cannot do so. Second, it is not clear on how does the proposed U2GAN achieve it. The current paper only provides some high-level descriptions. A more specific or theoretical discussion is desired.\n2. Since the attention based aggregation is similar to GAT, a discussion on the difference is important.\n3. Several model designs are not well justified. In Eq. 2, 3, the reason to employ Layernorm is missing. In Eq. 7, the intuition on how does the loss function help learn effective embeddings remains to be clarified. Also, it may be better to evaluate different pooling method to obtain graph embedding to justify the choice of sum in Eq. 1.\n4. Since the proposed method aims to learn node embeddings in an unsupervised manner, it is better to see some descriptions on why graph classification was selected as the task in evaluation, instead of node classification.\n5. In the experiments, some methods such as deepwalk, node2vec, graphsage and GAT are missing in comparison. In particular, due to the similarity between the proposed method GAT, it is interesting to evaluate GAT by replacing its supervised loss by Eq. 7 as a compared method. Moreover, in fig. 4, other visualizations of other methods can be compared to demonstrate the difference between the proposed method and others.\n", "belong_id": "HJeLBpEFPB"}, {"uid": "BkltXbVRKB", "paper_title": "Unsupervised Universal Self-Attention Network for Graph Classification", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n\n\nThe paper presents an new unsupervised model for graph classification. It borrows the idea from universal self-attention network and applies it to graph learning. It achieves surprisingly good results on benchmark datasets. Despite the good results, I do not think the technical quality is good enough to make it accepted. My concerns contain the following aspects:\n\n1.\tIf we compare the proposed model with the graph attention networks (GAT), it just adds the recurrent transition and the layer normalizer, which are also from the universal self-attention. This makes the paper not novel enough.  Furthermore, adding these components are not so related to unsupervised learning, it does not add any value to the unsupervised learning strategy.\n2.\tThe description of the unsupervised learning objective is not clear. From Algorithm 1, it seems $o_v$ is equal to $h_v^T$, I cannot understand the meaning of Eq. (7) at all.\n3.\tThe results are too good to be true. Although we cannot judge it based on this belief, the authors have to convince the readers and explain how the huge performance gain is obtained (on some datasets U2GAN is even 27% higher than all of other methods).  I understand the experimental setting is transductive, but even that cannot explain everything. To justify the experiments, the authors need to do a lot of ablation study, such as comparing with supervised learning version of this model, while in the paper there is no ablation model to explain it.\n\n\n", "belong_id": "HJeLBpEFPB"}, {"uid": "Hkl1LHjCYS", "paper_title": "Unsupervised Universal Self-Attention Network for Graph Classification", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The submission proposes a graph neural network based on propagation with the attention mechanism. Then the output function uses the summation of node vectors to read out information about the graph. \n\n While the design is good, all components are all known techniques: the sampling procedure is like GraphSAGE; the propagation rule is similar to GAT, and the output function is wide uses in graph neural networks. \n\nCritics: \n\nThe writing is not clear. At the top of page 4: quote: '... and produce an output sequence {h_vi^t}i=1^N+1'. Do you keep only the vector h_v1 and throw away other vectors? Because you will also put v_2 at the center and compute its vector in a different self-attention computation. If this is the case, why not just say the output is h_v1? If this is not the case, then each node will have multiple vectors: one is computed when the node is at the center, and others are computed when the node is sampled as a neighbor. \n\nBelow Boris Knyazev has several comments, which are not well addressed by authors. There is a discussion about transductive learning and inductive learning. However, it seems the authors still don't know how to run inductive learning on the graph classification task (quote '... still do not have a standard inductive setting for the graph classification task where we only use a part of each graph during training.'). Boris does not suggest to use part of each graph; instead, he suggests not using test graphs. I believe this is the standard practice in inductive learning (e.g. kernel methods). \n\nAnother comment from Boris about the case when T=1, and the response is 'T=1 does not correspond to a single layer network'. I don't understand the response either. When T=1, a node only gets information from its neighbors. It is similar to a one-layer GCN or GAT, in which a node also only gets information from its immediate neighbors. \n\nI also don't understand why the author insists that the proposed model has a layer-based architecture. In my view, it is a graph neural network by the standard of propagation rule and output function. \n\n\n\n", "belong_id": "HJeLBpEFPB"}, {"uid": "H1eplpk-5H", "paper_title": "Towards Stable and Efficient Training of Verifiably Robust Neural Networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new method for training certifiably robust models that achieves better results than the previous SOTA results by IBP, with a moderate increase in training time. It uses a CROWN-based bound in the warm up phase of IBP, which serves as a better initialization for the later phase of IBP and lead to improvements in both robust and standard accuracy. The CROWN-based bound uses IBP to compute bounds for intermediate pre-activations and applies CROWN only to computing the bounds of the margins, which has a complexity between IBP and CROWN. The experimental results are verify detailed to demonstrate the improvement.\n\nThe improvement is significant enough to me and I tend to accept the paper. The results on CIFAR10 with epsilon=8/255 is so far the state-of-the-art. However, it is far from being scalable enough to large networks and datasets, which has already been achieved by randomized smoothing based approaches. On CIFAR10, it takes 32 TPU cores to train a 4-conv-layer network. Still, such an approach has the advantage of making robust inferences much more efficiently than randomized smoothing, and thus still worth further explorations.", "belong_id": "Skxuk1rFwB"}, {"uid": "HJgnxLMMcH", "paper_title": "Towards Stable and Efficient Training of Verifiably Robust Neural Networks", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new variation on certified adversarial training method that builds on two prior works IBP and CROWN. They showed the method outperformed all previous linear relaxation and bound propagation based certified defenses. \n\nPros:\n1. The empirical results are strong. The method achieved SOTA.\n\nCons:\n1. Novelty seems small. It is a straightforward combination of prior works, by adding two bounds together.\n2. Adds a new hyperparameter for tuning.\n3. Lack of any theoretical insights/motivation for the proposed method. Why would we want to combine the two lower bounds? The reason given in the paper is not very convincing:\n\n'IBP has better learning power at larger epsilon and can achieve much smaller verified error.\nHowever, it can be hard to tune due to its very imprecise bound at the beginning of training; on the\nother hand, linear relaxation based methods give tighter lower bounds which stabilize training, but it\nover-regularizes the network and forbids us to achieve good accuracy.'\n\nMy questions with regards to this:\n(i) Why does loose bound result in unstable training? Tighter bound stabilize training?\n(ii) If we're concerned with using a tighter bound could result in over-regularization, then why not just combine the natural loss with the tight bound, as natural loss can be seen as the loosest bound. Is IBP crucial? and why?\n", "belong_id": "Skxuk1rFwB"}, {"uid": "SJeZNqlpqH", "paper_title": "Towards Stable and Efficient Training of Verifiably Robust Neural Networks", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes CROWN-IBP - novel and efficient certified defense method against adversarial attacks, by combining linear relaxation methods which tend to have tighter bounds with the more efficient interval-based methods. With an attempt to augment the IBP method with its lower computation complexity with the tight CROWN bounds, to get the best of both worlds. One of the primary contributions here is that reduction of computation complexity by an order of \\Ln while maintaining similar or better bounds on error. The authors show compelling results with varied sized networks on both MNIST and CIFAR dataset, providing significant improvements over past baselines.\n\nThe paper itself is very well written, lucidly articulating the key contributions of the paper and highlighting the key results. The method and rationale behind it quite easy to follow.\n\n\nPros:\n> Show significant benefits over previous baseline with 7.02% verified test error on MNIST at  \\epsilon = 0.3, and 66.94% on CIFAR-10 with \\epsilon = 8/255\n> The proposed method is computationally viable, with up to 20X faster than linear relaxation methods with similar. better test errors and within 5-7X slower than the conventional IBP methods with worse errors\n\nCons:\n> Extensive experiments with more advanced networks/datasets would have been more convincing, esp. given the computation efficiency that enables such experiments\n> More elaborate insights into the choice of the training config/hyper-params esp. with the choice of \\K_start, \\K_end across the different datasets\n\n\nOther comments:\n> For the computational efficiency studies, it would be helpful to provide a breakdown of the costs between the different layers and operations, to better asses/confirm that benefits of CROWN-IBP method\n> Impact of other complementary techniques such a lower precision/quantization? One fo the references compared against is the Gowal et al. 2018 for the as a baseline, however, it seems to be those results were obtained on a different HW platform (TPUs - motioned in Appendix-B), with potentially different computational accuracies (BFLOAT16 ?). So, this bears to question of the impact of precision on these methods and also the computation complexity.\n> Since I'm not very well versed with the current baseline and state-of-art for variable robust training of DNN, it would be good to get an additional confirmation on the validity of the used baselines.", "belong_id": "Skxuk1rFwB"}, {"uid": "Bkg21BVsYH", "paper_title": "Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis paper proposes ConNet, a new label aggregation method for sequence labeling tasks, including crowd-annotation and cross-domain model adaptation. The model consists of a decoupling phase, which learns annotator-specific transforming matrices A, and an aggregation phase with an attention module. Extensive experimental results demonstrate the superiority of the proposed model over baselines. \nThe paper is generally well-written and easy to follow, and the results seem convincing, so I think it can inspire other works on this topic. My main concern on the paper is its generalization. Crowdsourcing usually involves lots of annotators, and some of them only give very few labels. In these situations, the proposed model introduces lots of new parameters (A, Q), which may cause difficulty during training. So the positive results in Fig 3(b) are very important to dispel my worry, which requires more explanation. \nBelow are some detailed questions:\n-      How do you calculate the sentence embeddings h_i during training?\n-      Have you introduced some regularization terms on A and Q?\n-      What are the most important hyper-parameters for this model, and how to tune?\n-      Can this method be extended to new tasks other than sequence labeling?\n-      Can you compare your method with the aggregation method used in on-the-job learning paper [1]?\n-      92.33 in tab 2 shouldnt be bold.\n \n[1] Werling K , Chaganty A , Liang P , et al. On-the-Job Learning with Bayesian Decision Theory[J]. Computer Science, 2015.\n", "belong_id": "HJe9cR4KvB"}, {"uid": "BJlZKk6pKS", "paper_title": "Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n## Updated review\n\nI have read the rebuttals. The new version of the paper is clearer and the new baseline experiments are a good addition. \n\n## Original review\n\nThis paper presents an approach to train a neural networks-based model for sequence modelling using labels from different sources. The proposed approach explicitly models the annotator and uses an attention model to select the best aggregation method. The model is used on two scenarios: learning with crowd annotation and cross-domain adaptation. For the first scenario, noisy annotators are simulated with models trained on subsets of the data, the proposed model is compared with related works and is shown to achieve the highest f-1 score. For the second scenario, different domains in three NLP tasks are used and the model is shown to yield the best performance.\n\nI think this paper should be accepted, for the following reasons:\n- The approach is novel as far as I can tell, and the approach of learning to aggregate labels is significant, as it could also be applied to tasks where inter-annotators agreement is a problem.\n- The experiments are convincing and show the potential of the proposed approach. \n- The comparison with related works is thorough.\n\nDetailed comments\n- I don't understand the notion of 'normalized expertise' in Section 5.4, can the authors briefly describe it in the paper ?\n- The paper is not easy to read, for instance the first paragraph of Section 5 contains critical information to understand the experiments, maybe it should be moved the Section 4 and developed more, typically in two subsections 'Application to crowd annotation' and 'Application to cross-domain' for example.\n- Typos:\n    - Section 2, 3rd paragraph 'for traget corpora' -> 'target'\n    - Same paragraph: 'Yang & Eisenstien (2018) represented' -> 'represents' to be consistent\n    - Section 4.1: 'BiLSTM-CRF' -> 'BLSTM-CRF'\n ", "belong_id": "HJe9cR4KvB"}, {"uid": "B1xyg9XT5B", "paper_title": "Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes a method to learn how to aggregate weak supervision sources in the context of sequence labeling. In particular, the model has two main steps: i) it learns a source dependent transformation and ii) it learns a mechanism to combine them. \n\nIn general, the paper is well organized but it is not easy to follow. It was not clear to me how the authors combine the source dependent representation in Section 4.2 with the aggregation phase presented in Section 4.3. In particular, it is confusing to me how the model combines during the training Eq. 3 and Eq. 5. Similarly, it is not clear to me how in Eq. 6 the model can calculate attention coefficients based only on information from the sentence embedding (h^(i)).\n\nThis work assumes a BLSTM-CRF architecture as a baseline, but it does not explore alternative approaches, such as transformer.   \n\nIn terms of the experiments, authors evaluate the resulting model using two application settings: combining noisy crowd annotations (AMT) and unsupervised cross-domain model adaptation. Results are encouraging, the proposed method is able to outperforms several recent works in terms of F1 metric for the case of AMT and accuracy for the case of cross-domain adaptation. Qualitative results also shows reasonable performance. The supplemental material also include an ablation study.\n\nIn summary, the proposed method is interesting and results seem to be encouraging, however, there are parts of the proposed method that are not clear to me. I rate the paper as weak reject. ", "belong_id": "HJe9cR4KvB"}, {"uid": "Hke48Wc_KS", "paper_title": "BERT-AL: BERT for Arbitrarily Long Document Understanding", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed another variant of BERT, called BERT-AL, which can deal with arbitrarily long inputs. The authors constructed the proposed method by combining the segment-wise BERT with the multi-channel LSTM. The authors validated the proposed method on the text summarization task and achieved higher performance than existing works. Their proposed method can be combined with other transformer-based approaches, such as XLNet or RoBERTa.\n\nThe paper tackles a relevant topic that is interesting to the attendees of ICLR, which is, How do we get the overall information of a document, which has a number of sentences?.  However, the paper has several issues that I will try to cover in the following.\n\n(1) (No sufficient results which support authors claim) \nFirst: In the introduction, authors claim that the model for NLP tasks whose inputs are usually long texts should be able to take arbitrarily long inputs so that the model can extract important information from the entire long documents. However, there is no concrete experimental example, where the existing methods (standard BERT or BERTSUM) fail to extract information.\nSecond: In the discussion of the experiment section, the authors claim that the proposed method BERT-AL can be parallelized and runs faster than existing methods. However, there is no comparison with existing methods about the running time. \n\n(2) (Clarity about the proposed method)\nThe presentation of the proposed method is slightly confusing. There is no concrete explanation about the learning procedure. Are any pre-training and fine-tuning procedures the same with those of the standard BERT?\nThe main part of the proposed method is to combine the segment-wise BERT with the multi-channel LSTM. How is the pre-training procedure conducted with parallelized computation? A detailed explanation should be shown.\n\n(3) (Clarity about the experimental results)\nIn table 4, the experimental results are shown. There are results of the baseline 1 & 2 (existing methods), the proposed method, and the existing method (BERTSUM). There is no discussion about the comparison of the proposed method with BERTSUM. It seems that BERTSUMs result is superior to the proposed methods result. Please explain more details of those results.\nThe paper includes an explanation about the dataset and the task but does not include the experimental environment (used machines and the time to conduct the experiment). For the fair evaluation of results, we need the information on the overall experimental environment.\n\n(4) (Presentation)\nThe presentation around mathematical formulas is a bit confusing. Please fix the following.\n- Subscripts of the formula should be \\mathrm\n- <= should be \\le\n- The n of the right-hand side of equation (5) is N?\n- The multiplication * should be \\times", "belong_id": "SklnVAEFDB"}, {"uid": "SkeVjGKGqH", "paper_title": "BERT-AL: BERT for Arbitrarily Long Document Understanding", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a BERT based document summary model that has the capability of modeling arbitrarily long documents. Experiments on CNN/Daily dataset show the improvement of the proposed model compared with its baselines.\nThe advantage of this paper is that the time-consuming training process of BERT can be avoided.\n\nI think this paper is not good enough to be accepted. The model does not have the ability of understanding Arbitrarily long documents, it just aggregates multiple (n_segment) BERT segments and extends the BERT capability from l_bert to n_segment*l_bert. It is not as flexible as the LSTM which can capture the real arbitrary long document. The model looks bloated and the performance is not persuasive. The authors reproduced the baseline BERTSUM but the reproduced performance is significantly lower than performance in the original BERTSUM paper. (without explanation) The proposed model cant beat the performance in the original BERTSUM paper. Even to their reproduced BERTSUM results, the proposed model gives very closed performances.\n\nBesides, this paper doesnt compare their model with other SOTA models and all the experiments are conducted in one dataset. The result analysis is not enough. As the main contribution in this paper is to understand the longer documents, the performance on long documents should be evaluated separated.", "belong_id": "SklnVAEFDB"}, {"uid": "S1gQytXw5r", "paper_title": "BERT-AL: BERT for Arbitrarily Long Document Understanding", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The author proposes an extended version of the BERT architecture, BERTAL for text summarization. BERTAL aims to overcome the limit of maximal allowable text length in BERT, and the experiments show some consistent performance enhancement to baseline architecture using BERT and approaching performance to state-of-the-art architecture using BERT (BERTSUM), where BERTSUM has the maximal length limit .\n           The experimental procedures as well as the choice of architectural design are well explained and designed in a logical way. Substantial comparison experiments also pinpoint the performance.\n            However, it doesnt compare the result of BERTAL to other text summarizers with arbitrary-length text, which is not based on BERT.", "belong_id": "SklnVAEFDB"}, {"uid": "SJelPR1a9B", "paper_title": "BERT-AL: BERT for Arbitrarily Long Document Understanding", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a methodology to overcome the problem of processing long sequences with a pre-trained Transformer model, which suffers from high computational costs due to the complexity being quadratic in the length of the sequence. The authors also point out that BERT needs to be retrained from scratch if sequences longer than the specified maximum length (512) are to be processed. Their method (BERT-AL) chunks the input text into segments of maximum length. Each segment is propagated through several layers of a pre-trained Transformer layer followed by a multi-channel LSTM. Herein, the positional embeddings in each segment are the same. The model is applied to extractive summarization, and directly compared to the BERTSUM model, which can only process documents up to length 512. The comparison is made for 4 application scenarios, each corresponding to an artificial maximum length after which the BERTSUM model truncates the input: after 8, 16, 128, and 256 tokens. The experimental results suggest that BERT-AL outperforms BERTSUM in all 4 scenarios: Substantially for max length 8 and 16, and marginally for 128 and 256. BERTSUM without any truncation still performs best, however.\n\nI think the paper should be rejected for three main reasons: (1) The idea of using RNNs to overcome the long sequence problem in Transformers is already well studied. The specific proposed architecture may be new, but the design choices are not well justified. (2) The experimental evaluation is not convincing: The application scenarios are unrealistic, the dataset is not well-chosen, and the results are not impressive. (3) The structure and language of the paper needs to be polished.\n\nRegarding (1): The problem of dealing with long sequences in Transformer models has been known for a long time, and there already exist several proposed solutions based on an RNN component, which are acknowledged in this paper. The authors propose another model for a similar purpose. While the model is interesting, the individual design decisions are not well-justified, e.g., why the LSTM is applied at each layer and why it needs to be a multi-channel LSTM. Neither a comparison with models from previous works nor an ablation study is provided, so that there is also no empirical justification for the model design. \nThe authors argue that the novelty in their model comes from its applicability to pre-trained models, e.g., BERT and XLNET. While the motivation for BERT is reasonable, it is questionable whether this will still be a relevant for future research, as XLNET already mitigates the problem through the reliance on Transformer-XL, which is conceptually very similar to BERT-AL. If future pretrained models account for long sequences already during pre-training, the motivation for this work is rather low.\n\nRegarding (2): In order to show the superiority of BERT-AL over BERTSUM, it would've been natural to employ it to datasets whose data are beyond what standard BERT can handle, i.e., longer than 512 tokens. Instead, the authors chose to evaluate on 4 rather unrealistic application scenarios derived from the CNN/DailyMail dataset, which artificially limit the range of the pre-trained BERT model to much less than what it can actually handle. While I understand the idea behind this setup, the insight that BERT performs poorly if the text is truncated to 8 or 16 tokens is not helpful. Since the improvement of BERT-AL over the baselines becomes marginal for longer sequences, and the performance of BERTSUM, without truncating the text, is not reached, I am not convinced of the model's value. An evaluation on more suitable datasets could help here.\n\nRegarding (3): The paper is difficult to read for two reasons. First, the paper is not structured in a way that facilitates the understanding of the proposed method. For example, it is not clear why the pre-training tasks of BERT (masked language modeling and next sentence prediction) are relevant enough to be so prominently described. If the method is applicable to many pre-trained Transformers (as was claimed), then there is no need to go into this level of detail. On the same note, there is no need to explain BERTSUM's Inter-Sentence Transformer and Recurrent Neural Network variants for half a page if they are dismissed in the next paragraph. Second, the paper contains a lot of typos and grammatical mistakes. This is not a deal-breaker by itself, but it makes it obvious that the paper needs substantial polishing before it should be published. \n\nQuestions and suggestions to the authors:\n\n1) What is the advantage of choosing a multi-channel LSTM, where each segment is a channel? Wouldn't a single channel suffice that processes aggregated information from the segment (as the Transformer computes)? I think your paper would benefit from exploring different options like that experimentally to better justify your model decisions.\n\n2) It is not clear to me how the information exchange between layers happens. If each layer consists of a single layer from a pre-trained Transformer followed by an LSTM, the input to the following layer of the pre-trained Transformer is the output of the LSTM, which is not part of the pre-trained model. That is, the following Transformer layer receives an input that is very different from what it has seen during pre-training, which we can not expect to work. If there is a misunderstanding on my side, could you please clarify that, or otherwise make it clearer in the paper?\n\n3) I think a good baseline would be to do segment-wise encoding via the pre-trained Transformer, and then feed all sentence representations (i.e., the respective CLS tokens) from all segments into the Transformer-based summarization layer from BERTSUM. This would be a compromise between your model (doing segment-wise encoding but not using an LSTM) and BERTSUM, and thus could potentially highlight the importance of the LSTM.\n\n4) I don't think showing the number of examples in your datasets as in Table 2 contributes much to the paper. Instead, I suggest to show statistics on the length of the documents / number of sentences etc., because these are directly relevant to your study.", "belong_id": "SklnVAEFDB"}, {"uid": "B1lMGBlhtH", "paper_title": "A Generalized Training Approach for Multiagent Learning", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper studies -Rank, a scalable alternative to Nash equilibrium, across a number of areas. Specifically the paper establishes connections between Nash and -Rank in specific instances, presents a novel construction of best response that guarantees convergence to the -Rank in several games, and demonstrates empirical results in poker and soccer games. \n\nThe paper is well-written and well-argued. Even without a deep understanding of the subject I was able to follow along across the examples and empirical results. In particular, it was good to see the authors clearly lay out where their novel approach would work and where it would not and to be able to identify why in both cases. \n\nMy only real concern stems from the empirical results compared to some of the claims made early in the paper. Given the strength of the claims comparing the authors approach and prior approaches, it seems that the empirical results are somewhat weak. The authors make sure to put these results into context, but given the clarity of the results in the toy domains I would have expected clearer takeaways from the empirical results as well. \n\nEdit: The authors greatly improved the paper, addressing all major reviewer concerns.", "belong_id": "Bkl5kxrKDr"}, {"uid": "S1eXzEgTtS", "paper_title": "A Generalized Training Approach for Multiagent Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper extends the original PSRO paper to use an $\\alpha$-Rank based metasolver instead of the projected replicator dynamics and Nash equilibria based metasolvers in the original. To this end, the paper modifies the original idea of Best-Response (BR) oracle since it can ignore some strategies in $\\alpha$-Rank defining SSCC to introduce the idea of _preference-based_ Best-Response (PBR) oracle. The need for a different oracle is well justified especially with the visualization in the Appendix. The main contributions that the paper seems to be going for is a theoretical analysis of $\\alpha$-Rank based PSRO compared to standard PSRO. From the PBR's description (especially in Sec 4.3) it seems the paper is intereseted in expanding the population with novel agents rather than finding the 'best' single agent which is not well defined for complex games with intransitivities. Nevertheless, it seems that BR is mostly compatible with PBR for symmetric zero-sum two-player games.\nThe paper performs empirical experiments on different versions of poker. First set of experiments compare BR and PBR with $\\alpha$-Rank based metasolver on random games and finds that PBR does better than BR at population expansion as defined. The second set of experiments compare the metasolvers. $\\alpha$-Rank performs similarly to Nash where applicable. Moreover it's faster than Uniform (fictitious self-play) on Kuhn. Then the paper tacks on the MuJoCo soccer experiment as a teaser for ICLR crowd.\n\nOverall the paper is quite interesting from the perspective of multiagent learning and I would lean towards accepting. However the paper needs to clarify a lot of details to have any chance of being reproducible.\n\n** Clarifications needed:\n\n- Tractability of PBR-Score and PCS-Score\nIt's unclear how tractable these are. Moreover these were only reported for random games. What did these scores look like for the Poker games? Could you clarify how exactly these were computed?\n\n- It's somewhat unclear what the lack of convergence without novelty-bound oracle implies. Does this have to do with intransitivities in the game?\n\n- Dependence of $\\alpha$?\nThe original $\\alpha$-Rank paper said a lot about the importance of choosing the right value for $\\alpha$. How were these chosen? Do you do the sweep after every iteration of PSRO?\n\n- Oracle in experiments?\nThe paper fails to mention the details about the Oracles being used in the experiments. They weren't RL oracles but more details would be useful. \n\n- BR not compatible with PBR, albeit not the other way around, meaning one of the solutions you get from PBR might be BR, but can we say which one?\n\n- For MuJoCo soccer was it true PSRO or cognitive hierarchy. In general, the original PSRO paper was partly talking about the scalable approach via DCH. This paper doesn't mention that at all. So were the MuJoCo experiments with plain PSRO? What was the exact protocol there? From the appendix it's unclear how the team-vs-team meta game works with individual RL agents. Moreover how are the meta-game evaluation matrices computed in general? How many samples were needed for the Poker games and MuJoCo soccer?\n\n- The counterexamples in Appendix B3 are quite interesting. Do you have any hypotheses about the disjoint support from games' correlated equilibria?", "belong_id": "Bkl5kxrKDr"}, {"uid": "r1xBh9CaYS", "paper_title": "A Generalized Training Approach for Multiagent Learning", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Review Update (18/11/2019)\nThank you for the detailed replies and significant updates to the paper in response to all reviewers. You have comfortably addressed all of my concerns and so I have updated my score. I think the paper has improved significantly through the rebuttal stage and therefore the update in my score is also significant to match the far larger contribution to the community that the paper now represents.\n\n--\nThis paper considers alpha-rank as a solution concept for multi-agent reinforcement learning with a focus on its use as a meta-solver for PSRO. Based on theoretical findings showing shortcomings of using the typical best response oracle, the paper finds a necessity for a new response oracle and proposes preference-based best response.\n\nThe theoretical contributions help further the community's understanding of alpha-rank but the method remains somewhat disconnected from other recent related literature. Therefore, I think the paper's subsequent impact could be significantly improved by making more direct comparison to recent results. Specifically:\n\n1) In the 2-player games comparisons are currently made to PRD based on its use in Lanctot et al (NeurIPS, 2017) instead of the more recent PSRO Rectified Nash approach proposed by Balduzzi et al. (ICML, 2019). Please make this direct comparison or justify its exclusion.\n\n2) The preliminary MuJoCo soccer results in Appendix G significantly increase the relevance of this work to the ICLR community given the prior publication of this environment at ICLR 2019. However, the results are currently incomplete. In particular, to again strengthen the link to existing work, comparison of the method proposed in this paper to the agents trained by population based training in Liu et al. (ICLR, 2019) would be a more informative comparison than the preliminary results presented in comparison to the naive uniform meta-solver.\n\n3) Appendix A includes a brief literature survey. This is important material to position the paper in relation to existing work, particularly for readers not familiar with the area that will rely on this to understand the paper as a self contained reference. Please move this section into the main body of the paper and expand to fully credit the work this paper builds upon.\n\n\nMinor Comments:\nIn Appendix C.4 should the reference to Figure C.7 be to Figure C.7a specifically? and the reference to Figure C. 7a be to Figure C. 7b-f inclusive? If so, I believe the available joint strategies in step 4 is missing (1,1,2) as shown in Figure C. 7f.\n", "belong_id": "Bkl5kxrKDr"}, {"uid": "SygY7oEctB", "paper_title": "Deep Multiple Instance Learning for Taxonomic Classification of Metagenomic read sets", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors tackle the task of taxonomic classification of meta genomic read sets. The combine 2 existing approaches for taxonomic classification, with established methods for Multiple Instance Learning, namely DeepSets and an attention-based pooling layer. \n\nWhile the domain of taxonomic classification is interesting, I find there is a lack of novelty on the machine learning part. The authors combine well established methods in a straight-forward manner and while the resulting increase in performance for some datasets may be relevant in the domain, the conceptual advances are too incremental for a machine learning audience. \n\nUpdate: I have read the response and am still think there is a lack of novelty here. ", "belong_id": "HJl8AaVFwB"}, {"uid": "Skgk1ABTKH", "paper_title": "Deep Multiple Instance Learning for Taxonomic Classification of Metagenomic read sets", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "~The authors propose the addition of multiple instance learning mechanism to existing deep learning models to predict taxonomic labels from metagenomic sequences.~\n\nI appreciate the focus area and importance of the problem the authors have outlined. However, I do not think the authors have achieved the conclusions they mention on page 2, as well as other issues throughout the work. I also think the inclusion of the multiple instance learning framework is incremental and does not provide sufficient benefit.\n\nA new method to generate synthetic read sets with realistic co-occurence patterns from collections of reference genomes. I do not think there was any systematic analysis of the parameterization of their generative framework. I would appreciate empirical comparison of previous synthetic read generation techniques to the current proposed framework. Also, there is no comparison of the generative framework to real data. How are the parameters chosen in section 3.1.1? Finally, how the authors propose to alleviate bias of composition of databases? Rare species that may be present in abundance in metagenomic data may be swamped out by more common species sequenced again and again in databases.\n\nA thorough empirical assessment of our proposed model, showing superior performance in prediction the distributions of higher level taxa from read sets. A comparison to existing alignment-based methods is absolutely required for this work. The authors of GeNet compare to state-of-art Kraken and Centrifuge, and when reading Rojas-Carulla et al., these models have still performed worse than Kraken and Centrifuge, and that should be reported in your assessment.\n\nA few minor points:\n\n-More description of your neural network architecture is needed. Im not sure what a ResNet-like neural network actually means, and how something built for images deals with sequences. There are also different ResNets with different numbers of parameters.\n\n-Why isnt a 1D convolutional neural network used to process the input sequences? This would make the sequences translation invariant, and would have a similar effect to working with kmers, where k = convolution width.\n\n-It may be useful to interpret the attention mechanism to understand which reads are likely influencing the decision of taxonomic assignment.\n", "belong_id": "HJl8AaVFwB"}, {"uid": "Bkgk_SKkqS", "paper_title": "Deep Multiple Instance Learning for Taxonomic Classification of Metagenomic read sets", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I enjoyed reading this paper and found much of their exposition clear. Also found their extension of previous single read metagenomic classification models with DeepSets and attentional pooling layers to be well explained. However, there are two significant flaws that unfortunately make this paper incomplete as written:\n\n1) The abstract states that this paper will 'attempt to solve the task of directly predicting the distribution over the taxa of whole metagenomic read sets'. However, 'whole metagenomic read sets' typically contain many millions of reads, whereas the maximum bag size explored in this paper is 2048.  The authors never explain how their MIL metagenomic model should be applied to a full metagenomic sequencing dataset. Should the MIL model be applied to random 2048 read subsets of the many-million read real world datasets? Should these bags of reads be sampled with or without replacement? And, when applied to a whole metagenomic read set, are the improvements in classification accuracy observed for 2048 read bags recapitulated?\n\n2) There is no comparison to standard metagenomic classification tools such as Kraken and Centrifuge. While previous work such as GeNet have compared to these tools the read generation and testing assumptions in this paper are not identical. Also, GeNet was found to be inferior to Kraken and Centrifuge in many scenarios, it would be good to know where in the spectrum of accuracy these new models fall. \n\nUpdate: Having read the rebuttal, my review stands. This paper will be much better once the authors add in estimation from full read sets and evaluate against standard tools. Presumably that will have to be for a future conference submission. ", "belong_id": "HJl8AaVFwB"}, {"uid": "ByxhvaYpYr", "paper_title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "# 1. Summary\nThe paper introduces a pre-training procedure for visual-linguistic representations. The model is an extension of BERT (with transformer backbone) to deal with visual input. Images are encoded using object detectors which regions are masked at pixel level. Experiments show state-of-the-art results on different downstream tasks.    \n\nStrengths of the paper:\n* State-of-the-art results on 3 vision-language tasks\n      \nThe weak reject decision was mainly guided by the following two weaknesses of the paper:\n* Clarity of the paper needs to be improved to make the readers understanding the details of the model (see point 2 below)\n* Limited novelty: the paper is an extension of BERT to the visual domain (see point 3 below)\n\n      \n# 2. Clarity\nThe paper reads quite well, although some points need to be improved:\n* How were words split in sub-words (Sec 3.2)?      \n* 'For each input element, its embedding feature is the summation of four types of embedding, ...': it is not clear how you sum embeddings. E.g., token embedding has 30k dimensions while image one has 2048 dimensions.\n* 'It is attached to each of the input elements, which is the output of a fully connected layer taking the concatenation of visual appearance feature and visual geometry embedding as input' -> this is not clear; what output are we talking about? What is the geometry embedding? I suggest to describe the two features first and then say at the end of the paragraph that the representation is the concatenation.\n* 'For the non-visual elements, the corresponding visual appearance features are of features extracted on the whole input image' -> what is the intuition of having the full image here? Some terms do not need to have an image associated (e.g., verbs or articles). Do you take care somehow of that?\n* Once textual embeddings are masked by [MASK], the related visual embedding (whole image) is also masked? To my understanding the answer is no: what's the intuition of this?\n* Segment embedding: is this important? This should be easy to show with an experiment in the ablation study of Table 4?\n* It seems that there is a semantic asymmetry of input to the loss during training when considering only the text information (bookscorpus) and the image-text information (conceptual captions): how is training coping with this? Doesn't it make more sense to have 2 pre-training phases: first on text information only and then on image-text information?\n\n\n# 3. Novelty and Motivation\nThe novelty of the paper is quite limited. It strongly relies on transformer networks and then recent success of BERT in the NLP domain. The proposal is an extension of these two ideas to visual domain.\n\nMoreover, there is a body of concurrent work that is very similar to the proposed idea with slight differences (ViLBERT, VisualBERT, LXBERT, UNITER, B2T2), i.e., using transformers with masking operation on the RoIs. It is not clear what is the intuition related to the differences between the methods, i.e.\n* Why one is better than the other; why should someone prefer this pre-training technique wrt others?\n* Why a unified network (this work) is preferred wrt a two-stream one (ViLBERT, LXMERT)?\nIt seems that everything heavily depends on the experiments and empirical results obtained by trying many variants during the prototyping phase. It is missing a bit of understanding and intuition on the reasons why this technique should be used.\n\n\n# 4. Experimentation\nExperiments are the strength of the paper showing state-of-the-art results on 3 vision-language tasks. Some additional analysis is missing:\n* If masking is conducted on the raw pixel, this makes training much slower since you need to perform inference many times. What is the impact in terms of accuracy? Did you carried out an experiment showing that it is better to mask raw pixels instead of conv maps?\n* How long is the model trained for?\n* What is the performance/accuracy on the pre-training tasks?\n* How important is the segment embedding?\n* Footnote 2 should be in the main text (Sec 4.1). It is too hidden, but very important to let the reader knowing about it.\n", "belong_id": "SygXPaEYvH"}, {"uid": "SyxJtUoRtr", "paper_title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "### Summary:\n\nThis paper propose a new model for learning generic feature representations for visual-linguistic tasks by pretraining on large-scale vision and language datasets like Conceptual Captions and language-only datasets like BookCorpus and English Wikipedia. They demonstrate that the pre-training procedure can help improve performance on down-streaming tasks like visual question answering, visual commonsense reasoning. \n\nOverall I liked the design choices made in the presentation. Although the paper doesn't provide insights around what the representations have learned and how they differ from representations learned / used by existing methods, they have provided substantial evidence to suggest that pre-training helps in a lot of downstream tasks. \n\nAlthough it's hard to evaluate the paper without putting it in context with other concurrent works that have come out recently, I tried my best to evaluate the merits of the paper in isolation. \n\n### Strengths:\n\n- The paper explores an interesting direction of learning generic feature representations for visual-linguistic tasks for down-streaming tasks. Traditionally, people learn feature representations from scratch for each downstream task which might not always be possible if the training data is limited.\n- The paper does a decent job mentioning all the concurrent work in the space of learning multi-modal representations that have come out very recently. They distinguish the proposed method from existing work and also compare the performance of the proposed approach with concurrent work on downstream tasks showing performance on-par or better than existing methods.\n- I liked some of the design choices made in the paper. (1)  Instead of training a separate  transformer network for each type of input segments (question, ans, caption, image, etc). This makes the model easily extensible to other tasks as long as the correct segment embeddings are used to identify different input sources. (2) They also use a separate embedding for visual features instead of a common embedding  for both language tokens and visual tokens.\n- Unlike the pre-training task in concurrent work, the model was pre-trained not just on multi-modal datasets like conceptual captions but also on text-only corpus like BookCorpus and English Wikipedia. The authors claim that this leads them to learn better representations for longer sentences which they found useful for VCR task.\n\n### Weaknesses:\n\n- The authors claim that attention mechanism in cross-modal transformer by concurrent approaches is restrictive but doesn't give substantial evidence that this is true. What are the limitations for cross-modal attention mechanisms compared to a single transformer model as described in this paper.\n- On the contrary, by having a cross modal architecture, they can pre-train each modality separately on unaligned data. For instance, the text only transformer can be trained using large text corpora similar to BERT while the image only transformer can be trained on big datasets like OpenImages, ImageNet etc\n- While the paper gives a lot of empirical evidence that pre-training helps, it would have been interesting to develop an understanding of what the model is actually learning and how are these representations better than learning representations from scratch for each task. For instance maybe the authors can visualize attention similar to [1].\n\n### Other questions:\n\n- When training on text-only datasets, what is the input on Visual Feature Embedding since there are no associated images. The authors mention that for non-visual elements, the features are extracted on the whole image. It's still unclear what the associated visual features are for text-only datasets.\n- One of the pre-training tasks is masked ROI classification but it assigns a hard label to each ROI feature. It might be interesting to instead try learn the entire probability distribution (the output of a pre-trained classifier) by either minimizing the KL-divergence or by using softmax with soft-targets.\n- While the model was learnt on text-only data, as mentioned in the above section, will the model help from image-only datasets such as large-scale classification datasets?\n- While the models are tested on vision-and-language datasets, will these generic representations also be useful for unimodal tasks?", "belong_id": "SygXPaEYvH"}, {"uid": "S1ludY4GqS", "paper_title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a pre-trainable generic representation for visual-linguistic tasks call VL-BERT. VL-BERT extend BERT by changing the input from subsequent sentence to image regions and modify the caption words has the additional visual feature embedding. The authors pre-train the VL-BERT on the conceptual caption dataset and Wikipedia and book corpus dataset, empirical results show that the VL-BERT achieve the SOTA performance on the VCR, VQA and refer expression tasks. \n\nAs the authors mentioned in Table 5, pre-training the visolinguistic representation for vision and language tasks is very popular recently, and 5~6 similar works have appeared recently. One of the nice features I found on this work is it's joint train with text-only corpus and faster RCNN weight. While ViLBERT designs for easier extendable for other modalities, VLBERT is more focus on the representation learning on the vision and language, since the caption input also combines with the visual feature embedding. \n\nOverall the paper is well written and performs extensive experiments/ablations. There is some specific point that is not clear to me or needs further clarifications from the authors. \n\n1: The authors mentioned the improvement over tuning the visual parameters, I wonder what is the details on that? is the region proposal network's weight fixed? if not, how to avoid the shift on the proposal layer? Is the model still has the visual genome target or objective? Which layer is fixed/updated? and what is the optimizer and learning rate scheduler? \n\n2: I notice there is a change in the textual input which take visual feature embeddings. I wonder what is the performance without these features? What is the visual feature input for textual corpus? \n\n3: For the Masked RoI classification with Linguistic Clues, what if there are overlapped regions? what if the detection label from faster rcnn is incorrect? will this introduce any noise? \n\n4: For VCR tasks, it seems the VL-BERT_base w/o pre-training is performed similar compare to the with pre-training (only 0.7% lower on val of Q->A) I wonder what is the reason of this? Is this show the pre-training is not important for the VCR tasks? \n\n5: The VCR tasks also have the object bounding box correspondence, is VL-BERT take any of this supervision for input? If not, how does the VL-BERT learn the correspondence? \n\n6: For refer expression tasks, the VL-BERT_base is actually worse than ViLBERT on the detected regions. It's not a fair comparison since other models use bert-base model. \n\nOverall, I think this paper is well written and has solid experiment results. It will be great if the authors can further clarify the above questions. ", "belong_id": "SygXPaEYvH"}, {"uid": "H1xzzmnIFr", "paper_title": "BUZz: BUffer Zones for defending  adversarial examples in image classification", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a defense against black box adversarial attack. The authors train an ensemble of deep networks, and output a null label when the ensemble disagree. Success of adversarial attack is defined as the ensemble outputs an incorrect label that is not null. The authors experimentally show improved robustness to adversarial attack. \n\nThe idea is itself new, but very similar ideas are well known in the literature, and it is difficult to conclude that the proposed approach is superior. Several examples are:\n\nDefense by majority vote with ensembles has appeared several times in the literature (e.g. Pang et al 2019). The pro is that this paper proposes a novel way to create an ensemble by applying random linear transformations and rescaling to the input. But it is not clear this is superior compared to existing methods. \n\nRandomized smoothing (Cohen et al, 2019) guarantees smoothness of the classifier (and thus robustness to perturbation attack under certain norms). Note that randomized smoothing  provides certified guarantee against a stronger attack model (white box); it also guarantees the size of the margin (or buffer as the authors call it). The intuition of this paper is based on similar ideas so it seems necessary to at least compare with randomized smoothing. \n\nOutputting a null label is a major workhorse of adversarial defense. For example, previous work use a generative model to detect out of distribution samples; use a calibrated classifier to output null when low confidence. \n\nBecause the idea is only mildly interesting, good experimental support becomes crucial. However, I think there are several short-comings with the experiments:\n\nThe experiment contains only one (fairly old) attack method. Several recent alternatives such as SimBA (Guo et al 2019) can make the experiments more convincing. \n\nThe architecture is no longer the same for the target model (which is an ensemble with an additional random transformations) and surrogate model. It is unclear if the improvement is simply because of the difference in architecture. \n\nThe comparison to baselines seem unfair because it seems that the compared baselines do not have the option of outputting the null label. For example, a simple baseline of randomized smoothing + output the null label if the logit scores are below a threshold can make the story much stronger.\n\t\nMinor comments:\n\nSeveral suggestions on writing: the introduction contains much technical detail and even experimental results, and these are repeated again in later sections. The experimental section has many minor implementation details that could go into the appendix. \n", "belong_id": "HJlY_6VKDr"}, {"uid": "SkgjFhCcKr", "paper_title": "BUZz: BUffer Zones for defending  adversarial examples in image classification", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "First of all, I think the authors do not do enough literature research on the topic of adversarial examples:\n\n1. In Sec 2., the authors only mention the FGSM in White-box Attacks. It is widely accepted that when evaluating the white-box attacks, you should at least test PGD attacks and/or C&W attacks.\n\n2. When you try to propose an adversarial defense, you should report the performance under white-box adaptive attacks. Only claiming effectiveness under black-box attacks is not informative or convincing.\n\nThe experiment results are also weird. For example, in Figure 8, why the Vanilla clean accuracy on CIFAR-10 is only 88.35%? Besides, the clean accuracy on CIFAR-10 of 2-Networks Buzz is 75%, 8-Networks Buzz is 60%. This clean performance is not acceptable, no matter how robust is the model.", "belong_id": "HJlY_6VKDr"}, {"uid": "B1gGCEmg5S", "paper_title": "BUZz: BUffer Zones for defending  adversarial examples in image classification", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: This paper proposes the concept of buffer zones and suggests to use unanimous voting as a way to induce such buffer zones. To widen the buffer zone, they further propose to diversity the model. They then proposed a new metric for measuring defense and demonstrated that their method is effective.\n\nDecision: Weak Reject. This paper is fairly intuitive, but I am not sure about the fairness of the comparisons in the paper, and the level of rigor of the experiments.\n\nI think the conjecture that buffer zones are widened when the models are diverse deserve to be empirically tested. It is not clear to me a prior how exactly the buffer zones widen (even though the belief that they widen is intuitively appealing). I think one way to potentially characterize the buffer zone is by actually performing white-box attack experiments comparing:\nWhite-box attack vulnerability of unanimous voting vanilla models\nWhite-box attack vulnerability of unanimous voting diversified models\nI would also encourage the authors to think creatively about other ways to back up the the buffer zone claims put forth in the paper.\n\nI am also a little puzzled with the authors choice of the diversification procedure. The procedure c(x) = Ax + b will only linearly transform each column of the image, but not each row. This design choice feels rather ad hoc. Why did the authors settle on Ax + b in particular? Why not xA + b? Or AxC + b?\n\nRegarding the fairness of the experiments, do the models that the authors compare against have the luxury of returning a Undecided label? If not, then the problem formulation is fundamentally different, and I do not think the comparisons are necessarily fair. Are there any papers out there that also allow for an Undecided label? If so, they should be the baselines that one compares against. I have a rather hard time believing that this is the first paper to try unanimous voting across an ensemble.\n\nI am generally inclined to switch to weak accept so long as the other reviewers are willing to accept that the experiments are sufficient and the comparisons are fair. I am not opposed to the new problem setting, since I think the setting makes sense. I just want to know that the paper is doing due diligence regarding related work in this setting.", "belong_id": "HJlY_6VKDr"}, {"uid": "Bygq9J7zKr", "paper_title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a hierarchical extension to existing work in vision-based model predictive control. Here, a hierarchical model is optimised to find sub-goals that minimise the planning cost (bottleneck states), so as to allow for improved planning to goal states expressed in higher dimensional state spaces. As expected, results show that this hierarchy improves tasks execution success rates. \n\nThe paper is clearly written, although many sub-components of the architecture are described in other articles. While I appreciate that the work is incremental, and that it relies on a number of previously modules, this brevity and the architectural complexity means that the work will be challenging to reproduce or adapt to different tasks. \n\nNevertheless, I believe the idea is interesting, and the paper does achieve its stated goals of allowing for entirely self-supervised learning without motion primitives, demonstrations and rewards, so makes a useful contribution here.  Compositional planning through sub-goals is a good idea, and this paper adds a useful mechanism to identify suitable sub-goals in a latent space and plan through these.\n\nHowever, I don't believe that this approach is in any way practically feasible at present, and the paper makes a number of hyperbolic claims that should be toned down to give a more balanced perspective (I appreciate that the authors attempted to do so in their limitations section, but this needs to be done throughout the paper).\n\nSpecifically:\n\n1.) Please avoid the use of relative improvement percentages (200% performance improvements in the abstract - the 20% absolute performance is strong enough to stand alone).\n\n2.) The paper oversimplifies the importance of exploration and data collection in the proposed approach. The paper states that a uniform random policy in the continuous action space of the agent works well, and offsets the data collection to 'any exploration policy', but this is a key requirement if the proposed approach is to be useful. At present the method seems to be relying on typewriting monkeys writing Shakespeare as a precursor to control. An exploration policy using identified sub-goal of interest may be a particularly valuable extension in future work, but for now the paper glosses over this aspect.\n\n3.) The use of the term 'long-horizon tasks' and 'manipulation'. In line with point 2, my personal feeling is that any task or state that can conceivably be explored and accessed by uniformly sampling from a continuous action space does not qualify as a long-horizon task. From the results and videos, it seems that all of the tasks could be solved by following a single trajectory in 3D space, ie. swing left to knock objects off the table, then right to close the door. Terming this 'long-horizon' is a bit of a stretch, as is the notion that flopping about a table bashing into objects counts as 'manipulation'. Motion planning and trajectory following are not long-term horizon manipulation tasks, solving towers of Hanoi is. Moreover, I appreciate that experiments were conducted on real robot datasets, but this seems to be more of an exercise in latent space anthropomorphism than practical evidence of a feasible control policy.\n\n4.) Limitations regarding latent space expressiveness.  As mentioned in the limitations section of the paper, the proposed approach is heavily reliant on a latent space that fully captures the scene and can roll-out future states sensibly. This is an extremely challenging problem, and one which appears to affect the proposed approach, for example, in the accompanying video (3:.40 - 3:59), the desired goal state contains two objects standing on the table and a closed door, but the policy only closes the door, while objects are knocked over. This seems to indicate that the latent space and planning is unable to learn good object embeddings and spatial representations, and that the type of task that can be solved is along the lines of move forward and to the right when the black blob is on the left of the image, or move down to the left, when there are some coloured blobs on the table. \n\n5.) Limitations around goal state representations . Following on from 4, the identification of important aspects in a given image or latent space represents a major challenge to the proposed approach. Given an image of a desired state, how can the proposed approach identify which elements in the scene are required for success, and which are simply distractors? At present, I see no mechanism by which this could ever be learned in a self-play setting or through an image-based goal state. Eg. what if the image indicated that I want the two objects to be in a specific position, and I never cared about the state of the door?\n\nDespite these limitations, the granularity of the solution is fine for a proof of concept work like this, and is itself a commendable achievement. Unfortunately, the paper's choice of language, relying on terms like 'long term horizon' and 'manipulation' for simple reaching and pushing tasks exaggerates the state of robot learning, and is potentially misleading to those less familiar with the field.\n\nDespite my gripes, the paper definitely meets the ICLR threshold of 'accept if you'd share it with a colleague', and I believe it is is a useful approach to sub-goal identification and a nice piece of work, so I recommend acceptance.\n\n", "belong_id": "H1gzR2VKDH"}, {"uid": "rylPuR2RKB", "paper_title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a method, hierarchical visual foresight (HVF) that learns to break down the long horizon tasks into short horizon segments. It first generates the subgoals conditioned on the main goal. These subgoals are optimized to have meaningful states and used for planning. The experiments on Maze navigation, simulated desk manipulation, and real robot manipulation show significant performance gain over the planning method without subgoals and model-free RL. \n\nThe paper tackles a novel and challenging problem for the long-horizon tasks. Also, the paper is well written and easy to understand.\n\nQuestions/Concerns:\n\n- How important is the output quality of the video prediction model? \n\n- Finding subgoals that split into the optimal subtasks seems not easy. Are the learned subgoals actually reasonable for the final task? I can see that Fig. 6 has some examples of subgoals but is hard to recognize what are the subtasks. \n- Also, is there a chance that subgoals are randomly selected? Could you verify that subgoals are consistent across different initialization/runs? \n- Could you provide us any failure cases if there's any and explain us why this happens? \n\n- The number of subgoals needs to be fixed. I assume this number depends on how complex of the task is. It is surprising that only 2 subgoals were enough for the BAIR Robot push dataset (Tab. 1). The authors commented that 'the sampling budget allocated for subgoal optimization is likely insufficient to find a large sequence of subgoals.'. Will it actually be solved by increasing the sampling? Or is it possible that the dataset is too simple or simply finding optimal subgoals is hard.  Could authors comment more about this issue?\n- Also, how can you find the right number of subgoals? \n\n- The computational requirement is one of the weaknesses. Please provide the comparison of how much is the computational cost of the proposed method compared to other ones such as VF (without subgaols), TAP, and RIG. \n", "belong_id": "H1gzR2VKDH"}, {"uid": "BkgoQYmKKS", "paper_title": "Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Disclosure on reviewer's experience: I am not an expert on adversarial attack methods or defenses, but I am well read in the general literature on robustness and uncertainty in deep neural networks.\n\nThe authors present a biologically inspired sleep algorithm for artificial neural networks (ANNs) that aims to improve their generalization and robustness in the face of noisy or malicious inputs. They hypothesize that 'sleep' could aid in generalization by decorrelating noisy hidden states and reducing the overall impact of imperceptible perturbations of the input space. The proposed sleep algorithm broadly involves 1) converting the trained ANN to a 'spike' neural network (SNN), 2) converting the input signal (pixels) to a Poisson distributed 'spike train' where brighter pixels have higher firing rates than darker pixels, 3) propagating the neuronal spikes through the SNN, updating weights based on a simplified version of spike-timing-dependent plasticity (STDP), and 4) converting the network back to an ANN after the sleep phase has finished. They present a detailed comparative study spanning three datasets, four types of adversarial attacks and distortions, and two other baseline defense mechanisms, in which they demonstrate significant improvements (in some cases) of the sleep algorithm over the baselines.\n\nThe core concept behind the authors' work is novel and interesting, and the experimental design is thorough and well controlled. Although the results are (I would argue) somewhat mixed, they are nonetheless positive enough to encourage more work in applying 'sleep' and other relevant ideas from neuroscience to the problem of robustness in deep neural networks. I have some questions and concerns which I will detail per-section below, but overall, I believe that this paper is a valuable contribution to the literature and should be accepted once the authors have made a few necessary revisions.\n\nSection 1: Introduction\n\n'We report positive results for four types of adversarial attacks tested on three different datasets (MNIST, CUB200, and a toy dataset) ...'\n\nIt's debatable whether or not the results from the CUB-200 dataset are positive. The sleep algorithm fails to outperform the baselines for each attack type (except for an almost negligible advantage in accuracy on JSMA) and barely even outperforms the control network in most cases (2/4 attacks it actually underperforms the control). I think the authors should consider rephrasing this statement to better reflect the actual results.\n\nSection 2: Adversarial Attacks and Distortions\n\nFGSM: The notation used here is somewhat inconsistent with the source paper. Goodfellow et al use epsilon to denote what I think the authors call eta, and call the second term, epsilon*sign(grad(J)), eta. Furthermore, the authors state that 'this represents the direction to change each pixel in the original input in order to decrease the loss function.' But this doesn't make sense. An adversary should want to *increase* the loss function enough to cause a misclassification. Goodfellow et al use this expression to formulate a L1-like regularization term and describe the training procedure 'minimizing the worst case error when the data is perturbed by an adversary', which seems more sensible. This section should be rewritten to be more consistent with the source.\n\nSection 3: Adversarial defenses\n\nRegarding distillation: 'We use T=50 to compare with the sleep algorithm'\n\nThe authors should elaborate a bit more on the reasoning for this choice. It seems very arbitrary.\n\nSectioin 4: Sleep algorithm\n\n1. Algorithm 1: Why is line 9 inside of the for loop? It doesn't seem to be at all dependent on t. One would expect the input to only need to be converted once. Additionally, in lines 11-13, the l's in W(l,l-1) and similar should be unbolded. It's confusing that the format changes (unless I am missing something and it's actually a different variable).\n\n2. Spike trains should be more rigorously defined, preferably with formalized notation. It's a bit unclear exactly what they are from the current text. Are they just parameters for a Poisson? Or outputs from a poison over T time steps? Or something else?\n\n3. 'weights are scaled by a parameter to induce high firing rates in later layers'\nIt would be good to include more details on this parameter, how the values are chosen, and the intuition behind this idea. I assume it's because of higher level feature representations in later layers of deep neural networks.\n\nSection 5: Results\n\n1. It's confusing that sometimes accuracy refers to classification accuracy and sometimes adversarial attack accuracy. I would recommend assigning a different name to the latter, or making sure that a qualifier precedes every reference to 'accuracy' in this section.\n\n2. In the second section of the results table (which is missing a label), why is the JSMA value for Defensive Distillation bolded? The distance measures for both the control network and for fine-tuning are higher. It seems like fine-tuning should be the one bolded.\n\n3. Figure 1: caption is incorrect; it states 'adversarial attack accuracy' and it should be 'classification accuracy', otherwise the plots make no sense.\n\n4. 'we observe that in the Patches and CUB-200 dataset, sleep has beneficial results in moving the accuracy function above the other defense methods'\nIt should be noted that this is only true for eta < 0.1. After that, sleep and the control both converge to 50% accuracy. Also this sentence should be reworded to be less visual and more quantitative (e.g. sleep tends to have higher median accuracy scores than the other methods for eta < 0.1).\n\n5. 'We observe that performance continued to drop after a sufficiently large amount of noise was added'\nMore than that, the other methods converged to a small band of accuracy values; sleep continued to deteriorate. This is a significant difference. It would be a good idea to re-run this experiment with a binary classification problem (e.g. only two digits of MNIST) and see if this phenomenon still occurs. Then, the noisy sleep classifier predictions could simply be inverted to get improved accuracy scores.\n\n6. In the analysis of JSMA, as noted before,, it's rather dubious to claim that sleep had any kind of significant effect on the attack success rate (or distance) for CUB-200. I would rewrite this section to better represent the results.\n\n7. Figure 2 formatting: Legend is overflowing out of the first figure. Additionally, the legend colors should be made to match across all three figures, and the legend should either appear in all three (if necessary for some reason) or only in one.\n\n8. Figure 2: The caption is incomplete and possibly incorrect. It's not clear why the first and last figures differ from each other, and the caption does not indicate this. The caption also only mentions two datasets, even though it says 'for the following three datasets'.\n\nAppendix:\n\nGeneral formatting needs improvement. A lot of figures are off-centered, text misaligned, missing axis labels, etc.", "belong_id": "r1xGnA4Kvr"}, {"uid": "HJerk1855H", "paper_title": "Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes an ANN training method for improving adversarial robustness and generalization, inspired by biological sleep.\n\nI'm leaning towards accepting as it seems to be an original concept and has fairly extensive empirical results that are somewhat promising.\n\nThe idea of a sleep phase as an alternative to explicit adversarial or generalization training is interesting. The results suggest that the approach works reasonably well in many cases.\n\nSuggestions for improvement / clarification:\n- The mapping from biological sleep to the actual algorithm + pseudocode used could benefit from more thorough explanation. It is not clear which choices are arbitrary vs well-principled.\n- Was the optimal sleep duration determined empirically for each experiment?\n- I agree with the authors' proposed future work of better understanding and standardizing this approach.\n- Consider combining this approach with the existing adversarial or generalizing approaches (instead of as an alternative). Do they complement each other?", "belong_id": "r1xGnA4Kvr"}, {"uid": "H1e8e81ntS", "paper_title": "Scalable and Order-robust Continual Learning with Additive Parameter Decomposition", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a training framework that: \n(i) can efficiently handle catastrophic forgetting in a large number of tasks\n(ii) is robust to the ordering of the tasks\n\nThe high-level idea is to decompose learning parameters into two sets - one set that depends on the task and one set that is task agnostic. Hierarchal clustering is used to improve the efficiency of the training process by considering the decomposition of parameters at multiple levels (and not just 2). The paper shows improvements in terms of accuracy, stability, and order-robustness and provides ablation results (for various modifications of their proposed model) and considers a setup with around 100 tasks. The paper/idea is quite interesting and the results seem promising:\n\n* The results in figure 5: (d) and (e) are very interesting. It seems as if all the 'previous' knowledge is being contained in the task-specific parameters. In general, I like the idea of being able to 'forget' all the previous knowledge. I want to clarify one thing here: My understanding is that after training the model on tasks 1 to 5, the weights corresponding to task 1 are dropped (that is just the task-specific parameters tau and not the mask). Then before even one gradient update is applied, the model is re-evaluated on task 1. Is that correct?\n\n* Figure 7 seems to suggest that the drift is reduced by the proposed approach. What does the presence of multiple markers (of the same color) mean? For example, there are two green triangles in (b).\n\n====================\n\nBut, many things should be clarified for understanding the paper properly and for making a fair assessment of the claims made in the paper. I would be happy to update my score based on the authors' response to the following:\n\n* My biggest concern is the choice of baselines. The paper (rightly)  highlights that their work improves over many existing works as they provide a mechanism to 'retroactively update task-adaptive parameters' for the previous task. But none of their baselines have this mechanism built-in. So while there is a clear advantage with the proposed approach, the comparison is unfair and the baselines should have considered approaches like GEM [0[ and A-GEM[1] while also provide a kind-of retrospective mechanism to correct the weights corresponding to the subsequent tasks. Without such a comparison, it is difficult to comment on the benefits of the approach.\n\n* On page 2, paragraph 3, the paper mentions that 'APD does not increase the intrinsic network complexity as existing expansion-based approaches do'. This claim seems to be loose since a new mask is learned for each task and needs to be persisted for all the subsequent tasks. So there is an 'expansion'-like step involved. The authors should clarify this detail in the context of being memory efficient.\n\n* The authors propose a simplistic regularization approach (L2) to ensure that the shared parameters do not share too much across tasks. While it is good that a simple approach works so well, it would help if the authors discussed what do they think the reason is. EWC [2] and the authors' results indicate that regularization in the parameter space does not work as well as regularization in the function space. Thus the L2 regularization approach is not likely to work well.\n\n* Some parts of the paper needs to be reworded or clarified more. For example, since a per-task mask is being learned, there are two sets of weights being learned per task (the mask and the task-specific parameters). So there are more parameters to learn and not just the sparse task-specific parameters.\n\n* As I understand, the paper uses 'order robustness'  to mean avoiding 'concept drift' (or 'catastrophic forgetting'). I might be missing something (in plain sight) but when I think of 'order robustness', the order of tasks should not matter. This is somewhat different than avoiding concept drift.\n\n* Is the hierarchical clustering being done for each neuron (of the given model)? If yes, how does this approach scale to large neural nets? In general, how does the cost of doing the hierarchical clustering affect the training cost (of the model)?\n\n* Metrics: I am a little confused by the definition of the 'final task-average performance' metric and could interpret it in at-least two ways. Could the authors please clarify this.\n\n* I do not understand some of the results in Figures 3 and 7, for the capacity of Progressive Neural Nets (PGNs). In general, PGNs add one new column (copy of base model) for each task. So the capacity of the PGNs should always be a multiple of intial model capacity. The results do not indicate that.\n\n* For the STL, the value of AOPD and MOPD suggests there is a good amount of variance when training the models. In this context, it would be helpful to know the variance associated with the other reported results as well.\n\n* Figure 6 is somewhat misleading as it does not account for the mask parameters that also need to be learned per-task.\n\n====================\n\nThings that should be clarified in the paper (but did not impact the score):\n\n* Is the attention (sigma) hard-attention or soft attention? \n\n* Is the attention applied per task (ie one scalar value per task) or layer or neuron?\n\n* Equation 2 seems to add a lot of complexity to the training mechanism (to correct for the weights of the previous tasks). Did the authors consider some other update/corrective mechanism that could be applied once a task has been learned? Please note that I am not criticizing the equation because it is complex. I am curious about the alternatives that the authors considered.\n\n* Are there any kind of mathematical guarantees when using equation 2? If not, why should it be a better alternative to approaches like GEM[0]?\n\n* Did the authors consider Piggyback like network for the remaining tasks as well?\n\n====================\n\nCertain important citations seem to be missing:\n\n* Works like GEM[0] and AGEM[1] fix the problem of 'unidirectional' transfer of knowledge to some extent.\n\n====================\n\nSome minor corrections for the updated version:\n\n* Typos: eg 'catastropihc', \n* In the section on Large-scale training, the STL model uses 100 times more params and not 10 times.\n\n====================\n\nReferences:\n\n[0]: GEM: https://arxiv.org/abs/1706.08840\n[1]: A-GEM: https://openreview.net/pdf?id=Hkf2_sC5FX\n[2]: EWC: https://arxiv.org/pdf/1612.00796.pdf\n\n", "belong_id": "r1gdj2EKPB"}, {"uid": "S1e4AYBAtB", "paper_title": "Scalable and Order-robust Continual Learning with Additive Parameter Decomposition", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: The paper addresses continual learning challenges such as catastrophic forgetting and task-order robustness by introducing a new hybrid algorithm that uses architecture growth as well as parameter regularization where parameters of each layer are decomposed into task-specific and task-private parameters. They also use a simple trick to The authors perform experiments on Split CIFAR100, CIFAR100 Superclass, Omniglot, and a sequence of 3 datasets (SVHN,CIFAR10,CIFAR100). The maximum number of tasks in the experiments is 100 for Omniglot-rotation. The authors show superior performance to EWC (a regularization-based method), P&C (architecture-based method), DEN (architecture-based method), PGN (architecture-based method), RCL (architecture-based method), etc. \n\nPros:\n+ The paper is well-written and has motivated the problem of scalability and forgetting\n+ Proposing a new hybrid approach that benefits from the best of both worlds (maximum usage of the capacity with parameter regularization followed by logarithmic architecture growth at arrival of new task using layer-wise parameter decomposition.\n\nCons that significantly affected my score and resulted in rejecting the paper are as follows:\n\t\n1- Lack of measuring forgetting: \nAuthors indicate in the abstract that a continual learning model should effectively handle catastrophic forgetting and reiterate on this on other parts of the paper yet there is no table/figure that shows the initial performance of the model on each task so that readers can compare it with the reported accuracy after being done with all tasks. Having a method with high average accuracy does not necessarily mean it has minimum forgetting. You can use forgetting measurements such as BackWard Transfer, introduced in [1] or forgetting ratio defined in [2] for this assessment. A continual learning paper without proper measurement of forgetting is incomplete.\n\n2 - Large-scale experiment is not convincing:\nAuthors believe scalability has not been addressed well in the literature (page 1&2) and claim it as one of their main contributions and making it crucial to support this claim. However, the experimental setting chosen for this claim is not convincing. Authors have chosen Omniglot-rotation as their longest sequence of tasks with 100 tasks where each task has 12 classes and in each class, there exists 80 images. This will make the total dataset of size 96K images which is still far from being large-scale. While I am aware of the fact that in the current CL literature, the maximum task sequences length is only 20 (Split CIFAR100) and I  agree that having an order of magnitude increase in the # of tasks is beneficial,  however, Omniglot is still a toy benchmark and does not serve as a large-scale dataset by only extending it to different random rotations. Moreover, the architecture used for this experiment is LeNet which oversimplifies the problem to address. For incremental learning, I would personally think of ImageNet as a good example and for continual learning of multiple datasets you can consider the existing sequence of 8 tasks benchmarked in [2] and [3] where you can evaluate your method on more realistic images with a total of over 400K images and significant shift in the distributions. As a side note, the key idea behind the proposed method is that this method is able to decompose the parameters into task-specific and task-private whereas in the Omniglot experiment it is not intuitive that what is there between the random rotations that is shared among the task. A more detailed discussion on this would be enlightening. \n\n3 - No standard deviations shown in the results:\nAlthough the results are said to be average over 3 runs, no STD is reported. Given that in the most important experiment of this paper (Omniglot) the difference between Accuracy obtained by PGN and APD is not significant (79.35% vs 81.6%). \nIn the current CL literature, robustness to the order of the tasks is shown by performing multiple permutations of the tasks and reporting average and STD. It is needed that authors show results for this for a fair comparison. \n\n4 - Lack of regularization-based baselines:\nConsidering the fact that the proposed method is a hybrid approach, it is reasonable to compare against both architecture-based and regularization-based approaches. However, most of the baselines are chosen from the former category and EWC is the only baseline for the latter category which is relatively old and has been outperformed by large margins in the past couple of years such as SI [4], VCL[5], HAT [2], PackNet [6], MASS [7], and UCB [3]. \n\nLess major (only to help, and not necessarily part of my decision assessment):\n\nPlease consider explaining connection to prior work (HAT): While the literature review seems comprehensive, authors have missed one important previous work from ICML 2018 [2] called Overcoming Catastrophic Forgetting with Hard Attention to the Task or HAT. Both HAT and APD use an attention mechanism to alleviate forgetting. Considering HAT is a very strong baseline, I highly recommend authors provide a comparison with it. Its an efficient and relatively scalable method that has very small BWT. \nI recommend authors provide their methods ability for zero-shot transfer or so called forward transfer metric to further support their method.\nHyper parameter tuning: It is also worth mentioning how the tuning process was performed. In continual learning we cannot assume that we have access to all tasks' data, hence authors might want to shed some light on this.\n\nMinor point: On page 8, last paragraph, the authors state that a masked-based pruning technique (Piggyback) is immune to forgetting which is not an accurate statement (Note that PGN is indeed zero-forgetting by definition). All masked-based methods lose some of their performance prior to pruning. While it is correct to say that their post-pruning performance is 100% recoverable by saving the mask, forgetting should be measured with respect to their performance prior to pruning because that is their trade-off to give up accuracy in lieu of freeing space for future tasks.\n\nReferences:\n[1] Lopez-Paz, David, and Marc'Aurelio Ranzato. 'Gradient episodic memory for continual learning.' Advances in Neural Information Processing Systems. 2017.\n\n[2] Serra, J., Suris, D., Miron, M. & Karatzoglou, A.. (2018). Overcoming Catastrophic Forgetting with Hard Attention to the Task. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:4548-4557\n\n[3] Ebrahimi, Sayna, et al. 'Uncertainty-guided Continual Learning with Bayesian Neural Networks.' arXiv preprint arXiv:1906.02425 (2019).\n\n[4] Zenke, Friedemann, Ben Poole, and Surya Ganguli. 'Continual learning through synaptic intelligence.' Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n\n[5] Nguyen, Cuong V., et al. 'Variational continual learning.' arXiv preprint arXiv:1710.10628 (2017).\n\n[6] Mallya, Arun, and Svetlana Lazebnik. 'Packnet: Adding multiple tasks to a single network by iterative pruning.' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n\n[7] Aljundi, Rahaf, et al. 'Memory aware synapses: Learning what (not) to forget.' Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\nPost-rebuttal response:\n\nThank you for taking the time to go through comments and providing your responses. \n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n[Authors' response:] In Table 2 (paragraph Continual learning with heterogeneous datasets), we have experimental results with heterogeneous datasets, where continual learning models are evaluated on a sequence of different datasets (CIFAR-100, CIFAR-10, SVHN). We agree that experiments with massive datasets will be helpful, but we do not have sufficient time to perform all the experiments during the short rebuttal period. Hence, we only compared against HAT in our revision (Please see Table A.7 and Figure A.10) on the sequence of 8 tasks [2][3] you mentioned. We directly followed all experimental settings on the paper and the code of the authors (https://github.com/joansj/hat). APD (82.42 +- 0.5 %) outperms HAT (80.36 +- 1.2 %) in terms of accuracy. Although HAT shows a marginal forgetting (0.14 %) during training, the models is task-order sensitive (AOPD: 7.95%, MOPD: 23.15%) on difficult sequences of tasks as CIFAR10, CIFAR100, and FaceScrub (Please see Figure A.10) while APD consistently shows a reliable performance with lower OPDs (AOPD: 2.09%, MOPD: 4.40%) regardless of the task order. We will add in all baselines and APD variants in the final version of the paper for this 8-dataset experiment, if it gets accepted.\n\n[Reviewer's response:] while I thank the authors in providing this comparison, I would not call this 'significant outperforming'. According to Table A.7 HAT method achieves  80.36% using the memory needed to store one single network in the memory on which they learn attention masks without using extra memory while APD uses 81% more memory only to achieve 82.42% average accuracy (2.13% increase) which is clearly not a fair comparison to me. Authors should either use the same memory for HAT (using a larger network architecture) or use a smaller memory size for APD and re-evaluate this comparison otherwise it is not conclusive which method is superior. Given the large difference in memory usage I suspect HAT will outperform ADP if given more capacity. While I agree with authors that regularization based approaches can be limited by the number of tasks, in the experimental setting used in this paper, this is not proven to be the case as these methods have not reached their maximum capacity and in fact are still performing strongly well compared to a hybrid approach which is presumably supposed to be better. I appreciate the novelty of the idea of decomposing parameters but it is not clear whether this factorization is actually performed given the high capacity needed to learn these tasks. Therefore, the results are still not convincing to me. \nIn addition,  as also brought up by R1, 'My biggest concern is the choice of baselines. The paper (rightly)  highlights that their work improves over many existing works as they provide a mechanism to 'retroactively update task-adaptive parameters' for the previous task. But none of their baselines have this mechanism built-in. So while there is a clear advantage with the proposed approach, the comparison is unfair and the baselines should have considered approaches like GEM [0[ and A-GEM[1] while also provide a kind-of retrospective mechanism to correct the weights corresponding to the subsequent tasks. Without such a comparison, it is difficult to comment on the benefits of the approach.' --> regarding the comparison between APD and HAT in the order-robustness, this also seems as a big concern to me. Maybe I am missing it in the long list of comments and replies, but I am not able to find a clear response from authors in providing a fair assessment without their order robustness constraint mechanism (Eq. 2). This is an auxiliary advantage that only APD is benefitting from and makes the comparison difficult.  It should be either given to all methods or none.\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n[Authors' response:] This is a critical misunderstanding as those results are already provided in the paper.  We already clearly report the performance evolution of several tasks during the course of training in Figure 5 (a)-(c). Figure 5 (a)-(c) clearly show that APD does not suffer from catastrophic forgetting and even improves performance on previously trained task during continual learning (Figure 5 (b)), which is an effect of update on the task-shared parameters. Please see Page 7, Preventing catastrophic forgetting paragraph for more detailed discussions. \n\n[Reviewer's response:] I disagree with authors' response regarding Figure 5 being a complete forgetting measurement supported by this sentence in Preventing catastrophic forgetting paragraph: 'APD-Nets do not show any sign of catastrophic forgetting' and showing the performance of only 3 tasks out of 20 in which accuracies are barely readable due to the coarse scale of the figure and more importantly authors do not provide other methods' performance on these tasks. However, I thank authors for providing the comparison with stronger baselines and proper forgetting measurement in A.6 and A.7, I highly recommend swapping Figure 5 with your newly obtained quantitative forgetting measurements shown in A.6 and A.7 in the appendix \n (once fairly compared according to my comment above) as they provide a better support for forgetting avoidance. \n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n[Authors' response:] The only hyperparameters required for APD are \\lambda_1 and \\lambda_2, which controls the capacity of the task-adaptive parameters and model drift respectively.  Since there is a trade-off between efficiency (network capacity) and accuracy, the users only need to tune them according to their priority. The model is not sensitive to hyperparameter configurations unless they are in the correct scale, and the details of the hyperparameter configurations are given in A.1. \n\n[Reviewer's response:] Hyper-parameters can be a lot more than just \\lambda_1 and \\lambda_2 in your experiments. Batch size, optimizer's learning rate, weight decay, validation set size, etc are the parameters that are usually left out  in the CL settings and are not explained how they were tuned. As I said before, it is worth explaining what this sentence from A.1 means 'All hyperparameters are determined from a validation set.' so if this validation is composed of the data from all tasks or they followed some procedure like A-GEM paper in which it is assumed data of only 3 tasks is available in the beginning for tuning purposes.\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n[Authors' response:]  What you mentioned about mask-based methods may be true in general, but not in the case of Piggyback. Piggyback is composed of frozen backbone and task-specific masks, such that the backbone network is fixed without any updates during training, and the model only learns the task-specific pruning masks, which are *stored*, such that we can recover the performance on any previous tasks at any future points. Thus Piggyback does not perform actual pruning, and thus there is no loss of accuracy and forgetting on the previous tasks.\n\n[Reviewer's response:] I disagree with your statement about 'no loss of accuracy'. Restating from Piggyback paper on its page 4: 'The key idea behind our method is to learn to selectively mask the fixed weights of a base network, so as to improve performance on a new task. We achieve this by maintaining a set of real-valued weights that are passed through a deterministic thresholding function to obtain binary masks, that are then applied to existing weights. By updating the real-valued weights through backpropagation, we hope to learn binary masks appropriate for the task at hand.' \nThis simply means they learn binary mask per task by using a thresholding function and save this mask. However, if you evaluate the model on a given task **prior to making** you obtain a different performance compared to evaluating after masking (this is what you save) where the former is usually higher or sometimes similar to the former because prior to masking, there exist more parameters while by masking some parameters will be 'freed' to be used for future tasks. This difference is what I am referring to as true forgetting and is zero only if evaluation prior and post masking are exactly the same because by storing the learned masks you can only recover the post masking performance. \n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Authors' response:] The scalability problem we aim to tackle in our work is the scalability to number of tasks. This is because scalability to the size of the network or the number of data instances is basically the problem with generic machine learning and are not the main problem associated with continual learning. Since continual learning models learns on a sequence of tasks, we were more interested in how the existing (expansion-based) continual learning methods and ours behave on large number of tasks, in terms of catastrophic forgetting and network capacity. However, we agree that the term large-scale continual learning may be misleading and have renamed the paragraph to scalability to large number of tasks in the revision. \n\n[Reviewer's response:] I disagree with the first sentence that dataset size is not the main problem associated with CL. It is an important factor that should be considered because dataset size and number/diversity of classes can significantly increase forgetting on early tasks as the distribution shift between the tasks will be significant. I understand the intention of authors and their interest in modeling large sequence of tasks, however introducing the method as scalable is misleading and in addition to the text which is corrected now, should be also corrected in the title of the paper.\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nI accept the response for the remaining questions from authors but intend to keep my score. However, I will be happy to update it based on the authors' response to the very first comment above regarding providing a fair comparison in memory size and order-robustness mechanism.", "belong_id": "r1gdj2EKPB"}, {"uid": "HylrvBHXqH", "paper_title": "Scalable and Order-robust Continual Learning with Additive Parameter Decomposition", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper introduces an additive parameter decomposition (APD) approach to continual learning in the sequential task classification setting and evaluates it across a number of dimensions, including task order robustness, which is comparatively less well researched. Extensive experiments show the novel approach has superior performance to relevant baselines, and provides important data about the order robustness of popular existing approaches.\n\nPros:\n- Paper tackles the task-order sensitivity challenge in continual learning and introduces an effective order-robust approach.\n- Method is scalable, parameter growth is logarithmic, forgetting of irrelevant knowledge comes for free.\n- Baselines are relevant, although they only cover one of the families of approaches. Performance looks consistently better than baselines both in terms of classification accuracy as well as order robustness.\n- Although non-standard, the Omniglot experiment with 100 tasks is interesting w.r.t. both measures. Performance is on par with STL, also in terms of robustness to order.\n- Order robustness measures are introduced and motivated. This is particularly relevant for future research beyond simple accuracy comparisons.\n\nCons:\n- Network architecture used for experiments as well as the exact details of the datasets are non-standard, making results very hard to compare with other papers, so one needs to rely on provided baselines only. Please post citations for papers where the experimental methodologies were adopted from if this is not the case, I many not be familiar with them!\n- Classification accuracies seems relatively low across the board, especially for CIFAR-100 results. Could you please report some results in the experimental setting used by one of your baselines in the original paper?\n\nI am inclined to recommend acceptance due to novelty of order robustness analyses and competitive properties of the method, but I would like clarifications to my experimental questions.", "belong_id": "r1gdj2EKPB"}, {"uid": "H1gXFeAOFr", "paper_title": "Ecological Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary\n\nThis paper discusses the value of creating more challenging environments for training reinforcement learning agents. Specifically, the paper focuses on three characteristics of the environment that the paper claims are necessary for developing intelligent agents. The first of these properties is stochasticity in the environment transitions, specifically stochasticity that is independent of the action taken by the agent. The next is sparsity of rewards; discontinuing the use of reward shaping to define desired behavior. Finally the paper argues that environments should not be episodic, that natural environments are continuing tasks so research focus should be around solving continuous tasks.\n\nReview\n\nMy primary concern about this paper is the largely insufficient literature review. Many of the claims made in the motivation of this paper are not novel to this paper and are, in fact, incredibly vibrant sub-communities of study within the field of Reinforcement Learning. A more careful literature review should have easily found these communities and more nuanced claims could have been made. I will give concrete examples of the most important cases of missing literature in the following paragraphs, but this list is not exhaustive.\n\nThe paper claims that most standard RL environments include detailed reward functions that unnecessarily shape learning and inject bias into the learning process. While I agree that this is problematic, I disagree that this paper provides any novel insights towards this problem. The problem of learning from sparse rewards is well-known in the RL community and is a hot-topic of study. Even in the standard environments cited by the paper we have Montezuma's Revenge and Pitfall, two environment notorious for their difficulty due to sparse reward; each of which with its own host of literature surrounding only the single environment (for instance, I encourage the authors to investigate the highly controversial Go-Explore paper by Uber and its references). Other environments not considered by this paper include the Malmo (Minecraft) learning environment, around which NeurIPS 2019 hosted an extremely sparse reward competition. Another (overlapping) community of RL research interested in the sparse reward setting is the intrinsic reward community, one such paper being Riedmiller and Hafner et al. 2018.\n\nThis paper claims that all standard environments are episodic. Of the environments listed as 'standard' by this paper, this claim does not even hold. However, there is a large chunk of the RL community that is not represented here. The continual learning and life-long learning communities are focused exclusively on the problem of non-episodic learning. Some example environments used by the community include Malmo, MuJoCo, DeepMind's Lab environment, and many smaller toy domains designed to showcase individual problems including Cart-Pole, RiverSwim, Pendulum, and Acrobot; with the smaller environments from OpenAi Gym cited by this paper. Another smaller community to investigate would be the average reward formulation of the RL problem, which fairly exclusively focuses on the continual learning problem.\n\nFinally, this paper seems vaguely reminiscent of a few particular environments that I have seen in the literature previously. For example, Berkeley's robot task discovery playpen (see for example Singh, Yang, Hartikainen, Finn, Levine 2019). Or an even more similar simulated environment being the Playroom environment by Singh, Barto, Chentanez 2005. Finally, Malmo has been used in a similar way as the tool-building examples mentioned in this paper.\n\nThere were a few key issues with the experiments discussed in this paper. The first of which being 'Hypothesis 1' which states: 'Non-episodic learning is more difficult than episodic learning because the agent must handle a non-stationary learning problem.' This hypothesis alone does not appear to be uniformly true. In fact, imagine a simple 5-state random walk markov chain environment without termination. Each state is visited infinitely many times, so the chain is ergodic and there are no non-stationary points. The empirical section uses meta-parameters that were ill-motivated with no discussion about meta-parameter selection. It is critical to point out that the stepsize used for an episodic problem will likely not be the optimal stepsize for the corresponding non-episodic problem, as the magnitude and variance of the considered returns are necessarily different (in response to Figure 3). Further, evaluating over 3 random seeds simply is not sufficient to make any statistically significant claims when comparing any of these curves (for instance in Figure 5).\n\nAdditional Comments (do not influence rating)\n\nFor a paper exclusively introducing a new control environment, it is critical to include a discussion about the exploration problem. At the very least, I would appreciate seeing sensitivity curves for values of epsilon.\n\nThis paper makes many strong claims about the nature of intelligence that are neither supported in the work or are accepted in the community. While it is intuitive that the environment plays a critical role in developing intelligence, the lack of universal definition of intelligence makes this a non-falsifiable claim. Although I appreciate the point the authors are trying to make, which is that RL research frequently is done in the realm of toy simulated domains, I do not think that this paper includes the appropriate supporting evidence to validate such lofty claims.\n\nIt would be interesting to change the exploration method from epsilon-greedy to sampling according to the softmax action distribution. This can have dramatically improved performance on non-adversarial exploration problems, and reduces the need for scheduled epsilon decay. It additionally reduces the need for two extra meta-parameters, allowing the empirical claims to be made more strongly without performing some sweep over parameters.\n\n--------------------\nEdit after reading discussion, rebuttal, and edits to manuscript.\n\nThe paper's intended contribution was different than I had realized during the initial review phase. I appreciate the effort the author put into the response and the changes made to the literature review section of the paper. I think these help to demonstrate the scope and placement of the work considerably.\n\nI still believe the paper over-states the novelty of the results and I still find it difficult to understand their utility. I believe that the entirety of the paper falls under the domain of exploration in RL, but this is not made clear through the introduction or related works section (though the updates to the related works section help considerably). \n\nStochasticity in the environment helping the agent to learn is not a surprising finding at all if the exploration method is insufficient. To give a small example, imagine an agent wandering in a tabular gridworld. If the agent has no method of exploration, and the environment has sparse reward, then it is not surprising that the agent would get stuck in a corner. If the environment was modified so that each transition had a 5% chance of randomly going another direction (e.g. the 'right' action has a 5% chance of becoming an 'up' action), then we've effectively encoded epsilon-greedy exploration through the environment dynamics. All of this comes down to say, it is difficult to separate the dynamics of the environment from the agent's exploration method. I think it would require a careful study that considers these aspects more explicitly.\n\nTo summarize, I think the paper could easily be accepted to a future conference, but I think it is important to:\n- Make connection between the insights and exploration clear, specifically designing the introduction, lit review, and experiments around this connection.\n- Make sure the contributions are extremely clear in the writing. Demonstrate those contributions directly in the empirical section.\n- Tone down the claims in the writing. Many of the claims about the state of the field in RL are demonstrably incorrect. I agree with the sentiment trying to be expressed, but the absolutism makes it difficult to separate that sentiment from a deep understanding of the current RL literature with only the most important papers cited, or a fundamentally insufficient lit review. To make claims about the state of a field the lit review should be rather extensive.", "belong_id": "S1xxx64YwH"}, {"uid": "S1gUDMlCtr", "paper_title": "Ecological Reinforcement Learning", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper discussed several properties of environments used in reinforcement learning research experiments. The main conclusion is the environment should be dynamic and non-episodic, and the environment shaping is introduced to be effective. In general, the idea is well-presented and easy to follow. However, I have some concerns about the proposed method and experiments:\n\n----\n1. In general, I think the arguments in the paper are still a bit vague to be demonstrated using only experimental results. One improvement could be using some mathematical formulation to describe the argument and conducting some analysis. For instance, the paper can formulate the environment shaping and reward shaping concretely and prove that environment shaping could replace reward shaping. \n\n\n2. For the experiment comparing the reward shaping and environment shaping: the environment shaping method is designed and more complicated than the reward shaping, I think it could be more convincing if the authors investigate and develop more approaches for reward shaping. Otherwise, it is a bit hard to argue the environment shaping could outperform reward shaping a lot. \n\n3. To argue that the non-episodic environment is better than episodic ones, I think the paper should consider more tasks besides the two mentioned in the experiment section. From figure 3, the non-episodic dynamic environment is not very clearly better than episodic one from all scenarios. \n", "belong_id": "S1xxx64YwH"}, {"uid": "r1eZLxow9B", "paper_title": "Ecological Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors study what they refer to as ecological reinforcement learning, defined as the interaction between properties of the environment and the reinforcement learning agent. They introduce environments with characteristics that reflect natural environments: non-episodic learning, uninformative reward signals, and natural dynamics that cause the environment to change. These factors are shown to significantly affect the learning progress of RL agents and, unexpectedly, the agents can sometimes learn more efficiently in these more challenging conditions.\n\nClarity:\n\nThe paper seems to be clearly written. The code will be made publicly available.\n\nNovelty:\n\nThe main contribution from the paper seems to be two novel benchmark problems with characteristics that reflect natural environments and an exhaustive evaluation of the performance of standard algorithms on them. While the experimental results show some light about the performance of existing methods in the proposed environment, the paper does not contain any methodological contributions. Because of this, it is hard to assess the novelty of the work.\n\nQuality and significance:\n\nWhile the paper makes some interesting points, I feel that the proposed environments are too few, and too simple and unrealistic when compared to real-world problems. Because of this, one cannot know how general and significant the conclusions obtained are.", "belong_id": "S1xxx64YwH"}, {"uid": "HJl3w-spFH", "paper_title": "Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the autoencoder model combining the usual information bottleneck and the Gaussian mixture model (GMM). Using an approximation to deal with GMMs, the authors derive a bound on the cost function generalizing the ELBO. The performance of the proposed method is tested on three benchmark datasets and compared with existing methods combining VAE with GMM.\n\nWhile the framework and the performance of the proposed method are interesting and promising, some of its main parts are unclearly explained.\n\n- Although Remark 1 well explains the difference between VaDE and the proposed objective functions, little is discussed how this difference affects the learnt model.\n- I wonder why Q_\\phi(x|u) in Eq. (11) doesnt have to be a distribution. What does the expression [\\hat{x}] mean?\n- Eq. (17) is not explained clearly. Since the equality symbol is used, it is unclear where is approximation. Although this approximation is one of the main parts of the proposed method, little is discussed on the influence of this approximation.\n- Are the information plane and latent representations in Figs 5 and 6 also available for DEC and VaDE and not limited to the proposed method?\n\nMinor comments:\np.5, the math expression between Eqs. (16) and (17): The distribution q(x_i|u_i,m) is undefined.\np.6, l.4 from the bottom: ACC is defined later.\np.7, l.14: Should J be n_u?\n", "belong_id": "HyxQ3gSKvr"}, {"uid": "Byx_WBDRtr", "paper_title": "Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The author(s) posit a Mixture of Gaussian's prior for a compressed latent space representation of high-dimensional data (e.g. images and documents). They propose fitting this model using the Variational Information Bottleneck paradigm and explicate its derivation and tie it to the variational objective used by similar models. They empirically showcase their model and optimization methodology on the MNIST, STL-10, and Reuters10k benchmarks.\n\nThe idea of using a latent mixture of Gaussian's to variationally encode high-dimensional data is not new. The author(s) appropriately cite VaDE (Jiang, 2017) and DEC (Xie, 2016), 'Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders' (Dilokthanakul, 2017). However, the author(s) unfortunately did not mention 'Approximate Inference for Deep Latent Gaussian Mixtures' (Nalisnick, 2016). Nalisnick et al. consider the exact generative process as the proposed by the author(s), but with the addition of a Dirichlet prior on the Categorical distribution. Nalisnick et al. fit their model with a VAE and circumvent Dirichlet intractability by positing a Kumarswamy stick-breaking variational posterior (the resulting distribution is on the simplex). They achieve 91.58% accuracy, but do so after fitting a KNN to the latent space. New techniques such as 'Implicit Reparameterization Gradients' (Figurnov, 2018) and 'Pathwise Derivatives Beyond the Reparameterization Trick' (Jankowiak, 2018) allow direct use of a Dirichlet posterior in VAEs. These methods are respectively implemented in the TensorFlow and PyTorch APIs. My point here is that there are many ways to fit this pre-existing model. From my review of these other works we have, for 'best run' on MNIST, that author(s) > VaDE > Nalisnick > Dilokthanakul > DEC. Thus, the author(s) are SoTA for this particular generative process for their best run. It would be nice to see their standard deviation to gauge how statistically significant their results are. However, I am dubious of the SoTA claim as I detail later.\n\nThe author(s) derivation was sound, but a bit confusing for me. In particular, I found keeping track of P's and Q's very burdensome after reading sections 2.1 and 2.2. In my experience, Q is typically reserved for a variational distribution that approximates some intractable P distribution, while P is used to describe the generative process (i.e. likelihood) and other exact distributions. Furthermore, one typically introduces the generative process first using P distributions. Once, I got past this confusion everything else made sense. I might suggest introducing the generative process first and with P distributions instead of Q's. The variational model can follow with the Q distributions as the author(s) have it. Equations 4, 5, and 6 all exactly match the unsupervised information bottleneck objective (Alemi, 2017)--see appendix B. I am therefore confident in those equations. I carefully checked their derivation of the VADe comparison (equation 10 and appendix B). Their derivation is straightforward. The principle trick is using the MC assumption C->X->U for P distributions to claim p(u|x,c) = p(u|x). Equations 12-16 all follow naturally from equation 11. If equation 17 is indeed an approximation or bound approximation, I would suggest not using the equals sign. Instead, consider another appropriate operator or rename Dkl to indicate it is the approximated version (just as in equation 24).\n\nIf the author(s) like my suggestion regarding generative vs variational nomenclature, I would also change the second sentence of page 6 to something like 'We use our variational Qc|u distribution to compute assignments.' Thereafter, I would drop the star indicator for optimal parameters. These parameters are not necessarily optimal given the non-convexity of the DNN. Replace with, 'after convergence, we ...' Similarly, drop optimal from line 2 of algorithm 1.\n\nCircling back to the experiments, the author(s) use reported values from DEC and VaDE. Those works compute cluster assignments using a KNN classifier on the latent space. This paper however uses the arg max of equation 19. I much prefer this article's method, but for comparison purposes, the author(s) should similarly use a KNN classifier on their latent space to compute accuracy in the same manner. The use of KNN in these other works allows them to consider a number of latent clusters larger than the number of true classes (e.g. 20 clusters for MNIST). I like that the author(s) stick to 10 clusters for MNIST, but for comparison I would have liked to see a KNN generated accuracy alongside their equation 19 based accuracy. It looks like the author(s) implemented VADe for STL-10 based on table 1. If so, it seems they could easily implement equation 19 for their STL-10 value for VADe. If not, please correct this. Not to belabor further, but I would really like to see a table 2 that reports both equation 19 and KNN accuracies when available. Namely, the author(s) should report both values for their model and can leave equation 19 accuracies blank for reported values.\n\nLastly, I always raise an eyebrow when I see tSNE latent space representations. STL-10 was used to generate figure 6, where one needs dim(u) >> 2. However, MNIST can be well reconstructed using just 2 latent dimensions. In this case, tSNE is unnecessary. The author(s) state 'Colors are used to distinguish between clusters.' This statement is unclear as to whether the author(s) are using the class label or learned latent cluster. If it is the former, figure 6 makes sense in that it shows misclassifications (i.e. red dots in the green cluster). However, if it is the latter then I am concerned tSNE is doing something weird.\n\nTo summarize, I enjoyed the paper. My only concern is novelty. Being the first to pair an existing model with an existing method, in my eyes, does not necessarily meet the ICLR bar. The author(s) seemingly achieve SoTA, but without KNN-based accuracies for their model it is hard to compare to cited works. Having these KNN results and error bars would strengthen their case substantially to: achieving SoTA for an existing model by being the first to pair it with an existing method.\n", "belong_id": "HyxQ3gSKvr"}, {"uid": "rkgw2B4Jqr", "paper_title": "Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper purposes to cluster data in an unsupervised manner that estimates the distribution with GMM in latent space instead of original data space. Also, to better describe the distribution of latent code, the authors involve VIB to constraint the latent code.\n\nThe whole pipeline is clear and makes sense. And the experiment proves it's effective. \n\nHowever, in my eyes, it's almost like an extension of VaDE which combines VIB and GMM too. The main difference is the authors use some hyperparameters to control the optimization of the whole model and make some more proper hypothesis. All of those make sense but may not contribute much to the research field.", "belong_id": "HyxQ3gSKvr"}, {"uid": "HyeleOROFS", "paper_title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary : \u000b\n\nThe paper proposes an exploratory objective that can maximize state coverage in RL. They show that a formal objective for maximizing state coverage is equivalent to maximizing the entropy of a goal distribution. The core idea is to propose a method to maximize entropy of a goal distribution, or a state distribution since goals are full states. They show that the proposed method to maximize the state or goal distribution can lead to diverse exploration behaviour sufficient for solving complex image based manipulation tasks. \n\n\nComments and Questions : \n\n\t- The core idea is to maximize the entropy of the state visitation frequency H(s). It is not clear from the paper whether the authors talk about the normalized discounted weighting of states (a distribution) or the stationary distribution? The entropy of the state visitation distribution only deals with valid states - but I am not sure what it means to maximize the entropy of this term exactly in terms of exploration, since it is neither the discounted weighting of states or the stationary distribution for an infinite horizon task? \n\t- The authors do mention that maximizing the entropy of H(s) is not sufficient - so instead suggests for maxmizing entropy of H(s|g). But why is this even sufficient for exploration - if I do not consider new tasks at test time but only the training task? How is this a sufficient exploration objective? Furthermore, since it is the conditional entropy given goal states, the fundamental idea of this is not clear from the paper. \n\t- Overall, I am not convinced that an objective based on H(s|g) is equivalent to an maximizing H(s), and why is this even a good objective for exploration? The meaning of H(s) to me is a bit vague from the text (due to reasons above) and therefore H(s|g) does not convince to be a good exploration objective either?\n\t- The paper then talks about the MI(S;G) to be maximized for exploration - what does this MI formally mean? I understand the breakdown from equation 1, but why is this a sufficient exploration objective? There are multiple ideas introduced at the same time - the MI(s;g) and talking about test time and training time exploration - but the idea itself is not convincing for a sufficient exploration objective. In light of this, I am not sure whether the core idea of the paper is convincing enough to me. \n\t- I think the paper needs more theoretical insights and details to show why this form of objective based on the MI(s;g) is good enough for exploration. Theoretically, there are a lot of details missing from the paper, and the paper simply proposes the idea of MI(s;g) and talks about formal or computationally tractable ways of computing this term. While the proposed solutuon to compute MI(s;g) seems reasonable, I don't think there is enough contribution or details as to why is maximizing H(s) good for exploration in the first place.\n\t- Experimentally, few tasks are proposed comparing skew-fit with other baselines like HER and AutoGoal GAN - but the differences in all the results seem negligible (example : Figure 5). \n\t- I am not sure why the discussion of goal conditioned policies is introduced rightaway. To me, a more convincing approach would have been to first discuss why H(s) and the entropy of this is good for exploration (discounted weighting or stationary state distribution and considering episodic and  infinite horizon tasks). If H(s) is indeed a difficult or not sufficient term to maximize the entropy for, then it might make sense to introduce goal conditioned policies? Following then, it might be convincing to discuss why goal conditioned policies are indeed required, and then tractable ways of computing MI(s;g). \n\t- Experimentally, I think the paper needs significantly more work - especially considering hard exploration tasks (it might be simple setups too like mazes to begin with), and then to propose a set of new experimental results, without jumping directly to image based tasks as discussed here and then comparing to all the goal conditioned policy baselines. \n\nOverall, I would recommend to reject this paper, as I am not convinced by the proposed solution, and there are lot of theoretical details missing from the paper. It skips a lot of theoretical insights required to propose a new exploration based objective, and the paper proposes a very specific solution for a set a very specific set of experimental setups. \n\n\n\n", "belong_id": "r1gIdySFPH"}, {"uid": "SJxzk8sitS", "paper_title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces SKEW-FIT, an exploration approach that maximizes the entropy of a distribution of goals such that the agent maximizes state coverage. \n\nThe paper is well-written and provides an interesting combination of reinforcement learning with imagined goals (RIG) and entropy maximization. The approach is well motivated and simulations are performed on several simulated and real robotics tasks.\n\nSome elements were unclear to me:\n- 'We also assume that the entropy of the resulting state distribution H(p(S | p)) is no less than the entropy of the goal distribution H(p(S)). Without this assumption, a policy could ignore the goal and stay in a single state, no matter how diverse and realistic the goals are.' How do you ensure this in practice?\n- In the second paragraph of 2.2, it is written 'Note that this assumption does not require that the entropy of p(S | p) is strictly larger than the entropy of the goal distribution, p.' Could you please clarify?\n\n\nThe experiments are interesting, yet some interpretations might be too strong (see below):\n- In the first experiment, 'Does Skew-Fit Maximize Entropy?', it is empirically illustrated that the method does result in a high-entropy state exploration. However, it is only compared to one very naive way of exploring and it is not discussed whether other techniques also achieve the same entropy maximization. The last sentences seems to imply that only this technique ends up optimizing the entropy of the state coverage, while I believe that the claim (given the experiment) should only be about the fact it does so faster.\n- On the comments of Figure 6, the paper mentions that 'The other methods only rely on the randomness of the initial policy to occasionally pick up the object, resulting in a near-constant rate of object lifts.' I'm unsure about the interpretation of this sentence given Figure 6 because other methods do not seem to fail entirely when given enough time.\n- In the experiment 'Real-World Vision-Based Robotic Manipulation', It is written that 'a near-perfect success rate [is reached] after five and a half hours of interaction time', while on the plot it is written 60% cumulative success after 5.5 hours and it is thus not clear where this '5.5 hours' comes from.", "belong_id": "r1gIdySFPH"}, {"uid": "BylJ3YZntB", "paper_title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduced a very interesting idea to facilitate exploration in goal-conditioned reinforcement learning. The key idea is to learn a generative model of goal distribution to match the weighted empirical distribution, where the rare states receive larger weights. This encourages the model to generate more diverse and novel goals for goal-conditioned RL policies to reach.\n\nPros:\nThe Skew-Fit exploration technique is independent of the goal-conditioned reinforcement learning algorithm and can be plugged in with any goal-conditioned methods. The experiments offer a comparison to several prior exploration techniques and demonstrate a clear advantage of the proposed Skew-Fit method. It is evaluated in a variety of continuous control tasks in simulation and a door opening task on a real robot. A formal analysis of the algorithm is provided under certain assumptions.\n\nCons:\nThe weakest part of this work is the task setup. The method has only been evaluated on simplistic short-horizon control tasks. Itd be interesting to see how this method is applied to longer-horizon multi-stage control tasks, where exploration is a more severe challenge. It is especially when the agent has no access to task reward and only explores the environment to maximize state coverage. It is unclear to me how many constraints are enforced in the task design in order for the robot to actually complete the full tasks through such exploration.\n\nI would also like to see how Skew-Fit works with different goal-conditioned RL algorithms, and how the performances of the RL policy in reaching the goals would affect the effectiveness of this method in exploring a larger set of states.\n\nSection E: it seems that theres a logic jump before the conclusion goal-conditioned RL methods effectively minimize H(G|S). More elaboration on this point is necessary.\n\nMinor:\nAppendix has several broken references.", "belong_id": "r1gIdySFPH"}, {"uid": "SkgKHY_aKS", "paper_title": "Multi-source Multi-view Transfer Learning in Neural Topic Modeling with Pretrained Topic and Word Embeddings", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "On the basis of existing topic modelling approaches, the authors apply a transfer learning approach to incorporate additional knowledge to topic models, using both word embeddings and topic models. The underlying idea is that topic models contain a global view that differs on a thematic level, while word embeddings contain a local, immediate contextual view. The combination of both local and global view transfer to enhance a topic model is the main contribution of this paper, especially when using multiple sources (therefore the title: multi-source multi-view transfer).\nGiven a document collection, DocNADE is used to generate the topic-word matrix. In the local view transfer step, the pre-trained WordPool is used, from which knowledge is transferred on the target document. The global view transfer is done by transferring knowledge from the pre-trained TopicPool to the target. As described in Algorithm 1 in the paper, both Word- and TopicPool are jointly used in the transfer learning process. \nFor evaluation, three different measures are taken into account: Perplexity, Topic Coherence and Precision (Information Retrieval). In comparison to a DocNADE only approach, all values are better in the settings that use the transfer learning approach. Compared to DocNADE + word embeddings, the results are competitive as well. In both experiments, the multi-source setting evaluates best overall.\n\nIn conclusion, the paper shows that exploiting multiple sources and views in transfer learning leads to an overall improvement in the given tasks. The main contribution is the usage topic models in a transfer learning framework. Additionally the use of multi-source word embeddings is novel too, especially in the joint setting with the topic model transfer. The paper shows how the DocNADE approach is enhanced to make use of both local and global view transfer and how this enhancement leads to improved performance on various related tasks. \nStill, the overall contribution is mostly in combining existing methods and can be judged as rather incremental.\n\nMinor note: A small mistake has been found in Table 5. The best perplexity value in the first column is not the bold 638, but the 630 in the local-view transfer setting.\n\nEdit after rebuttal: In my review I did not value the contribution of the transfer learning approach enough. So, when also considering the extensive evaluation I am now leaning towards accept.", "belong_id": "ByxODxHYwB"}, {"uid": "HyeGN5C85B", "paper_title": "Multi-source Multi-view Transfer Learning in Neural Topic Modeling with Pretrained Topic and Word Embeddings", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes a multi-source and multi-view transfer learning for neural topic modelling with the pre-trained topic and word embedding. The method is based on NEURAL AUTOREGRESSIVE TOPIC MODELs --- DocNADE (Larochelle&Lauly,2012). DocNADE learns topics using language modelling framework. DocNADEe (Gupta et al., 2019) extended DocNADE by incorporating word embeddings, the approach the authors described as a single source extension of the existing method.\n\nIn this paper, the proposed method adds a regularizer term to the DocNADE loss function to minimize the overall loss whereas keeping the existing single-source extension. The authors claimed that incorporating the regularizer will facilitate learning the (latent) topic features in the trainable parameters simultaneously and inherit relevant topical features from each of the source domains and generate meaningful representations for the target domain. The analysis and evaluation were presented to show the effectiveness of the proposed method. However, the results are not significantly improved than the based line model DocNADE. \n\nOverall, the paper is written well. However, it is not clear to me that the improved results are resulted due to multi-source multi-view transfer learning or for the better leaning of the single-source model due to the incorporation of the regularizer. \n\n\n", "belong_id": "ByxODxHYwB"}, {"uid": "SygTrGoa9r", "paper_title": "Multi-source Multi-view Transfer Learning in Neural Topic Modeling with Pretrained Topic and Word Embeddings", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This is an emergency review.\n\nThis work proposes a novel method to use pre-trained topic embeddings and pre-trained word embeddings obtained from various corpora in the transfer learning framework. \n\nTheir model architecture is based on DocNADE, unsupervised neural-network based topic model, and the authors propose two strategies to use pre-trained topic embeddings and pre-trained word vectors.\n1) Addition of a weighted sum of pre-trained word embeddings and the hidden vector of DocNADE.\n2) L2-Regularization term between topic embedding of DocNADE and pre-trained topic embeddings. They propose to align these two embeddings by multiplying align matrix 'A' to the topic embedding of DocNADE.\n\nThey show the transfer learning performance of their model on various source/target domain datasets, including medical target corpora, and verify that their model outperforms on a short text and small document collection.\n\nStrengths.\n1. Comparison with the data augmentation baseline shows the performance gain is not only from bigger training data. Even though comparison with the naive baseline (data augmentation) seems too obvious, I think the results clearly show their claim about the importance of using transfer learning in neural topic modeling domain.\n2. As the first approach that introduces a novel transfer learning framework with pre-trained topic embeddings, they show tons of experimental results with various datasets and metrics to show the specification of their method. Their experimental setting is well designed.\n\nWeaknesses and comments:\nTheir method to combine pre-trained word embeddings and pre-trained topic embeddings is too simple. Since this is the first approach to use topic embedding in the transfer learning field, the simplicity of the proposed method is somewhat necessary. However, a weighted sum of pre-trained topic/word vectors seems not enough to transfer multisource knowledge. For instance, word vectors obtained from individual training processes do not share embedding vector space. As you apply the alignment method to topic embeddings from various sources, you should align word embeddings too.", "belong_id": "ByxODxHYwB"}, {"uid": "rJxR8XGzFH", "paper_title": "Laconic Image Classification: Human vs. Machine Performance", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nIn this empirical study, the authors attempt to identify a minimal entropy version of an image such that the image may be correctly classified by a human or computer. The authors then compare the efficacy of a human and computer to maintain accuracy in the presence of a reduced entropy representation of an image. The authors find that machines are more sensitive to reductions in entropy due to image resolution than humans (as opposed to color or cropping). In addition, the authors find that humans are generally better at identifying minimal entropy images than machines.\n\n1. Corruption results not surprising. \n\nAlthough the authors offer some intriguing methods, I found the results to not be compelling nor improve our understanding of the relative differences between human and machine perception. While identifying that humans are less sensitive to a reduction in resolution, this result is not terribly surprising given that networks are known to suffer from aliasing artifacts, e.g.\n\n  Geodesics Of Learned Representations\n  Olivier J. Henaff & Eero P. Simoncelli\n  https://www.cns.nyu.edu/pub/lcv/henaff16b-reprint.pdf\n\n2. Unclear what we learn from the method.\n\nI am not too clear about what specific insights the methods provide in this paper. Estimating the entropy associated with each image corruption -- while interesting -- does not lead to any substantive analysis nor conclusions as far as I can tell. Given the lack of benefit to analyzing the entropy, I am left to really just consider these methods to be image corruptions that downsample the resolution or desaturate the images. These corruptions are not terribly novel with respect to previous work, e.g.\n\n  Generalisation in humans and deep neural networks\n  https://arxiv.org/pdf/1808.08750.pdf\n\n  Benchmarking Neural Network Robustness to Common Corruptions and Perturbations\n  https://arxiv.org/abs/1903.12261\n\nTo summarize, my feedback is the following:\n\n1. Please justify the use of entropy to quantify the distortion. What does entropy provide above and beyond just parameterizing the distortion (e.g. image resolution, color saturation)?\n\n2. Are there other results above-and-beyond sensitivity to image resolution that distinguishes human and machine in these experiments? These results seem to be largely known by just considering corruptions such as low pass filters, etc. presented in the above papers.\n\n\nMinor Comments:\n\n- The authors should provide a figure with example images in the main text showcasing how each method corrupts an image.", "belong_id": "rJgPFgHFwr"}, {"uid": "r1xx7qp5YB", "paper_title": "Laconic Image Classification: Human vs. Machine Performance", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a method to understand and compare the performance of DNNs classifier, which is different from the precise prediction in the notion of correct/wrong. With approximate minimal-entropy of input images, the classifiers can recognize this image so that different classifiers including human and DNNs will need different reduction methods (cropping, downsampling, color reduction )for the same image to give correct prediction and also will give us different performance in a same test dataset. By comparing the results with humans and DNNs, the author claims that it will have more challenges for DNNs in this laconic image classification task than human will have.  \nFor the motivation in this paper, the author tries to propose a new perspective to evaluate the robustness of image classifiers. Especially compared with humans understanding, this paper would like to rethink the influence of reduction for this task. However, it is not clear that why three reduction methods the author used can help with understanding the difference between humans and DNNs because when training a DNN for image classification, we usually use these methods to augment our training set, but for humans, it is a totally different story that how to recognize an image.\nFor the theoretical demonstration, in this paper, the author uses approximating minimal-entropy to quantify the minimal content of an image DNNs or humans need to give correct category. The intuition of this method is suitable. But in section 3, the author didnt give a clear demonstration of how to compute the entropy reduction in 3.1. I think if it is better to introduce how to measure 3.2 in detail, then 3.1 may be more clear. And it also makes me confused about the atomic reduction step in the last paragraph in 3.1. For the 3.5, I think the authors should focus on how to demonstrate MEPIs for humans more mathematically so that it will be more reliable.\nFor the experiments, the author tries to answer two questions: 1. How does the entropy required by DNN and human classifiers compare?2. How do the classifiers perform in terms of precision for each others MEPIs? However, the experiments do not provide convincing evidence to existing approaches. First of all, for a single DNN, how different entropy reduction methods influence the classification? Secondly, how different reduction scales in the same model influence the results? At last, the comparison between different models should give a more visualized figure to illuminate the difference. It will be better to provide more ablation study experiments for this paper.\n", "belong_id": "rJgPFgHFwr"}, {"uid": "rJxzyUwf9H", "paper_title": "Laconic Image Classification: Human vs. Machine Performance", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes and studies a task where the goal is to classify an image that has been intentionally degraded to reduce information content - hence the name 'laconic' image classification. The motivation for this task is to compare human and machine performance in a task that deviates from the standard ImageNet setup. To make different content reductions comparable, the authors measure the (approximate) entropy of an image via its PNG-compressed file size. As image transformations, the authors utilize quantization, downsampling, cropping, and a combination of the above. The authors find that convnets with higher accuracy are also more robust to these perturbations, and that humans perform well on the minimum-entropy examples of the networks (but not vice-versa).\n\nOverall I find the comparison of human and machine performance interesting and hence recommend accepting the paper. However, there are multiple directions that could possibly strengthen the core experiment. Hence I only give a weak accept at this point. Concretely, these directions are:\n\n- Do the results in the paper change if a different entropy measure is used (e.g., JPEG compression)?\n\n- As suggested by the authors, training networks to be robust to the 'laconic' image perturbations could be an interesting direction. For instance, standard data augmentation with the proposed perturbations would be a relevant baseline.\n\n- To aid replicability and to compare the performance of different human test subjects, it would be interesting to conduct the experiment also on a crowdsourcing platform such as Mechanical Turk (as a complement to, not a replacement for, the university population in the paper).\n\n- Also to add replicability and to make it easier to compare different human accuracy evaluations, it would be good to measure how well the annotators perform on the unperturbed images and on a simple noise transformation (e.g., Gaussian noise).\n\n- It would be good to know how approximate the entropy measures in the paper are, e.g., to understand why humans perform worse in the 'combined' perturbation setting.\n\n- How did the results from the control group and the open / online evaluation differ?\n\n\nIn addition, I have the following suggestions for improving the paper:\n\n- For the related work section, the authors may find the following papers on robustness of convnets to distortions interesting:\n\n* Manitest: Are classifiers really invariant?\nhttps://arxiv.org/abs/1507.06535\n\n* Exploring the Landscape of Spatial Robustness\nhttps://arxiv.org/abs/1712.02779\n\n* Spatially Transformed Adversarial Examples\nhttps://arxiv.org/abs/1801.02612\n\n* Semantic Adversarial Examples\nhttps://arxiv.org/abs/1804.00499\n\n- It could be helpful for the reader to see some example images of the different transformations in the main text.\n\n- Section 6: '[...] a bias in texture for images trained on the ILSVRC dataset [...]' - should this be 'classifiers' instead of 'images'?\n\n- Section 6: '[...] would be interesting to explore in future' - insert 'the' before 'future'?", "belong_id": "rJgPFgHFwr"}, {"uid": "HJx2eN_6Kr", "paper_title": "Double Neural Counterfactual Regret Minimization", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors proposed a double neural counterfactual regret minimization algorithm (DNCFR) that uses a RegretSumNetwork to approximate cumulative regret and an AvgStrategyNetwork to approximate the average strategy. To help the training, the authors use robust sampling and a mini-batch training methods. The main contributions of the paper are: First, the DNCFR algorithm and the recurrent neural network architecture; Second, an efficient sampling and training method; Third, plenty of experiments that corroborate the effectiveness of DNCFR.\n\nIt is interesting and meaningful to develop neural-based CFR, in order to eliminate the manual abstraction and apply CFR to large-scale imperfect information games. The authors tested their algorithm on a medium scale game HUNL(1) (with 2 * 10 ^ 8 information sets) and trained a blueprint strategy on large scale game HUNL(2), which is combined with value networks from DeepStack and beats ABS-CFR a lot. It is great to see that DNCFR works on large scale games. However, both HUNL(1) and HUNL(2) are one-round games and it not clear how to combine the blueprint strategy trained by DNCFR with DeepStack. Whats more, as DNCFR is only effective on first round as the blueprint strategy trainer when played against ABS-CFR, it is more likely that DeepStack beats ABS-CFR, instead of DNCFR beats it. So the result in Figure 7(c) is not so convincing.\n\nUnlike tabular CFR that save regrets and strategies for all the information sets or other neural-based algorithms that need large reservoir buffers. It only needs to save data sampled from the most recent iterations, which saves much memory. In fact, this is a bootstrap method borrowed from Reinforcement learning. Though the method save memory and has lower variance than methods that use reservoir buffers, it is bias as it trains the new RSN and ASN based on the output of the old networks. It seems good when the game size is small and the CFR iterations is small. It may needs very large CFR batches and very many gradient descent updates when training on large scale games, in order to control the bias. The results in Figure 7(a) and 7(b) are limited in CFR iterations. Experiments using different gradient descent updates and different CFR batch while given more CFR iterations should be tested, in order to show the effect of the bias training.\nIn Algorithm 4. The calculation of average strategy seems wrong. Because you are using MCCFR, According to Monte Carlo sampling and regret minimization for equilibrium computation and decision-making in large extensive form games, you may need a method call stochastically-weighted averaging. It should be noted that the sampling probability of\neach information set is not equal. You may need to discuss this.\n\nThe authors train the network for 2000 updates when the batch size is 256 for Leduc and 100000 for HUNL(1) and HUNL(2) in every CFR iteration (I am not sure how much gradient updates are used in HUNL(2), it is not given). There's quite a lot of updates in every CFR iteration. But it is acceptable when compared to Deep CFR proposed by Brown, which uses 4000 updates and the batch size is 10000.\n\nExperiments:\n1. In the ablation studies, the algorithms are tested on small scale game Leduc(5). It is quite a small game that event the size neural parameters is larger than the size of information sets. It is OK but larger games make more sense. Especially in the\nexperiment of Individual network, as this experiment is important to show that\nDNCFR is comparable to tabular CFR and the bias is acceptable.\n2. The paper didnt show what the learned regret and average strategy looks. If they are\nshowed, it would be helpful to understand the bias in the bootstrap learning.\n3. In the part Is robust sampling helpful, the authors want to show that the robust sampling with k=1 is better than outcome sampling. But I didnt find how they set the exploration parameter in outcome sampling and I am afraid that it doesnt make sense. Because outcome sampling has a parameter to adjust the exploration. According to 'Monte Carlo sampling and regret minimization for equilibrium computation and decision-making in large extensive form games', the best exploration parameter is different in different game, but it is almost sure that totally exploration is not the best setting (it is equivalent to the robust sampling with k = 1).\n4. In the part Do the neural networks generalize to unseen infosets. The authors claims that it is true. But the experiment only shows that the neural network dont forget\ninformation sets that trained before.\n5. In the part How well does DNCFR on larger games, the DNCFR is limited to 100\niterations while is allow to run for 1000 iterations in other experiments. 100 iterations\nare too few to show the effectiveness of DNCFR on these games.\n6. The algorithm is tested on HUNL(1) and HUNL(2), which are one round and action- abstracted version of HUNL. But the authors should give more detail description of\nthese games.\n7. It is not clear how to combine the blueprint strategy trained by DNCFR with\nDeepStack, as DeepStack uses continual resolving and dont need any blueprint strategy. And it would be interesting if the head-to-head performance of DNCFR agent on large scale games (for example, the FHP with two rounds and more than 1e^9 information sets) is reported, instead of the performance of the agent that combined with DeepStack.\n8. In section 5.4, When variance reduction techniques are applied, Figure 7(c).... The authors didnt explain why the variance reduction techniques are needed here, but in order to compare the algorithm directly, some other advanced techniques should not be used here.", "belong_id": "ByedzkrKvH"}, {"uid": "ryg_h5Lk9r", "paper_title": "Double Neural Counterfactual Regret Minimization", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces two neural networks, one for average regrets and one for average strategy, to (approximately) run CFR algorithm in (large) IIG. New sampling techniques are used to train these networks to represent the resulting strategy.\n\nI remember seeing this paper in the last year's OpenReview for ICLR - I was not reviewing the paper.\nI enjoyed the first version of the paper, but it was missing experiments on larger games and some questions on smaller games were unanswered.\nIn this version, authors clearly spent a large amount of time (including re-implementing DeepStack!) so that they could compare on large games (namely HU no-limit Poker) and overall greatly improved the paper and evaluation.\n\nThe evaluation on small games includes comparison to NFSP/XFP, as well as investigating time/space trade-off.\nFor the large game, I like that the authors evaluated against an ACPC agent.\nPrevious work is well cited, and authors have a good overall map of related work (both older results and new papers).\n\nIssues:\n\n1) One downside of the paper is that it is very close to the 'Deep Counterfactual Regret Minimization'.\nWhile authors devote a full paragraph in section 6 to contrast these, the difference is relatively small.\nI do not think it is fair to dwell too much on this though, since the first version of the paper with this idea originally came *before* DeepCFR publication!\n\n2) Since the approach is so similar to DeepCFR, it would be nice to include it in comparison (not just NFSP/XFP).\n\n\nMinor details:\n\n- Page 9: '...which has no limited number of actions, ...' rephrase please, this sounds like the game is infinite.\n\n- Page 9: ', more abstracted action leads to better strategy...' more abstracted sounds like it is smaller, rephrase please to something like 'finer grained abstraction'.\n\n- Minor frequent grammatical issues, but does not derail from the flow and semantics of the paper.\n\nConclusion:\n\nOverall, the paper introduces method that is interesting to the community, scales to large games and the paper includes comprehensive evaluation section.\nI believe it should be accepted.\n", "belong_id": "ByedzkrKvH"}, {"uid": "H1lxTev3Yr", "paper_title": "Neural Text Generation With Unlikelihood Training", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis paper proposes training losses, unlikelihood objective, for mitigating the repetition problem of the text generated by recent neural language models. The problem is well-motivated by evidence from the existing literature. Specifically, the paper argues that the main cause of the degenerated output is the maximum likelihood objective commonly used to train language models. Their main contribution is to introduce additional objectives to penalize unlikely word probabilities. The proposed penalty is derived into 2 objectives: token level (previous words in context) and sentence level (future decoded words). The prior objective is used along with the MLE, while the later and more expensive is used for fine-tuning. They perform experiments on Wikitext-103 and evaluate models on the perplexity of the models, and n-gram statistics such as repetition, and uniqueness of the decoded texts. The proposed training scheme (UL-token+seq) is shown to have the closest statistics to the original corpus while the perplexity slightly suffers. The additional manual analysis shows that human annotators prefer the outputs (sentence completion) of the proposed method over the other baselines.\n\nOverall, this paper tackles a relevant problem and could propose a novel method.\n\nFor the unlikelihood objectives, there are a few clarifications on the design decision. There are some correct repetitions in the ground-truth text as well. However, the proposed loss minimizes all repetitions regardless. In addition, it is unclear how the proposed method mitigates the token distribution mismatch. Finally, there is a similar work where they attempt to match repetition (or n-gram distribution) of a reference corpus (https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16961). A discussion of how this paper distinguishes from previous work would be helpful.\n\nFor the experiments, there are some missing detail and concerns:\n\n1. The fine-tuning using UL-seq (eq 7) procedure is not well explained. For example, how many sequences are used per update? How many times that you decode in the 1,500 updates?\n\n2. the stochastic decoding results are related as the paper motivation is built on top of Holtzman et al., 2019, the results also support the claim. However, how do you explain the discrepancy between the experts and the crowd workers?\n\n3. GPT-2 results show that UL-seq does not significantly improve several evaluation metrics from MLE. This is a conflict with the result in Table 2. This could be a sign of the generalization problem of the proposed method. Additional results on different model architectures would be helpful.\n\nMinor question:\n1. It is uncertain how these repetition and uniqueness statistics translate to a downstream task (e.g. summarization or NMT). Do you have results regarding this?\n\n2. It appears that UL-token does not do much. What is the increase in training time? Do you recommend using the token loss? \n", "belong_id": "SJeYe0NtvH"}, {"uid": "HJeHygz6YH", "paper_title": "Neural Text Generation With Unlikelihood Training", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nContributions:\n\nThe main contribution of this paper lies in the proposed unlikelihood training objective for open-ended text generation. The key idea is to enforce the unlikely generations to be assigned lower probability by the model. Both token and sequence-level unlikelihood training objectives are provided. Impressively, the authors show that models trained with the proposed method can generate high-quality text via only beam search, without using top-k, nucleus sampling, or beam blocking methods. \n\nStrengths:\n\n(1) Writing & Clarity: The proposed model is very well motivated, the paper is well written, and clearly presented. I enjoyed reading the paper. \n\n(2) Novelty: Though the proposed model is simple, I think it has novelty inside. The proposed model makes connection to negative sampling, and is very intuitive to reduce repetition during the training stage, instead of decoding stage. I find the gradient analysis in Section 5.1 is especially interesting. \n\n(3) Experiments: The authors did a careful job in experiments design, and conducting the experiments. Human evaluation is also provided. A lot of additional results are provided in Appendix. I feel the experiments are solid and convincing.  \n\nWeaknesses:\n\n(1) Clarity: I have three questions regarding this paper. \n\na) Using previous generated tokens as the unlikely tokens for the current generation step in the token-level unlikelihood training is intuitive, but also seems too simple. Can the authors provide some comments on this? Or, are there any better designs? \n\nb) How is the training looking like? Do we need Gumbel-softmax-like trick to backpropagate through the generated tokens in the sequence-level training? Or, this is not needed? Can the authors clarity the training process? \n\nc) The proposed model can be directly applied to dialog response generation task, which also requires diversity in generated responses. Any reason why this conditional generation task is not performed? Or, do the authors plan to also apply the proposed method to this application?\n \n\n\n\n\n", "belong_id": "SJeYe0NtvH"}, {"uid": "ryeEqCDG5B", "paper_title": "Neural Text Generation With Unlikelihood Training", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper targets on solving the dull and repetitive outputs in MLE training for neural text generation. The authors propose a new unlikelyhood training to avoid assigning much probability to frequent and repetitive words. The authors combine the proposed algorithms and beam search and state that the results improved over beam blocking and neculus decoding.  \n\nThe unlikelyhood training is to provide a set of negative candidates and minimize the probability of these tokens. This raises several practical issues: how to choose a reasonable $\\alpha$. \nThis set can be chosen as the previous tokens in the sequence. This is a reasonable choice, but the author does not state why the other choices are not working,e.g. Sharpening the distribution using temperature. A potential counter case is that there are similar words exists in the sequences, but the unlikely loss trends to distinguish these synonyms.  The other unlikelyhood training choice is called sequence-level set. However, it seems not sequence-level but just n-gram center.  A question would be why not chose the whole n-gram instead of just choosing the center of n-gram. Also, why a prefix is really needed is questionable. \n\n\nEq 8 seems wrong, why$i \\leq n \\leq j$\n\nTable 2 should have shown the original sequences on the repetition metrics to show it indeed make sense.ppl should be enough, acc seems redundant. It seems that unlikely training may be harmful to ppl, which is the common metric to evaluate generation quality. A better discussion should be made on this to explain why it performance or if ppl has some problem.\n\nTable 3 comparison may not be reasonable. As Nucleus sampling and beam blocking is not in training phase. This comparison is not really fair.\n", "belong_id": "SJeYe0NtvH"}, {"uid": "rJeQ281nFr", "paper_title": "From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a multi-modal learning framework that links the inference stage and generation stage for seeking the possibility of generating the human face from voice solely based on the audio-visual data without any human-labeled annotations. Experimental results show that the proposed network can not only match the relationship between the human face and speech, but can also generate the high-quality human face sample conditioned on its speech.\n\nThe writing and presentation are clear.\n\nMy concerns are as below.\n1) What are the training computational complexity and testing time cost of the proposed method?\n2) How can we determine truncation threshold more elegant? Any theoretical analysis and sensitive analysis?\n3) How did the authors handle model collapse during training? \n4) The format of references should be consistent.", "belong_id": "H1guaREYPr"}, {"uid": "r1lI1bEZcH", "paper_title": "From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work aims to build one conditional face image generation framework from the audio signal. The whole framework is built on top of cGAN-based projector discriminator framework where the input condition vector is also used in the discriminator stage.  The authors compared with one recent work and demonstrated improvements on face retrieval experiment. \n\n\nQuestions:\n1\tIn terms of differences with previous approaches,  not sure if the self-supervised learning is one of them, since it is also applied in previous Speech2Face framework.\n2\tBased on the listed generated examples in Fig.2, most faces are frontal, especially along Z axis, not sure if the variation of Z can determine the head orientation. Faces in the third row and the forth may not keep the identity that well, especially the comparison between the face at the first column with others.   \n3\tOne typo may need to be addressed in the first paragraph of page 5, there are the the before word inference.\n4\tAs listed in Table 2, it seems the proposed approach achieves better performance when using 10 way training compared to other frameworks, any more analysis  why the proposed framework can achieve better performance in 10 way but obtain less accuracy in 2 way settings?   \n\n5\tDoes the test data of 2 way or 10 way experiments also include the same ratio of positive and negative pairs? If so, how about the performance on the same validation data?\n\n6\tAbout the results listed in Table 2, does the number 35^2 indicate 0.35? \n\n7.    It may be necessary to include these conditional face video generation works in the related work.\n\n", "belong_id": "H1guaREYPr"}, {"uid": "BJeqNfIsqB", "paper_title": "From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary - \nIn this work the authors propose a two-stage procedure for training a GAN which generates plausible faces conditioned on the raw waveform of a speech signal. In the first stage two embedding functions are trained, one taking as input a frame from a video (of a person speaking), the other taking as input the raw waveform of the same video's audio. These embeddings are trained to maximize the inner product of positively sampled embeddings (frame and audio from same video), and minimize the inner product of negatively sampled embeddings (frame and audio from different videos). In the second stage a GAN is trained where the input to the generator is the a random latent code (z), concatenated with the learned embedding of a speech signal from stage 1 (c). They also initialize the first half (\\phi) of the discriminator using the face embedding network from stage 1, and propose a modification of the relativistic GAN loss to prevent \\phi from losing the ability to produce face embeddings that have low inner product with the wrong speech embedding.\n\nThe authors explore the properties of their learned pipeline in a number of experiments. In 4.1 they compare their self-supervised speaker matching pipeline with prior work, showing that it has competitive performance with prior work that relies on either supervised pretraining, or additional labels such as age and gender. In particular they show that as the number of negative samples increases from 1 to 9, their identity matching performance significantly improves over prior work (in the K=2 regime their method underperforms prior work). Qualitatively they show that their generator has some reasonable success (it is hard to judge what perfect success would look like, and how far they are from reaching it) at disentangling aspects of facial appearence that can and cannot be inferred from the speech signal (QLA1). They also show qualitatively that the output of their generator can be smoothly controlled by interpolating between speech conditioning vectors, producing reasonable faces at each intermediary step (QLA2). In experiment QTA1 they quantitatively validate the conditioning vector c is affecting the generated image. In experiment QTA2 the authors (with a small issue, see weaknesses) show that their proposed modification to the loss function causes their outputs to be better matched to the speech conditioning as measured by the fixed embedding network trained in stage 1. Finally in QT3 they measure how well their generated faces can be used with the original face embedding network to perform image retrieval (I am a bit unclear on the details of this experiment, see questions)\n\n\nStrengths -\n* The authors' proposed pipeline and modified loss function offers a generalized framework for jumpstarting conditional GAN training\n* Their pipeline does not assume human-specified labels, but instead access to paired data from different modalities, which is can easily be obtained for many conditional image generation tasks (inpainting, super-resolution, colorization, etc.)\n* Their pipeline also doesn't seem particularly finetuned for the speech-driven image synthesis task, so it seems reasonable to believe it could be adapted to other tasks\n* Their results are qualitatively compelling, and they make a convincing efforts in experiments QTA1-3 to quantitatively show that the conditioning information is affecting the output\n* The paper is generally well-written and easy to follow\n\nWeaknesses - \n* Without completing the loop, and showing that the second stage of this pipeline helps with identity matching for speakers, I'm not clear on what the motivation for this particular form of conditional image generation is (this is unfortunate, because their framework is quite general, and it seems feasible that the authors could have applied it directly to a task where the output of conditional image generation is directly useful)\n* I interpret the main purpose of experiment QTA 2 as validating the effectiveness of their proposed loss modification, it seems like a natural experiment to make this claim more convincing is to compare a generator trained with Eq 4. against real images (hopefully getting a much lower matching probability than the 76.65% reported in the first experiment of this section)\n* It seems like an important ablation study is testing the effect of jumpstarting the GAN training with the pretrained networks of stage 1, and reporting what happens when one or both of these networks are initialized randomly (assuming that initializing randomly hurts performance significantly, this would be further evidence of their framework's strengths)\n* The novelty of the proposed approach is limited so far as I can tell (slight architectural modifications, and adding a negative sampling term to the discriminator loss)\n\nInitial Rating - Weak Accept\n\nQuestions - \n* In experiment QTA3 I'm confused by how including multiple faces from the same video clip affects measuring retrieval accuracy. Does retrieving any of the 50 faces from the same clip count as correct? \n\nExplanation of Rating - The novelty of the work is limited, and it doesn't seem clearly useful for any practical task. However the stated task is certainly a non-trivial one, and the qualitative results and experiments give compelling evidence that the authors are proposing a powerful framework for conditional image generation. I think that the high quality writing, experiments, and results; coupled with potential impact for other conditional image generation tasks warrant acceptance. However the paper is held back by the lack of novelty and lack of clear motivation.\n\nRevised Rating After Rebuttal: Accept\n\nExplanation of Revised Rating: The authors addressed my concrete concerns about missing experiments in the rebuttal. Despite my concerns about the motivation for this particular task, I think the good results produced by author's methodology indicate this work will be valuable in the context of conditional image generation more broadly. I am upgrading my rating to accept.\n", "belong_id": "H1guaREYPr"}, {"uid": "HyeKwG52tH", "paper_title": "DO-AutoEncoder: Learning and Intervening Bivariate Causal Mechanisms in Images", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presented an image data that are generated from two variables using some physics law. It also proposed a model to identify the causal relationship between the two variables using the image dataset. The method, in general, utilize the general idea that the causal direction is easier for the model to describe than the anti-causal direction. So the image is fad into a VAE based model in two different ways. The one with lower loses represents the correct causal direction. \n\nPros:\n1. Causal discovery is, in general, an interesting problem and causal discovery based on representation learning are of great importance.  \n2. The dataset presented can be used for generic causal discovery evaluation which can be useful for the community.\n\nCons and other details:\n1. The method assumes that A and B are known and given which is very unrealistic in natural images. Also with this assumption, the problem is not much different from causal discovery from measurement data rather than image data. \n2. Based on the previous point, the method, in general, does not match the motivation in the introduction where a causal representation needs to be learned as the images are already separated into different components. \n3. The method cannot be scaled to more than two variables even with all components given as it requires exponentially many trials of the method. This setting is not so interesting anymore with image input. \n4. There is much-related work with causality and representation learning also causality with NN or VAE. None of these related work has been discussed.  for example Leon Bottou https://arxiv.org/pdf/1907.02893.pdf; Many works from Mingming Gong etc\n5. The math is not very rigorous in general. For example, Eq(2) s a valid-loss but not likelihood. Also, the work did not say what likelihood under what distribution. This is propositional to Gaussian likelihood which may work fine in practice but the math presentation is not rigorous.  \n6. For the method (see figure 2), I did not see why the first part needs to be there as the second part takes the ground truth A as input. Using only the second part of the model which tries to see whether A->B is easier or B->A is easier is sufficient for the aim of identifying the relationship between given A and B. \n7. The dataset may be more useful to the causality community if it is released as a simulator rather than the images. \n", "belong_id": "r1e7NgrYvH"}, {"uid": "SyxvfpcntB", "paper_title": "DO-AutoEncoder: Learning and Intervening Bivariate Causal Mechanisms in Images", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In summary: This paper is not ready for publication. The paper contains some potentially interesting ideas, but the presentation quality is not sufficient for publication. The paper should be substantially improved before re-submission.\n\nStrengths:\n- Causality is an important and established research area, and papers on this topic would be timely.\n- Paper contains some interesting ideas to integrate causality into an auto-encoder (but see weaknesses below)\n- Paper proposes a new dataset for evaluating causal mechanisms (but the approach is not evaluated)\n\nWeaknesses:\n- The quality of the writing is inappropriate for a scientific venue. Language throughout the paper is loose, eg 'physics is a hot topic' or 'People have studied causality for a long time' or 'Causality is a bridge between science and philosophy' The paper should be re-written so that it is precise and clear.\n- The technical approach has several typos and lacks discussion of the approach. Instead, several high-level statements are made, with long equations. This makes appreciating the contribution of the paper difficult. \n- The dataset is potentially interesting, but it is artificial. A much more exciting dataset would be realistic data.\n- The experiments only evaluate the likelihood, but it is not clear whether this is on a training or testing set. \n", "belong_id": "r1e7NgrYvH"}, {"uid": "SkgN4O6TFB", "paper_title": "DO-AutoEncoder: Learning and Intervening Bivariate Causal Mechanisms in Images", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Thank you for your submission.\n\n- What is the specific question/problem tackled by the paper?\n\nThe paper proposes a VAE architecture to learn causal relations and allow for interventions. The architecture requires knowledge of the causal graph, and the direction of the causal arrows are inferred by comparing the log-likelihoods of generated images. The architecture may also require knowledge that an arrow exists between two vertices. This relies on the principle that 'low-capacity' neural networks can predict better along the causal arrows (with the cause as input and the effect as the output) than in the opposite direction (with the effect as input and the cause as the output).\n\nThe paper focuses on the graph (A, B) where one wants to understand whether A causes B, or B causes A. The paper also discusses intervening in this graph.\n\nThe paper uses a new dataset for evaluating the approach, based on simple Newtonian systems. \n\n- Is the approach well motivated, including being well-placed in the literature?\n\nI think the motivation is adequate, but the review of the literature glosses over related work (or the absence thereof) in predicting the direction of arrows in causal graphs. The comparison of the proposed dataset against existing ones is missing.\n\n- Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\n\nThe procedure for determining whether A causes B (or B causes A) is qualitative. The paper demonstrates that the performance gap between the correct and incorrect explanations is consistently distinguishable across multiple experiments.\n\nVisual inspection of the generated images is also used for assessing the quality of the models.\n\nBecause the results are qualitative, the support for the claims is not as strong as it could be (with quantitative results).\n\n- Summarize what the paper claims to do/contribute. Be positive and generous.\n\nThe paper has two main contributions:\n* Evidence to the Independent Mechanism principle (in a setting different from Bengio et al.'s transfer setup).\n* A new dataset for evaluating learning causal arrows (with accessible ground-truths).\n\nI think these are interesting contributions.\n\n- Is the paper clearly written?\n\nThe paper has a number of grammatical errors that should be fixed.\n\nThe explanation of how the latent interventions are made is important and should be included.\n\n- Clearly state your decision (accept or reject) with one or two key reasons for this choice.\n\nI vote for a weak accept.\n\n- Provide supporting arguments for the reasons for the decision.\n\nI trust that the writing issues will be addressed in due course, but I am also concerned about the fact that evaluations are qualitative. The qualitative results provide support for the contributions that could be strengthened. \n\nThe dataset is also an interesting contribution and it is a good idea to give it visibility. For this, though, it is important that the paper assess its strengths and limitations in comparison to alternative datasets.\n\n- Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\n\nI am not convinced that mentioning Kolmogorov complexity is an efficient use of the space. I think the content could be improved by making the motivation section more concise and adding a few more experimental results or discussion.\n\nWhich discussions would be good to have? I think it should be noted that the intervention on effect should behave as demonstrated (creating implausible scenarios). Also some more development on the spring example: What is the right causal graph for it, and can the arrows in that graph be learned?\n\nQuantitative results would also improve the paper. Maybe decide between A->B or A<-B based on a statistical test?\n\nYou give an example about elephant-grassland association. Please cite a source for that.\n\nSuppose that both likelihoods for A->B and B<-A are about the same. How do you decide if your model is too rich, or if there's no relationship? (This is an important question to understand if the method requires knowledge that an (A,B) arrow exists or not.)\n\nThe panels in Figure 5 do not support the claim. The simple net gets better at the cause, but in some cases the rich representation does a better job at the effect.\n\nI think the physics dataset is also a contribution, so its originality & impact should be discussed in comparison to related work. Why is this an adequate benchmark? How does it address limitations of other benchmarks that could be used to evaluate proposed solutions for the problem in question?\n\nIn summary, my suggestions for improving the paper are:\n1) Make sure & demonstrate (by adequate discussion of related work) the originality of the contributions:\n1.1) The method for detecting the direction of causal arrows.\n1.2) The dataset as a benchmark for the problem being studied.\n2) Report quantitative results across the dataset and maybe across multiple setups for each name/physical law, with good coverage. You may consider a test set where the parameters are within the sampling range of your training set, and also outside the sampling range (where success of the method would be even more interesting). ", "belong_id": "r1e7NgrYvH"}, {"uid": "SJlz8BM9KS", "paper_title": "Quantum Expectation-Maximization for Gaussian Mixture Models", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Quantum machine learning is a hot topic, recently. There are several 'schools of thought': \n-- quantum kernel embeddings, which work per-observation, and allow for application on noisy intermediate-scale devices,\n-- work based on quantum optimization solvers, which try to claim one could apply quantum eigensolvers to various training problems, but which does not seem to be applicable on intermediate-scale devices, given the scale of the requisite input,\n--  novel quantum algorithms. \nIn principle, the novel quantum algorithms hold the promise of an exponential speed-up. \n\nThe authors propose a novel variant of expectation minimisation for parameter estimation of a gaussian mixture model with uniform mixing coefficients, targeting an ill-defined model of quantum computing of their own coinage.  Unfortunately, the paper is very sloppy. \n\nThe sloppiness starts with the model of quantum computing, which seems to assume anything the authors needed:\n-- Definition 1: one can perform |k|d arithmetic operations in time polylog(d)\n-- sentence above Lemma 3.4: post-selection (which makes it possible to solve at least all of PP, cf. https://arxiv.org/abs/quant-ph/0412187)\n-- sentence below Lemma 3.6: 'quantum linear algebra subroutines and tomography' in no time at all (?), wherein there are information-theoretic limits \\Omega(exp(kd)) on the state tomography.\n\nIn terms of statistics and learning theory, the authors:\n-- do not consider the separation of the Gaussians as a parameter. It is well-known that with o(1) separation, exponentially many samples are required, and with sqrt(log(k)) separation, polynomially many samples suggest (http://ieee-focs.org/FOCS-2017-Papers/3464a085.pdf). Unless Definition 1 of the authors 'subsumes' processing of exponentially large numbers of samples in polylog(d) time, the authors may need to add an assumption on the separation.\n-- the authors explain that their approach works only for the gaussian mixture model with uniform mixing coefficients only at the top of page 4, in a completely obscure notation of their own, and do not reference it as an assumption in the theorems later. \n\nThe sloppiness continues with the description of the Experiments in Section 4. Authors present a table of some results, but do not mention whether these have been obtained on quantum-computing hardware, a simulator thereof (what simulator? with noise?), or whether this is some fully-classical variant of the algorithm being tested. This clearly violates any 'reproducibility checklist'.\n\nWithin the 'minor comments' category:\n-- the introduction of the EM algorithms for GMM is sloppy. It is well known that EM for GMM is super-sensitive to noise and balance of the mixing coefficients (https://ieeexplore.ieee.org/document/8635825) and can get stuck in arbitrarily bad local optima, which is not mentioned once. \n-- the references to other algorithms for GMM mention only Dasgupta 1999, rather than the subsequent 20 years of research, e.g., of Ankur Moitra at MIT (https://math.mit.edu/directory/profile.php?pid=1502). \n-- on the other hand, there are plentiful references to arxiv pre-prints of Kerenidis et al, at least some of which have been shown to be vacuous, e.g., https://arxiv.org/abs/1808.09266 with an infinite upper bound on the run-time and no relation to the present paper? \n-- there are some formulae missing or ending half-way through, e.g. Page 3 before 'alas', Page 3 after 'GMM would be'. \n-- there are a number of language issues: 'we can thresholding', 'some other real-world dataset'.\n-- the discussion starting with 'Let's have a first high-level comparison' is completely wrong. Especially the condition number estimates of 5 seem to have no justification what so ever. \n-- authors say that 'polynomial dependence on the rank, the error, and the condition number, make these algorithms impractical on interesting datasets' -- but it is not clear whether they mean that their algorithm is also impractical?\n\nOverall, while I like the idea of parameter estimation on a quantum computer, I could not recommend accepting the paper in its current form.  ", "belong_id": "Hkgs3aNYDS"}, {"uid": "r1xbMt9AFH", "paper_title": "Quantum Expectation-Maximization for Gaussian Mixture Models", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present a quantum algorithm for expectation maximization on a quantum machine, which is poly-logarithmic in the size of the dataset (and polynomial in other parameters such as the dimension of the feature space, and number of mixture components, condition number of the data and covariance matrices, some precision/error parameters etc) per iteration. So, compared to the . regular EM algorithm, this yields exponential speedup in the number of data points, but is worse in other factors (such as a k^4.5 dependence on the number of mixture components). So, the quantum system could be superior to a conventional computer for some settings of parameters. They run some (simulation) experiments on a dataset (VoxForge) to report accuracies (though there is no comparison of the accuracy of the classical algorithm). Also, there does not seem to be any experimental results to confirm the theoretical analysis of the scaling characteristics (i.e., to show that the scaling is as predicted by the theory).\n", "belong_id": "Hkgs3aNYDS"}, {"uid": "HJgrkphAcS", "paper_title": "Quantum Expectation-Maximization for Gaussian Mixture Models", "experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors present and analyze a quantum computing algorithm for learning GMMs.\n\nI think this paper cannot be accepted because it violates formatting guidelines. Also, I think it is not appropriate for ICLR since it assumes knowledge of quantum computing that most people at this conference would not have, and I as a reviewer do not possess, and hence cannot evaluate this paper. For example, I do not know bra-ket notation.\n\nIf the ACs disagree, I am happy to revise my review for this paper and try to be more thorough.", "belong_id": "Hkgs3aNYDS"}, {"uid": "Bye8oOS6Yr", "paper_title": "Continual Density Ratio Estimation (CDRE): A new method for evaluating generative models in continual learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This submission proposes a continual density ratio estimation method and suggested its application to evaluate a continually learned generative model without requiring the previous training data. The basis of the continual density estimation is based on a recursive relationship between the density ratio at step t and that at t - 1. Continually estimating the density ratio without storing the raw data is an interesting topic and could be useful for continual learning. To my knowledge, I have not seen this in earlier publications.\n\nHowever, I give reject to this paper because of the following reason: \n\nThe writing of this paper is not easy to follow. \n\n- The beginning of section 3 (CDRE in continual learning), I found it difficult to understand why the model q needs to be updated (indexed by t) while p(x) is not dependent on t. As far as I know, under the continual learning setting the data distribution p(x) is also conditioned on t. I interpret it as a general introduction on how density ratio could be estimated continually.\n- The Lagrange multiplier and the bias / variance statements need elaboration, I don't understand how it is affecting the bias and variance.\n- In the second part of section 3, the continual learning setting is introduced (in equation 11), however, it is no longer reasonable to use the symbol r_t in equation 12 which was initially defined in equation 5. \n- A loss for continual VAE is proposed in seciton Feature generation for CDRE, however, the p(x) is again independent of t. And I'm also suspicious that equation 13 is the correct way of adjusting VAE's objective with VCL. In VCL, the KL divergence is on the parameter distribution, which could help prevent forgetting, however, here the KL is between VAE's approximate posteriors, which alone is not sufficient for keeping the information of previous tasks.\n- There's lack of analysis / interpretation of results for section 4.1, e.g. what is the motivation of the experiments and what is the conclusion.\n- Through out section 4.2 - 4.3, it is not explained what is the source of variance in the experiment results.\n", "belong_id": "HJemQJBKDr"}, {"uid": "Skl_FzIaYS", "paper_title": "Continual Density Ratio Estimation (CDRE): A new method for evaluating generative models in continual learning", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "######### Updated Review ###########\n\nI'd like to thank the author(s) for their rebuttal. However, I am still on the same boat with R1 and recommend rejection for this submission. \n\n\n################################\n\n\nThis submission seeks to evaluate generative models in a continual learning setup without storing real samples from the target distribution.  The main technique the author(s) have used is the likelihood-ratio trick. I question the scope of this paper, as this is not a topic of general interest to the community. Additionally, the density ratio estimation technique is fairly standard. I vote to reject this submission for the lack of highlights and relevant potential applications. \n\nMy main argument for rejection. \nWhile continual learning is a trendy topic in the AI community, it's less well-received in the context of generative modeling, probably for the lack of real applications. Such works, including this one, fail to address any real challenge, as the hypothesized scenario is unrealistic. For example, I am not convinced of the significance of using f-div to evaluate model performance. And since importance sampling is notorious for its variance issues (the essential mathematical tool used in this model), the estimate is not expected to be reliable, say subsequent tasks q_t and q_{t-1} differ somehow. This submission feels more like playing a game with the rules defined by the author(s), not driven by practical considerations. \n\n", "belong_id": "HJemQJBKDr"}, {"uid": "rJevdma6Fr", "paper_title": "Continual Density Ratio Estimation (CDRE): A new method for evaluating generative models in continual learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, authors propose a continual learning for density-ratio estimation. The formulation of the CDRE Eq.(5)is quite intuitive, and it makes sense. Then, a log-linear model is employed for a density-ratio model, and it is estimated by using a density-ratio estimation algorithm. Through experiments, the authors show that the proposed algorithm outperforms existing methods.\n\nThe paper is clearly written and easy to read. The density-ratio estimation algorithm for continual learning is new and interesting.\n\nDetailed comments:\n1. I am pretty new to the continual learning. The formulation of CDRE is interesting. However, I am still not that certain whether the setup is realistic. In the introduction, authors describe that we cannot obtain data points due to privacy or limited cost budget. More specifically, if it is a privacy issue, we may not be able to use the model trained by the private data as well. Also, could you give me a couple of examples of the limited cost budget case?\n\n2. In this paper, authors employed the log-linear model. If we use another model, performance can be changed?\n\n", "belong_id": "HJemQJBKDr"}, {"uid": "Bye06G7Btr", "paper_title": "TabFact: A Large-scale Dataset for Table-based Fact Verification", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a new dataset for table-based fact verification and introduces a couple of methods for the task. I think that the dataset would be a useful resource (see some comments nevertheless on its construction), however the methods proposed are not particularly interesting, and the contributions to ML and NLP are overstated in my opinion. In addition the paper needs proof reading as there are many typos, some of which make comprehension problematic. In detail:\n\n- The dataset is the main contribution of this paper. Its size is great. However I have some concerns on its construction. The guidelines described for constructing simple and complex claims are rathe vague, e.g. how does one define 'too much symbolic reasoning' and explain it to crowdworkers? How was the linguistic complexity difference between the two channels measured? How were the claims sanity checked? In the beginning of section 2.3 it is stated that quality control filtered a substantial proportion of the statements. How was this done?\n\n- A troublesome aspect in my opinion of the dataset is that it only allows for evaluation of the Entailed/Refuted binary classification task, but not on whether the model used the right evidence to reach its conclusion. Thus it will always be possible for models to score highly without doing the right thing. Avoiding trivial re-writes helps, but it is unlikely to be enough as human crowd workers will try to optimize their earnings.\n\n- The two models discussed are OK as baselines, but not particularly interesting or appropriate. Both require substantial rule-based processing (named entity linking, latent program construction. templates) and eventually linearize structured data (the program or the table). I understand that this is not the main contribution of the paper, but given the substantial amount of work on semantic parsing and question answering I was expecting more appropriate baselines taking previous work into account. The performance of the model is not great either, especially if we consider that they are only evaluated on returning a binary label, not the correct evidence from the table.\n\n- While it is true that most of the fact verification work has focus on textual sources, the challenge of combining reasoning over continuous and discrete representations is not new. The various QA works mentioned in the related work section address this, as well as work on theorem proving: https://arxiv.org/abs/1705.11040 Furthermore, there has been at least one more previous work on table based verification against  FreeBase tables: https://www.aclweb.org/anthology/D15-1312/. Thus I believe the discussion of the challenges posed by this dataset should be re-framed, especially given that the kinds of programs that need to be constructed are of similar complexity to previous work like the WikiTableQuestions.\n\n- writing: 'the model is expected to excel... but to fall short', 'we follow the human subject research protocols' (which ones?), 'in case of obvious stylistic patterns' (which ones). On the whole it is understandable, but the writing should be improved.\n\n---- Post author response\n\nI have responded to the author response and I have revised my score.", "belong_id": "rkeJRhNYDH"}, {"uid": "H1gN3TMEcH", "paper_title": "TabFact: A Large-scale Dataset for Table-based Fact Verification", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work proposes the problem of fact verification with semi-structured data source such as tables. Specifically, the authors created a new dataset TabFact and evaluated two baseline models with different variations. They applied two criteria and different rewards for workers to collect two subsets of different levels (simple and complex). They also applied a negative rewriting strategy to avoid exploitable cues or patterns in the annotations. They evaluated two baselines models: (1) latent program algorithm (LPA), which makes use of simple string match entity linking and systematic search and trains a neural network discriminator, and (2) Table-BERT, which linearize the table into a sequence through concatenation or template, and treat it as a classfication problem. Both showed reasonable accuracy (~68%), but still below human performance (92.1%) on a held out test set.\n\nI would like to see the paper accepted because:\n(1) it proposes an interesting task (table fact verification) with a clean dataset, and the experiments evaluated the ability of the current neural network models, such as BERT, or hybrid models, such as the LPA baseline, to perform (symbolic) reasoning;\n(2) special care is done to ensure the dataset doesn't contain simple cues or patterns, which is a common pitfall in dataset collection, and the dataset is also validated through two reasonable baseline models. \n\nSome weakness and concerns are:\n(1) some error analysis of the baseline models are missing. For example, what types questions are hard/simple for table-BERT and what are hard/simple for LPA, and some comparison between them. This helps point out where the difficulty come from, for example, whether the difficulty is language understanding or symbolic reasoning.\n(2) 'To focus on statement verification against the table, we do not feed the caption to the model and simply mask the phrases in the statements which links to the caption with placeholders.'\nI am not sure this is the right thing to do since even the human annotators requires the caption to understand the context, why not also feed the caption into the model? \n(3) Although the Figure 2 showed that the higher order operations are indeed used in a majority of questions, which measures the breadth of the reasoning, it is unclear about the depths of the required reasoning, i.e., how many operations / steps are required to achieve the correct answer. It would help if the average number of steps / operations required for answering the questions are shown. \n\nIf the above concerns are addressed, I will be willing to raise my score.\n\n\nMinor comments:\n\nThe paper, especially the Appendix, requires some proof reading, for example, I believe the caption for Figure 5 in Appendix A is misplaced.\n\n'... they are explicitly banned bring ...' -> 'banned from bringing'\n\nAs illustrated in Figure 6, the title only acts as a placeholder in the statements to make it sound more natural. -> From the example, it seems the name of the player is kept unchanged in the sentence, which is different from a placeholder.\n\nSuggestions:\n\nI understand this might require some works, but it would be really helpful to add more comments and maybe examples for the functions shown in Figure 5 (you might need to split the table into two pages or have another table for examples), so that it can be used by works that follows the LPA approach.\n\nDuring the annotation, the workers are explicitly guided to modify the words, phrases or sentence structures but retain the sentence style/length to prevent from artificial cues\nAvoiding simple cues or patterns are important, so it will be good if more details can be shared (for example, the instructions/guidelines you showed to the workers). \n\n==================================\n\nPost rebuttal update:\n\nThanks for the update to the paper. I think this work provides a good dataset and some reasonable baselines, thus should be accepted. \n\nHowever, I am still a bit concerned about the categorization of questions and the analysis on reasoning depth, because they are all based on the programs generated by LPA, while systematic search's recall is just 77% and the programs potentially contains a lot of spurious ones. So the categorization of the questions in Figure 11 and the number of reasoning steps in Figure 12 has to be taken with a grain of salt. It is worth annotating a few hundred, say 100-300 questions, manually with the ground truth programs for question distribution analysis or reasoning depth analysis, and confirm the numbers estimated using programs from LPA. Those manually annotated examples would also be a great addition to the dataset to aid the researchers for better analysis. A good example is the WikiTableQuestions dataset https://github.com/ppasupat/WikiTableQuestions/releases, which contains 300 manually annotated examples (annotated-all.examples), and they are used to analyze the distribution of the questions in the paper. ", "belong_id": "rkeJRhNYDH"}, {"uid": "r1lp3ZLT5S", "paper_title": "TabFact: A Large-scale Dataset for Table-based Fact Verification", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Updated review:\nThank you for addressing the comments and making relevant edits. Additionally, the HIT section provides a lot more insights into the data collection process. I've updated my score based on the responses/edits made.\n\n------\n\nThis paper is about a dataset (TABFACT) aimed at promoting research for fact-verification using semi-structured data as evidence. The paper highlights how the existing fact-verification studies have been restricted to work with unstructured evidence, and hence lack generalization to use-cases where the evidence is in a structured format (eg. databases). The paper also highlights how fact-verification with semi-structured evidence is challenging, since it involves both linguistic reasoning (for paraphrasing, entailment etc.) and symbolic reasoning (for operations like count, min, max etc.). To tackle this, the authors suggest two approaches as baselines on the dataset - one uses off-the-shelf BERT model for NLI; the other one focuses on symbolic reasoning and is based on program execution - which primarily uses lexical matching and a set of predefined operations (like count/max/min) to construct a program. \n\nApart from a few issues (mentioned below), the paper is well written. The authors have provided a detailed overview of their data collection/verification pipeline and related model/experiments. Overall, it seems like an interesting dataset and I'm inclined towards accepting the paper. \n\nA few remarks/concerns are:\n1. Usefulness of the dataset:\nIt seems limiting for a fact-verification dataset to restrict itself to a binary space i.e. entailed vs refuted. It is often the case, that statements are not completely true or false. For example, the 3rd refuted statement in Figure 1 is partially true (there are five candidates in total). With a binary space for supervision, we dont really know if the system is actually able to capture the linguistic and symbolic nuances present in the task. It is entirely possible for the system to do well without learning well, if the learning/output space is this coarse (as opposed to a dataset like Vlachos and Riedel, 2014).\n2. Related work:\nThe paper talks about introducing a new format of evidence (structured text) and talks about unstructured text as the only other format of evidence. It misses out on a highly related task that uses image as evidence (notable datasets being: CLEVR-Humans, NLVR/2, GQA). Either these should be included in the related work, or the authors should make it explicit that this work only deals with textual evidence.\n3. The dataset statistics in Table 1 dont seem to add up (train+dev+test = (92,283 + 12,792 + 12,779) = 117,854 != 118,275 (=Total #Sentence)).\n4. Page 3, Section 2.3: we further perform quality control -> a line or two to explain quality control?\n5. Appendix C: No data/statistics have been provided to support the conclusion of the ablation study.\n\nMinor remarks:\n- Page 2: Section 2: \n1. overtly -> overly\n2. huge tables(e.g. -> huge tables (e.g.\n\n- Page 3: Section 2.3\n1. to filter 18% entailed of entailed statements -> to filter 18% entailed statements\n\n- Page 4: \n1. candidate) . we need to  -> candidate), we need to", "belong_id": "rkeJRhNYDH"}, {"uid": "Bkl1dQB2YH", "paper_title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper focus on how activation functions nonlinearities shape the loss surface of neural networks. The authors first show why the loss surface of every neural network has infinite spurious local minima. Secondly, the authors prove one theorem to show four properties of the loss surfaces of nonlinear neural networks.\nAlthough this paper is generally easy to follow, and the motivation about nonlinearities and the loss surface is clear, the insight of this paper is somehow shortcoming. Though this work can prove such properties within different preconditions, whether other works conditions are inconvenient or not may remain further discussions. This work is established based on several preconditions, while it is hard to assert that most kinds of neural networks can satisfy them perfectly. For instance, this work mentions Deep learning without poor local minima (NeurIPS2016), which requires full-rank and conditional independence of each node. It could be feasible when training a stacked network with particular limitations. This work requires all hidden layers are wider than the output layer, which may not be suitable for image segmentation, generative tasks or super-resolution, etc. Besides, it is laudable to prove fundamental rules in neural networks, while showing or inspiring researchers about how to implement or approximate such results to improve neural networks might be more helpful.\nSome questions:\n\n1. The authors assert that the loss surface of *every* neural network has infinite spurious local minima in the abstract, while in chapter 3 line 2, authors mention, We find that *almost all* practical neural networks have infinitely many spurious local minima. Which one is correct? Based on the following description in this paper, I guess this result is conditionally tenable.\n\n2. In lemma 3, authors construct the local minima by adding very negative biases and show they are spurious. However, it is less likely to learn such negative biases in the real case. Besides, some networks require biases equal to zero to achieve some specific target. My question is: if biases are conditioned on real-world data distribution, will lemma 3 and 4 still work in this case?\n\n3. This paper mentions infinite many times. Based on the reference, I believe that the neural network in this work refers to the artificial neural network, which is majorly stored within float tensors. So the number of combinations of parameters is finite. So why use infinite instead of many? Finite means I can train a small scale of networks with fewer precisions and check the global minima with a fixed dataset.\n\nAll in all, I believe this paper can be significantly improved if more details and experiments are provided.", "belong_id": "B1x6BTEKwr"}, {"uid": "B1g3dnrRKH", "paper_title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the theoretical property of neural network's loss surface. The main contribution is to prove that the loss surface of every neural network (with arbitrary depth) with piecewise linear activations has infinite spurious local minima. Moreover, the paper further characterizes the partition of the local minima. More precisely, the loss surface is partitioned into multiple smooth and multilinear open cells and within each cell, the local minima are equally good. This result can also explain the linear neural network case where there is only one cell, implying that all local minima are global. \n\nOn one hand, I find the paper very clear and the result very clean, which unites a lot of existing results. On the other hand, with a reasonable initialization in practice, we will not attain the local minima constructed in the paper since it requires all the activations to be positive. This limits the plausible implication from this theoretical study. Overall, I am very positive of the paper, the following are some detailed comments. \n\na. Please be more precise in the abstract that the activation function need to be piecewise linear. \nThe current sentence 'the loss surface of every neural network has infinite spurious local minima' does not include this specification. Moreover, if the activation is differentiable, is the claim still hold? It seems to me from the middle of page 3 that Li et al 2018 shows a non-local minima result in this case.\n\nb. How different is the analysis comparing to existing result?\nI have only go through the skeleton of the proof and have not read into the details. It seems to me the construction of the local minima is very similar to [1], since the main idea is to consider the linear region by activating all the neurons. Could you summarize the main difficulty to extend their results to multi-layer cases? (Maybe it would be good to illustrate with a simple case like 3 layers few neurons per layer)\nMoreover, when considering the local convexity, is it sufficient to say that locally in each cell it is a linear network and then the results on linear network transfers to it locally?\n\n[1] Small nonlinearities in activation functions create bad local minima in neural networks, Yun et al, 2019", "belong_id": "B1x6BTEKwr"}, {"uid": "SJxEpgtGcS", "paper_title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper studies the landscape of deep neural networks with piecewise-linear activation functions. The paper showed that under very mild assumptions, the loss surface admits infinite spurious local minima. Further, it is shown that the loss surface is partitioned into many multilinear cells. If the network is two-layer with two-piece linear activations, it is proved that within each cell every local minimum is global. \n\nPros:\n  --Constructed spurious local minima for piecewise linear activations, for a broader setting than previous papers. \n  --The paper is well written, with detailed explanation of proof skeleton. \n\nCons: \nThe significance of the results are not clear. Details are given below. \n\n1.\tThis paper only considers piecewise linear activation, which is a special type of non-linear activation. The major examples are ReLU and leaky ReLU. However, related results for ReLU have been studied for a few previous works mentioned in the last two paragraphs of Sec. 3.2. In particular, Yun et al. 2019b already proves a similar result for 1-hidden-layer neural-net with ReLU activation. I think extending the construction to broader settings (any depth, any piecewise linear and more losses) is mathematically nice, but the motivation of this extension is somewhat unclear to me. One motivation is that this is helpful for the purpose of understanding a big picture of the landscape, which I will discuss next.\n\n2.\tThe second major result is Theorem 2, on the big picture with ReLU-like activations. However, Theorem 2 is somewhat trivial to prove, and the link to Theorem 1 is rather weak.\n   (a) The main message of Thm 2 is the partition of the surface into multiple pieces, and each piece has good property. This partition is somewhat straightforward, and has been studied before, in, e.g., [R1].\n   For a global big picture, partitioning itself is not very interesting. Theorem 2 mainly describes the property of each region separately for 2-layer network, which is weaker than [R1]. \n   (b) Theorem 2 seems easy to prove. The 1st, 3rd and 4th property are all straightforward. The 2nd property local analogous convexity was given a 2-page proof in the paper. However, I dont understand why not use the following simple argument: for each region, the network behaves like a deep linear network, thus directly applying existing result shall imply every local minimum in the region is the global minimum of the region, right? If not, what is the difficulty?\n   (c) The 3rd property says some local minima are concentrated as a valley in some cell. What are the formal definitions of concentrated and valley in this sentence?\n   (d) The link to Theorem 1 is weak: the link is the 3rd property of Theorem 2 that some local minima are in a valley. It is just about some special local minima and weakly related to the other properties on the global view. In addition, the fact that some of them are in a valley may be due to the very special construction, thus it is not surprising and does not reveal anything interesting about the big picture. \n\n\n3.\tOther issues:\na) While ReLU-type activations are popular, there are still commonly-used activation functions are not piece-wise linear, e.g., tanh, swish. It is not proper to claim that 'this paper presents how nonlinearities in activations substantially shape the loss surface' and 'almost every practical neural network ....'. I suggest replacing 'nonlinearity' with 'piecewise linearity' in both the title and the abstract, and modifying the over-statements. \n   b) In Property 1 of Theorem 2, smooth and multilinear partition might be a bit misleading. The loss surface should be fractional in general, where multilinear cells are separated by non-smooth boundaries. Smooth partition seems to imply that the boundaries are smooth or the partition method is smooth in some sense.\n   c) The name analogous convexity is not appropriate. Analogous convexity is not formally defined in the paper. According to Sec. 4.3 third paragraph, the property of analogous convexity that the local minima wherein are equally good. It seems that analogous convexity is just all local minima are good, which is very different from convexity. It is a weaker property than quasi-convexity, star-convexity, etc, and thus it is better not to call it analogous convexity.\n    d) Property 3 of Theorem 2 is very far from mode connectivity. The proof of Property 3 relies on a special construction of Theorem 1, and the latter is for two arbitrary global minima. \n\n\n[R1] Soudry and Hoffer. 'Exponentially vanishing sub-optimal local minima in multilayer neural networks.' arXiv preprint arXiv:1702.05777 (2017).\n\n\nConclusion:  I think this paper is studying an important and interesting question, and the efforts of constructing local minima and understanding big picture are both interesting to me. However, Im afraid the current form of the paper does not meet the standard of the conference. That being said, it would be a nice paper if the big picture can be explored deeper, and the link to the spurious local minima can be built stronger. \n", "belong_id": "B1x6BTEKwr"}, {"uid": "SyliC__PYB", "paper_title": "Model-based Saliency for the Detection of Adversarial Examples", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method for training networks to detect adversarial examples and by virtue of doing so, providing defense against adversarial attacks. Two different approaches are examined, in which a saliency map is used in combination with the input as a mask. In one instance the saliency mask is based on a classifier used to distinguish 'normal' from adversarial examples. In the other instance, the salient pixels themselves form the basis for defense. In both cases, the saliency map is combined with the image for training a CNN by way of an element-wise product.\nOverall, this presents a relatively simplistic way of deriving representations of saliency and combining these with inputs for training that builds robustness against white and black box attacks. At the same time, the empirical results presented reveal a considerable degree of success in providing a defense against such attacks. I find that this presents an interesting contribution to the literature addressing both adversarial attacks, and new notions on ways of characterizing saliency.", "belong_id": "HJe5_6VKwS"}, {"uid": "rklB0nk6tB", "paper_title": "Model-based Saliency for the Detection of Adversarial Examples", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper studies methods for detecting adversarial examples using saliency maps. The authors propose using the method of Dabkowski and Gal (2017) to generate saliency maps and then train a classifier on these maps (or their dot product with the input image) to distinguish natural from adversarial examples. They perform experiments evaluating the white-box and black-box robustness of their detection scheme.\n\nFrom a technical perspective, the contribution of the paper is rather incremental. The detection of adversarial examples by training a classifier on saliency maps has already been studied in prior work. The only modification proposed in this work is using an (existing) alternative method for producing the saliency maps and utilizing the dot product of maps with images.\n\nFrom a conceptual perspective, the impact of detecting specific adversarial attacks is not clear. In a realistic setting, an adversary could use a very different attack or even utilize a different set of transformations (e.g. image rotations). Thus, in order to demonstrate the utility of their method in a black-box scenario, the authors would need to evaluate the defense in a variety of different scenarios. At the very least, they should consider generalization to difference attacks (e.g., train against FGSM and BIM, and test against DF).\n\nMoreover, the robustness against white-box adversaries is not sufficiently studied. Firstly, the robustness of the non-adversarially trained detector is suspiciously high. There is little reason to expect that a composition of two neural networks (the saliency map methods and the classifier) would be non-trivially robust. The authors should consider alternative attacks perhaps using more iterations with a smaller step size. Secondly, after adversarial training, only the robustness against the same attack is considered. In order to argue about white-box robustness, the authors would need to evaluate against a variety of diverse adversaries.\n\nOverall, the technical and conceptual contribution of this paper is insufficient for publication at ICLR, even ignoring the concerns about its experimental evaluation.", "belong_id": "HJe5_6VKwS"}, {"uid": "r1luCy_aYS", "paper_title": "Model-based Saliency for the Detection of Adversarial Examples", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an adversarial defense method that is a saliency-based adversarial example detector. The method is motivated by the well-known fact that saliency maps and adversarial perturbations are having similar mathematical formulations and derivations. By using model-based saliency maps rather than gradient-based ones, it seems to detect hard attacks with smaller perturbation size as well. As far as the authors mentioned, the proposed method is simply using different techniques to derive saliency maps compared to the previous methods.\n\nOverall, the intuition and motivation of this paper are from the previous works and the main contribution is to use another (powerful) saliency map extractor for learning an adversarial detector. Although the overall results are improved from the previous methods, the proposed method is lack of novelty. \n\n- For SMD (Saliency Map Defense), what is the reason that the input image is not used together? computational issue? performance degradation? \n- Is it possible to train a single detector that can handle all different adversarial attacks?\n- Would the distance between saliency maps from different attacks be small? How does the saliency map change under different attacks?\n- Have you tried any other powerful saliency maps other than Dabkowski & Gal (2017)?", "belong_id": "HJe5_6VKwS"}, {"uid": "HyeYaubg5r", "paper_title": "Hierarchical Disentangle Network for Object Representation Learning", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a method for solving the following problem: given an Image I from a labelled dataset with a label hierarchy as a tree of depth L, produce a set of vector representations {F_1, F_2 ... F_L}, such that a) the set can be used to reconstruct I as well as possible and b) each representation in the set only contains information about the label at the corresponding level in the label tree.\n\nWhile I am not extremely familiar with work in disentangled representation learning, the authors claim is true to my knowledge that most work on disentangling factors does not explicitly take into account hierarchical structure as this work does. Therefore, this work appears novel and interesting to me. I will leave the assessment of the degree of novelty to other reviewers/AC who may be more familiar with the literature.\n\nThe approach is also effective, and the authors demonstrate through visualizations and experiments that the proposed model can be trained and accomplishes its objectives reasonably well. My overall decision is to accept this paper, but there are some improvements I'd like to see since I found it difficult to understand in some places. \n\n- There is repeated use of the term 'granularity' in the abstract and Sec. 1 which is undefined. What, according to the authors, is the difference between having a hierarchical structure and multi-granularity? I suggest clarifying what is meant by this, or avoid using the term (used hierarchy instead).\n\n- In Sec. 3.2, it appears that what is meant by R_l is actually the set {R_1, ..., R_L}. This would imply that the R's from different levels are randomly combined, and the number of representations combined is always L. Is this correct? In either case, what happens here should be made much clearer. It took me several readings to arrive at this interpretation.\n\n- It took me a while to infer how the results in Figure 3 are generated. There is a sudden switch in Sec. 4.1 from model training details to its use for semantic translation, which was not explained.\n\nMinor suggestions:\n\n- Please use parenthetical citations throughout the paper where appropriate (use \\citep{}) to avoid breaking the flow of reading.\n- Pg. 1, last line: 'us human' -> 'humans'\n- Pg. 2, line 1: 'with others' -> 'to others'\n- Pg. 2, line 2: 'hierarchy structure' -> 'hierarchical structure'\n- Pg. 2: 'multi-granularity nature' -> 'multi-granular nature' or 'hierarchical nature'\n- Pg. 7, line 2: 'an significant' -> 'a significant'\n- Pg. 7, line 4: 'At the last' -> 'At the end'\n- Pg. 7, 'it can be reached' -> 'it can be seen that'\n- Pg. 7: 'Table.4.1 gives the evaluation results' -? 'Table 1 gives ...'\n- Pg. 7, Please revise: 'which is deserved to make more efforts'\n- Pg. 7: 'to applied' -> 'to be applied'\n- Pg. 8, line 1: Remove 'quite'; typo in 'leraning'", "belong_id": "rkg8xTEtvB"}, {"uid": "S1e_7VtxqH", "paper_title": "Hierarchical Disentangle Network for Object Representation Learning", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the problem of learning disentangled representation in a hierarchical manner. It proposed a hierarchical disentangle network (HDN) which tackles the disentangling process in a coarse-to-fine manner. Specifically, common representations are captured at root level and unique representations are learned at lower hierarchical level. The HDN is trained in a generative adversarial network (GAN) manner, with additional hierarchical classification loss enforcing the disentanglement. Experiments are conducted on CelebA (attributes), Fashion-MNIST (category), and CAD Cars (category & pose).\n\nOverall, this papers contribution seems quite outdated and presentation is not very clear.\n\n(1) Learning hierarchical representation using GAN has been explored in Kaneko et al. 2018 but not even mentioned in the paper. \n\nGenerative Adversarial Image Synthesis with Decision Tree Latent Controller. Kaneko et al. In CVPR 2018.\n\nAs far as reviewer understands, the bottomline for publication at ICLR is to demonstrate significant improvement/technical novelty compared to prior art. \n\nThis paper should also compare against GANs or other state-of-the-art generative models with flat representation (especially on face generation) in terms of SSIM, inception score, and FID score. Without such comparisons, it is unclear what is the value of hierarchical representation proposed here.\n\n-- Glow: Generative Flow with Invertible 1x1 Convolutions. Kingma and Dhariwal. In NeurIPS 2018.\n-- Progressive Growing of GANs for Improved Quality, Stability and Variation. Karras et al. In ICLR 2018.\n-- A Style-based Generator Architecture for Generative Adversarial Networks. Karras et al. In CVPR 2019.\n\n(2) The interpolation results (see Figure 5) look a bit strange as the transition between last columns are not very smooth. Also, please provide details about this experiment: are you applying linear interpolation? Whats the interpolation parameter for each of the column?\n\n(3) For image retrieval experiment, it is not clear if the proposed method is better than any state-of-the-art generative models with flat representations. One strong baseline is to use the latent representation of a pre-trained GAN model as comparison.\n", "belong_id": "rkg8xTEtvB"}, {"uid": "HyxXXnZ25S", "paper_title": "Hierarchical Disentangle Network for Object Representation Learning", "experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed the hierarchical disentangle network (HDN) that leverages hierarchical characteristics of object categories to learn disentangled representation in multiple levels. Their coarse-to-fine manner approach allows each level to focus on learning specific representations in its granularity. This is achieved through supervised learning on each level where they train classifiers to distinguish each particular category from its sibling categories which are close to each other. Experiments are conducted on four datasets to validate the method.  \n\nExploiting object hierarchy to learn disentangled representation is a promising direction but I lean towards rejecting this submission due to\n1. No results on commonly used disentanglement metrics (e.g. see [1])\n2. No comparison with existing supervised/unsupervised methods on disentangled representations (e.g. [2][3])\n3. The needs for full supervision on each level and manually designed fixed hierarchy require labels for the full hierarchy and make it not applicable to many existing data. This probably is why the proposed approach did not work well for more complex datasets like ImageNet.\n\n\nThese also should be addressed:\n1. The choice of adaptive instance normalization should be discussed. AdaIN could be used to account for small changes like color or local changes, but can it be used for larger and more global change (for example from animal category to human). If not, is it a limitation of this method?\n2. Justification for several choices made in the method, for example in the form of qualitative/quantitative ablation studies - usage of local brother categories, image/feature reconstruction losses\n3. Can the metric in Table 1 prove disentanglement is achieved? What if E and G learned some way to fool the classifiers\n4. Authors use conditional generative adversarial networks but it seems that there is no noise.\n5. Discussion of failure cases. For example, the authors mentioned that the proposed approach did not work well for ImageNet. Why is this the case?\n\n\nMinor comments:\n- some citations are not properly formatted\n\n[1] Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations, Locatello et al.\n[2] Disentangled Sequential Autoencoder, Li and Mandt\n[3] Exploring Disentangled Feature Representation Beyond Face Identification, Liu et al.\n\n", "belong_id": "rkg8xTEtvB"}, {"uid": "r1eVNoThcH", "paper_title": "Hierarchical Disentangle Network for Object Representation Learning", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an algorithm for supervising networks for image classification and reconstruction with the object's hierarchical categories in mind. The claimed benefits are the improved generalizability and interpretability. The paper reports per-category-level analysis on the semantic image reconstruction task and retrieval on seen and unseen object categories.\n\nI am currently leaning towards weak accept because I find the paper's claim and technical details generally convincing and simultaneously extracting low-level and high-level features trained using the hierarchical levels of categories is novel. Generalization to unseen categories tends to be a good proxy for real world performance and directly learning the high level categories is a useful idea for doing so.\n\nAlthough I am leaning towards weak accept, I think this paper is close to borderline because the findings do not seem experimentally well validated. It would be more interesting to see Table 3 on multiple unseen categories instead of one special case per dataset. Another idea for experiments is doing cross-dataset evaluations where different datasets may have different leaf categories but shared high level ones. I think it may also be interesting to compare with a non-hierarchical retrieval model and then obtain their high-level prediction accuracy using the corresponding parent level categories.\n\nThe paper generally needs polishing. Minor typos I found:\n\nPage 5: classificaiton, classifers\nPage 6: intuitional->intuitive\nPage 7: an significant\nPage 8: leraning\nPage 1: human\nFigure 3: arbitary", "belong_id": "rkg8xTEtvB"}, {"uid": "ByxRt-tpFr", "paper_title": "DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper propose to modify the existing work [1] of self-training framework for graph convolutional networks. It tracks three limitations of [1] and propose three  use a threshold-based rule to insert new pseudo-labels and dynamic change the pseudo-label set. Moreover personalized weight are assigned to each activate pseudo-label proportional to its current classification margin. Evaluation of the proposed framework is performed on four networks for semi-supervised node classification task with varying label rates.\nPros:\n1. This work tracks and addresses the limitations of existing work.\n2. Authors conduct experiments on multiple dataset with varying 2-hop coverage ratio. \n3. The overall paper is well written, except some typos, e.g. in page 6, section 5.1 'Each of three dataset is ......'. Should 'three' be 'four'.  \nCons:\n1. The proposed framework makes modification on the existing work, which is a good extension but the novelty is limited.\n2. The gap of the experiment results between the proposed method and the baseline methods are quite small.\n3. Only GCN instantiation are provided, it is suggested to evaluate the effectiveness on the other GNN variants, such as GraphSage, GAT and MoNet.\n[1] Li et al. Deeper insights into graph convolutional networks for semi-supervised learning.", "belong_id": "SJgCEpVtvr"}, {"uid": "r1lIXkR6YH", "paper_title": "DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes an approach for learning graph convolutional networks for inferring labels on the nodes of a partially labeled graph  when only limited amount of labeled nodes are available.\n\nThe proposal is inspired from Graph convolution Networks with the idea of overcoming the major drawback of these models that lies of their behavior in case of limited coverage of the labeled nodes, which implies using deeper versions of the model leading at the price of what the authors call the over-smoothing problem.  \n\nThe main idea here consists in relying on self training to get a better coverage of labeled nodes enabling learning with less deep models, this translates to a simple and intuitive algorithm. Using self training is not new in GCN but the way it is used here, computing adaptively a threshold for incorporating pseudo labels and using weights according to the confidence off predictions is new.\n\nExperimental results are reported on citation datasets and compared with many baselines show similar results as baselines when the coverage increases up to 50 labeled nodes /class, but the method brings significant improvements when the coverage is low (e.g. only few, <20, labels /class). \n\nAlthough the difference with previous approaches do not look like a huge step, the method seems to be quite justified empirically and achieve real good results wrt state of the art.", "belong_id": "SJgCEpVtvr"}, {"uid": "H1eZGztV9B", "paper_title": "DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "#Summary\n\nThis paper proposes a generalised self-training framework to build a Graph Neural Network to label graphs.  Of importance is the dynamic nature of the self-training. The authors do not change the GCN but extend the self-training portion as per the prior GCN paper by introducing Dynamic Self-Training that keeps a confidence score of labels predicted for unlabelled nodes.\n\n# Comments\n\nThis is a very interesting paper in terms of looking at the effects of changing the self-training framework to better utilise the underlying structure. As such we can exploit information from other nodes that are yet to be labelled.\n\n1. As the self-training is going on, are there different computational costs or are they about the same?\n2. For CiteSeer 20 and 50, why does \\beta = 0.45 switch from the other experiments?\n3. Will such self-training be useful for general NN self-training procedures\n4. If we had soft-labelling or uncertainty on which label each node has, how would the dynamic self-training be changed?\n\n#Other notes\nPlease remove the \n\nAn appendix\nYou may include other additional sections here", "belong_id": "SJgCEpVtvr"}, {"uid": "Bkl2KwYTtH", "paper_title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work, the authors focus on the high-dimensional regime in which both the dataset size and the number of features tend to infinity. They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations. I think it is a solid work and vote for acceptance. \nPros: \n(1) This paper has a solid theoretical foundation. Although I have not checked in detail, I think the deduction is clear and the contribution is well-established.\n(2) It extends some traditional bounds to more general cases. I think it will provide useful guidance to real applications, such as the network design in deep learning.\n(3) The authors have explained the results in a clear way. Thus, it will benefit the following readers and give deep insights about the related research areas.\nMinor comments:\n(1) I think some assumptions should be explained. For example, why the authors focus only on linear model. Due to the simplicity or the requirement from real applications?\n(2) More experimental results on large data sets should be added to validate the effectiveness.", "belong_id": "BJx7N1SKvB"}, {"uid": "rklsQVQCtS", "paper_title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper investigates the asymptotic spectral density of a random feature model F(Wx + B).  This is an extension of existing result that analyzed a model without the bias term, i.e., F(WX). This extension requires a modification of the proof technique. In addition to that, it analyzed a mixture of linear and non-linear activation functions, and show that mixture is better than single nonlinearity in terms of expected training error for ridge regression estimators.\n\nPros:\n- This paper investigates an interesting problem and it successfully extends the existing work. The theoretical curve well matches the simulated curve.\n- The finding that mixture of nonlinearities gives better expected training error is interesting.\n\nCons:\n- The extension to the model with bias seems a bit incremental. In practice, we may consider an input with additional constant feature, X <- [X,1], to deal with both models in a unified manner. There should be more discussion about why this kind of trivial argument cannot be applied in the analysis.\n- The effect of mixture of activation functions is investigated in the 'training error,' but I don't see much significance on investigating the training error thoroughly. Instead, people are interested in the test error. I guess there does not appear such a trade-off for the test error and the linear activation function would be always better because the true function is the linear model. Hence, more expositions about why the training error is investigated should be provided.\n\nMore minor comment:\n- I guess the definition of Etrain  (Eq.(17)) requires an expectation with respect to the training data.\n- Assumptions of the activation function f should be provided; is it just assumed to be differentiable?, ReLU is included?\n- The definition of G(\\gamma) in page 6 had better to be consistent to that in previous pages.\n\n", "belong_id": "BJx7N1SKvB"}, {"uid": "SJgdT5JY5S", "paper_title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper analyzed the asymptotic training error of a simple regression model trained on the random features for a noisy autoencoding task and proved that a mixture of nonlinearities can outperform the best single nonlinearity on such tasks.\n\nComments:\n1.The paper is well written and provides sound derivation for the theories.\n\n2. Since this area is out of my expertise, Im not sure whether merely extending the work of Pennington & Worah (2017) to non-Gaussian data distributions is significant enough or not.\n\n3. Except for Fig 4, the other figures seem out of the context. There is no explanation for the purpose of those figures in the main contents. It is a bit hard for the audience to figure out what to look at in the figures or what the figures try to prove. \n\n4. In ..., and our analysis actually extends to general such distributions, ... , general should be generalize.\n\n5. In And whether these products generate a medical diagnosis or a navigation decision or some other important output, .., whether should be no matter.\n\n6. ..., they may not be large in comparison to the number of constraints they are designed asked satisfy. should be ...  they are designed to satisfy.\n", "belong_id": "BJx7N1SKvB"}, {"uid": "ryePl_e5Kr", "paper_title": "Implicit Bias of Gradient Descent based Adversarial Training on Separable Data", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper applies theory on implicit bias of gradient descent to an adversarial training toy example. It attempts to utilize the theoretical results for deriving insights on how adversarial training establishes robustness. The theoretical results are complemented with experiments on linear classifiers and small neural networks.\n\nThe key technical contributions look mostly like an application of the existing theory on implicit bias of gradient descent, thus I would rate the originality of this work as moderate. From the point of view of adversarial machine learning, I don't consider the results to be significant. In particular, there is a large discrepancy between the theoretical results on the toy example and the empirical convergence behaviour of adversarial training of neural networks (compare Figure 2 and 3). In particular, the theoretically derived exponential speed-up of adversarial training for the toy example is not reflected by empirical observations for practically relevant adversarial training tasks (e.g. consider the convergence behaviours reported by Goodfellow et al. (2014) or Madry et al. (2017)). Thus, the theory doesn't seem to be generally applicable beyond the linear toy model. A clear discussion of its limitations is missing.\n\nOverall, while this may be an interesting extension of the existing body of literature on the implicit bias of gradient descent, I find that from an adversarial machine learning perspective this paper does not meet its objective to derive generally applicable insights on how adversarial training works. ", "belong_id": "HkgTTh4FDH"}, {"uid": "H1eIh4uhYB", "paper_title": "Implicit Bias of Gradient Descent based Adversarial Training on Separable Data", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies the implicit bias of gradient based adversarial training for linear models on separable data with the exponential loss. Both l2 and general lq perturbations are studied.\n\nFor l2, the authors show that for small enough perturbations, the algorithm converges in direction to the max l2 margin solution, with faster convergence rates compared to standard gradient descent, both for the clean loss and the parameter direction (O(1/sqrt(T)) instead of O(1/log(T))).\nFor lq, convergence to a different max-margin direction is shown, given according to a mixture of the l2 and lp norms, though the parameter convergence is slower.\nThese results are further illustrated by numerical experiments.\n\nThe topic of the paper is novel and interesting, in particular the finding that adversarial training can lead to benefits in terms of optimization in addition to robustness, as well as the characterization of the inductive bias obtained when using lq perturbations in adversarial training (i.e. a mixture of lp and l2 norms, rather than just lp). Granted, the setting of linear models is a bit limited, but it provides a first step for more realistic models. The paper is also well written, and also has a nice numerical validation of the results. Overall, I am in favor of acceptance.\n\nHere are some questions/comments that could be further discussed:\n* for l2, while the obtained rates for parameter convergence are better in their dependence on T, they depend on a data-dependent quantity alpha in contrast to standard GD, suggesting that the rate could be worse for small alpha: is there a trade-off here? would standard GD be preferable if alpha is small, or could the current rate (7) automatically adapt to such settings?\n\n* in Theorem 3.4, how does the choice of c come into play? can you obtain better rates by optimizing it (ideally this should happen when q=2)? Regarding the comment on tending to the lq-norm margin for c -> gamma_q, is this at the cost of poorer convergence?\n\n* for lq perturbations, while I agree that the studied algorithm is interesting since that's what's used in practice for deep networks, I do wonder if in this specific setting you could get better convergence (and with the right norm lp instead of the mixture) by optimizing using the appropriate geometry, e.g. with mirror descent.\n\n* the analyzed algorithm considers optimal perturbations -- do you have a sense of how robust the theory is to errors in the inner optimization?\n\n\nminor comments/typos:\n* second bullet in 'main contributions': 'mixed-norm margin' could be further explained, or at least point to the definition. The terminology is also confusing as it sounds like a matrix mixed-norm\n* 'polyhedral cone ...': bar{u} should be u?\n* after Theorem 3.2 'not an issue': not so clear, this should be discussed further (see comment above)\n* section 3.2, 'defer discussion for 1, infty': you could provide a flavor of the results for these cases in the main text, given their prominence in practice\n* figure 1(b): for the direction plot, is it actually u_2? how should this plot be interpreted given that the two curves converge to a different direction?", "belong_id": "HkgTTh4FDH"}, {"uid": "SkeJiVQ0Fr", "paper_title": "Implicit Bias of Gradient Descent based Adversarial Training on Separable Data", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "**Contributions:**\nThis paper extends results on the implicit bias of gradient descent (GD) Soudry et al. (2018)[3] for the case of gradient descent based adversarial training (GDAT).\n\n**1) Theoretical result for L2 norm:** Convergence of standard risk in GDAT is significantly faster than its counterpart in the standard clean training using GD: For any fixed iteration T, when the adversarial perturbation during training has bounded l2-norm, the classifier learned by GDAT converges to the maximum l2-norm margin classifier at the rate of O(1/T), exponentially faster than the rate O (1/ log T ) obtained when training with only cleaned data Soudry et al. (2018) [3].\n\n**2) theoretical result for Lq norm:** When the adversarial perturbation during training has bounded l_q-norm with q > 1, with a proper choice of c, the gradient descent based adversarial training is directionally convergent to a maximum mixed-norm margin classifier, which has a natural interpretation of robustness, as being the maximum $l_2$-norm margin classifier under worst-case $l_q$-norm perturbation to the data.\n\nThe paper is well written and easy to understand. I havent checked the proofs, but I focused on readability and motivations. \n\nI have the following questions.\n\n**Theoretical questions:**\n- Can you please explain the intuition of lemma 3.2 and 3.3 for convergence?\n**Experiments questions:**\nKeeping in mind that the core of the paper is about proving that GDAT enjoys the same implicit bias as GD and better convergence performance on clean data, I cant help myself by asking you how these properties reflect in experiments with deep Neural Networks:\n    - Can you please clarify your motivation for the reduction of the performance gap when the with of the hidden layer increases (Figure 2)? It is not clear what do you mean by ...as network width increases, the margin on the samples outputted by the hidden layer also increases...\nMy personal interpretation of the results is that the inner maximization problem is much harder to solve and being approximated, the effect of the GDAT is much less prominent, meaning that PGD is not enough to benefit from adversarial training in this sense. What do you think about it?\n    - Take away message of the paper: adversarial training accelerates convergence. Theoretical results are for the training empirical loss.\nIs this also the case of the validation loss? You observe this behavior for the case of a single MLP on MNIST binary problem 2 vs 9 (appendix E) claiming that adversarial training improves generalization performance.\nThis is not observed in the literature, it is actually the opposite [1, 2]: it exists a tension between robustness and performance on the clean test set. Why do you think you are not observing this? My guesses:\n    - you are analysing a very simple binary problem\n    - your model is very simple and comparable to a linear classifier, so your theoretical results hold.\n\n[1][1805.12152 Robustness May Be at Odds with Accuracy](https://arxiv.org/abs/1805.12152)\n[2][1810.12715 On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models](https://arxiv.org/abs/1810.12715)\n[3][1710.10345 The Implicit Bias of Gradient Descent on Separable Data](https://arxiv.org/abs/1710.10345)\n", "belong_id": "HkgTTh4FDH"}, {"uid": "SJxFSI0cKH", "paper_title": "First-Order Preconditioning via Hypergradient Descent", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an interesting optimization algorithm called first-order preconditioning (FOP). \nThe basic idea of FOP is updating the preconditioned matrix by its gradient, which avoid calculating or approximating the Hessian directly. To make the algorithms more practical, the authors also conduct the low-rank FOP and the momentum-type version. The empirical studies on CIFAR-10 and ImageNet validate the effectives of the proposed algorithms.\n\nMajor comments:\n\n1. Section 2.1 says we follow the example of Almeida et al. (1998) and assume that J does not dramatically. However, the goal of FOP is to encourage J reduce faster. Is there any conflict?\n\n2. In low-rank FOP, the initial preconditioner P contains the term I_m which does not exist in standard FOP (section 2.1). How does this term affect the update procedure? Can you provide some details?\n\n3. Theorem 2 provide a linear convergence of FOP under convex, Lipschitz and PL condition. The proof relaxes the preconditioner P into its minimum and maximum eigenvalues. Since P changes over the course of training, it is difficult to check weather the result of Theorem 2 is stronger than gradient descent method.\n\n4. Why the experimental results not include the other second order optimization algorithms such as K-FAC and KFC?\n\nMinor comment:\n\nThe notations M in (1) (2) and (5) are ambiguous. It is prefer to use another letter to present the preconditioner in (1).\n", "belong_id": "Skg3104FDS"}, {"uid": "rJxhtuoCYB", "paper_title": "First-Order Preconditioning via Hypergradient Descent", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies hypergradient descent for precondition matrices. The goal is to learn an adaptable preconditioning for the task while training. Specifically, they take the gradient of the loss wrt the precondition matrix and update the precondition matrix to decrease the loss. They reparametrize the precondition matrix to ensure it is positive-definite and provide low-rank approximations and they provide cheap approximations for CNNs.\n\nPros:\n- Figure 3 and 4 show promising results on cifar10 with a 9-layer cnn.\n- Figure 4 shows FOP can improve the accuracy for particular hyper-parameters. In cases improving by 2%.\n\nCons:\n- Results on imagnet are not particularly good. The improvement is not significant.\n- Why positive-definite precondition matrix rather than positive-semi-definite?\n- Section 5: why is a degenerate precondition matrix bad? Fisher and Hessian for deep networks can be highly ill-conditioned.\n- Theo 1 seems to have errors. The term M_t in the update rule should show up in the bound on P as an exponential term in the first upper bound.\n- Figure 2: On mnist after 20 epochs the model has not reached 1% test error. Not clear if we can make any conclusions from this figure.\n\nAfter rebuttal:\nI keep my rating as weak reject. I reiterate that results look promising. However, the quality and accuracy of the writing are not acceptable for a paper on optimization. In my original review I only named a few problematic statements. I have to clarify that I do not think fixing only those few is enough.\n\nI am also not convinced about the proof of Theorem 1. Basically, section 6 looks very much like section 5 from Baydin et al. 2018. Even the wording is mostly the same. Theorem 5.1 in Baydin et al. 2018 is based on their update rule in Eq 6 in the form of alpha_t = alpha_{t-1} - beta nabla^T nabla, where alpha does not appear in the second term. However, in this paper, the update rule on line 7 in Algorithm 1 is M_t = M_{t-1} + rho * eps *(.) M_{t-1}, where M_t appears in the second term. Hence, the first bound in Theorem 1 in this paper cannot simply be the same as in Baydin et al. 2018.", "belong_id": "Skg3104FDS"}, {"uid": "BJlu-ffY5H", "paper_title": "First-Order Preconditioning via Hypergradient Descent", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a first-order preconditioning (FOP) method to generalize previous work on hypergradient descent to learn a preconditioning matrix that only makes use of first-order information.\n\nPros:\nThis paper extends the idea of hypergradient descent in [Almeida et al., 1998; Maclaurin et al., 2015; Baydin et al., 2017] with a preconditioning method. A low-rank FOP is further proposed to lighten the computation burden for the preconditioning matrix. \n\nCons:\n1-\tThe novelty and contribution is not clear.\n2-\tThe ideas of approximating the preconditioning matrix or factorized approximate inverse have been well studied in the literature, which are not sufficiently cited in the paper, such as Adagrad (Duchi et al. 2011), review in Bottou et al. 2016, etc. \n3-\tDerivation of Eq.(4) seems to be missing. \n4-\tTypo errors such as is can in page 5. \n5-\tA mistaken derivation in A.1 Eq.(20). k should be k+1.\n\nTherefore, I tend to give this paper a Weak Reject score. ", "belong_id": "Skg3104FDS"}, {"uid": "r1x_NdV6FB", "paper_title": "Target-Embedding Autoencoders for Supervised Representation Learning", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work introduces the idea of target embedding autoencoders for supervised prediction, designed to learn intermediate latent representations jointly optimized to be both predictable from features and predictive of targets. This is meant to help with generalization and has certain theoretical guarantees. \n\nIt is an interesting problem setting to consider where  Y is high dimensional instead of X. More examples of this would be useful to provide in the intro. I think this is crucial to understand where this method might be useful. \n\nFigure 1 is super informative and very nice!\n\nSection 2: \nWhy do we expect that this paradigm of autoencoder based regularization generalizes better?\nI like the explicit and honest discussion of prior work in this section. \nOne question is how important is the choice of reconstruction loss function - L2, vs max likelihood gaussian, vs L1, vs cross entropy, etc for performance?\nAnother question: how bad is performance if the learning is done stagewise - first the Y-Z-Y^ representation is learned and then the X->Z predictor is learned. \nIf something is out of distribution, how easy are TEA based learners to finetune?\nOverall the idea seems reasonable - if the targets have some common set of factors, just predict those instead of predicting the full target value which might be harder to get right. Its just a question of whether this holds true in many domains and how well this reconstruction loss generalizes across problems?\n\nSection 3: \nWe havenoted that TEA components can in principle be instantiated by any architecture. Does its benefit extend beyond the commonly-studied domain of static classification? -> not clear what this means? Does this mean this algorithm has been proposed before or is it that it can ALSO work on non static classification tasks? Not clear how to situate this claim\n\nThe theoretical section seems to follow largely from Le et al, but with important distinctions on dimensionalities of various spaces involved. I wonder if the authors can comment on how often Assumption 1 and 2 are actually satisified?\n\nRelated Work: \nIs the main difference between Yu 2014 and this just in the norm based regularization? I dont think so, can this be made more clear. This seems also fairly related to Yeh, is it just a generalization of that paradigm? Or is there more to it? In light of the contribution of Yeh, this seems like slightly more marginal of a contribution? Is the main points of contribution the theoretical analysis and the extended experiments to sequence data rather than static classification?\n\nThe results do seem to show a signficant benefit as compared to FEA or base models. It also seems like this is applicable across multiple disease datasets. Do the authors think that this could be applicable to other domains altogether? Would it be quick to run a comparison on these?\n\nGenerally seems like a well grounded and meaningful contribution with many improvements. Would be curious to see applications to other datasets and also some improvements/clarifications noted above?\n\n", "belong_id": "BygXFkSYDH"}, {"uid": "ryx7d3w75H", "paper_title": "Target-Embedding Autoencoders for Supervised Representation Learning", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "1. Summary: In this paper, the authors proposed a Target-Embedding Autoendocer (TEA) model for supervised representation learning. Different from the traditional feature embedding autoencoder model, TEA tries to learn a compact latent representation that can reconstruct the target vector. Hypothetically, this model should be especially useful when the target vector has a much higher dimension than the feature vector. The authors analyzed the proved some characteristics of this framework and conducted empirical experiments on three datasets to prove its effectiveness.\n2. Overall assessment: The motivation of this paper is well justified. It's easy to follow and fun to read, even for a person who is not an expert in this area, like me. However, there still exist some problems in this paper. It needs more improvement to get published in a competitive conference like ICLR.\n3. Comments:\n3.1 Datasets used in this paper cannot fully prove the effectiveness of this framework. These datasets are all from very similar domains. The dimension of target vectors is comparable to that of feature vectors. In my view, it's necessary to test on more different types of datasets to prove the usefulness of a model, especially if it is a general framework like TEA.\n3.2 Models used in this paper are relatively simple. Demonstrate the performance of TEA on more advanced models and more difficult tasks can deliver more insights to the community.\n3.3 No state-of-the-art models are used in experiments. It's very likely that some existing work has already adopted the idea of target embedding. There also exist much other work on dealing with high dimensional target vector problem. How are the performances of these models? What is the advantage of the proposed framework over these existing work?\n3.4 The source of gain part on page 8 should contain more explanations and analysis. This part is one of the most important parts of this paper. It can provide quite valuable insights to readers. I hope the author can expand it.\n3.5 More details about training and inference are needed. The authors only use a few sentences to describe their three staged training process. I still have some questions left after reading it, such as how do you train the shared parts in TEA? Do you update its parameters in all stages? What the effect of the order of training? What will happen if I change it?", "belong_id": "BygXFkSYDH"}, {"uid": "rkecJZjucH", "paper_title": "Target-Embedding Autoencoders for Supervised Representation Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This is an extremely well-written and well-motivated paper. The idea of target-embedding autoencoders is extremely relevant for problems where the dimension of the label space is as large (or larger) than the dimension of the input features. The experiments are thorough, the theoretical guarantees are extremely well thought of and derived. The applications to modelling the progression of cystic fibrosis and Alzheimer's are extremely useful and timely.  I vote for a strong accept for this paper. \n\nI would like to see some references to the extreme multi-label classification problems (http://manikvarma.org/downloads/XC/XMLRepository.html) and some of the other probabilistic approaches attempted in this domain (please see https://papers.nips.cc/paper/5770-large-scale-bayesian-multi-label-learning-via-topic-based-label-embeddings and the references and citations). ", "belong_id": "BygXFkSYDH"}, {"uid": "H1gkm4ET5H", "paper_title": "Target-Embedding Autoencoders for Supervised Representation Learning", "experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper examines target-embedding autoencoders (TEAs) in theory and practice. TEAs autoencode the output (rather than input) space and find a mapping from the input to the latent representation of the output. The forward pass of the decoder (for the output space) is shared by the input-to-output computation.\n\nTarget-embedding autoencoders (TEAs) have previously been proposed and used in practice (though not necessarily by the 'TEA' name).  The paper's presentation is confusing on this matter, at it claims to be the first to 'motivate and formalize' TEAs; I do not believe it is appropriate to claim such a contribution in light of prior work. [Girdhar et al.] clearly utilizes a target-embedding autoencoder (see [Girdhar et al.] Figure 2). In addition, more recent published work clearly utilizes TEAs (though not named as such) as the centerpiece of their approaches. See, for example:\n\n[A] Adrian V. Dalca, John Guttag, Mert R. Sabuncu. Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation. CVPR, 2018.\n\n[B] Mohammadreza Mostajabi, Michael Maire, Gregory Shakhnarovich. Regularizing Deep Networks by Modeling and Predicting Label Structure. CVPR, 2018.\n\nFigure 2 of [A] and Figure 1 of [B] both clearly depict applying target-embedding autoencoders on semantic image segmentation problems. [B] operates in the same supervised representation-learning setting proposed here. Notably, [B] utilizes staged training -- learning the autoencoder first -- as discussed in Section 2 of the submitted paper, and finds that to be important for achieving a regularization effect.\n\nThe real applications explored by [A] and [B] are perhaps more challenging than the datasets used in experiments here.  The concluding sentence of the paper,'Target-representation learning is potentially applicable to any high-dimensional prediction task, and exploring its utility for specific domain-architectures may be a practical direction for future research' should be changed -- prior work has already successfully utilized TEAs in the specific domain of image segmentation.\n\nGiven that the paper has missed (not cited) highly related published work that applies TEAs in practice, a rewrite of Section 4 is required. In the appendix, Table 6, Table 7 and Section B.1 also need significant updates. The proposed approach is no longer a unique entry in Table 6 or 7 -- e.g. [B] already contributed 'autoencoder component as regularization for learning predictor' (Table 7). Additionally, toy experiments in Section 5 appear less significant a contribution when multiple full-scale systems already employ TEAs.\n\nThis paper's theoretical analysis does appear to set it apart from prior work. However, theorems are developed for an extremely limited context (linear TEAs) and it is unclear whether or how they might extend to practical use cases (i.e. TEAs that are nonlinear, deep neural networks).\n\n---\n\nThe extensive author response and updated paper address many of my original concerns.  I have updated my overall rating.", "belong_id": "BygXFkSYDH"}, {"uid": "HkgAT1el5S", "paper_title": "Deep Interaction Processes for Time-Evolving Graphs", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper considers modeling continuous time-evolving graphs using a temporal point process framework. It introduces a time gate in the LSTM to handle the temporal dependency and uses an attention mechanism to select relevant nodes to learn the underlying dynamics. \n\nOverall, this paper is not easy to understand in detail. Firstly, it is unclear how and why the temporal point process can deal with growing/shrinking graph nodes and changing interactions. Secondly, how does the DIP-UNIT handle the continuous graph changing? What if the graph changes with an uneven speed? Thirdly, how do all the small pieces work together to achieve the goal of the paper? An overview diagram or a toy example would greatly improve the readability of the paper. \n\nBesides, what is the computational cost of the proposed network? How large a graph could be and how fast its changes could be captured? ", "belong_id": "HyxWteSFwS"}, {"uid": "SyeMN893cr", "paper_title": "Deep Interaction Processes for Time-Evolving Graphs", "experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper is concerned with modeling continuous time-evolving graphs, for which it proposes to combine temporal point processes with a recurrent architecture to learn dynamic node representations. In addition, the paper proposes to stack multiple recurrent layers (to obtain node representations over multiple time scales) and use a temporal attention mechanism (to select relevant past interactions).\n\nModeling temporal and dynamic graphs is an important problem with many applications in ML and AI. The focus of the paper, i.e., to develop improved models by combining TPPs and representation learning, is a promising approach to this task and fits well into ICLR. Furthermore, the presented experimental results are promising.\n\nHowever, I'm concerned about different aspects of the current version: The main contributions of the paper are a recurrent (LSTM-based) architecture to model the intensity function of a TPP, stacking multiple LSTM to form a deep architecture, and a temporal attention mechanism. However, none of these contributions on its own are particularly novel. For instance, prior work that introduces similar approaches include \n- Recurrent networks to parameterize intensity functions: (Dai, 2017), (Mei & Eisner, 2017), (Trivedi, 2019), ... \n- Temporal attention: (Trivedi, 2019) \n\nHence, the main novelty seems to lie in the stacked architecture and the particular combination of modules (which is of limited novelty). The experimental results are certainly interesting, but it would be important to provide a more detailed analysis of the model to get insights into the causes for these improvements.\n\nWith regard to the model: The Log-likelihood function in Section 3.6.1 seems to be incorrect as the LL for a TPP would be L = \\sum_{i:t_i \\leq T} \\log\\lambda(t_i) - \\int_0^T \\lambda(s)ds, which is quite different from the equations in the paper. Is the LL in Section 3.6.1 the actual objective that has been optimized?\n\nRegarding the experimental results: All models are trained on the same grid of embedding dimensions, but the proposed method is the only deep model. Hence, its maximum number of parameters can be up to 4x compared to the shallow models. How do the results look if all for models with comparable number of parameters (i.e., can the improvements be explained due to this difference)? It would also be good to get results on commonly used benchmarks (e.g. data used in DyRep or NeuralHawkes) to make the results of the new model comparable to prior experiments and datasets.", "belong_id": "HyxWteSFwS"}, {"uid": "HkxlmTgAqS", "paper_title": "Deep Interaction Processes for Time-Evolving Graphs", "experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper focuses on the problem of modeling interaction processes over dynamically evolving graphs and perform inference tasks such future interaction prediction and interaction classification. Specifically, the paper proposes a temporal point process based formulation to model the interaction dynamics where the conditional intensity function is parameterized by a recurrent network. With an occurrence of any event, the recurrent architecture updates the embeddings of the nodes involved in that event which then affects the intensity function and hence the likelihood of future events. The paper uses intensity based likelihood to train for future interaction prediction task while cross-entropy based loss for classification task. The paper demonstrates the efficacy of the method through experiments across multiple datasets and compare against representative baselines and further provides ablation analysis for the proposed architecture.\n\nThe paper demonstrates markedly improved empirical performance on multiple datasets and also performs the task of interaction classification which is not seen in recent works on evolving graphs, which are plus points. However, there are several concerns with the overall work that makes this paper weaker: (1) The main concern is with the novelty and more importantly the justification/analysis of the contributions proposed approach. (2) Further, while the ablation study provides some insights into architecture, it is not adequate (3) The paper misses comparison with a very important and recently proposed baseline, JODIE [1].\n\n\nMain Comments:\n--------------\n- The paper leverages existing techniques built for learning over evolving graphs and augments it with three modifications: explicit use LSTM with time gate, stacked LSTM approach with fusion (Aggregation) and attention mechanism to select important neighbors to contribute to embedding update. The use of LSTM with time gate and fusion mechanism is very incremental contribution.  The attention mechanism proposed here is novel compared to existing works. However, there is very little justification or analysis provide or either of the contributions. This is big drawback of this contribution.\n \n- For instance, the authors mention that stacked LSTM is used to capture multiple resolution. Can they provide some analysis or empirical demonstration that this actually happens? Also, the authors mention they use K in range of {1,2,3,4} but do not provide details what was useful for each dataset and how is it useful. How does the scaling parameter and alpha affect the performance and what are their roles? Also, what does superscript 'task' signify?\nSimilarly, they propose coattention mechanism with adaptive gate functions but does not provide any analysis of why they are useful and what characteristics they capture in the data that allows it to select most relevant neighbors. Is the attention mechanism temporally dependent?\n\n- The authors perform ablation studies by switching off each component as a whole but considering the way this architecture is built, this is not a very useful exercise except knowing that each component contributes to the performance. A more detailed analysis and ablation is required. For instance, can the authors show performance with different K and  how it deteriorates/improves with it? Also, for stacked LSTM case, the authors show what happens when you use last layer, but what happens if the authors use only one layer (I guess this is K=1?) or don't use residual connections? When the time gate is switched off, does the authors also remove deltas from intensity function? what happens in this scenario? How does subgraph depth affect the quality of performance? What happens if authors don't sue adaptive gate functions?\n\n- Figure 2 shows an example of bipartite graph, however, it seems datasets in experiments does have non-bipartite case? Is this true or the method only works for bipartite case?\n\n- The use of proposed Algorithm 2 is not well justified. Why does the author need coloring and hashing mechanism instead of simpler BFS/randomwalk routine to collect previous interactions? Also, is this subgraph created for each event or it is computed offline during training? Further, the subgraph used for selection mechanism same as subgraph used for backtracking in LSTM? \n\n- Further, is it true that the training is done in order of ColorGraphSeq or is it done in order of dataset? How does the authors capture dependencies across dataset in later case?\n\n- Do you also update cell states with selection mechanism? The DIP-UNIT equation in selection section does not show that update. Also, are the embeddings updated only during train or also during validation/evaluation?\n\n- The authors only present the results as-is without any insights on the performance of DIP model vs others and why they are able to demonstrate good performance. It is highly desired that authors add discussion section for each set of results to provide such information\n\n- The authors include support for new nodes for interaction classification task but remove them for interaction prediction task which is strange. Is there a specific reason for this? What is the effect on the performance if new nodes are allowed in test? Further, why is interaction classification not compared with temporal baselines? All baselines produce embeddings and the authors mention that classification for this paper is independent of marker history. While the temporal baselines do not train for the task, the authors can train a second stage classifier with learned embeddings to perform classification\n\n- The authors do not compare with recently proposed JODIE [1] which is a big miss. The comparison is required as it also models interaction processes in a  novel way by actually predicting the next embedding directly instead of modeling the intensity. An empirical comparison and discussion of this method is required to compare with various state-of-art methods.\n\nMinor:\n-------\n\n- The authors need to use better and consistent notations. Also, as the overall approach uses similar flow as previous papers such as DeepCoevolve, it is recommended that the authors make the presentation simpler to position it clearly with existing works. On page 3, section 3.2 both bold-face and normal letters are used as vectors. Is $\\hat{x}_{u(t)}$ a vector?\n\n- Please provide numbers to equations for better referencing\n\n[1] Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks, Kumar et. al. KDD 2019", "belong_id": "HyxWteSFwS"}, {"uid": "Hyg_DV7DFS", "paper_title": "New Loss Functions for Fast Maximum Inner Product Search", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe authors propose a new loss function for solving large-scale inner product search that rely on quantization, based on the intuition that all pairs of (query, database vector) are not equally important for a given query. In particular, the authors weight the reconstruction error so that the pairs with a higher scalar product are more precisely quantized (as they lie among the most plausible candidates).\n\nStrengths of the paper:\n- The paper is well written and easy to follow. In particular, the intuition of the method is well explained in Figure 1 and the setup in Section 3 is well formulated.\n- The proposed method works with a variety of quantization approaches, such as binary, simple PQ or LSQ (even if the authors aren't able to report results for this last method due to technical issues as explained in Appendix 7.7)\n\nWeaknesses of the paper:\n- The related work could be more detailed, see for example: 'Spreading vectors for Similarity Search', Sablayrolles et al. ; 'Pairwise Quantization',  Babenko et al ; 'Unsupervised Neural Quantization for Compressed-Domain Similarity Search', Morozov et al.\n\nJustification of rating:\nThe paper proposes a new loss function that weights the scalar products differently according to their importance than can be applied to a wide range of existing quantization methods. However, the strength of experimental results (in particular the fact that LSQ or other cited references above are missing) remains unclear. ", "belong_id": "BkeaxAEKvB"}, {"uid": "S1xgHrkiFB", "paper_title": "New Loss Functions for Fast Maximum Inner Product Search", "experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes a novel quantization algorithm for maximum inner product search. Instead of imposing equal weights on all possible queries, the proposed framework considers giving larger weights on queries having larger inner product values. The authors shows that this derives a loss function having different weights on parallel and orthogonal component of residual of quantization. The paper shows the performance of the algorithm mainly by using the Glove1.2M dataset.\n\nThe basic idea and the weighting approach would be reasonable. Empirical evaluation might be slightly weak.\n\nIn practice, how can the hyper-parameter (T and b) be determined? T = 0.2 shows the best performance on Fig. 2 (b), but its seems sensitive, and generality of this setting is not clear.\n\nIn (7), only <q,x> larger than T is penalized, but larger <q,\\tidel{x}> seems important as well. If the encoded \\tilde{x} has a large inner product value wrongly, it would deteriorate performance.\n\nThe tightness of the the upper bound of the inequality in the end of page 4 would be unclear. Replacing norm with its possible maximum value seemingly has large effect.\n\nMinor comment:\nFigure 1 is not referred from the main text.\n\nIn Fig. 2 (b), reconstruction and proposed have the save value at T = 0.1. Is this just a coincident?", "belong_id": "BkeaxAEKvB"}, {"uid": "BygEF9_ycS", "paper_title": "New Loss Functions for Fast Maximum Inner Product Search", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes new loss functions for quantization when the task of interest is maximum inner product search (MIPS).\nThe paper is well written with clear descriptions, fairly comprehensive analysis and empirical exploration, and good results, and in general I agree that learning quantization so as to minimize quantization related errors on task at hand is a good strategy.  \nSpecific comments and suggestions for strengthening the paper are:\na) The proposed loss function in (2) includes a weight function that serves as a proxy for the task objective of giving more emphasis to quantization errors on samples with larger inner product.  Instead, why not use the true task objective which for the MIPS task is stated in the Introduction section?  If this was considered please comment on reasons for not including / discussing this in the paper, otherwise perhaps thisll be good to discuss.\nb) Did the authors consider using a task dependent training data set which will capture both q and x distributions and potentially lead to even further improved quantization?  This has the disadvantage of making quantization dependent on query distribution, but in cases where such data is available it will be very valuable to know if incorporating data distributions in quantization process helps performance and to what extent.\nc) It will also be valuable to consider the closely related task of cosine distance based retrieval and comment on how that impacts the modifications of loss functions.\nd) The idea of learning quantization under objective of interest using observed data distribution has been studied earlier (e.g. see Marcheret et al., Optimal quantization and bit allocation for compressing large discriminative feature space transforms, ASRU 2009), perhaps worth citing as related work.\n", "belong_id": "BkeaxAEKvB"}, {"uid": "HylOroWRYB", "paper_title": "Learning to Learn by Zeroth-Order Oracle", "experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a zeroth-order optimization framework that employs an RNN to modulate the sampling used to estimate gradients and a second RNN that models the parameter update. More specifically, query directions are sampled from a Gaussian distribution with a diagonal covariance whose evolution is determined by an RNN (QueryRNN). The resulting gradient estimates are used by an RNN (UpdateRNN) that learns the parameter update rule. This framework has the stated advantage that, unlike existing work in ZO optimization, it does not rely upon hand-designed strategies to reduce the variance common to ZO gradient estimators. The paper evaluates the proposed framework on MNIST and CFAR tasks, as well as a synthetic binary classification task. The results demonstrate faster convergence compared to baseline zeroth-order optimization algorithms, while ablations indicate the contributions of the different model components.\n\nThe primary contribution of the proposed method is the inclusion of a second network that learns to adapt the covariance of the Gaussian from which  query directions are sampled. The use of an RNN to model the parameter update rule is borrowed from previous work. The ablation studies show that the adaptive sampling strategy noticeably improves convergence as does the inclusion of the RNN update (the contribution of UpdateRNN is more significant in one ablation study, while the contribution of QueryRNN is greater in the other). \n\nGiven that a stated advantage of QueryRNN is reducing variance, it would be beneficial to compare against baselines such as Liu et al., 2018a,b and/or Guo et al., 2016 which similarly seek to reduce variance.\n\nThe paper includes a large number of typos (e.g., CIFAR-->CIAFR) and grammatical errors, but is otherwise clear.\n\nADDITIONAL COMMENTS/QUESTIONS\n\n* The related work discussion would benefit from a discussion of how Liu et al. 2018 and Guo et al. 2016 reduce variance\n\n* The computational complexity of the proposed method as compared to the baselines is unclear as is the scalability with dimensionality. At various points, the paper comments that other methods scale poorly with the dimensionality of the query space, which is true of the proposed method unless the operations are parallelized. Is this not possible with the baseline methods?\n\n* The paper makes hand wavy claims to the fact that modulating the diagonal covariance matrix allows the method to focus on certain subspaces. It would be helpful to make these claims more formal, particularly in light of the fact that the mean does not change.\n\n* The method relies upon a bit of a hack that samples from a standard Gaussian at random times. How important is this to performance? How sensitive is convergence to the frequency with which standard Gaussian sampling is used?\n\n* The discussion of Eqn. 5 as it relates to Eqn. 1 is unclear.\n\n\nUPDATE AFTER AUTHOR RESPONSE\n\nI appreciate the authors' thorough response, which resolved my primary questions/concerns, including comparisons to existing variance reduction methods (which should be incorporated into the main paper).", "belong_id": "ryxz8CVYDH"}, {"uid": "BJgs4nvCFB", "paper_title": "Learning to Learn by Zeroth-Order Oracle", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposed a novel learning to learn framework based on zeroth-order optimization. Specifically, the framework consists of three parts: (1) UpdateRNN for learning the parameter update rules (2) Guided gradient estimation and search (3) QueryRNN that dynamically adapts the Gaussian sampling rules for covariance estimation in (2). \n\nExperimental results on generating adversarial examples from black-box machine learning models as well as a binary classification problem demonstrate improved performance over several existing baselines, such as better query efficiency or faster empirical convergence in the loss function. An ablation study is also conducted to study the effect of each component in the proposed framework. \n\nOverall, this paper is pleasant to read and well-motivated. The applications are of practical importance. Given that the empirical results suggest faster convergence than the compared methods, it will be great if the authors can also discuss how to prove the improved convergence in theory.\n\n*** Post-rebuttal comments\nI thank the authors for the clarification.\n***", "belong_id": "ryxz8CVYDH"}, {"uid": "rylVGWoAFB", "paper_title": "Learning to Learn by Zeroth-Order Oracle", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a learning to learn (L2L) framework for zeroth-order (ZO) optimization, and demonstrated its effectiveness on black-box adversarial example generation.  The approach is novel since L2L provides a new perspective to zeroth-order optimization. \n\nHowever, I have some concerns about the current version. \n\n1) The knowledge that one should use to train UpdateRNN and  QueryRNN is not clear. A clear presentation is required \n\n2) Please clarify the optimization variables in (4). In general, the problem is not clearly defined. \n\n3) Eq. 5 is a query-expensive gradient estimate. Will it make training extremely expensive?\n\n4) The computation and query complexity are unclear during training and testing.\n\n5) Pros and cons of L2L? It seems that training a L2L network is not easy. Does its advantage exist only when inference? A better discussion should be made.\n\n########## post-feedback #######\nThanks for the response. In the pros of L2L, the authors mentioned 'The learned optimizer is trained on a small subset of optimization problems and apply in a wide range of problems in similar classes.' In the setting of attack generation, does it mean that there exists an attack transferbility from a small group of training images to a large group of testing images? Is the transferbility a requirement for applying L2L in design of attacks. Please try to make these points clearer in the revised version. I keep my decision 'weak accept'. ", "belong_id": "ryxz8CVYDH"}, {"uid": "H1g5dPCpFr", "paper_title": "Neural Clustering Processes", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper introduces a novel deep learning architecture for efficient amortized Bayesian inference over mixture models. Unlike previous approaches to amortized clustering, the proposed method allows us to treat local discrete labels of data points and infer the unbounded number of mixture components, making it more flexible as in the case of Bayesian nonparametrics. It is shown that the resulting algorithm can be parallelized and applied to both conjugate and non-conjugate models. The authors also suggest an extension to models of random communities and a novel approach to neural spike sorting for high-density multielectrode arrays based on the proposed method.\n\nStrengths:\nThe paper is generally well written and the relationship to previous works is well described. Empirical results seem quite convincing, for example, the clustering results presented in Fig. 2 and Fig. 3 clearly show not only the inferred number of clusters, but also the posterior probability which indicates that reasonable samples are assigned higher probability.\n\nWeaknesses:\n- Overall, the idea looks very original and promising, but I find some technical details are not easy to understand under the current form, especially for non-experts in this domain. I would recommend the authors to elaborate a bit more on the proposed architecture and the variable-input soft-max function in Sect. 2.1.\n- On page 8, the authors mention that the NCP is much more efficient compared with MCMC, for example, in the Gaussian 2D example. However, regarding the DPMM clustering model, it is known that MCMC methods are generally slower compared with variational inference, which is computationally faster. I think it would be interesting to add a discussion or comparison with variational inference in terms of computational efficiency.\n- If I understand correctly, the NCP is essentially based on a sequential sampling procedure. The authors claim that the proposed method is easily parallelized using a GPU, but there does not seem to be sufficient details on the GPU-parallelization of sequential sampling.\n\nMinor comments:\nThe size of some figures appears too small, for example Fig. 6 and Fig. 10, which may hinder readability.", "belong_id": "ryxF80NYwS"}, {"uid": "Hyemu9Byqr", "paper_title": "Neural Clustering Processes", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a neural network based clustering process where the number of clusters is not known a-priori.  Proposed approach requires conjecturing a generative process (where number of clusters/classes is a random variable) and the model learns to uncover the posterior distribution over clusters given the observed samples.\nOverall I think it is a valuable contribution, well written paper with good results.  \nSpecific comments:\na) Even though the model allows for variable number of clusters, I feel there may be a strong dependence between the number of clusters the model can hypothesize and the number of clusters in the training data.  It will be useful to give further insights into this.  For instance, if an MNIST model is trained with only digits 0-5 training data, how well would it perform in detecting all 10 clusters at test time?  Understanding models biases based on training data is one area I feel is important and the paper could add to.\nb) The neural clustering process could potentially be viewed as a transductive inference model for classification of test data.  Typically at test time classification is done for each test sample independently, and the clustering process allows one to bring in other similar test samples to help with classification.  Have the authors considered this and have any comments on potential value / feasibility of this?\nc) In the examples presented in Section 2.3, please clarify how training & testing was done.  Specifically what training data was used (all of MNIST training data?), and the test time clustering was done on a subset of MNIST test data?\nd) Use of q for a neural-network and q_\\theta for posterior distribution is a little confusing, will be better to have different notation for these.\n", "belong_id": "ryxF80NYwS"}, {"uid": "SJxZrE8BcS", "paper_title": "Neural Clustering Processes", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nIn this paper, the authors consider the neural amortized inference for clustering processes, in which the number of cluster can be automatically adapted based on the observed samples. The proposed algorithm largely follows the standard variational auto-encoder. The major contribution of the paper is the design of the posterior parametrization so that the posterior satisfies the permutation invariant within a cluster, between clusters, and unassigned data, based on the DeepSet method. The model can be incorporated into random communities models. Finally, the authors apply the algorithm for neural spike sorting problem. \n\n\nThe paper is well-organized and easy to follow. However, there are two issues should be addressed:\n\n1, The novelty of the proposed algorithm might not enough. The two major components in this paper, i.e., VAE and DeepSet, are all carefully investigated before. This paper applies the DeepSet parameterization in the VAE framework.\n\n2, The details of the amortized inference training is not clearly explained. It is well-known that gradient through the discrete random variable is quite difficult. How the gradient for the parameter of the proposed model is calculated should be carefully discussed. The REINFORCE gradient in this model, whose support of c can be as large as the number of samples, can be quite huge. \n\n3, In the empirical evaluation, I was curious why the mean-field and MCMC have not been considered in the spike sorting problem.\n\nI am expecting the authors can address my concerns during rebuttal. \n\n=======================================================================\n\nThanks for the responses to clarify my concerns. \n\nThe learning procedure and experiments are clear now.  Indeed, as a purely variational inference paper,  the discrete variable problem is absent, since the model is always *fixed* and not updated. \n\nThe major contribution of the paper becomes the design of the posterior parametrization by DeepSets, which I think still not enough. \n\nI will keep my score.", "belong_id": "ryxF80NYwS"}, {"uid": "BklZP2GiFB", "paper_title": "Barcodes as summary of objective functions' topology", "experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This work is focused on topological characterization of target surfaces of optimization objectives (i.e. loss functions) by computing so called barcodes, which are lists of pairs of local minima and their connected saddle points. The authors claim that the barcodes constitute a representation of target objectives that is invariant under homeomorphisms of input to the objectives. The authors present an algorithm for computing the barcodes from graph-based representation of a surface, and present barcodes computed on toy examples in numerical analysis. \n\nIn my opinion, the main contribution of the work i.e. creation of barcodes is based on a rather trivial idea. The concept of characterizing optimization objectives through pairs of local minima and one-index saddle points is straightforward given that one can (thoroughly if not exhaustively) compute them in a computationally feasible manner; this is however hardy the case in any realistic scenario. I therefore struggle to see how the idea can be practically significant. Maybe the authors can put more emphasis on the theoretical aspect of their work, which is about the invariance nature of barcodes. They can for instance demonstrate how one can exploit the invariance property of barcodes for parameter optimization. \n\nThe authors can consider application of their work to hyper-parameter optimization, which is usually low-dimensional and one can also compare with other approaches such as Gaussian processes or other Bayesian methodologies. \n\nIn numerical experiments, for the toy task solved using neural network I don't find it very surprising that the barcodes descend lower as the capacity of the network is increased. Can the authors further clarify why it is a significant finding for them?", "belong_id": "S1gwC1StwS"}, {"uid": "BkeqaMZJ5S", "paper_title": "Barcodes as summary of objective functions' topology", "experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper aims to study the topology of loss surfaces of neural networks using tools from algebraic topology. From what I understood, the idea is to effectively (1) take a grid over the parameters of a function (say a parameters of a neural net), (2) evaluate the function at those points, (3) compute sub-levelset persistent homology and (4) study the resulting barcode (for 0/1-dim features) (i.e., the mentioned 'canonical form' invariants). Some experiments are presented on extremely simple toy data.\n\nOverall, the paper is very hard to read, as different concepts and terminology appear all over the place without a precise definition (see comments below). Given the problems in the writing of the paper, my assessment is that this idea boils down to computing persistent homology of the sub-levelset filtration of the loss surface sampled at fixed parameter realizations. I do not think that this will be feasible to do, even for small-scale real-world neural networks, simply due to the difficulty of finding a suitable grid, let alone the vast number of function evaluations involved.\n\nThe paper is also unclear in many parts. A selection is listed below:\n\n(1) What do you mean by gradient flow? One can define a gradient flow in a linear space X and for a function F: X->R, e.g., as  a smooth curve R->X, such that x'(t) = -\\nabla F(x(t)); is that what is meant? \n\n(2) What do you mean by 'TDA package'? There are many TDA packages these days (maybe the CRAN TDA package?)\n\n(3) 'It was tested in dimensions up to 16 ...' What is meant by dimension here? The dimensionality of the parameter space?\n\n(4) The author's talk about the 'minima's barcode' - I have no idea what is meant by that either; the barcode is the result of sub-levelset persistent homology of a function -> it's not associated to a minima.\n\n(5) Is Theorem 2.3. not just a restatement of a theorem from Barannikov '94? At least the proof in the appendix seems to be .\n\n(6) Right before Theorem 2.3., what does the notation F_sC_* mean? This needs to be introduced somewhere.\n\nFrom my perspective, the whole story revolves around how to compute persistence barcodes from the sub-levelset filtration of the loss surface, obtained from function values taken on a grid over the parameters. The paper devotes quite some time to the introduction of these concepts, but not in a very clear or understandable manner. The experiments are effectively done on toy data, which is fine, but the paper stops at that point. I do not buy the argument that 'it is possible to apply it [the method] to large-scale modern neural networks'. Without a clear strategy to extend this, or at least some preliminary 'larger'-scale results, the paper does not meet the ICLR threshold. The more theoretical part is too convoluted and, from my perspective, just a restatement of earlier results.\n\n\n\n\n\n\n\n\n\n\n", "belong_id": "S1gwC1StwS"}, {"uid": "SkxeCjmJqS", "paper_title": "Barcodes as summary of objective functions' topology", "experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces the notion of barcodes as a topological invariant of loss surfaces that encodes the 'depth' of local minima by associating to each minimum the lowest index-one saddle. An algorithm is presented for the computation of barcodes, and some small-scale experiments are conducted. For very small neural networks, the barcodes are found to live at small loss values, and the authors argue that this suggests it may be hard to get stuck in a suboptimal local minimum.\n\nI believe the concept of barcodes will be new to most members of the ICLR community (at least it was to me), and I appreciate the authors' effort to convey the ideas through multiple definitions in Section 2. I wasn't able to fully appreciate the importance of Definition 3, and Definitions 1 and 2 were tough to digest owing to imprecise language, but I think I got the main point. I was also unable to fully comprehend the definitions of 'birth' and 'death' in this context. I'd strongly encourage the authors to improve the readability of this section so that non-experts can follow the story.\n\nIt seems like the main contribution is a new algorithm for computing barcodes of minima. I am unfamiliar with prior work in this direction, and I was also unable from the paper to infer what the main improvements were relative to the existing algorithms. I'd encourage the authors to state their explicit algorithmic improvements, and to demonstrate empirically that the new algorithm outperforms the prior ones in the expected ways.\n\nThe main experiments are on extremely tiny neural networks, presumably owing to computational restrictions. The authors state that 'it is possible to apply it to large-scale modern neural networks', but it's not clear to me how that would work or what additional algorithmic improvements (if any) would need to be made in order to do so. I don't think that the results on tiny neural networks have much relevance to practice, so I think the empirical data presented in this paper will have very limited impact. If there were results for practical models, it would be a different story. So I'd encourage the authors to devote additional effort to scaling up the method for use on practical neural network architectures.\n\nOverall, I think there may be some really nice ideas in this paper that could help shape our understanding of neural network loss surfaces, but the current paper does not explore those ideas fully and does not convey them in a sufficiently clear manner. I hope to see an improved version of this paper at a future conference, but I cannot recommend acceptance of this version to ICLR.", "belong_id": "S1gwC1StwS"}, {"uid": "BJe7sjL9tB", "paper_title": "PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS", "experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an approach to incrementally learn hierarchical representations using a variational autoencoder (VAE). This is shown to be useful qualitatively and quantitatively in terms of disentanglement in the representations.\n\nTo learn the hierarchy, the authors use a ladder architecture based on variational ladder autoencoder (VLAE) but incrementally activate the lateral connections across the layers at varying depth of the encoder and the decoder. A vanilla VAE is first trained. Followed by adding stochastic later connections and then retraining the updated architecture. This combined with beta-VAE inspired upweighting of the KL term leads to learning a hierarchy of representations. Each level of the hierarchy, the representations are disentangled. \n\nInspired by progressive GANs, the authors employ ````'fade-out' when traversing the hierarchy. \n\nThe authors also introduce a new metric to capture the one-to-one mapping of the ground truth factors to the latent dimensions.\n\nAblation studies by varying/removing fadeout compared to incremental learning will be useful. Can fade-out (different weighting of each level) be added directly to VLAE without incremental learning? \n\nOverall the paper is well motivated and easy to read. The results look impressive and the learned hierarchy and latent traversals are convincing. A more thorough comparison with VLAE will make the paper stronger.\n\n", "belong_id": "SJxpsxrYPS"}, {"uid": "ryx1b4oaYH", "paper_title": "PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS", "experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduce pro-VLAE, an extension to VAE that promotes disentangled representation learning in a hierarchical fashion.\nEncoder and decoder are made of multiple layers and latent variables are not only present in the bottleneck but also between intermediate layers; in such a way, it is possible to encode information at different scales, hence the hierarchical representation. Latent variables can be learned in an incremental way, by making them visible to the whole model progressively, so that as more latent variables become available, they encode lesser and lesser abstract factors.\n\nExperiments are carried out on two benchmarks for disentanglement with annotations and pro-VLAE is compared to other methods in the state of the art.\nHere, the authors introduce an extension of the Mutual Information Gap (MIG) metric, namely MIG-sup: it penalizes when multiple generative factors are encoded in the same latent variable. Qualitative results are also shown for 2 non-annotated datasets.\n\nPROS\n- The idea is fresh, well explained and experiments are sufficiently thorough. The novelty introduced is enough, provided that not much literature has explored progressive representation learning in the context of disentanglement.\n- Results suggest that this is a promising direction for disentangling representations as pointed out by the authors in the conclusions.\n- We appreciated the smart solutions for what concerns the implementation and training stabilization.\n\nCOMMENTS/IMPROVEMENTS\nTo improve the quality of the paper, consider the following comments:\n\n- For the sake of completeness, experiments on Information flow should be also quantitative: it would be interesting to see how the information is captured by the latent variables on average on multiple runs, possibly trying different numbers of latent variables z_i.\n- In sec 3.1 'z from different abstraction' is too vague and should be better formalized.\n- In sec 2: 'the presented progressive learning strategy provides an entirely different approach to improve disentangling that is ORTHOGONAL to these existing methods and a possibility to augment them in the future.': you should change to 'different'.", "belong_id": "SJxpsxrYPS"}, {"uid": "Bkxu1H8y9B", "paper_title": "PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposed a method for training Variational Ladder Autoencoder (VLAE) using a progressive learning strategy. In comparison to the generative model using a progressive learning strategy, the proposed method focuses not only on the image generation but also on extracting and disentangling hierarchical representation.\n\nOverall, I think the purpose of this paper should be written clearly. It is not clear whether the purpose is learning the disentangled representation or the hierarchical representation. In my opinion, I think the focus of the proposed method lies in the hierarchical representation through progressive learning, but the experiments are involved more with disentanglement. Furthermore, I believe the authors need to explain the relationship between hierarchical representation and disentangled representation. In particular, it is not clear why learning hierarchical representation is helpful for disentangled representations.\n\nThe qualitative experiments are not convincing since the proposed model looks worse in both the reconstruction and hierarchical disentanglement for MNIST dataset than the base model VLAE, as shown in Figure 5 in [1]. Regarding the metric used in the experiments, the authors mention that the proposed disentanglement metric MIG-sup is what they first developed for one-to-one property, but it seems that it was already proposed in [2]. In addition, the proposed metric requires ground truth for the generative factors, so its usage is limited and not practical.\n\nI think this work is similar to [3] in that both learn disentangled representations by progressively increasing the capacity of the model. I think the authors need to discuss about this work.\n\nAblation studies should be presented to verify the individual effects of the progressive learning method and implementation strategies on performance, respectively.\n\nIn Figures 2 and 3, the performance gap in the reconstruction error of the proposed method is greater than the base model when beta changes from 20 to 30. Therefore, it is necessary to show if it is robust against the hyperparameter beta. \n\nThere is no definition of v_k in Equation (12), so it is difficult to understand the proposed metric clearly.\n\nIn summary, I do not think the paper is ready for publication. \n\n[1] Learning Hierarchical Features from Generative Models, Zhao et al., ICML 2017\n[2] A Framework for the Quantitative Evaluation of Disentangled Representations, Eastwood et al., ICLR 2018\n[3] Understanding disentangling in beta-VAE, Burgess et al., NIPS 2017 Workshop on Learning Disentangled Representations\n\n\n-------------------------------------\nAfter rebuttal:\n\nThanks for the revision of the paper and the additional experiments.\n\nThe authors' comments and further experiments address most of my concerns. In particular, new experiments show that pro-VLAE performs quantitatively and qualitatively better than VLAE. Also, Figure 10 and the result of the information flow experiment using MNIST show that the first layer learns the intended representations properly.\n\nI appreciate the authors efforts put into the rebuttal, and the results of additional experiments are reasonably good. Therefore, I increase my final score to 6: Weak Accept.", "belong_id": "SJxpsxrYPS"}, {"uid": "BkermuA2tS", "paper_title": "How noise affects the Hessian spectrum in overparameterized neural networks", "experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the problem of how noise coming from the gradient-based update affects the geometry of the hessian matrix when training a neural network. \n\nThe paper makes an interesting claim that around a local minimum, if the noise in SGD is aligned with the hessian matrix of the network, then doing SGD update is implicitly minimizing the trace of the hessian matrix, biasing the current point towards a wide valley. \n\nThe paper also makes a very interesting observation that isotropic noise will decrease the determinate of the hessian while the SGD noise will decrease the trace of the hessian matrix. \n\nI find the theorem in the paper quite interesting, especially Lemma 1 stating that the loss function can be locally approximated by a quadratic function whose variables are the non-degenerate directions and the coefficients only depends on the degenerate directions. This Lemma appears to be quite novel to me. With this Lemma, it is then easy to see that as long as the noise is aligned with the non-degenerate directions, then the trace of the Hessian is decreasing in expectation at every step. \n\nThe main question I have about this paper is the clarity: First of all, the main concept 'noise covariance matrix aligned with the hessian' is not mathematically defined anywhere. I can intuitively understand the term from the explanation in section 2.1.2 but I can not formally justify the correctness. Second, where is the timescale separation used? Is it for justifying the assumption of the local stationary point approximation or for Lemma 1 or something else? At the current level of writing in the paper, I can not formally verity the theorems. \n\nMissing citation: 'an alternative view, when does SGD escape local minimal.'\n\n\n\nAfter Rebuttal: I have read the authors' responses and acknowledge the sensibility of the statement.\n\n\n\n", "belong_id": "Hklcm0VYDS"}, {"uid": "H1lum-xpYr", "paper_title": "How noise affects the Hessian spectrum in overparameterized neural networks", "experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proves that under certain conditions, SGD on average decreases the\ntrace of the Hessian of the loss. The paper is overall clear and the topic addressed in the paper is relevant in machine learning but I dont think this paper is ready for publication. There are numerous assumptions that are made in the paper, but not properly stated or backed up. In my view, the experimental results are also not sufficient to compensate for the shortcomings of the theoretical part.\n\nLemma 1\nThis is a corollary of Morse lemma, which is also used in Dauphin et al: https://arxiv.org/pdf/1406.2572.pdf (see Equation 1). I dont see why you would need to re-derive it in your paper and most importantly this should be clearly stated in your paper.\n\nPage 2: existence stationary state?\nWhat are the conditions for the existence of a stationary state? Is bounded noise sufficient?\n\nEquation 4 and local approximation\nRegarding the standard decomposition of the Hessian in eq 4 (sometimes referred to as Gauss-Newton decomposition), the discussion is not precise. The authors claim Empirically, negative eigenvalues are exponentially small in magnitude compared to major positive ones when a model is close to a minimum\nBut clearly from the formula itself, one needs to differentiate between minima with low and high function values. For local minima that are high in the energy landscape, the second term might have a large function value. If of course all **depends on the size of the approximation region**. This is extremely important and it is not properly characterized in the paper. The authors simply claim that the loss can be locally approximated while I think these assumptions should be clearly stated. Note that similar types of analyses are usually done in dynamical systems where the behavior of a system is linearized around a critical point. In some cases, one can however characterize the size of a basin of attraction, see e.g. the book Nonlinear Systems (3rd Edition): Hassan K. Khalil.\n\nAssumption on timescale separation\nThis section makes numerous claims that are not properly backed up.\n1) This assumption is because there is a timescale separation between\nthe dynamics of \\theta_bar, which relax quickly and the dynamics of \\theta_hat, which evolve much more slowly as the minimal valley is traversed.\nAre you claiming the absolute value of the eigenvalue of the non-degenerate space are much smaller than the degenerate space? Why would that be so?\n2) Ornstein-Uhlenbeck: OU processes require the noise to be Brownian motion. This assumption needs to be clearly stated. Note that there is actually evidence that the noise of SGD is heavy-tail:\nSimsekli, Umut, Levent Sagun, and Mert Gurbuzbalaban. 'A tail-index analysis of stochastic gradient noise in deep neural networks.' arXiv preprint arXiv:1901.06053 (2019).\n\nTake away\nI am unsure what the added value of this paper is. Second-order methods can already be shown to decrease the maximum eigenvalue, see e.g. convergence results derived in Cubic regularization of Newton method and its global performance by Nesterov and Polyak. These methods have also been analyzed in stochastic settings where similar convergence results hold as well. What particular insight do we gain from the results of Theorem 2? The authors claim this could potentially improve generalization but this is not justified and no reference is cited.\n\nThis indicates that the trace itself is not sufficient to describe generalization (Neyshabur et al., 2017).\nI do not see what aspect of Neyshabur et al. justifies your claim, please explain.\n\nExperiments\nAll the experiments performed in the paper are on very small models. Given the rather strong assumptions made in the paper, I feel that the paper should provide stronger empirical evidence to back up their claims.\n", "belong_id": "Hklcm0VYDS"}, {"uid": "BJxOztS89r", "paper_title": "How noise affects the Hessian spectrum in overparameterized neural networks", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the behavior of SGD after it has reached a steady state and settled in a minimal valley. The authors show that even if SGD has reached a minimal valley, the noisy updates provided by a minibatch of samples result in the trace of the Hessians reducing with further SGD iterations. The authors also show that by changing the type of noise that is added to GD, one can get different functions of the Hessian to reduce with SGD iterations. The theoretical conclusions are then verified empirically with synthetic examples and real deep learning examples\n\nIn my opinion this is an interesting paper and research direction that helps us understand how SGD biases solutions towards 'flatter' minima as measured by the trace norm of the Hessian of the loss function. I have a few concerns though:\n\n1. Is the steady state assumption valid for neural network training? Especially when training on exponential losses (like cross entropy) which drives the parameters towards large norm solutions? This seems to be an important yet unsupported assumption in the analysis.\n\n2. Does the Hessian-noise covariance alignment exist for squared loss functions as well? Is this specific to log-likelihood models?\n\n3. In the experiments to delineate how different types of noise can result in regularization of different quantities, is there a reason only a synthetic example is used? Can this be replicated for deep networks, or are the two quantities (trace vs determinant) closely related? It would be nice to see how this behavior extends to deep networks.\n\nAdditional questions/comments:\n1. While overparameterization seems to be important to the analysis (the parameters move in the degenerate subspace but not in the non-degenerate one), is there anyway one can amend this framework to analyze different levels of overparameterization? Is there any difference in the rates of decrease in the trace norms for highly overparameterized models vs lightly overparameterized ones?\n\n2. This paper talks about how SGD has a bias towards flatter minima. Is there a reason to prefer flatter solutions over sharper ones to get better generalization? Can one prove this connection?\n\n3. In the Densenet experiments in Figure 4a, it seems as though this relationship between flatness and generalization might not hold? There are points along the optimization trajectory where the validation loss seems to be better than at the last few points along the optimization path. However at the former set of points the trace norm of the hessian is larger than at the latter points. Is this from a stray experiment, or are the plots averaged over a number of different runs? Please add these details as well as other details such as learning rates, criteria to decide when steady state was reached, whether or not batch norm was used, etc.\n\n4. The Fluctuation-Dissipation Relations need to be explained more clearly. For people unfamiliar with statistical physics (and Yaida 2019) the notation and formulation is not immediately clear and that makes the paper harder to read.\n\nOverall I believe this paper explains an important phenomenon. I am willing to update my score if my concerns are addressed/if there is any misunderstanding.", "belong_id": "Hklcm0VYDS"}, {"uid": "H1eaaz-3dr", "paper_title": "Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search", "experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces a novel approach to parallelizing Monte Carlo Tree Search \nwhich achieves speedups roughly linear in the number of parallel workers while \navoiding significant loss in performance. The key idea to the\napproach is to keep additional statistics about the number of \non-going simulations from each of the nodes in the tree. The approach is \nevaluated in terms of speed and performance on the Atari benchmark and in a \nuser pass-rate prediction task in a mobile game.\n\nI recommend that this paper be accepted. The approach is well motivated and clearly \nexplained, and is supported by the experimental results. The experiments are reasonably thorough and \ndemonstrate the claims made in the paper. The paper itself is very well-written, and all-around \nfelt very polished. Overall I am enthusiastic about the paper and have only a few concerns, detailed below.\n\n- I suggest doing more runs of the Atari experiment. Three runs of the experiment does not \nseem large enough to make valid claims about statistical significance. This is especially \nconcerning because claims of statistical significance are made via t-testing, which assumes \nthat the data is normally distributed. Three runs is simply too few to be making conclusions \nabout statistical significance using t-testing. I think that this is a fair request to make and \ncould reasonably be done before the camera-ready deadline, if the paper is accepted.\n\n- The experiments in Atari compare against a model-free Reinforcement Learning baseline, PPO. \nWas there a set clock time that all methods had to adhere to? Or alternatively, was it verified that \nPPO and the MCTS methods are afforded approximately equal computation time? If not, it seems \nlike the MCTS methods  could have an unfair advantage against PPO, especially if they are \nallowed to take as long as  necessary to complete their rollouts. This computational bias \ncould potentially be remedied by  allowing PPO to use sufficiently complex function \napproximators, or by setting the number of simulations used by the MCTS methods \nsuch that their computation time is roughly equal to that of PPO.\n\n- I would be careful about stating that PPO is a state-of-the-art baseline. State-of-the-art is a big claim, and I'm not quite sure that it's true for PPO. PPO's performance is typically only compared to other policy-based RL methods; it's hard to say that it's a state-of-the-art method when there's a lack of published work comparing it against the well-known value-based approaches, like Rainbow. I suggest softening the language there unless you're confident that PPO truly is considered a state-of-the-art baseline.", "belong_id": "BJlQtJSKDB"}, {"uid": "r1gt3TnWqB", "paper_title": "Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search", "experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper introduces a new algorithm for parallelizing monte carlo tree search (MCTS). MCTS is hard to parallelize as we have to keep track of the statistics of the node of the tree, which are typically not up-to-date in a parallel execution. The paper introduces a new algorithm that updates the visitation counts before evaluating the rollout (which takes long), and therefore allows other workers to explore different parts of the tree as the exploration bonus is decreased for this node. The algorithm is evaluated on the atari games as well on a proprietary game and compared to other parallelized MCTS variants.\n\nThe makes intuitively a lot of sense, albeit it is very simple and it is a surprise that this has not been tried yet. Anyhow, simplicity is not a disadvantage. The algorithm seems to be effective and the evaluations are promising and the paper is also well written. I have only 2 main concerns with the paper:\n\n- The paper is very long (10 pages), and given that, we reviewers should use stricter reviewing rules. As the introduced algorithm is very simple, I do not think that 10 pages are justified. The paper should be considerably shortened (e.g. The 'user pass rate prediction system' does not add much to the paper, could be skipped. Moreover, the exact architecture is maybe also not that important).\n\n- The focus of the paper is planning, not learning. Planning conferences such as ICAPS would maybe be a better fit than ICLR. \n\nGiven the stricter reviewing guidelines, I am leaning more towards rejects as the algorithmic contribution is small and I do not think 10 pages are justified.", "belong_id": "BJlQtJSKDB"}, {"uid": "SJlF9m02qr", "paper_title": "Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search", "experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces a new algorithm for parallelizing Monte-Carlo Tree Search (MCTS). Specifically, when expanding a new node in the search tree, the algorithm updates the parent nodes statistics of the visit counts but not their values; it is only when the expansion and simulation steps are complete that the values are updated as well. This has the effect of shrinking the UCT exploration term, and making other workers less likely to explore that part of the tree even before the simulation is complete. This algorithm is evaluated in two domains, a mobile game called Joy City as well as on Atari. The proposed algorithm results in large speedups compared to serial MCTS with seemingly little impact in performance, and also results in higher scores on Atari than existing parallelization methods.\n\nScaling up algorithms like MCTS is an important aspect of machine learning research. While significant effort has been made by the RL community to scale up distributed model-free algorithms, less effort has been made for model-based algorithms, so it is exciting to see that emphasis here. Overall I thought the main ideas in paper were clear, the proposed method for how to effectively parallelize MCTS was compelling, and the experimental results were impressive. Thus, I tend to lean towards accept. However, there were three aspects of the paper that I thought could be improved. (1) It was unclear to me how much the parallelization method differs from previous approaches (called TreeP in the paper) which adjust both the visit counts and the value estimate. (2) The paper is missing experiments showing the decrease in performance compared to a serial version of the algorithm. (3) The paper did not always provide enough detail and in some cases used confusing terminology. If these three things can be addressed then I would be willing to increase my score.\n\nNote that while I am quite familiar with MCTS, I am less familiar with methods for parallelizing it, though based on a cursory Google Scholar search it seems that the paper is thorough in discussing related approaches.\n\n1. When performing TreeP, does the traversed node also get an increased visit count (in addition to the loss which is added to the value estimate)? In particular, [1] and [2] adjust both the visit counts and the values, which makes them quite similar to the present method (which just adjusts visit counts). Its not clear from the appendix whether TreeP means that just the values are adjusted, or both the values and nodes. If it is the former, then I would like to see experiments done where TreeP adjusts the visit counts as well, to be more consistent with prior work. (Relatedly, I thought the baselines could be described in significantly more detail than they currently are-pseudocode would in the appendix would be great!)\n\n2. I appreciate the discussion in Section 4 of how much one would expect the proposed parallelization method to suffer compared to perfect parallelization. However, this argument would be much more convincing if there were experiments to back it up: I want to know empirically how much worse the parallel version of MCTS does in comparison to the serial version of MCTS, controlling for the same number of simulations. \n\n3. While the main ideas in the paper were clear, I thought certain descriptions/terminology were confusing and that some details were missing. Here are some specifics that I would like to see addressed, roughly in order of importance:\n\n- I strongly recommend that the authors choose a different name for their algorithm than P-UCT, which is almost identical (and pronounced the same) as PUCT, which is a frequently used MCTS exploration strategy that incorporates prior knowledge (see e.g. [1] and [2]). P-UCT is also not that descriptive, given that there are other existing algorithms for parallelizing MCTS.\n\n- Generally speaking, it was not clear to me for all the experiments whether they were run for a fixed amount of wallclock time or a fixed number of simulations, and what the fixed values were in either of those cases. The fact that these details were missing made it somewhat more difficult for me to evaluate the experiments. I would appreciate if this could be clarified in the main text for all the experiments.\n\n- The master-slave phrasing is a bit jarring due to the association with slavery. Id recommend using a more inclusive set of terms like master-worker or manager-worker instead (this shouldnt be too much to change, since worker is actually used in several places throughout the paper already).\n\n- Figure 7c-d: What are game steps? Is this the number of steps taken to pass the level? Why not indicate pass rate instead, which seems to be the main quantity of interest?\n\n- Page 9: are these p-values adjusted for multiple comparisons? If not, please perform this adjustment and update the results in the text. Either way, please also report in the text what adjustment method is used.\n \n- Figure 7: 3D bar charts tend to be hard to interpret (and in some cases can be visually misleading). Id recommend turning these into heatmaps with a visually uniform colormap instead.\n\n- Page 1, bottom: the first time I read through the paper I did not know what a user pass-rate was (until I got to the experiments part of the paper which actually explained this term). I would recommend phrasing this in clearer way, such as estimating the rate at which users pass levels of the mobile game...\n\n- One suggestion just to improve the readability of the paper for readers who are not as familiar with MCTS is to reduce the number of technical terms in the first paragraph of the introduction. Readers unfamiliar with MCTS may not know what the expansion/simulation/rollout steps are, or why its necessary to keep the correct statistics of the search tree. I would recommend explaining the problem with parallelizing MCTS without using these specific terms, until they are later introduced when MCTS is explained.\n \n- Page 2: states correspond to nodes, so why introduce additional notation (n) to refer to nodes? It would be easier to follow if the same variable (s) was used for both.\n\nSome additional comments:\n\n- Section 5: Im not sure its necessary to explain so much of the detail of the user-pass rate prediction system in the main text. Its neat that comparing the results of different search budgets of MCTS allow predicting user behavior, but this seems to be a secondary point besides the main point of the paper (which is demonstrating that the proposed parallelization method is effective). I think the right part of Figure 5, as well as Table 1 and Figure 6, could probably go in the supplemental material. As someone with a background in cognitive modeling, I think these results are interesting, but that they are not the main focus of the paper. I was actually confused during my first read through as it was unclear to me initially why the focus had shifted from demonstrating that parallel MCTS works to \n\n- The authors may be interested in [3], which also uses a form of tree search to model human decisions in a game.\n\n- Page 9: the citation to [2] does not seem appropriate here since AlphaGo Zero did not use a pretrained search policy, I think [1] would be correct instead.\n\n[1] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Dieleman, S. (2016). Mastering the game of Go with deep neural networks and tree search. nature, 529(7587), 484.\n[2] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ... & Chen, Y. (2017). Mastering the game of go without human knowledge. Nature, 550(7676), 354.\n[3] van Opheusden, B., Bnaya, Z., Galbiati, G., & Ma, W. J. (2016, June). Do people think like computers?. In International conference on computers and games (pp. 212-224). Springer, Cham.", "belong_id": "BJlQtJSKDB"}]