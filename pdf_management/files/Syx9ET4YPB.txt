Title:
```
Under review as a conference paper at ICLR 2020 DO IMAGE CLASSIFIERS GENERALIZE ACROSS TIME?
```
Abstract:
```
We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.
```

Figures/Tables Captions:
```
Figure 1: Three examples of natural perturbations from nearby video frames and resulting classifier confidences from a ResNet-152 model fine-tuned on ImageNet-Vid. While the images appear almost identical to the human eye, the classifier confidence changes substantially.
Figure 2: Temporally adjacent frames may not be visually similar. We show three randomly sampled frame pairs where the nearby frame was marked as "dissimilar" to the anchor frame during human review and then discarded from our dataset.
Figure 3: Model accuracy on original vs. perturbed images. Each data point corresponds to one model in our testbed (shown with 95% Clopper-Pearson confidence intervals). Each perturbed frame was taken from a ten frame neighborhood of the original frame (approximately 0.3 seconds). All frames were reviewed by humans to confirm visual similarity to the original frames.
Figure 4: Naturally perturbed examples for detection. Red boxes indicate false positives; green boxes indicate true positives; white boxes are ground truth. Classification errors are common failures, such as the fox on the left, which is classified correctly in the anchor frame, and misclassified as a sheep in a nearby frame. However, detection models also have localization errors, where the object of interest is not correctly localized in addition to being misclassified, such as the airplane (middle) and the motorcycle (right). All visualizations show predictions with confidence greater than 0.5.
Figure 5: We plot the fraction of times each offset caused an error, across all evaluated models, for frames with and without review. Frames further away more frequently cause classifiers to misfire. Our review process reduces the number of errors, especially for frames further in time, by removing dissimilar frames.
Table 1: Statistics of ImageNet-Vid-Robust and YTBB-Robust. For YTBB-Robust, we updated the labels from for 41% (834) of the accepted anchors due to labeling errors in Youtube-BB.
Table 2: Accuracies of five different model types and the best performing model. The model architecture is ResNet-50 unless noted otherwise. 'FT' is 'fine-tuning.' See Section 3.1 for details.
Table 3: Detection and localization mAP for two Faster R-CNN backbones. Both detection and localization suffer from significant drops in mAP due to the perturbations. (*Model trained on ILSVRC Det and VID 2015 datasets, and evaluated on the 2015 subset of ILSVRC-VID 2017.)
Table 4: Impact of human review on original and perturbed accuracies for ImageNet-Vid-Robust and YTBB-Robust, using a ResNet-152 fine-tuned on ImageNet-Vid and Youtube-BB, respectively.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Convolutional neural networks (CNNs) still exhibit many troubling failure modes. At one extreme, p -adversarial examples cause large drops in accuracy for state-of-the-art models while relying only on visually imperceptible changes to the input image ( Goodfellow et al., 2014 ;  Biggio and Roli, 2018 ). However, this failure mode usually does not pose a problem outside a fully adversarial context because carefully crafted p -perturbations are unlikely to occur naturally in the real world. To study more realistic failure modes, researchers have investigated benign image perturbations such as rotations & translations, colorspace changes, and various image corruptions ( Fawzi and Frossard, 2015 ;  Engstrom et al., 2017 ;  Fawzi and Frossard, 2015 ;  Hendrycks and Dietterich, 2019 ). However, it is still unclear whether these perturbations reflect the robustness challenges arising in real data since the perturbations also rely on synthetic image modifications. Recent work has therefore turned to videos as a source of naturally occurring perturbations of images ( Zheng et al., 2016 ;  Azulay and Weiss, 2018 ;  Gu et al., 2019 ). In contrast to other failure modes, the perturbed images are taken from existing image data without further modifications that make the task more difficult. As a result, robustness to such perturbations directly corresponds to performance improvements on real data. However, it is currently unclear to what extent such video perturbations pose a significant robustness challenge.  Azulay and Weiss (2018)  and  Zheng et al. (2016)  only provide anecdotal evidence from a small number of videos.  Gu et al. (2019)  go beyond individual videos and utilize a large video dataset ( Real et al., 2017 ) in order to measure the effect of video perturbations more quantitatively. In their evaluation, the best image classifiers lose about 3% accuracy for video frames up to 0.3 seconds away. However, the authors did not employ humans to review the frames in their videos. Hence the accuracy drop could also be caused by significant changes in the video frames (e.g., due to fast camera or object motion). Since the 3% accuracy drop is small to begin with, it remains unclear whether video perturbations are a robustness challenge for current image classifiers. We address these issues by conducting a thorough evaluation of robustness to natural perturbations arising in videos. As a cornerstone of our investigation, we introduce two test sets for evaluating model robustness: ImageNet-Vid-Robust and YTBB-Robust, carefully curated from the ImageNet-Vid and Youtube-BB datasets, respectively ( Russakovsky et al., 2015 ;  Real et al., 2017 ). All images in the two datasets were screened by a set of expert labelers to ensure high annotation quality and minimize selection biases that arise when filtering a dataset with CNNs. To the best of Under review as a conference paper at ICLR 2020 our knowledge these are the first datasets of their kind, containing tens of thousands of images that are human reviewed and grouped into thousands of perceptually similar sets. In total, our datasets contain 3,139 sets of temporally adjacent and visually similar images (57,897 images total). We then utilize these datasets to measure the accuracy of current CNNs to small, naturally oc- curring perturbations. Our testbed contains over 45 different models, varying both architecture and training methodology (adversarial training, data augmentation, etc.). To better understand the drop in accuracy due to natural perturbations, we also introduce a robustness metric that is more stringent than those employed in prior work. Under this metric, we find that natural perturba- tions from ImageNet-Vid-Robust and YTBB-Robust induce a median accuracy drop of 16% and 10% respectively for classification tasks and a median 14 point drop in mAP for detection tasks. 1 Even for the best-performing classification models, we observe an accuracy drop of 14% for ImageNet-Vid-Robust and 8% for YTBB-Robust. Our results show that robustness to natural perturbations in videos is indeed a significant challenge for current CNNs. As these models are increasingly deployed in safety-critical environments that require both high accuracy and low latency (e.g., autonomous vehicles), ensuring reliable predictions on every frame of a video is an important direction for future work.

Section Title: CONSTRUCTING A TEST SET FOR ROBUSTNESS
  CONSTRUCTING A TEST SET FOR ROBUSTNESS ImageNet-Vid-Robust and YTBB-Robust are sourced from videos in the ImageNet-Vid and Youtube-BB datasets ( Russakovsky et al., 2015 ;  Real et al., 2017 ). All object classes in ImageNet-Vid and Youtube-BB are from the WordNet hierarchy (Miller, 1995) and direct ancestors of ILSVRC-2012 classes. Using the WordNet hierarchy, we construct a canonical mapping from ILSVRC-2012 classes to ImageNet-Vid and Youtube-BB classes, which allows us to evaluate off-the-shelf ILSVRC-2012 models on ImageNet-Vid-Robust and YTBB-Robust. We provide more background on the source datsets in Appendix A.

Section Title: CONSTRUCTING IM A G ENE T-VI D-RO B U S T AND YTBB-RO B U S T
  CONSTRUCTING IM A G ENE T-VI D-RO B U S T AND YTBB-RO B U S T Next, we describe how we extracted sets of naturally perturbed frames from ImageNet-Vid and Youtube-BB to create ImageNet-Vid-Robust and YTBB-Robust. A straightforward approach would be to select a set of anchor frames and use temporally adjacent frames in the video with the assumption that such frames contain only small perturbations from the anchor. However, as  Fig. 2  illustrates, this assumption is frequently violated, especially due to fast camera or object motion. Instead, we first collect preliminary datasets of natural perturbations following the same approach, and then manually review each of the frame sets. For each video, we randomly sample an anchor frame and take k = 10 frames before and after the anchor frame as candidate perturbation images. 2 This results in two datasets containing one anchor frame each from 3,139 videos, with approximately 20 candidate perturbation per anchor frame. Next, we curate the dataset with the help of four expert human annotators. The goal of the curation step is to ensure that each anchor frame and its nearby frames are correctly labeled with the same ground truth class, and that the anchor frame and the nearby frames are visually similar.

Section Title: Denser labels for Youtube-BB
  Denser labels for Youtube-BB As Youtube-BB contains only a single category label per frame at 1 frame per second, annotators first viewed each anchor frame individually and marked any missing labels. In total, annotators corrected the labels for 834 frames, adding an average of 0.5 labels per anchor frame. These labels are then propagated to nearby, unlabeled frames at the native frame rate and verified in the next step. ImageNet-Vid densely labels all classes per frame, so we skip this step.

Section Title: Frame pairs review
  Frame pairs review Next, for each pair of anchor and candidate perturbation frames, a human annotates (i) whether the pair is correctly labeled in the dataset, and (ii) whether the pair is similar. We took several steps to mitigate the subjectivity of this task and ensure high annotation quality. First, we trained reviewers to mark frames as dissimilar if the scene undergoes any of the following transformations: significant motion, significant background change, or significant blur change. We asked reviewers to mark each dissimilar frame with one of these transformations, or "other", and to mark a pair of images as dissimilar if a distinctive feature of the object is only visible in one of the two frames (such as the face of a dog). If an annotator was unsure about the correct label, she could mark the pair as "unsure". Second, we present only a single pair of frames at a time to reviewers because presenting videos or groups of frames could cause them to miss large changes due to the phenomenon of change blindness ( Pashler, 1988 ).

Section Title: Verification
  Verification In the previous stage, all annotators were given identical labeling instructions and individually reviewed a total of 71,660 images pairs. To increase consistency in annotation, annotators jointly reviewed all frames marked as dissimilar, incorrectly labeled, or "unsure". A frame was only considered similar to its anchor if a strict majority of the annotators marked the pair as such. After the reviewing was complete, we discarded all anchor frames and candidate perturbations that annotators marked as dissimilar or incorrectly labeled. The final datasets contain a combined total of 3,139 anchor frames with a median of 20 similar frames each.

Section Title: THE P M-K EVALUATION METRIC
  THE P M-K EVALUATION METRIC Given the datasets introduced above, we propose a metric to measure a model's robustness to natural perturbations. In particular, let A = {a 1 , ..., a n } be the set of valid anchor frames in our dataset. Let Y = {y 1 , ..., y n } be the set of labels for A. We let N k (a i ) be the set of frames marked as similar to anchor frame a i . In our setting, N k is a subset of the 2k temporally adjacent frames (plus/minus k frames from the anchor). Under review as a conference paper at ICLR 2020 Classification. Classification accuracy is defined as acc orig = 1 − 1 N N i=0 L 0/1 (f (a i ), y i ), where L 0/1 is the standard 0-1 loss function. We define the pm-k analog of accuracy as acc pmk = 1 − 1 N N i=0 max b∈N k (ai) L 0/1 (f (b), y i ) , (1) which corresponds to picking the worst frame from each set N k (a i ) before computing accuracy.

Section Title: Detection
  Detection The standard metric for detection is mean average precision (mAP) of the predictions at a fixed intersection-over-union (IoU) threshold  Lin et al. (2014) . We define the pm-k metric analogous to that for classification: We replace each anchor frame with the nearest frame that minimizes the average precision (AP, averaged over recall thresholds) of the predictions, and compute pm-k as the mAP on these worst-case neighboring frames.

Section Title: MAIN RESULTS
  MAIN RESULTS We evaluate a testbed of 45 classification and three detection models on ImageNet-Vid-Robust and YTBB-Robust. We first discuss the various types of classification models evaluated with the pm-k classification metric. Second, we evaluate the performance of detection models on ImageNet-Vid-Robust using use the bounding box annotations inherited from ImageNet-Vid using a variant of pm-k for detection. We then analyze the errors made on the detection adversarial examples to isolate the effects of localization errors vs. classification errors.

Section Title: CLASSIFICATION
  CLASSIFICATION The classification robustness metric is acc pmk defined in Equation (1). For frames with multiple labels, we count a prediction as correct if the model predicts any of the correct classes for a frame. In  Figure 3 , we plot the benign accuracy, acc orig , versus the robust accuracy, acc pmk , for all classification models in our test bed and find that the relationship between acc orig and acc pmk is approximately linear. This relationship indicates that improvements in the benign accuracy do result in improvements in the worst-case accuracy, but do not suffice to resolve the accuracy drop due to natural perturbations. Our test bed consists of five model types with increasing levels of supervision. We present results for representative models from each model type in  Table 2  and defer the full classification results table to Appendix B.2.

Section Title: ILSVRC Trained
  ILSVRC Trained The WordNet hierarchy enables us to repurpose models trained for the 1,000 class ILSVRC dataset on ImageNet-Vid-Robust and YTBB-Robust (see Appendix A.1). We evaluate a wide array of ILSVRC-2012 models (available from Cadene) against our natural perturbations. Since these datasets present a substantial distribution shift from the original ILSVRC- 2012 validation, we expect the benign accuracy acc orig to be lower than the comparable accuracy on the ILSVRC-2012 validation set. However, our main interest here is in the difference between the original and perturbed accuracies acc orig - acc pmk . A small drop in accuracy would indicate that the model is robust to small changes that occur naturally in videos. Instead, we find significant drops of 15.0% and 13.2% in accuracy on our two datasets, indicating sensitivity to such changes.

Section Title: Noise augmentation
  Noise augmentation One hypothesis for the accuracy drop from original to perturbed accuracy is that subtle artifacts and corruptions introduced by video compression schemes could degrade performance when evaluating on these corrupted frames. The worst-case nature of the pm-k metric could then be focusing on these corrupted frames. One model for these corruptions are the perturba- tions introduced in  Hendrycks and Dietterich (2019) . To test this hypothesis, we evaluate models augmented with a subset of the perturbations (exactly one of: Gaussian noise, Gaussian blur, shot noise, contrast change, impulse noise, or JPEG compression). We found that these augmentation schemes did not improve robustness against our perturbations substantially, and still result in accuracy drop of 15.6% and 16.6% on the two datasets. ∞ robustness. We evaluate the model from  Xie et al. (2018) , which currently performs best against ∞ attacks on ImageNet. We find that this model has a smaller accuracy drop than the two aforementioned model types on both datasets. However, we note that the robust model achieves significantly lower original and perturbed accuracy than either of the two model types above, and the robustness gain is modest (3% compared to models of similar benign accuracy).

Section Title: Fine-tuning on video frames
  Fine-tuning on video frames To adapt to the new class vocabulary and the video domain, we fine-tune several network architectures on the ImageNet-Vid and Youtube-BB training sets. For Youtube-BB, we train on the anchor frames used for training in  Gu et al. (2019) , and for ImageNet-Vid we use all frames in the training set. We provide hyperparameters for all models in Appendix K. The resulting models significantly improve in accuracy over their ILSVRC pre-trained counterparts (e.g., 13% on ImageNet-Vid-Robust and 34% on YTBB-Robust for ResNet-50). This im- provement in accuracy results in a modest improvement in the accuracy drop for YTBB-Robust, but a finetuned ResNet-50 still suffers from a significant 9.4% drop. On ImageNet-Vid-Robust, there is almost no change in the accuracy drop from 15.0% to 15.1%.

Section Title: Fine-tuning for detection on video frames
  Fine-tuning for detection on video frames We further analyze whether additional supervision in the form of bounding box annotations improves robustness. To this end, we train the Faster R-CNN detection model  Ren et al. (2015)  with a ResNet-50 backbone on ImageNet-Vid. Following standard practice, the detection backbone is pre-trained on ILSVRC-2012. To evaluate this detector Under review as a conference paper at ICLR 2020 for classification, we assign the class with the most confident bounding box as label to the image. We find that this transformation reduces accuracy compared to the model trained for classification (77.6% vs. 80.8%). While there is a slight reduction in the accuracy drop caused by natural perturbations, the reduction is well within the error bars for this test set.

Section Title: DETECTION
  DETECTION We further study the impact of natural perturbations on object detection. Specifically, we report results for two related tasks: object localization and detection. Object detection is the standard computer vision task of correctly classifying an object and finding the coordinates of a tight bounding box containing the object. "Object localization", meanwhile, refers to only the subtask of finding the bounding box, without attempting to correctly classify the object. We present our results on ImageNet-Vid-Robust, which contains dense bounding box labels unlike Youtube-BB, which only labels boxes at 1 frame per second. We use the popular Faster R-CNN  Ren et al. (2015)  and  R-FCN Dai et al. (2016) ;  Xiao and Jae Lee (2018)  architectures for object detection and localization and report results in  Table 3 . For the R-FCN architecture, we use the model from  Xiao and Jae Lee (2018)  4 . We first note the significant drop in mAP of 12 - 15 points for object detection due to perturbed frames for both the Faster R-CNN and R-FCN architectures. Next, we show that localization is indeed easier than detection, as the mAP is higher for localization than for detection (e.g., 76.6 vs 62.8 for Faster R-CNN with a ResNet-50 backbone). Perhaps surprisingly, however, switching to the localization task does not improve the drop between original and perturbed frames, indicating that natural perturbations induce both classification and localization errors. We show examples of detection failures in  Figure 4 .

Section Title: IMPACT OF DATASET REVIEW
  IMPACT OF DATASET REVIEW We analyze the impact of our human review, described in Section 2.1, on the classifiers in our test bed. First, we compare the original and perturbed accuracies of a representative classifier (ResNet- 152 finetuned) with and without review in  Table 4 . Our review improves the original accuracy by 3-4% by throwing away mislabeled or blurry anchor frames, and improves perturbed accuracy by 5-6% by discarding pairs of dissimilar frames. Our review reduces the accuracy drop by 1.8% on Under review as a conference paper at ICLR 2020 ImageNet-Vid-Robust and 1.1% on YTBB-Robust, but still results in large accuracy drops. These results indicate that the changes in model predictions are indeed due to a lack of robustness, rather than due to significant differences between adjacent frames. To further analyze the impact of our review on model errors, we plot how frequently each offset distance from the anchor frame results in a model error across all model types in  Figure 5 . For both datasets, larger offsets (indicating pairs of frames further apart in time) lead to more frequent model errors. Our review reduces the fraction of errors across offsets, and especially for large offsets, which are more likely to display large changes from the anchor frame.

Section Title: Adversarial examples
  Adversarial examples While various forms of adversarial examples have been studied, the major- ity of research focuses on p robustness  Goodfellow et al. (2014) ;  Biggio and Roli (2018) . However, it is unclear whether adversarial examples pose a problem for classifier robustness outside of a truly worst case context. It is an open question whether perfect robustness against a p adversary will induce robustness to realistic image distortions such as those studied in this paper. Recent work has proposed more realistic image modifications such as small rotations & translations Engstrom et al. Under review as a conference paper at  ICLR 2020 (2017) ;  Azulay and Weiss (2018) ;  Fawzi and Frossard (2015) ;  Kanbak et al. (2017) , hue and color changes  Hosseini and Poovendran (2018) , image stylization  Geirhos et al. (2018a)  and synthetic image corruptions such as Gaussian blur and JPEG compression  Hendrycks and Dietterich (2019) ;  Geirhos et al. (2018b) . Even though the above examples are more realistic than the p model, they still synthetically modify the input images to generate perturbed versions. In contrast, our work performs no synthetic modification and instead uses images that naturally occur in videos.

Section Title: Utilizing videos to study robustness
  Utilizing videos to study robustness In work concurrent to ours,  Gu et al. (2019)  exploit the temporal structure in videos to study robustness. However, their experiments suggest a substantially smaller drop in classification accuracy. The primary reason for this is a less stringent metric used in  Gu et al. (2019) . By contrast, our "pm-k" metric is inspired by the "worst-of-k" metric used in prior work  Engstrom et al. (2017) , highlighting the sensitivity of models to natural perturbations. In Appendix E we study the differences between the two metrics in more detail. Furthermore, the lack of human review and the high label error-rate we discovered in Youtube-BB( Table 1 ) presents a troubling confounding factor that we resolve in our work.

Section Title: Distribution shift
  Distribution shift Small, benign changes in the test distribution are often referred to as distribution shift. Recht et al. (2019) explore this phenomenon by constructing new test sets for CIFAR-10 and ImageNet and observe performance drops for a large suite of models on the newly constructed test sets. Similar to our  Figure 3 , the relationship between original and new test set accuracy is also approximately linear. However, the images in their test set bear little visual similarity to images in the original test set, while all of our failure cases in ImageNet-Vid-Robust and YTBB-Robust are on perceptually similar images. In a similar vein of study,  Torralba et al. (2011)  studies distribution shift across different computer vision data sets such as Caltech-101, PASCAL, and ImageNet.

Section Title: Computer vision
  Computer vision A common issue when applying image based models to videos is flickering, where object detectors spuriously produce false-positives or false-negatives in isolated frames or groups of frames.  Jin et al. (2018)  explicitly identify such failures and use a technique reminiscent of adversarially robust training to improve image-based models. A similar line of work focuses on improving object detection in videos as objects become occluded or move quickly  Kang et al. (2017) ;  Feichtenhofer et al. (2017) ;  Zhu et al. (2017) ;  Xiao and Jae Lee (2018) . The focus in this line of work has generally been on improving object detection when objects transform in a way that makes recognition difficult from a single frame, such as fast motion or occlusion. In this work, we document a broader set of failure cases for image-based classifiers and detectors and show that failures occur when the neighboring frames are imperceptibly different.

Section Title: CONCLUSION
  CONCLUSION Our study quantifies the sensitivity of image classifiers to naturally occuring temporal perturbations. We show that these perturbations can cause significant drops in accuracy for a wide range of models for both classification and detection. Our work on analyzing this failure mode opens multiple avenues for future research:

Section Title: Building more robust models
  Building more robust models Our ImageNet-Vid-Robust and YTBB-Robust datasets provide a standard measure for robustness that can be applied to any classification or detection model. In  Table 2 , we evaluated several commonly used models and found that all of them suffer from substantial accuracy drops due to natural perturbations. In particular, we found that model improvements with respect to artificial perturbations (such as image corruptions or ∞ adversaries) induce at best modest improvements in robustness. We hope that our standardized datasets and evaluation metric will enable future work to quantify improvements in natural robustness directly.

Section Title: Further natural perturbations
  Further natural perturbations Videos provide a straightforward method for collecting natural perturbations of images, admitting the study of realistic forms of robustness for machine learning methods. Other methods for generating these natural perturbations are likely to provide additional insights into model robustness. As an example, photo sharing websites contain a large number of near-duplicate images: pairs of images of the same scene captured at different times, viewpoints, or from a different camera Recht et al. (2019). More generally, devising similar, domain-specific strategies to collect, verify, and measure robustness to natural perturbations in domains such as natural language processing or speech recognition is a promising direction for future work. Under review as a conference paper at ICLR 2020

```
