Title:
```
Published as a conference paper at ICLR 2020 RANKING POLICY GRADIENT
```
Abstract:
```
Sample inefficiency is a long-lasting problem in reinforcement learning (RL). The state-of-the-art estimates the optimal action values while it usually involves an extensive search over the state-action space and unstable optimization. Towards the sample-efficient RL, we propose ranking policy gradient (RPG), a policy gradient method that learns the optimal rank of a set of discrete actions. To accelerate the learning of policy gradient methods, we establish the equivalence between maximiz- ing the lower bound of return and imitating a near-optimal policy without accessing any oracles. These results lead to a general off-policy learning framework, which preserves the optimality, reduces variance, and improves the sample-efficiency. We conduct extensive experiments showing that when consolidating with the off-policy learning framework, RPG substantially reduces the sample complexity, comparing to the state-of-the-art.
```

Figures/Tables Captions:
```
Figure 1: The off-policy learning as supervised learning framework for general policy gradient methods. The proof of Corollary 2 can be found in Appendix 10.8. This corollary shows that the variance of regular policy gradient is upper-bounded by the square of time horizon and the maximum trajectory reward. It is aligned with our intuition and empirical observation: the longer the horizon the harder the learning. Also, the common reward shaping tricks such as truncating the reward to [−1, 1] (Castro et al., 2018) can help the learning since it reduces variance by decreasing the range of trajectory reward.
Figure 2: The training curves of the proposed RPG and state-of-the-art. All results are averaged over random seeds from 1 to 5. The x-axis represents the number of steps interacting with the environment (we update the model every four steps) and the y-axis represents the averaged training episodic return. The error bars are plotted with a confidence interval of 95%.
Figure 3: The trade-off between sample efficiency and optimality on DOUBLEDUNK,BREAKOUT, BANKHEIST. As the trajectory reward threshold (c) increase, more samples are needed for the learning to converge, while it leads to better final performance. We denote the value of c by the numbers at the end of legends.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION One of the major challenges in reinforcement learning (RL) is the high sample complexity (Kakade et al., 2003), which is the number of samples must be collected to conduct successful learning. There are different reasons leading to poor sample efficiency of RL ( Yu, 2018 ). Because policy gradient algorithms directly optimizing return estimated from rollouts (e.g., REINFORCE (Williams, 1992)) could suffer from high variance ( Sutton & Barto, 2018 ), value function baselines were introduced by actor-critic methods to reduce the variance and improve the sample-efficiency. However, since a value function is associated with a certain policy, the samples collected by former policies cannot be readily used without complicated manipulations (Degris et al., 2012) and extensive parameter tuning (Nachum et al., 2017). Such an on-policy requirement increases the difficulty of sample-efficient learning. On the other hand, off-policy methods, such as one-step Q-learning (Watkins & Dayan, 1992) and variants of deep Q networks (DQN) (Mnih et al., 2015; Hessel et al., 2017;  Dabney et al., 2018 ; Van Hasselt et al., 2016; Schaul et al., 2015), enjoys the advantage of learning from any trajectory sampled from the same environment (i.e., off-policy learning), are currently among the most sample- efficient algorithms. These algorithms, however, often require extensive searching ( Bertsekas & Tsitsiklis, 1996 , Chap. 5) over the large state-action space to estimate the optimal action value function. Another deficiency is that, the combination of off-policy learning, bootstrapping, and function approximation, making up what  Sutton & Barto (2018)  called the "deadly triad", can easily lead to unstable or even divergent learning ( Sutton & Barto, 2018 , Chap. 11). These inherent issues limit their sample-efficiency. Towards addressing the aforementioned challenge, we approach the sample-efficient reinforcement learning from a ranking perspective. Instead of estimating optimal action value function, we concen- trate on learning optimal rank of actions. The rank of actions depends on the relative action values. As long as the relative action values preserve the same rank of actions as the optimal action values (Q-values), we choose the same optimal action. To learn optimal relative action values, we propose the ranking policy gradient (RPG) that optimizes the actions' rank with respect to the long-term reward by learning the pairwise relationship among actions. Ranking Policy Gradient (RPG) that directly optimizes relative action values to maximize the return is a policy gradient method. The track of off-policy actor-critic methods (Degris et al., 2012; Gu et al., 2016; Wang et al., 2016) have made substantial progress on improving the sample-efficiency Published as a conference paper at ICLR 2020 of policy gradient. However, the fundamental difficulty of learning stability associated with the bias-variance trade-off remains (Nachum et al., 2017). In this work, we first exploit the equivalence between RL optimizing the lower bound of return and supervised learning that imitates a specific optimal policy. Build upon this theoretical foundation, we propose a general off-policy learning framework that equips the generalized policy iteration ( Sutton & Barto, 2018 , Chap. 4) with an external step of supervised learning. The proposed off-policy learning not only enjoys the property of optimality preserving (unbiasedness), but also largely reduces the variance of policy gradient because of its independence of the horizon and reward scale. Besides, we empirically show that there is a trade-off between optimality and sample-efficiency. Last but not least, we demonstrate that the proposed approach, consolidating the RPG with off-policy learning, significantly outperforms the state-of-the-art (Hessel et al., 2017;  Bellemare et al., 2017 ;  Dabney et al., 2018 ; Mnih et al., 2015).

Section Title: RELATED WORKS
  RELATED WORKS

Section Title: Sample Efficiency
  Sample Efficiency The sample efficient reinforcement learning can be roughly divided into two categories. The first category includes variants of Q-learning (Mnih et al., 2015; Schaul et al., 2015; Van Hasselt et al., 2016; Hessel et al., 2017). The main advantage of Q-learning methods is the use of off-policy learning, which is essential towards sample efficiency. The representative DQN (Mnih et al., 2015) introduced deep neural network in Q-learning, which further inspried a track of successful DQN variants such as Double DQN (Van Hasselt et al., 2016), Dueling networks (Wang et al., 2015), prioritized experience replay (Schaul et al., 2015), and RAINBOW (Hessel et al., 2017). The second category is the actor-critic approaches. Most of recent works (Degris et al., 2012; Wang et al., 2016;  Gruslys et al., 2018 ) in this category leverage importance sampling by re-weighting the samples to correct the estimation bias and reduce variance. Its main advantage is in the wall-clock times due to the distributed framework, firstly presented in (Mnih et al., 2016), instead of the sample-efficiency. As of the time of writing, the variants of DQN (Hessel et al., 2017;  Dabney et al., 2018 ;  Bellemare et al., 2017 ; Schaul et al., 2015; Van Hasselt et al., 2016) are among the algorithms of most sample efficiency, which are adopted as our baselines for comparison.

Section Title: RL as supervised learning
  RL as supervised learning Numerous amount of works have developed the connections between RL and supervised learning such as Expectation-Maximization algorithms (Dayan & Hinton, 1997; Peters & Schaal, 2007; Kober & Peters, 2009;  Abdolmaleki et al., 2018 ), Entropy-Regularized RL ( Oh et al., 2018 ;  Haarnoja et al., 2018 ), and Interactive Imitation Learning (IIL) (Daumé et al., 2009; Syed & Schapire, 2010; Ross & Bagnell, 2010; Ross et al., 2011; Sun et al., 2017;  Hester et al., 2018 ;  Osa et al., 2018 ). EM-based approaches utilize the probabilistic framework to transfer RL maximizing lower bound of return as a re-weighted regression problem while it requires on-policy estimation on the expectation step. Entropy-Regularized RL optimizing entropy augmented objectives can lead to off-policy learning without the usage of importance sampling while it converges to soft optimality ( Haarnoja et al., 2018 ). Of the three tracks in prior works, the IIL is most closely related to our work. The IIL works firstly pointed out the connection between imitation learning and reinforcement learning (Ross & Bagnell, 2010; Syed & Schapire, 2010; Ross et al., 2011) and explore the idea of facilitating reinforcement learning by imitating experts. However, most of imitation learning algorithms assume the access to the expert policy or demonstrations. Our off-policy learning framework can be interpreted as an online imitation learning approach that constructs expert demonstrations during the exploration without soliciting experts, and conducts supervised learning to maximize return at the same time. In conclusion, our approach is different from the prior work in terms of at least one of the following aspects: objectives, oracle assumptions, the optimality of learned policy, and on-policy requirement. More concretely, the proposed method is able to learn both deterministic and stochastic optimal policy in terms of long-term reward, without access to the oracle (such as expert policy or expert demonstration) and it can be trained both empirically and theoretically in an off-policy fashion. Due to the space limits, we defer the detailed discussion of the related work in the Appendix Section 10.1.

Section Title: NOTATIONS AND PROBLEM SETTING
  NOTATIONS AND PROBLEM SETTING In this paper, we consider a finite horizon T , discrete time Markov Decision Process (MDP) with a finite discrete state space S and for each state s ∈ S, the action space A s is finite. The environment dynamics is denoted as P = {p(s |s, a), ∀s, s ∈ S, a ∈ A s }. We note that the dimension of action space can vary given different states. We use m = max s A s to denote the maximal action dimension among all possible states. Our goal is to maximize the expected sum of rewards, or return Published as a conference paper at ICLR 2020 J(θ) = E τ,π θ [ T t=1 r(s t , a t )], where |r(s, a)| < ∞, ∀s, a. In this case, the optimal deterministic Markovian policy always exists (Puterman, 2014)[Proposition 4.4.3]. The upper bound of trajectory reward (r(τ )) is denoted as R max = max τ r(τ ). A comprehensive list of notations are elaborated in the Appendix Table 1.

Section Title: RANKING POLICY GRADIENT
  RANKING POLICY GRADIENT Value function estimation is widely used in advanced RL algorithms (Mnih et al., 2015;  2016 ;  Schulman et al., 2017 ;  Gruslys et al., 2018 ; Hessel et al., 2017;  Dabney et al., 2018 ) to facilitate the learning process. In practice, the on-policy requirement of value function estimations in actor- critic methods has largely increased the difficulty of sample-efficient learning (Degris et al., 2012;  Gruslys et al., 2018 ). With the advantage of off-policy learning, the DQN (Mnih et al., 2015) variants are currently among the most sample-efficient algorithms (Hessel et al., 2017;  Dabney et al., 2018 ;  Bellemare et al., 2017 ). For complicated tasks, the value function can align with the relative relationship of action's return, but the absolute values are hardly accurate (Mnih et al., 2015;  Ilyas et al., 2018 ). The above observations motivate us to look at the decision phase of RL from a different prospect: Given a state, the decision making is to perform a relative comparison over available actions and then choose the best action, which can lead to relatively higher return than others. Therefore, an alternative solution is to learn the optimal rank of the actions. In this section, we show how to optimize the rank of actions to maximize the return, and thus avoid the necessity of accurate estimation for optimal action value function. To learn the rank of actions, we focus on learning relative action value (λ-values), defined as follows: Definition 1 (Relative action value (λ-values)). For a state s, the relative action values of m actions (λ(s, a k ), k = 1, ..., m) is a list of scores that denotes the rank of actions. If λ(s, a i ) > λ(s, a j ), then action a i is ranked higher than action a j . The optimal relative action values should preserve the same optimal action as the optimal action values: arg max a λ(s, a) = arg max a Q π* (s, a) where Q π* (s, a i ) and λ(s, a i ) represent the optimal action value and the relative action value of action a i , respectively. We omit the model parameter θ in λ θ (s, a i ) for concise presentation. Remark 1. The λ-values are different from the advantage function A π (s, a) = Q π (s, a) − V π (s). The advantage functions quantitatively show the difference of return taking different actions following the current policy π. The λ-values only determine the relative order of actions and its magnitudes are not the estimations of returns. To learn the λ-values, we can construct a probabilistic model of λ-values such that the best action has the highest probability to be selected than others. Inspired by learning to rank ( Burges et al., 2005 ), we consider the pairwise relationship among all actions, by modeling the probability (denoted as p ij ) of an action a i to be ranked higher than any action a j as follows: p ij = exp(λ(s, a i ) − λ(s, a j )) 1 + exp(λ(s, a i ) − λ(s, a j )) , (1) where p ij = 0.5 means the relative action value of a i is same as that of the action a j , p ij > 0.5 indicates that the action a i is ranked higher than a j . Given the independent Assumption 1, we can represent the probability of selecting one action as the multiplication of a set of pairwise probabilities in Eq (1). Formally, we define the pairwise ranking policy in Eq (2). Please refer to Section 10.10 in the Appendix for the discussions on feasibility of Assumption 1. Definition 2. The pairwise ranking policy is defined as: π(a = a i |s) = Π m j=1,j =i p ij , (2) where the p ij is defined in Eq (1). The probability depends on the relative action values q = [λ 1 , ..., λ m ]. The highest relative action value leads to the highest probability to be selected. Assumption 1. For a state s, the set of events E = {e ij |∀i = j} are conditionally independent, where e ij denotes the event that action a i is ranked higher than action a j . The independence of the events is conditioned on a MDP and a stationary policy. Our ultimate goal is to maximize the long-term reward through optimizing the pairwise ranking policy or equivalently optimizing pairwise relationship among the action pairs. Ideally, we would like the pairwise ranking policy selects the best action with the highest probability and the highest λ-value. To achieve this goal, we resort to the policy gradient method. Formally, we propose the ranking policy gradient method (RPG), as shown in Theorem 1. Theorem 1 (Ranking Policy Gradient Theorem). For any MDP, the gradient of the expected long- term reward J(θ) = τ p θ (τ )r(τ ) w.r.t. the parameter θ of a pairwise ranking policy (Def 2) can be approximated by: ∇ θ J(θ) ≈ E τ ∼π θ T t=1 ∇ θ m j=1,j =i (λ i − λ j )/2 r(τ ) , (3) and the deterministic pairwise ranking policy π θ is: a = arg max i λ i , i = 1, . . . , m, where λ i denotes the relative action value of action a i (λ θ (s t , a t ), a i = a t ), s t and a t denotes the t-th state- action pair in trajectory τ , λ j , ∀j = i denote the relative action values of all other actions that were not taken given state s t in trajectory τ , i.e., λ θ (s t , a j ), ∀a j = a t . The proof of Theorem 1 is available in Appendix Section 10.2. Theorem 1 states that optimizing the discrepancy between the relative action values of the best action and all other actions, is optimizing the pairwise relationships that maximize the return. One limitation of RPG is that it is not convenient for the tasks where only optimal stochastic policies exist since the pairwise ranking policy takes extra efforts to construct a probability distribution [see Section 10.3 in Appendix]. In order to learn the stochastic policy, we introduce Listwise Policy Gradient (LPG) that optimizes the probability of ranking a specific action on the top of a set of actions, with respect to the return. In the context of RL, this top one probability is the probability of action a i to be chosen, which is equal to the sum of probability all possible permutations that map action a i in the top. Inspired by listwise learning to rank approach ( Cao et al., 2007 ), the top one probability can be modeled by the softmax function. Therefore, LPG is equivalent to the REINFORCE (Williams, 1992) algorithm with a softmax layer. LPG provides another interpretation of REINFORCE algorithm from the perspective of learning the optimal ranking and enables the learning of both deterministic policy and stochastic policy. Due to the space limit, we defer the detailed description of LPG in Appendix Section 10.4. To this end, seeking sample-efficiency motivates us to learn the relative relationship (RPG (Theorem 1) and LPG (Theorem 4)) of actions, instead of seeking accurate estimation of optimal action values and then choosing action greedily. However, both of the RPG and LPG belong to policy gradient methods, which suffers from large variance and the on-policy learning requirement ( Sutton & Barto, 2018 ). Therefore, the direct implementation of RPG or LPG is still far from sample-efficient. In the next section, we will describe a general off-policy learning framework empowered by supervised learning, which provides an alternative way to accelerate learning, preserve optimality, and reduce variance.

Section Title: OFF-POLICY LEARNING AS SUPERVISED LEARNING
  OFF-POLICY LEARNING AS SUPERVISED LEARNING In this section, we discuss the connections and discrepancies between RL and supervised learning, and our results lead to a sample-efficient off-policy learning paradigm for RL. The main result in this section is Theorem 2, which casts the problem of maximizing the lower bound of return into a supervised learning problem, given one relatively mild Assumption 2 and practical Assumptions 1,3. As we show by Lemma 4 in the Appendix that assumptions are valid in a range of RL tasks. The central idea is to collect only the near-optimal trajectories when the learning agent interacts with the environment, and imitate the near-optimal policy by maximizing the log likelihood of the state-action pairs from near-optimal trajectories. With the road map in mind, we then begin to introduce our approach as follows. In a discrete action MDP with finite states and horizon, given the near-optimal policy π * , the stationary state distribution is given by: p π* (s) = τ p(s|τ )p π* (τ ), where p(s|τ ) is the probability of a certain state given a specific trajectory τ and is not associated with any policies, and only Published as a conference paper at ICLR 2020 p π* (τ ) is related to the policy parameters. The stationary distribution of state-action pairs is thus: p π* (s, a) = p π* (s)π * (a|s). In this section, we consider the MDP that each initial state will lead to at least one (near)-optimal trajectory. For a more general case, please refer to the discussion in Appendix 10.5. In order to connect supervised learning (i.e., imitating a near-optimal policy) with RL and enable sample-efficient off-policy learning, we first introduce the trajectory reward shaping (TRS), defined as follows: Definition 3 (Trajectory Reward Shaping, TRS). Given a fixed trajectory τ , its trajectory reward is shaped as follows: where c = R max − is a problem-dependent near-optimal trajectory reward threshold that indicates the least reward of near-optimal trajectory, ≥ 0 and R max . We denote the set of all possible near-optimal trajectories as T = {τ |w(τ ) = 1}, i.e., w(τ ) = 1, ∀τ ∈ T . Remark 2. The threshold c indicates a trade-off between the sample-efficiency and the optimality. The higher the threshold, the less frequently it will hit the near-optimal trajectories during exploration, which means it has higher sample complexity, while the final performance is better (see  Figure 3 ). Remark 3. The trajectory reward can be reshaped to any positive functions that are not related to policy parameter θ. For example, if we set w(τ ) = r(τ ), the conclusions in this section still hold (see Eq (38) in Appendix, Section 10.6). For the sake of simplicity, we set w(τ ) = 1. Different from the reward shaping works (Ng et al., 1999), we directly shape the trajectory reward, which will enable the smooth transform from RL to SL. After shaping the trajectory reward, we can transfer the goal of RL from maximizing the return to maximize the long-term performance (Def 4). Definition 4 (Long-term Performance). The long-term performance is the expected shaped trajectory reward, as shown in Eq (4). By Def 3, the expectation over all trajectories is the equal to that over the near-optimal trajectories in T , i.e., The optimality is preserved after trajectory reward shaping ( = 0, c = R max ) since the opti- mal policy π * maximizing long-term performance is also an optimal policy for original MDP, i.e., τ p π* (τ )r(τ ) = τ ∈T p π* (τ )r(τ ) = R max , where π * = arg max π θ τ p π θ (τ )w(τ ) and p π* (τ ) = 0, ∀τ / ∈ T (see Lemma 2 in Appendix 10.6). Similarly, when > 0, the optimal policy after trajectory reward shaping is a near-optimal policy for original MDP. Note that most policy gradient methods use softmax function, in which we have ∃τ / ∈ T , p π θ (τ ) > 0 (see Lemma 3 in Appendix 10.6). Therefore when softmax is used to model a policy, it will not converge to an exact optimal policy. On the other hand, ideally, the discrepancy of the performance between them can be arbitrarily small based on the universal approximation (Hornik et al., 1989) with general conditions on the activation function and Theorem 1. in (Syed & Schapire, 2010). Essentially, we use TRS to filter out near-optimal trajectories and then we maximize the probabilities of near-optimal trajectories to maximize the long-term performance. This procedure can be approx- imated by maximizing the log-likelihood of near-optimal state-action pairs, which is a supervised learning problem. Before we state our main results, we first introduce the definition of uniformly near-optimal policy (Def 5) and a prerequisite (Asm. 2) specifying the applicability of the results.

Section Title: Definition 5 (Uniformly Near-Optimal Policy, UNOP)
  Definition 5 (Uniformly Near-Optimal Policy, UNOP) Based on Lemma 4 in Appendix Section 10.9, Assumption 2 is satisfied for certain MDPs that have deterministic dynamics. Other than Assumption 2, all other assumptions in this work (Assump- tions 1,3) can almost always be satisfied in practice, based on empirical observation. With these relatively mild assumptions, we present the following long-term performance theorem, which shows the close connection between supervised learning and RL. Theorem 2 (Long-term Performance Theorem). Maximizing the lower bound of expected long-term performance (Eq (4)) is maximizing the log-likelihood of state-action pairs sampled from an uniformly (near)-optimal policy π * , which is a supervised learning problem: The optimal policy of maximizing the lower bound is also the optimal policy of maximizing the long-term performance and the return. Remark 4. It is worth noting that Theorem 2 does not require a uniformly near-optimal policy π * to be deterministic. The only requirement is the existence of a uniformly near-optimal policy. Remark 5. Maximizing the lower bound of long-term performance is to maximize the lower bound of long-term reward since we can set w(τ ) = r(τ ) and τ p θ (τ )r(τ ) ≥ T p θ (τ )w(τ ). An optimal policy of maximizing this lower bound is also an optimal policy of maximizing the long-term performance when c = R max , thus maximizing the return. The proof of Theorem 2 can be found in Appendix, Section 10.6. Theorem 2 indicates that we break the dependency between current policy π θ and the environment dynamics, which means off-policy learning is able to be conducted by the above supervised learning approach. Furthermore, we point out that there is a potential discrepancy between imitating UNOP by maximizing log likelihood (even when the optimal policy's samples are given) and the reinforcement learning since we are maximizing a lower bound of expected long-term performance (or equivalently the return over the near-optimal trajectories only) instead of return over all trajectories. In practice, the state-action pairs from an optimal policy is hard to construct while the uniform characteristic of UNOP can alleviate this issue (see Sec 6). Towards sample-efficient RL, we apply Theorem 2 to RPG, which reduces the ranking policy gradient to a classification problem by Corollary 1. Corollary 1 (Ranking performance policy gradient). Optimizing the lower bound of expected long- term performance (defined in Eq (4)) using pairwise ranking policy (Eq (2)) can be approximately optimized by the following loss: min θ s,ai p π* (s, a i ) m j=1,j =i max(0, margin + λ(s, a j ) − λ(s, a i )) , (6) where margin is a small positive value. We set margin equal to one in our experiments. The proof of Corollary 1 can be found in Appendix, section 10.7. Similarly, we can reduce LPG to a classification problem (see Appendix 10.7.1). One advantage of casting RL to SL is variance reduction. With the proposed off-policy supervised learning, we can reduce the upper bound of the policy gradient variance, as shown in the Corollary 2. Before introducing the variance reduction results, we first make the following standard assumption similar to (Degris et al., 2012, A1). Furthermore, the assumption is guaranteed for bounded continuously differentiable policy such as softmax function.

Section Title: Assumption 3
  Assumption 3 Corollary 2 (Policy gradient variance reduction). The upper bound of the variance of each dimension of policy gradient is O(T 2 C 2 M 2 ). The upper bound of gradient variance of maximizing the lower bound of long-term performance Eq (5) is O(C 2 ), where C is the maximum norm of log gradient based on Assumption 3, M is the maximum absolute value of trajectory reward (i.e., M ≥ |r(τ )|, ∀τ ), and T is the horizon. The upper bound of gradient variance by supervised learning compared to that of the regular policy gradient is reduced by With supervised learning, we concentrate the difficulty of long-time horizon into the exploration phase, which is an inevitable issue for all RL algorithms, and we drop the dependence on T and M for policy variance. Thus, it is more stable and efficient to train the policy using supervised learning. One potential limitation of this method is that the trajectory reward threshold c is task-specific, which is crucial to the final performance and sample-efficiency. In many applications such as Dialogue system ( Li et al., 2017 ), recommender system (Melville & Sindhwani, 2011), etc., we design the reward function to guide the learning process, in which c is naturally known. For the cases that we have no prior knowledge on the reward function of MDP, we treat c as a tuning parameter to balance the optimality and efficiency, as we empirically verified in  Figure 3 . The major theoretical uncertainty on general tasks is the existence of a uniformly near-optimal policy, which is negligible to the empirical performance. The rigorous theoretical analysis of this problem is beyond the scope of this work.

Section Title: AN ALGORITHMIC FRAMEWORK FOR OFF-POLICY LEARNING
  AN ALGORITHMIC FRAMEWORK FOR OFF-POLICY LEARNING Based on the discussions in Section 5, we exploit the advantage of reducing RL into supervised learning via a proposed two-stages off-policy learning framework. As we illustrated in  Figure 1 , the proposed framework contains the following two stages:

Section Title: Generalized Policy Iteration for Exploration
  Generalized Policy Iteration for Exploration The goal of the exploration stage is to collect different near-optimal trajectories as frequently as possible. Under the off-policy framework, the exploration agent and the learning agent can be separated. Therefore, any existing RL algorithm can be used during the exploration. The principle of this framework is using the most advanced RL agents as an exploration strategy in order to collect more near-optimal trajectories and leave the policy learning to the supervision stage.

Section Title: Supervision
  Supervision In this stage, we imitate the uniformly near-optimal policy, UNOP (Def 5). Although we have no access to the UNOP, we can approximate the state-action distribution from UNOP by collecting the near-optimal trajectories only. The near-optimal samples are constructed online and we are not given any expert demonstration or expert policy beforehand. This step provides a sample-efficient approach to conduct exploitation, which enjoys the superiority of stability ( Figure 2 ), variance reduction (Corollary 2), and optimality preserving (Theorem 2). The two-stage algorithmic framework can be directly incorporated in RPG and LPG to improve sample efficiency. The implementation of RPG is given in Algorithm 1, and LPG follows the same procedure except for the difference in the loss function. The main requirement of Alg. 1 is on the exploration efficiency and the MDP structure. During the exploration stage, a sufficient amount of the different near-optimal trajectories need to be collected for constructing a representative supervised learning training dataset. Theoretically, this requirement always holds [see Appendix Section 10.9, Lemma 5], while the number of episodes explored could be prohibitively large, which makes this algorithm sample-inefficient. This could be a practical concern of the proposed algorithm. However, according to our extensive empirical observations, we notice that long before the value function based state-of-the-art converges to near-optimal performance, enough amount of near-optimal trajectories are already explored. Therefore, we point out that instead of estimating optimal action value functions and then choosing action greedily, using value function to facilitate the exploration and imitating UNOP is a more sample-efficient approach. As illustrated in  Figure 1 , value based methods with off-policy learning, bootstrapping, and function approximation could lead to a divergent optimization ( Sutton & Barto, 2018 , Chap. 11). In contrast to resolving the instability, we circumvent this issue via constructing a stationary target using the samples from (near)-optimal trajectories, and perform imitation learning. This two-stage approach can avoid the extensive exploration of the suboptimal state-action space and reduce the substantial number of samples needed for estimating optimal action values. In the MDP where we have a high probability of hitting the near-optimal trajectories (such as PONG), the supervision stage can further facilitate the exploration. It should be emphasized that our work focuses on improving the sample-efficiency through more effective exploitation, rather than developing novel exploration method. Please refer to the Appendix Section 10.11 for more discussion on exploration efficiency.

Section Title: EXPERIMENTAL RESULTS
  EXPERIMENTAL RESULTS To evaluate the sample-efficiency of Ranking Policy Gradient (RPG), we focus on Atari 2600 games in OpenAI gym  Bellemare et al. (2013) ;  Brockman et al. (2016) , without randomly repeating the previous action. We compare our method with the state-of-the-art baselines including DQN Mnih et al. (2015),  C51 Bellemare et al. (2017) ,  IQN Dabney et al. (2018) , RAINBOW Hessel et al. (2017), and self-imitation learning (SIL)  Oh et al. (2018) . For reproducibility, we use the implementation provided in Dopamine framework 1  Castro et al. (2018)  for all baselines and proposed methods, except Published as a conference paper at ICLR 2020 for SIL using the official implementation 2 . Following the standard evaluation protocol  Oh et al. (2018) ; Hessel et al. (2017);  Dabney et al. (2018) ;  Bellemare et al. (2017) , we report the training performance of all baselines as the increase of interactions with the environment, or proportionally the number of training iterations. We run the algorithms with five random seeds and report the average rewards with 95% confidence intervals. The implementation details of the proposed RPG and its variants are given as follows 3 : EPG: EPG is the stochastic listwise policy gradient (see Appendix Eq (18)) incorporated with the proposed off-policy learning. More concretely, we apply trajectory reward shaping (TRS, Def 3) to all trajectories encountered during exploration and train vanilla policy gradient using the off-policy samples. This is equivalent to minimizing the cross-entropy loss (see Appendix Eq (69)) over the near-optimal trajectories. LPG: LPG is the deterministic listwise policy gradient with the proposed off-policy learning. The only difference between EPG and LPG is that LPG chooses action deterministically (see Appendix Eq (17)) during evaluation. RPG: RPG explores the environment using a separate EPG agent in PONG and IQN in other games. Then RPG conducts supervised learning by minimizing the hinge loss Eq (6). It is worth noting that the exploration agent (EPG or IQN) can be replaced by any existing exploration method. In our RPG implementation, we collect all trajectories with the trajectory reward no less than the threshold c without eliminating the duplicated trajectories and we empirically found it is a reasonable simplification. More details of hyperparameters are provided in the Appendix Section 10.12.

Section Title: Sample-efficiency
  Sample-efficiency As the results shown in  Figure 2 , our approach, RPG, significantly outperform the state-of-the-art baselines in terms of sample-efficiency at all tasks. Furthermore, RPG not only achieved the most sample-efficient results, but also reached the highest final performance at ROBOTANK, DOUBLEDUNK, PITFALL, and PONG, comparing to any model-free state-of-the-art. In reinforcement learning, the stability of algorithm should be emphasized as an important issue. As we can see from the results, the performance of baselines varies from task to task. There is no single baseline consistently outperforms others. In contrast, due to the reduction from RL to supervised learning, RPG is consistently stable and effective across different environments. In addition to the stability and efficiency, RPG enjoys simplicity at the same time. In the environment PONG, it is surprising that RPG without any complicated exploration method largely surpassed the sophisticated value-function based approaches.

Section Title: ABLATION STUDY
  ABLATION STUDY The effectiveness of pairwise ranking policy and off-policy learning as supervised learning. To get a better understanding of the underlying reasons that RPG is more sample-efficient than DQN variants, we performed ablation studies in the PONG environment by varying the combination of policy functions with the proposed off-policy learning. The results of EPG, LPG, and RPG are shown in the bottom right,  Figure 2 . Recall that EPG and LPG use listwise policy gradient (vanilla policy gradient using softmax as policy function) to conduct exploration, the off-policy learning minimizes the cross-entropy loss Eq (69). In contrast, RPG shares the same exploration method as EPG and LPG while uses pairwise ranking policy Eq (2) in off-policy learning that minimizes hinge loss Eq (6). We can see that RPG is more sample-efficient than EPG/LPG. We also compared the most advanced on-policy method Proximal Policy Optimization (PPO)  Schulman et al. (2017)  with EPG, LPG, and RPG. The proposed off-policy learning largely surpassed the best on-policy method. Therefore, we conclude that off-policy as supervised learning contributes to the sample-efficiency substantially, while pairwise ranking policy can further accelerate the learning. In addition, we compare RPG to off-policy policy gradient approaches: ACER Wang et al. (2016) and self-imitation learning  Oh et al. (2018) . As the results shown, the proposed off-policy learning framework is more sample-efficient than the state-of-the-art off-policy policy gradient approaches. The optimality-efficiency trade-off. As reported in  Figure 3 , we empirically demonstrated the trade-off between the sample-efficiency and optimality, which is controlled by the trajectory reward threshold (as defined in Def 3). The higher value of trajectory reward threshold suggests we have higher requirement on defining near-optimal trajectory. This will increase the difficulty of collecting near-optimal samples during exploration, while it ensures a better final performance. These experimental results also justified that RPG is also effective in the absence of prior knowledge on trajectory reward threshold, with a mild cost on introducing an additional tuning parameter.

Section Title: CONCLUSIONS
  CONCLUSIONS In this work, we introduced ranking policy gradient (RPG) methods that, for the first time, resolve RL problem from a ranking perspective. Furthermore, towards the sample-efficient RL, we propose an off-policy learning framework that allows RL agents to be trained in a supervised learning paradigm. The off-policy learning framework uses generalized policy iteration for exploration and exploit the stableness of supervised learning for policy learning, which accomplishes the unbiasedness, variance reduction, off-policy learning, and sample efficiency at the same time. Last but not least, empirical results show that RPG achieves superior performance as compared to the state-of-the-art.

Section Title: ACKNOWLEDGEMENT
  ACKNOWLEDGEMENT

```
