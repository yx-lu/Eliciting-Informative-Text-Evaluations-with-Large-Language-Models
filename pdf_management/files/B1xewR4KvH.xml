<article article-type="research-article"><front><article-meta><title-group><article-title>MANIFOLD FORESTS: CLOSING THE GAP ON NEURAL NET-</article-title></title-group><abstract><p>Decision forests (DF), in particular random forests and gradient boosting trees, have demonstrated state-of-the-art accuracy compared to other methods in many supervised learning scenarios. In particular, DFs dominate other methods in tabular data, that is, when the feature space is unstructured, so that the signal is invariant to permuting feature indices. However, in structured data lying on a manifold-such as images, text, and speech-neural nets (NN) tend to outperform DFs. We conjecture that at least part of the reason for this is that the input to NN is not simply the feature magnitudes, but also their indices (for example, the convolution operation uses "feature locality"). In contrast, na&#239;ve DF implementations fail to explicitly consider feature indices. A recently proposed DF approach demonstrates that DFs, for each node, implicitly sample a random matrix from some specific distribution. Here, we build on that to show that one can choose distributions in a manifold aware fashion. For example, for image classification, rather than randomly selecting pixels, one can randomly select contiguous patches. We demonstrate the empirical performance of data living on three different manifolds: images, time-series, and a torus. In all three cases, our Manifold Forest (MO R F) algorithm empirically dominates other state-of-the-art approaches that ignore feature space structure, achieving a lower classification error on all sample sizes. This dominance extends to the MNIST data set as well. Moreover, both training and test time is significantly faster for manifold forests as compared to deep nets. This approach, therefore, has promise to enable DFs and other machine learning methods to close the gap with deep nets on manifold-valued data.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Decision forests, including random forests and gradient boosting trees, have solidified themselves in the past couple decades as a powerful ensemble learning method in supervised settings (<xref ref-type="bibr" rid="b5">Fern&#225;ndez-Delgado et al., 2014</xref>; <xref ref-type="bibr" rid="b2">Caruana &amp; Niculescu-Mizil, 2006</xref>), including both classification and regression (<xref ref-type="bibr" rid="b7">Hastie et al., 2001</xref>). In classification, each forest is a collection of decision trees whose individual classifications of a data point are aggregated together using majority vote. One of the strengths of this approach is that each decision tree need only perform better than chance for the forest to be a strong learner, given a few assumptions (<xref ref-type="bibr" rid="b14">Schapire, 1990</xref>; <xref ref-type="bibr" rid="b0">Biau et al., 2008</xref>). Additionally, decision trees are relatively interpretable because they can provide an understanding of which features are most important for correct classification (<xref ref-type="bibr" rid="b1">Breiman, 2001</xref>). Breiman originally proposed decision trees that partition the data set using hyperplanes aligned to feature axes (<xref ref-type="bibr" rid="b1">Breiman, 2001</xref>). Yet, this limits the flexibility of the forest and requires deep trees to classify some data sets, leading to overfitting. He also suggested that algorithms which partition based on sparse linear combinations of the coordinate axes can improve performance (<xref ref-type="bibr" rid="b1">Breiman, 2001</xref>). More recently, Sparse Projection Oblique Randomer Forest (SP O R F), partitions a random projection of the data and has shown impressive improvement over other methods (<xref ref-type="bibr" rid="b16">Tomita et al., 2015</xref>).</p><p>Yet random forests and other machine learning algorithms frequently operate in a tabular setting, viewing an observation x = (x 1 , . . . , x p ) T &#8712; R p as an unstructured feature vector. In doing so, they neglect the indices in settings where the indices encode additional information. For structured data, e.g. images or time series, traditional decision forests are not able to incorporate known continuity between features to learn new features.</p><p>For decision forests to utilize known local structure in data, new features encoding this information must be manually constructed. Prior research has extended random forests to a variety of computer vision tasks (<xref ref-type="bibr" rid="b9">Lepetit et al., 2005</xref>; <xref ref-type="bibr" rid="b6">Gall et al., 2011</xref>; <xref ref-type="bibr" rid="b1">Bosch et al., 2007</xref>; <xref ref-type="bibr" rid="b4">Shotton et al., 2011</xref>) and augmented random forests with structured pixel label information (<xref ref-type="bibr" rid="b8">Kontschieder et al., 2011</xref>). Yet these methods either generate features a priori from individual pixels, and thus do not take advantage of the local topology, or lack the flexibility to learn relevant patches. Decision forests have been used to learn distance metrics on unknown manifolds (<xref ref-type="bibr" rid="b4">Criminisi et al., 2012</xref>), but such manifold forest algorithms are unsupervised and aim to learn a low dimensional representation of the data.</p><p>Inspired by SP O R F, we propose a projection distribution that takes into account continuity between neighboring features while incorporating enough randomness to learn relevant projections. At each node in the decision tree, sets of random spatially contiguous features are randomly selected using knowledge of the underlying manifold. Summing the intensities of the sampled features yields a set of projections which can then be evaluated to partition the observations. We describe this proposed classification algorithm, Manifold Forests (MO R F) in detail and show its effectiveness in three simulation settings as compared to common classification algorithms. Furthermore, the optimized and parallelizable open source implementation of MO R F in R and Python is available. This addition makes for an effective and flexible learner across a wide range of manifold structures.</p></sec><sec><title>BACKGROUND AND RELATED WORK</title></sec><sec><title>CLASSIFICATION</title><p>In the two-class classification setting, there is a data set D = {(x i , y i )} n i=1 of n pairs (x i , y i ) drawn from an unknown distribution F XY where x i &#8712; X &#8834; R p and y i &#8712; Y = {0, 1}. Our goal is to train a classifier h(x; D n ) : X &#215;(X &#215;Y) n &#8594; Y based on our observations that generalizes to correctly predict the class of an observed x. The performance of this classifier is evaluated via the 0-1 Loss function L(h(x), y) = I[h(x) = y] to find the optimal classifier h * = argmin h E[L(h(x), y)], which minimizes the probability of an incorrect classification.</p></sec><sec><title>RANDOM FORESTS</title><p>Originally popularized by Breiman, the random forest (RF) classifier is empirically very effective (<xref ref-type="bibr" rid="b5">Fern&#225;ndez-Delgado et al., 2014</xref>) while maintaining strong theoretical guarantees (<xref ref-type="bibr" rid="b1">Breiman, 2001</xref>). A random forest is an ensemble of decision trees whose individual classifications of a data point are aggregated together using majority vote. Each decision tree consists of split nodes and leaf nodes. A split node is associated with a subset of the data S = {(x i , y i )} &#8838; D and splits into two child nodes, each associated with a binary partition of S. Let e j &#8712; R p denote a unit vector in the standard basis (that is, a vector with a single one and the rest of the entries are zero) and &#964; a threshold value. Then S is partitioned into two subsets given the pair &#952; j = {e j , &#964; }.</p><p>To choose the partition, the optimal &#952; * = (e * j , &#964; * ) pair is selected via a greedy search from among a set of d randomly selected standard basis vectors e j . The selected partition is that which maximizes some measure of information gain. A typical measure is a decrease in impurity, calculated by the Gini impurity score I(S), of the resulting partitions (<xref ref-type="bibr" rid="b7">Hastie et al., 2001</xref>). Letp k = 1 |S| yi&#8712;S I[y i = k] be the fraction of elements of class k in partition S, then the optimal split is found as</p><p>A leaf node is created once the partition reaches a stopping criterion, typically either falling below an impurity score threshold or a minimum number of observations (<xref ref-type="bibr" rid="b7">Hastie et al., 2001</xref>). The leaf nodes of the tree form a disjoint partition of the feature space in which each partition of observations S k is assigned a class label c * k corresponding to the class majority.</p><p>A decision tree classifies a new observation by assigning it the class of the partition into which the observation falls. The forest averages the classifications over all decision trees to make the final classification (<xref ref-type="bibr" rid="b7">Hastie et al., 2001</xref>). For good performance of the ensemble and strong theoretical guarantees, the individual decision trees must be relatively uncorrelated from one another. Breiman's random forest algorithm does this in two ways:</p><p>1. At every node in the decision tree, the optimal split is determined over a random subset d of the total collection of features p.</p><p>2. Each tree is trained on a randomly bootstrapped sample of data points D &#8834; D from the full training data set. Applying these techniques means that random forests do not overfit and lowers the upper bound of the generalization error (<xref ref-type="bibr" rid="b1">Breiman, 2001</xref>).</p></sec><sec><title>SPARSE PROJECTION OBLIQUE RANDOMER FORESTS</title><p>SP O R F is a recent modification to random forest that has shown improvement over other versions (<xref ref-type="bibr" rid="b16">Tomita et al., 2015</xref>; <xref ref-type="bibr" rid="b16">Tomita et al., 2017</xref>). Recall that RF split nodes partition data along the coordinate axes by comparing the projection e T j x i of observation x i on standard basis e j to a threshold value &#964; . SP O R F generalizes the set of possible projections, allowing for the data to be partitioned along axes specified by any sparse vector a.</p><p>Rather than partitioning the data solely along the coordinate axes (i.e. the standard basis), SP O R F creates partitions along axes specified by sparse vectors. In other words, let the dictionary A be the set of atoms {a}, each atom a p-dimensional vector defining a possible projection a T x i . In axis-aligned forests, A is the set of standard basis vectors {e j }. In SP O R F, the dictionary D can be much larger, because it includes, for example, all 2-sparse vectors. At each split node, SP O R F samples d atoms from D according to a specified distribution. By default, each of the d atoms are randomly generated with a sparsity level drawn from a Poisson distribution with a specified rate &#955;. Then, each of the non-zero elements are uniformly randomly assigned either +1 or &#8722;1. Note that the size of the dictionary for SP O R F is 3 p (because each of the p elements could be &#8722;1, 0, or +1), although the atoms are sampled from a distribution heavily skewed towards sparsity.</p></sec><sec><title>METHODS</title></sec><sec><title>RANDOM PROJECTION FORESTS ON MANIFOLDS</title><p>In the structured setting, the dictionary of projection vectors A = {a} is modified to take advantage of the underlying manifold on which the data lies. We term this method the Manifold Forest (MO R F). Each atom a projects an observation to a real number and is designed with respect to prior knowledge of the data manifold. Nonzero elements of a effectively select and weight features. Since the feature space is structured, each element of a maps to a location on the underlying manifold. Thus, patterns of contiguous points on the manifold define the atoms of A; the distribution of those patterns yields a distribution over the atoms. At each node in the decision tree, MO R F samples d atoms, yielding d new features per observation. MO R F proceeds just like SP O R F by optimizing the best split according to the Gini index. Algorithm pseudocode, essentially equivalent to that of SP O R F , can be found in the Appendix.</p><p>In the case of two-dimensional arrays, such as images, an observation x i &#8712; R p is a vectorized representation of a data-matrix X i &#8712; R W &#215;H . To capture the relevance of neighboring pixels, MO R F creates projections by summing the intensities of pixels in rectangular patches. Thus the atoms of A are the vectorized representations of these rectangular patches.</p><p>A rectangular patch is fully parameterized by the location of its upper-left corner (u, v), its height h, and width w. To generate a patch, first the index of the upper left corner is uniformly sampled. Then its height and width are independently sampled from separate uniform distributions. MO R F hyperparameters determine the minimum and maximum heights heights {h min , h max } and widths {w min , w max }, respectively, to sample from. Let unif {&#945;, &#946;} denote the discrete uniform distribution. An atom a is sampled as follows. Note that the patch cannot exceed the data-matrix boundaries.</p><p>The vectorized atom a yields a projection of the data a T x i , effectively selecting and summing pixel intensities in the sampled rectangular patch.</p><p>By constructing features in this way, MO R F learns low-level features in the structured data, such as edges or corners in images. The forest can therefore learn the features that best distinguish a class. The structure of these atoms is flexible and task dependent. In the case of data lying on a cyclic manifold, the atoms can wrap-around borders to capture the added continuity. Atoms can also be used in one-dimensional arrays, such as univariate time-series data, in which case h min = h max = 1</p></sec><sec><title>FEATURE IMPORTANCE</title><p>One of the benefits to decision trees is that their results are fairly interpretable in that they allow for estimation of the relative importance of each feature. Many approaches have been suggested (<xref ref-type="bibr" rid="b1">Breiman, 2001</xref>; <xref ref-type="bibr" rid="b11">Lundberg &amp; Lee, 2017</xref>) and here a projection forest specific metric is used in which the number of times a given feature was used in projections across the ensemble of decision trees is counted. A decision tree T is composed of many nodes k, each one associated with an atom a * k and threshold that partition the feature space according to the projection a * k &#183; x i . Thus, the indices corresponding to nonzero elements of a * k indicate important features used in the projection. For each feature j, the number of times &#960; j it is used in a projection, across all split nodes and decision trees, is counted.</p><p>These normalized counts represent the relative importance of each feature in making a correct classification. Such a method applies to both SP O R F and MO R F , although different results between them would be expected due to different projection distributions yielding different hyperplanes.</p></sec><sec><title>SIMULATION RESULTS</title><p>To test MO R F , we evaluate its performance in three simulation settings as compared to logistic regression (Log. Reg), linear support vector machine (Lin. SVM), support vector machine with a radial basis function kernel (SVM), k-nearest neighbors (kNN), random forest (RF), Multi-layer Perceptron (MLP), and SP O R F (SPORF). For each experiment, we used our open source implementation of MO R F and that of SP O R F . All decision forest algorithms used 100 decision trees on the simulations. Each of the other classifiers were run from the Scikit-learn Python package (<xref ref-type="bibr" rid="b13">Pedregosa et al., 2011</xref>) with default parameters. Additionally, we tested against a Convolutional Neural Network (CNN) built using PyTorch (Paszke et al., 2017) with two convolution layers, ReLU activations, and maxpooling, followed by dropout and two densely connected layers. The CNN results were averaged over 5 runs for the simulations and training was stopped early if the loss plateaued.</p></sec><sec><title>SIMULATION SETTINGS</title><p>Experiment (A) is a non-Euclidean example inspired by <xref ref-type="bibr" rid="b18">Younes (2018)</xref>. Each observation is a discretization of a circle into 100 features with two non-adjacent segments of 1's in two differing patterns: class 1 features two segments of length five while class 2 features one segment of length four and one of length six. MO R F chose one-dimensional rectangles in this setting as the observations were one-dimensional in nature. These projection patches had a width between one and fifteen pixels and each split node of SP O R F and MO R F considered 40 random projections. Figure 1(A) shows examples from the two classes and classification results across various sample sizes.</p><p>In experiment (B) consists of a simple 28 &#215; 28 binary image classification problem. Images in class 0 contain randomly sized and spaced horizontal bars while those in class 1 contain randomly sized and spaced vertical bars. For each sampled image, k &#8764; P oisson(&#955; = 10) bars were distributed among the rows or columns, depending on the class. The distributions of the two classes are identical if a 90 degree rotation is applied to one of the classes. Projection patches were between one and four pixels in both width and height and each split node of SP O R F and MO R F considered 28 random projections. Figure 1(B) shows examples from the two classes and classification results across various sample sizes.</p><p>Experiment (C) is a signal classification problem. One class consists of 100 values of Gaussian noise while the second class has an added exponentially decaying unit step beginning at time 20.</p><p>Projection patches were 1D with a width between one and five timesteps. Each split node of SP O R F and MO R F considered the default number of random projections, the square root of the number of features. Figure 1(C) shows examples from the two classes and classification results across various sample sizes. In all three simulation settings, MO R F outperforms all other classifiers, doing especially better at low sample sizes, except the CNN for which there is no clear winner. The performance of MO R F is particularly good in the discretized circle simulation for which most other classifiers perform at chance levels. MO R F also performs well in the signal classification problem although all the classifiers are close in performance. This may be because the exponential signal is prevalent throughout most of the time-steps and so perfect continuity is less relevant.</p></sec><sec><title>RUN TIME</title><p>All experiments were run on a single core CPU. MO R F has train and test times on par with those of SP O R F and RF and so is not particularly more computationally intensive to run. The CNN, however, took noticeably longer to run-especially in terms of training, but also in testing-in two of the three simulations. Thus its strong performance in those settings comes at an added computational cost, a typical issue for deep learning methods (<xref ref-type="bibr" rid="b10">Livni et al., 2014</xref>).</p></sec><sec><title>REAL DATA RESULTS</title></sec><sec><title>CLASSIFICATION ACCURACY</title><p>MO R F's performance was evaluated on the MNIST dataset, a collection of handwritten digits stored in 28 by 28 square images (<xref ref-type="bibr" rid="b9">Lecun et al.</xref>), and compared to the algorithms used in the simulations. 10,000 images were held out for testing and a subset of the remaining images were used for training. The results are displayed in <xref ref-type="fig" rid="fig_2">Figure 3</xref>. All three forest algorithms were composed of 500 decision trees and MO R F was restricted to use patches up to only 3 pixels in height and width. MO R F showed an improvement over the other algorithms, especially for smaller sample sizes. Thus, even this trivial modification can improve performance by several percentage points. Specifically, MO R F achieved a better (lower) classification error than all other algorithms besides CNNs for all sample sizes on this real data problem.</p></sec><sec><title>FEATURE IMPORTANCE</title><p>To evaluate the capability of MO R F to identify importance features in manifold-valued data as compared to SP O R F and RF. All methods were run on a subset of the MNIST dataset: we only used threes and fives, 100 images from each class. The feature importance of each pixel is shown in <xref ref-type="fig" rid="fig_3">Figure 4</xref>. MO R F visibly results in a smoother pixel importance, a result most likely from the continuity of neighboring pixels in selected projections. Although <xref ref-type="bibr" rid="b16">Tomita et al. (2015)</xref> demonstrated empirical improvement of SP O R F over RF on the MNIST data, its projection distribution yields scattered importance of unimportant background pixels as compared to RF. Since projections in SP O R F have no continuity constraint, those that select high importance pixels will also select pixels of low importance by chance. This may be a nonissue asymptotically, but is a relevant problem in low sample size settings. MO R F , however, shows little or no importance of these background pixels by virtue of the modified projection distribution.</p></sec><sec><title>DISCUSSION</title><p>The success of sparse oblique projections in decision forests has opened up many possible ways to improve axis-aligned decision forests (including random forests and gradient boosting trees) by way of specialized projection distributions. Traditional decision forests have already been applied to structured data, using predefined features to classify images or pixels. Decision forest algorithms like that implemented in the Microsoft Kinect showed great success but ignore pixel continuity and specialize for a specific data modality, namely images (<xref ref-type="bibr" rid="b4">Shotton et al., 2011</xref>).</p><p>We expanded upon sparse oblique projections and introduced a structural projection distribution that uses prior knowledge of the topology of a feature space. The open source implementation of SP O R F has allowed for a relatively easy implementation of MO R F, creating a flexible classification method for a variety of data modalities. We showed in various simulated settings that appropriate domain knowledge can improve the projection distribution to yield impressive results that challenge the strength of deep learning techniques on manifold-valued data. On the MNIST data set MO R Fshowed modest improvements over the other algorithms besides CNNs and smoother importance plots than the other decision forest algorithms. This is in spite of the the data set's low resolution images which are harder for the modified projection distributions to take advantage of.</p><p>Research into other, task-specific convolution kernels may lead to improved results in real-world computer vision tasks. Such structured projection distributions, while incorporated into SP O R F here, may also be incorporated into other state of the art algorithms such as XGBO O S T (<xref ref-type="bibr" rid="b3">Chen &amp; Guestrin, 2016</xref>).</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>MO R F outperforms other algorithms in three two-class classification settings. Upper row shows examples of simulated data from each setting and class. Lower row shows misclassification rate in each setting, tested on 10,000 test samples. (A) Two segments in a discretized circle. Segment lengths vary by class. (B) Image setting with uniformly distributed horizontal or vertical bars. (C) White noise (class 0) vs. exponentially decaying unit impulse plus white noise (class 1).</p></caption><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Algorithm train times (above) and test times (below). MO R F runtime is not particularly costly and well below CNN runtime in most examples.</p></caption><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>MO R F improves classification accuracy over all other non-CNN algorithms for all sample sizes, especially in small sample sizes.</p></caption><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Averages of images in the two classes and their difference (above). Feature importance from MO R F (bottom right) shows less noise than SP O R F (bottom middle) and is smoother than RF (bottom left).</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Consistency of random forests and other averaging classifiers</article-title><source>J. Mach. Learn. Res.</source><year>2008</year><volume>9</volume><fpage>2015</fpage><lpage>2033</lpage><person-group person-group-type="author"><name><surname>References G&#233;rard Biau</surname><given-names>Luc</given-names></name><name><surname>Devroye</surname><given-names>G&#225;bor</given-names></name><name><surname>Lugosi</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Image classification using random forests and ferns</article-title><source>Machine Learning</source><year>2007</year><volume>45</volume><issue>1</issue><fpage>1</fpage><lpage>8</lpage><person-group person-group-type="author"><name><surname>Bosch</surname><given-names>A</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name><name><surname>Munoz</surname><given-names>X</given-names></name><name><surname>Breiman</surname><given-names>Leo</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><source>Proceedings of the 23rd International Conference on Machine Learning, ICML '06</source><year>2006</year><fpage>161</fpage><lpage>168</lpage><person-group person-group-type="author"><name><surname>Caruana</surname><given-names>Rich</given-names></name><name><surname>Niculescu-Mizil</surname><given-names>Alexandru</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><source>Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source><year>2016</year><fpage>785</fpage><lpage>794</lpage><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Tianqi</given-names></name><name><surname>Guestrin</surname><given-names>Carlos</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning</article-title><source>Found. Trends. Comput. Graph. Vis.</source><year>2012</year><volume>7</volume><issue>2&amp;#8211</issue><fpage>81</fpage><lpage>227</lpage><person-group person-group-type="author"><name><surname>Criminisi</surname><given-names>Antonio</given-names></name><name><surname>Shotton</surname><given-names>Jamie</given-names></name><name><surname>Konukoglu</surname><given-names>Ender</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Do we need hundreds of classifiers to solve real world classification problems?</article-title><source>Journal of Machine Learning Research</source><year>2014</year><volume>15</volume><fpage>3133</fpage><lpage>3181</lpage><person-group person-group-type="author"><name><surname>Fern&#225;ndez-Delgado</surname><given-names>Manuel</given-names></name><name><surname>Cernadas</surname><given-names>Eva</given-names></name><name><surname>Barro</surname><given-names>Sen&#233;n</given-names></name><name><surname>Amorim</surname><given-names>Dinani</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Hough forests for object detection, tracking, and action recognition</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2011</year><volume>33</volume><issue>11</issue><fpage>2188</fpage><lpage>2202</lpage><person-group person-group-type="author"><name><surname>Gall</surname><given-names>J</given-names></name><name><surname>Yao</surname><given-names>A</given-names></name><name><surname>Razavi</surname><given-names>N</given-names></name><name><surname>Van Gool</surname><given-names>L</given-names></name><name><surname>Lempitsky</surname><given-names>V</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><source>Springer Series in Statistics</source><year>2001</year><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>Trevor</given-names></name><name><surname>Tibshirani</surname><given-names>Robert</given-names></name><name><surname>Friedman</surname><given-names>Jerome</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Structured class-labels in random forests for semantic image labelling</article-title><source>International Conference on Computer Vision</source><year>2011</year><fpage>2190</fpage><lpage>2197</lpage><person-group person-group-type="author"><name><surname>Kontschieder</surname><given-names>P</given-names></name><name><surname>Bul&#242;</surname><given-names>S R</given-names></name><name><surname>Bischof</surname><given-names>H</given-names></name><name><surname>Pelillo</surname><given-names>M</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>The MNIST Database of Handwritten Digits. V. Lepetit, P. Lagger, and P. Fua. Randomized trees for real-time keypoint recognition</article-title><source>IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)</source><year>2005</year><volume>2</volume><fpage>775</fpage><lpage>781</lpage><person-group person-group-type="author"><name><surname>Lecun</surname><given-names>Yann</given-names></name><name><surname>Cortes</surname><given-names>Corinna</given-names></name><name><surname>Burges</surname><given-names>Christopher J C</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><year>2014</year><fpage>855</fpage><lpage>863</lpage><person-group person-group-type="author"><name><surname>Livni</surname><given-names>Roi</given-names></name><name><surname>Shalev-Shwartz</surname><given-names>Shai</given-names></name><name><surname>Shamir</surname><given-names>Ohad</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>A unified approach to interpreting model predictions</article-title><source>ArXiv</source><year>2017</year><person-group person-group-type="author"><name><surname>Lundberg</surname><given-names>Scott</given-names></name><name><surname>Lee</surname><given-names>Su-In</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Automatic differentiation in pytorch</article-title><source>NIPS-W</source><year>2017</year><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>Adam</given-names></name><name><surname>Gross</surname><given-names>Sam</given-names></name><name><surname>Chintala</surname><given-names>Soumith</given-names></name><name><surname>Chanan</surname><given-names>Gregory</given-names></name><name><surname>Yang</surname><given-names>Edward</given-names></name><name><surname>Devito</surname><given-names>Zachary</given-names></name><name><surname>Lin</surname><given-names>Zeming</given-names></name><name><surname>Desmaison</surname><given-names>Alban</given-names></name><name><surname>Antiga</surname><given-names>Luca</given-names></name><name><surname>Lerer</surname><given-names>Adam</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Scikit-learn: Machine learning in Python</article-title><source>Journal of Machine Learning Research</source><year>2011</year><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name><name><surname>Vanderplas</surname><given-names>J</given-names></name><name><surname>Passos</surname><given-names>A</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Brucher</surname><given-names>M</given-names></name><name><surname>Perrot</surname><given-names>M</given-names></name><name><surname>Duchesnay</surname><given-names>E</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>The strength of weak learnability</article-title><source>Mach. Learn.</source><year>1990</year><volume>5</volume><issue>2</issue><fpage>197</fpage><lpage>227</lpage><person-group person-group-type="author"><name><surname>Robert</surname><given-names>E</given-names></name><name><surname>Schapire</surname><given-names /></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Real-time human pose recognition in parts from single depth images</article-title><source>CVPR</source><year>2011</year><fpage>1297</fpage><lpage>1304</lpage><person-group person-group-type="author"><name><surname>Shotton</surname><given-names>J</given-names></name><name><surname>Fitzgibbon</surname><given-names>A</given-names></name><name><surname>Cook</surname><given-names>M</given-names></name><name><surname>Sharp</surname><given-names>T</given-names></name><name><surname>Finocchio</surname><given-names>M</given-names></name><name><surname>Moore</surname><given-names>R</given-names></name><name><surname>Kipman</surname><given-names>A</given-names></name><name><surname>Blake</surname><given-names>A</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>Random Projection Forests</article-title><source>arXiv e-prints, art. arXiv:1506.03410</source><year>2015</year><person-group person-group-type="author"><name><surname>Tyler</surname><given-names>M</given-names></name><name><surname>Tomita</surname><given-names>James</given-names></name><name><surname>Browne</surname><given-names>Cencheng</given-names></name><name><surname>Shen</surname><given-names>Jesse L</given-names></name><name><surname>Patsolic</surname><given-names>Jason</given-names></name><name><surname>Yim</surname><given-names>Carey E</given-names></name><name><surname>Priebe</surname><given-names>Randal</given-names></name><name><surname>Burns</surname><given-names>Mauro</given-names></name><name><surname>Maggioni</surname><given-names>Joshua T</given-names></name><name><surname>Vogelstein</surname><given-names /></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Roflmao: Robust oblique forests with linear matrix operations</article-title><source>Proceedings of the 2017 SIAM International Conference on Data Mining</source><year>2017</year><fpage>498</fpage><lpage>506</lpage><person-group person-group-type="author"><name><surname>Tyler</surname><given-names>M</given-names></name><name><surname>Tomita</surname><given-names>Mauro</given-names></name><name><surname>Maggioni</surname><given-names>Joshua T</given-names></name><name><surname>Vogelstein</surname><given-names /></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Diffeomorphic learning</article-title><source>ArXiv, abs/1806.01240</source><year>2018</year><person-group person-group-type="author"><name><surname>Younes</surname><given-names>Laurent</given-names></name></person-group></element-citation></ref></ref-list></back></article>