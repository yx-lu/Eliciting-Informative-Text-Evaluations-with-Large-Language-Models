Title:
```
Under review as a conference paper at ICLR 2020 CONTEXTUAL INVERSE REINFORCEMENT LEARNING
```
Abstract:
```
We consider the Inverse Reinforcement Learning problem in Contextual Markov Decision Processes. In this setting, the reward, which is unknown to the agent, is a function of a static parameter referred to as the context. There is also an "expert" who knows this mapping and acts according to the optimal policy for each context. The goal of the agent is to learn the expert's mapping by observing demonstrations. We define an optimization problem for finding this mapping and show that when it is linear, the problem is convex. We present and analyze the sample complexity of three algorithms for solving this problem: the mirrored descent algorithm, evolution strategies, and the ellipsoid method. We also extend the first two methods to work with general reward functions, e.g., deep neural networks, but without the theoretical guarantees. Finally, we compare the different techniques empirically in driving simulation and a medical treatment regime.
```

Figures/Tables Captions:
```
Figure 1: The COIRL framework (left): a context vector parametrizes the environment. For each context, the expert uses the true mapping from contexts to rewards, W * , and provides demonstrations. The agent learns an estimation of this mappingŴ and acts optimally with respect to it.
Figure 2: Experimental results in the autonomous driving simulation with a linear mapping (a & b) and a nonlinear mapping (c & d) Nonlinear: For the nonlinear task, we consider two reward coefficient vectors r 1 and r 2 , and define the mapping by f * (c) = r 1 if ||c|| ∞ ≥ 0.55, and r 2 otherwise - an illustration is provided in the appendix. In order to learn the nonlinear mapping, we represent f W (c) using a DNN, a multi-layered perceptron, which maps from context to reward vector. DNNs have proven to be capable of extracting meaningful features from complex high-dimensional data, e.g., images - in these scenarios, the linear assumption no longer holds, yet DNNs often overcome such issues. In this setting, the superiority of the descent methods rises; as the linear assumption in the ellipsoid algorithm is not met, it fails to generalize and keeps requiring new demonstrations. We believe these results to be crucial when considering real-life applications, in which the problem is not necessarily linear. Such cases highlight the strength of the descent methods, which, as Fig. 2 shows, are capable of scaling to nonlinear high dimensional mappings.
Figure 3: Experimental results in the dynamic treatment regime with a linear mapping
Figure 4: Experimental results in the dynamic treatment regime with a non-linear mapping of the context vector. We use a DNN to learn the mapping and follow Section 3.1 (PSGD). As seen in Fig. 4, the PSGD algorithm minimizes the loss and achieves a value that is close to that of the expert. In addition, similarly to Fig. 7, accuracy and performance do not necessarily correlate one to another.
Figure 5: Comparison between COIRL and AL on a large MDP
Table 1: Comparison between various approaches.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION We study sequential decision-making in a Contextual Markov Decision Process (CMDP, Hallak et al. (2015)), where the reward, while unknown to the agent, depends on a static parameter referred to as the context. For a concrete example, consider the dynamic treatment regime (Chakraborty & Murphy, 2014). Here, there is a patient and a clinician which acts to improve the patient's health. The context is composed of static information of the patient (such as age and weight); the state is composed of the patient's dynamic measurements (such as heart rate and blood pressure); and the clinician's actions are a set of intervention categories (e.g., infusion). The reward is different for each patient (context), and there is a mapping from the context to the reward. Recent trends in personalized medicine motivate this model - instead of treating the "average patient", patients are separated into different groups for which the medical decisions are tailored (Fig. 1b). For example, in Wesselink et al. (2018), the authors study organ injury, which may occur when a specific measurement (mean arterial pressure) decreases below a certain threshold. They found that this threshold varies across different patient groups (context). In other examples, clinicians set treatment goals for the patients, i.e., they take actions to make the patient measurements reach some pre-determined values. For instance, in acute respiratory distress syndrome (ARDS), clinicians argue that these treatment goals should depend on the static patient information (the context) (Berngard et al., 2016). There are serious issues when trying to manually define a reward signal in real-world tasks. When treating patients with sepsis, for example, the only available signal is the mortality of the patient at the end of the treatment (Komorowski et al., 2018). This signal is sparse, and it is unclear how to manually tweak the reward to maximize the patient's health condition (Leike et al., 2017; Raghu et al., 2017; Lee et al., 2019). To address these issues, we propose the Contextual Inverse Reinforcement Learning (COIRL) framework. Similarly to Inverse Reinforcement Learning (Ng & Russell, 2000, IRL), we focus on trying to infer the mapping from contexts to rewards by observing experts. The main challenge in our problem is that for each context there is a different reward, hence, a different optimal policy for each context. Therefore, Apprenticeship Learning algorithms (Abbeel & Ng, 2004; Syed & Schapire, 2008) that try to mimic the expert cannot be used and, instead, we focus on directly learning the mapping.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 In particular, our main contributions are: 1. We formulate COIRL with a linear mapping as a convex optimization problem. 2. We propose and analyze the sample complexity of three algorithms for COIRL: the mirrored descent alg. (MDA), evolution strategies (ES), and the ellipsoid method. 3. For nonlinear mappings, we implement a deep learning version for MDA and ES (without theoretical guarantees). 4. We compare these methods empirically on two frameworks: an autonomous driving simulator (Abbeel & Ng, 2004) and a dynamic treatment regime (Komorowski et al., 2018).

Section Title: PRELIMINARIES
  PRELIMINARIES Contextual MDPs: A Markov Decision Process (Puterman, 1994, MDP) is defined by the tuple (S, A, P, ξ, R, γ) where S is a finite state space, A a finite action space, P : S × S × A → [0, 1] the transition kernel, ξ the initial state distribution, R : S → R the reward function and γ ∈ [0, 1) is the discount factor. A Contextual MDP (Hallak et al., 2015, CMDP) is an extension of an MDP, and is defined by (C, S, A, M, γ) where C is the context space, and M is a mapping from contexts c ∈ C to MDPs: M(c) = (S, A, P, R c , ξ, γ). In addition, each state is associated with a feature vector φ : S → [0, 1] k . Note that P and ξ are not context dependent. We consider a setting in which the reward for context c is a linear combination of the state features: R * c (s) = f * (c) T φ(s). The goal is to approximate f * (c) using a function f W (c), with parameters W . This notation allows us to present our algorithms for any function approximator f W (c), and in particular, a deep neural network (DNN). For the theoretical analysis, we will further assume a linear setting, where f * (c) = c T W * , f W (c) = c T W and that W * is in some convex set W. We assume that c ∈ C = ∆ d−1 , the standard d − 1 dimensional simplex. This assumption makes the contexts bounded (which we use in our proofs), and it also allows a straight-forward expansion to a model in which the transitions are also a linear mapping of the context (Modi et al., 2018). One way of viewing this model is that each row in the mapping W * is a base rewards coefficient vector, and the reward for a specific context is a convex combination of these base rewards. We consider deterministic policies π : S → A which dictate the agent's behaviour at each state. The value of a policy π for reward coefficients vector r is: V π r = E ξ,P,π [ ∞ t=0 γ t R(s t )] = r T µ(π) where µ(π) := E ξ,P,π [ ∞ t=0 γ t φ(s t )] ∈ R k is called the feature expectations of π. For the optimal policy with respect to (w.r.t.) a reward coefficients vector r, we denote the value by V * r . For any context c, π * c denotes the optimal policy w.r.t. reward R * c (s) = f * (c) T φ(s) andπ c (W ) denotes the optimal policy w.r.t. rewardR c (s) = f W (c) T φ(s).

Section Title: Inverse Reinforcement Learning in CMDPs
  Inverse Reinforcement Learning in CMDPs In standard IRL, the goal is to learn a reward which best explains the behavior of an observed expert. The model describing this scenario is the MDP\R - Under review as a conference paper at ICLR 2020 an MDP without a reward function (also commonly called a controlled Markov chain). Similarly, we denote a CMDP without a mapping of context to reward by CMDP\M. The goal in Contextual IRL is to approximate the mapping f * (c) by observing an expert. The expert knows f * (c), and for each context c, can provide a demonstration from π * c .

Section Title: Contextual dynamics
  Contextual dynamics Learning a transition kernel and an initial state distribution that is parametrized by the context is an orthogonal problem to COIRL. Therefore, we focus only on a contextual reward which simplifies our analysis. Existing methods, such as in Modi et al. (2018), can be used to learn the mappings for the transition kernel and initial distribution in a contextual model. In conjunction with the simulation lemma (Kearns & Singh, 2002), these methods can extend our results to the more general CMDP setting.

Section Title: OPTIMIZATION METHODS FOR COIRL
  OPTIMIZATION METHODS FOR COIRL In this section, we propose and analyze optimization algorithms for minimizing the following loss function; Lemma 1 below justifies its use for COIRL. Lemma 1. Loss(W ) satisfies the following properties: (1) ∀W, Loss(W ) ≥ 0, and Loss(W * ) = 0. (2) If Loss(W ) = 0 then ∀c ∈ C, the expert policy π * c is the optimal policy w.r.t. reward c T W. To evaluate the loss, the optimal policyπ c (W ) and its features expectations µ(π c (W )) must be computed for all contexts. For a specific context, findingπ c (W ) can be solved with standard RL methods such as Value or Policy Iteration. Computing µ(π c (W )) is equivalent to policy evaluation (solving linear equations). The challenge is that Eq. (1) is is not differentiable in W . We tackle this problem using two methods for computing descent directions that do not involve differentiation: (i) computing subgradients and (ii) randomly perturbing the loss function. In addition, as the loss is defined in expectation over the contexts, computing it requires to calculate the optimal policy for all contexts. We deal with this issue at the end of Section 3.1. In the special case that f W (c) is a linear function, Eq. (1) is convex. The following Lemma characterizes Eq. (1) in this case. Lemma 2. Let L lin (W ) = E c c T W · µ(π c (W )) − µ(π * c ) . We have that: (1) L lin (W ) is a convex function. (2) g(W ) = E A technical proof (by definition) is provided in the supplementary material. Note that g(W ) ∈ R d×k ; we will sometimes refer to it as a matrix and sometimes as a flattened vector, no confusion will arise. Remark 1. The Lipschitz of L Lin (W ) is related to the simulation lemma (Kearns & Singh, 2002); a small change in the reward results in a small change in the optimal value. Remark 2. As g(W ) is a subgradient of Loss(W ), it can be used to back-propagate DNNs. Clearly, we cannot guarantee convexity (hence no theoretical guarantees), but we can design Loss(W ) to be Lipschitz continuous in W using the methods presented in Cisse et al. (2017); Arjovsky et al. (2017). Remark 3. The subgradient g(W ) is given in expectation over contexts, and in expectation over trajectories (feature expectations). We will later see how to replace it with an unbiased estimate, which can be computed by observing a single expert trajectory for a single context.

Section Title: MIRRORED DESCENT FOR COIRL
  MIRRORED DESCENT FOR COIRL Lemma 2 identifies L Lin (W ) as a convex function and provides a method to compute its subgradients. A standard method for minimizing a convex function over a convex set is the subgradient projection algorithm (Bertsekas, 1997): w t+1 = Proj W {w t − α t g(w t )}, where f (w t ) is a convex function, g(w t ) is a subgradient of f (w t ), and α t the learning rate. W is a convex set, and specifically, we consider the 2 ball (Abbeel & Ng, 2004) and the simplex (Syed & Schapire, 2008) 1 . We focus on Under review as a conference paper at ICLR 2020 We refer the reader to Beck & Teboulle (2003) and Bubeck (2015) for the proof. Next, we provide two MDA instances (see, for example Beck & Teboulle (2003) for derivation) and analyze them for COIRL.

Section Title: Projected subgradient descent (PSGD)
  Projected subgradient descent (PSGD) Exponential Weights (EW): Let W be the standard dk − 1 dimensional simplex. Let ψ(w) = i w(i) log(w(i)). ψ is strongly convex w.r.t. || · || 1 with σ = 1. We get that the associated Bregman divergence is given by D ψ (w 1 , w 2 ) = i w 1 (i) log( w1(i) w2(i) ), also known as the Kullback-Leibler divergence. In addition, D 2 = max x,y∈W D ψ (w 1 , w 2 ) ≤ log(dk) and according to Lemma 2, L = 2 1−γ . Furthermore, the projection onto the simplex w.r.t. to this distance amounts to a simple renormalization w ← w/||w|| 1 . Thus, we get that MDA is equivalent to the exponential weights algorithm and L lin Practical MDA: One of the "miracles" of MDA is its robustness to noise. If we replace g t with an unbiased estimateg t , such that Eg t = g t and E g t ≤ L, we obtain the same convergence results as in Lemma 2 (Robbins & Monro, 1951) (see, for example, (Bubeck, 2015, Theorem 6.1)). Such an unbiased estimate can be obtained in the following manner: (i) sample a context c t , (ii) compute Under review as a conference paper at ICLR 2020 µ(π * c T t wt ), (iii) observe a single expert demonstration τ E i = {s i 0 , a 0 , s i 1 , a 1 , . . . , }, where a i is chosen by the expert policy π * c T t w * (iv) letμ i = t∈[0,...,|τ E i |−1] γ t φ(s i t ) be the accumulated discounted features across the trajectory such that Eμ i = µ(π * c ). The challenge is, that forμ i to be an unbiased estimate of µ(π * c T t w * ), τ E i needs to be of infinite length. There are two ways in which we can tackle this issue. We can either (1) execute the expert trajectory online, and terminate it at each time step with probability 1 − γ (as in (Kakade & Langford, 2002)), or (2) execute a trajectory of length H = 1 1−γ log(1/ H ). The issue with the first approach is that since the trajectory length is unbounded, the estimateμ i cannot be shown to concentrate to µ(π * c ) via Hoeffding type inequalities. Nevertheless, it is possible to obtain a concentration inequality using the fact that the length of each trajectory is bounded in high probability (similar to Zahavy et al.). The second approach can only guarantee that g t − Eg t ≤ H (Syed & Schapire, 2008). Therefore, using the robustness of MDA to adversarial noise (Zinkevich, 2003), we get that MDA converges with an additional error of H , i.e., L lin 1 T T t=1 w t − L lin (w * ) ≤ O 1 √ T + H . While this sampling mechanism comes with the cost of a controlled bias, usually it is more practical, in particular when the trajectories are given as a set demonstrations (offline data).

Section Title: EVOLUTION STRATEGIES FOR COIRL
  EVOLUTION STRATEGIES FOR COIRL To minimize Eq. (1), we also design a derivative free algorithm (Algorithm 2) that is based on Evolution Strategies (Salimans et al., 2017, ES). For convex optimization problems, ES is a gradient- free descent method that is based on computing finite differences (Nesterov & Spokoiny, 2017), whose sample complexity is provided below in Theorem 2. The Theorem is given in terms of the Lipschitz constant, which is upper bounded by 2 √ dk 1−γ (Section 3.1). While this approach has looser upper-bound guarantees compared to MDA (Theorem 1), Nesterov & Spokoiny (2017) observed that in practice, it often outperforms subgradient based methods. Thus, we test this method empirically and compare it with the subgradient method (Section 3.1). ES is also known to perform well in practice, even with nonconvex objectives. Specifically, Salimans et al. (2017) has shown that ES can be used to optimize the parameters of a DNN to solve challenging high dimensional RL tasks like playing Atari. Theorem 2 (ES Convergence Rate (Nesterov & Spokoiny, 2017)). Let L lin (W ) be a non-smooth convex function with Lipschitz constant L, such that ||x 0 − x * || ≤ D, step size of α t = D (dk+4) √ T +1L and ν ≤ 2L √ dk then in T = 4(dk+4) 2 D 2 L 2 2 ES finds a solution which is bounded by E U T −1 [L lin (x T )] − L lin (x * ) ≤ , where U T = {u 0 , . . . , u T } denotes the random variables of the algorithm up to time T andx T = arg min t=1,...,T L lin (x t ).

Section Title: ELLIPSOID ALGORITHMS FOR COIRL
  ELLIPSOID ALGORITHMS FOR COIRL Algorithm 3 Ellipsoid algorithm for COIRL The final algorithm we consider is an el- lipsoid method, introduced to the IRL set- ting by Amin et al. (2017). In this sec- tion we extend it to the contextual set- ting, specifically, we focus on finding a linear mapping W and further assume that W = {W : ||W || ∞ ≤ 1}, and that W * ∈ W. The algorithm maintains an ellipsoid- shaped feasibility set that contains W * . At any step, the current estimation W t of W * is defined as the center of the ellip- soid, and the agent acts optimally w.r.t. this estimation. If the agent performs sub- optimally, the expert provides a demonstration in the form of the optimal feature expectations for c t , µ(π * ct ). The feature expectations are used to generate a linear constraint (hyperplane) on the ellipsoid that is crossing its center. Under this constraint, we construct a new feasibility set that is half of the Under review as a conference paper at ICLR 2020 previous ellipsoid, and still contains W * . For the algorithm to proceed, we compute a new ellipsoid that is the minimum volume enclosing ellipsoid (MVEE) around this "half-ellipsoid" 3 . These updates are guaranteed to gradually reduce the volume of the ellipsoid (a well-known result (Boyd & Barratt, 1991)) until its center is a mapping which induces -optimal policies. Theorem 3 shows that this algorithm achieves a polynomial upper bound on the number of sub-optimal time-steps. Finally, note that in Algorithm 3 we use an underline notation to denote a "flattening" operator for matrices, and to denote a composition of an outer product and the flattening operator. The proofs in this section are provided in the supplementary material, and are adapted from (Amin et al., 2017). Theorem 3. In the linear setting where R * c (s) = c T W * φ(s), for an agent acting according to Algorithm 1, the number of rounds in which the agent is not -optimal is O(d 2 k 2 log( dk (1−γ) )). Remark 4. Note that the ellipsoid method presents a new learning framework, where demonstrations are only provided when the agent performs sub-optimally. Thus, the theoretical results in this section cannot be directly compared with those of the descent methods. We further discuss this in the experiments and discussion sections. Remark 5. The ellipsoid method does not require a distribution over contexts - an adversary may choose them. MDA can also be easily extended to the adversarial setting via known regret bounds on online MDA (Hazan, 2016).

Section Title: Practical ellipsoid algorithm
  Practical ellipsoid algorithm In many real-world scenarios, the expert cannot evaluate the value of the agent's policy and cannot provide its policy or feature expectations. To address these issues, we follow Amin et al. (2017) and consider a relaxed approach, in which the expert evaluates each of the individual actions performed by the agent rather than its policy, and provides finite rollouts instead of a policy or feature expectations (see the supplementary material (Algorithm 4) for pseudo code). We define the expert criterion for providing a demonstration to be Q * c T t W * (s, a) + < V * c T t W * (s) for each state-action pair (s, a) in the agent's trajectory.

Section Title: Near-optimal experts
  Near-optimal experts In addition, we relax the optimality requirement of the expert and instead assume that, for each context c t , the expert acts optimally w.r.t. W * t which is close to W * ; the expert also evaluates the agent w.r.t. this mapping. This allows the agent to learn from different experts, and from non-stationary experts whose judgment and performance slightly vary over time. If a sub-optimal action w.r.t. W * t is played at state s, the expert provides a roll-out of H steps from s to the agent. As this roll-out is a sample of the optimal policy w.r.t. W * t , we aggregate n examples to assure that with high probability, the linear constraint that we use in the ellipsoid algorithm does not exclude W * from the feasibility set. Note that these batches may be constructed across different contexts, different experts, and different states from which the demonstrations start. Theorem 4 below upper bounds the number of sub-optimal actions that Algorithm 4 chooses.

Section Title: EXPERIMENTS
  EXPERIMENTS The simulations in this section include two domains: (1) an autonomous driving simulation (Abbeel & Ng, 2004), that we adapted to the contextual setup and (2) a medical treatment regime, constructed from a data set of expert (clinician) trajectories for treating patients with sepsis 5 . In each of these domains we compare the algorithms in two setups: the ellipsoid learning framework and an offline framework. All the results are averaged across 10 random seeds in Section 4.1 and 5 seeds in Section 4.2 (we report the mean and the standard deviation). Due to space considerations we present the simulations in the ellipsoid framework only for the car domain, and the simulations in the offline framework only in the dynamic treatment regime. Complementary simulations can be found in the supplementary material.

Section Title: DRIVING SIMULATION - THE ELLIPSOID FRAMEWORK
  DRIVING SIMULATION - THE ELLIPSOID FRAMEWORK In the ellipsoid framework, an expert evaluates the agent policy. If the agent's policy is sub-optimal, the expert provides the agent its feature expectations; otherwise, no demonstration is given. The algorithm performs learning in between demonstrations. This setup enables a proper comparison with the ellipsoid algorithm, which requires the additional expert supervision. We measure performance w.r.t. the following criteria: (1) # demonstrations - the amount of contexts on which each algorithm requested an expert demonstration (y-axis) as a function of time, i.e., the total number of contexts (x-axis). (2) Value - the difference in value, between the agent policy and the expert policy w.r.t. the true reward mapping, i.e., c∈Ctest f W * (c) · µ(π c (W )) − µ(π * c ) , where C test is a holdout (test) set of contexts. The x-axis measures the amount of demonstrations given.

Section Title: Setup
  Setup This domain simulates a three-lane highway with two visible cars - cars A and B (illustration provided in the appendix). The agent, controlling car A, can drive both on the highway and off-road. Car B drives on a fixed lane, at a slower speed than car A. Upon leaving the frame, car B is replaced by a new car, appearing in a random lane at the top of the screen. The feature vector φ(s) is composed of 3 features: (1) a speed feature, (2) a collision feature, which is valued 0 in case of a collision and 0.5 otherwise, and (3) an off-road feature, which is 0.5 if the car is on the road and 0 otherwise. In this task, the context vector implies different priorities for the agent; should it prefer speed or safety? Is going off-road to avoid collisions a valid option? For example, an ambulance will prioritize speed and may allow going off-road as long as it goes fast and avoids collisions, while a bus will prioritize avoiding both collisions and off-road driving as safety is its primary concern. To demonstrate the effectiveness of our solutions, the mapping f : C → [−1, 1] k is constructed in a way that induces different behaviors for different contexts, making generalization a challenging task. We provide additional details on the domain as well as the hyper parameter selection in the appendix.

Section Title: Linear
  Linear The optimal behavior is defined using a linear mapping W * . In this setting, all three approaches obtain competitive results, in terms of generalization, although the ES is capable of obtaining these results faster, as is seen through the regret and number of required demonstrations.

Section Title: DYNAMIC TREATMENT REGIME - THE OFFLINE FRAMEWORK
  DYNAMIC TREATMENT REGIME - THE OFFLINE FRAMEWORK In the offline framework, we focus on the ability to learn from previously collected data. A data set of previously collected trajectories is given, such that a single trajectory of finite length is observed for each context and no context is observed more than once. We measure performance w.r.t. the following Under review as a conference paper at ICLR 2020 criteria: (1) Value - as in the ellipsoid framework above, but here the x-axis corresponds to the amount of iterations. Each iteration corresponds to a single subgradient step, where the subgradient is computed from a mini batch of 10 contexts. (2) Loss - as in Eq. (1). (3) Accuracy % - the percent of actions on which the expert and the agent agree on. All these criteria are evaluated on a holdout set.

Section Title: Setup
  Setup In the dynamic treatment regime, there is a clinician which acts to improve a sick patient's medical condition. The context (static information) represents patient features, which do not change during treatment, such as age and gender. The state summarizes the dynamic measurements of the patient, e.g., blood pressure and EEG readouts. The actions are the forms of intervention a clinician may take, including combinations of various treatments provided in parallel. Dynamic treatment regimes are particularly useful for managing chronic disorders and fit well into the broader paradigm of personalized medicine (Komorowski et al., 2018; Prasad et al., 2017). The agent needs to choose the right treatment for a patient that is diagnosed with sepsis. We use the MIMIC-III data set (Johnson et al., 2016) and follow the data processing steps that were taken in Jeter et al. (2019). As performing off-policy evaluation is not possible using this data set, due to it not satisfying basic requirements (Gottesman et al., 2018; 2019), we designed a simulator of a CMDP. The simulator is based on this data set; a complete overview and explanation on how it was created is provided in the appendix. The mapping W * is linear, W * ∈ R 8×42 , which we constructed from the data. In the simulator, the expert acts optimally w.r.t. this W * . Specifically, when treating a sepsis patient, the clinician has several decisions to make, such as whether or not to provide a patient with vasopressors, drugs which are commonly provided to restore and maintain blood pressure in patients with sepsis. However, what is regarded as healthy blood pressure differs based on the age and weight of the patient (Wesselink et al., 2018). In our setting, W captures this information - as it maps from contextual (e.g., age) and dynamic information (e.g., blood pressure) to reward. Results.  Fig. 3  presents the ability of the descent methods to generalize to unseen contexts by learning from offline data (without supervision). The data is composed of a set of trajectories, i.e., offline data, that were collected from experts (clinicians treating patients). In each iteration, we sample a mini- batch of 10 contexts, i.i.d, from the context distribution. For each context, there is a corresponding expert trajectory of length H = 40. Performance is measured on a holdout set of 300 contexts (that are sampled from the same context distribution) according to Theorem 1. We can see that both ES and PSGD attain near-optimal performance using only previously collected expert trajectories. Looking at Fig. 3a, we can see that all the algorithms manage to minimize the loss to roughly the same error. The small bias is explained by the fact that we use truncated trajectories (as we discussed in the practical MDA paragraph) where in the ellipsoid framework experiments we used feature expectations. We can also see that minimizing the loss leads to policies that attain −optimal value w.r.t. the true reward Fig. 3b. Finally, in  Fig. 3  we can see that all the algorithms reach around 70% accuracy with the expert policy. We emphasize here that 100% accuracy should not be expected for two reasons: (i) different policies may have the same feature expectations (hence the same value) but make different decisions (ii) there exists reward for which there is more than one optimal policy. Nevertheless,  Fig. 3  suggests that accuracy is correlated with minimizing the COIRL loss (Eq. (1)). Finally, we present results in the nonlinear setting. Here, there is a non-linear function of the context that determines which one of the two reward coefficient vectors is used, i.e., f * (c) = r 1 if age > 0.1, and r 2 otherwise. Where age refers to the normalized age of the patient, which is an element Under review as a conference paper at ICLR 2020 (a) Loss (b) Value (c) Accuracy %

Section Title: RELATED WORK
  RELATED WORK We begin with a short discussion on contextual policies, i.e., a policy that is a function of both the state and the context. While there is empirical evidence that learning such a policy may perform well in practice (e.g., Xu et al. (2018); Fu et al. (2019)), from a theoretical point of view, there exist hardness results in this setting. Specifically, given an MDP with k + 1 states, there is a reduction from the training problem of the union of k hyperplanes to the policy (see appendix E for proof). Alternatively, one may consider applying an AL algorithm on a single, large MDP that includes all the states and the contexts. For a concrete example, consider a reduction from the CMDP model to a large MDP where each state is expanded by concatenating the context to it. The new states are s = (s, c), and the new features are φ(s ) = c φ(s). Generally speaking, applying an AL algorithm to this large MDP will give the same scalability and sample complexity as COIRL. However, as the large MDP has |S | = |C||S| states, computing the optimal policy in each iteration of the algorithm will require at least |C| times more time. To illustrate this problem we conducted a simple grid world experiment on a 3 × 4 grid world MDP (with 12 states) and one-hot features (φ(s i ) = e i ∈ R 12 ). The dynamics are deterministic, and the actions correspond to going up, down, left and right (with cyclic transitions on the borders). The contexts correspond to "preferences" on the grid; mathematically, each context is sampled from a uniform distribution over the simplex. The mapping W * is set to be I 12·12 , and for AL, we let w * to be a flattened version of W * . We compare the performance of PSGD with the projection algorithm of (Abbeel & Ng, 2004) in  Fig. 5 . We measure performance by three metrics: run-time, value, and accuracy. Inspecting the results, we can see that AL in the large MDP requires significantly more time to run as the number of contexts grows, while the run time of PSGD (COIRL) is not affected by the number of contexts. We can also see that both methods achieve roughly the same performance: COIRL performs slightly better in terms of value while AL performs slightly better in terms of accuracy. To conclude, AL on a large MDP does not scale to problems with large context spaces. In addition, this construction is Under review as a conference paper at ICLR 2020 only possible when there is a finite number of context and does not provide generalization results. We avoided all of these issues in the COIRL framework.

Section Title: SUMMARY AND DISCUSSION
  SUMMARY AND DISCUSSION In this work, we formulated and studied the COIRL problem. We presented two types of algorithms to solve it: (1) cutting plane methods (ellipsoid) and (2) iterative descent approaches (MDA and ES). We summarize the theoretical guarantees of the different algorithms in  Table 1 . We can see that the iterative descent approaches have better dependence in dk than the ellipsoid method, i.e., they scale better with the dimensions of the problem. In particular, the EW algorithm has a logarithmic dependence in dk, which makes it computationally comparable to standard IRL/AL (on a single, noncontextual MDP). In addition, the iterative methods extend naturally to the more general scenario where the mapping from contexts to rewards is not linear, and f W is modeled as a DNN. As Sutton (2019) puts it: "The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin". The ellipsoid method has better sample complexity (as a function of ) than the descent methods in the deterministic setting. However, both methods attain the same complexity in the more realistic, stochastic setting. Our empirical findings suggest that the iterative methods always outperform the ellipsoid algorithm. Among these methods, we found the ES method to perform better than the MDA method. Similar findings were reported in (Nesterov & Spokoiny, 2017) for other convex problems. The iterative methods have another advantage over the ellipsoid method - they can learn from previously collected demonstrations (i.e., offline learning). The ellipsoid framework, on the other hand, requires expert supervision throughout the entire learning process. Finally, an attractive property of the ellipsoid learning framework is its safety, i.e., an IRL algorithm that is being supervised by an expert will never perform sub-optimally. In each step, either that the agent performs -optimally or that the expert acts on its behalf (provides a demonstration). This property is appealing in mission-critical domains where errors have a high cost; for instance, in health-care, a failure may result in a loss of lives. In the experimental section, we have seen that we can use this learning framework for the iterative methods as well while enjoying improved efficiency.

```
