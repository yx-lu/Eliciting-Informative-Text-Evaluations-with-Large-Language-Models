Title:
```
Under review as a conference paper at ICLR 2020 AN EMPIRICAL STUDY ON POST-PROCESSING METHODS FOR WORD EMBEDDINGS
```
Abstract:
```
Word embeddings learnt from large corpora have been adopted in various appli- cations in natural language processing and served as the general input representa- tions to learning systems. Recently, a series of post-processing methods have been proposed to boost the performance of word embeddings on similarity comparison and analogy retrieval tasks, and some have been adapted to compose sentence rep- resentations. The general hypothesis behind these methods is that by enforcing the embedding space to be more isotropic, the similarity between words can be better expressed. We view these methods as an approach to shrink the covariance/gram matrix, which is estimated by learning word vectors, towards a scaled identity matrix. By optimising an objective in the semi-Riemannian manifold with Cen- tralised Kernel Alignment (CKA), we are able to search for the optimal shrinkage parameter, and provide a post-processing method to smooth the spectrum of learnt word vectors which yields improved performance on downstream tasks.
```

Figures/Tables Captions:
```
Figure 1: Performance change of each post-processing method when the dimension of learnt word vectors increases. Our method is comparable to the method that removes top PCs on skip- gram, and better than that on CBOW, and slightly worse than that on GloVe. However, generally Skipgram and CBOW themselves provide better performance, thus boosting performance by post- processing on those two learning algorithms is more important. In addition, our model doesn't require manually picking the dimensions to remove instead the optimal β is found by optimisation.
Figure 2: Averaged score on 17 tasks vs. Percentage of training data. To simulate the situa- tion where only limited data is available for shrinkage estimation, two algorithms are trained with subsampled data with different portions, and three postprocessing methods including ours and two comparison partners are applied afterwards. The observation here is that our method is able to recover the similarity between words better than others when only small amount of data is available.
Table 1: Results of three post-processing methods including ours on our pretrained word vec- tors from three learning algorithms. In each cell, the three numbers refer to word vectors produced by "Skipgram / CBOW / GloVe", and the dimension of them is 500. Bold indicates the best macro- averaged performance of post-processing methods. It shows that overall, our method is effective.
Table 2: Performance on word translation. The translation is done through k-NN with two dis- tances, in which one is the cosine similarity (noted as "NN" in the table), and the other one is the Cross-domain Similarity Local Scaling (CSLS) (Lample et al., 2018). The Ledoit & Wolf's method didn't converge on unsupervised training so we excluded results from the method in the table.
Table 3: Performance of our post-processing method and the other two comparison partners on SemEval datasets. The task is to make good predictions of sentence vectors composed of averaging word vector. The word-level post-processing means that all methods are applied on word vectors before averaging, and the sentence-level one means that all methods are applied on sentence vectors. Our method performs better than others at sentence-level and slightly worse than the method that removes top principal components on the word level. Overall the sentence-level post-processing results in superior performance. Our method has the best overall performance on each dataset.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Distributed representations of words have been widely dominating the Natural Language Processing research area and other related research areas where discrete tokens in text are part of the learning systems. Fast learning algorithms are being proposed, criticised, and improved. Despite the fact that there exist various learning algorithms ( Mikolov et al., 2013a ;  Pennington et al., 2014 ;  Bojanowski et al., 2017 ) for producing high-quality distributed representations of words, the main objective is roughly the same, which is drawn from the Distributional Hypothesis ( Harris, 1954 ). The algorithms assume that there is a smooth transition of meaning at the word-level, thus they learn to assign higher similarity for adjacent word vectors than those that are not adjacent. The overwhelming success of distributed word vectors leads to subsequent questions and analyses on the information encoded in the learnt space. As the learning algorithms directly and only utilise the co-occurrence provided by large corpora, it is easy to hypothesise that the learnt vectors are correlated with the frequency of each word, which may not be relevant to the meaning of a word ( Turney & Pantel, 2010 ) and might hurt the expressiveness of the learnt vectors. One can theorise the frequency-related components in the learnt vector space, and remove them ( Arora et al., 2017 ;  Mu et al., 2018 ;  Ethayarajh, 2018 ). These post-processing methods, whilst very effective and appealing, are derived from heavy assumptions on the representation geometry and the similarity measure. In this work we re-examine the problem of post-processing word vectors as a shrinkage estimation of the true/underlying oracle gram matrix of words, which is a rank-deficient matrix due to the existence of synonyms and antonyms. Constrained from the semi-Riemannian manifold ( Benn & Tucker, 1987 ;  Abraham et al., 1983 ) where positive semi-definite matrices, including gram matrices, exist, and Centralised Kernel Alignment ( Cortes et al., 2012 ), we are able to define an objective to search for the optimal shrinkage parameter, also called mixing parameter, that is used to mix the estimated gram matrix and a predefined target matrix with the maximum similarity with the oracle matrix on the semi-Riemannian manifold. Our contribution can be considered as follows: 1. We define the post-processing on word vectors as a shrinkage estimation, in which the estimated gram matrix is calculated from the pretrained word embeddings, and the target matrix is a scaled identity matrix for smoothing the spectrum of the estimated one. The goal is to find the optimal mixing parameters to combine the estimated gram matrix with the target in the semi-Riemannian manifold that maximises the similarity between the combined matrix and the oracle one. Under review as a conference paper at ICLR 2020 2. We choose to work with the CKA, as it is invariant to isotropic scaling and also rotation, which is a desired property as the angular distance (cosine similarity) between two word vectors is commonly used in downstream tasks for evaluating the quality of learnt vectors. Instead of directly deriving formulae from the CKA (Eq. 1), we start with the logarithm of CKA (Eq. 2), and then find an informative lower bound (Eq. 5). The remaining analysis is done on the lower bound. 3. A useful objective is derived to search for the optimal mixing parameter, and the performance on the evaluation tasks shows the effectiveness of our proposed method. Compared with two other methods, our approach shows that the shrinkage estimation on a sub-Riemannian manifold works better than that on an Euclidean manifold for word embedding matrices, and it provides an alter- native way to smooth the spectrum by adjusting the power of each linearly independent component instead of removing top ones.

Section Title: RELATED WORK
  RELATED WORK Several post-processing methods have been proposed for readjusting the learnt word vectors ( Mu et al., 2018 ;  Arora et al., 2017 ;  Ethayarajh, 2018 ) to make the vector cluster more isotropic w.r.t. the origin such that cosine similarity or dot-product can better express the relationship between word pairs. A common practice is to remove top principal components of the learnt word embedding matrix as it has been shown that those directions are highly correlated with the occurrence. Despite the effectiveness and efficiency of this family of methods, the number of directions to remove and also the reason to remove are rather derived from empirical observations, and they don't provide a principled way to estimate these two factors. In our case, we derive a useful objective that one could take to search for the optimal mixing parameter on their own pretrained word vectors. As described above, we view the post-processing method as a shrinkage estimation on the semi- Riemannian manifold. Recent advances in machine learning research have developed various ways to learn word vectors in hyperbolic space ( Nickel & Kiela, 2017 ;  Dhingra et al., 2018 ;  Gulcehre et al., 2019 ). Although the Riemannian manifold is a specific type of hyperbolic geometry, our data samples on the manifold are gram matrices of words instead of word vectors themselves in previous work, , which makes our work different from previous ones. The word vectors themselves are still considered in Euclidean space for the simple computation of similarity between pairs, and the intensity of the shrinkage estimation is optimised to adjust the word vectors. Our work is also related to previously proposed shrinkage estimation of covariance matrices ( Ledoit & Wolf, 2004 ;  Schäfer & Strimmer, 2005 ). These methods are practically useful when the number of variables to be estimated is much more than that of the samples available. However, the mixing operation between the estimated covariance matrix and the target matrix is a weighted sum, and l 2 distance is used to measure the difference between the true/underlying covariance matrix and the mixed covariance matrix. As known, covariance matrices are positive semi-definite, thus they lie on a semi-Riemannian manifold with a manifold-observing distance measure. A reasonable choice is to conduct the shrinkage estimation on the semi-Riemannian manifold, and CKA is applied to measure the distance as rotations and istropic scaling should be ignored with measuring two sets of word vectors. The following sections will discuss our approach in details.

Section Title: SHRINKAGE OF GRAM MATRIX
  SHRINKAGE OF GRAM MATRIX Learning word vectors can be viewed as estimating the oracle gram matrix where the relationship of words is expressed, and the post-processing method we propose here aims to best recover the oracle gram matrix. Suppose there exists an ideal gram matrix K ∈ R n×n where each entry represents the similarity of a word pair defined by a kernel function k(wi, wj). Given the assumption of a gram matrix and the existence of polysemies, the oracle gram matrix K is positive semi-definite and its rank is between 0 and n and denoted as k. A set of pretrained word vectors is provided by a previously proposed algorithm, and for simplicity, an embedding matrix E ∈ R n×d is constructed in which each row is a word vector vi. The estimated gram matrix K = EE ∈ R n×n has rank d and d ≤ k. It means that overall, the previously proposed algorithms give us a low-rank approximation of the oracle gram matrix K.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 The goal of shrinkage is to, after the estimated gram matrix K is available, find an optimal post- processing method to maximise the similarity between the oracle gram matrix K and the estimated one K given simple assumptions about the ideal one without directly finding it. Therefore, a proper similarity measure that inherits and respects our assumptions is crucial.

Section Title: CENTRALISED KERNEL ALIGNMENT (CKA)
  CENTRALISED KERNEL ALIGNMENT (CKA) The similarity measure between two sets of word vectors should be invariant to any rotations and also to isotropic scaling. A reasonable choice is the Centralised Kernel Alignment (CKA) ( Cortes et al., 2012 ) defined below: ρ(Kc, K c ) = Kc, K c F ||Kc||F ||K c ||F (1) where Kc = I − 1 n 11 K I − 1 n 11 , and Kc, K c F = Tr(KcK c ). For simplicity of the deriva- tions below, we assume that the ideal gram matrix K is centralised, and the estimated one K can be centralised easily by removing the mean of word vectors. In the following section, K and K are used to denote centralised gram matrices. As shown in Eq. 1, it is clear that ρ(K, K ) is invariant to any rotations and isotropic scaling, and doesn't suffer from the issues of K and K being low-rank. The centralised kernel alignment has been recommended recently as a proper measure for comparing the similarity of features learnt by neural networks ( Kornblith et al., 2019 ). As learning word vectors can be thought of as a one-layer neural network with linear activation function, ρ(K, K ) is a reasonable similarity measure between the ideal gram matrix and our shrunken one provided by learning algorithms. Given the Cauchy- Schwartz inequality and also the non-negativity of the Frobenius norm, ρ(K, K ) ∈ [0, 1]. Since both K and K are positive semi-definite, their eigenvalues are denoted as {λσ 1 , λσ 2 , ..., λσ k } and {λν 1 , λν 2 , ..., λν d } respectively, and the eigenvalues of KK are denoted as {λ1, λ2, ..., λ d } as the rank is determined by the matrix with lower rank. 1 The log-transformation is conducted to simplify the derivation. where the lower bound is given by the AM−GM inequality, and the equality holds when all eigen- values of KK are the same λ1 = λ2 = ... = λ d .

Section Title: SHRINKAGE OF GRAM MATRIX ON SEMI-RIEMANNIAN MANIFOLD
  SHRINKAGE OF GRAM MATRIX ON SEMI-RIEMANNIAN MANIFOLD As the goal is to find a post-processing method that maximises the similarity ρ(K, K ), a widely adopted approach is to shrink the estimated gram matrix K towards the target matrix T with a predefined structure. The target matrix T is usually positive semi-definite and has same or higher rank than the estimated gram matrix. In our case, we assume T is full rank as it simplifies equations. Previous methods rely on a linear combination of K and T in Euclidean space ( Ledoit & Wolf, 2004 ;  Schäfer & Strimmer, 2005 ). However, as gram matrices are positive semi-definite, they nat- urally lie on the semi-Riemannian manifold. Therefore, a suitable option for shrinkage is to move the estimated gram matrix K towards the target matrix T on a semi-Riemannian manifold ( Brenier, 1987 ), and the resulting matrix Y is given by Y = T 1/2 (T −1/2 K T −1/2 ) β T 1/2 (3) where β ∈ [0, 1] is the mixing parameter that indicates the strength of the shrinkage, and T 1/2 is the square root of the matrix T . The objective is to find the optimal β that maximises log ρ(K, K ). The objective defined in Eq. 4 is hard to work with, instead, we maximise the lower bound in Eq. 2 to find optimal mixing parameter β . By plugging Y into Eq. 2 and denoting λy i as the i-th eigenvalue of Y , we have:

Section Title: SCALED IDENTITY MATRIX AS TARGET FOR SHRINKAGE
  SCALED IDENTITY MATRIX AS TARGET FOR SHRINKAGE Recent progress in analysing pretrained word vectors ( Mu et al., 2018 ;  Arora et al., 2017 ) recom- mended to make them isotropic by removing top principal components as they highly correlate with the frequency information of words, and it results in a more compressed eigenspectrum. In Eu- clidean space-based shrinkage methods ( Anderson, 2004 ;  Schäfer & Strimmer, 2005 ), the spectrum is also smoothed by mixing the eigenvalues with ones to balance the overestimated large eigenvalues and underestimated small eigenvalues. In both cases, the goal is to smooth the spectrum to make it more isotropic, thus an easy target matrix to start with is the scaled identity matrix, which is noted as T = αI, where α is the scaling parameter and I is the identity matrix. Thus, the resulting matrix Y and the lower bound in Eq. 2 become It is easy to show that, when β = 0, the resulting matrix Y becomes the target matrix αI, and when β = 1, no shrinkage is performed as Y = K . Eq. 7 indicates that the lower bound is only a function of β with no involvement from the scaling parameter α brought by the target matrix as the CKA is invariant to isotropic scaling.

Section Title: NOISELESS ESTIMATION
  NOISELESS ESTIMATION Starting from the simplest case, we assume that the estimation process from learning word vectors E to constructing the estimated gram matrix K is noiseless, thus the first order and the second order derivative of L(β) with respect to β are crucial for finding the optimal β : Since L(β) is invariant to isotropic scaling on {λν i |i ∈ 1, 2, ..., d}, we are able to scale them by the inverse of their sum, then pν i = λν i / d i=1 λν i defines a distribution. To avoid losing information of the estimated gram matrix K , we redefine the plausible range for β as (0, 1]. Eq. 8 can be interpreted as where DKL(q||p) is the Kullback-Leibler divergence and it is non-negative, H(q) is the entropy of the uniform distribution q, and H (r(β), p) is the cross-entropy for r and p. The inequality derives from the fact that the uniform distribution has the maximum entropy. Thus, the first order derivative L (β) is always less than 0. With the same definition of qi and r(β)i in Eq. 10, the second order derivative L (β) can be rewritten as As shown, L (β) < 0 means that L(β) monotonically decreases in the range of (0, 1] for β, and the re- sulting matrix Y = αI completely ignores the estimated gram matrix K . L (β) also monotonically decreases in the range of (0, 1] for β because L (β) < 0. The observation is similar to the finding reported in previous work ( Schäfer & Strimmer, 2005 ) on shrinkage estimation of covariance matrices that any target will increase the similarity between the oracle covariance matrix and the mixed covariance matrix. In our case, simply reducing β from 1 to 0 increases Eq. 2, and consequently, β → 0+ loses all information estimated which is not ideal. However, one can rewrite Eq. 11 as L (β) = H (r(β), p) 2 − H(q)H (r(β), p) and q is the uniform distribution which has maximum entropy, the derivative of L (β) with respect to H (r(β), p) gives Under review as a conference paper at ICLR 2020 2H (r(β), p)−H(q), and by setting the derivative to 0, it tells us that there exists a β which results in a specific r that gives H (r(β), p) = 1 2 H(q). Since H (r(β), p) monotonically increases when β moves from 0+ to 1, and lim β→0 + H (r(β), p) = H (q, p) < 1 2 H(q) and H (r(1), p) > 1 2 H(q), there exists a single β ∈ (0, 1] that gives the smallest value of L (β). This indicates that there exists aβ that leads to the slowest change in the function value L(β). Then, we simply set β =β. In practice, β should be larger than 0.5 in order not to make the resulting matrix overpowered by the target matrix, so one could run a binary search on β ∈ [0.5, 1] that gives smallest value of L (β). Intuitively, this method is similar to using an elbow plot to find the optimal number of components to keep when running Principal Component Analysis (PCA) on observed data. To translate in our case, a larger value of β leads to flatter spectrum and higher noise level as the scaled identity matrix represents isotropic noise. The optimal β gives us a point where the increase of L(β) is the slowest. Once optimal mixing parameters β are found, the resulting matrix Y = K β = E β (E β ) , and α is omitted here. As a post-processing method for word vectors, we propose to transform the word vectors in the following fashion: 2. Singular Value Decomposition E = U SV 3. Optimal Mixing Parameter β = arg min β L 4. Reconstruct the matrix E = U (S) β V

Section Title: EXPERIMENTS
  EXPERIMENTS Three learning algorithms are applied to derive word vectors, including skipgram (Mikolov et al., 2013b), CBOW ( Mikolov et al., 2013a ) and GloVe ( Pennington et al., 2014 ). 2 EE serves as the estimated gram matrix K . Hyperparameters of each algorithm are set to the recommended values. The training corpus is collected from Wikipedia and it contains around 4600 million tokens. The unique tokens include ones that occur at least 100 times in the corpus and the resulting dictionary size is 366990. After learning, the word embedding matrix is centralised and then the optimal mixing parameter β is directly estimated from the singular values of the centralised word vectors. After- wards, the postprocessed word vectors are evaluated on a series of word-level tasks to demonstrate the effectiveness of our method. Although the range of β is set to [0.5, 1.0] and the optimal value is found by minimising L (β), it seldomly hits 0.5, which means that searching is still necessary.

Section Title: COMPARISON PARTNERS
  COMPARISON PARTNERS Two comparison partners are chosen to demonstrate the effectiveness of our proposed method. One is the method ( Mu et al., 2018 ) that removes the top 2 or 3 principal components on the zero-centred word vectors, and the number of top principal components to remove depends on the training algo- rithm used for learning word vectors, and possibly the training corpus. Thus, the selection process is not automatic, while ours is. The hypothesis behind this method is similar to ours, as the goal is to make the learnt word vectors more isotropic. However, removing top principal components, which are correlated with the frequency of each word, could also remove relevant information that indicates the relationship between words. The observation that our method is able to perform on par with or better than this comparison partner supports our hypothesis. The other comparison partner is the optimal shrinkage ( Ledoit & Wolf, 2004 ) on the estimated covariance matrices as the estimation process tends to overestimate the directions with larger power and underestimate the ones with smaller power, and the optimal shrinkage aims to best recover the true covariance matrix with minimum assumptions about it, which has the same goal as our method. The resulting matrix Y is defined to be a linear combination of the estimated covariance matrix and the target with a predefined structure, and it is denoted as Y = (1 − β)αI + βΣ , where in our case, Σ = E E ∈ R d×d . The optimal shrinkage parameter β is found by minimising the Euclidean distance between the true covariance matrix Σ and the resulting matrix Y . The issue here is that Euclidean distance and linear combination may not be the optimal choice for covariance matrices, and also Euclidean distance is not invariant to isotropic scaling which is a desired property when measuring the similarity between sets of word vectors.

Section Title: WORD-LEVEL EVALUATION TASKS
  WORD-LEVEL EVALUATION TASKS Three categories of word-level evaluation tasks are considered, including 8 tasks for word similarity, 3 for word analogy and 6 for concept categorisation. The macro-averaged results for each category of tasks are presented in  Table 1 . Details about tasks are included in the appendix.

Section Title: WORD TRANSLATION TASKS
  WORD TRANSLATION TASKS   Table 2  presents results on supervised and unsupervised word translation tasks ( Lample et al., 2018 ) 3 given pretrained word vectors from FastText ( Bojanowski et al., 2017 ). The supervised word translation is formalised as solving a Procrustes problem directly. The unsupervised task first trains a mapping through adversarial learning and then refines the learnt mapping through the Procrustes problem with a dictionary constructed from the mapping learnt in the first step. The evaluation is done by k-NN search and reported in top-1 precision.  Table 3 .

Section Title: DISCUSSION
  DISCUSSION The results presented in  Table 1 , 2 and 3 indicate the effectiveness of our methods. In addition, by comparing to two most related post-processing methods, which are removing top principal com- ponents ( Mu et al., 2018 ;  Arora et al., 2017 ) and the optimal shrinkage estimation on covariance matrices in Euclidean space ( Ledoit & Wolf, 2004 ;  Schäfer & Strimmer, 2005 ), our method com- bines the best of the both worlds - the good performance of the first method and the automaticity property of the second one. Specifically, our method derives the resulting gram matrix in a semi-Riemannian manifold, and the shrinkage is done by moving the estimated gram matrix K to the scaled identity matrix αI on the geodesic on the semi-Riemannian manifold defined in Eq. 6. Compared to measuring the Euclidean distance between two covariance matrices ( Ledoit & Wolf, 2004 ), we chose a distance measure that respects the structure of the space where positive semi-definite matrices exist. Also, since we are working with improving the quality of learnt word vectors, given that mostly the angular distance between a pair of word vectors matters, the similarity measure between two spaces needs to be invariant to rotation and isotropic scaling, and the Centralised Kernel Alignment is a good fit in our case. Compared to the post-processing method that removes top principal components, our method defines an objective function where the unique minimum point in the specific range indicates the optimal shrinkage parameter β , and which doesn't require any human supervision on finding the parameters (the number of principal components to remove in that method). The derivations in our work are mostly done on the lower bound in Eq. 2 of the original log of CKA ρ(K, K ), and the bound is tight when KK has a flat spectrum. In this sense, the lower bound essentially defines the similarity of the resulting matrix Y and a scaled identity matrix where the scaling factor is the average of the eigenvalues of KK . As the isotropic scaling can be ignored in our case, intuitively, the lower bound (Eq. 2) gives us how to shrink an estimated gram matrix to an identity matrix. Since the target matrix T = αI is also a scaled identity matrix, the lower bound keeps increasing as Y travels from K to T , and it explains why the first order derivative is negative and the second order derivative is more informative. Our method is very similar to using an elbow plot to find the number of eigenvalues to keep in PCA. Future work should focus on finding a tighter bound that considers the interaction between the true gram matrix K and the estimated one K .  Figure 1  shows that our methods give solid and stable improvement against two comparison part- ners on learnt word vectors provided by skipgram and CBOW with different dimensions, and slightly worse than removing top PCs on GloVe. However, skipgram and CBOW provide much better per- formance thus improvement on these algorithms is more meaningful, and our method doesn't require manual speculations on the word vectors to determine the number of PCs to remove. An interesting observation is that there is a limit on the performance improvement from increasing the dimensional- ity of word vectors, which was hypothesised and analysed in prior work ( Yin & Shen, 2018 ).  Figure 2  shows that our method consistently outperforms two comparison partners when limited amount of Under review as a conference paper at ICLR 2020 training corpus is provided, and the observation is consistent across learnt word vectors with varying dimensions. From the perspective of shrinkage estimation of covariance matrices, the main goal is to optimally recover the oracle covariance matrix under a certain assumption about the oracle one when limited data is provided. The results presented in  Figure 2  indicate that our method is better at recovering the gram matrix of words, where relationships of word pairs are expressed, as it gives better performance on word-level evaluation tasks on average.

Section Title: CONCLUSION
  CONCLUSION We define a post-processing method in the view of shrinkage estimation of the gram matrix. Armed with CKA and geodesic measured on the semi-Riemannian manifold, a meaningful lower bound is derived and the second order derivative of the lower bound gives the optimal shrinkage parameter β to smooth the spectrum of the estimated gram matrix which is directly calculated by pretrained word vectors. Experiments on the word similarity and sentence similarity tasks demonstrate the effectiveness of our model. Compared to the two most relevant post-processing methods ( Mu et al., 2018 ;  Ledoit & Wolf, 2004 ), ours is more general and automatic, and gives solid performance im- provement. As the derivation in our paper is based on the general shrinkage estimation of positive semi-definite matrices, it is potentially useful and beneficial for other research fields when Euclidean measures are not suitable. Future work could expand our work into a more general setting for shrink- age estimation as currently the target in our case is an isotropic covariance matrix.

```
