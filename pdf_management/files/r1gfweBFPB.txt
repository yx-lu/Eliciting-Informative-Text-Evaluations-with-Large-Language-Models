Title:
```
Under review as a conference paper at ICLR 2020 LEARNING BY SHAKING: COMPUTING POLICY GRADI- ENTS BY PHYSICAL FORWARD-PROPAGATION
```
Abstract:
```
Model-free and model-based reinforcement learning are two ends of a spectrum. Learning a good policy without a dynamic model can be prohibitively expensive. Learning the dynamic model of a system can reduce the cost of learning the pol- icy, but it can also introduce bias if it is not accurate. We propose a middle ground where instead of the transition model, the sensitivity of the trajectories with re- spect to the perturbation (shaking) of the parameters is learned. This allows us to predict the local behavior of the physical system around a set of nominal poli- cies without knowing the actual model. We assay our method on a custom-built physical robot in extensive experiments and show the feasibility of the approach in practice. We investigate potential challenges when applying our method to physical systems and propose solutions to each of them. (a) (b) (c) (d) Figure 1: Physical finger platform in action with different policies.
```

Figures/Tables Captions:
```
 
Figure 2: Gaussian (left) and uniform (right) shaking examples.
Figure 3: The effect of voxels on supressing spatial noise of the physical system. The trajectories are produced by linear open-loop controllers as those in section 4.1 for the purpose of illustrating the effect of voxelization.
Figure 4: PD controller with various noise intensities on K p parameter.
Figure 5: The time evolution of the GP approximatedĝ t for PD feedback controller at some exem- plary time instances
Figure 6: The time evolution of the GP approximatedĝ t for nonlinear sinusoidal open-loop con- troller at some exemplary time instances.
Figure 7: Pysical gradients computed for various time steps along a source trajetcory using two perturbed trajectories of linear open-loop controller.
Figure 8: Zero-shot planning with constraint satisfaction. The orange trajectory is the source pro- duced by the nominal controller. The green and blue are two sampled trajectoreis that are produced by perturbing k p to k * p by eq. (21)
Table 1: The aggregate performance of our method to predict physical derivatives in unseen direc- tions of perturbations to the parameters. · shows time average. The first column is the normalized time averaged MSE. The second column is the time averaged GP score (closer to 1 is better. See ap- pendix E.4 for definition). The third column is the time averaged misalignment between derivatives. Every experiment is repeated for 10 voxel sizes and the values are chosen for the best voxel size. N: Gaussian sampling, U: uniform sampling.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Traditional reinforcement learning crucially relies on reward(Sutton & Barto, 2018). However, re- ward binds the agent to a certain task for which the reward represents success. Aligned with the recent surge of interest in unsupervised methods in reinforcement learning (Baranes & Oudeyer, 2013; Bellemare et al., 2016; Gregor et al., 2016; Hausman et al., 2018; Houthooft et al., 2016) and previously proposed ideas (Schmidhuber, 1991a; 2010), we argue that there exist properties of a dy- namical system which are not tied to any particular task, yet highly useful, and their knowledge can help solve other tasks more efficiently. This work focuses on the sensitivity of the produced trajecto- ries of the system with respect to the policy so called Physical Derivatives. The term physical comes from the fact that it uses the physics of the system rather than any idealized model. We learn a map from the directions in which policy parameters change to the directions in which every state of the trajectory changes. In general, our algorithm learns the Jacobian matrix of the system at every time step through the trajectory. The training phase consists of physically calculating directional deriva- tives by the finite difference after applying perturbed versions of a nominal policy (a.k.a. controller). Perturbing the parameters of the controller is the reason for naming our method shaking. The test phase uses these directional derivatives to compute derivatives along unseen directions. Due to the difficulty of computing the Jacobian matrix by the finite difference in higher dimensions, we use random controllers joint with probabilistic learning methods to obtain a robust estimate of the Jaco- bian matrix at each instant of time along a trajectory. We are capable of this generalization to unseen perturbations because the trajectories of physical systems live on an intrinsic low-dimensional man- ifold and change slowly with the small changes in the parameters of the system (Koopman, 1931). This assumption holds as long as the system is not chaotic or close to a bifurcation condition (Khalil, 2002).

Section Title: PRELIMINARIES
  PRELIMINARIES A reward function describes how close the agent is to the solution of the target task. In the absence of the reward, the agent will be given no means to find its way towards the solution. Let x ∈ X ⊆ R d be a d-dimensional state vector that fully describes the environment with which the agent interacts. At each state, the agent is allowed to take action u ∈ U ⊆ R q from a q-dimensional action space via a parameterised policy function u = π(x; θ). The agent will be rewarded r(x, u) by the function r : X × U → R when it takes action u at state x. The goal of learning is to update θ such that some desired target is achieved. The target can be anything as long as a concrete reward function is associated with it. In stochastic cases, return R : Π(Θ) → R is defined as a cumulative future discounted reward whose expectation is often of main interest. For parametric policies, the space of feasible parameters Θ has a one-to-one correspondence to the policy space Π. The agent who takes on the policy π from state x 0 produces the trajectory T ∈ T where T is the space of possible trajectories. For a return function R : T → R, the expected return becomes a function of the policy as J(π θ ) = E T {R(T )} where the expectation is taken with respect to the probability distribution P (T |π θ ). There exist two major classes of approaches in reinforcement learning: value-based methods and value-free methods. In the first class, a surrogate function is defined to approximate the value of either a state V (x) or a state-action pair Q(x, u). The policy is updated such that the agent tends towards states with higher values. The value-free methods update the policy directly without any need for an auxiliary function such as V or Q. This paper mainly concerns the second class. The policy parameters are updated as θ t+1 = θ t + α ∂J(π θ ) ∂θ θ=θt (1) and the gradient ∂J(π θ )/∂θ is written as ∂J(π θ ) ∂θ = T ∂p(T |π θ ) ∂θ R(T ) dT (2) which is normally difficult to compute in practice. As can be seen in eq. (2), the integrand of the r.h.s. consists of two terms. The second term R(T ) is the return which is defined according to the target task. Hence, this term is task-dependent. The first term ∂p(T |π θ )/∂θ though shows how the trajectories change with respect to a change in the policy. Notice that there is no notion of reward or any task-dependent quantities in this term. For an empirical distribution p e (T |π) = 1 M M i=1 δ(T − T (i) ), the dependence of partial derivative of the distribtion of T on the partial derivative of T can be explicitely derived as ∂p e (T |π θ ) ∂θ = 1 M M i=1 u 1 (T − T (i) ) ∂T ∂θ (3) where u 1 is the unit doublet function (derivative of the Dirac delta function). This examplary dis- tribution makes it clear that the change in the distribution of trajetories relates to the change of the trajectories themselves. As an unsupervised object, ∂T /∂θ is of main interest in this paper.

Section Title: PHYSICAL DERIVATIVE
  PHYSICAL DERIVATIVE In this paper, we investigate the feasibility of learning a less explored unsupervised quantity, the so called Physical Derivative which is computed directly from the physical system. In abstract terms, we perturb the policy and learn the effect of its perturbation on the resulting trajectory. The difference from traditional RL whose algorithms are based on eq. (1) is the absence of a specified reward function. Instead, we generate samples from ∂p(T |π θ )/∂θ of eq. (2) that makes it possible to compute ∂J(π θ )/∂θ for an arbitrary return function R. If the exact model of the system is known, control theory has a full set of tools to intervene in the system with stability and performance guarantees. When the system is unknown, one could identify the system as a preliminary step followed by a normal control synthesis process from control theory (Ljung, 2001). Otherwise, the model and the policy can be learned together in a model-based RL (Sutton, 1996) or in some cases adaptive control (Sastry & Bodson, 2011). We argue that learning physical derivatives is a middle ground. It is not model-based in the sense that it does not assume knowing the exact model of the system. Rather, it knows how the trajectories of the system change as a result of perturbing the policy Under review as a conference paper at ICLR 2020 parameters. This differential information of the system has applications in many downstream tasks. This work focuses on the concept and introduction of physical derivatives and direct applications would go significantly beyond the scope of this work. Few potential applications are discussed with more details in appendix C. Our contributions- In summary, the key contributions of the current paper are as follows: • A method to generate training pairs to learn the map from the policy perturbations to the resulting changes in the trajectories. • Learning the above map as a probabilistic function and showing that it generalizes to unseen perturbations in the policy. • Use the inverse of the above map to perturb the policy in the desired direction to achieve certain goals without conventional RL methods. • Use a physical custom-built robotic platform to test the method and propose solutions to deal with the inherent issues of the physical system to ensure the practicality of the method (see fig. 1 for images of the platform and and appendix A for technical details). • The supplementary materials for the paper, including code and the videos of the robot in action can be found in https://sites.google.com/view/ physicalderivatives/

Section Title: METHOD
  METHOD In this section, we describe our pipeline to estimate the physical derivatives and our proposed solu- tions to the inevitable challenges that are likely to occur while working with a real physical robot. We are interested in ∂T /∂θ which denotes how a small change in the parameters θ of the con- troller results in a different trajectory produced by the system. We normally consider a finite period of time [0, T ] and the trajectory is an ordered list of states T = [x 0 , x 1 , . . . , x T ] where the sub- script shows the time step. Therefore, having ∂T /∂θ is equivalent with having ∂x t /∂θ for every t ∈ {1, . . . , T }. Notice that the initial state x 0 is chosen by us. Hence we can see it either as a constant or as a changeable parameter in θ. We kept it fixed in our experiments. Assume x t ∈ R d and θ ∈ R m . Hence, ∇ θ x t = ∂x t /∂θ ∈ R d×m where the t th row of this matrix is ∇ θ x it = (∂x it /∂θ) T ∈ R m showing how the i th dimension of the state vector changes in response to a perturbation in θ. The directional derivative of x it in the direction δθ is defined as If (4) is available for m linearly independent and orthonormal directions, {δθ (1) , δθ (2) , . . . , δθ (m) }, the directional derivative along an arbitrary δθ can be approximated by ∇ δθ θ x it = m j=1 c j ∇ θ x it , δθ (j) (5) where c j = δθ, δθ (j) is the coordinates of the desired direction in the coordinate system formed by the orthonormal bases. In practice, m directions δθ (j) can be randomly chosen or can be along some pre- In the matrix form for x ∈ R d , we can compute ∇ δθ (j) θ x = [∇ δθ (j) θ x 1 , ∇ δθ (j) θ x 1 , . . . , ∇ δθ (j) θ x d ] T in a single run by computing (6) for all d dimensions of the states. Let's define Therefore, if ∆ δθ θ x shows the directional derivative of x along δθ, we can write it as: ∇ δθ θ x = ∆ θ x(Λ T δθ) (8) which is only a vectoral representation of eq. (4). Even though the linear formula of eq. (8) requires only m directional derivatives, it has two major downsides. First, it does not give a clear way to incorporate more than m training directional physical derivatives. Second, the linear approximation remains valid only for very small δθ. We propose Gaussian Process (GP) as a nonlinear probabilistic function approximator (Rasmussen, 2003) to capture the mapsĝ t defined aŝ g t : Θ → X (9) g t (δθ) = δx (10) where subscript t shows the function that maps δθ to the change of the states δx t at time step t. We considered distinct functions for every time step. Taking into account the commonality among the function approximators corresponding to different time steps is deferred to future research. Learn- ing this map requires training data that comes from an initial data collection phase called shaking. Shaking refers to perturbing parameters of the controller to obtain the set of trajectories produced by the perturbed controllers. The perturbation can be either regular or stochastic. Stochastic perturbations have the advantage over regular perturbations that the agent does not need to be worried about perturbing the parameters in a particular direction. Besides, in some cases, perturbing the parameters of the policy in certain directions is infeasible. We propose two methods of shaking called Gaussian and Uniform shaking. Gaussian shaking- Likely values of θ create nominal policies encoded by {θ (1) , θ (2) , . . . , θ (m) }. We put Gaussian distributions centered at each of the nominal values resulting in a mixture of Gaus- sians. To reduce the hyper-parameters, we assume the variances of the Gaussians are themselves sampled from an exponential distribution making sure they all take positive values (See  fig. 2  left). Here, we manually choose a reasonable value for the rate parameter of the exponential distribu- tion. Doing inference on the hyper-parameters of the sampling distributions can be a topic for future research especially in active learning for a more clever less costly sampling stratgey. Uniform shaking- In this setting, the state space of the changeable parameters of the policy is dis- cretized and a uniform distribution is assumed around each value of this grid with some overlapping with the neighboring cells (See  fig. 2  right). We show the effect of each of these sampling methods later in section 4. We observed that the results are less sensitive to the hyper-parameters of the uniform sampling than Gaussian sampling. A carelessly chosen rate for the exponential distribution that generates the variances of the Gaussians in Gaussian sampling can result in too local or global sampling that gives rise to a large variance or bias in the estimated gradients.

Section Title: REAL WORLD CHALLENGES
  REAL WORLD CHALLENGES In this section, we present two major low-level challenges that are common when dealing with physical systems. There exist inherent noise and imperfection in the system that results in a change in the produced trajectories while the policy parameters are kept fixed. In our finger platform, we observed two different major sources of noise which are likely to occur in other physical systems too. We call them temporal and spatial noise for the reasons that come in the following.

Section Title: Temporal noise
  Temporal noise The temporal noise represented by n affects trajectories by shifting them in time Notice that the absence of subscript t in n shows that this noise is not time-dependent, i.e., the time shift does not change along the trajectory as time proceeds.

Section Title: Spatial noise
  Spatial noise The trajectories affected by spatial noise cannot be aligned with each other by shifting forward or backward in time. We can model this noise as a state-dependent influence on the state of the system at every time step. The following definition makes the distinction more concrete. Definition 1. Consider two trajectories T (1) (t) and T (2) (t) as two temporal signals. Assume S t• is the shift-in-time operator defined as S t• T (t) = T (t + t • ) (13) for an arbitrary function of time T (t). We say T (2) (t) is temporally noisy version of T (1) (t) if ∃t • ∈ R s.t. T (2) − S t• T (1) 1 ≤ (14) where is a hyper-parameter threshold that reflects our prior confidence about the accuracy of the motors, joints, physical and electrical elements (in general construction process) of the robot. On the other hand, T (2) is called a spatially noisy version of T (1) if

Section Title: SOLUTION TO TEMPORAL NOISE
  SOLUTION TO TEMPORAL NOISE Fortunately, this type of noise is not state-dependent by definition. If we find out how much a trajectory is shifted in time with respect to another trajectory, we can simply shift the trajectory for those many time steps and compensate for the delay. Hence, the problem becomes detecting the lagged trajectories with respect to a reference trajectory and also estimate the amount of the required time shift to compensate for the delay. We can either use physical landmarks in the trajectories to align them or use the correlation between them as a measure of alignment. The later gave better results, hence, we postpone the description of the former to the appendix D.1.

Section Title: Correlation-based delay estimation
  Correlation-based delay estimation In this method, we use the correlation between zero-meaned trajectories T (i) and T (j) to check if one is the lagged version of the other one. The delay τ is found by τ * = argmax τ T −τ t=0 S τ x (i) t , x (j) t (16) where S τ is a shift-operator by τ ∈ Z time steps. In practice, we take one trajectory of {T (1) , T (2) , . . . , T (M ) }, e.g. T (r) as the reference and synchronize other trajectories with respect to it using eq. (16). The trajectories must be initially normalized to avoid trivial solutions where ev- ery trajectory is pushed towards the larger parts of the reference trajectory. For illustrative purposes, the plots of fig. 14 show a sample of the lagged trajectory from the finger platform and its correction by the above method.

Section Title: SOLUTION TO SPATIAL NOISE
  SOLUTION TO SPATIAL NOISE The spatial noise can be a stochastic function of the actuator, environmental change, and electronic drivers. In a perfect model of the transition dynamics x t+1 = f (x t , u t ), applying the same control sequence {u 0 , u 1 , . . . , u T −1 } always results in the same sequence of states {x 1 , x 2 , . . . , x T } when it starts from the same initial state x 0 . This assumption is often violated in physical systems as different runs of the same policy may result in different trajectories as can be seen in fig. 10 in the Appendix. The noise in the dynamics can be any function of states, input, and time. Therefore, it is difficult to model this noise since it requires a prohibitively large number of random experiments. The good news is that if the physical system is built properly, the effect of this noise is expectedly low. Based on our observations from the finger platform, we can assume the following. Assumption 2. Limit on the physical noise: Let's the control sequence U = {u 0 , u 1 , . . . , u T −1 } be applied to the system M times resulting in multiple sequence of states T (1) , T (2) , . . . , T (M ) . There exists a relatively small ζ such that The word relatively here means that the change of the trajectory due to the inherent physical noise of the system must be small compared to the change of the trajectories when the parameters of the policy are perturbed. To reduce the sensitivity of the estimated gradient to this unwanted spatial noise, we divide the state space of the physical system into regularly located adjacent cells called voxels. Each voxel vox(c) is represented by its center c and is defined as vox(c) = {x ∈ X | x − c ∞ ≤ γ} (18) where γ is the parameter of the voxelization. The concept of the voxel is roughly used as a super- state. Every state that ends up within vox(c) gives rise to the same superstate. After recording the trajectories form the robot, every state is mapped to the center of the voxel it belongs to as After voxelization, we work with c instead of x. For example, all the gradients of (7) are computed as ∇ θ c rather than ∇ θ x. To illustrate the positive effect of voxelization of the state space, it can be seen in  fig. 3  that increasing the voxel size improves the overlapping between two trajectories that deviate from each other due to the inherent spatial noise of the system not because of perturbing the parameters of the policy, but because of the inherent imperfection of the mechanical and elec- trical components of the system. This benefit comes with a cost which is the error introduced by voxelization. Fortunately, this error is bounded due to the following lemma Lemma 3. The error caused by voxelization is bounded and inversely proportional to the size of each voxel (see appendix F.1 for a brief proof). After dealing with the challenge of inherent noise, we pursue the main goal of this paper which is estimating ∂T /∂θ directly from the physical system. In the following, we investigate the use of the different type of controllers to emphasize the extent of applicability of the proposed method.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we show how physical derivatives can be estimated in practice through several ex- periments. Notice that our work is different from computing gradients around the working point of a system by finite-difference. We aim to collect samples from such gradients by perturbing a grid of nominal values of the policy parameters and then generalize to unseen perturbations by Gaussian process as a probabilistic regression method. The experiments are designed to show each challenge separately and the efficacy of our proposed solution to it. Due to space constraints, details to the physical platform can be found in section A in the Appendix. See 1 for videos of the robot while collecting data for different experiments and more backup materials.

Section Title: LINEAR OPEN-LOOP CONTROLLER
  LINEAR OPEN-LOOP CONTROLLER As a simple yet general policy, in this section, we consider an open-loop controller which is a linear function of time. The policy u t = [u 1t , u 2t , u 3t ] constitutes the applied torques to the three motors {m 1 , m 2 , m 3 } of the system and is assigned as Notice that the torque consists of two terms. The first term w i t grows with time and the second term remains constant. The controller has 6 parameters in total denoted by θ. The task is to predict ∇ θ x t for every t along the trajectory. In the training phase, the training data is obtained via shaking as described in section 2.  fig. 7  shows examples of nominal trajectories + trajectories produced by the perturbed controller and the computed derivatives. The arrows are plotted as they originate from the perturbed trajectories only for easier distinction. Each arrow corresponds to the change of the states at a certain time step on the source trajectory as a result of perturbing the policy. Each figure corresponds to a pair of nominal values of {w, b} for the linear open-loop controller. See fig. 29 for examples.

Section Title: NONLINEAR OPEN-LOOP CONTROLLER
  NONLINEAR OPEN-LOOP CONTROLLER Physical derivatives can naturally be computed for either linear or nonlinear controllers which makes it different from taking the gradient of models through time. In model-based methods, if the model's transition dynamics is not differentiable, taking the derivative is theoretically challenging. However, our method is taking advantage of the real physics of the system to compute the gradients regardless of whether the approximating model is differentiable or not. To elaborate more on this, we test our method for a simple but nonlinear policy, i.e., u t = A sin(ωt). The sinusoidal torque is applied Under review as a conference paper at ICLR 2020 to either one or two motors of the system to investigate the performance of our method. We tested Gaussian and uniform shaking for θ = {A, ω} as parameters of this controller. The GP interpolation for the partial derivatives at some time instances along the trajectory can be seen in  fig. 6  and more extensively in figs. 16 to 18 in the Appendix. One might be interested in the direction of the predicted derivative instead of its exact size. To this end, we take several test perturbations for every time step and use cos(α) as a measure of alignment between the predicted and ground-truth derivative vectors. The time evolution of the histogram of this measure along the trajectory shows a better alignment as time proceeds. This effect can be seen in figs. 27 and 28. This confirms our observation of initial transient noise in the system that dies out gradually by the progression of time. The overall performance of our method in predicting physical derivatices in unseen directions for two different shaking methods is shown in appendix E.

Section Title: FEEDBACK CONTROLLER
  FEEDBACK CONTROLLER Often in practice, the policy incorporates some function of the states of the system. Some well- known examples which have been extensively used in control applications are P, PD, PI and PID controllers. Here, we consider two member of this family, i.e., P and PD controllers. The policy becomes u = K p e for P controllers and u = K p e + K dė for PD controllers. The error e shows the difference between the current state x and the desired state x * . The parameters of the controller {K p , K d } are scalar values that are multiplied by the error vector element wise. This implies that the controller parameters are the same for three motors leaving the controller of the whole plat- form with two parameters that weights the value and the rate of the error. We applied the uniform and Gaussian shaking for the set of parameters θ = {K p , K d } with different scenarios. The GP interpolation for the physical derivatives at some time instances along the trajectory can be seen in  fig. 6  and more extensively in figs. 19 to 24 in the Appendix. The time evolution of the histogram of misalignment between predicted and ground-truth directional derivatives (see figs. 25 and 28 in the appendix) once again confirms the existence of the initial transient noise as was also observed in the section 4.2. Similar to the sinusoidal experiment, the overall performance of our method is presented in appendix E.

Section Title: ZERO-SHOT PLANNING TASK
  ZERO-SHOT PLANNING TASK Our previous experiments in sections 4.1,4.2 and 4.3 showed that learning the physical derivative map is feasible for various types of controllers. In this section, we demonstrate an example of a constrain satisfaction task by means of the physical derivative map. In this experiment, the su- perscript (s) corresponds to the nominal trajectory which is called source. Assuem the system is controlled by a PD controller to reach a target state x * , i.e., the control torques are designed as u = k (s) p (x − x * ) + k (s) dẋ . The controller does a decent job to reach the target state given rea- sonable values for k p and k d . However, such controller does not give us a clear way to shape the trajectory that starts from x • and ends at x * . Assume it is desired that the nominal controlled trajectory T (s) passes through an intermediate state x * t at time t on its way towards the target state x * (we can equally assume that the system must avoid some regions of the state space be- cause of safety reasons). The solution with physical derivatives is as follows . Assume k (s) d is fixed and only k (s) p is changeable. If the physical derivatives map is available, we have access tô g t (k * p − k (s) p ) = (x * t − x (s) t )/(k * p − k (s) p ). By simple algebraic rearrangement, we have The new parameter of the policy is supposed to push the source trajectory T (s) towards a target trajectory T * that passes through the desired state x * t at time t. The result of this experiment on our physical finger platform can be seen in  fig. 8 .

Section Title: RELATED WORKS
  RELATED WORKS A truly intelligent agent must develop some sort of general competence that allows it to combine primitive skills to master a range of tasks not only a single task associated with a specified re- ward function. The major part of such competence come from unsupervised experiences. Animals use a similar competence to quickly adapt to new environments (Weng et al., 2001). and function efficiently soon after birth before being exposed to massive supervised experience (Zador, 2019). Due to its generality, such basic skills can be inherited over generations rather than being learned from scratch (Gaier & Ha, 2019). Despite traditional RL that the learning is driven by an extrin- sic reward signal, intrinsically motivated RL concerns task-agnostic learning. Similar to animals' babies (Touwen et al., 1992), the agent may undergo a developmental period in which it acquires reusable modular skills (Kaplan & Oudeyer, 2003; Weng et al., 2001) such as curiosity and confi- dence (Schmidhuber, 1991a; Kompella et al., 2017). Another aspect of such general competence is the ability of the agent to remain safe during its learning and deployment period (Garcıa & Fernández, 2015). In physical systems especially continuous control, stability is a major aspect of safety that implies states of the system converge to some invariant sets or remain within a certain bound (Lyapunov, 1992). Control theory often assumes the model of the system known to guarantee stability (Khalil, 2002). In the absence of the model, model-based RL learns the model along with Under review as a conference paper at ICLR 2020 the policy. Hence, learning the transition model to predict the states in the future can be another intrinsic reward. From a technical point of view, our work is relevant to sensitivity analysis and how it is used to train the parameters of models such as in Chen et al.'s NeuralODE. The method seemed to be effective in many tasks including learning dynamics (Rudy et al., 2019) , optimal control (Han et al., 2018), and generative models (Grathwohl et al., 2018). Our method can be seen as a mode-free sensitivity analysis in real-world systems. In NeuralODE, the gradient with respect to the parameters requires solving ODEs for both states and adjoint states that require a transition model. Since we are working directly on the physical system, we don't need to calculate the integrals forward in time. The systems itself acts as a physical ODE solver. We refer to appendix F for a more detailed review of the related works.

Section Title: CONCLUSION
  CONCLUSION In this paper, we present a method to learn the way that the trajectories of a physical real world dynamical system changes with respect to a change in the policy parameters. We tested our method on a custom-built platform called finger robot that allows testing a couple of controllers with various settings to show the applicability of our method for linear, nonlinear, open-loop, and feedback con- trollers. By estimating the physical derivative function, we showed that our method is able to push a controlled trajectory towards a target intermediate state. We investigate the real-world challenges when doing a fine sensitive task such as estimating physical derivatives on a real robot and proposed solutions to make our algorithm robust to inherent imperfection and noise in physical systems. We focused mainly on low-level issues of physical derivative and showing the feasibility of estimating it robustly. We expect that physical derivatives will contribute to research areas such as safety, control with constrain satisfaction and trajectory planning, robust or safe control.

```
