Title:
```
Under review as a conference paper at ICLR 2020 SCHEDULED INTRINSIC DRIVE: A HIERARCHICAL TAKE ON INTRINSICALLY MOTIVATED EXPLORATION
```
Abstract:
```
Exploration in sparse reward reinforcement learning remains an open challenge. Many state-of-the-art methods use intrinsic motivation to complement the sparse extrinsic reward signal, giving the agent more opportunities to receive feedback during exploration. Commonly these signals are added as bonus rewards, which results in a mixture policy that neither conducts exploration nor task fulfillment resolutely. In this paper, we instead learn separate intrinsic and extrinsic task policies and schedule between these different drives to accelerate exploration and stabilize learning. Moreover, we introduce a new type of intrinsic reward denoted as successor feature control (SFC), which is general and not task-specific. It takes into account statistics over complete trajectories and thus differs from previ- ous methods that only use local information to evaluate intrinsic motivation. We evaluate our proposed scheduled intrinsic drive (SID) agent using three different environments with pure visual inputs: VizDoom, DeepMind Lab and DeepMind Control Suite. The results show a substantially improved exploration efficiency with SFC and the hierarchical usage of the intrinsic drives. A video of our exper- imental results can be found at https://gofile.io/?c=HpEwTd.
```

Figures/Tables Captions:
```
Figure 1: The four-room domain (Sutton et al., 1999). The agent starts at the red cross and transitions to an adjacent state at each time step. The goal is to explore the four rooms when no extrinsic reward is provided. In a) each state is annotated by its SD (Eq.3) to the starting state and b) shows for each state the highest possible SFC reward (Eq.4) for a one-step transition from it. Here the successor features are learned using a random walk. c) and d) show a comparison between visitation counts of each state from a random agent and an agent that uses the SFC rewards for control via Q-learning. In the latter case the successor features are learned from scratch via TD. In this environment, the agent receives high rewards for crossing bottleneck states, when the SF are learned beforehand, using a random policy. But even when the SF are learned during exploration, bottleneck states are still visited disproportionately high. Furthermore the intrinsic reward greatly improves exploration compared to a random agent. For implementation details see Appendix. D.4 features (SF) (Kulkarni et al., 2016b; Barreto et al., 2017) extend the concept to an arbitrary feature embedding φ : S → R m . For a fixed policy π and embedding φ the SF is defined by the |m|- dimensional vector
Figure 2: VizDoom environments we evaluated on. 2a and 2b show the top-down views of My- WayHome and FlytrapEscape with the same downscaling ratio, with red dots marking the starting locations, green dots indicating the goal locations; 2c and 2d to 2f show exemplary first-person views captured from the marked poses (blue dots with arrows) from those two maps respectively.
Figure 3: Extrinsic rewards per episode obtained in MyWayHome (left) and FlytrapEscape (right). Each plot shows the mean over 5 non-tuned random seeds. Figures showing the learning curves for each run can be found in the Appendix in Figure 14 and 13.
Figure 4: Extrinsic rewards per episode obtained in AppleDistractions (left) and Cartpole (right). Each plot shows the mean over 5 non-tuned random seeds. Left: Each agent is evaluated on the same 5 sets of random floor and wall textures, with 5 non-tuned environment seeds. In the ablation study (Appendix A) the SID variant outperforms the reward bonus variant of each of the 3 types of intrinsic rewards. Right: Ours also outperforms all baseline agents in the very different domain of classic control from pixels, which shows the general applicability of our proposed agent. Figures showing the learning curves for each run can be found in the Appendix in Figure 15 and 16.
Figure 5: Ablation study results for AppleDistractions. Each plot shows the mean over 5 non-tuned random seeds.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) agents learn on evaluative feedback (reward signals) instead of in- structive feedback (ground truth labels), which takes the process of automating the development of intelligent problem-solving agents one step further ( Sutton & Barto, 2018 ). With deep networks as powerful function approximators bringing traditional RL into high-dimensional domains, deep reinforcement learning (DRL) has shown great potential ( Mnih et al., 2015 ; 2016;  Schulman et al., 2017 ;  Horgan et al., 2018 ). However, the success of DRL often relies on carefully shaped dense ex- trinsic reward signals. Although shaping extrinsic rewards can greatly support the agent in finding solutions and shortening the interaction time, designing such dense extrinsic signals often requires substantial domain knowledge, and calculating them typically requires ground truth state informa- tion, both of which is hard to obtain in the context of robots acting in the real world. When not carefully designed, the reward shape could sometimes serve as bias or even distractions and could potentially hinder the discovery of optimal solutions. More importantly, learning on dense extrinsic rewards goes backwards on the progress of reducing supervision and could prevent the agent from taking full advantage of the RL framework. In this paper, we consider terminal reward RL settings, where a signal is only given when the final goal is achieved. When learning with only an extrinsic terminal reward indicating the task at hand, intelligent agents are given the opportunity to potentially discover optimal solutions even out of the scope of the well established domain knowledge. However, in many real-world problems defining a task only by a terminal reward means that the learning signal can be extremely sparse. The RL agent would have no clue about what task to accomplish until it receives the terminal reward for the first time by chance. Therefore in those scenarios guided and structured exploration is crucial, which is where intrinsically-motivated ex- ploration ( Oudeyer & Kaplan, 2008 ;  Schmidhuber, 2010 ) has recently gained great success ( Pathak et al., 2017 ;  Burda et al., 2018b ). Most commonly in current state-of-the-art approaches, an intrinsic reward is added as a reward bonus to the extrinsic reward. Maximizing this combined reward signal, however, results in a mixture policy that neither acts greedily with regard to extrinsic reward max- Under review as a conference paper at ICLR 2020 imization nor to exploration. Furthermore, the non-stationary nature of the intrinsic signals could potentially lead to unstable learning on the combined reward. In addition, current state-of-the-art methods have been mostly looking at local information calculated out of 1-step lookahead for the estimation of the intrinsic rewards, e.g. one step prediction error ( Pathak et al., 2017 ), or network distillation error of the next state ( Burda et al., 2018b ). Although those intrinsic signals can be prop- agated back to earlier states with temporal difference (TD) learning, it is not clear that this results in optimal long-term exploration. We seek to address the aforementioned issues as follows: 1. We propose a hierarchical agent scheduled intrinsic drive (SID) that focuses on one moti- vation at a time: It learns two separate policies which maximize the extrinsic and intrinsic rewards respectively. A high-level scheduler periodically selects to follow either the extrin- sic or the intrinsic policy to gather experiences. Disentangling the two policies allows the agent to faithfully conduct either pure exploration or pure extrinsic task fulfillment. More- over, scheduling (even within an episode) implicitly increases the behavior policy space exponentially, which drastically differs from previous methods where the behavior policy could only change slowly due to the incremental nature of TD learning. 2. We introduce successor feature control (SFC), a novel intrinsic reward that is based on the concept of successor features. This feature representation characterizes states through the features of all its successor states instead of looking at local information only. This implicitly makes our method temporarily extended, which enables more structured and far- sighted exploration that is crucial in exploration-challenging environments. We note that both the proposed intrinsic reward SFC and the hierarchical exploration framework SID are without any task-specific components, and can be incorporated into existing DRL methods with minimal computation overhead. We present experimental results in three sets of environments, evaluating our proposed agent in the domains of visual navigation and control from pixels, as well as its capabilities of finding optimal solutions under distraction.

Section Title: RELATED WORK
  RELATED WORK Intrinsic Motivation and Auxiliary Tasks Intrinsic motivation can be defined as agents conduct- ing actions purely out of the satisfaction of its internal rewarding system rather than the extrinsic rewards ( Oudeyer & Kaplan, 2008 ;  Schmidhuber, 2010 ). There exist various forms of intrinsic mo- tivation and they have achieved substantial improvement in guiding exploration for DRL, in tasks where extrinsic signals are sparse or missing altogether. ( Pathak et al., 2017 ) proposed to evaluate curiosity, one of the most widely used kinds of intrinsic motivation, with the 1-step prediction error of the features of the next state made by a forward dynamics model. Their ICM module has been shown to work well in visual domains including first-person view navigation. Since ICM is potentially susceptible to stochastic transitions ( Burda et al., 2018a ),  Burda et al. (2018b)  propose as a reward bonus the error of predicting the features of the current state output by a randomly initialized fixed embedding network. The value function is decomposed for extrinsic and intrinsic reward, but different to us a single mixture policy is learned. Another form of curiosity, learning progress or the change in the prediction error, has been connected to count-based exploration via a pseudo-count ( Bellemare et al., 2016 ;  Ostrovski et al., 2017 ) and has also been used as a reward bonus.  Savinov et al. (2018)  propose to train a reachability network, which gives out a reward based on whether the current state is reachable within a certain amount of steps from any state in the current episode. Similar to our proposed SFC, their intrinsic motivation is related to choosing states that could lead to novel trajectories. However, we use two different distance metrics, theirs is explicitly learned to be proportional to the time step differences while ours is based on successor features which measures two states by the difference of the average feature activations of future trajectories. Moreover, their method rewards states with high distance to the states in the current episode while our method rewards states with high distance to the states also from past trajectories, as the successor features are trained from samples of the replay buffer. Auxiliary tasks have been proposed for learning more representative and distinguishable features.  Mirowski et al. (2016)  add depth prediction and loop closure prediction as auxiliary tasks for learn- ing the features.  Jaderberg et al. (2016)  learn separate policies for maximizing pixel changes (pixel control) and activating units of a specific hidden layer (feature control). However, their proposed Under review as a conference paper at ICLR 2020 UNREAL agent never follows those auxiliary policies as they are only used to learn more suitable features for the main extrinsic task. Hierarchical RL Various HRL approaches have been proposed ( Kulkarni et al., 2016a ;  Bacon et al., 2017 ;  Vezhnevets et al., 2017 ;  Krishnan et al., 2017 ). In the context of intrinsic motivation, feature control ( Jaderberg et al., 2016 ) has been adopted into a hierarchical setting ( Dilokthanakul et al., 2017 ), in which options are constructed for altering given features. However, they report that a flat policy trained on the intrinsic bonus achieves similar performance to the hierarchical agent. Our hierarchical design is perhaps inspired mostly by the work of  Riedmiller et al. (2018) . Un- like other HRL approaches that try to learn a set of options ( Sutton et al., 1999 ) to construct the optimal policy, their proposed SAC agent aims to learn one flat policy that maximizes the extrinsic reward. While SAC schedules between following the extrinsic task and a set of pre-defined auxiliary tasks such as maximizing touch sensor readings or translation velocity, in this paper we investigate scheduling between the extrinsic task and intrinsic motivation that is general and not task-specific. A concurrent work along this line is presented by  Beyer et al. (2019) .

Section Title: Successor Representation
  Successor Representation The successor representation (SR) was first introduced to improve gen- eralization in TD learning ( Dayan, 1993 ). While previous works extended SR to the deep setting for better generalized navigation and control algorithms across similar environments and changing goals ( Kulkarni et al., 2016b ;  Barreto et al., 2017 ;  Zhang et al., 2017 ), we focus on its temporarily extended property to accelerate exploration. SR has also been investigated under the options framework.  Machado et al. (2017) ;  Tomar* et al. (2019)  evaluate successor features with random policies to discover bottlenecks or landmarks based on the clustering of such features. Options are then learned to navigate to those sub-goals. However, it remained unclear if the options framework would help in sparse exploration setups. When using SR to measure the intrinsic motivation, the most relevant work to ours is that of  Machado et al. (2018) . They also design a task-independent intrinsic reward based on SR, how- ever they rely on the concept of count-based exploration and propose a reward bonus, that vastly differs from ours. Their bonus is inverse proportional to the norm of the SR while our formulation rewards change in the SR of two successive states. We will present our proposed method in the next section.

Section Title: METHODS
  METHODS We use the RL framework for learning and decision-making under uncertainty. It is formalized by Markov decision processes (MDPs) defined by the tuple S, A, p, r, γ . At time step t the agent samples an action a ∈ A according to policy π(·|s), which depends on its current state s ∈ S. The agent receives a scalar reward r ∈ R and transits to the next state s ∈ S. The distribution of the corresponding state, action and reward process (S t , A t , R t+1 ) is determined by the distribution of the initial state S 0 , the transition operator p and the policy π. The goal of the agent is to find a policy that maximizes the expectation of the sum of discounted rewards T k=0 γ k R t+k+1 . We seek to speed up learning in sparse reward RL, where the reward signal is uninformative for almost all transitions. We set the focus on terminal reward scenarios, where the agent only receives a single reward of +1 for successfully accomplishing the task and 0 otherwise. We will first introduce our proposed intrinsic reward successor feature control (SFC) (3.1,3.2), then present our proposed hierachical framework for accelerating intrinsically motivated exploration, which we denote as scheduled intrinsic drive (SID) (Sec.3.3,3.4).

Section Title: SUCCESSOR DISTANCE METRIC
  SUCCESSOR DISTANCE METRIC In order to encode long-term statistics into the design of intrinsic rewards for far-sighted exploration, we build on the formulation of successor represention (SR), which introduces a temporally extended view of the states.  Dayan (1993)  introduced the idea of representing a state s by the occupancies of all other states from a process starting in s following a fixed policy π, where the occupancies denote the average number of time steps the state process stays in each state per episode. Analogously, the SF represent the average discounted feature activations, when starting in s and following π. They can be learned by temporal difference (TD) updates SF have several interesting properties which make them appealing as a basis for an intrinsic reward signal: 1) They can be learned even in the absence of extrinsic rewards and without learning a tran- sition model and therefore combine advantages of model-based and model-free RL ( Stachenfeld et al., 2014 ). 2) They can be learned via computationally efficient TD. 3) They capture the expected feature activations for complete episodes. Therefore they contain information even of spatially and temporarily distant states which might help for effective far-sighted exploration. Given the discus- sion, we introduce the successor distance (SD) metric that measures the distance between states by the similarity of their SF Fig.1 a) shows an example of the successor distance metric in the tabular case. There the SD roughly correlates to the length of the shortest path between the states. Using this metric to evaluate the intrinsic motivation, one choice could be to use the SD to a fixed anchor state as the intrinsic reward, which depends heavily on the anchor position. Even when a sensible choice for the anchor can be found, e.g. the initial state of an episode, the SDs of distant states from the anchor assimilate. For a pair of states with a fixed spatial distance, their SD is higher when they are located in different rooms and the SD increases substantially when crossing rooms. Therefore the metric might capture the connectivity of the underlying state space.

Section Title: SUCCESSOR FEATURE CONTROL
  SUCCESSOR FEATURE CONTROL This observation motivates us to define the intrinsic reward successor feature control (SFC) as the squared SD of a pair of consecutive states A high SFC reward indicates a big change in the future feature activations when π is followed. We argue this big change is a strong indicator of bottleneck states, since in bottlenecks a minor change in the action selection can lead to a vastly different trajectory being taken. Fig.1b) shows that those highly rewarding states under SFC and the true bottlenecks agree, which can be very valuable for exploration ( Lehnert et al., 2018 ).

Section Title: SCHEDULED INTRINSIC DRIVE
  SCHEDULED INTRINSIC DRIVE The classical way of adding the intrinsic reward to the extrinsic reward has several drawbacks. First, the final policy is not trained to maximize the actual objective but a mixed version. Second, the intrinsic reward signal is usually changing over time. Including this non-stationary signal in the overall reward can make learning of the actual task unstable. Furthermore, the performance is often extremely sensitive to the scaling of the intrinsic reward relative to the extrinsic and hence it has to be tuned very carefully for every environment. To overcome these issues we propose scheduled intrinsic drive (SID), which learns two separate policies, one for each reward signal. During each episode the scheduler samples several times which of the two policies to follow for the next time steps. Each policy is trained off-policy from all of the transitions irrespective of which policy collected the data. As SID does not add the two reward signals no scaling parameter is needed. A policy is learned that exclusively maximizes extrinsic reward and hence neither the final policy nor the learning process is disturbed by the intrinsic reward. At the same time exploration is ensured as there is experience collected by the policy that learns from the intrinsic reward. Furthermore, scheduling can help exploration as each policy is acted on for an extended time interval, allowing long-term exploration instead of local exploration. Besides that the agent is less susceptible to always go to a nearby small reward instead of looking for other larger rewards that maybe further away. A mixture policy might be attracted to the small reward while with SID the exploration policy is followed for several timesteps which can bring the agent to new states with larger rewards that it did not know of before. We investigated several types of high-level schedulers, however, none of them consistently outper- forms a random one. We present possible explanations why a random scheduler already performs well and present them in Appendix F along with the different scheduler choices we tested.

Section Title: ALGORITHM IMPLEMENTATION
  ALGORITHM IMPLEMENTATION Our proposed method can be combined with an any approach that allows off-policy learning. This section describes an instantiation of the SID framework when using Ape-X DQN as a basic off- policy DRL algorithm  Horgan et al. (2018)  with SFC as the intrinsic reward, which we used for all experiments. We depict this algorithm instance in Appendix Figure 8 and more details are provided in Appendix C. The algorithm is composed of: • A Q-Net {θ ϕ , θ E , θ I }: Contains a shared embedding θ ϕ and two Q-value output heads θ E (extrinsic) and θ I (intrinsic). • A SF-Net {θ φ , θ ψ }: Contains an embedding θ φ and a successor feature head θ ψ . θ φ is ini- tialized randomly and kept fixed during training. The output of SF-Net is used to calculate the SFC intrinsic reward (Eq.4). The SF-net is trained with the samples of the replay buffer, which contains the experience generated by the behavior policy. • A high-level scheduler: Instantiated in each actor, selects which policy to follow (extrin- sic or intrinsic) after a fixed number of environment steps (max episode length/M ). The scheduler randomly picks one of the tasks with equal probability. • N parallel actors (N = 8): Each actor instantiates its own copy of the environment, peri- odically copies the latest model from the learner. We learn from K-step targets (K = 5), so each actor at each environment step stores (s t−K , a t−K , K k=1 γ k−1 r t−K+k , s t ) into a shared replay buffer. Each actor will act according to either the extrinsic or the intrinsic policy based on the current task selected by its scheduler. • A learner: Learns the Q-Net (θ E and θ I are learned with the extrinsic and intrinsic reward re- spectively) and the SF-Net from samples (Eq.2) from the same shared replay buffer, which contains all experiences collected from following different policies.

Section Title: EXPERIMENTS
  EXPERIMENTS We evaluate our proposed intrinsic reward SFC and the hierarchical framework of intrinsic motiva- tion SID in three sets of simulated environments: VizDoom ( Kempka et al., 2016 ), DeepMind Lab ( Beattie et al., 2016 ) and DeepMind Control Suite ( Tassa et al., 2018 ). Throughout all experiments, agents receive as input only raw pixels with no additional domain knowledge or task specific in- formation. We mainly compare the following agent configurations: M: Ape-X DQN with 8 actors, train with only the extrinsic main task reward; ICM: train a single policy with the ICM reward bonus ( Pathak et al., 2017 ); RND: train a single policy with the RND reward bonus ( Burda et al., 2018b ); Ours: with our proposed SID framework, schedule between following the extrinsic main task policy and the intrinsic policy trained with our proposed SFC reward. We carried out an ablation study, where we compare the performance of an agent with intrinsic and extrinsic reward summed up, to the corresponding SID agent for each intrinsic reward type (ICM, RND, SFC). We present the plots and discussions in Section 4.4 Appendix A. For the intrinsic reward normalization and the scaling for the extrinsic and intrinsic rewards we do a parameter sweep for each environment (Appendix C.4) and choose the best setting for each agent. We notice that our scheduling agent is much less sensitive to different scalings than agents with added reward bonus. Since our proposed SID setup requires an off-policy algorithm to learn from experiences generated by following different policies, we implement all the agents under the Ape-X DQN framework  Horgan et al. (2018) . After a parameter sweep we set the number of scheduled tasks per episode to M = 8 for our agent in all experiments, meaning each episode is divided into up to 8 sub-episodes, and for each of which either the extrinsic or the intrinsic policy is sampled as the behavior policy. Appendix C and D contain additional information about experimental setups and model training details.

Section Title: VIZDOOM: SPARSE NAVIGATION
  VIZDOOM: SPARSE NAVIGATION We start by verifying our implementation of the baseline algorithms in "DoomMyWayHome" which was previously used in several state-of-the-art intrinsic motivation papers ( Pathak et al., 2017 ;  Savi- nov et al., 2018 ). The agent needs to navigate based only on first-person view visual inputs through 8 rooms connected by corridors (Fig.2a), each with a distinct texture (Fig.2c). The experimental results are shown in Fig.3 (left). Since our basic RL algorithm is doing off-policy learning, it has Under review as a conference paper at ICLR 2020 relatively decent random exploration capabilities. We see that the M agent is able to solve the task sometimes without any intrinsically generated motivations, but that all intrinsic motivation types help to solve the task more reliably and speed up the learning. Our method solve the task the fastest, but also ICM and RND learn to reach the goal reliably and efficiently. We wanted to test the agents on a more difficult VizDoom map where structured exploration would be of vital importance. We thus designed a new map which scales up the navigation task of MyWay- Home. Inspired by how flytraps catch insects, we design the layout of the rooms in a geometrically challenging way that escaping from one room to the next with random actions is extremely unlikely. We show the layout of MyWayHome (Fig.2a) and FlytrapEscape (Fig.2b) with the same downscal- ing ratio. The maze consists of 4 rooms separated by V-shaped walls pointing inwards the rooms. The small exits of each room is located at the junction of the V-shape, which is extremely difficult to maneuver into without a sequence of precise movements. As in the MyWayHome task, in each episode, the agent starts from the red dot shown in Fig.2b with a random orientation. An episode terminates if the final goal is reached and the agent will receive a reward of +1, or if a maximum episode steps of 10,000 (2100 for MyWayHome) is reached. The task is to escape the fourth room. The experimental results on FlytrapEscape are shown in Fig.3 (right). Neither M nor RND manages to learn any useful policies. ICM solves the task in sometimes, while we can clearly observe that our method efficiently explores the map and reliably learns how to navigate to the goal. We visualize the learned successor features in Appendix E and its evolution over time is shown in the video https://gofile.io/?c=HpEwTd.

Section Title: DEEPMIND LAB: EXPLORATION UNDER DISTRACTION
  DEEPMIND LAB: EXPLORATION UNDER DISTRACTION In the second experiment, we set out to evaluate if the agents would be able to reliably collect the faraway big reward in the presence of small nearby distractive rewards. For this experiment we use the 3D visual navigation simulator of DeepMind Lab ( Beattie et al., 2016 ). We constructed a challenging level "AppleDistractions" (Fig.10b) with a maximum episode length of 1350. In this level, the agent starts in the middle of the map (blue square) and can follow either of the two corridors. Each corridor has multiple sections and each section consists of two dead-ends and an entry to next section. Each section has different randomly generated floor and wall textures. One of the corridors (left) gives a small reward of 0.05 for each apple collected, while the other one (right) contains a single big reward of 1 at the end of its last section. The optimal policy would be to go for the single faraway big reward. But since the small apple rewards are much closer to the spawning location of the agent, the challenge here is to still explore other areas sufficiently often so that the optimal solution could be recovered. The results are presented in Fig.4 (left). Ours received on average the highest rewards and is the only method that learns to navigate to the large reward in every run. The baseline methods get eas- ily distracted by the small short-term rewards and do not reliably learn to navigate away from the distractions. With a separate policy for intrinsic motivation the agent can for some time interval completely "forget" about the extrinsic reward and purely explore, since it does not get distracted by the easily reachable apple rewards and can efficiently learn to explore the whole map. In the mean- while the extrinsic policy can simultaneously learn from the new experiences and might learn about Under review as a conference paper at ICLR 2020 the final goal discovered by the exploration policy. This highlights a big advantage of scheduling over bonus rewards, that it reduces the probability of converging to bad local optimums. In Section 4.4 we further showed that SID is generally applicable and also helps ICM and RND in this task.

Section Title: DEEPMIND CONTROL SUITE: CLASSIC CONTROL FROM PIXELS
  DEEPMIND CONTROL SUITE: CLASSIC CONTROL FROM PIXELS To show that our methods can be used in domains other than first-person visual navigation, we evaluate on the classic control task "cartpole: swingup sparse" ( DeepMind Control Suite Tassa et al. (2018) ), using third-person view images as inputs (Fig.11). The pole starts pointing down and the agent receives a single terminal reward of +1 for swinging up the unactuated pole using only horizontal forces on the cart. Additional details are presented in Appendix D.3. The results are shown in Fig.4 (right). Compared to the previous tasks, this task is easy enough to be solved without intrinsic motivation, but we can see also that all intrinsic motivation methods significantly reduce the interaction time. Ours still outperforms other agents even in the absence of clear bottlenecks which shows its general applicability, but since the task is relatively less challenging for exploration, the performance gain is not as substantial as the previous experiments.

Section Title: ABLATION STUDY
  ABLATION STUDY Further, we conducted an ablation study on AppleDistractions. We denote with "M+SFC", "M+RND", "M+ICM" the agents with one policy where the respective intrinsic reward is added to the extrinsic one. With "SID(M,SFC)", "SID(M,RND)", "SID(M,ICM)" the agents are named that have two policies, one for the respective intrinsic and one for the extrinsic reward and use SID Under review as a conference paper at ICLR 2020 to schedule between them. We note that the "SID(M,ICM)" agent corresponds to the "Ours" agent from the previous experiments. The results are presented in  Figure 5 . Our SID(M, SFC) agent received on average the highest rewards. Furthermore, we see that scheduling helped both ICM and SFC to find the goal and not settle for the small rewards, and SID also helps improve the per- formance of RND. The respective reward bonus counterparts of the three SID agents were more attracted to the small nearby rewards. This behavior is expected: By scheduling, the intrinsic policy of the SID agent is assigned with its own interaction time with the environment, during which it could completely "forget" about the extrinsic rewards. The agent then has a much higher probability of discovering the faraway big reward, thus escaping the distractions of the nearby small rewards. Once the intrinsic policy collects these experiences of the big reward, the extrinsic policy can im- mediately learn from those since both policies share the same replay buffer. Ablations for the other environments are reported in Appendix A.

Section Title: CONCLUSION
  CONCLUSION In this paper, we investigate an alternative way of utilizing intrinsic motivation for exploration in DRL. We propose a hierarchical agent SID that schedules between following extrinsic and intrinsic drives. Moreover, we propose a new type of intrinsic reward SFC that is general and evaluates the intrinsic motivation based on longer time horizons. We conduct experiments in three sets of envi- ronments and show that both our contributions SID and SFC help greatly in improving exploration efficiency. We consider many possible research directions that could stem from this work, including design- ing more efficient scheduling strategies, incorporating several intrinsic drives (that are possibly or- thogonal and complementary) instead of only one into SID, testing our framework in other control domains such as manipulation, combining the successor representation with learned feature repre- sentations and extending our evaluation onto real robotics systems.

```
