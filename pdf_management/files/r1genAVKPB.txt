Title:
```
Published as a conference paper at ICLR 2020 IS A GOOD REPRESENTATION SUFFICIENT FOR SAM- PLE EFFICIENT REINFORCEMENT LEARNING?
```
Abstract:
```
Modern deep learning methods provide effective means to learn good represen- tations. However, is a good representation itself sufficient for sample efficient reinforcement learning? This question has largely been studied only with respect to (worst-case) approximation error, in the more classical approximate dynamic programming literature. With regards to the statistical viewpoint, this question is largely unexplored, and the extant body of literature mainly focuses on conditions which permit sample efficient reinforcement learning with little understanding of what are necessary conditions for efficient reinforcement learning. This work shows that, from the statistical viewpoint, the situation is far subtler than suggested by the more traditional approximation viewpoint, where the re- quirements on the representation that suffice for sample efficient RL are even more stringent. Our main results provide sharp thresholds for reinforcement learning methods, showing that there are hard limitations on what constitutes good function approximation (in terms of the dimensionality of the representation), where we focus on natural representational conditions relevant to value-based, model-based, and policy-based learning. These lower bounds highlight that having a good (value- based, model-based, or policy-based) representation in and of itself is insufficient for efficient reinforcement learning, unless the quality of this approximation passes certain hard thresholds. Furthermore, our lower bounds also imply exponential separations on the sample complexity between 1) value-based learning with perfect representation and value-based learning with a good-but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning.
```

Figures/Tables Captions:
```
Table 1: Summary of theoretical results on reinforcement learning with linear function approximation. See Section 2 for discussion on this table. RL, Generative Model, Known Transition are defined in Section 3.3. Exact linear Q * : Assumption 4.1 with δ = 0. Approx linear Q * : Assumption 4.1 with δ = Ω
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Modern reinforcement learning (RL) problems are often challenging due to the huge state space. To tackle this challenge, function approximation schemes are often employed to provide a compact representation, so that reinforcement learning can generalize across states. A common paradigm is to first use a feature extractor to transform the raw input to features (a succinct representation) and then apply a linear predictor on top of the features. Traditionally, the feature extractor is often handcrafted ( Sutton & Barto, 2018 ), while more modern methods often train a deep neural network to extract features. The hope of this paradigm is that, if there exists a good low dimensional (linear) representation, then efficient reinforcement learning is possible. Empirically, combining various RL function approximation algorithms with neural networks for feature extraction has lead to tremendous successes on various tasks ( Mnih et al., 2015 ;  Schulman et al., 2015 ; 2017). A major problem, however, is that these methods often require a large amount of samples to learn a good policy. For example, deep Q-network requires millions of samples to solve certain Atari games ( Mnih et al., 2015 ). Here, one may wonder if there are fundamental statistical Published as a conference paper at ICLR 2020 limitations on such methods, and, if so, under what conditions it would be possible to efficiently learn a good policy? In the supervised learning context, it is well-known that empirical risk minimization is a statistically efficient method when using a low-complexity hypothesis space ( Shalev-Shwartz & Ben-David, 2014 ), e.g. a hypothesis space with bounded VC dimension. For example, polynomial number of samples suffice for learning a near-optimal d-dimensional linear classifier, even in the agnostic setting 1 . In contrast, in the more challenging RL setting, we seek to understand if efficient learning is possible (say from a sample complexity perspective) when we have access to an accurate (and compact) parametric representation - e.g. our policy class contains a near-optimal policy or our hypothesis class accurately approximates the optimal value function. In particular, this work focuses on the following question:

Section Title: Is a good representation sufficient for sample-efficient reinforcement learning?
  Is a good representation sufficient for sample-efficient reinforcement learning? This question has largely been studied only with respect to approximation error in the more classical approximate dynamic programming literature, where it is known that algorithms are stable to certain worst-case approximation errors. With regards to sample efficiency, this question is largely unexplored, where the extant body of literature mainly focuses on conditions which are sufficient for efficient reinforcement learning though there is little understanding of what are necessary conditions for efficient reinforcement learning. In reinforcement learning, there is no direct analogue of empirical risk minimization as in the supervised learning context, and it is not evident what are the statistical limits of learning based on properties of our underlying hypothesis class (which may be value-based, policy-based, or model-based). Many recent works have provided polynomial upper bounds under various sufficient conditions, and in what follows we list a few examples. For value-based learning, the work of  Wen & Van Roy (2013)  showed that for deterministic systems 2 , if the optimal Q-function can be perfectly predicted by linear functions of the given features, then the agent can learn the optimal policy exactly with polynomial number of samples. Recent work ( Jiang et al., 2017 ) further showed that if certain complexity measure called Bellman rank is bounded, then the agent can learn a near-optimal policy efficiently. For policy-based learning,  Agarwal et al. (2019)  gave polynomial upper bounds which depend on a parameter that measures the difference between the initial distribution and the distribution induced by the optimal policy.

Section Title: Our Contributions
  Our Contributions This paper gives, perhaps surprisingly, strong negative results to this question. The main results are exponential lower bounds in terms of planning horizon H for value-based, model- based, and policy-based algorithms with given good representations 3 . Notably, the requirements on the representation that suffice for sample efficient RL are even more stringent than the more traditional approximation viewpoint. A comprehensive summary of previous upper bounds and our lower bounds is given in  Table 1 , and here we briefly summarize our hardness results. 1. For value-based learning, we show even if Q-functions of all policies can be approximated by linear functions of the given representation with approximation error δ = Ω H d where d is the dimension of the representation and H is the planning horizon, then the agent still needs to sample exponential number of trajectories to find a near-optimal policy. 2. For model-based learning, we show even if the transition matrix and the reward function can be approximated by linear functions of the given representation with approximation error δ = Ω H d (in ∞ sense), the agent still needs to sample exponential number of trajectories to find a near-optimal policy. 3. We show even if optimal policy can be perfectly predicted by a linear function of the given representation with a strictly positive margin, the agent still requires exponential number of trajectories to find a near-optimal policy.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 These lower bounds hold even in deterministic systems and even if the agent knows the transition model. Note these negative results apply to the case where the Q-function, the model, or the optimal policy can be predicted well by a linear function of the given representation. Since the class of linear functions is a strict subset of many more complicated function classes, including neural networks in particular, our negative results imply lower bounds for these more complex function classes as well. Our results highlight the following conceptual insights: • The requirements on the representation that suffice for sample efficient RL are significantly more stringent than the more traditional approximation viewpoint; our statistical lower bounds show that there are hard thresholds on the worst-case approximation quality of the representation which are not necessary from the approximation viewpoint. • Since our lower bounds apply even when the agent knows the transition model, the hardness is not due to the difficulty of exploration in the standard sense. The unknown reward function is sufficient to make the problem exponentially difficult. • Our lower bounds are not due to the agent's inability to perform efficient supervised learning, since our assumptions do admit polynomial sample complexity upper bounds if the data distribution is fixed. • Our lower bounds are not pathological in nature and suggest that these concerns may arise in practice. In a precise sense, almost all feature extractors induce a hard MDP instance in our construction (see Section 4.4). Instead, one interpretation is that the hardness is due to a distribution mismatch in the following sense: the agent does not know which distribution to use for minimizing a (supervised) learning error (see  Kakade (2003)  for discussion), and even a known transition model is not information-theoretically sufficient to reduce the sample complexity. Furthermore, our work implies several interesting exponential separations on the sample complexity between: 1) value-based learning with perfect representation and value-based learning with a good- but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning. We provide more details in Section 5.

Section Title: RELATED WORK
  RELATED WORK A summary of previous upper bounds, together with lower bounds proved in this paper, is provided in  Table 1 . Some key assumptions are formally stated in Section 3 and Section 4. Our lower bounds highlight that classical complexity measures in supervised learning including small approximation error and margin, and standard assumptions in reinforcement learning including optimality gap and deterministic systems, are not enough for efficient RL with function approximation. We need additional assumptions, e.g., ones used in previous upper bounds, for efficient RL.

Section Title: PREVIOUS LOWER BOUNDS
  PREVIOUS LOWER BOUNDS Existing exponential lower bounds, to our knowledge, construct unstructured MDPs with an ex- ponentially large state space and reduce a bandit problem with exponentially many arms to an MDP ( Krishnamurthy et al., 2016 ;  Sun et al., 2017 ). However, these lower bounds cannot apply to MDPs whose transition models, value functions, or policies can be approximated with some natural function classes, e.g., linear functions, neural networks, etc. The current paper gives the first set of lower bounds for RL with linear function approximation (and thus also hold for super classes of linear functions such as neural networks).

Section Title: PREVIOUS UPPER BOUNDS
  PREVIOUS UPPER BOUNDS We divide previous algorithms (with provable guarantees) into three classes: those that utilize uncertainty-based bonuses (e.g. UCB variants or Thompson sampling variants); approximate dynamic programming variants (which often make assumptions with respect to concentrability coefficients); and direct policy search-based methods (such as conserve policy iteration ( CPI, see Kakade (2003) ) or policy gradient methods, which make assumptions with respect to distribution mismatch coefficients). The first class of methods include those based on witness rank, Belman rank, and the Eluder dimension, while the latter two classes of algorithms make assumptions either on concentrability coefficients or on distribution mismatch coefficients (see  Agarwal et al. (2019) ;  Scherrer (2014)  for discussions).

Section Title: Uncertainty bonus-based algorithms
  Uncertainty bonus-based algorithms Now we discuss existing theoretical results on value-based learning with function approximation. The most relevant work is  Wen & Van Roy (2013)  which showed in deterministic systems, if the optimal Q-function is within a pre-specified function class which has bounded Eluder dimension, for which the class of linear functions is a special case, then the agent can learn the optimal policy using polynomial number of samples. This result has recently been generalized by  Du et al. (2019a)  which can deal with stochastic reward and low variance transition but requires strictly positive optimality gap. As we listed in  Table 1 , it is an open problem whether the condition that the optimal Q-function is linear itself is sufficient for efficient RL.  Li et al. (2011)  proposed a Q-learning algorithm which requires the Know-What-It-Knows oracle. However, it is in general unknown how to implement such oracle in practice.  Jiang et al. (2017)  proposed the concept of Bellman Rank to characterize the sample complexity of value-based learning methods and gave an algorithm that has polynomial sample complexity in terms of the Bellman Rank, though the proposed algorithm is not computationally efficient. Bellman rank is bounded for a wide range of problems, including MDP with small number of hidden states, linear MDP, LQR, etc. Later work gave computationally efficient algorithms for certain special cases ( Dann et al., 2018 ;  Du et al., 2019a ;  Yang & Wang, 2019b ;  Jin et al., 2019 ). Recently, Witness rank, a generalization of Bellman rank to model-based methods, is studied in  Sun et al. (2019) .

Section Title: Approximate dynamic programming-based algorithms
  Approximate dynamic programming-based algorithms We now discuss approximate dynamic programming-based results characterized in terms of the concentrability coefficient. While classical approximate dynamic programming results typically require ∞ -bounded errors, the notion of concentrability (originally due to ( Munos, 2005 )) permits sharper bounds in terms of average-case function approximation error, provided that the concentrability coefficient is bounded (e.g. see  Munos (2005) ;  Szepesvári & Munos (2005) ;  Antos et al. (2008) ;  Geist et al. (2019) ). Under the assumption that this problem-dependent parameter is bounded,  Munos (2005) ;  Szepesvári & Munos (2005)  and  Antos et al. (2008)  proved sample complexity and error bounds for approximate dynamic programming methods when there is a data collection policy (under which value-function fitting occurs) that induces a finite concentrability coefficient. The assumption that the concentrability coefficient is finite is in fact quite limiting. See  Chen & Jiang (2019)  which provides a more detailed discussion on this quantity.

Section Title: Direct policy search-based algorithms
  Direct policy search-based algorithms Stronger guarantees over approximate dynamic programming-based algrithm can be obtained with direct policy search-based methods, where instead of having a bounded concentrability coefficient, one only needs to have a bounded distribution mismatch coefficient. The latter assumption requires the agent to have access to a "good" initial state distribution (e.g. a measure which has coverage over where an optimal policy tends to visit); note that this assumption does not make restrictions over the class of MDPs. There are two classes of algo- rithms that fall into this category. First, there is Conservative Policy Iteration ( Kakade & Langford, 2002 ), along with Policy Search by Dynamic Programming (PSDP) ( Bagnell et al., 2004 ), and other boosting-style of policy search-based methods  Scherrer & Geist (2014) ;  Scherrer (2014) , which have guarantees in terms of bounded distribution mismatch ratio. Second, more recently,  Agarwal et al. (2019)  showed that policy gradient styles of algorithms also have comparable guarantees.

Section Title: Recent extensions
  Recent extensions Subsequent to this work, the work by  Van Roy & Dong (2019)  and  Lattimore & Szepesvari (2019)  made notable contributions to the misspecified linear bandit problem. In particular, both papers found that Theorem 4.1 in our paper can be extended to the misspecified linear bandit problem and gave upper bounds for this problem showing that our lower bound has tight dependency on δ and d.  Lattimore & Szepesvari (2019)  further gave an upper bound for the setting where the Q-functions of all policies can be approximated by linear functions with small approximation errors and the agent can interact with the environment using a generative model. This upper bound also demonstrates that our lower bound has tight dependency on δ and d.

Section Title: PRELIMINARIES
  PRELIMINARIES Throughout this paper, for a given integer H, we use [H] to denote the set {0, 1, . . . , H − 1}.

Section Title: EPISODIC REINFORCEMENT LEARNING
  EPISODIC REINFORCEMENT LEARNING Let M = (S, A, H, P, R) be an Markov Decision Process (MDP) where S is the state space, A is the action space whose size is bounded by a constant, H ∈ Z + is the planning horizon, P : S × A → (S) is the transition function which takes a state-action pair and returns a distribution over states and R : S × A → (R) is the reward distribution. Without loss of generality, we assume a fixed initial state s 0 4 . A policy π : S → (A) prescribes a distribution over actions for each state. The policy π induces a (random) trajectory s 0 , a 0 , r 0 , s 1 , a 1 , r 1 , . . . , s H−1 , a H−1 , r H−1 where a 0 ∼ π(s 0 ), r 0 ∼ R(s 0 , a 0 ), s 1 ∼ P (s 0 , a 0 ), a 1 ∼ π(s 1 ), etc. To streamline our analysis, for each h ∈ [H], we use S h ⊆ S to denote the set of states at level h, and we assume S h do not intersect with each other. We also assume H−1 h=0 r h ∈ [0, 1] almost surely. Our goal is to find a policy π that maximizes the expected total reward E H−1 h=0 r h | π . We use π * to denote the optimal policy. We say a policy π is ε-optimal if E In this paper we prove lower bounds for deterministic systems, i.e., MDPs with deterministic transition P , deterministic reward R. In this setting, P and R can be regarded as functions instead of distributions. Since deterministic systems are special cases of general stochastic MDPs, lower bounds proved in this paper still hold for more general MDPs.

Section Title: Q-FUNCTION AND OPTIMALITY GAP
  Q-FUNCTION AND OPTIMALITY GAP An important concept in RL is the Q-function. Given a policy π, a level h ∈ [H] and a state-action pair (s, a) ∈ S h × A, the Q-function is defined as For simplicity, we denote Q * h (s, a) = Q π * h (s, a). In addition to these definitions, we list below an important assumption, the optimality gap assumption, which is widely used in reinforcement learning and bandit literature. To state the assumption, we first define the function gap : S × A → R as gap(s, a) = arg max a ∈A Q * (s, a ) − Q * (s, a). Now we formally state the assumption. Assumption 3.1 (Optimality Gap). There exists ρ > 0 such that ρ ≤ gap(s, a) for all (s, a) ∈ S × A with gap(s, a) > 0. Here, ρ is the smallest reward-to-go difference between the best set of actions and the rest. Recently,  Du et al. (2019b)  gave a provably efficient Q-learning algorithm based on this assumption and  Simchowitz & Jamieson (2019)  showed that with this condition, the agent only incurs logarithmic regret in the tabular setting.

Section Title: QUERY MODELS
  QUERY MODELS Here we discuss three possible query oracles interacting with the MDP. • RL: The most basic and weakest query oracle for MDP is the standard reinforcement learning query oracle where the agent can only interact with the MDP by choosing actions and observe the next state and the reward. • Generative Model: A stronger query model assumes the agent can transit to any state ( Kearns & Singh, 2002 ;  Kakade, 2003 ;  Sidford et al., 2018 ). This query model is available in certain robotic applications where one can control the robot to reach the target state. • Known Transition: The strongest query model considered is that the agent can not only transit to any state, but also knows the whole transition function. In this model, only the reward is unknown. In this paper, we will prove lower bounds for the strongest Known Transition query oracle. Therefore, our lower bounds also apply to RL and Generative Model query oracles.

Section Title: MAIN RESULTS
  MAIN RESULTS In this section we formally present our lower bounds. We also discuss proof ideas in Section 4.4.

Section Title: LOWER BOUND FOR VALUE-BASED LEARNING
  LOWER BOUND FOR VALUE-BASED LEARNING We first present our lower bound for value-based learning. A common assumption is that the Q- function can be predicted well by a linear function of the given features (representation) ( Bertsekas & Tsitsiklis, 1996 ). Formally, the agent is given a feature extractor φ : S × A → R d which can be hand-crafted or a pre-trained neural network that transforms a state-action pair to a d-dimensional embedding. The following assumption states that the given feature extractor can be used to predict the Q-function with approximation error at most δ using a linear function. Assumption 4.1. There exists δ > 0 and θ 0 , θ 1 , . . . , θ H−1 ∈ R d such that for any h ∈ [H] and any Here δ is the approximation error, which indicates the quality of the representation. If δ = 0, then Q-function can be perfectly predicted by a linear function of φ (·, ·). In general, δ becomes smaller as we increase the dimension of φ, since larger dimension usually has more expressive power. When the feature extractor is strong enough, previous papers ( Chen & Jiang, 2019 ;  Farahmand, 2011 ) assume that linear functions of φ can approximate the Q-function of any policy. Assumption 4.2 (Policy Completeness). There exists δ > 0, such that for any h ∈ [H] and any policy π, there exists θ π h ∈ R d such that for any In the theoretical reinforcement learning literature, Assumption 4.2 is often called the (approximate) policy completeness assumption. This assumption is crucial in proving polynomial sample complexity guarantee for value iteration type of algorithms ( Chen & Jiang, 2019 ;  Farahmand, 2011 ). The following theorem shows when δ = Ω H d , the agent needs to sample exponential number of trajectories to find a near-optimal policy. Theorem 4.1 (Exponential Lower Bound for Value-based Learning). There exists a family of MDPs with |A| = 2 and a feature extractor φ that satisfy Assumption 4.2, such that any algorithm that returns a 1/2-optimal policy with probability 0.9 needs to sample Ω min{|S|, 2 H , exp(dδ 2 /16)} trajectories. Note this lower bound also applies to MDPs that satisfy Assumption 4.1, since Assumption 4.2 is strictly stronger. We would like to emphasize that since linear functions is a subclass of more complicated function classes, e.g., neural networks, our lower bound also holds for these function classes. Moreover, in many scenarios, the feature extractor φ is the last layer of a neural network. Modern neural networks are often over-parameterized, which makes d large. In this case, d is much larger than H. Thus, our lower bound holds even if the representation has small approximation error. Furthermore, the assumption that |A| = 2 is only for simplicity. Our lower bound can be easily generalized to the case that |A| > 2, in which case the sample complexity lower bound is Ω min{|S|, |A| H , exp(dδ 2 /16)} .

Section Title: LOWER BOUND FOR MODEL-BASED LEARNING
  LOWER BOUND FOR MODEL-BASED LEARNING Here we present our lower bound for model-based learning. Recently,  Yang & Wang (2019b)  proposed the linear transition assumption which was later studied in  Yang & Wang (2019a) ;  Jin et al. (2019) . Again, we assume the agent is given a feature extractor φ : S × A → R d , and now we state the assumption formally as follow. It has been shown in  Yang & Wang (2019b ;a);  Jin et al. (2019)  if P (· | s, a) − ψ(·), φ (s, a) 1 is bounded, then the problem admits an algorithm with polynomial sample complexity. Now we show that when δ = Ω H d in Assumption 4.3, the agent needs exponential number of samples to find a near-optimal policy. Theorem 4.2 (Exponential Lower Bound for Linear Transition Model). There exists a family of MDPs with |A| = 2 and a feature extractor φ that satisfy Assumption 4.3, such that any algorithm that returns a 1/2-optimal policy with probability 0.9 needs to sample Ω min{|S|, 2 H , exp(dδ 2 /16)} trajectories. Again, our lower bound can be easily generalized to the case that |A| > 2. We do note that an ∞ approximation for a transition matrix may be a weak condition. Under the stronger condition that the transition matrix can be approximated well under the total variational distance, there exists polynomial sample complexity upper bounds that can tolerate approximation errors ( Yang & Wang, 2019b ;a;  Jin et al., 2019 ).

Section Title: LOWER BOUND FOR POLICY-BASED LEARNING
  LOWER BOUND FOR POLICY-BASED LEARNING Next we present our lower bound for policy-based learning. This class of methods use function approximation on the policy and use optimization techniques, e.g., policy gradient, to find the optimal policy. In this paper, we focus on linear policies on top of a given representation. A linear policy π is a policy of the form π(s h ) = arg max a∈A θ h , φ(s h , a) where s h ∈ S h , φ (·, ·) is a given feature extractor and θ h ∈ R d is the linear coefficient. Note that applying policy gradient on softmax parameterization of the policy is indeed trying to find the optimal policy among linear policies. Similar to value-based learning, a natural assumption for policy-based learning is that the optimal policy is realizable 5 , i.e., the optimal policy is linear. Assumption 4.4. For any h ∈ [H], there exists θ h ∈ R d that satisfies for any s ∈ S h , we have π * (s) ∈ arg max a θ h , φ (s, a) . Here we discuss another assumption. For learning a linear classifier in the supervised learning setting, one can reduce the sample complexity significantly if the optimal linear classifier has a margin. Assumption 4.5. We assume φ (s, a) ∈ R d satisfies φ(s, a) 2 = 1 for any (s, a) ∈ S × A. For any h ∈ [H], there exists θ h ∈ R d with θ h 2 = 1 and > 0 such that for any s ∈ S h , there is a unique optimal action π * (s), and for any a = π * (s), θ h , φ (s, π * (s)) − θ h , φ (s, a) ≥ . Here we restrict the linear coefficients and features to have unit norm for normalization. Note that Assumption 4.5 is strictly stronger than Assumption 4.4. Now we present our result for linear policy. Theorem 4.3 (Exponential Lower Bound for Policy-based Learning). There exists an absolute constant 0 , such that for any ≤ 0 , there exists a family of MDPs with |A| = 2 and a feature extractor φ that satisfy Assumption 3.1 with ρ = 1 2 min{H,d} and Assumption 4.5, such that any algorithm that returns a 1/4-optimal policy with probability at least 0.9 needs to sample Ω min{2 H , 2 d } trajectories. Again, our lower bound can be easily generalized to the case that |A| > 2. Compared with Theorem 4.1, Theorem 4.3 is even more pessimistic, in the sense that even with perfect representation with benign properties (gap and margin), the agent still needs to sample exponential number of samples. It also suggests that policy-based learning could be very different from supervised learning.

Section Title: PROOF IDEAS
  PROOF IDEAS The binary tree hard instance. All our lower bound are proved based on reductions from the following hard instance. In this instance, both the transition P and the reward R are deterministic. There are H levels of states, which form a full binary tree of depth H. There are 2 h states in level h, and thus 2 H − 1 states in total. Among all the 2 H−1 states in level H − 1, there is only one state with reward R = 1, and for all other states in the MDP, the corresponding reward value R = 0. Intuitively, to find a 1/2-optimal policy for such MDPs, the agent must enumerate all possible states in level H − 1 to find the state with reward R = 1. Doing so intrinsically induces a sample complexity of Ω(2 H ). This intuition is formalized in Theorem A.1 using Yao's minimax principle ( Yao, 1977 ).

Section Title: Lower bound for value-based and model-based learning
  Lower bound for value-based and model-based learning We now show how to construct a set of features so that Assumption 4.1-4.3 hold. Our main idea is to the utilize the following fact regarding the identity matrix: ε-rank(I 2 H ) ≤ O(H/ε 2 ). Here for a matrix A ∈ R n×n , its ε-rank (a.k.a approximate rank) is defined to be min{rank(B) : B ∈ R n×n , A − B ∞ ≤ ε}, where we use · ∞ to denote the entry-wise ∞ norm of a matrix. The upper bound ε-rank(I n ) ≤ O(log n/ε 2 ) was first proved in  Alon (2009)  using the Johnson-Lindenstrauss Lemma ( Johnson & Lindenstrauss, 1984 ), and we also provide a proof in Lemma A.1. The concept of ε-rank has wide applications in theoretical computer science (Alon, 2009;  Barak et al., 2011 ;  Alon et al., 2013 ;  2014 ;  Chen & Wang, 2019 ), but to our knowledge, this is the first time that it appears in reinforcement learning. This fact can be alternatively stated as follow: there exists Φ ∈ R 2 H ×O(H/ε 2 ) such that I 2 H − ΦΦ ∞ ≤ ε. We interpret each row of Φ as the feature of a state in the binary tree. By construction of Φ, now features of states in the binary tree have a nice property that (i) each feature vector has approximately unit norm and (ii) different feature vector are nearly orthogonal. Using this set of features, we can now show that Assumption 4.1-4.3 hold. Here we prove Assumption 4.1 holds as an example and prove other assumptions also hold in the appendix. To prove Assumption 4.1, we note that in the binary tree hard instance, for each level h, only a single state satisfies Q * = 1, and all other states satisfy Q * = 0. We simply take θ h to be the feature of the state with Q * = 1. Since all feature vectors are nearly orthogonal, Assumption 4.1 holds. Since the above fact regarding the ε-rank of the identity matrix can be proved by simply taking each row of Φ to be a random unit vector, our lower bound reveals another intriguing (yet pessimistic) aspect of Assumption 4.1-4.3: for the binary tree instance, almost all feature extractors induce a hard MDP instance. This again suggests that a good representation itself may not necessarily lead to efficient RL and additional assumptions (e.g. on the reward distribution) could be crucial.

Section Title: Lower bound for policy-based learning
  Lower bound for policy-based learning It is straightfoward to construct a set of feature vectors for the binary tree instance so that Assumption 4.4 holds, even if d = 1. We set φ(s, a) to be +1 if a = a 1 and −1 if a = a 2 . For each level h, for the unique state s in level h with Q * = 1, we set θ h to be 1 if π * (s) = a 1 and −1 if π * (s) = a 2 . With this construction, Assumption 4.4 holds. To prove that the lower bound under Assumption 4.5, we use a new reward function for states in level H − 1 in the binary tree instance above so that there exists a unique optimal action for each state in the MDP. See Figure 2 for an example with H = 3 levels of states. Another nice property of the new reward function is that for all states s we always have π * (s) = a 1 . Now, we define 2 H−1 different new MDPs as follow: for each state in level H − 1, we change its original reward (defined in Figure 2) to 1. An exponential sample complexity lower bound for these MDPs can be proved using the same argument as the original binary tree hard instance, and now we show this set of MDPs satisfy Assumption 4.5. We first show in Lemma A.2 that there exists a set N ⊆ S d−1 with |N | = (1/ ) Ω(d) , so that for each p ∈ N , there exists a hyperplane L that separates p and N \ {p}, and all vectors in N have distance at least to L. Equivalently, for each p ∈ N ,we can always define a linear function f p so that f p (p) ≥ and f p (q) ≤ − for all q ∈ N \ {p}. This can be proved using standard lower bounds on the size of ε-nets. Now we simply use vectors in N as features of states. By construction of the reward function, for each level h, there could only be two possible cases for the optimal policy π * . I.e., either π * (s) = a 1 for all states in level h, or π * (s) = a 2 for a unique state s and π * (s ) = a 1 for all s = s . In both cases, we can easily define a linear function with margin to implement the optimal policy π * , and thus Assumption 4.5 holds. Notice that in this proof, we critically relies on d = Θ(H), so that we can utilize the curse of dimensionality to construct a large set of vectors as features.

Section Title: SEPARATIONS
  SEPARATIONS Perfect representation vs. good-but-not-perfect representation. For value-based learning in deterministic systems,  Wen & Van Roy (2013)  showed polynomial sample complexity upper bound when the representation can perfectly predict the Q-function. In contrast, if the representation is only able to approximate the Q-function, then the agent requires exponential number of trajectories. This exponential separation demonstrates a provable exponential benefit of better representation.

Section Title: Value-based learning vs. policy-based learning
  Value-based learning vs. policy-based learning Note that if the optimal Q-function can be perfectly predicted by the provided representation, then the optimal policy can also be perfectly predicted using the same representation. Since  Wen & Van Roy (2013)  showed polynomial sample complexity upper bound when the representation can perfectly predict the Q-function, our lower bound on policy-based learning, which applies to perfect representations, thus demonstrates that the ability of predicting the Q-function is much stronger than that of predicting the optimal policy.

Section Title: Supervised learning vs. reinforcement learning
  Supervised learning vs. reinforcement learning For policy-based learning, if the planning horizon H = 1, the problem becomes learning a linear classifier, for which there are polynomial sample complexity upper bounds. For policy-based learning, the agent needs to learn H linear classifiers sequentially. Our lower bound on policy-based learning shows the sample complexity dependency on H is exponential.

Section Title: Imitation learning vs. reinforcement learning
  Imitation learning vs. reinforcement learning In imitation learning (IL), the agent can observe trajectories induced by the optimal policy (expert). If the optimal policy is linear in the given representation, it can be shown that the simple behavior cloning algorithm only requires polynomial number of samples to find a near-optimal policy ( Ross et al., 2011 ). Our Theorem 4.3 shows if the agent cannot observe expert's behavior, then it requires exponential number of samples. Therefore, our lower bound shows there is an exponential separation between policy-based RL and IL when function approximation is used.

Section Title: ACKNOWLEDGMENTS
  ACKNOWLEDGMENTS

```
