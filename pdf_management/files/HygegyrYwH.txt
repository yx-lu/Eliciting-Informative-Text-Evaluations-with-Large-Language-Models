Title:
```
Published as a conference paper at ICLR 2020 POLYLOGARITHMIC WIDTH SUFFICES FOR GRADIENT DESCENT TO ACHIEVE ARBITRARILY SMALL TEST ER- ROR WITH SHALLOW RELU NETWORKS
```
Abstract:
```
Recent theoretical work has guaranteed that overparameterized networks trained by gradient descent achieve arbitrarily low training error, and sometimes even low test error. The required width, however, is always polynomial in at least one of the sample size n, the (inverse) target error 1 / , and the (inverse) failure probability 1 /δ. This work shows that Θ( 1 / ) iterations of gradient descent with Ω( 1 / 2 ) training examples on two-layer ReLU networks of any width exceeding polylog(n, 1 / , 1 /δ) suffice to achieve a test misclassification error of . We also prove that stochastic gradient descent can achieve test error with polylogarithmic width and Θ( 1 / ) samples. The analysis relies upon the separation margin of the limiting kernel, which is guaranteed positive, can distinguish between true labels and random labels, and can give a tight sample-complexity analysis in the infinite- width setting.
```

Figures/Tables Captions:
```

```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Despite the extensive empirical success of deep networks, their optimization and generalization properties are still not fully understood. Recently, the neural tangent kernel (NTK) has provided the following insight into the problem. In the infinite-width limit, the NTK converges to a limiting kernel which stays constant during training; on the other hand, when the width is large enough, the function learned by gradient descent follows the NTK (Jacot et al., 2018). This motivates the study of overparameterized networks trained by gradient descent, using properties of the NTK. In fact, parameters related to the NTK, such as the minimum eigenvalue of the limiting kernel, appear to affect optimization and generalization (Arora et al., 2019). However, in addition to such NTK-dependent parameters, prior work also requires the width to depend polynomially on n, 1/δ or 1/ , where n denotes the size of the training set, δ denotes the failure probability, and denotes the target error. These large widths far exceed what is used empirically, constituting a significant gap between theory and practice.

Section Title: Our contributions
  Our contributions In this paper, we narrow this gap by showing that a two-layer ReLU network with Ω(ln(n/δ)+ln(1/ ) 2 ) hidden units trained by gradient descent achieves classification error on test data, meaning both optimization and generalization occur. Unlike prior work, the width is fully polylogarithmic in n, 1/δ, and 1/ ; the width will additionally depend on the separation margin of the limiting kernel, a quantity which is guaranteed positive (assuming no inputs are parallel), can distinguish between true labels and random labels, and can give a tight sample-complexity analysis in the infinite-width setting. The paper organization together with some details are described below. Section 2 studies gradient descent on the training set. Using the 1 geometry inherent in classifi- cation tasks, we prove that with any width at least polylogarithmic and any constant step size no larger than 1, gradient descent achieves training error in Θ(1/ ) iterations (cf. Theorem 2.2). As is common in the NTK literature (Chizat & Bach, 2019), we also show the parameters hardly change, which will be essential to our generalization analysis. Section 3 gives a test error bound. Concretely, using the preceding gradient descent analysis, and standard Rademacher tools and exploiting how little the weights moved, we show that with Ω(1/ 2 ) samples and Θ(1/ ) iterations, gradient descent finds a solution with test error (cf. Theorem 3.2 and Corollary 3.3). (As discussed in Remark 3.4, Ω(1/ ) samples also suffice via a smoothness-based generalization bound, at the expense of large constant factors.) Section 4 considers stochastic gradient descent (SGD) with access to a standard stochastic on- line oracle. We prove that with width at least polylogarithmic and Θ(1/ ) samples, SGD achieves an arbitrarily small test error (cf. Theorem 4.1). Section 5 discusses the separation margin, which is in general a positive number, but reflects the difficulty of the classification problem in the infinite-width limit. While this margin can degrade all the way down to O(1/ √ n) for random labels, it can be much larger when there is a strong relationship between features and labels: for example, on the noisy 2-XOR data introduced in (Wei et al., 2018), we show that the margin is Ω(1/ ln(n)), and our SGD sample complexity is tight in the infinite-width case. Section 6 concludes with some open problems.

Section Title: RELATED WORK
  RELATED WORK There has been a large literature studying gradient descent on overparameterized networks via the NTK. The most closely related work is (Nitanda & Suzuki, 2019), which shows that a two-layer network trained by gradient descent with the logistic loss can achieve a small test error, under the same assumption that the NTK with respect to the first layer can separate the data distribution. However, they analyze smooth activations, while we handle the ReLU. They require Ω(1/ 2 ) hidden units, Ω(1/ 4 ) data samples, and O(1/ 2 ) steps, while our result only needs polylogarithmic hidden units, Ω(1/ 2 ) data samples, and O(1/ ) steps. Additionally on shallow networks, Du et al. (2018b) prove that on an overparameterized two-layer network, gradient descent can globally minimize the empirical risk with the squared loss. Their result requires Ω(n 6 /δ 3 ) hidden units. Oymak & Soltanolkotabi (2019); Song & Yang (2019) further reduce the required overparameterization, but there is still a poly(n) dependency. Using the same amount of overparameterization as (Du et al., 2018b), Arora et al. (2019) further show that the two- layer network learned by gradient descent can achieve a small test error, assuming that on the data distribution the smallest eigenvalue of the limiting kernel is at least some positive constant. They also give a fine-grained characterization of the predictions made by gradient descent iterates; such a characterization makes use of a special property of the squared loss and cannot be applied to the logistic regression setting. Li & Liang (2018) show that stochastic gradient descent (SGD) with the cross entropy loss can learn a two-layer network with small test error, using poly( , 1/ ) hidden units, where is at least the covering number of the support of the feature distribution using balls whose radii are no larger than the smallest distance between two data points with different labels. Allen-Zhu et al. (2018a) consider SGD on a two-layer network, and a variant of SGD on a three-layer network. The three-layer analysis further exhibits some properties not captured by the NTK. They assume a ground truth network with infinite-order smooth activations, and they require the width to depend polynomially on 1/ and some constants related to the smoothness of the activations of the ground truth network. On deep networks, a variety of works have established low training error (Allen-Zhu et al., 2018b; Du et al., 2018a; Zou et al., 2018; Zou & Gu, 2019). Allen-Zhu et al. (2018c) show that SGD can minimize the regression loss for recurrent neural networks, and Allen-Zhu & Li (2019b) further prove a low generalization error. Allen-Zhu & Li (2019a) show that using the same number of training examples, a three-layer ResNet can learn a function class with a much lower test error than any kernel method. Cao & Gu (2019a) assume that the NTK with respect to the second layer of a two-layer network can separate the data distribution, and prove that gradient descent on a deep network can achieve test error with Ω(1/ 4 ) samples and Ω(1/ 14 ) hidden units. Cao & Gu (2019b) consider SGD with an online oracle and give a general result. Under the same assumption as in (Cao & Gu, 2019a), their result requires Ω(1/ 14 ) hidden units and sample complexity O(1/ 2 ). By contrast, with the same online oracle, our result only needs polylogarithmic hidden units and sample complexity O(1/ ).

Section Title: NOTATION
  NOTATION The dataset is denoted by {(x i , y i )} n i=1 where x i ∈ R d and y i ∈ {−1, +1}. For simplicity, we assume that x i 2 = 1 for any 1 ≤ i ≤ n, which is standard in the NTK literature. The two-layer network has weight matrices W ∈ R m×d and a ∈ R m . We use the following parameterization, which is also used in (Du et al., 2018b; Arora et al., 2019): Note that in this paper, w s,t denotes the s-th row of W at step t. We fix a and only train W , as in (Li & Liang, 2018; Du et al., 2018b; Arora et al., 2019; Nitanda & Suzuki, 2019). We consider the ReLU activation σ(z) := max {0, z}, though our analysis can be extended easily to Lipschitz continuous, positively homogeneous activations such as leaky ReLU. We use the logistic (binary cross entropy) loss (z) := ln 1 + exp(−z) and gradient descent. For any 1 ≤ i ≤ n and any W , let f i (W ) := f (x i ; W, a). The empirical risk and its gradient are given by For any t ≥ 0, the gradient descent step is given by W t+1 := W t − η t ∇ R(W t ). Also define Note that f (t) i (W t ) = f i (W t ). This property generally holds due to homogeneity: for any W and any 1 ≤ s ≤ m,

Section Title: EMPIRICAL RISK MINIMIZATION
  EMPIRICAL RISK MINIMIZATION In this section, we consider a fixed training set and empirical risk minimization. We first state our assumption on the separability of the NTK, and then give our main result and a proof sketch. The key idea of the NTK is to do the first-order Taylor approximation: In other words, we want to do learning using the features given by ∇f i (W 0 ) ∈ R m×d . A natural assumption is that there exists U ∈ R m×d which can separate ∇f i (W 0 ), y i n i=1 with a positive margin: The infinite-width limit of eq. (2.1) is formalized as Assumption 2.1, with an additional bound on the (2, ∞) norm of the separator. A concrete construction of U using Assumption 2.1 is given in eq. (2.2). Let µ N denote the Gaussian measure on R d , given by the Gaussian density with respect to the Lebesgue measure on R d . We consider the following Hilbert space For any x ∈ R d , define φ x ∈ H by φ x (z) := x1 z, x > 0 , and particularly define φ i := φ xi for the training input x i . Assumption 2.1. There existsv ∈ H and γ > 0, such that v(z) 2 ≤ 1 for any z ∈ R d , and for any 1 ≤ i ≤ n,

Section Title: ♦
  ♦ As discussed in Section 5, the space H is the reproducing kernel Hilbert space (RKHS) induced by the infinite-width NTK with respect to W , and φ x maps x into H. Assumption 2.1 supposes that the induced training set {(φ i , y i )} n i=1 can be separated by somev ∈ H, with an additional bound on v(z) 2 which is crucial in our analysis. It is also possible to give a dual characterization of the separation margin (cf. eq. (5.2)), which also allows us to show that Assumption 2.1 always holds when there are no parallel inputs (cf. Proposition 5.1). However, it is often more convenient to constructv directly; see Section 5 for some examples. With Assumption 2.1, we state our main empirical risk result. Theorem 2.2. Under Assumption 2.1, given any risk target ∈ (0, 1) and any δ ∈ (0, 1/3), let Then for any m ≥ M and any constant step size η ≤ 1, with probability 1 − 3δ over the random initialization, While the number of hidden units required by prior work all have a polynomial dependency on n, 1/δ or 1/ , Theorem 2.2 only requires m = Ω ln(n/δ) + ln(1/ ) 2 . The required width has a polynomial dependency on 1/γ, which is an adaptive quantity: while 1/γ can be poly(n) for random labels (cf. Proposition 5.2), it can be polylog(n) when there is a strong feature-label relationship, for example on the noisy 2-XOR data introduced in (Wei et al., 2018) (cf. Proposition 5.3). Moreover, we show in Proposition 5.4 that if we want ∇f i (W 0 ), y i n i=1 to be separable, which is the starting point of an NTK-style analysis, the width has to depend polynomially on 1/γ. In the rest of Section 2, we give a proof sketch of Theorem 2.2. The full proof is given in Ap- pendix A.

Section Title: PROPERTIES AT INITIALIZATION
  PROPERTIES AT INITIALIZATION In this subsection, we give some nice properties of random initialization. Given an initialization (W 0 , a), for any 1 ≤ s ≤ m, definē u s := 1 √ m a sv (w s,0 ), (2.2) wherev is given by Assumption 2.1. Collectū s into a matrix U ∈ R m×d . It holds that ū s 2 ≤ 1/ √ m, and U F ≤ 1. Lemma 2.3 ensures that with high probability U has a positive margin at initialization. Lemma 2.3. Under Assumption 2.1, given any δ ∈ (0, 1) and any 1 ∈ (0, γ), if m ≥ 2 ln(n/δ) / 2 1 , then with probability 1 − δ, it holds simultaneously for all 1 ≤ i ≤ n that For any W , any 2 > 0, and any 1 ≤ i ≤ n, define Lemma 2.4 controls α i (W 0 , 2 ). It will help us show that U has a good margin during the training process. Lemma 2.4. Under the condition of Lemma 2.3, for any 2 > 0, with probability 1 − δ, it holds simultaneously for all 1 ≤ i ≤ n that Finally, Lemma 2.5 controls the output of the network at initialization. Lemma 2.5. Given any δ ∈ (0, 1), if m ≥ 25 ln(2n/δ), then with probability 1 − δ, it holds simultaneously for all 1 ≤ i ≤ n that

Section Title: CONVERGENCE ANALYSIS OF GRADIENT DESCENT
  CONVERGENCE ANALYSIS OF GRADIENT DESCENT We analyze gradient descent in this subsection. First, define We have the following observations. • For any W and any 1 ≤ s ≤ m, ∂f i /∂w s 2 ≤ 1/ √ m, and thus ∇f i (W ) F ≤ 1. Therefore by the triangle inequality, ∇ R(W ) F ≤ Q(W ). • The logistic loss satisfies 0 ≤ − ≤ 1, and thus 0 ≤ Q(W ) ≤ 1. • The logistic loss satisfies − ≤ , and thus Q(W ) ≤ R(W ). The quantity Q first appeared in the perceptron analysis (Novikoff, 1962) for the ReLU loss, and has also been analyzed in prior work (Ji & Telgarsky, 2018; Cao & Gu, 2019a; Nitanda & Suzuki, 2019). In this work, Q specifically helps us prove the following result, which plays an important role in obtaining a width which only depends on polylog(1/ ). Consequently, if we use a constant step size η ≤ 1 for 0 ≤ τ < t, then

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 The proof of Lemma 2.6 starts from the standard iteration guarantee: We can then handle the inner product term using the convexity of and homogeneity of ReLU, and control ∇ R(W t ) 2 F by R(W t ) using the above properties of Q(W t ). Lemma 2.6 is similar to (Allen-Zhu & Li, 2019a, Fact D.4 and Claim D.5), where the squared loss is considered. Using Lemmas 2.3 to 2.6, we can prove Theorem 2.2. Below is a proof sketch; the full proof is given in Appendix A. 1. We first show that as long as w s,t − w s,0 2 ≤ 4λ/(γ √ m) for all 1 ≤ s ≤ m, it holds that R (t) W 0 + λU ≤ /4. To see this, let us consider R (0) first. For any 1 ≤ i ≤ n, Lemma 2.5 ensures that | ∇f i (W 0 ), W 0 | is bounded, while Lemma 2.3 ensures that ∇f i (W 0 ), U is concentrated around γ with a large width. As a result, with the chosen λ in Theorem 2.2, we can show that ∇f i (W 0 ), W 0 + λU is large, and R (0) (W 0 + λU ) is small due to the exponential tail of the logistic loss. To further handle R (t) , we use a standard NTK argument to control ∇f i (W t ) − ∇f i (W 0 ), W 0 + λU under the condition that w s,t − w s,0 2 ≤ 4λ/(γ √ m). 2. We then prove by contradiction that the above bound on w s,t − w s,0 2 holds for at least the first T iterations. The key observation is that as long as R (t) (W 0 + λU ) ≤ /4, we can use it and Lemma 2.6 to control τ <t Q(W τ ), and then just invoke w s,t − w s,0 2 ≤ η τ <t Q(W τ )/ √ m. The quantity τ <t Q(W τ ) has also been considered in prior work (Cao & Gu, 2019a; Nitanda & Suzuki, 2019), where it is bounded by √ t τ <t Q(W τ ) 2 using the Cauchy- Schwarz inequality, which introduces a √ t factor. To make the required width depend only on polylog(1/ ), we also need an upper bound on τ <t Q(W τ ) which depends only on polylog(1/ ). Since the above analysis results in a √ t factor, and in our case Ω(1/ ) steps are needed, it is unclear how to get a polylog(1/ ) width using the analysis in (Cao & Gu, 2019a; Nitanda & Suzuki, 2019). By contrast, using Lemma 2.6, we can show that τ <t Q(W τ ) ≤ 4λ/γ, which only depends on ln(1/ ). 3. The claims of Theorem 2.2 then follow directly from the above two steps and Lemma 2.6.

Section Title: GENERALIZATION
  GENERALIZATION To get a generalization bound, we naturally extend Assumption 2.1 to the following assumption. Assumption 3.1. There existsv ∈ H and γ > 0, such that v(z) 2 ≤ 1 for any z ∈ R d , and for almost all (x, y) sampled from the data distribution D.

Section Title: ♦
  ♦ The above assumption is also made in (Nitanda & Suzuki, 2019) for smooth activations. (Cao & Gu, 2019a) make a similar separability assumption, but in the RKHS induced by the second layer a; by contrast, Assumption 3.1 is on separability in the RKHS induced by the first layer W . Here is our test error bound with Assumption 3.1. Theorem 3.2. Under Assumption 3.1, given any ∈ (0, 1) and any δ ∈ (0, 1/4), let λ and M be given as in Theorem 2.2:

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 Then for any m ≥ M and any constant step size η ≤ 1, with probability 1 − 4δ over the random initialization and data sampling, where k denotes the step with the minimum empirical risk before 2λ 2 /η . Below is a direct corollary of Theorem 3.2. Corollary 3.3. Under Assumption 3.1, given any , δ ∈ (0, 1), using a constant step size no larger than 1 and let it holds with probability 1 − δ that P (x,y)∼D yf (x; W k , a) ≤ 0 ≤ , where k denotes the step with the minimum empirical risk in the first Θ( 1 /γ 2 ) steps. The proof of Theorem 3.2 uses the sigmoid mapping − (z) = e −z /(1+e −z ), the empirical average Q(W k ), and the corresponding population average Q(W k ) := E (x,y)∼D − yf (x; W k , a) . As noted in (Cao & Gu, 2019a), because P (x,y)∼D yf (x; W k , a) ≤ 0 ≤ 2Q(W k ), it is enough to control Q(W k ). As Q(W k ) is controlled by Theorem 2.2, it is enough to control the generalization error Q(W k ) − Q(W k ). Moreover, since − is supported on [0, 1] and 1-Lipschitz, it is enough to bound the Rademacher complexity of the function space explored by gradient descent. Invoking the bound on W k − W 0 2,∞ finishes the proof. The proof details are given in Appendix B. Remark 3.4. To get Theorem 3.2, we use a Lipschitz-based Rademacher complexity bound. One can also use a smoothness-based Rademacher complexity bound (Srebro et al., 2010, Theorem 1) and get a sample complexity O( 1 /γ 4 ). However, the bound will become complicated and some large constant will be introduced. It is an interesting open question to give a clean analysis based on smoothness.

Section Title: STOCHASTIC GRADIENT DESCENT
  STOCHASTIC GRADIENT DESCENT There are some different formulations of SGD. In this section, we consider SGD with an online oracle. We randomly sample W 0 and a, and fix a during training. At step i, a data example (x i , y i ) is sampled from the data distribution. We still let f i (W ) := f (x i ; W, a), and perform the following update Note that here i starts from 0. Still with Assumption 3.1, we show the following result. Theorem 4.1. Under Assumption 3.1, given any , δ ∈ (0, 1), using a constant step size and m = Below is a proof sketch of Theorem 4.1; the complete proof is given in Appendix C. For any i and W , define

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 The first step is an extension of Lemma 2.6 to the SGD setting, with a similar proof. Lemma 4.2. With a constant step size η ≤ 1, for any W and any i ≥ 0, With Lemma 4.2, we can also extend Theorem 2.2 to the SGD setting and get a bound on i<n Q i (W i ), using a similar proof. To further get a bound on the cumulative population risk i<n Q(W i ), the key observation is that i<n Q(W i ) − Q i (W i ) is a martingale. Using a mar- tingale Bernstein bound, we prove the following lemma; applying it finishes the proof of Theo- rem 4.1. In this section we give some discussion on Assumption 2.1, the separability of the NTK. The proofs are all given in Appendix D. Given a training set (x i , y i ) n i=1 , the linear kernel is defined as K 0 (x i , x j ) := x i , x j . The maximum margin achievable by a linear classifier is given by γ 0 := min q∈∆n (q y) K 0 (q y). (5.1) where ∆ n denotes the probability simplex and denotes the Hadamard product. In addition to the dual definition eq. (5.1), when γ 0 > 0 there also exists a maximum margin classifierū which gives a primal characterization of γ 0 : it holds that ū 2 = 1 and y i ū, x i ≥ γ 0 for all i. In this paper we consider another kernel, the infinite-width NTK with respect to the first layer: Here φ and H are defined at the beginning of Section 2. Similar to the dual definition of γ 0 , the margin given by K 1 is defined as We can also give a primal characterization of γ 1 when it is positive. Proposition 5.1. If γ 1 > 0, then there existsv ∈ H such that v H = 1, and y i v, φ i H ≥ γ 1 for any 1 ≤ i ≤ n. Additionally v(z) 2 ≤ 1/γ 1 for any z ∈ R d . The proof is given in Appendix D, and uses the Fenchel duality theory. Using the upper bound v(z) 2 ≤ 1/γ 1 , we can see that γ 1v satisfies Assumption 2.1 with γ ≥ γ 2 1 . However, such an upper bound v(z) 2 ≤ 1/γ 1 might be too loose, which leads to a bad rate. In fact, as shown later, in some cases we can constructv directly which satisfies Assumption 2.1 with a large γ. For this reason, we choose to make Assumption 2.1 instead of assuming a positive γ 1 . However, we can use γ 1 to show that Assumption 2.1 always holds when there are no parallel inputs. Oymak & Soltanolkotabi (2019, Corollary I.2) prove that if for any two feature vectors x i and x j , we have x i − x j 2 ≥ θ and x i + x j 2 ≥ θ for some θ > 0, then the minimum eigenvalue of K 1 is at least θ/(100n 2 ). For arbitrary labels y ∈ {−1, +1} n , since q y 2 ≥ 1/ √ n, we have the worst case bound γ 2 1 ≥ θ /100n 3 . A direct improvement of this bound is θ /100n 3 S , where n S denotes the number of support vectors, which could be much smaller than n with real world data. On the other hand, given any training set (x i , y i ) n i=1 which may have a large margin, replacing y with random labels would destroy the margin, which is what should be expected. Published as a conference paper at ICLR 2020 Proposition 5.2. Given any training set (x i , y i ) n i=1 , if the true labels y are replaced with random labels ∼ unif {−1, +1} n , then with probability 0.9 over the random labels, it holds that γ 1 ≤ 1/ √ 20n. Although the above bounds all have a polynomial dependency on n, they hold for arbitrary or random labels, and thus do not assume any relationship between the features and labels. Next we give some examples where there is a strong feature-label relationship, and thus a much larger margin can be proved.

Section Title: THE LINEARLY SEPARABLE CASE
  THE LINEARLY SEPARABLE CASE Suppose the data distribution is linearly separable with margin γ 0 : there exists a unit vectorū such that y ū, x ≥ γ 0 almost surely. Then we can definev(z) :=ū for any z ∈ R d . For almost all (x, y), we have and thus Assumption 2.1 holds with γ = γ 0 /2.

Section Title: THE NOISY 2-XOR DISTRIBUTION
  THE NOISY 2-XOR DISTRIBUTION We consider the noisy 2-XOR distribution introduced in (Wei et al., 2018). It is the uniform distri- bution over the following 2 d points: The factor 1 / √ d−1 ensures that x 2 = 1, and × above denotes the Cartesian product. Here the label y only depends on the first two coordinates of the input x. To constructv, we first decompose R 2 into four regions: Thenv can de defined as follows. It only depends on the first two coordinates of z. The following result shows that γ = Ω(1/d). Note that n could be as large as 2 d , in which case γ is basically O 1/ ln(n) . Proposition 5.3. For any (x, y) sampled from the noisy 2-XOR distribution and any d ≥ 3, it holds that We can prove two other interesting results for the noisy 2-XOR data.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 The width needs a poly(1/γ) dependency for initial separability. The first step of an NTK analysis is to show that ∇f i (W 0 ), y i n i=1 is separable. Proposition 5.4 gives an example where ∇f i (W 0 ), y i n i=1 is nonseparable when the network is narrow. Proposition 5.4. Let D = {(x i , y i )} 4 i=1 denote an arbitrary subset of the noisy 2-XOR dataset such that x i 's have the same last (d − 2) coordinates. For any d ≥ 20, if m ≤ √ d − 2/4, then with probability 1/2 over the random initialization of W 0 , for any weights V ∈ R m×d , it holds that y i V, ∇f i (W 0 ) ≤ 0 for at least one i ∈ {1, 2, 3, 4}. For the noisy 2-XOR data, the separatorv given by eq. (5.3) has margin γ = Ω(1/d), and 1/γ = O(d). As a result, if we want ∇f i (W 0 ), y i n i=1 to be separable, the width has to be Ω(1/ √ γ). For a smaller width, gradient descent might still be able to solve the problem, but a beyond-NTK analysis would be needed. A tight sample complexity upper bound for the infinite-width NTK. (Wei et al., 2018) give a d 2 sample complexity lower bound for any NTK classifier on the noisy 2-XOR data. It turns out that γ could give a matching sample complexity upper bound for the NTK and SGD. (Wei et al., 2018) consider the infinite-width NTK with respect to both layers. For the first layer, the infinite-width NTK K 1 is defined in Section 5, and the corresponding RKHS H and RKHS mapping φ is defined in Section 2. For the second layer, the infinite width NTK is defined by The corresponding RKHS K and inner product w 1 , w 2 K are given by Given any x ∈ R d , it is mapped into ψ x ∈ K, where ψ x (z) := σ z, x . It holds that K 2 (x i , x j ) = ψ xi , ψ xj K . The infinite-width NTK with respect to both layers is just K 1 +K 2 . The corresponding RHKS is just H × K with the inner product The classifierv considered in eq. (5.3) has a unit norm (i.e., v H = 1) and margin γ on the space H. On H × K, it is enough to consider (v, 0), which also has a unit norm and margin γ. Since the infinite-width NTK model is a linear model in H × K, (Ji & Telgarsky, 2018, Lemma 2.5) can be used to show that SGD on the RKHS H×K could obtain a test error of with a sample complexity of O( 1 /γ 2 ). (The analysis in (Ji & Telgarsky, 2018) is done in R d , but it still works with a well-defined inner product.) Since γ = Ω(1/d), to achieve a constant test accuracy we need O(d 2 ) samples. This mathces (up to logarithmic factors) the sample complexity lower bound of d 2 given by Wei et al. (2018).

Section Title: OPEN PROBLEMS
  OPEN PROBLEMS In this paper, we analyze gradient descent on a two-layer network in the NTK regime, where the weights stay close to the initialization. It is an interesting open question if gradient descent learns something beyond the NTK, after the iterates move far enough from the initial weights. It is also interesting to extend our analysis to other architectures, such as multi-layer networks, convolutional networks, and residual networks. Finally, in this paper we only discuss binary classification; it is interesting to see if it is possible to get similar results for other tasks, such as regression.

```
