<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 GLOBAL ADVERSARIAL ROBUSTNESS GUARANTEES FOR NEURAL NETWORKS</article-title></title-group><abstract><p>We investigate global adversarial robustness guarantees for machine learning models. Specifically, given a trained model we consider the problem of com- puting the probability that its prediction at any point sampled from the (unknown) input distribution is susceptible to adversarial attacks. Assuming continuity of the model, we prove measurability for a selection of local robustness properties used in the literature. We then show how concentration inequalities can be employed to compute global robustness with estimation error upper-bounded by , for any &gt; 0 selected a priori. We utilise the methods to provide statistically sound analysis of the robustness/accuracy trade-off for a variety of neural networks ar- chitectures and training methods on MNIST, Fashion-MNIST and CIFAR. We empirically observe that robustness and accuracy tend to be negatively correlated for networks trained via stochastic gradient descent and with iterative pruning techniques, while a positive trend is observed between them in Bayesian settings.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Adversarial attacks are potentially imperceptible input manipulations that when applied to a test point result in misclassification. Since even state-of-the-art deep learning models have been shown susceptible to such attacks (<xref ref-type="bibr" rid="b9">Goodfellow et al., 2014</xref>), adversarial examples have raised serious con- cerns about the security and robustness of models learned from data (<xref ref-type="bibr" rid="b1">Biggio &amp; Roli, 2018</xref>). As standard accuracy measures fail to capture model behaviour in an adversarial setting, the develop- ment of techniques capable of quantifying the robustness of machine learning models is an essential pre-condition for their application in safety-critical scenarios, where model failures have already led to fatal accidents (<xref ref-type="bibr" rid="b28">Yadron &amp; Tynan, 2016</xref>). In such settings, we require models that are not only accurate, but also have guaranteed robust behaviour (<xref ref-type="bibr" rid="b18">Michelmore et al., 2019</xref>). However, while tech- niques for the computation of local adversarial robustness guarantees (i.e., specific to a particular test point) have been developed for a variety of machine learning models (<xref ref-type="bibr" rid="b4">Katz et al., 2017</xref>; <xref ref-type="bibr" rid="b1">Biggio &amp; Roli, 2018</xref>; <xref ref-type="bibr" rid="b2">Cardelli et al., 2019b</xref>), to the best of our knowledge relatively few studies of global robustness (<xref ref-type="bibr" rid="b0">Bastani et al., 2016</xref>; <xref ref-type="bibr" rid="b20">Ruan et al., 2019</xref>; <xref ref-type="bibr" rid="b24">Webb et al., 2018</xref>) have been considered, and none rigorously quantify the probability that a neural network is robust to adversarial perturbations.</p><p>Given a neural network (NN) f we investigate the global robustness properties of f .We start from a local notion of robustness specific to a point (e.g. robustness against bounded norm perturbations or adversarial attacks) and pose the problem of computing the probability that the prediction of f in a test point x, sampled from a (possibly unknown) data distribution P , is robust according to a given local property. Unfortunately, exact computation of such global robustness measures is intractable. Nevertheless, we show how statistically sound approximations with quantifiable and arbitrarily small error can be computed by making use of concentration inequalities (<xref ref-type="bibr" rid="b16">Massart, 2007</xref>). Crucially, this allows us to provide statistical guarantees over the estimation of the global robustness measures, while keeping computations to a minimum. In order to do so, we prove that the global adversarial robustness property defined here is measurable for a range of commonly used notions of adversarial examples and NN architectures, and show how global robustness can be reformulated as a sum of independent and identically distributed random variables. Hence, bounds such as additive Chernoff or Hoeffding's inequality can be used to quantify the error with respect to an empirical estimator.</p><p>We utilise our techniques to statistically quantify the robustness of a variety of different neural network architectures and training methods, and experimentally study their robustness profile on Under review as a conference paper at ICLR 2020 multiple datasets, including MNIST, Fashion-MNIST and CIFAR. In particular, we analyse the ef- fects of iterative magnitude pruning on network robustness. Interestingly, while weight pruning may increase test set accuracy (as observed by <xref ref-type="bibr" rid="b7">Frankle &amp; Carbin (2018)</xref>), we find that it does not have any positive effect on global robustness for the examples we analysed. We further empirically eval- uate the robustness accuracy trade-off (<xref ref-type="bibr" rid="b22">Tsipras et al., 2018</xref>) for a wide selection of different network architectures and hyper-parameters, which allows us to compare different training regimes. We find that accuracy and robustness tend to be at odds in networks trained via stochastic gradient descent, whereas there seems to exist a positive correlation between accuracy and robustness in Bayesian set- tings. More specifically, for the datasets analysed, we observe that optimising a Bayesian network architecture for accuracy leads to networks that are more robust against adversarial examples.</p><p>In summary, this paper makes the following main contributions:</p><p>&#8226; We consider probabilistic global adversarial robustness measures for neural networks and prove their measurability.</p><p>&#8226; We show how concentration inequalities can be used to provide statistical guarantees on global robustness estimations with an a priori error bound.</p><p>&#8226; We investigate the trade off between robustness and accuracy for a variety of networks architectures and training methods. We provide empirical results that suggest that networks trained by Bayesian methods might be more robust than their deterministic counter-parts.</p><p>Related Works. Various methods, both heuristic and based on formal verification, have been de- rived to evaluate the local adversarial robustness of neural networks (<xref ref-type="bibr" rid="b9">Goodfellow et al., 2014</xref>; <xref ref-type="bibr" rid="b2">Wicker et al., 2018</xref>; <xref ref-type="bibr" rid="b21">Tram&#232;r et al., 2017</xref>; <xref ref-type="bibr" rid="b4">Katz et al., 2017</xref>; <xref ref-type="bibr" rid="b2">Cardelli et al., 2019a</xref>; <xref ref-type="bibr" rid="b4">Carlini et al., 2017</xref>). In ad- dition to local robustness, which focuses on a specific test point, global measures of robustness have also been discussed in the literature (<xref ref-type="bibr" rid="b0">Bastani et al., 2016</xref>; <xref ref-type="bibr" rid="b20">Ruan et al., 2019</xref>; <xref ref-type="bibr" rid="b24">Webb et al., 2018</xref>). Intu- itively, these generalise the notion of local robustness to a set of test points (or a data distribution), so as to marginalise out their influence. This yields a global measure of robustness for the network that describes the probability wrt the input distribution that a certain local property is satisfied (<xref ref-type="bibr" rid="b5">Dreossi et al., 2019</xref>). While these approaches consider similar global robustness measures to those discussed here, they lack a characterisation of error bounds. In contrast, our approach provides statistical error bounds on the estimation of global robustness, up to any a priori specified tolerance &gt; 0.</p><p>Formal bounds on global adversarial robustness are given by (<xref ref-type="bibr" rid="b6">Fawzi et al., 2018</xref>). However, im- plementation details are given only for linear and quadratic classifiers, since the computation of the bound relies on the distance from the decision boundary. Our approach instead generalises to any continuous model for which local robustness can be computed. A worst-case notion of global ro- bustness is discussed by <xref ref-type="bibr" rid="b4">Katz et al. (2017)</xref>; <xref ref-type="bibr" rid="b10">Gopinath et al. (2018)</xref>, where a network is considered globally adversarially robust if there are no input points that are vulnerable to adversarial attacks. However, this notion of global robustness tends to be too pessimistic and hard to compute in practice, since it is valid for all points in the input space (independently of their likelihood wrt the input distri- bution). Instead, the global robustness measure considered here is probabilistic, in that it computes the probability that a point drawn from the data distribution is vulnerable to adversarial attacks.</p><p>Similar techniques to those used in this paper are applied for the computation of PAC bounds for the evaluation of the generalisation error of learning models (<xref ref-type="bibr" rid="b17">McAllester, 1999</xref>; <xref ref-type="bibr" rid="b23">Vapnik, 2013</xref>) and their robustness (<xref ref-type="bibr" rid="b27">Xu &amp; Mannor, 2012</xref>; <xref ref-type="bibr" rid="b11">Gourdeau et al., 2019</xref>). This differs from the problem studied here as PAC aims at bounding the generalisation capabilities of a family of learning models, whereas our method yields bounds on a specific trained model. Additionally, when computing global robustness, we focus on analyzing the probability that a perturbation applied to a test point causes a prediction change, independently of the point ground truth (i.e. independently of the generalisation error).</p></sec><sec><title>GLOBAL ADVERSARIAL ROBUSTNESS OF NEURAL NETWORKS</title><p>In this section we discuss two notions of global robustness for neural networks. Namely, we first introduce qualitative and quantitative variants of local robustness, and then define global robustness as their expected values wrt the (possibly unknown) input distribution. Throughout this paper, we consider a neural network f : R m &#8594; R n , with any activation function and arbitrarily many layers, such that, for a test point x &#8712; R m , f (x) = (f 1 (x), ..., f n (x)) represents the vector of the confidence Under review as a conference paper at ICLR 2020 values for each of the n class labels. D = {(x, y)|x &#8712; R m , y &#8712; {1, ..., n}} is a test dataset comprising |D| input points, which we assume to be iid sampled from a distribution P .</p></sec><sec><title>A Measure of Global Robustness</title><p>Given a test point x sampled from P , we consider the indicator function g : R m &#8594; {0, 1}, which returns 1 if f is locally robust in x, and is defined as follows. Definition 1. (Local Robustness) Given x &#8712; R m and &#948; &gt; 0, let T x = {x &#8712; R m | |x &#8722; x| p &#8804; &#948;}, where | &#183; | p is an L p norm, be a &#948;&#8722;ball around x. Call C(x) = arg max i&#8712;{1,...,n} f i (x) the set of classes for which the confidence of f in x is maximised. Then we say that f is locally robust in x around T x iff g(x) = 1, where:</p><p>That is, f is locally robust in x if small perturbations in the input do not cause a classification change. Note that we require the predicted class of x to be unambiguous, i.e. C(x) comprises only one class, and otherwise we view the point as not locally robust (as the classification decision output is not uniquely defined in this case). Local robustness is widely used to investigate the properties of neural networks and various algorithms have been derived for its computation and approximation (<xref ref-type="bibr" rid="b5">Dreossi et al., 2019</xref>). Details on how we compute or approximate g(x) are given in Section 3.1.</p><p>Building on local robustness, we are interested in computing the global robustness of f , which is defined below as the probability that a test point sampled from P is locally robust.</p><p>Definition 2. (Global Robustness) The robustness of f is defined as: R(g) = E x&#8764;P [g(x)] = g(x)p(x)dx, (1) where p is the density probability associated to P.</p><p>It is important to emphasise that R(g) is a property of the neural network, as the effect of each single input point is marginalised out when taking the expectation. We remark that, since g is an indicator function, under the assumption that g is measurable (discussed in detail in Section 3), it follows that R(g) is a well defined probability measure.</p></sec><sec><title>A Quantitative Measure of Global Robustness</title><p>As discussed in the previous section, we define local robustness g as an indicator function. It is often useful to consider a quantitative notion for the robustness of f . For example, given a &#948;&#8722;ball around a test point x, we may want to know not just if f is locally robust in x, but also the maximal variation in the classification confidence values of the various classes, which evaluate the network's robustness independently of the decision procedure used for the classification. Further, one could require the model confidence to be robust up to a specific output threshold (<xref ref-type="bibr" rid="b4">Katz et al., 2017</xref>). Specifically, given f and a test point x &#8712; R m , we consider a function&#7713;(x) that quantifies the robustness of f in x and is defined as follows.</p><p>Definition 3. (Quantitative Local Robustness) Given x &#8712; R m and &#948; &gt; 0, let T x = {x &#8712; R m | |x &#8722; x| p &#8804; &#948;}, where | &#183; | p is an L p norm, be a &#948;&#8722;ball around x. We define the quantitative local robustness of f in x around T x as:&#7713; (x) = max x&#8712;T x h(f (x), f (x)), where h : R n &#215; R n &#8594; R is a given function that measure differences between f predictions.</p><p>In Section 4 we provide experimental results using h defined as the L &#8734; norm between predictions, as well as by checking whether this norm is greater than a given threshold &#947; &gt; 0 (which is akin to the definition introduced by <xref ref-type="bibr" rid="b4">Katz et al. (2017)</xref>). Similarly to R(g), the following definition generalises quantitative local robustness to a global property, by taking the expectation of&#7713;(x) with respect to the input data distribution.</p><p>Definition 4. (Quantitative Global Robustness) The quantitative robustness of f is defined as: D(&#7713;) = E x&#8764;P [&#7713;(x)] = &#7713;(x)p(x)dx, (2) where p is the density probability associated to P.</p><p>Exact computation of R(g) and D(&#7713;) is infeasible as it requires the computation of an integral with respect to an unknown input distribution. Nevertheless, in what follows, we show that under mild assumptions it is possible to estimate these quantities with a priori arbitrarily stringent guarantees. Under review as a conference paper at ICLR 2020</p></sec><sec><title>STATISTICAL GUARANTEES ON ADVERSARIAL ROBUSTNESS</title><p>In this section we derive estimations of R(g) and D(&#7713;) with arbitrarily stringent a priori statistical guarantees. In particular, in Equation 3 we consider empirical estimators of these quantities and then in Theorem 1 and 2 we show that the probability that the error between these quantities and the real measures is greater than a threshold can be upper bounded by employing concentration inequalities.</p><p>In Section 3.1 we review how g and&#7713; are computed or approximated in practice, and finally describe the global robustness guarantees computation pipeline in Section 3.2. Proofs for the Propositions and Theorems state in this Section are reported in the Appendix Section A.</p><p>We consider the following empirical estimators: R emp (g, S) = (x,y)&#8712;S g(x) |S| D emp (&#7713;, S) = (x,y)&#8712;S&#7713; (x) |S| (3) where S is a set of size |S| of test points iid sampled from P . In order to derive a worst-case scenario bound on the distance between R(g) and R emp (g, S), we first have to show that g is a measurable function 1 . In fact, the measurability of g guarantees that R(g) is a well defined probability measure induced by P , the data distribution. Hence, concentration inequalities can be applied to bound the error between R(g) and R emp (g, S).</p><p>Proposition 1. Assume that, for i &#8712; {1, ..., n}, f i , the i-th component of the neural network f is a continuous function, then g(x), as defined in Definition 1, is measurable.</p><p>Proof of Proposition is based on noticing that the set {x &#8712; R m | g(x) &gt; 0} can be rewritten as the union of pre-images of measurable sets for a measurable function. Note that the overwhelming majority of the neural networks commonly used in practice are continuous. Hence, the assumption in Proposition 1 is almost always verified in practice. At this point, we can state the following theorem, which bounds the probability that the distance between R(g) and R emp (g, S) is greater than , for any &gt; 0.</p><p>Theorem 1. Assume that g is measurable, then for any &gt; 0 it holds that that for any (possibly unknown) input data distribution P</p><p>Theorem 1 gives a bound that is problem and architecture independent, that is it holds for any data distribution and any architecture of the network f . As illustrated in Section 4.1, the required number of samples for a given error tolerance , although exponential in , is generally under control in practice. The proof of Theorem 1 relies on reformulating Equation 1 as a probability measure and then the application of the Chernoff bound to a sum of independent Bernoulli random variables.</p><p>In the remaining part of this section, we show how to derive statistical guarantees on the error be- tween D(&#7713;) and D emp (&#7713;, S). Again, we first show that under mild assumptions&#7713;(x) is measurable. Then, we use this result to bound the error by using concentration inequalities.</p><p>Proposition 2. Consider&#7713;, as defined in Definition 3. Then, under the assumption that l(x 1 , x 2 ) = h(f (x 1 ), f (x 2 )) is continuous in both x 1 and x 2 , we have that&#7713; measurable.</p><p>In the following theorem we show that a result similar to that of Theorem 1 can be obtained also for the more general case of quantitative global robustness.</p><p>Theorem 2. Assume that&#7713; is measurable and that, for any input point x,&#7713;(x) &#8712; [A, B] &#8834; R n . Then, for any (possibly unknown) input data distribution P , it holds that</p><p>Note that in practice it is often the case that the output of a neural network is bounded (notably for classification problems and regression problems over a bounded input space). The proof of Theorem 2 relies on the application of union bound and Hoeffding inequality (<xref ref-type="bibr" rid="b23">Vapnik, 2013</xref>), and the resulting upper bound is again problem and architecture independent.</p></sec><sec><title>COMPUTATION OF LOCAL ROBUSTNESS g(x) AND&#7713;(x)</title><p>The computation of global robustness depends on the ability to compute local robustness g(x) (or analogously&#7713;(x)) for every input point x, that is, to establish the presence or absence of an adver- sarial attack in a neighbourhood of x. For deep neural networks this is known to be NP-complete (<xref ref-type="bibr" rid="b4">Katz et al., 2017</xref>). For exact computation of g(x) we employ the verification method introduced by (<xref ref-type="bibr" rid="b13">Huang et al., 2017</xref>), which builds on input space discretisation and constraint solving to perform exhaustive search of the neighbourhood. Results for this are discussed in Section 4.1. Unfortunately, the computational complexity for the exact computation of g(x) quickly gets prohibitive for large networks. As in (<xref ref-type="bibr" rid="b0">Bastani et al., 2016</xref>), in these cases we proceed by approximating local robust- ness. The trade-off between scalability and approximation quality of an adversarial attack method is empirically discussed in (<xref ref-type="bibr" rid="b4">Carlini et al., 2017</xref>). We remark that the statistical bounds we compute are fully transparent to the way in which g is computed or approximated. More precisely, the meth- ods discussed in Section 3 provide sound, statistical bounds on the global robustness estimation, independently on the definition and computation of g(x).</p></sec><sec><title>COMPUTATION OF GLOBAL ROBUSTNESS R(g) AND D(&#7713;)</title><p>We detail how global robustness with statistical guarantees can be computed. In Theorems 1 and 2 we proved that statistical guarantees can be computed on standard estimators for R(g) and D(&#7713;). In fact, given a statistical tolerance &gt; 0, this can be done by computing the smallest number of samples N = |S| that satisfies Equation 4 (respectively Equation 5 for the computation of D(&#7713;)). Let S = {x i , i = 1, . . . , N } be N test points randomly taken from the test dataset D, we then com- pute the values of g(x i ) (respectively&#7713;(x i )) using the methods discussed in the previous subsection. We finally use these values to evaluate Equation 3. This provides us with an estimator R emp (g, S) (respectively D emp (&#7713;, S)) that meets the required statistical guarantees by construction.</p></sec><sec><title>EXPERIMENTS AND APPLICATIONS</title><p>We apply the presented techniques to obtain statistically sound estimation of the robustness profile for an array of NN architectures. In Section 4.1 we analyse the convergence rate of the bound on global robustness, using the exact local robustness method of <xref ref-type="bibr" rid="b13">Huang et al. (2017)</xref>. In Section 4.2 we report empirical robustness/accuracy trade-off for a variety of different networks trained on MNIST and CIFAR, using FGSM (<xref ref-type="bibr" rid="b9">Goodfellow et al., 2014</xref>) for approximate computation of local robustness. Section 4.3 analyses the effects of pruning on the network generalisation and global robustness on MNIST and Fashion-MNIST. Results for Bayesian networks are discussed in Section 4.4 2 .</p></sec><sec><title>CONVERGENCE ANALYSIS OF BOUND</title><p><xref ref-type="fig" rid="fig_0">Figure 1</xref> depicts the convergence rate of our robustness estimation method wrt the error tolerance . Namely, we train a CNN on MNIST and estimate its global robustness up to a decreasing value of .</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>For each value of we compute 50 different estimates of global robustness (blue dots in the figure), statistically guaranteed to be -close to the actual robustness of the network. In order to do so, for every we compute the number of samples required to meet the guarantees (using Equations 4 and 5), and independently build 50 subset of the test dataset D. Hence, for each value of this give rise to 50 different estimation of global robustness. As decreases, the variance of the estimated values decreases as well, quickly converging at around 5% error. The red line in the plot shows the number of samples needed to obtain the required guarantees. Notice that, though the number of samples is exponential wrt , 5% tolerance is already obtained with just above 700 samples taken from the input distribution. This is orders of magnitudes smaller than test datasets used for training deep networks.</p></sec><sec><title>ROBUSTNESS OF DETERMINISTIC NEURAL NETWORKS</title><p>In this section, we empirically evaluate the trade-off between global robustness and accuracy for deterministic networks. Recent works have suggested that robustness and accuracy might be at odds, and observed that this was the case when training neural networks on a selection of datasets (<xref ref-type="bibr" rid="b22">Tsipras et al., 2018</xref>). While the trade-off between robustness and accuracy is generally different for each application, we contribute to these analyses by quantifying (i.e. with guaranteed statistical error) the robustness/accuracy trade-off in fully connected networks (FCNs) and convolutional neu- ral networks (CNNs) trained on MNIST, Fashion-MNIST (<xref ref-type="bibr" rid="b26">Xiao et al., 2017</xref>) and CIFAR-10 using Stochastic Gradient Descent (SGD). <xref ref-type="fig" rid="fig_1">Figure 2</xref> depicts the empirical robustness/accuracy trade-off we observe, computed with statistical error = 0.05. Overall, the observed trade-off is computed on approximately 3800 different neu- ral network models (each blue dot in the figure represents robustness and accuracy obtained with a particular neural network), namely 1000 fully-connected networks (FCNs) and 1000 convolutional neural networks (CNNs) for MNIST, 500 FCNs and 500 CNNs for Fashion MNIST, and 800 CNNs for CIFAR-10. The different networks were obtained by means of a grid search over the hyperparam- eters space, where we vary depths, widths, activation functions, learning rates and training epochs. In the case of CNNs we also vary the number of convolutional layers, number of filters in each convolutional layer, and the size of the kernel used (additional details about the hyperparameters used and values explored can be found in the Appendix Section B). For CIFAR-10 approximately 200 models are taken from the DEMOGEN model dataset (<xref ref-type="bibr" rid="b14">Jiang et al., 2018</xref>). The global properties checked in these analyses are global robustness induced by: checking whether NNs prediction on L &#8734; &#948;-ball produces confidence variations above 0.50 (shown in Figures (a) and (c)); computing maximum softmax variation in L &#8734; norm (shown in Figures (b), (d) and (e)); checking for changes in classification (shown in Figure (c)). These are further reported in each figure box. Notice that the &#948; used for Fashion-MNIST and CIFAR-10 is smaller than the one used for MNIST, as every network was found fully non-robust for larger values of &#948;. We find that for harder problems, the network per- formance is more varied which results in greater noise in the quantification of the trade-off (as seen in <xref ref-type="fig" rid="fig_1">Figure 2</xref>).</p><p>For each of the datasets analysed and for all tested measures of robustness, we find a negative correlation between accuracy and robustness. This supports the current hypothesis of the robust- ness/accuracy trade off, and demonstrates that optimizing hyperparameters for accuracy might have a negative effect on the resulting network robustness. We perform empirical analysis to find the hyper-parameters that have a most profound effect on the network robustness. These are shown in the boxplots in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, which are computed by averaging out the results across all the networks tested. We find that both increasing model capacity (purple boxplots) and increasing training du- ration (green boxplots) tend to have a negative effect on robustness. Interestingly, these suggest a relationship between adversarial vulnerability and overfitting. We remark, however, that, though they align with previous hypothesis and similar empirical results (<xref ref-type="bibr" rid="b22">Tsipras et al., 2018</xref>), these results are empirical, and inevitably based on observations from only finitely many networks.</p></sec><sec><title>EFFECT OF ITERATIVE MAGNITUDE PRUNING ON ROBUSTNESS</title><p>Iterative Magnitude Pruning (IMP) is a network compression technique which has shown, empir- ically, to be effective at reducing the number of weight parameters in neural networks. Empirical analyses demonstrated that IMP can at times remove up to 99% of the weights, while keeping (and even improving on) the original network test accuracy (<xref ref-type="bibr" rid="b7">Frankle &amp; Carbin, 2018</xref>). As the existence of Under review as a conference paper at ICLR 2020 (a) (c) (e) (b) (d) (f) a non-trivial interplay between model capacity and adversarial robustness has been suggested in the literature (<xref ref-type="bibr" rid="b9">Goodfellow et al., 2014</xref>), we here explore the effect of IMP on global robustness on the same architectures described in Section 4.2 for MNIST and Fashion-MNIST. Results for this anal- ysis are shown in <xref ref-type="fig" rid="fig_2">Figure 3</xref>, where we show the effect of IMP when removing 50%, 75%, 90% and 99% of the weights (IMP results reported in green dots and normal training results reported using blue dots in (a) and (b)). We find for IMP a similar trade-off to the one observed when pruning is not applied, with a slight reduction in accuracy when severe pruning strength is applied (shown in (c)). As the boxplots show, we find that the amount of pruning has no statistically significant effect on any of the global robustness measures investigated here. These observations are inline with the impact of other notions of weight regularization observed in (<xref ref-type="bibr" rid="b9">Goodfellow et al., 2014</xref>), further highlighting a relationship between weight pruning and network regularisation.</p></sec><sec><title>ROBUSTNESS OF BAYESIAN NEURAL NETWORKS</title><p>In this section we explore the global robustness of networks trained in Bayesian settings, which in principle should not suffer from overfitting-like issues (<xref ref-type="bibr" rid="b8">Gal &amp; Smith, 2018</xref>). We perform approx- imate Bayesian training on MNIST and Fashion-MNIST using Hamiltonian Monte Carlo (HMC), which is the gold-standard for Bayesian inference (<xref ref-type="bibr" rid="b19">Neal et al., 2011</xref>). Unfortunately, this does not allow us to scale to CIFAR-10. While this could be possible by using other Bayesian approx- imate training techniques, it would add a non-trivial interplay between the results observed and the quality of the approximation. We again explore the model architecture space using the same hyper-parameters discussed in Section 4.1 (instead of varying learning rates and epochs, we vary the parameters of the numerical integrator for the Hamiltonian dynamics and the number of samples that we use in order to approximate the posterior distribution). As such, the explored architectures are exactly the same as those explored for deterministic NNs and the only difference lies hence in the training methods (that is, deterministic vs. Bayesian). Results for this analysis are given <xref ref-type="fig" rid="fig_3">Fig- ure 4</xref>, with pink dots showing Bayesian results and blue dots the deterministic ones (as in Figure Under review as a conference paper at ICLR 2020 (a) (b) (c) 2). Interestingly, we find that, not only do Bayesian networks not exhibit the negative correlation between robustness and accuracy, they in fact reverse this correlation. This shows that, for the ar- chitectures analysed here, and for MNIST and Fashion-MNIST, robustness and accuracy are not at odds. Actually, the results suggest that selecting a Bayesian network hyper-parameters to optimise test accuracy leads to more globally robust networks as well. We further inspect the effect of model capacity on the global adversarial robustness. Interestingly, we show that there is a weak positive correlation between model capacity and robustness for both MNIST and Fashion-MNIST, which again reverses the trend observed for deterministic networks.</p></sec><sec><title>CONCLUSION</title><p>We considered a probabilistic measure of global adversarial robustness of neural networks and gave methods for its estimation with a-priori statistical guarantees. The presented techniques were em- ployed to provide statistically sound estimates of the robustness profile of an array of neural network architectures on MNIST, CIFAR-10 and Fashion-MNIST. We further investigated how the robust- ness/accuracy trade-off is affected by different training approaches, including Bayesian and iterative pruning methods. The methods discussed here rely only on the continuity of neural network models, and thus generalise to any machine learning model characterised by a continuous function, provided that the given notion of local robustness can be computed in practice.</p></sec><sec><title>A PROOFS</title><p>In this Section of the Appendix we provide proofs for the Propositions and Theorems stated in the main paper.</p></sec><sec><title>Proof of Theorem 1 By Definition we have</title><p>Since g(x) is an indicator function, and under the assumption that g is measurable, E x&#8764;P [g(x)] defines the probability of a random variable taking values in {0, 1}. Hence, the difference between E x&#8764;P [g(x)] and its empirical frequency (x,y)&#8712;S g(x) |S| can be upper bounded by employing additive Chernoff bounds (<xref ref-type="bibr" rid="b23">Vapnik (2013)</xref>), yielding Equation 4.</p><p>Each of these terms can now be bounded by using Hoeffding's inequalities (<xref ref-type="bibr" rid="b23">Vapnik, 2013</xref>).</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Convergence of the global robustness estimation wrt error tolerance (decreasing values along the x-axis). For each we compute 50 empirical estimates (blue dots). As the error tolerance decreases the estimated quantities converge to a tight cluster. The red line interpolates the number of sampled images needed for the computation of each blue dot.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Analysis of empirical accuracy/robustness trade-off computed on 4000 deterministic FCNs and CNNs (each blue dot in the center plots represents the accuracy/robustness result for each net- work). Each set of plots has been labeled with the functions used to compute robustness. First row: analyses on the MNIST dataset. Second row: analyses on the Fashion-MNIST dataset. Third row: analyses on the CIFAR-10 dataset. We observe a negative trend between accuracy and robustness.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Quantitative analysis of global adversarial robustness on models trained with iterative magnitude pruning. (a): robustness results on MNIST; (b): robustness results on Fashion-MNIST; (c): quantitative robustness on MNIST (top) and Fashion-MNIST (bottom). We observe that prun- ing leads to comparable trade-off wrt when pruning is not applied. The boxplots show a lack of correlation between the proportion of weights pruned and global robustness.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Global robustness analysis of Bayesian NNs. (a): results on MNIST; (b): results on Fashion-MNIST. Each plot is labeled with the notion of robustness used. In the rightmost plot, SGD was measured with attacks an order of magnitude weaker as the models were more vulnerable.</p></caption><graphic /><graphic /><graphic /><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Measuring neural net robustness with constraints</article-title><source>Advances in neural infor- mation processing systems</source><year>2016</year><fpage>2613</fpage><lpage>2621</lpage><person-group person-group-type="author"><name><surname>References</surname><given-names>Osbert</given-names></name><name><surname>Bastani</surname><given-names>Yani</given-names></name><name><surname>Ioannou</surname><given-names>Leonidas</given-names></name><name><surname>Lampropoulos</surname><given-names>Dimitrios</given-names></name><name><surname>Vytiniotis</surname><given-names>Aditya</given-names></name><name><surname>Nori</surname><given-names>An- Tonio</given-names></name><name><surname>Criminisi</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Wild patterns: Ten years after the rise of adversarial machine learning</article-title><source>Pattern Recognition</source><year>2018</year><volume>84</volume><fpage>317</fpage><lpage>331</lpage><person-group person-group-type="author"><name><surname>Biggio</surname><given-names>Battista</given-names></name><name><surname>Roli</surname><given-names>Fabio</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Statistical guarantees for the robustness of bayesian neural networks</article-title><source>arXiv preprint arXiv:1903.01980</source><year>2019</year><person-group person-group-type="author"><name><surname>Cardelli</surname><given-names>Luca</given-names></name><name><surname>Kwiatkowska</surname><given-names>Marta</given-names></name><name><surname>Laurenti</surname><given-names>Luca</given-names></name><name><surname>Paoletti</surname><given-names>Nicola</given-names></name><name><surname>Patane</surname><given-names>Andrea</given-names></name><name><surname>Wicker</surname><given-names>Matthew</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Robustness guarantees for bayesian inference with gaussian processes</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><year>2019</year><volume>33</volume><fpage>7759</fpage><lpage>7768</lpage><person-group person-group-type="author"><name><surname>Cardelli</surname><given-names>Luca</given-names></name><name><surname>Kwiatkowska</surname><given-names>Marta</given-names></name><name><surname>Laurenti</surname><given-names>Luca</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Provably minimally-distorted adver- sarial examples</article-title><source>arXiv preprint arXiv:1709.10207</source><year>2017</year><person-group person-group-type="author"><name><surname>Carlini</surname><given-names>Nicholas</given-names></name><name><surname>Katz</surname><given-names>Guy</given-names></name><name><surname>Barrett</surname><given-names>Clark</given-names></name><name><surname>Dill</surname><given-names>David L</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>A formalization of robustness for deep neural networks</article-title><source>arXiv preprint arXiv:1903.10033</source><year>2019</year><person-group person-group-type="author"><name><surname>Dreossi</surname><given-names>Tommaso</given-names></name><name><surname>Ghosh</surname><given-names>Shromona</given-names></name><name><surname>Sangiovanni-Vincentelli</surname><given-names>Alberto</given-names></name><name><surname>Seshia</surname><given-names>Sanjit</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Analysis of classifiers robustness to adversarial perturbations</article-title><source>Machine Learning</source><year>2018</year><volume>107</volume><issue>3</issue><fpage>481</fpage><lpage>508</lpage><person-group person-group-type="author"><name><surname>Fawzi</surname><given-names>Alhussein</given-names></name><name><surname>Fawzi</surname><given-names>Omar</given-names></name><name><surname>Frossard</surname><given-names>Pascal</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>The lottery ticket hypothesis: Finding sparse, trainable neural networks</article-title><source>arXiv preprint arXiv:1803.03635</source><year>2018</year><person-group person-group-type="author"><name><surname>Frankle</surname><given-names>Jonathan</given-names></name><name><surname>Carbin</surname><given-names>Michael</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Sufficient conditions for idealised models to have no adversarial examples: a theoretical and empirical study with bayesian neural networks</article-title><source>arXiv preprint arXiv:1806.00667</source><year>2018</year><person-group person-group-type="author"><name><surname>Gal</surname><given-names>Yarin</given-names></name><name><surname>Smith</surname><given-names>Lewis</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Explaining and harnessing adversarial examples</article-title><source>arXiv preprint arXiv:1412.6572</source><year>2014</year><person-group person-group-type="author"><name><surname>Ian</surname><given-names>J</given-names></name><name><surname>Goodfellow</surname><given-names>Jonathon</given-names></name><name><surname>Shlens</surname><given-names>Christian</given-names></name><name><surname>Szegedy</surname><given-names /></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><source>International Symposium on Automated Technology for Verification and Analysis</source><year>2018</year><fpage>3</fpage><lpage>19</lpage><person-group person-group-type="author"><name><surname>Gopinath</surname><given-names>Divya</given-names></name><name><surname>Katz</surname><given-names>Guy</given-names></name><name><surname>P&#515;s&#515;reanu</surname><given-names>Corina S</given-names></name><name><surname>Barrett</surname><given-names>Clark</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>On the hardness of robust classification</article-title><source>arXiv preprint arXiv:1909.05822</source><year>2019</year><person-group person-group-type="author"><name><surname>Gourdeau</surname><given-names>Pascale</given-names></name><name><surname>Kanade</surname><given-names>Varun</given-names></name><name><surname>Kwiatkowska</surname><given-names>Marta</given-names></name><name><surname>Worrell</surname><given-names>James</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><source>Infinite dimensional analysis</source><year>2006</year><person-group person-group-type="author"><name><surname>Hitchhikers Guide</surname><given-names>A</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><source>International Conference on Computer Aided Verification</source><year>2017</year><fpage>3</fpage><lpage>29</lpage><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Xiaowei</given-names></name><name><surname>Kwiatkowska</surname><given-names>Marta</given-names></name><name><surname>Wang</surname><given-names>Sen</given-names></name><name><surname>Wu</surname><given-names>Min</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Predicting the generalization gap in deep networks with margin distributions</article-title><source>arXiv preprint arXiv:1810.00113</source><year>2018</year><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Yiding</given-names></name><name><surname>Krishnan</surname><given-names>Dilip</given-names></name><name><surname>Mobahi</surname><given-names>Hossein</given-names></name><name><surname>Bengio</surname><given-names>Samy</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><source>International Conference on Computer Aided Verification</source><year>2017</year><fpage>97</fpage><lpage>117</lpage><person-group person-group-type="author"><name><surname>Katz</surname><given-names>Guy</given-names></name><name><surname>Barrett</surname><given-names>Clark</given-names></name><name><surname>David</surname><given-names>L</given-names></name><name><surname>Dill</surname><given-names>Kyle</given-names></name><name><surname>Julian</surname><given-names>Mykel</given-names></name><name><surname>Kochenderfer</surname><given-names /></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><source>Concentration inequalities and model selection</source><year>2007</year><person-group person-group-type="author"><name><surname>Massart</surname><given-names>Pascal</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Some pac-bayesian theorems</article-title><source>Machine Learning</source><year>1999</year><volume>37</volume><issue>3</issue><fpage>355</fpage><lpage>363</lpage><person-group person-group-type="author"><name><surname>David</surname><given-names>A</given-names></name><name><surname>Mcallester</surname><given-names /></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><source>Uncertainty quantification with statistical guarantees in end-to-end autonomous driving control</source><year>2019</year><person-group person-group-type="author"><name><surname>Michelmore</surname><given-names>Rhiannon</given-names></name><name><surname>Wicker</surname><given-names>Matthew</given-names></name><name><surname>Laurenti</surname><given-names>Luca</given-names></name><name><surname>Cardelli</surname><given-names>Luca</given-names></name><name><surname>Gal</surname><given-names>Yarin</given-names></name><name><surname>Kwiatkowska</surname><given-names>Marta</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Mcmc using hamiltonian dynamics</article-title><source>Handbook of markov chain monte carlo</source><year>2011</year><volume>2</volume><issue>11</issue><fpage>2</fpage><lpage>2</lpage><person-group person-group-type="author"><name><surname>Radford M Neal</surname><given-names /></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance</article-title><source>International Joint Conference on Artificial Intelligence</source><year>2019</year><person-group person-group-type="author"><name><surname>Ruan</surname><given-names>Wenjie</given-names></name><name><surname>Wu</surname><given-names>Min</given-names></name><name><surname>Sun</surname><given-names>Youcheng</given-names></name><name><surname>Huang</surname><given-names>Xiaowei</given-names></name><name><surname>Kroening</surname><given-names>Daniel</given-names></name><name><surname>Kwiatkowska</surname><given-names>Marta</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>Ensemble adversarial training: Attacks and defenses</article-title><source>arXiv preprint arXiv:1705.07204</source><year>2017</year><person-group person-group-type="author"><name><surname>Tram&#232;r</surname><given-names>Florian</given-names></name><name><surname>Kurakin</surname><given-names>Alexey</given-names></name><name><surname>Papernot</surname><given-names>Nicolas</given-names></name><name><surname>Goodfellow</surname><given-names>Ian</given-names></name><name><surname>Boneh</surname><given-names>Dan</given-names></name><name><surname>Mc- Daniel</surname><given-names>Patrick</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Robustness may be at odds with accuracy</article-title><source>arXiv preprint arXiv:1805.12152</source><year>2018</year><person-group person-group-type="author"><name><surname>Tsipras</surname><given-names>Dimitris</given-names></name><name><surname>Santurkar</surname><given-names>Shibani</given-names></name><name><surname>Engstrom</surname><given-names>Logan</given-names></name><name><surname>Turner</surname><given-names>Alexander</given-names></name><name><surname>Madry</surname><given-names>Aleksander</given-names></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><source>The nature of statistical learning theory</source><year>2013</year><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>Vladimir</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>Statistical verification of neural networks</article-title><source>arXiv preprint arXiv:1811.07209</source><year>2018</year><person-group person-group-type="author"><name><surname>Webb</surname><given-names>Stefan</given-names></name><name><surname>Rainforth</surname><given-names>Tom</given-names></name><name><surname>Teh</surname><given-names>Yee Whye</given-names></name><name><surname>Kumar</surname><given-names>M Pawan</given-names></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><source>International Conference on Tools and Algorithms for the Construc- tion and Analysis of Systems</source><year>2018</year><fpage>408</fpage><lpage>426</lpage><person-group person-group-type="author"><name><surname>Wicker</surname><given-names>Matthew</given-names></name><name><surname>Huang</surname><given-names>Xiaowei</given-names></name><name><surname>Kwiatkowska</surname><given-names>Marta</given-names></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>Fashion-mnist: a novel image dataset for benchmark- ing machine learning algorithms</article-title><source>arXiv preprint arXiv:1708.07747</source><year>2017</year><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>Han</given-names></name><name><surname>Rasul</surname><given-names>Kashif</given-names></name><name><surname>Vollgraf</surname><given-names>Roland</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><article-title>Robustness and generalization</article-title><source>Machine learning</source><year>2012</year><volume>86</volume><issue>3</issue><fpage>391</fpage><lpage>423</lpage><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Huan</given-names></name><name><surname>Mannor</surname><given-names>Shie</given-names></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>Tesla driver dies in first fatal crash while using autopilot mode</article-title><source>the Guardian</source><year>2016</year><volume>1</volume><person-group person-group-type="author"><name><surname>Yadron</surname><given-names>Danny</given-names></name></person-group></element-citation></ref></ref-list></back></article>