Title:
```
Published as a conference paper at ICLR 2020 DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION
```
Abstract:
```
Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.
```

Figures/Tables Captions:
```
Figure 1: Dreamer learns a world model from past experience and efficiently learns farsighted behaviors in its latent space by backpropagating value estimates back through imagined trajectories.
Figure 2: Image observations for 5 of the 20 visual control tasks used in our experiments. The tasks pose a variety of challenges including contact dynamics, sparse rewards, many degrees of freedom, and 3D environments. Several of these tasks could previously not be solved through world models.
Figure 3: Components of Dreamer. (a) From the dataset of past experience, the agent learns to encode observations and actions into compact latent states ( ), for example via reconstruction, and predicts environment rewards ( ). (b) In the compact latent space, Dreamer predicts state values ( ) and actions ( ) that maximize future value predictions by propagating gradients back through imagined trajectories. (c) The agent encodes the history of the episode to compute the current model state and predict the next action to execute in the environment. See Algorithm 1 for pseudo code of the agent.
Figure 4: Imagination horizons. We compare the final performance of Dreamer, learning an action model without value prediction, and online planning using PlaNet. Learning a state value model to estimate rewards beyond the imagination horizon makes Dreamer more robust to the horizon length. The agents use pixel reconstruction for representation learning and an action repeat of R = 2.
Figure 5: Reconstructions of long-term predictions. We apply the representation model to the first 5 images of two hold-out trajectories and predict forward for 45 steps using the latent dynamics, given only the actions. The recurrent state space model (RSSM; Hafner et al., 2018) performs accurate long-term predictions, enabling Dreamer to learn successful behaviors in a compact latent space.
Figure 6: Performance comparison to existing methods. Dreamer inherits the data-efficiency of PlaNet while exceeding the asymptotic performance of the best model-free agents. After 5 × 10 6 environment steps, Dreamer reaches an average performance of 823 across tasks, compared to PlaNet at 332 and the top model-free D4PG agent at 786 after 10 9 steps. Results are averages over 5 seeds.
Figure 7: Dreamer succeeds at visual control tasks that require long-horizon credit assignment, such as the acrobot and hopper tasks. Optimizing only imagined rewards within the horizon via an action model or by online planning yields shortsighted behaviors that only succeed in reactive tasks, such as in the walker domain. The performance on all 20 tasks is summarized in Figure 6 and training curves are shown in Appendix D. See Tassa et al. (2018) for performance curves of D4PG and A3C.
Figure 8: Comparison of representation learning objectives to be used with Dreamer. Pixel recon- struction performs best for the majority of tasks. The contrastive objective solves about half of the tasks, while predicting rewards alone was not sufficient in our experiments. The results suggest that future developments in learning representations are likely to translate into improved task performance for Dreamer. The performance curves for all tasks are included in Appendix E.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Intelligent agents can achieve goals in complex environments even though they never encounter the exact same situation twice. This ability requires building representations of the world from past experience that enable generalization to novel situations. World models offer an explicit way to represent an agent's knowledge about the world in a parametric model that can make predictions about the future. When the sensory inputs are high-dimensional images, latent dynamics models can abstract observations to predict forward in compact state spaces (Watter et al., 2015; Oh et al., 2017;  Gregor et al., 2019 ). Compared to predictions in image space, latent states have a small memory footprint that enables imagining thousands of trajectories in parallel. Learning effective latent dynamics models is becoming feasible through advances in deep learning and latent variable models (Krishnan et al., 2015;  Karl et al., 2016 ;  Doerr et al., 2018 ;  Buesing et al., 2018 ). Behaviors can be derived from dynamics models in many ways. Often, imagined rewards are maximized with a parametric policy ( Sutton, 1991 ;  Ha and Schmidhuber, 2018 ; Zhang et al., 2019) or by online planning ( Chua et al., 2018 ;  Hafner et al., 2018 ). However, considering only rewards within a fixed imagination horizon results in shortsighted behaviors (Wang et al., 2019). Moreover, prior work commonly resorts to derivative-free optimization for robustness to model errors ( Ebert et al., 2017 ;  Chua et al., 2018 ; Parmas et al., 2019), rather than leveraging analytic gradients offered by neural network dynamics (Henaff et al., 2019;  Srinivas et al., 2018 ). We present Dreamer, an agent that learns long-horizon behaviors from images purely by latent imagination. A novel actor critic algorithm accounts for rewards beyond the imagination horizon while making efficient use of the neural network dynamics. For this, we predict state values and actions in the learned latent space as summarized in  Figure 1 . The values optimize Bellman consistency for imagined rewards and the policy maximizes the values by propagating their analytic gradients back through the dynamics. In comparison to actor critic algorithms that learn online or by experience replay (Lillicrap et al., 2015;  Mnih et al., 2016 ; Schulman et al., 2017;  Haarnoja et al., 2018 ;  Lee et al., 2019 ), world models can interpolate past experience and offer analytic gradients of multi-step returns for efficient policy optimization. The key contributions of this paper are summarized as follows: • Learning long-horizon behaviors by latent imagination Model-based agents can be short- sighted if they use a finite imagination horizon. We approach this limitation by predicting both actions and state values. Training purely by imagination in a latent space lets us efficiently learn the policy by propagating analytic value gradients back through the latent dynamics. • Empirical performance for visual control We pair Dreamer with existing representation learning methods and evaluate it on the DeepMind Control Suite with image inputs, illustrated in  Figure 2 . Using the same hyper parameters for all tasks, Dreamer exceeds previous model-based and model-free agents in terms of data-efficiency, computation time, and final performance.

Section Title: CONTROL WITH WORLD MODELS
  CONTROL WITH WORLD MODELS

Section Title: Reinforcement learning
  Reinforcement learning We formulate visual control as a partially observable Markov decision process (POMDP) with discrete time step t ∈ [1; T ], continuous vector-valued actions a t ∼ p(a t | o ≤t , a <t ) generated by the agent, and high-dimensional observations and scalar rewards o t , r t ∼ p(o t , r t | o <t , a <t ) generated by the unknown environment. The goal is to develop an agent that maximizes the expected sum of rewards E p T t=1 r t .  Figure 2  shows a selection of our tasks.

Section Title: Agent components
  Agent components The classical components of agents that learn in imagination are dynamics learning, behavior learning, and environment interaction ( Sutton, 1991 ). In the case of Dreamer, the behavior is learned by predicting hypothetical trajectories in the compact latent space of the world model. As outlined in  Figure 3  and detailed in Algorithm 1, Dreamer performs the following operations throughout the agent's life time, either interleaved or in parallel: • Learning the latent dynamics model from the dataset of past experience to predict future re- wards from actions and past observations. Any learning objective for the world model can be incorporated with Dreamer. We review existing methods for learning latent dynamics in Section 4. • Learning action and value models from predicted latent trajectories, as described in Section 3. The value model optimizes Bellman consistency for imagined rewards and the action model is updated by propagating gradients of value estimates back through the neural network dynamics. • Executing the learned action model in the world to collect new experience for growing the dataset. Latent dynamics Dreamer uses a latent dynamics model that consists of three components. The representation model encodes observations and actions to create continuous vector-valued model states s t with Markovian transitions (Watter et al., 2015; Zhang et al., 2019;  Hafner et al., 2018 ). The transition model predicts future model states without seeing the corresponding observations that will later cause them. The reward model predicts the rewards given the model states, Representation model: Transition model: Reward model: We use p for distributions that generate samples in the real environment and q for their approximations that enable latent imagination. Specifically, the transition model lets us predict ahead in the compact latent space without having to observe or imagine the corresponding images. This results in a low memory footprint and fast predictions of thousands of imagined trajectories in parallel. The model mimics a non-linear Kalman filter ( Kalman, 1960 ), latent state space model, or HMM with real-valued states. However, it is conditioned on actions and predicts rewards, allowing the agent to imagine the outcomes of potential action sequences without executing them in the environment.

Section Title: LEARNING BEHAVIORS BY LATENT IMAGINATION
  LEARNING BEHAVIORS BY LATENT IMAGINATION Dreamer learns long-horizon behaviors in the compact latent space of a learned world model by efficiently leveraging the neural network latent dynamics. For this, we propagate stochastic gradients of multi-step returns through neural network predictions of actions, states, rewards, and values using reparameterization. This section describes the main contribution of our paper.

Section Title: Imagination environment
  Imagination environment The latent dynamics define a Markov decision process (MDP;  Sutton, 1991 ) that is fully observed because the compact model states s t are Markovian. We denote imagined quantities with τ as the time index. Imagined trajectories start at the true model states s t of observation sequences drawn from the agent's past experience. They follow predictions of the transition model s τ ∼ q(s τ | s τ −1 , a τ −1 ), reward model r τ ∼ q(r τ | s τ ), and a policy a τ ∼ q(a τ | s τ ). The objective is to maximize expected imagined rewards E q ∞ τ =t γ τ −t r τ with respect to the policy. Update θ using representation learning. Compute value estimates V λ (s τ ) via Equation 6. the action model. Add exploration noise to action. Action and value models Consider imagined trajectories with a finite horizon H. Dreamer uses an actor critic approach to learn behaviors that consider rewards beyond the horizon. We learn an action model and a value model in the latent space of the world model for this. The action model implements the policy and aims to predict actions that solve the imagination environment. The value model estimates the expected imagined rewards that the action model achieves from each state s τ , Action model: Value model: The action and value models are trained cooperatively as typical in policy iteration: the action model aims to maximize an estimate of the value, while the value model aims to match an estimate of the value that changes as the action model changes. We use dense neural networks for the action and value models with parameters φ and ψ, respectively. The action model outputs a tanh-transformed Gaussian ( Haarnoja et al., 2018 ) with sufficient statistics predicted by the neural network. This allows for reparameterized sampling ( Kingma and Welling, 2013 ;  Rezende et al., 2014 ) that views sampled actions as deterministically dependent on the neural network output, allowing us to backpropagate analytic gradients through the sampling operation,

Section Title: Value estimation
  Value estimation To learn the action and value models, we need to estimate the state values of imagined trajectories {s τ , a τ , r τ } t+H τ =t . These trajectories branch off of the model states s t of sequence batches drawn from the agent's dataset of experience and predict forward for the imagination horizon H using actions sampled from the action model. State values can be estimated in multiple ways that trade off bias and variance ( Sutton and Barto, 2018 ), where the expectations are estimated under the imagined trajectories. V R simply sums the rewards from τ until the horizon and ignores rewards beyond it. This allows learning the action model without a value model, an ablation we compare to in our experiments. V k N estimates rewards beyond k steps with the learned value model. Dreamer uses V λ , an exponentially-weighted average of the estimates for different k to balance bias and variance.  Figure 4  shows that learning a value model in imagination enables Dreamer to solve long-horizon tasks while being robust to the imagination horizon. The experimental details and results on all tasks are described in Section 6.

Section Title: Learning objective
  Learning objective To update the action and value models, we first compute the value estimates V λ (s τ ) for all states s τ along the imagined trajectories. The objective for the action model q φ (a τ | s τ ) is to predict actions that result in state trajectories with high value estimates. The objective for the value model v ψ (s τ ), in turn, is to regress the value estimates, The value model is updated to regress the targets, around which we stop the gradient as typical ( Sutton and Barto, 2018 ). The action model uses analytic gradients through the learned dynamics to maximize the value estimates. To understand this, we note that the value estimates depend on the reward and value predictions, which depend on the imagined states, which in turn depend on the imagined actions. Since all steps are implemented as neural networks, we analytically compute ∇ φ E q θ ,q φ t+H τ =t V λ (s τ ) by stochastic backpropagation ( Kingma and Welling, 2013 ;  Rezende et al., 2014 ). We use reparameterization for continuous actions and latent states and straight-through gradients ( Bengio et al., 2013 ) for discrete actions. The world model is fixed while learning behaviors. In tasks with early termination, the world model also predicts the discount factor from each latent state to weigh the time steps in Equations 7 and 8 by the cumulative product of the predicted discount factors, so terms are weighted down based on how likely the imagined trajectory would have ended.

Section Title: Comparison to actor critic methods
  Comparison to actor critic methods Agents using Reinforce gradients ( Williams, 1992 ), such as A3C and PPO ( Mnih et al., 2016 ; Schulman et al., 2017), employ value baselines to reduce gradient variance, while Dreamer backpropagates through the value model. This is similar to deterministic or reparameterized actor critics (Silver et al., 2014), such as DDPG and SAC (Lillicrap et al., 2015;  Haarnoja et al., 2018 ). However, these do not leverage gradients through transitions and only maximize immediate Q-values. MVE and STEVE ( Feinberg et al., 2018 ;  Buckman et al., 2018 ) extend them to multi-step Q-learning with learned dynamics to provide more accurate Q-value targets. We predict state values, which is sufficient for policy optimization since we backpropagate through the dynamics. Refer to Section 5 for a more detailed comparison to related work.

Section Title: LEARNING LATENT DYNAMICS
  LEARNING LATENT DYNAMICS Learning behaviors in imagination requires a world model that generalizes well. We focus on latent dynamics models that predict forward in a compact latent space, facilitating long-term predictions and allowing the agent to imagine thousands of trajectories in parallel. Several objectives for learning representations for control have been proposed (Watter et al., 2015;  Jaderberg et al., 2016 ;  Oord et al., 2018 ;  Eslami et al., 2018 ). We review three approaches for learning representations to use with Dreamer: reward prediction, image reconstruction, and contrastive estimation.

Section Title: Reward prediction
  Reward prediction Latent imagination requires a representation model p(s t | s t−1 , a t−1 , o t ), transition model q(s t | s t−1 , a t−1 , ), and reward model q(r t | s t ), as described in Section 2. In principle, this could be achieved by simply learning to predict future rewards given actions and past observations (Oh et al., 2017;  Gelada et al., 2019 ; Schrittwieser et al., 2019). With a large and diverse dataset, such representations should be sufficient for solving a control task. However, with a finite dataset and especially when rewards are sparse, learning about observations that correlate with rewards is likely to improve the world model ( Jaderberg et al., 2016 ;  Gregor et al., 2019 ).

Section Title: Reconstruction
  Reconstruction We first describe the world model used by PlaNet ( Hafner et al., 2018 ) that learns latent dynamics by reconstructing images as shown in Figure 3a. The world model consists of the following components, where the observation model is only used to provide a learning signal, Representation model: The components are optimized jointly to increase the variational lower bound (ELBO;  Jordan et al., 1999 ) or more generally the variational information bottleneck (VIB; Tishby et al., 2000;  Alemi et al., 2016 ). As derived in Appendix B, the bound includes reconstruction terms for observations and rewards and a KL regularizer. The expectation is taken under the dataset and representation model, We implement the transition model as a recurrent state space model (RSSM;  Hafner et al., 2018 ), the representation model by combining the RSSM with a convolutional neural network (CNN; LeCun et al., 1989) applied to the image observation, the observation model as a transposed CNN, and the reward model as a dense network. The combined parameter vector θ is updated by stochastic backpropagation ( Kingma and Welling, 2013 ;  Rezende et al., 2014 ).  Figure 5  shows video predictions of this model. We refer to Appendix A and  Hafner et al. (2018)  model details.

Section Title: Contrastive estimation
  Contrastive estimation Predicting pixels can require high model capacity. We can also encourage mutual information between model states and observations by instead predicting the states from the images ( Guo et al., 2018 ). This replaces the observation model with a state model, State model: While the reconstruction objective used the fact that the observation marginal is a constant, we now face the state marginal. As shown in Appendix B, this can be estimated via noise contrastive estimation (NCE;  Gutmann and Hyvärinen, 2010 ;  Oord et al., 2018 ) by averaging the state model over observations o of the current sequence batch. Intuitively, q(s t | o t ) makes the state predictable from the current image while ln o q(s t | o ) keeps it diverse to prevent collapse, We implement the state model as a CNN and again optimize the bound with respect to the combined parameter vector θ using stochastic backpropagation. While avoiding pixel prediction, the amount of information this bound can extract efficiently is limited ( McAllester and Statos, 2018 ). We empirically compare reward, reconstruction, and contrastive objectives in our experiments in  Figure 8 .

Section Title: RELATED WORK
  RELATED WORK Prior works learn latent dynamics for visual control by derivative-free policy learning or online planning, augment model-free agents with multi-step predictions, or use analytic gradients of Q- values or multi-step rewards, often for low-dimensional tasks. In comparison, Dreamer uses analytic gradients to efficiently learn long-horizon behaviors for visual control purely by latent imagination. Control with latent dynamics E2C (Watter et al., 2015) and RCE ( Banijamali et al., 2017 ) embed images to predict forward in a compact space to solve simple tasks. World Models ( Ha and Schmid- huber, 2018 ) learn latent dynamics in a two-stage process to evolve linear controllers in imagination. PlaNet ( Hafner et al., 2018 ) learns them jointly and solves visual locomotion tasks by latent online planning. SOLAR (Zhang et al., 2019) solves robotic tasks via guided policy search in latent space. I2A ( Weber et al., 2017 ) hands imagined trajectories to a model-free policy, while  Lee et al. (2019)  and  Gregor et al. (2019)  learn belief representations to accelerate model-free agents. Imagined multi-step returns VPN (Oh et al., 2017), MVE ( Feinberg et al., 2018 ), and STEVE ( Buckman et al., 2018 ) learn dynamics for multi-step Q-learning from a replay buffer. AlphaGo (Silver et al., 2017) combines predictions of actions and state values with planning, assuming access to the true dynamics. Also assuming access to the dynamics, POLO ( Lowrey et al., 2018 ) plans to explore by learning a value ensemble. MuZero (Schrittwieser et al., 2019) learns task-specific reward and value models to solve challenging tasks but requires large amounts of experience. PETS ( Chua et al., 2018 ), VisualMPC ( Ebert et al., 2017 ), and PlaNet ( Hafner et al., 2018 ) plan online using derivative-free optimization. POPLIN ( Wang and Ba, 2019 ) improves over online planning by self-imitation.  Piergiovanni et al. (2018)  learn robot policies by imagination with a latent dynamics model. Planning with neural network gradients was shown on small problems ( Schmidhuber, 1990 ;  Henaff et al., 2018 ) but has been challenging to scale (Parmas et al., 2019). Analytic value gradients DPG (Silver et al., 2014), DDPG (Lillicrap et al., 2015), and SAC ( Haarnoja et al., 2018 ) leverage gradients of learned immediate action values to learn a policy by experience replay. SVG ( Heess et al., 2015 ) reduces the variance of model-free on-policy algorithms by analytic value gradients of one-step model predictions. Concurrent work by  Byravan et al. (2019)  uses latent imagination with deterministic models for navigation and manipulation tasks. ME-TRPO ( Kurutach et al., 2018 ) accelerates an otherwise model-free agent via gradients of predicted rewards for proprioceptive inputs. DistGBP (Henaff et al., 2017;  2019 ) uses model gradients for online planning in simple tasks.

Section Title: EXPERIMENTS
  EXPERIMENTS We experimentally evaluate Dreamer on a variety of control tasks. We designed the experiments to compare Dreamer to current best methods in the literature, and to evaluate its ability to solve tasks with long horizons, continuous actions, discrete actions, and early termination. We further compare the orthogonal choice of learning objective for the world model. The source code for all our experiments and videos of Dreamer are available at https://danijar.com/dreamer.

Section Title: Control tasks
  Control tasks We evaluate Dreamer on 20 visual control tasks of the DeepMind Control Suite ( Tassa et al., 2018 ), illustrated in  Figure 2 . These tasks pose a variety of challenges, including sparse rewards, contact dynamics, and 3D scenes. We selected the tasks on which  Tassa et al. (2018)  report non-zero performance from image inputs. Agent observations are images of shape 64 × 64 × 3, actions range from 1 to 12 dimensions, rewards range from 0 to 1, episodes last for 1000 steps and have randomized initial states. We use a fixed action repeat of R = 2 across tasks. We further evaluate the applicability of Dreamer to discrete actions and early termination on a subset of Atari games ( Bellemare et al., 2013 ) and DeepMind Lab levels ( Beattie et al., 2016 ) as detailed in Appendix C.

Section Title: Implementation
  Implementation Our implementation uses TensorFlow Probability ( Dillon et al., 2017 ). We use a single Nvidia V100 GPU and 10 CPU cores for each training run. The training time for our Dreamer implementation is below 5 hours per 10 6 environment steps on the control suite, compared to 11 hours for online planning using PlaNet, and the 24 hours used by D4PG to reach similar performance. We use the same hyper parameters across all continuous tasks, and similarly across all discrete tasks, detailed in Appendix A. The world models are learned via reconstruction unless specified.

Section Title: Baseline methods
  Baseline methods The highest reported performance on the continuous tasks is achieved by D4PG ( Barth-Maron et al., 2018 ), an improved variant of DDPG (Lillicrap et al., 2015) that uses distributed collection, distributional Q-learning, multi-step returns, and prioritized replay. We include the scores for D4PG with pixel inputs and A3C ( Mnih et al., 2016 ) with state inputs from  Tassa et al. (2018) . PlaNet ( Hafner et al., 2018 ) learns the same world model as Dreamer and selects actions via online planning without an action model and drastically improves over D4PG and A3C in data efficiency. We re-run PlaNet with R = 2 for a unified experimental setup. For Atari, we show the final performance of SimPLe (Kaiser et al., 2019), DQN ( Mnih et al., 2015 ) and Rainbow ( Hessel et al., 2018 ) reported by  Castro et al. (2018) , and for DeepMind Lab that of IMPALA ( Espeholt et al., 2018 ) as a guideline.

Section Title: Performance
  Performance To evaluate the performance of Dreamer, we compare it to state-of-the-art reinforce- ment learning agents. The results are summarized in  Figure 6 . With an average score of 823 across tasks after 5 × 10 6 environment steps, Dreamer exceeds the performance of the strong model-free D4PG agent that achieves an average of 786 within 10 9 environment steps. At the same time, Dreamer inherits the data-efficiency of PlaNet, confirming that the learned world model can help to generalize from small amounts of experience. The empirical success of Dreamer shows that learning behaviors by latent imagination with world models can outperform top methods based on experience replay.

Section Title: Long horizons
  Long horizons To investigate its ability to learn long-horizon behaviors, we compare Dreamer to alternatives for deriving behaviors from the world model at various horizon lengths. For this, we learn an action model to maximize imagined rewards without a value model and compare to online planning using PlaNet.  Figure 4  shows the final performance for different imagination horizons, confirming that the value model makes Dreamer more robust to the horizon and performs well even for short horizons. Performance curves for all 19 tasks with horizon of 20 are shown in Appendix D, where Dreamer outperforms the alternatives on 16 of 20 tasks, with 4 ties.

Section Title: Representation learning
  Representation learning Dreamer can be used with any differentiable dynamics model that pre- dicts future rewards given actions and past observations. Since the representation learning objective is orthogonal to our algorithm, we compare three natural choices described in Section 4: pixel recon- struction, contrastive estimation, and pure reward prediction.  Figure 8  shows clear differences in task performance for different representation learning approaches, with pixel reconstruction outperform- ing contrastive estimation on most tasks. This suggests that future improvements in representation learning are likely to translate to higher task performance with Dreamer. Reward prediction alone was not sufficient in our experiments. Further ablations are included in the appendix of the paper.

Section Title: CONCLUSION
  CONCLUSION We present Dreamer, an agent that learns long-horizon behaviors purely by latent imagination. For this, we propose an actor critic method that optimizes a parametric policy by propagating analytic gradients of multi-step values back through learned latent dynamics. Dreamer outperforms previous methods in data-efficiency, computation time, and final performance on a variety of challenging continuous control tasks with image inputs. We further show that Dreamer is applicable to tasks with discrete actions and early episode termination. Future research on representation learning can likely scale latent imagination to environments of higher visual complexity.

```
