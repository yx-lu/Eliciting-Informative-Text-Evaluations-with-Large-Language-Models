Title:
```
Under review as a conference paper at ICLR 2020 TOWARDS SIMPLICITY IN DEEP REINFORCEMENT LEARNING: STREAMLINED OFF-POLICY LEARNING
```
Abstract:
```
The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms. Their pop- ularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution of the entropy term to the perfor- mance of maximum entropy algorithms. For the Mujoco benchmark, we demon- strate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we show how streamlined algorithms without entropy maximization can match the performance of SAC. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training. We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.
```

Figures/Tables Captions:
```
Figure 1: SAC performance with and without entropy maximization
Figure 2: µ k and a k values from SAC and SAC without entropy maximization normalization procedure is as follows. If G > 1, then we reset µ k ← µ k /G for all k = 1, . . . , K; otherwise, we leave µ unchanged. With this simple normalization, we are assured that the average of the normalized magnitudes is never greater than one. Henceforth we assume the policy network has been modified with the simple normalization scheme just described.
Figure 3: Streamlined Off-Policy (SOP) versus SAC, TD3+ and IG
Figure 4: (a) to (e) show the performance of SOP and SAC with ERE sampling. (f) shows over a period of 1000 updates, the expected number of times the tth data point is sampled (with η = 0.996). ERE allows new data to be sampled many times soon after being collected.
Table 1: Performance comparison at one million samples. Last column shows percentage improve- ment of SOP+ERE over SAC.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Off-policy Deep Reinforcement Learning (RL) algorithms aim to improve sample efficiency by reusing past experience. Recently a number of new off-policy Deep Reinforcement Learning algo- rithms have been proposed for control tasks with continuous state and action spaces, including Deep Deterministic Policy Gradient (DDPG) and Twin Delayed DDPG (TD3) ( Lillicrap et al., 2015 ;  Fu- jimoto et al., 2018 ). TD3, which introduced clipped double-Q learning, delayed policy updates and target policy smoothing, has been shown to be significantly more sample efficient than popular on-policy methods for a wide range of Mujoco benchmarks. The field of Deep Reinforcement Learning (DRL) has also recently seen a surge in the popularity of maximum entropy RL algorithms. Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In particular, Soft Actor Critic (SAC), which combines off-policy learning with maximum-entropy RL, not only has many attractive theoretical properties, but can also give superior performance on a wide-range of Mujoco environments, including on the high-dimensional environment Humanoid for which both DDPG and TD3 perform poorly ( Haarnoja et al., 2018a ;b;  Langlois et al., 2019 ). SAC has a similar structure to TD3, but also employs maximum entropy reinforcement learning. In this paper, we first seek to understand the primary contribution of the entropy term to the per- formance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that when using the standard objective without entropy along with standard additive noise exploration, there is often insufficient exploration due to the bounded nature of the action spaces. Specifically, the outputs of the policy network are often way outside the bounds of the action space, so that they need to be squashed to fit within the action space. The squashing results in actions persistently taking on their maximal values, so that there is insufficient exploration. In contrast, the entropy term in the SAC objective forces the outputs to have sensible values, so that even with squashing, exploration is maintained. We conclude that the entropy term in the objective for Soft Actor Critic principally addresses the bounded nature of the action spaces in the Mujoco environments. With this insight, we propose Streamlined Off Policy (SOP), a streamlined algorithm using the standard objective without the entropy term. SOP employs a simple normalization scheme to ad- dress the bounded nature of the action spaces, allowing satisfactory exploration throughout training. We also consider replacing the aforementioned normalization scheme with inverting gradients (IG) Under review as a conference paper at ICLR 2020  Hausknecht & Stone (2015) . Our results show that SOP and IG match the sample-efficiency and ro- bustness performance of SAC, including on the more challenging Ant and Humanoid environments. This demonstrates a need to revisit the importance of entropy maximization in DRL. Keeping with the theme of simplicity with the goal of meeting Occam's principle, we also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training. In vanilla SOP (as well as in DDPG, TD3, and SAC), samples from the replay buffer are chosen uniformly at random during training. Our method, called Emphasizing Recent Experience (ERE), samples more aggressively recent experience while not neglecting past experience. Unlike Priority Experience Replay (PER) ( Schaul et al., 2015 ), a popular non-uniform sampling scheme for the Atari environments, ERE is only a few lines of code and does not rely on any sophisticated data structures. We show that SOP combined with ERE out-performs SAC and provides state of the art performance. For example, for Ant and Humanoid, it improves over SAC by 21% and 24%, respectively, with one million samples. Furthermore, we also investigate combining SOP with PER, and show SOP+ERE also out-performs the more complicated SOP+PER scheme. The contributions of this paper are thus threefold. First, we uncover the primary contribution of the entropy term of maximum entropy RL algorithms when the environments have bounded action spaces. Second, we propose a streamlined algorithm which do not employ entropy maximization but nevertheless matches the sampling efficiency and robustness performance of SAC for the Mu- joco benchmarks. And third, we combine our streamlined algorithms with a simple non-uniform sampling scheme to achieve state-of-the art performance for the Mujoco benchmarks. We provide anonymized code for reproducibility 1 .

Section Title: PRELIMINARIES
  PRELIMINARIES We represent an environment as a Markov Decision Process (MDP) which is defined by the tuple (S, A, r, p, γ), where S and A are continuous multi-dimensional state and action spaces, r(s, a) is a bounded reward function, p(s |s, a) is a transition function, and γ is the discount factor. Let s(t) and a(t) respectively denote the state of the environment and the action chosen at time t. Let π = π(a|s), s ∈ S, a ∈ A denote the policy. We further denote K for the dimension of the action space, and write a k for the kth component of an action a ∈ A, that is, a = (a 1 , . . . , a K ). The expected discounted return for policy π beginning in state s is given by: Standard MDP and RL problem formulations seek to maximize V π (s) over policies π. For finite state and action spaces, under suitable conditions for continuous state and action spaces, there exists an optimal policy that is deterministic ( Puterman, 2014 ;  Bertsekas & Tsitsiklis, 1996 ). In RL with unknown environment, exploration is required to learn a suitable policy. In DRL with continuous action spaces, typically the policy is modeled by a parameterized policy network which takes as input a state s and outputs a value µ(s; θ), where θ represents the current parameters of the policy network ( Schulman et al., 2015 ; 2017;  Vuong et al., 2018 ;  Lillicrap et al., 2015 ;  Fujimoto et al., 2018 ). During training, typically additive random noise is added for explo- ration, so that the actual action taken when in state s takes the form a = µ(s; θ) + where is a K-dimensional Gaussian random vector with each component having zero mean and variance σ. During testing, is set to zero.

Section Title: ENTROPY MAXIMIZATION RL
  ENTROPY MAXIMIZATION RL Maximum entropy reinforcement learning takes a different approach than (1) by optimizing policies to maximize both the expected return and the expected entropy of the policy ( Ziebart et al., 2008 ;  Ziebart, 2010 ;  Todorov, 2008 ;  Rawlik et al., 2013 ;  Levine & Koltun, 2013 ;  Levine et al., 2016 ;  Nachum et al., 2017 ;  Haarnoja et al., 2017 ;  2018a ;b).

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 In particular, with maximization entropy RL, the objective is to maximize where H(π(·|s)) is the entropy of the policy when in state s, and the temperature parameter λ determines the relative importance of the entropy term against the reward. For entropy maximization DRL, when given state s the policy network will typically output a K- dimensional vector σ(s; θ) in addition to the vector µ(s; θ). The action selected when in state s is then modeled as µ(s; θ) + where ∼ N (0, σ(s; θ)). Maximum entropy RL has been touted to have a number of conceptual and practical advantages for DRL ( Haarnoja et al., 2018a ;b). For example, it has been argued that the policy is incentivized to explore more widely, while giving up on clearly unpromising avenues. It has also been argued that the policy can capture multiple modes of near-optimal behavior, that is, in problem settings where multiple actions seem equally attractive, the policy will commit equal probability mass to those actions. In this paper, we show for the Mujoco benchmarks that the standard additive noise exploration suffices and can achieve the same performance as maximum entropy RL.

Section Title: THE SQUASHING EXPLORATION PROBLEM
  THE SQUASHING EXPLORATION PROBLEM

Section Title: BOUNDED ACTION SPACES
  BOUNDED ACTION SPACES Continuous environments typically have bounded action spaces, that is, along each action dimension k there is a minimum possible action value a min k and a maximum possible action value a max k . When selecting an action, the action needs to be selected within these bounds before the action can be taken. DRL algorithms often handle this by squashing the action so that it fits within the bounds. For example, if along any one dimension the value µ(s; θ) + exceeds a max , the action is set (clipped) to a max . Alternatively, a smooth form of squashing can be employed. For example, suppose a min k = −M and a max k = +M for some positive number M , then a smooth form of squashing could use a = M tanh(µ(s; θ) + ) in which tanh() is being applied to each component of the K-dimensional vector. DDPG ( Hou et al., 2017 ) and TD3 ( Fujimoto et al., 2018 ) use clipping, and SAC ( Haarnoja et al., 2018a ;b) uses smooth squashing with the tanh() function. For concreteness, henceforth we will assume that smooth squashing with the tanh() is employed. We note that an environment may actually allow the agent to input actions that are outside the bounds. In this case, the environment will typically first clip the actions internally before passing them on to the "actual" environment ( Fujita & Maeda, 2018 ). We now make a simple but crucial observation: squashing actions to fit into a bounded action space can have a disastrous effect on additive-noise exploration strategies. To see this, let the output of the policy network be µ(s) = (µ 1 (s), . . . , µ K (s)). Consider an action taken along one dimension k, and suppose µ k (s) >> 1 and | k | is relatively small compared to µ k (s). Then the action a k = M tanh(µ k (s)+ k ) will be very close (essentially equal) to M . If the condition µ k (s) >> 1 persists over many consecutive states, then a k will remain close to 1 for all these states, and consequently there will be essentially no exploration along the kth dimension. We will refer to this problem as the squashing exploration problem. A similar observation was made in  Hausknecht & Stone (2015) . We will argue that algorithms such as DDPG and TD3 based on the standard objective (1) with additive noise exploration can be greatly impaired by squashing exploration.

Section Title: WHAT DOES ENTROPY MAXIMIZATION BRING TO SAC FOR THE MUJUCO
  WHAT DOES ENTROPY MAXIMIZATION BRING TO SAC FOR THE MUJUCO

Section Title: ENVIRONMENTS?
  ENVIRONMENTS? SAC is a maximum-entropy based off-policy DRL algorithm which provides good performance across all of the Mujuco benchmark environments. To the best of our knowledge, it currently pro- vides state of the art performance for the Mujoco benchmark. In this section, we argue that the prin- cipal contribution of the entropy term in the SAC objective is to resolve the squashing exploration problem, thereby maintaining sufficient exploration when facing bounded action spaces. To argue this, we consider two DRL algorithms: SAC with adaptive temperature ( Haarnoja et al., 2018b ), and Under review as a conference paper at ICLR 2020 SAC with entropy removed altogether (temperature set to zero) but everything else the same. We refer to them as SAC and as SAC without entropy. For SAC without entropy, for exploration we use additive zero-mean Gaussian noise with σ fixed at 0.3. Both algorithms use tanh squashing. We compare these two algorithms on two Mujoco environments: Humanoid-v2 and Walker-v2.  Figure 1  shows the performance of the two algorithms with 10 seeds. For Humanoid, SAC performs much better than SAC without entropy. However, for Walker, SAC without entropy performs nearly as well as SAC, implying maximum entropy RL is not as critical for this environment. To understand why entropy maximization is important for one environment but less so for another, we examine the actions selected when training these two algorithms. Humanoid and Walker have action dimensions K = 17 and K = 6, respectively. Here we show representative results for one dimension for both environments, and provide the full results in the Appendix. The top and bottom rows of  Figure 2  shows results for Humanoid and Walker, respectively. The first column shows the µ k values for an interval of 1,000 consecutive time steps, namely, for time steps 599,000 to 600,000. The second column shows the actual action values passed to the environment for these time steps. The third and fourth columns show a concatenation of 10 such intervals of 1000 time steps, with each interval coming from a larger interval of 100,000 time steps. The top and bottom rows of  Figure 2  are strikingly different. For Humanoid using SAC with entropy, the |µ k | values are small, mostly in the range [-1.5,1.5], and fluctuate significantly. This allows the action values to also fluctuate significantly, providing exploration in the action space. On the other hand, for SAC without entropy the |µ k | values are typically huge, most of which are well outside the interval [-10,10]. This causes the actions a k to be persistently clustered at either M or -M , leading to essentially no exploration along that dimension. As shown in the Appendix, this property (lack of exploration for SAC without entropy maximization) holds for all 17 action dimensions. For Walker, we see that for both algorithms, the µ k values are sensible, mostly in the range [-1,1] and therefore the actions chosen by both algorithms exhibit exploration. In conclusion, the principle benefit of maximum entropy RL in SAC for the Mujuco environments is that it resolves the squashing exploration problem. For some environments (such as Walker), the outputs of the policy network take on sensible values, so that sufficient exploration is maintained and overall good performance is achieved without the need for entropy maximization. For other environments (such as Humanoid), entropy maximization is needed to reduce the magnitudes of the outputs so that exploration is maintained and overall good performance is achieved.

Section Title: STREAMLINED OFF-POLICY (SOP) ALGORITHM
  STREAMLINED OFF-POLICY (SOP) ALGORITHM Given the observations in the previous section, a natural question is: is it possible to design a stream- lined off policy algorithm that does not employ entropy maximization but offers performance com- parable to SAC (which has entropy maximization)? As we observed in the previous section, without entropy maximization, in some environments the policy network output values |µ k |, k = 1, . . . , K can become persistently huge, which leads to insufficient exploration due to the squashing. A simple solution is to modify the outputs of the policy network by normalizing the output values when they collectively (across the action dimensions) become too large. To this end, let µ = (µ 1 , . . . , µ K ) be the output of the original policy network, and let G = k |µ k |/K. The G is simply the average of the magnitudes of the components of µ. The Under review as a conference paper at ICLR 2020 (a) Humanoid-v2 (b) Walker2d-v2 Our Streamlined Off Policy (SOP) algorithm is described in Algorithm 1. The algorithm is es- sentially DDPG plus the normalization described above, plus clipped double Q-learning and target policy smoothing ( Fujimoto et al., 2018 ). Another way of looking at it is as TD3 plus the normal- ization described above, minus the delayed policy updates and the target policy parameters. SOP also uses tanh squashing instead of clipping, since tanh gives somewhat better performance in our experiments. The SOP algorithm is "streamlined" as it has no entropy terms, temperature adapta- tion, target policy parameters or delayed policy updates. In our experiments, we also consider TD3 plus the simple normalization, and also another streamlined algorithm in which we replace the sim- ple normalization scheme described above with the inverting gradients (IG) scheme as described in  Hausknecht & Stone (2015) . The basic idea is: when gradients suggest increasing the action mag- nitudes, gradients will be downscaled if actions are within the boundaries, and inverted entirely if actions are outside the boundaries. More implementation details can be found in the Appendix. Update Q-functions by one step of gradient descent using Update policy by one step of gradient ascent using Update target networks with

Section Title: EXPERIMENTAL RESULTS FOR SOP
  EXPERIMENTAL RESULTS FOR SOP   Figure 3  compares SAC (with temperature adaptation ( Haarnoja et al., 2018a ;b)) with SOP, TD3+ (that is, TD3 plus the simple normalization), and inverting gradients (IG) for five of the most chal- Under review as a conference paper at ICLR 2020 lenging Mujuco environments. Using the same baseline code, we train with ten different random seeds for each of the two algorithms. Each algorithm performs five evaluation rollouts every 5000 environment steps. The solid curves correspond to the mean, and the shaded region to the standard deviation of the returns over the ten seeds. Results show that SOP, SAC and IG have similar sample-efficiency performance and robustness across all environments. TD3+ has slightly weaker asymptotic performance for Walker and Hu- manoid. IG initially learns slowly for Humanoid with high variance across random seeds, but gives similar asymptotic performance. This confirms that with a simple output normalization scheme in the policy network, the performance of SAC can be achieved without maximum entropy RL. In the Appendix we provide an ablation study for SOP, which shows a major performance drop when removing either double Q-learning or normalization, whereas removing target policy smoothing ( Fujimoto et al., 2018 ) results in only a small performance drop in some environments.

Section Title: NON-UNIFORM SAMPLING
  NON-UNIFORM SAMPLING We now show how a small change in the sampling scheme for SOP can achieve state of the art performance for the Mujoco benchmark. We call this sampling scheme Emphasizing Recent Ex- perience (ERE). ERE has 3 core features: (i) It is a general method applicable to any off-policy algorithm; (ii) It requires no special data structure, is very simple to implement, and has near-zero computational overhead; (iii) It only introduces one additional important hyper-parameter. The basic idea is: during the parameter update phase, the first mini-batch is sampled from the entire buffer, then for each subsequent mini-batch we gradually reduce our range of sampling to sample more aggressively from more recent data. Specifically, assume that in the current update phase we are to make 1000 mini-batch updates. Let N be the max size of the buffer. Then for the k th update, we sample uniformly from the most recent c k data points, where c k = N · η k and η ∈ (0, 1] is a hyper-parameter that determines how much emphasis we put on recent data. η = 1 is uniform sampling. When η < 1, c k decreases as we perform each update. η can made to adapt to the learning speed of the agent so that we do not have to tune it for each environment. The effect of such a sampling formulation is twofold. The first is recent data have a higher chance of being sampled. The second is that we do this in an ordered way: we first sample from all the data in the buffer, and gradually shrink the range of sampling to only sample from the most recent data. This scheme reduces the chance of over-writing parameter changes made by new data with parameter changes made by old data ( French, 1999 ;  McClelland et al., 1995 ;  McCloskey & Cohen, 1989 ;  Ratcliff, 1990 ;  Robins, 1995 ). This process allows us to quickly obtain new information Under review as a conference paper at ICLR 2020 from recent data, and better approximate the value functions near recently-visited states, while still maintaining an acceptable approximation near states visited in the more distant past. What is the effect of replacing uniform sampling with ERE? First note if we do uniform sampling on a fixed buffer, the expected number of times a data point is sampled is the same for all data points. Now consider a scenario where we have a buffer of size 1000 (FIFO queue), we collect one data at a time, and then perform one update with mini-batch size of one. If we start with an empty buffer and sample uniformly, as data fills the buffer, each data point gets less and less chance of being sampled. Specifically, over a period of 1000 updates, the expected number of times the tth data is sampled is: 1/t + 1/(t + 1) + · · · + 1/T . Figure 4f shows the expected number of times a data is sampled as a function of its position in the buffer. We see that older data are expected to get sampled much more than newer data. This is undesirable because when the agent is improving and exploring new areas of the state space; new data points may contain more interesting information than the old ones, which have already been updated many times. When we apply the ERE scheme, we effectively skew the curve towards assigning higher expected number of samples for the newer data, allowing the newer data to be frequently sampled soon after being collected, which can accelerate the learning process. In the Appendix, we provide further algorithmic detail and analysis on ERE, and compare ERE to two other sampling schemes: an exponential sampling scheme and Prioritized Experience Replay ( Schaul et al., 2015 ).

Section Title: EXPERIMENTAL RESULTS FOR SOP+ERE
  EXPERIMENTAL RESULTS FOR SOP+ERE   Figure 4  compares the performance of SOP, SOP+ERE, SAC and SAC+ERE. With ERE, both SAC and SOP gain a significant performance improvement in all environments. SOP+ERE learns faster than SAC and vanilla SOP in all Mujoco environments. SOP+ERE also greatly improves overall performance for the two most challenging environments, Ant and Humanoid, and has the best per- formance for Humanoid. In  table 1 , we show the mean test episode return and std across 10 random seeds at 1M timesteps for all environments. The last column displays the percentage improvement of SOP+ERE over SAC, showing that SOP+ERE achieves state of the art performance. In Ant and Humanoid, SOP+ERE improves performance by 21% and 24% over SAC at 1 million timesteps, respectively. As for the std, SOP+ERE gives lower values, and for Humanoid a higher value. By taking clipping in the Mujoco environments explicitly into account,  Fujita & Maeda (2018)  modified the policy gradient algorithm to reduce variance and provide superior performance among on-policy algorithms.  Eisenach et al. (2018)  extend the work of  Fujita & Maeda (2018)  for when an action may be direction.  Hausknecht & Stone (2015)  introduce Inverting Gradients, for which we provide expermintal results in this paper for the Mujoco environments.  Chou et al. (2017)  also explores DRL in the context of bounded action spaces.  Dalal et al. (2018)  consider safe exploration in the context of constrained action spaces. Uniform sampling is the most common way to sample from a replay buffer. One of the most well- known alternatives is prioritized experience replay (PER) ( Schaul et al., 2015 ). PER uses the abso- lute TD-error of a data point as the measure for priority, and data points with higher priority will have a higher chance of being sampled. This method has been tested on DQN ( Mnih et al., 2015 ) and double DQN (DDQN) ( Van Hasselt et al., 2016 ) with significant improvement and applied suc- cessfully in other algorithms ( Wang et al., 2015 ;  Schulze & Schulze, 2018 ;  Hessel et al., 2018 ;  Hou et al., 2017 ) and can be implemented in a distributed manner ( Horgan et al., 2018 ). There are other methods proposed to make better use of the replay buffer. The ACER algorithm has an on-policy part and an off-policy part, with a hyper-parameter controlling the ratio of off-policy to on-policy updates ( Wang et al., 2016 ). The RACER algorithm ( Novati & Koumoutsakos, 2018 ) selectively removes data points from the buffer, based on the degree of "off-policyness", bringing improve- ment to DDPG ( Lillicrap et al., 2015 ), NAF ( Gu et al., 2016 ) and PPO ( Schulman et al., 2017 ). In  De Bruin et al. (2015) , replay buffers of different sizes were tested, showing large buffer with data diversity can lead to better performance. Finally, with Hindsight Experience Replay( Andrychowicz et al., 2017 ), priority can be given to trajectories with lower density estimation( Zhao & Tresp, 2019 ) to tackle multi-goal, sparse reward environments.

Section Title: CONCLUSION
  CONCLUSION In this paper we first showed that the primary role of maximum entropy RL for the Mujoco bench- mark is to maintain satisfactory exploration in the presence of bounded action spaces. We then de- veloped a new streamlined algorithm which does not employ entropy maximization but nevertheless matches the sampling efficiency and robustness performance of SAC for the Mujoco benchmarks. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. Finally, we combined our streamlined algorithm with a simple non-uniform sampling scheme to achieve state-of-the art performance for the Mujoco benchmark. Under review as a conference paper at ICLR 2020

```
