Title:
```
None
```
Abstract:
```
Model-based reinforcement learning (MBRL) is widely seen as having the potential to be significantly more sample efficient than model-free RL. However, research in model-based RL has not been very standardized. It is fairly common for authors to experiment with self-designed environments, and there are several separate lines of research, which are sometimes closed-sourced or not reproducible. Accordingly, it is an open question how these various existing algorithms perform relative to each other. To facilitate research in MBRL, in this paper we gather a wide collection of MBRL algorithms and propose over 18 benchmarking environments specially designed for MBRL. We benchmark these algorithms with unified problem settings, including noisy environments. Beyond cataloguing performance, we explore and unify the underlying algorithmic differences across MBRL algorithms. We characterize three key research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. Finally, to facilitate future research on MBRL, we open-source our benchmark . 1 The link to the code base is invisible during reviewing process.
```

Figures/Tables Captions:
```
Figure 1: A subset of all 18 performance curve figures of the bench-marked algorithms. All the algorithms are run for 200k time-steps and with 4 random seeds. The remaining figures are in appendix C.
Figure 2: Performance curve for each algorithm trained for 1 million time-steps.
Figure 3: The relative performance with different planning horizon.
Table 1: Final performance for 18 environments of the bench-marked algorithms. All the algorithms are run for 200k time-steps. Blue refers to the best methods using ground truth dynamics, red to the best MBRL algorithms, and green to the best MFRL algorithms. The results show the mean and standard deviation averaged over 4 random seeds and a window size of 5000 times-steps. "GT" indicates the model is using the ground-truth dynamics.
Table 2: Wall-clock time in hours for each algorithm trained for 200k time-steps. MF baselines: SAC and TD3 are two very powerful baselines with very stable performance across different environments. In general model-free and model-based methods are two almost evenly matched rivals when trained for 200,000 time-steps.
Table 3: The relative changes of performance of each algorithm in noisy HalfCheetah environments. We use bold text to indicates a decrease of performance >10% of the performance without noise.
Table 4: Bench-marking performance for 1 million time-steps.
Table 5: The ranking of the MBRL algorithms in the 18 benchmarking environments
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) algorithms are most commonly classified in two categories: model-free RL (MFRL), which directly learns a value function or a policy by interacting with the environment, and model-based RL (MBRL), which uses interactions with the environment to learn a model of it. While model-free algorithms have achieved success in areas including robotics ( Lillicrap et al., 2015 ;  Schulman et al., 2017 ;  Heess et al., 2017 ;  Andrychowicz et al., 2018 ), video-games ( Mnih et al., 2013 ; 2016), and character animation ( Peng et al., 2018 ), their high sample complexity limits largely their application to simulated domains. By learning a model of the environment, model-based methods learn with significantly lower sample complexity. However, learning an accurate model of the environment has proven to be a challenging problem in certain domains. Modelling errors cripple the effectiveness of these algorithms, resulting in policies that exploit the deficiencies of the models, which is known as model-bias ( Deisenroth & Rasmussen, 2011 ). Recent approaches have been able to alleviate the model-bias problem by characterizing the uncertainty of the learned models by the means of probabilistic models and ensembles. This has enabled model-based methods to match model-free asymptotic performance in challenging domains while using much fewer samples ( Kurutach et al., 2018 ;  Chua et al., 2018 ;  Clavera et al., 2018 ). These recent advances have led to a great excitement in the field of model-based reinforcement learning. Despite the impressive results achieved, how these methods compare against each other and against standard baselines remains unclear. Reproducibility and lack of open-source code are persistent problems in RL ( Henderson et al., 2018 ;  Islam et al., 2017 ), which makes it difficult to compare novel algorithms against prior lines of research. In MBRL, this problem is exacerbated by the modifications made to the environments: pre-processing of the observations, modification of the reward functions, or using different episode horizons. Such lack of standardized implementations and environments in MBRL makes it difficult to quantify scientific progress. Systematic evaluation and comparison will not only further our understanding of the strengths and weaknesses of existing algorithms, but also reveal their limitations and suggest directions for future research. Benchmarks play a crucial role in other fields of research. For instance, MFRL has benefited Under review as a conference paper at ICLR 2020 greatly from the introduction of benchmarking code bases and environments such as rllab ( Duan et al., 2016 ), OpenAI Gym ( Brockman et al., 2016 ), and DM Control Suite ( Tassa et al., 2018 ); where the latter two have been the de facto benchmarking platforms. Besides RL, benchmarking platforms have also accelerated areas such as computer vision (Deng et al., 2009;  Lin et al., 2014 ), machine translation ( Koehn et al., 2007 ) and speech recognition ( Panayotov et al., 2015 ). In this paper, we benchmark 11 MBRL algorithms and 4 MFRL algorithms across 18 environments based on the standard OpenAI Gym ( Brockman et al., 2016 ). The environments, designed to hold the common assumptions in model-based methods, range from simple 2D tasks, such as Cart-Pole, to complex domains that are usually not evaluated on, such as Humanoid. The benchmark is further extended by characterizing the robustness of the different methods when stochasticity in the observations and actions is introduced. Based on the empirical evaluation, we propose three main causes that stagnate the performance of model-based methods: 1) Dynamics bottleneck: algorithms with learned dynamics are stuck at performance local minima significantly worse than using ground- truth dynamics, i.e. the performance does not increase when more data is collected. 2) Planning horizon dilemma: while increasing the planning horizon provides more accurate reward estimation, it can result in performance drops due to the curse of dimensionality and modelling errors. 3) Early termination dilemma: early termination is commonly used in MFRL for more directed exploration, to achieve faster learning. However, similar performance gain are not yet observed in MBRL algorithms, which limits their effectiveness in complex environments.

Section Title: PRELIMINARIES
  PRELIMINARIES We formulate all of our tasks as a discrete-time finite-horizon Markov decision process (MDP), which is defined by the tuple (S, A, p, r, ρ 0 , γ, H). Here, S denotes the state space, A denotes the action space, p(s |a, s) : S × A × S → [0, 1] is transition dynamics density function, r(s, a, s ) : S × A × S → R defines the reward function, ρ 0 is the initial state distribution, γ is the discount factor, and H is the horizon of the problem. Contrary to standard model-free RL, we assume access to an analytic differentiable reward function. The aim of RL is to learn an optimal policy π that maximizes the expected total reward J(π) = Eat∼π st∼p [ H t=1 γ t r(s t , a t )]. Dynamics Learning: MBRL algorithms are characterized by learning a model of the environment. After repeated interactions with the environment, the experienced transitions are stored in a data- set D = {(s t , a t , s t+1 )} which is then used to learn a dynamics functionf φ . In the case where ground-truth dynamics are deterministic, the learned dynamics functionf φ predicts the next state. In stochastic settings, it is common to represent the dynamics with a Gaussian distribution, i.e., p(s t+1 |a t , s t ) ∼ N (µ(s t , a t ), Σ(s t , a t )) and the learned dynamics model corresponds tof φ = (μ φ (s t , a t ),Σ φ (s t , a t )).

Section Title: ALGORITHMS
  ALGORITHMS In this section, we introduce the benchmarked MBRL algorithms, which are divided into: 1) Dyna- style Algorithms, 2) Policy Search with Backpropagation through Time, and 3) Shooting Algorithms.

Section Title: DYNA-STYLE ALGORITHMS
  DYNA-STYLE ALGORITHMS In the Dyna algorithm ( Sutton, 1990 ; 1991a;b), training iterates between two steps. First, using the current policy, data is gathered from interaction with the environment and then used to learn the dynamics model. Second, the policy is improved with imagined data generated by the learned model. Dyna algorithms learn policies using model-free algorithms with rich imaginary experience without interaction with the real environment. Dyna can also be applied to tasks with image input as in world models ( Ha & Schmidhuber, 2018a ;b). Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) ( Kurutach et al., 2018 ): Instead of using a single model, ME-TRPO uses an ensemble of neural networks to model the dynamics, which effectively combats model-bias. The ensemblef φ = {f φ1 , ...,f φ K } is trained using standard squared L2 loss. In the policy improvement step, the policy is updated using Trust-Region Policy Under review as a conference paper at ICLR Optimization (TRPO) ( Schulman et al., 2015 ), on experience generated by the learned dynamics models. Stochastic Lower Bound Optimization (SLBO) ( Luo et al., 2019 ): SLBO is a variant of ME-TRPO with theoretical guarantees of monotonic improvement. In practice, instead of using single-step squared L2 loss, SLBO uses a multi-step L2-norm loss to train the dynamics. Model-Based Meta-Policy-Optimzation (MB-MPO) ( Clavera et al., 2018 ): MB-MPO forgoes the reliance on accurate models by meta-learning a policy that is able to adapt to different dynamics. Similar to ME-TRPO, MB-MPO learns an ensemble of neural networks. However, each model in the ensemble is considered as a different task to meta-train ( Finn et al., 2017 ) on. MB-MPO meta-trains a policy that quickly adapts to any of the different dynamics of the ensemble, which is more robust against model-bias.

Section Title: POLICY SEARCH WITH BACKPROPAGATION THROUGH TIME
  POLICY SEARCH WITH BACKPROPAGATION THROUGH TIME Contrary to Dyna-style algorithms, where the learned dynamics models are used to provide imagined data, policy search with backpropagation through time exploits the model derivatives. Consequently, these algorithms are able to compute the analytic gradient of the RL objective with respect to the policy, and improve the policy accordingly. Probabilistic Inference for Learning Control (PILCO) ( Deisenroth & Rasmussen, 2011 ;  Deisen- roth et al., 2015 ;  Kamthe & Deisenroth, 2017 ): In PILCO, Gaussian processes (GPs) are used to model the dynamics of the environment. The dynamics model f D (s t , a t ) is a probabilistic and non- parametric function of the collected data D. The policy π θ is trained to maximize the RL objective by computing the analytic derivatives of the objective with respect to the policy parameters θ. The training process iterates between collecting data using the current policy and improving the policy. Inference in GPs does not scale in high dimensional environments, limiting its application to simpler domains. Iterative Linear Quadratic-Gaussian (iLQG) ( Tassa et al., 2012 ): In iLQG, the ground-truth dynamics are assumed to be known by the agent. The algorithm uses a quadratic approximation on the RL reward function and a linear approximation on the dynamics, converting the problem solvable by linear-quadratic regulator (LQR) ( Bemporad et al., 2002 ). By using dynamic programming, the optimal controller for the approximated problem is a linear time-varying controller. iLQG is a model predictive control (MPC) algorithm, where re-planning is performed at each time-step. Guided Policy Search (GPS) ( Levine & Abbeel, 2014 ;  Levine et al., 2015 ;  Zhang et al., 2016 ;  Finn et al., 2016b ;  Montgomery & Levine, 2016 ;  Chebotar et al., 2017 ): Guided policy search essentially distills the iLQG controllers π G into a neural network policy π θ by behavioural cloning, which minimizes E[D KL (π G (·|s t ) π θ )]. The dynamics are modelled to be Gaussian-linear time-varying. To prevent over-confident policy improvement that deviates from the last real-world trajectory, the reward function is augmented asr(s t , a t ) = r(s t , a t ) − ηD KL (π G (·|s t )||p(·|s t )), where p(·|s t ) is the passive dynamics distribution from last trajectories. In this paper, we use the MD-GPS variant ( Montgomery & Levine, 2016 ). Stochastic Value Gradients (SVG) ( Heess et al., 2015 ): SVG tackles the problem of compounding model errors by using observations from the real environment, instead of the imagined one. To accommodate mismatch between model predictions and real transitions, the dynamics models in SVG are probabilistic. The policy is improved by computing the analytic gradient of the real trajectories with respect to the policy. Re-parametrization trick is used to permit back-propagation through the stochastic sampling.

Section Title: SHOOTING ALGORITHMS
  SHOOTING ALGORITHMS This class of algorithms provide a way to approximately solve the receding horizon problem posed in model predictive control (MPC) when dealing with non-linear dynamics and non-convex reward functions. Their popularity has increased with the use of neural networks for modelling dynamics. Random Shooting (RS) ( Richards, 2005 ;  Rao, 2009 ): RS optimizes the action sequence a t:t+τ to maximize the expected planning reward under the learned dynamics model, i.e., max at:t+τ E s t ∼f φ [ t+τ t =t r(s t , a t )]. In particular, the agent generates K candidate random sequences Under review as a conference paper at ICLR 2020 of actions from a uniform distribution, and evaluates each candidate using the learned dynamics. The optimal action sequence is approximated as the one with the highest return. A RS agent only applies the first action from the optimal sequence and re-plans at every time-step. Mode-Free Model-Based (MB-MF) ( Nagabandi et al., 2017 ): Generally, random shooting has worse asymptotic performance when compared with model-free algorithms. In MB-MF, the authors first train a RS controller π RS , and then distill the controller into a neural network policy π θ using DAgger ( Ross et al., 2011 ), which minimizes D KL (π θ (s t ), π RS ). After the policy distillation step, the policy is fine-tuned using standard model-free algorithms. In particular the authors use TRPO ( Schulman et al., 2015 ). Probabilistic Ensembles with Trajectory Sampling (PETS-RS and PETS-CEM) ( Chua et al., 2018 ): In the PETS algorithm, the dynamics are modelled by an ensemble of probabilistic neural networks models, which captures both epistemic uncertainty from limited data and network capacity, and aleatoric uncertainty from the stochasticity of the ground-truth dynamics. PETS-RS is the same as RS except for different modeling of the dynamics. In PETS-CEM, the online optimization problem is solved using cross-entropy method (CEM) ( De Boer et al., 2005 ;  Botev et al., 2013 ) to obtain a better solution. PETS-CEM can also plan in latent space of image observations ( Hafner et al., 2018 ).

Section Title: MODEL-FREE BASELINES
  MODEL-FREE BASELINES In our benchmark, we include MFRL baselines to quantify the sample complexity and asymp- totic performance gap between MFRL and MBRL. Specifically, we compare against representative MFRL algorithms including Trust-Region Policy Optimization (TRPO) ( Schulman et al., 2015 ), Proximal-Policy Optimization (PPO) ( Schulman et al., 2017 ;  Heess et al., 2017 ), Twin Delayed Deep Deterministic Policy Gradient (TD3) ( Fujimoto et al., 2018 ), and Soft Actor-Critic (SAC) ( Haarnoja et al., 2018 ). The former two are state-of-the-art on-policy MFRL algorithms, and the latter two are considered the state-of-the-art off-policy MFRL algorithms.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we present the results of our benchmarking and examine the causes that stagnate the performance of MBRL methods. Specifically, we designed the benchmark to answer the following questions: 1) How do existing MBRL approaches compare against each other and against MFRL methods across environments with different complexity (Section 4.3)? 2) Are MBRL algorithms robust against observation and action noise (Section 4.4)? and 3) What are the main bottlenecks in the MBRL methods? Aiming to answer the last question, we present three phenomena inherent of MBRL methods, which we refer to as dynamics bottleneck (Section 4.5), planning horizon dilemma (Section 4.6), and early termination dilemma (Section 4.7).

Section Title: BENCHMARKING ENVIRONMENTS
  BENCHMARKING ENVIRONMENTS Our benchmark consists of 18 environments with continuous state and action space based on OpenAI Gym ( Brockman et al., 2016 ). We include a full spectrum of environments with different difficulty and episode length, from CartPole to Humanoid. More specifically, we have the following modifications: • To accommodate traditional MBRL algorithms such as iLQG and GPS, we modify the reward function so that the gradient with respect to observation always exists or can be approximated. • We note that early termination has not been applied in MBRL, and we specifically have both the raw environments and the variants with early termination, indicated by the suffix ET. • The original Swimmer-v0 in OpenAI Gym was unsolvable for all algorithms. Therefore, we modified the position of the velocity sensor so that it's easier to solve. We name this easier version as Swimmer while still keep the original one as a reference, named as Swimmer-v0. For a detailed description of the environments and the reward functions used, we refer readers to Ap- pendix A. We also provide open-sourced version of the tasks based on Roboschool or Pybullet ( AMD, 2014 ;  Ellenberger, 2018 ;  Klimov & Schulman, 2017 ), which we refer to Appendix H.

Section Title: EXPERIMENT SETUP
  EXPERIMENT SETUP Performance Metric and Hyper-parameter Search: Each algorithm is run with 4 random seeds. In the learning curves, the performance is averaged with a sliding window of 5 algorithm iterations. The error bars were plotted by the default Seaborn ( Waskom, 2010 ) smoothing scheme from the mean and standard deviation of the results. Similarly, in the tables, we show the performance averaged across different random seeds with a window size of 5000 time-steps. We perform a grid search for each algorithm separately, which is summarized in appendix B. For each algorithm, We show the results using the hyper-parameters producing the best average performance.

Section Title: Training Time
  Training Time In MFRL, 1 million time-step training is common, but for many environments, MBRL algorithms converge much earlier than 200k time-steps and it takes an impractically long time to train for 1 million time-steps for some of the MBRL algorithms. We therefore show both the performance of 200k time-step training for all algorithms and show the performance of 1M time-step training for algorithms where computation is not a major bottleneck.

Section Title: BENCHMARKING PERFORMANCE
  BENCHMARKING PERFORMANCE In  Table 1 , we summarize the performance of each algorithm trained with 200,000 time-steps. We also include some representative performance curves in  Figure 1 . The learning curves for all the environments can be seen in appendix C. The engineering statistics shown in  Table 2  include the computational resources, the estimated wall-clock time, and whether the algorithm is fast enough to run at real-time at test time, namely, if the action selection can be done faster than the default time-step of the environment. In  Table 5 , we summarize the performance ranking.

Section Title: Shooting Algorithms
  Shooting Algorithms RS is very effective on simple tasks such as InvertedPendulum, CartPole and Acrobot, but as task difficulty increases RS gradually gets surpassed by PETS-RS and PETS-CEM, which indicates that modelling uncertainty aware dynamics is crucial for the performance. At the same time, PETS-CEM is better than PETS-RS in most of the environments, showing the importance of an effective planning module. However, PETS-CEM search is not as effective as PETS-RS in Ant, Walker2D and SlimHumanoid, indicating that we need more expressive and general planning module for more complex environments. MB-MF does not have obvious gains compared to other shooting algorithms, but like other model-free controllers, MB-MF can jump out of performance local-minima in MountainCar. Shooting algorithms are effective and robust across different environments.

Section Title: Dyna-Style Algorithms
  Dyna-Style Algorithms MB with Ground-truth Dynamics: Algorithms with ground-truth dynamics can solve the majority of the tasks, except for some of the tasks such as MountainCar. With the increasing complexity of the environments, shooting methods gradually have much better performance than the policy search methods such as iLQG, whose linear quadratic assumption is not a good approximation anymore. Early termination cause a lot of troubles for model-based algorithms, both with and without ground-truth dynamics, which is further studied in section 4.7.

Section Title: NOISY ENVIRONMENTS
  NOISY ENVIRONMENTS In this section, we study the robustness of each algorithm with respect to the noise added to the observation and actions. Specifically, we added Gaussian white noise to the observations and actions with standard deviation σ o and σ a , respectively. In  Table 3  we show the results for the HalfCheetah environment, for the full results we refer the reader to appendix D. As expected, adding noise is in general detrimental to the performance of the MBRL algorithms. ME-TRPO and SLBO are more likely to suffer from a catastrophic performance drop when compared to shooting methods such as PETS and RS, suggesting that re-planning successfully compensates for the uncertainty. On the other hand, the Dyna-style method MB-MPO presents to be very robust against noise. Due to the limited exploration in baseline, the performance is sometimes increased after adding noise that encourages exploration.

Section Title: DYNAMICS BOTTLENECK
  DYNAMICS BOTTLENECK We further run MBRL algorithms for 1M time-steps on HalfCheetah, Walker2D, Hopper, and Ant environments to capture the asymptotic performance, as are shown in  Table 4  and  Figure 2 . The results show that MBRL algorithms plateau at a performance level well below their model-free counterparts and themselves with ground-truth dynamics. This points out that when learning models, more data does not result in better performance. For instance, PETS's performance plateaus after 400k time-steps at a value much lower than the performance when using the ground-truth dynamics. We also study the performance with different network capacity, as well as using linear of RBF parameterization, as summarized in Appendix G. The following assumptions can potentially explain the dynamics bottleneck. 1) The prediction error accumulates with time, and MBRL inevitably involves prediction on unseen states. While techniques such as probabilistic ensemble were proposed to capture uncertainty, it can be seen empirically in our paper as well as in  Chua et al. (2018) , that prediction becomes unstable and inaccurate with time. 2) The policy and the learning of dynamics is coupled, which makes the agents more prone to performance local-minima. While exploration and off-policy learning have been studied in  Bellemare et al. (2016) ;  Dearden et al. (1999) ;  Wiering & Schmidhuber (1998) ;  Houthooft et al. (2016) ;  Schaul et al. (2019) ;  Fujimoto et al. (2018) , it has been barely addressed on current model-based approaches. One of the critical choices in shooting methods is the plan- ning horizon. In  Figure 3 , we show the performance of iLQG, CEM and RS, using the same number of candidate planning sequences, but with different planning horizon. We notice that increasing the planning horizon does not necessarily increase the performance, and more often instead decreases the perfor- mance. This happens both when using ground-truth dynamics and using learned dynamics. We argue that this is result of insufficient planning in a search space which increases expo- nentially with planning depth, i. e., the curse of dimensionality, as is also observed in  Vemula et al. (2019) ;  Hafner et al. (2018) . However, in more complex environments such as the ones with early terminations, short planning horizon can lead to catastrophic performance drop, which we discuss in appendix I. We further experiment with the imaginary environment length in Dyna algorithms. We have similar results that increasing horizon does not necessarily help the performance, which is summarized in appendix F.

Section Title: EARLY TERMINATION DILEMMA
  EARLY TERMINATION DILEMMA Early termination, when the episode is finalized before the horizon has been reached, is a standard technique used in MFRL algorithms to prevent the agent from visiting unpromising states or damaging states for real robots ( Peng et al., 2018 ; 2016; Merel et al., 2017;  Heess et al., 2016 ;  Brockman et al., 2016 ). When early termination is applied to the real environments, MBRL can correspondingly also apply early termination in the planned trajectories, or generate early terminated imaginary data. However, we find this technique hard to integrate into the existing MB algorithms. The results, shown in  Table 1 , indicates that early termination does in fact decrease the performance for MBRL algorithms of different types. We further experiment with addition schemes to incorporate early termination, summarized in appendix I. However none of them were successful. We argue that to perform efficient learning in complex environments, such as Humanoid, early termination is almost necessary. We leave it as an important request for research.

Section Title: CONCLUSIONS
  CONCLUSIONS In this paper, we benchmark the performance of a wide collection of existing MBRL algorithms, evalu- ating their sample efficiency, asymptotic performance and robustness. Through systematic evaluation and comparison, we characterize three key research challenges for future MBRL research. Across this very substantial benchmarking, there is no clear consistent best MBRL algorithm, suggesting lots of opportunities for future work bringing together the strengths of different approaches.

```
