Title:
```
Under review as a conference paper at ICLR 2020 LEARNING REUSABLE OPTIONS FOR MULTI-TASK REINFORCEMENT LEARNING
```
Abstract:
```
Reinforcement learning (RL) has become an increasingly active area of research in recent years. Although there are many algorithms that allow an agent to solve tasks efficiently, they often ignore the possibility that prior experience related to the task at hand might be available. For many practical applications, it might be unfeasible for an agent to learn how to solve a task from scratch, given that it is generally a computationally expensive process; however, prior experience could be leveraged to make these problems tractable in practice. In this paper, we propose a framework for exploiting existing experience by learning reusable options. We show that after an agent learns policies for solving a small number of problems, we are able to use the trajectories generated from those policies to learn reusable options that allow an agent to quickly learn how to solve novel and related problems.
```

Figures/Tables Captions:
```
Figure 1: Results on four-room domain. Six tasks were used for training and 24 for testing.
Figure 2: Visualization of our framework in four rooms domain. A novel task is seen in the top left, where the agent (red) has to navigate to a goal (green). On the top right, we show the solution found by the agent. The three rows below show how the options were learned and exploited in the new task. The highlighted area in the top row shows a sample trajectory and the color corresponds to the probability that the option would take the demonstrated action. Notice that this trajectory was obtained on a previous tasks, so it does not correspond to the new task on top. The arrows show the action that is most likely at each state. After training (first row) each option specializes in a specific skill (a navigation pattern). In this case, the demonstrated trajectory can be generated by using option 3 and 2. The middle rows shows a heat-map indicating where an option is likely to terminate (close to walls), and the last row shows where each options is likely to be used by the policy learned in the new task. The agent learns to use each option in specific situations; for example, option 1 is likely to be called to make the agent move up, if it is located in one of the bottom rooms.
Figure 3: Comparison on Atari domains for primitives (blue), options before training (orange) and learned options for different values of λ 1 and λ 2 . Shaded regions indicate standard error.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) techniques have experienced much of their success in simulated en- vironments, such as video games ( Mnih et al., 2015 ) or board games ( Silver et al., 2016 ; Tesauro, 1995). One of the main reasons why RL has worked so well in these applications is that we are able simulate millions of interactions with the environment in a relatively short period of time. In many real world applications, however, where the agent interacts with the physical world, it might not be easy to generate such a large number of interactions. The time and cost associated with training such systems could render RL an unfeasible approach for training in large scale. As a concrete example, consider training a large number of humanoid robots (agents) to move quickly, as in the Robocup competition ( Farchy et al., 2013 ). Although the agents have similar dynamics, subtle variations mean that a single policy shared across all agents would not be an ef- fective solution. Furthermore, learning a policy from scratch for each agent is too data-inefficient to be practical. As shown by  Farchy et al. (2013) , this type of problem can be addressed by leveraging the experience obtained from solving a related task (e.g., walking) to quickly learn a policy for each individual agent that is tailored to a new task (e.g., running). These situations also occurs in industry, such as robots tasked with sorting items in fulfillment centers. A simple approach, like using PD controllers, would fail to adapt to the forces generated from picking up objects with different weight distributions, causing the arm to drop the objects. RL is able to mitigate this problem by learning a policy for each arm that is able to make corrections quickly, which is tailored to the robot's dynam- ics. However, training a new policy for each agent would be far too costly to be a practical solution. In these scenarios, it is possible to use a small number of policies learned a subset of the agents, and then leverage the experience obtained from learning those policies to allow the remaining agents to quickly learn their corresponding policies. This approach can turn problems that are prohibitively expensive to solve into relatively simple problems. To make use of prior experience and improve learning on new related problems in RL, several lines of work, which are complementary to each other, have been proposed and are actively being studied. Transfer learning ( Taylor & Stone, 2009 ) refers to the problem of adapting information acquired while solving one task to another. One might consider learning a mapping function that allows for a policy learned in one task to be used in a different task ( Ammar et al., 2015 ) or simply learn a mapping of the value function learned in one task to another ( Taylor et al., 2007 ). These techniques can be quite effective, but are also limited in that they consider mapping information from one source task to another target task. Another approach to reusing prior knowledge is through meta learning Under review as a conference paper at ICLR 2020 or learning to learn ( Schmidhuber, 1995 ;  Schmidhuber et al., 1998 ). In the context of RL, the goal under this framework for an agent to be exposed to a number of tasks where it can learn some general behavior that generalizes to new tasks ( Finn et al., 2017 ). One last technique to leverage prior experience, and the one this paper focuses on, is through tem- porally extended actions or temporal abstractions ( McGovern & Sutton, 1998 ;  Sutton et al., 1999 ). While in the standard RL framework the agent has access to a set of primitive actions (i.e., actions that last for one time step), temporally extended actions allow an agent to execute actions that last for several time-steps. They introduce a bias in the behavior of the agent which, if appropriate for the problem at hand, results in dramatic improvements in how quickly the agent learns to solve a new task. A popular representation for temporally extended actions is the options framework ( Sutton & Precup, 1998 ;  Sutton et al., 1999 ) (formally introduced in the next section), which is the focus of this work. It has been shown that options learned in a specific task or set of tasks, can be reused to improve learning on new tasks ( Machado et al., 2017 ;  Bacon et al., 2017 ); however, this often requires knowledge from the user about which options or how many options are appropriate for the type of problems the agent will face. In this paper, we propose learning reusable options for a set of related tasks with minimal infor- mation provided by the user. Throughout this paper, we refer as (near)-optimal policies to those policies that were learned to solve a particular task, but are not strictly speaking optimal. We con- sider the scenario where the agent must solve a large numbers of tasks and show that after learning a (near)-optimal policy for a small number of problems, we can learn an appropriate number of options that facilitates learning in a remaining set of tasks. To do so, we propose learning a set of options that minimize the expected number of decisions needed to represent trajectories generated from the (near)-optimal policies learned by the agent, while also maximizing the probability of gen- erating those trajectories. Unlike techniques that learn options to rach bottleneck states ( McGovern & Barto, 2001 ) or states deemed of high value ( Machado et al., 2017 ), our method seeks to learn options that are able to generate trajectories known to perform well. This does not necessarily lead to learn options that reach states one might consider "interesting".

Section Title: BACKGROUND AND NOTATION
  BACKGROUND AND NOTATION A Markov decision process (MDP) is a tuple, M = (S, A, P, R, γ, d 0 ), where S is the set of possible states of the environment, A is the set of possible actions that the agent can take, P (s, a, s ) is the probability that the environment will transition to state s ∈ S if the agent executes action a ∈ A in state s ∈ S, R(s, a, s ) is the expected reward received after taking action a in state s and transitioning to state s , d 0 is the initial state distribution, and γ ∈ [0, 1] is a discount factor for rewards received in the future. We use t to index the time-step and write S t , A t , and R t to denote the state, action, and reward at time t. A policy, π : S × A → [0, 1], provides a conditional distribution over actions given each possible state: π(s, a) = Pr(A t = a|S t = s). We denote a trajectory of length t as h t = (s 0 , a 0 , r 0 , . . . , s t−1 , a t−1 , r t−1 , s t ), that is, h t is defined as a sequence of states, actions and rewards observed after following some policy for t time-steps. This work focuses on learning options that can be used for a set of related tasks. We consider the setting where an agent must solve a set of related tasks, where each task is an MDP, M = (S, A, P M , R M , γ, d M 0 ); that is, each task is an MDP with its own transition function, reward function and initial state distribution, with shared state and action sets. An option, o = (I o , µ o , β o ), is a tuple in which I o ⊆ S is the set of states in which option o can be executed (the initiation set), µ o is a policy that governs the behavior of the agent while executing o, and β o : S → [0, 1] is a termination function that determines the probability that o terminates in a given state. We assume that I o = S for all options o; that is, the options are available at every state. The options framework does not dictate how an agent should choose between available options or how options should be discovered. A common approach to selecting between options is to a learn a policy over options, which is defined by the probability of choosing an option in a particular state. Two recent popular approaches to option discovery are eigenoptions ( Machado et al., 2017 ) and the option-critic architecture ( Bacon et al., 2017 ). The eigenoptions ( Machado et al., 2017 ) of an MDP are the optimal policies for a set of implicitly defined reward functions called eigenpurposes. Eigenpurposes are defined in terms of proto-value functions ( Mahadevan, 2005 ), which are in turn derived from the eigenvectors of a modified adja- cency matrix over states for the MDP. The intuition is that no matter the true reward function, the Under review as a conference paper at ICLR 2020 eigenoptions allow an agent to quickly traverse the transition graph, resulting in better exploration of the state space and faster learning. However, there are two major downsides: 1) the adjacency matrix is often not known a priori, and may be difficult to construct for large MDPs, and 2) for each eigenpurpose, constructing the corresponding eigenoption requires solving a new MDP. The option-critic architecture ( Bacon et al., 2017 ) is a more direct approach to learn options and a policy over options simultaneously using policy gradient methods. One issue that often arises within this framework is that the termination functions of the learned options tend to collapse to "always termi- nate". In a later publication, the authors built on this work to consider the case where there is a cost associated with switching options ( Harb et al., 2018 ). This method resulted in the agent learning to use a single option while it was appropriate and terminate when an option switch was needed, allow- ing it to discover improved policies for a particular task. The authors argue that minimizing the use of the policy over options may be desirable, as the cost of choosing an option may be greater than the cost of choosing a primitive action when using an option. Recent work by  Harutyunyan et al. (2019)  approaches the aforementioned termination problem by explicitly optimizing the termination function of options to focus on small regions of the state space. However, in contrast to the work presented in these paper, these methods do not explicitly take into consideration that the agent might face many related tasks in the future. We build on the idea that minimizing the number of decisions made by an agent leads to the discov- ery of general reusable options, and propose an offline method where they are learned by solving a small number of tasks. The options are then leveraged to quickly solve new problems the agent will face in the future. We use the trajectories generated while learning (near)-optimal policies, and learn an appropriate set of options by directly minimizing the expected number of decisions the agent makes while simultaneously maximizing the probability of generating the observed trajectories.

Section Title: LEARNING REUSABLE OPTIONS FROM EXPERIENCE
  LEARNING REUSABLE OPTIONS FROM EXPERIENCE In this section, we introduce the objective for learning a set of reusable options for a set of related tasks. Our algorithm introduces one option at a time until introducing a new option does not improve the objective further. This procedure results in a natural way of learning an adequate number of options without having to predefine it; a new option is included if it is able to improve the probability of generating optimal behavior while minimizing the number of decisions made by the agent. Our method assumes that the agent has learn a policy for a small number of tasks, and sample trajectories are obtained from these (near)-optimal policies. Notice that the propose algorithm is only concerned with being able to recreate the demonstrated trajectories, so if these were sample from a poorly performing policy the options learned are unlikely to provide any benefits.

Section Title: PROBLEM FORMULATION
  PROBLEM FORMULATION In the options framework, at each time-step, t, the agent chooses an action, A t , based on the current option, O t . Let T t be a Bernoulli random variable, where T t = 1 if the previous option, O t−1 , terminated at time t, and T t = 0 otherwise. If T t = 1, O t is chosen using the policy over options, π. If T t = 0, then the previous option continues, that is, O t = O t−1 . To ensure we can represent any trajectory, we consider primitive actions to be options which always select one specific action and then terminate; that is, for an option, o, corresponding to a primitive, a, for all s ∈ S, the termination function would be given by β o (s) = 1, and the policy by µ(s, a ) = 1 if a = a and 0 otherwise. Let O = O A ∪ O O denote a set of options, {o 1 , . . . , o n }, where O A refers to the set of options corresponding to primitive actions and O O to the set corresponding to temporal abstractions. Fur- thermore, let H be a random variable denoting a trajectory of length |H| generated by a near-optimal policy, and let H t be a random variable denoting the sub-trajectory of H up to the state encountered at time-step t. We seek to find a set, O * = {o * 1 , . . . , o * n }, that maximizes the following objective: J(π, O) = E |H| t=1 Pr(T t = 0, H t |π, O) + λ 1 g(H, O O ) , (1) where g(h, O O ) is a regularizer that encourages a diverse set of options, and λ 1 is a scalar hyper- parameter. If we are also free to learn the parameters of π, then O * ∈ arg max One choice for g is the average KL divergence on a given trajectory over the set of m options Under review as a conference paper at ICLR 2020 being learned: g(h, O O ) = 2 m(m−1) o,o ∈O O |h|−1 t=0 D KL ( µ o (s t )|| µ o (s t )). 1 Intuitively, we seek to find options that are capable of generating near-optimal trajectories with a small number of terminations. Notice that minimizing the number of terminations is the same as minimizing the number of decisions made by the policy over options, as each termination requires the policy to choose a new option. Given a set of options, a policy over options, and a near-optimal sample trajectory, we can calculate the joint probability for a trajectory exactly, and estimate equation 1 by averaging over a set of near-optimal trajectories.

Section Title: OPTIMIZATION OBJECTIVE FOR LEARNING OPTIONS
  OPTIMIZATION OBJECTIVE FOR LEARNING OPTIONS Given that the agent must solve a set of tasks, we can use the experienced gathered on a subset of tasks to obtain trajectories demonstrating optimal behavior. Given a set, H, of trajectories generated from an initial subset of tasks, we can now estimate the expectation in equation 1 to learn options that can be leveraged in the remaining problems. Because the probability of generating any trajectory approaches 0 as the length of the trajectory increases, we make modify the original objective for better numerical stability, and arrive to the objectiveĴ that we optimize in practice. A more detailed discussion on how we arrived to this objective is provided in Appendix A. We can express equation 2 entirely in terms of the policy over options π, options O = {o 1 , . . . , o n } and the transition function, P (which we estimate from samples). The following theorems show how to calculate the first two terms in equation 2, allowing us to maximize the proposed objective. Theorem 1. Given a set of options, O, and a policy, π, over options, the expected number of terminations for a trajectory h is given by: Proof. See Appendix B. Theorem 2. Given a set of options O and a policy π over options, the probability of generating a trajectory h of length |h| is given by: where f is a recursive function defined as: Given a parametric representation of the op- tion policies and termination functions for each o ∈ O and for the policy π over options, we use Theorems 1 and 2 to differentiate the objective in equation 2 with respect to their parameters and optimize with any numerical optimization technique.

Section Title: LEARNING OPTIONS INCREMENTALLY
  LEARNING OPTIONS INCREMENTALLY One common issue in option discovery is iden- tifying how many options are needed for a given problem. Oftentimes this number is pre- defined by the user based on intuition. In such a scenario, one could learn options by simply randomly initializing the parameters of a num- ber of options and optimizing the proposed ob- jective in equation 2. Instead, we propose not only learning options, but also the number of options needed, by the procedure shown in Al- gorithm 1. This algorithm introduces one op- tion at a time and optimizes the objectiveĴ with respect to the policy over options π θ , with pa- rameters θ, and the newly introduced option, o = (µ φ , β ψ ), with parameters φ and ψ, for N epochs. Optimizing both o and π θ allows us to estimate how much we can improveĴ given that we keep any previously introduced option fixed. After the new option is trained, we measure how muchĴ has improved; if it fails to improve above some threshold, ∆, the procedure terminates. This results in a natural way of obtaining an appropriate number of options, as options stop being added once a new option no longer improves the ability to represent the demonstrated behavior.

Section Title: EXPERIMENTAL RESULTS
  EXPERIMENTAL RESULTS This section describes experiments used to evaluate the proposed technique approach. We show results in the "four rooms" domain to allow us to visualize and understand the options produced by our method, and to show empirically that these options produce a clear improvement in learning. We use this domain to show that options generated by our method are able to generalize to tasks where the option-critic architecture ( Bacon et al., 2017 ) and eigenoptions ( Machado et al., 2017 ) would fail to do so. We then extend our experiments to evaluate our technique in a few selected problems from the Atari 2600 emulator provided by OpenAI Gym ( Brockman et al., 2016 ). These experiments demonstrate that by using the trajectories obtained from solving a small subset of tasks, our approach is able to discover options that significantly improve the learning ability of the agent in the tasks it has yet to solve. For the four room experiment, we assume the transition function was known in advance. In all ATARI experiments, we estimated the transition functions by fitting the parameters of a linear Gaussian model to all the transitions experienced during training.

Section Title: EXPERIMENTS ON FOUR ROOMS ENVIRONMENT
  EXPERIMENTS ON FOUR ROOMS ENVIRONMENT We tested our approach in the four rooms domain: a gridworld of size 40 × 40, in which the agent is placed in a start state and needs to reach a goal state. At each time-step, the agent executes one of four available actions: moving left, right, up or down, and receives a reward of −1. Upon reaching the goal state, the agent receives a reward of +10. We generated 30 different task variations by changing the goal and start locations, and collected six sample trajectories from optimal policies learned in six tasks. We evaluated our method on the remaining 24 tasks. Figure 1a shows the change in the average expected number of terminations and average probability of generating the observed trajectories while learning options, as new options are introduced and adapted to the sampled trajectories. Options were learned over the six sampled optimal trajectories and every 50 epochs a new option was introduced. For every new option, the change in probability of Under review as a conference paper at ICLR 2020 (a) Visualization of loss over 200 training epochs for the four rooms domain. The decreasing aver- age number of decisions made by π is shown in blue and the increasing probability of generating the sampled trajectories is shown in red. (b) Performance comparison on four rooms domain. Six tasks were used for training and 24 different for testing. The plot shows the average return (and stan- dard error) on the y-axis as a function of the episode number on the test tasks. generating the observed trajectories as well as the change in expected number of decisions reaches a plateau after 30 or 40 training epochs. When a new option is introduced, there is a large jump in the loss because a new policy, π, is initialized arbitrarily to account for the new option set being evaluated. However, after training the new candidate option, the overall loss improves beyond what it was possible before introducing the new option. In Figure 1b, we compare the performance of Q-learning on 24 novel test tasks using options dis- covered by our method (with and without regularization using KL divergence), eigenoptions, and option critic. We allowed each competing method to learn options from the same six training tasks and, to ensure a fair comparison, we used the original code provided by the authors. As baselines, we also compare against primitive actions and randomly initialized options. It might seem surpris- ing that both eigenoptions and the option-critic failed to reach an optimal policy when they were shown to work well in this type of problem; for that we offer the following explanation. Our imple- mentation of four rooms is defined in a much larger state space than the ones where these methods were originally tested, making each individual room much larger. Since the options identified by these methods tend to lead the agent from room to room, it is possible that, once in the correct room, the agent executes an option leading to a different room before it had the opportunity to find the goal. When testing our approach in the smaller version of the four room problem, we found no clear difference in performance of the competing methods. In this experiment, we set the threshold ∆ for introducing a new option to 10% ofĴ at the previous iteration and the hyperparameter λ 2 = 100.0. When adding KL regularization, we set λ 1 = 0.001.  Figure 2  shows a visualization of the policy learned by the agent for a specific task. The policy leads the agent to navigate from a specific location in the bottom-left room to a location in the top-right room in a small "four-room" domain of size 10 × 15. 2 The new task to solve is shown in the top-left figure, while the solution found is shown in the top-right figure. The remaining rows of images depict the learned option policies, termination functions, and how they were used in the new task. The first row shows the learned option policies after training, the center row depict the termination functions and the bottom row shows a heat-map depicting where each option is likely to be called. The figure shows that while the options are defined over the entire state space, they are only useful in specific regions-that is, they are specialized. These options, when used in combination in specific regions, allow the agent to learn how to solve new problems more efficiently.

Section Title: EXPERIMENTS USING ATARI 2600 GAMES
  EXPERIMENTS USING ATARI 2600 GAMES We evaluated the quality of the options learned by our framework in two different Atari 2600 games: Breakout and Amidar. We trained the policy over options using A3C ( Mnih et al., 2016 ) with grayscale pixel input. Options were represented by a two layer convolutional neural network, and Under review as a conference paper at ICLR 2020 were given the previous two frames as input. In both experiments the task variations consisted in changing the number of frames skipped after taking an action (randomly selected between 2 and 10), the reward function by scaling the reward with a real number between 0.1 and 10.0, and initial state distribution by letting the agent execute between 0 and 20 actions before start it starts learning. The full implementation details for these experiments are given in Appendix E. Figures 3a and 3b show the performance of the agent as a function of training time in Breakout and Amidar, respectively. The plots show that given good choices of hyperparameters, the learned options led to a clear improvement in performance during training. For both domains, we found that λ 2 = 5, 000 led to a reasonable trade-off between the first two term inĴ, and report results with three different regularization values: λ 1 = 0.0,, λ 1 = 0.01 and λ 1 = 0.1. Note that our results do not necessarily show that the options result in a better final policy, but they improve exploration early in training and enable the agent to learn more effectively. Figure 4a depicts the behavior for one of the learned options on Breakout. The option efficiently catches the ball after it bounces off the left wall, and then terminates with high probability before the ball has to be caught again. Bear in mind that the option remains active for many time-steps, significantly reducing the number of decisions made by the policy over options. However, it does not maintain control for so long that the agent is unable to respond to changing circumstances. Note that the option is only useful in specific case; for example, it was not helpful in returning a ball bounced off the right wall. That is to say, the option specialized in a specific sub-task within the larger problem: a highly desirable property for generally useful options. Figure 4b shows the selection of two of the options learned for Amidar when starting a new game. At the beginning of the game, option 1 is selected, which takes the agent to a specific intersection before terminating. The agent then selects option 2, which chooses a direction at the intersection, follows the resulting path, and terminates at the next intersection. Note that the agent does not need to repeatedly select primitive actions in order to simply follow a previously chosen path. Having access to these types of options enables an agent to easily replicate known good behaviors, allowing for faster and more meaningful exploration of the state space. (a) Visualization of a learned option executed until termination on Breakout. The option learned to catch the ball bouncing off the left wall and terminates with high probability before the ball bounces a wall again (ball size increased for visualization). (b) Visualization of two learned options on Amidar. The agent is shown in yellow and enemies in pink. Option 1 learned to move up, at the beginning of the game, and turn left until reaching an intersection. Option 2 learned to turn in that intersection and move up until reaching the next one.

Section Title: CONCLUSION AND FUTURE WORK
  CONCLUSION AND FUTURE WORK In this work we presented an optimization objective for learning options offline from demonstrations of near-optimal behavior on a set of tasks. Optimizing the objective results in a set of options that allows an agent to reproduce the behavior while minimizing the number of decisions made by the policy over options, which are able to improve the learning ability of the agent on new tasks. We provided results showing how options adapt to the trajectories provided and showed, through several experiments, that the identified options are capable of significantly improving the learning ability of an agent. The resulting options encode meaningful abstractions that help the agent interact with and learn from its environment more efficiently. Under review as a conference paper at ICLR 2020
  This term is only defined when we consider more than one option. Otherwise, we set this term to 0.

```
