<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 RISK AVERSE VALUE EXPANSION FOR SAMPLE EFFI- CIENT AND ROBUST POLICY LEARNING</article-title></title-group><abstract><p>Model-based Reinforcement Learning(RL) has shown great advantage in sample- efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some chal- lenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>In contrast to the tremendous progress made by model-free reinforcement learning algorithms in the domain of games(Mnih et al., 2015; Silver et al., 2017; Vinyals et al., 2019), poor sample efficiency has risen up as a great challenge to RL, especially when interacting with the real world. Toward this challenge, a promising direction is to integrate the dynamics model to enhance the sample efficiency of the learning process(Sutton, 1991; Calandra et al., 2016; Kalweit &amp; Boedecker, 2017; Oh et al., 2017; Racani&#232;re et al., 2017). However, classic model-based reinforcement learning(MBRL) meth- ods tend to lag behind the model-free methods(MFRL) asymptotically, especially in cases of noisy environments and long trajectories. The hybrid combination of MFRL and MBRL(Hybrid-RL for short) has attracted much attention due to this reason. A lot of efforts has been devoted to this field, including the dyna algorithm(Kurutach et al., 2018), model-based value expansion(Feinberg et al., 2018), I2A(Weber et al., 2017), etc.</p><p>The robustness of the learned policy is another concern in RL. For stochastic environments, pol- icy can be vulnerable to tiny disturbance and occasionally drop into catastrophic consequences. In MFRL, off-policy RL(such as DQN, DDPG) typically suffers from such problems, which in the end leads to instability in the performance including sudden drop in the rewards. To solve such problem, risk sensitive MFRL not only maximize the expected return, but also try to reduce those catastrophic outcomes(Garc&#305;a &amp; Fern&#225;ndez, 2015; Dabney et al., 2018a; Pan et al., 2019). For MBRL and Hybrid-RL, without modeling the uncertainty in the environment(especially for contin- uous states and actions), it often leads to higher function approximation errors and poorer perfor- mances. It is proposed that complete modeling of uncertainty in transition can obviously improve the performance(Chua et al., 2018), however, reducing risks in MBRL and Hybrid-RL has not been sufficiently studied yet.</p><p>In order to achieve both sample efficiency and robustness at the same time, we propose a new Hybrid-RL method more capable of solving stochastic and risky environments. The proposed method, namely Risk Averse Value Expansion(RAVE), is an extension of the model-based value ex- pansion(MVE)(Feinberg et al., 2018) and stochastic ensemble value expansion(STEVE)(Buckman Under review as a conference paper at ICLR 2020 et al., 2018). We systematically analyse the approximation errors of different methods in stochastic environments. We borrow ideas from the uncertainty modeling( Chua et al. (2018)) and risk averse reinforcement learning. The probabilistic ensemble environment model is used, which captures not only the variance in estimation(also called epistemic uncertainty), but also stochastic transition na- ture of the environment(also called aleatoric uncertainty). Utilizing the ensemble of estimations, we further adopt a dynamic confidence lower bound of the target value function to make the policy more risk-sensitive. We compare RAVE with prior MFRL and Hybrid-RL baselines, showing that RAVE not only yields SOTA expected performance, but also facilitates the robustness of the policy.</p></sec><sec><title>RELATED WORKS</title><p>The model-based value expansion(MVE)(Feinberg et al., 2018) is a Hybrid-RL algorithm. Un- like typical MFRL such as DQN that uses only 1 step bootstrapping, MVE uses the imagination rollouts of length H to predict the target value. The assistance of environment model can greatly improve the sample efficiency at the start, but the precision of long term inference becomes limited asymptotically. In order to properly balance the contribution of the value expansion of different horizons, stochastic ensemble value expansion(STEVE)(Buckman et al., 2018) adopts an interpola- tion of value expansion of different horizon. The accuracy of the expansion is estimated through the ensemble of environment models as well as value functions. Ensemble of environment models also models the uncertainty to some extent, however, ensemble of deterministic model captures mainly epistemic uncertainty instead of stochastic transitions(Chua et al., 2018).</p><p>The uncertainty or the function approximation error is typically divided into three classes(Geman et al., 1992): the noise exists in the objective environment, e.g., the stochastic transitions, which is also called aleatoric uncertainty(Chua et al., 2018). The model bias is the error produced by the limited expressive power of the approximating function, which is measured by the expectation of ground truth and the prediction of the model, in case that infinite training data is provided. The variance is the uncertainty brought by insufficient training data, which is also called epistemic un- certainty. Dabney et al. (2018b) discuss the epistemic and aleatoric uncertainty in their work and focus on the latter one to improve the distributional RL. Recent work suggests that ensemble of probabilistic model(PE) is considered as more thorough modeling of uncertainty(Chua et al., 2018), while simply aggregate deterministic model captures only variance or epistemic uncertainty. The stochastic transition is more related to the noise(or aleatoric uncertainty), and the epistemic uncer- tainty is usually of interest to many works in terms of exploitation&amp;exploration(Pathak et al., 2017; Schmidhuber, 2010; Oudeyer &amp; Kaplan, 2009). Other works adopt ensemble of deterministic value function for exploration(Osband et al., 2016; Buckman et al., 2018).</p><p>Risks in RL typically refer to the inherent uncertainty of the environment and the fact that policy may perform poorly in some cases(Garc&#305;a &amp; Fern&#225;ndez, 2015). Risk sensitive learning requires not only maximization of expected rewards, but also lower variances and risks in performance. To- ward this object, some works adopt the variance of the return(Sato et al., 2001; Pan et al., 2019; Reddy et al., 2019), or the worst-case outcome(Heger, 1994; Gaskett, 2003) in either policy learn- ing(Pan et al., 2019; Reddy et al., 2019), exploration(Smirnova et al., 2019), or distributional value estimates(Dabney et al., 2018a). An interesting issue in risk reduction is that reduction of risks is typically found to be conflicting with exploration and exploitation that try to maximize the reward in the long run. Authors in (Pan et al., 2019) introduce two adversarial agents(risk aversion and long- term reward seeking) that act in combination to solve this problem. Still, it remains quite tricky and empiristic to trade-off between risk-sensitive and risk-seeking(exploration) in RL. In this paper, we propose a dynamic confidence bound for this purpose.</p><p>A number of prior works have studied function approximation error that leads to overestimation and sub-optimal solution in MFRL. Double DQN(DDQN)(Van Hasselt et al., 2016) improves over DQN through disentangling the target value function and the target policy that pursues maximum value. In TD3(Fujimoto et al., 2018) the authors suggest that systematic overestimation of value function also exists in actor-critic MFRL. They use an ensemble of two value functions, with the minimum estimate being used as the target value. Selecting the lower value estimation is similar to using uncertainty or lower confidence bound which is adopted by the other risk sensitive methods(Pan et al., 2019), though they claimed different motivations.</p></sec><sec><title>PRELIMINARIES</title></sec><sec><title>ACTOR-CRITIC MODEL-FREE REINFORCEMENT LEARNING</title><p>The Markov Decision Processes(MDP) is used to describe the process of an agent interacting with the environment. The agent selects the action a t &#8712; A at each time step t. After executing the action, it receives a new observation s t+1 &#8712; S and a feedback r t &#8712; R from the environment. As we focus mainly on the environments of continuous action, we denote the parametric deterministic policy that the agent uses to decide its action as a t = &#181; &#952; (s t ). Typically we add Gaussian exploration noise on top of the deterministic policy, such that we have a stochastic behavioral policy &#960; &#952;,&#963; : S &#215;A &#8594; R. It is calculated as &#960; &#952;,&#963; (s t , a t ) = p N (a t |&#181; &#952; (s t ), &#963; 2 ), where p N (x|m, &#963; 2 ) represents the probability density at x in a Gaussian distribution N (m, &#963; 2 ). As the interaction process continues, the agent generates a trajectory &#964; = (s 0 , a 0 , r 0 , s 1 , a 1 , r 1 , ...) following the policy &#960; &#952;,&#963; . For finite horizon MDP, we use the indicator d : S &#8594; {0, 1} to mark whether the episode is terminated. The objective of RL is to find the optimal policy &#960; * to maximize the expected discounted sum of rewards along the trajectory. The value performing the action a with policy &#960; at the state s is defined by Q &#960; (s, a) = E s0=s,a0=a,&#964; &#8764;&#960; &#8734; t=0 &#947; t r t , where 0 &lt; &#947; &lt; 1 is the discount factor. The value iteration in model-free RL tries to approximate the optimal value Q &#960; * with a parametric value functionQ &#966; by minimizing the Temporal Difference(TD) error, where &#966; is the parameter to be optimized. The TD error between the estimates of Q-value and the corresponding target values is shown in equation. 1, where &#966; is a delayed copy of the parameter &#966;, and a &#8764; &#960; &#952; , with &#952; being a delayed copy of &#952;(Lillicrap et al., 2015).</p><p>To optimize the deterministic policy function in a continuous action space, deep deterministic policy gradient(DDPG)(Lillicrap et al., 2015) maximizes the value function (or minimizes the negative value function) under the policy &#181; &#952; with respect to parameter &#952;, shown in equation. 2.</p></sec><sec><title>ENVIRONMENT MODELING</title><p>To model the environment in continuous space, an environment model is typically composed of three individual mapping functions:f r,&#950;r : S &#215; A &#215; S &#8594; R,f s,&#950;s : S &#215; A &#8594; S, andf d,&#950; d : S &#8594; [0, 1], which are used to approximate the feedback, next state and probability of the terminal indicator respectively(Gu et al., 2016; Feinberg et al., 2018). Here &#950; r , &#950; s and &#950; d are used to represent the parameters of the corresponding mapping functions. With the environment model, starting from s t , a t , we can predict the next state and reward b&#375;</p><p>and this process might go on to generate a complete imagined trajectory of [s t , a t ,r t ,&#349; t+1 , ...]. The neural network is commonly used as an environment model due to its powerful express ability. To optimize the parameter &#950; we need to minimize the mean square error(or the cross entropy) of the prediction and the ground truth, given the trajectories &#964; under the behavioral policy.</p></sec><sec><title>UNCERTAINTY AWARE PREDICTION</title><p>The deterministic model approximates the expectation only. As we mentioned in the previous sec- tions, overestimation is attributed to the error in function approximation. Following Chua et al. (2018), we briefly review different uncertainty modeling techniques. Probabilistic models output a distribution (e.g., mean and variance of a Gaussian distribution) in- stead of an expectation. We take the reward component of the environment model as an example, Under review as a conference paper at ICLR 2020 the probabilistic model is written as r &#8764; N (f r,&#950;r ,&#963; 2 r,&#950;r ), and the loss function is the negative log likelihood(equation. 4).</p><p>Ensemble of deterministic(DE) models maintains an ensemble of parameters, which is typically trained with different training samples and different initial parameters. E.g, given the ensemble of parameters &#950; r,1 , &#950; r,2 , ..., &#950; r,N , the expectation and the variance of the prediction is acquired from equation. 6&#202;</p><p>We define the operator&#202; x andV x as the average and the variance on the ensemble of xes respec- tively. As proposed by (Chua et al., 2018), the variance&#963; 2 in equation. 4 mainly captures the aleatoric uncertainty, and the varianceV mainly captures the epistemic uncertainty. Ensemble of probabilistic models(PE) keeps track of an collection of distributions {N (f r,&#950;r,i ,&#963; 2 r,&#950;r,i )}, i &#8712; [1, N ], which can further give the estimation of both uncertainties. A sampling form PE goes as follow</p></sec><sec><title>MODEL-BASED VALUE EXPANSION</title><p>In MVE(Feinberg et al., 2018) the learned environment modelf &#950;r,&#950;s,&#950; d = (f s,&#950;s ,f r,&#950;r ,f d,&#950; d ) to- gether with the policy &#181; &#952; are used to image a trajectory starting from state s t and action a t , which is represented by&#964; &#950;r,&#950;s,&#950; d ,&#952; ,H (r t , s t+1 ). It produces a trajectory up to horizon H(H &#8805; 0). We can write&#964; &#950;r,&#950;s,&#950; d ,&#952; ,H (r t , s t+1 ) = (r t , s t+1 ,&#226; t+1 ,r t+1 ,&#349; t+2 , ...,&#349; t+H+1 ,&#226; t+H+1 ).</p><p>Then the target valueQ target in equation. 2 is replaced with estimated returnQ MVE &#950;r,&#950;s,&#950; d ,&#952; ,&#966; ,H on the sampled trajectory&#964; &#950;r,&#950;s,&#950; d ,&#952; ,H (r t , s t+1 ), which is expressed in equation 7.</p></sec><sec><title>STOCHASTIC ENSEMBLE VALUE EXPANSION</title><p>Selecting proper horizon H for value expansion is important to achieve high sample efficiency and asymptotic accuracy at the same time. Though the increase of H brings increasing prophecy, the asymptotic accuracy is sacrificed due to the increasing reliance on the environment model. In STEVE(Buckman et al., 2018), interpolating the value expansionsQ MVE &#950;r,&#950;s,&#950; d ,&#952; ,&#966; ,H of different H &#8712; [0, H max ] is proposed. The weight for each horizon is decided by the inverse of its variance, which is calculated by estimating an ensemble of values switching the combination of parameters in environment modelf and value functionQ &#966; . Through our notation, STEVE can be written as equation. 8.Q</p></sec><sec><title>INVESTIGATION OF THE APPROXIMATION ERROR IN STOCHASTIC ENVIRONMENTS</title><p>To thoroughly investigate the impact of aleatoric uncertainty on hybri-RL methods, we construct a demonstrative toy environment(fig. 1(a)). The agent starts from s 0 = 0, chooses an action a t from A = [&#8722;1, +1] at each time step t. The transition of the environment is s t+1 = s t + at |at| + k &#183; N (0, 1). We compare two different environments:k = 0 and k = 1, where k = 0 represents the deterministic transition, and k = 1 represents the stochastic transition. The episode terminates at (|s| &gt; 5 ), where the agent acquires a final reward. The agent gets a constant penalty(-100) at each time step to encourage it to reach the terminal state as soon as possible. Note that the deterministic environment actually requires more steps in expectation to reach |s &gt; 5| compared with the stochastic environment, thus the value function at the starting point of k = 1 (Ground truth = 380+) tends to be lower than that of k = 0(Ground truth = 430+). We apply the learning methods including DDPG, MVE, STEVE to this environment, and plot the changes of estimate values at the starting point(see <xref ref-type="fig" rid="fig_4">fig. 5</xref>).</p><p>The results show that, in the deterministic environment, the Q-values estimated by all methods converge to the ground-truth asymptotically in such a simple environment. However, after adding the noise, previous MFRL and Hybrid-RL methods show various level of overestimation. The au- thors of (Feinberg et al., 2018) have claimed that value expansion improves the quality of esti- mated values, but MVE and STEVE actually give even worse prediction than model-free DDPG in the stochastic environment. A potential explanation is that the overall overestimation comes from the unavoidable imprecision of the estimator(Thrun &amp; Schwartz, 1993; Fujimoto et al., 2018), but Hybrid-RL also suffers from the approximation error of the dynamics model. When using a de- terministic environment model, the predictive transition of both environments would be identical, because the deterministic dynamics model tends to approximate the expectation of next states(e.g, f s,&#950;s (s t = 0, a t &gt; 0) = 1.0,f s,&#950;s (s t = 1.0, a t &gt; 0) = 2.0). This would result in the same value es- timation for k = 0 and k = 1 for both value expansion methods, but the ground truth of Q-values are different in these two environments. As a result, the deterministic environment introduces additional approximation error, leading to more severe overestimation.</p></sec><sec><title>RISK AVERSE VALUE EXPANSION</title><p>We proposed mainly two improvements based on MVE and STEVE. Firstly, we apply an ensemble of probabilistic models (PE) to enable the environment model to capture the uncertainty over possi- ble trajectories. Secondly, inspired by risk sensitive RL, we calculate the confidence lower bound of the target value(fig 1(c)). Before introducing RAVE we start with the Distributional Value Expansion(DVE). Compared with MVE that uses a deterministic environment model and value function, DVE uses a probabilistic en- vironment model, and we independently sample the reward and the next state using the probabilistic environment models(see equation 9 and <xref ref-type="fig" rid="fig_1">fig. 2</xref>).</p><p>We apply the distributional expansion starting from r t , s t+1 to acquire the trajector&#7929; &#964; &#950;r,&#950;s,&#950; d ,&#952; ,H (r t , s t+1 ) = (r t , s t+1 ,&#227; t+1 ,r t+1 ,s t+2 , ...,s t+H+1 ,&#227; t+H+1 ), based on which we write DVE as equation. 10.</p><p>We then keep track of an ensemble of the combination of the parameters {&#950; r , &#950; s , &#950; d , &#966; }. For each group of parameters we use an ensemble of N parameters, which gives us 4N parameters in all. Then we select a random combination of four integers {i, j, k, l}, which gives the parameter com- bination of {&#950; r,i , &#950; s,j , &#950; d,k , &#966; l }. By switching the combination of integers we acquire an ensemble of DVE estimation. Then we count the average and the variance on the ensemble of DVE, and by subtracting a certain proportion(&#945;) of the standard variance, we acquire a lower bound of DVE esti- mation. We call this estimation of value function the &#945;-confidence lower bound(&#945;-CLB), written as equation. 11.Q</p><p>Subtraction of variances is commonly used in risk-sensitive RL(Sato &amp; Kobayashi, 2000; Pan et al., 2019; Reddy et al., 2019). The motivation is straight forward, we try to suppress the utility of the Under review as a conference paper at ICLR 2020 high-variance trajectories, in order to avoid possible risks. However, the &#945; here is left undecided. We will come to this problem in the next part.</p><p>Finally, we define RAVE, which adopts the similar interpolation among different horizons as STEVE based on DVE and CLB, shown in equation. 13.</p><p>While adopting the lower confidence bound may introduce the bias of underestimation, it makes the policy less preferable to actions with large variance of future return.</p></sec><sec><title>ADAPTIVE CONFIDENCE BOUND</title><p>An unsolved problem in RAVE is to select proper &#945;. The requirement of risk aversion and ex- ploration is somehow competing: risk aversion seek to minimize the variance, while exploration searches states with higher variance. To satisfy both requirements, previous work proposed two competing agents, and each will make decision for a short amount of time(Pan et al. (2019)). Here we propose another solution to this problem. We argue that the agent needs to aggressively explore at the beginning, and it should get more risk sensitive as the model converges. A key indicator of this is the epistemic uncertainty. The epistemic uncertainty measures how well our model get to know the state space. In MBRL and Hybrid-RL, there is a common technique to easily monitor the epistemic uncertainty, by evaluating the ability of the learned environment model to predict the consequence of its own actions(Pathak et al., 2017).</p><p>Following this motivation, we set the confidence bound factor to be related to its current state and action, denoted as &#945;(s, a). We want &#945;(s, a) to be larger when the environment model could perfectly predict the state to get more risk sensitive, and smaller when the prediction is noisy to allow more exploration. We have</p><p>where Z is a scaling factor for the prediction error. With a little abuse of notations, we use &#945; here to represent a constant hyperparameter, and &#945;(s, a) is the factor that is actually used in &#945;-CLB. &#945;(s t , a t ) picks the value near zero at first, and gradually increases to &#945; with the learning process.</p></sec><sec><title>EXPERIMENTS AND ANALYSIS</title><p>We evaluate RAVE on continuous control environments using the MuJoCo physics simula- tor(Todorov et al., 2012). The baselines includes the model-free DDPG and STEVE that cur- rently yields the SOTA Hybrid-RL performance in MuJoCo. We also align our performance with the SOTA MFRL methods including twin delayed deep deterministic (TD3) policy gradient algo- rithm(Fujimoto et al., 2018), soft actror-critic(SAC) algorithm(Haarnoja et al., 2018), and proximal policy optimization(PPO)(Schulman et al., 2017), using the implementation provided by the au- thors. To further demonstrate the robustness in complex environments, we also evaluate RAVE on OpenAI's Roboschool (Klimov &amp; Schulman, 2017), where STEVE has shown a large improvement than the other baselines.We detail hyper-parameters and the implementation in the supplementary materials.</p></sec><sec><title>EXPERIMENTAL RESULTS</title><p>We carried out experiments on eight environments shown in <xref ref-type="fig" rid="fig_2">fig. 3</xref>. Among the compared methods, PPO is the only on-policy updating method, which has very poor sample-efficiency compared with either STEVE or off-policy MFRL methods, as PPO needs a large batch size to learn stably(Haarnoja et al., 2018). DDPG achieves quite high performance in HalfCheetah-v1 and Swimmer-v1, but fails on almost all the other environments, especially on challenging environments such as Humanoid. In Hopper-v1 and Walker2d-v1, STEVE can not compare with TD3 and SAC, which yields quite good performance in many environments. However, RAVE performed favorably in most environ- ments in both asymptotic performance and the rising speed(meaning sample efficiency), except for HalfCheetah-v1 and Swimmer-v1, where DDPG has already achieved satisfying performance and the margin between DDPG and Hybrid-RL is not that large.</p></sec><sec><title>ANALYSIS</title></sec><sec><title>Distribution of Value Function Approximation</title><p>To investigate whether the proposed method predicts value function more precisely, we plot the of the predicted valuesQ against the ground truth values of Hopper-v1 in <xref ref-type="fig" rid="fig_3">fig. 4</xref>. The ground truth value here is calculated by directly adding the rewards of the left trajectory, thus it is more like a monte carlo sampling from ground truth distribution, which is quite noisy. To better demonstrate the distribution of points, we draw the confidence ellipses representing the density. The points are extracted from the model at environment steps of 1M. In DDPG and STEVE, the predicted value and ground truth aligned poorly with the ground truth, while RAVE yields better alignments, though a little bit of underestimation. From fig. 5(a) we can see that &#945; = 0(which means ensemble of DVE only) already surpasses the performance of STEVE in Hopper-v1, showing that modeling aleatoric uncertainty through PE indeed benefits the performance of value expansion. Larger margin is attained by introducing &#945;- CLB. A very large &#945;(such as constant &#945; = 2.0, which means lower CLB) can quickly stabilize the performance, but its performance stayed low due to lack of exploration, while a smaller &#945;(constant &#945; = 0.0, 0.5 generates larger fluctuation in performance. The dynamic adjustment of &#945; facilitates quick rise and stable performance.</p></sec><sec><title>Analysis on Robustness</title><p>We also investigate the robustness of RAVE and baselines on the most challenging Mujoco environment, Humanoid-v1. Humanoid-v1 involve complex humanoid dynam- ics, where the agent is prone to fall. We evaluate the robustness with the possibility of falling by the learned policy. As shown in fig.5(b), RAVE achieves the lowest falling rate compared with the baselines.</p></sec><sec><title>Computational Complexity</title><p>A main concern toward RAVE may be its computational complexity. On the one hand, hybrid-RL involves the usage of environment model which introduces additional computational cost. On the other hand, RAVE and STEVE involves ensemble of trajectory rollouts, which is a little bit costly. We keep the ensemble size the same as STEVE and the details about the hyper-parameter can be found in the supplements.</p><p>For the training stages, the additional training cost of RAVE compared with STEVE comes from modeling aleatoric uncertainty and additional sampling cost. We tested the training speed of STEVE and RAVE, and the time for RAVE to finish training 500 batches with a batch size 512 is 13.20s, an increase of 24.29%, compared to STEVE(10.62s). The time reported here is tested in 2 P40 Nvidia GPUs with 8 CPUs(2.00GHz). For the inference stages, RAVE charges exactly the same computa- tional resources just as the other model-free actor critic methods as long as the model architecture of the policy is equal, which is a lot more cost efficient compared with MBRL that adopts a planning procedure.</p><p>Also we want to emphasize here that the computation complexity is typically less important com- pared with sample efficiency, as the interaction cost matters more than computational cost in training procedure.</p></sec><sec><title>CONCLUSION</title><p>In this paper, we raise the problem of incomplete modeling of uncertainty and insufficient robustness in model-based value expansion. We introduce ensemble of probabilistic models to approximate the environment, based on which we introduce the distributional value expansion(DVE), &#945;-Confidence Lower Bound, which further leads to RAVE. Our experiments demonstrate the superiority of RAVE in both sample efficiency and robustness, compared with state-of-the-art RL methods, including the model-free TD3 algorithm and the Hybrid-RL STEVE algorithm. We hope that this algorithm will facilitate the application of reinforcement learning in real-world, complex and risky scenarios.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Curves of estimated Q values in the toy environment at the starting point over the environ- ment steps.Each experiment is run four times.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>A casual illustration of three Distributional Value Expansion(DVE) trajectories.</p></caption><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Average return over environment frames in MuJoCo and Roboschool environments. Each experiment is run four times.</p></caption><graphic /><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Confidence ellipses of the distribution of estimated valuesQ versus the ground truth in Hopper-v1. The points are extracted from environment steps of 1M, each includes statistics from 10,000 points. The x-axis and y-axis represent the statistical cumulative discounted returns(ground truth) and the predictive Q-values(both are normalized), respectively. . Under review as a conference paper at ICLR 2020 Investigation on dynamic confidence bound. In order to study the role played by the &#945;-confidence lower bound separately, we further carried out series of ablative experiments in Hopper-v1 environ- ment. We compare RAVE(&#945; =constant), RAVE(dynamic &#945;(s, a)) and other baselines in chart. For all the algorithms, we set H max = 3, and the experiments are replicated for 4 times.</p></caption><graphic /><graphic /></fig><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>Examining the performance of RAVE variants and robustness of the learned policy. Each experiment is run four times. (a) Performance of RAVE with different &#945;-CLB.(b) The falling rate of various algorithms over the first 1M environment steps. Each point is evaluated on the outcome of 1000 episodes.</p></caption><graphic /><graphic /><graphic /><graphic /></fig></sec></body><back /></article>