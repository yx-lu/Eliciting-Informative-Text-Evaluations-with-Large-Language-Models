Title:
```
Published as a conference paper at ICLR 2020 PLUG AND PLAY LANGUAGE MODELS: A SIMPLE APPROACH TO CONTROLLED TEXT GENERATION
```
Abstract:
```
Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and en- tailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the gener- ation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differen- tiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.
```

Figures/Tables Captions:
```
Figure 1: Simplified illustration of the proposed approach in three phases. In Step 1, a forward pass is performed through the language model to compute the likelihood of a desired attribute using an attribute model that predicts p(a|x). In Step 2, a backward pass updates the internal latent represen- tations of the LM, using gradients from the attribute model, to increase the likelihood of the passage having the desired attribute. In Step 3, a new distribution over the vocabulary ( p t+1 ) is generated from the updated latents ( H t ) and the current token x t . The next token is then sampled from the updated distribution. This process of updating the latents is repeated at each time-step, leading to a gradual transition towards the desired attribute. For computational efficiency, one may choose to modify only the latents within some window of the recent past, depicted as the dotted-red region.
Figure 2: An oversimplified view into why steps that maximize both log p(a|x) and log p(x) are needed. The sentence under consideration is shown as a black dot, which is first pushed in the direction of maximizing log p(a|x) and then in the direction of maximizing log p(x). In practice we use a single step and simply add the log proba- bilities; we take steps in continuous space of hid- den representations H rather than in the discrete x (byte pair) space, and rather than resampling the entire sentence each step, we take one step in H space per byte-pair sample.
Table 1: The PPLM employs a pre-trained language model (LM) without any changes to the model parameters and can generate text with controlled attributes such as topic and sentiment. We demon- strate control with two tiny and easy to construct attribute models: a bag of words (BoW) related to a topic and a linear discriminator trained on top of LM latent representations to control sentiment. The underlined prefix is what the LM is conditioned on to generate a passage of text (e.g. The potato
Table 2: Comparison of the different models and distributions. All models in this table are useful in different scenarios. The particular advantage of PPLM is that very small, custom attribute models, p(a|x), may be combined with powerful, general pre-trained language models, p(x), to create cheap but still powerful conditional generative models, p(x|a).
Table 3: Comparison of different samples generated by (top row) baseline GPT-2 and (other rows) PPLM with different BoW corresponding to different topics (e.g. [Military] ), all conditioned on a single prefix: "The issue focused
Table 4: For each treatment in the ablation study, we report mean±std-dev across (human and au- tomated) fluency metrics. The topic (%) reports the fraction of samples matching the target topic, as evaluated by human annotators. Table S8 provides per-topic results. Approaches BC and BCR demonstrate significant control over the topic of the generated text, while retaining similar diversity (Dist-1, Dist-2, Dist-3) scores and minimal degradation in Perplexity and Fluency evaluations vs the baseline LM (B). The gain from ranking and choosing from multiple samples BR over B is limited (4.7%). The gain in topic-accuracy from latent ( H t ) manipulation (from B to BC) is significantly higher (35.8%). Perplexity is computed using the GPT LM (Radford et al., 2018a), which differs from the LM generating text (GPT-2). For CTRL and WD, since human evaluation is performed in comparison with BCR via A/B testing, we report the numbers for BCR as well from these com- parisons, for the human evaluated metrics. Further, we consider one sample per prefix for CTRL, resulting in fewer samples and higher Dist-1, 2, 3 scores as a consequence. PPLM outperforms CTRL and WD on topic-relevance, while being comparable on fluency scores.
Table 5: Sentence samples in triplets, generated by {baseline GPT-2, PPLM-Discrim POSITIVE, PPLM-Discrim NEGATIVE}, conditioned on prefixes: The chicken
Table 6: Evaluation of models/ variants on the sentiment control task, with mean±std-dev reported across fluency metrics. Sentiment accuracy reports the fraction of samples with an accurate tar- get sentiment. Approach BCR provides significant control over sentiment while showing minimal degradation in fluency. See Table S9 for full results on individual sentiments. *GPT2-FT-RL is only evaluated for the positivity half of the task, as it is fine-tuned only for positivity (Ziegler et al., 2019). For human evaluation metrics, we compare the baselines CTRL, GPT2-FT-RL and WD with BCR and perform A/B style testing. We include both numbers for comparison.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The Transformer architecture (Vaswani et al., 2017) has enabled large-scale language models (LMs) trained on a huge amount of data (Radford et al., 2019; Dai et al., 2019b; Radford et al., 2018b) to greatly improve the state-of-the-art on natural language processing tasks. These models are used to extract contextualized word embeddings for transfer learning purposes (Devlin et al., 2019) and as natural language generators. The latter can leverage large amounts of unannotated data and a simple log-likelihood training objective. However, once such models are trained, controlling attributes of generated text becomes difficult without modifying the model architecture to allow for extra input attributes or fine-tuning with attribute-specific data (Keskar et al., 2019; Ziegler et al., 2019). • Summary of contributions: SD, RL & JY conceptualized PPLMs and led the manuscript writing. SD led the project, implemented the PPLM, set up and ran all modeling experiments, engineered how to obtain workable gradients via the weighted embedding approach, and made the model work. AM helped with preparing datasets for discriminator training, automated evaluation, running experiments, and writing the manuscript. SD, RL & AM ran the external baselines. RL & JL built and oversaw the human evaluation pipeline and computed the statistics. JH ran the story generation with skeleton prefixes. EF assisted with detoxification experiments. PM led efforts to migrate to the new pytorch transformer, helped with code release. JY helped with the annotation pipeline, finding bugs, navigating model and experimental directions, engineering workable gradients, and posing the model mathematically. RL implemented preliminary experiments and multi-attribute control, and cleaned and coordinated release of the code. RL & JY oversaw the project. To conclude, the most significant and lasting damage from the economic crisis in 2008 was that many governments, including those in the political center, lost power for the first time in modern history. Controllable generation entails modeling p(x|a), where a is some desired controllable attribute(s) and x the generated sample. However, generative models only learn p(x). In computer vision, Plug & Play Generative Networks (PPGN) from Nguyen et al. (2017) developed a mechanism for generating images with different attributes by plugging a discriminator (attribute model) p(a|x) together with a base generative model p(x) and sampling from the resulting p(x|a) ∝ p(a|x)p(x), effectively creating a conditional generative model on the fly from any supplied attribute model. In a similar manner, we propose the Plug and Play Language Model (PPLM) for conditional language generation that combines one or more simple attribute models p(a|x)-either in the form of a bag- of-words (BoW) or single layer classifiers-with a pre-trained, unconditional language model p(x). We sample from the resulting combined model by following gradients in the latent representation space in a manner inspired by the approximate Metropolis-adjusted Langevin (MALA) (Roberts et al., 1996; Roberts & Rosenthal, 1998) sampler deployed in Nguyen et al. (2017). Optimization is performed ex post facto in the activation space, therefore no re-training or fine- tuning is needed. Control is fine-grained, with a strength parameter determining how strong the attribute influence should be; a strength of 0 fully recovers the original model p(x). This design allows vast flexibility: users can combine a state-of-the-art generative model, which may be large and difficult to train, with any number of attribute controllers. Attribute models may be easier to train or untrained (in the case of BoW models), and multiple controllers may be combined flexibly during inference. In this paper, we demonstrate the PPLM approach using a GPT-2 345M model (Radford et al., 2019) as the general-purpose LM p(x), but the method applies in any representation space from any transformer-based text generator and allows combination with any attribute model p(a|x). We demonstrate controlled generation with a number of attribute controllers, assembled and com- bined during generation, each with a different strength, acting as a set of "control knobs" that tune generation towards the desired attribute (see examples in  Table 1 ). Code for the experiments is available at: https://github.com/uber-research/PPLM. Our key contributions are: • We introduce the Plug and Play LM for controlled language generation, discuss its relation to existing work, and how sampling from a PPLM works (Sections 2 and 3). • We demonstrate controlling of text generation on a range of attributes, including 7 topics each defined using a bag of words, and 1 simple discriminator on sentiments. We quantify effectiveness using both automated evaluation (separately trained perplexity and sentiment Published as a conference paper at ICLR 2020 models) as well as human evaluation (for attribute relevance and fluency). All evaluations point toward the ability of PPLMs to generate attribute controlled, fluent text (Section 4). • We compare PPLM with CTRL (Keskar et al., 2019) and GPT-2 finetuned for positivty (Ziegler et al., 2019). Our method, without any LM training, is on par and often outper- forms the baselines on attribute relevance and fluency (Section 4.2, and Section 4.3). • We show that the PPLM approach can be used to detoxify instances where generation of toxic content is likely by following the negative gradient of a model trained to detect toxicity (Section 4.4). We also show how PPLM can be used for structurally constrained story writing (Section 4.5).

Section Title: RELATED WORK
  RELATED WORK Controlled generation Current methods for controlled text generation involve either fine-tuning existing models with Reinforcement Learning (RL) (Ziegler et al., 2019), training Generative Ad- versarial Networks (Yu et al., 2017), or training conditional generative models (Kikuchi et al., 2016; Ficler & Goldberg, 2017). Different from our approach, these methodologies are not plug and play, since the entire model needs to be separately fine-tuned for each specific attribute. Keskar et al. (2019) train a large language model with over 50 different control codes. The results are high quality because they train exactly to maximize p(x|a), but this comes at the expense of fixing control codes upfront and of training a very large model (1.6B parameters). Our method does not require retraining any conditional generative model, and both the language model and the conditional model can be flexibly assembled.  Table 2  gives a comparison of recent approaches to language modeling tuned for specific attributes. In another interesting but tangential piece of work, Subramani et al. (2019) recently showed that a pre-trained language model can be steered to recover arbitrary sen- tences. In earlier works Gu et al. (2016; 2017); Chen et al. (2018) explored the idea of using a small neural network to steer an LM. Noisy Channel Modeling Yu et al. (2016), and more recently Yu et al. (2019); Yee et al. (2019); Ng et al. (2019), leveraged the Shannon Noisy Channel Theory (Shannon, 1948) for improving sequence-to-sequence modeling. Their approach translates a source language sentence y into a target language sentence x by first sampling from a forward model proposal distribution p forward (x|y) and then reranking samples based on probabilities given by p backward (x|y) ∝ p(x)p(y|x). PPLM scores samples using the same basic equation, but as we have no forward or proposal model p forward (x|a), we rely on the latent space updates, similar to Nguyen et al. (2017). As a baseline, we consider using p(x) as a "forward model" and then reranking, which we will see works moderately well in some scenarios and poorly in others (see  Tables 4  and 6). Weighted decoding Holtzman et al. (2018); Ghazvininejad et al. (2017) consider controlled lan- guage generation - the former with discriminators, and the latter with a bag of words - where the decoding procedure is modified to consider the scoring function used for decoding. See et al. (2019) note that control with weighted decoding (WD) is difficult and often leads to sacrificing fluency and coherence. Further, Ghazvininejad et al. (2017) strongly relies on sampling from a set of keywords on a specific topic and it does not allow to bias generation towards a topic in a manner that does not necessary include a set of keywords. Similarly, Baheti et al. (2018) proposed a decoding strategy for generating interesting responses in dialogue systems, using bags of words and word embed- dings. Sophisticated sampling methods (Metropolis et al., 1953) can be used to constrain the model generation to certain keywords and topics. We evaluate WD as a baseline. Text Style Transfer Outside of language modeling, the text style transfer studies a related task. Shen et al. (2017); Hu et al. (2017) train variational auto-encoders for style transfer that rely on learning disentangled latent representations for style and content. Li et al. (2018) demonstrate the efficacy of a simple approach based on replacing attribute related n-grams with n-grams correspond- ing to the desired attribute based on a conditional generative model. A key difference between the above and our approach is that we use an offline discriminator and perform optimization based on this discriminator, which as suggested by Elazar & Goldberg (2018) may outperform adversarial training approaches. More recently, Lample et al. (2019) adapt an approach from unsupervised language translation to style transfer, where a denoised auto-encoder is trained with an objective Published as a conference paper at ICLR 2020

Section Title: LANGUAGE MODELING WITH TRANSFORMERS
  LANGUAGE MODELING WITH TRANSFORMERS Given a sequence of tokens X = {x 0 , · · · , x n }, LMs are trained to compute the unconditional prob- ability of the sequence p(X). This probability can be rewritten in terms of product of conditional probabilities by recursively applying the chain-rule (Manning et al., 1999; Bengio et al., 2003) as: In this paper, we use a transformer (Vaswani et al., 2017) to model the distribution of natural lan- guage. To present our approach clearly, we first briefly summarize the transformer using recur- rent notation. Let us define the history matrix H t to consist of the key-value pairs from the past i.e H t = [(K (1) t , V (1) t ), · · · , (K (l) t , V (l) t )], where (K (i) t , V (i) t ) corresponds to the key-value pairs from the i-th layer generated at all time-steps from 0 to t. Efficient implementations of the trans- former (Wolf et al., 2019) use the cached H t to generate x t+1 , given x t . This recurrent interpretation of a transformer can be summarized as: o t+1 , H t+1 = LM(x t , H t ), (2) and then x t+1 is sampled as x t+1 ∼ p t+1 = Softmax(W o t+1 ), where W is a linear transformation that maps the logit vector o t+1 to a vector of vocabulary size. This allows for efficient language gen- eration without repeated forward passes corresponding to the prior conditioning text x 0 , . . . , x t−1 .

Section Title: STEERING GENERATION: ASCENDING log p(a|x)
  STEERING GENERATION: ASCENDING log p(a|x) In order to control the output of the language model, at every generation step t, we shift the history H t in the direction of the sum of two gradients: one toward higher log-likelihood (LL) of the attribute a under the conditional attribute model p(a|x) and one toward higher LL of the unmodified language model p(x). Combining these factors with a variable multiplier provides us with a controllable "knob" to guide generation in a given direction with a specified strength. The updates are restricted to H t and not the other model activations because future predictions depend on the past only via H t (note that H t is composed of all transformer key and value pairs generated up to time t). Taking steps in H t space leads to gradual changes to model activations - which may be thought of as gradual reinterpretations of the past - that guide future generation in the desired direction. at zero and updated with gradients from an attribute model that measures the extent to which the generated text possesses the desired attribute (e.g. positivity). We rewrite the attribute model p(a|x) as p(a|H t + ∆H t ) and then make gradient based updates to ∆H t as follows: ∆H t ← ∆H t + α ∇ ∆Ht log p(a|H t + ∆H t ) ∇ ∆Ht log p(a|H t + ∆H t ) γ (3) where α is the step size, γ is the scaling coefficient for the normalization term. 1 This update step can be repeated m times; in practice we use 3 to 10. Subsequently, a forward pass through the LM with the updated key-value pairs is performed to obtain the updated logits o t+1 as o t+1 , H t+1 = LM(x t , H t ), where H t = H t + ∆H t . The perturbed o t+1 is then used to generate a new distribution p t+1 as in Equation 2.

Section Title: ENSURING FLUENCY: ASCENDING log p(x)
  ENSURING FLUENCY: ASCENDING log p(x) The approach described in the previous section is able to generate text tuned for a particular dis- criminator, but left unchecked it will quickly result in unrealistic adversarial or fooling examples (Szegedy et al., 2013; Nguyen et al., 2015) as the text moves into low probability regions. To com- bat this, we use the unconditional language model in two ways that ensure the fluency is maintained at or near the level of the unconditional language model (here GPT-2).

Section Title: Kullback-Leibler (KL) Divergence
  Kullback-Leibler (KL) Divergence We update ∆H t to minimize the KL divergence between the output distribution of the modified and unmodified language models in addition to the step above. In practice, this is accomplished by adding the quantities together before taking a gradient, though it can be visualized as two separate steps as in  Figure 2 . We scale the KL coefficient by a scalar λ KL , and in practice, setting this hyperparameter to 0.01 works well in general across tasks.

Section Title: Post-norm Geometric Mean Fusion
  Post-norm Geometric Mean Fusion In addition to minimizing KL divergence, which affects the past via ∆H t , we perform post-norm fusion similarly to Stahlberg et al. (2018). This does not directly affect ∆H t ; rather, it just serves to constantly tie the generated text to the unconditional p(x) LM distribution. We accomplish this by sampling from x t+1 ∼ 1 β p γgm t+1 p 1−γgm t+1 , where p t+1 and p t+1 are the unmodified and modified output distributions, respectively, and β is a normalizing factor such that it forms a valid distribution. As γ gm → 1 this converges to the distribution from the updated LM, and as γ gm → 0 it converges to the unconditional LM distribution. We find that in practice values for γ gm in the range 0.8 − 0.95 work well.

Section Title: SAMPLING AND RANKING
  SAMPLING AND RANKING The attribute model p(a|x) in PPLM provides two functionalities: first, a score that can be used to rank samples based on the LL of the desired attribute (forward pass only; Step 1,  Figure 1 ), and second, a gradient ascent direction to perform an update in the latent space (Step 2 & 3;  Figure 1 ). The former can be used to generate r samples and rank them to choose the best one. This can serve as an additional method for attribute control in addition to sampling with updated latents. Further, to avoid the problem of repetitive, low quality text (Holtzman et al., 2018), we compute the mean over the Dist-1, Dist-2 and Dist-3 scores (for the generated passage), which is an indicator of repetitiveness (Li et al., 2015), and then discard samples with a mean score below a threshold τ .

Section Title: EXPERIMENTS, RESULTS, AND EVALUATION
  EXPERIMENTS, RESULTS, AND EVALUATION In this section, we describe our evaluation methodology and then show controlled generation results under various attribute models. We also show use cases of PPLM in language detoxification and in controlled story telling. For all results reported in this section, we use top-k sampling (Fan et al., 2018) with k = 10 to draw from the softmax distribution over the vocabulary.

Section Title: EVALUATION METHODS AND ABLATION STUDY
  EVALUATION METHODS AND ABLATION STUDY We evaluate to assess two properties: whether PPLM generates text that satisfies the desired attribute (topic or sentiment) and whether the quality of its text deteriorates as we intensify control of the attribute. Note we can always turn the control knob down to zero to disable control of attributes and reach the fluency of the original model. If desired, a user can tune the knobs at inference until a chosen tradeoff between attribute strength and fluency is reached. We evaluate using both automated methods and human annotators:

Section Title: Automated Eval
  Automated Eval Perplexity is an automated measure of fluency, though its effectiveness has been questioned in open-domain text generation (Liu et al., 2016). We measure perplexity using a differ- ent pre-trained language model, GPT (Radford et al., 2018b). The diversity of text in the passages is measured using the number of distinct n-grams (normalized by the length of text) as in Li et al. (2015). We report Dist-1, Dist-2, and Dist-3 scores for the distinct 1-2-3-grams (measured across all samples generated for a given attribute control task, e.g. a specific topic for topic control). Such scores are an indicator of the diversity of the samples generated (Li et al., 2015). We also use external sentiment classifiers for sentiment evaluation.

Section Title: Human Eval
  Human Eval We consider two types of human annotation: fluency and A/B testing on attribute relevance. Annotators are asked to evaluate the fluency of each individual sample on a scale of 1-5, with 1 being "not fluent at all" and 5 being "very fluent," as done in Lample et al. (2019). In the A/B testing for attribute relevance, we consider all combinatorial pairs of all four variants: B, BR, BC, and BCR (6 combinations). We then ask annotators to rank the pair on the desired attribute (e.g. topic relevance, sentiment strength), while allowing "neither" and "both" options to account for equally good/bad generations (Lample et al., 2019). We obtain annotations from nine external occupational annotators. Each pair of samples is evaluated by three individuals and we use majority-voting to Published as a conference paper at ICLR 2020 compute attribute relevance. For fluency, we use average of the three annotations. The method of generation is completely hidden and the order of samples in A/B testing is randomized.

Section Title: Ablation study and baselines
  Ablation study and baselines

Section Title: BOW ATTRIBUTE MODELS
  BOW ATTRIBUTE MODELS The simplest attribute model we use gives the log of the sum of likelihoods of each word in some predefined Bag of Words (BoW). Given a set of keywords {w 1 , · · · , w k } that specify a topic of interest and the output distribution of the language model p t+1 , the log likelihood is: We construct BoWs that represent seven distinct topics: SCIENCE, MILITARY, LEGAL, COMPUT- ERS, SPACE, POLITICS, and RELIGION (see Section S17 for complete word lists). Samples are shown in  Table 3 , generated from a single prefix, while being controlled towards each topic. Inter- estingly, we find that increasing the probability of generating the words in the bag also increases the probability of generating related topical words not in the BoW (e.g. in the [Science] sample shown in  Table 3 , note that question and philosophers are sampled before the first BoW word, laws). Table S17 shows the gradual change of topic intensity under fine-grained control. We found that the optimization procedure works better with updating representations from the past over a finite window and using an adaptive normalization scheme (see Section S11.3). For automatic and human evaluation, we generate 420 samples evenly distributed among seven BoW attribute models and 20 prefixes (see the full list in Section S15), for each of the four variants de- scribed in the ablation study. See Section S8 for further details on evaluation and results.  Table 4  shows that human annotators find text from BCR (51.7%) and BC (46.9%) to be significantly more Published as a conference paper at ICLR 2020 on topic than B (15.8%) and BR (11.1%). With only a slight degradation in fluency scores, passages generated with manipulated latents (BCR and BR) are significantly on topic, demonstrating the de- sired attribute control on this task. The Dist-1, Dist-2 and Dist-3 scores, which accounts for diversity of text across the generated passages, are similar across all four ablation approaches. Further, BCR slightly outperforms CTRL (51.7% & 50.0%), and significantly outperforms WD (36 %). BC itself outperforms WD (36 %). BCR, CTRL and WD all score similarly on the fluency metric. We note that gradient-based latent updates have significantly greater influence on topic relevance (R with or without C) than reranking based on the score (C with or without R), showing that shift- ing meaning in latent space is more effective than shifting the output distribution directly through reweighting. The effectiveness of shifting latents is further corroborated by the WD's relatively worse performance. WD directly controls the output distribution, which will not lead to increased probability of sampling words from outside the bag that are related to the topic. Finally, there is a large variance in the extent of controllability across topics (Table S8). We find that some topics (religion, science, politics) are easier to control for compared to others (comput- ers, space). Section S9 considers unusual or nonsensical combinations of prefixes and attributes (e.g. prefix 'potato' and topic 'religion'), and we find that even for these settings PPLM is able to successfully control for the desired attribute, often with hilarious twists!

Section Title: DISCRIMINATOR ATTRIBUTE MODELS
  DISCRIMINATOR ATTRIBUTE MODELS While BoW models have been demonstrated to be able to control text attributes such as sentiment (e.g., Li et al. (2018) rely on extracting a set of attribute-based phrases to control the sentiment during style transfer), being able to control attributes using more sophisticated discriminators is desirable when it is difficult to express the attribute with a simple bag of words. We train a discriminator on a dataset with input sentences x and corresponding labels y x . For an input x of length t, we compute o x :t and train f on the mean (ō t ) of the embeddings across time. All discriminators in this work consist of a single layer classifier that predicts the target label fromō x t . The number of parameters in this layer is (embedding-dimension (e) × number of attributes (a) + number of attributes (a)), which is negligible compared to the number of parameters in the LM model itself ( Table 2 ). Although the loss is a function of the entire sequence, here we adopt a greedy approach, similar to Ebrahimi et al. (2018); Wallace et al. (2019), in which we optimize for Published as a conference paper at ICLR 2020 a higher-probability of the sequence having a specific attribute by considering changes only to the next token to be generated. This objective can be described as follows, where f is the discriminator: Note that o t+2 is a function of x t+1 . Further, x t+1 ∼ Softmax(Wõ t+1 ), which depends on ∆H t . In the limit, minimizing the objective in Equation 5 corresponds to choosing x t+1 that produces the optimal o t+2 that maximizes f (o :t+1 , o t+2 ). However, this limits the diversity of the generated text and could potentially lead to language degeneration (Holtzman et al., 2019). Alternatively, we focus on a softer optimization approach where we aim to shift the distributionp t+1 = Softmax(Wõ t+1 ) towards one that in expectation has a higher likelihood of having the desired attribute a. Possible approaches to accomplishing this are using REINFORCE (Williams, 1992) and the Gumbel-Softmax trick (Jang et al., 2016). However, both of these would slow down convergence. Instead, as in Dai et al. (2019a), we use the distributionp t+1 (instead of a hard sample x t+1 ), and feed it forward to obtain (a biased) estimate of the next token's embedding and then update ∆H t . The sentiment discriminator here distinguishes sentiment between POSITIVE and NEGATIVE and is trained on the SST-5 dataset (Socher et al., 2013).  Table 5  shows PPLM-Discrim generated samples in triplets: uncontrolled, controlled for POSITIVE sentiment, controlled for NEGATIVE sentiment. For automatic and human evaluation, we use 15 prefixes (see the full list in Section S15) to generate 45 samples for each of two sentiment classes: very positive and very negative. Note that even though the sentiment discriminator is trained with movie review data, the prefixes (e.g. "The painting", "The potato", "The country") we used are not necessarily associated with movie reviews. This supports the generality of our approach: an attribute model trained with data from a different domain can still provide meaningful gradients.  Table 6  shows evaluation results. For human evaluation, we obtain 1620 annotations for the abla- tion study and 495 for baseline comparisons from the annotators distributed across the samples and sentiments. Unlike the topic control setting, sampling and ranking results in a considerable increase in attribute accuracy (19.3% → 41.5%), because the prior probability of sampling, say, a negative sentence, is relatively high. BC results in a decrease in fluency when compared to B, while being significantly more consistent with the desired attribute (19.3% → 39.6%). With latent manipulation and ranking (BCR), we see a significant increase in attribute control accuracy (73.7%) while retain- ing fluency similar to B and BR. Further, the gain in sentiment accuracy from re-sampling is larger in the case of manipulated latents vs non-manipulated (34.1% increase from BC to BCR > 22.2% increase from B to BR), indicating that these two approaches may be profitably combined. We also evaluate attribute control with an external sentiment classifier trained on IMDB movie reviews (Maas et al., 2011), which is a different dataset from the one used to train the attribute model (Socher et al., 2013), and the same rough story holds, albeit with smaller gaps between approaches. We compare to baselines CTRL, GPT2-FT-RL, and WD. BCR performs comparably to CTRL (73.7% and 80.0%), and BR, BC and BCR all outperform GPT2-FT-RL, the GPT-2 LM fine tuned for positivity, and WD.

Section Title: LANGUAGE DETOXIFICATION
  LANGUAGE DETOXIFICATION Language models trained with large corpora of Internet data reflect biases and discrimination ex- isting in the data. A recent paper by Wallace et al. (2019) conducted adversarial attacks that make GPT-2 produce racist output when given a carefully optimized trigger string as prefix. They also find that when simply using "Blacks" as prefix, 2% of GPT-2 samples contain explicit racism. Other prefixes (e.g., "Asians" or "Jews") are mentioned but no percentage is reported. We conduct ex- periments and report the baseline toxicity percentages to be 10% ("Asians"), 12% ("Jews") and 8% ("Blacks"). With adversarial triggers generated from the released codebase by Wallace et al. (2019) the average toxicity percentage is 63.6%. Further details can be found in Section S13. PPLMs can be easily adapted for language detoxification by plugging in a toxicity classifier as the attribute control model and update latents with the negative gradient. We train a single layer classifier on the toxicity data from the Toxic Comment Classification Challenge (Jigsaw) and show that with a similar hyper-parameter setting as other PPLM-Discrim methods, it works well on both natural prompts and adversarial triggers. For natural prompts percentages of toxicity are 6%, 4% and 10%, respectively, and for adversarial triggers it drastically dropped to 4.6% on average, with statistical significance. Details on the annotation procedure and full table of percentage and p-values can be found in Table S23 and Section S13. Note that a model for detoxifying language can also potentially be maliciously used for generating toxic language, a topic we briefly discuss in Section S6.

Section Title: CONTROLLED STORY WRITING
  CONTROLLED STORY WRITING We explore controlled generation for assistive story writing (Peng et al., 2018; Luo et al., 2019; Yao et al., 2019; Fan et al., 2018). Using uncontrolled LMs for assistive art creation can be difficult. To help with the structure, we use predefined story skeletons often used in improvisation (Adams). We fill in the blank between these prefixes with a PPLM. See examples in Table S20 and Table S21.

Section Title: CONCLUSION
  CONCLUSION We have presented PPLM, a plug and play method for controlled language generation that flexibly combines a large, pre-trained LM and a BoW or a small, easy-to-train discriminator. In Section S6 we discuss the ethics of controlled LMs. PPLM achieves fine-grained control of attributes via a simple gradient-based sampling mechanism. Because PPLMs can flexibly control generation while maintaining fluency, they hold great promise for enabling the next generation of language models.

```
