Title:
```
None
```
Abstract:
```
In this paper, we propose a novel kind of kernel, random forest kernel, to en- hance the empirical performance of MMD GAN. Different from common forests with deterministic routings, a probabilistic routing variant is used in our innovated random-forest kernel, which is possible to merge with the CNN frameworks. Our proposed random-forest kernel has the following advantages: From the perspec- tive of random forest, the output of GAN discriminator can be viewed as feature inputs to the forest, where each tree gets access to merely a fraction of the features, and thus the entire forest benefits from ensemble learning. In the aspect of kernel method, random-forest kernel is proved to be characteristic, and therefore suitable for the MMD structure. Besides, being an asymmetric kernel, our random-forest kernel is much more flexible, in terms of capturing the differences between distri- butions. Sharing the advantages of CNN, kernel method, and ensemble learning, our random-forest kernel based MMD GAN obtains desirable empirical perfor- mances on CIFAR-10, CelebA and LSUN bedroom data sets. Furthermore, for the sake of completeness, we also put forward comprehensive theoretical analysis to support our experimental results.
```

Figures/Tables Captions:
```
Figure 1: Example of a random forest with T trees: blue nodes denote the internal nodes N := {d1, ..., d7} while purple nodes are the leaf nodes L := { 1, · · · , 8}. The black thick path illustrates the route of a sample x falling into the 6 leaf node of the t-th tree.
Figure 2: Example of a connection between CNN network d -dimensional output h(·; θN ) and random forest with T trees and depth 2. The internal node d 1 3 in the left tree has the probability
Figure 3: Visualization of three kernels by drawing filled contours: (a) & (b) - A direct visualization of 2-dimensional kernels with reference to (0, 0); (c) - A 2-dimensional visualization of the multi- dimensional random-forest kernel with the help of t-SNE (Maaten & Hinton, 2008). The details of visualization are shown in Appendix A.2.
Table 1: Score evaluation results for three datasets: Inception Score (IS), Fréchet Inception Distance (FID) and Kernel Inception Distance (KID). The best score within two setups are marked in bold
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Generative adversarial nets (GANs;  Goodfellow et al., 2014 ) are well-known generative models, which largely attribute to the sophisticated design of a generator and a discriminator which are trained jointly in an adversarial fashion. Nowadays GANs are intensely used in a variety of practical tasks, such as image-to-image translation ( Tang et al., 2019 ;  Mo et al., 2019 ); 3D reconstruction ( Gecer et al., 2019 ); video prediction ( Kwon & Park, 2019 ); text-to-image generation ( Zhu et al., 2019 ); just to name a few. However, it's well-known that the training of GANs is a little tricky, see e.g. ( Salimans et al., 2016 ). One reason of instability of GAN training lies in the distance used in discriminator to measure the divergence between the generated distribution and the target distribution. For instance, concerning with the Jensen-Shannon divergence based GANs proposed in  Goodfellow et al. (2014) ,  Arjovsky & Bottou (2017)  points out that if the generated distribution and the target distribution are supported on manifolds where the measure of intersection is zero, Jensen-Shannon divergence will be constant and the KL divergences be infinite. Consequently, the generator fails to obtain enough useful gradient to update, which undermines GAN training. Moreover, two non-overlapping distributions may be judged to be quite different by the Jensen-Shannon divergence, even if they are nearby with high probability. As a result, to better measure the difference between two distributions, Integral Probability Metrics (IPM) based GANs have been proposed. For instance,  Arjovsky et al. (2017)  utilizes Wasserstein distance in GAN discriminator, while  Li et al. (2017)  adopts maximum mean discrepancy (MMD), managing to project and discriminate data in reproducing kernel Hilbert space (RKHS). To mention, the RKHS with characteristic kernels including Gaussian RBF kernel ( Li et al., 2017 ) and rational quadratic kernel ( Bińkowski et al., 2018 ) has strong power in the discrimination of two distributions, see e.g. ( Sriperumbudur et al., 2010 ). In this paper, inspired by non-linear discriminating power of decision forests, we propose a new type of kernel named random-forest kernel to improve the performance of MMD GAN discriminator. In order to fit with back-propagation training procedure, we borrow the decision forest model with Under review as a conference paper at ICLR 2020 stochastic and differentiable decision trees from  Kontschieder et al. (2015)  in our random-forest kernel. To be specific, each dimension of the GAN discriminator outputs is randomly connected to one internal node of a soft decision forest, serving as the candidate to-be-split dimension. Then, the tree is split with a soft decision function through a probabilistic routing. Other than the typical decision forest used in classification tasks where the value of each leaf node is a label, the leaf value of our random forest is the probability of a sample x i falling into a certain leaf node of the forest. If the output of the discriminator is denoted as h θ N (x i ) and the probability output of the t-th tree is denoted as µ t (h θ N (x i ); θ F ), the random forest kernel k RF can be formulated as where T is the total number of trees in the forest, θ N and θ F denote the parameters of the GAN discriminator and the random forest respectively. Recall that random forest and deep neural networks are first combined in  Kontschieder et al. (2015) , where differentiable decision tree model and deep convolutional networks are trained together in an end-to-end manner to solve classification tasks. Then,  Shen et al. (2017)  extends the idea to label distribution learning, and  Shen et al. (2018)  makes further extensions in regression regime. Moreover,  Zuo & Drummond (2017)  ,  Zuo et al. (2018)  and  Avraham et al. (2019)  also introduce deep decision forests. Apart from the typical ensemble method that averages the results across trees, they aggregate the results by multiplication. As for the combination of random forest and GAN,  Zuo et al. (2018)  introduce forests structure in GAN discriminator, combining CNN network and forest as a composited classifier, while  Avraham et al. (2019)  uses forest structure as one of non-linear mapping functions in regularization part. On the other hand, in the aspect of relationship between random forest and kernel method,  Breiman (2000)  initiates the literature concerning the link. He shows the fact that a purely random tree partition is equivalent to a kernel acting on the true margin, of which form can be viewed as the probability of two samples falling into the same terminal node.  Shen & Vogelstein (2018)  proves that random forest kernel is characteristic. Some more theoretical analysis can be found in  Davies & Ghahramani (2014) ,  Arlot & Genuer (2014) ,  Scornet (2016) . However, despite their theoretical breakthroughs, forest decision functions used in these forest kernels are non-differentiable hard margins rather than differentiable soft ones, and thus cannot be directly used in back propagation regime. To the best of our knowledge, MMD GAN with our proposed random-forest kernel is the first to combine random forest with deep neural network in the form of kernel MMD GAN. Through the- oretical analysis and numerical experiments, we evaluate the effectiveness of MMD GAN with our random-forest kernel. From the theoretical point of view, our random-forest kernel enjoys the prop- erty of being characteristic, and the gradient estimators used in the training process of random-forest kernel GAN are unbiased. In numerical experiments, we evaluate our random-forest kernel under the setting of both the original MMD GAN ( Li et al., 2017 ) and the one with repulsive loss ( Wang et al., 2019 ). Besides, we also compare our random-forest kernel with Gaussian RBF kernel ( Li et al., 2017 ), rational quadratic kernel ( Bińkowski et al., 2018 ), and bounded RBF kernel ( Wang et al., 2019 ). As a result, MMD GAN with our random-forest kernel outperforms its counterparts with respect to both accuracy and training stability. This paper is organized as follows. First of all, we introduce some preliminaries of MMD GAN in Section 2. Then we review the concept of deep random forest and show how it is embedded within a CNN in 3.1. After that, random-forest kernels and MMD GAN with random-forest kernels are proposed in 3.2 and 3.3 respectively. Besides, the training techniques of MMD GAN with random-forest kernel are demonstrated in Section 3.4 and the theoretical results are shown in Section 3.5. Eventually, Section 4 presents the experimental setups and results, including the comparison between our proposed random-forest kernel and other kernels. In addition, all detailed theoretical proofs are included in the Appendices.

Section Title: MMD GAN
  MMD GAN GANs are recently introduced as a novel way of training a generative model. To learn a distribution P X , we build an adversarial model composed of two parts, the generator G and the discriminator D.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 The generative model captures the data distribution P X , by building a mapping function G : Z → X from a prior noise distribution P Z to data space. While the discriminative model D : X → R is used to distinguish generated distribution P Y from real data distribution P X . Taking X, X ∼ P X and Y, Y ∼ P Y := P G (Z) where Y := G(Z) and Y := G(Z ), the squared MMD is expressed as The loss of generator and discriminator in MMD GAN proposed in  Li et al. (2017)  is: min G L G = MMD 2 [P X , P Y ], min D L D = −MMD 2 [P X , P Y ].  Wang et al. (2019)  proposed MMD GAN with repulsive loss, where the objective functions for G and D are: Given i.i.d. samples D X = {x 1 , . . . , x n } ∼ P X and D Y = {G(z 1 ), . . . , G(z m )} ∼ P Y , we can write an unbiased estimator of the squared MMD in terms of k as When k is a characteristic kernel, we have MMD 2 [P X , P Y ] ≥ 0 with equality applies if and only if P X = P Y . The best-known characteristic kernels are gaussian RBF kernel and rational quadratic kernel ( Bińkowski et al., 2018 ).

Section Title: RANDOM-FOREST KERNEL
  RANDOM-FOREST KERNEL In this section, we review a stochastic and differentiable variant of random forest and how it is embedded within a deep convolutional neural network proposed in  Kontschieder et al. (2015) . Then we propose random-forest kernel and we apply it in MMD GAN. We illustrate the advantages of our random-forest kernel, show the training technique of MMD GAN with random-forest kernel, and study its theoretical properties.

Section Title: DEEP RANDOM FOREST
  DEEP RANDOM FOREST Suppose that a random forest consists of T ∈ N random trees. For the t-th tree in the forest, t ∈ {1, . . . , T }, we denote N t := {d t j } |Nt| j=1 as the set of its internal nodes and if T trees have the same structure, then we have |N t | = |N |, see  Figure 1 . Furthermore, we denote L t as the set of its leaf nodes and θ t F as the parameters of the t-th tree. Here we introduce the routing function µ t (x; θ t F ) which indicates the probability of the sample x falling into the -th leaf node of the t-th tree. In order to provide an explicit form for the routing function µ t (x; θ t F ), e.g. the thick black line in  Figure 1 , we introduce the following binary relations that depend on the tree structure: d t j is true, if belongs to the left subtree of node d t j , and d t j is true, if belongs to the right subtree of node d t j . Moreover, let p t j (x; θ t F ) be the decision function of the j-th internal node in the t-th tree, that is the probability of the sample x falling into the left child of node d t j in the t-th tree. Then, µ t can be expressed by these relations as where R t denotes the unique path from node 1 to node of the t-th tree. Now, let us derive the explicit form of the decision function p t j (x; θ t F ). Here, to utilize the power of deep learning, we consider using the convolutional neural network to construct decision functions of random forest. To be specific, given the parameter θ N trained from a CNN network, we denote h(·; θ N ) as the d -dimension output of a convolutional neural network, which is the unit of the last fully- connected layer in the CNN, and h i (·; θ N ) is the i-th element of the CNN output. We denote C : {1, . . . , T |N |} → {1, . . . , d } as the connection function, which represents the connec- tion between the internal node d t j and the former CNN output h i . Note that during the whole training process, the form of the connection function C is not changed and every internal node d t j is randomly assigned to an element h C(T (t−1)+j) (·; θ). If we choose the sigmoid function σ(x) = (1+e −x ) −1 as the decision function, and let the parameters of the t-th tree be θ t F := (w t , b t ) with w t = (w t 1 , . . . , w t |N | ) and b t = (b t 1 , . . . , b t |N | ), then the decision functions p t j delivering a stochastic routing can be defined as For example, we have the probability p 1 1 (x; θ N , θ 1 F ) = σ(w 1 1 h C(1) (x; θ N ) + b 1 1 ) for the node in the first layer, see  Figure 2  for an explicit example of the random forest. where Lf denotes the set of all left son nodes of its father node.

Section Title: RANDOM-FOREST KERNEL
  RANDOM-FOREST KERNEL Here, we propose the random-forest kernel as follows: Definition 1 (Random-Forest Kernel) Let x, y be a pair of kernel input, let θ t F = (w t , b t ) denotes the weights and bias of the t-th tree of the random forest, and θ F := (θ t F ) T t=1 . The random-forest kernel can be defined as where L t denotes the set of leaf nodes in the t-th tree, µ t = (µ t ) ∈Lt and µ (T ) = (µ t ) T t=1 .

Section Title: MMD GAN WITH RANDOM-FOREST KERNELS
  MMD GAN WITH RANDOM-FOREST KERNELS We write and introduce the objective functions of MMD GAN with random-forest kernel by where y = G ψ (z), z is noise vector, and R is the regularizer of random-forest kernel (the detail is shown in Section 3.4). In addition, the objective functions of MMD GAN with repulsive loss are Random-forest kernel MMD GAN enjoys the following advantages: • Our proposed random-forest kernel used in MMD GAN benefits from ensemble learning. From the perspective of random forest, the output of MMD GAN discriminator h(·; θ N ) can be viewed as feature inputs to the forest. To mention, each tree only gets access to merely a fraction of the features by random connection functions, and thus the entire forest benefits from ensemble learning.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 • Our random-forest kernel MMD GAN enjoys the advantages of three powerful discrimi- native methods, which are CNN, kernel method, and ensemble learning. To be specific, CNN is good at extracting useful features from images; Kernel method utilize RKHS for discrimination; Ensemble learning utilizes the power of randomness and ensemble. • Our proposed random-forest kernel has some good theoretical properties. In one aspect, random-forest kernel is proved to be characteristic in  Shen & Vogelstein (2018) . In another, in Section 3.5, the unbiasedness of the gradients of MMD GAN with random-forest kernel is proved.

Section Title: RANDOM-FOREST KERNEL TRAINING TECHNIQUE
  RANDOM-FOREST KERNEL TRAINING TECHNIQUE In  Frosst & Hinton (2017) , the authors mention that the tree may get stuck on plateaus if internal nodes always assign the most of probability to one of its subtree. The gradients will vanish because the gradients of the logistic-type decision function will be very closed to zero. In order to stabi- lize the training of random-forest kernel and avoid the stuck on bad solutions, we add penalty that encourage each internal node to split in a balanced style as  Frosst & Hinton (2017)  does, that is, we penalize the cross entropy between the desired 0.5, 0.5 average probabilities of falling into two subtrees and the actual average probability α, 1 − α. The actual average probability of the i-th internal node α i is α i = x∈Ω P i (x)p i (x) x∈Ω P i (x) , where P i (x) is the routing probability of x from root node to internal node i, p i (x) is the probability of x falling into the left subtree of the i-th internal node, and Ω is the collection of mini-batch samples. Then, the formulation of the regularizer is: R(Ω) = −λ i∈Internal Nodes 0.5 log (α i ) + 0.5 log (1 − α i ) , where λ is exponentially decayed with the depth of d of the internal node by multiplying the coeffi- cient 2 −d , for the intuition that less balanced split in deeper internal node may increase the non-linear discrimination power. When training random-forest kernel, a mini-batch of real samples X and generated pictures Y are both fed into the discriminator, and then k(X, X), k(X, Y ) and k(Y, Y ) are calculated, where k := k RF • h(·; θ N ). Here, to notify, we find that the Ω in the regularizer formulation does matter in forest-kernel setting. It's better to calculate α i and R(Ω) in the case of Ω = X, Ω = Y , Ω = X ∪ Y respectively, and then sum up three parts of regularizer as final regularizer R. Therefore, the formulation of regularizer R added in the training of random-forest kernel is

Section Title: THEORETICAL RESULTS
  THEORETICAL RESULTS In this subsection, we present our main theoretical results. Theorem 2 (Unbiasedness) Let X be the true data on X with the distribution P X and Z be the noise on Z with the distribution P Z satisfying E P X X α < ∞ and E P Z Z α < ∞ for some α ≥ 1. Moreover, let G ψ : Z → X be a generator network, h θ N : X → R d be a discriminator network, k RF be the random-forest kernel, and θ D := (θ N , θ F ) be the parameters of the GAN discriminator. Then, for µ-almost all θ D ∈ R |θ D | and ψ ∈ R |ψ| , there holds In other words, during the training process of Random-Forest Kernel MMD GAN, the estimated gradients of MMD with respect to the parameters ψ and θ D are unbiased, that is, the expectation and the differential are exchangeable.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we evaluate our proposed random-forest kernel in the setting of MMD GAN in ( Li et al., 2017 ) and the MMD GAN with repulsive loss ( Wang et al., 2019 ). To illustrate the efficacy of our random-forest kernel, we compare our random-forest kernel with Gaussian kernel ( Li et al., 2017 ), rational quadratic kernel ( Bińkowski et al., 2018 ) in the setting of the original MMD GAN loss, and compare our random-forest kernel with bounded Gaussian kernel ( Wang et al., 2019 ) in the setting of MMD GAN with repulsive loss.

Section Title: EXPERIMENTAL SETUP
  EXPERIMENTAL SETUP

Section Title: DATASETS
  DATASETS The experiments are evaluated on three benchmark datasets: (1) the Cifar10 dataset of 32 × 32 pictures ( Krizhevsky et al., 2009 ); (2) the LSUN dataset of bedroom pictures resized to 64 × 64 ( Yu et al., 2015 ); (3) the CelebA dataset of celebrity faces pictures randomly cropped and resized to 160 × 160 ( Liu et al., 2015 ). The images are scaled to range [0, 1].

Section Title: KERNELS AND LOSSES
  KERNELS AND LOSSES Under the setting of MMD GAN loss proposed in  Li et al. (2017) , we compare our proposed random- forest kernel with the Gaussian RBF kernel ( Li et al., 2017 ) with mixture of kernel scales σ and the Rational quadratic kernel ( Bińkowski et al., 2018 ) with mixture of kernel scales α. Moreover, under another setting of MMD GAN with repulsive loss proposed in  Wang et al. (2019) , we compare our random-forest kernel with the bounded RBF kernel ( Wang et al., 2019 ). As is shown in  Figure 3 , the shapes of the Gaussian RBF kernel and the rational quadratic kernel are both symmetric. However, the local structure of random-forest kernel (w.r.t reference points except 70-dimensional zero vector) is asymmetric and very complex. The asymmetry and complexity of random-forest kernel may be helpful to discriminate two distributions in MMD GAN training.

Section Title: NETWORK ARCHITECTURE
  NETWORK ARCHITECTURE For dataset Cifar10 and dataset LSUN bedroom, DCGAN ( Radford et al., 2016 ) architecture with hyper parameters from Miyato et al. (2018) is used for both generator and discriminator; and for dataset CelebA, we use a 5-layer DCGAN discriminator and a 10-layer ResNet generator. Further details of the network architecture are given in Appendix A.3. We mention that in all experiments, batch normalization ( Ioffe & Szegedy, 2015 ) is used in the generator and spectral normalization (Miyato et al., 2018) is used in the discriminator. The hyper-parameter details of kernels used in Under review as a conference paper at ICLR 2020

Section Title: TRAINING HYPER-PARAMETERS
  TRAINING HYPER-PARAMETERS We set the initial learning rate 10 −4 and decrease the learning rate by coefficient 0.8 in iteration 30000, 60000, 90000, and 120000. Adam optimizer ( Kingma & Ba, 2015 ) is used with momentum parameters β 1 = 0.5 and β 2 = 0.999. The batch size of each model is 64. All models were trained for 150000 iterations on CIFAR-10, CelebA, and LSUN bedroom datasets, with five discriminator updates per generator update.

Section Title: EVALUATION METRICS
  EVALUATION METRICS The following three metrics are used for quantitative evaluation: Inception score (IS) ( Salimans et al., 2016 ), Fréchet inception distance (FID) ( Heusel et al., 2017 ), and Kernel inception distance (KID) ( Bińkowski et al., 2018 ). In general, higher IS and Lower FID, KID means better quality. However, outside the dataset Imagenet, the metric IS has some problem, especially for datasets celebA and LSUN bedroom. Therefore, for inception score, we only report the inception score of CIFAR-10. Quantitative scores are calculated based on 50000 generator samples and 50000 real samples.

Section Title: EXPERIMENTAL RESULTS
  EXPERIMENTAL RESULTS We compare our proposed random-forest kernel with mix-rbf kernel and mix-rq kernel in the setting of the MMD GAN loss, and compare our proposed random-forest kernel with rbf-b kernel in the setting with MMD GAN repulsive loss. The Inception Score, the Fréchet Inception Distance and the Kernel Inception Distance of applying different kernels and different loss functions on three benchmark datasets are shown in  table 1 . We find that, in the perspective of the original MMD GAN loss, our newly proposed random-forest kernel shows better performance than the mix-rbf kernel and the mix-rq kernel in CIFAR-10 dataset and LSUN bedroom dataset; and in the perspective of the repulsive loss, the performance of our newly proposed random-forest kernel is comparable or better than the rbf-b kernel. The efficacy of our newly proposed random-forest kernel is shown under the setting of both MMD GAN loss and MMD GAN repulsive loss. Some randomly generated pictures of model learned with various kernels and two different loss functions are visualized in Appendix D. Under review as a conference paper at ICLR 2020

```
