Title:
```
Under review as a conference paper at ICLR 2020 TOWARDS CERTIFIED DEFENSE FOR UNRESTRICTED ADVERSARIAL ATTACKS
```
Abstract:
```
Certified defenses against adversarial examples are very important in safety-critical applications of machine learning. However, existing certified defense strategies only safeguard against perturbation-based adversarial attacks, where the attacker is only allowed to modify normal data points by adding small perturbations. In this paper, we provide certified defenses under the more general threat model of unrestricted adversarial attacks. We allow the attacker to generate arbitrary inputs to fool the classifier, and assume the attacker knows everything except the classifiers' parameters and the training dataset used to learn it. Lack of knowledge about the classifiers parameters prevents an attacker from generating adversarial examples successfully. Our defense draws inspiration from differential privacy, and is based on intentionally adding noise to the classifier's outputs to limit the attacker's knowledge about the parameters. We prove concrete bounds on the minimum number of queries required for any attacker to generate a successful adversarial attack. For a simple linear classifiers we prove that the bound is asymptotically optimal up to a constant by exhibiting an attack algorithm that achieves this lower bound. We empirically show the success of our defense strategy against strong black box attack algorithms.
```

Figures/Tables Captions:
```
Figure 1: The attack-defense protocol
Figure 2: Success rate of attacker vs. number of queries on synthetic dataset and MNIST. Left to right: Left and Middle: the number of attacker queries vs. attack success rate for different training set sizes. Right: the no-answer (NR) and margin error (ME) on clean data x ∼ p * (x). Top to bottom: Top: naive Bayes classifier trained on synthetic Gaussian; Middle: naive Bayes classifier trained on MNIST; Bottom: logistic regression trained on synthetic Gaussian. Logistic regression trained on MNIST fails to achieve robustness for the training set sizes we experimented (up to 10k). For both cases, defended classifiers require orders of magnitude more queries for successful adversarial attack, while the adverse effect on clean data x ∼ p * (x) is negligible.
Figure 3: Success rate vs. number of queries for Simba attacker on Inception network. We adjust the threshold such that both methods have similar no response rate (NR) on the clean data. The undefended network is signif- icantly more vulnerable to attack.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Classifiers trained using machine learning can often achieve high accuracy on test data coming from the same distribution as the training one. Unfortunately, for a class of inputs called adversarial examples ( Szegedy et al., 2014 ), many classifiers can be fooled to give wrong predictions with high confidence. This poses a significant security risk for real world deployment of machine learning models. To mitigate this threat, many methods have been proposed to defend against adversarial examples (see, e.g.,  Papernot et al. (2016) ;  Song et al. (2018a) ;  Ma et al. (2018) ;  Buckman et al. (2018) ). However, empirically grounded methods are often found to be ineffective using newer attack methods that they had originally not considered ( Carlini & Wagner, 2017 ;  Athalye et al., 2018 ). To end this arms race, defenses with certified theoretical guarantees against a threat model have recently gained more traction (see, e.g.,  Raghunathan et al. (2018) ;  Wong & Kolter (2017) ;  Dvijotham et al. (2018) ;  Cohen et al. (2019) ). Existing certified defense methods are all restricted to perturbation-based attacks ( Song et al., 2018a ), a threat model where the attacker is only allowed to generate an adversarial example by perturbing existing data points using small adversarial noise. In addition, a certified defense for one type of perturbation (e.g., based on l 2 norm) can still be vulnerable to a different type of perturbation (e.g. spatial transform (Xiao et al., 2018)). Following  Song et al. (2018b)  and Brown et al. (2018), we consider the much more general threat model of unrestricted adversarial attacks, where the small perturbation restriction is removed: any input is considered a valid adversarial example as long as it induces the classifier to predict a different label than an oracle classifier. As shown in  Song et al. (2018b) , existing certified defenses are not robust to unrestricted adversarial attacks. In this paper, we provide a defense strategy with (asymptotically) certified guarantees against unrestricted adversarial attacks. The attacker can use any strategy to choose any input, and the attack is successful as long as the input is confidently classified into the wrong class by the target classifier. Note that in the white-box setup, where the attacker has access to all parameters of the classifier, defending against unrestricted adversarial examples is essentially impossible unless the classifier is Under review as a conference paper at ICLR 2020 perfect, because the attacker can simply enumerate all possible inputs to find adversarial ones (we do not assume any computational restriction of the attacker). As a result, we focus our discussion on black-box unrestricted attacks, where the attacker does not know the exact parameters of the model nor the exact training data (otherise, the attacker could recover the parameters by simulating the learning algorithm), but can query the model for a limited number of times before making unrestricted adversarial examples. For example, for online services, it is easy to limit the number of queries allowed per user.

Section Title: Consider a classifier learned using empirical risk minimization
  Consider a classifier learned using empirical risk minimization Because the training data are randomly drawn from the data-generating distribution, the parameters of the classifier will be random even if the training algorithm is deterministic. Since we assume a black-box setting where the attacker does not know the learned parameters or the training dataset, the attacker has to obtain extra information by querying the model, even though the exact training algorithm and even the data-generating distribution might be known to the attacker. As a result, we can adopt methods similar to those used in differential privacy ( Dwork et al., 2014 ) to reduce the amount of information about the parameters leaked from each query issued by the attacker. We give concrete lower bounds on the minimum number of queries an attacker needs in order to successfully generate adversarial examples. For most classifiers the guarantees are asymptotic, meaning that the guarantees are valid if the number of training examples is sufficiently large. For very simple classifiers we prove finite sample guarantees. Our bound is also asymptotically minimax optimal with respect to important parameters of the problem - the dimension of the input and the size of the training set. We prove this by exhibiting a class of classifiers and a concrete attacking algorithm that successfully generates adversarial examples while querying no more than the minimum necessary according to the lower bounds (up to a constant). Experimentally, we show that the defense strategy is effective against strong black-box unrestricted attacks, even when we use complex classifiers for which no theoretical guarantee with finite sample size is available. Compared to models without any defense, the number of queries needed to identify vulnerabilities often increases by an order of magnitude using our defense strategy, with negligible effect on the accuracy on "clean" inputs. Providing finite sample guarantees for complex models such as deep neural networks is an open problem for future work.

Section Title: PROBLEM DEFINITION
  PROBLEM DEFINITION

Section Title: CLASSIFICATION
  CLASSIFICATION Given a training dataset {(x i , y i ) ∈ X × Y} n i=1 i.i.d. ∼ p * (x)p * (y | x), we consider the task of learning a classifier to predict the label y for an input x. To simplify our discussion, we assume that X = [−1, 1] d , and Y = {−1, 1}. Let {g(w, ·) : X → R} be a class of score functions with parameter w, whose signs determine the class labels, i.e., y = sign(g(w, x)), where we define sign(x) = 1 when x ≥ 0, and sign(x) = −1 otherwise. The model family is assumed to be well-specified, i.e., there exists a ground-truth parameter w * such that the true labeling function is given by y = sign(g(w * , x)). We can use any learning algorithm to obtain w based on a training dataset. Since the training data are randomly generated from the underlying distribution p * (x)p * (y | x), w is also a random vector and we denote its distribution as p learn (w). With a reasonable learning algorithm, p learn (w) should be increasingly concentrated around w * while the size of the training dataset n grows. When deploying the classifier to real world applications, we might want to use a (possibly stochastic) surrogate score function f (w, ·) : X → R which depends on the learned parameter w of our model. Usually, it is just g(w, ·), but to mitigate adversarial attacks we might choose f (w, ·) to be some special classifier with a defense strategy. To avoid low confidence incorrect predictions, we give no response when the confidence is low. Specifically, we allow a threshold α ∈ R + such that given an input query x, the predicted label is

Section Title: Interacting procedure:
  Interacting procedure: 1. The attacker chooses the distributions q t , t = 1, · · · , T and Q attack defined as below. 2. The defender samples w ∼ p learn (w). 3. For t ← 1, · · · , T (a) The attacker observed o 1:t−1 = (x 1 , z 1 ), · · · , (x t−1 , z t−1 ) and sample x t ∼ q t (x t |o 1:t−1 ) as the next query. (b) The defender sends z t = f (w, x t ) to the attacker. 4. The attacker samples q ∼ Q attack (q | o 1:T ), where q is a probability distribution on X . The attacker outputs q(x).

Section Title: PERFORMANCE EVALUATION
  PERFORMANCE EVALUATION We evaluate the expected performance of the classifier f (w, x) on a test distribution q(x)p * (y | x). The traditional setting in statistical machine learning assumes that q(x) = p * (x). In adversarial settings, however, q(x) is the distribution of adversarial examples and is very different from p * (x). Since we consider unrestricted attacks, we assume no relationship between q(x) and p * (x). The classifier f (w, x) can make two kinds of errors for a given input x. First, f (w, x) might give no response. We call this the no-response error, which is denoted as NR(x; α) I(|f (w, x)| ≤ α), where I[P] is the indicator function whose value is 1 when the property P holds; other it produces 0. Secondly, f (w, x) can produce a wrong prediction with high confidence. We call this the margin error, which is defined as An ideal classifier f (w, x) should minimize the expected no-response error E q(x) [NR(x; α)] and margin error E q(x) [ME(x; α)] simultaneously. When defending against an adversary, the margin errors are more important because a confident but wrong prediction is arguably more harmful than giving no prediction at all. Consequently, we only focus on bounding the margin errors achievable by attackers in the sequel. We will only consider no response error for clean data x ∼ p * (x).

Section Title: THE ATTACK-DEFENSE PROTOCOL
  THE ATTACK-DEFENSE PROTOCOL In  Figure 1 , we provide a specific protocol of attack and defense to capture our assumptions of the threat model. Since we consider the threat model of unrestricted adversarial attacks, we assume the attacker to be fully general: The attacker is allowed to use any adaptive set of intermediate distributions {q t (x | o 1:t−1 )} T t=1 to issue queries based on all previous observations. For the final attack distribution q, the attacker is allowed to sample it from any distribution of distributions Q attack (q | o 1:T ). We call a specific choice of {q t (x | o 1:t−1 )} T t=1 and Q attack an attack strategy, and a specific choice of p learn (w) and f (·, ·) a defense strategy. We additionally allow the attacker to know the exact defense strategy. Note that when the attack and defense strategies are fixed beforehand, the attack-defense protocol defines a joint probability over w, o 1:T and q. The protocol can be modified in the defender's favor if defender only sends y t according to Eq.(1) instead of z t . However, using z t allows normal users (sampling from p * (x)) to get a confidence score, and any certified defense results are more general (they are still true if attacker only has access to y t ). Intuitively, the attack strategy is successful under the attack-defense protocol, if with high probability the final attack distribution q incurs large expected margin errors for f (w, x). To formalize this intuition, we define a winning strategy of the attacker as follows. Under review as a conference paper at ICLR 2020 Definition 1. For a given defense strategy, an attack strategy in the attack-defense protocol is an (α, γ, δ)-winning strategy if with probability at least γ, the attacker outputs a q such that In the following part, we will propose a concrete defense strategy and prove that no winning strategies can exist under some conditions.

Section Title: TOWARDS A CERTIFIED DEFENSE
  TOWARDS A CERTIFIED DEFENSE In order for a certified defense strategy to exist we need some reasonable conditions on the problem. All the proofs are available in the appendix.

Section Title: DEFENSIBILITY
  DEFENSIBILITY For an omniscient attacker who knows f (w, ·) and g(w * , ·) (this is beyond the capabilities granted by our attack-defense protocol), there will not be any effective defense strategies unless f (w, x) never produces wrong predictions. This is because the attacker can simply enumerate an input x bad so that |f (w, x bad ) − g(w * , x bad )| is maximal, and set q(x) = δ x bad (here δ x denotes a point mass distribution on x). This results in the maximal average margin error for f (w, x). Under our attack-defense protocol, the attacker does not have access to w and hence do not know f (w, ·). The randomness of p learn (w) makes it more difficult for the attacker to infer w through queries. To guarantee the existance of a certified defense for all attacks, we need to make sure the randomness of p learn (w) is sufficient such that no fixed distribution q(x) can incur large average margin error for f (w, ·) with high probability for a random draw of w ∈ p learn (w). We formalize this intuition as the following condition. Condition 1. Given a classifier g(·, ·), margin α, and true parameter w * , a distribution p learn (w) is t-defensible if ∀x ∈ X , we have In other words, the above condition says that if p learn (w) is t-defensible, there will not exist any x ∈ X such that g(w, x) significantly deviates from the ground-truth score g(w * , x) with large probability over w ∼ p learn (w).

Section Title: QUERY PRIVACY
  QUERY PRIVACY As discussed before, the attacker cannot find an attack distribution q a-priori to fool f (w, ·) when p learn (w) is t-defensible. To strive for successful attacks, the attacker must then obtain more information about the specific w used by the defender through queries. Therefore, a successful defense strategy should not leak too much information through each query from the attacker. We formalize this intuition as follows. Condition 2. A defense strategy in the attack-defense protocol is s-query private if for any x ∈ X we have I(z; w | x) ≤ s.

Section Title: CERTIFYING THE DEFENSE STRATEGY
  CERTIFYING THE DEFENSE STRATEGY To satisfy this condition and prevent leakage of information on w, we take inspiration from differential privacy and perturb the scores with Gaussian noise. By adding the right amount of noise, we hope to hit the sweet spot where the accuracy on clean input x ∼ p * (x) is mostly preserved, while at the same time the attacker obtains least information from queries. We use the following stochastic surrogate function f (·, ·) as the key of our defense strategy. As the first main result of our paper, the following theorem asserts that under the above two conditions no attacker has a winning strategy using a limited number of queries. Theorem 1. Suppose p learn (w) is t-defensible, and the defense strategy is s-private. ∀α > 0, 0 < γ, δ < 1, and for τ 2 ≤ α 2 8 log 2/δ , there is no (2α, γ, 2δ) winning strategy if T ≤ 1 s (γt+γ log δ−log 2).

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Note that in the definition above s implicitly depends on τ 2 . Adding more noise (larger τ 2 ) reduces the amount of leaked information on w. In practice, it can be hard to verify the defensibility of p learn (w) or the query-privacy of the defense strategy for complicated models. In the following, we show examples of simple models where these two conditions are either strictly satisfied or asymptotically satisfied in the limit of infinite data. In the next section, we will first show that for logistic regression and kernel logistic regression models, the conditions hold asymptotically when the models are well-specified. After that, we show that the conditions can hold exactly for a simple naïve Bayes model. We leave the analysis of defensibility and query-privacy for more complicated models as an open problem for future research.

Section Title: ASYMPTOTIC ANALYSIS OF (KERNEL) LOGISTIC REGRESSION
  ASYMPTOTIC ANALYSIS OF (KERNEL) LOGISTIC REGRESSION We start by considering a linear model g(w, x) = (w, x), where (·, ·) denotes the inner product in Euclidean space. One common approach to learning this model is logistic regression. When the model is well-specified, i.e., there exists w * such that p * (y | x) = 1 1+exp(−y(w * ,x)) , we can prove that p learn (w) converges in distribution to a Gaussian distribution with mean w * as n → ∞. Specifically, we have the following lemma. Lemma 1. Assume that w * ≤ λ. Given i.i.d. samples {(x i , y i )} n i=1 ∼ p * (x)p * (y | x), the estimator w n obtained by solving the following objective function The asymptotic Gaussianity of p learn (w) greatly facilitates the investigation in its defensibility. However, the assumption that a linear model g(w, x) being well-specified is rather restrictive. We therefore additionally consider a generalization called kernel logistic regression, where g(w, x) = w, φ(x) . Here φ(x) is the natural basis vector of the Reproducing Kernel Hilbert Space H corresponding to a kernel k(·, ·), and we use ·, · to denote the inner product in H. When the kernel is universal ( Micchelli et al., 2006 ), e.g., the Gaussian RBF kernel, functions in RKHS can approximate any bounded continuous function arbitrarily well w.r.t. the uniform norm. Therefore, it is a much milder assumption that there exists w * ∈ H such that For kernel logistic regression, we can similarly prove that p learn (w) is asymptotically Gaussian (see Lemma 6 in the appendix), with asymptotic variance where ⊗ denotes the tensor product. For both logistic regression and kernel logistic regression, under the assumptions of well-specification and in the asymptotic regime, we can prove both defensibility and query privacy. Formally, Proposition 1. For logistic regression, the asymptotic distribution of p learn (w) is nα 2 2 Σ 2 d -defensible and our defense strategy is Σ 2 d 2nτ 2 -query private. For kernel logistic regression, the asymptotic distribution of p learn (w) is nα 2 2 Σ H op -defensible, and our defense is Σ H op 2nτ 2 -query private. Therefore, our defense strategy is asymptotically certified for logistic regression and kernel logistic regression models.

Section Title: NON-ASYMPTOTIC VERIFICATION OF ASSUMPTIONS
  NON-ASYMPTOTIC VERIFICATION OF ASSUMPTIONS Asymptotic conditions hold for sufficiently large training set size n, but to be fully verifiable we need results that hold for finite n. These bounds are very difficult to obtain, and we show some bounds for very simple classifiers (naive Bayes).

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Before showing the results for naive Bayes classifiers, we first show defensibility of computing averages. Proposition 2. Let p(x) be any distribution with covariance Σ and w * = E p(x) [x]. Let x 1 , · · · , x n ∼ p(x) and w = 1 n i x i , then the distribution of w is nα 2 2d Σ 2 +2/3dα -defensible. Note that the results of this theorem is almost as good as if we knew the distribution p(x) is Gaussian N (0, Σ). In that case, we can apply Proposition 1 and conclude that it is nα 2 2d Σ 2 -defensible. The following Corollary shows a similar bound for a naive Bayes classifier. This is a direct outcome of the above proposition. Therefore, we can use the training data to upper bound Σ 2 ( El Karoui et al., 2008 ) and obtain a non-asymptotic certificate of defensibly. For query privacy non-asymptotic guarantees are easier to get. All we need is to upper bounding the covariance spectral radius of p learn (w). Proposition 3. If g(w, x) = (w, x) and p learn (w) has covariance Σ, it is d Σ 2 2τ 2 -query private.

Section Title: RATE ANALYSIS AND MINIMAX OPTIMALITY
  RATE ANALYSIS AND MINIMAX OPTIMALITY To demonstrate more concrete asymptotic rates, we will analyze a simple setup. We study a classifier with g(w, x) = (w, x) and p learn (w) = N (w * , σ 2 I) where σ 2 = 1/n. We also assume that w * ∞ < b for some b > 0 (we need b to be fixed as we analyze the asymptotic rate when d → ∞). We give an example of a classifier where this distribution could arise in the appendix. Based on this setup, we have the following specialization of Theorem 1. This bound is desirable if α 1 (there is a large margin) and n d (more data than input dimensions).

Section Title: AN ATTACKER STRATEGY
  AN ATTACKER STRATEGY Corollary 2 is the minimum number of queries any attacker strategy needs to win. Now we show a concrete attacker strategy that (almost) achieves this lower bound. The concrete attacker strategy is shown below. Intuitively, the attacker queries each of the d dimen- sions of the input x ∈ R d to find out the sign of w − w * . The attacker finds a x such that (x, w − w * ) is large but (x, w * ) < 0.

Section Title: PROTOCOL 2. THE SIGN ATTACK PROTOCOL
  PROTOCOL 2. THE SIGN ATTACK PROTOCOL 1. Defender samples w ∼ p learn (w). 2. For round = t = 1, · · · , T (a) Attacker chooses x t = t mod d , where i is the i-th basis vector for R d . (b) Defender sends z t = (x t , w) + N (0, τ 2 ) to attacker. 3. Attacker computesw i = avg{z t |x t = i }. Without loss of generality (by re-indexing the dimensions) The extra condition n ≤ d 2 160α 2 log 2/δ ensures that adversarial examples actually exist; if the classifier is sufficiently accurate (i.e. w − w * 1 ≤ α), then no example will be falsely classified.

Section Title: RATE SUMMARY AND COMPARISON
  RATE SUMMARY AND COMPARISON Note that Corollary 2 is a lower bound on required number of queries: no attacker can have high success rate unless T > α 4 n 2 γ 2d 2 log 2/δ ; while Theorem 2 is an upper bound: with T = 20α 4 n 2 d 2 there is at least one attacker that can achieve high success rate. The two bounds have the same asymptotic with respect to α, n and d, which shows that the bound is asymptotically optimal with respect to these parameters. To better study the asymptotic behavior for large d and large n, we will choose α = √ d. For both methods Pr p * (x) [NR(x)] can be arbitrarily small for sufficiently large d. This is proved in the Appendix. We have the following rates for the smallest T such that an attacker can have a (α, 1, 1−γ 2 ) winning strategy for any γ > 0 and α = √ d. With this choice of α, T no longer depend on d for both upper and lower bounds, and we can get: • For defended classifier with τ 2 = α 2 2 log 2/δ we have T = Θ(n 2 ) queries. • For undefended classifier with τ 2 = 0 we have T = O(n).

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: LINEAR MODELS
  LINEAR MODELS We will first verify our method empirically on linear classification tasks. We will use two classification datasets.

Section Title: Synthetic Gaussian
  Synthetic Gaussian

Section Title: Mnist
  Mnist Because linear models cannot classify MNIST with high accuracy, we use deep network features (last layer of a pretrained AlexNet) as the input X instead of the pixel space. We train with the entire dataset and use the resulting vector as w * (for the attacker).

Section Title: Attack Methods
  Attack Methods We will use two attacking method: the first one is the attack strategy of Protocol 2 (which we will denote the sign attack); the second one is Simba ( Guo et al., 2019 ), a recently proposed black-box attacking algorithm with very good empirical performance.

Section Title: Classifiers
  Classifiers We will use two classifiers, the naive Bayes classifier defined in Corollary 1 and logistic regression. For both classifiers we choose α such that the no-response rate on p * (x) (estimated using the validation dataset) is no more than 10%. We either choose τ 2 = 0 (undefended) or τ 2 = α 2 2 log 2/δ as in Theorem 1 (defended) where δ is chosen to be 5%.

Section Title: RESULTS
  RESULTS The results are shown in  Figure 2 . For defended classifier, the number of queries required for successful attack typically increases by an order of magnitude. In addition, the effect on the clean data p * (x) is almost negligible. Another interesting observation is that, the naive Bayes classifier actually performs much better in terms of adversarial robustness than logistic regression. We conjecture that this is because logistic regression has high parameter estimation variance tr(Cov(w)) compared to naive Bayes. This indicates that adversarial robustness has very different requirements compared to classification accuracy. Even though logistic regression usually performs better in classification unless the training data size is tiny ( Ng & Jordan, 2002 ), its adversarial robustness is poor. Our theoretical guarantees are far from effective for deep models. It is unlikely the analysis will be ef- fective for current network architectures, so special architectures and analysis will be necessary for cer- tified defense under out framework. Nevertheless we perform simple experiments on deep networks to empirically verify that our defense strategy can effectively protect our model against black box attack methods. The setup is identical to ( Guo et al., 2019 ) on ImageNet, except we add a threshold α to the out- put sigmoid probabilities: if no class has a predicted probability greater than the threshold α, the model answers "I don't know". Here we choose α = 0.7. The results are shown in  Figure 3 . Similar to linear models, defended models require orders of magni- tude more queries to attack. We hope these promising experimental performance will motivate further the- oretical analysis and empirical investigation.

Section Title: DISCUSSION
  DISCUSSION In this paper we propose a new type of adversarial defense guarantee, one based on the concept of defensibility and query privacy. We show theoretical guarantees for simple classifiers and good empirical performance for complex classifiers. Several open questions remain to be answered. The first one to verify defensibility and query privacy for more complex classifiers. The second one is to design classifiers with better defensibility and query privacy properties. The third one is large scale empirical investigation of the defense strategy or similar defense strategies, i.e. studying the performance of adding noise to different layers of a deep neural network against a larger suite of black box attack algorithms. We hope our work can serve as a first step in this exciting direction of research. Under review as a conference paper at ICLR 2020

Section Title: A.1 PROOFS RELATED TO THE LOWER BOUND OF T
  A.1 PROOFS RELATED TO THE LOWER BOUND OF T Proof of Theorem 1. If a strategy is (2α, γ, 2δ) winning then with probability γ If Eq.(3) is true one of the following must be true For Eq.(4) we can bound by Therefore, Eq.(4) is false if the RHS is less than δ, which is when τ 2 ≤ α 2 2 log 2/δ . now we show the condition for Eq.(5) to be true. We first define the following notation. • Let q be any distribution on X , let q(X (w)) denote the probability mass q assigns to X (w). The following lemma is needed. Lemma 2. In out attack-defense protocol, if with probability at least γ, we have Pr[|g(w, x) − g(w * , x)| ≥ α] ≥ δ, then I(w; o 1:T ) ≥ γ(t + log δ) − log 2. Therefore, if I(w; o 1:T ) < γ(t + log δ) − log 2, Eq. (5) is false with at least γ probability. If we additionally have the condition that the attack-defense protocol is s-private, by a sequence of data processing inequalities, and by the fact that x t only depends on w through o <t , and y t only depends on x t and w, we have Therefore Eq. (5) is false with at least γ probability if I(y t ; w|x T ) ≤ sT < γ(t + log δ) − log 2 which is equivalent to T < 1 s (γt + γ log δ − log 2). What remains is to prove Lemma 2, which we do below. Proof of Lemma 2. By the condition of t defensibility we have We abbreviate E p learn (w) by E w . Define s(q; w) = max(q(X (w)), e −t ), then

```
