Title:
```
Published as a conference paper at ICLR 2020 SCALABLE AND ORDER-ROBUST CONTINUAL LEARN- ING WITH ADDITIVE PARAMETER DECOMPOSITION
```
Abstract:
```
While recent continual learning methods largely alleviate the catastrophic problem on toy-sized datasets, some issues remain to be tackled to apply them to real-world problem domains. First, a continual learning model should effectively handle catastrophic forgetting and be efficient to train even with a large number of tasks. Secondly, it needs to tackle the problem of order-sensitivity, where the performance of the tasks largely varies based on the order of the task arrival sequence, as it may cause serious problems where fairness plays a critical role (e.g. medical diagnosis). To tackle these practical challenges, we propose a novel continual learning method that is scalable as well as order-robust, which instead of learning a completely shared set of weights, represents the parameters for each task as a sum of task-shared and sparse task-adaptive parameters. With our Additive Parameter Decomposition (APD), the task-adaptive parameters for earlier tasks remain mostly unaffected, where we update them only to reflect the changes made to the task-shared parameters. This decomposition of parameters effectively prevents catastrophic forgetting and order-sensitivity, while being computation- and memory-efficient. Further, we can achieve even better scalability with APD using hierarchical knowledge consolidation, which clusters the task-adaptive parameters to obtain hierarchically shared parameters. We validate our network with APD, APD-Net, on multiple benchmark datasets against state-of-the-art continual learning methods, which it largely outperforms in accuracy, scalability, and order-robustness.
```

Figures/Tables Captions:
```
Figure 1: Description of crucial challenges for continual learning with Omniglot dataset experiment. Catas- trophic forgetting: Model should not forget what it has learned about previous tasks. Scalability: The increase in network capacity with respect to the number of tasks should be minimized. Order sensitivity: The model should have similar final performance regardless of the task order. Our model with Additive Parameter Decomposition effectively solves these three problems.
Figure 2: An illustration of Additive Parameter Decomposition (APD) for continual learning. APD effectively prevents catastrophic forgetting and suppresses order-sensitivity by decomposing the model parameters into shared σ and sparse task-adaptive τ t, which will let later tasks to only update shared knowledge. Mt is the task-adaptive mask on σ to access only the relevant knowledge. Sparsity on τ t and hierarchical knowledge consolidation which hierarchically rearranges the shared parameters greatly enhances scalability.
Figure 3: Accuracy over efficiency of expansion-based continual learning methods and our methods. We report performance over capacity and performance over training time on both datasets.
Figure 4: Performance disparity of continual learning baselines and our models on CIFAR-100 Split. Plots show per-task accuracy for 3 task sequences of different order. Performance disparity of all methods for 5 task sequences of different order are given in Figure A.8 in the Appendix.
Figure 5: (a)-(c) Catastrophic Forgetting on CIFAR-100 Superclass: Performance of our models on the 1 st , 6 th ,and 11 th task during continual learning. (d)-(e) Task Forgetting on CIFAR-100 Split: Per-task Performance of APD(1) (T1:5) when 1 st task is dropped during continual learning.
Figure 6: Left: Performance comparison with several benchmarks on Omniglot-rotation (standard deviation into parenthesis). Right: The number of the parameters which is obtained during course of training on Omniglot-rotation.
Figure 7: Visualizations of the model paramters during continual learning. The colored markers denote the parameters for each task i, and the empty markers with black outlines denote the task- shared parameters. Dashed arrows indicate the drift in the parameter space as the model trains on a sequence of tasks.
Table 1: Experiment results on CIFAR-100 Split and CIFAR-100 Superclass datasets. The results are the mean accuracies over 3 runs of experiments with random splits, performed with 5 different task order sequences. STL is the single-task learning model that trains a separate network for each task independently. Standard deviations for accuracy are given in Table A.3 in the Appendix.
Table 2: Accuracy comparison on diverse datasets according to two opposite task order (arrows). The results are the mean accuracies over 3 runs of experiments. VGG16 with batch normalization is used for a base network.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Continual learning ( Thrun, 1995 ), or lifelong learning, is a learning scenario where a model is incrementally updated over a sequence of tasks, potentially performing knowledge transfer from earlier tasks to later ones. Building a successful continual learning model may lead us one step further towards developing a general artificial intelligence, since learning numerous tasks over a long-term time period is an important aspect of human intelligence. Continual learning is often formulated as an incremental / online multi-task learning that models complex task-to-task relationships, either by sharing basis vectors in linear models ( Kumar & Daume III, 2012 ;  Ruvolo & Eaton, 2013 ) or weights in neural networks ( Li & Hoiem, 2016 ). One problem that arises here is that as the model learns on the new tasks, it could forget what it learned for the earlier tasks, which is known as the problem of catastrophic forgetting. Many recent works in continual learning of deep networks ( Li & Hoiem, 2016 ;  Lee et al., 2017 ;  Shin et al., 2017 ;  Kirkpatrick et al., 2017 ;  Riemer et al., 2019 ;  Chaudhry et al., 2019 ) tackle this problem by introducing advanced regularizations to prevent drastic change of network weights. Yet, when the model should adapt to a large number of tasks, the interference between task-specific knowledge is inevitable with fixed network capacity. Recently introduced expansion-based approaches handle this problem by expanding the network capacity as they adapt to new tasks ( Rusu et al., 2016 ;  Fang et al., 2017 ;  Yoon et al., 2018 ;  Li et al., 2019 ). These recent advances have largely alleviated the catastrophic forgetting, at least with a small number of tasks. However, to deploy continual learning to real-world systems, there are a number of issues that should be resolved. First, in practical scenarios, the number of tasks that the model should train on may be large. In the lifelong learning setting, the model may even have to continuously train on an unlimited number of tasks. Yet, conventional continual learning methods have not been verified for their scalability to a large number of tasks, both in terms of effectiveness in the prevention of Published as a conference paper at ICLR 2020 catastrophic forgetting, and efficiency as to memory usage and computations (See  Figure 1 (a) , and (b)). Another important but relatively less explored problem is the problem of task order sensitivity, which describes the performance discrepancy with respect to the task arrival sequence (See  Figure 1 (c) ). The task order that the model trains on has a large impact on the individual task performance as well as the final performance, not only because of the model drift coming from the catastrophic forgetting but due to the unidirectional knowledge transfer from earlier tasks to later ones. This order-sensitivity could be highly problematic if fairness across tasks is important (e.g. disease diagnosis). To handle these practical challenges, we propose a novel continual learning model with Additive Parameter Decomposition (APD). APD decomposes the network parameters at each layer of the target network into task-shared and sparse task-specific parameters with small mask vectors. At each arrival of a task to a network with APD, which we refer to as APD-Net, it will try to maximally utilize the task-shared parameters and will learn the incremental difference that cannot be explained by the shared parameters using sparse task-adaptive parameters. Moreover, since having a single set of shared parameters may not effectively utilize the varying degree of knowledge sharing structure among the tasks, we further cluster the task-adaptive parameters to obtain hierarchically shared parameters (See  Figure 2 ). This decomposition of generic and task-specific knowledge has clear advantages in tackling the previously mentioned problems. First, APD will largely alleviate catastrophic forgetting, since learning on later tasks will have no effect on the task-adaptive parameters for the previous tasks, and will update the task-shared parameters only with generic knowledge. Secondly, since APD does not change the network topology as existing expansion-based approaches do, APD-Net is memory-efficient, and even more so with hierarchically shared parameters. It also trains fast since it does not require multiple rounds of retraining. Moreover, it is order-robust since the task-shared parameters can stay relatively static and will converge to a solution rather than drift away upon the arrival of each task. With the additional mechanism to retroactively update task-adaptive parameters, it can further alleviate the order-sensitivity from unidirectional knowledge transfer as well. We validate our methods on several benchmark datasets for continual learning while comparing against state-of-the-art continual learning methods to obtain significantly superior performance with minimal increase in network capacity while being scalable and order-robust. The contribution of this paper is threefold: • We tackle practically important and novel problems in continual learning that have been overlooked thus far, such as scalability and order robustness. • We introduce a novel framework for continual deep learning that effectively prevents catastrophic forgetting, and is highly scalable and order-robust, which is based on the decomposition of the network parameters into shared and sparse task-adaptive parameters with small mask vectors. • We perform extensive experimental validation of our model on multiple datasets against recent continual learning methods, whose results show that our method is significantly superior to them in terms of the accuracy, efficiency, scalability, as well as order-robustness.

Section Title: Continual Learning
  Continual Learning The literature on continual (lifelong) learning ( Thrun, 1995 ) is vast ( Ruvolo & Eaton, 2013 ) as it is a long-studied topic, but we only mention the most recent and relevant works. Most continual deep learning approaches are focused on preventing catastrophic forgetting, in which case the retraining of the network for new tasks shifts the distribution of the learned representations. A simple yet effective regularization is to enforce the representations learned at the current task to be closer to ones from the network trained on previous tasks ( Li & Hoiem, 2016 ). A more advanced approach is to employ deep generative models to compactly encode task knowledge ( Shin et al., 2017 ) and generate samples from the model later when learning for a novel task.  Kirkpatrick et al. (2017) , and  Schwarz et al. (2018)  proposed to regularize the model parameter for the current tasks with parameters for the previous task via a Fisher information matrix, to find a solution that works well for both tasks, and  Lee et al. (2017)  introduces a moment-matching technique with a similar objective.  Serrà et al. (2018)  proposes a new binary masking approach to minimize drift for important prior knowledge. The model learns pseudo-step function to promote hard attention, then builds a compact network with a marginal forgetting. But the model cannot expand the network capacity and performs unidirectional knowledge transfer thus suffers from the order-sensitivity.  Lopez-Paz & Ranzato (2017) ;  Chaudhry et al. (2019)  introduces a novel approach for efficient continual learning with weighted update according to the gradients of episodic memory under single-epoch learning scenario.  Nguyen et al. (2018)  formulates continual learning as a sequential Bayesian update and use coresets, which contain important samples for each observed task to mitigate forgetting when estimating the posterior distribution over weights for the new task.  Riemer et al. (2019)  addresses the stability-plasticity dilemma maximizing knowledge transfer to later tasks while minimizing their interference on earlier tasks, using optimization-based meta-learning with experience replay.

Section Title: Dynamic Network Expansion
  Dynamic Network Expansion Even with well-defined regularizers, it is nearly impossible to completely avoid catastrophic forgetting, since in practice, the model may encounter an unlimited number of tasks. An effective way to tackle this challenge is by dynamically expanding the network capacity to handle new tasks. Dynamic network expansion approaches have been introduced in earlier work such as  Zhou et al. (2012) , which proposed an iterative algorithm to train a denoising autoencoder while adding in new neurons one by one and merging similar units.  Rusu et al. (2016)  proposed to expand the network by augmenting each layer of a network by a fixed number of neurons for each task, while keeping the old weights fixed to avoid catastrophic forgetting. Yet, this approach often results in a network with excessive size.  Yoon et al. (2018)  proposed to overcome these limitations via selective retraining of the old network while expanding each of its layer with only the necessary number of neurons, and further alleviate catastrophic forgetting by splitting and duplicating the neurons.  Xu & Zhu (2018)  proposed to use reinforcement learning to decide how many neurons to add.  Li et al. (2019)  proposes to perform an explicit network architecture search to decide how much to reuse the existing network weights and how much to add. Our model also performs dynamic network expansion as the previous expansion-based methods, but instead of adding in new units, it additively decomposes the network parameters into task-shared and task-specific parameters. Further, the capacity increase at the arrival of each task is kept minimal with the sparsity on the task-specific parameters and the growth is logarithmic with the hierarchical structuring of shared parameters.

Section Title: CONTINUAL LEARNING WITH ADDITIVE PARAMETER DECOMPOSITION
  CONTINUAL LEARNING WITH ADDITIVE PARAMETER DECOMPOSITION In a continual learning setting, we assume that we have sequence of tasks {T 1 , . . . , T T } arriving to a deep network in a random order. We denote the dataset of the t th task as D t = {x i t , y i t } Nt i=1 , where x i t and y i t are i th instance and label among N t examples. We further assume that they become inaccessible after step t. The set of parameters for the network at step t is then given as Θ t = {θ l t }, where {θ l t } represents the set of weights for each layer l; we omit the layer index l when the context is clear. Then the training objective at the arrival of task t can be defined as follows: minimize Θt L (Θ t ; Θ t−1 , D t )+λR(Θ t ), where R(·) is a regularization term on the model parameters. In the next paragraph, we introduce our continual learning framework with task-adaptive parameter decomposition and hierarchical knowledge consolidation.

Section Title: Additive Parameter Decomposition
  Additive Parameter Decomposition To minimize the effect of catastrophic forgetting and the amount of newly introduced parameters with network expansion, we propose to decompose θ into a task-shared parameter matrix σ and a task-adaptive parameter matrix τ , that is, θ t = σ ⊗ M t + τ t for task t, where the masking variable M t acts as an attention on the task-shared parameter to guide the learner to focus only on the parts relevant for each task. This decomposition allows us to easily control the trade-off between semantic drift and predictive performance of a new task by imposing separate regularizations on decomposed parameters. When a new task arrives, we encourage the shared parameters σ to be properly updated, but not deviate far from the previous shared parameters σ (t−1) . At the same time, we enforce the capacity of τ t to be as small as possible, by making it sparse. The objective function for this decomposed parameter model is given as follows: where L denotes a loss function, σ (t−1) denotes the shared parameter before the arrival of the current task t, · 1 indicates an element-wise 1 norm defined on the matrix, and λ 1 , λ 2 are hyperparameters balancing efficiency catastrophic forgetting. We use 2 transfer regularization to prevent catastrophic forgetting, but we could use other types of regularizations as well, such as Elastic Weight Consolidation ( Kirkpatrick et al., 2017 ). The masking variable M t is a sigmoid function with a learnable parameter v t , which is applied to output channels or neurons of σ in each layer. We name our model with decomposed network parameters, Additive Parameter Decomposition (APD). The proposed decomposition in (1) makes continual learning efficient, since at each task we only need to learn a very sparse τ t that accounts for task-specific knowledge that cannot be explained with the transformed shared knowledge σ ⊗ M t . Thus, in a way, we are doing residual learning with τ t . Further, it helps the model achieve robustness to the task arrival order, because semantic drift occurs only through the task-shared parameter that corresponds to generic knowledge, while the task-specific knowledge learned from previous tasks are kept intact. In the next section, we introduce additional techniques to achieve even more task-order robustness and efficiency.

Section Title: Order Robust Continual Learning with Retroactive Parameter Updates
  Order Robust Continual Learning with Retroactive Parameter Updates We observe that a naive update of the shared parameters may induce semantic drift in parameters for the previously trained tasks which will yield an order-sensitive model, since we do not have access to previous task data. In order to provide high degree of order-robustness, we impose an additional regularization to further prevent parameter-level drift without explicitly training on the previous tasks. To achieve order-robustness in (1), we need to retroactively update task adaptive parameters of the past tasks to reflect the updates in the shared parameters at each training step, so that all previous tasks are able to maintain their original solutions. Toward this objective, when a new task t arrives, we first recover all previous parameters (θ i for task i < t): θ * i = σ (t−1) ⊗ M (t−1) i + τ (t−1) i and then update τ 1:t−1 by constraining the combined parameter σ ⊗ M i + τ i to be close to θ * i . The learning objective for the current task t is then described as follows: Compared to (1), the task-adaptive parameters of previous tasks now can be retroactively updated to minimize the parameter-level drift. This formulation also constrains the update of the task-shared parameters to consider order-robustness. 1: Let σ (1) = θ1, and optimize for the task 1 5: end for 6: Minimize (3) to update σ and {τ i, vi} t

Section Title: Hierarchical Knowledge Consolidation
  Hierarchical Knowledge Consolidation The objective function in (2) does not directly consider local sharing among the tasks, and thus it will inevitably result in the redundancy of information in the task-adaptive parameters. To further minimize the capacity increase, we perform a process called hierarchical knowledge consolidation to group relevant task-adaptive parameters into task- shared parameters (See  Figure 2 ). We first group all tasks into K disjoint sets {G g } K g=1 using K-means clustering on {τ i } t i=1 , then decompose the task-adaptive parameters in the same group into locally-shared parameters σ g and task-adaptive parameters {τ i } i∈Gg (with higher sparsity) by simply computing the amount of value discrepancy in each parameter as follows: where τ i,j denotes the jth element of the ith task-adaptive parameter matrix, and µ g is the cluster center of group G g . We update the locally-shared parameters σ g after the arrival of every s tasks for efficiency, by performing K-means clustering while initializing the cluster centers with the previous locally-shared parameters σ g for each group. At the same time, we increase the number of centroids to K + k to account for the increase in the variance among the tasks. Our final objective function is then given as follows: Algorithm 1 describes the training of our APD model.

Section Title: Selective task forgetting
  Selective task forgetting In practical scenarios, some of earlier learned tasks may become irrelevant as we continually train the model. For example, when we are training a product identification model, recognition of discontinued products will be unnecessary. In such situations, we may want to forget the earlier tasks in order to secure network capacity for later task learning. Unfortunately, existing continual learning methods cannot effectively handle this problem, since the removal of some features or parameters will also negatively affect the remaining tasks as their parameters are entangled. Yet, with APDs, forgetting of a task t can be done by dropping out the task adaptive parameters τ t . Trivially, this will have absolutely no effect on the task-adaptive parameters of the remaining tasks.

Section Title: EXPERIMENT
  EXPERIMENT We now validate APD-Net on multiple datasets against state-of-the-art continual learning methods.

Section Title: DATASETS
  DATASETS 1) CIFAR-100 Split ( Krizhevsky & Hinton, 2009 ) consists of images from 100 generic object classes. We split the classes into 10 group, and consider 10-way multi-class classification in each group as a single task. We use 5 random training/validation/test splits of 4, 000/1, 000/1, 000 samples. 2) CIFAR-100 Superclass consists of images from 20 superclasses of the CIFAR-100 dataset, where each superclass consists of 5 different but semantically related classes. For each task, we use 5 random training/validation/test splits of 2, 000/500/500 samples. 3) Omniglot-rotation ( Lake et al., 2015 ) contains OCR images of 1, 200 characters (we only use the training set) from various writing systems for training, where each class has 80 images, including 0, 90, 180, and 270 degree rotations of the original images. We use this dataset for large-scale continual learning experiments, by considering the classification of 12 classes as a single task, obtaining 100 tasks in total. For each class, we use 5 random training/test splits of 60/20 samples. We use a modified version of LeNet-5 ( LeCun et al., 1998 ) and VGG16 network ( Simonyan & Zisserman, 2015 ) with batch normalization as base networks. For experiments on more datasets, and detailed descriptions of the architecture and task order sequences, please see the supplementary file.

Section Title: BASELINES AND OUR MODELS
  BASELINES AND OUR MODELS 1) L2-Transfer. Deep neural networks trained with the L2-transfer regularizer λ θ t − θ t−1 2 F when training for task t. 2) EWC. Deep neural networks regularized with Elastic Weight Consolida- tion ( Kirkpatrick et al., 2017 ). 3) P&C. Deep neural networks with two-step training: Progress, and Compresss ( Schwarz et al., 2018 ). 4) PGN. Progressive Neural Networks ( Rusu et al., 2016 ) which constantly increase the network size by k neurons with each task. 5) DEN. Dynamically Expandable Networks ( Yoon et al., 2018 ) that selectively retrain and dynamically expand the network size by introducing new units and duplicating neurons with semantic drift. 6) RCL. Reinforced Continual Learning proposed in ( Xu & Zhu, 2018 ) which adaptively expands units at each layer using reinforce- ment learning. 7) APD-Fixed. APD-Net without the retroactive update of the previous task-adaptive parameters (Eq. (1)). 8) APD(1). Additive Parameter Decomposition Networks with depth 1, whose parameter is decomposed into task-shared and task-adaptive parameters. 10) APD(2). APD-Net with depth 2, that also has locally shared parameters from hierarchical knowledge consolidation.

Section Title: QUANTITATIVE EVALUATION
  QUANTITATIVE EVALUATION

Section Title: Task-average performance
  Task-average performance We first validate the final task-average performance after the comple- tion of continual learning. To perform fair evaluation of performance that is not order-dependent, we report the performance on three random trials over 5 different task sequences over all experiments.  Table 1  shows that APD-Nets outperform all baselines by large margins in accuracy. We attribute this performance gain to two features. First, an APD-Net uses neuron(filter)-wise masking on the shared parameters, which allows it to focus only on parts that are relevant to the task at the current training stage. Secondly, an APD-Net updates the previous task-adaptive parameters to reflect the changes made to the shared parameters, to perform retroactive knowledge transfer. APD-Fixed, without these retroactive updates, performs slightly worse. APD(2) outperforms APD(1) since it further allows local knowledge transfer with hierarchically shared parameters. Moreover, when compared with expansion based baselines, our methods yield considerably higher accuracy with lower capacity ( Figure 3 ). This efficiency comes from the task-adaptive learning performing only residual learning for each task with minimal capacity increase, while maximally utilizing the task-shared parameters. We further validate the efficiency of our methods in terms of training time. Existing approaches with network expansion are slow to train. DEN should be trained with multiple steps, namely selective retraining, dynamic network expansion and split/duplication, each of which requires retraining of the network. RCL is trained with reinforcement learning, which is inherently slow since the agent should determine exactly how many neurons to add at each layer in a discrete space. PGN trains much faster, but the model increases the fixed number of neurons at each layer when a new task arrives, resulting in overly large networks. On the contrary, APD-Net, although it requires updates to the previous task-adaptive parameters, can be trained in a single training step.  Figure 3  shows that both APD(1) and APD(2) have training time comparable to the base model, with only a marginal increase.

Section Title: Order fairness in continual learning
  Order fairness in continual learning We now evaluate the order-robustness of our model in comparison to the existing approaches. We first define an evaluation metric for order-sensitivity for each task t, which we name as Order-normalized Performance Disparity (OPD), as the disparity between its performance on R random task sequences: where P r t denotes the performance of task t to the task sequence r. Then we define the Maximum OPD as M OP D = max(OP D 1 , ..., OP D t ), and the Average OPD as AOP D = 1 T T t=1 OP D t , to evaluate order-robustness on the entire task set. A model that is sensitive to the task sequence order will have high MOPD and AOPD, and an order-robust model will have low values for both metrics. In  table 1 , we show the experimental results on order-robustness for all models, obtained on 5 random sequences. We observe that expansion-based continual learning methods are more order-robust than fixed-capacity methods, owing to their ability to introduce task-specific units, but they still suffer from a large degree of performance disparity due to asymmetric direction of knowledge transfer from earlier tasks to later ones. On the other hand, APD-Nets obtain significantly lower MOPD and AOPD compared to baseline models that have high performance disparity between task sequences given in different orders. APD(1) and APD(2) are more order-robust than APD-Fixed, which suggests the effectiveness of the retroactive updates of τ 1:t−1 .  Figure 4  further shows how the per-task performance of each model changes to task sequences of three different orders. We observe that our models show the least disparity in performance to the order of the task sequence.

Section Title: Preventing catastrophic forgetting
  Preventing catastrophic forgetting We show the effectiveness of APD on its prevention of catas- trophic forgetting by examining how the model performance on earlier tasks change as new tasks arrive.  Figure 5, (a)-(c)  show the results on task 1, 6, 11 from CIFAR-100 Superclass, which has 20 tasks in total. APD-Nets do not show any sign of catastrophic forgetting, although their performances marginally change with the arrival of each task. In fact, APD(2) even improves on task 6 (by 0.40%p) as it learns on later tasks, which is possible both due to the update of the shared parameters and the retroactive update of the task-adaptive parameters for earlier tasks, which leads to better solutions.

Section Title: Selective task forgetting
  Selective task forgetting To show that APD-Net can perform selective task forgetting without any harm on the performance of non-target tasks, in  Figure 5, (d)-(e) , we report the performance change in Task 1-5 when removing parameters for Task 3 and 5. As shown, there is no performance degeneration on non-target tasks, which is expected since dropping out a task-adaptive parameter for a specific task will not affect the task-adaptive parameters for the remaining tasks. This ability to selectively forget is another important advantage of our model that makes it practical in lifelong learning scenarios.

Section Title: Scalability to large number of tasks
  Scalability to large number of tasks We further validate the scalability of our model with large- scale continual learning experiments on the Omniglot-Rotation dataset, which has 100 tasks. Re- gardless of random rotations, tasks could share specific features such as circles, curves, and straight lines.  Gidaris et al. (2018)  showed that we can learn generic representations even with rotated images, where they proposed a popular self-supervised learning technique where they train the model to predict the rotation angle of randomly rotated images. We do not compare against DEN or RCL for this experiment since they are impractically slow to train.  Figure 6  (Left) shows the results of this experiment. For PGN, we restrict the maximum number of links to the adapter to 3 in order to avoid it from establishing exponentially many connections. We observe that continual learning models achieve significantly lower performance and high OPDs compared to single task learning. On the contrary, our model outperforms them by large amount, obtaining performance that is almost equal to STL which uses 100 times more network parameters. To show that our model scales well, we plot the number of parameters for our models as a function of the number of tasks in  Figure 6  (Right). The plot shows that our APD-Net scales well, showing logarithmic growth in network capacity (the number of parameters), while PGN shows linear growth. This result suggests that our model is highly efficient especially in large-scale continual learning scenarios.

Section Title: Continual learning with heterogenerous datasets
  Continual learning with heterogenerous datasets We further consider a more challenging con- tinual learning scenario where we train on a series of heterogeneous datasets. For this experiment, we use CIFAR-10 ( Krizhevsky & Hinton, 2009 ), CIFAR100, and the Street View House Numbers (SVHN) ( Netzer et al., 2011 ) dataset, in two different task arrival sequences (SVHN→CIFAR- 10→CIFAR-100, CIFAR-100→CIFAR-10→SVHN). We use VGG-16 as the base network, and compare against an additional baseline, Piggyback ( Mallya et al., 2018 ), which handles a newly arrived task by learning a task-specific binary mask on a network pretrained on ImageNet; since we cannot assume the availability of such large-scale datasets for pretraining in a general setting, we pretrain it on the inital task.  Table 2  shows the results, which show that existing models obtain Published as a conference paper at ICLR 2020

Section Title: QUALITATIVE ANALYSIS
  QUALITATIVE ANALYSIS As a further qualitative analysis of the effect of APD, we visualize the parameters using our method and baselines by projecting them onto a 2D space ( Figure 7 ). For this experiment, we use a modified MNIST-split dataset whose images are cropped in the center by 8 × 8 pixels, and create 5 tasks, where each task is the binary classification between two classes. As for the base network, we use a 2-layer multi-layer perceptron with 10 units at each layer. Then we use Principle Component Analysis (PCA) to reduce the dimensionality of the parameters to two. We visualize the 2D projections of both the task-shared and task-adaptive parameters for each step of continual learning. For example, for task 3, we plot three green markers which visualize teh parameters when training on task 4 and 5. For the last task (Task 5), we only have a single marker since this is the last task. We observe that the model parameters using L2-Transfer drift away in a new direction, as it trains on a sequence of tasks, which brings in catastrophic forgetting. APD-Fixed (Figure 7(b)) largely alleviates the semantic drift, as the update on later tasks only affects the task-shared parts while the task-adaptive parameters are kept intact. However, the update to the task-shared parameters could result in small drift in the combined task-specific parameters. On the other hand, APD-Net with retroactive update of task-adaptive parameters successfully prevents the drift in the task-specific parameters (Figure 7(c)) .

Section Title: CONCLUSION
  CONCLUSION We proposed a novel continual learning model with Additive Parameter Decomposition, where the task-shared parameters capture knowledge generic across tasks and the task-adaptive parameters capture incremental differences over them to capture task-specific idiosyncrasies. This knowledge decomposition naturally solves the catastrophic forgetting problem since the task-adaptive parameters for earlier tasks will remain intact, and is significantly more efficient compared to expansion-based approaches, since the task-adaptive parameters are additive and do not increase the number of neurons or filters. Moreover, we also introduce and tackle a novel problem we refer to as task order sensitivity, where the performance for each task varies sensitively to the order of task arrival sequence; with our model, the shared parameters will stay relatively static regardless of the task order, and retroactive updates of the task-adaptive parameters prevent them from semantic drift. With extensive experimental validation, we showed that our model obtains impressive accuracy gains over the existing continual learning approaches, while being memory- and computation-efficient, scalable to large number of tasks, and order-robust. We hope that our paper initiates new research directions for continual learning on the relatively unexplored problems of scalability, task-order sensitivity, and selective task forgetting.

```
