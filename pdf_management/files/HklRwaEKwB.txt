Title:
```
Published as a conference paper at ICLR 2020 RIDGE REGRESSION: STRUCTURE, CROSS- VALIDATION, AND SKETCHING
```
Abstract:
```
We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. We study the bias of K-fold cross-validation for choosing the regulariza- tion parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are illustrated by simulations and by analyzing empirical data.
```

Figures/Tables Captions:
```
Figure 1: Ridge regression bias-variance tradeoff. Left: γ = p/n = 0.2; right: γ = 2. The data matrix X has iid Gaussian entries. The coefficient β has distribution β ∼ N (0, I p /p), while the noise ε ∼ N (0, I p ).
Figure 2: Left: Cross-validation on the Million Song Dataset (MSD, Bertin-Mahieux et al., 2011). For the error bar, we take n = 1000, p = 90, K = 5, and average over 90 different sub-datasets. For the test error, we train on 1000 training datapoints and fit on 9000 test datapoints. The debiased λ reduces the test error by 0.00024, and the minimal test error is 0.8480. Right: Cross-validation on the flights dataset Wickham (2018). For the error bar, we take n = 300, p = 21, K = 5, and average over 180 different sub-datasets. For the test error, we train on 300 datapoints and fit on 27000 test datapoints. The debiased λ reduces the test error by 0.0022, and the minimal test error is 0.1353.
Figure 3: Primal orthogonal sketching with n = 500, γ = 5, λ = 1.5, α = 3, σ = 1. Left: MSE of primal sketching normalized by the MSE of ridge regression. The error bar is the standard deviation over 10 repetitions. Right: Bias and variance of primal sketching normalized by the bias and variance of ridge regression, respectively.
Figure 4: Left: Ratio of optimal MSE of marginal regression to that of optimally tuned ridge regres- sion, for three values of γ = p/n, as a function of the SNR α 2 /σ 2 . Right: Gaussian dual sketch when there is no noise. γ = 0.4, α = 1, λ = 1 (both for original and sketching). Standard error over 50 experiments.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Ridge or 2 -regularized regression is a widely used method for prediction and estimation when the data dimension p is large compared to the number of datapoints n. This is especially so in problems with many good features, where sparsity assumptions may not be justified. A great deal is known about ridge regression. It is Bayes optimal for any quadratic loss in a Bayesian linear model where the parameters and noise are Gaussian. The asymptotic properties of ridge have been widely studied (e.g.,  Tulino & Verdú, 2004 ;  Serdobolskii, 2007 ;  Couillet & Debbah, 2011 ;  Dicker, 2016 ;  Dobriban & Wager, 2018 , etc). For choosing the regularization parameter in practice, cross-validation (CV) is widely used. In addition, there is an exact shortcut (e.g.,  Hastie et al., 2009 , p. 243), which has good consistency properties ( Hastie et al., 2019 ). There is also a lot of work on fast approximate algorithms for ridge, e.g., using sketching methods (e.g., el  Alaoui & Mahoney, 2015 ;  Chen et al., 2015 ;  Wang et al., 2018 ;  Chowdhury et al., 2018 , among others). Here we seek to develop a deeper understanding of ridge regression, going beyond existing work in multiple aspects. We work in linear models under a popular asymptotic regime where n, p → ∞ at the same rate ( Marchenko & Pastur, 1967 ;  Serdobolskii, 2007 ;  Couillet & Debbah, 2011 ;  Yao et al., 2015 ). In this framework, we develop a fundamental representation for ridge regression, which shows that it is well approximated by a linear scaling of the true parameters perturbed by noise. The scaling matrices are functions of the population-level covariance of the features. As a consequence, we derive formulas for the training error and bias-variance tradeoff of ridge. Second, we study commonly used methods for choosing the regularization parameter. Inspired by the observation that CV has a bias for estimating the error rate (e.g.,  Hastie et al., 2009 ,  p. 243 ), we study the bias of CV for selecting the regularization parameter. We discover a surprisingly simple form for the bias, and propose a downward scaling bias correction procedure. Third, we study the accuracy loss of a class of randomized sketching algorithms for ridge regression. These algorithms approximate the sample covariance matrix by sketching or random projection. We show they can be surprisingly accurate, e.g., they can sometimes cut computational cost in half, only incurring 5% extra error. Even more, they can sometimes improve the MSE if a suboptimal regularization parameter is originally used.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 Our work leverages recent results from asymptotic random matrix theory and free probability theory. One challenge in our analysis is to find the limit of the trace tr (Σ 1 + Σ −1 2 ) −1 /p, where Σ 1 and Σ 2 are p × p independent sample covariance matrices of Gaussian random vectors. The calculation requires nontrivial aspects of freely additive convolutions (e.g.,  Voiculescu et al., 1992 ;  Nica & Speicher, 2006 ). Our work is connected to prior works on ridge regression in high-dimensional statistics ( Serdobol- skii, 2007 ) and wireless communications ( Tulino & Verdú, 2004 ;  Couillet & Debbah, 2011 ). Among other related works,  El Karoui & Kösters (2011)  discuss the implications of the geometric sensitivity of random matrix theory for ridge regression, without considering our problems. El  Karoui (2018)  and  Dicker (2016)  study ridge regression estimators, but focus only on the risk for identity covari- ance.  Hastie et al. (2019)  study "ridgeless" regression, where the regularization parameter tends to zero. Sketching is an increasingly popular research topic, see  Vempala (2005) ;  Halko et al. (2011) ;  Ma- honey (2011) ;  Woodruff (2014) ;  Drineas & Mahoney (2017)  and references therein. For sketched ridge regression,  Zhang et al. (2013a ;b) study the dual problem in a complementary finite-sample setting, and their results are hard to compare.  Chen et al. (2015)  propose an algorithm combining sparse embedding and the subsampled randomized Hadamard transform (SRHT), proving relative approximation bounds.  Wang et al. (2017)  study iterative sketching algorithms from an optimization point of view, for both the primal and the dual problems.  Dobriban & Liu (2018)  study sketching using asymptotic random matrix theory, but only for unregularized linear regression.  Chowdhury et al. (2018)  propose a data-dependent algorithm in light of the ridge leverage scores. Other related works include  Sarlos (2006) ;  Ailon & Chazelle (2006) ;  Drineas et al. (2006 ;  2011 );  Dhillon et al. (2013) ;  Ma et al. (2015) ;  Raskutti & Mahoney (2016) ;  Gonen et al. (2016) ;  Thanei et al. (2017) ;  Ahfock et al. (2017) ;  Lopes et al. (2018) ;  Huang (2018) . The structure of the paper is as follows: We state our results on representation, risk, and bias- variance tradeoff in Section 2. We study the bias of cross-validation for choosing the regularization parameter in Section 3. We study the accuracy of randomized primal and dual sketching for both orthogonal and Gaussian sketches in Section 4. We provide proofs and additional simulations in the Appendix. Code reproducing the experiments in the paper are available at https://github. com/liusf15/RidgeRegression.

Section Title: RIDGE REGRESSION
  RIDGE REGRESSION We work in the usual linear regression model Y = Xβ + ε, where each row x i of X ∈ R n×p is a datapoint in p dimensions, and so there are p features. The corresponding element y i of Y ∈ R n is its continous response (or outcome). We assume mean zero uncorrelated noise, so Eε = 0, and Cov [ε] = σ 2 I n . We estimate the coefficient β ∈ R p by ridge regression, solving the optimization problemβ = arg min β∈R p 1 n Y − Xβ 2 2 + λ β 2 2 , where λ > 0 is a regularization parameter. The solution has the closed form We work in a "big data" asymptotic limit, where both the dimension p and the sample size n tend to infinity, and their aspect ratio converges to a constant, p/n → γ ∈ (0, ∞). Our results can be interpreted for any n and p, using γ = p/n as an approximation. We recall that the empirical spectral distribution (ESD) of a p×p symmetric matrix Σ is the distribu- tion 1 p p i=1 δ λi where λ i , i = 1, . . . , p are the eigenvalues of Σ, and δ x is the point mass at x. This is a standard notion in random matrix theory, see e.g.,  Marchenko & Pastur (1967) ;  Tulino & Verdú (2004) ;  Couillet & Debbah (2011) ;  Yao et al. (2015) . The ESD is a convenient tool to summarize all information obtainable from the eigenvalues of a matrix. For instance, the trace of Σ is proportional to the mean of the distribution, while the condition number is related to the range of the support. As is common, we will work in models where there is a sequence of covariance matrices Σ = Σ p , and their ESDs converges in distribution to a limiting probability distribution. The results become simpler, because they depend only on the limit.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 By extension, we say that the ESD of the n × p matrix X is the ESD of X X/n. We will consider some very specific models for the data, assuming it is of the form X = U Σ 1/2 , where U has iid entries of zero mean and unit variance. This means that the datapoints, i.e., the rows of X, have the form x i = Σ 1/2 u i , i = 1, . . . , p, where u i have iid entries. Then Σ is the "true" covariance matrix of the features, which is typically not observed. These types of models for the data are very common in random matrix theory, see the references mentioned above. Under these models, it is possible to characterize precisely the deviations between the empirical covariance matrix Σ = n −1 X X and the population covariance matrix Σ, dating back to the well known classical Marchenko-Pastur law for eigenvectors ( Marchenko & Pastur, 1967 ), extended to more general models and made more precise, including results for eigenvectors (see e.g.,  Tulino & Verdú, 2004 ;  Couillet & Debbah, 2011 ;  Yao et al., 2015 , and references therein). This has been used to study methods for estimating the true covariance matrix, with several applications (e.g.,  Paul & Aue, 2014 ;  Bun et al., 2017 ). More recently, such models have been used to study high dimensional statistical learning problems, including classification and regression (e.g.,  Zollanvari & Genton, 2013 ;  Dobriban & Wager, 2018 ). Our work falls in this line. We start by finding a precise representation of the ridge estimator. For random vectors u n , v n of growing dimension, we say u n and v n are deterministic equivalents, if for any sequence of fixed (or random and independent of u n , v n ) vectors w n such that lim sup w n 2 < ∞ almost surely, we have |w n (u n − v n )| → 0 almost surely. We denote this by u n v n . Thus linear combinations of u n are well approximated by those of v n . This is a somewhat non-standard definition, but it turns out that it is precisely the one we need to use prior results from random matrix theory such as from ( Rubio & Mestre, 2011 ). We extend scalar functions f : R → R to matrices in the usual way by functional calculus, applying them to the eigenvalues and keeping the eigenvectors. If M = V ΛV is a spectral decomposition of M , then we define f (M ) := V f (Λ)V , where f (Λ) is the diagonal matrix with entries f (Λ ii ). For a fixed design matrix X, we can write the estimator aŝ However, for a random design, we can find a representation that depends on the true covariance Σ, which may be simpler when Σ is simple, e.g., when Σ = I p is isotropic. Theorem 2.1 (Representation of ridge estimator). Suppose the data matrix has the form X = U Σ 1/2 , where U ∈ R n×p has iid entries of zero mean, unit variance and finite 8 + c-th moment for some c > 0, and Σ = Σ p ∈ R p×p is a deterministic positive definite matrix. Suppose that n, p → ∞ with p/n → γ > 0. Suppose the ESD of the sequence of Σs converges in distribution to a probability measure with compact support bounded away from the origin. Suppose that the noise is Gaussian, and that β = β p is an arbitrary sequence of deterministic vectors, such that lim sup β 2 < ∞. Then the ridge regression estimator is asymptotically equivalent to a random vector with the follow- ing representation:β Here Z ∼ N (0, I p ) is a random vector that is stochastically dependent only on the noise ε, and A, B are deterministic matrices defined by applying the scalar functions below to Σ: Here c p := c(n, p, Σ, λ) is the unique positive solution of the fixed point equation This result gives a precise representation of the ridge regression estimator. It is a sum of two terms: the true coefficient vector β scaled by the matrix A(Σ, λ), and the noise vector Z scaled by the matrix B(Σ, λ). The first term captures to what extent ridge regression recovers the "signal". Morever, the Published as a conference paper at ICLR 2020 noise term Z is directly coupled with the noise in the original regression problem, and thus also the estimator. The result would not hold for an independent noise vector Z. However, the coefficients are not fully explicit, as they depend on the unknown population covari- ance matrix Σ, as well as on the fixed-point variable c p . Some comments are in order: 1. Structure of the proof. The proof is quite non-elementary and relies on random matrix theory. Specifically, it uses the language of the recently developed "calculus of determin- istic equivalents" ( Dobriban & Sheng, 2018 ), and results by ( Rubio & Mestre, 2011 ). A general takeaway is that for n not much larger than p, the empirical covariance matrix Σ is not a good estimator of the true covariance matrix Σ. However, the deviation of linear functionals of Σ, can be quantified. In particular, we have ( Σ + λI) −1 (c p Σ + λI) −1 , in the sense that linear combinations of the entries of the two matrices are close (see the proof for more details). 2. Understanding the resolvent bias factor c p . Thus, c p can be viewed as a resolvent bias factor, which tells us by what factor Σ is multiplied when evaluating the resolvent ( Σ + λI) −1 , and comparing it to its naive counterpart (Σ + λI) −1 . It is known that c p is well defined, and this follows by a simple monotonicity argument, see  Hachem et al. (2007) ;  Rubio & Mestre (2011) . Specifically, the left hand side of (2) is decreasing in c p , while the right hand size is increasing in Also c p is the derivative of c p , when viewing it as a function of z := −λ. An explicit expression is provided in the proof in Section A.1, but is not crucial right now. Here we discuss some implications of this representation. For uncorrelated features, Σ = I p , A, B reduce to multiplication by scalars. Hence, each coor- dinate of the ridge regression estimator is simply a scalar multiple of the corresponding coordinate of β. One can use this to find the bias in each individual coordinate.

Section Title: Training error and optimal regularization parameter
  Training error and optimal regularization parameter This theorem has implications for under- standing the training error, and optimal regularization parameter of ridge regression. As it stands, the theorem itself only characterizes the behavior og linear combinations of the coordinates of the estimator. Thus, it can be directly applied to study the bias Eβ(λ) − β of the estimator. How- ever, it cannot directly be used to study the variance; as that would require understanding quadratic functionals of the estimator. This seems to require significant advances in random matrix theory, going beyond the results of  Rubio & Mestre (2011) . However, we show below that with additional assumptions on the structure of the parameter β, we can derive the MSE of the estimator in other ways.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 We work in a random-effects model, where the p-dimensional regression parameter β is random, each coefficient has zero mean Eβ i = 0, and is normalized so that Varβ i = α 2 /p. This ensures that the signal strength E β 2 = α 2 is fixed for any p. The asymptotically optimal λ in this setting is always λ * = γσ 2 /α 2 see e.g.,  Tulino & Verdú (2004) ;  Dicker (2016) ;  Dobriban & Wager (2018) . The ridge regression estimator with λ = pσ 2 /(nα 2 ) is the posterior mean of β, when β and ε are normal random variables. For a distribution F , we define the quantities θ i (λ) = 1 (x + λ) i dF γ (x), (i = 1, 2, . . .). These are the moments of the resolvent and its derivatives (up to constants). We use the following loss functions: mean squared estimation error: M (β) = E β − β 2 2 , and residual or training error: R(β) = E [ ] Y − Xβ 2 2 . Theorem 2.2 (MSE and training error of ridge). Suppose β has iid entries with Eβ i = 0, Var [β i ] = α 2 /p, i = 1, . . . , p and β is independent of X and ε. Suppose X is an arbitrary n × p matrix depending on n and p, and the ESD of X converges weakly to a deterministic distribution F as n, p → ∞ and p/n → γ. Then the asymptotic MSE and residual error of the ridge regression estimatorβ(λ) has the form

Section Title: Bias-variance tradeoff
  Bias-variance tradeoff Building on this, we can also study the bias-variance tradeoff of ridge regression. Qualitatively, large λ leads to more regularization, and decreases the variance. However, it also increases the bias. Our theory allows us to find the explicit formulas for the bias and variance as a function of λ. See  Figure 1  for a plot and Sec. A.3 for the details. As far as we know, this is one of the few examples of high-dimensional asymptotic problems where the precise form of the bias and variance can be evaluated. Bias-variance tradeoff at optimal λ * = γσ 2 /α 2 . (see Figure 6) This can be viewed as the "pure" effect of dimensionality on the problem, keeping all other parameters fixed, and has intriguing prop- erties. The variance first increases, then decreases with γ. In the "classical" low-dimensional case, most of the risk is due to variance, while in the "modern" high-dimensional case, most of it is due to bias. This is consistent with other phenomena in proportional-limit asymptotics, e.g., that the map between population and sample eigenvalue distributions is asymptotically deterministic ( Marchenko & Pastur, 1967 ).

Section Title: Future applications
  Future applications This fundamental representation may have applications to important statistical inference questions. For instance, inference on the regression coefficient β and the noise variance σ 2 are important and challenging problems. Can we use our representation to develop debiasing techniques for this task? This will be interesting to explore in future work.

Section Title: CROSS-VALIDATION
  CROSS-VALIDATION How can we choose the regularization parameter? In practice, cross-validation (CV) is the most popular approach. However, it is well known that CV has a bias for estimating the error rate, because it uses a smaller number of samples than the full data size (e.g.,  Hastie et al., 2009 ,  p. 243 ). In this section, we study related questions, proposing a bias-correction method for the optimal regularization parameter. This is closely connected to the previous section, because it relies on the same random-effects theoretical framework. In fact, our conclusions here are a direct consequence of the properties of that framework.

Section Title: Setup
  Setup Suppose we split the n datapoints (samples) into K equal-sized subsets, each containing n 0 = n/K samples. We use the k-th subset (X k , Y k ) as the validation set and the other K − 1 subsets (X −k , Y −k ), with total sample size n 1 = (K − 1)n/K as the training set. We find the ridge Published as a conference paper at ICLR 2020 The expected cross-validation error is, for isotropic covariance, i.e., Σ = I, Bias in CV. When n, p tend to infinity so that p/n → γ > 0, and in the random effects model with Eβ i = 0, Varβ i = α 2 /p described above, the minimizer of CV (λ) tends to λ * k =γσ 2 /α 2 , wherẽ γ is the limiting aspect ratio of X −k , i.e.γ = γK/(K − 1). Since the aspect ratios of X −k and X differ, the limiting minimizer of the cross-validation estimator of the test error is biased for the limiting minimizer of the actual test error, which is λ * = γσ 2 /α 2 . Bias-correction. Suppose we have foundλ * k , the minimizer of CV (λ). Afterwards, we usually refit ridge regression on the entire dataset, i.e., find Based on our bias calculation, we propose to use a bias-corrected parameter So if we use 5 folds, we should multiply the CV-optimal λ by 0.8. We find it surprising that this theoretically justified bias-correction does not depend on any unknown parameters, such as β, α 2 , σ 2 .While the bias of CV is widely known, we are not aware that this bias-correction for the regularization parameter has been proposed before. Numerical examples.  Figure 2  shows on two empirical data examples that the debiased estima- tor gets closer to the optimal λ than the original minimizer of the CV. However, in this case it does not significantly improve the test error. Simulation results in Section A.4 also show that the bias-correction correctly shrinks the regularization parameter and decreases the test error. We also consider examples where p n (i.e., γ 1), because this is a setting where it is known that the bias of CV can be large ( Tibshirani & Tibshirani, 2009 ). However, in this case, we do not see a significant improvement.

Section Title: Extensions
  Extensions The same bias-correction idea also applies to train-test validation. In addition, there is a special fast "short-cut" for leave-one-out cross-validation in ridge regression (e.g.,  Hastie et al., Published as a conference paper at ICLR 2020 2009 , p. 243), which has the same cost as one ridge regression. The minimizer converges to λ * ( Hastie et al., 2019 ). However, we think that the bias-correction idea is still valuable, as the idea applies beyond ridge regression: CV selects regularization parameters that are too large. See Sec- tion A.5 for more details and experiments comparing different ways of choosing the regularization parameter.

Section Title: SKETCHING
  SKETCHING A final important question about ridge regression is how to compute it in practice. In this section, we study that problem in the same high-dimensional model used throughout our paper. The computa- tion complexity of ridge regression, O(np min(n, p)), can be intractable in modern large-scale data analysis. Sketching is a popular approach to reducing the time complexity by reducing the sample size and/or dimension, usually by random projection or sampling (e.g.  Mahoney, 2011 ;  Woodruff, 2014 ;  Drineas & Mahoney, 2016 ). Specifically, primal sketching approximates the sample covari- ance matrix X X/n by X L LX/n, where L is an m × n sketching matrix, and m < n. If L is chosen as a suitable random matrix, then this can still approximate the original sample covariance matrix. Then the primal sketched ridge regression estimator iŝ Dual sketching reduces p instead. An equivalent expression for ridge regression isβ = n −1 X XX /n + λI n −1 Y . Dual sketched ridge regression reduces the computation cost of the Gram matrix XX , approximating it by XRR X for another sketching matrix R ∈ R p×d (d < p), soβ The sketching matrices R and L are usually chosen as random matrices with iid entries (e.g., Gaus- sian ones) or as orthogonal matrices. In this section, we study the asymptotic MSE for both or- thogonal (Section 4.1) and Gaussian sketching (Section 4.2). We also mention full sketching, which performs ridge after projecting down both X and Y . In section A.11, we find its MSE. However, the other two methods have better tradeoffs, and we can empirically get better results for the same computational cost.

Section Title: ORTHOGONAL SKETCHING
  ORTHOGONAL SKETCHING First we consider primal sketching with orthogonal projections. These can be implemented by subsampling, Haar distributed matrices, or subsampled randomized Hadamard transforms ( Sarlos, 2006 ). We recall that the standard Marchenko-Pastur (MP) law is the probability distribution which is the limit of the ESD of X X/n, when the n × p matrix X has iid standard Gaussian entries, and n, p → ∞ so that p/n → γ > 0, which has an explicit density ( Marchenko & Pastur, 1967 ;  Bai & Silverstein, 2010 ). Theorem 4.1 (Primal orthogonal sketching). Suppose β has iid entries with Eβ i = 0, Var [β i ] = α 2 /p, i = 1, . . . , p and β is independent of X and ε. Suppose X has iid standard normal entries. We compute primal sketched ridge regression (5) with an m × n orthogonal matrix L (m < n, LL = I m ). Let n, p and m tend to infinity with p/n → γ ∈ (0, ∞) and m/n → ξ ∈ (0, 1). Then the MSE ofβ p (λ) has the limit where θ i (γ, λ) = (x + λ) −i dF γ (x) and F γ is the standard Marchenko-Pastur law with aspect ratio γ.

Section Title: Structure of the proof
  Structure of the proof The proof is in Section A.6, with explicit formulas in Section A.6.1. The θ i are related to the resolvent of the MP law and its derivatives. In the proof, we decompose the MSE as the sum of variance and squared bias, both of which further reduce to the traces of certain random matrices, whose limits are determined by the MP law F γ and λ. The two terms on the RHS of Equation (7) are the limits of squared bias and variance, respectively. There is an additional key step in the proof, which introduces the orthogonal complement L 1 of the matrix L such that L L + L 1 L 1 = I n , which leads to some Gaussian random variables appearing in the proof, and simplifies calculations.

Section Title: Simulations
  Simulations A simulation in  Figure 3  (left) shows a good match with our theory. It also shows that sketching does not increase the MSE too much. In this case, by reducing the sample size to half the original one, we only increase the MSE by a factor of 1.05. This shows sketching can be very effective. We also see in  Figure 3  (right) that variance is compromised much more than bias.

Section Title: Robustness to tuning parameter
  Robustness to tuning parameter The reader may wonder how strongly this depends on the choice of the regularization parameter λ. Perhaps ridge regression works poorly with this λ, so sketching cannot worsen it too much? What happens if we take the optimal λ instead of a fixed one? In experiments in Section A.12 we show that the behavior is quite robust to the choice of regularization parameter. The next theorem states a result for dual orthogonal sketching. Theorem 4.2 (Dual orthogonal sketching). Under the conditions of Theorem 4.1, we compute the dual sketched ridge regression with an orthogonal p × d sketching matrix R (d p, R R = I d ). Let n, p and d go to infinity with p/n → γ ∈ (0, ∞) and d/n → ζ ∈ (0, γ). Then the MSE ofβ d (λ) has the limit

Section Title: Proof structure and simulations
  Proof structure and simulations The proof in Section A.7 follows similar path to the previous one. Hereθ i comes in because of the companion Stieltjes transform of MP law. The simulation results shown in Figure 11 agrees well with our theory. They are similar to the ones before: sketching has favorable properties, and the bias increases less than the variance.

Section Title: Optimal tuning parameters
  Optimal tuning parameters For both primal and dual sketching, the optimal regularization pa- rameter minimizing the MSE seems analytically intractable. Instead, we use a numerical approach in our experiments, based on a binary search. Since this is one-dimensional problem, there are no numerical issues. See Figure 13 in Section A.12.3.

Section Title: EXTREME PROJECTION - MARGINAL REGRESSION
  EXTREME PROJECTION - MARGINAL REGRESSION It is of special interest to investigate extreme projections, where the sketching dimension is much reduced compared to the sample size, so m n. This corresponds to ξ = 0. This can also be viewed as a scaled marginal regression estimator, i.e.,β ∝ X Y . For dual sketching, the same case can be recovered with ζ = 0. Another interest of studying this special case is that the formula for MSE simplifies a lot. The proof is in Section A.8. When is the optimal MSE of marginal regression small? Compared to the MSE of the zero estimator α 2 , it is small when γ(σ 2 /α 2 + 1) + 1 is large. In  Figure 4  (left), we compare marginal and ridge regression for different aspect ratios and SNR. When the signal to noise ratio (SNR) α 2 /σ 2 is small or the aspect ratio γ is large, marginal regression does not increase the MSE much. As a concrete example, if we take α 2 = σ 2 = 1 and γ = 0.7, the marginal MSE is 1 − 1/2.4 ≈ 0.58. The optimal ridge MSE is about 0.52, so their ratio is only ca. 0.58/0.52 ≈ 1.1. It seems quite surprising that a simple-minded method like marginal regression can work so well. However, the reason is that when the SNR is small, we cannot expect ridge regression to have good performance. Large γ can also be interpreted as small SNR, where ridge regression works poorly and sketching does not harm performance too much.

Section Title: GAUSSIAN SKETCHING
  GAUSSIAN SKETCHING In this section, we study Gaussian sketching. The following theorem states the bias of dual Gaussian sketching. The bias is enough to characterize the performance in the high SNR regime where α/σ → ∞, and we discuss the extension to low SNR after the proof. Theorem 4.4 (Bias of dual Gaussian sketch). Suppose X is an n × p standard Gaussian random matrix. Suppose also that R is a p × d matrix with i.i.d. N (0, 1/d) entries. Then the bias of dual sketch has the expression Bias 2 (β d ) = α 2 + α 2 /γ · [m (z) − 2m(z)] | z=0 , where m is a function described below, and m (z) denotes the derivative of m w.r.t. z. Below, we use the branch of the square root with positive imaginary part. The function m is characterized by its inverse function, which has the explicit formula m −1 (z) = 1/[1 + z/ζ] − [γ + 1 − (γ − 1) 2 + 4λz]/(2z) for complex z with positive imaginary part.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 About the proof. The proof is in Section A.9.We mention that the same result holds when the matrices involved have iid non-Gaussian entries, but the proof is more technical. The current proof is based on free probability theory (e.g.,  Voiculescu et al., 1992 ;  Hiai & Petz, 2006 ;  Couillet & Debbah, 2011 ). The function m is the Stieltjes transform of the free additive convolution of a standard MP law F 1/ξ and a scaled inverse MP law λ/γ · F −1 1/γ (see the proof).

Section Title: Numerics
  Numerics To evaluate the formula, we note that m −1 (m(0)) = 0, so m(0) is a root of m −1 . Also, dm(0)/dz equals 1/(dm −1 (y)/dy| y=m(0) ), the reciprocal of the derivative of m −1 evaluated at m(0). We use binary search to find the numerical solution. The theoretical result agrees with the simulation quite well, see  Figure 4 . Somewhat unexpectedly, the MSE of dual sketching can be below the MSE of ridge regression, see  Figure 4 . This can happen when the original regularization parameter is suboptimal. As d grows, the MSE of Gaussian dual sketching converges to that of ridge regression. We have also found the bias of primal Gaussian sketching. However, stating the result requires free probability theory, and so we present it in the Appendix, see Theorem A.1. To further validate our results, we present additional simulations in Sec. A.12, for both fixed and optimal regularization parameters after sketching. A detailed study of the computational cost for sketching in Sec. A.13 concludes, as expected, that primal sketching can reduce cost when p < n, while dual sketching can reduce it when p > n; and also provides a more detailed analysis.

```
