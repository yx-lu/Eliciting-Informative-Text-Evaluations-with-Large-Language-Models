Title:
```
Under review as a conference paper at ICLR 2020 LIPSCHITZ LIFELONG REINFORCEMENT LEARNING
```
Abstract:
```
We consider the problem of knowledge transfer when an agent is facing a series of Reinforcement Learning (RL) tasks. We introduce a novel metric between Markov Decision Processes and establish that close MDPs have close optimal value functions, that is that optimal value functions are Lipschitz continuous with respect to tasks. These theoretical results lead us to a value transfer method for Lifelong RL, which we use to build a PAC-MDP algorithm with improved convergence rate. We illustrate the benefits of the method in Lifelong RL experiments.
```

Figures/Tables Captions:
```
Figure 1: Experimental results our study with the LRMaxQInit algorithm. Similarly, LRMaxQInit(x) consists in the latter algorithm, benefiting from prior knowledge D max = x.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Lifelong Reinforcement Learning (RL) is an online problem where an agent faces a series of RL tasks, drawn sequentially. Transferring the knowledge of prior experience while solving new tasks is a key question in that setting ( Lazaric, 2012 ;  Taylor and Stone, 2009 ). We elaborate on the intuitive idea that similar tasks should allow a large amount of transfer. An agent able to compute online a similarity measure between source tasks and the current target task should be able to perform transfer. By measuring the amount of inter-task similarity, we design a novel method for value transfer, practically deployable in the online Lifelong RL setting. Specifically, we introduce a metric between MDPs and prove that the optimal Q-value function is Lipschitz continuous with respect to MDPs. This property allows to compute a provable upper-bound on the optimal value function of a target task, given the learned optimal value function of a source task. Knowing this upper bound allows to accelerate the convergence of an RMax algorithm ( Brafman and Tennenholtz, 2002 ). This transfer method is non-negative (it cannot cause performance degradation) as the computed upper bound does not underestimate the optimal Q-value function. Our contributions are as follows. First, we study theoretically the Lipschitz continuity of the optimal value function in the task space (Section 3). Then, we use this continuity property to propose a value-transfer method based on a local distance between MDPs (Section 4). Full knowledge of both MDPs is not required and the transfer is non-negative, which makes the method both practical and safe. In Section 4.2, we build a PAC-MDP algorithm called Lipschitz RMax, applying this transfer method online in the Lifelong RL setting. We provide sample and computational complexity bounds and showcase the algorithm in Lifelong RL experiments (Section 5).

Section Title: BACKGROUND AND RELATED WORK
  BACKGROUND AND RELATED WORK Reinforcement Learning (RL) ( Sutton and Barto, 1998 ) is a framework for sequential decision making. The problem is typically modeled as a Markov Decision Process (MDP) ( Puterman, 2014 ) consisting in a 4-tuple S, A, R, T where S is a state space, A an action space, R a s is the expected reward of taking action a in state s and T a ss is the transition probability of reaching state s when taking action a in state s. Without loss of generality, we assume R a s ∈ [0, 1]. Given a discount factor γ ∈ [0, 1), the expected cumulative return t γ t R at st obtained along a trajectory starting with state s and action a is noted Q(s, a) and called the Q-function. The optimal Q-function Q * is the highest attainable expected return from s, a and V * (s) = max a∈A Q * (s, a) is the optimal value function in s. Lifelong RL ( Silver et al., 2013 ;  Brunskill and Li, 2014 ) is the problem of experiencing online a series of MDPs drawn from an unknown distribution. Each time an MDP is sampled, a classical RL problem takes place where the agent is able to interact with the environment to maximize its expected return. In this setting, it is reasonable to think that knowledge gained on previous MDPs could be re-used to improve the performance in new MDPs. In this paper, we provide a novel method for such Under review as a conference paper at ICLR 2020 transfer by characterizing the way the optimal Q-function can evolve across tasks. We restrict the scope of the study to the case where sampled MDPs share the same state-action space S × A. For brevity, we will refer indifferently to MDPs, models or tasks, and write them M = R, T . Using a metric between MDPs has the appealing characteristic of quantifying the amount of similarity between tasks, which intuitively should be linked to the amount of transfer achievable.  Song et al. (2016)  define a metric based on the bi-simulation metric introduced by  Ferns et al. (2004)  and the Wasserstein metric ( Villani, 2008 ). Value transfer is performed between states with low bi-simulation distances. However, this metric requires knowing both MDPs completely and is thus unusable in the Lifelong RL setting where we expect to perform transfer before having learned the current MDP. Further, the transfer technique they propose does allow negative transfer (see Appendix, Section A).  Carroll and Seppi (2005)  also define a value-transfer method based on a measure of similarity between tasks. However, this measure is not computable online and thus not applicable to the Lifelong RL setting.  Mahmud et al. (2013)  and  Brunskill and Li (2013)  propose MDP clustering methods respectively using a metric quantifying the regret of running the optimal policy of one MDP in the other MDP and the L 1 norm between the MDP models. An advantage of clustering is to prune the set of possible source tasks. They use their approach for policy transfer, which differs from the value-transfer method proposed in this paper.  Ammar et al. (2014)  learn the model of a source MDP and view the prediction error on a target MDP as a dissimilarity measure in the task space. Their method makes use of samples from both tasks and is not readily applicable to the online setting considered in this paper.  Lazaric et al. (2008)  provide a practical method for sample transfer, computing a similarity metric reflecting the probability of the models to be identical. Their approach is applicable in a batch RL setting as opposed to our online setting. The approach developed by Sorg and Singh (2009) is very similar to ours in the sense that they prove bounds on the optimal Q-function for new tasks, assuming that both MDPs are known and that a soft homomorphism exists between the state spaces.  Brunskill and Li (2013)  also provide a method that can be used for Q-function bounding in multi-task RL.  Abel et al. (2018)  present the MaxQInit algorithm, providing transferred bounds on the Q-function with high probability while preserving PAC-MDP guarantees. Given a set of solved tasks, they derive the probability that the maximum over the Q-values of previous MDPs is an upper bound on the current task's optimal Q-function. This results in a method for non-negative transfer with high probability once enough tasks have been sampled.

Section Title: LIPSCHITZ CONTINUITY OF Q-FUNCTIONS
  LIPSCHITZ CONTINUITY OF Q-FUNCTIONS The intuition we build on is that similar MDPs should have similar optimal Q-functions. Formally, this insight can be translated into a continuity property of the optimal Q-functions over the MDP space M. The remainder of this section mathematically formalizes this intuition that will be used in the next Section to derive a practical method for value transfer. To that end, we introduce a local pseudo-metric characterizing the distance between the models of two MDPs at a particular state-action pair. A reminder and a detailed discussion on the metrics (and related objects) used herein can be found in the Appendix, Section B. Definition 1. Given two tasks M = R, T andM = R ,T , and a function f : S → R + , we define the pseudo-metric between models at (s, a) ∈ S × A w.r.t. f as: This pseudo-metric is relative to a positive function f . We implicitly cast this definition in the context of discrete state spaces. The extension to continuous spaces is straightforward but beyond the scope of this paper. Let Q * M denote the optimal Q-function of MDP M ∈ M. with the MDPs local pseudo-metric ∆ MM (s, a) min dM M (s, a), d M M (s, a) , and the local MDP dissimilarity dM M : S × A → R is the unique solution to the following fixed-point equation for d: Under review as a conference paper at ICLR 2020 All the proofs of the paper can be found in the Appendix. This result establishes that the distance between the optimal Q-functions of two MDPs at (s, a) ∈ S × A is controlled by a local dissimilarity between the MDPs. The latter follows a fixed-point equation (Equation 3), which can be solved by Dynamic Programming (DP) ( Bellman, 1957 ). Note that, although the local MDP dissimilarity dM M is asymmetric, ∆ MM (s, a) is a pseudo-metric, hence the name pseudo-Lipschitz continuity. Similar results for the value function of a fixed policy and the optimal value function V * M can easily be derived, as well as a global pseudo-Lipschitz continuity property (Appendix, Sections D and E). Thus, overall, the optimal Q-functions of two close MDPs (in the sense of Equation 1) are themselves close to each other. A direct consequence in Lifelong RL is that previously solved tasks can help bound the value function of a new task. Even a partially learned Q-function can be used for that purpose if error bounds are known or if it provably overestimates the true Q * . The next section exploits this property to build a PAC-MDP algorithm that performs provably non-negative transfer between successive tasks and accelerates learning.

Section Title: TRANSFER USING THE LIPSCHITZ CONTINUITY
  TRANSFER USING THE LIPSCHITZ CONTINUITY A purpose of value transfer, when interacting online with a new MDP, is to initialize the value function and drive the exploration to accelerate learning. We aim to exploit value transfer in a method guaranteeing three conditions: C1. the resulting algorithm is PAC-MDP ( Strehl et al., 2009 ); C2. the transfer accelerates learning; C3. the transfer is non-negative. From Proposition 1, one can naturally define a local upper bound on the optimal Q-function of an MDP given the optimal Q-function of another MDP. Definition 2. Given two tasks M andM , for all (s, a) ∈ S × A, the Lipschitz upper bound on Q * M induced by Q *M is defined as UM (s, a) ≥ Q * M (s, a) with: The optimism in the face of uncertainty principle leads to consider that the long-term expected return from any state is the 1 1−γ maximum return, unless proven otherwise. The RMax algorithm ( Brafman and Tennenholtz, 2002 ) in particular explores an MDP so as to shrink this upper bound. RMax is a model-based, online RL algorithm with PAC-MDP guarantees ( Strehl et al., 2009 ) which means that convergence to near-optimal policy is guaranteed in a polynomial number of steps. It relies on an optimistic model initialization that yields an optimistic upper bound U on the optimal Q-function, then acts greedily w.r.t. U . By default, it takes the maximum value U (s, a) = 1 1−γ but any tighter upper bound is admissible. Thus, shrinking U with Equation 4 is expected to improve the learning speed for new tasks in Lifelong RL. In RMax, during the resolution of a task M , S × A is split into a subset of known state-action pairs K and its complement K c of unknown pairs. A state-action pair is known if the number of collected reward and transition samples allows estimating an -accurate model in L 1 -norm with probability higher than 1 − δ. We refer to and δ as the RMax precision parameters. This translates into a threshold n known on the number of visits n(s, a) to a pair s, a that are necessary to reach this precision. Given the experience of a set of m MDPsM = {M 1 , . . . ,M m }, we define the total bound as the minimum over all the Lipschitz bounds induced by each previous MDP. Proposition 2. Given a partially known task M = R, T , the set of known state-action pairs K, and the set of Lipschitz bounds on Q * M induced by previous tasks UM 1 , . . . , UM m , the function Q defined below is an upper bound on Q * M for all s, a ∈ S × A. Traditionally in RMax, Equation 5 is solved to a precision Q via Value Iteration. This yields a function Q that is a valid heuristic (provable upper bound on Q * M ) for the exploration of MDP M . The key issue addressed in this Section is how to actually compute U (s, a). Consider two tasks M andM , on which vanilla RMax has been applied, yielding the respective sets of known state-action pairs K andK, along with the learned modelsM = T ,R andM = T ,R , and the upper bounds Q andQ respectively on Q * M and Q *M . Equation 5 allows the transfer of knowledge fromM to M if UM (s, a) can be computed. Unfortunately, the true optimal value functions, transition and reward models, necessary to compute UM , are unknown. Thus, we propose to compute a looser upper bound based on the learned models and value functions. First, we provide an upper boundD MM on the pseudo metric between models M andM . Proposition 3. Given two tasks M andM , K andK the respective sets of state-action pairs where their models are known with accuracy in L 1 -norm with probability at least 1 − δ, with the following definition of the upper bound on the pseudo-metric between modelsD MM : This upper boundD MM on the distance between MDPs can be calculated analytically (see Appendix, Section H). The magnitude of the B term is controlled by . In the case where no information is available on the maximum value ofV , B = 1−γ . measures the accuracy with which the tasks are known: the smaller , the tighter the B bound. Note thatV is used as an upper bound on the true V * M . In many cases, max s V * M (s ) 1 1−γ ; e.g. for stochastic shortest path problems, which feature rewards only upon reaching terminal states, max s V * M (s ) = 1 and thus B = (1 + γ) is a tighter bound for transfer. UsingD MM and Equation 3, one can derive an upper bounddM M on dM M , detailed in Proposition 4. Proposition 4. Given two tasks M andM , K the set of state-action pairs for which R, T is known with accuracy in L 1 -norm with probability at least 1 − δ. If γ(1 + ) < 1, the solutiondM M of the following fixed-point equation ond is an upper bound on dM M with probability at least 1 − δ: Similarly as in Proposition 3, the condition γ(1 + ) < 1 illustrates the fact that for a large return horizon (large γ), a high accuracy (small ) is needed for the bound to be computable. Finally, a tractable upper bound on Q * M givenM with high probability is given bŷ And the associated upper bound on U (s, a) (Equation 5) given previous tasksM = {M i } m i=1 iŝ This upper bound can be used to transfer knowledge from a partially solved task to a target task. IfÛ (s, a) ≤ 1 1−γ for some (s, a) pairs, then the convergence rate can be improved. As complete knowledge of both tasks is not needed, it can be applied online in a Lifelong RL setting. In the next section, we explicit an algorithm that leverages this value transfer method. ComputeÛ (Eq. 9) Compute and return Q (DP on Eq. 5 usingÛ )

Section Title: LIPSCHITZ RMAX
  LIPSCHITZ RMAX In Lifelong RL, MDPs are encountered sequentially. Applying RMax to task M yields the set of known state-action pairs K, the learned modelsT andR, and the upper bound Q on Q * M . Saving this information when the task changes allows to compute the upper bound of Equation 9 for the new task, and to use it to shrink the optimistic heuristic of RMax. This effectively transfers value functions between tasks based on task similarity. As the new task is explored online, the task similarity is progressively assessed with better confidence, refining the values ofD MM ,dM M and eventuallyÛ , allowing for more efficient transfer where the task similarity is appraised. The resulting algorithm, Lipschitz RMax (LRMax), is presented in Algorithm 1. To avoid ambiguities withM, we useM to store learned features (T ,R, K, Q) about previous MDPs. In a nutshell, the behavior of LRMax on a given task M is precisely that of RMax, but with a tighter admissible heuristicÛ that becomes better as the new task is explored (while this heuristic remains constant in vanilla RMax). LRMax is PAC-MDP (Condition C1) as stated in Propositions 5 and 6 below. With S = |S| and A = |A|, the sample complexity of vanilla RMax isÕ(S 2 A/( 3 (1 − γ) 3 )), which is improved by LRMax in Proposition 5 and meets Condition C2. FinallyÛ is a proved upper bound with high probability on Q * M , which avoids negative transfer and meets Condition C3. samples (when logarithmic factors are ignored), withÛ defined in Equation 9 a non-static, decreasing quantity, upper bounded by 1 1−γ . Consequently from Proposition 5, the sample complexity of LRMax is no worse than that of RMax. Proposition 6 (Computational complexity). The total computational complexity of Lipschitz RMax is with τ the number of interaction steps, Q the precision of value iteration and N the number of tasks.

Section Title: REFINING LRMAX BOUNDS WITH MAXIMUM MODEL DISTANCE
  REFINING LRMAX BOUNDS WITH MAXIMUM MODEL DISTANCE LRMax relies on upper bounds on the local distances between tasks (Equation 7). The quality of the Lipschitz bound on Q * M greatly depends on the quality of those estimates and can be improved accordingly. We discuss two methods to provide finer estimates. First, from the definition of D MM γV * M (s, a), it is easy to show that model pseudo-distances are al- ways upper bounded by 1+γ 1−γ . However, in practice, the tasks experienced in Lifelong RL might not cover the full span of possible MDPs and may be systematically closer to each other than 1+γ 1−γ . For instance, the distance between two games in the Arcade Learning Environment (ALE) ( Bellemare et al., 2013 ), is smaller than the maximum distance between any two MDPs defined on the common state-action space of the ALE (extended discussion in Appendix, Section L). Let D max (s, a) max M,M ∈M 2 {D MM γV * M (s, a)} be the maximum model distance at a particular s, a pair. Prior knowledge might indicate a smaller upper bound for D max (s, a) than 1+γ 1−γ . We will note such an upper bound D max . Solving Equation 7 boils down to accumulatingD MM (s, a) values ind(s, a). Reducing aD MM (s, a) estimate in a single (s, a) pair actually reducesd(s, a) in all (s, a) pairs. Thus, replacingD MM (s, a) in Equation 7 by min{D max ,D MM (s, a)}, provides a smaller upper bounddM M on dM M , and thus a smallerÛ which allows transfer if it is lesser than 1 1−γ . Consequently, such an upper bound D max can make a difference between successful and unsuccessful transfer, even if its value is of little importance. Conversely, setting a value for D max quantifies the distance between MDPs where transfer is efficient. Furthermore, one can estimate online the value of D max (s, a), lifting the previous hypothesis of available prior knowledge. One can build an empirical estimate of the maximum model distance at s, a: D max (s, a) max M,M ∈M 2 {D MM (s, a)},M being the set of explored tasks. The pitfall being that, with few explored tasks,D max (s, a) could underestimate D max (s, a). Proposition 7 provides a lower bound on the probability thatD max (s, a) does not underestimate D max (s, a), depending on the number of sampled tasks. In turn this indicates whenD max (s, a) upper bounds D max (s, a) with high probability, which can be combined with Algorithm 1 to improve the performance. Proposition 7. Consider an algorithm producing -accurate in L 1 -norm model estimates with probability at least 1 − δ for a subset of S × A after interacting with an MDP. For all s, a ∈ S × A, after sampling m tasks with p min = min M ∈M Pr(M ), the following lower bound holds: The assumption of a lower bound p min on the sampling probability of a task implies that M is finite and is commonly seen as a non-adversarial task sampling strategy ( Abel et al., 2018 ).

Section Title: EXPERIMENTS
  EXPERIMENTS The experiments reported here 1 illustrate how 1) LRMax allows for early performance increase in Lifelong RL by efficiently transferring knowledge between tasks; 2) the Lipschitz bound of Equation 8 improves the sample complexity compared to RMax by providing a tighter upper bound on Q * . Graphs are displayed with 95% confidence intervals. Information in line with the Machine Learning Reproducibility Check-list ( Pineau, 2019 ) is documented in the Appendix, Section Q. We evaluate different variants of LRMax in a Lifelong RL experiment. The RMax algorithm will be used as a no-transfer baseline. LRMax(x) denotes Algorithm 1 with prior D max = x. MaxQInit denotes the MAXQINIT algorithm from  Abel et al. (2018) , consisting in a state-of-the art PAC-MDP algorithm achieving transfer with PAC guarantees. Both LRMax and MaxQInit algorithms achieve value transfer by providing a tighter upper-bound on Q * than 1 1−γ . Computing both upper-bounds and taking the minimum results in combining the two approaches. We include such a combination in 1 Link to open-source code omitted for anonymity. The environment we used in all experiments is a variant of the "tight" environment used by  Abel et al. (2018) . This is a 11 × 11 grid-world, the initial state is in the centre, actions are the cardinal moves (Appendix, Section M). The reward is zero everywhere except for the three goal cells in the upper-right corner. Each time a task is sampled, a new reward value is drawn from [0.8, 1] for each of the three goal cells and a probability of slipping (performing a different action than the one selected) is picked in [0, 0.1]. Hence, tasks have different reward and transition functions. We sample 15 tasks in sequence among a pool of 5 possible different sampled tasks. Each is run for 2000 episodes of length 10. The operation is repeated 10 times to provide narrow confidence intervals. We used n known = 10, δ = 0.05 and = 0.01 (discussion in Appendix, Section P). We drew tasks from a finite set of five MDPs. This allows the application of MaxQInit and the subsequent comparison below. Note, however, that LRMax does not require the set of MDPs to be finite, which is a noticeable advantage in applicability. Other lifelong RL experiments are reported in the Appendix, Section N. The results are reported in  Figure 1 . Figure 1a displays the discounted return for each task, averaged across episodes. Similarly, Figure 1b displays the discounted return for each episode, averaged across tasks (same color code as Figure 1a). Figure 1c displays the discounted return for five specific instances, detailed below. To avoid inter-task disparities, all the aforementioned discounted returns are displayed relatively to an estimator of the optimal expected return for each task. For readability purposes, Figures 1b and 1c display a moving average over 100 episodes. Figure 1d reports the benefits of various values of D max on the algorithmic properties. In Figure 1a, we first observe that LRMax benefits from the transfer method, as the average discounted return increases as more tasks are experienced. Moreover, this advantage appears as early as the second task. Conversely, the MaxQInit algorithm needs to wait for task 12 before benefiting from Under review as a conference paper at ICLR 2020 transfer. As suggested in Section 4.3, various amounts of prior allow the LRMax transfer method to be more or less efficient: a smaller known upper-bound D max onD MM causes a larger discounted return gain. Combining both approaches in the LRMaxQInit algorithm outperforms all other methods. Episode-wise, we observe in Figure 1b that the LRMax transfer method allows for faster convergence, hence decreases the sample complexity. Interestingly, LRMax features three stages in the learning process. 1) The first episodes are characterized by a direct exploitation of the transferred knowledge, causing these episodes to yield high payoff. This is due to the combined facts that the Lipschitz bound of Equation 8 is larger on promising regions of S × A seen on previous tasks and the fact that LRMax acts greedily w.r.t. that bound. 2) This high performance regime is followed by the exploration of unknown regions of S × A, in our case yielding low returns. Indeed, as promising regions are explored first, the bound becomes tighter for the corresponding state-action pairs, enough for the Lipschitz bound of unknown pairs to become larger, thus driving the exploration towards low payoff regions. Such regions are quickly identified and never revisited thereafter. 3) Eventually, LRMax stops exploring and converges to the optimal policy. Importantly, in all experiments, LRMax never features negative transfer as supported by the provability of the Lipschitz upper-bound with high probability. This is indeed demonstrated by the fact that it is at least as efficient in learning as the no-transfer RMax baseline. Figure 1c displays the collected returns of RMax, LRMax(0.1), and MaxQInit for specific tasks. We observe that LRMax benefits from the transfer as early as task 2, where the aforementioned 3-stages behavior is visible. Again, MaxQInit needs to wait for task 12 to leverage the transfer method. However, the bound it provides are tight enough to allow for almost zero exploration of the task. In Figure 1d, we display the following quantities for various values of D max : ρ Lip , is the ratio of the time the Lipschitz bound was tighter than the RMax bound 1 1−γ ; ρ Speed−up , is the relative gain of time steps before convergence when comparing LRMax to RMax. This quantity is estimated based on the last updates of the empirical modelM ; ρ Return , is the relative total return gain on 2000 episodes of LRMax w.r.t. RMax. First, we observe an increase of ρ Lip as D max becomes tighter. This means that the Lipschitz bound of Equation 8 becomes effectively smaller than 1 1−γ . This phenomenon leads to faster convergence, indicated by ρ Speed−up . Eventually, this increased convergence rate allows for a net total return gain, illustrated by the increase of ρ Return . Overall, in this analysis, we have showed that LRMax benefits from an enhanced sample complexity thanks to the value transfer method. The knowledge of a prior D max further increases this benefit. The method is comparable to the MaxQInit method and has some advantages such as the early fitness for use and the applicability to infinite sets of tasks. Moreover, the transfer is non-negative while preserving the PAC-MDP guarantees of the algorithm. Additionally to the analysis performed here, we show in the Appendix, Section O that, when provided with any prior knowledge D max , LRMax increasingly stops using this prior as the task is explored. This confirms the claim of section 4.3 that providing D max enables transfer even if it's value is of little importance.

Section Title: CONCLUSION
  CONCLUSION We have studied theoretically the Lipschitz continuity property of the optimal Q-function in the MDP space. This led to a local Lipschitz continuity result, establishing that the optimal Q-functions of two close MDPs are themselves close to each other. This distance between Q-functions can be computed by Dynamic Programming. We then proposed a value-transfer method using this continuity property with the Lipschitz RMax algorithm, practically implementing this approach in the Lifelong RL setting. The algorithm preserves PAC-MDP guarantees, accelerates the learning in subsequent tasks and performs non-negative transfer. Potential improvements of the algorithm were discussed in the form of prior knowledge introduction on the maximum distance between models and online estimation with high probability of this distance. We showcased the algorithm in lifelong RL experiments and demonstrated empirically its ability to accelerate learning. The results also confirm that no negative transfer occurs, regardless of parameter settings. It should be noted that our approach can directly extend other PAC-MDP algorithms ( Szita and Szepesvári, 2010 ;  Rao and Whiteson, 2012 ;  Pazis et al., 2016 ; Dann et al., 2017) to the Lifelong setting. In hindsight, we believe this contribution provides a sound basis to non-negative value transfer via MDP similarity, a development that was lacking in the literature. Key insights for the practitioner lie both in the theoretical analysis and in the practical derivation of a transfer scheme that achieves non-negative transfer with PAC guarantees.

```
