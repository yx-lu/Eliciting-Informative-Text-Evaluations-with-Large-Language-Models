Title:
```
Felix Gimeno DeepMind fgimeno@google.com
```
Abstract:
```
We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two optimization tasks for computation graphs: minimizing running time and peak memory usage. In comparison to an extensive set of baselines, our approach achieves significant improvements over classical and other learning-based methods on these two tasks.
```

Figures/Tables Captions:
```
Fig. 1: Overview of our approach. The Biased Random Key Genetic Algorithm (BRKGA) is used to optimize execution decisions for a computation graph (e.g., placement and scheduling of nodes) with respect to a cost metric (e.g., running time, peak memory) computed using the performance model. BRKGA requires proposal distributions for each node in the graph to generate candidate solutions in its search loop. The default choice is agnostic to the input graph: uniform distribution over [0, 1] at all nodes. We use a graph neural network policy to predict node-specific non-uniform proposal distribution choices (parameterized as beta distributions over [0, 1]). BRKGA is then run with those choices and outputs the best solution found by its iteration limit. By controlling the non-uniformity of the distributions, the policy directs how BRKGA's search effort is allocated such that a better solution can be found with the same search budget.
Fig. 2: Histogram of percent improvements in objective value on the TensorFlow runtime (left) and peak memory (right) datasets for test graphs on which REGAL is better (green) and worse (red) than BRKGA. (Ties are omitted from the figure for clarity but are included in the histogram percentage calculation.)
Fig. 3: Average percent improvement over BRKGA 5K given by REGAL and BRKGA on the TensorFlow test set for running time (left) and peak memory (right) as the evaluation limit is increased.
Fig. 4: The agent learns to utilize the graph structure. (a) The TF runtime agent picks a diverse set of actions. This plot shows the histogram of the entropy of the agent's actions across graphs in the dataset. (b) This plot shows the best validation reward achieved within a sweep of hyperparameters for each number of graph message passing rounds T . The performance gets overall better as T increases, and models with T > 0 perform better than T = 0, which does not utilize the structure.
Table 1: Comparison of methods on the TensorFlow and Synthetic test sets. Results are averages over test set graphs. Higher is better for % Improvement over BRKGA5K, and lower is better for % Gap from best known. Note: CP-SAT, an enumerative algorithm, is run for up to 24 hours only to establish provably global optima (if possible) for evaluation purposes.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep Learning frameworks such as MXNet ( Chen et al., 2015 ), PyTorch ( Paszke et al., 2017 ), and TensorFlow ( TensorFlow Authors, 2016a ) represent neural network models as computation graphs. Efficiently executing such graphs requires optimizing discrete decisions about how to map the computations in a graph onto hardware so as to minimize a relevant cost metric (e.g., running time, peak memory). Given that execution efficiency is critical for the success of neural networks, there is growing interest in the use of optimizing static compilers for neural network computation graphs, such as Glow ( Rotem et al., 2018 ), MLIR (MLIR Authors, 2018), TVM ( Chen et al., 2018a ), and XLA ( XLA team, 2017 ). Here we consider the model parallelism setting where a computation graph can be executed using multiple devices in parallel. Nodes of the graph are computational tasks, and directed edges denote dependencies between them. We consider jointly optimizing over placement, i.e., which nodes are executed on which devices, and schedule, i.e., the node execution order on each device. These decisions are typically made in either one or two passes in the compiler. We consider two different objectives: 1) minimize running time, subject to not exceeding device memory limits, and 2) minimize peak memory usage. In the optimization literature, such problems are studied under the class of task scheduling, which is known to be NP-hard in typical settings ( Sinnen, 2007 ;  Kwok & Ahmad, 1999 ). As scheduling and placement are just a few of the many complex decisions made in a compiler, it is essential in a production setting that a solution 1) produce solutions of acceptable quality fast, even on large graphs (e.g., thousands of nodes) and decision spaces, and 2) handle diverse graphs from various types of applications, neural network architectures, and users. In this work we consider Published as a conference paper at ICLR 2020 learning an optimizer that satisfies these requirements. Crucially, we aim to learn an optimizer that generalizes to a broad set of previously unseen computation graphs, without the need for training on such graphs, thus allowing it to be fast at test time. Previous works on learning to optimize model parallelism decisions ( Mirhoseini et al., 2017 ;  2018 ;  Addanki et al., 2019 ) have not considered generalization to a broad set of graphs nor joint optimization of placement and scheduling. In  Mirhoseini et al. (2017 ;  2018 ), learning is done from scratch for each computation graph and for placement decisions only, requiring hours (e.g., 12 to 27 hours per graph). This is too slow to be broadly useful in a general-purpose production compiler. We propose an approach that takes only seconds to optimize similar graphs. In concurrent work to ours,  Addanki et al. (2019)  shows generalization to unseen graphs, but they are generated artificially by architecture search for a single learning task and dataset. In contrast, we collect real user-defined graphs spanning a broad set of tasks, architectures, and datasets. In addition, both  Mirhoseini et al. (2017 ;  2018 ) and  Addanki et al. (2019)  consider only placement decisions and rely on TensorFlow's dynamic scheduler; they do not address the static compiler setting where it is natural to jointly optimize scheduling and placement. The key idea of our approach ( Figure 1 ) is to learn a neural network that, conditioned on the input graph to be optimized, directs an existing optimization algorithm's search such that it finds a better solution in the same search budget. We choose the Biased Random-Key Genetic Algorithm (BRKGA ( Gon√ßalves & Resende, 2011 )) as the optimization algorithm after an extensive evaluation of several choices showed that it gives by far the best speed-vs-quality trade-off for our application. BRKGA produces good solutions in just a few seconds even for real-world TensorFlow graphs with thousands of nodes, and we use learning to improve the solution quality significantly at similar speed. We train a graph neural network ( Battaglia et al., 2018 ) to take a computation graph as input and output node-specific proposal distributions to use in the mutant generation step of BRKGA's inner loop. BRKGA is then run to completion with those input-dependent distribution choices, instead of input- agnostic default choices, to compute execution decisions. The distributions are predicted at each node, resulting in a high-dimensional prediction problem. There is no explicit supervision available, so we use the objective value as a reward signal in a contextual bandit approach with REINFORCE ( Williams, 1992 ). Our approach, "Reinforced Genetic Algorithm Learning" (REGAL), uses the network's ability to generalize to new graphs to significantly improve the solution quality of the genetic algorithm for the same objective evaluation budget. We follow the static compiler approach of constructing a coarse static cost model to evaluate execution decisions and optimizing them with respect to it, as done in ( Addanki et al., 2018 ;  Jia et al., 2018 ). This is in contrast to evaluating the cost by executing the computation graph on hardware ( Mirhoseini Published as a conference paper at ICLR 2020 et al., 2017 ;  2018 ). A computationally cheap cost model enables fast optimization. It is also better suited for distributed training of RL policies since a cost model is cheap to replicate in parallel actors, while hardware environments are not. Our cost model corresponds to classical NP-hard scheduling problems, so optimizing it is difficult. In this paper we focus fully on learning to optimize this cost model, leaving integration with a compiler for future work. We structure the neural network's task as predicting proposal distributions to use in the search over execution decisions, rather than the decisions themselves directly. Empirically we have found the direct prediction approach to be too slow at inference time for our application and generalizes poorly. Our approach potentially allows the network to learn a more abstract policy not directly tied to detailed decisions that are specific to particular graphs, which may generalize better to new graphs. It can also make the learning task easier as the search may succeed even with sub-optimal proposal distribution predictions, thus smoothening the reward function and allowing the network to incrementally learn better proposals. The node-specific proposal distribution choices provide a rich set of knobs for the network to flexibly direct the search. Combining learning with a search algorithm has been shown to be successful (e.g., ( Silver et al., 2017 ;  2018 )), and our work can be seen as an instance of the same high-level idea. This paper makes several contributions: ‚Ä¢ We are the first to demonstrate learning a policy for jointly optimizing placement and scheduling that generalizes to a broad set of real-world TensorFlow graphs. REGAL significantly outperforms all baseline algorithms on two separate tasks of minimizing runtime and peak memory usage (section 5.3) on datasets constructed from 372 unique real-world TensorFlow graphs, the largest dataset of its kind in the literature and at least an order of magnitude larger than the ones in previous works ( Mirhoseini et al., 2017 ;  2018 ;  Chen et al., 2018b ;  Addanki et al., 2018 ;  2019 ). ‚Ä¢ We use a graph neural network to predict mutant sampling distributions of a genetic algo- rithm, specifically BRKGA, for the input graph to be optimized. This directs BRKGA's search in an input-dependent way, improving solution quality for the same search budget. ‚Ä¢ We compare extensively to classical optimization algorithms, such as enumerative search, local search, genetic search, and other heuristics, and analyze room-for-improvement in the objective value available to be captured via learning. Both are missing in previous works.

Section Title: RELATED WORK
  RELATED WORK Learning to optimize computation graphs: AutoTVM ( Chen et al., 2018b ) applies learning to the very different problem of optimizing low-level implementations of operators in a tensor program, while we focus on optimizing higher-level decisions such as placement and scheduling of ops.  Mao et al. (2019)  use graph neural nets and RL to learn a scheduling policy for data processing jobs on clusters. These works are conceptually similar to ours in their use of learning, applied to a different domain.

Section Title: Learning for combinatorial optimization
  Learning for combinatorial optimization Our work is an instance of applying learning for combinato- rial optimization ( Bengio et al., 2018 ). Previous works on learning graph combinatorial optimization algorithms (e.g.,  Li et al. (2018) ;  Khalil et al. (2017) ) have focused on problems such as Minimum Vertex Cover, Maximum Clique, Maximum Independent Set, etc. The task scheduling problem we consider is significantly different in that the objective value is a more complex function on node-level decisions. Also, we focus on large-scale, real-world TensorFlow graphs, while e.g.,  Khalil et al. (2017)  uses small-scale, synthetic graph distributions. Learning a proposal distribution for stochastic search:  Bunel et al. (2017)  learns a policy for pre- dicting instance-dependent proposal distributions to be used in the stochastic optimizer STOKE ( Schkufza et al., 2013 ) for superoptimizing programs. However, it uses handcrafted instance fea- tures and shows results on relatively simple, small programs. In contrast, we automatically learn the instance representations and show results on real-world graphs. An earlier work by  Paige & Wood (2016)  similarly learns a neural network to predict input-dependent proposal distributions for sequential Monte Carlo search for inference in a graphical model.

Section Title: Optimization without learning
  Optimization without learning Parallel task scheduling ( Sinnen, 2007 ;  Kwok & Ahmad, 1999 ) is a classical problem for scheduling ops in a computational graph to minimize runtime. Learning is not Published as a conference paper at ICLR 2020 traditionally a part of the approaches proposed in this literature.  Mayer et al. (2017)  studies greedy task scheduling approaches for TensorFlow.  Jia et al. (2018)  develops a simulation-based optimizer for deep learning computation graphs that uses a larger decision space by combining data, model, and attribute parallelism. Our approach can potentially be extended to such larger decisions spaces to achieve even bigger improvements in execution cost.

Section Title: BACKGROUND
  BACKGROUND   Figure 1  shows an overview of our approach. Given an input graph to optimize, instead of applying BRKGA directly with the default uniform distribution at all nodes, a graph neural network predicts beta distribution choices at each node. BRKGA is run with these choices to optimize placement and scheduling decisions with respect to the objective defined by the performance model. We first explain the performance model and BRKGA in this section, and the learning component in the next.

Section Title: PERFORMANCE MODEL
  PERFORMANCE MODEL A computation graph has a set of ops to run. Each op produces zero or more tensors and requires zero or more tensors as input. The runtime of each op is known and fixed (e.g., given by a simulator as in  Jia et al. (2018) ). The memory use of each tensor is known (an assumption that holds in static compilers like XLA). We assume a collection of d homogeneous devices that have separate local memory and can run at most one op at a time. An op can run only when its input tensors are present in the local memory. Tensors can be transferred across devices by synchronous (blocking) transfers. Tensors are freed from local memory after all local consumers have run. In this setting, we consider the problem of finding an assignment of ops to devices and an overall schedule such that each op is run once with the objectives of (1) minimizing the peak local memory use across devices (e.g., to find a feasible way to run a large computation graph), or (2) minimizing the runtime subject to a constraint on the peak memory used on any device. The performance model does not consider rematerialization of tensors, fragmentation when computing memory use, and asynchronous transfers between devices. Despite these simplifications, the model yields slight variants of problems that are known to be NP-hard ( Eyraud-Dubois et al., 2015 ) and therefore remains a challenging setting in which to study how to learn an optimizer. See section A.4 for more details of the model.

Section Title: BIASED RANDOM-KEY GENETIC ALGORITHM
  BIASED RANDOM-KEY GENETIC ALGORITHM Biased random-key genetic algorithm (BRKGA) is a meta-heuristic framework that has been success- ful in a wide array of applications for solving hard combinatorial optimization problems ( Gon√ßalves & Resende, 2011 ). In BRKGA, chromosomes in a population are encoded as n-dimensional vectors with entries in [0, 1] for some fixed n. This random-key encoding decouples the application from the genetic algorithm, specifically the crossover and mutant generation procedures ( Bean, 1994 ). The BRKGA variant we use is specified by (1) a fitness evaluation function f : [0, 1] n ‚Üí R, (2) scalar integer parameters œÄ, œÄ e , and œÄ c representing the population size, number of elites, and number of children, resp., (3) an elite bias œÅ ‚àà [0.5, 1.0), and (4) a mutant generation distribution D over [0, 1] n . The procedure aims to find a chromosome that maximizes f . The initial population (a collection of œÄ chromosomes) is created by sampling from D. (Known good solutions may also be used to initialize a population.) One evolution step is completed as follows. 1. Sort the chromosomes in order of decreasing fitness using f . Denote the first œÄ e chromo- somes as elites and the remaining chromosomes as nonelites. 2. Construct the next generation from three different sources of chromosomes: (a) Copy the elite chromosomes unmodified from the last generation. (b) For each of the œÄ c new children, select two parent chromosomes uniformly at random, one from the nonelites and one from the elites. Apply the crossover procedure (described below) to generate a new chromosome given the two parents. (c) Generate the remaining œÄ ‚àí œÄ e ‚àí œÄ c by sampling from D.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 We continue the evolution procedure for a fixed number of evaluations, i.e., calls to f . Given an elite and nonelite chromosome a, b ‚àà [0, 1] n (resp.), the crossover procedure produces a child chromosome c by independently combining entries from the parents. Specifically, for each index i ‚àà 1, . . . , n independently, let c i = a i with probability œÅ and c i = b i with probability 1 ‚àí œÅ. Our use of BRKGA is standard except for the mutant-sampling distribution D, which is usually fixed to the uniform distribution. We generalize BRKGA for instance-specific learning by sampling from n independent beta distributions, whose parameters can vary by index. Beta flexibly admits non-uniform distribution choices and also subsumes the uniform choice. Given a computation graph, let d be the number of devices, o the number of ops, and t the number of tensors. We define the chromosome encoding a scheduling solution to have three distinct parts: (1) o √ó d entries specifying op-to-device affinities; (2) o entries specifying scheduling priorities for each op; (3) t √ó d entries specifying tensor-to-device priorities for transfers that may be needed. Given a chromosome, op placements are picked by maximum affinity. Transfer ops are created as implied by the placements. We then obtain a schedule by performing a topological sort over the ops given their tensor dependencies, breaking ties by using the corresponding node priorities. Once a schedule is constructed, the performance model is used to evaluate its peak memory and/or runtime. When enforcing a memory constraint, the fitness of a schedule is encoded such that all memory-feasible schedules have better fitness than infeasible schedules. An example is provided in section A.5.

Section Title: REGAL
  REGAL

Section Title: CONTEXTUAL BANDIT FORMULATION
  CONTEXTUAL BANDIT FORMULATION We train a contextual bandit policy that predicts beta distribution choices for each of the nodes of a computation graph to be used by BRKGA to optimize it. For each round in the bandit setting, first the context is observed by drawing a computation graph G as an i.i.d. sample from a distribution D (e.g., a distribution over TensorFlow graphs). G has a set of nodes V and a set of edges E, with features associated with the nodes and edges. A policy p(a|G) is applied to make a set of decisions at each node. These decisions, denoted a v for each v ‚àà V , across all nodes form one action a = {a v‚ààV }. One decision in a corresponds to playing one arm of the bandit, and specifying the entire a corresponds to playing several arms together in a single round. This can be viewed as a combinatorial multi-armed bandit problem ( Chen et al., 2013 ). The action a specifies all the node-specific beta distributions BRKGA needs to optimize placement and scheduling decisions for G. To enable a policy over discrete choices, we quantize the mean and variance parameters of the beta distribution. The environment then runs BRKGA with those distribution choices with a fixed iteration limit. The final objective value is used to compute the reward. To make the reward values comparable across different graphs, we divide the objective value o a (G) achieved on a graph G with action a by the objective value o s (G) achieved by standard BRKGA using uniform distributions. Since we want to minimize the objective (e.g., runtime or peak memory), we define the reward as r(a, G) = ‚àí oa(G) os(G) . So a reward > ‚àí1 corresponds to an action that achieves a better objective value than standard BRKGA on a graph. We maximize the expected reward L = E G [ a p(a|G)r(a, G)], where E G is an expectation over graphs in our training set. Learning is done by REINFORCE ( Williams, 1992 ). We added a scalar baseline b(G) to reduce the variance of the gradient estimates.

Section Title: GRAPH NEURAL NETWORK POLICY
  GRAPH NEURAL NETWORK POLICY From computation graphs, we derive multigraphs with attributed nodes and directed edges. Denote a multigraph G = (V, E). In our setup, the nodes V correspond 1:1 to the ops. An edge e ‚àà E exists from u to v for each tensor that op v requires that is produced by op u. As a tensor can be required by multiple ops, the correspondence from edges to tensors may be many to one. Each node v ‚àà V and edge e ‚àà E has an attribute vector x v and x e . The attributes contain respective features, e.g., sizes of the tensors. We learn a model that predicts good mutant sampling distributions for BRKGA given this multigraph. Each node has d + 1 independent beta distributions, corresponding to device affinities and scheduling Published as a conference paper at ICLR 2020 priorities, whose parameters are represented as a vector a v . These are the model's actions in RL terms, and our model specifies a distribution over actions a = {a v } v‚ààV for each graph, p(a|G). Note the action space is different from graph to graph. We use Graph Neural Networks (GNNs) ( Scarselli et al., 2009 ;  Li et al., 2015 ;  Gilmer et al., 2017 ;  Battaglia et al., 2018 ) to learn representations for computation graphs. Given a (multi)graph G, a GNN computes representation vectors h v for each node through an iterative message passing process as follows: where e s is the source node of edge e and e t is the target node. In our formulation, MLP n and MLP e are multilayer perceptrons (MLPs) that encode node and edge attributes, MLP msg and MLP msg compute messages along the edges in the edge direction (m (t) e ) and the opposite direction (m (t) e ), MLP node updates node representations and [.] represents flat vector concatenation. After T rounds of message passing, the representation for each node h v = h (T ) v will contain information from the T -hop neighborhood around v in the graph. Given the h v 's, we produce a v 's through conditionally independent predictions, where the prediction for one node v does not depend on the predictions of other nodes given the computed representations: MLP a is shared across all nodes for predicting the parameters of the output distributions. In our experiments, we quantize the continuous beta distribution parameters and use a discrete action space. The outputs are therefore categorical, and we use the MLP to compute the logits of the corresponding softmax distributions. More details are included in section A.6. The baseline is computed using a separate GNN, where after we obtained the node representations h v , we aggregate across nodes and

Section Title: EXPERIMENTAL RESULTS
  EXPERIMENTAL RESULTS

Section Title: TASKS AND DATASETS
  TASKS AND DATASETS We consider two tasks, one is minimizing peak memory and the other is minimizing running time, both on two homogeneous devices with 16 GiB of memory each and synchronous tensor transfers with zero cost (zero latency and infinite bandwidth). We train a separate neural network for each task-dataset pair for the case of two devices. We have collected a dataset of 372 topologically-unique real-world TensorFlow graphs by mining machine learning jobs on Google's internal compute cluster (see A.1.2). These jobs are from a wide range of production and research use cases. The dataset is split into {train, valid, test} sets containing {60%, 20%, 20%} graphs, respectively. These sets are disjoint with respect to graph topology, so at test time the policy needs to generalize to new topologies. We augment the dataset by applying multiplicative noise to tensor sizes and op running times to create several variants per graph. Even though the variants of the same graph share the same topology, they represent different optimization problem instances. We create separate datasets for minimizing runtime and peak memory. The TF runtime dataset has 16329 training, 5470 validation, and 5266 test graphs. The TF peak memory dataset has 22400 training, 7400 validation, and 7400 test graphs. For reproducibility, we have released 1 a synthetic dataset of computation graphs with 10000 training, 1000 validation, and 1000 test cases. The graph topologies are generated from several classical random graph models, and the op running times and tensor sizes are sampled from Gaussian distributions (see A.1.4). On this dataset we minimize running time without a memory constraint (e.g., on two homogeneous devices with infinite memory).

Section Title: BASELINES
  BASELINES Graph Partitioning + Depth First Search (GP+DFS): Combines a graph partitioning (GP) baseline for device placement to minimize communication across devies and a Depth-First Search heuristic similar to the one implemented in XLA ( TensorFlow Authors, 2016b ) to compute per-device schedules given placements. This is representative of the XLA compiler's solution for model parallelism.

Section Title: Local Search
  Local Search The method starts with a random placement and schedule and greedily improves it by moving an op across devices or changing an op's order in the current schedule. Graph-As-Sequence Model (GAS): Like  Mirhoseini et al. (2017 ;  2018 ), we convert the graph into a sequence using a topological sort and apply a recurrent neural network to predict node-level distributions to be used by BRKGA. This comparison measures the usefulness of graph structure for learning. BRKGA XK: Run BRKGA for X thousand iterations with uniform sampling distributions using default hyperparameters consistent with  Gon√ßalves & Resende (2011) . This comparison measures the performance of the default version of BRKGA.

Section Title: Tuned BRKGA
  Tuned BRKGA Apply grid search to BRKGA's hyperparameters on the training set and pick the best. This represents how well BRKGA performs by customizing it to the distribution of computation graphs, but without instance-dependent customization.

Section Title: Instance-dependent Random Search (IDRS)
  Instance-dependent Random Search (IDRS) Same as REGAL, but BRKGA is replaced with random search. This is done by running BRKGA for only one generation using the proposal distributions computed by the neural network. Additionally, we use a Constraint Programming (CP) approach with the CP-SAT solver of Google OR- tools ( Google, 2019 ) to establish a provably global optimum for each computation graph optimization problem instance by running for up to 24 hours. As an enumerative algorithm, it is generally not competitive when run only for seconds. For a fair comparison, we fix the number of performance model evaluations allowed per graph to be the same across algorithms. (Except GP+DFS, which does not allow fixing it.) Given typical TensorFlow graph sizes and compiler running time constraints, we estimate that a budget of 5000 evaluations is feasible in practice, so we use that in the experiments.

Section Title: Learning to directly predict a solution
  Learning to directly predict a solution We have explored two more approaches for training a graph neural network to predict placement and scheduling solutions directly, without BRKGA. We used supervised learning to train a network to predict BRKGA's solutions. The best accuracy was achieved by predicting the solution autoregressively, one variable at a time conditioned on previously predicted variables. We also used RL to learn a policy with IMPALA ( Espeholt et al., 2018 ) to optimize the objective value by incrementally predicting the solution one variable at a time, and once complete, iteratively improving it with a learned local search policy. The inference cost for both approaches is quadratic in the number of nodes (the graph net is applied a linear number of times, each with linear cost), while REGAL's inference cost is linear, making them orders of magnitude slower than REGAL at test time. An evaluation on a test set of small graphs showed that neither approach improves on BRKGA5K. Improving the scalability and the generalization of these approaches is left as future work, and we do not present their results here.

Section Title: COMPARISON TO BASELINE ALGORITHMS
  COMPARISON TO BASELINE ALGORITHMS We use two metrics to compare algorithms. 1) Average percent improvement over BRKGA 5K: For a given graph, compute the percent improvement in the objective achieved by an algorithm relative to BRKGA with evaluation limit set to 5000. BRKGA 5K is a natural reference for measuring the effect of learning approaches that predict proposal distributions for it. 2) Average percent gap from best known solution: Compute the best known objective value among all the algorithms. (This will be found by CP-SAT if it finishes within the time limit.) Compute the percent difference between an algorithm's solution and the best known objective value. We report averages over test set graphs. Training set results are similar and reported in section A.3.  Table 1  compares REGAL to other algorithms on the two TensorFlow test sets and the synthetic dataset. REGAL outperforms all the baselines on all three tasks. It gives 1.9√ó and 4.4√ó bigger improvements Published as a conference paper at ICLR 2020 To further test the limits of generalization for the policies learned using REGAL, we evaluate them on XLA graphs from a production compiler team's internal performance benchmark. XLA uses a different set of ops from TensorFlow, and the benchmark graphs on average have about an order of magnitude more nodes and edges than the TensorFlow graphs in our training set, so this is a difficult generalization challenge. REGAL achieves 0.58% average runtime improvement over BRKGA 5K on 94 graphs, and 3.74% average peak memory improvement on 32 graphs. It is promising that any improvements are possible at all despite training only on TensorFlow graphs, and points to the possibility of bigger improvements by training directly on XLA graphs.

Section Title: Optimizer running times
  Optimizer running times

Section Title: COMPARING REGAL VS. BRKGA
  COMPARING REGAL VS. BRKGA   Figure 2  shows histograms of percent improvements in runtime (left) and peak memory (right) achieved by REGAL over BRKGA 5K on the test sets. Green bars correspond to graphs on which REGAL improved over BRKGA 5K, while red bars correspond to graphs on which REGAL was worse. (Ties have been omitted for clarity.) REGAL matches or beats BRKGA 5K on 87.4% of the runtime test set, and 88.9% of the peak memory test set. The highest improvement is 26.0% for run time and 54.3% for peak memory, while the worst regression is 24.0% for run time and 17.9% for peak memory. To assess whether the improvements provided by REGAL's policy generalize to evaluation limits other than the one for which it was trained, we varied the evaluation limit used by both BRKGA and REGAL at test time. The results are shown in  figure 3 . REGAL's performance improves with more evaluations, confirming that the policy generalizes to higher evaluation limits. In other words, there exist node-level choices for the distributions used in BRKGA that perform well regardless of the evaluation limit, and REGAL learns to predict those choices. This is particularly useful in cases where the actual evaluation limit to use will be known only at test time, so that the same policy can be applied without re-training. Interestingly, even with 50K evaluations, BRKGA is not able to match REGAL's performance with just 5K evaluations!

Section Title: GRAPH-DEPENDENT POLICY
  GRAPH-DEPENDENT POLICY The RL agent's actions are instance dependent. The agent that performs the best on the TF Runtime dataset has a choice of 16 different node placement actions for each node in a graph. For each graph in the TF Runtime test set, we compute the entropy of the distribution of the node placement actions taken by the agent and plot a histogram of these entropies in Figure 4(a). The mean of this distribution is 1.71 nats which implies that the actions are neither uniform random, nor constant, and vary from graph to graph. Furthermore, the agent's performance overall gets better with more graph message passing iterations T . Figure 4(b) shows the peak validation reward reached within a hyperparameter sweep for each T for the TF runtime optimization task. Models that utilize the GNN with message passing (T > 0) reach higher performance than T = 0 (i.e., ignoring the graph structure).

Section Title: CONCLUSIONS AND FUTURE WORK
  CONCLUSIONS AND FUTURE WORK By training a graph neural network policy to predict graph-conditional node-level distributions for BRKGA, REGAL successfully generalizes to new graphs, significantly outperforms all baselines in solution quality, and computes solutions in about one second on average per TensorFlow test set graph. REGAL's speed and generalization make it a strong choice for use in a production compiler that needs to handle a diverse set of graphs under a limited time budget. We foresee several extensions. Integrating REGAL into a neural network compiler would allow us to evaluate the end-to-end gains due to better placement and scheduling decisions. To further improve REGAL's own performance, one could use a Mixture of Experts architecture. Given the diversity of graphs, a mixture model can train specialized sub-models on different types of graphs (e.g., convolutional networks, recurrent networks, etc.). Another is to replace BRKGA with alternatives, e.g., combining learned neural policies with local search.

```
