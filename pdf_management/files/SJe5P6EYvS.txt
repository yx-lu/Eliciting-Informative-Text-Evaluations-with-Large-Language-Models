Title:
```
Published as a conference paper at ICLR 2020 MOGRIFIER LSTM
```
Abstract:
```
Many advances in Natural Language Processing have been based upon more expres- sive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3-4 perplexity points on Penn Treebank and Wikitext-2, and 0.01-0.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.
```

Figures/Tables Captions:
```
Figure 1: Mogrifier with 5 rounds of updates. The previous state h 0 = hprev is transformed linearly (dashed arrows), fed through a sigmoid and gates x −1 = x in an elementwise manner producing x 1 . Conversely, the linearly transformed x 1 gates h 0 and produces h 2 . After a number of repetitions of this mutual gating cycle, the last values of h * and x * sequences are fed to an LSTM cell. The prev subscript of h is omitted to reduce clutter.
Figure 2: "No-zigzag" Mogrifier for the ablation study. Gating is always based on the original inputs.
Figure 3: Perplexity vs the rounds r in the PTB ablation study.
Figure 4: Cross-entropy vs sequence length in the reverse copy task with i.i.d. tokens. Lower is better. The Mogrifier is better than the LSTM even in this synthetic task with no resemblance to natural language.
Table 1: Word-level perplexities of near state-of-the-art models, our LSTM baseline and the Mogrifier on PTB and Wikitext-2. Models with Mixture of Softmaxes (Yang et al. 2017) are denoted with MoS, depth N with dN. MC stands for Monte-Carlo dropout evaluation. Previous state-of-the-art results in italics. Note the comfortable margin of 2.8-4.3 perplexity points the Mogrifier enjoys over the LSTM.
Table 2: Bits per character on character-based datasets of near state-of-the-art models, our LSTM baseline and the Mogrifier. Previous state-of-the-art results in italics. Depth N is denoted with dN. MC stands for Monte-Carlo dropout evaluation. Once again the Mogrifier strictly dominates the LSTM and sets a new state of the art on all but the Enwik8 dataset where with dynamic evaluation it closes the gap to the Transformer-XL of similar size († Krause et al. (2019), ‡ Ben Krause, personal communications, May 17, 2019). On most datasets, model size was set large enough for underfitting not to be an issue. This was very much not the case with Enwik8, so we grouped models of similar sizes together for ease of comparison. Unfortunately, a couple of dynamic evaluation test runs diverged (NaN) on the test set and some were just too expensive to run (Enwik8, MC).
Table 3: PTB ablation study validation perplexities with 24M parameters.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The domination of Natural Language Processing by neural models is hampered only by their limited ability to generalize and questionable sample complexity ( Belinkov and Bisk 2017 ;  Jia and Liang 2017 ;  Iyyer et al. 2018 ;  Moosavi and Strube 2017 ;  Agrawal et al. 2016 ), their poor grasp of grammar ( Linzen et al. 2016 ;  Kuncoro et al. 2018 ), and their inability to chunk input sequences into meaningful units ( Wang et al. 2017 ). While direct attacks on the latter are possible, in this paper, we take a language-agnostic approach to improving Recurrent Neural Networks (RNN,  Rumelhart et al. (1988) ), which brought about many advances in tasks such as language modelling, semantic parsing, machine translation, with no shortage of non-NLP applications either ( Bakker 2002 ;  Mayer et al. 2008 ). Many neural models are built from RNNs including the sequence-to-sequence family ( Sutskever et al. 2014 ) and its attention-based branch ( Bahdanau et al. 2014 ). Thus, innovations in RNN architecture tend to have a trickle-down effect from language modelling, where evaluation is often the easiest and data the most readily available, to many other tasks, a trend greatly strengthened by ULMFiT ( Howard and Ruder 2018 ), ELMo (Peters et al. 2018) and BERT (Devlin et al. 2018), which promote language models from architectural blueprints to pretrained building blocks. To improve the generalization ability of language models, we propose an extension to the LSTM ( Hochreiter and Schmidhuber 1997 ), where the LSTM's input x is gated conditioned on the output of the previous step h prev . Next, the gated input is used in a similar manner to gate the output of the previous time step. After a couple of rounds of this mutual gating, the last updated x and h prev are fed to an LSTM. By introducing these additional of gating operations, in one sense, our model joins the long list of recurrent architectures with gating structures of varying complexity which followed the invention of Elman Networks ( Elman 1990 ). Examples include the LSTM, the GRU ( Chung et al. 2015 ), and even designs by Neural Architecture Search (Zoph and Le 2016). Intuitively, in the lowermost layer, the first gating step scales the input embedding (itself a representa- tion of the average context in which the token occurs) depending on the actual context, resulting in a contextualized representation of the input. While intuitive, as Section 4 shows, this interpretation cannot account for all the observed phenomena. In a more encompassing view, our model can be seen as enriching the mostly additive dynamics of recurrent transitions placing it in the company of the Input Switched Affine Network ( Foerster et al. Published as a conference paper at ICLR 2020 2017) with a separate transition matrix for each possible input, and the Multiplicative RNN ( Sutskever et al. 2011 ), which factorizes the three-way tensor of stacked transition matrices. Also following this line of research are the Multiplicative Integration LSTM ( Wu et al. 2016 ) and - closest to our model in the literature - the Multiplicative LSTM ( Krause et al. 2016 ). The results in Section 3.4 demonstrate the utility of our approach, which consistently improves on the LSTM and establishes a new state of the art on all but the largest dataset, Enwik8, where we match similarly sized transformer models.

Section Title: MODEL
  MODEL To allow for ease of subsequent extension, we present the standard LSTM update ( Sak et al. 2014 ) with input and state of size m and n respectively as the following function: The updated state c and the output h are computed as follows: where σ is the logistic sigmoid function, is the elementwise product, W ** and b * are weight matrices and biases. While the LSTM is typically presented as a solution to the vanishing gradients problem, its gate i can also be interpreted as scaling the rows of weight matrices W j* (ignoring the non-linearity in j). In this sense, the LSTM nudges Elman Networks towards context-dependent transitions and the extreme case of Input Switched Affine Networks. If we took another, larger step towards that extreme, we could end up with Hypernetworks ( Ha et al. 2016 ). Here, instead, we take a more cautious step, and equip the LSTM with gates that scale the columns of all its weight matrices W ** in a context-dependent manner. The scaling of the matrices W *x (those that transform the cell input) makes the input embeddings dependent on the cell state, while the scaling of W *h does the reverse. The Mogrifier 1 LSTM is an LSTM where two inputs x and h prev modulate one another in an alternating fashion before the usual LSTM computation takes place (see  Fig. 1 ). That is, Mogrify(x, c prev , h prev ) = LSTM(x ↑ , c prev , h ↑ prev ) where the modulated inputs x ↑ and h ↑ prev are defined as the highest indexed x i and h i prev , respectively, from the interleaved sequences Published as a conference paper at ICLR 2020 h i prev = 2σ(R i x i−1 ) h i−2 prev , for even i ∈ [1 . . . r] (2) with x −1 = x and h 0 prev = h prev . The number of "rounds", r ∈ N, is a hyperparameter; r = 0 recovers the LSTM. Multiplication with the constant 2 ensures that randomly initialized Q i , R i matrices result in transformations close to identity. To reduce the number of additional model parameters, we typically factorize the Q i , R i matrices as products of low-rank matrices: Q i = Q i left Q i right with Q i ∈ R m×n , Q i left ∈ R m×k , Q i right ∈ R k×n , where k < min(m, n) is the rank.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: THE CASE FOR SMALL-SCALE
  THE CASE FOR SMALL-SCALE Before describing the details of the data, the experimental setup and the results, we take a short detour to motivate work on smaller-scale datasets. A recurring theme in the history of sequence models is that the problem of model design is intermingled with optimizability and scalability. Elman Networks are notoriously difficult to optimize, a property that ultimately gave birth to the idea of the LSTM, but also to more recent models such as the Unitary Evolution RNN ( Arjovsky et al. 2016 ) and fixes like gradient clipping ( Pascanu et al. 2013 ). Still, it is far from clear - if we could optimize these models well - how different their biases would turn out to be. The non-separability of model and optimization is fairly evident in these cases. Scalability, on the other hand, is often optimized for indirectly. Given the limited ability of current models to generalize, we often compensate by throwing more data at the problem. To fit a larger dataset, model size must be increased. Thus the best performing models are evaluated based on their scalability 3 . Today, scaling up still yields tangible gains on down-stream tasks, and language mod- elling data is abundant. However, we believe that simply scaling up will not solve the generalization problem and better models will be needed. Our hope is that by choosing small enough datasets, so that model size is no longer the limiting factor, we get a number of practical advantages: Generalization ability will be more clearly reflected in evaluations even without domain adaptation. Turnaround time in experiments will be reduced, and the freed up computational budget can be put to good use by controlling for nuisance factors. The transient effects of changing hardware performance characteristics are somewhat lessened. Thus, we develop, analyse and evaluate models primarily on small datasets. Evaluation on larger datasets is included to learn more about the models' scaling behaviour and because of its relevance for applications, but it is to be understood that these evaluations come with much larger error bars and provide more limited guidance for further research on better models.

Section Title: DATASETS
  DATASETS We compare models on both word and character-level language modelling datasets. The two word- level datasets we picked are the Penn Treebank (PTB) corpus by  Marcus et al. (1993)  with prepro- cessing from  Mikolov et al. (2010)  and Wikitext-2 by  Merity et al. (2016) , which is about twice the size of PTB with a larger vocabulary and lighter preprocessing. These datasets are definitely on the small side, but - and because of this - they are suitable for exploring different model biases. Their main shortcoming is the small vocabulary size, only in the tens of thousands, which makes them inappropriate for exploring the behaviour of the long tail. For that, open vocabulary language modelling and byte pair encoding ( Sennrich et al. 2015 ) would be an obvious choice. Still, our primary goal here is the comparison of the LSTM and Mogrifier architectures, thus we instead opt for character-based language modelling tasks, where vocabulary size is not an issue, the long tail is not truncated, and there are no additional hyperparameters as in byte pair encoding that make fair comparison harder. The first character-based corpus is Enwik8 from the Hutter Prize dataset ( Hutter 2012 ). Following common practice, we use the first 90 million characters for training and the remaining 10 million evenly split between validation and test. The character-level task on the Published as a conference paper at ICLR 2020 Mikolov preprocessed PTB corpus ( Merity et al. 2018 ) is unique in that it has the disadvantages of closed vocabulary without the advantages of word-level modelling, but we include it for comparison to previous work. The final character-level dataset is the Multilingual Wikipedia Corpus (MWC,  Kawakami et al. (2017) ), from which we focus on the English and Finnish language subdatasets in the single text, large setting.

Section Title: SETUP
  SETUP We tune hyperparameters following the experimental setup of  Melis et al. (2018)  using a black-box hyperparameter tuner based on batched Gaussian Process Bandits ( Golovin et al. 2017 ). For the LSTM, the tuned hyperparameters are the same: input_embedding_ratio, learning_rate, l2_penalty, input_dropout, inter_layer_dropout, state_dropout, output_dropout. For the Mogrifier, the number of rounds r and the rank k of the low-rank approximation is also tuned (allowing for full rank, too). For word-level tasks, BPTT ( Werbos et al. 1990 ) window size is set to 70 and batch size to 64. For character-level tasks, BPTT window size is set to 150 and batch size to 128 except for Enwik8 where the window size is 500. Input and output embeddings are tied for word-level tasks following  Inan et al. (2016)  and  Press and Wolf (2016) . Optimization is performed with Adam ( Kingma and Ba 2014 ) with β 1 = 0, a setting that resembles RMSProp without momentum. Gradients are clipped ( Pascanu et al. 2013 ) to norm 10. We switch to averaging weights similarly to  Merity et al. (2017)  after a certain number of checkpoints with no improvement in validation cross-entropy or at 80% of the training time at the latest. We found no benefit to using two-step finetuning. Model evaluation is performed with the standard, deterministic dropout approximation or Monte- Carlo averaging ( Gal and Ghahramani 2016 ) where explicitly noted (MC). In standard dropout evaluation, dropout is turned off while in MC dropout predictions are averaged over randomly sampled dropout masks (200 in our experiments). Optimal softmax temperature is determined on the validation set, and in the MC case dropout rates are scaled ( Melis et al. 2018 ). Finally, we report results with and without dynamic evaluation ( Krause et al. 2017 ). Hyperparameters for dynamic evaluation are tuned using the same method (see Appendix A for details). We make the code and the tuner output available at https://github.com/deepmind/lamb.

Section Title: RESULTS
  RESULTS   Table 1  lists our results on word-level datasets. On the PTB and Wikitext-2 datasets, the Mogrifier has lower perplexity than the LSTM by 3-4 perplexity points regardless of whether or not dynamic evaluation ( Krause et al. 2017 ) and Monte-Carlo averaging are used. On both datasets, the state of the art is held by the AWD LSTM ( Merity et al. 2017 ) extended with Mixture of Softmaxes ( Yang Published as a conference paper at ICLR 2020   Table 2  lists the character-level modelling results. On all datasets, our baseline LSTM results are much better than those previously reported for LSTMs, highlighting the issue of scalability and experimental controls. In some cases, these unexpectedly large gaps may be down to lack of hyperparameter tuning as in the case of  Merity et al. (2017) , or in others, to using a BPTT window size (50) that is too small for character-level modelling ( Melis et al. 2017 ) in order to fit the model into memory. The Mogrifier further improves on these baselines by a considerable margin. Even the smallest improvement of 0.012 bpc on the highly idiosyncratic, character-based, Mikolov preprocessed PTB task is equivalent to gaining about 3 perplexity points on word-level PTB. MWC, which was built for open-vocabulary language modelling, is a much better smaller-scale character-level dataset. On the English and the Finnish corpora in MWC, the Mogrifier enjoys a gap of 0.033-0.046 bpc. Finally, on the Enwik8 dataset, the gap is 0.029-0.039 bpc in favour of the Mogrifier. Of particular note is the comparison to Transformer-XL ( Dai et al. 2019 ), a state-of-the-art model on larger datasets such as Wikitext-103 and Enwik8. On PTB, without dynamic evaluation, the Transformer-XL is on par with our LSTM baseline which puts it about 3.5 perplexity points behind the Mogrifier. On Enwik8, also without dynamic evaluation, the Transformer-XL has a large, 0.09 bpc advantage at similar parameter budgets, but with dynamic evaluation this gap disappears. However, we did not test the Transformer-XL ourselves, so fair comparison is not possible due to differing experimental setups and the rather sparse result matrix for the Transformer-XL.

Section Title: ANALYSIS
  ANALYSIS

Section Title: ABLATION STUDY
  ABLATION STUDY The Mogrifier consistently outperformed the LSTM in our experiments. The optimal settings were similar across all datasets, with r ∈ {5, 6} and k ∈ [40 . . . 90] (see Appendix B for a discussion of hyperparameter sensitivity). In this section, we explore the effect of these hyperparameters and show that the proposed model is not unnecessarily complicated. To save computation, we tune all models using a shortened schedule with only 145 epochs instead of 964 and a truncated BPTT window size of 35 on the word-level PTB dataset, and evaluate using the standard, deterministic dropout approximation with a tuned softmax temperature.  Fig. 3  shows that the number of rounds r greatly influences the results. Second, we found the low-rank factorization of Q i and R i to help a bit, but the full-rank variant is close behind which is what we observed on other datasets, as well. Finally, to verify that the alternating gating scheme is not overly complicated, we condition all newly introduced gates on the original inputs x and h prev (see  Fig. 2 ). That is, instead of Eq. 1 and Eq. 2 the no-zigzag updates are In our experiments, the no-zigzag variant underperformed the baseline Mogrifier by a small but significant margin, and was on par with the r = 2 model in  Fig. 3  suggesting that the Mogrifier's iterative refinement scheme does more than simply widen the range of possible gating values of x and h prev to (0, 2 r/2 ) and (0, 2 r/2 ), respectively.

Section Title: COMPARISON TO THE MLSTM
  COMPARISON TO THE MLSTM The Multiplicative LSTM ( Krause et al. 2016 ), or mLSTM for short, is closest to our model in the literature. It is defined as mLSTM(x, c prev , h prev ) = LSTM(x, c prev , h m prev ), where h m prev = Published as a conference paper at ICLR 2020 50 100 150 200 0 0.5 1 1.5 LST M M ogrif ier (a) 10M model parameters with vocabulary size 1k. 50 100 150 200 0 2 4 LST M M ogrif ier (b) 24M model parameters with vocabulary size 10k. (W mx x) (W mh h prev ). In this formulation, the differences are readily apparent. First, the mLSTM allows for multiplicative interaction between x and h prev , but it only overrides h prev , while in the Mogrifier the interaction is two-way, which - as the ablation study showed - is important. Second, the mLSTM can change not only the magnitude but also the sign of values in h prev , something with which we experimented in the Mogrifier, but could not get to work. Furthermore, in the definition of h m prev , the unsquashed linearities and their elementwise product make the mLSTM more sensitive to initialization and unstable during optimization. On the Enwik8 dataset, we greatly improved on the published results of the mLSTM ( Krause et al. 2016 ). In fact, even our LSTM baseline outperformed the mLSTM by 0.03 bpc. We also conducted experiments on PTB based on our reimplementation of the mLSTM following the same methodology as the ablation study and found that the mLSTM did not improve on the LSTM (see  Table 3 ).  Krause et al. (2016)  posit and verify the recovery hypothesis which says that having just suffered a large loss, the loss on the next time step will be smaller on average for the mLSTM than for the LSTM. This was found not to be the case for the Mogrifier. Neither did we observe a significant change in the gap between the LSTM and the Mogrifier in the tied and untied embeddings settings, which would be expected if recovery was affected by x and h prev being in different domains.

Section Title: THE REVERSE COPY TASK
  THE REVERSE COPY TASK Our original motivation for the Mogrifier was to allow the context to amplify salient and attenuate nuisance features in the input embeddings. We conduct a simple experiment to support this point of view. Consider the reverse copy task where the network reads an input sequence of tokens and a marker token after which it has to repeat the input in reverse order. In this simple sequence-to- sequence learning ( Sutskever et al. 2014 ) setup, the reversal is intended to avoid the minimal time lag problem ( Hochreiter and Schmidhuber 1997 ), which is not our focus here. The experimental setup is as follows. For the training set, we generate 500 000 examples by uniformly sampling a given number of tokens from a vocabulary of size 1000. The validation and test sets are constructed similarly, and contain 10 000 examples. The model consists of an independent, unidirectional encoder and a decoder, whose total number of parameters is 10 million. The decoder is initialized from the last state of the encoder. Since overfitting is not an issue here, no dropout is necessary, and we only tune the learning rate, the l2 penalty, and the embedding size for the LSTM. For the Mogrifier, the number of rounds r and the rank k of the low-rank approximation are also tuned. We compare the case where both the encoder and decoder are LSTMs to where both are Mogrifiers. Fig. 4a shows that, for sequences of length 50 and 100, both models can solve the task perfectly. At higher lengths though, the Mogrifier has a considerable advantage. Examining the best hyperparameter settings found, the embedding/hidden sizes for the LSTM and Mogrifier are 498/787 vs 41/1054 at 150 steps, and 493/790 vs 181/961 at 200 steps. Clearly, the Mogrifier was able to work with a much smaller embedding size than the LSTM, which is in line with our expectations for a model with a more flexible interaction between the input and recurrent state. We also conducted experiments with a larger model and vocabulary size, and found the effect even more pronounced (see Fig. 4b).

Section Title: WHAT THE MOGRIFIER IS NOT
  WHAT THE MOGRIFIER IS NOT The results on the reverse copy task support our hypothesis that input embeddings are enriched by the Mogrifier architecture, but that cannot be the full explanation as the results of the ablation study indicate. In the following, we consider a number of hypotheses about where the advantage of the Mogrifier lies and the experiments that provide evidence against them. Hypothesis: the benefit is in scaling x and h prev . We verified that data dependency is a crucial feature by adding a learnable scaling factor to the LSTM inputs. We observed no improvement. Also, at extremely low-rank (less than 5) settings where the amount of information in its gating is small, the Mogrifier loses its advantage. Hypothesis: the benefit is in making optimization easier. We performed experiments with different optimizers (SGD, RMSProp), with intra-layer batch normalization and layer normalization on the LSTM gates. While we cannot rule out an effect on optimization difficulty, in all of these experiments the gap between the LSTM and the Mogrifier was the same. Hypothesis: exact tying of embeddings is too constraining, the benefit is in making this rela- tionship less strict. Experiments conducted with untied embeddings and character-based models demonstrate improvements of similar magnitude. Hypothesis: the benefit is in the low-rank factorization of Q i , R i implicitly imposing structure on the LSTM weight matrices. We observed that the full-rank Mogrifier also performed better than the plain LSTM. We conducted additional experiments where the LSTM's gate matrices were factorized and observed no improvement. Hypothesis: the benefit comes from better performance on rare words. The observed advantage on character-based modelling is harder to explain based on frequency. Also, in the reverse copy experiments, a large number of tokens were sampled uniformly, so there were no rare words at all. Hypothesis: the benefit is specific to the English language. This is directly contradicted by the Finnish MWC and the reverse copy experiments. Hypothesis: the benefit is in handling long-range dependencies better. Experiments in the episodic setting (i.e. sentence-level language modelling) exhibited the same gap as the non-episodic ones. Hypothesis: the scaling up of inputs saturates the downstream LSTM gates. The idea here is that saturated gates may make states more stable over time. We observed the opposite: the means of the standard LSTM gates in the Mogrifier were very close between the two models, but their variance was smaller in the Mogrifier.

Section Title: CONCLUSIONS AND FUTURE WORK
  CONCLUSIONS AND FUTURE WORK We presented the Mogrifier LSTM, an extension to the LSTM, with state-of-the-art results on several language modelling tasks. Our original motivation for this work was that the context-free representation of input tokens may be a bottleneck in language models and by conditioning the input embedding on the recurrent state some benefit was indeed derived. While it may be part of the explanation, this interpretation clearly does not account for the improvements brought by conditioning the recurrent state on the input and especially the applicability to character-level datasets. Positioning our work on the Multiplicative RNN line of research offers a more compelling perspective. To give more credence to this interpretation, in the analysis we highlighted a number of possible alternative explanations, and ruled them all out to varying degrees. In particular, the connection to the mLSTM is weaker than expected as the Mogrifier does not exhibit improved recovery (see Section 4.2), and on PTB the mLSTM works only as well as the LSTM. At the same time, the evidence against easier optimization is weak, and the Mogrifier establishing some kind of sharing between otherwise independent LSTM weight matrices is a distinct possibility. Finally, note that as shown by  Fig. 1  and Eq. 1-2, the Mogrifier is a series of preprocessing steps composed with the LSTM function, but other architectures, such as Mogrifier GRU or Mogrifier Elman Network are possible. We also leave investigations into other forms of parameterization of context-dependent transitions for future work.
  ) take great advantage of this approach.

```
