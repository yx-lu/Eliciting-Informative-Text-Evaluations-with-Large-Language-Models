Title:
```
Under review as a conference paper at ICLR 2020 IF MAXENT RL IS THE ANSWER, WHAT IS THE QUESTION?
```
Abstract:
```
Experimentally, it has been observed that humans and animals often make de- cisions that do not maximize their expected utility, but rather choose outcomes randomly, with probability proportional to expected utility. Probability matching, as this strategy is called, is equivalent to maximum entropy reinforcement learning (MaxEnt RL). However, MaxEnt RL does not optimize expected utility. In this paper, we formally show that MaxEnt RL does optimally solve certain classes of control problems with variability in the reward function. In particular, we show (1) that MaxEnt RL can be used to solve a certain class of POMDPs, and (2) that MaxEnt RL is equivalent to a two-player game where an adversary chooses the reward function. These results suggest a deeper connection between MaxEnt RL, robust control, and POMDPs, and provide insight for the types of problems for which we might expect MaxEnt RL to produce effective solutions. Specifically, our results suggest that domains with uncertainty in the task goal may be especially well-suited for MaxEnt RL methods.
```

Figures/Tables Captions:
```
Figure 1: MaxEnt RL solves the Meta-POMDP. On a 5-armed bandit problem, we show that solving a MaxEnt RL problem with a reward of r(i) = 1 2 log p(i) mini- mizes regret on the meta-POMDP defined by p(i). The thick line is the average across 10 randomly-generated bandit problems (thin lines). Lower is better.
Figure 2: MaxEnt RL = Robust-Reward Control: The policy obtained by running MaxEnt RL on reward function r is the optimal robust policy for a collection of rewards, Rr. (Left) We plot the original reward function, r as a red dot, and the collection of reward functions, Rr, as a blue line. (Center) For each policy, parameterized solely by its probability of choosing action 1, we plot the expected reward for each reward function in Rr. The robust-reward control problem is to choose the policy whose worst-case reward (dark blue line) is largest. (Right) For each policy, we plot the MaxEnt RL objective (i.e., the sum of expected reward and entropy).
Figure 3: MaxEnt RL solves a robust-reward con- trol problem. On a 5-armed bandit problem, MaxEnt RL converges to the optimal minimax policy. Fictitious play, a prior method for solving adversarial problems, fails to solve this task, but an oracle variant achieves re- ward similar to MaxEnt RL. The thick line is the average over 10 random seeds (thin lines). Higher is better.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) searches for a policy that maximizes the expected, cumulative reward. In fully observed Markov decision processes (MDPs), this maximization always has a deterministic policy as a solution. Maximum entropy reinforcement learning (MaxEnt RL) is a modification of the RL objective that further adds an entropy term to the objective. This additional entropy term causes MaxEnt RL to seek policies that (1) are stochastic, and (2) have non-zero probability of sampling every action. MaxEnt RL can equivalently be viewed as probability matching between trajectories visited by the policy and a distribution defined by exponentiating the reward (See Section 2). MaxEnt RL has appealing connections to probabilistic inference ( Dayan & Hinton, 1997 ;  Neumann et al., 2011 ;  Todorov, 2007 ;  Kappen, 2005 ;  Toussaint, 2009 ;  Rawlik et al., 2013 ;  Theodorou et al., 2010 ;  Ziebart, 2010 ), prompting a renewed interest in recent years ( Haarnoja et al., 2018b ;  Abdolmaleki et al., 2018 ;  Levine, 2018 ). MaxEnt RL can also be viewed as using Thompson sampling ( Thompson, 1933 ) to collect trajectories, where the posterior belief is given by the exponentiated return. Empirically, MaxEnt RL algorithms achieve good performance on a number of simulated ( Haarnoja et al., 2018b ) and real-world ( Haarnoja et al., 2018a ;  Singh et al., 2019 ) control tasks, and can be more robust to perturbations ( Haarnoja et al., 2018c ). There is empirical evidence that behavior similar MaxEnt RL is used by animals in the natural world. While standard reinforcement learning is often used as a model for decision making ( Scott, 2004 ;  Liu & Todorov, 2007 ;  Todorov & Jordan, 2002 ), many animals, including humans, do not consistently make decisions that maximize expected utility. Rather, they engage in probability matching, choosing actions with probability proportional to how much utility that action will provide. Examples include ants ( Lamb & Ollason, 1993 ), bees ( Greggers & Menzel, 1993 ), fish ( Bitterman et al., 1958 ), ducks ( Harper, 1982 ), pigeons ( Bullock & Bitterman, 1962 ;  Graf et al., 1964 ), and humans, where it has been documented so extensively that  Vulkan (2000)  wrote a survey of surveys of the field. This effect has been observed not just in individuals, but also in the collective behavior of groups of animals (see  Stephens & Krebs (1986) ), where it is often described as obtaining the ideal free distribution. Probability matching is not merely a reflection of youth or ignorance. Empirically, more intelligent creatures are more likely to engage in probability matching. For example, in a comparison of Yale students and rats,  Gallistel (1990)  found that the students nearly always performed probability Under review as a conference paper at ICLR 2020 matching, while rats almost always chose the maximizing strategy. Similarly, older children and adults engage in probability matching more frequently than young children ( Stevenson & Odom, 1964 ;  Weir, 1964 ). While prior work has offered a number of explanations of probability matching ( Vulkan, 2000 ;  Gaissmaier & Schooler, 2008 ;  Wozny et al., 2010 ;  Sakai & Fukai, 2008 ), its root cause remains an open problem. The empirical success of MaxEnt RL algorithms on RL problems is surprising, as MaxEnt RL optimizes a different objective than standard RL. The solution to every MaxEnt RL problem is stochastic, while deterministic policies can always be used to solve standard RL problems ( Puterman, 2014 ). While RL can be motivated from the axioms of utility theory ( Russell & Norvig, 2016 ), MaxEnt RL has no such fundamental motivation. It remains an open question as to whether the standard MaxEnt RL objective actually optimizes some well-defined notion of risk or regret that would account for its observed empirical benefits. This paper studies this problem, and aims to answer the following question: if MaxEnt RL is the solution, then what is the problem? Answering this question is a first step towards understanding the empirical success of MaxEnt RL algorithms, and our analysis will suggest that MaxEnt RL might be applicable to problems typically considered to be much more complex than standard RL. In this paper, we show that MaxEnt RL provides the optimal control solution in settings with uncertainty and variability in the reward function. More precisely, we show that MaxEnt RL is equivalent to two more challenging problems: (1) regret minimization in a meta-POMDP, and (2) robust-reward control. The first setting, the meta-POMDP, is a partially observed MDP where the reward depends on an unobserved portion of the state, and where multiple episodes in the original MDP correspond to a single extended trial in the meta-POMDP. While seemingly Byzantine, this type of problem setting arises in a number of real-world settings discussed in Section 3. Optimal policies for the meta-POMDP must explore at test-time, behavior that cannot result from maximizing expected utility. In the second setting, robust-reward control, we consider an adversary that chooses some aspects of the reward function. Intuitively, we expect stochastic policies to be most robust because they are harder to exploit, as we formalize in Section 5. Even if the agent will eventually be deployed in a setting without adversaries, the adversarial objective bounds the worst-case performance of that agent. Our result in this setting can be viewed as an extension of prior work connecting the principle of maximum entropy to two-player games ( Ziebart et al., 2011 ;  Grünwald et al., 2004 ). While both robust-reward control and regret minimization in a meta-POMDP are natural problems that arise in many real-world scenarios, neither is an expected utility maximization problem, so we cannot expected optimal control to solve these problems. In contrast, we show that MaxEnt RL provides solutions to both. In summary, our analysis suggests that the empirical benefits of MaxEnt RL arise by implicitly solving control problems with variability in the reward.

Section Title: PRELIMINARIES
  PRELIMINARIES We begin by defining notation and discussing some previous motivations for MaxEnt RL. An agent observes states s t , takes actions a t ∼ π(a t | s t ), and obtains rewards r(s t , a t ). The initial state is sampled s 1 ∼ p 1 (s 1 ), and subsequent states are sampled s ∼ p(s | s, a). Episodes have T steps, which we summarize as a trajectory τ (s 1 , a 1 , · · · , s T , a T ). Without loss of generality, we can assume that rewards are undiscounted, as any discount can be addressed by modifying the dynamics to transition to an absorbing state with probability 1 − γ. The RL objective is: In fully observed MDPs, there always exists a deterministic policy as a solution ( Puterman, 2014 ). The MaxEnt RL problem, also known as the entropy-regularized control problem, is to maximize the sum of expected reward and conditional action entropy, H π [a | s]: The MaxEnt RL objective results in policies that are stochastic, with higher-entropy action distribu- tions in states where many different actions lead to similarly optimal rewards, and lower-entropy Under review as a conference paper at ICLR 2020 distributions in states where a single action is much better than the rest. Moreover, MaxEnt RL results in policies that have non-zero probability of sampling any action. MaxEnt RL can equiva- lently be defined as a form of probability matching, minimizing a reverse Kullback Leibler (KL) divergence ( Rawlik et al., 2013 ): where the policy distribution π(τ ) and the target distribution p r (τ ) are defined as Prior work on MaxEnt RL offers a slew of intuitive explanations for why one might prefer MaxEnt RL. We will summarize three common explanations and highlights problems with each.

Section Title: Exploration
  Exploration MaxEnt RL is often motivated as performing good exploration. Unlike many other RL algorithms, such as DQN ( Mnih et al., 2015 ) and DDPG ( Lillicrap et al., 2015 ), MaxEnt RL performs exploration and policy improvement with the same (stochastic) policy. One problem with this motivation is that stochastic policies can be obtained directly from standard RL, without adding an entropy term ( Heess et al., 2015 ). More troubling, while MaxEnt RL learns a stochastic policy, many MaxEnt RL papers evaluate the corresponding deterministic policy ( Haarnoja et al., 2018b ), suggesting that the stochastic policy is not what should be optimized. While improved exploration may be an ancillary benefit of MaxEnt RL, it remains unclear why we should expect MaxEnt RL to explore better than standard RL algorithms that learn stochastic policies.

Section Title: Probabilistic inference
  Probabilistic inference Connections with probabilistic inference offer a second motivation for MaxEnt RL ( Abdolmaleki et al., 2018 ;  Haarnoja et al., 2018b ;  Todorov, 2007 ;  Levine, 2018 ;  Toussaint, 2009 ). These approaches cast optimal control as an inference problem by defining additional optimality binary random variables O t , equal one with probability proportional to exponentiated reward. These methods then maximize the following log-likelihood: The last term is a cumulant generating function (i.e., the logarithm of a moment generating func- tion ( Gut, 2013 ,  Chpt. 6 )), which can be approximated as the sum of expected reward and variance of returns ( Mihatsch & Neuneier, 2002 ). Thus, directly maximizing likelihood leads to risk-seeking behavior, not optimal control. Equation 1 can also be directly obtained by considering an agent with a risk-seeking utility function ( O'Donoghue, 2018 ). While risk seeking behavior can be avoided by maximizing a certain lower bound on Equation 1 ( Levine, 2018 ), artificially constraining algorithms to maximize a lower bound suggests that likelihood is not what we actually want to maximize.

Section Title: Easier optimization
  Easier optimization Finally, some prior work ( Ahmed et al., 2018 ;  Williams & Peng, 1991 ) argues that the entropy bonus added by MaxEnt RL makes the optimization landscape smoother. However, it does not suggest why optimizing the wrong but smooth problem yields a good solution to the original optimization problem.

Section Title: WHAT PROBLEMS DOES MAXENT RL SOLVE?
  WHAT PROBLEMS DOES MAXENT RL SOLVE? MaxEnt RL produces stochastic policies, so we first discuss when stochastic policies may be optimal. Informally, the two strengths of stochastic policies are that they (1) are guaranteed to eventually try every action sequence and (2) do not always choose the same sequence of actions. Under review as a conference paper at ICLR 2020

Section Title: ANSWER 1: PARTIALLY OBSERVED ENVIRONMENTS
  ANSWER 1: PARTIALLY OBSERVED ENVIRONMENTS The first strength of stochastic policies guarantees that they will not have to wait infinitely long to find a good outcome. Imagine that a cookie is hidden in one of two jars. A policy that always chooses to look in the same jar (say, the left jar) may never find the cookie if it is hidden in the other jar (the right jar). Such a policy would incur infinite regret. This need to try various approaches arises in many realistic settings where we do not get to observe the true reward function, but rather have a belief over what the true reward is. For example, in a health-care setting, consider the course of treatment for a patient. The desired outcome is to cure the patient. However, whether the patient is cured by different courses of treatment depends on their illness, which is unknown. A physician will prescribe medications based on his beliefs about the patient's illness. If the medication fails, the patient returns to the physician the next week, and the physician recommends another medication. This process will continue until the patient is cured. The physician's aim is to minimize the number of times the patient returns. Another example is a robot that must perform chores in the home based on a user's commands. The true goal in this task is to satisfy the user. Their desires are never known with certainty, but must be inferred from the user's behavior. Indeed, arguably the majority of problems to which we might want to apply reinforcement learning algorithms are actually problems where the true reward is unobserved, and the reward function that is provided to the agent represents an imperfect belief about the goal. In Section 4, we define a meta-level POMDP for describing these sorts of tasks and show that MaxEnt RL minimizes regret in such settings.

Section Title: ANSWER 2: ROBUSTNESS TO EXPLOITATION
  ANSWER 2: ROBUSTNESS TO EXPLOITATION The second strength of stochastic policies is that they are harder to exploit. For example, in the game rock-paper-scissors ("ro-sham-bo"), it is bad to always choose the same action (say, rock) because an adversary can always choose an action that makes the player perform poorly (e.g., by choosing paper). Indeed, the Nash existence theorem ( Nash et al., 1950 ) requires stochastic policies to guarantee that a Nash equilibrium exists. In RL, we might likewise expect that a randomized policies are harder to exploit than deterministic policies. To formalize the intuition that MaxEnt policies are robust against adversaries, we define the robust-reward control problem. Definition 3.1. The robust-reward control problem for a set of reward functions R = {r i } is arg max We can think of this optimization problem as a two-player, zero-sum game between a policy player and an adversarial reward player. The policy player chooses the sequence of actions in response to observations, while the reward player chooses the reward function against which the states and actions will be evaluated. This problem is slightly different from typical robust control ( Zhou & Doyle, 1998 ), as it considers perturbations to rewards, not dynamics. Typically, solving the robust-reward control problem is challenging because it is a saddle-point problem. Nonetheless, in Section 5, we show that MaxEnt RL is exactly equivalent to solving a robust-reward control problem.

Section Title: SUMMARY
  SUMMARY Together, these two properties suggest that stochastic policies, such as those learned with MaxEnt RL, can be robust to variability in the reward function. This variability may be caused by (1) a designer's uncertainty about what the right reward should be, (2) the presence of perturbations to the reward (e.g., for an agent that interacts with human users, who might have different needs and wants in each interaction), or (3) partial observability (e.g., a robot in a medical setting may not observe the true cause for a patient's illness). In this paper, we formally show that MaxEnt RL algorithms produce policies that are robust to two distinct sources of reward variability: unobserved rewards in partially observed Markov decision processes (POMDPs) and adversarial variation in the rewards.

Section Title: MAXIMUM ENTROPY RL AND PARTIALLY OBSERVED ENVIRONMENTS
  MAXIMUM ENTROPY RL AND PARTIALLY OBSERVED ENVIRONMENTS In this section, we formalize the intuition from Section 3 that stochastic policies are preferable in settings with unknown tasks. We first describe the problem of solving an unknown task as a special Under review as a conference paper at ICLR 2020 class of POMDPs, and then show that MaxEnt RL provides the optimal solution for these POMDPs. Our results in this section suggest a tight coupling between MaxEnt RL and regret minimization. We begin by defining the meta-POMDP as a MDP with many possible tasks that could be solved. Solving a task might mean reaching a particular goal state or performing a particular sequence of actions. We will use the most general definition of success as simply matching some target trajectory, τ * . Crucially, the agent does not know which task it must solve (i.e., τ * is not observed). Rather, the agent has access to a belief p(τ ) over what the target trajectory may be. This results in a POMDP, where the agent's ignorance of the true task makes the problem partial observed. Each meta-step of the meta-POMDP corresponds to one episode of the original MDP. A meta-episode is a sequence of meta-steps, which ends when the agent solves the task in the original MDP. Intuitively, each meta-episode in the meta-POMDP corresponds to multiple trials in the original MDP, where the task remains the same across trials. The agent keeps interacting with the MDP until it solves the task. While the meta-POMDP might seem counter-intuitive, it captures many practical scenarios. For example, in the health-care setting in Section 3, the physician does not know the patient's illness, and may not even know when the patient has been cured. Each meta-step corresponds to one visit to the physician, which might entail running some tests, performing an operation, and prescribing a new medication. The meta-episode is the sequence of patient visits, which ends when the patient is cured. As another example, Appendix A.2 describes how meta-learning can also be viewed as a meta-POMDP. Before proceeding, we emphasize that defining the meta-POMDP in terms of trajectory distributions is strictly more general than defining it in terms of state distributions. However, Section 4.3 will discuss how goal-reaching, a common problem setting in current RL research ( Lee et al., 2019b ;  Warde-Farley et al., 2018 ;  Pong et al., 2019 ), can be viewed as a special case of this general formulation.

Section Title: REGRET IN THE META-POMDP
  REGRET IN THE META-POMDP The meta-POMDP has a simple reward function: +1 when the task is completed, and 0 otherwise. Since the optimal policy would solve the task immediately, its reward on every meta-step would be one. Therefore, the regret is given by 1 − 0 = 1 for every meta-step when the agent fails to solve the task, and 1 − 1 = 0 for the (final) meta-step when the agent solves the task. Thus, the cumulative regret of an agent is the expected number of meta-steps required to complete the task. For example, in the health-care example, the regret is the number of times the patient visits the physician before being cured. Our analysis of the meta-POMDP will consider policies that are Markovian within a meta-episode: while the policy can be updated between meta-episodes, the policy cannot use information from one meta-step to take better actions in a future meta-step within the same meta-episode. Our results will therefore be lower bounds on the performance of non-Markovian policies. The Markovian assumption is equivalent to saying that agents lack memory, and might be seen as an instantiation of bounded rationality. That MaxEnt RL is optimal under this assumption suggests that the probability matching observed in nature is optimal under memory constraints. Mathematically, we use π(τ * ) to denote the probability that policy π produces target trajectory τ * . Then, the number of episodes until it matches trajectory τ * is a geometric random variable with parameter π(τ * ). The expected value of this random variable is 1/π(τ * ), so we can write the regret of the meta-POMDP as: Note that this regret is a function of a particular policy π(a | s), evaluated over potentially infinitely many steps in the original MDP. A policy that never replicates the target trajectory incurs infinite regret. Thus, we expect that optimal policies for the meta-POMDP will be stochastic.

Section Title: SOLVING THE META-POMDP
  SOLVING THE META-POMDP We solve the meta-POMDP by finding an optimal distribution over trajectories: Under review as a conference paper at ICLR 2020 Using Lagrange multipliers (see Appendix A.1), we find that the optimal policy is: This policy is stochastic and matches the unnormalized distribution p(τ ). This result suggests that we can find the optimal policy by solving a MaxEnt RL problem, with a trajectory-level reward function r τ (τ ) 1 2 log p(τ ). To make this statement precise, we consider the bandit setting and MDP setting separately. If the underlying MDP is a bandit, then trajectories are equivalent to actions. We can define a reward function as r(a) = 1 2 log p(a). Applying MaxEnt RL to this reward function yields the following policy, which is optimal for the meta-POMDP: π(a) ∝ p(a). For MDPs with horizon lengths greater than one, we can make a similar statement: Lemma 4.1. Let a goal trajectory distribution p(τ ) be given, and assume that there exists a policy π whose trajectory distribution is proportional to the square-root of the target distribution: π(τ ) ∝ p(τ ). Then there exists a reward function r(s, a) such that the MaxEnt RL problem with r(s, a) and the meta-POMDP have the same solution(s). Proof. Let π * be the solution to the meta-POMDP, so it must satisfy π * (τ ) ∝ p(τ ). Thus, π * is the solution to the MaxEnt RL problem with the trajectory-level reward r(τ ) = 1 2 log p(τ ): The normalizing constant c, which is independent from π * , is introduced to handle the fact that p(τ ) does not integrate to one. The implication comes from the fact that the KL is minimized when its arguments are equal. We show in Appendix A.3 that a trajectory-level reward r(τ ) can always be decomposed into an state-action reward r(s t , a t ) with the same MaxEnt RL solution. Thus, there exists a reward function such that MaxEnt RL solves the meta-POMDP: The obvious criticism of the proof above is that it is not constructive, failing to specify how the MaxEnt RL reward might be obtained. Nonetheless, our analysis illustrates why MaxEnt RL methods might work well: even when the meta-POMDP is unknown, MaxEnt RL methods will minimize regret in some meta-POMDP, which could account for their good performance, particularly in the presence of uncertainty and perturbations.

Section Title: GOAL-REACHING META-POMDPS
  GOAL-REACHING META-POMDPS We can make the connection between MaxEnt RL and meta-POMDPs more precise by considering a special class of meta-POMDPs: meta-POMDPs where the target distribution is defined only in terms of the last state in a trajectory, corresponding to goal-reaching problems. While prior work on goal-reaching ( Kaelbling, 1993 ;  Schaul et al., 2015 ;  Andrychowicz et al., 2017 ) assumes that the goal state is observed, the goal-reaching meta-POMDP only assumes that the policy has a belief about the goal state. Lemma 4.2. Let a meta-POMDP with target distribution that depends solely on the last state and action in the trajectory be given. That is, the target distribution p(τ ) satisfies s T (τ ) = s T (τ ) and a T (τ ) = a T (τ ) =⇒ p(τ ) = p(τ ) ∀τ, τ where s T (τ ) and a T (τ ) are functions that extract the last state and action in trajectory τ . We can thus write the density of a trajectory under the goal trajectory distribution as a function of the last state and action: p(τ ) =p(s T (τ ), a T (τ )), wherep(s T , a T ) is an unnormalized density. Assume that there ex- ists a policy whose marginal state density at the last time step, ρ T π (s, a), satisfies ρ T π (s, a) ∝ p(s, a) for all states s. Then the MaxEnt RL problem with reward r(s t , a t ) 1 2 1(t = T ) · logp(s t , a t ) and the meta-POMDP have the same solutions. Under review as a conference paper at ICLR 2020 Proof. We simply combine Lemma 4.1 with the definition of the reward function r: While our assumption that there exists a policy that exactly matches some distribution ( p(s T , a T )) may seem somewhat unnatural, we provide a sufficient condition in Appendix A.4. Further, while the analysis so far has considered the equivalence of MaxEnt RL and the meta-POMDP at optimum, in Appendix A.5 we bound the difference between these problems away from their optima. In summary, the meta-POMDP allows us to represent goal-reaching tasks with uncertainty in the true goal state. Moreover, solving these goal-reaching meta-POMDPs with MaxEnt RL is straightforward, as the reward function for MaxEnt RL is a simple function of the last transition.

Section Title: A COMPUTATIONAL EXPERIMENT
  A COMPUTATIONAL EXPERIMENT To conclude this section, we present a simple computational experiment to verify that Max- Ent RL does solve the meta-POMDP. We instantiated the meta-POMDP using 5-armed ban- dits. Each meta-POMDP is specified by a prior belief p(i) over the target arm. We sam- ple this distribution from a Dirichlet(1). To solve this meta-POMDP, we applied MaxEnt RL using a reward function r(i) = 1 2 log p(i), as derived above. When the agent pulls arm i, it observes a noisy reward r i ∼ N (r(i), 1). To implement the MaxEnt RL approach, we maintained the posterior of the reward r(i), given our observations so far. We initialized our beliefs with a zero-mean, unit-variance Gaussian prior. To obtain a policy with MaxEnt RL, we chose actions with probability proportional to the exponentiated expected reward. Through- out training, we tracked the regret (Eq. 4.1). Since the minimum regret possible for each meta-POMDP is different, we normalized the regret for each meta-POMDP by dividing by the minimum possible regret. Thus, the nor- malized regret lies in the interval [1,  ∞ ], with lower being better.  Figure 1  shows that MaxEnt RL converges to the regret-minimizing policy for each meta-POMDP. Code to reproduce all experiments is available online.

Section Title: MAXIMUM ENTROPY RL AND ADVERSARIAL GAMES
  MAXIMUM ENTROPY RL AND ADVERSARIAL GAMES While the meta-POMDP considered in the previous setting was defined in terms of task uncertainty, that uncertainty was fixed throughout the learning process. We now consider uncertainty introduced by an adversary who perturbs the reward function, and show how MaxEnt RL's aversion to deterministic policies provides robustness against these sorts of adversaries. In particular, we show MaxEnt RL is equivalent to solving robust-reward control, and run a computational experiment to support our claims. We generalize these results in Appendix B.

Section Title: MAXENT RL SOLVES ROBUST-REWARD CONTROL
  MAXENT RL SOLVES ROBUST-REWARD CONTROL Our main result on reward robustness builds on the general equivalence between entropy maximization and game theory from prior work. To start, we note two results from prior work that show how entropy maximization can be written as a robust optimization problem: Lemma 5.1 ( Grünwald et al. (2004) ). Let x be a random variable, and let P be the set of all distributions over x. The problem of choosing a maximum entropy distribution for x and maximizing the worst-case log-loss are equivalent: Under review as a conference paper at ICLR 2020 An immediately corollary is that maximizing the entropy of any conditional distribution is equivalent to a robust optimization problem: Corollary 5.1.1 ( Grünwald et al. (2004) ;  Ziebart et al. (2011) ). Let x and y be random variables, and let P x|y be the set of all conditional distributions p(x | y). The problem of choosing a maximum entropy distribution for the conditional distribution p(x | y) and maximizing the worst-case log-loss of x given y are equivalent: In short, prior work shows that the principle of maximum entropy results minimizes worst-case performance on prediction problems that use log-loss. Our contribution extends this result to show that MaxEnt RL minimizes worst-case performance on reinforcement learning problems for certain classes of reward functions. Theorem 5.2. The MaxEnt RL objective for a reward function r is equivalent to the robust-reward control objective for a certain class of reward functions: For completeness, we provide a proof in Appendix B.1. We will call the set R r of reward functions a robust set. While the adversary in Theorem 5.2 may seem peculiar, this result can also be viewed as providing a lower bound on worst-case performance against an unknown reward function. As an aside, we note that the log q(a | s) term in the definition of the robust set arises from the fact that we consider MaxEnt RL algorithms using Shannon entropy. MaxEnt RL algorithms using other notions of entropy ( Lee et al., 2019a ;  Chow et al., 2018 ) would result in different robust sets (see  Grünwald et al. (2004) ). We leave this generalization for future work.

Section Title: A SIMPLE EXAMPLE
  A SIMPLE EXAMPLE In  Figure 2 , we consider a simple, 2-armed bandit, with the following reward function: The robust set is then defined as   Figure 2  (left) traces the original reward function and this robust set. Plotting the robust-reward control objective (center) and the MaxEnt RL objective (right), we observe that they are equivalent. We ran an experiment to support our claim that MaxEnt RL is equivalent to solving the robust- reward problem. The mean for arm i, µ i , is drawn from a zero-mean, unit-variance Gaus- sian distribution, µ i ∼ N (0, 1). When the agent pulled arm i, it observes a noisy reward r i ∼ N (µ i , 1). We implement the MaxEnt RL approaches as in Section 4.4. As a baseline, we compare to fictitious play (Brown, 1951), an al- gorithm for solving two-player, zero-sum games. Fictitious play alternates between choosing the best policy w.r.t. the historical average of ob- serves rewards, and choosing the worst reward function for the historical average of policies. We choose the worst reward function from the robust set (Eq. 2). For fair comparison, the pol- icy only observes the (noisy) reward associated with the selected arm. We also compared to an oracle version of fictitious play that observes the (noisy) rewards associated with all arms, including arms not selected. We ran each method on the same set of 10 bandit problems, and evaluated the worst-case reward for each method (i.e., the expected reward of the policy, if the reward function were adversarially chosen from the robust set). Because each problem had a different minimax reward, we normalized the worst-case reward by the worst-case reward of the optimal policy. The normalized rewards are therefore in the interval [0, 1], with 1 being optimal.  Figure 3  plots the normalized reward throughout training. The main result is that MaxEnt RL converges to a policy that achieves optimal minimax reward, supporting our claim that MaxEnt RL is equivalent to a robust-reward control problem. The failure of fictitious play to solve this problem illustrates that the robust-reward control problem is not trivial to solve. Only the oracle version of fictitious play, which makes assumptions not made by MaxEnt RL, is competitive with MaxEnt RL. In Appendix C, we run a similar experiment on four robotic control tasks and find that MaxEnt RL optimizes the minimax reward better than standard RL and fictitious play.

Section Title: DISCUSSION
  DISCUSSION In summary, this paper studies connections between MaxEnt RL and control problems with variability in the reward function. While MaxEnt RL is a relatively simple algorithm, the problems that it solves, such as robust-reward control and regret minimization in the meta-POMDP, are typically viewed as quite complex. This result hints that MaxEnt RL might also be used to solve even broader classes of control problems. The principle of maximum entropy has also been applied to inverse RL, and we encourage future work to consider whether MaxEnt IRL ( Ziebart et al., 2008 ) is implicitly robust to some sort of variability. Finally, we speculate that our results may help understand behavior in the natural world. The abundance of evidence for probability matching in nature suggests that, in the course of evolution, creatures that better handled uncertainty and avoided adversaries were more likely to survive. We encourage RL researchers to likewise focus their research on problem settings likely to occur in the real world.

```
