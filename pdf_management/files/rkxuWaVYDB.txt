Title:
```
Under review as a conference paper at ICLR 2020 OPTIMAL ATTACKS ON REINFORCEMENT LEARNING POLICIES
```
Abstract:
```
Control policies, trained using the Deep Reinforcement Learning, have been re- cently shown to be vulnerable to adversarial attacks introducing even very small perturbations to the policy input. The attacks proposed so far have been designed using heuristics, and build on existing adversarial example crafting techniques used to dupe classifiers in supervised learning. In contrast, this paper investigates the problem of devising optimal attacks, depending on a well-defined attacker's objective, e.g., to minimize the main agent average reward. When the policy and the system dynamics, as well as rewards, are known to the attacker, a scenario referred to as a white-box attack, designing optimal attacks amounts to solving a Markov Decision Process. For what we call black-box attacks, where neither the policy nor the system is known, optimal attacks can be trained using Reinforcement Learning techniques. Through numerical experiments, we demonstrate the efficiency of our attacks compared to existing attacks (usually based on Gradient methods). We further quantify the potential impact of attacks and establish its connection to the smoothness of the policy under attack. Smooth policies are naturally less prone to attacks (this explains why Lipschitz policies, with respect to the state, are more resilient). Finally, we show that from the main agent perspective, the system uncertainties and the attacker can be modelled as a Partially Observable Markov Decision Process. We actually demonstrate that using Reinforcement Learning techniques tailored to POMDP (e.g. using Recurrent Neural Networks) leads to more resilient policies.
```

Figures/Tables Captions:
```
Figure 1: Average performance loss vs. attack amplitude ε: (top-left) discrete MountainCar, (top- right) Cartpole, (bottom-left) continuous MountainCar, (bottom-right) LunarLander. The attack policy φ has been trained only for ε = 0.05 In red is shown the FGM attack, in blue the attack proposed in this paper.
Figure 2: Game of Pong. (Left part) Images and corresponding agent's action distributions before and after the attack. (Right part) Average agent's reward at different phases of the attack training.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Advances in Deep Reinforcement Learning (RL) have made it possible to train end-to-end policies achieving superhuman performance on a large variety of tasks, such as playing Atari games  Mnih et al. (2013 ; 2016), playing  Go Silver et al. (2016 ; 2017), as well as controlling systems with continuous state and action spaces  Lillicrap et al. (2016) ;  Schulman et al. (2015) ;  Levine et al. (2016) . Recently, some of these policies have been shown to be vulnerable to adversarial attacks at test time see e.g.  Huang et al. (2017) ;  Pattanaik et al. (2018) . Even if these attacks only introduce small perturbations to the successive inputs of the policies, they can significantly impact the average collected reward by the agent. So far, attacks on RL policies have been designed using heuristic techniques, based on gradient methods, essentially the Fast Gradient Sign Method (FGSM). As such, they do not explicitly aim at minimizing the reward collected by the agent. In contrast, in this paper, we investigate the problem of casting optimal attacks with well-defined objectives, for example minimizing the average reward collected by the agent. We believe that casting optimal attacks is crucial when assessing the robustness of RL policies, since ideally, the agent should learn and apply policies that resist any possible attack (of course with limited and reasonable amplitude). For a given policy learnt by the agent, we show that the problem of devising an optimal attack can be formulated as a Markov Decision Process (MDP), defined through the system dynamics, the agent's reward function and policy. We are mainly interested in black-box attacks: to devise them, the adversary only observes the variables from which she builds her reward. For example, if the objective is to lead the system to a certain set of (bad) states, the adversary may just observe the states as the system evolves. If the goal is to minimize the cumulative reward collected by the agent, the Under review as a conference paper at ICLR 2020 adversary may just observe the system states and the instantaneous rewards collected by the agent. In black-box attacks, the aforementioned MDP is unknown, and optimal attacks can be trained using RL algorithms. The action selected by the adversary in this MDP corresponds to a perturbed state to be fed to the agent's policy. As a consequence, the action space is typically very large. To deal with this issue, our optimal attack is trained using DDPG  Lillicrap et al. (2016) , a Deep RL algorithm initially tailored to continuous action space. We train and evaluate optimal attacks for some OpenAI Gym  Brockman et al. (2016)  environments with discrete action spaces (discrete MountainCar, Cartpole, and Pong), and continuous state-action spaces (continuous MountainCar and continuous LunarLander). As expected, optimal attacks outper- form existing attacks. We discuss how the methods used to train RL policies impact their resilience to attacks. We show that the damages caused by attacks are upper bounded by a quantity directly related to the smoothness of the policy under attack 1 . Hence, training methods such as DDPG leading to smooth policies should be more robust. Finally, we remark that when under attack, the agent faces an environment with uncertain state feedback, and her sequential decision problem can be modelled as a Partially Observable MDP (POMDP). This suggests that training methods specifically tailored to POMDP (e.g. DRQN  Hausknecht & Stone (2015) ) result in more resistant policies. These observations are confirmed by our experiments.

Section Title: RELATED WORK
  RELATED WORK Adversarial attacks on RL policies have received some attention over the last couple of years. As for now, attacks concern mainly the system at test time, where the agent has trained a policy using some Deep RL algorithm (except for Behzadan & Munir (2017), which considers an attack during training). The design of these attacks  Huang et al. (2017) ;  Pattanaik et al. (2018) ;  Lin et al. (2017)  are inspired by the Fast Gradient Sign Method (FGSM)  Goodfellow et al. (2015) , which was originally used to fool Deep Learning classifiers. Fast Gradient Methods (FGMs) are gradient methods that changes the input with the aim of optimizing some objective function. This function, and its gradient, are sampled using observations: in case of an agent trained using Deep RL, this function can be hence related to a distribution of actions given a state (the actor-network outputs a distribution over actions), or the Q-value function (the critic outputs approximate Q-values). In  Huang et al. (2017) , FGM is used to minimize the probability of the main agent selecting the optimal action according to the agent. On the other hand, in  Pattanaik et al. (2018) , the authors develop a gradient method that maximizes the probability of taking the action with minimal Q-value. The aforementioned attacks are based on the unperturbed policy of the agent, or its Q-value function, and do not consider optimizing the attack over all possible trajectories. Therefore, they can be considered a first-order approximation of an optimal attack. If the adversary wishes to minimize the average cumulative reward of the agent, she needs to work on the perturbed agent policy (to assess the impact of an attack, we need to compute the rewards after the attack). Proactive and reactive defence methods have been proposed to make RL policies robust to gradient attacks. Proactive methods are essentially similar to those in supervised learning, i.e., they are based on injecting adversarial inputs during training  Kos & Song (2017) ;  Pattanaik et al. (2018) . A few reactive methods have also been proposed  Lin et al. (2017) . There, the idea is to train a separate network predicting the next state. This network is then used to replace the state input by the predicted state if this input is believed to be too corrupted. But as we show in the paper, adversarial examples introduce partial observability, and the problem is not Markov anymore. It is known  Singh et al. (1994)  that deterministic memory-less policies are inadequate to deal with partial observability, and some kind of memory is needed to find a robust policy. In this work, we make use of recurrent neural networks to show that we can use techniques from the literature of Partially Observable MDPs to robustify RL policies. It is finally worth mentioning that there has been some work studying the problem of robust RL with a game-theoretical perspective  Morimoto & Doya (2005) ;  Pinto et al. (2017) . However, these papers Under review as a conference paper at ICLR 2020 consider a very different setting where the adversary has a direct impact on the system (not indirectly through the agent as in our work). In a sense, they are more related to RL for multi-agent systems.

Section Title: PRELIMINARIES
  PRELIMINARIES This section provides a brief technical background on Markov Decision Process and Deep Reinforce- ment Learning. The notions and methods introduced here are used extensively in the remainder of the paper.

Section Title: Markov Decision Process
  Markov Decision Process A Markov Decision Process (MDP) defines a discrete-time controlled Markov chain. It is defined by a tuple M = S, (A s , s ∈ S), P, p 0 , q . S is the finite state space, A s is the finite set of actions available in state s, P represents the system dynamics where P (·|s, a) is the distribution of the next state given that the current state is s and the selected action is a, p 0 is the distribution of the initial state, and q characterizes rewards where q(·|s, a) is the distribution of the collected reward in state is s when action a is selected. We denote by r(s, a) the expectation of this reward. We assume that the average reward is bounded: for all (s, a), |r(s, a)| ≤ R. A policy π for M maps the state and to a distribution of the selected action. For a ∈ A s , π(a|s) denotes the probability of choosing action a in state s. The value function V π of a policy π defines for every s the average cumulative discounted reward under π when the system starts in state s: V π (s) = E[ t≥0 γ t r(s π t , a π t )|s 0 = s] where s π t and a π t are the state and the selected action at time t under policy π, and γ ∈ (0, 1) is the discount factor. The Q-value function Q π of policy π maps a (state, action) pair (s, a) to the average cumulative discounted reward obtained starting in state s when the first selected action is a and subsequent actions are chosen according to π: Q π (s, a) = r(s, a) + γE s ∼P (·|s,a) [V π (s )]. For a given MDP M, the objective is to devise a policy with maximal value in every state.

Section Title: Deep Reinforcement Learning
  Deep Reinforcement Learning To train policies with maximal rewards for MPDs with large state or action spaces, Deep Reinforcement Learning consists in parametrizing the set of policies or some particular functions of interest such as the value function or the Q-value function of a given policy by a deep neural network. In this paper, we consider various Deep RL techniques to train either the policy of the agent or the attack. These include: DQN  Mnih et al. (2013) : the network aims at approximating the Q-function of the optimal policy. DDPG  Lillicrap et al. (2016) : an actor-critic method developed to deal with continuous state-action spaces. To this aim, the actor returns a single action rather than a distribution over actions. DRQN  Hausknecht & Stone (2015) : introduces memory in DQN by replacing the (post-convolutional) layer by a recurrent LSTM. The use of this recurrent structure allows us to deal with partial observ- ability, and DRQN works better with POMDPs.

Section Title: OPTIMAL ADVERSARIAL ATTACKS
  OPTIMAL ADVERSARIAL ATTACKS To attack a given policy π trained by the main agent, an adversary can slightly modify the sequence of inputs of this policy. The changes are imposed to be small, according to some distance, so that the attack remains difficult to detect. The adversary aims at optimizing inputs with a precise objective in mind, for example, to cause as much damage as possible to the agent. We denote by M = S, (A s , s ∈ S), P, p 0 , q the MDP solved by the agent, i.e., the agent policy π has been trained for M.

Section Title: THE ATTACK MDP
  THE ATTACK MDP To attack the policy π the adversary proceeds as follows. At time t, the adversary observes the system state s t . She then selects a perturbed states t , which becomes the input of the agent policy π. The agent hence chooses an action according to π(·|s t ). The adversary successively collects a random reward defined depending on her objective, which is assumed to be a function of the true state and the action selected by the agent. An attack is defined by a mapping φ : S → S wheres = φ(s) is the perturbed state given that the true state is s.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020

Section Title: Constrained perturbations
  Constrained perturbations For the attack to be imperceptible, the input should be only slightly modified. Formally, we assume that we can define a notion of distance d(s, s ) between two states s, s ∈ S. The state space of most RL problems can be seen as a subset of a Euclidean space, in which case d is just the Euclidean distance. We impose that given a state s of the system, the adversary can only select a perturbed states inĀ s = {s ∈ S : d(s,s) ≤ }. is the maximal amplitude of the state perturbation.

Section Title: System dynamics and agent's reward under attack
  System dynamics and agent's reward under attack Given that the state is s t = s at time t and that the adversary selects a modified states ∈Ā s , the agent selects an action according to the distribution π(·|s). Thus, the system state evolves to a random state with distribution P π (·|s,s) := a π(a|s)P (·|s, a). The agent instantaneous reward is a random variable with distribution a π(a|s)q(·|s, a).

Section Title: Adversary's reward
  Adversary's reward The attack is shaped depending on the adversary's objective. The adversary typically defines her reward as a function of the true state and the action selected by the agent, or more concisely as a direct function of the reward collected by the agent. The adversary might be interested in guiding the agent to some specific states. In control systems, the adversary may wish to induce oscillations in the system output or to reduce its controllability (this can be realized by choosing a reward equal to the energy spent by the agent, i.e., proportional to a 2 if the agent selects a). The most natural objective for the adversary is to minimize the average cumulative reward collected by the agent. In this case, the adversary would set her instantaneous reward equal to the opposite of the agent's reward. We denote byq π (·|s,s) the distribution of the reward collected by the adversary in state s when she modifies the input tos, and byr π (s,s) its expectation. For example, when the adversary wishes to minimize the agent's average cumulative reward, we haver π (s,s) = − a π(a|s)r(s, a). We have shown that designing an optimal attack corresponds to identifying a policy φ that solves the following MDP:M π = S, (Ā s , s ∈ S),P π , p 0 ,q π . When the parameters of this MDP are known to the adversary, a scenario referred to as white box attack, finding the optimal attack accounts to solving the MDP, e.g., by using classical methods (value or policy iteration). More realistically, the adversary may ignore the parameters ofM, and only observe the state evolution and her successive instantaneous rewards. This scenario is called black-box attack, and in this case, the adversary can identify the optimal attack using Reinforcement Learning algorithms. For ease of notation, we will denote the value of the adversarial policy φ as V φ (for the MDPM π ), even though it depends on π. Finally, observe that when the adversarial attack policy is φ, then the system dynamics and rewards correspond to a scenario where the main agent applies the perturbed policy π • φ, which is defined by the distributions (π • φ)(·|s) := Es ∼φ(s) [π(·|s)], s ∈ S. Hence, we can denote the value of the perturbed policy as V π•φ (for the MDP M).

Section Title: MINIMIZING AGENT'S AVERAGE REWARD
  MINIMIZING AGENT'S AVERAGE REWARD Next, we give interesting properties of the MDPM π . When the adversary aims at minimizing the average cumulative reward of the agent, then the reward collected by the agent can be considered as a cost by the adversary. In this case,r π (s,s) = − a π(a|s)r(s, a), and one can easily relate the value function of an attack policy φ to that of the agent policy π • φ: The Q-value function of an attack policy φ can also be related to the Q-value function of the agent policy under attack π • φ. Indeed: for all s,s ∈ S. In particular, if the agent policy is deterministic (π(s) ∈ A s is the action selected under π in state s), we simply have: Equation (2) has an important consequence when we train an attack policy using Deep Learning algorithms. It implies that evaluating the Q-value of an attack policy φ can be done by evaluating Under review as a conference paper at ICLR 2020 the Q-value function of the perturbed agent policy π • φ. In the case of off-policy learning, the former evaluation would require to store in the replay buffer experiences of the type (s,s, r) (r is the observed reward), whereas the latter just stores (s, π(s), r) (but it requires to observe the actions of the agent). This simplifies considerably when the state space is much larger than the action space.

Section Title: TRAINING OPTIMAL ATTACKS
  TRAINING OPTIMAL ATTACKS Training an optimal attack can be very complex, since it corresponds to solving an MDP where the action space size is similar to that of the state space. In our experiments, we use two (potentially combined) techniques to deal with this issue: (i) feature extraction when this is at all possible, and (ii) specific Deep RL techniques tailored to very large (or even continuous) state and action spaces, such as DDPG.

Section Title: Feature extraction
  Feature extraction One may extract important features of the system admitting a low-dimensional representation. Formally, this means that by using some expert knowledge about the system, one can identify a bijective mapping between the state s in S, and its features z in Z of lower dimension. In general, this mapping can also be chosen such that its image of the ball A s is easy to represent in Z (typically this image would also be a ball around z, the feature representation of s). It is then enough to work in Z. When Z is of reasonable size, one may then rely on existing value-based techniques (e.g. DQN) to train the attack. An example where features can be easily extracted is the Atari game of pong: the system state is just represented by the positions of the ball and the two bars. Finally, we assume to be very likely that an adversary trying to disrupt the agent policy would conduct an advanced feature engineering work before casting her attack.

Section Title: Deep RL: DDPG
  Deep RL: DDPG In many systems, it may not be easy to extract interesting features. In that case, one may rely on Deep RL algorithms to train the attack. The main issue here is the size of the action space. This size prevents us to use DQN (that would require to build a network with as many outputs as the number of possible actions), but also policy-gradient and actor-critic methods that parametrize randomized policies such as TRPO (indeed we can simply not maintain distributions over actions). This leads us to use DDPG, an actor-critic method that parametrizes deterministic policies. In DDPG, we consider policies parametrized by ω (φ ω is the policy parametrized by ω), and its approximated Q-value function Q θ parametrized by θ. We also maintain two corresponding target networks. The objective function to maximize is J(ω) = E s∼p0 [V φω (s)]. φ is updated using the following gradient  Silver et al. (2014) : ∇ ω J(ω) = E s∼ρ φ e [∇sQ θ (s, φ ω (s))∇ ω φ ω (s)], (3) where φ e corresponds to φ ω with additional exploration noise, and ρ φ e denotes the discounted state visitation distribution under the policy φ e . In the case where the adversary's objective is to minimize the average reward of the agent, in view of (2), the above gradient becomes: Finally, to ensure that the selected perturbation computed by φ ω belongs to A s , we add a last fixed layer to the network that projects onto the ball centred in 0, with radius . This is done by applying the function x → min 1, x x to x + e, where x is the output before the projection layer, and e is the exploration noise. After this projection, we add the state s to get φ e (s). The pseudo-code of the algorithm is provided in Algorithm 1 in the Appendix. Algorithm 2 presents a version of the algorithm using the gradient (4). A diagram of the actor and critic networks are also provided in Figure 8 of the Appendix.

Section Title: Gradient-based exploration
  Gradient-based exploration In case the adversary knows the policy π of the main agent, we can leverage this knowledge to tune the exploration noise e. Similarly, to gradient methods  Huang et al. (2017) ;  Pattanaik et al. (2018) , we suggest using the quantity ∇ s J(π(·|s), y) to improve the exploration process , where J is the cross-entropy loss between π(·|s) and a one-hot vector y that encodes the action with minimum probability in state s. This allows exploring directions that might minimize the value of the unperturbed policy, which boosts the rate at which the optimal attack is learnt. At time t, the exploration noise could be set to e t , a convex combination of a white exploration noise e t and f (s t ) = g(−∇ s J(π(·|s t ), y t )), where g is a normalizing function. Following this gradient introduces bias, hence we randomly choose when to use e t or e t (more details can be found in the Appendix).

Section Title: ROBUSTNESS OF POLICIES
  ROBUSTNESS OF POLICIES In this section, we briefly discuss which training methods should lead to agent policies more resilient to attacks. We first quantify the maximal impact of an attack on the cumulative reward of the agent policy, and show that it is connected to the smoothness of the agent policy. This connection indicates that the smoother the policy is, the more resilient it is to attacks. Next, we show that from the agent perspective, an attack induces a POMDP. This suggests that if training the agent policy using Deep RL algorithms tailored to POMDP yields more resilient policies.

Section Title: Impact of an attack and policy smoothness
  Impact of an attack and policy smoothness When the agent policy π is attacked using φ, one may compute the cumulative reward gathered by the agent. Indeed, the system evolves as if the agent policy was π • φ. The corresponding value function satisfies the Bellman equation: V π•φ (s) = E a∼π•φ(·|s) r(s, a) + γE s ∼P (·|s,a) [V π•φ (s )] . Starting from this observation, we derive an upper bound on the impact of an attack (see the Appendix for a proof): Proposition 5.1. For any s ∈ S, let 2 α π,ε (s) = maxs ∈A ε s π(·|s) − π(·|s) T V . We have: Assume now that π is smooth in the sense that, for all s, s ∈ S, π(·|s) − π(·|s ) T V ≤ Ld(s, s ), then In the above, α π,ε (s) quantifies the potential impact of applying a feasible perturbation to the state s on the distribution of actions selected by the agent in this state (note that it decreases to 0 as ε goes to 0). The proposition establishes the intuitive fact that when the agent policy is smooth (i.e., varies smoothly with the input), it should be more resilient to attacks. Indeed, in our experiments, policies trained using Deep RL methods known to lead to smooth policies (e.g. DDPG for RL problems with continuous state and action spaces) resist better to attacks.

Section Title: Induced POMDP and DRQN
  Induced POMDP and DRQN When the agent policy is under the attack φ, the agent perceives the true state s only through its perturbation φ(s). More precisely, the agent observes a perturbed statē s only, with probability O(s|s) = 1 {φ(s)=s} (in general O could also be a distribution over states if the attack is randomized). Therefore, an optimall perturbation policy φ induces a deterministic POMDP. It is known Spaan (2012);  Astrom (1965) ;  Singh et al. (1994)  that solving POMDPs requires "memory", i.e., policies whose decisions depend on the past observations (non-Markovian). Hence, we expect that training the agent policy using Deep RL algorithms tailored to POMDP produces more resilient policies. In  Hausknecht & Stone (2015)  they empirically demonstrate that recurrent controllers have a certain degree of robustness against missing information, even when trained with full state information. We use this to pro-actively extract robust features, not biased by a specific adversary policy φ, as in adversarial training. This is confirmed in our experiments, comparing the impact of attacks on policies trained by DRQN to those trained using Deep RL algorithms without recurrent structure (without memory).

Section Title: EXPERIMENTAL EVALUATION
  EXPERIMENTAL EVALUATION We evaluate our attacks on four OpenAI Gym  Brockman et al. (2016)  environments (A: discrete MountainCar, B: Cartpole, C: continuous MountainCar, and D: continuous LunarLander), and on E: Pong, an Atari 2600 game in the Arcade Learning Environment  Bellemare et al. (2013) . In Appendix, we also present the results of our attacks for the toy example of the Grid World problem, to illustrate why gradient-based attack are sub-optimal.

Section Title: Agent's policies
  Agent's policies The agent policies are trained using DQN or DRQN in case of discrete action spaces (environments A, B, and E), and using DDPG when the action space is continuous (environments C and D). In each environment and for each training algorithm, we have obtained from 3 policies for DQN, 1 for DRQN (achieving at least 95% of the performance of the best policy reported in OpenAI Gym).

Section Title: Adversarial attack
  Adversarial attack For each environment A, B, C, and D, we train one adversarial attack using DDPG against one of the agent's policies, and for a perturbation constraint ε = 0.05 (we use the same normalization method as in  Huang et al. (2017) ). For results presented below, we use uniformly distributed noise; results with gradient-based noise are discussed in the Appendix. To get attacks for other values of ε, we do not retrain our attack but only change the projection layer in our network. For environment E, we represent the state as a 4-dimensional feature vector, and train the attack using DQN in the feature space. The full experimental setup is presented in the Appendix.

Section Title: OPTIMAL VS. GRADIENT-BASED ATTACKS
  OPTIMAL VS. GRADIENT-BASED ATTACKS We compare our attacks to the FGM attacks (gradient-based) proposed in Pattanaik  et al. (2018)  (we found that these are the most efficient). To test a particular attack, we run it against the 3 agent's policies, using 10 random seeds (e.g. involved in the state initialization), and 30 episodes. We hence generate up to 1800 episodes, and average the cumulative reward of the agent. In  Figure 1 , we plot the performance loss for different the perturbation amplitudes ε in the 4 environments. Observe that the optimal attack consistently outperforms the gradient-based attack, and significantly impact the performance of the agent. Also note that since we have trained our attack for ε = 0.05, it seems that this attack generalizes well for various values of ε. In Appendix, we present similar plots but not averaged over the 3 agent's policies, and we do not see any real difference - suggesting that attacks generalize well to different agent's policies. From  Figure 1 , for environments with discrete action spaces (the top two sets of curves), the resilience of policies trained by DRQN is confirmed: these policies are harder to attack. Policies trained using DDPG for environments with continuous action spaces (the bottom two sets of curves in  Figure 1 ) are more difficult to perturb than those trained in environments with discrete action spaces. The gradient-based attack does not seem to be efficient at all, at least in the range of attack amplitude ε considered. In the case of LunarLander some state components were not normalized, which may explain why a bigger value of ε is needed in order to see a major performance loss (at least ε = 0.1). Our optimal attack performs better, but the impact of the attack is not as significant as in environments with discrete action spaces: For instance, for discrete and continuous MountainCar, our attack with ε = 0.07 decreases the performance of the agent by 30% and 13%, respectively.

Section Title: ATTACK ON PONG, BASED ON FEATURE EXTRACTION
  ATTACK ON PONG, BASED ON FEATURE EXTRACTION We consider the game of Pong where the state is an 84x84 pixel image. The agent's policies were trained using DQN using as inputs the successive images. As for the attack, we extracted a 4- dimensional feature vector fully representing the state: this vector encodes the position of the centre Under review as a conference paper at ICLR 2020 Unperturbed image Perturbed image Unperturbed image Perturbed image of mass of the three relevant objects in the image (the ball and the two bars). Extracting the features is done using a high pass filter to detect the contours. We need 4-dimensional vectors since the two bars have a single degree of freedom, and the ball has two. We limit the amplitude of the attack in the feature space directly. More precisely, we impose that the adversary just changes one of the 4 components of the feature vector by one pixel only. For example, the adversary can move up one bar by one pixel. We found that the amplitude of the resulting attack in the true state space (full images) corresponds to ε = 0.013. The attack is trained using DQN in the feature space. In  Figure 2  (left part), we present a few true and perturbed images, and below the corresponding distributions of the actions selected by the agent without and with the attack (the probabilities are calculated from the output of the network using a softmax function with temperature T = 1). Moving objects by one pixel can perturb considerably the agent's policy. In  Figure 2  (right part), we show the evolution of the performance of the agent's policy under our attack during the training of the latter. Initially, the agent's policy achieves the maximum score, but after 500 episodes of training, the agent's score has become close to the lowest possible score in this game (-20). This score can be reached in an even fewer number of episodes when tuning the training parameters. The gradient-based attacks performed in  Huang et al. (2017)  cannot be directly compared to ours, since these attacks change 4 frames at every time step, while ours modifies only the last observation. They also modify the values of every pixel in frames (using 32-bit precision over the reals [0,1]), whereas we change only the values of just a few (using 8-bit precision over the integers [0,255]).

Section Title: CONCLUSION
  CONCLUSION In this paper we have formulated the problem of devising an optimal black-box attack that does not require access to the underlying policy of the main agent. Previous attacks, such as FGM  Huang et al. (2017) , make use of white-box assumptions, i.e., knowing the action-value function Q, or the policy, of the main agent. In our formulation, we do not assume this knowledge. Deriving an optimal attack is important in order to understand how to build RL policies robust to adversarial perturbations. The problem has been formulated as a Reinforcement Learning problem, where the goal of the adversary is encapsulated in the adversarial reward function. The problem becomes intractable when we step out of toy problems: we propose a variation of DDPG to compute the optimal attack. In the white-box case, instead of using FGM, we propose to use a gradient-based method to improve exploration. Adapting to such attacks requires solving a Partially Observable MDP, hence we have to resort to non-markovian policies  Singh et al. (1994) . It can be achieved by adopting recurrent layers, as in DRQN  Hausknecht & Stone (2015) . We also show that Lipschitz policies have desirable robustness properties. We validated our algorithm on different environments. In all cases we have found out that our attack outperforms gradient methods. This is more evident in discrete action spaces, whilst in continuous spaces it is more difficult to perturb for small values of ε, which may be explained by the bound we provide for Lipschitz policies. Finally, policies that use memory seem to be more robust in general. Under review as a conference paper at ICLR 2020
  1 A policy is smooth when the action it selects smoothly varies with the state. Refer to proposition 5.1 for details.

```
