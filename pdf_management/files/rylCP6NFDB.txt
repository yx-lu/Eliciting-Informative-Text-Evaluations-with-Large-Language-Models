Title:
```
Under review as a conference paper at ICLR 2020 HINDSIGHT TRUST REGION POLICY OPTIMIZATION
```
Abstract:
```
As reinforcement learning continues to drive machine intelligence beyond its con- ventional boundary, unsubstantial practices in sparse reward environment severely limit further applications in a broader range of advanced fields. Motivated by the demand for an effective deep reinforcement learning algorithm that accommo- dates sparse reward environment, this paper presents Hindsight Trust Region Pol- icy Optimization (HTRPO), a method that efficiently utilizes interactions in sparse reward conditions to optimize policies within trust region and, in the meantime, maintains learning stability. Firstly, we theoretically adapt the TRPO objective function, in the form of the expected return of the policy, to the distribution of hindsight data generated from the alternative goals. Then, we apply Monte Carlo with importance sampling to estimate KL-divergence between two policies, tak- ing the hindsight data as input. Under the condition that the distributions are sufficiently close, the KL-divergence is approximated by another f -divergence. Such approximation results in the decrease of variance and alleviates the insta- bility during policy update. Experimental results on both discrete and continu- ous benchmark tasks demonstrate that HTRPO converges significantly faster than previous policy gradient methods. It achieves effective performances and high data-efficiency for training policies in sparse reward environments.
```

Figures/Tables Captions:
```
Figure 1: Demonstration of experimental environments
Figure 2: Evaluation curves for discrete environments. The full lines represent the average evalua- tion over 10 trails and the shaded regions represent the corresponding standard deviation.
Figure 3: Evaluation curves for continuous environments. The full lines represent the average eval- uation over 10 trails and the shaded regions represent the corresponding standard deviation.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement Learning has been a heuristic approach confronting a great many real-world prob- lems from playing complex strategic games ( Mnih et al., 2015 ;  Silver et al., 2016 ;  Justesen et al., 2019 ) to the precise control of robots( Levine et al., 2016 ; Mahler & Goldberg, 2017;  Quillen et al., 2018 ), in which policy gradient methods play very important roles( Sutton et al., 2000 ;  Deisenroth et al., 2013 ). Among them, the ones based on trust region including Trust Region Policy Opti- mization ( Schulman et al., 2015a ) and Proximal Policy Optimization ( Schulman et al., 2017 ) have achieved stable and effective performances on several benchmark tasks. Later on, they have been verified in a variety of applications including skill learning( Nagabandi et al., 2018 ), multi-agent control( Gupta et al., 2017 ), imitation learning( Ho et al., 2016 ), and have been investigated further to be combined with more advanced techniques( Nachum et al., 2017 ; Houthooft et al., 2016;  Heess et al., 2017 ). One unresolved core issue in reinforcement learning is efficiently training the agent in sparse reward environments, in which the agent is given a distinctively high feedback only upon reaching the desired final goal state. On one hand, generalizing reinforcement learning methods to sparse reward scenarios obviates designing delicate reward mechanism, which is known as reward shaping( Ng et al., 1999 ); on the other hand, receiving rewards only when precisely reaching the final goal states also guarantees that the agent can focus on the intended task itself without any deviation. Despite the extensive use of policy gradient methods, they tend to be vulnerable when dealing with sparse reward scenarios. Admittedly, policy gradient may work in simple and sufficiently rewarding environments through massive random exploration. However, since it relies heavily on the expected return, the chances in complex and sparsely rewarding scenarios become rather slim, which often makes it unfeasible to converge to a policy by exploring randomly. Recently, several works have been devoted to solving the problem of sparse reward, mainly applying either hierarchical reinforcement learning ( Kulkarni et al., 2016 ;  Vezhnevets et al., 2017 ;  Le et al., 2018 ;  Marino et al., 2019 ) or a hindsight methodology, including Hindsight Experience Replay Under review as a conference paper at ICLR 2020 ( Andrychowicz et al., 2017 ), Hindsight Policy Gradient ( Rauber et al., 2019 ) and their extensions ( Fang et al., 2019 ;  Levy et al., 2019 ). The idea of Hindsight Experience Replay(HER) is to regard the ending states obtained through the interaction under current policy as alternative goals, and therefore generate more effective training data comparing to that with only real goals. Such augmentation overcomes the defects of random exploration and allows the agent to progressively move towards intended goals. It is proven to be promising when dealing with sparse reward reinforcement learning problems. For Hindsight Policy Gradient(HPG), it introduces hindsight to policy gradient approach and im- proves sample efficiency in sparse reward environments. Yet, its learning curve for policy update still oscillates considerably. Because it inherits the intrinsic high variance of policy gradient meth- ods which has been widely studied in  Schulman et al. (2015b) ,  Gu et al. (2016)  and  Wu et al. (2018) . Furthermore, introducing hindsight to policy gradient methods would lead to greater vari- ance ( Rauber et al., 2019 ). Consequently, such exacerbation would cause obstructive instability during the optimization process. To design an advanced and efficient on-policy reinforcement learning algorithm with hindsight expe- rience, the main problem is the contradiction between on-policy data needed by the training process and the severely off-policy hindsight experience we can get. Moreover, for TRPO, one of the most significant property is the approximated monotonic converging process. Therefore, how these ad- vantages can be preserved when the agent is trained with hindsight data also remains unsolved. In this paper, we propose a methodology called Hindsight Trust Region Policy Optimization (HTRPO). Starting from TRPO, a hindsight form of policy optimization problem within trust region is theoretically derived, which can be approximately solved with the Monte Carlo estimator using severely off-policy hindsight experience data. HTRPO extends the effective and monotonically iter- ative policy optimization procedure within trust region to accommodate sparse reward environments. In HTRPO, both the objective function and the expectation of KL divergence between policies are estimated using generated hindsight data instead of on-policy data. To overcome the high variance and instability in KL divergence estimation, another f -divergence is applied to approximate KL divergence, and both theoretically and practically, it is proved to be more efficient and stable. We demonstrate that on several benchmark tasks, HTRPO can significantly improve the performance and sample efficiency in sparse reward scenarios while maintains the learning stability. From the experiments, we illustrate that HTRPO can be neatly applied to not only simple discrete tasks but continuous environments as well. Besides, it is verified that HTRPO can be generalized to different hyperparameter settings with little impact on performance level.

Section Title: PRELIMINARIES
  PRELIMINARIES

Section Title: Reinforcement Learning Formulation and Notation
  Reinforcement Learning Formulation and Notation Consider the standard infinite-horizon re- inforcement learning formulation which can be defined by tuple (S, A, π, ρ 0 , r, γ). S represents the set of states and A denotes the set of actions. π : S → P(A) is a policy that represents an agent's behavior by mapping states to a probability distribution over actions. ρ 0 denotes the distribution of the initial state s 0 . Reward function r : S → R defines the reward obtained from the environment and γ ∈ (0, 1) is a discount factor. In this paper, the policy is a differentiable function regarding parameter θ. We follow the standard formalism of state-action value function Q(s, a), state value function V (s) and advantage function A(s, a) in  Sutton & Barto (2018) . We also adopt the defini- tion of γ-discounted state visitation distribution as ρ θ (s) = (1 − γ) ∞ t=0 γ t P (s t = s) ( Ho et al., 2016 ), in which the coefficient 1 − γ is added to keep the integration of ρ θ (s) as 1. Correspondingly, γ-discounted state-action visitation distribution ( Ho et al., 2016 ), also known as occupancy measure ( Ho & Ermon, 2016 ), is defined as ρ θ (s, a) = ρ θ (s) × π θ (a|s), in which π θ (a|s) stands for the policy under parameter θ. Trust Region Policy Optimization(TRPO).  Schulman et al. (2015a)  proposes an iterative trust region method that effectively optimizes policy by maximizing the per-iteration policy improvement. The optimization problem proposed in TRPO can be formalized as follows: Under review as a conference paper at ICLR 2020 s.t. E s∼ρθ(s) D KL (πθ(a|s)||π θ (a|s)) ≤ (2) in which ρθ(s) = ∞ t=0 γ t P (s t = s). θ denotes the parameter of the new policy whileθ is that of the old one. Trajectory is represented by τ = s 1 , a 1 , s 2 , a 2 , .... The objective function L T RP O (θ) can be given out in the form of expeted return: Hindsight Policy Gradient(HPG). After generalizing the concept of hindsight,  Rauber et al. (2019)  combines the idea with policy gradient methods. Though goal-conditioned reinforcement learning has been explored for a long time and actively investigated in recent works( Peters & Schaal, 2008 ;  Schaul et al., 2015 ;  Andrychowicz et al., 2017 ;  Nachum et al., 2018 ;  Held et al., 2018 ; Nair et al., 2018;  Veeriah et al., 2018 ), HPG firstly extends the idea of hindsight to goal-conditioned policy gradient and shows that the policy gradient can be computed in expectation over all goals. The goal-conditioned policy gradient is derived as follows: Then, by applying hindsight formulation, it rewrites goal-conditioned policy gradient with trajecto- ries conditioned on some other goal g using importance sampling ( Bishop, 2016 ) to improve sample efficiency in sparse-reward scenarios. In this paper, we propose an approach that introduces the idea of hindsight to TRPO, called Hind- sight Trust Region Policy Optimization(HTRPO), aiming to further improve policy performance and sample efficiency for reinforcement learning with sparse rewards. In Section 3 and Section 4, we demonstrate how to redesign the objective function and the constraints starting from TRPO respectively.

Section Title: EXPECTED RETURN AND POLICY GRADIENTS OF HTRPO
  EXPECTED RETURN AND POLICY GRADIENTS OF HTRPO In order to apply hindsight methodology, this section presents the main steps for the derivation of HTRPO objective function. Starting from the original optimization problem in TRPO, the objective function can be written in the following variant form: The derivation process of this variant form is shown explicitly in Appendix A.1 and in  Schulman et al. (2015a) . Given the expression above, we consider the goal-conditioned objective function of TRPO as a premise for hindsight formulation. Similar to equation 4, Lθ(θ) can be correspondingly given out in the following form: For the record, though it seems that equation 6 makes it possible for off-policy learning, it can be used as the objective only when policy π θ is close to the old policy πθ, i.e. within the trust region. Using severely off-policy data like hindsight experience will make the learning process diverge. Therefore, importance sampling need to be integrated to correct the difference of the trajectory distribution caused by changing the goal. Based on the goal-conditioned form of the objective function, the following theorem gives out the hindsight objective function conditioned on some goal g with the distribution correction derived from importance sampling. Theorem 3.1 (HTRPO Objective Function). For the original goal g and an alternative goal g , the object function of HTRPO Lθ(θ) is given by: Under review as a conference paper at ICLR 2020 in which, τ = s 1 , a 1 , s 2 , a 2 , ..., s t , a t . Appendix A.2 presents an explicit proof on how the hindsight-form objective function derives from equation 6. It will be solved under a KL divergence expectation constraint, which will be discussed in detail in Section 4. Intuitively, equation 7 provides a way to compute the expected return in terms of the advantage with new-goal-conditioned hindsight experiences which are generated from interactions directed by old goals. Naturally, Theorem 3.2 gives out the gradient of HTRPO objective function that will be applied to solve the optimization problem. Detailed steps of computing the gradient is presented in Appendix A.3. Theorem 3.2 (Gradient of HTRPO Objective Function). For the original goal g and an alternative goal g , the gradient ∇ θ Lθ(θ) of HTRPO object function with respect to θ is given by the following expression:

Section Title: EXPECTATION OF KL DIVERGENCE ESTIMATION
  EXPECTATION OF KL DIVERGENCE ESTIMATION This section firstly demonstrates some techniques, with strict proof, that can be used to estimate the expectation of KL-divergence and further reduce the variance, and then presents how hindsight is applied to the constraint function of TRPO. In TRPO, the KL divergence expectation under ρθ(s) is estimated by averaging all the values of KL divergence. When they are respectively conditioned on all states collected using the old policy, this kind of estimation is exactly Monte Carlo estimation which is unbiased. However, when we only have access to hindsight experience data, the state distribution may inevitably change and the previous method for estimating the expectation of KL divergence is no longer valid. To solve this problem, we firstly transform the KL divergence to an expectation under occupancy measure ρθ(s, a) = ρθ(s) × πθ(a|s). It can be estimated using collected state-action pair (s, a), whose changed distribution can be corrected by importance sampling. Then, by making use of another f -divergence, the variance of estimation is theoretically proved to be reduced so as to facilitating a more stable training. The constraint function in KL-divergence can be naturally converted to a logarithmic form. Ap- pendix B.1 provides a more explicit version of this conversion. Theorem 4.1 (Logarithmic Form of Constraint Function). Given two policies πθ(a|s) and π θ (a|s), the expectation of their KL-divergence over states s ∼ ρθ(s) is written as: However, simply expanding the KL-divergence into logarithmic form still leaves several problems unhandled. Firstly, the variance remains excessively high, which would cause considerable insta- bility during the learning process. Secondly, current estimation of KL-divergence is of possible negativity. If encountering negative expectation of KL-divergence, the learning process would result in fatal instability. The following Theorem 4.2 describes a technique to reduce the variance and Theorem 4.3 gives out the strict proof for the decrease of variance. Under review as a conference paper at ICLR 2020 Theorem 4.2 demonstrates that when θ andθ is of limited difference, the expectation of log πθ(a|s)− log π θ (a|s) can be sufficiently estimated by the expectation of its square. The proof is provided in Appendix B.2. In fact, Es,a∼ρθ(s,a) 1 2 (log πθ(a|s) − log π θ (a|s)) 2 is the expectation of an f - divergence, where f (x) = 1 2 x(log x) 2 . Noticeably, f (x) is a strictly convex function when x ∈ ( 1 e , ∞), and f (1) = 0. Moreover, it is noteworthy that there are two corresponding major improvements through this kind of estimation. Firstly, it is guaranteed to reduce the variance which leads to a more stable performance. This merit will be explained in detail in Theorem 4.3. Another significant improvement is manifested in the elimination of negative KL-divergence, since the estimation presents itself in the form of a square which is always non-negative. Theorem 4.3 (Variance of Constraint Function). For policy πθ(a|s) and π θ (a|s), let Var denotes the variance of a variable. For any action a ∈ A and any state s ∈ S, when log πθ(a|s) − log π θ (a|s) ∈ [−0.5, 0.5], then Theorem 4.3 illustrates that there is a decrease from the variance of log πθ(a|s) − log π θ (a|s) to the variance of its square, and furthermore indicates that the variance is effectively reduced. The proof is given in detail in Appendix B.3. In fact, the closer it is betweenθ and θ, the more the variance decreases. Based on Theorem 4.1 to Theorem 4.3, in this paper, we adopt the following form of constraint condition: In Theorem 4.4, we demonstrate that hindsight can also be introduced to the constraint function. The proof follows the methodology similar to that in Section 3, and is deducted explicitly in Appendix B.4. Theorem 4.4 (HTRPO Constraint Function). For the original goal g and an alternative goal g , the constraint between policy πθ(a|s) and policy π θ (a|s) is given by: Theorem 4.4 implies the practicality of using hindsight data under condition g to estimate the ex- pectation. From all illustration above, we give out the final form of the optimization problem for HTRPO: The solving process for HTRPO optimization problem is explicitly demonstrated in Appendix C and the complete algorithm procedure is included in Appendix D.

Section Title: EXPERIMENTS
  EXPERIMENTS This section demonstrates the validation of HTRPO on several sparse reward benchmark tasks 1 . The design of our experiments aims to conduct an in-depth investigation in the following aspects: • How is the effectiveness of HTRPO? • How does each component of HTRPO contribute to its effectiveness? • How is the performance of policy gradient methods trained with hindsight data in continu- ous environments? • How sensitive is HTRPO to network architecture and some key parameters?

Section Title: EXPERIMENTAL SETTINGS
  EXPERIMENTAL SETTINGS We implement HTRPO on a variety of reinforcement learning environments, including Bit Flipping, Grid World and Fetch. Among them, Bit Flipping, Grid World, Fetch Reach and Fetch Push are implemented as descrete-action environments while we also conduct continuous version of experi- ments in Fetch Reach, Fetch Push and Fetch Slide. A glimpse of these environments is demonstrated in  Figure 1  while the detailed introductions are included in Appendix F.1. The reward mechanisms are intentionally modified to sparse reward regulations. Besides, for continuous version of Fetch experiments, we apply an additional policy entropy bonus to encourage more exploration. For each trail of interaction, reward for the agent is set as the remaining number of time steps plus one, and all goals during exploration are chosen uniformly at random for both training and evalua- tion. During the training process, we terminate one episode either when the maximum number of time steps has elapsed or when the goal state is reached. We evaluate agents' performance by docu- menting 10 learning trails in the form of average return and their corresponding standard deviation. In Bit Flipping and Grid World environments, the network architecture is of two hidden layers, each with 64 hyperbolic tangent units; in Fetch environment, for both discrete and continuous implemen- tations, the network contains two 256-unit hidden layers. For all environments mentioned above, we compare HTRPO with HPG ( Rauber et al., 2019 ) and TRPO ( Schulman et al., 2015a ), which are chosen as the baseline algorithms. Since HPG is never applied to continuous environments in  Rauber et al. (2019) , we implement HPG to be adapted to continuous environments. Note that the way we scale the time axis is significantly different from that in  Rauber et al. (2019) . Instead of regarding a certain number of training batches as interval between evaluation steps, we directly uses the accumulated time steps the agent takes while interacting with the environments throughout episodes and batches. Besides comparing with baselines, we also ablate each component of HTRPO to investigate how significant it is for the final performance. To be specific, we adopt the "vanilla" estimation of KL- divergence which we call "HTRPO with KL1" instead of the proposed one in Section 4; we also observe the performance of our algorithm without weighted importance sampling, which is denoted as "HTRPO without WIS" in this paper.

Section Title: COMPARATIVE ANALYSIS
  COMPARATIVE ANALYSIS In discrete environments, we test both the official version of HPG released in  Rauber et al. (2019)  and our HPG implementation while for continuous environments of Fetch, we only test our HPG due to the lack of surpport for continuous tasks in  Rauber et al. (2019) . We apply input normalization in Under review as a conference paper at ICLR 2020 the continuous Fetch environments for better performance. However, for fair comparison with the official HPG, we do not employ this trick in any of the discrete environments. The evaluation curves for the trained policy are demonstrated in  Figure 2  and 3 and the training curves and success rate of these experiments are supplemented in Appendix F.3. Detailed settings of hyperparameters for all experiments are listed in Appendix E. From results demonstrated in  Rauber et al. (2019) , the officially released version of HPG eventually converges to similar performances with that of HTRPO in discrete environments, but sometimes, unlike our HPG, it is still far from converging under this time-step evaluation setting. This kind of distinction in converging speed between our HPG and the official HPG may be caused by the reduction of noises, since we use TD- error to update policies instead of the return corrected by importance sampling, which is adopted in HPG. Thus, for the fairness of comparison, in the following analysis, we mainly compare the properties between HTRPO and our HPG.

Section Title: How is the effectiveness of HTRPO?
  How is the effectiveness of HTRPO? From the results we can see that in both discrete and continuous environments, HTRPO outper- forms HPG significantly. Aside from assuring a good converging property, the sample efficiency of HTRPO also exceed that of HPG, for it reaches a higher average return within less time in most Under review as a conference paper at ICLR 2020 environments. As for TRPO, though it can converge in several simple tasks like Bit Flipping, Grid World and continuous Fetch Reach, it remains incompetent in dealing with complex control tasks including Fetch Push and Fetch Slide, in all of which HTRPO can learn a good policy. The reason is that for TRPO, it is basically impossible to acquire a positive reward at the beginning of the training in such environments, which makes the policy updates meaningless.

Section Title: How does each component of HTRPO contribute to its effectiveness?
  How does each component of HTRPO contribute to its effectiveness? In both  Figure 2  and  Figure 3 , "HTRPO with KL1" and "HTRPO without WIS" performs much worse than the complete version of HTRPO. When we estimate the KL-divergence using the "vanilla" KL-divergence defined as equation 9, it causes severe instability, which means that the estimated KL-divergence can be negative with an unacceptable probability. Considering the prac- ticality of the experiment, the corresponding iteration will be skipped without any updates of the policy in this senario. Given the phenomenon stated above, the final performance of "HTRPO with KL1" is much worse and more unstable in all environments. As for the study of Weighted Impor- tance Sampling, it is widely known for significantly reducing the variance ( Bishop, 2016 ), which is once again proved by the results of "HTRPO without WIS". Admittedly, we can see that the performance of "HTRPO without WIS" matches the full version of HTRPO in several simple envi- ronments in  Figure 2 (a)-(d)  and  Figure 3 (a) . However, for more complex environments like Fetch Push and Fetch Slide, the variance is detrimentally larger than that in simple environments. In short, the performance of "HTRPO without WIS" has a severe degradation comparing to the full version of HTRPO.

Section Title: How is the performance of policy gradient methods trained with hindsight data in continuous environments?
  How is the performance of policy gradient methods trained with hindsight data in continuous environments? As mentioned in  Plappert et al. (2018) , it still remains unexplored that to what extent the policy gradient methods trained with hindsight data can solve continuous control tasks. In this section, we will provide the answer. We implement HTRPO in continuous control tasks including Fetch Reach, Fetch Push and Fetch Slide. HPG is tested as well for comparison. From the results, we can see that with the help of input normalization, HPG can learn a valid policy in continuous control tasks. Still, HTRPO performs much better than HPG in all three environments, benefiting from a faster and more stable convergence. As illustrated in  Figure 3 , HTRPO eventually achieves an average success rate of 92% for Fetch Push and 82.5% for Fetch Slide.

Section Title: How sensitive is HTRPO to network architecture and some key parameters?
  How sensitive is HTRPO to network architecture and some key parameters? To study the sensitivity of HTRPO to different network architectures, we observe the performance of HTRPO with different network settings. From the results demonstrated in Appendix F.2.1, HTRPO achieves commendable performances with all three different network architectures while HPG only converges under certain settings. As for the sensitivity of HTRPO to key parameters, we mainly ob- serve the impact of different number of alternative goals. Based on the learning curves in Appendix F.2.2, we can see that Hindishgt TRPO with more alternative goals achieves better converging speed.

Section Title: CONCLUSION
  CONCLUSION We have extended the monotonically converging on-policy algorithm TRPO to accommodate sparse reward environments by adopting the hindsight methodology. The optimization problem in TRPO is scrupulously derived into hindsight formulation and, when the KL-divergence in the constraint function is small enough, it can be tactfully approximated by another f -divergence in order to re- duce estimation variance and improve learning stability. Experimental results on a variety of en- vironments demonstrate effective performances of HTRPO, and validate its sample efficiency and stable policy update quality in both discrete and continuous scenarios. Therefore, this work reveals HTRPO's vast potential in solving sparse reward reinforcement learning problems.
  The source code and video can be found at https://github.com/HTRPOCODES/HTRPO.

```
