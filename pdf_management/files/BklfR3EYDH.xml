<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 KEYFRAMING THE FUTURE: DISCOVERING TEMPORAL HIERARCHY WITH KEYFRAME-INPAINTER PREDICTION</article-title></title-group><abstract><p>To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them. We do so using a hierarchical Keyframe-Inpainter (KEYIN) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KEYIN finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KEYIN outperforms other recent proposals for learning hierarchical representations.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>When thinking about the future, humans focus their thoughts on the important things that may happen (When will the plane depart?) without fretting about the minor details that fill each intervening moment (What is the last word I will say to the taxi driver?). Because the vast majority of elements in a temporal sequence contains redundant information, a temporal abstraction can make reasoning and planning both easier and more efficient. How can we build such an abstraction? Consider the example of a lead animator who wants to show what happens in the next scene of a cartoon. Before worrying about every low-level detail, the animator first sketches out the story by keyframing, drawing the moments in time when the important events occur. The scene can then be easily finished by other animators who fill in the rest of the sequence from the story laid out by the keyframes. In this paper, we argue that learning to discover such informative keyframes from raw sequences is an efficient and powerful way to learn to reason about the future.</p><p>Our goal is to learn such an abstraction for future image prediction. In contrast, much of the work on future image prediction has focused on frame-by-frame synthesis (<xref ref-type="bibr" rid="b0">Oh et al. (2015)</xref>; <xref ref-type="bibr" rid="b1">Finn et al. (2016)</xref>). This strategy puts an equal emphasis on each frame, irrespective of the redundant content it may contain or its usefulness for reasoning relative to the other predicted frames. Other recent work has considered predictions that "jump" more than one step into the future, but these approaches either used fixed-offset jumps (<xref ref-type="bibr" rid="b3">Buesing et al., 2018</xref>) or used heuristics to select the predicted frames (<xref ref-type="bibr" rid="b0">Neitz et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Jayaraman et al., 2019</xref>; <xref ref-type="bibr" rid="b3">Gregor et al., 2019</xref>). In this work, we propose a method that selects the keyframes that are most informative about the full sequence, so as to allow us to reason about the sequence holistically while only using a small subset of the frames. We do so by ensuring that the full sequence can be recovered from the keyframes with an inpainting strategy, similar to how a supporting animator finishes the story keyframed by the lead.</p><p>One possible application for a model that discovers informative keyframes is in long-horizon planning. Recently, predictive models have been employed for model-based planning and control (<xref ref-type="bibr" rid="b11">Ebert et al. (2018)</xref>). However, they reason about every single future time step, limiting their applicability to short horizon tasks. In contrast, we show that a model that reasons about the future using a small set of informative keyframes enables visual predictive planning for horizons much greater than previously possible by using keyframes as subgoals in a hierarchical planning framework. To discover informative frames in raw sequence data, we formulate a hierarchical probabilistic model in which a sequence is represented by a subset of its frames (see <xref ref-type="fig" rid="fig_0">Fig. 1</xref>). In this two-stage model, a keyframing module represents the keyframes as well as their temporal placement with stochastic latent variables. The images that occur at the timepoints between keyframes are then inferred by an inpainting module. We parametrize this model with a neural network and formulate a variational lower bound on the sequence log-likelihood. Optimizing the resulting objective leads to a model that discovers informative future keyframes that can be easily inpainted to predict the full future sequence.</p><p>Our contributions are as follows. We formulate a hierarchical approach for the discovery of infor- mative keyframes using joint keyframing and inpainting (KEYIN). We propose a soft objective that allows us to train the model in a fully differentiable way. We first analyze our model on a simple dataset with stochastic dynamics in a controlled setting and show that it can reliably recover the underlying keyframe structure on visual data. We then show that our model discovers hierarchical temporal structure on more complex datasets of demonstrations: an egocentric gridworld environ- ment and a simulated robotic pushing dataset, which is challenging for current approaches to visual planning. We demonstrate that the hierarchy discovered by KEYIN is useful for planning, and that the resulting approach outperforms other proposed hierarchical and non-hierarchical planning schemes on the pushing task. Specifically, we show that keyframes predicted by KEYIN can serve as useful subgoals that can be reached by a low-level planner, enabling long-horizon, hierarchical control.</p></sec><sec><title>RELATED WORK</title></sec><sec><title>Hierarchical temporal structure</title><p>Hierarchical neural models for efficiently modeling sequences were proposed in <xref ref-type="bibr" rid="b0">Liu et al. (2015)</xref>; <xref ref-type="bibr" rid="b3">Buesing et al. (2018)</xref>. These approaches were further extended to predict with an adaptive step size so as to leverage the natural hierarchical structure in language data (<xref ref-type="bibr" rid="b7">Chung et al., 2016</xref>; <xref ref-type="bibr" rid="b0">K&#225;d&#225;r et al., 2018</xref>). However, these models rely on autoregressive techniques for text generation and applying them to structured data, such as videos, might be impractical. The video processing community has used keyframe representations as early as 1991 in the MPEG codec (<xref ref-type="bibr" rid="b15">Gall, 1991</xref>). <xref ref-type="bibr" rid="b0">Wu et al. (2018)</xref> adapted this algorithm in the context of neural compression; however, these approaches use constant offsets between keyframes and thus do not fully reflect the temporal structure of the data. Recently, several neural methods were proposed to leverage such temporal structure. <xref ref-type="bibr" rid="b0">Neitz et al. (2018)</xref> and <xref ref-type="bibr" rid="b0">Jayaraman et al. (2019)</xref> propose models that find and predict the least uncertain "bottleneck" frames. <xref ref-type="bibr" rid="b3">Gregor et al. (2019)</xref> construct a representation that can be used to predict any number of frames into the future. In contrast, we propose an approach for hierarchical video representation that discovers the keyframes that best describe a certain sequence.</p><p>In parallel to our work, <xref ref-type="bibr" rid="b0">Kipf et al. (2019)</xref> propose a related method for video segmentation via generative modeling. <xref ref-type="bibr" rid="b0">Kipf et al. (2019)</xref> focus on using the discovered task boundaries for training hierarchical RL agents, while we show that our model can be used to perform efficient hierarchical planning by representing the sequence with only a small set of keyframes. Also concurrently, <xref ref-type="bibr" rid="b0">Kim et al. (2019)</xref> propose a similar method to KEYIN for learning temporal abstractions. While <xref ref-type="bibr" rid="b0">Kim et al. (2019)</xref> focuses on learning hierarchical state-space models, we propose a model that operates directly in the observation space and performs joint keyframing and inpainting.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title></sec><sec><title>Video modeling</title><p>Early approaches to probabilistic video modeling include autoregressive models that factorize the distribution by considering pixels sequentially (<xref ref-type="bibr" rid="b0">Kalchbrenner et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Reed et al., 2017</xref>). To reason about the images in the video holistically, latent variable approaches were developed based on variational inference (<xref ref-type="bibr" rid="b7">Chung et al., 2015</xref>; <xref ref-type="bibr" rid="b3">Rezende et al., 2014</xref>; <xref ref-type="bibr" rid="b0">Kingma &amp; Welling, 2014</xref>), including (<xref ref-type="bibr" rid="b1">Babaeizadeh et al., 2018</xref>; <xref ref-type="bibr" rid="b9">Denton &amp; Fergus, 2018</xref>; <xref ref-type="bibr" rid="b11">Lee et al., 2018</xref>) and large-scale models such as (<xref ref-type="bibr" rid="b5">Castrejon et al., 2019</xref>; <xref ref-type="bibr" rid="b17">Villegas et al., 2019</xref>). <xref ref-type="bibr" rid="b0">Kumar et al. (2019)</xref> is a recently proposed approach that uses exact inference based on normalizing flows (<xref ref-type="bibr" rid="b7">Dinh et al., 2014</xref>; <xref ref-type="bibr" rid="b3">Rezende &amp; Mohamed, 2015</xref>). We build on existing video modeling approaches and show how they can be used to learn temporal abstractions with a novel keyframe-based generative model.</p></sec><sec><title>Visual planning and model predictive control</title><p>We build on recent work that explored applica- tions of learned visual predictive models to planning and control. Several groups (<xref ref-type="bibr" rid="b0">Oh et al., 2015</xref>; <xref ref-type="bibr" rid="b1">Finn et al., 2016</xref>; <xref ref-type="bibr" rid="b6">Chiappa et al., 2017</xref>) have proposed models that predict the consequences of actions taken by an agent given its control output. Recent work (<xref ref-type="bibr" rid="b4">Byravan et al., 2017</xref>; <xref ref-type="bibr" rid="b17">Hafner et al., 2018</xref>; <xref ref-type="bibr" rid="b11">Ebert et al., 2018</xref>) has shown that visual model predictive control based on such models can be applied to a variety of different settings. In this work, we show that the hierarchical representation of a sequence in terms of keyframes improves planning performance in the hierarchical planning setting.</p></sec><sec><title>KEYFRAMING THE FUTURE</title><p>Our goal is to develop a model that generates se- quences by first predicting key observations and the time steps when they occur and then filling in the remaining observations in between. To achieve this goal, in the following we (i) define a probabilistic model for joint keyframing and inpainting, and (ii) show how a maximum likelihood objective leads to the discovery of keyframe structure.</p></sec><sec><title>A PROBABILISTIC MODEL FOR JOINT KEYFRAMING AND INPAINTING</title><p>We first describe a probabilistic model for joint keyframing and inpainting of a sequence I 1:T . The model consists of two parts: the keyframe predictor and the sequence inpainter (see <xref ref-type="fig" rid="fig_1">Fig. 2</xref>).</p><p>The keyframe predictor takes in C conditioning frames I co and produces N keyframes K 1:N as well as the corresponding time indices &#964; 1:N :</p><p>From each pair of keyframes, the sequence inpainter generates the sequence of frames in between:</p><p>which completes the generation of the full sequence. The inpainter additionally observes the number of frames it needs to generate &#964; n+1 &#8722; &#964; n . The temporal spacing of the most informative keyframes is data-dependent: shorter keyframe intervals might be required in cases of rapidly fluctuating motion, while longer intervals can be sufficient for steadier motion. Our model handles this by predicting the keyframe indices &#964; and inpainting &#964; n+1 &#8722;&#964; n frames between each pair of keyframes. We parametrize the prediction of &#964; n in relative terms by predicting offsets &#948; n : &#964; n = &#964; n&#8722;1 + &#948; n .</p></sec><sec><title>KEYFRAME DISCOVERY</title><p>To produce a complex multimodal distribution over K we use a per-keyframe latent variable z with prior distribution p(z) and approximate posterior q(z|I, I co ). 1 We construct a variational lower bound Under review as a conference paper at ICLR 2020 on the likelihood of both I and K as follows 2 :</p><p>In practice, we use a weight &#946; on the KL-divergence term, as is common in amortized variational inference (<xref ref-type="bibr" rid="b0">Higgins et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Alemi et al., 2018</xref>; <xref ref-type="bibr" rid="b9">Denton &amp; Fergus, 2018</xref>).</p><p>If a simple model is used for inpainting, most of the representational power of the model has to come from the keyframe predictor. We use a relatively powerful latent variable model for the keyframe predictor and a simpler Gaussian distribution produced with a neural network for inpainting. Because of this structure, the keyframe predictor has to predict keyframes that describe the underlying sequence well enough to allow a simpler inpainting process to maximize the likelihood. We will show that pairing a more flexible keyframe predictor with a simpler inpainter allows our model to discover semantically meaningful keyframes in video data.</p></sec><sec><title>CONTINUOUS RELAXATION BY LINEAR INTERPOLATION IN TIME</title><p>Our model can dynamically predict the keyframe placement &#964; n . However, learning a distribution over the dis- crete variable &#964; n is challenging due to the expensive evaluation of the ex- pectation over p(&#964; n |z 1:n , I co ) in the objective in Eq. 3. To be able to eval- uate this term efficiently and in a dif- ferentiable manner while still learn- ing the keyframe placement, we pro- pose a continuous relaxation of the objective. The placement distribution &#964; n defines a probability for each pre- dicted frame to match to a certain frame in the ground truth sequence. Instead of sampling from this distri- bution to pick a target frame we pro- duce a soft target for each predicted frame by computing the expected tar- get frame, i.e. the weighted sum of all frames in the true sequence, each mul- tiplied with the probability of match- ing to the predicted frame. When the entropy of &#964; n converges to zero, the continuous relaxation objective is equivalent to the original, discrete objective.</p></sec><sec><title>Keyframe targets</title><p>To produce a keyframe target,K n , we linearly interpolate between the ground truth images according to the predicted distribution over the keyframe's temporal placement &#964; n : K n = t &#964; n t I t , where &#964; n t is the probability that the n th keyframe occurs at timestep t. This process is depicted in <xref ref-type="fig" rid="fig_2">Fig. 3</xref>.</p><p>We parametrize temporal placement prediction in terms of offsets &#948; with a maximum offset of J. Because of this, the maximum possible length of the predicted sequence is N J. It is desirable for J to be large enough to be able to capture the distribution of keyframes in the data, but this may lead to Under review as a conference paper at ICLR 2020 Ground Truth Predicted, KeyIn (ours) Predicted, Jumpy Segment 1 Segment 2 Segment 3 Keyframe Keyframe the generation of sequences longer than the target N J &gt; T . To correctly compute the value of the relaxed objective in this case, we discard predicted frames at times &gt; T and normalize the placement probability output by the network so that it sums to one over the first T steps. Specifically, for each keyframe we compute this probability as c n : c n = t&#8804;T &#964; n t . The loss corresponding to the last two terms of Eq. (3) then becomes:</p></sec><sec><title>Inpainting targets</title><p>1:J between each pair of keyframes K n , K n+1 . As in the previous section, the targets for ground truth images are given as an interpolation between generated images weighted by the probability of the predicted frame&#206; n j being matched to ground truth frame I t : I t = ( n,j m n j,t&#206; n j )/ n,j m n j,t . Here, m n j,t is the probability that the j-th predicted image in segment n has an offset of t from the beginning of the predicted sequence, which can be computed from &#964; n . To obtain a probability distribution over produced frames, we normalize the result with n,j m n j,t . The full loss for our model is:</p></sec><sec><title>DEEP VIDEO KEYFRAMING</title><p>We show how to instantiate KEYIN with deep neural networks and train it on high-dimensional observations, such as images. We further describe an effective training procedure for KEYIN.</p></sec><sec><title>ARCHITECTURE</title><p>We use a common encoder-recurrent-decoder architecture (<xref ref-type="bibr" rid="b9">Denton &amp; Fergus (2018)</xref>; <xref ref-type="bibr" rid="b17">Hafner et al. (2018)</xref>). Video frames are first processed with a convolutional encoder module to produce image embeddings &#953; t = CNN enc (I t ). Inferred frame embeddings&#953; are decoded with a convolutional de- coder&#206; n j = CNN dec (&#953; n j ). The keyframe predictor p(K 1:N , &#964; 1:N |z 1:N , I co ) is parametrized with a Long Short-Term Memory network (<xref ref-type="bibr" rid="b0">LSTM, Hochreiter &amp; Schmidhuber (1997)</xref>). To condi- tion the keyframe predictor on past frames, we initialize its state with the final state of another LSTM that processes the conditioning frames. Similarly, we parametrize the sequence inpainter p(I &#964; n :&#964; n+1 |K n , K n+1 , &#964; n+1 &#8722; &#964; n ) with an LSTM. We condition the inpainting on both keyframe embeddings,&#954; n&#8722;1 and&#954; n , as well as the temporal offset between the two, &#948; n , by passing these inputs through a multi-layer perceptron that produces the initial state of the inpainting LSTM. We use a Gaussian distribution with identity variance as the output distribution for both the keyframe predictor and the inpainting model and a multinomial distribution for &#948; n . We parametrize the inference q(z 1:N |I &#8722;C+1:T ) with an LSTM with attention over the entire input sequence. The inference distribution is a diagonal covariance Gaussian, and the prior p(z 1:N ) is a unit Gaussian. Further details of the inference procedure are given in Sec. B and Fig. 8 of the Appendix.</p></sec><sec><title>TRAINING PROCEDURE</title><p>We train our model in two stages. First, we train the sequence inpainter to inpaint between ground truth frames sampled with random offsets, thus learning interpolation strategies for a variety of different inputs. In the second stage, we train the keyframe predictor using the loss from Eq. 5 by feeding the predicted keyframe embeddings to the inpainter. In this stage, the weights of the inpainter are frozen and are only used to backpropagate errors to the rest of the model. We found that this simple two-stage procedure improves optimization of the model.</p><p>We use L1 reconstruction losses to train the keyframe predictor. We found that this and adding a reconstruction loss on the predicted embeddings of the keyframes, weighted with a factor &#946; &#954; , improved the ability of the model to produce informative keyframes. Target embeddings are computed using the same soft relaxation used for the target keyframes. More details of the loss computation are given in Sec. E and Algorithm 1 of the Appendix.</p></sec><sec><title>EXPERIMENTS</title><p>We evaluate the quality of KEYIN's representation for future sequences by addressing the following questions: (i) Can it discover and predict informative keyframes? (ii) Can it model complex data distributions? (iii) Is the discovered hierarchy useful for long-horizon hierarchical planning?</p></sec><sec><title>Datasets</title><p>We evaluate our model on three datasets containing structured long-term behavior. The Structured Brownian motion (SBM) dataset consists of binary image sequences of size 32 &#215; 32 pixels in which a ball randomly changes directions after periods of straight movement of six to eight frames. The Gridworld Dataset consists of 20k sequences of an agent traversing a maze with different objects. The agent sequentially navigates to objects and interacts with them following a task sketch.We use the same maze for all episodes and randomize the initial position of the agent and the task sketch. We use 64 &#215; 64 pixel image observations and further increase visual complexity by constraining the field of view to a 5 &#215; 5-cells egocentric window.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>The Pushing Dataset consists of 50k sequences of a robot arm pushing a puck towards a goal on the opposite side of a wall. Each sequence consists of six consecutive pushes. We vary start and target position of the puck, as well as the placement of the wall. The demonstrations were generated with the MuJoCo simulator (<xref ref-type="bibr" rid="b0">Todorov et al., 2012</xref>) at a resolution of 64 &#215; 64 pixels. For more details on the data generation process, see Sec.D of the Appendix.</p><p>Further details about the experimental setup are given in Sec. C of the Appendix.</p></sec><sec><title>KEYFRAME DISCOVERY</title><p>To evaluate KEYIN's ability to dis- cover keyframes, we train KEYIN on all three datasets with N = 6, which can be interpreted as selecting the N most informative frames from a se- quence. We show qualitative exam- ples of keyframe discovery for the SBM dataset in <xref ref-type="fig" rid="fig_3">Fig. 4</xref> and for the Grid- world and Pushing datasets in <xref ref-type="fig" rid="fig_4">Fig. 5</xref>. On all datasets the model discovers meaningful keyframes which mark direction changes of the ball, transitions between pushes or interactions with objects, adapting its keyframe prediction patterns to the data. Consequently, the inpainter network is able to produce frames of high visual quality. Misplaced keyframes yield blurry interpolations, as can be seen for the jumpy prediction in <xref ref-type="fig" rid="fig_3">Fig. 4</xref>. This suggests that keyframes found by KEYIN describe the overall sequences better. To show that KEYIN discovers informative keyframes, we compare keyframe predictions against an alternative approach that measures the surprise associated with ob- serving a frame given the previous frames. This ap- proach selects keyframes as the N frames with the largest peaks in "surprise" as measured by the KL-divergence D KL [q(z t |I 1:t )||p(z t )] between the prior and the posterior of a stochastic predictor based on <xref ref-type="bibr" rid="b9">Denton &amp; Fergus (2018)</xref> (see Sec. F and Algorithm 2 of the Appendix for details). We provide comparisons to alternative formulations of surprise in Appendix Sec. F, Tab. 3.</p><p>For quantitative analysis, we define approximate ground truth keyframes to be the points of direction change for the SBM dataset, the moments when the robot lifts its arm to transitions between pushes, or when the agent interacts with objects in the gridworld. We report F1 scores that capture both the precision and recall of keyframe discovery. We additionally compare to random keyframe placement, and a learned but static baseline that is the same for all sequences. The evaluation in <xref ref-type="table" rid="tab_0">Tab. 1</xref> shows that KEYIN discovers better keyframes than alternative methods. The difference is especially large on the more complex Pushing and Gridworld datasets. The surprise-based method does not reason about which frames are most helpful to reconstruct the entire trajectory and thus is unable to discover the correct structure on the more complex datasets. In addition to the F1 scores, we report temporal distance between predicted and annotated keyframes in Appendix, Tab. 4, also indicating that KEYIN is better able to discover the temporal structure in both datasets.</p></sec><sec><title>KEYFRAME-BASED VIDEO MODELING</title><p>Even though the focus of this work is on discovering temporal structure via keyframing and not on improving video prediction quality, we verify that KEYIN can represent complex data distributions in terms of discovered keyframes and attains high diversity and visual quality. We show sample generations from our model on the Pushing and Gridworld datasets on the supplementary website 5 .</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>We see that KeyIn is able to faithfully model complex distributions of video sequences. We further visualize multiple sampled Pushing sequences from our model conditioned on the same start position in <xref ref-type="fig" rid="fig_5">Fig. 6</xref>, showing that KEYIN is able to cover both modes of the demonstration distribution. We further show that KEYIN compares favorably to prior work on video prediction metrics on sequence modeling in Tab. 5 of the Appendix, and outperforms prior approaches in terms of keyframe modeling in Appendix, Tab. 6.</p></sec><sec><title>ROBUSTNESS OF KEYFRAME DETECTION</title><p>In the previous sections, we showed that when the sequence can indeed be summarized with N keyframes, KEYIN predicts the keyframes that correspond to our notion of salient frames. However, what happens if we train KEYIN to select a larger or a smaller amount of keyframes?</p><p>To evaluate this, we measure KEYIN recall with extra and precision with fewer available keyframes. We note that high precision is un- achievable in the first case and high recall is un- achievable in the second case, since these prob- lems are misspecified. As these numbers are not informative, we do not report them. In <xref ref-type="table" rid="tab_1">Tab. 2</xref>, we see that KEYIN is able to find informative keyframes even when N does not exactly match the structure of the data. We further qualitatively show that KEYIN selects a superset or a subset of the original keyframes respectively in Sec. G. This underlines that our method's ability to discover keyframe structure is robust to the choice of the number of predicted keyframes.</p><p>As a first step towards analyzing the robustness of KEYIN under more realistic conditions we report keyframe discovery when trained and tested on sequences with additive Gaussian noise, a noise characteristic commonly found in real-world camera sensors. We find that KEYIN is still able to discover the temporal structure on both the Pushing and the Gridworld dataset. For qualitative and quantitative results, see Appendix Fig. 11 and Tab. 7.</p></sec><sec><title>HIERARCHICAL KEYFRAME-BASED PLANNING</title><p>We have seen that KEYIN can find frames that correspond to an intuitive notion of keyframes. This demonstrates that the keyframes discovered by KEYIN do indeed capture an abstraction that compactly describes the sequence. In light of this, we hypothesize that an informative set of keyframes contains sufficient information about a sequence to effectively follow the trajectory it shows. To test this, we use the inferred keyframes as subgoals for hierarchical planning in the pushing environment. During task execution, we first plan a sequence of keyframes that reaches the target using our learned keyframe predictor. Specifically, we generate keyframe trajectories from our model by sampling latent variables z from the prior and using them to roll out the keyframe prediction model. We optimize for a sequence of latent variables z that results in a keyframe trajectory which reaches the goal using the Cross-Entropy Method (<xref ref-type="bibr" rid="b0">CEM, Rubinstein &amp; Kroese (2004)</xref>). We then execute the plan by using the keyframes as subgoals for a low-level planner. This planner reaches each subgoal via model predictive control using ground truth dynamics, again employing CEM for optimization of the action trajectory. This planning procedure is illustrated in <xref ref-type="fig" rid="fig_6">Fig. 7</xref> (left). For more details, see Sec. I and Algs. 3 and 4 of the Appendix.</p><p>We find that KEYIN is able to plan coherent subgoal paths towards the final goal that often lead to successful task execution (executions are shown on the supplementary website 6 ). To quantitatively evaluate the keyframes discovered, we compare to alternative subgoal selection schemes: fixed time offset (<xref ref-type="bibr" rid="b0">Jumpy, similar to Buesing et al. (2018)</xref>), a method that determines points of peak surprise (Surprise, see Sec. 6.1), and a bottleneck-based subgoal predictor (time-agnostic prediction or TAP, <xref ref-type="bibr" rid="b0">Jayaraman et al. (2019)</xref>). We additionally compare to an approach that plans directly towards the final goal using the low-level planner (Flat). We evaluate all methods with the shortest path between Under review as a conference paper at ICLR 2020 the target and the actual position of the object after the plan is executed. All compared methods use the same low-level planner as we only want to measure the quality of the predicted subgoals. As shown in <xref ref-type="fig" rid="fig_6">Fig. 7</xref> (right), our method outperforms all prior approaches. TAP shows only a moderate increase in performance over the Flat planner, which we attribute to the fact that it fails to predict good subgoals and often simply predicts the final image as the bottleneck. This is likely due to the relatively large stochasticity of our dataset and the absence of the clear bottlenecks that TAP is designed to find. Our method outperforms the planners that use Jumpy and Surprise subgoals. This further confirms that KEYIN is able to produce keyframes that are informative about the underlying trajectory, such that planning toward these keyframes makes it easier to follow the trajectory.</p></sec><sec><title>DISCUSSION</title><p>We presented KEYIN, a method for representing a sequence by its informative keyframes by jointly keyframing and inpainting. KEYIN first generates the keyframes of a sequence and their temporal placement and then produces the full sequence by inpainting between keyframes. We showed that KEYIN discovers informative keyframes on several datasets with stochastic dynamics. Furthermore, by using the keyframes for planning, we showed our method outperforms several other hierarchical planning schemes. Our method opens several avenues for future work. First, an improved training procedure that allows end-to-end training is desirable. Second, more powerful hierarchical planning approaches can be designed using the keyframe representation to scale to long-term real-world tasks. Finally, the proposed keyframing method can be applied to a variety of applications, including video summarization, video understanding, and multi-stage hierarchical video prediction.</p><p>Under review as a conference paper at ICLR 2020</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Keyframing the future. Instead of predicting one frame after the other, we propose to represent the sequence with the keyframes that depict the interesting moments of the sequence. The remaining frames can be inpainted given the keyframes.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>A probabilistic model for jointly keyframing and inpainting a future sequence. First, a sequence of keyframes K 1:N is gen- erated, as well as corresponding temporal in- dices &#964; 1:N , defining the structure of the un- derlying sequence. In the second stage, for each pair of keyframes K n and K n+1 , the frames I &#964; n :&#964; n+1 &#8722;1 are inpainted.</p></caption><graphic /><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Soft keyframe loss in the relaxed formulation. For each predicted keyframeK n we compute a target imag&#7869; K n as the sum of the ground truth images weighted with the corresponding distribution over index &#964; n . Finally, we compute the reconstruction loss between the estimated imag&#234; K n and the soft targetK n .</p></caption><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Sequences generated by KEYIN and a method with constant temporal keyframe offset (Jumpy) on Brownian Motion data. Generation is conditioned on the first five frames. The first half of the sequence is shown. Movement direction changes are marked red in the ground truth sequence and predicted keyframes are marked blue. We see that KEYIN can correctly reconstruct the motion as it selects an informative set of keyframes. The sequence generated by the Jumpy method does not reproduce the direction changes since they cannot be inferred from the selected keyframes.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>Example generations by KEYIN on (top) Pushing and (bottom) Gridworld data. The generation is conditioned on a single ground truth frame. Twelve of the 30 predicted frames are shown. We observe that for each transition between pushes and each action of the Gridworld agent our network predicts a keyframe either exactly at the timestep of the event or one timestep apart. Note, although agent position is randomized, objects not visible in the first image can be predicted in Gridworld because the maze is fixed across episodes.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>F1 accuracy score for keyframe discovery on all three datasets. Higher is better.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_5"><object-id>fig_5</object-id><label>Figure 6:</label><caption><title>Figure 6:</title><p>Distribution of trajectories sampled from KEYIN. Each black line denotes one of 100 trajectories of the ma- nipulated object. The obstacle is shown in blue and the initial position in pink. We see that our model covers both modes of the distribution, producing both trajec- tories that go to the right and to the left of the obstacle.</p></caption><graphic /><graphic /></fig><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Keyframe discovery for varied number of predicted keyframes. The data has approximately 6 keyframes. Uninterpretable entries are omitted for clarity: see the text for details.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_6"><object-id>fig_6</object-id><label>Figure 7:</label><caption><title>Figure 7:</title><p>Hierarchical planning on the Pushing dataset. Left: We use the model to produce keyframes that represent the sequence between the current observation image and the goal. A low-level planner based on model predictive control produces the actions, a t , executed to reach each keyframe, until the final goal is reached. Right: Planning performance on a Pushing task. The hierarchy discovered by KEYIN outperforms comparable planning approaches.</p></caption><graphic /><graphic /></fig></sec></body><back><sec><p>We find this occurs most of the time in practice.</p></sec><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Fixing a broken ELBO</article-title><source>Proceedings of International Conference on Machine Learning (ICML)</source><year>2018</year><person-group person-group-type="author"><name><surname>References Alexander Alemi</surname><given-names>Ben</given-names></name><name><surname>Poole</surname><given-names>Ian</given-names></name><name><surname>Fischer</surname><given-names>Joshua</given-names></name><name><surname>Dillon</surname><given-names /></name><name><surname>Rif</surname><given-names>A</given-names></name><name><surname>Saurous</surname><given-names>Kevin Murphy</given-names></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Stochastic variational video prediction</article-title><source>Proceedings of International Conference on Learning Representations (ICLR)</source><year>2018</year><person-group person-group-type="author"><name><surname>Babaeizadeh</surname><given-names>Mohammad</given-names></name><name><surname>Finn</surname><given-names>Chelsea</given-names></name><name><surname>Erhan</surname><given-names>Dumitru</given-names></name><name><surname>Campbell</surname><given-names>Roy H</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Neural machine translation by jointly learning to align and translate</article-title><source>Proceedings of International Conference on Learning Representations (ICLR)</source><year>2015</year><person-group person-group-type="author"><name><surname>Bahdanau</surname><given-names>D</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Learning and querying fast generative models for reinforcement learning</article-title><year>2018</year><person-group person-group-type="author"><name><surname>Buesing</surname><given-names>Lars</given-names></name><name><surname>Weber</surname><given-names>Theophane</given-names></name><name><surname>S&#233;bastien Racani&#232;re</surname><given-names>S M</given-names></name><name><surname>Ali Eslami</surname><given-names>Danilo Jimenez</given-names></name><name><surname>Rezende</surname><given-names>David P</given-names></name><name><surname>Reichert</surname><given-names>Fabio</given-names></name><name><surname>Viola</surname><given-names>Frederic</given-names></name><name><surname>Besse</surname><given-names>Karol</given-names></name><name><surname>Gregor</surname><given-names>Demis</given-names></name><name><surname>Hassabis</surname><given-names>Daan</given-names></name><name><surname>Wierstra</surname><given-names /></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Se3-pose-nets: Structured deep dynamics models for visuomotor planning and control</article-title><source>Proceedings of IEEE International Conference on Robotics and Automation</source><year>2017</year><person-group person-group-type="author"><name><surname>Byravan</surname><given-names>Arunkumar</given-names></name><name><surname>Leeb</surname><given-names>Felix</given-names></name><name><surname>Meier</surname><given-names>Franziska</given-names></name><name><surname>Fox</surname><given-names>Dieter</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Improved conditional vrnns for video prediction</article-title><year>1904</year><person-group person-group-type="author"><name><surname>Castrejon</surname><given-names>Lluis</given-names></name><name><surname>Ballas</surname><given-names>Nicolas</given-names></name><name><surname>Courville</surname><given-names>Aaron</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Recurrent environment simulators</article-title><source>Proceedings of International Conference on Learning Representations (ICLR)</source><year>2017</year><person-group person-group-type="author"><name><surname>Chiappa</surname><given-names>Silvia</given-names></name><name><surname>Racani&#232;re</surname><given-names>S&#233;bastien</given-names></name><name><surname>Wierstra</surname><given-names>Daan</given-names></name><name><surname>Mohamed</surname><given-names>Shakir</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>A recurrent latent variable model for sequential data</article-title><source>Proceedings of Neural Information Processing Systems (NeurIPS)</source><year>2015</year><person-group person-group-type="author"><name><surname>Chung</surname><given-names>Junyoung</given-names></name><name><surname>Kastner</surname><given-names>Kyle</given-names></name><name><surname>Dinh</surname><given-names>Laurent</given-names></name><name><surname>Goel</surname><given-names>Kratarth</given-names></name><name><surname>Aaron</surname><given-names>C</given-names></name><name><surname>Courville</surname><given-names>Yoshua</given-names></name><name><surname>Bengio</surname><given-names /></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><source>Hierarchical multiscale recurrent neural networks</source><year>2016</year><person-group person-group-type="author"><name><surname>Chung</surname><given-names>Junyoung</given-names></name><name><surname>Ahn</surname><given-names>Sungjin</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Stochastic video generation with a learned prior</article-title><source>Proceedings of International Conference on Machine Learning (ICML)</source><year>2018</year><person-group person-group-type="author"><name><surname>Denton</surname><given-names>E</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Nice: Non-linear independent components estimation</article-title><source>arXiv preprint arXiv:1410.8516</source><year>2014</year><person-group person-group-type="author"><name><surname>Dinh</surname><given-names>Laurent</given-names></name><name><surname>Krueger</surname><given-names>David</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Self-supervised visual planning with temporal skip connections</article-title><source>Conference on Robotic Learning (CoRL)</source><year>2017</year><person-group person-group-type="author"><name><surname>Ebert</surname><given-names>Frederik</given-names></name><name><surname>Finn</surname><given-names>Chelsea</given-names></name><name><surname>Lee</surname><given-names>Alex X</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Vi- sual foresight: Model-based deep reinforcement learning for vision-based robotic control</article-title><year>2018</year><person-group person-group-type="author"><name><surname>Ebert</surname><given-names>Frederik</given-names></name><name><surname>Finn</surname><given-names>Chelsea</given-names></name><name><surname>Dasari</surname><given-names>Sudeep</given-names></name><name><surname>Xie</surname><given-names>Annie</given-names></name><name><surname>Lee</surname><given-names>Alex</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Deep visual foresight for planning robot motion</article-title><source>Proceedings of IEEE International Conference on Robotics and Automation</source><year>2017</year><person-group person-group-type="author"><name><surname>Finn</surname><given-names>Chelsea</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Unsupervised learning for physical interaction through video prediction</article-title><source>Proceedings of Neural Information Processing Systems (NeurIPS)</source><year>2016</year><person-group person-group-type="author"><name><surname>Finn</surname><given-names>Chelsea</given-names></name><name><surname>Goodfellow</surname><given-names>Ian</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>MPEG: A video compression standard for multimedia applications</article-title><source>Commun. ACM</source><year>1991</year><volume>34</volume><issue>4</issue><fpage>46</fpage><lpage>58</lpage><person-group person-group-type="author"><name><surname>Gall</surname><given-names>Le</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><source>Temporal difference variational auto-encoder</source><year>2019</year><person-group person-group-type="author"><name><surname>Gregor</surname><given-names>Karol</given-names></name><name><surname>Papamakarios</surname><given-names>George</given-names></name><name><surname>Besse</surname><given-names>Frederic</given-names></name><name><surname>Buesing</surname><given-names>Lars</given-names></name><name><surname>Weber</surname><given-names>Theophane</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Learning latent dynamics for planning from pixels</article-title><source>arXiv preprint arXiv:1811.04551</source><year>2018</year><person-group person-group-type="author"><name><surname>Hafner</surname><given-names>Danijar</given-names></name><name><surname>Lillicrap</surname><given-names>Timothy</given-names></name><name><surname>Fischer</surname><given-names>Ian</given-names></name><name><surname>Villegas</surname><given-names>Ruben</given-names></name><name><surname>Ha</surname><given-names>David</given-names></name><name><surname>Lee</surname><given-names>Honglak</given-names></name><name><surname>Davidson</surname><given-names>James</given-names></name></person-group></element-citation></ref></ref-list></back></article>