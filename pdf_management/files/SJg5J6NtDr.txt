Title:
```
Published as a conference paper at ICLR 2020 WATCH, TRY, LEARN: META-LEARNING FROM DEMONSTRATIONS AND REWARDS
```
Abstract:
```
Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical num- ber of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by lever- aging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough informa- tion; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and- error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself au- tonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demon- stration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.
```

Figures/Tables Captions:
```
Figure 1: Each column displays the first and last frame of an episode top-to-bottom. After watching one demonstration (left), the scene is re-arranged. With one trial episode (middle), our method can learn to solve the task (right) by leveraging both the demo and trial-and-error experience.
Figure 2: Meta-training Overview: First, we meta-train π I θ according to Eq. 2. Next, we collect L trial trajectories per meta-training task Ti in the environment using our trained π I θ . We denote the trial {τ i, } ∼ π I θ (a|s, {d i,k }), and store the resulting demo-trial pairs {({d i,k }, {τi,j})} in a new dataset (blue). Finally, we meta-train π II φ according to Eq. 4.
Figure 3: Our vision-based Phase II architecture: Upper left: we pass the RGB observation for each timestep through a 4-layer CNN with ReLU activations and layer normalization, followed by a spatial softmax layer that extracts 2D keypoints (Levine et al., 2016). We flatten the output keypoints and concatenate them with the current gripper pose, gripper velocity, and context embedding. Upper Right: We pass the resulting vector through the actor network, which predicts the parameters of a Gaussian mixture over the commanded end- effector position, axis-angle orientation, and finger angle. Lower left: To produce the context embedding, the embedding network applies a vision network to 40 ordered observations sampled randomly from the demo and trial trajectories. We concatenate the demo and trial outputs with the trial episode rewards along the embedding feature dimension, then apply a 10x1 convolution across the time dimension, flatten, and apply a MLP to produce the final context embedding. The Phase I policy architecture (Appendix Fig. 8) is the same as shown here, but omits the concatenation of trial embeddings and trial rewards.
Figure 5: Illustration of example episodes in four distinct task families: button pressing, grasping, sliding, and pick-and-place. Each column shows the first and last frames of an episode top-to-bottom. We meta-train each model on hundreds of tasks from each of these task families. For each task within a task family, we use a unique pair of kitchenware objects sampled from a set of nearly one hundred different objects.
Figure 4: Average return of each method on held out meta-test tasks in the reaching environment, after one demonstration and one trial. Our Watch-Try-Learn (WTL) method is quickly able to learn to imitate the demonstrator. Each line shows the average over 5 sepa- rate training runs with identical hyperparameters, eval- uated on 50 randomly sampled meta-test tasks. Shaded regions indicate 95% confidence intervals.
Figure 6: The average success rate of different methods in the gripper control environment, for both state space (non-vision) and vision based policies. The leftmost column displays aggregate results across all task families. Our Watch-Try-Learn (WTL) method significantly outperforms the meta-imitation (MIL) baseline, which in turn outperforms the behavior cloning (BC) baseline. We conducted 5 training runs of each method with identical hyperparameters and evaluated each run on 40 held out meta-test tasks. Error bars indicate 95% confidence intervals.
Table 1: Average success rates across meta-test tasks using state space observations. For BC + SAC we pre- train with behavior cloning and use RL to fine-tune a separate agent on each meta-test task. The table shows BC + SAC performance after 1500, 2000, and 2500 tri- als per task.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Imitation learning enables autonomous agents to learn complex behaviors from demonstrations, which are often easy and intuitive for users to provide. However, learning expressive neural net- work policies from imitation requires a large number of demonstrations, particularly when learning from high-dimensional inputs such as images. Meta-imitation learning has emerged as a promising approach for allowing an agent to leverage data from previous tasks in order to learn a new task from only a handful of demonstrations ( Duan et al., 2017 ;  Finn et al., 2017b ;  James et al., 2018 ). However, in many practical few-shot imitation settings, there is an identifiability problem: it may not be possible to precisely determine a policy from one or a few demonstrations, especially in a new situation. And even if a demonstration precisely communicates what the task entails, it might not precisely communicate how to accomplish it in new situations. For example, it may be difficult to discern from a single demonstration where to grasp an object when it is in a new position or how much force to apply in order to slide an object without knocking it over. It may be expensive to col- lect more demonstrations to resolve such ambiguities, and even when we can, it may not be obvious to a human demonstrator where the agent's difficulty is arising from. Alternatively, it is easy for the user to provide success-or-failure feedback, while exploratory interaction is useful for learning how to perform the task. To this end, our goal is to build an agent that can first infer a policy from one demonstration, then attempt the task using that policy while receiving binary user feedback, and finally use the feedback to improve its policy such that it can consistently solve the task. This vision of learning new tasks from a few demonstrations and trials inherently requires some amount of prior knowledge or experience, which we can acquire through meta-learning across a range of previous tasks. To this end, we develop a new meta-learning algorithm that incorporates elements of imitation learning with trial-and-error reinforcement learning. In contrast to previous meta-imitation learning approaches that learn one-shot imitation learning procedures through imita- tion ( Duan et al., 2017 ;  Finn et al., 2017b ), our approach enables the agent to improve at the test task through trial-and-error. Further, from the perspective of meta-RL algorithms that aim to learn effi- cient RL procedures ( Duan et al., 2016 ;  Wang et al., 2016 ;  Finn et al., 2017a ), our approach also has significant appeal: as we aim to scale meta-RL towards broader task distributions and learn increas- ingly general RL procedures, exploration and efficiency becomes exceedingly difficult. However, a demonstration can significantly narrow down the search space while also providing a practical means for a user to communicate the goal, enabling the agent to achieve few-shot learning of be- havior. While the combination of demonstrations and reinforcement has been studied extensively in single task problems ( Kober et al., 2013 ;  Sun et al., 2018 ;  Rajeswaran et al., 2018 ;  Le et al., 2018 ), this combination is particularly important in meta-learning contexts where few-shot learning of new tasks is simply not possible without demonstrations. Further, we can even significantly improve upon prior methods that study this combination using meta-learning to more effectively integrate the information coming from both sources. The primary contribution of this paper is a meta-learning algorithm that enables learning of new behaviors with a single demonstration and trial experience. After receiving a demonstration illus- trating a new goal, the meta-trained agent can learn to accomplish that goal through a small amount of trial-and-error with only binary success-or-failure labels. We evaluate our algorithm and several prior methods on a challenging, vision-based control problem involving manipulation tasks from four distinct families of tasks: button-pressing, grasping, pushing, and pick and place. We find that our approach can effectively learn tasks with new, held-out objects using one demonstration and a single trial, while significantly outperforming meta-imitation learning, meta-reinforcement learning, and prior methods that combine demonstrations and reward feedback. To our knowledge, our exper- iments are the first to show that meta-learning can enable an agent to adapt to new tasks with binary reinforcement signals from raw pixel observations, which we show with a single meta-model for a variety of distinct manipulation tasks. We have published videos of our experimental results 1 and the experiment model code 2 .

Section Title: RELATED WORK
  RELATED WORK Learning to learn, or meta-learning, has a long-standing history in the machine learning litera- ture ( Thrun & Pratt, 1998 ;  Schmidhuber, 1987 ;  Bengio et al., 1992 ;  Hochreiter et al., 2001 ). We particularly focus on meta-learning in the context of control. Our approach builds on and signifi- Published as a conference paper at ICLR 2020 cantly improves upon meta-imitation learning ( Duan et al., 2017 ;  Finn et al., 2017b ;  James et al., 2018 ;  Paine et al., 2018 ) and meta-reinforcement learning ( Duan et al., 2016 ;  Wang et al., 2016 ;  Mishra et al., 2018 ;  Rakelly et al., 2019 ), extending contextual meta-learning approaches. Unlike prior work in few-shot imitation learning ( Duan et al., 2017 ;  Finn et al., 2017b ;  Yu et al., 2018 ;  James et al., 2018 ;  Paine et al., 2018 ), our method enables the agent to additionally improve upon trial-and-error experience. In contrast to work in multi-task and meta-reinforcement learning ( Duan et al., 2016 ;  Wang et al., 2016 ;  Finn et al., 2017a ;  Mishra et al., 2018 ;  Houthooft et al., 2018 ;  Sung et al., 2017 ;  Nagabandi et al., 2019 ;  Saemundsson et al., 2018 ;  Hausman et al., 2017 ), our approach learns to use one demonstration to address the meta-exploration problem ( Gupta et al., 2018 ;  Stadie et al., 2018 ). Our work also requires only one round of on-policy data collection, collecting only 1, 500 trials for the vision-based manipulation tasks, while nearly all prior meta-learning works re- quire thousands of iterations of on-policy data collection, amounting to hundreds of thousands of trials ( Duan et al., 2016 ;  Wang et al., 2016 ;  Finn et al., 2017a ;  Mishra et al., 2018 ). Combining demonstrations and trial-and-error experience has long been explored in the machine learning and robotics literature ( Kober et al., 2013 ). This ranges from simple techniques such as demonstration-based pre-training and initialization ( Peters & Schaal, 2006 ;  Kober & Peters, 2009 ;  Kormushev et al., 2010 ;  Kober et al., 2013 ;  Silver et al., 2016 ) to more complex methods that incor- porate both demonstration data and reward information in the loop of training ( Taylor et al., 2011 ;  Brys et al., 2015 ;  Subramanian et al., 2016 ;  Hester et al., 2018 ;  Sun et al., 2018 ;  Rajeswaran et al., 2018 ;  Nair et al., 2018 ;  Le et al., 2018 ). The key contribution of this paper is an algorithm that can learn how to learn from both demonstrations and rewards. This is quite different from RL algorithms that incorporate demonstrations: learning from scratch with demonstrations and RL involves a slow, iterative learning process, while fast adaptation with a meta-trained policy involves extracting in- herently distinct pieces of information from the demonstration and the trials. The demonstration provides information about what to do, while the small number of RL trials can disambiguate the task and how it s refinement. As a result, we get a procedure that significantly exceeds the efficiency of prior approaches, requiring only one demonstration and one trial to adapt to a new test task, even from pixel observations, by leveraging previous data. In comparison, single-task methods for learn- ing from demonstrations and rewards typically require hundreds or thousands of trials to learn tasks of comparable difficulty ( Rajeswaran et al., 2018 ;  Nair et al., 2018 ).

Section Title: META-LEARNING FROM DEMONSTRATIONS AND REWARDS
  META-LEARNING FROM DEMONSTRATIONS AND REWARDS We first introduce some meta-learning preliminaries, then formalize our particular problem state- ment before finally describing our approach and its implementation.

Section Title: PRELIMINARIES
  PRELIMINARIES Meta-learning, or learning to learn, aims to learn new tasks from very little data. To achieve this a meta-learning algorithm can first meta-train on a set of tasks {T i }, called the meta-train tasks. We then evaluate how quickly the meta-learner can learn an unseen meta-test task T j . We typi- cally assume that the meta-train and meta-test tasks are drawn from some unknown task distribution p(T ) ( Finn, 2018 ). Through meta-training, a meta-learner can learn some common structure be- tween the tasks in p(T ) which it can use to more quickly learn a new meta-test task. We define a task T i as a finite-horizon Markov decision process (MDP), {S, A, r i , P i , H}, with continuous state space S, continuous action space A, reward function r i : S × A → R, unknown dynamics P i (s t+1 |s t , a t ), and horizon H. In our manipulation experiments we will restrict the tasks to sparse binary rewards r i : S → {0, 1}. The state space S, reward r i , and dynamics P i may vary across tasks.

Section Title: PROBLEM STATEMENT
  PROBLEM STATEMENT Our goal is to meta-train an agent such that can quickly learn a new test task T j in two phases: Phase I: The agent observes and learns from k = 1, ..., K task demonstrations D * j := {d j,k }. It can then attempt the task in = 1, · · · , L trial episodes {τ j, }, for which it receives reward labels. Phase II: The agent learns from those trial episodes and the original demos to succeed at T j . A demonstration is a trajectory d = {(s t , a t )} of T states-action tuples that succeeds at the task, while a trial episode is a trajectory τ = {(s t , a t , r i (s t , a t ))} that also contains reward information.

Section Title: LEARNING TO IMITATE AND TRY AGAIN
  LEARNING TO IMITATE AND TRY AGAIN With the aforementioned problem statement in mind, we aim to develop a method that can learn to learn from both demonstration and trial-and-error experience. We wish to (1) meta-learn a Phase I policy that is suitable for gathering information about a task given demonstrations, and (2) meta- learn a Phase II policy which learns from both demonstrations and trials produced by the Phase I policy. Like prior few shot meta-learning works ( Finn et al., 2017b ;  Duan et al., 2017 ), we hope to achieve few shot success on a task by explicitly meta-training our Phase I and Phase II policies to learn from very little demonstration and trial data. We write the Phase I policy as π I θ (a|s, {d i,k }), where θ represents all the learnable parame- ters. π I conditions on the task demonstrations {d i,k } which helps it infer what the unknown task is before attempting the trials. We condition the Phase II policy on both demonstration and trial data and write it as π II φ (a|s, {d i,k }, {τ i, }), with parameters φ. One simple yet naive ap- proach would be to use a single model across both phases: e.g., using a single MAML ( Finn et al., 2017a ) policy that sees the demonstration, executes trial(s) in the environment and then adapts its own weights from them. However, a key challenge in meta-training such a single model is that updates based on Phase II behavior (after the trials) will also change Phase I be- havior (during the trials). Thus each meta-training update changes the distribution of trial tra- jectories τ i, ∼ π I θ (a|s, {d i,k }) that the Phase II policy learns from. Prior meta-reinforcement learning work ( Duan et al., 2016 ;  Finn et al., 2017a ) have addressed the issue of a changing trial distribution by re-collecting on-policy trial trajectories from the environment after every gradient step during meta-training, but this can be difficult in real-world problem settings with broad task distributions, where it is impractical to collect large amounts of on-policy experience. Instead, we represent and train π I θ , π II φ sep- arately, decoupling their optimization. In particular, we train π I θ first, freeze its weights, and collect trial data {τ i, } from the environment for each meta-training task T i . We then train π II φ using our col- lected trial data. Crucially, θ and φ are separate so training π II φ will not change π I θ behavior, and the distribution of trial data {τ i,l } for each task remains station- ary. How do we train each of these policies with off-policy demonstration and trial data? π I must be trained in a way that will provide useful exploration for in- ferring the task. One simple and effec- tive strategy for exploration is posterior or Thompson sampling ( Russo et al., 2018 ;  Rakelly et al., 2019 ), i.e. greedily act ac- cording to the policy's current belief of the task. To this end, we train π I using a meta-imitation learning setup, where for each task T i we assume access to a set of demonstrations D * i . π I conditions on K demonstrations {d i,k } ⊂ D * i and aims to maximize the likelihood of the actions under another demonstration of the same task d test i ∈ D * i (the test demo is distinct from the conditioning demos). This gives the following Phase I loss for T i : Then we can meta-train π I θ by minimizing Eq. 1 across the set of meta-train tasks {T i }: We train π II in a similar fashion, but additionally condition on L trial trajectories {τ i, }, which are the result of executing π I in the environment. Suppose for any task T i , we have a set of demo-trial Published as a conference paper at ICLR 2020 Perform task with re-trial policy π II φ (a|s, {d j,k }, {τ j,l }) 7: end for pairs D i = {({d i,k }, {τ i,j })}. Then the Phase II objective for T i is: Eq. 3 encourages π II to use and improve upon the trial experience {τ i, }, which includes reward information. If a trial has high reward, then π I likely inferred the task correctly from demonstration alone, and π II should reinforce that high reward trial behavior. If a trial has low reward, then π I probably inferred the task incorrectly and π II should avoid that low reward trial behavior. Hence even failed trials can help π II correctly infer the task by showing it what not to do. Note that since Eq. 3 evaluates log likelihood at states and actions from the held out demonstration d test i , π II cannot simply copy behavior from high reward trials in {τ i, } but instead must generalize from the trials to a new instance of the same task. We meta-train π II φ by minimizing Eq. 3 across the meta-train tasks: We refer to our approach as Watch-Try-Learn (WTL), and describe our meta-training and meta-test procedures in detail in Alg. 1 and Alg. 2, respectively. We also illustrate the meta-training flow in  Fig. 2 . In practice we meta-train π I θ and π II θ by solving Eqs. 2 and 4 using stochastic gradient descent or some variant. We iteratively sample (minibatches of) tasks T i to compute gradient updates on θ or φ, respectively. At meta-test time, for any test task T j we receive demonstrations {d j,k } and obtain the demo-conditioned Phase I policy π I θ (a|s, {d j,k }). We collect trial(s) {τ j,l } using π I θ in the environment. Then we obtain the Phase II policy π II φ (a|s, {d j,k }, {τ j,l }). Finally, we execute the π II φ in the environment to solve the task.

Section Title: WATCH-TRY-LEARN IMPLEMENTATION
  WATCH-TRY-LEARN IMPLEMENTATION WTL and Alg. 1 allow for general representations of the Phase I policy π I θ (a|s, {d i,k }) so long as it conditions on the task demonstrations {d i,k }. Similarly, the Phase II policy π II (a|s, {d i,k }, {τ i, }) must condition on both {d i,k } and the trials {τ i, }. Hence a variety of adaptation mechanisms could be used. We choose to implement this conditioning in each policy by embedding the demonstration data and (for Phase II) the trial data into context vectors using neural networks.  Figure 3  illustrates the π II φ architecture assuming a single demonstration and trial. The embedding network first applies a vision network to each demonstration and trial trajectory image s t , producing demo and trial feature matrices (respectively) where each row corresponds to one timestep's features. We concatenate the demo and trial feature matrices together on the feature (horizontal) dimension, along with the trial episode rewards to produce a single matrix combining demo and trial observation information and trial reward information. We then apply a 1-D convolution along the time (vertical) dimension of this matrix and flatten to obtain a single vector that integrates and aggregates information across time. Finally we apply a small MLP to produce the context embedding vector which contains information about both the demos {d i,k } and the trials {τ i, }. The Phase I policy π I θ , illustrated in Appendix Figure 8, produces a similar context embedding but only uses demonstration data. This architectural design resembles prior contextual meta-learning works ( Duan et al., 2016 ;  Mishra et al., 2018 ;  Duan et al., 2017 ;  Rakelly et al., 2019 ), which have previously considered how to meta-learn efficiently from one modality of data (trials or demonstrations), but not how to integrate multiple sources, including off-policy trial data. Since each policy is additionally conditioned on the current state s, we concatenate the context embedding with the current state features before feeding both as input to an actor network, which produces a Gaussian mixture distribution ( Bishop, 1994 ) over actions. The current state features in- clude visual features produced by another vision network, distinct from the one used to produce the context embedding. Both vision networks use a fairly standard convolutional neural network archi- tecture with a final spatial softmax layer that extracts keypoints ( Levine et al., 2016 ). The entire π II φ architecture, illustrated in  Figure 3 , is trainable end-to-end using backpropagation on Eq. 4, where the parameters φ represent the collective weights of all layers. Similarly, the entire π I θ architecture depicted in Appendix Figure 8 is trained by backpropagation on Eq. 2 where θ represents the set of weights across all layers. Note that since each neural network architecture is fixed and shared across tasks, we expect the input state dimensions to be the same across tasks, though the content (for example, the objects in the scene) may vary.

Section Title: EXPERIMENTS
  EXPERIMENTS In our experiments, we aim to evaluate our method on challenging few-shot learning domains that span multiple task families, where the agent must use both demonstrations and trial-and-error to effectively infer a policy for the task. Prior meta-imitation benchmarks ( Duan et al., 2017 ; Finn ) generally contain only a few tasks, and these tasks can be easily disambiguated given a single demonstration. Meanwhile, prior meta-reinforcement learning benchmarks ( Duan et al., 2016 ;  Finn et al., 2017a ) tend to contain fairly similar tasks that a meta-learner can solve with little exploration and no demonstration at all. Motivated by these shortcomings, we design two new problems where a meta-learner can leverage a combination of demonstration and trial experience: a toy reaching problem and a challenging multitask gripper control problem, described below. We evaluate how the following methods perform in those environments: BC: A behavior cloning method that does not condition on either demonstration or trial-and-error ex- perience, trained across all meta-training data. We train BC policies using maximum log-likelihood with expert demonstration actions. MIL ( Finn et al., 2017b ;  James et al., 2018 ): A meta-imitation learning method that conditions on demonstration data, but does not leverage trial-and-error experience. We train MIL policies to minimize Eq. 1 similar to the WTL Phase I policy, but MIL methods lack a Phase II step. To perform a controlled comparison, we use the same architecture for both MIL and WTL. WTL: Our Watch-Try-Learn method, which conditions on demonstration and trial experience. In all experiments, the agent receives K = 1 demonstration and can take L = 1 trial. BC + SAC: In the gripper environment we study how much trial-and-error experience soft actor critic (SAC) ( Haarnoja et al., 2018 ), a state of the art reinforcement learning algo- rithm, would require to solve a single task. While WTL meta-learns a single model that needs just one trial episode per meta-test task, in "BC + SAC" we fine-tune a separate RL agent for each meta-test task and analyze how much trial experience it needs to match WTL's single trial performance. We pre-train a policy similar to BC, then fine-tune for each meta-test task using SAC.

Section Title: REACHING ENVIRONMENT EXPERIMENTS
  REACHING ENVIRONMENT EXPERIMENTS To first verify that our method can actually leverage demonstration and trial experience in a simplified problem domain, we begin with toy planar reaching tasks inspired by  Finn et al. (2017b)  and illustrated in Appendix Fig 7. A demon- strator shows which of two objects to reach towards, but the agent's dynamics are randomized per task and may not match the demonstrator's. This simulates a domain adaptive setting such as a robot imitating a video of a human. Since the demonstrations {d i,k } do not help identify the unknown task dynamics, the agent must use trial episodes to successfully reach the target object. During meta-training, WTL meta-learns from demonstrations with the correct dynamics (d test i in Eq. 3) how to adapt to unknown dynamics in new tasks. To obtain expert demonstration data, we first train a re- inforcement learning agent using normalized advantage functions ( Gu et al., 2016 ), where the agent receives oracle observations that show only the true target. With our trained expert demonstration agent, we collect 2 demonstrations per task for 10000 meta-training tasks and 1000 meta-test tasks. For these toy experiments we use simplified versions of the architecture in  Fig. 3  as described in Appendix C. The results in  Fig 4  show WTL is able to quickly learn to imitate the expert, while methods that do not leverage trial information struggle due to the uncertainty in the task dynamics.

Section Title: GRIPPER ENVIRONMENT EXPERIMENTS
  GRIPPER ENVIRONMENT EXPERIMENTS The gripper environment is a realistic 3-D simulation as shown in  Figure 5 . Gripper tasks fall into four broad task families: button pressing, grasping, pushing, and pick and place. Within a task family, each task involves a different pair of kitchenware objects sampled from a set of nearly one hundred. Unlike prior meta-imitation learning benchmarks, tasks of the same family that are qual- itatively similar still have subtle but consequential differences. Some pushing tasks might require the agent to always push the left object towards the right object, while others might require the agent to always push, for example, the cup towards the teapot. The agent controls a free floating gripper with 7-D action space. The vision based policies receive image observations and gripper state, while state-space policies receive a vector of the gripper state and poses for all non-fixed ob- jects in the scene. Gripper environment episodes have a maximum length of 5 seconds and contain sparse binary rewards. In an HTC Vive virtual reality setup, a human demonstrator recorded 1536 demonstrations for 768 distinct tasks involving 96 distinct sets of kitchenware objects. We held out 40 tasks corresponding to 5 sets of kitchenware objects for our meta-validation dataset, which we used for hyperparameter selection. Similarly, we selected and held out 5 object sets of 40 tasks for our meta-test dataset, which we used for final evaluations. Refer to Appendix A for a more detailed description of the scene setup, task families, and reward functions. We trained and evaluated MIL, BC, and WTL policies with both state-space observations and vision observations. Appendix C describes hyperparameter selection using the meta-validation tasks and Appendix C.3 analyzes the sample and time complexity of WTL. The MIL policy uses an identical Published as a conference paper at ICLR 2020 architecture and objective to the WTL trial policy, while the BC policy architecture is the same as the WTL trial policy without the any embedding components. For vision based models, we crop and resize image observations from 300 × 220 to 100 × 100 before providing them as input. We show the meta-test task success rates in  Fig. 6 . Overall, in both state space and vision domains, we find that WTL outperforms MIL and BC by a substantial margin, indicating that it can effectively leverage information from the trial and integrate it with that of the demonstration in order to achieve greater performance. Finally, for the BC + SAC comparison we pre-trained an actor with behavior cloning and fine-tuned 4 RL agents per task with identical hyperparameters using the TFAgents ( Guadarrama et al., 2018 ) SAC implementation.  Table 1  shows that BC + SAC fine-tuning typically requires thousands of trial episodes per task to reach the same performance our meta-trained WTL method achieves after one demonstration and a single trial episode. Appendix C.4 shows the BC + SAC training curves averaged across the different meta-test tasks. We proposed a meta-learning algorithm that al- lows an agent to quickly learn new behavior from a single demonstration followed by trial experience and associated (possibly sparse) re- wards. The demonstration allows the agent to infer the type of task to be performed, and the trials enable it to improve its performance by resolving ambiguities in new test time situa- tions. We presented experimental results where the agent is meta-trained on a broad distribution of tasks, after which it is able to quickly learn tasks with new held-out objects from just one demonstration and a trial. We showed that our approach outperforms prior meta-imitation ap- proaches in challenging experimental domains. As illustrated in our qualitative failure analysis (Appendix D), one area of future improvement involves improving the informativeness of even failed trial trajectories generated by π I . In future work, we hope to explore alternatives to pos- terior sampling that produce more informative trials while maintaining sample and computational efficiency. We also plan to explore ways of extending our approach to be meta-trained on a much broader range of tasks, testing the performance of the agent on completely new held-out tasks rather than on held-out objects. The Watch-Try-Learn (WTL) approach enables a natural way for non-expert users to train agents to perform new tasks: by demonstrating the task and then observing and critiquing the performance of the agent on the task if it initially fails. WTL achieves this through a unique combination of demonstrations and trials in the inner loop of a meta-learning system, where the demonstration guides the exploration process for subsequent trials, and the use of trials allows the agent to learn new task objectives which may not have been seen during meta-training. We hope that this work paves the way towards more practical and general algorithms for meta-learning behavior.

Section Title: ACKNOWLEDGEMENTS
  ACKNOWLEDGEMENTS

```
