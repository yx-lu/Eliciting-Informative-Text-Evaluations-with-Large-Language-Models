Title:
```
CAPACITY-LIMITED REINFORCEMENT LEARNING: APPLICATIONS IN DEEP ACTOR-CRITIC METHODS FOR CONTINUOUS CONTROL
```
Abstract:
```
Biological and artificial agents must learn to act optimally in spite of a limited capacity for processing, storing, and attending to information. We formalize this type of bounded rationality in terms of an information-theoretic constraint on the complexity of policies that agents seek to learn. We present the Capacity-Limited Reinforcement Learning (CLRL) objective which defines an optimal policy sub- ject to an information capacity constraint. This objective is optimized by drawing from methods used in rate distortion theory and information theory, and applied to the reinforcement learning setting. Using this objective we implement a novel Capacity-Limited Actor-Critic (CLAC) algorithm and situate it within a broader family of RL algorithms such as the Soft Actor Critic (SAC) and discuss their similarities and differences. Our experiments show that compared to alternative approaches, CLAC offers improvements in generalization between training and modified test environments. This is achieved in the CLAC model while display- ing high sample efficiency and minimal requirements for hyper-parameter tuning.
```

Figures/Tables Captions:
```
Figure 1: Normal distribution representations of CLAC (blue) and SAC (orange) model policies with standard deviations of 0.8 and 0.9. Light blue displays the value of the hidden state feature defining the state transition probability. Beta distribution used to gereate state features in red.
Figure 2: Results from tests in the generalizability of learned policies in SAC and CLAC models, varied by mutual information coefficient and entropy coefficient. Again the number of states N=10, and the number of agents trained is 5. During training, hidden state features are resampled after each trial from the same Beta distribution.
Figure 3: For all tests 5 separate agents are trained. Humanoid and Ant are trained for 0.5M steps, Walker and Cheetah for 0.4 M, and the pendulums for 200K. CLAC mutual information coefficients are chosen to be the optimal values sampled from the range [0,0.3] in 0.025 increments.
Figure 4: Results from tests in the generalizability of learned policies in SAC and CLAC models with 5 agents each. Static parameters represent the same environment features used to test agents. Testing parameters rep- resent sampling from a disjoint set around the training parameters. Extreme testing parameters represent a disjoint set further from the training than the random parameters. Error bars represent 95% confidence interval.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The success of Deep RL has led to significant milestones in AI research (e.g  Tesauro (1994) ,  Mnih et al. (2013) , Silver et al. (2016)). However, at the same time, some have called into question the extent to which these approaches are able to demonstrate nontrivial generalization of learning, as compared to simply memorizing state-action sequences ( Packer et al. (2018) ,  Cobbe et al. (2018) ). In this paper, we look to improve sample efficient generalization in RL by making a connection to the idea of capacity limits in the field of Information Theory. As we will demonstrate, this idea has strong theoretical ties to recently popular approaches for Maximum Entropy Reinforcement Learn- ing (MERL) ( Haarnoja et al., 2018b ) and KL-regularized RL ( Tirumala et al., 2019 ). Furthermore, our new framework for capacity limited RL motivates a new algorithm called Capacity Limited Actor Critic (CLAC) that displays superior out of distribution generalization properties without sac- rificing sample efficiency or online learning performance. RL generally consists of a Markov Decision Process (MDP) defined by a state s, an action a, a transition function P (s |s, a) that determines the probability of the next state, and a reward function r(s, a). In this setting the primary objective is typically to learn a policy, which is defined as a probability distribution over actions conditioned on states, π(a|s). In fact, we would like to learn a policy that optimizes the expected return over horizon T following the state distribution of the policy ρ π : J(π) = E (st,at)∼ρπ [ T t=0 r(s t , a t )]. While MDPs define the setting for an idealized learning agent, any physical communication system (biological or artificial) is necessarily limited to transmitting information at a finite rate. In informa- tion theory, the information rate of a channel with input x and output y is quantified by the so-called mutual information, I(X, Y ). Hence, for any physical information processing system, I(X, Y ) ≤ C for some finite value of C. At the same time, agents with limited information processing capacities should seek to produce behavior that maximizes expected utility. Hence, for a physical agent, opti- mal behavior is the result of solving a constrained optimization problem (maximizing the utility of behavior, subject to constraints on available information capacity). Recent work in cognitive science has shown that this constrained optimization perspective on information processing can account for generalization in biological perception ( Sims, 2018 ). In the present work, we apply this perspective to the reinforcement learning framework. In particular, we consider an agent's policy as an informa- tion channel, that maps from the current state of the environment, to a probability distribution over actions, and define optimal behavior subject to a constraint on this information capacity of this chan- nel. More formally, an optimally efficient communication channel is one that minimizes expected loss in utility, E[L(x, y)], to this constraint: This objective is well-studied within the field of Rate Distortion theory ( Tretiak, 1974 ), a sub-field of information theory. In our application of RD theory onto RL, we consider the policy function π(a|s) learned by an RL agent to be a communication channel that maps from the state s onto a probability of performing an action a. This allows us to define the optimality condition of an RL agent with a constraint on information representation as Where I(π(a|s)) is the mutual information of the policy function when taken to be the information channel mapping states onto actions. The introduction of this policy mutual information term in this constraint connects the objective of a RL agent onto the desire of control over the amount of information used to represent agent behaviour. There are multiple ways of calculating this value which will be discussed in the next section. This allows us to define C, the desired maximum channel capacity, and optimize performance in the environment in relation to this capacity. This optimization can be used to define a learning objective that better reflects the reality of information constraints on physical agents, and as we will see, allow for better control over the trade off of immediate performance and generalization.

Section Title: CAPACITY LIMITED RL (CLRL)
  CAPACITY LIMITED RL (CLRL)

Section Title: CAPACITY-LIMITED LEARNING OBJECTIVE
  CAPACITY-LIMITED LEARNING OBJECTIVE Insight from an information theoretic perspective inspires the goal of maximizing reward obtained subject to some constraints on policy complexity (as measured by its mutual information). In prac- tice, the way we impose a limit on the amount of information that the agent uses to represent its policy is done by applying a penalty to the reward based on this value. This allows us to define a learning objective that regularizes the observed reward: The key difference with the standard RL objective the added penalty to the reward observed based on the amount of information that would be required to represent the policy. Policies with higher mutual information values have a greater complexity, in an information-theoretic sense, and this weighted value is used to discourage policies that would require a high information capacity channel. Thus, this learning objective will directly encourage the development of policies that are simple (use low information to represent) but have high utility. Additionally, if there are multiple policies that achieve the same performance, this objective will naturally favor the simplest among them. Higher values of β skew the learning objective to prefer policies with less required information.

Section Title: CALCULATING MUTUAL INFORMATION
  CALCULATING MUTUAL INFORMATION The mutual information in the CLRL learning objective can be defined in different ways depending on the application, and the features of the learning environment. Because CLRL describes a broad learning objective that can be applied to different existing RL methods, these different methods may be best suited by different approximations of mutual information. For discrete state and action spaces in tabular learning conditions the space is small enough that it can be defined in terms of the probability mass functions as follows: However, for more complicated problems, especially environments involving continuous state/action spaces, we will need a different approach. One possibility is to use the definition of mutual informa- tion in terms of the probability density functions for continuous distributions. However this method requires the integration over both the state and action spaces, as well as the approximation of both the marginal action p a (a) and marginal state probabilities p s (s). An alternative that avoids these complications is to break up the policy mutual information into the components of its constituent entropies. This calculation of mutual information also avoids the approximation of both the marginal state distribution and marginal action distribution, with only one approximation required, being cal- culated by either of:

Section Title: CONNECTIONS TO MAXIMUM ENTROPY REINFORCEMENT LEARNING (MERL)
  CONNECTIONS TO MAXIMUM ENTROPY REINFORCEMENT LEARNING (MERL) Maximum Entropy RL ( Haarnoja et al., 2018a ) is a popular RL framework that also makes use of a regularized learning objective to maximize This learning objective alters the traditional method by augmenting the reward maximization ob- jective with an additional weighted value based on the entropy of the policy αH(π(·|s t )). This encourages policies that are closer to the uniform distribution and thus more random, since these action distributions will have a higher entropy. The entropy coefficient α controls the weight bal- ancing the reward and entropy of the policy. As this value increases to infinity, the model will learn to always prefer policies that are completely uniform. When this value is 0, the model will only consider the reward returned by the environment and is equivalent to the traditional RL learning objective. As there is a close theoretical tie between the frameworks, we leverage the successful Maximum Entropy method Soft-Actor-Critic (SAC) as the main point of comparison for our approach. SAC is an off-policy RL method that displays high sample efficiency and resilience to changes in hy- perparameters even in difficult continuous control robot simulation environments ( Haarnoja et al., 2018b ). To make the comparison easier with SAC, we utilize the top equation for the calculation of I(π(a|s)) in equation 3. This is because it utilizes the entropy of the policy H(π(a|s)), which is calculated in existing MERL methods, instead of H(π(s|a)), which represents the difficult to define entropy of the probability of being in a state given that the agent has preformed an action. One ben- efit to this approach is that any MERL method will naturally need to compute the value H(π(a|s)) and can be altered with relative ease to include the additional marginal action term, as is done in this implementation. In the next section on the Deep CLAC algorithm, we apply this alteration to the existing Soft-Actor Critic method that uses the MERL objective to improve exploration. This is done to show that the Capacity-Limited objective can be used to alter existing methods with relative ease, especially ones that already compute the policy entropy H(π(a|s)), such as MERL methods.

Section Title: CONNECTIONS TO KL-REGULARIZED REINFORCEMENT LEARNING
  CONNECTIONS TO KL-REGULARIZED REINFORCEMENT LEARNING The KL-Regularized expected reward objective maximizes referring to the full history up to time t as x t = [s 0 , a 0 , ...s t , a t ]. This method is used in the hierarchical learning domain which separates broad high-level task focused decisions and specific low-level task agnostic actions ( Tirumala et al., 2019 ). In KL-RL, this is done by training a task specific policy π(a|s) as well as a default policy π 0 (a|s) and using the KL divergence between these two policies to regularize the reward observed by the agent. In this way the agent learns to generalize across tasks by increasing the reward observed when reusing learned behaviour across tasks. The default and task specific policies do not converge to the same policy as they have a different level of access to state information, encouraging π 0 to learn a task independent policy. This difference in state information access can come in the form of additional latent state features available to the task specific policy π. The structure of CLAC is not hierarchical and thus we do not compare the performance of CLAC to a KL-RL method, however these two methods do share a mathematical relation that mirrors the connection between KL-RL and SAC. As noted in  Tirumala et al. (2019)  the maximum entropy objective is a special case of the KL-regularized objective where the default policy is taken to be a uniform distribution. This is intuitively in line with the motivation behind the MERL objective, as it seeks to increase exploration by encouraging random behavior where possible. A similar relation can be made between KL-RL and the CLRL defined here, where the 'default policy' is taken to be the product of the marginal action and state distributions. Because of the motivation of discouraging high information capacity policies, a negative sign is applied to the regularization coefficient giving β = −α. This derivation relating KL-RL to CLRL is given by first taking the definition of the KL-RL objective: We can write the denominator of this logarithm term generally as a function of a state and action such as ρ(a, s). This allows us to define KL-RL to be the case where ρ 0 (a, s) = π 0 (a t |s t ). In this same manner SAC can be defined as the uniform distribution ρ(a, s) = 1/||A||. Finally, CLAC can be defined as ρ(a, s) = π a (a)π s (s) the product of the marginal action and state probabilities. As with the relation between SAC and KL-RL, this relation is in line with the motivation of these learning objectives, because this product of marginal state and action functions can be though of as a generic state-independent policy, and we encourage policies that are more similar to this generic policy because they require less information to represent. A clear conceptual example of this is the fact that the information capacity required to represent a policy is minimized by both a completely uniform policy and one that preforms the same action in all states.

Section Title: DEEP CAPACITY LIMITED ACTOR CRITIC LEARNING
  DEEP CAPACITY LIMITED ACTOR CRITIC LEARNING There are many RL approaches that could be applied in optimizing the capacity-limited learning objective. Here, we implement the CLAC algorithm based on a baseline implementation ( Hill et al., 2018 ) of the Soft Actor-Critic as described in ( Haarnoja et al., 2018a ). The reason for doing this is to allow for a comparison a CLRL and a MERL objective based method, while keeping other factors such as network structure static. Specifically, this network structure uses policy iteration consisting of a value network with target network updating, 2 q-function networks used to reduce the overestimation bias, and a policy network used to determine the action selection. The derivation of SAC relies on the so-called 'soft value function' V (s t ) = E at∼π [Q(s t , a t ) + αH(π(a t |s t )]. Here we use the analogous capacity-limited value function V (s t ) = E at∼π [Q(s t , a t ) − βI(π(a t |s t )]. Because we choose to define the mutual information value in terms of its constituent entropy's, a slight alteration to SAC method is carried forward into defining the gradients of the value function network, q-function networks, and policy network. This allows us to define the gradients used to update these three networks as follows (a full derivation is provided in the appendix), where f φ (η t ; s t ) defines the policy under the same reparameterization trick a t = f φ (η t ; s t ), used in SAC and KL-RL, where η t is a noise vector sampled from a spherical Gaussian distribution ( Haarnoja et al. (2018a) ). Together these give the capacity-limited actor-critic algorithm: Where π µ (a t ) ∼ D for a t ∈ D indicates the approximation of the marginal policy function based on an estimation obtained from all ac- tions in the current training mini-batch. This is done by using the batch of actions to create a multi-variate Gaussian distribution of the same order as the action space, and determining the probability of each action given that distribu- tion. Using a different approach to estimating the marginal action probability would be possi- ble, such as training an additional DNN to es- timate this marginal and updating it based on observed actions. This would replace the batch approximation step with a gradient update of the marginal action distribution approximation network: µ ← µ − λ M∇µ J M (µ).

Section Title: LEARNING ENVIRONMENTS
  LEARNING ENVIRONMENTS

Section Title: CONTINUOUS N-CHAIN
  CONTINUOUS N-CHAIN We explore a continuous action space version of the n-chain environment as described by  Strens (2000) . This environment consists of N states with agents starting in the S 1 state and S N as the terminal state. Agents act by selecting a continuous value from [0,1], and the probability of them moving to the next state, p(s t+1 |a t , s t ), is proportional to the difference between their action and the hidden value H depending on the state they are in, H st . These hidden values are sampled from a Beta distribution Beta(a, b). The reward is -1 for all states and 0 for the final state. For all tests shown here the number of states N = 10, and beta distribution shape parameters a = 50, b = 50. In Figure 5 of the appendix we show and graph the state transition function and a diagram for the state and hidden values sampled from the beta distribution. In  Figure 1  we compare the policies learned by CLAC and SAC. For both algorithms, after training we sample 100,000 actions from the policy for a particular state, and plot the resulting probabil- ity density function over actions. In this example, the optimal action in this state is ∼ 0.61. The regularization coefficients for CLAC and SAC (the mutual information term, and the entropy term, respectively) are chosen so that the resulting policies have the same standard deviation of the re- sulting policy. As shown, the mean of the CLAC action distribution shifts away from the hidden state value towards the across-state mean of the beta distribution. This demonstrates that the CLAC algorithm essentially learns a useful "prior" over its policy representation. Meanwhile, this shift in mean action distribution is not present in the SAC model. This is intuitively related to the dif- ference between these regularization methods as the mean of the distribution relates to the mutual information but not the entropy. We argue that in many natural tasks, policies are are not statistically independent across states, and hence the learning objective of CLAC may better capture useful sta- tistical regularities of the environment, and consequently allow for better learning as well as better generalization

Section Title: GENERALIZATION IN CONTINUOUS N-CHAIN
  GENERALIZATION IN CONTINUOUS N-CHAIN To test the generalizability of the policies learned by CLAC and SAC, these models are first trained on hidden state features generated from one beta distribution, and then placed in an environment with a shifted beta distribution used to generate hidden state features. To ensure that the agents learn the underlying beta distribution and not just a single set of random values, these parameters are re-sampled at the end of each trial. A randomization trial is used where the b parameter of the distribution Beta(a, b) is shifted by 10% from its original. An additional extreme randomization trial is used where the value of b in Beta(a, b) is randomly chosen to be either +20% or −20% from the training value.  Figure 2  displays these results with the left panel representing the learning environment, the middle representing the randomized environment, and the right panel representing the extreme randomization environment. These results show that, for the CLAC model, increasing the value of the mutual information co- efficient increases the performance in these perturbed environments. This contrasts the effect of the entropy coefficient in the SAC model, which displays no clear correlation between varying the entropy coefficient and generalized performance. Again this connects to the different motivations behind these models, as the CLAC model enables the shifting of mean actions in a policy as it re- lates to the policy mutual information, while the mean does not impact the policy entropy and is not captured by the entropy coefficient magnitude.

Section Title: CONTINUOUS CONTROL
  CONTINUOUS CONTROL In this section we compare the sample efficiency of CLAC against a suite of DRL baselines from the stable baseline implementation ( Hill et al. (2018) ). The models tested 1 are the Soft Actor-Critic (SAC;  Haarnoja et al. (2018a) ), Deep Deterministic Policy Gradient (DDPG;  Lillicrap et al. (2015) ), Proximal Policy Optimization (PPO1;  Schulman et al. (2017) ), and Advantage Actor Critic (A2C;  Mnih et al. (2016) ). These specific models were chosen to allow for a fair comparison of the current implementation of the Capacity-Limited Actor-Critic, as opposed to newer methods like PPO2, A3C, and TD3 which leverage improvements such as asynchronous training, parallel workers, or delayed policy update, which are not used in the current CLAC implementation. Although these additional features are not used in the present implementation, they are potential areas of future development in the CLAC method. As noted, CLAC and SAC are implemented in a similar fashion, and for this reason we compare these models in a series of environments similar to those used to showcase the high sample effi- ciency and low reliance on hyperparameter optimization of SAC. The learning environments used are the Roboschool environments ( Brockman et al. (2016) ), an open source version of the same en- vironments 2 used to compare the Soft Actor-Critic method against other models. Figure 6 displays visual representations from these learning environments, ranging from the relatively simple Inverse Pendulum task (top-right) which consists of 1 action dimension and 9 observation dimensions, to the humanoid environment consisting of 17 action dimensions and 44 observation dimensions. The results in  Figure 3  indicate that CLAC shares the high sample efficiency of the SAC method, while working with a different learning objective. Additionally, both SAC and CLAC require little hyperparameter optimization. The only parameter that is tuned in the CLAC model is the mutual information coefficient, with the used values for each model included in the Appendix. The SAC model uses an automatic adjustment of the entropy coefficient, that is discussed in the next sec- tion. Our CLAC model is implemented using the same policy, state value, and state-action value networks, and these results show that it has retained some key features of SAC while using a dif- ferent learning objective. This provides support for considering the capacity limited reinforcement learning objective as a general approach that can be applied to different existing RL approaches.

Section Title: GENERALIZATION IN CONTINUOUS CONTROL
  GENERALIZATION IN CONTINUOUS CONTROL To test generalization in continuous control environments, we utilize the standard OpenAI Gym Pendulum environment, a similar task as the Roboschool Pendulum environment, which will allow us to vary environment features like the mass and length of the pendulum. We adapt the specific method of generating randomized environment features from  Packer et al. (2018) , by varying both the mass and length of the pole in the Pendulum task from a given range after each episode of testing. In the training parameter case, the mass and length is kept the same as was used during training, equal to 1. In the testing parameter case the pole mass and length are both sampled randomly from the disjoint set, [0.5, 0.7] ∪ [1.3, 1.5]. In the extreme testing parameter case, the features are sampled from a disjoint set further from the training values: [0.3, 0.5] ∪ [1.5, 1.6]. These results show the improved generalization of the policies learned by the CLAC model com- pared to Soft-Actor Critic in a continuous control task. Because of the complexity of the task compared to the more simple continuous n-chain environment, this improvement in generalization comes at a cost to training performance. SAC and CLAC models were trained for the same num- ber of time steps before testing their generalization capability. This experiment did not compare the performance to other models like DDPG, PPO, and A2C because their lower sample efficiency meant that their policies at the same time in training were too poor to preform well in even the static parameter case.

Section Title: AUTOMATIC MUTUAL INFORMATION COEFFICIENT ADJUSTMENT
  AUTOMATIC MUTUAL INFORMATION COEFFICIENT ADJUSTMENT In defining the algorithm for encouraging policies that required less information to represent, we took the practical approach of applying a penalty to the reward observed by an agent based on the complexity of the agents policy. However, there is a way that we can regain our original objective of maximizing reward subject to a given policy channel capacity. This is done by defining a target mutual information for our policy, and automatically updating our coefficient to approach that de- sired target while training. This is done much in the same way as the automatic adjustment of the entropy coefficient α used in SAC that is described in  Haarnoja et al. (2018b) . A full derivation of this method is provided in the Appendix. This derivation provides us the following gradient used to update the value of β: where C is the desired maximum policy channel capacity. Thus we can define the automatic mutual information coefficient adjustment version of Algorithm 1, as including the update: As we have seen, the mutual information coefficient controls the extent to which the CLRL algorithm prioritizes some degree of policy generalizability over training reward maximization. However, as with the entropy coefficient in SAC, this mutual information coefficient is sensitive to the scale of the reward of the learning environment, because the penalty −βI(π(a|s)) is applied to the reward observed in the environment. Automatically adjusting the entropy coefficient α is partly motivated by invariance to reward scale, and the same benefit is gained by automatically adjusting the mutual information coefficient in CLAC. However with CLAC, this automatic adjustment has the additional connection to the original moti- vation for applying a capacity-limit, to allow us to define a channel capacity and maximize reward relative to it. Another benefit of this approach is mitigating some of the potential issues caused by negative transfer, a difficult open question in the area of generalization where attempting to transfer learning can negatively impact performance ( Taylor & Stone (2009) ). These negative effects could be mitigated by having the model update the β coefficient throughout learning and determine which value of the coefficient best reflects the degree to which knowledge of a policy in one state can be ap- plied to other states. Only tight constraints on information capacity should be disastrously impacted by negative transfer, as weaker constraints do not force agents to reuse policies in states where they are not associated with rewards.

Section Title: DISCUSSION
  DISCUSSION In this work we present a formalization of the Capacity-Limited Reinforcement Learning (CLRL) objective and relate it to existing methods in Deep Reinforcement Learning. We argue that CLRL defines a broad approach that can be applied onto existing RL methods to provide better of control over some aspects of generalization. To support this position we present the the Capacity-Limited Actor-Critic (CLAC), an application of CLRL onto a deep off-policy actor-critic model. We use a continuous N-chain environment to clarify the impact on performance and generalization afforded by altering parameters of the CLAC model, specifically the mutual information coefficient. Empirical results from robot simulation tasks show that the CLAC model can achieve similar sample efficiency and performance as state-of-the-art methods in complex tasks while using the CLRL ob- jective. This is a key result as this objective differs from that of the similar Maximum Entropy learn- ing objective. MERL based methods use an entropy coefficient to encourage exploration through more random behaviour where possible. Meanwhile, CLRL based methods use the mutual informa- tion coefficient to balance one aspect of generalization, and is motivated by the natural capacity for storing and processing information that exists in biological agents. This difference present in CLAC enabled it to outperform existing methods in environments with perturbed features, displaying the improvement in generalization afforded by a capacity-limited learning objective.
    Todorov et al. (2012) ), the environment originally used to test SAC. The Roboschool environment has higher penalties for certain sub-optimal actions, which explains the negative performance in for some models in some environments.

```
