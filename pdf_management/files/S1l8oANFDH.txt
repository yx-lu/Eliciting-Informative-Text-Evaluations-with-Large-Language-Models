Title:
```
Published as a conference paper at ICLR 2020 SYNTHESIZING PROGRAMMATIC POLICIES THAT INDUCTIVELY GENERALIZE
```
Abstract:
```
Deep reinforcement learning has successfully solved a number of challenging control tasks. However, learned policies typically have difficulty generalizing to novel environments. We propose an algorithm for learning programmatic state machine policies that can capture repeating behaviors. By doing so, they have the ability to generalize to instances requiring an arbitrary number of repetitions, a property we call inductive generalization. However, state machine policies are hard to learn since they consist of a combination of continuous and discrete structures. We propose a learning framework called adaptive teaching, which learns a state machine policy by imitating a teacher; in contrast to traditional imitation learning, our teacher adaptively updates itself based on the structure of the student. We show that our algorithm can be used to learn policies that inductively generalize to novel environments, whereas traditional neural network policies fail to do so.
```

Figures/Tables Captions:
```
Figure 1: Running example: retrieving an autonomous car from tight parking spots. The goal is to learn a state-machine policy (e) that is trained on scenarios (a), (b), and (c), that generalizes to scenario (d).
Figure 2: Flowchart connecting the different components of the algorithm.
Figure 3: Visualization showing the student-teacher interaction for two iterations. (a) The loop-free policies (with their corresponding rewards) learned by the teacher for two different initial states. Here, the boxes signify the different segments in the loop-free policies, the colors signify different actions, and the lengths of the boxes signify the durations of the segments. (b) The mapping between the segments and the modes in the state-machine-i.e., p(µ = m j ). Each box shows the composition of modes vertically distributed according their probabilities. For example, the third segment in the loop-free policy for x 1 0 has p(µ = Green) = 0.65 and p(µ = Brown) = 0.35. (c) The most probable rollouts from the state-machine policy learned by the student. Finally, (d), (e) and (f) are similar to (a), (b) and (c), but for the second iteration.
Figure 4: Comparison of performances on train (left) and test (middle) distributions. Our approach outperforms the baselines on all benchmarks in terms of test performance. An empty bar indicates that the policy learned for that experiment failed on all runs. We also plot test performance for different choices of training distribution for the Car benchmark (right).
Figure 5: (a-c) The RL policy generates unstructured trajectories, and therefore does not generalize from (a) the training distribution to (b) the test distribution. In contrast, our state machine policy in (c) generates a highly structured trajectory that generalizes well. (c-e) A user can modify our state machine policy to improve performance. In (d), the user sets the steering angle to the maximum value 0.5, and in (e), the user sets the thresholds in the switching conditions G m2 m1 , G m1 m2 to 0.1.
Figure 6: Left: Trajectories for the Quad (leftmost) and QuadPO (second from the left) benchmarks using our state machine policy. Right: Graph of vertical acceleration over time for both our policy (red) and the neural network policy (blue), for Quad (second from the right) and QuadPO (rightmost).
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Existing deep reinforcement learning (RL) approaches have difficulty generalizing to novel envi- ronments ( Packer et al., 2018 ;  Rajeswaran et al., 2017 ). More specifically, consider a task that requires performing a repeating behavior-we would like to be able to learn a policy that generalizes to instances requiring an arbitrary number of repetitions. We refer to this property as inductive generalization. In supervised learning, specialized neural network architectures have been proposed that exhibit inductive generalization on tasks such as list manipulation ( Cai et al., 2017 ), but it is not obvious how those techniques would generalize to the control problems discussed in this paper. Alternatively, algorithms have been proposed for learning programmatic policies for control problems that generalize better than traditional neural network policies ( Verma et al., 2019 ;  2018 ), but existing approaches have focused on simple stateless policies that make learning generalizable repetitive behaviors hard, e.g., a stateless program cannot internally keep track of the number of repetitions made so far and decide the next action based on that progress. We propose an algorithm for learning programmatic state machine policies. Such a policy consists of a set of internal states, called modes, each of which is associated with a controller that is applied while in that mode. The policy also includes transition predicates that describe how the mode is updated. These policies are sufficiently expressive to capture tasks of interest-e.g., they can perform repeating tasks by cycling through some subset of modes during execution. Additionally, state machine policies are strongly biased towards policies that inductively generalize, that deep RL policies lack. In other words, this policy class is both realizable (i.e., it contains a "right" policy that solves the problem for all environments) and identifiable (i.e., we can learn the right policy from limited data). However, state machine policies are challenging to learn because their discrete state transitions make it difficult to use gradient-based optimization. One standard solution is to "soften" the state transitions by making them probabilistic. However, these techniques alone are insufficient; they still run into local optima due to the the constraints on the structure of the policy function, as well as the relatively few parameters they possess. To address this issue, we propose an approach called adaptive teacher that alternatingly learns a teacher and a student. The teacher is an over-parameterized version of the student, which is a state-machine policy trained to mimic the teacher. Because the teacher is over-parameterized, it can be easily learned using model-based numerical optimization (but does not generalize as well as the student). Furthermore, our approach is different from traditional imitation learning ( Schaal, 1999 ;  Ross et al., 2011 ) since the teacher is regularized to favor strategies similar to the ones taken by the student, to ensure the student can successfully mimic the teacher. As the student improves, the teacher improves as well. This alternating optimization can naturally be derived within the framework of variational inference, where the teacher encodes the variational distribution ( Wainwright et al., 2008 ). We implement our algorithm and evaluate it on a set of reinforcement learning problems focused on tasks that require inductive generalization. We show that traditional deep RL approaches perform well on the original task, but fail to generalize inductively, whereas our state machine policies successfully generalize beyond the training distribution. We emphasize that we do not focus on problems that require large state-machines, which is a qualitatively different problem from ours and would require different algorithms to solve. We believe that state-machines are most useful when only a few modes are required. In particular, we are interested in problems where a relatively simple behavior must be repeated a certain number of times to solve the given task. The key premise behind our approach, as shown by our evaluation, is that, in these cases, compact state-machines can represent policies that both have good performance and are generalizable. In fact, our algorithm solves all of our benchmarks using state-machine policies with at most 4 modes. When many modes are needed, then the number of possible transition structures grows exponentially, making it difficult to learn the "right" structure without having an exponential amount of training data.

Section Title: Example
  Example Consider the autonomous car in  Figure 1 , which consists of a blue car (the agent) parked between two stationary black cars. The system state is (x, y, θ, d), where (x, y) is the center of the car, θ is the orientation, and d is the distance between the two black cars. The actions are (v, ψ), Published as a conference paper at ICLR 2020 where v is velocity and ψ is steering angle (we consider velocity control since the speed is low). The dynamics are standard bicycle dynamics. The goal is to drive out of the parked spot to an adjacent lane while avoiding collisions. This task is easy when d is large (Figure 1a). It is somewhat more involved when d is small, since it requires multiple maneuvers (Figures 1b and 1c). However, it becomes challenging when d is very small (Figure 1d). A standard RL algorithm will train a policy that performs well on the distances seen during training but does not generalize to smaller distances. In contrast, our goal is to train an agent on scenarios (a), (b), and (c), that generalizes to scenario (d). In Figure 1e, we show a state machine policy synthesized by our algorithm for this task. We use d f and d b to denote the distances between the agent and the front and back black cars, respectively. This policy has three different modes (besides a start mode m s and an end mode m e ). Roughly speaking, this policy says (i) immediately shift from mode m s to m 1 , and drive the car forward and to the left, (ii) continue until close to the car in front; then, transition to mode m 2 , and drive the car backwards and to the right, (iii) continue until close to the car behind; then, transition back to mode m 1 , (iv) iterate between m 1 and m 2 until the car can safely exit the parking spot; then, transition to mode m 3 , and drive forward and to the right to make the car parallel to the lane. This policy inductively generalizes since it captures the iterative behavior of driving forward and then backward until exiting the parking spot. Thus, it successfully solves the scenario in Figure 1d.

Section Title: Related work
  Related work There has been growing interest in using program synthesis to aid machine learn- ing ( Lake et al., 2015 ;  Ellis et al., 2015 ;  2018 ;  Valkov et al., 2018 ;  Young et al., 2019 ). Our work is most closely related to recent work using imitation learning to learn programmatic policies ( Verma et al., 2018 ;  Bastani et al., 2018 ;  Zhu et al., 2019 ;  Verma et al., 2019 ). These approaches use a neural network policy as the teacher. However, they are focused on learning stateless policies and hence, they use a supervised dataset of state-action pairs from the teacher and a domain-specific program synthesizer to learn programmatic policies. Building such a synthesizer for state machine policies is challenging since they contain both discrete and continuous parameters and internal state. The student in our algorithm needs to learn the state-machine policy from entire "trajectory traces" to learn the internal state. In particular, each trajectory trace consists of the sequence of states and actions from the initial state to the goal state visited by the teacher, but also encodes which states correspond to mode changes for the teacher. In the teacher's iteration, the teacher's mode changes are regularized to align more closely with the possible student mode changes. As a consequence, in the student's iteration, it is easier for the student to mimic the teacher's mode changes. Leveraging this connection between the teacher structure and student structure is critical for us to be able to learn state-machine policies Additionally, with the exception of ( Verma et al., 2019 ), for the other approaches, there is no feedback from the student to the teacher. State machines have been previously used to represent policies that have internal state (typically called memory). To learn these policies, gradient ascent methods assume a fixed structure and optimize over real-valued parameters ( Meuleau et al., 1999 ;  Peshkin et al., 2001 ;  Aberdeen & Baxter, 2002 ), whereas policy iteration methods uses dynamic programming to extend the structure ( Hansen, 1998 ). Our method combines both, but similarly to  Poupart & Boutilier (2004) , the structure space is bounded. In addition, programmatic state machines use programs to represent state transitions and actions rules, and as a result can perform well while remaining small in size. Hierarchies of Abstract Machines (HAM)s also use programmatic state machines for hierarchical reinforcement learning, but assumed a fixed, hand-designed structure ( Parr & Russell, 1998 ; Andre & Russell, 2002). Our inductive generalization goal is related to that of meta-learning ( Finn et al., 2017 ); however, whereas meta-learning trains on a few examples from the novel environment, our goal is to generalize without additional training. Our work is also related to guided policy search, which uses a teacher in the form of a trajectory optimizer to train a neural network student ( Levine & Koltun, 2013 ). However, training programmatic policies is more challenging since the teacher must mirror the structure of the student. Finally, it has recently been shown that over-parameterization is essential in helping neural networks avoid local minima ( Allen-Zhu et al., 2019 ). Relaxing optimization problems by adding more parameters is a well established technique; in many cases, re-parameterization can make difficult non-convex problems solve efficiently ( Carlone & Calafiore, 2018 ).

Section Title: Dynamics
  Dynamics We consider a safety specification φ S : X → R and a goal specification φ G : X → R. Then, the agent aims to reach a goal state φ G (x) ≤ 0 while staying in safe states φ S (x) ≤ 0. A positive value for φ S (x) (resp., φ G (x)) quantifies the degree to which x is unsafe (resp., away from the goal).

Section Title: Policies
  Policies We consider policies with internal memory π : O × S → A × S where S ⊆ R d S is the set of internal states; we assume the memory is initialized to a constant s 0 . Given such a policy π, we sample a rollout (or trajectory) τ = (x 0 , x 1 , ..., x N ) with horizon N ∈ N by sampling x 0 ∼ X 0 and then performing a discrete-time simulation x n+1 = x n + F (x n , a n ) · ∆, where (a n , s n+1 ) = π(Z(x n ), s n ) and ∆ ∈ R >0 is the time increment. Since F , Z, and π are deterministic, τ is fully determined by x 0 and π; τ can also be represented as a list of actions combined with the initial state i.e τ = x 0 , (a 0 , a 1 , · · · , a N ) . The degree to which φ S and φ G are satisfied along a trajectory is quantified by a reward function R(π, x 0 ) = −φ G (x N ) + − N n=0 φ S (x n ) + , where x + = max(0, x). The optimal policy π * in some policy class Π is one which maximizes the expected reward E x0∼X0 [R(π, x 0 )].

Section Title: Inductive generalization
  Inductive generalization Beyond optimizing reward, we want a policy that inductively generalizes to unseen environments. Formally, we consider two initial state distributions: a training distribution X train 0 , and a test distribution X test 0 that includes the extreme states never encountered during training. Then, the goal is to train a policy according to X train 0 -i.e., π * = arg max π∈Π E x0∼X train 0 [R(π, x 0 )], (1) but measure its performance according to X test 0 -i.e., E x0∼X test 0 [R(π, x 0 )].

Section Title: PROGRAMMATIC STATE MACHINE POLICIES
  PROGRAMMATIC STATE MACHINE POLICIES To achieve inductive generalization, we aim to synthesize programmatic policies in the form of state machines. At a high level, state machines can be thought of as compositions of much simpler policies, where the internal state of the state machines (called its mode) indicates which simple policy is currently being used. Thus, state machines are capable of encoding complex nonlinear control tasks such as iteratively repeating a complex sequence of actions (e.g., the car example in  Figure 1 ). At the same time, state machines are substantially more structured than more typical policy classes such as neural networks and decision trees. More precisely, a state machine π is a tuple M, H, G, m s , m e . The modes m i ∈ M of π are the internal memory of the state machine. Each mode m i ∈ M corresponds to an action function H mi ∈ H, which is a function H mi : O → A mapping observations to actions. When in mode m i , the agent takes action a = H mi (o). Furthermore, each pair of modes (m i , m j ) corresponds to a switching condition G mj mi ∈ G, which is a function G mj mi : O → R. When an agent in mode m i observes o such that G mj mi (o) ≥ 0, then the agent transitions from mode m i to mode m j . If there are multiple modes m j with non-negative switching weight G mj mi (o) ≥ 0, then the agent transitions to the one that is greatest in magnitude; if there are several modes of equal weight, we take the first one according to a fixed ordering. Finally, m s , m e ∈ M are the start and end modes, respectively; the state machine mode is initialized to m s , and the state machine terminates when it transitions to m e . where darg max is a deterministic arg max that breaks ties as described above. Action functions and switching conditions are specified by grammars that encode the space of possible functions as a space of programs. Different grammars can be used for different problems. Typical grammars for action functions include constants {C α : o → α} and proportional controls {P i α0,α1 : o → α 0 (o[i] − α 1 )}. A typical grammar for switching conditions is the grammar of Boolean predicates over the current observation o, where o[i] is the ith component of o. In all these grammars, α i ∈ R are parameters to be learned. The grammar for switching conditions also has discrete parameters encoding the choice of expression. For example, in  Figure 1 , the action functions are constants, and the switching conditions are inequalities over components of o.

Section Title: FRAMEWORK FOR SYNTHESIZING PROGRAMMATIC POLICIES
  FRAMEWORK FOR SYNTHESIZING PROGRAMMATIC POLICIES We now describe the adaptive teaching framework for synthesizing state machine policies. In this section, the teacher is abstractly represented as a collection of trajectories τ x0 (i.e., an open-loop controller consisting of a fixed sequence of actions) for each initial state x 0 . A key insight is that we can parameterize τ x0 in a way that mirrors the structure of the state machine student. As we discuss in Section 4.2, we parameterize τ x0 as a "loop-free" state machine. Intuitively, our algorithm efficiently computes τ x0 (from multiple initial states x 0 ) using gradient-based optimization, and then "glues" them together using maximum likelihood to construct a state machine policy.

Section Title: ADAPTIVE TEACHING VIA VARIATIONAL INFERENCE
  ADAPTIVE TEACHING VIA VARIATIONAL INFERENCE We derive the adaptive teaching formulation by reformulating the learning problem in the framework of probabilistic reinforcement learning, and also consider policies π that are probabilistic state machines (see Section 4.3). Then, we use a variational approach to break the problem into the teacher and the student steps. In this approach, the log-likelihood of a policy π is defined as follows: (π) = log E p(τ |π) [e λR(τ ) ] (3) where p(τ | π) is the probability of sampling rollout τ when using policy π from a random initial state x 0 , λ ∈ R ≥0 is a hyperparameter, and R(τ ) is the reward assigned to τ . We have (π) = log E q(τ ) e λR(τ ) · p(τ | π) q(τ ) ≥ E q(τ ) [λR(τ ) + log p(τ |π) − log q(τ )] (4) where q(τ ) is the variational distribution and the inequality follows from Jensen's inequality. Thus, we can optimize π by maximizing the lower bound Eq (4) on (π). Since the first and third term of Eq (4) are constant with respect to π, we have Next, the optimal choice for q (i.e., to minimize the gap in the inequality in Eq (4)) is q * = arg min q D KL (q(τ ) e λR(τ ) · p(τ | π)/Z) (6) where Z is a normalizing constant. We choose q to have form q(τ ) = p(x 0 ) · δ(τ − τ x0 ), where δ is the Dirac delta function, p(x 0 ) is the initial state distribution, and τ x0 are the parameters to be optimized, where τ x0 encodes a trajectory from x 0 . Then, up to constants, the objective of Eq (6) equals The first term is constant; the second term is degenerate, but it is also constant. Thus, we have Thus, we can optimize Eq (3) by alternatingly optimizing Eq (5) and Eq (7). We interpret these equations as adaptive teaching. At a high level, the teacher (i.e., the variational distribution q * in Eq (7)) is used to guide the optimization of the student (i.e., the state machine policy π * in Eq (5)). Rather than compute the teacher in closed form, we approximate it by sampling Published as a conference paper at ICLR 2020 finitely many initial states x k 0 ∼ X 0 and then computing the optimal rollout from x k 0 . Formally, on the ith iteration, the teacher and student are updated as follows: The teacher objective Eq (8) is to both maximize the reward R(τ ) from a random initial state x 0 and to maximize the probability p(τ | π, x 0 ) of obtaining the rollout τ from initial state x 0 according to the current student π. The latter encourages the teacher to match the structure of the student. Furthermore, the teacher is itself updated at each step to account for the changing structure of the student. The student objective Eq (9) is to imitate the distribution of rollouts according to the teacher.  Figure 2  shows the different components of our algorithm.

Section Title: TEACHER: COMPUTING LOOP-FREE POLICIES
  TEACHER: COMPUTING LOOP-FREE POLICIES We begin by describing how the teacher solves the trajectory optimization problem Eq (8)-i.e., computing τ k for a given initial state x k 0 .

Section Title: Parameterization
  Parameterization One approach is to parameterize τ as an arbitrary action sequence (a 0 , a 1 , ...) and use gradient-based optimization to compute τ . However, this approach can perform poorly-even though we regularize τ towards the student, it could exhibit behaviors that are hard for the student to capture. Instead, we parameterize τ in a way that mirrors the student. In particular, we parameterize τ like a state machine, but rather than having modes and switching conditions that adaptively determine the sequence of action functions to be executed and the duration of execution, the sequence of action functions is fixed and each action function is executed for a fixed duration. More precisely, we represent τ as an loop-free policy τ = H, T . To execute τ , each action function H i ∈ H is applied for the corresponding duration T i ∈ T , after which H i+1 is applied. The action functions are from the same grammar of action functions for the student. The obvious way to represent a duration T i is as a number of time steps T i ∈ N. However, with this choice, we cannot use continuous optimization to optimize T i . Instead, we fix the number of discretization steps P for which H i is executed, and vary the time increment ∆ i = T i /P -i.e., x n+1 ≈ x n + F (x n , H i (o)) · ∆ i . We enforce ∆ i ≤ ∆ max for a small ∆ max to ensure that the discrete-time approximation of the dynamics is sufficiently accurate. Figure 3(a) and (d) show examples of loop-free policies for two different initial states and two different teacher iterations. The loop-free policies in (d) are regularized to match the student's state-machine policy learned in the previous iteration (shown in Figure 3(c)).

Section Title: Optimization
  Optimization this probability is hard because of the discrete-continuous structure of π. Another alternative is to precompute the probabilities of all the trajectories τ that can be derived from π. However, this is also infeasible because the number of trajectories is unbounded. Thus, we perform trajectory optimization in two phases. First, we use a sampling-based optimization algorithm to obtain a set of good trajectories τ 1 , ..., τ L . Then, we apply gradient-based optimization, replacing p(· | π, x 0 ) with a term that regularizes τ to be close to {τ } L =1 . The first phase proceeds as follows: (i) sample τ 1 , · · · , τ L using π from x 0 , and let p be the probability of τ according to π, (ii) sort these samples in decreasing order of objective p ·e λR(τ ) , and (iii) discard all but the top ρ samples. This phase essentially performs one iteration of CEM ( Mannor et al., 2003 ). Then, in the second phase, we replace the probability expression with p(τ | π, x 0 ) ≈ ρ =1 p ·e −d(τ,τ ) ρ =1 p , which we use gradient-based optimization to optimize. Here, d(τ, τ ) is a distance metric between two loop-free policies, defined as the L 2 distance between the parameters of τ and τ . We chose the number of samples, ρ = 10. For our benchmarks, we did not notice any improvement in the number of student-teacher iterations by increasing ρ above 10. So, we believe we are not losing any information from this approximation.

Section Title: STUDENT: LEARNING STRUCTURED STATE MACHINE POLICIES VIA IMITATION
  STUDENT: LEARNING STRUCTURED STATE MACHINE POLICIES VIA IMITATION Next, we describe how the student solves the maximum likelihood problem Eq (9) to compute π * .

Section Title: Probabilistic state machines
  Probabilistic state machines Although the output of our algorithm is a student policy that is a deterministic state machine, our algorithm internally relies on distributions over rollouts induced by the student policy to guide the teacher. Thus, we represent the student policy as a probabilistic state machine during learning. To do so, we simply make the action functions H mj and switching conditions G mj 2 mj 1 probabilistic-instead of constant parameters in the grammar for action functions and switching conditions, now we have Gaussian distributions N (α, σ). Then, when executing π, we obtain i.i.d. samples of the parameters H mj ∼ H mj and {(G m j mj ) ∼ G m j mj } m j every time we switch to mode m j , and act according to H mj and {(G m j mj ) } until the mode switches again. By re-sampling these parameters on every mode switch, we avoid dependencies across different parts of a rollout or different rollouts. On the other hand, by not re-sampling these parameters within a mode switch, we ensure that the structure of π remains intact within a mode.

Section Title: Optimization
  Optimization Each τ k can be decomposed into segments (k, i) where action function H k,i is executed for duration T k,i . For example, each block in Figure 3(a) is a segment. Furthermore, for the student π, let H mj be the action function distribution for mode m j and G mj 2 mj 1 be the switching Published as a conference paper at ICLR 2020 condition distribution for mode m j1 to mode m j2 . Note that H mj and G mj 2 mj 1 are distributions whereas H k,i and T k,i are constants. We have For each (k, i), let µ k,i be the latent random variable indicating the ith mode used by π starting from x k 0 ; in particular, µ k,i is a categorical random variable that takes values in the modes {m j }. And µ k,i = m j means that H k,i is sampled from the distribution H mj and T k,i is determined by the sampled switching conditions from distributions {G m j mj }. Assuming the latent variable µ k,i allows the student to compute π * by computing H * mj and G mj 2 * mj 1 separately. In  Figure 3, (b)  and (e) show the learned mode mappings p(µ = m j ) for the segments in the loop-free policies shown in (a) and (d) respectively. Since directly optimizing the maximum likelihood π is hard in the presence of the latent variables µ k,i , we use the standard expectation maximization (EM) approach to optimizing π, where the E-step computes the distributions p(µ k,i = m j ) assuming π is fixed, and the M-step optimizes π assuming the probabilities p(µ k,i = m j ) are fixed. See Appendix A for details. In  Figure 3, (c)  and (f) show the most probable rollouts from the state-machine policies learned at the end of the EM approach for two different student iterations.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: Benchmarks
  Benchmarks We use 6 control problems, each with different training and test distributions (summa- rized in Figure 8 in Appendix C): (i) Car, the benchmark in  Figure 1 , (ii) Quad, where the goal is to maneuver a 2D quadcopter through an obstacle course by controlling its vertical acceleration, where we vary the obstacle course length, see  Figure 6  leftmost, (iii) QuadPO, a variant where the obstacles are unobserved but periodic (so the agent can perform well using a repeating motion), see  Figure 6  (second from left), (iv) Pendulum, where we vary the pendulum mass, (v) Cart-Pole, where we vary the time horizon and pole length, and (vi) Swimmer, where the goal is to move the swimmer forward through a viscous liquid, where we vary the length of the segments comprising the robot swimmer.

Section Title: Baselines
  Baselines We compare against: (i) RL: PPO with a feed-forward neural network policy, (ii) RL- LSTM: PPO with an LSTM, (iii) Direct-Opt: learning a state machine policy directly via numerical optimization. Hyper-parameters are chosen to maximize performance on the training distribution. More details about the baselines and the hyper-parameters can be found in Appendices B.2, B.3, & B.4. Each algorithm is trained 5 times; we choose the one that performs best on the training distribution. Note that for the comparison to RL approaches, we use model-free algorithms, whereas, in our algorithm, the teacher uses model-based optimization. We do not compare against model-based RL approaches because (a) even model-free RL approaches achieve almost perfect performance on the training distribution (see  Figure 4  left) and (b) our main goal is to compare the performance of our policies and the neural network policies on the test distribution and not the training distribution. Moreover, in case the model of the system is unknown, we can use known algorithms to infer the model from data ( Ahmadi et al., 2018 ) and then use this learned model in our algorithm.

Section Title: RESULTS
  RESULTS   Figure 4  shows results on both training and test distributions. We measure performance as the fraction of rollouts (out of 1000) that both satisfy the safety specification and reach the goal.

Section Title: Inductive generalization
  Inductive generalization For all benchmarks, our policy generalizes well on the test distribution. In four cases, we generalize perfectly (all runs satisfy the metric). For Quad and QuadPO, the policies result in collisions on some runs, but only towards the end of the obstacle course.

Section Title: Comparison to RL
  Comparison to RL The RL policies mostly achieve good training performance, but generalize poorly since they over-specialize to states seen during training. The exceptions are Pendulum and Swimmer. Even in these cases, the RL policies take longer to reach the goals than our state machine policies (see Figure 10 and Figure 11 in Appendix C). For QuadPO, the RL policy does not achieve a good training performance since the states are partially observed. We may expect the LSTM policies to alleviate this issue. However, the LSTM policies often perform poorly even on the training distribution, and also generalize worse than the feed-forward neural network policies.

Section Title: Comparison to direct-opt
  Comparison to direct-opt The state machine policies learned using direct-opt perform poorly even in training because of the numerous local optima arising due to the structural constraints. This illustrats the need to use adaptive teaching to learn state machine policies.

Section Title: QUALITATIVE ANALYSIS
  QUALITATIVE ANALYSIS Behavior of policy. We empirical analyze the policies.  Figure 5  shows the trajectory taken by the RL policy (a), compared to our policy (c), from a training initial state for the Car benchmark. The RL policy does not exhibit a repeating behavior, which causes it to fail on the trajectory from a test state shown in (b). Similarly,  Figure 6  (right) compares the actions taking by our policy to those taken by the RL policy on Quad and QuadPO. Our policy produces smooth repeating actions, whereas the RL policy does not. Action vs time graphs for other benchmarks can be found in the appendix (Figures 12, 13, & 14) and they all show similar behaviors.

Section Title: Varying the training distribution
  Varying the training distribution We study how the test performance changes as we vary the training distribution on the Car benchmark. We vary X train 0 as d ∼ [d min , 13], where d min = {13, 12.5, 12, 11.5, 11.2, 11}, but fix X test 0 to d ∼ [11, 12].  Figure 4  (right) shows how test per- formance varies with d min for both our policy and the RL policy. Our policy inductively generalizes for a wide range of training distributions. In contrast, the test performance of the RL policy initially increases as the train distribution gets bigger, but it eventually starts declining. The reason is that its training performance actually starts to decline. Thus, in some settings, our approach (even when trained on smaller distributions) can produce policies that outperform the neural network policies produced by RL (even when trained on the full distribution).

Section Title: Interpretability
  Interpretability An added benefit of our state machine policies is interpretability. In particular, we demonstrate the interpretability of our policies by showing how a user can modify a learned state machine policy. Consider the policy from Figure 1e for the autonomous car. We manually make the following changes: (i) increase the steering angle in H m1 to its maximum value 0.5, and (ii) decrease the gap maintained between the agent and the black cars by changing the switching condition G m2 m1 to d f ≤ 0.1 and G m1 m2 to d b ≤ 0.1.  Figure 5  demonstrates these changes-it shows trajectories obtained using the original policy (c), the first modified policy (d), and the second modified policy (e). There is no straightforward way to make these kinds of changes to a neural network policy.

Section Title: CONCLUSION
  CONCLUSION We have proposed an algorithm for learning state machine policies that inductively generalize to novel environments. Our approach is based on a framework called adaptive teaching that alternatively learns a student that imitates a teacher, who in-turn adapts to the structure of the student. We demonstrate that our policies inductively generalize better than RL policies. In the future, we will explore more complex grammars for the action functions and the switching conditions, for example, with some parts being small neural networks, while still retaining the ability to learn generalizable behaviors. Moreover, we will extend our approach to use model-free techniques in the teacher's algorithm to make our approach more aligned with the reinforcement learning premise. Finally, we believe that the idea of learning programmatic representations and using the adaptive teaching algorithm to deal with the mixed discrete-continuous problems can be applied to other learning settings such as supervised learning and unsupervised learning.

```
