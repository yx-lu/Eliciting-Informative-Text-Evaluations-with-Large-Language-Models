Title:
```
None
```
Abstract:
```
Standard gradient descent methods are susceptible to a range of issues that can im- pede training, such as high correlations and different scaling in parameter space. These difficulties can be addressed by second-order approaches that apply a pre- conditioning matrix to the gradient to improve convergence. Unfortunately, such algorithms typically struggle to scale to high-dimensional problems, in part be- cause the calculation of specific preconditioners such as the inverse Hessian or Fisher information matrix is highly expensive. We introduce first-order precon- ditioning (FOP), a fast, scalable approach that generalizes previous work on hy- pergradient descent (Almeida et al., 1998; Maclaurin et al., 2015; Baydin et al., 2017) to learn a preconditioning matrix that only makes use of first-order informa- tion. Experiments show that FOP is able to improve the performance of standard deep learning optimizers on visual classification and reinforcement learning tasks with minimal computational overhead. We also investigate the properties of the learned preconditioning matrices and perform a preliminary theoretical analysis of the algorithm.
```

Figures/Tables Captions:
```
Figure 1: A comparison of FOP to common optimizers on toy problems. The red dot indicates the initial position on the loss surface. The purpose of these visualizations is not to establish the su- periority of one optimizer over another, but rather to gain an intuition for their qualitative behavior. (Left) Gradient descent on the Booth function. FOP converges in 543 iterations, while SGD takes 832 steps and 6,221 for Adam. (Right) Gradient descent on Himmelbau's function. Adam, converg- ing in 5398 iterations, finds a different global minimum from FOP and SGD, which converge in 289 and 386 steps, respectively. In both cases, we can see that FOP moves more aggressively across the objective function surface compared to the other methods. The poor performance of Adam is likely attributable to the non-stochastic nature of these toy settings.
Figure 2: Results on CIFAR-10 (top) and ImageNet (bottom), averaged over 3 runs. All models are trained with momentum as the base optimizer. We can see that FOP converges more quickly than standard and baseline methods, with slightly superior generalization performance. Learning a spatial curvature matrix adds negligible computational cost to the training process.
Figure 3: Adding FOP improves the hyperparameter robustness of standard optimizers. (a) The final test accuracy of a 9-layer CNN trained on CIFAR-10 for different settings of SGD with momentum (top) and SGD with momentum and FOP (bottom), averaged over three runs. Settings in which adding FOP improves performance by at least one standard deviation are highlighted in blue. FOP appears to be most useful for higher values of the learning rate and momentum parameters. The FOP matrices were trained with Adam with a learning rate of 5 × 10 −4 . (b) The performance of Adam for a range of learning rates both with and without FOP, averaged over three runs. While performance is similar, the top performing models are improved by the addition of FOP. The FOP matrices were trained using the same settings as the models in (a).
Figure 4: Bipedal Walker on varying terrains. We plot the the median reward (of the best seen policy so far) per PPO step across 10 runs for (a) and 5 runs for (b), respectively. The shading denotes the 95% bootstrapped confidence intervals. Momentum with FOP outperforms momentum alone significantly, both in speed of learning and final score, in addition to beating Adam.
Figure 5: Understanding the learned preconditioning matrices (M M ) for CIFAR-10. (a) The evolution of an example preconditioning matrix throughout the training process from the ninth layer in a ResNet-18 model trained on ImageNet. Each layer learned a similar whitening structure. (b) The histograms of matrix values across layers during training. The training process is traced by going from back to front in the plots. We can see that the convergence of the values of the matrix, corresponding to a stronger decorrelation structure and a reduced L 2 norm, is stronger in the higher layers of a network. (c) The sorted eigenvalues of the final learned preconditioning matrices for the first seven layers of a 9-layer network trained on CIFAR-10 (the top two layers were 1 × 1 kernels). The distribution shifts downward and becomes more uniform in higher layers. This is interesting, as while a uniform distribution of eigenvalues is considered helpful in aiding convergence, the downward shift in values makes the matrix less invertible.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION High-dimensional nonlinear optimization problems often present a number of difficulties, such as strongly-correlated parameters and variable scaling along different directions in parameter space ( Martens, 2016 ). Despite this, deep neural networks and other large-scale machine learning models applied to such problems typically rely on simple variations of gradient descent to train, which is known to be highly sensitive to these difficulties. While this approach often works well in practice, addressing the underlying issues directly could provide stronger theoretical guarantees, accelerate training, and improve generalization. Adaptive learning rate methods such as Adam ( Kingma and Ba, 2014 ), Adagrad ( Duchi et al., 2011 ), and RMSProp ( Tieleman and Hinton, 2012 ) provide some degree of higher-order approximation to re-scale updates based on per-parameter behavior. Newton-based methods make use of the curvature of the loss surface to both re-scale and rotate the gradient in order to improve convergence. Natural gradient methods ( Amari, 1998 ) do the same in order to enforce smoothness in the evolution of the model's conditional distribution. In each of the latter two cases, the focus is on computing a specific linear transformation of the gradient that improves the conditioning of the problem. This transformation is typically known as a preconditioning, or curvature, matrix. In the case of quasi- Newton methods, the preconditioning matrix takes the form of the inverse Hessian, while for natural gradient methods it's the inverse Fisher information matrix. Computing these transformations is typically intractable for high-dimensional problems, and while a number of approximate methods exist for both (e.g., ( Byrd et al., 1996 ;  Martens and Grosse, 2015 ;  Grosse and Martens, 2016 )), they are often still too expensive for the performance gain they provide. These approaches also suffer from rigid inductive biases regarding the nature of the problems to which they are applied in that they seek to compute or approximate specific transformations. However, in large, non-convex problems, the optimal gradient transformation may be less obvious, or may even change over the course of training. In this paper, we address these issues through a method we term first-order preconditioning (FOP). Unlike previous approaches, FOP doesn't attempt to compute a specific preconditioner, such as the inverse Hessian, but rather uses first-order hypergradient descent ( Maclaurin et al., 2015 ) to learn an Under review as a conference paper at ICLR 2020 Update preconditioning matrices: adaptable transformation online directly from the task objective function. Our method adds minimal computational and memory cost to standard deep network training and results in improved conver- gence speed and generalization compared to standard approaches. FOP can be flexibly applied to any gradient-based optimization problem, and we show that when used in conjunction with stan- dard optimizers, it improves their performance. To our knowledge, this is also the first successful application of any hypergradient method to the ImageNet dataset ( Deng et al., 2009 ).

Section Title: FIRST ORDER PRECONDITIONING
  FIRST ORDER PRECONDITIONING

Section Title: THE BASIC APPROACH
  THE BASIC APPROACH Consider a parameter vector θ and a loss function J. A traditional gradient update with a precondi- tioning matrix P can be written as Our goal is to learn P . However, while we place no other constraints on our preconditioner, in order to ensure that it is positive semi-definite, and therefore does not reverse the direction of the gradient, we reparameterize P as the product of a matrix M with itself, changing Equation 1 to: Under reasonable assumptions, gradient descent is guaranteed to converge with the use of even a random symmetric, positive-definite preconditioner, as we show in Appendix A.3. Because θ (t) is a function of M (t−1) , we can then backpropagate from the loss at iteration t to the previous iteration's preconditioner via a simple application of the chain rule: By applying the chain and product rules to Equation 2, the gradient with respect to the preconditioner is then simply We provide a more detailed derivation in Appendix A.1. Note that ideally we would compute ∇ M (t) J (t+1) to update M (t) , but as we don't have access to J (t+1) yet, we follow the example of  Almeida et al. (1998)  and assume that J is smooth enough that this does not have an adverse effect on training. The basic approach is summarized in Algorithm 1. We use supervised learning as an example, but the same method applies to any gradient-based optimization. The preconditioned gradient can then be passed to any standard optimizer to produce an update for M . For example, Under review as a conference paper at ICLR 2020 we describe the procedure for using FOP with momentum ( Polyak, 1964 ) in Section 2.3. For multi- layer networks, in order to make the simplest modification to normal backpropagation, we learn a separate M for each layer, not a global curvature matrix. To get an intuition for the behavior of FOP compared to standard algorithms, we observed its tra- jectories on a set of low-dimensional optimization problems ( Figure 1 ). Interestingly, while FOP converged in fewer iterations than SGD and Adam ( Kingma and Ba, 2014 ), it took more jagged paths along the objective function surface, suggesting that while it takes more aggressive steps, it is perhaps also able to change direction more rapidly.

Section Title: SPATIAL PRECONDITIONING FOR CONVOLUTIONAL NETWORKS
  SPATIAL PRECONDITIONING FOR CONVOLUTIONAL NETWORKS In order to further reduce the computational cost of FOP in convolutional networks (CNNs), we implemented layer-wise spatial preconditioners, sharing the matrices across both input and output channels (results shown in Section 4). More concretely, if a convolutional layer has spatial kernels with shape k × k, we can learn a preconditioner that is k 2 × k 2 . To implement this, when θ is a 4- tensor of kernels of shape k×k×I×O, where I and O are the input and output channels, respectively, we can reshape it to a matrix of size k 2 × IO, left-multiply it by the learned curvature matrix, and then reshape it back to its original dimensions. When k is small, as is typically the case in deep CNNs, this preconditioner is small as well, resulting in both computational and memory efficiency. For large fully-connected networks, we adopt a low-rank preconditioner to save computation. In Appendix A.4, we offer further details and show that FOP is still effective even when the rank is very low.

Section Title: FOP FOR MOMENTUM
  FOP FOR MOMENTUM FOP can be implemented alongside any standard optimizer, such as gradient descent with momen- tum ( Polyak, 1964 ). Given a parameter vector θ and a loss function J, a basic momentum update with FOP matrix M is typically expressed as two steps: where v is the velocity term and α is the momentum parameter. Combining Equations 5 and 6 allows us to write the full update as If, as in  Maclaurin et al. (2015) , we were meta-learning M or only updating M after a certain number of iterations, we would then have to backpropagate through v to calculate the gradient for M . As we are updating M online, however, we only need to calculate ∇ M (t) J. Therefore, the update is the same as for standard gradient descent. The experiments in Section 4 were performed using this modification of momentum.

Section Title: RELATED WORK
  RELATED WORK   Almeida et al. (1998)  introduced the idea of using gradients from the objective function to learn opti- mization parameters such as the learning rate or a curvature matrix. However, their preconditioning matrix was strictly diagonal, amounting to an approximate Newton algorithm ( Martens, 2016 ), and they only tested their framework on simple optimization problems with either gradient descent or SGD. This is related to common deep learning optimizers such as Adagrad ( Duchi et al., 2011 ) and Adam ( Kingma and Ba, 2014 ), which also learn adaptive per-parameter learning rates, but unlike FOP, do not induce a rotation on the gradient. As we note in Section 2.3, an advantage of FOP is that it may also be used in conjunction with any such optimizer. More recently,  Maclaurin et al. (2015)  applied the backpropagation-across-iterations approach in a neural network context, terming the process hypergradient descent. Their method backpropagates through multiple iterations of the training process of a relatively shallow network to meta-learn a learning rate. However, this method can incur significant memory and computational cost for large models and long training times.  Bay- din et al. (2017)  instead proposed an online framework directly inherited from  Almeida et al. (1998)  that used hypergradient-based optimization in which the learning rate is updated after each itera- tion. Our method extends this idea to not only learn existing optimizer parameters (e.g., learning rate, momentum), but to introduce novel ones in the form of a non-diagonal, layer-specific precon- ditioning matrix for the gradient. It's also important to discuss the relationship of FOP to other, non-hypergradient preconditioning methods for deep networks. These mostly can be sorted into one Under review as a conference paper at ICLR 2020 of two categories, quasi-Newton algorithms and natural gradient approaches. Quasi-Newton meth- ods seek to learn an approximation of the inverse Hessian. L-BFGS, for example, does this through tracking the differences between gradients across iterations ( Byrd et al., 1996 ). This is significantly different from FOP, although the outer product of gradients used in the update for FOP can also be an approximation of the Hessian or Gauss-Newton matrices ( Bottou et al., 2016 ). Natural gradient methods, such as K-FAC ( Martens and Grosse, 2015 ) and KFC ( Grosse and Martens, 2016 ), which approximate the inverse Fisher information matrix, bear a much stronger resemblance to FOP. How- ever, there are notable differences. First, unlike FOP, these methods perform extra computation to ensure the invertibility of their curvature matrices. Second, the learning process for the precondi- tioner in these methods is completely different, as they do not backpropagate across iterations.

Section Title: EXPERIMENTS
  EXPERIMENTS We measured the performance of FOP on visual classification and reinforcement learning tasks. In order to measure the importance of the rotation induced by the preconditioning matrices in addition to the scaling, we also implemented hypergradient descent (HD) methods to learn a scalar layer-wise learning rate (S-HD) and a per-parameter (PP-HD) learning rate. The former is the same method implemented by  Baydin et al. (2017) , and the latter is equivalent to a strictly diagonal curvature matrix. We also implement a method we call normalized FOP (FOP-norm), in which we re-scale the preconditioning matrix to avoid any effect on the learning rate and rely solely on standard learning rate settings. Further details on this method can be found in Appendix A.2. All experiments were implemented using the TensorFlow library ( Abadi et al., 2015 ) 1 . For CIFAR-10 ( Krizhevsky, 2009 ), an image dataset consisting of 50,000 training and 10,000 test 32 × 32 RGB images divided into 10 object classes, we implemented a 9-layer convolutional archi- tecture inspired by  Springenberg et al. (2014) . We trained each model for 150 epochs with a batch size of 128 and initial learning rate 0.05, decaying the learning rate by a factor of 10 after 80 epochs. For S-HD, PP-HD, and FOP, we use Adam as the hypergradient optimizer with a learning rate of 1e-4. The results are plotted in  Figure 2 . FOP produces a significant speed-up in training and im- proves final test accuracy compared to baseline methods, including FOP-norm, indicating that both the rotation and the scaling learned by FOP is useful for learning.

Section Title: IMAGENET
  IMAGENET The ImageNet dataset consists of 1,281,167 training and 50,000 validation 299 × 299 RGB images divided into 1,000 categories ( Deng et al., 2009 ). Here, we trained a ResNet-18 ( He et al., 2015 ) model for 60 epochs with a batch size of 256 and an initial learning rate of 0.1, decaying by a factor of 10 at the 25th and 50th epochs. A summary of our results is displayed in  Figure 2 . We can see that the improved convergence speed and test performance observed on CIFAR-10 is maintained on this deeper model and more difficult dataset.

Section Title: HYPERPARAMETER ROBUSTNESS
  HYPERPARAMETER ROBUSTNESS In addition to measuring peak performance, we also tested the effect of FOP on the robustness of standard optimizers to hyperparameter selection.  Baydin et al. (2017)  demonstrated the ability of scalar hypergradients to mitigate the effect of the initial learning rate on performance. We therefore tested whether this benefit was preserved by FOP, as well as whether it extended to other hyperpa- rameter choices, such as the momentum coefficient. Our results, summarized in  Figure 3 , support this idea, showing that FOP can improve performance on a wide array of optimizer settings. For momentum (Fig. 3a), we see that the performance gap is greater for higher learning rates and mo- mentum values, and in several instances FOP is able to train successfully where pure SGD with momentum fails. For Adam, the difference is smaller, although in the highest-performance learning rate region adding FOP to Adam outperforms the standard method. We hypothesize that this smaller difference is in large part due to unanticipated effects that preconditioning the gradient has on the adaptive moment estimation performed by Adam. We leave further investigation into this interaction for future work.

Section Title: REINFORCEMENT LEARNING
  REINFORCEMENT LEARNING Reinforcement learning (RL) tasks are often more challenging than supervised learning, since the rewards in RL are sparse and the loss-landscape is frequently deceptive ( Sutton and Barto, 2018 ). We evaluated the performance of FOP on one such RL task - the training of a 2D bipedal robot walker ( Brockman et al., 2016 ;  Klimov, 2016 ) in two different terrains (see the inserts in  Figures 4 (a)  and (b) for illustration). This 2D robotic domain has been used in recent literature as a benchmark and for validation of new RL algorithms ( Ha, 2017 ; 2018;  Song et al., 2018 ;  Wang et al., 2019 ). The agent is trained to move forward (from left to right) in a given terrain, within a time limit of maximum 2000 steps and without falling over. The controller for the agent consists of simple feedforward policy and value networks. Both the networks are trained using the Proximal Policy Optimization (PPO) algorithm ( Schulman et al., 2017 ) (see Appendix A.5 for details).  Figures 4 (a)  and (b) plot the bipedal walker scores obtained from training the controller in two dif- ferent terrains. For both terrains, FOP with momentum trains faster and achieves better asymptotic performance than baseline methods (SGD with momentum and Adam), thus demonstrating the ef- fectiveness of FOP in RL tasks. We hypothesize that FOP's advantage results in part from deceptive loss surfaces, since FOP's ability to quickly change direction ( Figure 1 ) may help it avoid getting stuck in local optima (see Appendix A.5 for a discussion of deceptiveness in this task).

Section Title: WHAT IS FOP LEARNING?
  WHAT IS FOP LEARNING? By studying the learned preconditioning matrices, it's possible to gain an intuition for the effect FOP has on the training process. Interestingly, we found visual tasks induced similar structures in the preconditioning matrices across initializations and across layers. Visualizing the matrices (Figure 5a) shows that they develop a decorrelating, or whitening, structure: each of the 9 positions in the 3x3 convolutional filter sends a strong positive weight to itself, and negative weights to its immediate neighbors, without wrapping over the corners of the filter. This is interesting, as images are known to have a high degree of spatial autocorrelation ( Barlow, 1961 ;  1989 ). As a mechanism for reducing redundant computation, whitening visual inputs is known to be beneficial for both retinal processing in the brain ( Atick and Redlich, 1992 ;  Van Hateren, 1992 ) and in artificial networks ( Desjardins et al., 2015 ;  Huang et al., 2018 ). However, it is more unusual to consider whitening of the learning signal, as observed in FOP, rather than the forward activities.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 This learned pattern is accompanied by a shift in the norm of the curvature matrix, as the diagonal elements, initialized to one, shrink in value, and the off-diagonal elements grow. This shift in dis- tribution is visualized in Figure 5b, and grows stronger deeper in the network. It is possible that a greater degree of decorrelation is helpful for kernels that must disentangle high-level representations. We can also examine the eigenvalue spectra for the learned matrices across layers (Figure 5c). We can see that the basic requirement that in general a preconditioning matrix must be positive semi- definite, so as not to reverse the direction of the gradient, is met. However,the eigenvalues are very small in magnitude, indicating a near-zero determinant. This results in a matrix that is essentially non-invertible, an interesting property, as quasi-Newton and natural gradient methods seek to com- pute or approximate the inverse of either the Hessian or Fisher information matrix. The implications of this non-invertibility are avenues for future study. Furthermore, the eigenvalues grow smaller, and their distribution more uniform, higher in the network, in accordance with the pattern observed in Figure 5b. A uniform eigenspectrum is seen as an attribute of an effective preconditioning matrix ( Li, 2015 ), as it indicates an even convergence rate in parameter space.

Section Title: CONVERGENCE
  CONVERGENCE Our experiments indicate that the curvature matrices learned by FOP converge to a relatively fixed norm roughly two-thirds of the way through training (Figure 5b). This is important, as it indicates that the effective learning rate induced by the preconditioners stabilizes. This allows us to perform a preliminary convergence analysis of the algorithm in a manner analogous to  Baydin et al. (2017) . Consider a modification of FOP in which the symmetric, positive semi-definite preconditioning matrix P (t) = M (t) M (t) is rescaled at each iteration to have a certain norm γ (t) , such that γ (t) ≈ ||P (t) || 2 when t is small and γ (t) ≈ p (∞) when t is large, where p (∞) is some chosen constant. Specifically, as in  Baydin et al. (2017) , we set γ (t) = δ(t)||P (t) || 2 + (1 − δ(t))p (∞) , where δ(t) is some function that decays over time and starts training at 1 (e.g., 1/t 2 ). This formulation allows us to extend the convergence proof of  Baydin et al. (2017)  to FOP, under the same assumptions about the objective function J: Theorem 1 Suppose that J is convex and L-Lipschitz smooth with ∇ θ J < K for some fixed K and all model parameters θ. Then θ t → θ * if p (∞) < 1/L and tδ(t) → 0 as t → ∞, where the θ t are generated by (non-stochastic) gradient descent.

Section Title: Proof
  Proof where ρ is the hypergradient learning rate. Our assumptions about the limiting behavior of tδ(t) then imply that δ(t) P (t) → 0 and so γ (t) → p (∞) as t → ∞. For sufficiently large t, we therefore have γ (t) ∈ ( 1 L+1 , 1 L ). Note also that as P is symmetric and positive semi-definite, it will not prevent the convergence of gradient descent (Appendix A.3). Moreover, preliminary investigation shows that the angle of rotation induced by the FOP matrices is significantly below 90 • (Appendix Figure A.1).  Lillicrap et al. (2016)  showed that such a rotation does not impede the convergence of gradient descent. Because standard SGD converges under these conditions ( Karimi et al., 2016 ), FOP must as well.

Section Title: CONCLUSION
  CONCLUSION In this paper, we introduced a novel optimization technique, FOP, that learns a preconditioning ma- trix online to improve convergence and generalization in large-scale machine learning models. We tested FOP on several problems and architectures, examined the nature of the learned transforma- tions, and provided a preliminary analysis of FOP's convergence properties (Appendix 6). There are a number of opportunities for future work, including learning transformations for other optimization parameters (e.g., the momentum parameter in case of the the momentum optimizer), expanding and generalizing our theoretical analysis, and testing FOP on a wider variety of models and data sets. Under review as a conference paper at ICLR 2020
  https://drive.google.com/file/d/1vhB4fxDuxaYJcNP6ioEJQf4CLHxhy1ka/view?usp=sharing.

```
