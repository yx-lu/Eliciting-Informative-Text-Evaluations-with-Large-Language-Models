Title:
```
Under review as a conference paper at ICLR 2020 POWER UP! ROBUST GRAPH CONVOLUTIONAL NET- WORK BASED ON GRAPH POWERING
```
Abstract:
```
Graph convolutional networks (GCNs) are powerful tools for graph-structured data. However, they have been recently shown to be vulnerable to topological attacks. To enhance adversarial robustness, we go beyond spectral graph theory to robust graph theory. By challenging the classical graph Laplacian, we propose a new convolution operator that is provably robust in the spectral domain and is incorporated in the GCN architecture to improve expressivity and interpretability. By extending the original graph to a sequence of graphs, we also propose a robust training paradigm that encourages transferability across graphs that span a range of spatial and spectral characteristics. The proposed approaches are demonstrated in extensive experiments to simultaneously improve performance in both benign and adversarial situations.
```

Figures/Tables Captions:
```
Figure 1: From the original graph, we generate a series of graphs, which are weighted by param- eters that gauge the influence strengths, (optionally) sparsified, and eventually combined to form a variable power graph.
Figure 2: Our proposed framework can improve both clean and adversarial (10% attack by A DW3 (Bojchevski & Günnemann, 2019)) accuracy for semi-supervised learning benchmarks.
Figure 3: Comparison for SBM dataset. Shaded area indicates standard deviation over 10 runs.
Figure 4: Accuracy of first 100 frequency components of VPN (red) and adjacency matrix (blue).
Figure 5: Benign and adversarial accu- racy as power order increases. Dahsed lines correspond to vanilla GCN.
Figure 6: Robustness merit accuracy post-attack proposed-method − accuracy post-attack vanilla GCN reported in percentage un- der A DW3 attack. The error bar indicates standard deviation over 20 independent simulations.
Figure 7: Accuracy of first 500 frequency components for VPN (red) and adjacency matrix (blue). The x axis corresponds to the index k in Φ :,k:(k+1) Φ :,k:(k+1) X for signal reconstruction.
Figure 8: Accuracy of the leading frequency components in adversarial testing. We use Φ :,1:k Φ :,1:k X as reconstructed nodal features for testing, where k is 10 for Citeseer and 5 for Cora and Pubmed. Error bar indicates standard deviation over 10 runs.
Table 1: Results for semi-supervised node classification. We highlighted the best and the second best performances, where we broke the tie by choosing the one with the smallest standard deviation.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Graph convolutional networks (GCNs) are powerful extensions of convolutional neural networks (CNN) to graph-structured data. Recently, GCNs and variants have been applied to a wide range of domains, achieving state-of-the-art performances in social networks ( Kipf & Welling, 2017 ), traffic prediction ( Rahimi et al., 2018 ), recommendation systems ( Ying et al., 2018 ), applied chemistry and biology ( Kearnes et al., 2016 ;  Fout et al., 2017 ), and natural language processing (Atwood & Towsley, 2016;  Hamilton et al., 2017 ;  Bastings et al., 2017 ;  Marcheggiani & Titov, 2017 ), just to name a few ( Zhou et al., 2018 ;  Wu et al., 2019 ). GCNs belong to a family of spectral methods that deal with spectral representations of graphs ( Zhou et al., 2018 ;  Wu et al., 2019 ). A fundamental ingredient of GCNs is the graph convolution operation defined by the graph Laplacian in the Fourier domain: g θ x :=ĝ θ (L)x, (1) where x ∈ R n is the graph signal on the set of vertices V andĝ θ is a spectral function applied to the graph Laplacian L := D − A (where D and A are the degree matrix and the adjacency matrix, respectively). Because this operation is computational intensive for large graphs and non-spatially localized ( Bruna et al., 2014 ), early attempts relied on a parameterization with smooth coefficients ( Henaff et al., 2015 ) or a truncated expansion in terms of of Chebyshev polynomials ( Hammond et al., 2011 ). By further restricting the Chebyshev polynomial order by 2, the approach in ( Kipf & Welling, 2017 ) referred henceforth as the vanilla GCN pushed the state-of-the-art performance of semi-supervised learning. The network has the following layer-wise update rule: H (l+1) := ψ AH (l) W (l) , (2) where H (l) is the l-th layer hidden state (with H (1) := X as nodal features), W (l) is the l-th layer weight matrix, ψ is the usual point-wise activation function, and A is the convolution operator chosen to be the degree weighted Laplacian with some slight modifications ( Kipf & Welling, 2017 ). Subsequent GCN variants have different architectures, but they all share the use of the Laplacian matrix as the convolution operator ( Zhou et al., 2018 ;  Wu et al., 2019 ).

Section Title: WHY NOT GRAPH LAPLACIAN?
  WHY NOT GRAPH LAPLACIAN? Undoubtedly, the Laplacian operator (and its variants, e.g., normalized/powered Laplacian) plays a central role in spectral theory, and is a natural choice for a variety of spectral algorithms such as Under review as a conference paper at ICLR 2020 principal component analysis, clustering and linear embeddings ( Chung & Graham, 1997 ;  Belkin & Niyogi, 2002 ). So what can be problematic? From a spatial perspective, GCNs with d layers cannot acquire nodal information beyond its d- distance neighbors; hence, it severely limits its scope of data fusion. Recent works ( Lee et al., 2018 ;  Abu-El-Haija et al., 2018 ; 2019;  Wu et al., 2019 ) alleviated this issue by directly powering the graph Laplacian. From a spectral perspective, one could demand better spectral properties, given that GCN is funda- mentally a particular (yet effective) approximation of the spectral convolution (1). A key desirable property for generic spectral methods is known as "spectral separation," namely the spectrum should comprise a few dominant eigenvalues whose associated eigenvectors reveal the sought structure in the graph. A well-known prototype is the Ramanujan property, for which the second leading eigen- value of a r-regular graph is no larger than 2 √ r − 1, which is also enjoyed asymptotically by random r-regular graphs ( Friedman, 2004 ) and Erdős-Rényi graphs that are not too sparse ( Feige & Ofek, 2005 ). In a more realistic scenario, consider the stochastic block model (SBM), which attempts to capture the essence of many networks, including social graphs, citation graphs, and even brain networks ( Holland et al., 1983 ). Definition 1 (Simplified stochastic block model). The graph G with n nodes is drawn under SBM(n, k, a intra , a inter ) if the nodes are evenly and randomly partitioned into k communities, and nodes i and j are connected with probability a intra /n ∈ [0, 1] if they belong to the same community, and a inter /n ∈ [0, 1] if they are from different communities. It turns out that for community detection, the top k leading eigenvectors of the adjacency matrix A play an important role. In particular, for the case of 2 communities, spectral bisection algorithms simply take the second eigenvector to reveal the community structure. This can be also seen from the expected adjacency matrix E[A] under SBM(n, 2, a intra , a inter ), which is a rank-2 matrix with the top eigenvalue 1 2 (a intra + a inter ) and eigenvector 1, and the second eigenvalue 1 2 (a intra − a inter ) and eigenvector σ such that σ i = 1 if i is in community 1 and σ i = −1 otherwise. More generally, the second eigenvalue is of particular theoretical interests because it controls at the first order how fast heat diffuses through graph, as depicted by the discrete Cheeger inequality ( Lee et al., 2014 ). While one would expect taking the second eigenvector of the adjacency matrix suffices, it often fails in practice (even when it is theoretically possible to recover the clusters given the signal-to-noise ratio). This is especially true for sparse networks, whose average nodal degrees is a constant that does not grow with the network size. This is because the spectrum of the Laplacian or adjacency matrix is blurred by "outlier" eigenvalues in the sparse regime, which is often caused by high de- gree nodes ( Kaufmann et al., 2016 ). Unsurprisingly, powering the Laplacian would be of no avail, because it does not change the eigenvectors or the ordering of eigenvalues. In fact, those outliers can become more salient after powering, thereby weakening the useful spectral signal even further. Besides, pruning the largest degree nodes in the adjacency matrix or normalizing the Laplacian can- not solve the issue. To date, the best results for pruning does not apply down to the theoretical recovery threshold ( Coja-Oghlan, 2010 ;  Mossel et al., 2012 ;  Le et al., 2015 ); either outliers would persist or one could prune too much that the graph is destroyed. As for normalized Laplacian, it may overcorrect the large degree nodes, such that the leading eigenvectors would catch the "tails" of the graph, i.e., components weakly connected to the main graph. See Figure A.3 in the appendix for an experimental illustration. In summary, graph Laplacian may not be the ideal choice due to its limited spatial scope of information fusion, and its undesirable artefacts in the spectral domain.

Section Title: IF NOT LAPLACIAN, THEN WHAT?
  IF NOT LAPLACIAN, THEN WHAT? In searching for alternatives, potential choices are many, so it is necessary to clarify the goals. In view of the aforementioned pitfalls of the graph Laplacian, one would naturally ask the question: Can we find an operator that has wider spatial scope, more robust spectral properties, and is meanwhile interpretable and can increase the expressive power of GCNs? From a perspective of graph data analytics, this question gauges how information is propagated and fused on a graph, and how we should interpret "adjacency" in a much broader sense. An image can Under review as a conference paper at ICLR 2020 be viewed as a regular grid, yet the operation of a CNN filter goes beyond the nearest pixel to a local neighborhood to extract useful features. How to draw an analogy to graphs? From a perspective of robust learning, this question sheds light on the basic observation that real- world graphs are often noisy and even adversarial. The nice spectral properties of a graph topology can be lost with the presence or absence of edges. What are some principled ways to robustify the convolution operator and graph embeddings? In this paper, we propose a graph learning paradigm that aims at achieving this goal, as illustrated in  Figure 1 . The key idea is to generate a sequence of graphs from the given graph that capture a wide range of spectral and spatial behaviors. We propose a new operator based on this derived sequence. Definition 2 (Variable power operator). Consider an unweighted and undirected graph G. Let A [k] denote the k-distance adjacency matrix, i.e., A [k] ij = 1 if and only if the shortest distance (in the original graph) between nodes i and j is k. The variable power operator of order r is defined as: A (r) θ = r k=0 θ k A [k] , (3) where θ := (θ 0 , . . . , θ r ) is a set of parameters. Clearly, A (r) θ is a natural extension of the classical adjacency matrix (i.e., r = 1 and θ 0 = θ 1 = 1). With power order r > 1, one can increase the spatial scope of information fusion on the graph when applying the convolution operation. The parameters θ k also has a natural explanation-the magnitude and the sign of θ k can be viewed as "global influence strength" and "global influence propensity" at distance k, respectively, which also determines the participation factor of each graph in the sequence in the aggregated operator. Furthermore, we provide some theoretical justification of the proposed operator by establishing the following asymptotic property of spectral separation under the important SBM setting, which is, nevertheless, not enjoyed by the classical Laplacian operator or its normalized or powered versions. (All proofs are given in the appendix.) Theorem 3 (Asymptotic spectral separation of variable power operator). Consider a graph G drawn from SBM(n, 2, a intra , a inter ). Assume that the signal-to-noise ratio ξ 2 2 /ξ 1 > 1, where ξ 1 = 1 2 (a intra + a inter ) and ξ 2 = 1 2 (a intra − a inter ) (c.f., ( Decelle et al., 2011 )). Suppose r is on the order of c log(n) for a constant c, such that c log(ξ 1 ) < 1/4. Given nonvanishing θ k for k > r/2, the variable power operator A (r) θ has the following spectral properties: (i) the leading eigenvalue is on the order of Θ ( θ 1 ξ r 1 ), the second leading eigenvalue is on the order of Θ ( θ 1 ξ r 2 ), and the rest are bounded by θ 1 n ξ r/2 1 O(log(n)) for any fixed > 0; and (ii) the two leading eigenvectors are sufficient to recover the two communities asymptotically (i.e., as n goes to infinity). Intuitively, the above theoretical result suggests that the variable power operator is able to "magnify" benign signals from the latent graph structure while "suppressing" noises due to random artefacts. This is expected to improve spectral methods in general, especially when the benign signals tend to be overwhelmed by noises. For the rest of the paper, we will apply this insight to propose a robust graph learning paradigm in Section 2, as well as a new GCN architecture in Section 3. We also provide empirical evidence of the gain from this theory in Section 4 and conclude in Section 5.

Section Title: RELATED WORK
  RELATED WORK

Section Title: Beyond nearest neighbors
  Beyond nearest neighbors Several works have been proposed to address the issue of limited spatial scope by powering the adjacency matrix ( Lee et al., 2018 ;  Wu et al., 2019 ;  Li et al., 2019 ). However, simply powering the adjacency does not extract spectral gap and may even make the eigenspectrum more sensitive to perturbations.  Abu-El-Haija et al. (2018 ; 2019) also introduced weight matrices for neighbors at different distances. But this could substantially increase the risk of overfitting in the low-data regime and make the network vulnerable to adversarial attacks.

Section Title: Robust spectral theory
  Robust spectral theory   Decelle et al. (2011) . Our work is partly motivated by the graph powering approach by  Abbe et al. (2018) , which leveraged the result of  Massoulié (2014) ;  Bordenave et al. (2015)  to prove the spectral gap. The main difference with this line of work is that these operators are studied only for spectral clustering without incorporating nodal features. Our proposed variable power operator can be viewed as a kernel version of the graph powering operator ( Abbe et al., 2018 ), thus substantially increasing the capability of learning complex nodal interactions while maintaining the spectral property.

Section Title: Robust graph neural network
  Robust graph neural network While there is a surge of adversarial attacks on graph neural net- works (GNNs) ( Dai et al., 2018 ;  Zügner & Günnemann, 2019 ;  Bojchevski & Günnemann, 2019 ), very few methods have been proposed for defense ( Sun et al., 2018 ). Existing works employed known techniques from computer vision ( Szegedy et al., 2014 ;  Goodfellow et al., 2015 ;  Szegedy et al., 2016 ), such as adversarial training with "soft labels" ( Chen et al., 2019 ) or outlier detection in the hidden layers ( Zhu et al., 2019 ), but they do not exploit the unique characteristics of graph- structured data. Importantly, our approach simultaneously improves performance in both the benign and adversarial tests, as shown in  Figure 2  (details are presented in Section 4).

Section Title: GRAPH AUGMENTATION: ROBUST TRAINING PARADIGM
  GRAPH AUGMENTATION: ROBUST TRAINING PARADIGM Exploration of the spectrum of spectral and spatial behaviors. Given a graph G, consider a family of its powered graphs, G (2) , . . . , G (r) , where G (k) is obtained by connecting nodes with distance less than or equal to k. This "graph augmentation" procedure is similar to "data augmenta- tion", because instead of limiting the learning on a single graph that is given, we artificially create a series of graphs that are closely related to each other in the spatial and spectral domains. As we increase the power order, the graph also becomes more homogenized (Figure A.1). In partic- ular, it can help near-isolated nodes (i.e., low-degree vertices), since they become connected beyond their nearest neighbors. By comparison, simply raising the adjacency matrix to its r-th power will Under review as a conference paper at ICLR 2020 make them appear even more isolated, because it inadvertently promotes nodes with high degrees or nearby loops much more substantially as a result of feedback. Furthermore, the powered graphs can extract spectral gaps in the original graph despite local irregularities, thus boosting the signal- to-noise ratio in the spectral domain. Transfer of knowledge from the powered graph sequence. Consider a generic learning task on a graph G with data D. The loss function is denoted by (W; G, D) for a particular GCN architecture parametrized by W. For instance, in semi-supervised learning, D consists of features and labels on a small set of nodes (see Table A.1), and is the cross-entropy loss over all labeled examples. Instead of minimizing over (W; G, D) alone, we use all the powered graphs: (W; G, D) + r k=2 α k (W; G (k) , D), (r-GCN) where α k ≥ 0 gauges how much information one desires to transfer from powered graph G (k) to the learning process. By minimizing the (r-GCN) objective, one seeks to optimize the network parameter W on multiple graphs simultaneously, which is beneficial in two ways: (i) in the low-data regime, like semi-supervised learning, it helps to reduce the variance to improve generalization and transferability; (ii) in the adversarial setting, it robustifies the network since it is more likely that the perturbed network is contained in the wider spectrum during training.

Section Title: FROM FIXED TO VARIABLE POWER NETWORK
  FROM FIXED TO VARIABLE POWER NETWORK By using the variable power operator illustrated in  Figure 1 , we substantially increase the search space of graph operators. The proposed operator also leads to a new family of graph algorithms with broader scope of spatial fusion and enhanced spectral robustness. As the power grows, the network eventually becomes dense. To manage this increased complexity and make the network more robust against adversarial attacks in the feature domain, we propose a pruning mechanism.

Section Title: Graph sparsification
  Graph sparsification Given a graph G := (V, E [1] ), consider its powered version G (r) := (V, E (r) ) and a sequence of intermediate graphs G [2] , . . . , G [r] , where G [k] := (V, E [k] ) is constructed by connecting two vertices if and only if the shortest distance is k in G. Clearly, {E [k] } r k=1 forms a partition of E (r) . For each node i ∈ V, denote its r-neighborhood by N r (i) := {j ∈ V | d G (i, j) ≤ r}, which is identical to the set of nodes adjacent to i in G [r] . Next, for each edge within this neighborhood, we associate a value using some suitable distance metric φ to measure "aloofness." For instance, it can be the usual Euclidean distance or correlation distance in the feature space. Based on this formulation, we prune an edge e := (i, j) in E (r) if the value is larger than a threshold τ (i, j), and denote the edge set after pruningĒ (r) . Then, we can construct a new sequence of sparsified graphs, G [k] with edge setsĒ [k] = E [k] ∩Ē (r) and adjacency matrixĀ [k] . Hence, the variable power operator is given byĀ (r) θ = r k=0 θ kĀ [k] . Due to the influence of high-degree nodes in the spectral domain, one can adaptively choose the thresholds τ (i, j) to mitigate their effects. Specifically, we choose τ to be a small number if either i or j are nodes with high degrees, thereby making the sparsification more influential in densely connected neighborhoods than weakly connected parts.

Section Title: Layer-wise update rule
  Layer-wise update rule To demonstrate the effectiveness of the proposed operator, we adopt the vanilla GCN strategy (2). Importantly, we replace the graph convolutional operator A with the variable power operator to obtain the variable power network (VPN): A = D − 1 2 (I +Ā (r) θ )D − 1 2 , (VPN) where D ii = 1 + |{j ∈ V | d G (i, j) = 1}|. The introduction of I is reminiscent of the "renormal- ization trick" ( Kipf & Welling, 2017 ), but it can be also viewed as a regularization strategy in this context, which is well-known to improve the spectral robustness ( Amini et al., 2013 ;  Joseph et al., 2016 ). This construction immediately increases the scope of data fusion by a factor of r. Proposition 4. By choosing A with (VPN) in the layer-wise update rule (2), the output at each node from a L-layer GCN depends on neighbors within L * r hops. Since we proved that the variable power operator has nice spectral separation in Theorem 3, VPN is expected to promote useful spectral signals from the graph topology (similar to the preservation of Under review as a conference paper at ICLR 2020 useful information in images ( Jacobsen et al., 2018 ), our method preserves useful information in the graph topology). This claim is substantiated with the following proposition. Proposition 5. Given a graph with two hidden communities. Consider a 2-layer GCN architecture with layer-wise update rule (2). Suppose that A has a spectral gap. Further, assume that the leading two eigenvectors are asymptotically aligned with 1 and ν, i.e., the community membership vector, and that both are in the range of feature matrix X. Then, there exists a configuration of W (1) and W (2) such that the GCN outputs can recover the community with high probability.

Section Title: EXPERIMENTS
  EXPERIMENTS The proposed methods are evaluated against several recent models, including vanilla GCN ( Kipf & Welling, 2017 ) and its variant PowerLaplacian where we simply replace the adjacent matrix with its powered version, three baselines using powered Laplacian IGCN ( Li et al., 2019 ), SGC ( Wu et al., 2019 ) and LNet ( Liao et al., 2019 ), the recent method MixHop ( Abu-El-Haija et al., 2019 ) which attempts to increase spatial scope fusion, as well as a state-of-the-art baseline and RGCN ( Zhu et al., 2019 ), which is also aimed at improving the robustness of Vanilla GCN. All baseline methods on based on their public codes.

Section Title: REVISITING THE STOCHASTIC BLOCK MODEL
  REVISITING THE STOCHASTIC BLOCK MODEL

Section Title: SBM dataset
  SBM dataset We generated a set of networks under SBM with 4000 nodes and parameters such that the SNRs range from 0.58 to 0.91. To disentangle the effects from nodal features with that from the spectral signal, we set the nodal features to be one-hot vectors. The label rates are 0.1%, 0.5% and 1%, the validation rate is 1%, and the rest of the nodes are testing points.

Section Title: Performance
  Performance Since the nodal features do not contain any useful information, learning without topology such as multi-layer perceptron (MLP) is only as good as random guessing. The incor- poration of graph topology improves classification performance-the higher the SNR (i.e., ξ 2 2 /ξ 1 , see Theorem 3), the higher the accuracy. Overall, as shown in  Figure 3 , the performance of the pro- posed method (VPN) is superior than other baselines, which either use Laplacian (GCN, Chebyshev, RGCN, LNet) or its powered variants (IGCN, PowerLaplacian). Spectral separation and Fourier modes. From the eigenspectrum of the convolution operators (Figure A.3), we see that the spectral separation property is uniquely possessed by VPN, whose first two leading eigenvectors carry useful information about the underlying communities: without the help of nodal features, the accuracy is 87% even with label rate of 0.5%. Let Φ denote the Fourier modes of the the adjacency matrix or VPN, and X be the nodal features (i.e., identity matrix). We analyze the information from spectral signals (e.g., the k-th and k + 1-th eigenvectors) by estimating the accuracy of an MLP with filtered nodal features, namely Φ :,k:(k+1) Φ :,k:(k+1) X, as shown in  Figure 4 . The accuracy reflects the information content in the frequency components. We see that the two leading eigenvectors of VPN are sufficient to perform classification, whereas those of the adjacency matrix cannot make accurate inferences.

Section Title: SEMI-SUPERVISED NODE CLASSIFICATION
  SEMI-SUPERVISED NODE CLASSIFICATION

Section Title: Experimental setup
  Experimental setup Graph powering order can influence spatial and spectral behaviors. Our theory suggests powering to the order of log(n); in practice, orders of 2 to 4 suffice ( Figure 5 ). Here, we chose the power order to be 4 for r-GCN on Citeseer and Cora, and 3 for Pubmed, and reduced the order by 1 for VPN.

Section Title: Performance
  Performance By replacing Laplacian with VPN, we see an immediate improvement in performance ( Table 1 ). We also see that a succinct parametrization of the global in- fluence relation in VPN is able to increase the expressivity of the network. For instance, the learned θ at distances 2 and 3 for Citeseer are 3.15e-3 and 3.11e-3 with p-value less than 1e-5. This implies that the network tends to put more weights in closer neighbors.

Section Title: DEFENSE AGAINST EVASION ATTACKS
  DEFENSE AGAINST EVASION ATTACKS To evaluate the robustness of the learned network, we considered the setting of evasion attacks, where the model is trained on benign data but tested on adversarial data.

Section Title: Adversarial methods
  Adversarial methods Five strong global attack methods are considered, including DICE ( Zügner & Günnemann, 2019 ), A abr and A DW3 ( Bojchevski & Günnemann, 2019 ), Meta-Train and Meta- Self ( Zügner & Günnemann, 2019 ). We further modulated the severity of attack methods by varying the attack rate, which corresponds to the percentage of edges to be attacked.

Section Title: Robustness evaluation
  Robustness evaluation In general, both r-GCN and VPN are able to improve over baselines for the defense against evasion attacks, e.g.,  Figure 6  for the A DW3 attack (detailed results for other attacks are listed in the Appendix). It can be also observed that the proposed methods are more robust in Citeseer and Cora than Pubmed. In addition to the low label rates, we conjectured that topological Under review as a conference paper at ICLR 2020 attacks are more difficult to defend for networks with prevalent high-degree nodes, because the attacker can bring in more irrelevant vertices by simply adding a link to the high-degree nodes.

Section Title: Informative and robust low-frequency spectral signal
  Informative and robust low-frequency spectral signal It has been observed by  Wu et al. (2019) ;  Maehara (2019)  that GCNs share similar characteristics of low-pass filters, in the sense that nodal features filtered by low-frequency Fourier modes lead to accurate classifiers (e.g., MLP). However, one key question left unanswered is how to obtain the Fourier modes. In their experiments, they derive it from the graph Laplacian. By using VPN to construct the Fourier modes, we show that the information content in the low-frequency domain can be improved. More specifically, we first perform eigendecomposition of the graph convolutional operator (i.e., graph Laplacian or VPN) to obtain the Fourier modes Φ. We then reconstruct the nodal features X using only the k-th and the k + 1-th eigenvectors, i.e., Φ :,k:(k+1) Φ :,k:(k+1) X. We then use the reconstructed features in MLP to perform the classification task in a supervised learning setting. As  Figure 7  shows, features filtered by the leading eigenvectors of VPN lead to higher classification accuracy compared to the classical adjacency matrix. For the adversarial testing, we construct a new basis Φ based on the attacked graph, and then use Φ :,k:(k+1) Φ :,k:(k+1) X as test points for the MLP trained in the clean data setting. As can be seen in  Figure 8 , models trained based on VPN filtered features also have better adversarial robustness in evasion attacks. Since the eigenvalues of the corresponding operator exhibit low-pass filtering characteristics (Figure A.2), the enhanced benign and adversarial accuracy of VPN is attributed to the increased signal-to-noise ratio in the low-frequency domain. This is in alignment with the theoretical proof of spectral gap developed in this study.

Section Title: DISCUSSIONS
  DISCUSSIONS In this paper, we proposed a new graph operator to replace the graph Laplacian in the classic GCN. It is worth mentioning that our approach is very different from the k-th order polynomials of the Lapla- cian developed in ( Defferrard et al., 2016 ) or the high-order graph Laplacian employed in ( Lee et al., 2018 ;  Li et al., 2019 ;  Abu-El-Haija et al., 2019 ;  Wu et al., 2019 ). If the graph Laplacian L is pow- ered to the k-th order, the resulting matrix has the same eigenvectors as L, with only the eigenvalues Under review as a conference paper at ICLR 2020 powered to the k-th order. Thus, the corresponding k-th order polynomial has the same eigenvec- tors, with the polynomial function applied only to the eigenvalues. However, the proposed variable power operator has radically different eigenvectors as the graph Laplacian or its powered variant. This is an important difference because we observed empirically that the leading eigenvectors of a graph Laplacian are extremely sensitive to outliers under the SBM, and they often correspond to either tails or high-degree nodes (please see Figure A.3 for an illustration). However, our result in Theorem 3 suggests that the leading eigenvectors of the proposed operator asymptotically recovers the underlying community under SBM, and enjoys the "spectral gap" property. We also remark that the meaning of robustness in the machine learning literature ("adversarial ro- bustness") and in the spectral theory literature ("spectral robustness") appear to be different. How- ever, in this work, we argue that spectral robustness is closely related to adversarial robustness. This can be explained from a matrix perturbation point of view. Let λ k and φ k be the eigenvalue and the corresponding eigenvector of the adjacency matrix A. Then, the sensitivity of the eigenvector to the perturbation is given by ( Trefethen & Bau III, 1997 ): ∂φ k ∂A (ij) = n =1, =k φ (i) φ k(j) (2 − δ (ij) ) λ k − λ φ , where A (ij) is the (i, j)-th entry of A, φ k(j) is the j-th entry of φ k , δ (ij) is the perturbation of the (i, j)-th entry of A, and n is the dimension of A. It can be seen that the perturbation of the con- volutional operator is controlled by the inverse of the spectral gap. When the spectral gaps between the leading eigenvalues (i.e., λ 1 and λ 2 ) and the rest of the eigenvalues are large, the perturbations of the leading eigenvectors due to the adversarial attack are controlled. Since the eigenvalues of the graph convolutional operator decays very quickly (see Figure A.2 as an illustration), the learned rep- resentation is heavily influenced by the leading eigenvectors. Thus, by controlling the perturbations of the leading eigenvectors, one can expect to control the perturbations of the outputs. Our theoretical result in Theorem 3 was based on the standard SBM, which is a classic model proposed in mathematical sociology and has been adopted to model and analyze real-world social and biological networks ( Funke & Becker, 2019 ). The main advantage of SBM is that it encodes the structural information that nodes belong to the same community tend to be more connected with each other. Nevertheless, we recognize one limitation of our analysis is that SBM might not be a suitable model for some existing or future applications, in which case, models with more flexibility in the degree distributions, such as the degree-corrected SBM proposed by  Karrer & Newman (2011) , labeled SBM by  Heimlicher et al. (2012) , or hiearchical SBM proposed by  Peixoto (2017)  could be more applicable. Also our analysis is limited to two communities, and the extension to more than two communities is a challenging open problem. Nevertheless, through extensive experiments, we observed that our theory has strong implications for real-world graphs. This is evident from the informative and robust low-frequency spectral signal results in  Figures 7  and 8, which show that the proposed graph convolution operator has a clear advantage in the low-frequency regime over the classic graph Laplacian in terms of classification accuracy.

Section Title: CONCLUSION
  CONCLUSION This study goes beyond classical spectral graph theory to defend GCNs against adversarial attacks. We challenge the central building block of existing methods, namely the graph Laplacian, which Under review as a conference paper at ICLR 2020 is not robust to noisy links. For adversarial robustness, spectral separation is a desirable property. We propose a new operator that enjoys this property and can be incorporated in GCNs to improve expressivity and interpretability. Furthermore, by generating a sequence of powered graphs based on the original graph, we can explore a spectrum of spectral and spatial behaviors and encourage trans- ferability across graphs. The proposed methods are shown to improve both benign and adversarial accuracy over various baselines evaluated against a comprehensive set of attack strategies.

```
