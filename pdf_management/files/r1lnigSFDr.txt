Title:
```
None
```
Abstract:
```
Gating mechanisms are widely used in neural network models, where they allow gradients to backpropagate more easily through depth or time. However, their satura- tion property introduces problems of its own. For example, in recurrent models these gates need to have outputs near 1 to propagate information over long time-delays, which requires them to operate in their saturation regime and hinders gradient-based learning of the gate mechanism. We address this problem by deriving two synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyperparameters, and improve learnability of the gates when they are close to saturation. We show how these changes are related to and improve on alternative recently proposed gating mechanisms such as chrono-initialization and Ordered Neurons. Empirically, our simple gating mechanisms robustly improve the performance of recurrent models on a range of applications, including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning, particularly when long-term dependencies are involved.
```

Figures/Tables Captions:
```
Figure 1: LSTM with refine gate The refine gate r t modifies another gate, such as the forget gate f t for recurrent models. It interpolates between upperbound U ft and lowerbound L ft functions of the forget gate. The resulting effective forget gate g t is then used in place of f t in the state update (5). Finally, to simplify comparisons and ensure that we always use the same number of parameters as the standard gates, when using the refine gate we tie the input gate to the effective forget gate, i t = 1−g t . However, we emphasize that these techniques are extremely simple and broad, and can be applied to any gate (or more broadly, any bounded function) to improve initialization distribution and help optimization. For example, our methods can be combined in different ways in recurrent models, e.g. an independent input gate can be modified with its own refine gate. Alternatively, the refine gate can also be initialized uniformly, which we do in our experiments whenever both UGI and refine gates are used.
Figure 2: Refine gate in action: (a) [Solid] A function α(f t ) satisfying natural properties is chosen to define a band within which the forget gate is refined. (b) The forget gate f t (x) is conventionally defined with the sigmoid function (black). The refine gate interpolates around the original gate f t to yield an effective gate g t within the upper and lower curves, g t ∈ f t ±α(f t ). (c) Contours of the effective gate g t as a function of the forget and refine gates f t , r t . High effective activations can be achieved with more modest f t ,r t values. (d) The gradient ∇g t as a function of effective gate activation g t . [Black, blue]:
Figure 3: (Left) Copy task length 500 (Right) Adding task length 2000. Every method besides the LSTM solves the Adding task. The only methods capable of solving copy are OR-,UR-,O-, and C-LSTM models, with all other models aside from U-LSTM stuck at baseline. Refine gates are fastest.
Figure 4: Histograms of forget gate f t activations (averaged over time and batch) before (Top) and after (Bottom) training on Copy (y-axis independently scaled). C-LSTM initializes with extremal activations which barely change during training. Standard LSTM initialization cannot learn large enough f t and makes no progress on the task. U-LSTM makes progress by encouraging a range of forget gate values, but this distribution does not change significantly during training due to saturation. UR-LSTM starts with the same distribution, but is able to learn extremal gate values. Complementary to here when learning large activations is necessary, Appendix E.1 shows a reverse task where the UR-LSTM is able to un-learn from a saturated regime.
Figure 5: Learning curves with deviations on pixel image classification, at the best stable learning rate.
Figure 7: We evaluated the image matching tasks from Hung et al. (2018), which test memorization and credit assignment, using an A3C agent (Mnih et al., 2016) with an LSTM policy core. We observe that general trends from the synthetic tasks (Section (3.1)) transfer to this reinforcement learning setting.
Figure 8: The addition of distractor rewards changes the task and relative performance of different gating mechanisms. For both LSTM and RMA recurrent cores, the UR- gates still perform best.
Table 1: Summary of gating mechanisms considered in this work as applied to the main forget/input gates of recurrent models. To preserve parameters, refine gate methods use a tied input gate, and master gate methods use a downsize factor C > 1. (Left) Existing approaches and our main method. (right) Ablations of our gates with different components.
Table 2: Validation accuracies on pixel image classification. Asterisks denote divergent runs at the learning rate the best validation score was found at.
Table 3: Test acc. on pixel-by-pixel image classification benchmarks. Top: Recurrent baselines and variants. Middle: Non-recurrent sequence models with global receptive field. Bottom: Our methods.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Recurrent neural networks (RNNs) have become a standard machine learning tool for learning from sequential data. However, RNNs are prone to the vanishing gradient problem, which occurs when the gradients of the recurrent weights become vanishingly small as they get backpropagated through time (Hochreiter et al., 2001). A common approach to alleviate the vanishing gradient problem is to use gating mechanisms, leading to models such as the long short term memory (Hochreiter & Schmidhuber, 1997, LSTM) and gated recurrent units (Chung et al., 2014, GRUs). These gated RNNs have been very suc- cessful in several different application areas such as in reinforcement learning (Kapturowski et al., 2018; Espeholt et al., 2018) and natural language processing (Bahdanau et al., 2014; Kočiskỳ et al., 2018). At every time step, gated recurrent models form a weighted combination of the history summarized by the previous state, and (a function of) the incoming inputs, to create the next state. The values of the gates, which are the coefficients of the combination, control the length of temporal dependencies that can be addressed. This weighted update can be seen as an additive or residual connection on the recurrent state, which helps signals propagate through time without vanishing. However, the gates themselves are prone to a saturating property which can also hamper gradient-based learning. This is particularly troublesome for RNNs, where carrying information for very long time delays requires gates to be very close to their saturated states. We formulate and address two particular problems that arise with the standard gating mechanism of re- current models. First, typical initialization of the gates is relatively concentrated. This restricts the range of timescales the model can address, as the timescale of a particular unit is dictated by its gates. Our first proposal, which we call uniform gate initialization (Section 2.2), addresses this by directly initializing the activations of these gates from a distribution that captures a wider spread of dependency lengths. Second, learning when gates are in their saturation regime is difficult because of vanishing gradients through the gates. We derive a modification that uses an auxiliary refine gate to modulate a main gate, which allows it to have a wider range of activations without gradients vanishing as quickly. Combining these two independent modifications yields our main proposal, which we call the UR- gating mechanism. These changes can be applied to any gate (i.e. bounded parametrized function) and have minimal to no overhead in terms of speed, memory, code complexity, and (hyper-)parameters.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 We apply them to the forget gate of recurrent models, and evaluate on many benchmarks including synthetic long-term dependency tasks, sequential pixel-level image classification, language modeling, program execution, and reinforcement learning. Finally, we connect our methods to other proposed gating modifications, introduce a framework that allows each component to be replaced with similar ones, and perform theoretical analysis and extensive ablations of our method. Empirically, the UR- gating mechanism robustly improves on the standard forget and input gates of gated recurrent models. When applied to the LSTM, these simple modifications solve synthetic memory tasks that are pathologically difficult for the standard LSTM, achieve state-of-the-art results on sequential MNIST and CIFAR-10, and show consistent improvements in language modeling on the WikiText-103 dataset (Merity et al., 2016) and reinforcement learning tasks (Hung et al., 2018).

Section Title: GATED RECURRENT NEURAL NETWORKS
  GATED RECURRENT NEURAL NETWORKS Broadly speaking, RNNs are used to sweep over a sequence of input data x t to produce a sequence of recurrent states h t ∈ R d summarizing information seen so far. At a high level, an RNN is just a parametrized function in which each sequential application of the network computes a state update u : (x t ,h t−1 ) → h t . Gating mechanisms were introduced to address the vanishing gradient problem (Bengio et al., 1994; Hochreiter et al., 2001), and have proven crucial to the success of RNNs. This mechanism essentially smooths out the update using the following equation, where the forget gate f t and input gate i t are [0,1] d -valued functions that control how fast information is forgotten or allowed into the memory state. When the gates are tied, i.e. f t +i t = 1 as in GRUs, they behave as a low-pass filter, deciding the time-scale on which the unit will respond (Tallec & Ollivier, 2018). For example, large forget gate activations close to f t = 1 are necessary for recurrent models to address long-term dependencies. We will introduce our improvements to the gating mechanism primarily in the context of the LSTM, which is the most popular recurrent model. However, these techniques can be used in any model that makes similar use of gates. A typical LSTM (equations (2)-(7)) is an RNN whose state is rep- resented by a tuple (h t ,c t ) consisting of a "hidden" state and "cell" state. The basic gate equation (1) is used to create the next cell state c t (5). Note that the gate and update activations are a function of the previous hidden state h t−1 instead of c t−1 . Here, L stands for a parameterized linear function of its inputs with bias b , e.g. L f (x t ,h t−1 ) = W f x x t +W f h h t−1 +b f , (8) and σ(·) refers to the standard sigmoid activation function which we will assume is used for defining [0,1]-valued activations in the rest of this paper. The gates of the LSTM were initially motivated as a binary mechanism, switching on or off to allow information and gradients to pass through. However, in reality this fails to happen due to a combination of initialization and saturation. This can be problematic, such as when very long dependencies are present.

Section Title: THE UR-LSTM
  THE UR-LSTM We present two solutions which work in tandem to address the previously described issues. The first ensures a diverse range of gate values at the start of training by sampling the gate's biases so that the activations will be approximately uniformly distributed at initialization. We call this Uniform Gate Initialization (UGI). The second allows better gradient flow by reparameterizing the gate using an auxiliary "refine" gate. As our main application is for recurrent models, we present the full UR-LSTM model in equations (9)-(15). However, we note that Under review as a conference paper at ICLR 2020 these methods can be used to modify any gate (or more generally, bounded function) in any model. In this context the UR-LSTM is simply defined by applying UGI and a refine gate r on the original forget gate f to create an effective forget gate g (equation (12)). This effective gate is then used in the cell state update (13). Empirically, these small modifications to an LSTM are enough to allow it to achieve nearly binary activations and solve difficult memory problems ( Figure 4 ). In the rest of Section 2, we provide theoretical justifications for UGI and refine gates.

Section Title: UNIFORM GATE INITIALIZATION
  UNIFORM GATE INITIALIZATION Standard initialization schemes for the gates can prevent the learning of long-term temporal correla- tions (Tallec & Ollivier, 2018). For example, supposing that a unit in the cell state has constant forget gate value f t , then the contribution of an input x t in k time steps will decay by (f t ) k . This gives the unit an effective decay period or characteristic timescale of O( 1 1−ft ). 2 Standard initialization of linear layers L sets the bias term to 0, which causes the forget gate values (2) to concentrate around 1/2. A common trick of setting the forget gate bias to b f = 1.0 (Jozefowicz et al., 2015) does increase the value of the decay period to 1 1−σ(1.0) ≈ 3.7. However, this is still relatively small, and moreover fixed, hindering the model from easily learning dependencies at varying timescales. We instead propose to directly control the distribution of forget gates, and hence the corresponding distribution of decay periods. In particular, we propose to simply initialize the value of the forget gate activations f t according to a uniform distribution U(0,1), as described in Section 2.1. An important difference between UGI and standard or other (e.g. Tallec & Ollivier, 2018) initializations is that negative forget biases are allowed. The effect of UGI is that all timescales are covered, from units with very high forget activations remembering information (nearly) indefinitely, to those with low activations focusing solely on the incoming input. Additionally, it introduces no additional parameters; it even can have less hyperparameters than the standard gate initialization, which sometimes tunes the forget bias b f . Appendix B.2 and B.3 further discuss the theoretical effects of UGI on timescales.

Section Title: THE REFINE GATE
  THE REFINE GATE Given a gate f = σ(L f (x)) ∈ [0,1], the refine gate is an independent gate r = σ(L r (x)), and modulates f to produce a value g ∈ [0,1] that will be used in place of f downstream. It is motivated by considering how to modify the output of a gate f in a way that promotes gradient-based learning, derived below.

Section Title: An additive modification
  An additive modification The root of the saturation problem is that the gradient ∇f of a gate, which can be written solely as a function of the activation value as f (1−f ), decays rapidly as f approaches 0 or 1. Thus when the activation f is past a certain upper or lower threshold, learning effectively stops. This problem cannot be fully addressed only by modifying the input to the sigmoid, as in UGI and other techniques, as the gradient will still vanish by backpropagating through the activation function. Therefore to better control activations near the saturating regime, instead of changing the input to the sigmoid in f = σ(L(x)), we consider modifying the output. In particular, we consider adjusting f with an input-dependent update φ(f,x) for some function φ, to create an effective gate g = f +φ(f,x) that will be used in place of f downstream such as in the main state update (1). This sort of additive ("residual") connection is a common technique to increase gradient flow, and indeed was the motivation of the LSTM additive gated update (1) itself (Hochreiter & Schmidhuber, 1997).

Section Title: Choosing the adjustment function
  Choosing the adjustment function Although many choices seem plausible for selecting the additive update φ, we reason backwards from necessary properties of the effective activation g to deduce a principled function φ. The refine gate will appear as a result. First, note that f t might need to be increased or decreased, regardless of what its value is. For example, given a large activation f t near saturation, it may need to be even higher to address long-term dependencies in recurrent models; alternatively, if it is too high by initialization or needs to unlearn previous behavior, it may need to decrease. Therefore, the additive update to f should create an effective activation g t in the range f t ±α for some α. Note that the allowed adjustment range α = α(f t ) needs to be a function of f in order to keep g ∈ [0,1].

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 In particular, the additive adjustment range α(f ) should satisfy the following natural properties: Symmetry: Since 0 and 1 are completely symmetrical in the gating framework, α(f ) = α(1−f ). Differentiability: α(f ) will be used in backpropagation, requiring α ∈ C 1 (R). Figure 2a illustrates the general appearance of α(f ) based on these properties. In particular, Validity implies that that its derivative satisfies α (0) ≤ 1 and α (1) ≥ −1, Symmetry implies α (f ) = −α (1−f ), and Differentiability implies α is continuous. The simplest such function satisfying these is the linear α (f ) = 1−2f , yielding α(f ) = f −f 2 = f (1−f ). Given such a α(f ), recall that the goal is to produce an effective activation g = f +φ(f,x) such that g ∈ f ±α(f ) (Figure 2b). Our final observation is that the simplest such function φ satisfying this is φ(f,x) = α(f )ψ(f,x) for some ψ(·) ∈ [−1,1]. Using the standard method for defining [−1,1]-valued functions via a tanh non-linearity leads to φ(f,x) = α(f )(2r−1) for another gate r = σ(L(x)). The full update is given in Equation (16), Equation (16) has the elegant interpretation that the gate r linearly interpolates between the lower band f −α(f ) = f 2 and the symmetric upper band f +α(f ) = 1−(1−f ) 2 (Figure 2b). In other words, the original gate f is the coarse-grained determinant of the effective gate g, while the gate r "refines" it. This allows the effective gate g to reach much higher and lower activations than the constituent gates f and r (Figure 2c), bypassing the saturating gradient problem. For example, this allows the effective forget gate to reach g = 0.99 when the forget gate is only f = 0.9.

Section Title: REFINING RECURRENT MODELS
  REFINING RECURRENT MODELS Formally, the full mechanism of the refine gate as applied to gated recurrent models is defined in equations (11)-(13). Note that it is an isolated change where the forget gate (10) is modified before applying the the standard update (1).  Figure 1  illustrates the refine gate in an LSTM cell.  Figure 2  illustrates how the refine gate r t is defined and how it changes the effective gate f t to produce an effective gate g t .

Section Title: RELATED GATING MECHANISMS
  RELATED GATING MECHANISMS We highlight a few recent works that also propose small gate changes to address problems of long-term or variable-length dependencies. Like ours, they can be applied to any gated update equation. Lower and upper bounds on the ratio of the gradient when using a refine gate vs. without. Tallec & Ollivier (2018) suggest an initialization strategy to capture long-term dependencies on the order of T max , by sampling the gate biases from b f ∼ log U(1,T max −1). Although similar to UGI in definition, chrono initialization (CI) has key differences in the timescales captured, for example by using an explicit timescale parameter and having no negative biases. Due to its relation to UGI, we provide a more detailed comparison in Appendix B.3. As mentioned in Section 2.3, techniques such as these that only modify the input to a sigmoid gate do not fully address the saturation problem. The Ordered Neuron LSTM introduced by Shen et al. (2018) aims to induce an ordering over the units in the hidden states such that "higher-level" neurons retain information for longer and capture higher-level information. We highlight this work due to its recent success in NLP, and also because its novelties can be factored into introducing two mechanisms which only affect the forget and input gates, namely (i) the cumax := cumsum•softmax activation function which creates a monotonically increasing vector in [0,1], and (ii) a pair of "master gates" which are ordered by cumax and fine-tuned with another pair of gates. In fact, we observe that these are related to our techniques in that one controls the distribution of a gate activation, and the other is an auxiliary gate with modulating behavior. Despite its important novelties, we find that the ON-LSTM has drawbacks including speed/stability issues and theoretical flaws in the scaling of its gates. We provide the formal definition and detailed analysis of the ON-LSTM in Appendix B.4. In particular we flesh out a deeper relation between the master and refine gates and show how they can be interchanged for each other. We include a more thorough overview of other related works on RNNs in Appendix B.1. These methods are largely orthogonal to the isolated gate changes considered here and are not analyzed. We note that an important drawback common to all other approaches is the introduction of substantial hyperparameters in the form of constants, training protocol, and significant architectural changes. For example, even for chrono initialization, one of the less intrusive proposals, we experimentally find it to be particularly sensitive to the hyperparameter T max (Section 3).

Section Title: GATE ABLATIONS
  GATE ABLATIONS Our insights about previous work with related gate components allow us to perform extensive ablations of our contributions. We observe two independent axes of variation, namely, activation function/ini- tialization (cumax, constant bias sigmoid, CI, UGI) and auxiliary modulating gates (master, refine), where different components can be replaced with each other. Therefore we propose several other gate combinations to isolate the effects of different gating mechanisms. We summarize a few ablations here; precise details are given in Appendix B.5. O-: Ordered gates. A natural simplification of the main idea of ON-LSTM, while keeping the hierarchical bias on the forget activations, is to simply drop the auxiliary master gates and define f t ,i t (2)-(3) using the cumax activation function. UM-: UGI master gates. This variant of the ON-LSTM's gates ablates the cumax operation on the master gates, Under review as a conference paper at ICLR 2020 replacing it with a sigmoid activation and UGI which maintains the same initial distribution on the activation values. OR-: Refine instead of master. A final variant in between the UR- gates and the ON-LSTM's gates combines cumax with refine gates. In this formulation, as in UR- gates, the refine gate modifies the forget gate and the input gate is tied to the effective forget gate. The forget gate is ordered using cumax.  Table 1  summarizes the gating modifications we consider and their naming conventions. Note that we also denote the ON-LSTM method as "OM-LSTM" (M for master) for mnemonic ease. Finally, we remark that all methods here are controlled with the same number of parameters as the standard LSTM, aside from the OM-LSTM and UM-LSTM which use an additional 1 2C -fraction parameters where C is the downsize factor on the master gates (Appendix B.4).

Section Title: EXPERIMENTS
  EXPERIMENTS We first perform full ablations of the gating variants (Section 2.6) on benchmark synthetic memorization and pixel-by-pixel image classification tasks. We then evaluate our main method on important applications for recurrent models including language modeling and reinforcement learning, comparing against baseline methods where appropriate. The vanilla LSTM uses forget bias 1.0 (Section 2.2). When chrono-initialization is used and not explicitly tuned, we set T max to be proportional to the hidden size. This heuristic uses the intuition that if dependencies of length T exist, then so should dependencies of all lengths ≤ T . Moreover, the amount of information that can be remembered is proportional to the number of hidden units. All of our benchmarks have prior work with recurrent baselines, from which we used the same models, protocol, and hyperparameters whenever possible, changing only the gating mechanism. Since our simple gate changes are compatible with other recurrent cores, we evaluate them in tandem with recurrent models such as the GRU, Reconstructive Memory Agent (RMA; Hung et al., 2018), and Relational Memory Core (RMC; Santoro et al., 2018) whenever they were used on these tasks. Full protocols and details for all experiments are given in Appendix D.

Section Title: SYNTHETIC TASKS
  SYNTHETIC TASKS Our first set of experiments is on synthetic memory tasks (Hochreiter & Schmidhuber, 1997; Arjovsky et al., 2016) that are known to be hard for standard LSTMs to solve.

Section Title: Copy task
  Copy task The input is a sequence of N + 20 digits where the first 10 tokens (a 0 ,a 1 ,...,a 9 ) are randomly chosen from {1,...,8}, the middle N tokens are set to 0, and the last ten tokens are 9. The goal of the recurrent model is to output (a 0 ,...,a 9 ) in order on the last 10 time steps, whenever the cue token 9 is presented. We trained our models using cross-entropy with baseline loss log(8) (Appendix D.1).

Section Title: Adding task
  Adding task The input consists of two sequences: 1. N numbers (a 0 ,...,a N −1 ) sampled independently from U[0,1] 2. an index i 0 ∈ [0,N/2) and i 1 ∈ [N/2,N ), together encoded as a two-hot sequence. The target output is a i0 +a i1 and models are evaluated by the mean squared error with baseline loss 1/6.  Figure 3  shows the loss of various methods on the Copy and Adding tasks. The only gate combinations capable of solving Copy completely are OR-, UR-, O-, and C-LSTM. This confirms the mechanism of their gates: these are the only methods capable of producing high enough forget gate values either through the cumax non-linearity, the refine gate, or extremely high forget biases. The U-LSTM is the only other method able to make progress, but converges slower as it suffers from gate saturation Under review as a conference paper at ICLR 2020 without the refine gate. The vanilla LSTM makes no progress. The OM-LSTM and UM-LSTM also get stuck at the baseline loss, despite the OM-LSTM's cumax activation, which we hypothesize is due to the suboptimal magnitudes of the gates at initialization (Appendix B.4). On the Adding task, every method besides the basic LSTM is able to eventually solve it, with all refine gate variants fastest.  Figure 4  shows the distributions of forget gate activations of sigmoid-activation methods, before and after training on the Copy task. It shows that activations near 1.0 are important for a model's ability to make progress or solve this task, and that adding the refine gate makes this significantly easier.

Section Title: PIXEL-BY-PIXEL IMAGE CLASSIFICATION
  PIXEL-BY-PIXEL IMAGE CLASSIFICATION These tasks involve feeding a recurrent model the pixels of an image in a scanline order before producing a classification label. We test on the sequential MNIST (sMNIST), permuted MNIST (pMNIST) (Le et al., 2015), and sequential CIFAR-10 (sCIFAR) tasks. Each LSTM method was ran with a learning rate sweep with 3 seeds each. The best validation score found over any run is reported in the first two rows of  Table 2 . 3 We find in general that all methods are able to improve over the vanilla LSTM. However, the differences become even more pronounced when stability is considered. Although  Table 2  reports the best validation accuracies found on any run, we found that many methods were quite unstable. Asterisks are marked next to a score denoting how many of the 3 seeds diverged, for the learning rate that score was found at. Conversely,  Figure 5  shows the accuracy curves of each method at their best stable learning rate. The basic LSTM is noticeably worse than all of the others. This suggests that any of the gate modifications, whether better initialization, cumax non-linearity, or master or refine gates, are better than standard gates especially when long-term dependencies are present. Additionally, the uniform gate initialization methods are generally better than the ordered and chrono initialization, and the refine gate performs better than the master gate. We additionally consider applying other techniques developed for recurrent models that are independent of the gating mechanism.  Table 2  also reports scores when the same gating mechanisms are applied to the GRU model instead of the LSTM, where similar trends hold across the gating variants. In particular, UR-GRU is the only method that is able to stably attain good performance. As another example, the addition of a generic regularization technique-we chose Zoneout (Krueger et al., 2016) with default hyperparameters (z c = 0.5, z h = 0.05)-continued improving the UR-LSTM/GRU, outperforming even non-recurrent models on sequential MNIST and CIFAR-10.  Table 3  compares the test accuracy of our main model against other models. From Sections 3.1 and 3.2, we draw a few conclusions about the comparative performance of different gate modifications. First, the refine gate is consistently better than comparable master gates. CI solves the synthetic memory tasks but is worse than any other variant outside of those. We find ordered (cumax) gates to be effective, but speed issues prevent us from using them in more complicated tasks. UR- gates are consistently among the best performing and most stable.

Section Title: LANGUAGE MODELING
  LANGUAGE MODELING We consider word-level language modeling on the WikiText-103 dataset, where (i) the dependency lengths are much shorter than in the synthetic tasks, (ii) language has an implicit hierarchical structure and timescales of varying lengths. We evaluate our gate modifications against the exact hyperparameters of a SOTA LSTM-based baseline (Rae et al., 2018) without additional tuning (Appendix D). Addi- tionally, we compare against ON-LSTM, which was designed for this domain (Shen et al., 2018), and chrono initialization, which addresses dependencies of a particular timescale as opposed to timescale- agnostic UGI methods. In addition to our default hyperparameter-free initialization, we tested models with the chrono hyperparameter T max manually set to 8 and 11, values previously used for language modeling to mimic fixed biases of about 1.0 and 2.0 respectively (Tallec & Ollivier, 2018). Table 6a shows Validation and Test set perplexities for various models. We find that the OM-LSTM, U-LSTM, and UR-LSTM improve over the standard LSTM with no additional tuning. However, although the OM-LSTM was designed to capture the hierarchical nature of language with the cumax activation, it does not perform better than the U-LSTM and UR-LSTM. The chrono initialization with our default initialization strategy is far too large. While manually tweaking the T max hyperparameter helps, it is still far from any UGI-based methods. We attribute these observations to the nature of language having dependencies on multiple widely-varying timescales, and that UGI is enough to capture these without resorting to strictly enforced hierarchies such as in OM-LSTM.

Section Title: REINFORCEMENT LEARNING
  REINFORCEMENT LEARNING In many partially observable reinforcement learning (RL) tasks, the agent can observe only part of the environment at a time and thus requires a memory model to summarize what it has seen previously. However, designing memory architectures for reinforcement learning problems has been a challenging task (Oh et al., 2016; Wayne et al., 2018). These are usually based on an LSTM core to summarize what an agent has seen into a state. We investigated if changing the gates of these recurrent cores can improve the performance of RL agents, especially on difficult tasks involving memory and long-term credit assignment. We chose the Passive and Active Image Match tasks from Hung et al. (2018) using A3C agents (Mnih et al., 2016). In these tasks, agents are either initially shown a colored indicator (Passive) or must search for it (Active), before being teleported to a room in which they must press a switch with matching color to receive reward. In between these two phases is an intermediate phase where they can acquire distractor rewards, but the true objective reported is the final reward in the last phase. Episodes last 450−600 steps, so these tasks require memorization and credit assignment across long sequences. Hung et al. (2018) evaluated agents with different recurrent cores: the basic LSTM, the DNC (an LSTM with memory), and the RMA (which also uses an LSTM core). We modified each of these with our gates.  Figure 7  shows the results of different models on the Passive Matching and Active Matching tasks without distractors. These are the most similar to the synthetic tasks (Sec. 3.1), and we found that those trends largely transferred to the RL setting even with several additional confounders present such as agents learning via RL algorithms, being required to learn relevant features from pixels rather than being given the relevant tokens, and being required to explore in the Active Match case. We found that the UR- gates substantially improved the performance of the basic LSTM core on both Passive Match and Active Match tasks, with or without distractor rewards. On the difficult Active Match task, it was the only method to achieve better than random behavior.  Figure 8  shows performance of LSTM and RMA cores on the harder Active Match task with distractors. Here the UR- gates again learn the fastest and reach the highest reward. In particular, although the RMA is a memory architecture with an explicit memory bank designed for long-term credit assignment, its performance was also improved.

Section Title: ADDITIONAL RESULTS AND EXPERIMENTAL CONCLUSIONS
  ADDITIONAL RESULTS AND EXPERIMENTAL CONCLUSIONS Appendix (E.1) shows an additional synthetic experiment investigating the effect of refine gates on saturation. Appendix (E.3) has results on a program execution task, which is interesting for having explicit long and variable-length dependencies and hierarchical structure. It additionally shows another very different gated recurrent model where the UR- gates provide consistent improvement. Finally, we would like to comment on the longevity of the LSTM, which for example was frequently found to outperform newer competitors when tuned better (Melis et al., 2017). Although many improvements have been suggested over the years, none have been proven to be as robust as the LSTM across an enormously diverse range of sequence modeling tasks. By experimentally starting from well-tuned LSTM baselines, we believe our simple isolated gate modifications to actually be robust improvements. In Appendix B.3 and B.4, we offer a few conclusions for the practitioner about the other gate components considered based on our experimental experience.

Section Title: DISCUSSION
  DISCUSSION In this work, we introduce, analyze, and evaluate several modifications to the ubiquitous gating mechanism that appears in recurrent neural networks. We describe theoretically-justified methods that improve on the standard gating method by alleviating problems with initialization and optimization. The mechanisms considered include changes on independent axes, namely initialization method and auxiliary gates, and we perform extensive ablations on our improvements with previously considered modifications. Our main gate model robustly improves on standard gates across many different tasks and recurrent cores, while requiring less tuning Finally, we emphasize that these improvements are completely independent of the large body of research on neural network architectures that use gates, and hope that these insights can be applied to improve machine learning models at large.

```
