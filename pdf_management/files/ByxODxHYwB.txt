Title:
```
Under review as a conference paper at ICLR 2020 MULTI-SOURCE MULTI-VIEW TRANSFER LEARNING IN NEURAL TOPIC MODELING WITH PRETRAINED TOPIC AND WORD EMBEDDINGS
```
Abstract:
```
Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic model- ing to address data sparsity problem in short text or small collection of documents. However, no prior work has employed (pretrained latent) topics in transfer learn- ing paradigm. In this paper, we propose a framework to perform transfer learning in neural topic modeling using (1) pretrained (latent) topics obtained from a large source corpus, and (2) pretrained word and topic embeddings jointly (i.e., multi- view) in order to improve topic quality, better deal with polysemy and data sparsity issues in a target corpus. In doing so, we first accumulate topics and word repre- sentations from one or many source corpora to build respective pools of pretrained topic (i.e., TopicPool) and word embeddings (i.e., WordPool). Then, we identify one or multiple relevant source domain(s) and take advantage of corresponding topics and word features via the respective pools to guide meaningful learning in the sparse target domain. We quantify the quality of topic and document rep- resentations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. We have demonstrated the state-of- the-art results on topic modeling with the proposed transfer learning approaches.
```

Figures/Tables Captions:
```
Figure 1: (Left) DocNADE (LVT+MST): Multi-source transfer learning in TM by introducing pre- trained word embeddings from a WordPool at each autoregressive step i. Double circle → multino- mial (softmax) unit. (Right) Multi-source transfer learning in TM by introducing pretrained (latent) topic embeddings from a TopicPool, illustrating topic alignments between source and target cor- pora in GVT+MST configuration. Each outgoing row from Z k signify a topic embedding of the corresponding source corpus, DC k . Here, TM refers to a DocNADE topic model.
Figure 2: (a, b, c, d) Retrieval performance (precision) on four datasets. (e) Precision at recall fraction 0.02, each for a fraction (20%, 40%, 60%, 80%, 100%) of the training set of TMNtitle. (f) Zero-shot and data-augmentation (DA) experiments for topic coherence on TMNtitle and Ohsumed.
Table 1: Description of the notations used in this work
Table 2: Data statistics: Short/long texts and/or small/large corpora in target and source domains. Symbols- K: vocabulary size, L: average text length (#words), C: number of classes and k: thousand. For short-text, L<15. S 3 is also used in target domain. '-': unlabeled data.
Table 3: Domain overlap in source- target corpora.
Table 4: Baselines (related works) vs this work. Here, NTM and AuR refer to neural network-based TM and autoregressive assumption, respectively. DocNADEe → DocNADE+Glove embeddings.
Table 5: State-of-the-art comparisons with TMs: Perplexity (PPL), topic coherence (COH) and precision (IR) at retrieval fraction 0.02. Scores are reported on each of the target, given KBs from one or several sources. Please read column-wise. Bold: best in column. Gain%: Bold vs DocNADE.
Table 6: State-of-the-art comparisons with TMs using word embeddings: PPL, COH and IR at re- trieval fraction 0.02. Scores are reported on each of the target, given KBs. Here, MVT: LVT+GVT (Table 5), DocNADEe: DocNADE+Glove and Gain%: Bold vs DocNADEe. For all the configura- tions, we apply a projection on word embeddings concatenated from several sources.
Table 7: PPL, COH, IR at retrieval fraction 0.02. BioEmb and BioFastText: 200-dimensional word vectors from large biomedical corpus (Moen & Ananiadou, 2013). + BioEmb: MVT+BioEmb.
Table 8: Source S and target T topics before (-) and after (+) topic transfer(s) (GVT) from one or more sources. DNE: DocNADE
Table 9: Five nearest neighbors of the word chip in source and target semantic spaces before (-) and after (+) knowledge transfer (MST+GVT)
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Probabilistic topic models, such as LDA ( Blei et al., 2003 ), Replicated Softmax (RSM) ( Salakhut- dinov & Hinton, 2009 ) and Document Neural Autoregressive Distribution Estimator (DocNADE) ( Larochelle & Lauly, 2012 ) are often used to extract topics from text collections and learn latent document representations to perform natural language processing tasks, such as information re- trieval (IR). Though they have been shown to be powerful in modeling large text corpora, the topic modeling (TM) still remains challenging especially in the sparse-data setting, especially for the cases where word co-occurrence data is insufficient e.g., on short text or a corpus of few docu- ments. To this end, several works ( Das et al., 2015 ;  Nguyen et al., 2015 ;  Gupta et al., 2019 ) have introduced external knowledge in traditional topic models via word embeddings  Pennington et al. (2014) . However, no prior work in topic modeling has employed topical embeddings (obtained from large document collection(s)), complementary to word embeddings.

Section Title: Local vs Global Views
  Local vs Global Views Though word embeddings ( Pennington et al., 2014 ) and topics are com- plementary in how they represent the meaning, they are distinctive in how they learn from word occurrences observed in text corpora. Word embeddings have local context (view) in the sense that they are learned based on local collocation pattern in a text corpus, where the representation of each word either depends on a local context window ( Mikolov et al., 2013 ) or is a function of its sen- tence(s) ( Peters et al., 2018 ). Consequently, the word occurrences are modeled in a fine-granularity. On other hand, a topic ( Blei et al., 2003 ;  Gupta et al., 2019 ) has a global word context (view): TM in- fers topic distributions across documents in the corpus and assigns a topic to each word occurrence, where the assignment is equally dependent on all other words appearing in the same document. Therefore, it learns from word occurrences across documents and encodes a coarse-granularity de- scription. Unlike topics, the word embeddings do not capture thematic structures (topical semantics) underlying in the document collection. Motivation (1) Knowledge transfer using pretrained word and topic embeddings: Essentially, the application of TM aims to discover hidden thematic structures (i.e., topics) in text collection; however, it is challenging in data sparsity settings, e.g, in a short and/or small collection. This leads to suboptimal text representations and incoherent topics (e.g., topic Z 4 ). To alleviate the data sparsity issues, recent works ( Das et al., 2015 ;  Nguyen et al., 2015 ;  Gupta et al., 2019 ) have shown that TM can be improved by introducing external knowledge, where they leverage pretrained word embeddings (i.e., local view) only. However, the word embeddings ignore the thematically contextualized structures (i.e., document-level semantics), and can not deal with ambiguity. Given that the word and topic representations encode complementary information, no prior work has explored transfer learning in TM using pretrained topics obtained from a large corpus. Motivation (2) Knowledge transfer from multiple sources of word and topic embeddings: Knowledge transfer via word embeddings is vulnerable to negative transfer ( Cao et al., 2010 ) on the target domain when domains are shifted and not handled properly. For instance, consider a short-text document v: [apple gained its US market shares] in the target domain T . Here, the word apple refers to a company, and hence the word vector of apple (about fruit) is an irrelevant source of prior knowledge for both v and the topic Z 4 . In contrast, one can better model v and amend the noisy Z 4 for coherence, given the meaningful word and topic embeddings. Often, there are several topic-word associations in different domains, e.g., in topics Z 1 -Z 3 . Given a noisy topic Z 4 in T and meaningful topics Z 1 -Z 3 of S 1 -S 3 , we identify multiple relevant (source) domains and advantageously transfer their word and topic embeddings in order to facilitate mean- ingful and positive transfer learning in the sparse corpus, T .

Section Title: Contribution
  Contribution beddings instead of using word embeddings exclusively, and (b) Multi-view Transfer: Pretrained word and topic embeddings jointly obtained from a large source corpus in order to deal with poly- semy and alleviate data sparsity issues in a small target corpus.

Section Title: Contribution (2) Multi-source Transfer
  Contribution (2) Multi-source Transfer Moreover, we first learn word and topic representations on multiple source domains to build WordPool and TopicPool, respectively and then perform multi-view and multi-source transfer learning within neural topic modeling by jointly using the complementary representations. In doing so, we guide the (unsupervised) generative process of learning hidden topics of the target domain by embeddings in WordPool and TopicPool such that the hidden topics become more meaningful and representative in explaining the target corpus. We evaluate the effectiveness of our transfer learning approaches in neural topic modeling using 7 (5 low-resource and 2 high-resource) target and 5 (high-resource) source corpora from news and med- ical domains, consisting of short-text, long-text, small and large document collections. Particularly, we quantify the quality of text representations via generalization (perplexity), interpretability (topic coherence) and text retrieval. The code is provided with the supplementary.

Section Title: KNOWLEDGE TRANSFER IN NEURAL TOPIC MODELING
  KNOWLEDGE TRANSFER IN NEURAL TOPIC MODELING Consider a sparse target domain T and a set of |S| source domains S, we first prepare two knowledge bases (KBs) of representations (or embeddings) from each of the sources: (1) WordPool: Pretrained word embeddings matrices {E 1 , ..., E |S| }, where E k ∈ R E×K , and (2) TopicPool: Pretrained latent topic embeddings {Z 1 , ..., Z |S| }, where Z k ∈ R H×K encodes a distribution over a vocabulary of K words. E and H are word embedding and latent topic dimensions, respectively. While topic modeling on T , we introduce the two types of knowledge transfers from one or many sources: Local (LVT) and Global (GVT) View Transfer using the two KBs of pretrained word (i.e., WordPool) and topic (i.e., TopicPool) embeddings, respectively. Specially, we employ a neural autoregressive topic model (i.e., DocNADE ( Larochelle & Lauly, 2012 )) to build the WordPool and TopicPool. Notice that a superscript indicates a source. See  Table 1  for the notations used in this work.

Section Title: NEURAL AUTOREGRESSIVE TOPIC MODELS
  NEURAL AUTOREGRESSIVE TOPIC MODELS DocNADE ( Larochelle & Lauly, 2012 ) is an unsupervised neural-network based topic model that is inspired by the benefits of NADE ( Larochelle & Murray, 2011 ) and RSM ( Salakhutdinov & Hinton, 2009 ) architectures. RSM has difficulties due to intractability leading to approximate gradients of the negative log-likelihood, while NADE does not require such approximations. On other hand, RSM is a generative model of word count, while NADE is limited to binary data. Specifically, Doc- Under review as a conference paper at ICLR 2020 NADE factorizes the joint probability distribution of words in a document as a product of conditional distributions and efficiently models each conditional via a feed-forward neural network.

Section Title: DocNADE Formulation
  DocNADE Formulation for i ∈ {1, ...D}, where v <i is the subvector consisting of all v q such that q < i i.e., v <i ∈ {v 1 , ..., v i−1 }, g(·) is a non-linear activation function, W ∈ R H×K and U ∈ R K×H are weight matrices, c ∈ R H and b ∈ R K are bias parameter vectors. H is the number of hidden units (topics).  Figure 1  (left) (without WordPool) provides an illustration of the ith autoregressive step of the Doc- NADE architecture, where the parameter W is shared in the feed-forward networks and h i encodes topic-proportion embedding. Importantly, the topic-word matrix W has a property that the column vector W :,vi corresponds to embedding of the word v i , whereas the row vector W j,: encodes the jth topic. We leverage this property to introduce external knowledge via word and topic embeddings. Additionally, DocNADE has shown to outperform traditional models such as LDA ( Blei et al., 2003 ) and RSM ( Salakhutdinov & Hinton, 2009 ) in terms of both the log-probability on unseen documents and retrieval accuracy. Recently,  Gupta et al. (2019)  has improved topic modeling on short texts by introducing word embeddings ( Pennington et al., 2014 ) in DocNADE architecture. Thus, we adopt DocNADE to perform transfer learning within the neural topic modeling framework. Algorithm 1 (for DocNADE, set LVT and GVT to False) demonstrates the computation of log p(v) and negative log-likelihood L(v) that is minimized using gradient descent. Moreover, computing h i is efficient (linear complexity) due to NADE architecture that leverages the pre-activation a i−1 of (i − 1)th step in computing the pre-activation a i . See  Larochelle & Lauly (2012)  for further details.

Section Title: MULTI-VIEW (MVT) AND MULTI-SOURCE TRANSFERS (MST) IN TOPIC MODELING
  MULTI-VIEW (MVT) AND MULTI-SOURCE TRANSFERS (MST) IN TOPIC MODELING Here, we describe a transfer learning framework in topic modeling that jointly exploits the comple- mentary knowledge using the WordPool and TopicPool, the KBs of pretrained word and (latent) topic embeddings, respectively obtained from large document collections (DCs) from several sources. In Under review as a conference paper at ICLR 2020 doing so, we first apply the DocNADE to generate a topic-word matrix for each of the DCs, where its column-vector and row-vector generate E k and Z k , respectively for the kth source. LVT+MST Formulation for Multi-source Word Embedding Transfer: As illustrated in  Figure 1  (left) and Algorithm 1 with LVT=True, we perform transfer learning on a target T using the WordPool of pretrained word embeddings {E 1 , ..., E |S| } from several sources S (i.e., multi-source): Here, k refers to the kth source and λ k is a weight for E k that controls the amount of knowledge transferred in T , based on domain overlap between target and source(s). Recently, DocNADEe ( Gupta et al., 2019 ) has incorporated word embeddings ( Pennington et al., 2014 ) in extending Doc- NADE; however, it is based on a single source.

Section Title: GVT+MST Formulation for Multi-source Topic Embedding Transfer
  GVT+MST Formulation for Multi-source Topic Embedding Transfer Next, we perform knowl- edge transfer exclusively using the TopicPool of pretrained topic embeddings (e.g., Z k ) from one or several sources, S. In doing so, we add a regularization term to the loss function L(v) and require DocNADE to minimize the overall loss in a way that the (latent) topic features in W simultane- ously inherit relevant topical features from each of the source domains S, and generate meaningful representations for the target T . The overall loss L(v) due to GVT+MST in DocNADE is given by: Here, A k ∈R H×H aligns latent topics in the target T and kth source, and γ k governs the degree of imitation of topic features Z k by W in T . Consequently, the generative process of learning meaningful topics in W of T is guided by relevant features in {Z} |S| 1 to address data-sparsity. Algorithm 1 describes the computation of the loss, when GVT = True and LVT = False. Moreover,  Figure 1  (right) illustrates the need for topic alignments between target and source(s). Here, j indicates the topic (i.e., row) index in a topic matrix, e.g., Z k . Observe that the first topic (gray curve), i.e., Z 1 j=1 ∈ Z 1 of the first source aligns with the first row-vector (i.e., topic) of W (of target). However, the other two topics Z 1 j=2 , Z 1 j=3 ∈ Z 1 need alignment with the target topics. MVT+MST Formulation for Multi-source Word and Topic Embeddings Transfer: When LVT and GVT are True (Algorithm 1) for many sources, the two complementary representations are jointly used in transfer learning using WordPool and TopicPool, and therefore, the name multi-view and multi-source transfers.

Section Title: EVALUATION AND ANALYSIS
  EVALUATION AND ANALYSIS Datasets:  Table 2  describes the datasets used in high-resource source and low-and high-resource target domains for our experiments. The target domain T consists of four short-text corpora (20NSshort, TMNtitle, R21578title and Ohsumedtitle), one small corpus (20NSsmall) and Under review as a conference paper at ICLR 2020 two large corpora (TMN and Ohsumed). However in source S, we use five large corpora (20NS, R21578, TMN, AGnews and PubMed) in different label spaces (i.e, domains). Here, the corpora (T 5 , T 6 and S 5 ) belong to medical and others to news. Additionally,  Table 3  suggests domain overlap (in terms of label match) in the target and source corpora, where we define three types of overlap: I (identical) if all labels match, R (related) if some labels match, and D (distant) if a very few or no labels match. Note, our modeling approaches are completely unsupervised and do not use the data labels. See the data labels in appendices.

Section Title: Reproducibility
  Reproducibility For evaluations in the following sections, we follow the experimental setup similar to DocNADE ( Larochelle & Lauly, 2012 ) and DocNADEe ( Gupta et al., 2019 ), where the number of topics (H) is set to 200. While DocNADEe requires the dimension (i.e., E) of word embeddings be the same as the latent topic (i.e., H), we first apply a projection on the concatenation of the pre- trained word embeddings obtained from several sources and then, introduce the prior knowledge in each of the autoregressive step following DocNADEe. We apply it in configurations where Glove and/or FastText (E=300) ( Bojanowski et al., 2017 ) are employed. See appendices for the experimen- tal setup, hyperparameters 1 and optimal values of λ k ∈ [0.1, 0.5, 1.0] and γ k ∈ [0.1, 0.01, 0.001] (determined using development set) in different source-target configurations. (code provided)

Section Title: Baselines
  Baselines As summarized in  Table 4 , we consider several baselines including (1) LDA-based and neural network-based topic models that use the target data, (2) topic models using pretrained word embeddings (i.e., LVT) from  Pennington et al. (2014)  (Glove), (3) unsupervised document represen- tation, where we employ doc2vec ( Le & Mikolov, 2014 ) and EmbSum (to represent a document by summing the embedding vectors of its words using Glove) in order to quantify the quality of doc- ument representations, (4) zero-shot topic modeling, where we use all source corpora and no target corpus, and (5) data-augmentation, where we use all source corpora along with a target corpus for TM on T . Using DocNADE, we first prepare the two KBs: WordPool and TopicPool from each of the source corpora and then use them in knowledge transfer to T .  Tables 5  and 6 show the comparison of our proposed transfer learning approaches (i.e., LVT using WordPool, GVT using TopicPool, MVT and MST) with the baselines TMs that (1) do not, and (2) do employ pretrained word embeddings (e.g., DocNADE and DocNADEe, respectively).

Section Title: GENERALIZATION: PERPLEXITY (PPL)
  GENERALIZATION: PERPLEXITY (PPL) To evaluate generative performance of TM, we estimate the log-probabilities for the test documents and compute the average held-out perplexity per word as, P P L = exp − 1 N N t=1 1 |v t | log p(vt) , where N and |v t | are the number of documents and words in a document v t , respectively.  Tables 5  and 6 quantitatively show PPL scores on the five target corpora (four short-text and one long-text) by the baselines and proposed transfer learning approaches (i.e., GVT, MVT and MST) using one or four sources. In  Table 5  using TMN (as a single source) for LVT, GVT and MVT on TMNtitle, we see improved (reduced) PPL scores: (655 vs 706), (689 vs 706) and (663 vs 706) respectively in comparison to DocNADE. We also observe gains due to MST+LVT, MST+GVT and MST+MVT configurations on TMNtitle. Similarly in MST+LVT for R21578title, we observe a gain of 5.2% (182 vs 192), suggesting that transfer learning using pretrained word and topic em- beddings (jointly) from one or many sources helps due to positive knowledge transfer, and it also verifies domain relatedness (e.g., in TMN-TMNtitle and AGnews-TMN). Similarly,  Table 6  shows gains in PPL (e.g., on TMNtitle, R21578title, etc.) compared to DocNADEe. In  Table 7 , we show PPL scores on two medical target corpora: Ohsumtitle and Ohsumed using two sources: AGnews (news corpus) and PubMed (medical abstracts) to perform cross-domain and in-domain knowledge transfers. We see that using PubMed for LVT on both the target corpora improves generalization. Overall, we report a gain of 17.3% (1268 vs 1534) on Ohsumtitle and 8.55% (1497 vs 1637) on Ohsumtitle, compared to DocNADEe. Additionally, MST+GVT and MST+MVT boost generalization performance compared to DocNADE(e).

Section Title: INTERPRETABILTY: TOPIC COHERENCE (COH)
  INTERPRETABILTY: TOPIC COHERENCE (COH) While PPL is used for model selection, adjusting parameters (e.g. H) and quantitative comparisons,  Chang et al. (2009)  showed in some cases humans preferred TMs (based on the semantic quality of topics) with higher (worse) PPLs. Thus beyond perplexity, we compute topic coherence to estimate the meaningfulness of words in each of the topics captured. In doing so, we choose the coherence measure proposed by  Röder et al. (2015)  that identifies context features for each topic word using a sliding window over the reference corpus. We follow  Gupta et al. (2019)  and compute COH with the top 10 words in each topic. Essentially, the higher scores imply the more coherent topics.  Tables 5  and 6 (under COH column) demonstrate that our proposed approaches (GVT, MVT and MST) of transfer learning in TMs show noticeable gains in COH and thus, improve topic quality. For instance in  Table 5 , when AGnews is used as a single source for 20NSsmall datatset, we observe a gain in COH due to GVT (.563 vs .462) and MVT (.566 vs .462). Additionally, noticeable gains are reported due to MST+LVT (.542 vs .462), MST+GVT (.585 vs .462) and MST+MVT (.637 vs .462), compared to DocNADE. Importantly, we find a trend MVT>GVT>LVT in COH scores for both the single-source and multi-source transfers. Similarly,  Table 6  show noticeable gains (e.g., 39.3%, 9.95%, 7.08%, etc.) in COH due to MST and MVT with Glove and FastText word embeddings. Moreover,  Table 7  shows gains in COH due to GVT on Ohsumedtitle and Ohsumed, using pretrained knowledge from PubMed. Overall, the GVT, MVT and MST boost COH for all the five target corpora compared to the baseline TMs (i.e., DocNADE and DocNADEe). This suggests that there is a need for the two complementary (pretrained word and topics) representations and multi-source transfer learning in order to guide meaningful topic learning in T . The results on both the low- and high-resource targets across domains conclude that the proposed modeling scales.

Section Title: APPLICABILITY: INFORMATION RETRIEVAL (IR)
  APPLICABILITY: INFORMATION RETRIEVAL (IR) For a greater impact of TMs, we further evaluate the quality of document representations and per- form a document retrieval task on the target datasets, using their label information only to compute precision. We follow the experimental setup similar to  Lauly et al. (2017) , where all test documents are treated as queries to retrieve a fraction of the closest documents in the original training set using cosine similarity between their document vectors. To compute retrieval precision for each fraction (e.g., 0.02), we average the number of retrieved training documents with the same label as the query.  Tables 5  and 6 depict precision scores at retrieval fraction 0.02 (similar to  Gupta et al. (2019) ), where the configuration MST+MVT outperforms both the DocNADE and DocNADEe, respectively in retrieval performance on the four target (short-text) datasets. A gain in IR performance is noticeable for highly overlapping domains, e.g., TMN-TMNtitle (.555 vs .521 in  Table 5  and .576 vs .540 in  Table 6 ) than the related, e.g., AGnews-TMNtitle (.534 vs .521 in  Table 5  and .565 vs .540 in  Table 6 ). We observe large gains in precision at retrieval fraction 0.02: (a)  Table 5 : 20.7% (.326 vs .270) on 20NSsmall, 9.21% (.569 vs .521) on TMNtitle and 8.28% (.314 vs .290) on 20NSshort, (b)  Table 6 : 8.84% (.320 vs .294) on 20NSshort and 9.21% (.578 vs .540) on TMNtitle, and (c)  Table 7 : 4.91% (.192 vs .183) on Ohsumed and 4.0% (.182 vs .175) on Ohsumedtitle. Additionally, Figures 2a, 2b, 2c and 2d illustrate precision on 20NSshort, 20NSsmall, TMNtitle and R21578title, respectively, where our approaches (MST+GVT and MST+MVT) consistently outperform the baselines at all fractions. Moreover, we split the training data of TMNtitle into several sets: 20%, 40%, 60%, 80% of the training set and then retrain DocNADE, DocNADEe and DocNADE+MST+MVT. We demonstrate the impact of transfer learning in sparse-data settings using WordPool and TopicPool jointly on IR task. Figure 2e plots precision at retrieval (recall) fraction 0.02 and demonstrates that the proposed modeling consistently outperform DocNADE(e).

Section Title: ZERO-SHOT AND DATA-AUGMENTATION EVALUATIONS
  ZERO-SHOT AND DATA-AUGMENTATION EVALUATIONS Figures 2a, 2b, 2c and 2d show precision in the zero-shot (source-only training) and data- augmentation (source+target training) configurations. Observe that the latter helps in learning Under review as a conference paper at ICLR 2020 meaningful representations and performs better than zero-shot; however, it is outperformed by MST+MVT, suggesting that a naive (data space) augmentation does not add sufficient prior or rel- evant information to the sparse target. Thus, we find that it is beneficial to augment training data in feature space (e.g., LVT, GVT and MVT) especially for unsupervised TMs using WordPool and TopicPool. Beyond IR, we further investigate computing topic coherence (COH) for zero-shot and data-augmentation baselines, where the COH scores (Figure 2f) suggest that MST+MVT outper- forms DocNADEe, zero-shot and data-augmentation.

Section Title: QUALITATIVE ANALYSIS: TOPICS AND NEAREST NEIGHBORS (NN)
  QUALITATIVE ANALYSIS: TOPICS AND NEAREST NEIGHBORS (NN) For topic level inspection, we first extract topics using the rows of W of source and target corpora.  Table 8  shows the topics (top-5 words) from source and target domains. Observe that the target topics become more coherent after transfer learning (i.e., +GVT) from one or more sources. The blue color signifies that a target topic has imitated certain topic words from the source. Observe that we also show topics from source domain(s) that align with the topics from target. For word level inspection, we extract word representations using the columns of W.  Table 9  shows nearest neighbors (NNs) of the word chip in 20NSshort (target) corpus, before and after GVT using three knowledge sources. Observe that the NNs in the target become more meaningful.

Section Title: CONCLUSION
  CONCLUSION Within neural topic modeling, we have introduced transfer learning approaches using complemen- tary representations: pretrained word (local semantics) and topic (global semantics) embeddings exclusively or jointly from one or many sources (i.e., multi-view and multi-source). We have shown that the proposed approaches better deal with data-sparsity issues, especially in a short-text and/or small document collection. We have demonstrated learning meaningful topics and quality document representations on 7 (low- and high-resource) target corpora from news and medical domains.

```
