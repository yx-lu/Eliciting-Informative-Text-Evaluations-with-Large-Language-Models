<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 CRAFTING DATA-FREE UNIVERSAL ADVERSARIES WITH DILATE LOSS</article-title></title-group><abstract><p>We introduce a method to create Universal Adversarial Perturbations (UAP) for a given CNN in a data-free manner. Data-free approaches suite scenarios where the original training data is unavailable for crafting adversaries. We show that the adversary generation with full training data can be approximated to a formulation without data. This is realized through a sequential optimization of the adversarial perturbation with the proposed dilate loss. Dilate loss basically maximizes the Euclidean norm of the output before nonlinearity at any layer. By doing so, the perturbation constrains the ReLU activation function at every layer to act roughly linear for data points and thus eliminate the dependency on data for crafting UAPs. Extensive experiments demonstrate that our method not only has theoretical sup- port, but achieves higher fooling rate than the existing data-free work. Further- more, we evidence improvement in limited data cases as well.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Despite the phenomenal success of deep neural networks in many practical applications, adversarial attacks are being a constant plague. These attacks corrupt the input with a small and usually im- perceptible structured noise causing the model to output incorrect predictions. The sole existence of such a vulnerability not only raises concerns about the security of deep learning models, but also questions the robustness of the learned representations. To make it further worse, it has been shown that a single noise, called universal adversarial perturbation (UAP), can be added to any image and fool the network. UAPs do not require any optimization on the input image at attack time, but the corruption effectively works for most of the images. Interestingly, such perturbations created for one model exhibit transferability of attack and induce high fooling on other models. One drawback of UAPs though, is the requirement of training data for crafting perturbations. This is increasingly infeasible as the datasets are becoming quite large and might not be publicly released due to privacy or copyright reasons. In such cases where the original data is not available, data-free methods are gaining traction. In the data-free setting, the perturbation is created only with the trained neural net- work. Such methods typically rely on the trained weights and the CNN structure to find vulnerable patterns that can maximally disturb the normal propagation of activations across the network. A higher transfer of attack across networks is observed for data-free UAPs as well, raising its practical utility. Moreover, the study of these perturbations might lead to new insights on how deep neural networks actually work.</p><p>In this paper, we propose a new method for crafting data-free UAPs for any given CNN using ReLU nonlinearity. The approach relies on finding the singular vectors of a linearly approximated network (Section 3.1). A loss formulation is devised to enable this approximation under certain conditions. Dilate loss forms the major component of the method, which generates a perturbation that maximizes the Euclidean norm of the activation vector (before the nonlinearity) at a given layer (Section 3.2). We show that the perturbation crafted through dilation has the effect of linearly approximating the ReLU layer responses for any data points. These dilations are done sequentially for all the layers from the input to the last classification stage (Section 3.3). We argue that the sequential dilations results in a perturbation that aligns with the first singular vector of the linearly approximated network. Our approach outperforms the existing data-free method in fooling rates and the evaluation is also done for less data scenarios (Section 4).</p><p>In summary, the work contributes the following: &#8226; A new method that can create universal adversarial perturbation without using data and achieve state-of-the-art data-free fooling rates.</p><p>&#8226; A detailed theoretical analysis which formulates the proposed sequential dilation algorithm by approximating the adversary generation with full training data under certain conditions.</p></sec><sec><title>RELATED WORK</title><p>The vulnerability of deep neural networks to adversarial samples is first shown in <xref ref-type="bibr" rid="b7">Szegedy et al. (2013)</xref>. <xref ref-type="bibr" rid="b7">Following Szegedy et al. (2013)</xref>, several methods (<xref ref-type="bibr" rid="b7">Goodfellow et al., 2014</xref>; <xref ref-type="bibr" rid="b11">Kurakin et al., 2016</xref>; <xref ref-type="bibr" rid="b4">Dong et al., 2018</xref>; Madry et al., 2017; <xref ref-type="bibr" rid="b14">Moosavi-Dezfooli et al., 2016</xref>; <xref ref-type="bibr" rid="b1">Brendel et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Athalye et al., 2018</xref>; Carlini &amp; Wagner, 2017) are being proposed to craft such adversarial samples. One of the simplest method is the Fast Gradient Sign Method (FGSM) formulated in <xref ref-type="bibr" rid="b7">Goodfellow et al. (2014)</xref>. FGSM obtains the perturbation by single step gradient ascent of the loss function with respect to the input image. There are multi step variants to FGSM like iterative FGSM (<xref ref-type="bibr" rid="b11">Kurakin et al., 2016</xref>), Momentum (<xref ref-type="bibr" rid="b4">Dong et al., 2018</xref>), Projected Gradient Descent (PGD) (Madry et al., 2017), Deepfool (<xref ref-type="bibr" rid="b14">Moosavi-Dezfooli et al., 2016</xref>), etc. These attacks are image-specific, where the perturbation is a function of the input and requires a separate optimization for each image. <xref ref-type="bibr" rid="b14">Moosavi-Dezfooli et al. (2017)</xref> introduce the idea of Universal Adversarial Perturbations (UAP), a single perturbation that can fool the model for most of the input images. UAP is obtained by jointly maximizing the training loss for dataset images. There are also generative approaches like NAG (<xref ref-type="bibr" rid="b16">Reddy Mopuri et al., 2018b</xref>), AAA (<xref ref-type="bibr" rid="b16">Reddy Mopuri et al., 2018a</xref>), GAP (<xref ref-type="bibr" rid="b18">Poursaeed et al., 2018</xref>) for crafting universal adversaries. <xref ref-type="bibr" rid="b9">Khrulkov &amp; Oseledets (2018)</xref> propose a method based on singular vectors of Jacobian matrix to create universal adversaries. They show impressive fooling performance with a very small set of training images, but the method is not data-free. Though the study of adversarial attacks started with the classification task, there are several works (<xref ref-type="bibr" rid="b26">Xie et al., 2017</xref>; <xref ref-type="bibr" rid="b13">Metzen et al., 2017</xref>) that extend such attacks to other tasks like segmentation, detection, etc. Further, adversarial examples are shown to generalize to the physical world in <xref ref-type="bibr" rid="b11">Kurakin et al. (2016)</xref>. While most attacks changes each pixel in the image with small imperceptible noise, there are methods (<xref ref-type="bibr" rid="b22">Sharif et al., 2016</xref>; Brown et al., 2017; <xref ref-type="bibr" rid="b17">Papernot et al., 2016</xref>) that perturb limited number of pixels with large noise as these are more practical in nature.</p><p>The attacks discussed so far, in general, rely on maximizing the training loss. In contrast, <xref ref-type="bibr" rid="b16">Mopuri et al. (2018)</xref> devise a generalizable data-free objective for crafting UAPs (GDUAP). GDUAP maxi- mizes the activations at the output of all convolutional layers corrupting the feature representations learned and hence fooling the model. Our method has similarity to GDUAP, but with the crucial difference that the Euclidean norm maximization is performed before the nonlinearity in our case. Further, we maximize the norms of the layers one after the other in a sequential fashion as opposed to a single joint optimization. We show theoretically and experimentally that these changes cause a lot of difference in fooling performance. Moreover, no sound reasoning is available in <xref ref-type="bibr" rid="b16">Mopuri et al. (2018)</xref> to justify the formulation, whereas we provide theoretical explanation for the algorithm.</p></sec><sec><title>CRAFTING A DATA-FREE OBJECTIVE</title><p>Consider a deep neural network with L layers already trained for classification task. We assume the activation function employed in the network to be ReLU (<xref ref-type="bibr" rid="b16">Nair &amp; Hinton, 2010</xref>), defined as &#963; R (x) = x if x &gt; 0 0 otherwise, which basically zeros out all negative elements and retains the positive ones when applied on a matrix. Let f 1 (x) = W 1 x, f 2 (x) = W 2 &#963; R (W 1 x), ..., f l (x) = W l &#963; R (. . . W 2 &#963; R (W 1 x) . . .) be the outputs at different layers of the network for an input vector x. Note that the output f i for the ith layer is taken before the nonlinearity and f L represents the pre-softmax neuron layer. We ignore the bias terms for mathematical simplicity. The weights (W i s) of the network are trained with input and label pairs from the dataset D.</p><p>Our aim is to craft a perturbation vector p with Euclidean norm c such that the network incorrectly classifies for most of the data samples. Mathematically, the optimization can be written as, max p:|p|=c (xi,yi)&#8712;D I argmax(&#963; S (f L (xi+p))) =yi , (1) where &#963; S is the softmax function. Note that the condition for the indicator function (I) that checks for misclassification is dependent on the ground truth labels (y i s). Assuming a high clas- sification accuracy for the model, we approximate the condition to argmax(&#963; S (f L (x i + p))) = argmax(&#963; S (f L (x i ))). Since softmax function is monotonic, we further relax the objective 1 to max p:|p|=c (xi,yi)&#8712;D |f L (x i + p) &#8722; f L (x i )| 2 , (2) which amounts to finding a p that maximizes the network response after being added to the inputs. In other words, p should maximally disturb the output f L for all data points. Note that for some x i s, maximizing |f L (x i + p) &#8722; f L (x i )| 2 might not result in incorrect prediction. We assume such cases to be minority and the objective could lead to significant adversarial changes to f L responses for majority input samples. If X = {x 0 , x 1 , . . . , x N } denote the matrix formed by the assembling all the N data samples as columns and 1 represent a column vector of ones of appropriate size, then the optimization 2 can be rewritten as,</p><p>We recognize that optimization 3 is not exactly equivalent to the original objective 1, but is an ap- proximation which does not require the ground truth labels. But still the training data X is essential for computation and needs to be eliminated for a complete data-free approach. Now observe that if f L were to be a linear function, then the objective 3 reduces to, max p:|p|=c |f L (p)| 2 , (4) which means that p has to align along the first right singular vector of the linear f L map. The singular p could potentially disturb the output f L more for all the x i s than any other vector. Interestingly, note that the optimization 4 is a data-free objective under the linear assumption of f L . However, f L is nonlinear due to the presence of ReLU activation functions at every layer. Note that the formulation 4 is valid even if f L is not a complete linear map, but satisfies f L (X + p1 T ) = f L (X) + f L (p1 T ) for some p. Hence, we devise an algorithm to seek a perturbation that can approximately induce the above additivity property to the ReLU network.</p></sec><sec><title>LINEARLY APPROXIMATING THE NETWORK</title><p>We start by noting that the only nonlinearity in the network is due to the ReLU activation function at every layer. But ReLU is piece-wise linear; especially, observe that &#963; R (a + b) = &#963; R (a) + &#963; R (b) if vectors a and b are in the same orthant. Now consider the ReLU nonlinearity after the first layer, Under review as a conference paper at ICLR 2020 &#963; R (W 1 X + W 1 p1 T ), which becomes additive if column vectors in W 1 X are in the same orthant as W 1 p. We relax this criteria and favour the case of making the vectors as close as possible by, max p:|p|=c 1 T (W 1 X) T (W 1 p) = N (W 1x1 ) T (W 1 p), (5) wherex 1 stands for the mean of the N data samples in X. The solution of the optimization 5 is expected to minimize the error due to the additive approximation of the layer. In order to eliminate the data term from the objective, we make an assumption that the first singular vector of the weight matrices align along the mean vector of its corresponding input. In other words, the dot product of data meanx 1 with the singular vectors of W 1 is maximum for the first singular vector. Now we use the following lemma to argue that the objective 5 is maximum when p aligns with the first singular vector of W 1 (proof available in Appendix A).</p><p>Lemma 1. If x has positive and larger scalar projection on the first singular vector of W than remaining singular vectors, then argmax p xW T W p = argmax p |W p| 2 subject to |p| = c. Hence, the optimization problem 5 is equivalent to, max p:|p|=c |W 1 p| 2 , (6) which we call as the dilation of the first layer. We justify the assumption based on the premise that the singular vectors of the weights must have captured the discriminatory modes of data samples while training. By discriminatory mode we refer to the components of X that are essential for the classification task and most likely extracted by the hierarchy of weights in the network. These does not correspond to the modes of variation of data points. The assumption essentially means that the first singular vector carries the most important features common to most of the data points than the remaining singular directions. This is taken to be valid for any layer weight W l with difference that the mean vectorx l is averaged over the layer l &#8722; 1 output, i.e.x l = (1/N )&#963; R (f l&#8722;1 (X))1 for l &gt; 1. Now consider the second layer of the network given by &#963; R (W 2 &#963; R (W 1 X + W 1 p1 T )), where there are two ReLU functions in action. Suppose the first ReLU function is linearly approximated with dilation objective 6. Consequently, the second layer output can be written as &#963; R (W 2 &#963; R (W 1 X) + W 2 &#963; R (W 1 p1 T )). Note that the second ReLU can be linearly approximated if column vectors in W 2 &#963; R (W 1 X) are close to W 2 &#963; R (W 1 p). Considering the two approximations, we formulate the optimization as,</p><p>Again, we leverage the assumption that the data mean projects more to the first singular vector of the weight matrix and with Lemma 1, the problem becomes the dilation of the second layer,</p><p>We extend the same arguments to further layers and see that the dilations tends to make the net- work layers approximately additive with respect to the generated perturbation vector. For the last layer, the dilation terms are added to objective 4 to account for the errors introduced due to linear approximation of all the ReLU layers. Hence, the final optimization problem for UAP generation becomes, max p:|p|=c |f L (p)| 2 + L&#8722;1 l=1 |f l (p)| 2 , (10) which is clearly a completely data-free formulation.</p></sec><sec><title>SEQUENTIAL DILATION ALGORITHM</title><p>We leverage the theoretical intuitions from the previous Section to formulate an algorithm for UAP generation in a data-free manner. Note that the direct implementation of optimization 10 through any gradient descent algorithm would lead to sub-optimal solutions as the chances of getting stuck Under review as a conference paper at ICLR 2020</p><p>Algorithm 1: The sequential dilation algorithm for crafting data-free UAPs. The input is the multi-layer neural network f and the perturbation strength c. A set of adversarial perturbations {p l } L l=1 , one for each layer, is returned as the output. Note that &#955; is the learning rate.</p><p>Set |p l | &#8734; = c end end in local minimas is high. This is especially true since no data is used and the only variable being optimized is p with no sources of randomness. Hence, we perform the dilations of optimization 10 in sequential manner so as to avoid chances of reaching local minima solutions. Some more changes are applied in the way the original optimization is implemented, mainly for training stability and to compare fairly with existing methods. For numerical stability of the optimization, we follow <xref ref-type="bibr" rid="b16">Mopuri et al. (2018)</xref> and maximize logarithm of the Euclidean norm in the dilate loss. In order to compare with existing methods, l &#8734; norm is restricted instead of the l 2 in the problem 10. This constrains the maximum of absolute value of the adversarial noise.</p><p>Algorithm 1 elucidates our proposed sequential dilation algorithm for ReLU based neural networks. The procedure loops over all the layers of the network. For the first layer, we find a vector p 1 which maximizes the logarithm of l 2 norm of W 1 p 1 , essentially finding the first singular vector of W 1 . After the dilation of the first layer, the perturbation p 1 is used as an initialization for maximizing the Euclidean norm of second layer. But note the first loss term |W 1 p| 2 2 is still kept in the dilation of second layer. This loss formulation tries to maximize the norm of output at the current layer along with all the previous layers that feed into it. In short, dilation of lth layer starts the optimization with perturbation obtained from dilation of (l &#8722; 1)th layer and involves the joint dilation of all l layers. The method runs till the softmax layer of the network and the final perturbation p L is a UAP, created without using any training data and could potentially fool majority of input samples.</p><p>We only consider CNNs trained for classification task. The optimization is performed using standard ADAM optimizer (<xref ref-type="bibr" rid="b10">Kingma &amp; Ba, 2014</xref>) with a fixed learning rate schedule till the training loss sat- urates. Typical learning rate is 0.1. At every step of the optimization, the values of the perturbation are clipped to limit the allowed range. The l &#8734; norm is set as 10 for all our experiments. Although, Euclidean and maximum norms are not theoretically equivalent, practically we observe that the final perturbations are saturated, with roughly more than 78% of the values reaching &#177;10. This implies the l 2 norm also to be approximately restricted under the saturation assumption. Once the perturba- tion gets saturated while optimization, the loss might saturate and could be stuck in local minimas. To prevent this, after dilation at every layer, we rescale the perturbation by dividing the pixel values by 2. This does not make any difference to the procedure as only the magnitude is changed to make room for further optimization. Ideally, we should do sequential dilations for all the convolutional and fully connected layers of the CNN from input side to the end softmax classifier. But for very deep models like Inception and ResNet, the dilations are done only for every architectural blocks. Because of this the optimization might be more nonlinear than what is being assumed in Section 3.1, with the maximum norm further inducing the clipping nonlinearity. Hence, we initialize the perturbation (p 0 ) with the values drawn randomly from a uniform distribution U(&#8722;10, 10). Finally, note that absolutely no data in any form is used for creating adversarial noise, not even a validation set is employed in contrast to <xref ref-type="bibr" rid="b16">Mopuri et al. (2018)</xref>. The code for our approach is available for review at the anonymous link https://github.com/anoniclr/uap seq dilate.</p></sec><sec><title>EXPERIMENTS</title><p>We benchmark our proposed sequential dilation method with the existing data-free approaches. All the experiments are performed on popular classification models like VGG (<xref ref-type="bibr" rid="b23">Simonyan &amp; Zisserman, 2014</xref>), ResNet (<xref ref-type="bibr" rid="b8">He et al., 2016</xref>) and Inception (<xref ref-type="bibr" rid="b7">Szegedy et al., 2016</xref>). These models are already trained on Imagenet (<xref ref-type="bibr" rid="b4">Deng et al., 2009</xref>) dataset and delivers very high classification accuracy. <xref ref-type="fig" rid="fig_1">Figure 2</xref> shows the perturbations crafted using the proposed method for various networks. We follow other works (<xref ref-type="bibr" rid="b14">Moosavi-Dezfooli et al., 2017</xref>; <xref ref-type="bibr" rid="b16">Mopuri et al., 2018</xref>) and assess the performance of our adversarial attack using the fooling rate metric. Fooling rate is defined as the fraction of test images on which the network prediction differs before and after the addition of the adversarial noise. <xref ref-type="table" rid="tab_0">Table 1</xref> reports the fooling rates obtained by our method along with that of other works. The first comparison is with the random baseline, which is the fooling incurred with just random noise. Second baseline is with the only existing data-free approach GDUAP. Clearly, our proposed data-free objective achieves significantly higher fooling rates than the other data-free work. This indicates that sequential dilation algorithm, not only has theoretical backing, but also results in higher fooling rates in practice. Note that we have run our method ten times and the results produced in the table are mean fooling rate along with the standard deviation, to statistically validate the performance improvement.</p><p>Now we ablate the different aspects of the sequential dilation algorithm to demonstrate the useful- ness of the design choices. <xref ref-type="table" rid="tab_1">Table 2</xref> reports the results of the various ablative experiments. First experiment is non-sequential version of dilation, listed as single dilation in the table. This is a single joint optimization maximizing the norm of activations before the non-linearity. PSM maxi- mization refers to simple maximization of the pre-softmax layer (f L ) alone, which is same as the approximated objective 4. As described in Section 3.3, our dilation of a layer involves keeping the maximization terms of all the previous layers. We empirically validate the necessity of such a scheme by just sequentially maximizing the layer norms without the cumulative loss term for the experiment Ours without accumulation in <xref ref-type="table" rid="tab_1">Table 2</xref>. Note that each maximization starts with the per- turbation initialized from the previous optimization. The results for these ablations evidence that our exact formulation of sequential dilation achieves higher fooling rates in data-free scenario. Further, <xref ref-type="fig" rid="fig_2">Figure 3</xref> displays the perturbations obtained through sequential dilation at every layer for VGG-16 network, basically the p l s from Algorithm 1. We also indicate the corresponding fooling rates for each of the perturbation. It is interesting to observe that the fooling rate increases as we successively dilate layers and saturates towards the end, again emphasizing the need for the sequential process.</p><p>In many practical attack scenarios, the actual deployed model might not be available to generate the adversarial perturbation. Hence, the ability of the perturbation crafted for one network to cause reasonable fooling on another network is often highly sought-after property. This setting is known as black-box, for which we compare our method with GDUAP (<xref ref-type="bibr" rid="b16">Mopuri et al., 2018</xref>) in <xref ref-type="table" rid="tab_2">Table 3</xref>. The results evidence better black-box performance for our method than the existing data-free work, suggesting higher generalization for the perturbations from sequential dilation. The experiments performed so far show that proposed sequential dilate loss formulation achieves state-of-the-art fooling rates in data-free scenarios. We now consider the case where minimal train- ing data is available, called the less data setting. For this case, the sequential dilation is applied with the limited data. The input to the network at any stage of the optimization is the image added with the current perturbation (x i + p l for layer l). With the help of some data points, we expect the solution to approach more closer to the actual adversarial perturbation obtained with full data. <xref ref-type="table" rid="tab_3">Table 4</xref> indicates the fooling rates of the less data setting with varied amount of training samples. Note that to compare with GDUAP, we also use a validation set to select the best perturbation while training. Our approach performs significantly better than GDUAP when data samples are very less, increasing the practical utility of the method. We also observe that the fooling rates with less data, in general, have increased than data-free and became comparable to full data UAP (see <xref ref-type="table" rid="tab_0">Table 1</xref>). Furthermore, <xref ref-type="table" rid="tab_4">Table 5</xref> compares our approach with Singular Fool (<xref ref-type="bibr" rid="b9">Khrulkov &amp; Oseledets, 2018</xref>) in extremely less data scenario. For fair comparison with <xref ref-type="bibr" rid="b9">Khrulkov &amp; Oseledets (2018)</xref>, we use only 64 images for crafting the perturbation and no validation set is employed. The best perturbation is selected based on the training loss. As expected, our method achieves significantly higher fooling performance than <xref ref-type="bibr" rid="b9">Khrulkov &amp; Oseledets (2018)</xref>. Even more, we apply our algorithm with 64 ran- domly chosen images from Pascal VOC (<xref ref-type="bibr" rid="b6">Everingham et al., 2011</xref>). Interestingly, despite the models used are being trained for a different dataset, the fooling rates remain more or less similar and is higher than that of <xref ref-type="bibr" rid="b9">Khrulkov &amp; Oseledets (2018)</xref>. This shows that our approach works well in less data cases even when the available images are not from the dataset on which the model is trained.</p></sec><sec><title>CONCLUSIONS AND FUTURE WORK</title><p>In this paper, we have presented a new algorithm, called the sequential dilation, to craft universal adversaries in a data-free manner. The approach relies on finding the first singular vector of the linearly approximated neural network. The approximation is being enabled by optimizing with the proposed dilate loss. Elaborate experiments and ablations demonstrate that our approach achieves superior data-free fooling performance. One promising direction for future research would be to modify the algorithm and generate targeted UAP, where the objective is fooling to a specific class. Under review as a conference paper at ICLR 2020</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Demonstration of adversarial attack through perturbation generated from our proposed sequential dilation algorithm. The first row shows the clean images with the class predicted by the model, while the second row has the corresponding perturbed samples with the flipped labels.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Data-free Universal Adversaries crafted using our sequential dilation algorithm.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Comparison of fooling rates (in percentage) obtained for our approach with that of other works. We achieve higher fooling rate than the existing data-free method GDUAP (Mopuri et al., 2018). Note that UAP (Moosavi-Dezfooli et al., 2017) is data dependent (shown for completeness).</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Various ablative experiments regarding our approach. It is evident that the proposed se- quential dilation algorithm has higher performance than any of its variants.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Intermediate perturbations (p l s) crafted for different layers using our sequential dilation algorithm for VGG-16 network with corresponding fooling rates obtained.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><table-wrap id="tab_2"><label>Table 3:</label><caption><title>Table 3:</title><p>Black-box testing of data-free sequential dilation approach. Rows indicate the networks used to craft the adversary and the columns specify the one for which the fooling rate is computed. Blue values are the fooling rates obtained by our method, while the red ones correspond to GDUAP.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_3"><label>Table 4:</label><caption><title>Table 4:</title><p>Comparison of fooling rates obtained by our method and GDUAP (Mopuri et al., 2018) in less data setting with varied number of images available for crafting the perturbation.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_4"><label>Table 5:</label><caption><title>Table 5:</title><p>Benchmarking our approach in less data scenario with Singular Fool (Khrulkov &amp; Os- eledets, 2018) where the adversaries are crafted with just 64 images.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap></sec></body><back><sec><p>All our experiments reported in the main paper are run on NVIDIA DGX cluster (Dual 20-core Intel Xeon E5-2698 v4 2.2 GHz) within the Tensorflow docker. We use the pretrained classification models from Tensorflow Slim library S. <xref ref-type="bibr" rid="b21">Guadarrama (2016)</xref>.</p></sec><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Obfuscated gradients give a false sense of se- curity: Circumventing defenses to adversarial examples</article-title><source>International Conference on Machine Learning</source><year>2018</year><fpage>274</fpage><lpage>283</lpage><person-group person-group-type="author"><name><surname>References Anish Athalye</surname><given-names>Nicholas</given-names></name><name><surname>Carlini</surname><given-names>David</given-names></name><name><surname>Wagner</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</article-title><source>arXiv preprint arXiv:1712.04248</source><year>2017</year><person-group person-group-type="author"><name><surname>Brendel</surname><given-names>Wieland</given-names></name><name><surname>Rauber</surname><given-names>Jonas</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Adversarial patch</article-title><source>arXiv preprint arXiv:1712.09665</source><year>2017</year><person-group person-group-type="author"><name><surname>Tom B Brown</surname><given-names>Dandelion</given-names></name><name><surname>Man&#233;</surname><given-names>Aurko</given-names></name><name><surname>Roy</surname><given-names>Mart&#237;n</given-names></name><name><surname>Abadi</surname><given-names>Justin</given-names></name><name><surname>Gilmer</surname><given-names /></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Towards evaluating the robustness of neural networks</article-title><source>IEEE Symposium on Security and Privacy (SP)</source><year>2017</year><fpage>39</fpage><lpage>57</lpage><person-group person-group-type="author"><name><surname>Carlini</surname><given-names>Nicholas</given-names></name><name><surname>Wagner</surname><given-names>David</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>ImageNet: A Large-Scale Hierarchical Image Database</article-title><source>CVPR09</source><year>2009</year><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Dong</surname><given-names>W</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>L.-J</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Boost- ing adversarial attacks with momentum</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2018</year><fpage>9185</fpage><lpage>9193</lpage><person-group person-group-type="author"><name><surname>Dong</surname><given-names>Yinpeng</given-names></name><name><surname>Liao</surname><given-names>Fangzhou</given-names></name><name><surname>Pang</surname><given-names>Tianyu</given-names></name><name><surname>Su</surname><given-names>Hang</given-names></name><name><surname>Zhu</surname><given-names>Jun</given-names></name><name><surname>Hu</surname><given-names>Xiaolin</given-names></name><name><surname>Li</surname><given-names>Jianguo</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>The pascal visual ob- ject classes challenge 2012 (voc2012) results</article-title><year>2012</year><person-group person-group-type="author"><name><surname>Everingham</surname><given-names>M</given-names></name><name><surname>Van Gool</surname><given-names /></name><name><surname>Williams</surname><given-names /></name><name><surname>Winn</surname><given-names>A</given-names></name><name><surname>Zisserman</surname><given-names /></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Explaining and harnessing adversarial examples</article-title><source>arXiv preprint arXiv:1412.6572</source><year>2014</year><person-group person-group-type="author"><name><surname>Ian</surname><given-names>J</given-names></name><name><surname>Goodfellow</surname><given-names>Jonathon</given-names></name><name><surname>Shlens</surname><given-names>Christian</given-names></name><name><surname>Szegedy</surname><given-names /></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Deep residual learning for image recog- nition</article-title><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><year>2016</year><fpage>770</fpage><lpage>778</lpage><person-group person-group-type="author"><name><surname>He</surname><given-names>Kaiming</given-names></name><name><surname>Zhang</surname><given-names>Xiangyu</given-names></name><name><surname>Ren</surname><given-names>Shaoqing</given-names></name><name><surname>Sun</surname><given-names>Jian</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Art of singular vectors and universal adversarial perturba- tions</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2018</year><fpage>8562</fpage><lpage>8570</lpage><person-group person-group-type="author"><name><surname>Khrulkov</surname><given-names>Valentin</given-names></name><name><surname>Oseledets</surname><given-names>Ivan</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv preprint arXiv:1412.6980</source><year>2014</year><person-group person-group-type="author"><name><surname>Diederik</surname><given-names>P</given-names></name><name><surname>Kingma</surname><given-names>Jimmy</given-names></name><name><surname>Ba</surname><given-names /></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Adversarial examples in the physical world</article-title><source>arXiv preprint arXiv:1607.02533</source><year>2016</year><person-group person-group-type="author"><name><surname>Kurakin</surname><given-names>Alexey</given-names></name><name><surname>Goodfellow</surname><given-names>Ian</given-names></name><name><surname>Bengio</surname><given-names>Samy</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Towards deep learning models resistant to adversarial attacks</article-title><source>arXiv preprint arXiv:1706.06083</source><year>2017</year><person-group person-group-type="author"><name><surname>Madry</surname><given-names>Aleksander</given-names></name><name><surname>Makelov</surname><given-names>Aleksandar</given-names></name><name><surname>Schmidt</surname><given-names>Ludwig</given-names></name><name><surname>Tsipras</surname><given-names>Dimitris</given-names></name><name><surname>Vladu</surname><given-names>Adrian</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Univer- sal adversarial perturbations against semantic image segmentation</article-title><source>IEEE International Conference on Computer Vision (ICCV)</source><year>2017</year><fpage>2774</fpage><lpage>2783</lpage><person-group person-group-type="author"><name><surname>Hendrik Metzen</surname><given-names>Jan</given-names></name><name><surname>Kumar</surname><given-names>Mummadi Chaithanya</given-names></name><name><surname>Brox</surname><given-names>Thomas</given-names></name><name><surname>Fischer</surname><given-names>Volker</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Deepfool: a simple and accurate method to fool deep neural networks</article-title><source>Proceedings of the IEEE conference on com- puter vision and pattern recognition</source><year>2016</year><fpage>2574</fpage><lpage>2582</lpage><person-group person-group-type="author"><name><surname>Seyed-Mohsen Moosavi-Dezfooli</surname><given-names>Alhussein</given-names></name><name><surname>Fawzi</surname><given-names>Pascal</given-names></name><name><surname>Frossard</surname><given-names /></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Universal adversarial perturbations</article-title><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><year>2017</year><fpage>1765</fpage><lpage>1773</lpage><person-group person-group-type="author"><name><surname>Seyed-Mohsen Moosavi-Dezfooli</surname><given-names>Alhussein</given-names></name><name><surname>Fawzi</surname><given-names>Omar</given-names></name><name><surname>Fawzi</surname><given-names>Pascal</given-names></name><name><surname>Frossard</surname><given-names /></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>Generalizable data- free objective for crafting universal adversarial perturbations. IEEE transactions on pattern anal- ysis and machine intelligence, 2018. Under review as a conference paper at ICLR 2020 Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines</article-title><source>Proceedings of the 27th international conference on machine learning (ICML-10)</source><year>2010</year><fpage>807</fpage><lpage>814</lpage><person-group person-group-type="author"><name><surname>Konda Reddy Mopuri</surname><given-names>Aditya</given-names></name><name><surname>Ganeshan</surname><given-names>Venkatesh</given-names></name><name><surname>Babu Radhakrishnan</surname><given-names /></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>The limitations of deep learning in adversarial settings</article-title><source>IEEE European Sympo- sium on Security and Privacy (EuroS&amp;P)</source><year>2016</year><fpage>372</fpage><lpage>387</lpage><person-group person-group-type="author"><name><surname>Papernot</surname><given-names>Nicolas</given-names></name><name><surname>Mcdaniel</surname><given-names>Patrick</given-names></name><name><surname>Jha</surname><given-names>Somesh</given-names></name><name><surname>Fredrikson</surname><given-names>Matt</given-names></name><name><surname>Berkay Celik</surname><given-names>Ananthram</given-names></name><name><surname>Swami</surname><given-names /></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Generative adversarial pertur- bations</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2018</year><fpage>4422</fpage><lpage>4431</lpage><person-group person-group-type="author"><name><surname>Poursaeed</surname><given-names>Omid</given-names></name><name><surname>Katsman</surname><given-names>Isay</given-names></name><name><surname>Gao</surname><given-names>Bicheng</given-names></name><name><surname>Belongie</surname><given-names>Serge</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Ask, acquire, and attack: Data-free uap generation using class impressions</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><year>2018</year><fpage>19</fpage><lpage>34</lpage><person-group person-group-type="author"><name><surname>Konda Reddy Mopuri</surname><given-names>Phani Krishna</given-names></name><name><surname>Uppala</surname><given-names>R Venkatesh</given-names></name><name><surname>Babu</surname><given-names /></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Nag: Network for ad- versary generation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2018</year><fpage>742</fpage><lpage>751</lpage><person-group person-group-type="author"><name><surname>Konda Reddy Mopuri</surname><given-names>Utkarsh</given-names></name><name><surname>Ojha</surname><given-names>Utsav</given-names></name><name><surname>Garg</surname><given-names>R Venkatesh</given-names></name><name><surname>Babu</surname><given-names /></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>Tensorflow-slim: a lightweight library for defining, training and evaluating complex models in tensorflow</article-title><year>2016</year><person-group person-group-type="author"><name><surname>Silberman</surname><given-names>N</given-names></name><name><surname>Guadarrama</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition</article-title><year>2016</year><fpage>1528</fpage><lpage>1540</lpage><person-group person-group-type="author"><name><surname>Sharif</surname><given-names>Mahmood</given-names></name><name><surname>Bhagavatula</surname><given-names>Sruti</given-names></name><name><surname>Bauer</surname><given-names>Lujo</given-names></name><name><surname>Reiter</surname><given-names>Michael K</given-names></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv preprint arXiv:1409.1556</source><year>2014</year><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>Karen</given-names></name><name><surname>Zisserman</surname><given-names>Andrew</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>Intriguing properties of neural networks</article-title><source>arXiv preprint arXiv:1312.6199</source><year>2013</year><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>Christian</given-names></name><name><surname>Zaremba</surname><given-names>Wojciech</given-names></name><name><surname>Sutskever</surname><given-names>Ilya</given-names></name><name><surname>Bruna</surname><given-names>Joan</given-names></name><name><surname>Erhan</surname><given-names>Dumitru</given-names></name><name><surname>Goodfellow</surname><given-names>Ian</given-names></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><article-title>Rethink- ing the inception architecture for computer vision</article-title><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><year>2016</year><fpage>2818</fpage><lpage>2826</lpage><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>Christian</given-names></name><name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name><name><surname>Ioffe</surname><given-names>Sergey</given-names></name><name><surname>Shlens</surname><given-names>Jon</given-names></name><name><surname>Wojna</surname><given-names>Zbigniew</given-names></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>Adversarial examples for semantic segmentation and object detection</article-title><source>Proceedings of the IEEE Interna- tional Conference on Computer Vision</source><year>2017</year><fpage>1369</fpage><lpage>1378</lpage><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Cihang</given-names></name><name><surname>Wang</surname><given-names>Jianyu</given-names></name><name><surname>Zhang</surname><given-names>Zhishuai</given-names></name><name><surname>Zhou</surname><given-names>Yuyin</given-names></name><name><surname>Xie</surname><given-names>Lingxi</given-names></name></person-group></element-citation></ref></ref-list></back></article>