Title:
```
Under review as a conference paper at ICLR 2020 A MCTS: SEARCH WITH THEORETICAL GUARANTEE USING POLICY AND VALUE FUNCTIONS
```
Abstract:
```
Combined with policy and value neural networks, Monte Carlos Tree Search (MCTS) is a critical component of the recent success of AI agents in learning to play board games like Chess and Go (Silver et al., 2017). However, the theoretical foundations of MCTS with policy and value networks remains open. Inspired by MCTS, we propose A MCTS, a novel search algorithm that uses both the policy and value predictors to guide search and enjoys theoretical guarantees. Specifically, assuming that value and policy networks give reasonably accurate signals of the values of each state and action, the sample complexity (number of calls to the value network) to estimate the value of the current state, as well as the optimal one-step action to take from the current state, can be bounded. We apply our theoretical framework to different models for the noise distribution of the policy and value network as well as the distribution of rewards, and show that for these general models, the sample complexity is polynomial in D, where D is the depth of the search tree. Empirically, our method outperforms MCTS in these models.
```

Figures/Tables Captions:
```
Figure 1: Illustration of our overall method. We prioritize nodes to expand using the value network estimates. Algorithm 1 queries the value network for all children of the first node in the priority queue. In our next algorithm, Algorithm 2, we show how to use a policy network to choose which children nodes to add to the priority queue. We present the complexity bound in Theorem 1. The complexity bound is based on one very simple observation, which is that we end up choosing a sub-optimal node if the value network estimate of that node plus the upper bound on the possible error (optimism) is higher than the true optimal value. Otherwise we will not choose to expand that node, since we are sure that it will not be the optimal node. Therefore a sub-optimal internal node s d is chosen if V * ≤ U s d + c d = V s d + X s d + c d , where U s d is the value network estimate for state s d , and c d is the upper bound on the possible error for the value estimate. We should also account for the ancestors of s d , if we are lucky enough to be able to rule out one of the ancestors of s d , we would never expand beyond that ancestor to reach s d , and s d would not be in our priority queue. Theorem 1 states this complexity bound.
Figure 2: The constant gap model. The leaves have value 0 except for the optimal leaf, which has value η. A blue path is a sub-optimal path, the red path is the optimal path.
Figure 3: Using Value and Policy Networks for search in the Constant Gap Model with polynomially decaying noise and η = 0.5.
Figure 4: Using Value and Policy Networks for search in the Constant Gap Model with exponentially decaying noise and η = 0.5.
Figure 5: The generative model. A blue path is a sub-optimal path, the red path is the optimal path. The optimal leaf has value V * . Sub-optimal nodes inherit the gaps of their parent as well as an extra gap Y , so that the sub-optimal leaf values get smaller and smaller.
Figure 6: Using Value and Policy Networks for search in the Generative Model with polynomially decaying noise and η = 0.5.
Figure 7: Using Value and Policy Networks for search in the Generative Model with exponentially decaying noise and η = 0.5.
Table 1: Summary of main notations used in the paper. L(s) is the set of leaf nodes that share s as an ancestor, i.e., the set of leaf nodes in the subtree rooted at s. We assume that the highest value V * ≡ max l∈L V l = max s∈S V s is achieved at a unique leaf node l * = arg max l∈L V l .
Table 2: Value Network Only Results for the Constant Gap Model
Table 3: Value and Policy Network Results for the Constant Gap Model
Table 4: Value Network Only Results for the Generative Model
Table 5: Value and Policy Network Results for the Generative Model
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Monte Carlo Tree Search (MCTS) is an online heuristic search algorithm commonly used to find op- timal policies for problems in reinforcement learning. It is a vital component of recent breakthroughs for AI in game play, including AlphaGo and AlphaZero (Silver et al., 2016; 2017). An interesting difference between the MCTS used in AlphaGo/AlphaZero and traditional MCTS is the adoption of neural networks to predict the value of the current state (value network), and to prioritize the next action to search (policy network). The value network replaces random rollouts used to evaluate a non-terminal state, which saves both online computation time and substantially reduces the variance of the estimate. The policy network (coupled with PUCT (Rosin, 2011)) prioritizes promising actions for more fruitful exploration. Both networks are trained on pre-existing datasets (either from human replays or self-play) to incorporate prior knowledge. This strategy has been applied in other domains including Neural Architecture Search (Wang et al., 2018; Wistuba, 2017; Negrinho & Gordon, 2017; Wang et al., 2019b). Despite the empirical success and recent popularity, theoretical foundations are lacking. (Kocsis & Szepesvári, 2006; Coquelin & Munos, 2007) analyze MCTS with rollouts using a multi-armed bandit framework and gives asymptotic results without finite sample complexity bounds. Powering MCTS with value/policy neural networks instead of rollouts is even more poorly understood. To better understand how to efficiently learn optimal policies in decision trees using value and policy networks, we propose a novel algorithm, A MCTS, which is a combination of A (Delling et al., 2009; Kanoulas et al., 2006) and MCTS. Like A , it uses a priority queue to store all leaves currently being explored and picks the most optimistic one to expand, according to an upper confidence bound heuristic function. Like MCTS, it uses policy and value networks to prioritize the next state to be explored. To facilitate the analysis, the policy and value networks are treated as black-box functions and a sta- tistical model is built the accuracy of the predictions. In particular, we assume the standard deviation of the noisy estimates of intermediate state values decays (either polynomially or exponentially) as a function of the depth of the tree, so that the values of near-terminal states are estimated to a greater degree of accuracy. With this statistical model, we provide theoretical guarantees for the sample Under review as a conference paper at ICLR 2020 complexity (expected number of state expansions / rollouts) to determine optimal actions. We apply our theoretical framework to simple models for rewards and show that our algorithm enjoys a sample complexity that is polynomial in depth D. Our experiments validate the theoretical analysis and demonstrate the effectiveness of A MCTS over benchmark MCTS algorithms with value and policy networks. To our knowledge, our work is the first that studies tree search for optimal actions in the presence of pre-trained value and policy networks, and we hope that it inspires further progress on this important problem.

Section Title: BACKGROUND AND RELATED WORK
  BACKGROUND AND RELATED WORK Monte Carlo Tree Search (Browne et al., 2012) is an algorithm that tries to find optimal decisions in Markov Decision Processes by taking random samples in the decision space and building a tree based on the results. In each iteration, the agent traverses the tree by acting according to the tree policy (which usually involves some exploration) for the set of known states and then determines the value of an unexpanded state by performing random rollouts. MCTS is commonly used to search for a one-step optimal actions from the root. The agent then takes this action, observes the result, transitions to a new state, and repeats MCTS starting from this new state, with the benefit of the additional observation. The main theoretical question of interest in MCTS is how quickly and efficiently one can find this optimal one-step action, in terms of the number of random rollouts, or the number of nodes in the tree that should be expanded. On the theoretical side, there is a small body of literature that addresses the theoretical foundations of MCTS with random rollouts (Kocsis & Szepesvári (2006), Veness et al. (2011)). The algorithm, called UCT (UCB applied to trees) models the search as a complex multi-armed bandit problem where the rewards are allowed to drift over time. The analysis shows that after a period of time T , only the optimal branch will be followed. Veness et al. (2011) gives various ways of improving UCT with variance reduction techniques. Later followup work in Coquelin & Munos (2007) highlights some hard instances of UCT and offers some alternatives, including a modified UCT (with a different exploration bonus), Flat UCB, and BAST (smooth tree models). On the experimental side, MCTS was popularized by the story of AlphaGo (Silver et al. (2016)), which used MCTS using a weighted combination of the value network's output and random rollouts to defeat the leading champion in Go. Follow-up work in AlphaZero (Silver et al. (2017)) used MCTS with only policy and value neural networks, without random rollouts and reports faster training with better performance. However, there is no theoretical foundation for the search algorithm that AlphaZero uses. A (Nosrati et al., 2012) uses a priority queue to keep track of lowest cost actions. A combined with hand-designed heuristics, has been used in many path finding and routing problems (AlShawi et al., 2012; Rana & Zaveri, 2011; Ghaffari, 2014). Recent work (Wang et al., 2019a; Chen & Wei, 2011; Sigurdson & Bulitko, 2017) trains neural-based heuristic functions for A for specific problems. However, to the best of our knowledge, we are not aware of any prior work that performs theoretical analysis of such heuristic based A algorithms.

Section Title: PRELIMINARIES AND SUMMARY OF CONTRIBUTIONS
  PRELIMINARIES AND SUMMARY OF CONTRIBUTIONS In this work we consider a deterministic, finite D-horizon Markov decision process (MDP) with no intermediate rewards. With the set of states denoted by S, there is one deterministic root initial state s 0 and the process terminates at horizon D with a termination reward that depends on the end state. We assume that there are exactly K possible actions at every intermediate state s and for simplicity that the process does not backtrack, i.e., each action leads to a new and unique state. This is a Markov decision tree. The set of leaf states of the Markov decision tree are denoted as L. We shall denote an arbitrary state at depth d as s d and also a leaf state as l ∈ L. There is a value function V : S → R that takes as input a state s and returns the value of s, defined in the following way. If s ∈ L, then V s is the termination reward of that particular leaf. For an intermediate node s, V s = max l∈L(s) V l , where Under review as a conference paper at ICLR 2020 In this work, we focus on the problem of efficiently computing V * exactly or approximately with a controlled accuracy without direct access to the value function V : S → R, as a stepping stone to finding the optimal one-step action from s 0 . Instead, there is a pre-trained black-box resource, value network U : S → R that takes as input a state and returns a noisy value estimate, i.e., for a state s d at depth d, U s d ≡ V s d + X s d , where X s d are i.i.d. copies of a random variable X d that depends only on the depth d. We assume that at depth D, which is the level with all the leaf nodes, U l = V l (i.e., the value network returns the ground truth values without any noise). The efficiency is defined in terms of sample complexity, which is the number of queries made to the value network for the value of a state. In the worst case, all leaves need to be queried and the sample complexity is K D . Another type of available pre-trained black box resource is a policy network. Let C(s) = {r 1 , . . . r K } denote the set of K children of state s, each corresponding to the transition state of one of the K actions {a 1 , . . . a K }. The policy network outputs a probability estimate of the form p ri = e U π r i K k=1 e U π r k , where each U π ri is a perturbed estimate of V ri . Here the superscript π indicates that this is the approximation from the policy network. We assume that if the child r i is at depth d then the estimate U π ri = V ri + X π ri , where X π ri are i.i.d. copies of a random variable X π d that depends only on the depth d. We assume that the distribution of X d and X π d are known a priori for each depth d. Further, the variances of X d and X π d decrease sufficiently rapidly as the depth d increases (i.e., as one traverses deeper in the tree, the noise level is smaller). This assumption is motivated by basic approximate dynamic programming principles, which is that near-terminal states require a smaller sample complexity to estimate well, since the backwards induction will propagate this noise to earlier states, making it harder to estimate the values of initial states well.

Section Title: MAIN CONTRIBUTIONS
  MAIN CONTRIBUTIONS We develop and analyze (giving specific sample complexity bounds) algorithms to estimate V * exactly and approximately using value and policy networks. Since our value and policy networks are trained functions that give fixed, deterministic noisy outputs for each state (no matter how many times one calls the function on a particular state), where the noise variance is smaller at deeper levels of the search tree, we use our value and policy network estimates as signals for which are the promising states to expand. We rank the states based on our optimistic estimates U s d + c d in a priority queue, and we expand (query the value network for the value estimate) the children of the top nodes of the priority queue to get increasingly more accurate value estimates and determine the value of the optimal leaf V * . The main contributions are organized in the following sections: Under review as a conference paper at ICLR 2020 • Section 4 introduces and analyzes A MCTS-V * and A MCTS-π * V * that are our core techniques for estimating V * exactly. A MCTS-V * uses only a value network to determine V * exactly and outputs the optimal one-step from s 0 that leads to l * . We give an expected sample complexity in Theorem 1. A MCTS-π * V * builds on A MCTS-V * by using a policy network to further reduce the sample complexity, which we bound in Theorem 2. • Section 5 extends A MCTS-V * and A MCTS-π * V * to the case when we tolerate δ-additive approximations of V * . • Section 6 introduces some sample distribution models for the value functions and the noise and applies our results to those models. We show that for some reasonable models, our algorithms can achieve sample complexity that is polynomial in depth D. We also provide some experimental comparisons against benchmark MCTS implementations.

Section Title: FINDING V * EXACTLY USING VALUE AND POLICY NETWORKS
  FINDING V * EXACTLY USING VALUE AND POLICY NETWORKS This section presents our main techniques, A MCTS-V * and A MCTS-π * V * , which find V * exactly using a value network only in the former and a value network and a policy network in the latter. A MCTS-V * , Algorithm 1, uses only a value network to determine V * exactly and outputs the optimal one-step action from s 0 that takes the agent to an ancestor of l * . The idea behind the algorithm is very simple and similar to A search. We first expand (i.e., query the value network) the children of the root node s 0 , compute their value network estimate plus the exploration bonus, and then insert these nodes to a priority queue Q. The exploration bonus, denoted by c d , is chosen such that P(|V s d − U s d | ≤ c d ) ≥ 1 − β DK d . In each subsequent iteration, the algorithm picks the top element in the priority queue Q, which is the most optimistic state, and expands all the children of that state, and adds those children to Q. We terminate at the first time when the most optimistic element is a leaf node. Since the value network gives the values of leaf nodes exactly, this leaf node must be l * . Theorem 1. With probability 1 − β, Algorithm 1, A MCTS-V * , returns V * . The expected sample complexity (number of calls to the value network) is 11: Return U s and the action at s 0 that leads to s where A(l * ) is the set of ancestor states of the optimal leaf, l * , and ∆ s d = V * − V s d . Before we give the proof, we briefly note that getting an exact expression for the expected sample complexity depends on the problem model (the distribution for the gaps ∆ s d and the errors X s d ). We give two examples in Section 6 of different models and we apply this theorem to those models to derive the sample complexity, which we show is polynomial in depth D. Please check Appendix for the proof.

Section Title: COMBINING VALUE NETWORK WITH POLICY NETWORK
  COMBINING VALUE NETWORK WITH POLICY NETWORK Suppose that in addition to the value network U , one has access to a policy network U π that outputs, for each child of s, r 1 , . . . r K (each corresponding to the transition state of a different action a 1 , . . . a K ) a probability estimate of the form: p ri = e U π r i K k=1 e U π r k , where each U π ri = V ri + X π ri is a perturbed estimate of V ri . If r i is at depth d, the noise X π ri is an i.i.d. copy of the random variable X π d that depends only on the depth d. Since all the probabilities are known, we assume without loss of generality that states r 1 , . . . r K are ordered so that The following algorithm A MCTS-π * V * uses both the value and the policy networks to determine V * exactly. When we expand a given state s d , instead of querying for the value estimate for every child of s d and adding each child to the priority queue, we add the top 2 children (ordered by the probabilities given by the policy network) to the queue by default. For the k-th child node, where k > 2, we add this child if there is a small gap between the policy network probability of the k − 1-th child and the probability of the top child, where the threshold for the gap is given by the noise model. Intuitively, we are saying that if this gap is sufficiently big, even after accounting for the noise in the policy network, there is no way that this k-th child is part of the optimal policy, so we do not have to add it to the queue. The key insight behind this algorithm is that, if there is a large gap between the probabilities of the children as given by the policy network, one can infer that the children states associated with the smaller probabilities are not worth expanding because they have small values. This is made rigorous in the statement and proof of Theorem 2. Theorem 2. With probability 1 − β, Algorithm 2, A MCTS-π * V * , returns V * . The expected sample complexity (number of calls to the value network) is Under review as a conference paper at ICLR 2020 As with Theorem 1, getting an exact expression for the expected sample complexity depends on the problem model (the distribution for the gaps ∆ s d and the errors X s d ). We give two examples in Section 6 of different models and we apply this theorem to those models to derive the sample complexity, which we show is polynomial in depth D. Please check Appendix for the proof.

Section Title: APPROXIMATING V * USING VALUE AND POLICY NETWORKS
  APPROXIMATING V * USING VALUE AND POLICY NETWORKS This section extends the algorithms to the problem setting where we are willing to tolerate an additive error δ in the estimate for V * We first consider the extension of A MCTS-V * , Algorithm 1. The key idea is that it suffices to stop searching at a depth D whenever c D ≤ δ. As long as the most optimistic state at depth D is identified, the value of this state approximates V * up to an additive error δ. The main result is stated in Theorem 3 and the algorithm is Algorithm A MCTS-V , presented in Appendix 8.3. where A(l * ) is the set of ancestor states of the optimal leaf, l * , and ∆ s d = V * − V s d , and D is the minimum depth at which c D ≤ δ Using the same reasoning as the one for A MCTS-V , we can extend A MCTS-π * V * to the following result. Under review as a conference paper at ICLR 2020 Theorem 4. With probability 1 − β, one can find V * up to additive approximation error δ using expected sample complexity (number of calls to the value network): where D is the minimum depth at which c D ≥ δ.

Section Title: MODELS AND EXPERIMENTS
  MODELS AND EXPERIMENTS In this section, we discuss two simple yet representative models for the Markov decision tree and show that the sample complexity is polynomially bounded under reasonable noise models.

Section Title: CHOOSING c d FOR GAUSSIAN NOISE
  CHOOSING c d FOR GAUSSIAN NOISE The noises X d , X π d are taken to be Gaussian N (0, σ 2 d ) random variables. Two noise models are tested. The first is a Gaussian with an exponentially decaying standard deviation, i.e., X d , X π d ∼ N (0, 1 α 2d ) for some α > 1. The second is a Gaussian with a polynomially decaying standard deviation, i.e., X d , X π d ∼ N (0, 1 d 2γ ), for some γ > 1. The experiments are carried out for α = 1.3 and 1.5 for the exponential case and γ = 1.3 and 1.5 for the polynomial case. The assumption of decaying standard deviation as a function of depth is motivated by basic approxi- mate dynamic programming principles, which we elaborate on in Section 3. The choice of c d is essential to the performance of the algorithms. Standard concentration inequalities for sub-Gaussian random variables state that P(|X d | ≥ c d ) ≤ 2e −c 2 d 2σ 2 d , and this probability expression is required to be less than β DK d . After substituting it in, we arrive at c d = O( √ dσ d ).

Section Title: CONSTANT GAP MODEL
  CONSTANT GAP MODEL In the constant gap model, V * − V l * = η for a fixed constant η > 0 and V l = 0 for any other leaf l = l * . Therefore, the gap ∆(l) = η for any l = l * . The noise variables X d , X π d are mean-zero Gaussian random variables. Using value network. The expected sample complexity for Algorithm 1 is Under review as a conference paper at ICLR 2020 This translates to a sample complexity of E[N ] ≤ KD + D 2 (K − 1)K c , for the smallest constant c that satisfies η σc − √ c ≥ √ 2 log K. The analysis for this result, as well as similar analyses and sample complexity results for Algorithm 2 and the approximate counterparts are in Appendix 8.4.

Section Title: Experiments
  Experiments We compare the performance of A MCTS-V * with the MCTS algorithm that uses the UCB estimate Q s,a + 2c log( b N s,b ) Ns,a and queries the value network U s instead of expanding state s with random rollouts, where c is a constant, and N s,b are visitation counts for choosing action a from state s. We also compare A MCTS-π * V * with the PUCT algorithm described in Silver et al. (2017) that uses the UCB estimate Q s,a + c · p s,a · b N s,b Ns,a , where p s,a is the probability given by the policy network U π . The experiments are performed on a tree of depth D = 10 with K = 5 children per state. As defined in the constant gap model, the optimal leaf has reward η and all other leaves have reward 0, and we experiment with η = 1 and η = 0.5. In our experiments, we use c = 1 in the the bonus UCB expression for benchmark MCTS and PUCT algorithms. Our algorithms set c d = 5 · √ dσ d , where σ d is the variance of the Gaussian noise. This choice is justified earlier in Section 6.1. Since the main use case for these algorithms is to find the optimal one-step action from s 0 , we collect data on the one-step action that the algorithm would output at every 1, 000 expansions (or queries to the value network U ). MCTS algorithms would output the one-step action from s 0 with the highest visitation count. Our algorithms would output the action from s 0 that leads to s, where s is the highest-valued element in the current priority queue Q based on the value network estimate U s . We give an overall budget of 20,000 expansions and note whether or not (so the data is 1 or 0) the algorithm chooses the right one-step action. Each of our experiments average over 200 trials.  Tables 1  and 2 give the proportion of trials that return the correct optimal one-step action at the 20,000-th expansion. To give an idea about the performance gain,  Figures 3  and 4 show the success proportion over time, at successive 1000 expansion intervals for Algorithm 2 and PUCT for η = 0.5 in both noise models (polynomially and exponentially decaying).

Section Title: Using value networks
  Using value networks where Z is a standard normal distribution. The goal is to bound this by a quantity polynomial (rather than exponential) in D when η, K, and α are considered fixed. To proceed, we introduce Therefore, up to a term linear in D, the sample complexity can be bounded by Under review as a conference paper at ICLR 2020 is uniformly bounded by a constant C(K, η, α) due to the exponential decay in d. Therefore, the term (4) can be bounded by (DK) C(K,η,α) , which is polynomial (rather than exponential) in the depth D. Similar analysis and sample complexity statements can be made for the performance of Algorithm 2 and the approximate counterparts. We defer this treatment to Appendix 8.5.

Section Title: Experiments
  Experiments Our experimental results for the Generative Model follow the same setup and parame- ters as the experiments for the Constant Gap Model in Section 6.2.  Tables 3  and 4 give the proportion of trials that return the correct optimal one-step action at the 20,000-th expansion.  Figures 6  and 7 show the success proportion over time, at successive 1000 expansion intervals for Algorithm 2 and PUCT for η = 1 in both noise models (polynomially and exponentially decaying).

Section Title: CONCLUSION
  CONCLUSION We introduce A MCTS, a search algorithm that uses pre-trained value/policy networks to guide exploration and learn to make optimal decisions. We provide expected sample complexity bounds and demonstrate theoretically and experimentally that our algorithms work efficiently for reasonable models of reward distributions and noise (in value/policy network estimates) distribution. Future work includes more rigorous experimental analysis, improved search algorithms that automatically adapts to the noise level of the value/policy networks, and applications to real scenarios.

```
