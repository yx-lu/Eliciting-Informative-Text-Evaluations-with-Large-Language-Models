<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 DISENTANGLING IMPROVES VAES' ROBUSTNESS TO</article-title></title-group><abstract><p>This paper is concerned with the robustness of VAEs to adversarial attacks. We highlight that conventional VAEs are brittle under attack but that methods recently introduced for disentanglement such as &#946;-TCVAE (Chen et al., 2018) improve robustness, as demonstrated through a variety of previously proposed adversarial attacks (Tabacof et al. (2016); Gondim-Ribeiro et al. (2018); Kos et al.(2018)). This motivated us to develop Seatbelt-VAE, a new hierarchical disentangled VAE that is designed to be significantly more robust to adversarial attacks than existing approaches, while retaining high quality reconstructions.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Unsupervised learning of disentangled latent variables in generative models remains an open research problem, as is an exact mathematical definition of disentangling (Higgins et al., 2018). Intuitively, a disentangled generative model has a one-to-one correspondence between each input dimension of the generator and some interpretable aspect of the data generated.</p><p>For VAE-derived models (Kingma &amp; Welling, 2013; Rezende et al., 2014) this is often based around rewarding independence between latent variables. Factor VAE (Kim &amp; Mnih, 2018), &#946;-TCVAE (Chen et al., 2018) and HFVAE (Esmaeili et al., 2019) have shown that the evidence lower bound can be decomposed to obtain a term capturing the degree of independence between latent variables of the model, the total correlation. By up-weighting this term, we can obtain better disentangled representations under various metrics compared to &#946;-VAEs (Higgins et al., 2017a).</p><p>Disentangled representations, much like PCA or factor analysis, are not only human-interpretable but also offer more informative and robust latent space representations. In addition, information theoretic interpretations of deep learning show that having a disentangled hidden layer within a discriminative deep learning model increases robustness to adversarial attack (Alemi et al., 2017). Adversarial attacks on deep generative models, more difficult than those on discriminative models (Tabacof et al., 2016; Gondim-Ribeiro et al., 2018; Kos et al., 2018), attempt to fool a model into reconstructing a chosen target image by adding distortions to the original input image. Generally, the most effective attack mode involves making the latent-space representation of the distorted input match that of the target image (Gondim-Ribeiro et al., 2018; Kos et al., 2018). This kind of attack is particularly relevant to applications where the encoder's output is used downstream.</p><p>Projections of data from VAEs, disentangled or not, are used for tasks such as: text classification (Xu et al., 2017); discrete optimisation (Kusner et al., 2017); image compression (Theis et al., 2017; Townsend et al., 2019); and as the perceptual part of a reinforcement learning algorithm (Ha &amp; Schmidhuber, 2018; Higgins et al., 2017b), the latter of which uses a disentangled VAE's encoder to improve the robustness of the agent to domain shift.</p><p>Here we demonstrate that &#946;-TCVAEs are significantly more robust to 'latent-space' attack than standard VAEs, and are generally more robust to attacks that act to maximise the evidence lower bound for the adversarial input. The robustness of these disentangled models is highly relevant because of the use-cases for VAEs highlighted above.</p><p>However, imposing additional disentangling constraints on a VAE training objective degrades the quality of resulting drawn or reconstructed images (Higgins et al., 2017a; Chen et al., 2018). We sought whether more powerful, expressive models, can help ameliorate this and in doing so built Under review as a conference paper at ICLR 2020 a hierarchical disentangled VAE, Seatbelt-VAE, drawing on works like Ladder VAEs (S&#248;nderby et al., 2016) and BIVA (Maal&#248;e et al., 2019). We demonstrate that Seatbelt-VAEs are more robust to adversarial attacks than &#946;-TCVAEs and &#946;-TCDLGMs (the latter a simple generalisation we make of &#946;-TC penalisation to hierarchical VAEs). See <xref ref-type="fig" rid="fig_0">Figure 1</xref> for a demonstration. Rather than being concerned with human-interpretable controlled generation by our models, which has been the focus of much research into disentangling, instead we are interested in the robustness afforded by disentangled representations.</p><p>Thus our key contributions are:</p><p>&#8226; A demonstration that &#946;-TCVAEs are significantly more robust to adversarial attacks via their latents than vanilla VAEs.</p><p>&#8226; The introduction of Seatbelt-VAE, a hierarchical version of the &#946;-TCVAE, designed to further increase robustness to various types of adversarial attack, while also giving better perceptual quality of reconstructions even when regularised.</p></sec><sec><title>VARIATIONAL AUTOENCODERS</title><p>Variational autoencoders (VAEs) are a deep extension of factor analysis suitable for high-dimensional data like images (Kingma &amp; Welling, 2013; Rezende et al., 2014). They have a joint distribution Under review as a conference paper at ICLR 2020 over data x and latent variables z: p &#952; (x, z) = p &#952; (x|z)p(z) where p(z) = N (0, I) and p &#952; (x|z) is an appropriate distribution given the form of the data, the parameters of which are represented by deep nets with parameters &#952;. As exact inference is intractable for this model, in a VAE we perform amortised stochastic variational inference. By introducing an approximate posterior distribution q &#966; (z|x) = N (&#181; &#966; (x), &#931; &#966; (x)), we can perform gradient ascent on the evidence lower bound (ELBO)</p><p>w.r.t.both &#952; and &#966; jointly, using the reparameterisation trick to take gradients through Monte Carlo samples from q &#966; (z|x).</p></sec><sec><title>DISENTANGLING VAES</title><p>In a &#946;-VAE (Higgins et al., 2017a), a free parameter &#946; multiplies the D KL term in L(x) above. This objective L &#946; (x) remains a lower bound on the evidence.</p><p>Decompositions of L(x) shed light on its meaning. As shown in Hoffman &amp; Johnson (2016); Makhzani et al. (2016); Kim &amp; Mnih (2018); Chen et al. (2018); Esmaeili et al. (2019), one can define the evidence lower bound not per data-point, but instead write it over a dataset D of size N , D = {x n }, so we have L(&#952;, &#966;, D). Esmaeili et al. (2019) gives a decomposition of this dataset-level evidence lower bound:</p><p>where under the assumption that p(z) factorises we can further decompose 4 :</p><p>is the empirical data distribution. q &#966; (z) := 1 N N n=1 q &#966; (z|x n ) is called the average encoding distribution following Hoffman &amp; Johnson (2016).</p><p>A is the total correlation (TC) for q &#966; (z), a generalisation of mutual information to multiple variables (Watanabe, 1960). With this mean-field p(z), Factor and &#946;-TCVAEs upweight this term, so we have an objective:</p><p>Chen et al. (2018) gives a differentiable, stochastic approximation to E q &#966; (z) log q &#966; (z), rendering this decomposition simple to use as a training objective using stochastic gradient descent. We also note that A , the total correlation, is also the objective in Independent Component Analysis (Bell &amp; Sejnowski, 1995; Roberts &amp; Everson, 2001).</p></sec><sec><title>HIERARCHICAL VAES</title><p>We now have a set of L layers of z variables: z = [z 1 , z 2 , ..., z L ]. The evidence lower bound for models of this form is:</p><p>The simplest VAE with a hierarchy of conditional stochastic variables in the generative model is the Deep Latent Gaussian Model (DLGM) of Rezende et al. (2014). The forward model factorises as a chain:</p><p>Under review as a conference paper at ICLR 2020 Each p &#952; (z i |z i+1 ) is a Gaussian distribution with mean and variance parameterised by deep nets. p(z L ) is a unit isotropic Gaussian.</p><p>We can understand this additional expressive power as coming from having a richer family of distributions for the likelihood over data x marginalising out all intermediate layers: p &#952; (x|z L ) = L&#8722;1 i=1 dz i p &#952; (x, z) is a non-Gaussian, highly flexible, distribution. To perform amortised variational inference one introduces a recognition network, which can be any directed acyclic graph where each node, each distribution over each z i , is Gaussian conditioned on its parents. This could be a chain, as in Rezende et al. (2014):</p><p>Again, marginalising out intermediate z i layers, we see q &#966; (z L |x) = L&#8722;1 i=1 dz i q &#966; (z|x) is a non-Gaussian, highly flexible, distribution.</p><p>However, training DLGMs is challenging: the latent variables furthest from the data can fail to learn anything informative (S&#248;nderby et al., 2016; Zhao et al., 2017). Due to the factorisation of q &#966; (z|x) and p &#952; (x, z) in a DLGM, it is possible for a single-layer VAE to train in isolation within a hierarchical model: each p &#952; (z i |z i+1 ) distribution can become a fixed distribution not depending on z i+1 such that each D KL divergence present in the objective between corresponding z i layers can still be driven to a local minima. Zhao et al. (2017) gives a proof of this separation for the case where the model is perfectly trained, i.e. D KL (q &#966; (z, x)||p &#952; (x, z)) = 0.</p><p>This is the hierarchical version of the collapse of z units in a single-layer VAE (Burda et al., 2016), but now the collapse is over entire layers z i . It is part of the motivation for the Ladder VAE (S&#248;nderby et al., 2016) and BIVA (Maal&#248;e et al., 2019). We propose novel hierarchical disentangled VAEs where we aim to disentangle only in the top-most latent variables z L . Following the Factor and &#946;-TCVAEs we upweight the term of the form of A for z L . Empirically we find models of this type are unable to converge when disentangling at the bottom most layer, or when disentangling at each layer. Intuitively, we want to capture high-level disentangled information at the top, but leave lower layers free to learn rich entangled representations. If p &#952; (x|z) = p &#952; (x|z 1 ), we obtain the generalisation of &#946;-TC penalisation to a DLGM and call it &#946;-TCDLGM. It suffers from the problems of collapse described above.</p><p>Inspired by BIVA (Maal&#248;e et al., 2019), we choose instead to condition our likelihood on all z i layers:</p><p>where j is indexing over the coordinates in z L . See Appendix for the derivation. We call this model Seatbelt-VAE, as with the extra conditional dependencies and nodes we increase the safety of our model to adversarial attacks, to noise, and to decreases in perceptual quality as &#946; increases. We find that using free-bits regularisation (Kingma et al., 2016) greatly ameliorates the optimisation challenges associated with DLGMs. For L = 1 this reduces to a &#946;-TCVAE, and for L &gt; 1, &#946; = 1 it produces a DLGM with our augmented likelihood function.</p><p>For completeness, note that for &#946;-TCDLGM:</p></sec><sec><title>MINIBATCH TRAINING</title><p>VAEs and derived models are commonly trained using stochastic gradient ascent on the ELBO, on minibatches of the training data. With the ELBO in Eq (9), this would be challenging because of the presence of average encoding distributions, which depend on the entire dataset.</p><p>To avoid having to handle large mixture distributions in our objective functions, we derive minibatch estimators that are a simple generalisation to disentangled hierarchical VAEs of the Minibatch Weighted Sampling estimator proposed in Chen et al. (2018) in the context of &#946;-TCVAEs. See Appendix for further details.</p></sec><sec><title>ROBUSTNESS OF VAES TO ADVERSARIAL ATTACKS</title><p>Most adversarial attack research has focused on discriminative models (Akhtar &amp; Mian, 2018; Gilmer et al., 2018) and recently VAEs have found use in protecting discriminative models against attack (Schott et al., 2019; Ghosh et al., 2019). Currently, two adversarial modes have been proposed for attacking VAEs (Tabacof et al., 2016; Gondim-Ribeiro et al., 2018; Kos et al., 2018). In both attack modes the adversary wants draws from the model to be close to a target image x t , when given a distorted image x * = x + d as input. When attacking a discriminative model the aim is to manipulate the comparatively low-dimensional output layer of the network, commonly aiming with the attack to diminish or increase only a handful of the output units. However, for a generative model, the attacker is aiming to change a large number of pixel values in the output, changing the content of the reconstruction. Intuitively this is a harder task, and the attacks proposed in the above papers do not always result in adversarial examples that are very close to the initial image in appearance.</p><p>The first mode of attack, which we call the output attack, aims to reward draws from the decoder conditioned on z &#8764; q &#966; (z|x * ) that are close to x t via the ELBO.</p><p>For a vanilla VAE, this attack's adversarial objective is:</p><p>The second mode of attack, the latent attack, aims to find x * = x + d such that q &#966; (z|x * ) &#8776; q &#966; (z|x t ) under some similarity measure r(&#183;, &#183;), which implicitly means that the likelihood p &#952; (x t |z) is high when conditioned on draws from the posterior of the adversarial example. This attack is important if Under review as a conference paper at ICLR 2020 one is concerned with using the encoder network of a VAE as part of downstream task. For a single stochastic layer VAE, the latent-space adversarial objective is:</p><p>Note that both modes of attack penalise the L 2 norm of d, prioritising smaller distortions. We denote samples from q &#966; (z|x + d) asz.</p><p>For Tabacof et al. (2016); Gondim-Ribeiro et al. (2018) r(&#183;, &#183;) is D KL (q &#966; (z|x + d)||q &#966; (z|x)) and for Kos et al. (2018) it is the L 2 distance ||z &#8722; z * || 2 ,z &#8764; q &#966; (z|x + d), z * &#8764; q &#966; (z|x) between draws from the corresponding posteriors or ||&#181; &#966; (x) &#8722; &#181; &#966; (x + d)|| 2 between their means. We follow the former papers and use the D KL formulation. All three papers find that the latent attack mode is as or more effective than the output attack for single layer VAEs both under perceptual evaluation and various proposed metrics (Tabacof et al., 2016; Gondim-Ribeiro et al., 2018; Kos et al., 2018). For latent attacks, the choice of which layers to attack depends on model architecture. For DLGMs and &#946;-TCDLGMs the attacker only needs to match at the bottom latent layer as p &#952; (x|z) = p &#952; (x|z 1 ), see Eq (7). See Appendix for plots showing how effective this attack is regardless of &#946; and L. Even though the decoder is conditioned on all latent layers, one could choose to attack individual layers for Seatbelt-VAE. For example, one could attack just the first layer z 1 . If one were able to find a perfect latent-space attack in z 1 , D KL (q &#966; (z 1 |x + d)||q &#966; (z 1 |x t )) = 0, then the variational posteriors in higher layers would also be well matched. Attacks that do not perfectly match the target z 1 may have their mismatch with the target posterior amplified in higher layers. In Seatbelt-VAE the likelihood over data is conditioned on all z layers, being off-target in these higher layers matters. In the Appendix we show that targeting the top or base layers individually is not as effective as attacking all layers. Hence:</p></sec><sec><title>EXPERIMENTS</title><p>Here we perform four tranches of experiments. Firstly, we demonstrate that the reconstructions given by Seatbelt-VAEs (and &#946;-TCDLGMs) degrade much less strongly as &#946; is increased than in &#946;-TCVAEs. Secondly, we perform a variety of adversarial attacks on all models. We demonstrate that increasing &#946; makes &#946;-TCVAEs more robust to adversarial attacks than vanilla VAEs, and that Seatbelt-VAEs are more robust still. Thirdly, we show that these disentangled models are most robust than vanilla VAEs to unstructured noise distorting their inputs, with Seatbelt-VAEs again the most robust. Finally, we study the effect of disentangling on the sparsity of model weights.</p><p>We perform these experiments on Chairs (Aubry et al., 2014), 3D faces (Paysan et al., 2009), and CelebA (Liu et al., 2015). Additional results for dSprites (Higgins et al., 2017a) can be found in the Appendix. We used the same encoder and decoder architectures as Chen et al. (2018) for each dataset. For the details of neural network architectures and training, see Appendix and accompanying code. To show the degree to which our models are disentangling, the Appendix also contains the Mutual Information Gap (MIG) (Chen et al., 2018) at the top layer of each model. Though our models obtain high MIG at z L , this does not imply that decoding from latent traversals in z L will result in the generation of images with human-interpretable factors of variation. This is made abundantly clear in the latent space traversal plots, also shown in the Appendix. As such, we do not believe existing disentangling metrics directly apply to hierarchical models.</p></sec><sec><title>ELBO AND RECONSTRUCTION QUALITY: &#946;-TCVAES TO SEATBELT-VAES</title><p>We trained &#946;-TCVAEs, &#946;-TCDLGMs, and Seatbelt-VAEs for a range of &#946; penalisations. In <xref ref-type="fig" rid="fig_2">Figure 3</xref> we plot the final ELBO of our trained models, but calculated without the additional &#946; penalisation that was applied during training. The ELBO for &#946;-TCVAE [Eq (4)] declines with &#946; much more quickly than Seatbelt VAEs [Eq (10)] or &#946;-TCDLGMs [Eq (11)]. In the Appendix we also show that increasing &#946; reduces D KL collapse. This is interesting, as it shows that we can increase the &#946; Under review as a conference paper at ICLR 2020 penalisation for Seatbelt-VAEs, without a large degradation in the quality of the model as measured by the ELBO.</p><p>In <xref ref-type="fig" rid="fig_3">Figure 4</xref> we see the effect of depth and disentangling on reconstructions of CelebA. The bottom row, showing the reconstructions from a Seatbelt-VAE with L = 4 and &#946; = 20 clearly maintains facial identity better than those from a &#946;-TCVAE in the middle row. The effect is clearest for the 3 rd , 4 th and 7 th columns, where many of the individuals' finer facial features are lost by the &#946;-TCVAE but maintained by the Seatbelt-VAE. This fits with the results in <xref ref-type="fig" rid="fig_2">Figure 3</xref>, and shows that resistance of the quality of the reconstructions of Seatbelt to increasing &#946; is visually perceptible as well as measurable.</p></sec><sec><title>ADVERSARIAL ATTACK</title><p>We apply attacks minimising each of &#8710; output and &#8710; latent on: vanilla VAEs, &#946;-TCVAEs, &#946;- TCDLGMs and Seatbelt-VAEs; trained on: Chairs (Aubry et al., 2014), 3D faces (Paysan et al., 2009), and CelebA (Liu et al., 2015); for a range of &#946;, L and &#955; values.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>We randomly sampled 10 input-target pairs for each dataset. As in Tabacof et al. (2016); Gondim- Ribeiro et al. (2018), for each pair of images used &#955; takes 50 geometrically-distributed values from 2 &#8722;20 to 2 20 . Thus each model undergoes 500 attacks for each attack mode. Like Tabacof et al. (2016); Gondim-Ribeiro et al. (2018), we used L-BFGS-B for gradient descent (Byrd et al., 1995), We prefer to avoid classifier based metrics (Kos et al., 2018) as in general we think that such analysis can be hard to interpret given the many available choices of classifier. Instead, we evaluate the effectiveness of adversarial attacks from the values reached by &#8722; log p &#952; (x t |z), by the attack objectives {&#8710; output , &#8710; latent } and by visually appraising the adversarial input (x + d) and the adversarial reconstruction. Note that higher values of &#8722; log p &#952; (x t |z), &#8710; output , &#8710; latent indicate less effective attacks. <xref ref-type="fig" rid="fig_0">Figure 1</xref> shows latent space attacks and demonstrates that they are less effective on disentangled models. As in Gondim-Ribeiro et al. (2018), we are showing the attack for the &#955; that gives us an attack objective just better than the average objective over all attacks tried. Note that for Seatbelt-VAEs, for high values of &#946; and L latent attacks often result in the outputs from adversarial attack resembling the original inputs. See Appendix for more examples of the attacks for {&#8710; latent , &#8710; output } for the models trained on dSprites (a toy dataset for disentangling), Chairs, 3D Faces and CelebA; each over a range of values for &#946;, L, and &#955;. Note that we rarely observe perceptually effective output attacks regardless of model or settings, though vanilla VAEs are the most susceptible.</p><p>One might expect that adversarial attacks targeting a single factor of the data would be easier for the attacker. However, we find that disentangled models protect effectively against these attacks as well. See the Appendix for plots showing an attacker attempting to rotate a dSprites heart. <xref ref-type="fig" rid="fig_4">Figure 5</xref> quantitatively shows that &#946;-TCVAEs become harder to attack as &#946; increases. The values of &#8710; latent for &#946;-TCVAEs are &#8776; 10 3 times higher than for a standard VAE on Chairs, and still greater than a factor of 10 for 3D faces. &#8710; output attack is also less effective, by a smaller factor &#8776; 1.2. <xref ref-type="fig" rid="fig_5">Figure 6</xref> shows &#8722; log p &#952; (x t |z latent/output ) and <xref ref-type="fig" rid="fig_6">Figure 7</xref> shows &#8710; latent/output over a range of datasets for &#946;-TCDLGMs and Seatbelt-VAEs, varying L and &#946;. Larger values of these metrics correspond to less successful adversarial attacks. Generally, &#946;-TCDLGMs are very sensitive to latent attack, as we expect. Like &#946;-TCVAEs, Seatbelt-VAEs offer significant protection to latent attacks, and somewhat increased protection to output attacks compared to vanilla VAEs. For Seatbelt-VAEs, as we go to the largest values of &#946; and L for both Chairs and 3D Faces, &#8710; latent grows by a factor of &#8776; 10 7 .</p><p>The bottom rows of <xref ref-type="fig" rid="fig_5">Figures 6</xref> &amp; 7 (c) (d) have L = 1, and thus correspond to &#946;-TCVAEs. They contain relatively low values of the adversarial objectives compared to L &gt; 1. Similarly the first column, corresponding to &#946;=1 models, contains relatively low values. These results tell us that depth and disentangling together offer the most effective protection from the adversarial attacks studied.</p><p>In the Appendix we also calculate the L 2 distance between target images and adversarial outputs and show that the loss of effectiveness of adversarial attacks is not due to the degradation of reconstruction quality from increasing &#946;. By these metrics too Seatbelt-VAEs outperform other models.</p></sec><sec><title>ROBUSTNESS TO NOISE</title><p>In addition to studying the robustness of these models to highly structured distortion, we can also consider robustness to random noise. We add &#8764; N (0, I) to the datasets, which are scaled to &#8722;1 &#8804; x &#8804; 1, and then evaluate E q &#966; (z|x+ ) p &#952; (x|z * ), where z * corresponds to the encoder embedding of x + and x is the original (non-noisy) data. See <xref ref-type="fig" rid="fig_7">Figure 8</xref> for smoothed histogram plots of this for different models for different degrees of &#946;. Both &#946;-TC and Seatbelt-VAEs are effectively denoising autoencoders. They become more robust to noise with increasing &#946;, while &#946;-TCDLGMs get worse. See Appendix for plots showing the robustness of these models to smaller magnitude noise. Some of the robustness of disentangled models to adversarial attacks may be conferred by their robustness to random perturbations of their inputs.</p></sec><sec><title>TOTAL CORRELATION PENALISATION AS REGULARISATION</title><p>In the auto-encoder view of these models, the D KL terms in L(&#952;, &#966;, D) are associated with a form of regularisation of the model (Doersch, 2016). Recent work shows that for linear autoencoders, Under review as a conference paper at ICLR 2020 L 2 regularisation of the weights corresponds to orthogonality of the latent projections (Kunin et al., 2019). For deep models we expect that disentangling is associated with regularised decoders and more complex encoders. The decoder receives a simpler representation, but building this representation requires more calculation. Here we measure the L 2 norm of the weights of our networks as a function of &#946;, shown in <xref ref-type="table" rid="tab_0">Table 1</xref>. See Appendix for results for &#946;-TCDLGM.</p><p>As we increase &#946; for &#946;-TCVAEs and Seatbelt-VAEs for Chairs, 3D Faces, and CelebA the L 2 norm increases for the encoder and decreases for the decoder. A more complex encoder is more difficult to match in the latent space and regularised decoders may be contributing to the denoising properties seen in <xref ref-type="fig" rid="fig_7">Figure 8</xref>. That the changes are generally greater for &#946;-TCVAE than Seatbelt-VAE makes sense, as the encoder and decoder of the former interact directly with the disentangled representation. For the latter the decoder receives inputs from all z i , of varying degrees of disentanglement.</p></sec><sec><title>CONCLUSION</title><p>We have presented the increases in robustness to adversarial attack afforded by &#946;-TCVAEs. This increase in robustness is strongest for attacks via the latent space. While disentangled models are often motivated by their ability to provide interpretable conditional generation, many use cases for VAEs centre on the learnt latent representation of data. Given the use of these representations as inputs for other tasks, the latent attack mode is the most important to protect against.</p><p>Recent work by Shamir et al. (2019) gives a constructive proof for the existence of adversarial inputs for deep neural network classifiers with small Hamming distances. The proof holds with deterministic defence procedures that work as additional deterministic layers of the networks, and in the presence of adversarial training (Szegedy et al., 2014; Ganin et al., 2016; Tram&#232;r et al., 2018; Shaham et al., 2018). Shamir et al. (2019) thus give a theoretical grounding for using stochastic methods to defend against adversarial inputs. As VAEs are already used to defend deep net classifiers (Schott et al., 2019; Ghosh et al., 2019), more robust VAEs, like &#946;-TCVAEs, could find use in this area.</p><p>We introduce Seatbelt-VAE, a particular hierarchical VAE disentangled on the top-most layer with skip connections down to the decoder. This model further increases robustness to adversarial attacks, while also increasing the quality of reconstructions. The performance of our model under adversarial attack to robustness is mirrored in robustness to uncorrelated noise: these models are effective denoising autoencoders as well. We hope this work stimulates further interest in defending and attacking VAEs.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Latent-space adversarial attacks on Chairs, 3D Faces and CelebA for different models, including our proposed Seatbelt-VAE. &#946; = 10 for &#946;-TCVAE, &#946;-TCDLGM and Seatbelt-VAE. L is the number of stochastic layers. Clockwise within each plot we show the initial input, its reconstruction, the adversarial input, the adversarial distortion added to make it (shown normalised), the adversarial input's reconstruction, and the target image. Following Tabacof et al. (2016); Gondim-Ribeiro et al. (2018) we attack with different degrees of penalisation on the magnitude of the adversarial distortion; in choosing the distortion to show, we pick the one with the penalisation that resulted in the value of the attack objective just better than the mean. See Section 5 for more details.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>L = 2 Seatbelt-VAE. Shaded lines indicate &#946;-TC fac- torisation in a given node.</p></caption><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Plots showing the effect of varying &#946; under various datasets on the ELBO of &#946;-TCVAEs, &#946;-TCDLGMs and Seatbelt-VAEs [Eqs (4), (11) and (10) respectively]. Shading corresponds to the 95% CI over variation due to variation of ||z|| and L.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Top row shows CelebA input data. Below are reconstructions from &#946;-TCVAE, &#946; = 20 and then Seatbelt VAE, L = 4, &#946; = 20.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5: &#8710;</label><caption><title>Figure 5: &#8710;</title><p>latent/output for (a) Chairs (b) 3D Faces, for &#946;-TCVAE for different &#946; values. Shading corresponds to the 95% CI over variation due to our stable of images and our values of ||z|| and &#955;.</p></caption><graphic /></fig><fig id="fig_5"><object-id>fig_5</object-id><label>Figure 6: &#8722;</label><caption><title>Figure 6: &#8722;</title><p>log p &#952; (x t |z) for (a) (b) &#946;-TCDLGMs and (c) (d) Seatbelt-VAEs for Chairs and 3D Faces; over &#946; and L (total number of stochastic layers) values and for latent and output attacks. Larger values of &#8722; log p &#952; (x t |z) correspond to less successful adversarial attacks.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_6"><object-id>fig_6</object-id><label>Figure 7: {&#8710;</label><caption><title>Figure 7: {&#8710;</title><p>latent , &#8710; output } for (a) (b) &#946;-TCDLGMs and (c) (d) Seatbelt-VAEs for Chairs and 3D Faces; under varying &#946; and L (total number of stochastic layers) values.</p></caption><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_7"><object-id>fig_7</object-id><label>Figure 8:</label><caption><title>Figure 8:</title><p>Robustness of log p &#952; (x|z) to Gaussian noise &#8764; N (0, 1) scaled by different magnitudes and added to x on CelebA; for &#946;-TCVAE, &#946;-TCDLGM, Seatbelt-VAE; &#946; = 0, 10 Best viewed digitally.</p></caption><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Relative change of the L 2 of Encoders and Decoders by dataset for &#946;-TCVAE and Seatbelt- VAE (L = 4) when increasing &#946; from 1 to 10.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap></sec></body><back /></article>