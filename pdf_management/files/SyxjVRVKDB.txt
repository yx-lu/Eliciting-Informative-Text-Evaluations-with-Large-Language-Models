Title:
```
STATE SENSITIVITY FOR DEEP NEURAL NETWORK IN- TERPRETABILITY
```
Abstract:
```
We introduce switched linear projections for expressing the activity of a neuron in a ReLU-based deep neural network in terms of a single linear projection in the input space. The method works by isolating the active subnetwork, a series of lin- ear transformations, that completely determine the entire computation of the deep network for a given input instance. We also propose that for interpretability it is instructive and meaningful to focus on the patterns that deactive the neurons in the network, which are ignored by the exisiting methods that implicitly track only the active aspect of the network's computation. We introduce a novel interpretabil- ity method for the inactive state sensitivity (Insens). Comparison against existing methods shows that Insens is robust (in the presence of noise), complete (in terms of patterns that affect the computation) and a very effective interpretability method for deep neural networks.
```

Figures/Tables Captions:
```
Figure 1: Let's assume that for a particular input [x 1 x 2 ] going into the ReLU network shown in (a) the white neurons are inactive; then, for this particular input, the network from (a) is equivalent to network in (b) where the inactive neurons are treated as dead and the active ones operate in the linear part of their ReLU activation fuction; which makes both of these networks equivalent to one in (c); the grey hidden neurons form the active subnetwork.
Figure 2: The heatmaps of w of arbitrary neurons from 2 different MNIST inputs (first column), in the second convolutional layer (next 5 columns) and the first fully connected layer (last 5 columns) of the 2CONV neural network; normalised intensity of red and blue in each heatmap corresponds to the magnitude of the positive and negative value (with white indicating 0); the activity v = x w T + b and the switched bias b are shown above the heatmap of each neuron.
Figure 3: Graphical representation of the concept of a neuron's centre c in a 2D scenario, where input vector is x = [x 1 x 2 ]; the diagram shows input-space coordinate system - the neuron-specific one would have its origin at c.
Figure 4: Visualisations for a set of interpretability methods of a single 2CONV neural network trained on the clean MNIST data in response to clean as well as noisy input; top-left shows clean MNIST, top-right noisy bordered MNIST, bottom-left noisy background MNIST and bottom-right noisy framed MNIST; the intensity of red and blue correspond respectively to the magnitude of the positive and negative values in the heatmaps.
Figure 5: Visualisations for a set of interpretability methods on two 2CONV neural networks - one trained on the smallNORB dataset (left) and the other on the CIFAR10 dataset (right); the intensity of red and blue correspond respectively to the magnitude of the positive and negative values in the heatmaps.
Figure 6: Visualisations for Insens Ω (x) l for layers l=1, 3, and 5 of the 2CONV network trained on smallNORB dataset (left) and CIFAR10 datasets (right); the intensity of red and blue correspond respectively to the magnitude of the positive and negative values in the heatmaps.
Figure 7: Mean spearman rank-order correlation between visualisations derived from a random sam- ple of CIFAR-10 input for different methods including Insens Ω x L−1 between trained and randomly initialised 2CONV network (first left), between trained and random label CIFAR-10 trained 2CONV network (second left); the same correlation is shown over Insens based on different layers of the 2- CONV network (two figures on the right); Insens correlations are shown in red; vertical black lines show the variance.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION It is notoriously hard to interpret how deep networks accomplish the tasks for which they are trained. At the same time, due to the pervasiveness of deep learning in numerous aspects of computing, it is increasingly important to gain understanding of how they work. There are risks associated with the possibility that a neural network might not be "looking" at the "right" patterns ( Nguyen et al.; Geirhos et al., 2019 ), as well as opportunities to learn from the network's capable of better than human performance ( Sadler & Regan, 2019 ). Hence, there is ongoing effort to improve the interpretation and interpretability of the internal representation of neural networks. What makes this interpretation of the inside of a neural network hard is the high dimensionality and the distributed nature of its internal computation. Aside from the first hidden layer, neurons operate in an abstract high-dimensional space. If that was not hard enough, the analysis of individual components of the network (such as activity of individual neurons) is rarely instructive, since it is the intricate relationships and interplay of those components that contain the "secret sauce". The two broad approaches to dealing with this complexity is to either use simpler interpretable models to approximate what a neural network does, or to trace back the elements of the computation into the input space in order to make the internal dynamics relatable to the input. In the latter approach we are typically interested in neurons' sensitivity - how the changes in network input affect their output, and decomposition - how different components of the input contribute to the output. In this paper we propose a straightforward and elegant method for expressing the computation of an arbitrary neuron's activity to a single linear projection in the input space. This projection consists of a switched weight vector and a switched bias that easily lend themselves to sensitivity analysis (analogous to gradient-based sensitivity) and decomposition of the internal computation. We also in- troduce a new approach for interpretability analysis, called inactive state sensitivity (Insens), which uses switched linear projections to aggregate the contribution of patterns in the input that deactivate neurons in the network. We demonstrate on several networks and image-based datasets that Insens provides a comprehensive picture of a deep network's internal computation. The only constraint for the proposed methods is that the network must use ReLU activation functions for its hidden neurons.

Section Title: RELATED WORK
  RELATED WORK Previous work on deep learning interpretability is extensive with a wide variety of methods and approaches -  Simonyan et al. (2014) ; Zeiler & Fergus;  Bach et al. (2015) ; Mahendran & Vedaldi;  Montavon et al. (2017) ;  Sundararajan et al. (2017) ;  Zhou et al. (2019)  being just a selection of the most prominent efforts in this area. Our work on the single linear projection follows the approach akin to  Lee et al. and Erhan et al. (2009) , where the objective was to interpret the computation performed by an arbitrary neuron for a particular input vector as a projection in the input space. However, whereas these previous attempts were based on Deep Belief Nets ( Hinton et al., 2006 ) and required an approximation of the said projection, our method is a forward computation that gives the neuron's activity in terms of a linear projection in the input space. It works for any neural network, including convolutional ones, as long as all hidden neurons use piecewise linear activation functions. All existing methods for interpretability of deep learning, due to the nature of ReLU computation, necessarily provide information only about the active subnetwork of the ReLU-based architecture. Our own observations, as well as other evidence showing that in practice neural networks produce a relatively low number of activation regions ( Hanin & Rolnick, 2019 ), lead us to the hypothesis that the analysis of the patterns in the input that switch neurons off gives an excellent picture of a net- work's sensitivity. We also take the view that too much interpretation in interpretability introduces the risk of showing us what we expect to see and not what the network is actually focussing on. For instance, in Deep Taylor Decomposition ( Montavon et al., 2017 ) choices of different root-points for the decomposition of the relevance function lead to different rules for Layerwise Relevance Propaga- tion (LRP)( Bach et al., 2015 ), which can lead to different interpretations of what is important in the input. The LRP-α 1 β 0 rule, for example, emphasises the computation over the positive weights in the network while discounting the relevance of the information passing through the negative weights. This rule is justified by assumptions about desired properties of the explanation, but this comes with a risk of confirmation bias. Insens is an attempt to take into account the patterns in the input that cause the neurons inside the network to produce zero output. The information related to the inactive network may seem irrelevant, since inactive neurons do not directly contribute to the computation of the overall output. However, there is something in the input that switches a particular set of neurons off, thus regulating the active computation, and as we show in this paper, this something carries a lot of meaningful information.

Section Title: SWITCHED LINEAR PROJECTIONS
  SWITCHED LINEAR PROJECTIONS The basis of the switched network concept is the fact that neurons that produce output of zero do not contribute to the computation of the overall output of the network. The notion of dead neurons, that is neurons that always output zero, is not new, nor is the realisation that these neurons, along with their connecting weights, can be taken out the network without any impact on the computation. In a switched projection, we treat the zero-output neurons as temporarily dead for a given instance of input. We refer to these neurons as inactive, since they may become active for a different network input. Thus we isolate the subnetwork of the active neurons in a given computation. As it happens, Under review as a conference paper at ICLR 2020 for a ReLU neuron the active neurons are those that pass their activity, the weighted sum of its inputs plus bias, directly to its output 1 . This means that a subnetwork of active ReLU neurons is just a series of linear transformations, which is equivalent to a single linear transformation. As a result, we can express the computation performed by any neuron in a ReLU network as a projection onto a switched weight vector in the input space plus the switched bias. The term switched indicates that this weight and bias vector changes when the state of the network changes, the state corresponding to the particular combination of the active and inactive neurons in the network.  Figure 1  illustrates the concept graphically, and a formal description is given in the following proposition: Proposition 1 (Switched linear projections). Let x ∈ R d be a vector of inputs, w li ∈ R U l−1 the weight vector, and b li ∈ R the bias of neuron i in layer l (with U l−1 inputs from the previous layer). Let the activity of a neuron i in layer l be defined as: where W l = w T l1 . . . w T lU l , T denotes transpose, b l = [b l1 . . . b lU l ] and σ r (v) = max(v, 0) is the ReLU activation function. If we define an input-dependent state of the network as The proof is provided in Appendix A. Note that the ReLU derivative,σ r (v), is just a convenient definition for a step function, so thaṫ To simplify the notation, whenever referring to the parameters of the switched projection w, b, as well as activity v, we will drop the explicit dependency on x. While  Figure 1  illustrates the switching concepts on a small fully connected network, switched linear projections can be computed for networks with convolutional as well as pooling layers. A convolutional layer is just a special case of a fully connected layer with many weights being zero and groups of neurons constrained to share the weight values on their connections. For max pooling, the neurons that do not win the competition, and thus their output does not affect the computation from then on, are deemed to be inactive regardless of the output they produce.

Section Title: SENSITIVITY
  SENSITIVITY Equation 2 makes it obvious that a given neuron's switched weight vector is just the derivative of its activity with respect to the network input, w = ∂v(x) x . Thus, the switched weight vector is analogous to gradient-based sensitivity analysis.  Figure 2  shows the heatmaps of the switched weight sensitivity for the same set of hidden neurons with different inputs from the MNIST-trained 2CONV neural network (for details on the network architecture featured in this paper see Appendix B). In this visualisation we show normalised w, with intensity of red corresponding to the larger positive value, and the intensity of blue the negative value. The neurons were chosen from the 2nd convolutional layer and the penultimate fully connected layer respectively such that for the four Under review as a conference paper at ICLR 2020 considered inputs some neurons were always active, some inactive, and others sometimes active and sometimes not.  Figure 2  makes it clear that a given neuron is not necessarily sensitive to the same pattern for different network inputs - this is most evident in the sensitivity of the neurons of the fully connected layer. Also note that some neurons in the convolutional layers are active despite the fact that they only "see" the part of the input image that is "empty" (all pixels are black). This leads us to the conclusion that a given neuron is not necessarily a detector of a particular pattern in the input space, which is often the underlying assumption of the existing interpretability techniques. Something also apparent in our switched projection analysis, though not evident from  Figure 2 , is that for a given input most of the neurons in the network are inactive. On average, only 17% of the neurons were active for a given MNIST input in this architecture. The fact that only a subset of neurons are active in a given computation is not a quirk of one specific network, as observed by ( Hanin & Rolnick, 2019 ). Switched linear projections give us an interpretation of a deep network as a set of linear, input-dependent, transformations. Something about the input activates a subset of neurons, but also keeps all the remaining neurons inactive. Traditional sensitivity analysis, as well as the one shown in  Figure 2 , shows a direction (or magnitude) of the gradient of the input that would increase neurons activity provided the same state of the network remained unchanged. However, often in these networks the state does change even after small perturbations of the input, often resulting in same classification, but different input gradient. We reason that the analysis of the state, including information about what makes the neurons inactive, provides a missing and likely very important aspect, to interpretability than analysis of the active subnetwork alone.

Section Title: DECOMPOSITION
  DECOMPOSITION Switched linear projections can decompose the activity of a neuron into contributions from its input, in our case the pixels. For this we propose another re-interpretation of the computation of the output that will allow us to distribute the bias over the attributes of the input vector. Note that for a linear projection v = xw T + b = (x − c)w T , (4) where c = x − v ww T w, b = −cw T , and c ∈ R d . The vector c can be thought of as a translation of the coordinate system to a neuron-centered one, where w goes through the origin at c.  Montavon et al. (2017)  call this vector the nearest root point, but we will refer to it as the neuron's centre.  Figure 3  provides a geometric interpretation of c in a simple 2D scenario. Since c ∈ R d , we can break down the computation of v such that v = d j=1 (x j − c j )w j , (5) and take (x j − c j )w j to be the contribution of the input component j (in our examples, a single pixel) to the computation of neuron's activity. Since the switched linear projection is equivalent to Under review as a conference paper at ICLR 2020 weights and bias of a single neuron, we can compute the switched centre The switched centre c is related to the concept of reference in DeepLIFT ( Shrikumar et al., 2017 ) that gets subtracted from the input in order to extract a pattern of interest. However, whereas in DeepLIFT the reference is external to the model, and used for examination of perturbations it induces in the output, our proposed centre is a component of the actual computation of the network's output; one could say, c is a given neuron's inherent reference. Visualisations based on decomposition of active neurons do not offer anything more than existing methods. However, the concept of neuron's centre will be used in the method we propose next, for interpretability based on the sensitivity of the inactive network.

Section Title: INACTIVE STATE SENSITIVITY
  INACTIVE STATE SENSITIVITY Since a switched linear projection can be found for any neuron in the network, it can just as easily relate what it is about the input that drives a neuron into the negative just as well as positive. Tracking the patterns in the input that make the neuron more inactive tells us about the aspects of the input that would ensure the stability of the network's state. The bigger the magnitude of the negative activity, the less likely the inactive neurons are to switch on, and thus change the switched projection. Some of the inactive neurons are closer and others further away from the point where they would activate. Hence, we propose a definition of inactive sensitivity based on switched linear projection that takes the magnitude of activity into account, The difference x − c i can be thought of as the component of the input that projects onto a neuron's switched weight vector w i in the coordinate system centred at c i . Note that ω i is still a vector in the direction of w i , and so it is a measure of a neuron's sensitivity, but its magnitude is proportional to the absolute value of activity. In an attempt to capture the information about the state of the network, we propose averaging the sensitivity of all inactive neurons in a given layer of the network: Ω (x) l = i∈I l m i N ω i , (8) where I l is a set of all inactive neurons, v li ≤ 0 in layer l of the network (excluding the output neurons), and c i is the switched centre of neuron i; m i is the number of times neuron i is on when the network is evaluated over the set of N training samples. We refer to Ω (x) l ∈ R d as the inactive state sensitivity (Insens) of the network with respect to the input x. The m i /N factor weights the importance of a neuron based on its activity over the training set; a neuron that is never on, m i = 0, might represent an arbitrary pattern that has no useful information, while the neuron that is usually on, but not for the input for which Ω (x) l ∈ R d is computed, is taken to carry more weight.

Section Title: EVALUATION
  EVALUATION We evaluated Insens interpretability against plain gradient sensitivity, Deep Taylor decomposition ( Montavon et al., 2017 ), Integrated gradients ( Sundararajan et al., 2017 ) and Layerwise relevance propagation ( Bach et al., 2015 ) as implemented by the iNNvestigate toolbox ( Alber et al., 2018 ). For visualisations of the Insens-based interpretability we simply show Ω (x) l as a heatmap, with intensity of the red pixels relating the magnitude of its positive components, and the intensity of blue the magnitude of the negative component of the vector. Since the individual ω i gives a weighted gradient with respect to the input, and thus the change vector that would make neuron i less prone to become active, we take the average Ω (x) l ∈ R d to be indicative of the pattern in the input that is related to the stability of the network's state in layaer l induced by x. In all evaluations we examine visualisation of a 2CONV neural network (described in Appendix B) First, we examine visualisations of Ω (x) L−1 ∈ R d , that is the inactive sensitivity over the penultimate layer of the network trained on the MNIST dataset ( Lecun et al., 1998 ). We reason (and confirm below with saliency checks) that the last layer before the softmax output provides most information related to network's decision making out of all the hidden layers.  Figure 4  shows visualisations from existing methods against the Insens heatmaps for different instances of the input and the same network. In the first instance we use clean MNIST input (on which test accuracy of 2CONV is over 99%). Note that Insens visualisation shows something that other methods hint at but do not show explicitly - that the network is sensitive to the black and white contrast of the digit and its outline. The red and blue areas in the Insens heatmaps suggest that respectively lightening and darkening these regions would make the current network state more stable (though in this case it is not possible to make black pixels darker nor white pixels any lighter). To verify the information provided by Insens, we added different types of noise to the images, also shown in  Figure 4 , and used them as input to the same network. We wanted to confirm that adding noise to the background, but not the digit outline, as suggested by Insens, does not impact performance. Indeed, testing with images that contain random gaussian noise in the background, but not within the 3-pixel outline of the digit ( Figure 4  top-right) still gives 87% accuracy over 10000 test images. If we use images with random gaussian background everywhere (except the digit itself) or images with the same number of clean pixels as in the 3-pixel outline, but located around the frame of the image ( Figure 4  bottom-left and bottom-right), the accuracy drops to 32% and 45% respectively. This confirms that Insens patterns for the penultimate layer provide meaningful information about the network's sensitivity. Also note that the other methods tend to lose the patterns of the digits in the visualisations over noisy images, whereas Insens still shows the digits, to which the network is still sensitive, albeit also being affected by the noise. In  Figure 5  we show evaluations of Ω (x) L−1 -based Insens and other methods for two additional 2CONV networks - one trained on smallNORB (LeCun et al.) and the other on CIFAR10 dataset ( Krizhevsky, 2009 ) (for details of the training see Appendix B). Insens visualisation for the penul- timate layer of smallNORB and CIFAR10 datasets might not seem all that different from other methods. However, with Insens we can examine the network state of a specific layer in the network, obtaining information about what the network pays attention to at different stages of its computation. In  Figure 6  we show Insens visualisation for two convolutional layers (l = 1 and l = 3) as well as the penultimate, fully connected layer (l = 5) of the 2CONV network trained on smallNORB and CIFAR10. There are no Insens visualisations for the max-pooling layers, as these will contain only either active, or the arbitrary inactive chosen neurons by the max-pooling operation. Finally, to provide an objective measure of the quality of Insens visualisations, we perform sanity and saliency checks as prescribed by  Adebayo et al. (2018) . In these tests we measure correlation between interpretability visualisations for different networks; in each instance a 2CONV network trained on a given dataset against an untrained (randomly initialised) network, and in the second instance the same dataset trained network against a network trained on identical input with ran- domly shuffled target labels. High correlation between visualisations from the same input between the trained and either randomly initialised or random label trained network suggests that an inter- pretability method is model agnostic, most likely showing an echo of the input. In  Figure 7  we show average Spearman rank-order correlation coefficients between visualisations from 20 instances of independently trained 2CONV networks, for each using 100 randomly chosen images from the CI- FAR10 dataset. In the first instance, we check the correlation of the penultimate layer-based Insens against other methods. Insens correlation is low, on par with the Gradient and Integrated Gradi- Under review as a conference paper at ICLR 2020 ent methods. Note that the correlations between visualisation of the DeepTaylor method, which arguably provides the most visually striking visualisations, are very high, suggesting that most of the provided information is independent of the network's state. In the second instance we examine the correlations over Insens from different layers of the 2CONV architecture. It is very obvious that this correlation increases the closer the layer is to the input. We believe this is not an artefact of the Insens method, but an indication of the initial passing and then gradual filtering of information passing through the network.

Section Title: CONCLUSION
  CONCLUSION The switched linear projection is an interpretation of the computation inside a ReLU network that distinguishes between the active and inactive parts of the deep neural network architecture. The active subnetwork tends to be a smaller subset of the deep network (see Appendix B.1 for details) and the linear projection it provides is somewhat arbitrary, in the sense that it does not matter what the orientation of the switched weight vector is, as long as it produces the desired output. Hence interpretability analysis based on the active subnetwork may not give the full picture of the patterns from the input that the network relies on in order to produce its computation. Inactive state sensitivity (Insens), the proposed method for interpretability of ReLU networks, ag- gregates weighted sensitivity over a layer of inactive neurons. This sensitivity relates the gradient of the input that would potentially drive the activity of the inactive neurons further away from zero, thus corresponding to the patterns in a particular input that would keep the state of the particular layer stable and the output decision the same. As the name implies the method isolates the patterns in the input to which the network, in a sense, is insensitive. Our evaluations show that these patterns give a comprehensive picture of what and at what point of the computation the network reacts to patterns in a given input. It is worth noting that it is possible to extend Insens by summing over all, active and inactive, neurons in Equation 8. In our experiments we found that inclusion of active neurons did not change the visualisations in a significant way, and we reasoned that the effect of these neurons is already accounted for in the computation of the switched linear projections on which the Insens evaluation is based on. However, it might be worth investigating in the future whether there are circumstances, datasets, or types of problems where inclusion of active state affects the sensitivity visualisations. Since switched linear projections are just an interpretation of the computation inside a neural net- work, they may also become a useful tool for complexity analysis of deep networks. For instance, it might be possible to develop new regularisation methods based on switched weights, biases and centres of the neurons in the network. It remains to be investigated how the nature of the inactive subnetwork, and potential ways of manipulating it during training, would affect generalisation. Under review as a conference paper at ICLR 2020
 

```
