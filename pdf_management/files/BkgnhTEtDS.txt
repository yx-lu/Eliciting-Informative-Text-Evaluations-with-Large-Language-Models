Title:
```
FEATURE INTERACTION INTERPRETABILITY: A CASE FOR EXPLAINING AD-RECOMMENDATION SYSTEMS VIA NEURAL INTERACTION DETECTION
```
Abstract:
```
Recommendation is a prevalent application of machine learning that affects many users; therefore, it is important for recommender models to be accurate and in- terpretable. In this work, we propose a method to both interpret and augment the predictions of black-box recommender systems. In particular, we propose to interpret feature interactions from a source recommender model and explicitly en- code these interactions in a target recommender model, where both source and target models are black-boxes. By not assuming the structure of the recommender system, our approach can be used in general settings. In our experiments, we fo- cus on a prominent use of machine learning recommendation: ad-click prediction. We found that our interaction interpretations are both informative and predictive, e.g., significantly outperforming existing recommender models. What's more, the same approach to interpret interactions can provide new insights into domains even beyond recommendation, such as text and image classification. Code is available at: https://github.com/mtsang/interaction_interpretability
```

Figures/Tables Captions:
```
Figure 1: A simplified overview of GLIDER. 1 GLIDER utilizes Neural Interaction Detection and LIME together to interpret feature interactions learned by a source black-box model at a data instance, denoted by the large green plus sign. 2 GLIDER identifies interactions that consistently appear over multiple data samples, then explicitly encodes these interactions in a target black-box recommender model f rec .
Figure 2: Occurrence counts (Total: 1000) vs. rank of detected interactions from Au- toInt on Criteo and Avazu datasets. * indi- cates a higher-order interaction (details in Appendix G).
Figure 3: Test logloss vs. K of DeepFM on the Criteo dataset (5 trials).
Figure 4: Qualitative examples (more in Appendix D & E) Test prediction performances are shown in Table 5 for k ∈ {0, 1, L}.
Table 1: CTR dataset statistics
Table 2: Understanding feature interactions: top global feature interactions for (a) an ad targeting system via Algorithm 1 and (b) a text sentiment analyzer via §6.3.2 (later). The tables are juxtaposed to assist in understanding feature interactions, i.e., nuanced changes among interacting variables lead to significant changes in prediction probabilities. The prediction outcomes are ad-clicks by users for (a) and text sentiment for (b).
Table 3: Test prediction performance by encoding top-K global interactions in baseline recom- mender systems on the Criteo and Avazu datasets (5 trials). K are 40 and 10 for Criteo and Avazu respectively. "+ GLIDER" means the inclusion of detected global interactions to corresponding baselines. The "Setting" column is labeled relative to the source of detected interactions: AutoInt. * scores by Song et al. (2018).
Table 4: # parameters of the models in Table 3. M denotes million.
Table 5: Prediction performance (mean-squared error; lower is better) with (k > 0) and without (k = 0) interactions for random data instances in the test sets of respective black-box models. k = L corresponds to the interaction at a rank threshold. 2 ≤ k < L are excluded because not all instances have 2 or more interactions. Only results with detected interactions are shown. At least 94% (≥ 188) of the data instances had interactions across 5 trials for each model and score statistic.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Despite their impact on users, state-of-the-art recommender systems are becoming increasingly in- scrutable. For example, the models that predict if a user will click on an online advertisement are often based on function approximators that contain complex components in order to achieve optimal recommendation accuracy. The complex components come in the form of modules for better learn- ing relationships among features, such as interactions between user and ad features (Cheng et al., 2016; Guo et al., 2017; Wang et al., 2017; Lian et al., 2018; Song et al., 2018). Although efforts have been made to understand the feature relationships, there is still no method that can interpret the feature interactions learned by a generic recommender system, nor is there a strong commercial incentive to do so. In this work, we identify and leverage feature interactions that represent how a recommender system generally behaves. We propose a novel approach, Global Interaction Detection and Encoding for Recommendation (GLIDER), which detects feature interactions that span globally across multiple data-instances from a source recommender model, then explicitly encodes the interactions in a target recommender model, both of which can be black-boxes. GLIDER achieves this by first utilizing our ongoing work on Neural Interaction Detection (NID) (Tsang et al., 2017) with a data-instance perturbation method called LIME (Ribeiro et al., 2016) over a batch of data samples. GLIDER then explicitly encodes the collected global interactions into a target model via sparse feature crossing. In our experiments on ad-click recommendation, we found that the interpretations generated by GLIDER are illuminating, and the detected global interactions can significantly improve the target model's prediction performance. Because our interaction interpretation method is very general, we also show that the interpretations are informative in other domains: text, image, graph, and dna modeling. Our contributions are as follows: 1. We propose feature interaction interpretations of general prediction models via interaction detection. 2. Based on this approach, we propose GLIDER to detect and explicitly encode global feature interactions in black-box recommender systems. This process is a form of automatic feature engineering. 3. Through experiments, we demonstrate the overall interpretability of detected feature inter- actions on a variety of domains and show that the interactions can be leveraged to improve recommendation accuracy.

Section Title: NOTATIONS AND BACKGROUND
  NOTATIONS AND BACKGROUND Notations: Vectors are represented by boldface lowercase letters, such as x or z. The i-th entry of a vector x is denoted by x i . For a set S, its cardinality is denoted by |S|. Let d be the number of features in a dataset. An interaction, I, is a subset of feature indices: I ⊆ {1, 2, . . . , d}, where |I| is always ≥ 2. A higher-order interaction always has |I| ≥ 3. For a vector x ∈ R d , let x I ∈ R |I| be restricted to the dimensions of x specified by I. Let a black-box model be f (·) : R p → R. A black-box recommender model uses tabular feature types, as discussed later in this section. In classification tasks, we assume f is a class logit. p and d may be different depending on feature transformations.

Section Title: Feature Interactions
  Feature Interactions By definition, a model f learns a statistical (non-additive) feature interaction I if and only if f cannot be decomposed into a sum of |I| arbitrary subfunctions f i , each excluding a corresponding interaction variable (Friedman et al., 2008; Sorokina et al., 2008; Tsang et al., 2017), i.e., f (x) = i∈I f i (x {1,2,...,d}\i ). For example, a multiplication between two features, x 1 and x 2 , is a feature interaction because it cannot be represented as an addition of univariate functions, i.e., x 1 x 2 = f 1 (x 2 ) + f 2 (x 1 ). Recommendation Systems: A recommender system, f rec (·), is a model of two feature types: dense numerical features and sparse categorical features. Since the one-hot encoding of categorical feature x c can be high-dimensional, it is commonly represented in a low-dimensional embedding e c = one hot(x c )V c via embedding matrix V c .

Section Title: FEATURE INTERACTIONS IN BLACK-BOX MODELS
  FEATURE INTERACTIONS IN BLACK-BOX MODELS We start by explaining how to obtain a data-instance level (local) interpretation of feature interac- tions by utilizing interaction detection on feature perturbations.

Section Title: FEATURE PERTURBATION AND INFERENCE
  FEATURE PERTURBATION AND INFERENCE Given a data instance x ∈ R p , LIME proposed to perturb the data instance by sampling a separate binary representationx ∈ {0, 1} d of the same data instance. Let ξ : {0, 1} d → R p be the map from the binary representation to the perturbed data instance. Starting from a binary vector of all ones that map to the original features values in the data instance, LIME uniformly samples the number of random features to switch to 0 or the "off" state. In the data instance, "off" could correspond to a 0 embedding vector for categorical features or mean value over a batch for numerical features. It is possible for d < p by grouping features in the data instance to correspond to single binary features inx. An important step is getting black-box predictions of the perturbed data instances to create a dataset with binary inputs and prediction targets: D = {(x i , y i ) | y i = f (ξ(x i )),x i ∈ {0, 1} d }. Though we use LIME's approach, the next section is agnostic to the instance perturbation method.

Section Title: FEATURE INTERACTION DETECTION
  FEATURE INTERACTION DETECTION Feature interaction detection is concerned with identifying feature interactions in a dataset (Bien et al., 2013; Purushotham et al., 2014; Lou et al., 2013; Friedman et al., 2008). Typically, proper interaction detection requires a pre-processing step to remove correlated features that adversely af- fect detection performance (Sorokina et al., 2008). As long as features in dataset D are generated in an uncorrelated fashion, e.g., through random sampling, we can directly use D to detect feature interactions from black-box model f at data instance x.

Section Title: NEURAL INTERACTION DETECTION
  NEURAL INTERACTION DETECTION f can be an arbitrary function and can generate highly nonlinear targets in D, so we focus on de- tecting interactions that could have generic forms. In light of this, we leverage our method, Neural Interaction Detection (NID) (Tsang et al., 2017), which accurately and efficiently detects generic non-additive and arbitrary-order statistical feature interactions. NID detects these interactions by training a lasso-regularized multilayer perceptron (MLP) on a dataset, then identifying the features that have high-magnitude weights to common hidden units. NID is efficient by greedily testing the top-interaction candidates of every order at each of h first-layer hidden units, enabling arbitrary- order interaction detection in O(hd) tests within one MLP.

Section Title: GRADIENT-BASED NEURAL INTERACTION DETECTION
  GRADIENT-BASED NEURAL INTERACTION DETECTION Besides the non-additive definition of statistical interaction, a gradient definition also exists based on mixed partial derivatives (Friedman et al., 2008), i.e., a function F (·) exhibits statistical interaction I among features z i indexed by i 1 , i 2 , . . . , i |I| ∈ I if The advantage of this definition is that it allows exact interaction detection from model gradients (Ai & Norton, 2003); however, this definition contains a computationally expensive expectation, and typical neural networks with ReLU activation functions do not permit mixed partial derivatives. For the task of local interpretation, we only examine a single data instance x, which avoids the expectation. We turn F into an MLP g(·) with smooth, infinitely-differentiable activation functions such as softplus, which closely follows ReLU (Glorot et al., 2011). We then train the MLP with the same purpose as §3.2.1 to faithfully capture interactions in perturbation dataset D. Given these conditions, we define an alternate gradient-based neural interaction detector (GradientNID) as: ω(I) = ∂ |I| g(x) ∂x i1 ∂x i2 . . . ∂x i |I| 2 , where ω is the strength of the interaction I,x is the representation of x, and the MLP g is trained on D. While GradientNID exactly detects interactions from the explainer MLP, it needs to compute interaction strengths ω for feature combinations that grow exponentially in number as |I| increases. We recommend restricting GradientNID to low-order interactions. 5: sort G by most frequently occurring interactions 6: [optional] prune subset interactions in G within a target number of interactions K

Section Title: SCOPE
  SCOPE Based on §3.1 and §3.2, we define a function, MADEX(f, x), that takes as inputs black-box f and data instance x, and outputs S = {I i } k i=1 , a set of top-k detected feature interactions. MADEX stands for "Model-Agnostic Dependency Explainer". In some cases, it is necessary to identify a k threshold. Because of the importance of speed for local interpretations, we simply use a linear regression with additional multiplicative terms to approximate the gains given by interactions in S, where k starts at 0 and is incremented until the linear model's predictions stop improving.

Section Title: GLIDER: GLOBAL INTERACTION DETECTION AND ENCODING FOR RECOMMENDATION
  GLIDER: GLOBAL INTERACTION DETECTION AND ENCODING FOR RECOMMENDATION We now discuss the different components of GLIDER: detecting global interactions in §4.1, then encoding these interactions in recommender systems in §4.2. Recommender systems are interesting because they have pervasive application in real-world systems, and their features are often very sparse. By sparse features, we mean features with many categories, e.g., millions of user IDs. The sparsity makes interaction detection challenging especially when applied directly on raw data because the one-hot encoding of sparse features creates an extremely large space of potential feature combinations (Fan et al., 2015).

Section Title: GLOBAL INTERACTION DETECTION
  GLOBAL INTERACTION DETECTION In this section, we explain the first step of GLIDER. As defined in §3.3, MADEX takes as input a black- box model f and data instance x. In the context of this section, MADEX inputs a source recommender system f rec and data instance x = [x 1 , x 2 , . . . , x p ]. x i is the i-th feature field and is either a dense or sparse feature. p is both the total number of feature fields and the number of perturbation variables (p = d). We define global interaction detection as repeatedly running MADEX over a batch of data instances, then counting the occurrences of the same detected interactions, shown in Algorithm 1. The occurrence counts are not only a useful way to rank global interaction detections, but also a sanity check to rule out the chance that the detected feature combinations are random selections. One potential concern with Alg. 1 is that it could be slow depending on the speed of MADEX. In our experiments, the entire process took less than one hour when run in parallel over a batch of 1000 samples with ∼ 40 features on a 32-CPU server with 2 GPUs. This algorithm only needs to be run once to obtain the summary of global interactions.

Section Title: TRUNCATED FEATURE CROSSES
  TRUNCATED FEATURE CROSSES The global interaction I i , outputted by Alg. 1, is used to create a synthetic feature x Ii for a target recommender system. The synthetic feature x Ii is created by explicitly crossing sparse features indexed in I i . If interaction I i involves dense features, we bucketize the dense features before crossing them. The synthetic feature is sometimes called a cross feature (Wang et al., 2017; Luo et al., 2019) or conjunction feature (Rosales et al., 2012; Chapelle et al., 2015).

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 In this context, a cross feature is an n-ary Cartesian product among n sparse features. If we denote X 1 , X 2 , . . . , X n as the set of IDs for each respective feature x 1 , x 2 , . . . , x n , then their cross feature x {1,...,n} takes on all possible values in Accordingly, the cardinality of this cross feature is |X 1 | × · · · × |X n | and can be extremely large, yet many combinations of values in the cross feature are likely unseen in the training data. Therefore, we generate a truncated form of the cross feature with only seen combinations of values, x (j) I , where j is a sample index in the training data, and x (j) I is represented as a sparse ID in the cross feature x I . We further reduce the cardinality by requiring the same cross feature ID to occur more than T times in a batch of samples, or set to a default ID otherwise. These truncation steps significantly reduce the embedding sizes of each cross feature while maintaining their representation power. Once cross features {x Ii } i are included in a target recommender system, it can be trained as per usual.

Section Title: MODEL DISTILLATION VS. ENHANCEMENT
  MODEL DISTILLATION VS. ENHANCEMENT There are dual perspectives of GLIDER: as a method for model distillation or model enhancement. If a strong source model is used to detect global interactions which are then encoded in more resource- constrained target models, then GLIDER adopts a teacher-student type distillation process. If inter- action encoding augments the same model where the interactions were detected from, then GLIDER tries to enhance the model's ability to represent the interactions.

Section Title: RELATED WORKS
  RELATED WORKS

Section Title: Interaction Interpretations
  Interaction Interpretations A variety of methods exist to detect feature interactions learned in specific models but not black-box models. For example, RuleFit (Friedman et al., 2008), Additive Groves (Sorokina et al., 2008), and Tree-Shap (Lundberg et al., 2018) detect interactions specifi- cally in trees; likewise PaD2 (Gevrey et al., 2006) and NID (Tsang et al., 2017) detect interactions in multilayer perceptrons. Some methods have attempted to interpret feature groups in black-box models, such as Anchors (Ribeiro et al., 2018), Agglomerative Contextual Decomposition (Singh et al., 2019), and Context-Aware methods (Singla et al., 2019); however, these methods were not intended to identify feature interactions.

Section Title: Explicit Interaction Representation
  Explicit Interaction Representation There are increasingly methods for explicitly representing interactions in models. Cheng et al. (2016), Guo et al. (2017), Wang et al. (2017), and Lian et al. (2018) directly incorporate multiplicative cross terms in neural network architectures and Song et al. (2018) use attention as an interaction module, all of which are intended to improve the neural net- work's function approximation. This line of work found that predictive performance can improve with dedicated interaction modeling. Luo et al. (2019) followed up by proposing feature sets from data then explicitly encoding them via feature crossing, but this method's proposals are limited by beam search. Our work approaches this problem from a model interpretation standpoint.

Section Title: Black-Box Local vs. Global Interpretations
  Black-Box Local vs. Global Interpretations Data-instance level local interpretation methods are more flexible at explaining general black-box models; however, global interpretations, which cover multiple data instances, have become increasingly desirable to summarize model behavior. Locally Interpretable Model-Agnostic Explanations (LIME) (Ribeiro et al., 2016) and Integrated Gradients (Sundararajan et al., 2017) are some of the most used methods to locally interpret any classifier and neural predictor respectively. There are some methods for global black-box interpreta- tions, such as shuffle-based feature importance (Fisher et al., 2018), submodular pick (Ribeiro et al., 2016), and visual concept extraction (Kim et al., 2018). Our work offers a new tooling option.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: SETUP
  SETUP In our experiments, we study interaction interpretation and encoding on real-world data. The hyper- parameters in MADEX are as follows. For all experiments, our perturbation datasets D contain 5000 training samples and 500 samples for each validation and testing. Our usage of NID or Gradient- NID as the interaction detector (§3.2) depends on the experimental setting. For all experiments that only examine single data instances, we use GradientNID for its exactness and pairwise interaction Published as a conference paper at ICLR 2020 detection; otherwise, we use NID for its higher-order interaction detection. The MLPs for NID and GradientNID have architectures of 256-128-64 first-to-last hidden layer sizes, and they are trained with learning rate of 1e−2, batchsize of 100, and the ADAM optimizer. NID uses ReLU activa- tions and an 1 regularization of λ 1 = 1e−4, whereas GradientNID uses softplus activations and a structural regularizer as MLP+linear regression, which we found offers strong test performance. In general, models are trained with early stopping on validation sets. For LIME perturbations, we need to establish what a binary 0 maps to via ξ in the raw data instance (§3.1). In domains involving embeddings, i.e., sparse features and word embeddings, the 0 ("off") state is the zeroed embedding vector. For dense features, it is the mean feature value over a batch; for images, the mean superpixel RGB of the image. For our DNA experiment, we use a random nu- cleotide other than the original one. These settings correspond to what is used in literature (Ribeiro et al., 2016; 2018). In our graph experiment, the nodes within the neighborhood of a test node are perturbed, where each node is zeroed during perturbation.

Section Title: EXPERIMENTS ON CTR RECOMMENDATION
  EXPERIMENTS ON CTR RECOMMENDATION In this section, we provide experiments with GLIDER on models trained for click- through-rate (CTR) prediction. The rec- ommender models we study include com- monly reported baselines, which all use neural networks: Wide&Deep (Cheng et al., 2016), DeepFM (Guo et al., 2017), Deep&Cross (Wang et al., 2017), xDeepFM (Lian et al., 2018), and AutoInt (Song et al., 2018). AutoInt is the reported state-of-the-art in academic literature, so we use the model settings and data splits provided by AutoInt's official public repository 1 . For all other recommender models, we use public implementations 2 with the same original architectures reported in literature, set all embedding sizes to 16, and tune the learning rate and optimizer to reach or surpass the test logloss reported by the AutoInt paper (on AutoInt's data splits). From tuning, we use the Adagrad optimizer (Duchi et al., 2011) with learning rate of 0.01. All models use early stopping on validation sets. The datasets we use are benchmark CTR datasets with the largest number of features: Criteo 3 and Avazu 4 , whose data statistics are shown in  Table 1 . Criteo and Avazu both contain 40+ millions of user records on clicking ads, with Criteo being the primary benchmark in CTR research (Cheng et al., 2016

Section Title: GLOBAL INTERACTION DETECTION
  GLOBAL INTERACTION DETECTION For each dataset, we train a source AutoInt model, f rec , then run global interaction detection via Algo- rithm 1 on a batch of 1000 samples from the valida- tion set. A full global detection experiment finishes in less than one hour when run in parallel on either Criteo or Avazu datasets in a 32-CPU Intel Xeon E5-2640 v2 @ 2.00GHz server with 2 Nvidia 1080 Ti GPUs. The detection results across datasets are shown in  Figure 2  as plots of detection counts versus rank. Because the Avazu dataset contains non-anonymized features, we directly show its top-10 detected global interactions in Table 2a. From  Figure 2 , we see that the same interactions are detected very frequently across data instances, and many of the interactions are higher-order interactions. The interaction counts are very signifi- cant. For example, any top-1 occurrence count > 25 is significant for the Criteo dataset (p < 0.05), and likewise > 71 for the Avazu dataset, assuming a conservative search space of only up to 3-way interactions (|I| ≤ 3). Our top-1 occurrence counts are 691 ( 25) for Criteo and 525 ( 71) for Avazu. In Table 2a, the top-interactions are explainable. For example, the interaction between "device ip" and "hour" (in UTC time) makes sense because users - here identified by IP addresses - have ad-click behaviors dependent on their time zones. This is a general theme with many of the top-interactions 5 . As another example, the interaction between "device id" and "app id" makes sense because ads are targeted to users based on the app they're in.

Section Title: INTERACTION ENCODING
  INTERACTION ENCODING Based on our results from the previous section (§6.2.1), we turn our attention to explicitly encoding the detected global interactions in target baseline models via truncated feature crosses (detailed in §4.2). In order to generate valid cross feature IDs, we bucketize dense features into a maximum of 100 bins before crossing them and require that final cross feature IDs occur more than T = 100 times over a training batch of one million samples. We take AutoInt's top-K global interactions on each dataset from §6.2.1 with subset interactions excluded (Algorithm 1, line 6) and encode the interactions in each baseline model including AutoInt itself. K is tuned on valiation sets, and model hyperparameters are the same between a baseline and one with encoded interactions. We set K = 40 for Criteo and K = 10 for Avazu. In  Table 3 , we found that GLIDER often obtains significant gains in performance based on stan- dard deviation, and GLIDER often reaches or exceeds a desired 0.001 improvement for the Criteo dataset (Cheng et al., 2016; Guo et al., 2017; Wang et al., 2017; Song et al., 2018). The improve- ments are especially visible with DeepFM on Criteo. We show how this model's test performance varies with different K in  Figure 3 . All performance gains are obtained at limited cost of extra model parameters ( Table 4 ) thanks to the truncations applied to our cross features. To avoid extra parameters entirely, we recommend feature selection on the new and existing features. One one hand, the evidence that AutoInt's detected interactions can improve other baselines' perfor- mance suggests the viability of interaction distillation. On the other hand, evidence that AutoInt's performance on Criteo can improve using its own detected interactions suggests that AutoInt may benefit from learning interactions more explicitly. In either model distillation or enhancement set- 5 "device ip" and "device id" identify different sets of users (https://www.csie.ntu.edu.tw/ r01922136/slides/kaggle-avazu.pdf) tings, we found that GLIDER performs especially well on industry production models trained on large private datasets with thousands of features.

Section Title: INTERPRETATIONS ON OTHER DOMAINS
  INTERPRETATIONS ON OTHER DOMAINS Since the proposed interaction interpretations are not entirely limited to recommender systems, we demonstrate interpretations on more general black-box models. Specifically, we experiment with the function MADEX(·) defined in §3.3, which inputs a black-box f , data-instance x, and outputs a set of top-k interactions. The models we use are trained on very different tasks, i.e., ResNet152: an image classifier pretrained on ImageNet '14 (Russakovsky et al., 2015; He et al., 2016), Sentiment-LSTM: a 2-layer bi-directional long short-term memory network (LSTM) trained on the Stanford Sentiment Treebank (SST) (Socher et al., 2013; Tai et al., 2015), DNA-CNN: a 2-layer 1D convolutional neural network (CNN) trained on MYC-DNA binding data 6 (Mordelet et al., 2013; Yang et al., 2013; Alipanahi et al., 2015; Zeng et al., 2016; Wang et al., 2018; Barrett et al., 2012), and GCN: a 3-layer Graph Convolutional Network trained on the Cora dataset (Kipf & Welling, 2016; Sen et al., 2008). In order to make informative comparisons to the linear LIME baseline, we use LIME's sample weighting strategy and kernel size (0.25) in this section. We first provide quantitative validation for the detected interactions of all four models in §6.3.1, followed by qualitative results for ResNet152, Sentiment-LSTM, and DNA-CNN in §6.3.2.

Section Title: QUANTITATIVE
  QUANTITATIVE To quantitatively validate our interaction interpretations of general black-box models, we measure the local explanation fidelity of the interactions via prediction performance. As suggested in §3.3 and Published as a conference paper at ICLR 2020 The average number of features of D among the black-box models ranges from 18 to 112. Our quantitative val- idation shows that adding feature in- teractions for DNA-CNN, Sentiment- LSTM, and ResNet152, and adding node interactions for GCN result in significant performance gains when averaged over 40 randomly selected data instances in the test set.

Section Title: QUALITATIVE
  QUALITATIVE For our qualitative analysis, we pro- vide interaction interpretations via MADEX(·) of ResNet152, Sentiment- LSTM, and DNA-CNN on test sam- ples. The interpretations are given by S = {I i } k i=1 , a set of k detected in- teractions, which are shown in  Fig- ure 4  for ResNet152 and Sentiment- LSTM. For reference, we also show the top "main effects" by LIME's original linear regression, which se- lect the top-5 features that attribute towards the predicted class 7 . In Figure 4a, the "interaction" columns show selected features from MADEX's interactions between Quick- shift superpixels (Vedaldi & Soatto, 2008; Ribeiro et al., 2016). To reduce the number of interactions per image, we merged interactions that have overlap coefficient ≥ 0.5 (Vi- jaymeena & Kavitha, 2016). From Published as a conference paper at ICLR 2020 the figure, we see that the interactions form a single region or multiple regions of the image. They also tend to be complementary to LIME's main effects and are sometimes more informative. For example, the interpretations of the "shark" classification show that interaction detection finds the shark fin whereas main effects do not. Interpretations of Sentiment-LSTM are shown in Figure 4b, excluding common stop words (Appendix C). We again see the value of MADEX's interactions, which show salient combinations of words, such as "never, fails", "absolutely, no", and "lacks, punch". In our experiments on DNA-CNN, we consistently detected the interaction between "CACGTG" nucleotides, which form a canonical DNA sequence (Staiger et al., 1989). The interaction was detected 97.3% out of 187 CACGTG appearances in the test set. In order to run consistency experiments now on Sentiment-LSTM, word interactions need to be detected consistently across different sentences, which naïvely would require an exorbitant amount of sentences. Instead, we initially collect interaction candidates by running MADEX over all sentences in the SST test set, then select the word interactions that appear multiple times. We assume that word interactions are ordered but not necessarily adjacent or positionally bound, e.g., (not, good) = (good, not), but their exact positions don't matter. We use the larger IMDB dataset (Maas et al., 2011) to collect different sets of sentences that contain the same ordered words as each interaction candidate (but the sentences are otherwise random). The ranked detection counts of the target interactions on their individual sets of sentences are shown in Table 2b. The average sentence length is 33 words, and interaction occurrences are separated by 2 words on average.

Section Title: CONCLUSION
  CONCLUSION We proposed a way to interpret feature interactions in general prediction models, and we proposed GLIDER to detect and encode these interactions in black-box recommender systems. In our ex- periments on recommendation, we found that our detected global interactions are explainable and that explicitly encoding them can improve predictions. We further validated our interaction inter- pretations on image, text, graph, and dna models. We hope the interpretations encourage investiga- tion into the complex behaviors of prediction models, especially models with large societal impact. Some opportunities for future work are generating correct attributions for interaction interpretations, preventing false-positive interactions from out-of-distribution feature perturbations, and performing interaction distillation from multiple models rather than just one.

```
