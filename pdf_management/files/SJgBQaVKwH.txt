Title:
```
Under review as a conference paper at ICLR 2020 EFFECTIVE USE OF VARIATIONAL EMBEDDING CAPACITY IN EXPRESSIVE END-TO-END SPEECH SYNTHESIS
```
Abstract:
```
Recent work has explored sequence-to-sequence latent variable models for ex- pressive speech synthesis (supporting control and transfer of prosody and style), but has not presented a coherent framework for understanding the trade-offs be- tween the competing methods. In this paper, we propose embedding capacity (the amount of information the embedding contains about the data) as a unified method of analyzing the behavior of latent variable models of speech, comparing existing heuristic (non-variational) methods to variational methods that are able to explicitly constrain capacity using an upper bound on representational mutual information. In our proposed model (Capacitron), we show that by adding condi- tional dependencies to the variational posterior such that it matches the form of the true posterior, the same model can be used for high-precision prosody transfer, text-agnostic style transfer, and generation of natural-sounding prior samples. For multi-speaker models, Capacitron is able to preserve target speaker identity during inter-speaker prosody transfer and when drawing samples from the latent prior. Lastly, we introduce a method for decomposing embedding capacity hierarchically across two sets of latents, allowing a portion of the latent variability to be specified and the remaining variability sampled from a learned prior. Audio examples are available on the web 1 . 1 https://variational-embedding-capacity.github.io/demos/
```

Figures/Tables Captions:
```
Figure 1: Reconstruction loss vs. embedding dimensionality for a variety of heuristic and variational models. For the variational model (Var.), we vary the capacity limit, C. Notice how the reconstruction loss flattens out at lower values for higher values of C. The heuristic models are denoted by PT and GST for Prosody Transfer and Global Style Tokens. Figure B.1 in the appendix shows how the KL term changes when varying C as well as the KL weight, β.
Figure 2: Adding conditional dependencies to the variational posterior. Shaded nodes indicate observed variabes. [left] The true generative model. [center] Variational posterior missing conditional dependencies present in the true posterior. [right] Variational posterior that matches the form of the true posterior.
Figure 3: Hierarchical decomposition of the latents. Shaded nodes indicate observed variables. [left] The true generative model. [right] Variational posterior that matches the form of the true posterior.
Figure 4: Comparing same-text transfer (STT), inter-text transfer (ITT), and prior samples (Prior) for variational models with and without text dependencies in the variational posterior (Var+Txt and Var, respectively). Error bars show 95% confidence intervals for the subjective evaluations.
Figure 5: MCD-DTW reference distance and inter-sample distance for hierarchical latents when transferring via z H and z L .
Table 1: Inter-speaker same-text prosody transfer results for C = 150 with and without speaker dependencies in the variational posterior (Var+Txt+Spk and Var+Txt, respectively). SpkID denotes the fraction of the time the target speaker was chosen by the speaker classifier. For reference, we provide MOS and SpkID numbers for the baseline model and ground truth audio (though neither are "prior" samples).
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The synthesis of realistic human speech is a challenging problem that is important for natural human- computer interaction. End-to-end neural network-based approaches have seen significant progress in recent years (Wang et al., 2017;  Taigman et al., 2018 ;  Ping et al., 2018 ;  Sotelo et al., 2017 ), even matching human performance for short assistant-like utterances ( Shen et al., 2018 ). However, these neural models are sometimes viewed as less interpretable or controllable than more traditional models composed of multiple stages of processing that each operate on reified linguistic or phonetic representations. Text-to-speech (TTS) is an underdetermined problem, meaning the same text input has an infinite number of reasonable spoken realizations. In addition to speaker and channel characteristics, im- portant sources of variability in TTS include intonation, stress, and rhythm (collectively referred to as prosody). These attributes convey linguistic, semantic, and emotional meaning beyond what is present in the lexical representation (i.e., the text) ( Wagner & Watson, 2010 ). Recent end-to-end TTS research has aimed to model and/or directly control the remaining variability in the output.  Skerry-Ryan et al. (2018)  augment a Tacotron-like model (Wang et al., 2017) with a deterministic encoder that projects reference speech into a learned embedding space. The system can be used for prosody transfer between speakers ("say it like this"), but does not work for transfer between unrelated sentences, and does not preserve the pitch range of the target speaker.  Lee & Kim (2019)  partially address the pitch range problem by centering the learned embeddings using speaker-wise means.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Other work targets style transfer, a text-agnostic variation on prosody transfer. The Global Style Token (GST) system ( Wang et al., 2018 ) uses a modified attention-based reference encoder to transfer global style properties to arbitrary text, and  Ma et al. (2019)  use an adversarial objective to disentangle style from text.  Hsu et al. (2019)  and  Zhang et al. (2019)  use a variational approach ( Kingma & Welling, 2014 ) to tackle the style task. Advantages of this approach include its ability to generate style samples via the accompanying prior and the potential for better disentangling between latent style factors ( Burgess et al., 2018 ). Additionally,  Hsu et al. (2019)  use a Gaussian mixture prior over the latents, which (when interpreting the mixture component index as a high-level discrete latent) allows a form of hierarchical control. This work extends the above approaches by providing the following contributions: 1. We propose a unified approach for analyzing the characteristics of TTS latent variable models, independent of architecture, using the capacity of the learned embeddings (i.e., the representational mutual information between the embedding and the data). 2. We target specific capacities for our proposed model using a Lagrange multiplier-based opti- mization scheme, and show that capacity is correlated with perceptual reference similarity. 3. We show that modifying the variational posterior to match the form of the true posterior enables style and prosody transfer in the same model, helps preserve target speaker identity during inter-speaker transfer, and leads to natural-sounding prior samples even at high embedding capacities. 4. We introduce a method for controlling what fraction of the variation represented in an embedding is specified, allowing the remaining variation to be sampled from the model.

Section Title: MEASURING REFERENCE EMBEDDING CAPACITY
  MEASURING REFERENCE EMBEDDING CAPACITY

Section Title: LEARNING A REFERENCE EMBEDDING SPACE
  LEARNING A REFERENCE EMBEDDING SPACE Existing heuristic (non-variational) end-to-end approaches to prosody and style transfer ( Skerry-Ryan et al., 2018 ;  Wang et al., 2018 ;  Lee & Kim, 2019 ;  Henter et al., 2018 ) typically start with the teacher- forced reconstruction loss, (1), used to train Tacotron-like sequence-to-sequence models and simply augment the model with a deterministic reference encoder, g e (x), as shown in eq. (2). L(x, y T , y S ) ≡ − log p(x|y T , y S ) = f θ (y T , y S ) − x 1 + K (1) L (x, y T , y S ) ≡ − log p(x|y T , y S , g e (x)) = f θ (y T , y S , g e (x)) − x 1 + K (2) where x is an audio spectrogram, y T is the input text, y S is the target speaker (if training a multi- speaker model), f θ (·) is a deterministic function that maps the inputs to spectrogram predictions, and K is a normalization constant. Teacher-forcing implies that f θ (·) is dependent on x <t when predicting spectrogram frame x t . In practice, f θ (·) serves as the greedy deterministic output of the model, and transfer is accomplished by pairing the embedding computed by the reference encoder with different text or speakers during synthesis. In these heuristic models, the architecture chosen for the reference encoder determines the transfer characteristics of the model. This decision affects the information capacity of the embedding and allows the model to target a specific trade-off between transfer precision (how closely the output resembles the reference) and generality (how well an embedding works when paired with arbitrary text). Higher capacity embeddings prioritize precision and are better suited for prosody transfer to similar text, while lower capacity embeddings prioritize generality and are better suited for text-agnostic style transfer. The variational extensions from  Hsu et al. (2019)  and  Zhang et al. (2019)  augment the reconstruction loss in eq. (2) with a KL divergence term. This encourages a stochastic reference encoder (variational posterior), q(z|x), to align well with a prior, p(z) (eq. (3)). The overall loss is then equivalent to the negative evidence lower bound (ELBO) of the marginal likelihood of the data ( Kingma & Welling, 2014 ). Under review as a conference paper at ICLR 2020 Controlling embedding capacity in variational models can be accomplished more directly by manipu- lating the KL term in (3). Recent work has shown that the KL term provides an upper bound on the mutual information between the data, x, and the latent embedding, z ∼ q(z|x) ( Hoffman & Johnson, 2016 ;  Makhzani et al., 2015 ;  Alemi et al., 2018 ). where p D (x) is the data distribution, R is the the KL term in (3), R AVG is the KL term averaged over the data distribution, I q (X; Z) is the representational mutual information (the capacity of z), and q(z) is the aggregated posterior. This brief derivation is expanded in Appendix C.1. The bound in (8) follows from (7) and the non-negativity of the KL divergence, and (7) shows that the slack on the bound is D KL (q(z) p(z)), the aggregate KL. In addition to providing a tighter bound, having a low aggregate KL is desirable when sampling from the model via the prior, because then the samples of z that the decoder sees during training will be very similar to samples from the prior. Various approaches to controlling the KL term have been proposed, including varying a weight on the KL term, β ( Higgins et al., 2017 ), and penalizing its deviation from a target value ( Alemi et al., 2018 ;  Burgess et al., 2018 ). Because we would like to smoothly optimize for a specific bound on the embedding capacity, we adapt the Lagrange multiplier-based optimization approach of  Rezende & Viola (2018)  by applying it to the KL term rather than the reconstruction term. min θ max β≥0 E z∼q θ (z|x) [− log p θ (x|z, y T , y S )] + β(D KL (q θ (z|x) p(z)) − C) (9) where θ are the model parameters, β serves as an automatically-tuned weight on the KL term, C is the capacity limit, and updates to θ and β are interleaved during training. We constrain β to be non-negative by passing an unconstrained parameter through a softplus non-linearity, which makes the capacity constraint a limit rather than a target. This approach is less tedious than tuning β by hand and leads to more consistent behavior from run-to-run. It also allows more stable optimization than directly penalizing the 1 deviation from the target KL.

Section Title: ESTIMATING EMBEDDING CAPACITY
  ESTIMATING EMBEDDING CAPACITY Estimating heuristic embedding capacity Unfortunately, the heuristic methods do not come packaged with an easy way to estimate embedding capacity. We can estimate an effective capacity ordering, however, by measuring the test-time reconstruction loss when using the reference encoder from each method. In  Figure 1 , we show how the reconstruction loss varies with embedding dimensionality for the tanh-based prosody transfer (PT) and softmax-based global style token (GST) bottlenecks ( Skerry-Ryan et al., 2018 ;  Wang et al., 2018 ) and for variational models (Var.) with different capacity limits, C. We also compare to a baseline Tacotron model without a reference encoder. For this preliminary comparison, we use the expressive single-speaker dataset and training setup described in Section 4.2. Looking at the heuristic methods in  Figure 1 , we see that the GST bottleneck is much more restrictive than the PT bottleneck, which hurts transfer precision but allows sufficient embedding generality for text-agnostic style transfer.

Section Title: Bounding variational embedding capacity
  Bounding variational embedding capacity We saw in (8) that the KL term is an upper bound on embedding capacity, so we can directly target a specific capacity limit by constraining the KL term using the objective in eq. (9). For the three values of C in  Figure 1 , we can see that the reconstruction loss flattens out once the embedding reaches a certain dimensionality. This gives us a consistent way to control embedding capacity as it only requires using a reference encoder architecture with sufficient structural capacity (at least C) to achieve the desired representational capacity in the variational embedding. Because of this, we use 128-dimensional embeddings in all of our experiments, which should be sufficient for the range of capacities we target.

Section Title: MAKING EFFECTIVE USE OF EMBEDDING CAPACITY
  MAKING EFFECTIVE USE OF EMBEDDING CAPACITY

Section Title: MATCHING THE FORM OF THE TRUE POSTERIOR
  MATCHING THE FORM OF THE TRUE POSTERIOR In previous work ( Hsu et al., 2019 ;  Zhang et al., 2019 ), the variational posterior has the form q(z|x), which matches the form of the true posterior for a simple generative model p(x|z)p(z). However, for the conditional generative model used in TTS, p(x|z, y T , y S )p(z), it is missing conditional dependencies present in the true posterior, p(z|x, y T , y S ).  Figure 2  shows this visually. In order to match the form of the true posterior, we inject information about the text and the speaker into the network that predicts the parameters of the variational posterior. Speaker information is represented as learned speaker-wise embedding vectors, while the text information is summarized into a vector by passing the output of the Tacotron text encoder through a unidirectional RNN as done by  Stanton et al. (2018) . Appendix A.1 gives additional details. For this work, we use a simple diagonal Gaussian for the approximate posterior, q(z|x, y T , y S ) and a standard normal distribution for the prior, p(z). We use these distributions for simplicity and efficiency, but using more powerful distributions such as Gaussian mixtures or normalizing flows ( Rezende & Mohamed, 2015 ) should decrease the aggregate KL, leading to better prior samples. Because we are learning a conditional generative model, p(x|y T , y S ), we could have used a learned conditional prior, p(z|y T , y S ), in order to improve the quality of the output generated when sampling via the prior. However, in this work we focus on the transfer use case where we infer z ref ∼ q(z|x ref , y ref T , y ref S ) from a reference utterance and use it to re-synthesize speech using different text or speaker inputs, x ∼ p(x|z ref , y T , y S ). Using a fixed prior allows z to share a high probability region across all text and speakers so that an embedding inferred from one utterance is likely to lead to non-degenerate output when being used with any other text or speaker.

Section Title: DECOMPOSING EMBEDDING CAPACITY HIERARCHICALLY
  DECOMPOSING EMBEDDING CAPACITY HIERARCHICALLY In inter-text style transfer uses cases, we infer z ref from a reference utterance and then use it to generate a new utterance with the same style but different text. One problem with this approach is that z ref completely specifies all variation that the latent embedding is capable of conveying to the decoder, p(x|z ref , y T , y S ). So, even though there are many possible realizations of an utterance with a given style, this approach can produce only one 2 . To address this issue, we decompose the latents, z, hierarchically ( Sønderby et al., 2016 ) into high- level latents, z H , and low-level latents, z L , as shown in  Figure 3 . This differs from the hierarchical interpretation of the Gaussian mixture prior used by  Hsu et al. (2019)  in that here the high-level latents are continuous vectors rather than a single categorical variable. Factorizing continuous latents in this way allows us to specify how the joint capacity, I q (X; [Z H , Z L ]), is divided between z H and z L . This approach can also be extended to additional levels of latents, each containing a prescribed proportion of the overall joint capacity. As shown in eq. (8), the KL term, R AVG , is an upper bound on I q (X; Z). We can also derive similar bounds for I q (X; Z H ) and I q (X; Z L ). Derivations of these bounds are provided in Appendix C.2. If we define R AVG L ≡ R AVG − R AVG H , we end up with the following capacity limits for the hierarchical latents: The negative ELBO for this model can be written as: L ELBO (x, y T , y S ) = −E zL∼q(zL|x) [log p(x|z L , y T , y S )] + R H + R L (13) where R H and R L are single data point estimates of R AVG H and R AVG L computed from x. In order to specify how the joint capacity is distributed between the latents, we extend (9) to have two Lagrange multipliers and capacity targets. C H limits the information capacity of z H , and C L limits how much capacity z L has in excess of z H (i.e., the total capacity of z L is capped at C H + C L ). This allows us to infer z ref H ∼ q(z H |z L )q(z L |x ref , y ref T , y ref S ) from a reference utterance and use it to sample multiple realizations, x ∼ p(x|z L , y T , y S )p(z L |z ref H ). Intuitively, the higher C H is, the more the output will resemble the reference, and the higher C L is, the more variation we would expect from sample to sample when fixing z ref H and sampling z L from p(z L |z ref H ).

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: MODEL ARCHITECTURE AND TRAINING
  MODEL ARCHITECTURE AND TRAINING

Section Title: Model architecture
  Model architecture The baseline model we start with is a Tacotron-based system (Wang et al., 2017) that incorporates modifications from  Skerry-Ryan et al. (2018) , including phoneme inputs instead of characters, GMM attention ( Graves, 2013 ), and a WaveNet neural vocoder ( van den Oord et al., 2016 ) to convert the output mel spectrograms into audio samples ( Shen et al., 2018 ). The decoder RNN uses a reduction factor of 2, meaning that it produces two spectrogram frames per timestep. We use the CBHG text encoder from  Wang et al. (2018)  and the GMMv2b attention mechanism from  Battenberg et al. (2019) . For the heuristic models compared in Section 2.2, we augment the baseline Tacotron with the reference encoders described by  Skerry-Ryan et al. (2018)  and  Wang et al. (2018) . For the variational models that we compare in the following experiments, we start with the reference encoder from  Skerry-Ryan et al. (2018)  and replace the tanh bottleneck layer with an MLP that predicts the parameters of the variational posterior. When used, the additional conditional dependencies (text and speaker) are fed into the MLP as well.

Section Title: Training
  Training To train the models, the primary optimizer is run synchronously across 10 GPU workers (2 of them backup workers) for 300,000 training steps with an effective batch size of 256. It uses the Adam algorithm ( Kingma & Ba, 2015 ) with a learning rate that is annealed from 10 −3 to 5 × 10 −5 over 200,000 training steps. The optimizer for β is run asychronously on the 10 workers and uses SGD with momentum 0.9 and a fixed learning rate of 10 −5 . The updates for these two optimizers are interleaved, allowing β to converge to a steady state value that achieves the target value for the KL term, as demonstrated in Figure B.2 in the appendix. Additional architectural and training details are provided in Appendix A.

Section Title: EXPERIMENT SETUP
  EXPERIMENT SETUP

Section Title: Datasets
  Datasets For single-speaker models, we use an expressive English language audiobook dataset consisting of 50,086 training utterances (36.5 hours) and 912 test utterances spoken by Catherine Byers, the speaker from the 2013 Blizzard Challenge. Multi-speaker models are trained using high- quality English data from 58 voice assistant-like speakers, consisting of 419,966 training utterances (327 hours). We evaluate on a 9-speaker subset of the multi-speaker test data which contains 1808 utterances (comprising US, UK, Australian, and Indian speakers).

Section Title: Tasks
  Tasks The tasks that we explore include same-text prosody transfer, inter-text style transfer, and inter-speaker prosody transfer. We also evaluate the quality of samples produced when sampling via the prior. For these tasks, we compare performance when using variational models with and without the additional conditional dependencies in the variational posterior at a number of different capacity limits. For models with hierarchical latents, we demonstrate the effect of varying C H and C L for same-text prosody transfer when inferring z H and sampling z L or when inferring z L directly.

Section Title: Evaluation
  Evaluation We use crowd-sourced native speakers to collect two types of subjective evaluations. First, mean opinion score (MOS) rates naturalness on a scale of 1-5, 5 being the best. Second, we use the AXY side-by-side comparison proposed by  Skerry-Ryan et al. (2018)  to measure subjective similarity to a reference utterance relative to the baseline model on a scale of [-3,3]. For example, a score of 3 would mean that, compared to the baseline model, the model being tested produces samples much more perceptually similar to the ground truth reference. We also use an objective similarity metric that uses dynamic time warping to find the minimum mel cepstral distortion ( Kubichek, 1993 ) between two sequences (MCD-DTW). Lastly, for inter-speaker transfer, we follow  Skerry-Ryan et al. (2018)  and use a simple speaker classifier to measure how well speaker identity is preserved. Additional details on evaluation methodologies are provided in Appendix A.

Section Title: RESULTS
  RESULTS Single speaker For single-speaker models, we compare the performance on same and inter-text transfer and the quality of samples generated via the prior for models with and without text condi- tioning in the variational posterior (Var+Txt and Var, respectively) at different capacity limits, C. Similarity results for the transfer task are shown on the left side of  Figure 4  and demonstrate increas- ing reference similarity as C is increased, with the exception of the model without text conditioning on the inter-text transfer task. Looking at the MOS naturalness results on the right side of  Figure 4 , we see that both inter-text transfer and prior sampling take a serious hit as capacity is increased for the Var model, while the Var+Txt model is able to maintain respectable performance even at very high capacities on all tasks. Listening to the audio examples, we can hear that the Var model produces degenerate output at high capacities when attempting to transfer the style from a short utterance to a long utterance. This indicates that the decoder probably hasn't seen similar embeddings paired with long utterances during training, which suggests that z is improperly correlated with text length. Similar behavior is also observed when generating prior samples using shorter or longer text. This means that an arbitrary z (sampled from the prior or inferred from a reference) is unlikely to pair well with text of an arbitrary length.

Section Title: Multi-speaker
  Multi-speaker For multi-speaker models, we compare inter-speaker same-text transfer perfor- mance and prior sample quality with and without speaker conditioning in the variational posterior (Var+Txt+Spk and Var+Txt, respectively) at a fixed capacity limit of 150 nats. In  Table 1 , we see that both models are able to preserve characteristics of the reference utterance during transfer (AXY Ref. Similarity column), while the Var+Txt+Spk model has an edge in MOS for both inter-speaker transfer and prior samples (almost matching the MOS of the deterministic baseline model even at high embedding capacity). Similar to the utterance length argument in the single speaker section above, it is likely that adding speaker dependencies to the posterior allows the model to use the entire latent space for each speaker (meaning z is not correlated with speaker identity), thereby forcing the decoder to learn to map all plausible points in the latent space to natural-sounding utterances that preserve the target speaker's pitch range. The speaker classifier results show that the Var+Txt+Spk model preserves target speaker identity about as well as the baseline model and ground truth data (~5% of the time the classifier chooses a speaker other than the target speaker), whereas for the Var+Txt model this happens about 22% of the time. Though 22% seems like a large speaker error rate, it is much lower than the 79% figure presented by  Skerry-Ryan et al. (2018)  for a heuristic prosody transfer model. This demonstrates that even with a weakly conditioned posterior, the capacity limiting properties of variational models lead to better transfer generality and robustness.

Section Title: Hierarchical latents
  Hierarchical latents To evaluate hierarchical decomposition of capacity in a single speaker setting, we use the MCD-DTW distance to quantify reference similarity and same-reference inter-sample variability. As shown in Table B.1 in the appendix, MCD-DTW strongly (negatively) correlates with subjective similarity. The left side of  Figure 5  shows results for samples generated using high-level latents, z H , inferred from the reference. As C H is increased, we see a strong downward trend in the average distance to Under review as a conference paper at ICLR 2020 Using Capacitron with hierarchical latents increases the model's versatility for transfer tasks. By inferring just the high-level latents, z H , from a reference, we can sample multiple realizations of an utterance that are similar to the reference, with the level of similarity controlled by C H , and the amount of sample-to-sample variation controlled by C L . The same model can also be used for higher fidelity, lower variability transfer by inferring the low-level latents, z L , from a reference, with the level of similarity controlled by C = C H + C L . As mentioned before, this idea could also be extended to use additional levels of latents, thereby increasing transfer and sampling flexibility. To appreciate the results fully, it is strongly recommended to listen to the audio examples available on the web 3 .

Section Title: CONCLUSION
  CONCLUSION We have proposed embedding capacity (i.e., representational mutual information) as a useful frame- work for comparing and configuring latent variable models of speech. Our proposed model, Capac- itron, demonstrates that including text and speaker dependencies in the variational posterior allows a single model to be used successfully for a variety of transfer and sampling tasks. Motivated by the multi-faceted variability of natural human speech, we also showed that embedding capacity can be decomposed hierarchically in order to enable the model to control a trade-off between transfer fidelity and sample-to-sample variation. There are many directions for future work, including adapting the fixed-length variational embeddings to be variable-length and synchronous with either the text or audio, using more powerful distributions like normalizing flows, and replacing the deterministic decoder with a proper likelihood distribution. For transfer and control uses cases, the ability to distribute certain speech characteristics across specific subsets of the hierarchical latents would allow more fine-grained control of different aspects of the output speech. And for purely generative, non-transfer use cases, using more powerful conditional priors could improve sample quality.

```
