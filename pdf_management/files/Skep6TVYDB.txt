Title:
```
Published as a conference paper at ICLR 2020 GRADIENTLESS DESCENT: HIGH-DIMENSIONAL ZEROTH-ORDER OPTIMIZATION
```
Abstract:
```
Zeroth-order optimization is the process of minimizing an objective f (x), given oracle access to evaluations at adaptively chosen inputs x. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and present a novel analysis that shows convergence within an -ball of the optimum in O(kQ log(n) log(R/ )) evaluations, for any monotone transform of a smooth and strongly convex ob- jective with latent dimension k < n, where the input dimension is n, R is the diameter of the input space and Q is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on BBOB and MuJoCo benchmarks.
```

Figures/Tables Captions:
```
Figure 1: The average optimality gap on a quadratic objective function that is strongly convex and smooth objective (top); and its monotone transformation (bottom). Further experiments on non- convex BBOB functions show similar behavior and are in the appendix.
Table 1: Comparison of zeroth order optimization for well-conditioned convex functions where R = x 0 − x * and F = f (x 0 ) − f (x * ). 'Monotone' column indicates the invariance under monotone transformations (Definition 4). 'k-Sparse' and 'k-Affine' columns indicate that iteration complexity is poly(k, log(n)) when f (x) depends only on a k-sparse subset of coordinates or on a rank-k affine subspace.
Table 2: Final rewards by GLD with linear (L) and deep (H41) policies on Mujoco Benchmarks show that GLD is competitive. We apply an affine projection on HalfCheetah to test affine invariance. We use the reward threshold found from (Mania et al., 2018) with Reacher's threshold (Schulman et al., 2017) for a reasonable baseline.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION We consider the problem of zeroth-order optimization (also known as gradient-free optimization, or bandit optimization), where our goal is to minimize an objective function f : R n → R with as few evaluations of f (x) as possible. For many practical and interesting objective functions, gradients are difficult to compute and there is still a need for zeroth-order optimization in applications such as reinforcement learning ( Mania et al., 2018 ;  Salimans et al., 2017 ;  Choromanski et al., 2018 ), attacking neural networks ( Chen et al., 2017 ;  Papernot et al., 2017 ), hyperparameter tuning of deep networks ( Snoek et al., 2012 ), and network control ( Liu et al., 2017 ). The standard approach to zeroth-order optimization is, ironically, to estimate the gradients from function values and apply a first-order optimization algorithm ( Flaxman et al., 2005 ).  Nesterov & Spokoiny (2011)  analyze this class of algorithms as gradient descent on a Gaussian smoothing of the objective and gives an accelerated O(n √ Q log((LR 2 + F )/ )) iteration complexity for an L- Lipschitz convex function with condition number Q and R = x 0 − x * and F = f (x 0 ) − f (x * ). They propose a two-point evaluation scheme that constructs gradient estimates from the difference between function values at two points that are close to each other. This scheme was extended by ( Duchi et al., 2015 ) for stochastic settings, by (Ghadimi & Lan, 2013) for nonconvex settings, and by ( Shamir, 2017 ) for non-smooth and non-Euclidean norm settings. Since then, first-order techniques such as variance reduction ( Liu et al., 2018 ), conditional gradients ( Balasubramanian & Ghadimi, 2018 ), and diagonal preconditioning ( Mania et al., 2018 ) have been successfully adopted in this setting. This class of algorithms are also known as stochastic search, random search, or (natural) evolutionary strategies and have been augmented with a variety of heuristics, such as the popular CMA-ES ( Auger & Hansen, 2005 ). These algorithms, however, suffer from high variance due to non-robust local minima or highly non-smooth objectives, which are common in the fields of deep learning and reinforcement learn- Published as a conference paper at ICLR 2020 ing.  Mania et al. (2018)  notes that gradient variance increases as training progresses due to higher variance in the objective functions, since often parameters must be tuned precisely to achieve rea- sonable models. Therefore, some attention has shifted into direct search algorithms that usually finds a descent direction u and moves to x + δu, where the step size is not scaled by the function difference. The first approaches for direct search were based on deterministic approaches with a positive span- ning set and date back to the 1950s ( Brooks, 1958 ). Only recently have theoretical bounds surfaced, with  Gratton et al. (2015)  giving an iteration complexity that is a large polynomial of n and  Dodan- geh & Vicente (2016)  giving an improved O(n 2 L 2 / ). Stochastic approaches tend to have better complexities:  Stich et al. (2013)  uses line search to give a O(nQ log(F/ )) iteration complexity for convex functions with condition number Q and most recently,  Gorbunov et al. (2019)  uses impor- tance sampling to give a O(nQ log(F/ )) complexity for convex functions with average condition numberQ, assuming access to sampling probabilities.  Stich et al. (2013)  notes that direct search algorithms are invariant under monotone transforms of the objective, a property that might explain their robustness in high-variance settings. In general, zeroth order optimization suffers an at least linear dependence on input dimension n and recent works have tried to address this limitation when n is large but f (x) admits a low-dimensional structure. Some papers assume that f (x) depends only on k coordinates and  Wang et al. (2017)  applies Lasso to find the important set of coordinates, whereas  Balasubramanian & Ghadimi (2018)  simply change the step size to achieve an O(k(log(n)/ ) 2 ) iteration complexity. Other papers as- sume more generally that f (x) = g(P A x) only depends on a k-dimensional subspace given by the range of P A and  Djolonga et al. (2013)  apply low-rank approximation to find the low-dimensional subspace while  Wang et al. (2013)  use random embeddings.  Hazan et al. (2017)  assume that f (x) is a sparse collection of k-degree monomials on the Boolean hypercube and apply sparse recov- ery to achieve a O(n k ) runtime bound. We will show that under the case that f (x) = g(P A x), our algorithm will inherently pick up any low-dimensional structure in f (x) and achieve a con- vergence rate that depends on k log(n). This initial convergence rate survives, even if we perturb f (x) = g(P A x) + h(x), so long as h(x) is sufficiently small. We will not cover the whole variety of black-box optimization methods, such as Bayesian opti- mization or genetic algorithms. In general, these methods attempt to solve a broader problem (e.g. multiple optima), have weaker theoretical guarantees and may require substantial computation at each step: e.g. Bayesian optimization generally has theoretical iteration complexities that grow ex- ponentially in dimension, and CMA-ES lacks provable complexity bounds beyond convex quadratic functions. In addition to the slow runtime and weaker guarantees, Bayesian optimization assumes the success of an inner optimization loop of the acquisition function. This inner optimization is often implemented with many iterations of a simpler zeroth-order methods, justifying the need to understand gradient-less descent algorithms within its own context.

Section Title: OUR CONTRIBUTIONS
  OUR CONTRIBUTIONS In this paper, we present GradientLess Descent (GLD), a class of truly gradient-free algorithms (also known as direct search algorithms) that are parameter free and provably fast. Our algorithms are based on a simple intuition: for well-conditioned functions, if we start from a point and take a small step in a randomly chosen direction, there is a significant probability that we will reduce the objective function value. We present a novel analysis that relies on facts in high dimensional geometry and can thus be viewed as a geometric analysis of gradient-free algorithms, recovering the standard convergence rates and step sizes. Specifically, we show that if the step size is on the order of O( 1 √ n ), we can guarantee an expected decrease of 1 − Ω( 1 n ) in the optimality gap, based on geometric properties of the sublevel sets of a smooth and strongly convex function. Our results are invariant under monotone transformations of the objective function, thus our conver- gence results also hold for a large class of non-convex functions that are a subclass of quasi-convex functions. Specifically, note that monotone transformations of convex functions are not necessarily convex. However, a monotone transformation of a convex function is always quasi-convex. The maximization of quasi-concave utility functions, which is equivalent to the minimization of quasi- convex functions, is an important topic of study in economics (e.g.  Arrow & Enthoven (1961) ). Therefore, we can find x T such that x T −x * ≤ after T = O(nQ log(R/ )) function evaluations. Furthermore, for functions f (x) = g(P A x) + h(x) with rank k matrix P A and sufficiently small h(x), we only require O(kQ log(n) log(R/ )) evaluations. Another advantage of our non-standard geometric analysis is that it allows us to deduce that our rates are optimal with a matching lower bound (up to logarithmic factors), presenting theoretical evidence that gradient-free inherently requires Ω(nQ) function evaluations to converge. While gradient-estimation algorithms can achieve a better theoretical iteration complexity of O(n √ Q), they lack the monotone and affine invariance properties. Empirically, we see that invariance prop- erties are important to successful optimization, as validated by experiments on synthetic BBOB and MuJoCo benchmarks that show the competitiveness of GLD against standard optimization proce- dures.

Section Title: PRELIMINARIES
  PRELIMINARIES We first define a few notations for the rest of the paper. Let X be a compact subset of R n and let · denote the Euclidean norm. The diameter of X , denoted X = max x,x ∈X x − x , is the maximum distance between elements in X . Let f : X → R be a real-valued function which attains its minimum at x * . We use f (X ) = {f (x) : x ∈ X } to denote the image of f on a subset X of R n , and B(c, r) = {x ∈ R n : c − x ≤ r} to denote the ball of radius r centered at c. Published as a conference paper at ICLR 2020 Definition 4. We say that g•f is a monotone transformation of f if g : f (X ) → R is a monotonically (and strictly) increasing function. Monotone transformations preserve the level sets of a function in the sense that L x (f ) = L x (g • f ). Because our algorithms depend only on the level set properties, our results generalize to any monotone transformation of a strongly convex and strongly smooth function. This leads to our extended notion of condition number. Definition 5. A function f has condition number Q ≥ 1 if it is the minimum ratio β/α over all functions g such that f is a monotone transformation of g and g is α-strongly convex and β smooth. When we work with low rank extensions of f , we only care about the condition number of f within a rank k subspace. Indeed, if f only varies along a rank k subspace, then it has a strong convexity value of 0, making its condition number undefined. If f is α-strongly convex and β-smooth, then its Hessian matrix always has eigenvalues bounded between α and β. Therefore, we need a notion of a projected condition number. Let A ∈ R d×k be some orthonormal matrix and let P A = AA be the projection matrix onto the column space of A. Definition 6. For some orthonormal A ∈ R d×k with d > k, a function f has condition number restricted to A, Q(A) ≥ 1, if it is the minimum ratio β/α over all functions g such that f is a monotone transformation of g and h(y) = g(Ay) is α-strongly convex and β smooth.

Section Title: ANALYSIS OF DESCENT STEPS
  ANALYSIS OF DESCENT STEPS The GLD template can be summarized as follows: given a sampling distribution D, we start at x 0 and in iteration t, we choose a scalar radii r t and we sample y t from a distribution r t D centered around x t , where r t provides the scaling of D. Then, if f (y t ) < f (x t ), we update x t+1 = y t ; otherwise, we set x t+1 = x t . The analysis of GLD follows from the main observation that the sub- level set of a monotone transformation of a strongly convex and strongly smooth function contains a ball of sufficiently large radius tangent to the level set (Lemma 15). In this section, we show that this property, combined with facts of high-dimensional geometry, implies that moving in a random direction from any point has a good chance of significantly improving the objective. As we mentioned before, the key to fast convergence is the careful choice of step sizes, which we describe in Theorem 7. The intuition here is that we would like to take as large steps as possible while keeping the probability of improving the objective function reasonably high, so by insights in high-dimensional geometry, we choose a step size of Θ(1/ √ n). Also, we show that if f (x) admits a latent rank-k structure, then this step size can be increased to Θ(1/ √ k) and is therefore only dependent on the latent dimensionality of f (x), allowing for fast high-dimensional optimization. Lastly, our geometric understanding allows us to show that our convergence rates are optimal with a matching lower bound. Without loss of generality, this section assumes that f (x) is strongly convex and smooth with condition number Q.

Section Title: STEP SIZE
  STEP SIZE Theorem 7. For any x such that 3 5Q x − x * ∈ [C 1 , C 2 ], we can find integers 0 ≤ k 1 , k 2 < log C2 C1 such that if r = 2 k1 C 1 or r = 2 −k2 C 2 , then a random sample y from uniform distribution over with probability at least 1 4 . Proving the above theorem requires the following lemma about the intersection of balls in high dimensions and it is proved in the appendix. Lemma 8. Let B 1 and B 2 be two balls in R n of radii r 1 and r 2 respectively. Let be the distance between the centers. If r 1 ∈ [ 2 √ n , √ n ] and r 2 ≥ − 4n , then vol (B 1 ∩ B 2 ) ≥ c n vol (B 1 ) , where c n is a dimension-dependent constant that is lower bounded by 1 4 at n = 1.

Section Title: GAUSSIAN SAMPLING AND LOW RANK STRUCTURE
  GAUSSIAN SAMPLING AND LOW RANK STRUCTURE A direct application of Lemma 8 seems to imply that uniform sampling of a high-dimensional ball is necessary. Upon further inspection, this can be easily replaced with a much simpler Gaussian sampling procedure that concentrates the mass close to the surface to the ball. This procedure lends itself to better analysis when f (x) admits a latent low-dimensional structure since any affine projection of a Gaussian is still Gaussian. Lemma 9. Let B 1 and B 2 be two balls in R n of radii r 1 and r 2 respectively. Let be the distance between the centers. If r 1 ∈ [ 2 √ n , √ n ] and r 2 ≥ − n and X = (X 1 , ..., X n ) are independent Gaussians with mean centered at the center of B 1 and variance r 2 1 n , then Pr[X ∈ B 2 ] > c, where c is a dimension-independent constant. Assume that there exists some rank k projection matrix P A such that f (x) = g(P A x), where k is much smaller than n. Because Gaussians projected on a k-dimensional subspace are still Gaussians, we show that our algorithm has a dimension dependence on k. We let Q g (A) be the condition number of g restricted to the subspace A that drives the dominant changes in f (x). with constant probability. Note that the speed-up in progress is due to the fact that we can now tolerate the larger sampling radius of Ω(1/ √ k), while maintaining a high probability of making progress. If k is unknown, we can simply use binary search to find the correct radius with an extra factor of log(n) in our runtime. The low-rank assumption is too restrictive to be realistic; however, our fast rates still hold, at least for the early stages of the optimization, even if we assume that f (x) = g(P A x) + h(x) and |h(x)| ≤ δ is a full-rank function that is bounded by δ. In this setting, we can show that convergence remains fast, at least until the optimality gap approaches δ.

Section Title: LOWER BOUNDS
  LOWER BOUNDS We show that our upper bounds given in the previous section are tight up to logarithmic factors for any symmetric sampling distribution D. These lower bounds are easily derived from our geometric perspective as we show that a sampling distribution with a large radius gives an extremely low probability of intersection with the desired sub-level set. Therefore, while gradient-approximation algorithms can be accelerated to achieve a runtime that depends on the square-root of the condition number Q, gradient-less methods that rely on random sampling are likely unable to be accelerated according to our lower bound. However, we emphasize that monotone invariance allows these results to apply to a broader class of objective functions, beyond smooth and convex, so the results can be useful in practice despite the seemingly worse theoretical bounds. Theorem 12. Let y = x + v, where v is a random sample from rD for some radius r > 0 and D is standard Gaussian or any rotationally symmetric distribution. Then, there exist a region X with positive measure such that for any x ∈ X,

Section Title: GRADIENTLESS ALGORITHMS
  GRADIENTLESS ALGORITHMS In this section, we present two algorithms that follow the same Gradientless Descent (GLD) tem- plate: GLD-Search and GLD-Fast, with the latter being an optimized version of the former when an upper bound on the condition number of a function is known. For both algorithms, since they are monotone-invariant, we appeal to the previous section to derive fast convergence rates for any monotone transform of convex f (x) with good condition number. We show the efficacy of both algorithms experimentally in the Experiments section.

Section Title: GRADIENTLESS DESCENT WITH BINARY SEARCH
  GRADIENTLESS DESCENT WITH BINARY SEARCH Although the sampling distribution D is fixed, we have a choice of radii for each iteration of the algorithm. We can apply a binary search procedure to ensure progress. The most straightforward version of our algorithm is thus with a naive binary sweep across an interval in [r, R] that is un- changed throughout the algorithm. This allows us to give convergence guarantees without previous knowledge of the condition number at a cost of an extra factor of log(n/ ). Theorem 13. Let x 0 be any starting point and f a blackbox function with condition number Q. Running Algorithm 1 with r = √ n , R = X and D = N (0, I) as a standard Gaussian returns a point x T such that x T − x * ≤ 2Q 3/2 after O(nQ log(n X / ) 2 ) function evaluations with high probability. Furthermore, if f (x) = g(P A x) admits a low-rank structure with P A a rank k matrix, then we only require O(kQ g (A) log(n X / ) 2 ) function evaluations to guarantee P A (x T − x * ) ≤ . This holds analogously even if f (x) = g(P A x) + h(x) is almost low-rank where |h| ≤ δ and > 60δkQ g (A).

Section Title: GRADIENTLESS DESCENT WITH FAST BINARY SEARCH
  GRADIENTLESS DESCENT WITH FAST BINARY SEARCH GLD-Search (Algorithm 1) uses a naive lower and upper bound for the search radius x t − x * , which incurs an extra factor of log(1/ ) in the runtime bound. In GLD-Fast, we remove this extra factor dependence on log(1/ ) by drastically reducing the range of the binary search. This is done by exploiting the assumption that f has a good condition number upper boundQ and by slowly halfing the diameter of the search space every few iterations since we expect x t → x * as t → ∞. Theorem 14. Let x 0 be any starting point and f a blackbox function with condition number upper bounded by Q. Running Algorithm 2 with suitable parameters returns a point x T such that f (x T ) − f (x * ) ≤ after O(nQ log 2 (Q) log( X / )) function evaluations with high probability. Furthermore, if f (x) = g(P A x) admits a low-rank structure with P A a rank k matrix, then we only require O(kQ g (A) log(n) log 2 (Q g (A)) log( X / )) function evaluations to guarantee P A (x T − x * ) ≤ . This holds analogously even if f (x) = g(P A x) + h(x) is almost low-rank where |h| ≤ δ and > 60δkQ g (A).

Section Title: EXPERIMENTS
  EXPERIMENTS We tested GLD algorithms on a simple class of objective functions and compare it to Accelerated Random Search (ARS) by  Nesterov & Spokoiny (2011) , which has linear convergence guarantees on strongly convex and strongly smooth functions. To our knowledge, ARS makes the weakest as- sumption among the zeroth-order algorithms that have linear convergence guarantees and perform only a constant order of operations per iteration. Our main conclusion is that GLD-Fast is compa- rable to ARS and tends to achieve a reasonably low error much faster than ARS in high dimensions (≥ 50). In low dimensions, GLD-Search is competitive with GLD-Fast and ARS though it requires no information about the function. We let H α,β,n ∈ R n×n be a diagonal matrix with its i-th diagonal equal to α + (β − α) i−1 n−1 . In simple words, its diagonal elements form an evenly space sequence of numbers from α to β. Our objective function is then f α,β,n : R n → R as f α,β,n (x) = 1 2 x H α,β,n x, which is α-strongly convex and β-strongly smooth. We always use the same starting point x = 1 √ n (1, . . . , 1), which requires X = √ Q for our algorithms. We plot the optimality gap f (b t ) − f (x * ) against the num- ber of function evaluations, where b t is the best point observed so far after t evaluations. Although all tested algorithms are stochastic, they have a low variance on the objective functions that we use; hence we average the results over 10 runs and omit the error bars in the plots. We ran experiments on f 1,8,n with imperfect curvature informationα andβ (see Figure 3 in ap- pendix). GLD-Search is independent of the condition number. GLD-Fast takes only one parameter, which is the upper bound on the condition number; if approximation factor is z, then we pass 8z as the upper bound. ARS requires both strong convexity and smoothness parameters. We test three dif- ferent distributions of the approximation error; when the approximation factor is z, then ARS-alpha gets (α/z, β), ARS-beta gets (α, zβ), and ARS-even gets (α/ √ z, √ zβ) as input. GLD-Fast is more robust and faster than ARS when the condition number is over-approximated. When the condition number is underestimated, GLD-Fast still steadily converges.

Section Title: MONOTONE TRANSFORMATIONS
  MONOTONE TRANSFORMATIONS In  Figure 1 , we ran experiments on f 1,8,n for different settings of dimensionality n, and its monotone transformation with g(y) = − exp(− √ y). For this experiment, we assume a perfect oracle for the strong convexity and smoothness parameters of f . The convergence of GLD is totally unaffected by the monotone transformation. For the low-dimension cases of a transformed function (bottom half of the figure), we note that there are inflection points in the convergence curve of ARS. This means that ARS initially struggles to gain momentum and then struggles to stop the momentum when it gets close to the optimum. Another observation is that unlike ARS that needs to build up momentum, GLD-Fast starts from a large radius and therefore achieves a reasonably low error much faster than ARS, especially in higher dimensions.

Section Title: BBOB BENCHMARKS
  BBOB BENCHMARKS To show that practicality of GLD on practical and non-convex settings, we also test GLD algorithms on a variety of BlackBox Optimization Benchmarking (BBOB) functions ( Hansen et al., 2009 ). For each function, the optima is known and we use the log optimality gap as a measure of competance. Because each function can exhibit varying forms of non-smoothness and convexity, all algorithms are ran with a smoothness constant of 10 and a strong convexity constant of 0.1. All other setup details are same as before, such as using a fixed starting point. The plots, given in Appendix C, underscore the superior performance of GLD algorithms on various BBOB functions, demonstrating that GLD can successfully optimize a diverse set of functions even without explicit knowledge of condition number. We note that BBOB functions are far from convex and smooth, many exhibiting high conditioning, multi-modal valleys, and weak global structure. Due to our radius search produce, our algorithm appears more robust to non-ideal settings with non-convexity and ill conditioning. As expected, we note that GLD-Fast tend to outperform GLD- Search, especially as the dimension increases, matching our theoretical understanding of GLD.

Section Title: MUJOCO CONTROL BENCHMARKS AND AFFINE TRANSFORMATIONS
  MUJOCO CONTROL BENCHMARKS AND AFFINE TRANSFORMATIONS We also ran experiments on the Mujoco benchmarks with varying architectures, both linear and nonlinear. This demonstrates the viability of our approach even in the non-convex, high dimensional setting. We note that however, unlike e.g. ES which uses all queries to form a gradient direction, our algorithm removes queries which produce less reward than using the current arg-max, which can be an information handicap. Nevertheless, we see that our algorithm still achieves competitive performance on the maximum reward. We used a horizon of 1000 for all experiments. We further tested the affine invariance of GLD on the policy parameters from using Gaussian ball sampling, under the HalfCheetah benchmark by projecting the state s of the MDP with linear policy to a higher dimensional state W s, using a matrix multiplication with an orthonormal W . Specif- ically, in this setting, for a linear policy parametrized by matrix K, the objective function is thus J(KW ) where π K (W s) = KW s. Note that when projecting into a high dimension, there is a slowdown factor of log dnew d old where d new , d old are the new high dimension and previous base di- mension, respectively, due to the binary search in our algorithm on a higher dimensional space. For our HalfCheetah case, we projected the 17 base dimension to a 200-length dimension, which suggests that the slowdown factor is a factor log 200 17 ≈ 3.5. This can be shown in our plots in the appendix (Figure 15).

Section Title: CONCLUSION
  CONCLUSION We introduced GLD, a robust zeroth-order optimization algorithm that is simple, efficient, and we show strong theoretical convergence bounds via our novel geometric analysis. As demonstrated by our experiments on BBOB and MuJoCo benchmarks, GLD performs very robustly even in the non-convex setting and its monotone and affine invariance properties give theoretical insight on its practical efficiency. GLD is very flexible and allows easy modifications. For example, it could use momentum terms to keep moving in the same direction that improved the objective, or sample from adaptively cho- sen ellipsoids similarly to adaptive gradient methods. ( Duchi et al., 2011 ;  McMahan & Streeter, 2010 ). Just as one may decay or adaptively vary learning rates for gradient descent, one might use a similar change the distribution from which the ball-sampling radii are chosen, perhaps shrinking the minimum radius as the algorithm progresses, or concentrating more probability mass on smaller radii. Likewise, GLD could be combined with random restarts or other restart policies developed for gra- dient descent. Analogously to adaptive per-coordinate learning rates  Duchi et al. (2011) ;  McMahan & Streeter (2010) , one could adaptively change the shape of the balls being sampled into ellipsoids with various length-scale factors. Arbitrary combinations of the above variants are also possible.

```
