Title:
```
Under review as a conference paper at ICLR 2020 COMPOSITIONAL TRANSFER IN HIERARCHICAL REINFORCEMENT LEARNING
```
Abstract:
```
The successful application of flexible, general learning algorithms to real-world robotics applications is often limited by their poor data-efficiency. To address the challenge, domains with more than one dominant task of interest encourage the sharing of information across tasks to limit required experiment time. To this end, we investigate compositional inductive biases in the form of hierarchical policies as a mechanism for knowledge transfer across tasks in reinforcement learning (RL). We demonstrate that this type of hierarchy enables positive transfer while mitigating negative interference. Furthermore, we demonstrate the benefits of additional incentives to efficiently decompose task solutions. Our experiments show that these incentives are naturally given in multitask learning and can be easily introduced for single objectives. We design an RL algorithm that enables stable and fast learning of hierarchical policies and the effective reuse of both behavior components and transition data across tasks in an off-policy setting for complex, real-world domains. Finally, we evaluate our algorithm in simulated environments as well as physical robot experiments and demonstrate substantial improvements in data data-efficiency over competitive baselines.
```

Figures/Tables Captions:
```
Figure 1: Using a hierarchical policy with different component initialization (red curve) demonstrates benefits over homogeneous initialization as well as the flat Gaussian policy. The plot shows that the simple change in initialization is sufficient to enable component specialization and the correlated improvement in performance.
Figure 2: Results for the multitask robotic manipulation experiments in simulation. The dashed line corresponds to the performance of the SVG-based implementation of SAC-U. From left to right: Pile1, Pile2, Cleanup2. We show averages over 3 runs each, with corresponding standard deviation. RHPO outperforms both baselines across all tasks with the benefits increasing for more complex domains.
Figure 3: Left: Overview of the real robot setup with the Sawyer robot performing the Pile1 task. Screen pixelated for anonymization. Middle: Simulated Sawyer performing the same task. Right: Cleanup2 setup with the Jaco.
Figure 4: Robot Experiments. Left: While simpler tasks such as reaching are learned with comparable efficiency, the later, more complex tasks are acquired significantly faster with a hierarchical policy. Right: Similarities between tasks (based on their distribution over components) and similarities between components (based on the distribution over tasks which apply them).
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION While recent successes in deep (reinforcement) learning for computer games (Atari ( Mnih et al., 2013 ), StarCraft ( Vinyals et al., 2019 )), Go ( Silver et al., 2017 ) and other high-throughput domains, e.g. ( OpenAI et al., 2018 ), have demonstrated the potential of these methods in the big data regime, the high cost of data acquisition has so far limited progress in many tasks of real-world relevance. Data efficiency in machine learning generally relies on inductive biases to guide and accelerate the learning process; e.g. by including expert domain knowledge of varying granularity. Incorporating such knowledge can accelerate learning - but when inaccurate it can also inappropriately bias the space of solutions and lead to sub-optimal results. Robotics represents a domain in which data efficiency is critical, and human prior knowledge is commonly provided. However, for scalability and reduced dependency on human accuracy, we can instead utilise an agent's permanent embodiment and shared environment across tasks. Intuitively, such a scenario suggests the natural strategy of focusing on inductive biases that facilitate the sharing and reuse of experience and knowledge across tasks while other aspects of the domain can be learned. As a general principle this relieves us from the need to inject detailed knowledge about the domain, instead we can focus on general principles that facilitate reuse ( Caruana, 1997 ). Successes for transfer learning have, for example, built on optimizing initial parameters (e.g.  Finn et al., 2017 ), sharing models and parameters across tasks either in the form of policies or value functions (e.g.  Rusu et al., 2016 ;  Teh et al., 2017 ;  Galashov et al., 2018 ), data-sharing across tasks (e.g.  Riedmiller et al., 2018 ;  Andrychowicz et al., 2017 ), or through the use of task-related auxiliary objectives ( Jaderberg et al., 2016 ;  Wulfmeier et al., 2017 ). Transfer between tasks can, however, lead to either constructive or destructive transfer for humans ( Singley and Anderson, 1989 ) as well as for machines ( Pan and Yang, 2010 ;  Torrey and Shavlik, 2010 ). That is, jointly learning to solve different tasks can provide both benefits and disadvantages for individual tasks, depending on their similarity. Finding a mechanism that enables transfer where possible but avoids interference is one of the long-standing research challenges.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 In this paper we explore the benefits and limitations of hierarchical policies in single and multitask reinforcement learning. Similar to Mixture Density Networks ( Bishop, 1994 ) our models represent policies as state-conditional Gaussian mixture distributions, with separate Gaussian mixture com- ponents as low-level policies which can be selected by the high-level controller via a categorical action choice. In the multitask setting, to obtain more robust and versatile low-level behaviors, we additionally shield the mixture components from information about the task at hand. In this case, task information is only communicated through the choice of mixture component by the high-level controller, and the mixture components can be seen as domain-dependant, task-independent skills although the nature of these skills is not predefined and emerges during end-to-end training. We implement this idea by building on three forms of transfer: targeted exploration via the con- catenation of tasks within one episode ( Riedmiller et al., 2018 ), sharing transition data across tasks ( Andrychowicz et al., 2017 ;  Riedmiller et al., 2018 ), and reusing low-level components of the aforementioned policy class. To this end we develop a novel robust and data-efficient multitask actor-critic algorithm, Regularized Hierarchical Policy Optimization (RHPO). Our algorithm uses the multitask learning aspects of SAC ( Riedmiller et al., 2018 ) to improve data-efficiency and robust policy optimization properties of MPO ( Abdolmaleki et al., 2018a ) in order to optimize hierarchical policies. We furthermore demonstrate the generality of hierarchical policies for multitask learning via improving results also after replacing MPO as policy optimizer with another gradient-based, entropy-regularized policy optimizer ( Heess et al., 2015 ) (see Appendix A.10). We demonstrate that compositional, hierarchical policies - while strongly reducing training time in multitask domains - can fail to improve performance in single task domains if no additional inductive biases are given. While multitask domains provide sufficient pressure for component specialization, and the related possibility for composition, we are required to introduce additional incentives to encourage similar developments for single task domains. In the multitask setting, we demonstrate considerably improved performance, robustness and learning speed compared to competitive continuous control baselines demonstrating the relevance of hierarchy for data-efficiency and transfer. We finally evaluate our approach on a physical robot for robotic manipulation tasks where RHPO leads to a significant speed up in training, enabling it to solve challenging stacking tasks on a single robot 1 .

Section Title: PRELIMINARIES
  PRELIMINARIES We consider a multitask reinforcement learning setting with an agent operating in a Markov De- cision Process (MDP) consisting of the state space S, the action space A, the transition proba- bility p(s t+1 |s t , a t ) of reaching state s t+1 from state s t when executing action a t at the previ- ous time step t. The actions are drawn from a probability distribution over actions π(a|s) re- ferred to as the agent's policy. Jointly, the transition dynamics and policy induce the marginal state visitation distribution p(s). Finally, the discount factor γ together with the reward r(s, a) gives rise to the expected reward, or value, of starting in state s (and following π thereafter) V π (s) = E π [ ∞ t=0 γ t r(s t , a t )|s 0 = s, a t ∼ π(·|s t ), s t+1 ∼ p(·|s t , a t )]. Furthermore, we de- fine multitask learning over a set of tasks i ∈ I with common agent embodiment as follows. We assume shared state, action spaces and shared transition dynamics across tasks; tasks only differ in their reward function r i (s, a). Furthermore, we consider task conditional policies π(a|s, i). The overall objective is defined as

Section Title: METHOD
  METHOD This section introduces Regularized Hierarchical Policy Optimization (RHPO) which focuses on efficient training of modular policies by sharing data across tasks; extending the data-sharing and scheduling mechanisms from Scheduled Auxiliary Control with randomized scheduling (SAC-U) ( Riedmiller et al., 2018 ). We start by introducing the considered class of policies, followed by the required combination - and extension - of MPO ( Abdolmaleki et al., 2018a ) and SAC-U ( Riedmiller et al., 2018 ) for training structured hierarchical policies in a multitask, off-policy setting.

Section Title: HIERARCHICAL POLICIES
  HIERARCHICAL POLICIES We start by defining the hierarchical policy class which supports sharing sub-policies across tasks. Formally, we decompose the per-task policy π(a|s, i) as π θ (a|s, i) = M o=1 π L θ (a|s, o) π H θ (o|s, i) , (2) with π H and π L respectively representing a "high-level" switching controller (a categorical distribu- tion) and a "low-level" sub-policy (components of the resulting mixture distribution), where o is the index of the sub-policy. Here, θ denotes the parameters of both π H and π L , which we will seek to optimize. While the number of components has to be decided externally, the method is robust with respect to this parameter (Appendix A.8.3). Note that, in the above formulation only the high-level controller π H is conditioned on the task information i; i.e. we employ a form of information asymmetry ( Galashov et al., 2018 ;  Tirumala et al., 2019 ;  Heess et al., 2016 ) to enable the low-level policies to acquire general, task-independent behaviours. This choice strengthens decomposition of tasks across domains and inhibits degenerate cases of bypassing the high-level controller. Intuitively, these sub-policies can be understood as building reflex-like low-level control loops, which perform domain-dependent but task-independent behaviours and can be modulated by higher cognitive functions with knowledge of the task at hand.

Section Title: DATA-EFFICIENT MULTITASK POLICY OPTIMIZATION
  DATA-EFFICIENT MULTITASK POLICY OPTIMIZATION In the following sections, we present the equations underlying RHPO. For the complete pseudocode algorithm the reader is referred to the Appendix A.2.1. To optimize the policy class described above we build on the MPO algorithm ( Abdolmaleki et al., 2018a ) which decouples the policy improvement step (optimizing J independently of the policy structure) from the fitting of the hierarchical policy. Concretely, we first introduce an intermediate non-parametric policy q(a|s, i) and consider optimizing J(q) while staying close, in expectation, to a reference policy π ref (a|s, i) ≤ , (3) where KL(· ·) denotes the Kullback Leibler divergence, defines a bound on the KL, D denotes the data contained in a replay buffer, and assuming that we have an approximation of the ground-truth state-action value functionQ(s, a, i) ≈ Q π (s, a, i) available (see Equation (4) for details on learninĝ Q from off-policy data). Starting from an initial policy π θ0 we can then iterate the following steps to improve the policy π θ k : Policy Evaluation: UpdateQ such thatQ(s, a, i) ≈Q π θ k (s, a, i), see Equation (4).

Section Title: Policy Improvement:
  Policy Improvement: - Step 1: Obtain q k = arg max q J(q), under KL constraints with π ref = π θ k (Equation (3)). - Step 2: Obtain θ k+1 = arg min θ E s∼D,i∼I KL q k (·|s, i) π θ (·|s, i) , under additional regularization (Equation (6)).

Section Title: Multitask Policy Evaluation
  Multitask Policy Evaluation For data-efficient off-policy learning ofQ we build on scheduled auxiliary control with uniform scheduling (SAC-U) ( Riedmiller et al., 2018 ) which exploits two main Under review as a conference paper at ICLR 2020 ideas to obtain data-efficiency: i) experience sharing across tasks; ii) switching between tasks within one episode for improved exploration. Formally, we assume access to a replay buffer containing data gathered from all tasks, which is filled asynchronously to the optimization (similar to e.g.  Espeholt et al. (2018) ) where for each trajectory snippet τ = {(s 0 , a 0 , R 0 ), . . . , (s L , a L , R L )} we record the rewards for all tasks R t = [r i1 (s t , a t ), . . . , r i |I| (s t , a t )] as a vector in the buffer. Using this data we define the retrace objective for learningQ, parameterized via φ, following ( Munos et al., 2016 ;  Riedmiller et al., 2018 ) as min φ L(φ) = i∼I E τ ∼D r i (s t , a t ) + γQ ret (s t+1 , a t+1 , i) −Q φ (s t , a t , i)) 2 , (4) where Q ret is the L-step retrace target ( Munos et al., 2016 ), see the Appendix A.2.2 for details.

Section Title: Multitask Policy Improvement 1: Obtaining Non-parametric Policies
  Multitask Policy Improvement 1: Obtaining Non-parametric Policies We first find the interme- diate policy q by maximizing Equation (3). We obtain a closed-form solution with a non-parametric policy for each task, as q k (a|s, i) ∝ π θ k (a|s, i) exp Q (s, a, i) η , (5) where η is a temperature parameter (corresponding to a given bound ) which is optimized alongside the policy optimization (see Appendix A.1.1 for a detailed derivation of the multitask case). As mentioned above, this policy representation is independent of the form of the parametric policy π θ k ; i.e. q only depends on π θ k through its density. This, crucially, makes it easy to employ complicated structured policies (such as the one introduced in Section 3.1). The only requirement here, and in the following steps, is that we must be able to sample from π θ k and calculate the gradient (w.r.t. θ k ) of its log density (but the sampling process itself need not be differentiable).

Section Title: Multitask Policy Improvement 2: Fitting Parametric Policies
  Multitask Policy Improvement 2: Fitting Parametric Policies In the second step we fit a pol- icy to the non-parametric distribution obtained from the previous calculation by minimizing the divergence E s∼D,i∼I [KL(q k (·|s, i) π θ (·|s, i))]. Assuming that we can sample from q k this step corresponds to maximum likelihood estimation (MLE). Furthermore, we can regularize towards smoothly changing distributions during training - effectively mitigating optimization instabilities and introducing an inductive bias - by limiting the change of the policy (a trust-region constraint). The idea is commonly used in on- as well as in off-policy RL ( Schulman et al., 2015 ;  Abdolmaleki et al., 2018b ;a). The application to hierarchical policy classes highlights the importance of this constraint as investigated in Section 4.2. Formally, we aim to obtain the solution where m defines a bound on the change of the new policy. Here we drop constant terms and the negative sign in the second line (turning min into max), and explicitly insert the definition π θ (a|s, i) = M o=1 π L (a|s, o) π H (o|s, i), to highlight that we are marginalizing over the high-level choices in this fitting step (since q is not tied to the policy structure). Hence, the update is independent of the specific policy component from which the action was sampled, enabling joint updates of all components. This reduces the variance of the update and also enables efficient off-policy learning. Different approaches can be used to control convergence for both the "high-level" categorical choices and the action choices to change slowly throughout learning. The average KL constraint in Equation (6) is similar in nature to an upper bound on the computationally intractable KL divergence between the two mixture distributions and has been determined experimentally to perform better in practice than simple bounds. In practice, in order to control the change of the high level and low level policies independently we decouple the constraints to be able to set different for the means ( µ ), covariances ( Σ ) and the categorical distribution ( α ) in case of a mixture of Gaussian policy. To solve Equation Under review as a conference paper at ICLR 2020 (6), we first employ Lagrangian relaxation to make it amenable to gradient based optimization and then perform a fixed number of gradient descent steps (using Adam ( Kingma and Ba, 2014 )); details on this step, as well as an algorithm listing, can be found in the Appendix A.1.2.

Section Title: EXPERIMENTS
  EXPERIMENTS In the following sections, we investigate the effects of training hierarchical policies in single and multitask domains, finally demonstrating how RHPO can provide compelling benefits for multitask learning in real and simulated robotic manipulation tasks and significantly reduce platform interaction time. In the context of single-task domains from the DeepMind Control Suite ( Tassa et al., 2018 ), we first demonstrate how this type of hierarchy on its own fails to improve performance and that for the model to exploit compositionality, additional incentives for component specialization are required. Subsequently, we introduce suited incentives leading to improved performance and demonstrate that the variety of objectives in multitask domains can serve the same purpose. The evaluation includes experiments on physical hardware with robotic manipulation tasks for the Sawyer arm, emphasizing the importance of data-efficiency. More details on task hyperparameters as well as the results for additional ablations and all tasks from the multitask domains are provided in the Appendix A.4. Across all tasks, we build on a dis- tributed actor-critic framework (similar to ( Espeholt et al., 2018 )) with flexible hardware assignment ( Buchlovsky et al., 2019 ) to train all agents, performing critic and policy updates from a replay buffer, which is asynchronously filled by a set of actors. In all figures with error bars, we visualize mean and variance derived from 3 runs.

Section Title: SIMULATED SINGLE TASK EXPERIMENTS
  SIMULATED SINGLE TASK EXPERIMENTS We consider two high-dimensional tasks for continuous control: humanoid-run and humanoid-stand from  Tassa et al. (2018)  and compare a flat Gaussian policy to a hierarchical policy, a mixture of Gaussians with three components. We align the update rates of all approaches for fair comparison and to focus the comparison of the algorithm and not its specific implementation 2 .  Figure 1  visualizes the results in terms of the number of actor episodes. As can be observed, the hierarchical policy performs comparable to a flat policy with well aligned means and variances for all components as the model fails to decompose the problem. While both the flat and hierarchical policy are initialized with means close to zero, we now include another hierarchical policy with distributed initial means for the three components ranging for all dimensions from minimum to maximum of the allowed action range (here: -1, 0, 1). This simple change suffices to enable component specialization and significantly improved performance.

Section Title: SIMULATED MULTITASK EXPERIMENTS
  SIMULATED MULTITASK EXPERIMENTS We use three simulated multitask scenarios with the Kinova Jaco and Rethink Robotics Sawyer robot arms to test in a variety of conditions. Pile1: Here, the seven tasks of interest range from simple reaching for a block over tasks like grasping it, to the final task of stacking the block on top of another block. In addition to the experiments in simulation, which are executed with 5 actors in a distributed setting, the same Pile1 multitask domain (same rewards and setup) is investigated with a single, physical robot in Section 4.3. We further extend the evaluation towards two more complex multitask domains in simulation. The first extension includes stacking with both blocks on top of the respective Under review as a conference paper at ICLR 2020 other block, resulting in a setting with 10 tasks (Pile2). And a last domain including harder tasks such as opening a box and placing blocks into this box, consisting of a total of 13 tasks (Cleanup2). We compare RHPO for training hierarchical policies against a flat, monolithic policy shared across all tasks which is provided with the additional task id as input (displayed as Monolithic in the plot) as well as policies with task dependent heads (displayed as Independent in the plots) following ( Riedmiller et al., 2018 ) - both using MPO as the optimizer and a re-implementation of SAC-U using SVG ( Heess et al., 2015 ) (which is related to a version of the option critic ( Bacon et al., 2017 ) without temporal abstraction). The baselines provide the two opposite, naive perspectives on transfer: by using the same monolithic policy across tasks we enable positive as well as negative interference and independent policies prevent policy-based transfer. After experimentally confirming the robustness of RHPO with respect to the number of low-level sub-policies (see Appendix A.8.3), we set M proportional to the number of tasks in each domain.  Figure 2  demonstrates that the hierarchical policy (RHPO) outperforms the monolithic as well as the independent baselines. For simple tasks such as Pile1, the difference is smaller, but the more tasks are trained and the more complex the domain becomes (cf. Pile2 and Cleanup2), the greater is the advantage of composing learned behaviours across tasks. Compared to SVG ( Heess et al., 2015 ), we observe that the baselines based on MPO already result in an improvement, which becomes even bigger with the hierarchical policies. The results across all domains exhibit performance gains for the hierarchical model without the additional incentives from Section 4.1, demonstrating the sufficiency of variety in the training objectives to encourage component specialization and problem decomposition.

Section Title: PHYSICAL ROBOT EXPERIMENTS
  PHYSICAL ROBOT EXPERIMENTS For real-world experiments, data-efficiency is crucial. We perform all experiments in this section relying on a single robot (single actor) - demonstrating the benefits of RHPO in the low data regime. The performed task is the real world version of the Pile1 task described in Section 4.2. The main task objective is to stack one cube onto a second one and move the gripper away from it. We introduce an additional third cube which serves purely as a distractor. The setup for the experiments consists of a Sawyer robot arm mounted on a table, equipped with a Robotiq 2F-85 parallel gripper. A basket of size 20cm 2 in front of the robot contains the three cubes. Three cameras on the basket track the cubes using fiducials (augmented reality tags). As in simulation, Under review as a conference paper at ICLR 2020 the agent is provided with proprioception information (joint positions, velocities and torques), a wrist sensor's force and torque readings, as well as the cubes' poses - estimated via the fiducials. The agent action is five dimensional and consists of the three Cartesian translational velocities, the angular velocity of the wrist around the vertical axis and the speed of the gripper's fingers.  Figure 4  plots the learning progress on the real robot for two (out of 7) of the tasks, the simple reach tasks and the stack task - which is the main task of interest. Plots for the learning progress of all tasks are given in the appendix A.6. As can be observed, all methods manage to learn the reach task quickly (within about a few thousand episodes) but only RHPO with a hierarchical policy is able to learn the stacking task (taking about 15 thousand episodes to obtain good stacking success), which takes about 8 days of training on the real robot with considerably slower progress for all baselines. In addition, we compute distributions for each component over the tasks which activate it, as well as distributions for each task over which components are being used. For each set of distributions, we determine the Battacharyya distance metric to determine the similarity between tasks and the similarity between components in  Figure 4  (right). The plots demonstrate how the components specialise, but also provide a way to investigate our tasks, showing e.g. that the first reach task is fairly independent and that the last four tasks are comparably similar regarding the high-level components applied for their solution.

Section Title: ADDITIONAL ABLATIONS
  ADDITIONAL ABLATIONS We perform a series of ablations based on the earlier introduced Pile1 domain, providing additional insights into benefits and shortcomings of RHPO and important factors for robust training. The algorithm is well suited for sequential transfer learning based on solving new tasks with pre-trained low-level components (Appendix A.9). We demonstrate the robustness of RHPO with respect to the number of sub-policies as well as importance of choice of regularization respectively in Appendix A.8.1 and A.8.3. Finally, we ablate over the number of data-generating actors to evaluate all approaches with respect to data rate and illustrate how hierarchical policies are particularly relevant at lower data rates such as given by real-world robotics applications in Appendix A.8.2.

Section Title: RELATED WORK
  RELATED WORK Transfer learning, in particular in the multitask context, has long been part of machine learning (ML) for data-limited domains ( Caruana, 1997 ;  Torrey and Shavlik, 2010 ;  Pan and Yang, 2010 ;  Taylor and Stone, 2009 ). Commonly, it is not straightforward to train a single model jointly across different tasks as the solutions to tasks might not only interfere positively but also negatively ( Wang et al., 2018 ). Preventing this type of forgetting or negative transfer presents a challenge for biological ( Singley and Anderson, 1989 ) as well as artificial systems ( French, 1999 ). In the context of ML, a common scheme is the reduction of representational overlap ( French, 1999 ;  Rusu et al., 2016 ;  Wang et al., 2018 ).  Bishop (1994)  utilize neural networks to parametrize mixture models for representing multi-modal distributions thus mitigating shortcomings of non-hierarchical approaches.  Rosenstein et al. (2005)  demonstrate the benefits of hierarchical classification models to limit the impact of negative transfer.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Hierarchical approaches have a long history in the reinforcement learning literature (e.g.  Sutton et al., 1999 ;  Dayan and Hinton, 1993 ). Prior work commonly benefits from combining hierarchy with additional inductive biases such as ( Vezhnevets et al., 2017 ;  Nachum et al., 2018a ;b;  Xie et al., 2018 ) which employ different rewards for different levels of the hierarchy rather than optimizing a single objective for the entire model as we do. Other works have shown the additional benefits for the stability of training and data-efficiency when sequences of high-level actions are given as guidance during optimization in a hierarchical setting ( Shiarlis et al., 2018 ;  Andreas et al., 2017 ;  Tirumala et al., 2019 ). Instead of introducing additional training signals, we directly investigate the benefits of compositional hierarchy as provided structure for transfer between tasks. Hierarchical models for probabilistic trajectory modelling have been used for the discovery of behavior abstractions as part of an end-to-end reinforcement learning paradigm (e.g.  Teh et al., 2017 ;  Igl et al., 2019 ;  Tirumala et al., 2019 ;  Galashov et al., 2018 ) where the models act as learned inductive biases that induce the sharing of behavior across tasks. In a vein similar to the presented algorithm, (e.g  Heess et al., 2016 ;  Tirumala et al., 2019 ) share a low-level controller across tasks but modulate the low-level behavior via a continuous embedding rather than picking from a small number of mixture components. In related work  Hausman et al. (2018) ;  Haarnoja et al. (2018)  learn hierarchical policies with continuous latent variables optimizing the entropy regularized objective. Similar to our work, the options framework ( Sutton et al., 1999 ;  Precup, 2000 ) supports behavior hierarchies, where the higher level chooses from a discrete set of sub-policies or "options" which commonly are run until a termination criterion is satisfied. The framework focuses on the notion of temporal abstraction. A number of works have proposed practical and scalable algorithms for learning option policies with reinforcement learning (e.g.  Bacon et al., 2017 ;  Zhang and Whiteson, 2019 ;  Smith et al., 2018 ;  Riemer et al., 2018 ;  Harb et al., 2018 ) or criteria for option induction (e.g.  Harb et al., 2018 ;  Harutyunyan et al., 2019 ). Rather than the additional inductive bias of temporal abstraction, we focus on the investigation of composition as type of hierarchy in the context of single and multitask learning while demonstrating the strength of hierarchical composition to lie in domains with strong variation in the objectives - such as in multitask domains. We additionally introduce a hierarchical extension of SVG ( Heess et al., 2015 ), to investigate similarities to work on the option critic ( Bacon et al., 2017 ). With the use of KL regularization to different ends in RL, work related to RHPO focuses on contextual bandits ( Daniel et al., 2016 ). The algorithm builds on a 2-step EM like procedure to optimize linearly parametrized mixture policies. However, their algorithm has been used only with low dimensional policy representations, and in contextual bandit and other very short horizon settings. Our approach is designed to be applicable to full RL problems in complex domains with long horizons and with high-capacity function approximators such as neural networks. This requires robust estimation of value function approximations, off-policy correction, and additional regularization for stable learning.

Section Title: DISCUSSION
  DISCUSSION We introduce a novel framework to enable robust training and investigation of hierarchical, com- positional policies in complex simulated and real-world tasks as well as provide insights into the learning process and its stability. In simulation as well as on real robots, RHPO outperforms base- line methods which either handle tasks independently or utilize implicit sharing. Especially with increasingly complex tasks or limited data rate, as given in real-world applications, we demonstrate hierarchical inductive biases to provide a compelling foundation for transfer learning, reducing the number of environment interactions significantly and often leading to more robust learning as well as improved final performance. For single tasks with a single training objective all components can remain aligned, preventing problem decomposition and the hierarchical policy replicates a flat policy. Performance improvements appear only when the individual components specialize, either via variety in the training objectives or additional incentives. Furthermore, as demonstrated in Appendix A.9, a pre-trained set of specialized components can notably improve performance when learning new tasks. One important next step is identifying how to optimize a basis set of components which transfers well to a wide range of tasks Since with mixture distributions, we are able to marginalize over components when optimizing the weighted likelihood over action samples in Equation 6, the extension towards multiple levels of hierarchy is trivial but can provide a valuable direction for practical future work. While this approach partially mitigates negative interference between tasks in a parallel multitask learning scenario, addressing catastrophic inference in sequential settings remains a challenge.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 We believe that especially in domains with consistent agent embodiment and high costs for data generation learning tasks jointly and information sharing is imperative. RHPO combines several ideas that we believe will be important: multitask learning with hierarchical and compositional policy representations, robust optimization, and efficient off-policy learning. Although we have found this particular combination of components to be very effective we believe it is just one instance of - and step towards - a spectrum of efficient learning architectures that will unlock further applications of RL both in simulation and, importantly, on physical hardware.

```
