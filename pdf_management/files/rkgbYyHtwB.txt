Title:
```
Published as a conference paper at ICLR 2020 DISAGREEMENT-REGULARIZED IMITATION LEARNING
```
Abstract:
```
We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems on which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative ad- versarial imitation learning.
```

Figures/Tables Captions:
```
Figure 1: Example of a problem where behavioral cloning incurs quadratic regret.
Figure 2: Results on tabular MDP from (Ross & Bagnell, 2010). Shaded region represents range between 5 th and 95 th quantiles, computed across 500 trials. Behavior cloning exhibits poor worst- case regret, whereas DRIL has low regret across all trials.
Figure 3: Results on Atari environments. a) Median final policy performance for different numbers of expert trajectories, taken over 4 seeds (shaded regions are min/max performance) b) Evolution of policy reward and uncertainty cost during training with N = 3 trajectories.
Figure 4: Results on continuous control tasks.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Training artificial agents to perform complex tasks is essential for many applications in robotics, video games and dialogue. If success on the task can be accurately described using a reward or cost function, reinforcement learning (RL) methods offer an approach to learning policies which has proven to be successful in a wide variety of applications ( Mnih et al., 2015 ; 2016;  Lillicrap et al., 2016 ;  Hessel et al., 2018 ). However, in other cases the desired behavior may only be roughly specified and it is unclear how to design a reward function to characterize it. For example, training a video game agent to adopt more human-like behavior using RL would require designing a reward function which characterizes behaviors as more or less human-like, which is difficult. Imitation learning (IL) offers an elegant approach whereby agents are trained to mimic the demon- strations of an expert rather than optimizing a reward function. Its simplest form consists of training a policy to predict the expert's actions from states in the demonstration data using supervised learn- ing. While appealingly simple, this approach suffers from the fact that the distribution over states observed at execution time can differ from the distribution observed during training. Minor errors which initially produce small deviations become magnified as the policy encounters states further and further from its training distribution. This phenomenon, initially noted in the early work of ( Pomerleau, 1989 ), was formalized in the work of ( Ross & Bagnell, 2010 ) who proved a quadratic O( T 2 ) bound on the regret and showed that this bound is tight. The subsequent work of ( Ross et al., 2011 ) showed that if the policy is allowed to further interact with the environment and make queries to the expert policy, it is possible to obtain a linear bound on the regret. However, the ability to query an expert can often be a strong assumption. In this work, we propose a new and simple algorithm called DRIL (Disagreement-Regularized Im- itation Learning) to address the covariate shift problem in imitation learning, in the setting where the agent is allowed to interact with its environment. Importantly, the algorithm does not require any additional interaction with the expert. It operates by training an ensemble of policies on the demonstration data, and using the disagreement in their predictions as a cost which is optimized through RL together with a supervised behavioral cloning cost. The motivation is that the policies in the ensemble will tend to agree on the set of states covered by the expert, leading to low cost, but are more likely to disagree on states not covered by the expert, leading to high cost. The RL cost Published as a conference paper at ICLR 2020 thus guides the agent back towards the distribution of the expert, while the supervised cost ensures that it mimics the expert within the expert's distribution. Our theoretical results show that, subject to realizability and optimization oracle assumptions 1 , our algorithm obtains a O( κT ) regret bound, where κ is a measure which quantifies a tradeoff between the concentration of the demonstration data and the diversity of the ensemble outside the demon- stration data. We evaluate DRIL empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning, often recovering expert performance with only a few trajectories.

Section Title: PRELIMINARIES
  PRELIMINARIES We consider episodic finite horizon MDP in this work. Denote by S the state space, A the action space, and Π the class of policies the learner is considering. Let T denote the task horizon and π the expert policy whose behavior the learner is trying to mimic. For any policy π, let d π denote the distribution over states induced by following π. Denote C(s, a) the expected immediate cost of performing action a in state s, which we assume is bounded in [0, 1]. In the imitation learning setting, we do not necessarily know the true costs C(s, a), and instead we observe expert demonstrations. Our goal is to find a policy π which minimizes an observed surrogate loss between its actions and the actions of the expert under its induced distribution of states, i.e. For the following, we will assume is the total variation distance (denoted by · ), which is an upper bound on the 0 − 1 loss. Our goal is thus to minimize the following quantity, which represents the distance between the actions taken by our policy π and the expert policy π : Denote Q π t (s, a) as the standard Q-function of the policy π, which is defined as Q π t (s, a) = E T τ =t C(s τ , a τ )|(s t , a t ) = (s, a), a τ ∼ π . The following result shows that if is an upper bound on the 0 − 1 loss and C satisfies certain smoothness conditions, then minimizing this loss within translates into an O( T ) regret bound on the true task cost J C (π) = E s,a∼dπ [C(s, a)]: Theorem 1. ( Ross et al., 2011 ) If π satisfies J exp (π) = , and Q π T −t+1 (s, a) − Q π T −t+1 (s, π ) ≤ u for all time steps t, actions a and states s reachable by π, then J C (π) ≤ J C (π ) + uT . Unfortunately, it is often not possible to optimize J exp directly, since it requires evaluating the expert policy on the states induced by following the current policy. The supervised behavioral cloning cost J BC , which is computed on states induced by the expert, is often used instead: Minimizing this loss within yields a quadratic regret bound on regret: Theorem 2. ( Ross & Bagnell, 2010 ) Let J BC (π) = , then J C (π) ≤ J C (π ) + T 2 . Furthermore, this bound is tight: as we will discuss later, there exist simple problems which match the worst-case lower bound.

Section Title: ALGORITHM
  ALGORITHM Our algorithm is motivated by two criteria: i) the policy should act similarly to the expert within the expert's data distribution, and ii) the policy should move towards the expert's data distribution 10: end for if it is outside of it. These two criteria are addressed by combining two losses: a standard behavior cloning loss, and an additional loss which represents the variance over the outputs of an ensemble Π E = {π 1 , ..., π E } of policies trained on the demonstration data D. We call this the uncertainty cost, which is defined as: The motivation is that the variance over plausible policies is high outside the expert's distribution, since the data is sparse, but it is low inside the expert's distribution, since the data there is dense. Minimizing this cost encourages the policy to return to regions of dense coverage by the expert. Intuitively, this is what we would expect the expert policy π to do as well. The total cost which the algorithm optimizes is given by: The first term is a behavior cloning loss and is computed over states generated by the expert policy, of which the demonstration data D is a representative sample. The second term is computed over the distribution of states generated by the current policy and can be optimized using policy gradient. Note that the demonstration data is fixed, and this ensemble can be trained once offline. We then interleave the supervised behavioral cloning updates and the policy gradient updates which minimize the variance of the ensemble. The full algorithm is shown in Algorithm 1. We also found that dropout ( Srivastava et al., 2014 ), which has been proposed as an approximate form of ensembling, worked well (see Appendix D). In practice, for the supervised loss we optimize the KL divergence between the actions predicted by the policy and the expert actions, which is an upper bound on the total variation distance due to Pinsker's inequality. We also found it helpful to use a clipped uncertainty cost: C clip U (s, a) = −1 if C U (s, a) ≤ q +1 else where the threshold q is a top quantile of the raw uncertainty costs computed over the demonstration data. The threshold q defines a normal range of uncertainty based on the demonstration data, and values above this range incur a positive cost (or negative reward). The RL cost can be optimized using any policy gradient method. In our experiments we used advantage actor-critic (A2C) ( Mnih et al., 2016 ) or PPO ( Schulman et al., 2017 ), which estimate the expected cost using rollouts from multiple parallel actors all sharing the same policy (see Appendix C for details). We note that model-based RL methods could in principle be used as well if sample efficiency is a constraint.

Section Title: ANALYSIS
  ANALYSIS

Section Title: COVERAGE COEFFICIENT
  COVERAGE COEFFICIENT We now analyze DRIL for MDPs with discrete action spaces and potentially large or infinite state spaces. We will show that, subject to assumptions that the policy class contains an optimal policy and that we are able to optimize costs within of their global minimum, our algorithm obtains a regret bound which is linear in κT , where κ is a quantity which depends on the environment dynamics, the expert distribution d π , and our learned ensemble. Intuitively, κ represents a tradeoff between how concentrated the demonstration data is and how high the variance of the ensemble is outside the expert distribution.

Section Title: Assumption 1. (Realizability) π ∈ Π.
  Assumption 1. (Realizability) π ∈ Π. Assumption 2. (Optimization Oracle) For any given cost function J, our minimization procedure returns a policyπ ∈ Π such that J(π) ≤ arg min π∈Π J(π) + . The motivation behind our algorithm is that the policies in the ensemble agree inside the expert's distribution and disagree outside of it. This defines a reward function which pushes the learner back towards the expert's distribution if it strays away. However, what constitutes inside and outside the distribution, or sufficient agreement or disagreement, is ambiguous. Below we introduce quantities which makes these ideas precise. The notion of concentrability has been previously used to give bounds on the performance of value iteration ( Munos & Szepesvári, 2008 ). For a set U, α(U) will be low if the expert distribution has high mass at the states in U that are reachable by policies in the policy class. Definition 2. Define the minimum variance of the ensemble outside of U as β(U) = min s / ∈U ,a∈A Var π∼ΠE [π(a|s)]. We now define the κ coefficient as the minimum ratio of these two quantities over all possible subsets of S. Definition 3. We define κ = min U ⊆S α(U ) β(U ) . We can view κ as the quantity which minimizes the tradeoff over different subsets U between cov- erage by the expert policy inside of U, and variance of the ensemble outside of U.

Section Title: REGRET BOUND
  REGRET BOUND We now establish a relationship between the κ coefficient just defined, the cost our algorithm opti- mizes, and J exp defined in Equation (2) which we would ideally like to minimize and which trans- lates into a regret bound. All proofs can be found in Appendix A. Lemma 1. For any π ∈ Π, we have J exp (π) ≤ κJ alg (π). This result shows that if κ is not too large, and we are able to make our cost function J alg (π) small, then we can ensure J exp (π) is also small. This result is only useful if our cost function can indeed achieve a small minimum. The next lemma shows that this is the case. Lemma 2. min π∈Π J alg (π) ≤ 2 . Here is the threshold specified in Assumption 2. Combining these two lemmas with the previous result of  Ross et al. (2011) , we get a regret bound which is linear in κT . Theorem 3. Letπ be the result of minimizing J alg using our optimization oracle, and assume that Q π T −t+1 (s, a) − Q π T −t+1 (s, π ) ≤ u for all actions a, time steps t and states s reachable by π. Then Our bound is an improvement over that of behavior cloning if κ is less than O(T ). Note that DRIL does not require knowledge of κ. The quantity κ is problem-dependent and depends on the Published as a conference paper at ICLR 2020 environment dynamics, the expert policy and the policies in the learned ensemble. We next compute κ exactly for a problem for which behavior cloning is known to perform poorly, and show that it is independent of T . Example 1. Consider the tabular MDP given in ( Ross & Bagnell, 2010 ) as an example of a prob- lem where behavioral cloning incurs quadratic regret, shown in  Figure 1 . There are 3 states S = (s 0 , s 1 , s 2 ) and two actions (a 1 , a 2 ). Each policy π can be represented as a set of proba- bilities π(a 1 |s) for each state s ∈ S 2 . Assume the models in our ensemble are drawn from a posterior p(π(a 1 |s)|D) given by a Beta distribution with parameters Beta(n 1 + 1, n 2 + 1) where n 1 , n 2 are the number of times the pairs (s, a 1 ) and (s, a 2 ) occur, respectively, in the demon- stration data D. The agent always starts in s 0 and the expert's policy is given by π (a 1 |s 0 ) = 1, π (a 1 |s 1 ) = 0, π (a 1 |s 2 ) = 1. For any (s, a) pair, the task cost is C(s, a) = 0 if a = π (s) and 1 otherwise. Here d π = ( 1 T , T −1 T , 0). For any π, d π (s 0 ) = 1 T and d π (s 1 ) ≤ T −1 T due to the dynamics of the MDP, so dπ(s) d π (s) ≤ 1 for s ∈ {s 0 , s 1 }. Writing out α({s 0 , s 1 }), we get: Furthermore, since s 2 is never visited in the demonstration data, for each policy π i in the ensemble we have π i (a 1 |s 2 ), π i (a 2 |s 2 ) ∼ Beta(1, 1) = U nif orm(0, 1). It follows that Var π∼ΠE (π(a|s 2 )) is approximately equal 3 to the variance of a uniform distribution over [0, 1], i.e. 1 12 . Therefore: Applying our result from Theorem 3, we see that our algorithm obtains an O( T ) regret bound on this problem, in contrast to the O( T 2 ) regret of behavioral cloning 4 .

Section Title: RELATED WORK
  RELATED WORK The idea of learning through imitation dates back at least to the work of ( Pomerleau, 1989 ), who trained a neural network to imitate the steering actions of a human driver using images as input. The problem of covariate shift was already observed, as the author notes: "the network must not solely be shown examples of accurate driving, but also how to recover once a mistake has been made". This issue was formalized in the work of ( Ross & Bagnell, 2010 ), who on one hand proved an O( T 2 ) regret bound, and on the other hand provided an example showing this bound is tight. The subsequent work ( Ross et al., 2011 ) proposed the DAGGER algorithm which obtains linear regret, provided the agent can both interact with the environment, and query the expert policy. Our approach also requires environment interaction, but importantly does not need to query the expert. Also of Published as a conference paper at ICLR 2020 note is the work of ( Venkatraman et al., 2015 ), which extended DAGGER to time series prediction problems by using the true targets as expert corrections. Imitation learning has been used within the context of modern RL to help improve sample efficiency ( Chang et al., 2015 ;  Ross & Bagnell, 2014 ;  Sun et al., 2017 ;  Hester et al., 2018 ;  Le et al., 2018 ;  Cheng & Boots, 2018 ) or overcome exploration ( Nair et al., 2017 ). These settings assume the reward is known and that the policies can then be fine-tuned with reinforcement learning. In this case, covariate shift is less of an issue since it can be corrected using the reinforcement signal. The work of ( Luo et al., 2019 ) also proposed a method to address the covariate shift problem when learning from demonstrations when the reward is known, by conservatively extrapolating the value function outside the training distribution using negative sampling. This addresses a different setting from ours, and requires generating plausible states which are off the manifold of training data, which may be challenging when the states are high dimensional such as images. The work of ( Reddy et al., 2019 ) proposed to treat imitation learning within the Q-learning framework, setting a positive reward for all transitions inside the demonstration data and zero reward for all other transitions in the replay buffer. This rewards the agent for repeating (or returning to) the expert's transitions. The work of ( Sasaki et al., 2019 ) also incorporates a mechanism for reducing covariate shift by fitting a Q-function that classifies whether the demonstration states are reachable from the current state. Random Expert Distillation ( Wang et al., 2019 ) uses Random Network Distillation (RND) ( Burda et al., 2019 ) to estimate the support of the expert's distribution in state-action space, and minimizes an RL cost designed to guide the agent towards the expert's support. This is related to our method, but differs in that it minimizes the RND prediction error rather than the ensemble variance and does not include a behavior cloning cost. The behavior cloning cost is essential to our theoretical results and avoids certain failure modes, see Appendix B for more discusion. Generative Adversarial Imitation Learning (GAIL) ( Ho & Ermon, 2016 ) is a state-of-the-art algo- rithm which addresses the same setting as ours. It operates by training a discriminator network to distinguish expert states from states generated by the current policy, and the negative output of the discriminator is used as a reward signal to train the policy. The motivation is that states which are outside the training distribution will be assigned a low reward while states which are close to it will be assigned a high reward. This encourages the policy to return to the expert distribution if it strays away from it. However, the adversarial training procedure means that the reward function is chang- ing over time, which can make the algorithm unstable or difficult to tune. In contrast, our approach uses a simple fixed reward function. We include comparisons to GAIL in our experiments. Using disagreement between models in an ensemble to represent uncertainty has recently been ex- plored in several contexts. The works of ( Shyam et al., 2018 ;  Pathak et al., 2019 ;  Henaff, 2019 ) used disagreement between different dynamics models to drive exploration in the context of model- based RL. Conversely, ( Henaff et al., 2019 ) used variance across different dropout masks to prevent policies from exploiting error in dynamics models. Ensembles have also been used to represent un- certainty over Q-values in model-free RL in order to encourage exploration ( Osband et al., 2016 ). Within the context of imitation learning, the work of ( Menda et al., 2018 ) used the variance of the ensemble together with the DAGGER algorithm to decide when to query the expert demonstrator to minimize unsafe situations. Here, we use disagreement between different policies trained on demonstration data to address covariate shift in the context of imitation learning.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: TABULAR MDPS
  TABULAR MDPS As a first experiment, we applied DRIL to the tabular MDP of ( Ross & Bagnell, 2010 ) shown in  Figure 1 . We computed the posterior over the policy parameters given the demonstration data using a separate Beta distribution for each state s with parameters determined by the number of times each action was performed in s. For behavior cloning, we sampled a single policy from this posterior. For DRIL, we sampled an ensemble of 5 policies and used their negative variance to define an additional reward function. We combined this with a reward which was the probability density function of a given state-action pair under the posterior distribution, which corresponds to the supervised learning loss, and used tabular Q-learning to optimize the sum of these two reward functions. This experiment was repeated 500 times for time horizon lengths up to 500 and N = 1, 5, 10 expert demonstration trajectories.  Figure 2  shows plots of the regret over the 500 different trials across different time horizons. Al- though BC achieves good average performance, it exhibits poor worst-case performance with some trials incurring very high regret, especially when using fewer demonstrations. Our method has low regret across all trials, which stays close to constant independantly of the time horizon, even with a single demonstration. This performance is better than that suggested by our analysis, which showed a worst-case linear bound with respect to time horizon.

Section Title: ATARI ENVIRONMENTS
  ATARI ENVIRONMENTS We next evaluated our approach on six different Atari environments. We used pretrained PPO ( Schulman et al., 2017 ) agents from the stable baselines repository ( Hill et al., 2018 ) to generate N = {1, 3, 5, 10, 15, 20} expert trajectories. We compared against two other methods: standard be- havioral cloning (BC) and Generative Adversarial Imitation Learning (GAIL). Results are shown in Figure 3a. DRIL outperforms behavioral cloning across most environments and numbers of demon- strations, often by a substantial margin. In many cases, our method is able to match the expert's performance using a small number of trajectories. Figure 3b shows the evolution of the uncertainty cost and the policy reward throughout training. In all cases, the reward improves while the uncer- tainty cost decreases. We were not able to obtain meaningful performance for GAIL on these domains, despite performing a hyperparameter search across learning rates for the policy and discriminator, and across different numbers of discriminator updates. We additionally experimented with clipping rewards in an effort to stabilize performance. These results are consistent with those of ( Reddy et al., 2019 ), who also reported negative results when running GAIL on images. While improved performance might be possible with more sophisticated adversarial training techniques, we note that this contrasts with our method which uses a fixed reward function obtained through simple supervised learning. In Appendix D we provide ablation experiments examining the effects of the cost function clipping and the role of the BC loss. We also compare the ensemble approach to a dropout-based approxima- tion and show that DRIL works well in both cases.

Section Title: CONTINUOUS CONTROL
  CONTINUOUS CONTROL We next report results of running our method on 6 different continuous control tasks from the Py- Bullet 5 and OpenAI Gym ( Brockman et al., 2016 ) environments. We again used pretrained agents to generate expert demonstrations, and compared to Behavior Cloning and GAIL. Results for all methods are shown in  Figure 4 . In these environments we found Behavior Cloning to be a much stronger baseline than for the Atari environments: in several tasks it was able to match expert performance using as little as 3 trajectories, suggesting that covariate shift may be less of an issue. Our method performs similarly to Behavior Cloning on most tasks, except on Walker2D, where it yields improved performance for N = 1, 3, 5 trajectories. GAIL performs 5 https://github.com/bulletphysics/bullet3/tree/master/examples/ pybullet/gym/pybullet_envs/examples somewhat better than DRIL on HalfCheetah and Walker2D, but performs worse than both DRIL and BC on LunarLander and BipedalWalkerHardcore. The fact that DRIL is competitive across all tasks provides evidence of its robustness.

Section Title: CONCLUSION
  CONCLUSION Addressing covariate shift has been a long-standing challenge in imitation learning. In this work, we have proposed a new method to address this problem by penalizing the disagreement between an ensemble of different policies trained on the demonstration data. Importantly, our method requires no additional labeling by an expert. Our experimental results demonstrate that DRIL can often match expert performance while using only a small number of trajectories across a wide array of tasks, ranging from tabular MDPs to pixel-based Atari games and continuous control tasks. On the theoretical side, we have shown that our algorithm can provably obtain a low regret bound for problems in which the κ parameter is low. There are multiple directions for future work. On the theoretical side, characterizing the κ param- eter on a larger array of problems would help to better understand the settings where our method can expect to do well. Empirically, there are many other settings in structured prediction ( Daumé et al., 2009 ) where covariate shift is an issue and where our method could be applied. For example, in dialogue and language modeling it is common for generated text to become progressively less coherent as errors push the model off the manifold it was trained on. Our method could potentially be used to fine-tune language or translation models ( Cho et al., 2014 ;  Welleck et al., 2019 ) after training by applying our uncertainty-based cost function to the generated text.

```
