Title:
```
Under review as a conference paper at ICLR 2020 MULTITASK SOFT OPTION LEARNING
```
Abstract:
```
We present Multitask Soft Option Learning (MSOL), a hierarchical multitask framework based on Planning as Inference. MSOL extends the concept of options, using separate variational posteriors for each task, regularized by a shared prior. This allows fine-tuning of options for new tasks without forgetting their learned policies, leading to faster training without reducing the expressiveness of the hi- erarchical policy. MSOL avoids several instabilities during training in a multitask setting and provides a natural way to learn both intra-option policies and their ter- minations. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines in challenging multi-task environ- ments.
```

Figures/Tables Captions:
```
Figure 1: Two hierarchical posterior poli- cies (left and right) with common priors (middle). For each task i, the policy con- ditions on the current state s i t and the last selected option z i t−1 . It samples, in order, whether to terminate the last option (b i t ), which option to execute next (z i t ) and what primitive action (a i t ) to execute in the envi- ronment.
Figure 2: Performance during testing of the learned options and exploration priors. Each line is the median over 5 random seeds (2 for MLSH) and shaded areas indicate standard deviations. More details in text.
Figure 3: Options learned with MSOL on the taxi domain, before (top) and after pickup (bottom). The light gray area indicates walls. The left plots show the intra-option policies: arrows and colors indicated direction of most likely action, the size indicates its probability. A square indicates the pickup/dropoff action. The right plots show the termination policies: intensity and size of the circles indicate termination probability.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION A key challenge in Reinforcement Learning (RL) is to scale current approaches to higher complexity tasks without requiring a prohibitive number of environmental interactions. However, for many tasks, it is possible to construct or learn efficient exploration priors that allow to focus on more relevant parts of the state-action space, reducing the number of required interactions. These include, for example, reward shaping ( Ng et al., 1999 ;  Konidaris & Barto, 2006 ), curriculum learning ( Bengio et al., 2009 ), some meta-learning algorithms ( Wang et al., 2016 ;  Duan et al., 2016 ;  Gupta et al., 2018 ;  Houthooft et al., 2018 ;  Xu et al., 2018 ), and transfer learning ( Caruana, 1997 ;  Taylor & Stone, 2011 ;  Bengio, 2012 ;  Parisotto et al., 2015 ;  Rusu et al., 2015 ;  Teh et al., 2017 ). One promising way to capture prior knowledge is to decompose policies into a hierarchy of sub- policies (or skills) that can be reused and combined in novel ways to solve new tasks ( Dayan & Hinton, 1993 ;  Thrun & Schwartz, 1995 ;  Parr & Russell, 1998 ;  Sutton et al., 1999 ;  Barto & Mahade- van, 2003 ). The idea of Hierarchical RL (HRL) is also supported by findings that humans appear to employ a hierarchical mental structure when solving tasks ( Botvinick et al., 2009 ;  Collins & Frank, 2016 ). In such a hierarchical RL policy, lower-level, temporally extended skills yield directed behav- ior over multiple time steps. This has two advantages: i) it allows efficient exploration, as the target states of skills can be reached without having to explore much of the state space in between, and ii) directed behavior also reduces the variance of the future reward, which accelerates convergence of estimates thereof. On the other hand, while a hierarchical approach can therefore significantly speed up exploration and training, it can also severely limit the expressiveness of the final policy and lead to suboptimal performance when the temporally extended skills are not able to express the required policy for the task at hand ( Mankowitz et al., 2014 ). Many methods exist for constructing and/or learning skills for particular tasks ( Dayan & Hinton, 1993 ;  Sutton et al., 1999 ;  McGovern & Barto, 2001 ;  Menache et al., 2002 ; Ş imşek &  Barto, 2009 ; Gregor et al., 2016;  Kulkarni et al., 2016 ; Bacon et al., 2017;  Nachum et al., 2018a ). Training on multiple tasks simultaneously is one promising approach to learn skills that are both relevant and generalise across tasks ( Thrun & Schwartz, 1995 ;  Pickett & Barto, 2002 ; Fox et al., 2016;  Andreas et al., 2017 ;  Frans et al., 2018 ). Ideally, the entire hierarchy can be trained end-to-end on the obtained return, obviating the need to specify proxy rewards for skills ( Frans et al., 2018 ). However, learning hierarchical policies end-to-end in a multitask setting poses two major challenges: i) because skills optimize environmental rewards directly, correctly updating them relies on already (nearly) converged master policies that use them similarly across all tasks, requiring complex train- ing schedules ( Frans et al., 2018 ), and ii) the end-to-end optimization is prone to local minima in Under review as a conference paper at ICLR 2020 which multiple skills have learned similar behavior. This second points is explained in more detail in Appendix A. In this paper, we propose Multitask Soft Option Learning (MSOL), a novel approach to learning hierarchical policies in a multi-task setting that extends Options ( Sutton et al., 1999 ), a common definition for skills, and casts the concept into the Planning as Inference (PAI) framework (see, e.g.,  Levine, 2018 , for a review). MSOL brings multiple advantages: i) it stabilizes end-to-end multitask training, removing the need for complex training schedules like in  Frans et al. (2018) , ii) it gives rise to coordination between master policies, avoiding local minima of the type described in Appendix A, iii) it allows fine-tuning of options, i.e. adapting them to new tasks at test-time without the risk of unlearning previously acquired useful behavior, thereby avoiding suboptimal performance due to restricted expressiveness, iv) and lastly, we show how the soft option framework gives rise to a natural solution to the challenging task of learning option-termination policies. MSOL differentiates between a prior policy for each option, shared across all tasks, and a flexible task-specific posterior policy. The option prior can be fixed once it is fully trained, preventing un- learning of useful behavior even when the posteriors are updated. On new tasks, the option posteriors are initialized to the priors and regularized towards them, but are still adaptable to the specific task. This allows the same accelerated training as with 'hard' options, but can solve more tasks due to the adjustable posteriors. Furthermore, during option learning, we can train prior and posterior policies simultaneously ( Teh et al., 2017 ), all without the need for complex training schedules ( Frans et al., 2018 ): training is stabilized because only the priors are shared across tasks. Our experiments demonstrate that MSOL outperforms previous hierarchical and transfer learning algorithms during transfer tasks in a multitask setting. MSOL only modifies the regularized reward and loss function, but does not require any specialized architecture. In particular, it also does not require artificial restrictions on the expressiveness of either the higher-level or intra-option policies.

Section Title: PRELIMINARIES
  PRELIMINARIES An agent's task is formalized as a MDP (S, A, ρ, P, r), consisting of the state space S, the action space A, the initial state distribution ρ, the transition probability P (s t+1 |s t , a t ) of reaching state s t+1 by executing action a t in state s t , and the reward r(s t , a t ) an agent receives for this transition.

Section Title: PLANNING AS INFERENCE
  PLANNING AS INFERENCE Planning as inference (PAI) ( Todorov, 2008 ;  Toussaint, 2009 ;  Kappen et al., 2012 ) frames RL as a probabilistic-inference problem ( Levine, 2018 ). The agent learns a distribution q φ (a|s) over actions a given states s, i.e., a policy, parameterized by φ, which induces a distribution over trajectories τ of length T , i.e., τ = (s 1 , a 1 , s 2 , . . . , a T , s T +1 ): This can be seen as a structured variational approximation of the optimal trajectory distribution. Note that the true initial state probability ρ(s 1 ) and transition probability P (s t+1 |s t , a t ) are used in the variational posterior, as we can only control the policy, not the environment. An advantage of this formulation is that we can incorporate information both from prior knowledge, in the form of a prior policy distribution, and the task at hand through a likelihood function that is defined in terms of the achieved reward. The prior policy p(a t |s t ) can be specified by hand or, as in our case, learned (see Section 3). To incorporate the reward, we introduce a binary optimality variable O t ( Levine, 2018 ), whose likelihood is highest along the optimal trajectory that maximizes return: p(O t = 1|s t , a t ) = exp r(s t , a t )/β . The constraint r ∈ (−∞, 0] can be relaxed without changing the inference procedure ( Levine, 2018 ). For brevity, we denote O t = 1 as O t ≡ (O t = 1). If a given prior policy p(a t |s t ) explores the state-action space sufficiently, then p(τ, O 1:T ) is the distribution of desirable trajectories. PAI aims to find a policy such that the variational posterior in (1) approximates this distribution by minimizing the Kullback-Leibler (KL) divergence Under review as a conference paper at ICLR 2020

Section Title: MULTI-TASK LEARNING
  MULTI-TASK LEARNING In a multi-task setting, we have a set of different tasks i ∈ T , drawn from a task distribution with probability ξ(i). All tasks share state space S and action space A, but each task has its own initial- state distribution ρ i , transition probability P i (s t+1 |s t , a t ), and reward function r i . Our goal is to learn n tasks concurrently, distilling common information that can be leveraged to learn faster on new tasks from T . In this setting, the prior policy p θ (a t |s t ) can be learned jointly with the task- specific posterior policies q φi (a t |s t ) ( Teh et al., 2017 ). To do so, we simply extend (2) to is the regularised reward. Minimizing the loss in (3) is equivalent to maximizing the regularized reward R reg i,t . Moreover, minimizing the term E τ ∼q ln q φ i (at|st) p θ (at|st) implicitly minimizes the expected KL-divergence E st∼q D KL [q φi (·|s t ) p θ (·|s t )] . In practise (see Appendix C.1) we will also make use of a discount factor γ ∈ [0, 1]. For details on how γ arises in the PAI framework we refer to  Levine (2018) .

Section Title: OPTIONS
  OPTIONS Options ( Sutton et al., 1999 ) are skills that generalize primitive actions and consist of three com- ponents: i) an intra-option policy p(a t |s t , z t ) selecting primitive actions according to the currently active option z t , ii) a probability p(b t |s t , z t−1 ) of terminating the previously active option z t−1 , and iii) an initiation set I ⊆ S, which we simply assume to be S. Note that by construction, the higher- level (or master-) policy p(z t |z t−1 , s t , b t ) can only select a new option z t if the previous option z t−1 has terminated.

Section Title: METHOD
  METHOD We aim to learn a reusable set of options that allow for faster training on new tasks from a given distribution. We learn both intra-option and termination policies, while preventing multiple options from learning the same be- havior. To differentiate ourselves from classical 'hard' options, which, once learned, do not change during new tasks, we call our novel approach soft-options (this is further dis- cussed in Appendix B). Each soft-option consists of an option prior, which is shared across all tasks, and a task- specific option posterior. The priors of both the intra- option policy and the termination policy capture how an option typically behaves and remain fixed once they are fully learned. At the beginning of training on a new task, they are used to initialize the task-specific posterior dis- tribution. During training, the posterior is then regular- ized against the prior to prevent inadvertent unlearning. However, if maximizing the reward on certain tasks is not achievable with the prior policy, the posterior is free to deviate from it. We can thus speed up training using op- tions, while remaining flexible enough to solve any task. Additionally, this soft option framework also allows for learning good priors in a multitask setting while avoiding local minima in which several options learn the same be- havior. See  Figure 1  for an overview over the hierarchical prior-posterior architecture that we explain further below.

Section Title: HIERARCHICAL POSTERIOR POLICIES
  HIERARCHICAL POSTERIOR POLICIES To express options in the PAI framework, we introduce two additional variables at each time step t: option selections z t , representing the currently selected option, and decisions b t to terminate them and allow the higher-level (master) policy to choose a new option. The agent's behavior depends on the currently selected option z t , by drawing actions a t from the intra-option posterior policy q L φi (a t |s t , z t ). The selection z t itself is drawn from a master policy q H φi (z t |s t , z t−1 , b t ) = (1 − b t ) δ(z t − z t−1 ) + b t q H φi (z t |s t ), which conditions on b t ∈ {0, 1}, drawn by the termination posterior policy q T φi (b t |s t , z t−1 ). The master policy either continues with the previous z t−1 or draws a new option, where we set b 1 = 1 at the beginning of each episode. We slightly abuse notation by referring by δ(z t − z t−1 ) to the Kronecker delta δ zt,zt−1 for discrete and the Dirac delta distribution for continuous z t . The joint posterior policy is While z t can be a continuous variable, we consider only z t ∈ {1 . . . m}, where m is the number of available options. The induced distribution q φi (τ ) over trajectories of task i, τ = (s 1 , b 1 , z 1 , a 1 , s 2 , . . . , s T , b T , z T , a T , s T +1 ), is then

Section Title: HIERARCHICAL PRIOR POLICY
  HIERARCHICAL PRIOR POLICY Our framework transfers knowledge between tasks by a shared prior θ correctly, we can learn useful temporally extended options. The parameterized priors p T θ (b t |s t , z t−1 ) and p L θ (a t |s t , z t ) are structurally equivalent to the posterior policies q T φi and q L φi so that they can be used as initialization for the latter. Optimizing the regularized return (see next section) w.r.t. θ distills the common behavior into the prior policy and softly enforces similarity across posterior distributions of each option amongst all tasks i. The prior p H (z t |z t−1 , b t ) = (1 − b t ) δ(z t − z t−1 ) + b t 1 m selects the previous option z t−1 if b t = 0, and otherwise draws options uniformly to ensure exploration. Because the posterior master policy is different on each task, there is no need to distill common behavior into a joint prior.

Section Title: OBJECTIVE
  OBJECTIVE We extend the multitask objective in (3) by substituting p θ (τ, O 1:T ) and p φi (τ ) with those induced by our hierarchical posterior policy in (4) and the corresponding prior. The resulting objective has the same form but with a new regularized reward that is maximized: As we maximize E q [R reg i,t ], this corresponds to maximizing the expectation over r i (s t ,a t ) − β D KL (q H φi p H ) + D KL (q L φi p L θ ) + D KL (q T φi p T θ ) , along the on-policy trajectories drawn from q φi (τ ). Term 1 of the regularization encourages exploration in the space of options. It can also be seen as a form of deliberation cost (Harb et al., 2017) as it is only nonzero whenever we terminate an option and the master policy needs to select another to execute. Term 2 softly enforces similarity between option posteriors across tasks and updates the prior to- wards the 'average' posterior. It also encourages the master policy to pick the most specialized option whose posteriors across all tasks are most similar. In other words, the master policy q H φi is encouraged to pick option z t which maximizes r i , but minimizes term 2 by picking the option z t for which prior p L θ and posterior q L φi are the most similar. Because the prior is the average posterior, this rewards the master policy to pick the most specialized option (that still achieves high reward). As discussed in more detail in Appendix A, this allows us to escape the local optimization minima that hard options face in multitask learning, while still having fully specialized options after training.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Lastly, we can use 3 to also encourage temporal abstraction of options. To do so, during option learning, we fix the termination prior p T to a Bernoulli distribution p T (b) = (1−α) b α 1−b . Choosing a large α encourages prolonged execution of one option, but allows switching whenever necessary. This is also similar to deliberation costs (Harb et al., 2017) but with a more flexible cost model. Additionally, we can still distill a termination prior p T θ which can be used on future tasks. Instead of learning p T θ by minimizing the KL against the posterior termination policies, we can get more decisive terminations by minimizing andq φi (b = 1|s t , z t−1 ) = zt =zt−1 q H φi (z t |s t , z t−1 , b t = 1) i.e., the learned termination prior dis- tills the probability that the tasks' master policies would change the active option if they had the opportunity. Details on how we optimized the MSOL objective are given in Appendix C.

Section Title: RELATED WORK
  RELATED WORK Most hierarchical approaches rely on proxy rewards to train the lower level components and their terminations. Some of them aim to reach pre-specified subgoals ( Sutton et al., 1999 ;  Kulkarni et al., 2016 ), which are often found by analyzing the structure of the MDP ( McGovern & Barto, 2001 ;  Menache et al., 2002 ; Ş imşek et al., 2005; Ş imşek &  Barto, 2009 ), previously learned policies ( Goel & Huber, 2003 ;  Tessler et al., 2017 ) or predictability ( Harutyunyan et al., 2019 ). Those methods typically require knowledge, or a sufficient approximation, of the transition model, both of which are often infeasible. Recently, several authors have proposed unsupervised training objectives for learning diverse skills based on their distinctiveness (Gregor et al., 2016;  Florensa et al., 2017 ;  Achiam et al., 2018 ;  Ey- senbach et al., 2019 ). However, those approaches don't learn termination functions and cannot guarantee that the required behavior on the downstream task is included in the set of learned skills.  Hausman et al. (2018)  also incorporate reward information, but do not learn termination policies and are therefore restricted to learning multiple solutions to the provided task instead of learning a decomposition of the task solutions which can be re-composed to solve new tasks. A third usage of proxy rewards is by training lower level policies to move towards goals defined by the higher levels. When those goals are set in the original state space ( Nachum et al., 2018a ), this approach has difficulty scaling to high dimensional state spaces like images. Setting the goals in a learned embedding space ( Dayan & Hinton, 1993 ;  Vezhnevets et al., 2017 ;  Nachum et al., 2018b ) can be difficult to train, though. In both cases, the temporal extension of the learned skills are set manually. On the other hand,  Goyal et al. (2019)  also learn a hierarchical agent, but not to transfer skills, but to find decisions states based on how much information is encoded in the latent layer. HiREPS  Daniel et al. (2012)  also take an inference motivated approach to learning options. In particular  Daniel et al. (2016)  propose a similarly structured hierarchical policy, albeit in a single task setting. However, they do not utilize learned prior and posterior distributions, but instead use expectation maximization to iteratively infer a hierarchical policy to explain the current reward- weighted trajectory distribution. Several previous works try to overcome the restrictive nature of options that can lead to sub-optimal solutions by allowing the higher-level actions to modulate the behavior of the lower-level policies  Schaul et al. (2015) ;  Heess et al. (2016) ;  Haarnoja et al. (2018) . However, this significantly increases the required complexity of the higher-level policy and therefore the learning time. The multitask- and transfer-learning setup used in this work is inspired by  Thrun & Schwartz (1995)  and  Pickett & Barto (2002)  who suggest extracting options by using commonalities between so- lutions to multiple tasks. Prior multitask approaches often rely on additional human supervision like policy sketches ( Andreas et al., 2017 ) or desirable sub-goals ( Tessler et al., 2017 ;  Konidaris & Barto, 2007 ;  Mann et al., 2015 ) in order to learn skills which transfer well between tasks. In contrast, our work aims at finding good termination states without such supervision.  Tirumala et al. (2019)  investigate the use of different priors for the higher-level policy while we are focussing on learning transferrable option priors. Closest to our work is Meta Learning of Shared Hierarchies (MLSH) ( Frans et al., 2018 ) which, however, shares the lower-level policies across all tasks without Under review as a conference paper at ICLR 2020 distinguishing between prior and posterior and does not learn termination policies. As discussed, this leads to local minima and insufficient diversity in the learned options. Similarly to us,  Fox et al. (2016)  differentiate between prior and posterior policies on multiple tasks and utilize a KL- divergence between them for training. However, they do not consider termination probabilities and instead only choose one option per task. Instead of transferring option policies between tasks,  Am- mar et al. (2014)  aim to share behavior through a latent embedding. Another interesting approach to multitask learning is ( Mankowitz et al., 2016 ) which learns decision regions that are linear in the state instead of learning nonlinear master- and termination policies. Our approach is closely related to DISTRAL ( Teh et al., 2017 ) with which we share the multitask learning of prior and posterior policies. However, DISTRAL has no hierarchical structure and applies the same prior distribution over primitive actions, independent of the task. As a necessary hierar- chical heuristic, the authors propose to also condition on the last primitive action taken. This works well when the last action is indicative of future behavior; however, in Section 5 we show several failure cases where a learned hierarchy is needed.

Section Title: EXPERIMENTS
  EXPERIMENTS We conduct a series of experiments to show: i) when learning hierarchies in a multitask setting, MSOL successfully overcomes the local minimum of insufficient option diversity, as described in Appendix A; ii) MSOL can learn useful termination policies; iii) MSOL is equally applicable to discrete as well as continuous domains; and iv) using soft options yields fast transfer learning while still reaching optimal performance, even on new, out-of-distribution tasks. All architectural details and hyper-parameters can be found in the appendix. For all experiments, we first train the exploration priors and options on n tasks from the available task distribution T (training phase is plotted in Appendix E). Subsequently, we test how quickly we can learn new tasks from T (or another distribution T ). We compare the following algorithms: MSOL is our proposed method that utilizes soft options both during option learning and transfer. MSOL(frozen) uses the soft options framework during learning to find more diverse skills, but does not allow fine-tuning the posterior sub-policies after transfer. DISTRAL ( Teh et al., 2017 ) is a strong non-hierarchical transfer learning algorithm that also utilizes prior and posterior distributions. DISTRAL(+action) utilizes the last action as option-heuristic which works well in some tasks but fails when the last action is not sufficiently informative. MLSH ( Frans et al., 2018 ) is a multitask option learning algorithm like MSOL, but utilizes 'hard' options for both learning and transfer, i.e., sub-policies that are shared exactly across tasks. It also relies on fixed option durations and requires a complex training schedule between master and intra-option policies to stabilize training. We use the author's MLSH implementation. Lastly, we compare against Option Critic (OC) (Bacon et al., 2017), which takes the task-id as additional input in order to apply it to a multitask setting.

Section Title: MOVING BANDITS
  MOVING BANDITS We start with the 2D Moving Bandits environment proposed and implemented by  Frans et al. (2018) , which is similar to the example in Appendix A. In each episode, the agent receives a reward of 1 for each time step it is sufficiently close to one of two randomly sampled, distinguishable, marked positions in the environment. The agent can take actions that move it in one of the four cardinal directions. Which position is not signaled in the observation. Each episode lasts 50 time steps. We compare against MLSH and DISTRAL to highlight challenges that arise in multitask training. We allow MLSH and MSOL to learn two options. During transfer, optimal performance can only be achieved when both options successfully learned to reach different marked locations, i.e., when they are diverse. In Figure 2(a) we can see that MSOL is able to do so but the options learned by MLSH are not sufficiently diverse, for the reason explain in Appendix A. DISTRAL, even with the last action provided as additional input, is not able to quickly utilize the prior knowledge. The last action only conveys meaningful information when taking the goal locations into account: DISTRAL agents need to infer the intention based on the last action and the relative goal positions. While this is possible, in practice the agent was not able to do so, even with a much larger network. However, longer training allows DISTRAL to perform as well as MSOL, since its posterior is flexible, denoted Under review as a conference paper at ICLR 2020 by "DISTRAL(+action) limit". Lastly, MSOL(frozen) also outperforms DISTRAL(+action) and MLSH, but performs worse that MSOL. This highlights the utility of making options soft, i.e. adaptable.

Section Title: TAXI
  TAXI Next, we use a slightly modified version of the original Taxi domain ( Dietterich, 1998 ) to show learning of termination functions as well as transfer- and generalization capabilities. To solve the task, the agent must pick up a passenger on one of four possible locations by moving to their location and executing a special 'pickup/drop-off' action. Then, the passenger must be dropped off at one of the other three locations, again using the same action executed at the corresponding location. The domain has a discrete state space with 30 locations arranged on a grid and a flag indicating whether the passenger was already picked up. The observation is a one-hot encoding of the discrete state, excluding passenger- and goal location. This introduces an information-asymmetry between the task-specific master policy, and the shared options, allowing them to generalize well ( Galashov et al., 2018 ). Walls (see  Figure 3 ) limit the movement of the agent and invalid actions. We investigate two versions of Taxi. In the original ( Dietterich, 1998 , just called Taxi), the action space consists of one no-op, one 'pickup/drop-off' action and four actions to move in all cardinal directions. In Directional Taxi, we extend this setup: the agent faces in one of the cardinal directions and the available movements are to move forward or rotate either clockwise or counter-clockwise. In both environments the set of tasks T are the 12 different combinations of pickup/drop-off loca- tions. Episodes last at most 50 steps and there is a reward of 2 for delivering the passenger to its goal and a penalty of -0.1 for each time step. During training, the agent is initialized to any valid state. During testing, the agent is always initialized without the passenger on board. We allow four learnable options in MLSH and MSOL. This necessitates the options to be diverse, i.e., one option to reach each of the four pickup/drop-off locations. Importantly, it also requires the options to learn to terminate when a passenger is picked up. As one can see in Figure 2(b), MLSH struggles both with option-diversity and due to its fixed option duration which is not flexible enough for this environment. DISTRAL(+action) performs well in the original Taxi environment, as seen in Figure 2(b), since here the last action is a good indicator for the agent's intention. However, in the directional case shown in Figure 2(c), the actions are less informative and make it much harder Under review as a conference paper at ICLR 2020 for DISTRAL to use prior knowledge. By contrast, MSOL performs well in both taxi environments. Comparing its performance with MSOL(frozen) shows the utility of adaptable soft options during transfer.  Figure 3 , which visualizes the options learned by MSOL, shows that it successfully learns useful movement primitives and termination functions. The same soft option represents different behavior depending on whether it already picked up the passenger. This is expected as this behavior does not need to terminate the current option on three of the 12 tasks. Next we show how learning information-asymmetric soft options can help with transfer to unseen tasks. In Figure 2(d) we show learning on four tasks from T using options that were trained on the remaining eight, comparing against A2C and OC. Note that in OC, there is no information- asymmetry: We share the same networks across all tasks and provide the task-id as additional input, including to the option-policies. This prevents them from generalizing well to unseen tasks. On the other hand, withholding the task-information from them would be similar to MLSH, which we already showed to struggle with local minima. The strong performance of MSOL on this task shows that we need soft options to be able to train information-asymmetric options that generalize well. We also investigate the utility of flexible soft options: In Figure 2(e) we show learning performance on twelve changed tasks in which the pickup/dropoff locations where moved by one cell while the options were trained with the original locations. As expected, hard options are not able to solve this tasks. Even with additional access to primitive actions, exploration is inefficient ( Jong et al., 2008 ). On the other hand, MSOL is able to quickly learn this new task by adapting the previously learned options, outperforming hard options and flat policies learned from scratch.

Section Title: SWIMMER
  SWIMMER Lastly, we show that MSOL can also be applied to continuous multitask domains. In particular, we investigate the MuJoCo environment 'Swimmer' ( Todorov et al., 2012 ;  Brockman et al., 2016 ). Instead of rewarding forward movement as in the original implementation, now the rewarded move- ment direction depends on the task from T = {up, down, lef t, right}. We also include a small amount of additive action noise (details in the Appendix). We show that MSOL performs competi- tive even in the absence of known failure cases of DISTRAL (see Figure 2(f)).

Section Title: DISCUSSION
  DISCUSSION Multitask Soft Option Learning (MSOL) proposes reformulating options using the perspective of prior and posterior distributions. This offers several key advantages. First, during transfer, it allows us to distinguish between fixed, and therefore knowledge-preserving option priors, and flexible option posteriors that can adjust to the reward structure of the task at hand. This effects a similar speed-up in learning as the original options framework, while avoiding sub-optimal performance when the available options are not perfectly aligned to the task. Second, utilizing this 'soft' version of options in a multitask learning setup increases optimization stability and removes the need for complex training schedules between master and lower level policies. Fur- thermore, this framework naturally allows master policies to coordinate across tasks and avoid local minima of insufficient option diversity. It also allows for autonomously learning option-termination policies, a very challenging task which is often avoided by fixing option durations manually.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Lastly, using this formulation also allows inclusion of prior information in a principled manner with- out imposing too rigid a structure on the resulting hierarchy. We utilize this advantage to explicitly incorporate the bias that good options should be temporally extended. In future research, other types of information can be explored. As an example, one could investigate sets of tasks which would benefit from a learned master prior, like walking on different types of terrain.

```
