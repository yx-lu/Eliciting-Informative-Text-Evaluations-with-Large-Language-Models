Title:
```
Published as a conference paper at ICLR 2020 DOUBLE NEURAL COUNTERFACTUAL REGRET MINIMIZATION
```
Abstract:
```
Counterfactual regret minimization (CFR) is a fundamental and effective technique for solving Imperfect Information Games (IIG). However, the original CFR algorithm only works for discrete states and action spaces, and the resulting strategy is maintained as a tabular representation. Such tabular representation limits the method from being directly applied to large games. In this paper, we propose a double neural representation for the IIGs, where one neural network represents the cumulative regret, and the other represents the average strategy. Such neural representations allow us to avoid manual game abstraction and carry out end-to-end optimization. To make the learning efficient, we also developed several novel techniques including a robust sampling method and a mini-batch Monte Carlo Counterfactual Regret Minimization (MCCFR) method, which may be of independent interests. Empirically, on games tractable to tabular approaches, neural strategies trained with our algorithm converge comparably to their tabular counterparts, and significantly outperform those based on deep reinforcement learning. On extremely large games with billions of decision nodes, our approach achieved strong performance while using hundreds of times less memory than the tabular CFR. On head-to-head matches of hands-up no-limit texas hold'em, our neural agent beat the strong agent ABS-CFR 1 by 9.8±4.1 chips per game. It's a successful application of neural CFR in large games.
```

Figures/Tables Captions:
```
Figure 1: Extensive-Form IIG and Information Set • Notation.Figure 1 illustrates an extensive game for a finite set N = {0,1,...,n − 1} of n players. Define x v i as the hidden information of player i in IIG. x v −i refers to hidden variables of all players other than i. H refers to a finite set of histories. h∈H denotes a possible history (or state), which consists of each player's hidden variable and ac- tions taken by all players including chance. The empty sequence ∅ is a member of H. h j h denotes h j is a prefix of h. Z ⊆ H denotes the terminal histories and any member z ∈Z is not a prefix of any other sequences. A(h)={a:ha∈H} is the set of available actions after non-terminal history h ∈ H \Z. A player function P assigns a member of N ∪{c} to each non-terminal history, where c is the chance ( we set c=−1). P (h) is the player who takes an action after history h. For each player i, imperfect information is denoted by information set (infoset) I i . All states h∈I i are indistinguishable to i. I i refers to the set of infosets of i. The utility function u i (z) defines the payoff of i at state z. See appendix B.1 for more details.
Figure 2: (a) tabular CFR and (b) our double neural CFR framework.r σ t i ((a|Ii)|Qj) is the estimated regret in MCCFR, R t−1 i (a|Ii) is the cumulative regret, s t i (a|Ii) is the weighted additional strategy and S t−1 i (a|Ii) is the cumulative behavior strategy. In tabular CFR, cumulative regret and strategy are stored in the tabular memory, which limits it to solve large games. In DNCFR, we use double deep neural networks to approximate these two values. DNCFR needs less memory than tabular methods because of its generalization.
Figure 3: (a) recurrent neural network architecture with attention for extensive games. Both RSN and ASN are based on this architecture but with different parameters (θR and θS respectively). (b) an overview of the proposed robust sampling and mini-batch techniques. The trajectories marked by red arrows are the samples produced by robust sampling (k =2 here).
Figure 4: Log-log performance on Leduc(5). (a) different sampling methods, k refers to the number of sampling action for the proposed robust sampling method in each infoset. (b) neural architectures. (c) number of parameters. (d) proportion of observed infosets. Higher proportion indicates more working memory.
Figure 5: Log-log performance. (a) Individual effect of RSN and ASN. RS-MCCFR+ refers to the tabular mini-batch MCCFR+ method with the proposed robust sampling. RS-MCCFR+-RSN only uses one neural network RSN to learn cumulative regret while uses a table to save cumulative strategy. RS-MCCFR+-ASN only use one neural network ASN. RS-MCCFR+-RSN-ASN refers to DNCFR with both RSN and ASN. (b) Warm start from tabular CFR and RS-MCCFR+. (c) DNCFR vs XFP vs NFSP. (d) Large Leduc(10) and Leduc(15).
Figure 6: time space trade-off.
Figure 7: Performance of DNCFR on heads-up no-limit Texas Hold'em. (a) Log-log performance of DNCFR on HUNL(1) under different embedding size. (b) Log-Log performance of DNCFR on HUNL(1) under different numbers of gradient descent updates on each iteration. (c) DNCFR beats ABS-CFR by 9.8±4.1 chips per hand and achieves similar performance with its tabular version but using much less memory.
Table 1: Summary. #infoset is the number of infosets. #state is the number of states. %observed is the ratio of observed infosets in each iteration. #emd and #param are the embedding size and the number of parameters in DNCFR.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION While significant advance has been made in addressing large perfect information games, such as Go (Silver et al., 2016), solving imperfect information games remains a challenging task. For Imperfect Information Games (IIG), a player has only partial knowledge about her opponents before making a decision, so that she has to reason under the uncertainty about her opponents' information while exploiting the opponents' uncertainty about herself. Thus, IIGs provide more realistic modeling than perfect information games for many real-world applications, such as trading, traffic routing, and politics. Nash equilibrium is a typical solution concept for a two-player perfect-recall IIG. One of the most effective approaches is CFR (Zinkevich et al., 2007), which minimizes the overall counterfactual regret so that the av- erage strategies converge to a Nash equilibrium. However the original CFR only works for discrete states and action spaces, and the resulting strategy is maintained as a tabular representation. Such tabular representa- tion limits the method from being directly applied to large games. To tackle this challenge, one can simplify the game by grouping similar states together to solve the simplified (abstracted) game approximately via tabular CFR (Zinkevich et al., 2007; Lanctot et al., 2009). Constructing an effective abstraction, however, demands rich domain knowledge and its solution may be a coarse approximation of true equilibrium. Function approximation can be used to replace the tabular representation. Waugh et al. (2015) combines regression tree function approximation with CFR based on handcrafted features, which is called Regression CFR (RCFR). However, since RCFR uses full traversals of the game tree, it is still impractical for large games. Moravcik et al. (2017) propose a seminal approach DeepStack, which uses fully connected neural networks to represent players' counterfactual values, tabular CFR however was used in the subgame solving. Jin et al. (2017) use deep reinforcement learning to solve regret minimization problem for single-agent settings, which is different from two-player perfect-recall IIGs.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 To learn approximate Nash equilibrium for IIGs in an end-to-end manner, Heinrich et al. (2015) and Heinrich & Silver (2016) propose eXtensive-form Fictitious Play (XFP) and Neural Fictitious Self-Play (NFSP), respectively, based on deep reinforcement learning. In a NFSP model, the neural strategies are updated by selecting the best responses to their opponents' average strategies. These approaches are advantageous in the sense that they do not rely on abstracting the game, and accordingly their strategies can improve continuously with more optimization iterations. However fictitious play empirically converges much slower than CFR-based approaches. Srinivasan et al. (2018) use actor-critic policy optimization methods to minimize regret and achieve performance comparable to NFSP. Thus it remains an open question whether a purely neural-based end-to-end approach can achieve comparable performance to tabular based CFR approach. In the paper, we solve this open question by designing a double neural counterfactual regret minimization (DNCFR) algorithm 2 . To make a neural representation, we modeled imperfect information game by a novel recurrent neural network with attention. Furthermore, in order to improve the convergence of the neural algorithm, we also developed a new sampling technique which converged much more efficient than the outcome sampling, while being more memory efficient than the external sampling. In the experiment, we conducted a set of ablation studies related to each novelty. The experiments showed DNCRF converged to comparable results produced by its tabular counterpart while performing much better than NFSP. In addition, we tested DNCFR on extremely large game, heads-up no-limit Texas Hold'em (HUNL). The experiments showed that DNCFR with only a few number of parameters achieved strong neural strategy and beat ABS-CFR.

Section Title: BACKGROUND
  BACKGROUND

Section Title: Algorithm 1: CFR Algorithm
  Algorithm 1: CFR Algorithm A strategy profile σ = {σ i |σ i ∈ Σ i , i ∈ N} is a collection of strategies for all players, where Σ i is the set of all possible strategies for player i. σ −i refers to strategy of all players other than player i. For play i ∈ N, the strategy σ i (I i ) is a function, which assigns an action distribution over A(I i ) to infoset I i . σ i (a|h) denotes the probability of action a taken by player i at state h. In IIG, ∀h 1 ,h 2 ∈ I i , we have σ i (I i ) = σ i (h 1 ) = σ i (h 2 ). For iterative method such as CFR, σ t refers to the strategy profile at t-th iteration. The state reach probability of history h is denoted by π σ (h) if players take actions according to σ. The reach probability is also called range in DeepStack (Moravcik et al., 2017). Similarly, π σ i (h) refers to those for player i while π σ −i (h) refers to those for other players except for i. For an empty sequence π σ (∅) = 1. One can also show that the reach probability of the opponent is proportional to posterior Published as a conference paper at ICLR 2020 1 2 3 4 1 2 3 4 5 6 7 8 −1 ( | ) + ( | ) −1 ( | ) + 1 2 3 4 1 2 3 4 5 6 7 8 ( | ) + gradient descent + Tabular Method Neural Method gradient descent (( | )| ) (b) (a) Regret Matching Regret Matching RegretSumNetwork AvgStrategyNetwork (( | )| ) probability of the opponent's hidden variable, i.e.,p(x v −i |I i )∝π σ −i (h), where x v i and I i indicate a particular h (proof in Appendix D.1). Finally, the infoset reach probability of I i is defined as π σ (I i )= h∈Ii π σ (h). Similarly, we have π σ i (I i ) = h∈Ii π σ i (h) and π σ −i (I i ) = h∈Ii π σ −i (h). More details can be found in Appendix B.3. • Counterfactual Regret Minimization. CFR is an iterative method for finding a Nash equilibrium for zero-sum perfect-recall IIGs (Zinkevich et al., 2007) (Algorithm 1 and Figure 2(a)). Given strategy profile σ, the counterfactual value (CFV) v σ i (I i ) at infoset I i is defined by Eq. (1). The action CFV of taking action a is v σ i (a|I i ) and its regret is defined by Eq. (2). Then the cumulative regret of action a after T iterations is Eq. (3), where R 0 i (a|I i ) = 0. Define R t,+ i (a|I i ) = max(R t i (a|I i ),0), the current strategy (or behavior strategy) at t + 1 iteration will be updated by Eq. (4). Define s t i (a|I i ) = π σ t i (I i )σ t i (a|I i ) as the additional strategy in iteration t, then the cumulative strategy can be defined as Eq. (5), where S 0 (a|I i )=0. The average strategyσ i t after t iterations is defined by Eq. (6), which approaches a Nash equilibrium after enough iterations. • Monte Carlo CFR.Lanctot et al. (2009) proposed a Monte Carlo CFR (MCCFR) to compute the unbiased estimation of counterfactual value by sampling subsets of infosets in each iteration. Although MCCFR still needs two tabular storages for saving cumulative regret and strategy as CFR does, it needs much less working memory than the standard CFR (Zinkevich et al., 2007). This is because MCCFR needs only to maintain values for those visited nodes into working memory; Define Q={Q 1 ,Q 2 ,...,Q m }, where Q j ∈Z is a set (block) of sampled terminal histories in each iteration, such that Q j spans the set Z. Define q Qj as the probability of considering block Q j , where m j=1 q Qj =1. Define q(z)= j:z∈Qj q Qj as the probability of considering a particular terminal history z. For infoset I i , an estimate of sampled counterfactual value isṽ σ i (I i |Q j )= h∈Ii,z∈Qj,h z 1 q(z) π σ −i (z)π σ i (h,z)u i (z). Lemma 1 (Lanctot et al. (2009)) The sampled counterfactual value in MCCFR is the unbiased estimation of actual counterfactual value in CFR. E j∼q Q j [ṽ σ i (I i |Q j )]=v σ i (I i ). Define σ rs as sampled strategy profile, where σ rs i is the sampled strategy of player i and σ rs −i are those for other players except for i. The regret of the sampled action a ∈ A(I i ) is defined byr σ i ((a|I i )|Q j ) = z∈Qj,ha z,h∈Ii π σ i (ha,z)u rs i (z) − z∈Qj,h z,h∈Ii π σ i (h,z)u rs i (z), where u rs i (z) = ui(z) π σ rs i (z) is a new utility weighted by 1 π σ rs i (z) . The sampled estimation for cumulative regret of action a after t iterations is

Section Title: DOUBLE NEURAL COUNTERFACTUAL REGRET MINIMIZATION
  DOUBLE NEURAL COUNTERFACTUAL REGRET MINIMIZATION Double neural CFR algorithm will employ two neural networks, one for the cumulative regret R, and the other for the average strategy S shown in Figure 2(b).

Section Title: MODELING
  MODELING The iterative updates of CFR algorithm maintain the regret sum R t (a|I i ) and the average strategyσ t i (a|I i ). Thus, our two neural networks are designed accordingly. • RegretSumNetwork(RSN): according to Eq. (4), the current strategy σ t+1 (a|I i ) is computed from the cumulative regret R t (a|I i ). We only need to track the numerator in Eq. (4) since the normalization in the denominator can be computed easily when the strategy is used. Given infoset I i and action a, we design a neural network R(a,I i |θ t R ) to track R t (a|I i ), where θ t R are the network parameters. • AvgStrategyNetwork(ASN): according to Eq. (6), the approximate Nash equilibrium is the weighted average of all previous behavior strategies up to t iterations, which is computed by the normalization of cumulative strategy S t (a|I i ). Similar to the cumulative regret, we employ the other deep neural network S(a|θ t S ) with network parameter θ t S to track the cumulative strategy.

Section Title: RECURRENT NEURAL NETWORK REPRESENTATION WITH ATTENTION
  RECURRENT NEURAL NETWORK REPRESENTATION WITH ATTENTION In order to define our R and S networks, we need to represent the infoset in extensive-form games. In such games, players take actions in an alternating fashion and each player makes a decision according to the observed history. Because the action sequences vary in length, we model them with recurrent neural networks and each action in the sequence corresponds to a cell in the RNN. This architecture is different from the one in DeepStack (Moravcik et al., 2017), which used a fully connected deep neural network to estimate counterfactual value. Figure 3(a) provides an illustration of the proposed deep sequential neural network representation for infosets. Besides the vanilla RNN, there are several variants of more expressive RNNs, such as the GRU (Cho et al., 2014) and LSTM (Hochreiter & Schmidhuber, 1997). In our later experiments, we will compare these different neural architectures as well as a fully connected network representation. Furthermore, different position in the sequence may contribute differently to the decision making, we add an attention mechanism (Desimone & Duncan, 1995; Cho et al., 2015) to the RNN architecture to enhance the representation. For example, the player may need to take a more aggressive strategy after beneficial public cards are revealed in a poker game. Thus the information after the public cards are revealed may be more important. In practice, we find that the attention mechanism can help DNCFR obtain a better convergence rate. See Appendix E for more details on the architectures.

Section Title: OPTIMIZATION METHOD
  OPTIMIZATION METHOD The parameters in the two neural networks are optimized via stochastic gradient descent in a stage-wise fashion interleaving with CFR iterations.

Section Title: OPTIMIZING CURRENT STRATEGY
  OPTIMIZING CURRENT STRATEGY We use M t R = {(I i ,r σ t i ((a|I i )|Q j ))|for all sampled I i } to store the sampled I i and the corresponding regretr σ t i ((a|I i )|Q j )) for all players in t-th iteration, where Q j is the sampled block (shown in Figure 2(b)). These samples are produced by our proposed robust sampling and mini-batch MCCFR methods, which will be discussed in Section 4. According to Eq. (3), we optimize the cumulative regret neural network R(a,I i |θ t+1 R ) using the following loss function R refers to the old parameters and θ t+1 R is the new parameters we need to optimize. Note that, Eq. (7) is minimized based on the samples of all the players rather than a particular player i. In standard MCCFR, if the infoset is not sampled, the corresponding regret is set to 0, which leads to unbiased estimation according to Lemma 1. The design of the loss function in Eq. (7) follows the same intuition. Techniques in Schmid et al. (2018) can be used to reduce the variance.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 Sampling unobserved infosets? Theoretically, in order to optimize Eq. (7), we need to collect both observed and unobserved infosets. This approach requires us to design a suitable sampling method to select additional training samples from large numbers of unobserved infosets, which will need a lot of memory and computation. Clearly, this is intractable on large games, such as HUNL. In practice, we find that minimizing loss only based on the observed samples can help us achieve a converged strategy.

Section Title: Learning without forgetting?
  Learning without forgetting? Another concern is that, only a small proportion of infosets are sampled due to mini-batch training, which may result in the neural networks forgetting values for those unobserved infosets. To address this challenge, we will use the neural network parameters from the previous iteration as the initialization, which gives us an online learning/adaptation flavor to the updates. Experimentally, on large games, due to the generalization ability of the neural networks, even a small proportion of infosets are used to update the neural networks, our double neural approach can still converge to an approximate Nash equilibrium. See Appendix F for more details on implementation. Scaling regret for stable training? According to Theorem 6 in Burch (2017), the cumulative regret R t i (a|I i ) ≤ ∆ |A|T , where |A| = max Ii∈I |A(I i )| and ∆ = max Ii,a,t |R t (a|I i )−R t−1 (a|I i )|. It indicates that R t i (a|I i ) will become increasingly large. In practice, we scale the cumulative regret by a factor of √ t to make its range more stable. For example, defineR t i (a|I i )=R t i (a|I i )/ √ t, we can update the cumulative regret Eq. (3)

Section Title: OPTIMIZING AVERAGE STRATEGY
  OPTIMIZING AVERAGE STRATEGY The other memory M t S = {(I i ,s t i (a|I i )|for all sampled I i } will store the sampled I i and the weighted additional behavior strategy s t i (a|I i ) in t-th iteration. Similarly, the loss function L(S) of ASN is defined by: S refers to the old parameters and θ t+1 S is the new parameters we need to optimize. According to Algorithm 1, cumulative regret is used to generate behavior strategy in the next iteration while cumulative strategy is the summation of the weighted behavior strategy. In theory, if we have all the M t S in each iteration, we can achieve the final average strategy directly. Based on this concept, we don't need to optimize the average strategy network (ASN) S(·|θ t S ) in each iteration. However, saving all such values into a huge memory is very expensive on large games. A compromise is that we can save such values within multiple iterations into a memory, when this memory is large enough, the incremental value within multiple iterations can be learned by optimizing Eq. (8).

Section Title: Minimum squared loss versus maximum likelihood?
  Minimum squared loss versus maximum likelihood? The average strategy is a distribution over actions, which implies that we can use maximum likelihood method to directly optimize this average strategy. The maximum likelihood method should base on the whole samples up to t-th iteration rather than only the additional samples, so that this method is very memory-expensive. To address this limitation, we can use uniform reservoir sampling method (Osborne et al., 2014) to obtain the unbiased estimation of each strategy. In practice, we find this maximum likelihood method has high variance and cannot approach a less exploitable Nash equilibrium. Experimentally, optimization by minimizing squared loss helps us obtain a fast convergent average strategy profile and uses much less memory than maximum likelihood method.

Section Title: CONTINUAL IMPROVEMENT
  CONTINUAL IMPROVEMENT When solving large IIGs, prior methods such as Libratus (Brown & Sandholm, 2017) and Deep- Stack (Moravcik et al., 2017) are based on the abstracted HUNL which has a manageable number of infosets. The abstraction techniques are usually based on domain knowledge, such as clustering similar hand-strength cards into the same buckets or only taking discrete actions (e.g., fold, call, one-pot raise and all in). DNCFR is not limited by the specified abstracted cards or actions. For example, we can use the continuous variable to represent bet money rather than encode it by discrete action. In practice, DNCFR can clone an existing tabular representation or neural representation and then continually improve the strategy from the initialized point. More specifically, for infoset I i and action a, define R i (a|I i ) as the cumulative regret . We can use behavior cloning technique to learn the cumulative regret by optimizing θ * R ← argmin θ R Ii∈Ii R(·|θ R )−R (·|I i ) 2 . Similarly, the cumulative strategy can be cloned in the Published as a conference paper at ICLR 2020 same way. Based on the learned parameters, we can warm start DNCFR and continually improve beyond the tabular strategy profile. Algorithm 2 provides a summary of the proposed double neural counterfactual regret minimization approach. In the first iteration, if the system warm starts from tabular-based methods, the techniques in Section 3.4 will be used to clone the cumulative regrets and strategies. If there is no warm start ini- tialization, we can start our algorithm by randomly initializing the parameters in RSN and ASN. Then sampling methods will return the sampled infosets and values, which are saved in memories M t R and M t S respectively. These samples will be used by the NeuralAgent algorithm from Algorithm 3 to optimize RSN and ASN. Further details for the sampling methods will be discussed in the next section. Due to space limitation, we present NeuralAgent fitting algorithm in Appendix F.

Section Title: EFFICIENT TRAINING
  EFFICIENT TRAINING In this section, we will propose two techniques to improve the efficiency of the double neural method. These techniques can also be used separately in other CFR variants.

Section Title: ROBUST SAMPLING TECHNIQUE
  ROBUST SAMPLING TECHNIQUE In this section, we introduce a robust sampling method (RS), which is a general version of both external sampling and outcome sampling (Lanctot et al., 2009). RS samples k actions in one player's infosets and samples one action in the another player's infosets. Specifically, in the robust sampling method, the sampled profile is defined by σ rs(k) = (σ rs(k) i ,σ −i ), where player i will randomly select k actions according to sampled strategy σ rs(k) i (I i ) at I i and other players randomly select one action according to σ −i . We design an efficient sampling policy for robust sampling as follows and discuss the relationship among robust sampling, external sampling and outcome sampling in Appendix D.2. If k = max Ii∈I |A(I i )| and for each action σ rs(k) i (a|I i ) = 1, then robust sampling is identical with external sampling. If k = 1, σ rs(k) i =σ i and q(z)≥δ >0 (δ is a small positive number), then robust sampling is identical with outcome sampling. Specifically, if player i randomly selects min(k, |A(I i )|) actions according to discrete uni- form distribution unif(0, |A(I i )|) at I i , i.e., σ and the weighted utility u rs(k) i (z) will be a constant number in each iteration. In many settings, when k =1, we find such robust sampling schema converges more efficient than outcome sampling. In contrast, our robust sampling achieves comparable convergence with external sampling but using less working memory when specifying a suitable k. It's reasonable because our schema only samples k rather than all actions in player i s infosets, the sampled game tree is smaller than the one by external sampling. In the experiment, we will compare these sampling policies in our ablation studies.

Section Title: MINI-BATCH TECHNIQUE
  MINI-BATCH TECHNIQUE Traditional MCCFR only samples one block in an iteration and provides an unbiased estimation of origin CFV. In this paper, we present a mini-batch Monte Carlo technique and randomly sample b blocks in one iter- ation. Let Q j denote a block of terminals sampled according to the scheme in Section 4.1, then mini-batch CFV with mini-batch size b will beṽ σ We prove thatṽ σ i (I i |b) is an unbiased estimation of CFV in Appendix D.3. Following the similar ideas of CFR and CFR+, if we replace the regret matching by regret matching plus (Tammelin, 2014), we obtain a mini-batch MCCFR+ algorithm. Our mini-batch technique empirically can sample b blocks in parallel and converges faster than original MCCFR when performing on multi-core machines.

Section Title: EXPERIMENT
  EXPERIMENT To understand the contributions of various components in DNCFR algorithm, we will first conduct a set of ablation studies. Then we will compare DNCFR with tabular CFR and deep reinforcement learning method such as NFSP, which is a prior leading function approximation method in IIGs. At last, we conduct experiments on heads-up no-limit Texas Hold'em (HUNL) to show the scalability of DNCFR algorithm. The games and key information used in our experiment are listed in  Table 1 .

Section Title: SETTINGS AND METRIC
  SETTINGS AND METRIC We perform the ablation studies on Leduc Hold'em poker, which is a commonly used poker game in research community (Heinrich & Silver, 2016; Schmid et al., 2018; Steinberger, 2019; Lockhart et al., 2019). In our experiments, we test DNCFR on three Leduc Hold'em instances with stack size 5, 10, and 15, which are denoted by Leduc(5), Leduc(10), and Leduc(15) respectively. To test DNCFR's scalability, we develop a neural agent to solve HUNL, which contains about 10 161 infosets (Johanson, 2013) and has served for decades as challenging benchmark and milestones of solving IIGs. The rules for such games are given in Appendix A. The experiments are evaluated by exploitability, which was used as a standard win rate measure in many key articles (Zinkevich et al., 2007; Lanctot et al., 2009; Michael Bowling, 2015; Brown et al., 2018). The units of exploitability in our paper is chips per game. It denotes how many chips one player wins on average per hand of poker. The method with a lower exploitability is better. The exploitability of Nash equilibrium is zero. In extremely large game, which is intractable to compute exploitability, we use head-to-head performance to measure different agents. For reproducibility, we present the implementation details of the neural agent in Algorithm 2, Algorithm 3, Algorithm 4. Appendix F.4 provides the parameters used in our experiments. Solving HUNL is a challenging task. Although there are published papers (Moravcik et al., 2017; Brown & Sandholm, 2017), it lacks of available open source codes for such solvers. The development of HUNL solver not only needs tedious work, but also is difficult to verify the correctness of the implementation, because of its well known high variance and extremely large game size. In Appendix G, we provide several approaches to validate the correctness of our implementation for HUNL.

Section Title: ABLATION STUDIES
  ABLATION STUDIES We first conduct a set of ablation studies related to the mini-batch training, robust sampling, the choice of neural architecture on Leduc Hold'em. • Is mini-batch sampling helpful? we present the convergence curves of the proposed robust sampling method with k = max(|A(I i )|) under different mini-batch sizes in Figure 8(a) at Appendix C. The experimental results show that larger batch sizes generally lead to better strategy profiles. • Is robust sampling helpful?  Figure 4 (a)  presents convergence curves for outcome sampling, external sampling(k =max(|A(I i )|)) and the proposed robust sampling method under the different number of sampled actions. The outcome sampling cannot converge to a low exploitability( smaller than 0.1 after 1000 iterations). The proposed robust sampling algorithm with k =1, which only samples one trajectory like the outcome sampling, can achieve a better strategy profile after the same number of iterations. With an increasing k, the robust sampling method achieves an even better convergence rate. Experiment results show k = 3 and 5 have a similar trend with k = max(|A(I i )|), which demonstrates that the proposed robust sampling achieves similar performance but requires less memory than the external sampling. We choose k =3 for the later experiments in Leduc Hold'em. • Is attention in the neural architecture helpful? Figure 4(b) shows that all the neural architectures achieved similar results while LSTM with attention achieved slightly better performance with a large number of iterations. We select LSTM plus attention as the default architectures in the later experiments. • Do the neural networks just memorize but not generalize? One indication that the neural networks are generalizing is that they use much fewer parameters than their tabular counterparts. We experimented with LSTM plus attention networks, and embedding size of 8 and 16 respectively. These architectures contain 1048 and 2608 parameters respectively. Both of them are much less than the tabular memory (more than 11083 here) and can lead to a converging strategy profile as shown in Figure 4(c). We select embedding size 16 as the default parameters. In the later experiments, we will show the similar conclusion on HUNL. • Do the neural networks generalize to unseen infosets? To investigate the generalization ability, we perform the DNCFR with small mini-batch sizes (b=50, 100, 500), where only 3.08%, 5.59%, and 13.06% infosets are observed in each iteration. In all these settings, DNCFR can still converge and arrive at exploitability less than 0.1 within only 1000 iterations as shown in Figure 4(d). In the later experiments, we set b=100 as the default mini-batch size. We learn new parameters based on the old parameters and a subset of observed samples. All infosets share the same parameters, so that the neural network can estimate the values for unseen infosets. Note that, the number of parameters is orders of magnitude less than the number of infosets in many settings, which indicates the generalization of our method. Furthermore, Figure 4(d) shows that DNCFRs are slightly better than tabular MCCFR, we think it's because of the generalization to unseen infosets. • What is the individual effect of RSN and ASN? Figure 5(a) presents ablation study of the effects of RSN and ASN network respectively. Specifically, the method RSN denotes that we only employ RSN to learn the cumulative regret while the cumulative strategy is stored in a tabular memory. Similarly, the method ASN only employ ASN to learn the cumulative strategy. Both these single neural methods perform only slightly better than the DNCFR. • How well does continual improvement work? As shown in Figure 5(b), warm starting from either full-width based or sampling based CFR can lead to continual improvements. Specifically, the first 10 iterations are learned by tabular based CFR and RS-MCCFR+. After the behavior cloning in Section 3.4, the remaining iterations are continually improved by DNCFR. • How well does DNCFR on larger games? We test DNCFR on large Leduc(10) and Leduc(15), which contains millions of infosets. Even though only a small proportion of nodes are sampled in each iteration, Figure 5(d) shows that DNCFR can still converge on these large games.

Section Title: COMPARISON AND SPACE-TIME TRADE-OFF
  COMPARISON AND SPACE-TIME TRADE-OFF How does DNCFR compare to the tabular counterpart, XFP, and NFSP? NFSP is the prior leading function approximation method for solving IIG, which is based on reinforcement learning and fictitious self-play techniques. In the experiment, NFSP requires two memories to store 2×10 5 state-action pair Published as a conference paper at ICLR 2020 samples and 2×10 6 samples for supervised learning respectively. The memory sizes are larger than the number of infosets. Figure 5(c) demonstrates that NFSP obtains a 0.06-Nash equilibrium after touching 10 9 infosets. The XFP obtains the same exploitability when touching about 10 7 nodes. However, this method is the precursor of NFSP and updated by a tabular based full-width fictitious play. Our DNCFR achieves the same performance by touching no more than 10 6 nodes, which are much fewer than both NFSP and XFP. The experiment shows that DNCFR converges significantly better than the reinforcement learning counterpart.

Section Title: Space and time trade-off
  Space and time trade-off In this experiment, we investigate the time and space needed for DNCFR to achieve certain exploitability relative to tabular CFR algorithm. We compare their runtime and memory in  Figure 6 . It's clear that the number of infosets is much more than the number of pa- rameters used in DNCFR. For example, on Leduc(15), tabular CFR needs 128 times more memory than DNCFR. In the figure, we use the ratio between the runtime of DNCFR and CFR as horizontal axis, and the sampling(observed) infosets ratios of DNCFR and full-width tabular CFR as vertical axis. Note that, the larger the sampling ratio, the more memory will be needed to save the sampled values. Clearly, there is a trade-off between the relative runtime and relative memory in DNCFR: the longer the relative runtime, the less the relative memory needed for DNCFR. It is reasonable to expect that a useful method should lead to "fair" trade between space and time. That is onefold increase in relative runtime should lead onefold decreases in relative memory (the dashed line in  Figure 6 , slope -1). Interestingly, DNCFR achieves a much better trade-off between relative runtime and memory: for onefold increases in relative runtime, DNCFR may lead to fivefold decreases in relative memory consumption (red line, slope -5). We believe this is due to the generalization ability of the learned neural networks in DNCFR. To present the time space trade off under a range of exploitability, we set the fixed exploitability as 1.0, 0.5, 0.1, 0.05, 0.01 and 0.005 and perform both neural and tabular CFR on Leduc Hold'em.  Figure 6  presents DNCFR achieves a much better time and space trade-off. We believe the research on neural CFR is important for future work and the running time is not the key limitation of our DNCFR. Some recent works (Schmid et al., 2018; Davis et al., 2019) provide strong variance reduction techniques for MCCFR and suggest promising direction for DNCFR. In the future, we will combine DNCFR with the latest acceleration techniques and use multiple processes or distributed computation to make it more efficient.

Section Title: HEADS-UP NO-LIMIT TEXAS HOLD'EM
  HEADS-UP NO-LIMIT TEXAS HOLD'EM To test the scalability of the DNCFR on extremely large game, we develop a neural agent to solve HUNL. However, it's a challenging task to directly solve HUNL even with abstraction technique. For example, ABS-CFR uses k-means to cluster similar cards into thousands of clusters. Although it's a rough abstraction of original HUNL, such agent contains about 2 × 10 10 infosets and needs 80GB memory to store its strategies. The working memory for training ABS-CFR is even larger (more than about 200GB), because it needs to store cumulative regrets and other essential variables, such as the abstracted mapping. To make it tractable for solving HUNL via deep learning, we assemble the ideas from both DeepStack (Moravcik et al., 2017) and Libratus (Brown & Sandholm, 2017). Firstly, we train flop and turn networks like DeepStack and use these networks to predict counterfactual value when given two players' ranges and the pot size. Specifically, the flop network estimates values after dealing the first three public cards and the turn network estimates values after dealing the fourth public card. After that, we train blueprint strategies like Libratus. In contrast, the blueprint strategies in our settings are learned by DNCFR. Because we have value networks to estimate counterfactual values, there is no need for us to arrive at terminal nodes at the river. To demonstrate the convergence of DNCFR, firstly, we test it on HUNL(1). Such game has no limited number of actions, contains four actions in each infoset, and ends with the terminals where the first three public cards are dealt. HUNL(1) contains more than 2×10 8 infosets and 3×10 11 states. It's tractable to compute its exploitability within the limited time. We believe this game is suitable to evaluate the scalability and generalization of DNCFR. Figure 7(a) provides the convergence of DNCFR on different embedding size: emd=8, 16, 32, 64, 128. The smallest neural network only contains 608 parameters while the largest one contains 71168 parameters. It's reasonable to expect that a larger neural network typically achieves better performance because more parameters typically help neural networks represent more complicated patterns and structures. Figure 7(b) presents the performance of using the different number of stochastic gradient descent (SGD) updates to train neural network on each MCCFR iteration. The results show that the number of SGD updates on each iteration affects the asymptotic exploitability of DNCFR. It's Published as a conference paper at ICLR 2020 reasonable because the neural network achieves small loss as the number of gradient descent updates is increasing. Finally, we measure the head-to-head performance of our neural agent against its tabular version and ABS- CFR on HUNL. ABS-CFR is a strong HUNL agent, which is the advanced version of the third-place agent in ACPC 2018. Although ABS-CFR used both card and action abstraction techniques, it still needs 80GB memory to store its strategies. More details about ABS-CFR are provided in Appendix G.1. Although abstraction pathologies are well known in extensive games (Waugh et al., 2009), typically, finer grained abstraction leads to better strategy in many settings. Following this idea, we use DNCFR to learn blueprint strategies on HUNL(2), which is similar to HUNL(1) but contains eight actions in each infoset. HUNL(2) contains 8×10 10 infosets. Such large game size makes it intractable to perform subgame solving (Burch et al., 2014) in real-time. For the next rounds, we use continual resolving techniques to compute strategy in real-time. The action size in the look-ahead tree is similar to Table S3 in Moravcik et al. (2017). The tabular agent is similar to our neural agent except for using tabular CFR to learn blueprint strategies. When variance reduction techniques (Burch et al., 2018) are applied 3 , Figure 7(c) shows that our neural agent beats ABS-CFR by 9.8±4.1 chips per game and obtains similar performance (0.7±2.2 chips per game) with its tabular agent. In contrast, our neural only needs to store 1070592 parameters, which uses much less memory than both tabular agent and ABS-CFR.

Section Title: RELATED WORKS AND DISCUSSION
  RELATED WORKS AND DISCUSSION Solving IIGs via function approximation methods is an important and challenging problem. Neural Fictitious Self-Play (NFSP) (Heinrich & Silver, 2016) is a function approximation method based on deep reinforcement learning, which is a prior leading method to solve IIG. However, fictitious play empirically converges slower than CFR-based approaches in many settings. Recently, Lockhart et al. (2019) propose a new framework to directly optimize the final policy against worst-case opponents. However, the authors consider only small games. Regression CFR (RCFR) (Waugh et al., 2015) is a function approximation method based on CFR. However, RCFR needs to traverse the full game tree. Such traversal is intractable in large games. In addition, RCFR uses hand-crafted features and regression tree to estimate cumulative regret rather than learning features from data. Deep learning empirically performs better than regression tree in many areas, such as the Transformer and BERT in natural language models (Ashish Vaswani, 2017; Jacob Devlin, 2018). In the past year, concurrent works deep CFR (DCFR) (Brown et al., 2018) and single deep CFR (SD- CFR) (Steinberger, 2019) have been proposed to address this problem via deep learning. DCFR, SDCFR, RCFR and our DNCFR are based on the framework of counterfactual regret minimization. However, there are many differences in several important aspects, which are listed as follows. (1) We represent the extensive-form game by recurrent neural network. The proposed LSTM with attention performs better than fully connected network (see details in Section 3.2). (2) DNCFR updates the cumulative regret only based on the additionally collected samples in current iteration rather than using the samples in a big reservoir (see details in Section 3.3.1). (3) It's important to use squared-loss for the average strategies rather than log loss. Because the log loss is based on the big reservoir samples up to T -th iteration, it is very memory-expensive (see details in Section 3.3.2). (4) Another important aspect to make deep learning model work is that we divide regret by √ T and renormalize the regret, because the cumulative regret can grow unboundedly 3 It's well known that head-to-head evaluation of HUNL is challenging because of its high variance. AIVAT is the state-of-the-art technique to reduce evaluation variance on poker evaluation.
  Section 6.

```
