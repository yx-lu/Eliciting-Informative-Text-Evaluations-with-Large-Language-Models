<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 OPTIMAL UNSUPERVISED DOMAIN TRANSLATION</article-title></title-group><abstract><p>Unsupervised Domain Translation (UDT) consists in finding meaningful correspon- dences between two domains, without access to explicit pairings between them. Following the seminal work of CycleGAN, many variants and extensions of this model have been applied successfully to a wide range of applications. However, these methods remain poorly understood, and lack convincing theoretical guaran- tees. In this work, we define UDT in a rigorous, non-ambiguous manner, explore the implicit biases present in the approach and demonstrate the limits of theses approaches. Specifically, we show that mappings produced by these methods are biased towards low energy transformations, leading us to cast UDT into an Optimal Transport (OT) framework by making this implicit bias explicit. This not only allows us to provide theoretical guarantees for existing methods, but also to solve UDT problems where previous methods fail. Finally, making the link between the dynamic formulation of OT and CycleGAN, we propose a simple approach to solve UDT, and illustrate its properties in two distinct settings.</p></abstract></article-meta></front><body><sec><p>Given pairs of elements from two different domains, domain translation consists in learning a mapping from one domain to another, linking these paired elements together. If we consider a photograph of a given scene, and an artistic painting of the same scene, we may want to learn the map associating a photograph to paintings describing the same scene, and conversely, for paintings to photographs. A wide range of problems can be formulated as translation, including image-to- image (Isola et al. (2016)), or video-to-video (Wang et al. (2018)), image captioning (Zhang et al. (2016)), natural language translation (Bahdanau et al. (2015)), etc... However, obtaining paired examples can be difficult thus motivating the unpaired setting where only samples from both domains are available which allows us to tackle a wider range of problems. A seminal work in this direction has been the CycleGAN model proposed in Zhu et al. (2017a), which has led to extensions for many applications and has given impressive results.</p><p>The starting point of this work is to understand and study this successful approach as there remains little theoretical understanding of why these models work. Galanti et al. (2018); Yang et al. (2018) have observed that the approach first introduced in Zhu et al. (2017a) is ill-posed: in most cases, any pairing between samples of both domains - many of which are unwanted pairings - satisfies their objective function. This is in contradiction with the empirical results of the model and shows that there must be an implicit bias towards well-behaved mappings. Galanti et al. (2018) made a first step in this direction by demonstrating that desirable mappings are obtained by networks of minimal complexity, relating this notion of complexity to the number of hidden layers of the neural network implementing the mapping. They then show that the number of mappings satisfying the objective is expected to be small 1 . This is indeed a first step but some ambiguity remains: For example, when translating photographs to paintings, instead of preserving the content of the input, we may want to preserve the same color palette. Here, both tasks are in contradiction and no mapping can solve both problems: What task will be solved by the CycleGAN model? What is the right notion of complexity to explain this convergence? Can we steer the model toward a certain task?</p><p>In this work, we aim to answer these questions and further bridge the gap between practice and theory, by first rigorously and unambiguously defining the UDT problem. Optimal Transport theory then appears as a natural tool to regularize UDT models in this context and its dynamical formulation allows us to build a model which overcomes the shortcomings of CycleGAN-like models.</p><p>Our main contributions are the following:</p><p>Under review as a conference paper at ICLR 2020</p><p>&#8226; Highlighting the need for a more rigorous treatment of the problem of UDT, we reformulate it in a rigorous, non-ambiguous and generic way, showing the theoretical limits of the CycleGAN approach.</p><p>&#8226; We provide an explanation for the success of CycleGAN, bridging the gap between theory and empirical findings and shedding light on the implicit biases in those models.</p><p>&#8226; More specifically, this allows us to link UDT and Optimal Transport. We show that any UDT problem can be cast into the Optimal Transport framework and prove that any UDT Task can be solved by selecting the appropriate transportation cost.</p><p>&#8226; Exploiting the connection between residual networks and dynamical systems, we present a general and flexible approach to solve UDT, based on the Dynamical formulation of Optimal Transport. This allows us to train a steerable one-sided mapping, and obtain the inverse mapping for free after training, along with smooth interpolations between both domains.</p><p>&#8226; We empirically highlight the implicit bias and limitations of the CycleGAN approach, and demonstrate the potential of the proposed approach in several different scenarios.</p></sec><sec><title>OVERVIEW OF CYCLEGAN-LIKE MODELS</title><p>In this section, we describe the main features of the successful CycleGAN approach for solving Unsupervised Domain Translation (UDT).</p><p>In the seminal article (Zhu et al. (2017a)), along with many other subsequent works (Lample et al. (2018); Yuan et al. (2018); Chung et al. (2018); Choi et al. (2018)) the presented approach tackles the problem of Unsupervised Domain Translation (UDT), presented as follows: Given samples from two distributions &#945; and &#946; on compact domains A and B, respectively, find neural networks maps T and S, such that each network maps one distribution onto the other, while being each other's mutual inverse. More formally, this problem involves minimizing the following loss:</p><p>- L coh ensures what we will call coherence 2 , namely that:</p><p>- L inv ensures what we will call inversibility, or cycle-consistency in the original paper, namely that:</p><p>L inv is usually referred to as the Cycle-Consistency term, enforced using the L 1 norm. L coh is a GAN term (Goodfellow et al. (2014)), hence the name of the model, rendering the image distributions indistinguishable from the target distribution, and T and S are often implemented by residual networks (He et al. (2016)).</p><p>If the set of coherent and invertible mappings was reduced to a singleton, this loss would define a well- posed problem with a unique solution. However, this is not the case in any non-trivial setting (Galanti et al. (2018); Benaim et al. (2018)): If the two distributions are discrete and finite with N points each, there are N ! such mappings; this number becomes infinite when the distributions have a density w.r.t. the Lebesgue measure. This means that all mappings from &#945; to &#946; satisfying these minimal requirements are optimal w.r.t. the CycleGAN objective, contradicting empirical results which tend to show that for a certain number of tasks, the model robustly converges toward mappings which give satisfactory results.</p></sec><sec><title>UNSUPERVISED DOMAIN TRANSLATION</title><p>In this section, we define rigorously UDT and use this definition to study CycleGAN models theoretically and empirically.</p></sec><sec><title>A FORMAL DEFINITION</title><p>As seen previously, CycleGan's optimization problem admits as optima all possible coherent and invertible mappings, therefore not well-specifying the problem. This motivates the need for a clear and rigorous definition of an UDT Task.</p><p>Let X &#945;,&#946; = {X|X : A &#8594; B, X coherent and invertible w.r.t. (&#945;, &#946;)} denote the set of all mappings from &#945; to &#946;. For any given UDT Task, let T &#8834; X &#945;,&#946; be the set of all desirable mappings for the domain pair (&#945;, &#946;). In this case, T not only depends on the domain pair (&#945;, &#946;), but may also vary from one application to another. For instance, in the example task of translating photographs to paintings, we may want all solution mappings T &#8712; T from photographs to paintings to describe the same underlying reality, and thus to preserve as much as possible the content of the photograph. However, another valid task may be to link photographs to paintings with the same color palette, thus preserving as much as possible the colors of the input, and not necessarily the content of the photograph. As we will show in 2.2 and in the experiments of 4.1, in order to solve UDT in a generic way, a general approach must be able to treat both tasks, and recover desirable mappings for each task.</p><p>This clearly demonstrates the necessity of redefining the problem of UDT, in order to treat it in a generic way: Definition 2.1 (UDT Task). Consider &#945;, &#946;, two distributions respectively supported on A and B and representing each domain and T &#8834; X &#945;,&#946; . A UDT Task can then be defined as the triple (&#945;, &#946;, T ) where T represents the set of all mappings suitable for the considered task 3 .</p><p>It is now possible to formally define what it means for a UDT Task to be solved by a given approach: Definition 2.2 ((A) solves UDT Task (&#945;, &#946;, T )). Let (A) be an approach for UDT, defined by an optimization program P which yields the non-empty set of optima S &#8838; X &#945;,&#946; . A UDT Task (&#945;, &#946;, T ) is then said to be solved by (A) when S &#8838; T .</p><p>In short, the couple of domains (&#945;, &#946;) doesn't define a valid problem in itself and there has to be a more specific requirement defining valid mappings.</p></sec><sec><title>CYCLEGAN-LIKE MODELS ARE ILL-POSED</title><p>From the previous definitions and observations, it directly follows that:</p><p>Proposition 1. The only task CycleGAN-based methods solve is (&#945;, &#946;, X &#945;,&#946; ). As X &#945;,&#946; is the set of all minimal-requirement mappings from &#945; to &#946; and is of infinite size in general, solving this task is trivial, non restrictive, and thus not meaningful in practice. However, as demonstrated by empirical results, the effective task solved by these methods is one that is clearly more restrictive than (&#945;, &#946;, X &#945;,&#946; ), thereby contradicting the former theoretical result. This exhibits the presence of implicit bias in the chosen architectures, hyper-parameters or training methods, assuring the well-posedness the loss does not account for. Recovering these implicit biases, making them explicit is the goal of the following Section 2.3.</p><p>It is possible to take one step further and prove the following proposition for any approach of UDT: Proposition 2 (No Free Lunch for UDT). For any approach (A) with a non-empty set of optima for two domains (&#945;, &#946;) not reduced to singletons, there exists a UDT task which it doesn't solve. Proof. Consider S as the set of optima given by (A). If S = X &#945;,&#946; , as X &#945;,&#946; has more than one element because &#945;, &#946; aren't singletons by hypothesis, there exists T0 X &#945;,&#946; and the task (&#945;, &#946;, T0) verifies S &#8838; T0 and thus isn't solved by (A). If S = X &#945;,&#946; , we take T &#8712; X &#945;,&#946; &#8722; S and (&#945;, &#946;, T0 = {T }) isn't solved by (A).</p><p>These results demonstrate that no given method for UDT, in particular methods based on CycleGAN, can solve all UDT Tasks but that a method has to be tailored for each UDT task. In Section 3, we present a generalized, more specifically targeted approach for UDT, giving us an explicit control over the solutions we converge to, thus allowing us to tackle UDT problems that cannot be solved using previously presented methods.</p></sec><sec><title>LOW-DIMENSIONAL EMPIRICAL STUDY OF CYCLEGAN</title><p>CycleGAN's associated optimization problem is highly non-convex and challenging, with provably many optimal solutions, as shown in the previous section. In practice, it is solved using SGD-based methods, meaning that the retrieved solutions are highly dependent on the parametrization of the weights, and the choice of initialization and training hyper-parameters. As it is very difficult to study CycleGAN on real datasets, we conduct low-dimensional toy experiments 4 . Most notably, we have observed that the initialization gain &#963;, i.e. the standard deviation of the weights of the residual network, has a substantial impact on the retrieved mappings. In <xref ref-type="fig" rid="fig_0">Figure 1</xref>, we observe the effect of changing the gain from its original value, &#963; = 0.01, to a higher one, &#963; = 1. We can see that it does change the obtained mapping from a simple translation aligning the two distributions to a more disorderly one. In other words, higher initialization gains lead to higher energy mappings. A natural characterisation of the disorder/complexity of a mapping is the distance between a point x and its image T (x), averaged across points of &#945;. E.g. using the squared Euclidean distance, this corresponds to the kinetic energy of the displacement. As seen in the following section, this quantity, the transport cost, is the basis of Optimal Transport theory (Santambrogio (2015)). We use this metric to quantify the effect of initialization gain, <xref ref-type="fig" rid="fig_1">Figure 2</xref>: On the left, we observe that the larger init. gain becomes, the further CycleGAN's mapping is from the mapping of minimal transport cost w.r.t. the Euclidean distance. The right curve confirms and precises this impression: Before training, as the init. gain grows, so does the transport cost of CycleGAN. Note that the variance across runs also increases, i.e. the approach yields very different mappings across runs, corroborating the ill-posed nature described in the previous sections.</p><p>To summarize, small initialization, those used in practice in CycleGAN, bias the mappings to ones with low Euclidean transport cost, which are less "disorderly", thus giving the simplest alignment of the distributions as output of the model. This works well for many common UDT tasks, where the goal is often to find a "conservative" correspondence between the two domains, which explains the practical success of CycleGAN models despite their ill-posedness. However, for certain tasks, as shown in Section 4, this bias gives the wrong mapping and can't be steered toward the right one.</p></sec><sec><title>UNSUPERVISED DOMAIN TRANSLATION AS OPTIMAL TRANSPORT</title><p>Using OT as way to solve UDT arises a very natural thing to do, as for most applications, we wish to preserve input features as much as possible: this is precisely what is given by the OT mapping. Building on the empirical findings of the previous section, we make this implicit bias explicit by formalizing the link between Optimal Transport (OT) and UDT. We outline the main features of OT theory and show how they provide a powerful tool to tackle UDT in the general sense, as posed in Section 2.1. We then present a general and flexible approach to solve UDT, based the dynamical formulation of OT.</p></sec><sec><title>FROM UNSUPERVISED DOMAIN TRANSLATION TO OPTIMAL TRANSPORT</title><p>Let &#945; and &#946; be two absolutely continuous distributions, and c a cost function. We can then consider the classical Monge formulation of OT, referred to as (A c ):</p><p>We start with a technical definition: Definition 3.1 (Twist condition). If c : A &#215; B &#8594; R is a cost function, it verifies the Twist condition if it is differentiable w.r.t. its first variable and, for any x 0 &#8712; A, y &#8712; B &#8594; &#8711; x0 c(x 0 , y) is injective. The following result from OT theory, which is proved in Theorem 1.22 of Santambrogio (2015) 5 , not only links OT to UDT, but also ensures that by explicitly minimizing the transportation cost leads to a well-posed optimization problem:</p><p>Theorem 1. Let &#945;, &#946; be absolutely continuous measures. If c verifies the Twist condition, there exists a unique couple (T, S) of transformations such that:</p><p>&#8226; (T, S) are coherent and invertible w.r.t. the domain couple (&#945;, &#946;);</p><p>&#8226; C(T ) is minimal, and S is the minimal transport from &#946; to &#945;.</p><p>This result has several important implications: Existence and uniqueness of the minimal transforma- tion are guaranteed given a certain cost c, rendering the optimization problem well-posed. Moreover, coherence is not only satisfied for T , but also for S, along with invertibility for both.</p><p>Remark. The Twist condition is not very restrictive: Typically, it is verified for any c(x, y) = h(x&#8722;y) with h strictly convex.</p><p>Building on the previous theorem, there remains the question of knowing whether there exists a cost function adapted for any given UDT task, as defined in Section 2.1. The following answers affirmatively this question for any task which satisfies the preconditions 6 :</p><p>Proposition 3. For any UDT task (&#945;, &#946;, T ), assuming there is at least one map from T which is differentiable and whose Jacobian is invertible at every point, there exists a cost function c verifying the Twist condition such that (A c ) solves Task (&#945;, &#946;, T ).</p><p>Proof. Consider UDT Task (&#945;, &#946;, T ), and T &#8712; T , differentiable with invertible Jacobian. Let us define a cost, for instance c(x, y) = T (x) &#8722; y 2 2 . In this case, c is differentiable w.r.t. its first variable by differentiability of T . For x0 &#8712; A, we then have that: &#8704;y, &#8711;x 0 c(x0, y) = 2 t (Jacx 0 T )(T (x0) &#8722; y) which is clearly injective in y by invertibility of Jacx 0 T . Thus c verifies the Twist condition. Moreover, for any transport map T , we have that C(T ) &#8805; 0 and we also have that C(T ) = 0 which shows that T is indeed the OT map for c by unicity of the optimum.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>Note that this existence result is not constructive: the definition of c uses the optimal T we are looking for. In practice, a cost can be constructed using some sort of prior knowledge of the task at hand and this is a necessary requirement: without injecting any information about the task -implicitly or explicitly- it is clear that it is impossible to solve any UDT problem in a consistent way. In particular, we advocate for a more rigorous treatment of the problem of UDT, by carefully designing the cost function based on prior information of the task as a way to define the geometry of the problem, as opposed to conducting large hyper-parameter sweeps until the task is empirically solved.</p></sec><sec><title>SOLVING UDT USING THE DYNAMICAL FORMULATION OF OPTIMAL TRANSPORT</title><p>In the following, we give an equivalent formulation of the Monge problem (1), providing us with a mapping between domains that generalizes the residual architecture of the forward map in CycleGAN. This will also provide a robust computational method to calculate the retrieve the OT mappings in high-dimensional settings while obtaining the inverse without directly parametrizing it. The theoretical framework presented here has been pioneered in Benamou &amp; Brenier (2000) and a detailed modern presentation is given in chapters 4 and 5 of Santambrogio (2015).</p></sec><sec><title>A Dynamical Formulation of OT</title><p>In the previous section, T was seen as a static transformation between &#945; and &#946;. For a wide range of costs, e.g. cost of the form c(x, y) = x &#8722; y p p , there exists a dynamical point of view, similar in intuition and formulation to the equations of fluid dynamics. The general idea is to produce T by using a velocity field v which gradually transports particles from &#945; to &#946;. The optimal transport map can then be recovered from a path of minimal length, with v solving the optimization problem:</p><p>where (&#181; t ) t&#8712;[0,1] is the geodesic path from &#945; to &#946; in a measure space (see Figure 6 in the appendix). For costs of the form 7 c(x, y) = x &#8722; y p p , this formulation is equivalent to (1), meaning that the overall transformation from &#945; to &#946; yields the optimal transport cost and is thus the same. The mathematical details are summarized in appendix B.</p><p>Directly solving (2) requires solving the continuity equation &#8706; t &#181; t + &#8711; &#183; (&#181; t v t ) = 0, starting from the initial density of &#945;. However &#945; is unknown: we only have access to samples and estimating &#945; in high-dimensional spaces is prohibitive. An alternative is to model the trajectories induced by the elements x &#8712; A the support of &#945;, displaced along the vector field v, instead of densities. Letting &#966; : A &#215; [0, 1] &#8594; R d , (x, t) &#8594; &#966; x t describe the position of elements x of &#945; at time t, when they are displaced along v (see figure 6), then the optimization problem can be equivalently written as:</p><p>where function &#966; &#183; t : A &#8594; R d is the transport map at time t. This problem can be treated as a continuous-time optimal control problem, and can thus be solved using standard techniques Santambrogio (2015). A detailed instance of this approach, used in the experiments of the following section, is given in Section A of the appendix.</p><p>Link with the ResNet implementation of CycleGAN If v k corresponds to the residual block at layer k of the residual network defined by &#966; x k = &#966; x k&#8722;1 + v k (&#966; x k&#8722;1 ), taking the continuous time limit recovers the differential equation &#8706; t &#966; x t = v t (&#966; x t ) (Weinan (2017)). Thus, if we discretize the forward equation in (3) using an Euler numerical scheme, we recover the forward map in the CycleGAN 7 A more general family of costs can be considered in the dynamical setting at the expense of some technicali- ties, see Figalli (2008). architecture 8 (cf. A.1 in the appendix). Moreover, using small initialization gains for the network (cf. 2.3) tends to bias v t p L p ((&#966; &#183; t ) &#945;) to be small as well, linking latent trajectories of residual networks with these minimal length trajectories: refer to Section C of the appendix, where this is clearly illustrated on a 1d toy UDT Task. Notably, using this continuous formulation, there is no need to parametrize the inverse, as it can be simply obtained by taking the velocity field &#8722;v t for the reverse map, automatically dividing the number of necessary parameters by two. Note that continuous interpolations can be easily obtained, saving the intermediate outputs of the forward model. Transforming the input and output distributions Costs of the form c(x, y) = x &#8722; y p p might seem to be too restrictive. However, this apparent difficulty can be circumvented by choosing the right transformation of input and output distribution: Taking two diffeomorphisms &#968; (1) and &#968; (2) , we can replace (&#945;, &#946;) by ((&#968; (1) ) &#945;, (&#968; (2) ) &#946;) and solve the transformed problem instead, obtain a mapping T &#968; and the solution for the initial problem would be (&#968; (2) ) &#8722;1 &#8226; T &#968; &#8226; &#968; (1) . For example, one might use an encoder to a well chosen latent space. In this way, it is easy to prove that costs of this form do not limit the solvability of UDT tasks.</p></sec><sec><title>EXPERIMENTS</title><p>In this section, we describe two sets of experiments, comparing our approach based on OT with the CycleGAN model. Throughout all the experiments, we have used the numerical method based on Equation 3 which practical implementation is described in Section A of the appendix. In particular, in order to stabilize adversarial training, which enforces boundary conditions for both our model and CycleGAN, we have chosen to use an auto-encoder to a lower dimensional latent space. This limits the sharpness of output images but allows to produce consistent and reproducible results which allow meaningful comparisons. Note that in our approach, we only train a one-sided mapping, and retrieve the inverse after training.</p></sec><sec><title>MNIST DIGIT SWAP TASK</title><p>This toy task uses MNIST data in order to illustrate the fundamental limitation of the CycleGAN family of models and the benefits of using our OT formulation. We construct a dataset consisting of two domains using MNIST digits, for which two different (conflicting) Tasks may be considered. We then show that using our formulation, we are able to solve both Tasks. A detailed overview of the experiment is available in the Appendix, Section D.</p></sec><sec><title>CELEBA MALE TO FEMALE</title><p><xref ref-type="fig" rid="fig_2">Figure 3</xref> illustrates how our model works for Male to Female translation (forward) and back (reverse) on the CelebA dataset, displaying intermediate images as the input distribution gradually transforms into the target distribution. Note that no cycle-consistency is being explicitly enforced here and that the reverse is not directly parametrized nor trained but still performs well. The model changes relevant high-level attributes when progressively aligning the distributions but doesn't change non- relevant features (hair or skin color, background,...) which is coherent to what is expected for an optimal map w.r.t. an attractive cost function (here the squared Euclidean one). <xref ref-type="fig" rid="fig_3">Figure 4</xref> and <xref ref-type="table" rid="tab_0">Table 1</xref> compares our model with CycleGAN for various values of the initialization gain hyper-parameter, which effect has been discussed in low dimensions in section 2.3. This confirms our earlier findings: High values of this hyper-parameter make CycleGAN lead to high transport cost, Under review as a conference paper at ICLR 2020 leading to mappings which are not as satisfying for the implicit UDT task being solved here: finding a mapping which transforms men to women and vice-versa while preserving the other features of the mapped image as much as possible. Moreover, training with these high values also induces instability during training, especially in high-dimensional settings, requiring us to map the samples through an encoder before transporting them. Note that our approach is robust to these changes. Finally, note the resemblance between the outputs produced with our approach and CycleGAN with &#963; = 0.01.</p></sec><sec><title>BIASED CELEBA</title><p>In this experiment, we explore the utility of our approach applying our method for solving a UDT Task where the dataset is biased, e.g. samples from the dataset do not reflect the true underlying distributions. To this extent, we consider a subset of the CelebA dataset, where domain &#945; and &#946; correspond to female faces with black hair which are non-smiling for &#945;, and smiling for &#946;. However, we assume that we do not have access to samples from &#946;, only samples from an approximate &#946;, which in this case corresponds to female faces with blond hair and smiling. In this situation, one expects a mapping biased with a quadratic cost to map black-haired non smiling faces to blond smiling ones. This is indeed what happens for the CycleGAN model as we see in <xref ref-type="fig" rid="fig_4">figure 5</xref> where we can observe that hair color is modified along with the smile feature. However, if we want to have a mapping which preserves hair color then a stronger, more specific prior has to be added. We achieve this by using a cost over the color histograms of the images and the results show that we are thus able to find a mapping which preserves hair color while still finding an otherwise satisfying mapping between the two distributions.</p><p>This experiment shows a good example of how to construct a cost: The practitioner defines what needs to be preserved, finds a cost with an appropriate bias (taking into account the structure of the studied datasets) and can then find the desired UDT mappings. It also shows that this method can be used to correct biases in datasets, which were difficult to deal with in the original CycleGAN model.</p></sec><sec><title>DISCUSSION</title><p>In this work, we advocate for a more principled treatment of the problem of UDT, by carefully designing the cost function based on prior information of the task as a way to define the geometry of the problem, as opposed to conducting large hyper-parameter sweeps until the task is solved. It is important to note that this formulation does not miraculously solve all UDT problems, since the difficulty now resides in conceiving the right transport cost reflecting the task at hand. This difficulty is symptomatic of a more general difficulty present in unsupervised problems, where one tries to solve a problem without necessarily having a precise knowledge of it.</p><p>We believe the link between OT and UDT may provide us with algorithms to solve new UDT problems. Leveraging theoretical and practical advances in OT on UDT may also yield new ways to attack other problems. As we have shown in this work, using this link has allowed us to obtain the inverse mapping along with continuous interpolations of the domains. This may also allow us to tackle Multi-Domain Translation (Zhu et al. (2017b)) linking it to the problem of Multi-Marginal OT (see Peyr&#233; &amp; Cuturi (2018)), or even Many-to-Many Mappings (Almahairi et al. (2018)) which may be tackle using Entropic OT (Peyr&#233; &amp; Cuturi (2018)). We leave exploring these links more in depth as future work.</p></sec><sec><title>RELATED WORK</title><p>Explaining why practical UDT methods work well when the problem is ill posed has motivated some recent work. Galanti et al. (2018); Benaim et al. (2018), show that coherent mappings are obtained by NNs of minimal functional complexity, a notion related to the number of layers. However, this Under review as a conference paper at ICLR 2020 notion of complexity does not account for problem dependent semantics, and thus it is still not clear which UDT Task this method solves. In our approach, we define a problem-dependent notion of complexity, and solve the associated OT problem. This allows for theoretical results on the solution mapping, e.g. results on (non-)existence and (non-)unicity of the solution. They prove the number of such solution mappings is small, assuming that networks with a small number of layers lead to good pairings. Moreover, similarly to us, Galanti et al. (2018); Benaim &amp; Wolf (2017) show that learning a one-sided mapping is possible, however do not obtain the inverse mapping. Others have tried a hybrid approach between paired and unpaired translation Tripathy et al. (2018), which still doesn't solve the problem of ill-posedness.</p><p>The dynamical approach presented in Section 3.2 bears similarities with recent work (Chen et al. (2018); Grathwohl et al. (2018)), making use of the link between residual networks and ODEs made in Weinan (2017). Their objective is the design of new neural models and not solving a task like we do. Also different from us, they displace a known density (usually Gaussian) along a learned trajectory. This is not possible in our case since both domain distributions are unknown. Gong et al. (2018) uses a variant of the dynamical formulation for this problem but do not make any link with OT nor do they tackle the ill-posedness issue.</p><p>In the domain adaptation field, using Optimal Transport to help a classifier extrapolate has been around for some years, e.g. Courty et al. (2015); Damodaran et al. (2018) use a transport cost to align two distributions. The task, although related, is clearly different and so are the methods they develop. In particular they do not consider the dynamical aspect of the transformation process and the link with the dynamics of NNs.</p></sec><sec><title>CONCLUSION</title><p>We have defined UDT in a rigorous and general way, and have shown that CycleGAN is biased towards mappings with low transport cost for the squared Euclidean distance. This has allowed us to highlight the approach's limitations, showing both theoretically and empirically that this method cannot solve any given UDT Task. Making the implicit bias present in this model explicit has led us to formulate the problem as an OT problem, which is a very natural way to solve UDT. This approach comes with theoretical guarantees: we have then proven that it is possible to solve any UDT problem within this framework. We make the link between the dynamical formulation of OT and CycleGAN, and propose a simple and robust approach to solve UDT, illustrating its properties in different experimental settings. More generally, we believe that this framework opens many interesting research directions, along with potentially new practical methods to solve existing and novel UDT problems.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>CycleGAN for simple 2D distributions. Small initialization tend to yield simple, ordered mappings (Left), whereas larger initialization yield complex, disordered ones (Right).</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Left: Distance to the minimal transport mapping "Wasserstein 2 Transport" as a function of the initialization gain (domains are illustrated in Figure 1). Right: Transport cost of the CycleGAN mapping as a function of init. gain. Metrics are averaged across 5 runs, and the standard deviation is plotted.</p></caption><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Transport cost of Male to Female translation for different initialization gains.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Male to Female translation (top) and the inverse (bottom). Intermediate images correspond to the interpolations provided by the network's intermediate layers. The reverse mapping is obtained by simply inverting the forward network once trained (see text for explanation).</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>CelebA experiment. Each column associates one input image to its outputs for different models: CycleGAN and our model with different initial gain parameters. Our model gives consistently the same result while CycleGAN converges to different mappings depending on the parameter.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>Results for Biased CelebA. We wish to map faces that have the Non-Smiling and Black Hair attributes to Smiling, Black-Hair faces, while only accessing Smiling, Blond Hair faces for the target domain. Our formulation with c(x, y) = H(x) &#8722; H(y) 2 2 , where H(x) corresponds to the histogram of image x, is well adapted for solving this problem.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig></sec></body><back /></article>