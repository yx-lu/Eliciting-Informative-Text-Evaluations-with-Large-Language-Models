Title:
```
Under review as a conference paper at ICLR 2020 NESTED LEARNING FOR MULTI-GRANULAR TASKS
```
Abstract:
```
Standard deep neural networks (DNNs) used for classification are trained in an end-to-end fashion for very specific tasks - object recognition, face identification, character recognition, etc. This specificity often leads to overconfident models that generalize poorly to samples that are not from the original training distribu- tion. Moreover, they do not allow to leverage information from heterogeneously annotated data, where for example, labels may be provided with different levels of granularity. Finally, standard DNNs do not produce results with simultaneous dif- ferent levels of confidence for different levels of detail, they are most commonly an all or nothing approach. To address these challenges, we introduce the problem of nested learning: how to obtain a hierarchical representation of the input such that a coarse label can be extracted first, and sequentially refine this representa- tion to obtain successively refined predictions, all of them with the corresponding confidence. We explicitly enforce this behaviour by creating a sequence of nested information bottlenecks. Looking at the problem of nested learning from an in- formation theory perspective, we design a network topology with two important properties. First, a sequence of low dimensional (nested) feature embeddings are enforced. Then we show how the explicit combination of nested outputs can im- prove both robustness and finer predictions. Experimental results on CIFAR-10, MNIST, and FASHION-MNIST demonstrate that nested learning outperforms the same network trained in the standard end-to-end fashion. Since the network can be naturally trained with mixed data labeled at different levels of nested details, we also study what is the most efficient way of annotating data, when a fixed train- ing budget is given and the cost of labels increases with the levels in the nested hierarchy. 1 https://github.com/nestedlearning2019
```

Figures/Tables Captions:
```
Figure 1: On the left, an illustration of a set of nested predictions and their associated confidence given an input image of a face. The top block illustrates a desired behavior. Depending on the quality of the input data, one may be able to provide up to a certain level of prediction. This nested learning is the problem addressed in this paper. The bottom block illustrates how standard DNN-based models behave when they are trained in a end-to-end fashion to perform specific tasks such as face recognition. Clearly the traditional network is over-confident in its predictions (potentially wrong for the last 3 cases), and provides an all or nothing response instead of responding only what it can for the given input quality. On the right, we see a real example with results from the proposed framework; while a sharp image gets all the nested levels with high confidence in our proposed system, a low-quality one is getting the first two levels with confidence, while the finer one is correct but low confidence as expected. (Additional examples are presented in Figure 5 in the supplementary material.)
Figure 2: On the left we illustrate the taxonomy of an example of strictly nested labels. First handwritten characters are classified as "number" or "letter," and then these categories are refined into specific numbers and letters. X, Y1 and Y2 denote random variables representing the input, the coarse label, and the fine label respectively. Y k i represents the random variable associated to each value k in the coarser node, i.e., Yi given that yi−1 = k, k ∈ Yi−1. The diagram in the center, illustrate the entropy of a fine and coarse level, and how having information about a coarser level may reduce the entropy of the fine level. The right diagram illustrates the reduction of uncertainty on the labels given the input, and how the uncertainty on the fine labels can be reduced even further if input information and coarse information are combined.
Figure 3: Illustrative scheme of the proposed framework. From left to right, the input data x ∼ X, a first set of layers that extract from X a feature representation f1, which leads toŶ1 (estimation of the coarse label Y1). f1 is then jointly exploited in addition with complementary information of the input. This leads to a second representation f2 from which a finer classification is obtained. The same idea is repeated until the fine level of classification is achieved. It is important to highlight that this high level description of the proposed model can be implemented in multiple ways. In the following sections we present our own implementation and provide details of the specific architecture we chose for experimental validation.
Figure 4: Accuracy on the MNIST data when different ratios of coarse, middle, and fine samples are selected for training. Plots report the accuracy on the prediction of the finest category (ten classes: 0 − 9). If we define n1, n2 and n3 the number of samples for which we know only the coarse, middle, and fine label respectively, the budget associated to a training set is Bn 1 ,n 2 ,n 3 = n1g(|Y1|) + n2g(|Y2|) + n3g(|Y3|). g represents a cost function associated to labeling a coarse, middle, and fine sample. In this experiment, we tested three models for g, a linear model where the cost is linear, a concave model (we chose g(u) ∝ log(u)), and a convex model (we chose g(u) ∝ e u ). The colors represent the proportion of coarse samples in the training set. Blue represents more proportion of coarse samples, and red a larger proportion of fine labels. (Additional results are presented in Figure 10 in the supplementary material.)
Table 1: Accuracy and mean confidence (Acc%/Conf %) for Cifar10 dataset. Coarse, fine, and middle indicate the accuracy at each level of the label. End-to-end, denotes the model trained with exclusively data annotated for the fine label. We compare two end-to-end models trained with different amounts of data with fine labels. We first set D3 = 20%, which we increase afterwards to D3 = 32%. On the other hand, "nested" denotes the same architecture, trained with coarse middle and fine labeled data. In this experiment we set D1 = D2 = D3 = 20%. We repeated the distortion generation 10 times (for all the levels of distortion), the last column "Bound on Std" shows the maximum standard deviation obtained across "Distortion 1" to "Distortion 4," therefore the value on the last column can be interpreted as a bound on the variability of the reported results. (Similar results are reported for MNIST and fashion-MNIST datasets, see tables 5, and 6 in the supplementary material.)
Table 2: Comparison of the same network structure, trained on the same coarse, middle, and fine data, with and without skipped connections. 20% of fine middle and coarse samples of MNIST dataset where selected for training. As in the previous experiments, Distortion 1-4 correspond to test distorted samples with turbulence- like distortion (described in the supplementary material).
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep learning is providing remarkable computational tools for the automatic analysis and under- standing of complex high-dimensional problems [ Esteva et al. (2017) ;  Spanhol et al. (2016) ;  Parkhi et al. (2015) ]. Despite its tremendous value and versatility, methods based on Deep Neural Networks (DNNs) tend to be overconfident about their predictions and limited to the task and data they have been trained on [ Hein et al. (2018) ;  Guo et al. (2017) ;  Nguyen et al. (2014) ]. This happens, among other reasons, because the standard approach to train DNN models consists in optimizing its perfor- mance over a specific dataset and for a specific task in an end-to-end fashion  Szegedy et al. (2014) . Standard DNNs are not designed to be trained with data of different quality and to simultaneously provide results at multiple granularities. Take as an example the case illustrated in  Figure 1  (left); for high quality facial images, we may be able to infer the person's age group and identity; whereas for poor resolution or occluded examples, only a sub-set of these nested predictions may be achievable. We expect the network to automatically understand what can and cannot predict, and this is obtained with the framework proposed in this paper ( Figure 1 , right). Moreover, nested learning allows us to leverage training information from diverse datasets, with varying granularity and quality of labels, and combine this information into a single model. On the other hand, when heterogeneous data with different quality and granularity of annotations (as in the example illustrated in  Figure 1  (left)) is provided for training, low quality samples with coarse labels can help us to understand the structure of the coarser distributions (person, under 50) while simultaneously data with finer labels can contribute to the coarse and fine tasks. This will be formalized later in the paper with tools from information theory. Recently,  Alsallakh et al. (2017)  showed that convolutional neural networks (CNNs) naturally tend to learn hierarchical high-level features that discriminate groups of classes in the early layers, while the deeper layers develop more specialized feature detectors. We explicitly enforce this behaviour by creating a sequence of nested information bottlenecks. Looking at the problem of nested learning from an information theory perspective, we design a network topology with two important prop- erties. First, a sequence of low dimensional (nested) feature embeddings are enforced for each level in the taxonomy of the labels. This encourages generalization by forcing information bottle- necks [ Tishby et al. (1999) ,  Shwartz-Ziv & Tishby (2017) ]. Second, skipped connections allow finer embeddings to access information of the input that may be useful for finer classification but not in- formative on coarser categories  Ronneberger et al. (2015) . Additionally, we show how the explicit calibration and combination of nested outputs can improve the finer predictions and improve robust- ness. Finally, having the flexibility of merging data with different levels of granularity inspired us to study which is the most efficient way of annotating data given a fixed budget that takes into account that more detailed training data is more expensive to annotate. The source code associated to this work is open source. The main contributions of this paper are: (1) We introduce the concept of explicit nested learning, where a given level in the hierarchy strictly contains the previous one and strictly adds information; (2) We provide a deep learning architecture that can be trained with data from all levels of the nested structure (all levels of labels and data quality), each one affecting the corresponding component of the network; (3) We provide a model with multiple outputs, one per level of the nested hierar- chy, each one with its own confidence; the user does not need to know the "quality" of the data beforehand, the output confidences provide that information.

Section Title: RELATED WORK
  RELATED WORK The problem of adapting DNNs models and training protocols to encourage nested learning shares similarities with other popular problems in machine learning such as Multi-Task Learning (MTL). Though our work is related to MTL because of the similar training challenges, most of the MTL methods tackle the task in a parallel way, without imposing a task hierarchy in the architecture [ Ranjan et al. (2016) ,  Kokkinos (2017) ,  Bilen & Vedaldi (2016) ]. The idea of learning hierarchical representations to improve classification performance has been exploited prior the proliferation of DNNs, e.g., [ Zweig & Weinshall (2007) ,  Fergus et al. (2010) ,  Zhao et al. (2011) ,  Liu et al. (2013) ]. Some of these ideas have been incorporated into deep learning methods [ Alsallakh et al. (2017) ,  Wehrmann et al. (2018) ,  Deng et al. (2014) ,  Srivastava & Salakhutdinov (2013) ], and exploited in specific applications [ Clark et al. (2017) ,  Xuehong Mao et al. (2016) ,  Seo & shik Shin (2019) ]. Under review as a conference paper at ICLR 2020  Kim et al. (2018)  proposed a nested sparse network architecture with the emphasis on having a resource-aware versatile architecture to meet (simultaneously) diverse resource requirements.  Wehrmann et al. (2018)  proposed a neural network architecture capable of simultaneously opti- mizing local and global loss functions to exploit local and global information while penalizing hier- archical violations.  Triguero & Vens (2016)  investigated different alternatives to label hierarchical multi-label problems by selecting one or multiple thresholds to map output scores to hierarchical pre- dictions, focusing on performance measures such as the H-loss, HMC-loss and the micro-averaged F-measure.  Yan et al. (2015)  introduced hierarchical deep CNNs (HD-CNNs) which consists of em- bedding CNNs into a two-level category hierarchy. They propose to distinguish a coarse class using an initial classifier and then refine the classification into a second level for each individual coarse category. Although the works listed above are important, relevant, and related to the work presented here, there are notable differences between them and what we propose. For example, while  Kim et al. (2018)  propose a nested architecture providing different (potentially nested) outputs, they do not study how to combine these outputs into a refined single prediction, nor provide a reliable confidence measure associate to them. Furthermore, the architecture they propose has key differences with ours, while they propose a nested hierarchy in a end-to-end fashion (i.e., features associated to the coarse and fine levels are shared from the top to the bottom of the network), we enforce sequential information bottleneck. As we show in the following sections this sequence of coarse to fine low dimensional representations facilitate a robust calibration and combination of nested outputs.  Yan et al. (2015)  study the problem of nested learning for two nested levels of granularity, and optimize for a final fine prediction. Their design is specific for a two-level category hierarchy while our work generalize to any number of nested levels, and we simultaneously can train and test up to an arbitrary level of granularity. Another important difference is that the focus of their work is on the implementation details and performance of their two-hierarchy network versus traditional end-to-end learning. As they mention in the conclusion of their work, future work should aim to extend their ideas to more than two hierarchical levels and to contextualize their empirical results into a theoretical framework. Our work takes steps in these two specific directions. This is also a fundamental difference with the works developed by  Triguero & Vens (2016)  and  Wehrmann et al. (2018) . Moreover, all these approaches suppose that every sample is annotated for all the granularity levels. In contrast with them, we can efficiently train (and predict) our model on dataset that provide only coarse labels, intermediate levels, or fine labels. Our approach is therefore more general in that regard. Also, we show that if testing conditions shift from the ones on training, we can still provide relatively confident coarser labels while avoiding overconfident (erroneous) fine predictions (see, e.g.,  Figure 1 , right; and Figure 5 in the supplementary material). Also, the questions of neural network overconfidence and output readability are never addressed in those works, which is crucial for both results interpretation, and output combination. Finally, because our solution can leverage information from datasets with different granularity, we are able to analyze how different proportions of training coarse and fine data affects models cost, performance and robustness.

Section Title: NESTED LEARNING
  NESTED LEARNING Let us assume we want to classify the popular hand written digits of MNIST  LeCun & Cortes (2010) . An input image can be represented as a realization x of the random variable X. 2 We denote as X the alphabet of X. Associated to x, there is a label y that corresponds to the actual number the person writing the character wanted to represent. The label y is a realization of the random variable Y . In this illustrative case, Y can take 10 different values: Y = {0, 1, ..., 9}. Of course Y and X are not independent random variables. Generally, Y precedes X, and the problem of classification can be stated as the problem of inferring y from an observed sample x, i.e., Y → X →Ŷ .Ŷ denotes a new random variable (estimated from X) which approximate Y . More precisely, a common practice is to find a mapping X →Ŷ such that the probability P (Ŷ = Y |X) is minimized.

Section Title: Nested classification
  Nested classification In this work we focus on the inherent hierarchical structure most classification problems have. For example, imagine now that we have hand written characters including numbers, lower case letters, and capital letters. It would be intuitive to first attempt to classify these characters into three categories: numbers, lower case letters, and capital letters. Then, depending on this coarse Under review as a conference paper at ICLR 2020 classification we can perform a finer classification, i.e., classifying the numbers into 0 − 9 classes, the letters into a − z, and so forth. Of course, we could have an arbitrary number of nested random variables associated to different levels of labels granularity. Here, subscripts indicate the granularity of the label, for example, Y i−1 is the closest coarse level of Y i . Y k i represents the random variable associated to each k value in the closest coarse node, i.e., Y k i represents Y i given that y i−1 = k, k ∈ Y i−1 . Definition 3.1. We define Y 1 , ..., Y n as a discrete sequence of nested labels if H(Y i |Y i+1 ) = 0 ∀i ∈ [1, n − 1]. H denotes the standard definition of entropy for discrete random variables. Definition 3.2. A discrete sequence of nested labels Y 1 , ..., Y n is strictly nested if H(Y i |Y i−1 ) < H(Y i ) ∀i ∈ [2, n]. The core of this work is to formulate classification problems such that the information of the input is extracted in a hierarchical way. To this end, intermediate (coarse) predictionsŶ i are jointly learned. The two key components of the proposed approach are: (i) nested information bottlenecks and (ii) a combination of the predicted coarse and fine labels. The analysis provided in this section is agnostic to most of the implementation details (which are addressed in coming sections). Hierarchical information bottlenecks and the role of skipped connections. We assume the random variable X contains information about the sequence of strictly nested labels Y i , i.e., H(Y i |X) < H(Y i ), as illustrated in  Figure 2 . More precisely, H(Y i |X) > H(Y i−1 |X) and H(Y i |X) < H(Y i |Y i−1 , X). To exploit these properties, we use standard DNN layers (convolu- tional, pooling, normalization, and activation layers). As illustrated in  Figure 3 , we begin by guid- ing the network to find a low dimensional feature representation f 1 such that H(f 1 (X)) H(X) while, I(f 1 (X), Y 1 ) is close to I(X, Y 1 ) where I(·, ·) stands for the standard mutual information. (DNNs are remarkably efficient at compressing and extracting the mutual information between high dimensional inputs and target labels [ Shwartz-Ziv & Tishby (2017) ,  Moshkovitz & Tishby (2017) ].)

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 The second step consists in learning the complementary information such that when combined with the representation f 1 , allows us to achieve a second representation f 2 from which the second hierar- chical label Y 2 can be inferred. To this end, skipped connections play a critical role as will be further detailed next. As we discussed before, On the other hand, we want each feature embedding f i to compress the information of X while I(f i (X), Y i ) ≈ I(X, Y i ). Equation 1 means that the finer the classification the more informa- tion from X we need. Notice that while in many DNNs, skipped connections have proved to help on the networks compactness and to mitigate vanishing gradients  Szegedy et al. (2017) , in the present work they are included for a more fundamental reason. (See the experiments presented in the supplementary material Section F.2 for complementary empirical evidence and validation.) If we do not consider skipped connections, X → f i (X) → f i+1 (X) forms a Markov chain where I(X, f i+1 (X)) ≤ I(X, f i (X)) (data-processing inequality) which contradicts Equation 1. Sec- tion 4 presents experiments illustrating the impact of skipped connections on nested learning, and complements the discussion started here.

Section Title: Combination of nested outputs
  Combination of nested outputs We will present in the following sections experimental evidence showing that nested learning leads to an improvement in performance and robustness. In addition, the explicit combination of nested predictions, since the network simultaneously produces all the outputs Y i (with corresponding confidence), can improve the accuracy and robustness even further. Our motivation is to explicitly refine the fine prediction leveraging the information of all the coarser outputs, i.e., {Ŷ 1 , ...,Ŷ i } →Ỹ i . Let us define s i (q) the network output score associated to the event Y i = q. In general, if s i (q) > s i (w) most likely P (Y i = q) > P (Y i = w), but P (Y i = q) = s i (q). In other words, a score value of 0.3 does not mean the sample belongs to this class with 30% probability. This can be addressed by calibrating the outputs, which consists of mapping output scores into an estimation of the class probability s i (q) → PŶ i (q), as defined and thoroughly explained in  Zadrozny & Elkan (2002) . PŶ i (q) denotes the calibrated output of the network which approximates P (Y i = q). (We will precise how calibration is performed in the following section.) Then, we can use the estimated probability associated to a fine label PŶ i to compute the conditional probability P (Y i = y i |Y i−1 = k). This is achieved by re-normalizing the finer labels associated to the same coarse label, i.e., PŶ i|Ŷi−1 (q) = PŶ i (q) w∈Y kq i PŶ i (w) , (2) where Y kq i denotes the set of labels at granularity level i that share with q the same coarser label k q . Finally, the estimated conditional probability is combined with the prior of the coarser prediction to recompute the fine prediction P Ŷ i (q) = PŶ i|Ŷi−1 (q)PŶ i−1 (k q ).

Section Title: IMPLEMENTATION CHALLENGES AND DETAILS
  IMPLEMENTATION CHALLENGES AND DETAILS Training. Let G θ,η (x) = (f i (x, (θ j ) j=1,..,i ), g i (f i , η i )) i=1,..,m be the function coded by our net- work, where m denotes the number of granularity levels and as before x an input sample. Each sub-function g i corresponds to the output of granularity i (computed from the feature bottleneck f i ). G depends on parameters (θ j ) j=1,..,i which are common to the sub-functions of coarser granulari- ties, and some granularity-specific parameters η i . The general framework of the architecture follows  Figure 3 , meaning a trunk of convolutionnal filters with parameters θ and fully connected layers for each intermediate outputs with parameters η. Training this kind of model with a disparity of samples per granularity is hard, and naively sampling random batches of training data leads to a noisy gradient computation  Kokkinos (2017) . (See the supplementary material Section F.3 for complementary experimental validation and analysis.) In order to overcome this issue, we organize the training samples and train the network in a cascaded manner. The dataset D is organized in subsets of samples labeled up to granularity i for i = 1, .., m. Formally we can write D = (x, y) with x the set of inputs and y the set of labels. We consider that x = (x i ) i=1,..,m and y = (y i ) i=1,..,m where D i = (x i , y i ) represents the subset of data for which the label is known up to the granularity level i.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Having the dataset partitioned in this fashion naturally leads to a cascaded training of the network. We train the model to solve a sequence of optimization problems using (x i , y i ) as the training exam- ples at each step. We can write this sequence (P i ) as: (P i ) : min (θj ,ηj )j=1,..,i i j=1 α j L nj (Ŷ j , Y j ), where L n is the n-categorical cross-entropy and α are the weights for each individual loss. We first start to train the network on coarse labels and we gradually add finer labels and start optimizing deeper parameters in an iterative way. We support our cascaded training protocol against a more standard one with experimental results presented in the supplementary (Section F.3). (Additional details are presented as supplementary material Section B.)

Section Title: Scores calibration
  Scores calibration As shown by  Hein et al. (2018) , deep neural networks with ReLU activations tend to produce overconfident results for samples which are out of the training distribution. In order to mitigate this and have meaningful outputs that can be combined, we consider a two step calibration method. The first step consists in adding a "rejection" class for each level of granularity. Synthetic samples associated to this class are generated from an uniform distribution. By training the network with this supplementary class we mitigate the problem of overconfidence for sample that are far from the training distribution. However, keeping a fixed coverage on the input space would require a number of samples that grows exponentially with the dimension of the input space. To overcome this problem, we think of the network as a sequence of encoders in the latent spaces g i (f i (x)) (defined as the penultimate layer prior each prediction), and fine-tune the last layer adding synthetic out-of-distribution samples from an uniform distribution in the latent space. The latent space has a much smaller dimension than the original input space, and therefore is tractable to synthesise samples with a uniform coverage of the latent space. (Additional details are provided in the supplementary material Section B.) The second calibration step is a classic temperature scaling introduced in  Guo et al. (2017) . This technique consists in scaling the output of the fully-connected layer before the softmax activation by an optimal temperature parameter. Given x the input data and g the function coded by a neural network before the softmax activation σ, the new calibrated output is given byḡ = σ( g T ). The temperature parameter T is tuned so that the mean confidence of the predictions matches the empir- ical accuracy, more precisely, we want to minimize, E |P (Ŷ = Y |p = p) − p| , whereas before,Ŷ denotes the network prediction of Y , andp is the empirical confidence associated to it. The previous expression can be approximated, using a validation partition of the data, by computing the Expected Calibration Error (ECE) ECE = N j=1 |Bj | n |acc(B j ) − conf (B j )|,  Naeini et al. (2015) . This mea- sure takes the weighted average between the accuracy and confidence on N bins B j , j = 1...N . n denotes the total number of samples, and |B j | the number of samples on the bin B j .

Section Title: EXPERIMENTS AND DISCUSSION
  EXPERIMENTS AND DISCUSSION We consider three sets of publicly available datasets for experimental evaluation: the handwritten digits from MNIST ( LeCun & Cortes (2010) ), the small clothes images from Fashion-MNIST ( Xiao et al. (2017) ), and CIFAR10 ( Krizhevsky et al. ). First we study how the actual nested nature of the labels affects nested learning. Then we compare end-to-end training versus the proposed nested approach. In a third group of experiments we evaluate different combination methods to leverage all the predictions into a refined fine prediction. Then we evaluate the impact of skipped connec- tions. Finally, we study how the ratio of coarse and fine labels affects the network's performance. Additional experiments are presented in the supplementary material, section A, F Hierarchical structure versus random grouping. We group the MNIST samples into two groups of nested labels: "VG" and "RG," both with three labels of granularity. VG corresponds to a hierar- chy that groups visual similarities; the coarse class groups digits into {{3, 8, 5, 0, 6}, {9, 4, 7, 1, 2}}, the intermediate class into {{3, 8, 5}, {0, 6}, {9, 4, 7}, {1, 2}}, and of course the fine class into {0} − {9}. Additional information on how we grouped the labels for the three datasets is pre- sented in Appendix C. RG consists of a 3 level (coarse/middle/fine) grouping based on the order of N. Empirical results shows that grouping the labels based on visual similarities leads to better results in terms of accuracy, intuitively supporting the idea of nested learning. (See the results presented in Table 4 provided in the supplementary materials.)

Section Title: Standard end-to-end training versus nested learning
  Standard end-to-end training versus nested learning Let us define |D i | as the number of training samples that are annotated up to the level of granularity i. To understand if adding more samples with Under review as a conference paper at ICLR 2020 coarse annotation helps improving the performance on the fine task, we compared models trained exclusively with fine data D A = D 3 and models trained fine data plus coarsely annotated data D B = D 3 + D 2 + D 1 .  Table 1  shows the accuracy for the fine, middle, and coarse outputs for the standard end-to-end network versus the same architecture trained using nested learning and additional coarse and middle data. If we compare the lines for which D 3 = 20%, we observe that adding coarse and middle granularity samples improves the accuracy even for the fine task. Moreover, it also improves the robustness of the model. We tested this models when test data is distorted and shifted from the conditions at training. Distortion 1 to 4 correspond to four levels (increasing the severity) of "turbulence-like" image distortion, the implementation of this distortion is inspired on the work of  Meinhardt-Llopis & Micheli (2014)  (details are provided in the supplementary material, Section D). Again looking at  Table 1  we see that the model trained with additional coarse and middle samples is more robust and less overconfident. A similar pattern is observed for MNIST and fashion-MNIST datasets (see for example, tables 5, and 6 in the supplementary material). Complementing these results, Section F.1 in the supplementary material compared the proposed nested architecture with a Multi-Task learning approach. The previous discussion is interesting as it shows that including additional coarse data tends to help also the discrimination of the fine task. However one may argue that the comparison is unfair, as one model sees more data than the other. That is a very interesting point and we address it in following experiments where we study how the proportion of fine and coarse granularity data affect performance (for a fixed budget and different cost models). For now, let us observe what happens if we increase the amount of fine data from D 3 = 20% to 32% (which assuming a linear cost model equals the budget of training with D 1 = D 2 = D 3 = 20%). As expected, (see  Table 1 ) the performance on clean test data improves for the end-to-end model. However, (see columns Distortion 1-4) it generalizes less to unseen (distorted) test data and also becomes significantly more overconfident. Output combination.  Kuncheva (2004)  presented many useful combination methods for both one- hot encoding classifiers and continuous scores. We compare some of those combination methods (e.g., Mean, Product, and Majority Vote) to the one that we designed specifically for multiple nested outputs. (Details and numerical results are provided in the supplementary material, Section E and Table 3.) We observed that different combination methods perform similar on test data that matches the train data, while the proposed method outperforms the others when test samples are distorted.

Section Title: The role of skipped connections
  The role of skipped connections Skipped connections are included in order to allow information flow from the input to the finer feature bottlenecks.  Table 2  shows the accuracy for two networks with the same structure, trained on the same data, one including skipped connections (SC) and the other-one not. Section F.2 in the supplementary material complements these results, and provides empirical measurements of the mutual information between different components of the model with and without skipped connections.

Section Title: Working on a budget
  Working on a budget As we discussed before, establishing a fair comparison between models trained using more fine or coarse data is not trivial. To address this problem, we investigated three cost models: linear, convex, and concave. For the linear model, we assume the cost associated to annotate one sample (x, y i ) is proportional to |Y i |. Analogously, for the convex/concave cost model, we assume the cost of annotating a sample (x, y i ) is proportional to g(|Y i |) with g(·) being strictly convex/concave. For each budget and cost model, we created hundreds of train sets with different amounts of coarse, middle, and fine samples.  Figure 4  shows the accuracy on the MNIST test set for different budgets and cost models. It is interesting to observe that for a convex cost model, increasing the number of coarse annotations produces better results for the same budget; while as expected, the opposite is observed for a concave cost model. We present additional experiments in the Figure 10 in the supplementary material.

Section Title: CONCLUSION
  CONCLUSION In this work we introduced the concept of nested learning, which improves classification accuracy and robustness. Moreover, it allows to leverage information from datasets annotated with different levels of granularity. Additionally, experiments suggest that nested models have a very desired be- haviour, e.g., they gradually break as the quality of the test data deteriorates. We showed that imple- menting nested learning using a hierarchy of information bottlenecks provides a natural framework to also enforce calibrated outputs, where each level comes with its confidence value. Given a fixed budget and model, we studied what is the economically most efficient strategy for labeling data. Furthermore, experimental results show that if the amount of fine training samples is constant, then adding samples with only a coarse annotation increases the performance and ro- bustness for the fine task. To recap, the introduced nested learning framework performs as expected from our own human experience, where for good data we can provide high level inference with high confidence; and when the data is not so good, we can still provide with high confidence some level of inference on it. Under review as a conference paper at ICLR 2020
  2 Capital letters will be used to denote random variables and lower case letters to denote the value of a particular realization.

```
