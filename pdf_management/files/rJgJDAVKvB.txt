Title:
```
Published as a conference paper at ICLR 2020 LEARNING TO PLAN IN HIGH DIMENSIONS VIA NEURAL EXPLORATION-EXPLOITATION TREES
```
Abstract:
```
We propose a meta path planning algorithm named Neural Exploration-Exploitation Trees (NEXT) for learning from prior experience for solving new path planning problems in high dimensional continuous state and action spaces. Compared to more classical sampling-based methods like RRT, our approach achieves much better sample efficiency in high-dimensions and can benefit from prior experience of planning in similar envi- ronments. More specifically, NEXT exploits a novel neural architecture which can learn promising search directions from problem structures. The learned prior is then integrated into a UCB-type algorithm to achieve an online balance between exploration and exploita- tion when solving a new problem. We conduct thorough experiments to show that NEXT accomplishes new planning problems with more compact search trees and significantly outperforms state-of-the-art methods on several benchmarks. * indicates equal contribution.
```

Figures/Tables Captions:
```
Figure 1: Illustration of NEXT. In each epoch, NEXT is executed on a randomly generated planning problem. The search tree grows withṼ * andπ * guidance. {Ṽ * ,π * } will be updated according to the successful path. Such planning and learning iteration is continued interactively.
Figure 2: Our neural network model maps a N -link robot from the original planning space (a (N + 2)-d configuration space) to a 3d discrete latent planning space in which we plan a path using value iteration. The result of value iteration is then used as features for definingṼ * (s|U ) andπ * (s |s, U ).
Figure 3: Attention-based state embedding module. s w = (x, y) and z = s h . The upper part is spatial attention, with the first two channels being x and y, and the last two channels being constant templates with the row and column coordinates, as shown with a d set to 3. The bottom module learns the representation for z. The final embedding is obtained by outer-product of these two attention parts.
Figure 4: Overall model architecture. Current and goal states are embedded through attention module. Then the embedding of the goal state is concatenated with the map to produce ν *(0) andR as the input to the planning module. The output of the planning module is aggregated with the embedding of the current state to produce feature ψ(s) for definingṼ * andπ * .
Figure 5: Search trees and the learnedṼ * andπ * produced by NEXT. Obstacles are colored in blue. The start and goal locations are denoted by orange and brown dots. In (a) to (c), samples are represented with yellow circles. In (d), the level of redness denotes the value of the cost-to-go estimateṼ * . The cyan arrows point from a given state s to the mean of the learned policyπ * (s |s, U ).
Figure 6: Search trees and a solution path produced in an instance of spacecraft planning. The 7 DOF spacecraft has a yellow body and two 2 DOF red arms. NEXT-KS produced a nearly minimum viable search tree while RRT* failed to find a path within limited trials.
Figure 7: First row: histograms of results, in terms of success rate, average collision checks, and average cost of the solution paths; Second row: NEXT improvement curves in the 5d experiments. All algorithms are set to use up to 500 samples, except RRT * -10k, which uses 10,000 samples. The value of collision checks and path costs are normalized w.r.t. the performance of RRT * .
Figure 8: The success rate and average path cost of the different planners under varying time limits. Running NEXT for 1 second achieves the same success rate as running BIT* for 50 seconds.
Figure 9: The collision-free path produced by NEXT for robot arm planning. The start and goal configurations have end-effectors in different bins of the shelf.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Path planning is a fundamental problem with many real-world applications, such as robot manipulation and autonomous driving. A simple planning problem within low-dimensional state space can be solved by first discretizing the continuous state space into a grid, and then searching for a path on top of it using graph search algorithms such as A * (Hart et al., 1968). However, due to the curse of dimensionality, these approaches do not scale well with the number of dimensions of the state space. For high-dimensional planning problems, people often resort to sampling-based approaches to avoid explicit discretization. Sampling-based planning algorithms, such as probabilistic roadmaps (PRM) (Kavraki et al., 1996), rapidly-exploring random trees (RRT) (LaValle, 1998), and their variants (Karaman & Frazzoli, 2011) incrementally build an implicit representation of the state space using probing samples. These generic algorithms typically employ a uniform sampler which does not make use of the structures of the problem. Therefore they may require lots of samples to obtain a feasible solution for complicated problems. To improve the sample efficiency, heuristic biased samplers, such as Gaussian sampler (Boor et al., 1999), bridge test (Hsu et al., 2003) and reachability-guided sampler (Shkolnik et al., 2009) have been proposed. All these sampling heuristics are designed manually to address specific structural properties, which may or may not be valid for a new problem, and may lead to even worse performance compared to the uniform proposal. Online adaptation in path planning has also been investigated for improving sample efficiency in current planning problem. Specifically, Hsu et al. (2005) exploits online algorithms to dynamically adapts the mixture weights of several manually designed biased samplers. Burns & Brock (2005a;b) fit a model for the planning environment incrementally and use the model for planning. Yee et al. (2016) mimics the Monte-Carlos tree search (MCTS) for problems with continuous state and action spaces. These algorithms treat each planning problem independently, and the collected data from previous experiences and built model will be simply discarded when solving a new problem. However, in practice, similar planning problems may be solved again and again, where the problems are different but sharing common structures. For instance, grabbing a coffee cup on a table at different time are different problems, since the layout of paper and pens, the position and orientation of coffee cups may be different every time; however, all these problems show common structures of handling similar objects which are placed in similar fashions. Intuitively, if the common characteristics across problems can be learned via some shared latent representation, a planner based on such representation can then be transferred to new problems with improved sample efficiency.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 Several methods have been proposed recently to learn from past planning experiences to conduct more efficient and generalizable planning for future problems. These works are limited in one way or the other. Zucker et al. (2008); Zhang et al. (2018); Huh & Lee (2018) treat the sampler in the sampling-based planner as a stochastic policy to be learned and apply policy gradient or TD-algorithm to improve the policy. Finney et al. (2007); Bowen & Alterovitz (2014); Ye & Alterovitz (2017); Ichter et al. (2018); Kim et al. (2018); Kuo et al. (2018) apply imitation learning based on the collected demonstrations to bias for better sampler via variants of probabilistic models, e.g., (mixture of) Gaussians, conditional VAE, GAN, HMM and RNN. However, many of these approaches either rely on specially designed local features or assume the problems are indexed by special parameters, which limits the generalization ability. Deep representation learning provides a promising direction to extract the common structure among the planning problems, and thus mitigate such limitation on hand- designed features. However, existing work, e.g., motion planning networks (Qureshi et al., 2019), value iteration networks (VIN) (Tamar et al., 2016), and gated path planning networks (GPPN) (Lee et al., 2018), either apply off-the-shelf MLP architecture ignoring special structures in planning problems or can only deal with discrete state and action spaces in low-dimensional settings. In this paper, we present Neural EXploration-EXploitation Tree (NEXT), a meta neural path plan- ning algorithm for high-dimensional continuous state space problems. The core contribution is a novel attention-based neural architecture that is capable of learning generalizable problem struc- tures from previous experiences and produce promising search directions with automatic online exploration-exploitation balance adaption. Compared to existing learning-based planners, • NEXT is more generic. We propose an architecture that can embed high dimensional continuous state spaces into low dimensional discrete spaces, on which a neural planning module is used to extract planning representation. These module will be learned end-to-end. • NEXT balances exploration-exploitation trade-off. We integrate the learned neural prior into an upper confidence bound (UCB) style algorithm to achieve an online balance between exploration and exploitation when solving a new problem. Empirically, we show that NEXT can exploit past experiences to reduce the number of required samples drastically for solving new planning problems, and significantly outperforms previous state-of-the-arts on several benchmark tasks.

Section Title: RELATED WORKS
  RELATED WORKS Designing non-uniform sampling strategies for random search to improve the planning efficiency has been considered as we discussed above. Besides the mentioned algorithms, there are other works along this line, including informed RRT * (Gammell et al., 2014) and batch informed Trees (BIT*) (Gammell et al., 2015) as the representative work. Randomized A * (Diankov & Kuffner, 2007) and sampling-based A * (Persson & Sharf, 2014) expand the search tree with hand-designed heuristics. These methods incorporate the human prior knowledge via hard-coded rules, which is fixed and unable to adapt to problems, and thus, may not universally applicable. Choudhury et al. (2018); Song et al. (2018) attempt to learn search heuristics. However, both methods are restricted to planning on discrete domains. Meanwhile, the latter one always employs an unnecessary hierarchical structure for path planning, which leads to inferior sample efficiency and extra computation. The online exploration-exploitation trade-off is also an important issue in planning. For instance, Rickert et al. (2009) constructs a potential field sampler and tuned the sampler variance based on collision rate for the trade-off heuristically. Paxton et al. (2017) separates the action space into high-level discrete options and low-level continuous actions, and only considered the trade-off at the discrete option level, ignoring the exploration-exploitation in the fine action space. These existing works address the trade-off in an ad-hoc way, which may be inferior for the balance. There have been sevearl non-learning-based planning methods that can also leverage experi- ences (Kavraki et al., 1996; Phillips et al., 2012) by utilizing search graphs created in previous problems. However, they are designed for largely fixed obstacles and cannot be generalized to unseen tasks from the same planning problems distribution.

Section Title: SETTINGS FOR LEARNING TO PLAN
  SETTINGS FOR LEARNING TO PLAN Let S ⊆ R q be the state space of the problem, e.g., all the configurations of a robot and its base location in the workspace, S obs S be the obstacles set, S f ree := S \ S obs be the free space, s init ∈ S f ree be the initial state and S goal S f ree be the goal region. Then the space of all collision- free paths can be defined as a continuous function Ξ := {ξ (·) : [0, 1] → S f ree } . Let c (·) : Ξ → R Published as a conference paper at ICLR 2020 be the cost functional over a path. The optimal path planning problem is to find the optimal path in terms of cost c(·) from start s init to goal S goal in free space S f ree , i.e., Traditionally (Karaman & Frazzoli, 2011), the planner has direct access to (s init , S goal , c (·)) and the workspace map (Ichter et al., 2018; Tamar et al., 2016; Lee et al., 2018), map(·) : R 2 or R 3 → {0, 1}, (0: free spaces and 1: obstacles). Since S f ree often has a very irregular geometry (illustrated in Figure 10 in Appendix A), it is usually represented via a collision detection module which is able to detect the obstacles in a path segment. For the same reason, the feasible paths in Ξ are hard to be described in parametric forms, and thus, the nonparametric ξ, such as a sequence of interconnected path segments [s 0 , s 1 ], [s 1 , s 2 ], . . . , [s T −1 , s T ] ⊂ S with ξ(0) = s 0 = s init and ξ(1) = s T , is used with an additive cost Assuming given the planning problems {U i := (s init , S goal , S, S f ree , map, c (·))} N i=1 sampled from some distribution U, we are interested in learning an algorithm alg (·), which can produce the (nearly)-optimal path efficiently from the observed planning problems. Formally, the learning to plan is defined as alg * (·) = argmin alg∈A E U ∈U [ (alg(U ))] , (2) where A denotes the planning algorithm family, and (·) denotes some loss function which evaluates the quality of the generated path and the efficiency of the alg (·), e.g., size of the search tree. We elab- orate each component in Eq (2) in the following sections. We first introduce the tree-based sampling algorithm template in Section 3, upon which we instantiate the alg (·) via a novel attention-based neural parametrization in Section 4.2 with exploration-exploitation balance mechanism in Section 4.1. We design the -loss function and the meta learning algorithm in Section 4.3. Composing every component together, we obtain the neural exploration-exploitation trees (NEXT) which achieves outstanding performances in Section 5. The sampling-based planners are more practi- cal and become dominant for high-dimensional path planning problems (Elbanhawi & Simic, 2014). We describe a unifying view for many existing tree-based sampling algorithms (TSA), which we will also base our algorithm upon. More specifically, this family of algorithms maintain a search tree T rooted at the initial point s init and connecting all sampled points V in the configuration space with edge set E. The tree will be expanded by incorporating more sampled states until some leaf reaches S goal . Then, a feasible solution for the path planning Published as a conference paper at ICLR 2020 problem will be extracted based on the tree T . The template of tree-based sampling algorithms is summarized in Algorithm 1 and illustrated in Fig. 1(c). A key component of the algorithm is the Expand operator, which generates the next exploration point s new and its parent s parent ∈ V. To ensure the feasibility of the solution, the s new must be reachable from T , i.e., [s parent , s new ] is collision-free, which is checked by a collision detection function. As we will discuss in Appendix B, by instantiating different Expand operators, we will arrive at many existing algorithms, such as RRT (LaValle, 1998) and EST (Hsu et al., 1997; Phillips et al., 2004). One major limitation of existing TSAs is that they solve each problem independently from scratch and ignore past planning experiences in similar environments. We introduce the neural components into TSA template to form the learnable planning algorithm family A, which can explicitly take advantages of the past successful experiences to bias the Expand towards more promising regions.

Section Title: NEURAL EXPLORATION-EXPLOITATION TREES
  NEURAL EXPLORATION-EXPLOITATION TREES Based on the TSA framework, we introduce a learnable neural based Expand operator, which can balance between exploration and exploitation, to instantiate A in Eq (2). With the self-improving training, we obtain the meta NEXT algorithm illustrated in  Figure 1 .

Section Title: GUIDED PROGRESSIVE EXPANSION
  GUIDED PROGRESSIVE EXPANSION We start with our design of the Expand. We assume having an estimated value functionṼ * (s|U ), which stands for the optimal cost from s to target in plan- ning problem U , and a policyπ * (s |s, U ), which generates the promising action s from state s. The concrete parametriza- tion ofṼ * andπ * will be explained in Section 4.2 and learned in Section 4.3. We will use these functions to construct the learnable Expand with explicit exploration-exploitation balancing. The Expand operator will expand the current search tree T by a new neighboring state s new around T . We design the expansion as a two-step procedure: (i) select a state s parent from existing tree T ; (ii) expand a state s new in the neighborhood of s parent . More specifically, Selecting s parent from T in step (i). Consider the negative value function −Ṽ * (s|U ) as the rewards r (s), step (i) shares some similarity with the multi-armed bandit problem by viewing existing nodes s ∈ V as arms. However, the vanilla UCB algorithm is not directly applicable, since the number of states is increasing as the algorithm proceeds and the value of these adjacent states are naturally correlated. We address this challenge by modeling the correlation explicitly as in contextual bandits. Specifically, we parametrize the UCB of the reward function as φ (s), and select a node from T according to φ(s) s parent = argmax s∈V φ(s) :=r t (s) + λσ t (s) , (3) wherer t and σ t denote the average reward and variance estimator after t-calls to Expand. Denote the sequence of t selected tree nodes so far as S t = {s 1 parent , . . . , s t parent }, then we can use kernel smoothing estimator forr t (s) = s ∈S t k(s ,s)r(s ) s ∈S t k(s ,s) and σ t (s) = log s ∈S t w(s ) w(s) where k(s , s) is a kernel function and w(s) = s ∈St k(s , s). Other parametrizations ofr t and σ t are also possible, such as Gaussian Process parametrization in Appendix C. The average reward exploits more promising states, while the variance promotes exploration towards less frequently visited states; and the exploration versus exploitation is balanced by a tunable weight λ > 0. Expanding a reachable s new in step (ii). Given the selected s parent , we consider expanding a reachable state in the neighborhood s parent as an infinite-armed bandit problem. Although one can first samples k arms uniformly from a neighborhood around s parent and runs a UCB algorithm on the randomly generated finite arms (Wang et al., 2009), such uniform sampler ignores problem structures, and will lead to unnecessary samples. Instead we will employ a policyπ * (s |s, U ) 1 for guidance when generating the candidates. The final choice for next move will be selected from these candidates with max φ (s) defined in (3). As explained in more details in Section 4.2,π * will be Published as a conference paper at ICLR 2020 = , , 1 , ⋯ , ∈ ℝ +2 ( ) ∈ ℝ 3 ( ) ∈ ℝ 3 Original Planning Space Latent Planning Space trained to mimic previous successful planning experiences across different problems, that is, biasing the sampling towards the states with higher successful probability. With these detailed step (i) and (ii), we obtain NEXT :: Expand in Algorithm 2 (illustrated in Fig- ure 1(b) and (c)). Plugging it into the TSA in 1, we construct alg (·) ∈ A which will be learned. The guided progressive expansion bears similarity to MCTS but deals with high dimensional continu- ous spaces. Moreover, the essential difference lies in the way to select state in T for expansion: the MCTS only expands the leaf states in T due to the hierarchical assumption, limiting the exploration ability and incurring extra unnecessary UCB sampling for internal traversal in T ; while the proposed operation enables expansion from each visited state, particularly suitable for path planning problems.

Section Title: NEURAL ARCHITECTURE FOR VALUE FUNCTION AND EXPANSION POLICY
  NEURAL ARCHITECTURE FOR VALUE FUNCTION AND EXPANSION POLICY In this section, we will introduce our neural architectures forṼ * (s|U ) andπ * (s |s, U ) used in NEXT :: Expand. The proposed neural architectures can be understood as first embedding the state and problem into a discrete latent representation via an attention-based module in Section 4.2.1, upon which the neuralized value iteration, introduced in Section 4.2.2, is performed to extract features for definingṼ * (s|U ) andπ * (s |s, U ), as illustrated in  Figure 2 .

Section Title: CONFIGURATION SPACE EMBEDDING
  CONFIGURATION SPACE EMBEDDING Our network for embedding high-dimension configuration space into a latent representation is designed based on an attention mechanism. More specifically, let s w denote the workspace in state and s h denote the remaining dimensions of the state, i.e. s = (s w , s h ). s w and s h will be embedded by different sub-neural networks and combined for the final representation, as illustrated in  Figure 3 . For simplicity of exposition, we will focus on the 2d workspace, i.e., s w ∈ R 2 . However, our method applies to 3d workspace as well. • Spatial attention. The workspace information s w will be embedded as µ w (s w ) ∈ R d×d , d is a hyperparameter related to map (see remark below). The spatial embedding module (upper part in  Figure 3 ) is composed of k w convolution layers, i.e., • Configuration attention. The remaining configuration state information will be embedded as µ h (s h ) through k h fully-connected layers (bottom-left part in  Figure 3 ), i.e.   Figure 3 ). Intuitively, one can think of d a as the level of the learned discretization of the configuration space s h , and the entries in µ softly assign the actual state s to these discretized locations. θ := ({θ w i } kw−1 i=0 , {θ h i , b i } k h −1 i=0 ) are the parameters to be learned. Remark (different map size): To process map using convolution neural networks, we resize it to a d × d image, with the same size as the spatial attention in Eq (4), where d is a hyperparameter.

Section Title: NEURAL VALUE ITERATION
  NEURAL VALUE ITERATION We then apply neuralized value iteration on top of the configuration space embedding to extract further planning features (Tamar et al., 2016; Lee et al., 2018). Specifically, we first produce the embedding µ θ (s goal ) of the center of the goal region s goal . We execute T steps of neuralized Bellman updates (planning module in  Figure 4 ) in the embedding space, ν *(t) = min W 1 ⊕ ν *(t−1) ,R , with (ν *(0) ,R) = σ (W 0 ⊕ [µ θ (s goal ), map]) , and obtain ν *(T ) ∈ R d×d×da×p . Both W 0 and W 1 are 3d convolution kernels, min implements the pooling across channels. Accordingly, ν *(T ) now can be understood as a latent representation of the value functionṼ * (·) in learned embedding space for the problem U with s goal in map. To define the value function for particular state s, i.e.,Ṽ * (s|U ), from the latent representation ν *(T ) , we first construct another attention model between the embedding of state s using µ θ (s) and ν *(T ) , i.e., ψ (s) k = ijl ν *(T ) ijlk · µ θ (s) ijl , for k = 1, . . . , p. Finally we definẽ V * (s|U ) = h W2 (ψ(s)) , andπ * (s |s, U ) = N (h W3 (ψ(s)) , σ 2 ) (5) where h W2 and h W3 are fully connected dense layers with parameters W 2 and W 3 respectively, and N (h W3 (ψ(s)) , σ 2 ) is a Gaussian distribution with variance σ 2 . Note that we also parametrize the policyπ * (s |s, U ) using the embedding ν *(T ) , since the policy is connected to the value function via π * (s |s, U ) = argmin s ∈S c ([s, s ]) + V * (s |U ). It should also be emphasized that in our parametrization, the calculation of ν *(T ) only relies on the µ θ (s goal ), which can be reused for evaluatingṼ * (s|U ) andπ * (s |s, U ) over different s, saving computational resources. Using this trick the algorithm runs 10×-100× faster empirically. The overall model architecture in alg (·) is illustrated in  Figure 4 . The parameters W = (W 0 , W 1 , W 2 , W 3 , θ) will be learned together by our meta self-improving learning. For the de- tails of the parameterization and the size of convolution kernels in our implementation, please refer to Figure 12 in Appendix D.

Section Title: META SELF-IMPROVING LEARNING
  META SELF-IMPROVING LEARNING The learning of the planner alg (·) reduces to learning the parameters inṼ * (s|U ) andπ * (s |s, U ) and is carried out while planning experiences accumulate. We do not have an explicit training and testing phase separation. Particularly, we use a mixture of RRT :: Expand and NEXT :: Expand with probability and 1 − , respectively, inside the TSA framework in Algorithm 1. The RRT * postprocessing step is used in the template. The is set to be 1 initially since {Ṽ * ,π * } are not well-trained, and thus, the algorithm behaves like RRT * . As the training proceeds, we anneal gradually as the sampler becomes more efficient. The dataset D n = {(T j , U j )} n j=1 for the n-th training epoch is collected from the previous successful planning experiences across multiple random problems. We fix the size of dataset and update D in the same way as experience reply buffer (Lin, 1992; Schaul et al., 2015). For an experience (T , U ) ∈ D n , we can reconstruct the successful path {s i } m i=1 from the search tree T (m is the number of segments), and the value of each state s i in the path will be the sum of cost to the goal region, i.e., y i := m−1 l=i c([s l , s l+1 ]). We learn {Ṽ * ,π * } by optimizing objective

Section Title: Algorithm 3: Meta Self-Improving Learning
  Algorithm 3: Meta Self-Improving Learning The loss (6) pushes theṼ * andπ * to chase the successful trajectories, providing effective guidance in alg (·) for searching, and therefore leading to efficient searching procedure with less sample complexity and better solution. On one hand, the value function and policy esti- mation {Ṽ * ,π * } is improved based upon the successful outcomes from NEXT itself on pre- vious problems. On the other hand, the updated {Ṽ * ,π * } will be applied in the next epoch to improve the performance. Therefore, the train- ing is named as Meta Self-Improving Learn- ing (MSIL). Since all the trajectories we col- lected for learning are feasible, the reachability of the proposed samples is enforced implicitly via imitating these successful paths. By putting every components together into the learning to plan framework in Eq (2), the over- all procedure is summarized in Algorithm 3 and illustrated in  Figure 1 .

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we evaluate the proposed NEXT empirically on different planning tasks in a variety of environments. Comparing to the existing planning algorithms, NEXT achieves the state-of-the-art performances, in terms of both success rate and the quality of the found solutions. We further demonstrate the power of the proposed two components by the corresponding ablation study. We also include a case study on a real-world robot arm control problem at the end of the section.

Section Title: EXPERIMENT SETUP
  EXPERIMENT SETUP

Section Title: Benchmark environments
  Benchmark environments We designed four benchmark tasks to demonstrate the effectiveness of our algorithm for high-dimensional planning. The first three involve planning in a 2d workspace with a 2 DoF (degrees of freedom) point robot, a 3 DoF stick robot and a 5 DoF snake robot, respectively. The last one involves planning a 7 DoF spacecraft in a 3d workspace. For all problems in each benchmark task, the workspace maps were randomly generated from a fixed distribution; the initial and goal states were sampled uniformly randomly in the free space; the cost function c (·) was set as the sum of the Euclidean path length and the control effort penalty of rotating the robot joints.

Section Title: Baselines
  Baselines We compared NEXT with RRT * (Karaman & Frazzoli, 2011), BIT * (Gammell et al., 2015), CVAE-plan (Ichter et al., 2018), Reinforce-plan (Zhang et al., 2018), and an improved Published as a conference paper at ICLR 2020 (a) RRT* (w/o rewiring) (b) NEXT-KS search tree (c) NEXT-GP search tree (d) learnedṼ * andπ * version of GPPN (Lee et al., 2018) in terms of both planning time and solution quality. RRT * and BIT * are two widely used effective instances of TSA in Algorithm 1. In our experiments, we equipped RRT * with the goal biasing heuristic to improve its performance. BIT * adopts the informed search strategy (Gammell et al., 2015) to accelerate planning. CVAE-plan and Reinforce-plan are two learning-enhanced TSA planners proposed recently. CVAE-plan learns a conditional VAE as the sampler (Sohn et al., 2015), which will be trained by near-optimal paths produced by RRT * . Reinforce-plan learns to do rejection sampling with policy gradient methods. For the improved GPPN, we combined its architecture for map with a fully-connected MLP for the rest state, such that it can be applied to high-dimensional continuous spaces. Please refer to Appendix E for more details.

Section Title: Settings
  Settings For each task, we randomly generated 3000 different problems from the same distribution without duplicated maps. We trained all learning-based baselines using the first 2000 problems, and reserved the rest for testing. The parameters for RRT * and BIT * are also tuned using the first 2000 problems. For NEXT, we let it improve itself using MSIL over the first 2000 problems. In this period, for every 200 problems, we updated its parameters and annealed once.

Section Title: RESULTS AND ANALYSIS
  RESULTS AND ANALYSIS

Section Title: Comparison results
  Comparison results Examples of all four environments are illustrated in Appendix F.1 and  Figure 6 , where NEXT finds high-quality solutions as shown. We also illustrated the comparison of the search trees on two 2d and 7d planning tasks between NEXT and RRT* in  Figure 5 (a)-(c)  and  Figure 6 (b)  and (c). Obviously, the proposed NEXT algorithm explores with guidance and achieves better quality solutions with fewer samples, while the RRT* expands randomly which may fail to find a solution. The learnedṼ * andπ * in the 2d task are also shown in Figure 5(d). As we can see, they are consistent with our expectation, towards the ultimate target in the map. For more search tree comparisons for all four experiments, please check Figure 18, 19, 20 and 21 in Appendix F. To systematically evaluate the algorithms, we recorded the cost of time (measured by the number of collision checks used) to find a collision-free path, the success rate within time limits, and the cost of the solution path for each run. The results of the reserved 1000 test problems of each environment are shown in the top row of  Figure 7 . We set the maximal number of samples as 500 for all algorithms. Both the kernel smoothing (NEXT-KS) and the Gaussian process (NEXT-GP) version of NEXT achieves the state-of-the-art performances, under all three criteria in all test environments. Although the BIT* utilizes the heuristic particularly suitable for 3d maze in 7d task and performs quite well, the NEXT algorithm still outperform every competitor by a large margin, no matter learning-based or prefixed heuristic planner, demonstrating the advantages of the proposed NEXT algorithm.

Section Title: Self-improving
  Self-improving We plot the performance improvement curves of our algorithms on the 5d planning task in the bottom row of  Figure 7 . For comparison, we also plot the performance of RRT * . At the beginning phase of self-improving, our algorithms are comparable to RRT * . They then gradually learn from previous experiences and improve themselves as they see more problems and better solutions. In the end, NEXT-KS is able to match the performance of RRT * -10k using only one-twentieth of its samples, while the competitors perform consistently without any improvements. Due to the space limits, we put improvement curves on other environments in Figure 17 and the quantitative evaluation in Table 1, 2, and 3 in Appendix F. Please refer to the details there.

Section Title: ABLATION STUDIES
  ABLATION STUDIES

Section Title: Ablation study I: guided progressive expansion
  Ablation study I: guided progressive expansion To demonstrate the power of NEXT :: Expand, we replace it with breadth-first search (BFS) (Kim et al., 2018), another expanding strategy, while keeping other components the same. Specifically, BFS uses a search queue in planning. It repeatedly pops a state s out from the search queue, samples k states from π (·|s), and pushes all generated samples and state s back to the queue, until the goal is reached. For fairness, we use the learned sampling policy π (s |s, U ) by NEXT-KS in BFS. As shown in  Figure 7 , BFS obtained worse paths with a much larger number of collision checks and far lower success rate, which justifies the importance of the balance between exploration versus exploitation achieved by the proposed NEXT :: Expand.

Section Title: Ablation study II: neural architecture
  Ablation study II: neural architecture To further demonstrate the benefits of the proposed neural architecture for learning generalizable representations in high-dimension planning problems, We replaced our attention-based neural architecture with an improved GPPN, as explained in Appendix E, for ablation study. We extended the GPPN for continuous space by adding an extra reactive policy network to its final layers. We emphasize the original GPPN is not applicable to the tasks in our experiments. Intuitively, the improved GPPN first produces a 'rough plan' by processing the robot's discretized workspace positions. Then the reactive policy network predicts a continuous action from both the workspace feature and the full configuration state of the robot. We provide more preference to the improved GPPN by training it to imitate the near-optimal paths produced by RRT* in the training problems. During test time it is also combined with both versions of the guided progressive expansion operators. As we can see, both GPPN-KS and GPPN-GP are clearly much worse than NEXT-KS and NEXT-GP, demonstrating the advantage of our proposed attention-based neural architecture in high-dimensional planning tasks.

Section Title: Ablation study III: learning versus heuristic
  Ablation study III: learning versus heuristic The NEXT algorithm in  Figure 5  shows similar behavior as the Dijkstra heuristic, i.e. sampling on the shortest path connecting the start and the goal in workspace. However, in higher dimensional space, the Dijkstra heuristic will fail. To demonstrate that, we replace the policy and value network with Dijkstra heuristic, using the best direction in workspace to guide sampling. NEXT performs much better than Dijkstra in all but the 2d case, in which the workspace happens to be the state space.

Section Title: CASE STUDY: ROBOT ARM CONTROL
  CASE STUDY: ROBOT ARM CONTROL We conduct a real-world case study on controlling robot arms to move objects on a shelf. On this representative real-time task, we demonstrate the advantages of the NEXT in terms of the wall-clock. In each planning task, there is a shelf of multiple levels, with each level horizontally divided into multiple bins. The task is to plan a path from a location in one bin to another, i.e., the end effectors of the start and goal configurations are in different bins. The heights of levels, widths of bins, and the start and goal are randomly drawn from some fixed distribution. Different from previous experiments, the base of the robot is fixed. We consider the BIT* instead of RRT* as the imperfect expert in 3000 training problems. We then evaluate the algorithm on a separated 1000 testing problems. We compare NEXT(- KS) with the highly tuned BIT* and RRT* in OMPL, and also CVAE-plan and Reinforce-plan in  Figure 8 . As seen from the visualization of the found paths in  Figure 9 , this is a very difficult task. Our NEXT outperforms the baselines by a large margin, requiring only 1 second to reach the same success rate as running 50 seconds of BIT*. Due to space limits, we put details of the experiment setups, more results and analysis in Appendix F.4.

Section Title: CONCLUSION
  CONCLUSION In this paper, we propose a self-improving planner, Neural EXploration-EXploitation Trees (NEXT), which can generalize and achieve better performance with experiences accumulated. The algorithm achieves a delicate balance between exploration-exploitation via our carefully designed UCB-type expansion operation. To obtain the generalizable ability across different problems, we proposed a new parametrization for the value function and policy, which captures the Bellman recursive structure in the high-dimensional continuous state and action space. We demonstrate the power of the proposed algorithm by outperforming previous state-of-the-art planners with significant margins on planning problems in a variety of different environments.
  In the path planning setting, we useπ * (s |s, U ) andπ * (a|s, U ) interchangeably as the action is next state.

```
