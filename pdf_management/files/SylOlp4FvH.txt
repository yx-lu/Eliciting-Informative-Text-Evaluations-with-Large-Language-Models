Title:
```
Published as a conference paper at ICLR 2020 V-MPO: ON-POLICY MAXIMUM A POSTERIORI POLICY OPTIMIZATION FOR DISCRETE AND CONTINUOUS CONTROL
```
Abstract:
```
Some of the most successful applications of deep reinforcement learning to chal- lenging domains in discrete and continuous control have used policy gradient methods in the on-policy setting. However, policy gradients can suffer from large variance that may limit performance, and in practice require carefully tuned entropy regularization to prevent policy collapse. As an alternative to policy gradient algo- rithms, we introduce V-MPO, an on-policy adaptation of Maximum a Posteriori Policy Optimization (MPO) that performs policy iteration based on a learned state- value function. We show that V-MPO surpasses previously reported scores for both the Atari-57 and DMLab-30 benchmark suites in the multi-task setting, and does so reliably without importance weighting, entropy regularization, or population-based tuning of hyperparameters. On individual DMLab and Atari levels, the proposed algorithm can achieve scores that are substantially higher than has previously been reported. V-MPO is also applicable to problems with high-dimensional, continuous action spaces, which we demonstrate in the context of learning to control simulated humanoids with 22 degrees of freedom from full state observations and 56 degrees of freedom from pixel observations, as well as example OpenAI Gym tasks where V-MPO achieves substantially higher asymptotic scores than previously reported.
```

Figures/Tables Captions:
```
Figure 1: (a) Actor-learner architecture with a target network, which is used to generate agent experience in the environment and is updated every T target learning steps from the online network. (b) Schematic of the agents, with the policy (θ) and value (φ) networks sharing most of their parameters through a shared input encoder and LSTM [or Transformer-XL (TrXL) for single Atari levels]. The agent also receives the action and reward from the previous step as an input to the LSTM. For DMLab an additional LSTM is used to process simple language instructions.
Figure 2: (a) Multi-task DMLab-30. IMPALA results show 3 runs of 8 agents each; within a run hyperparameters were evolved via PBT. For V-MPO each line represents a set of hyperparameters that are fixed throughout training. The final result of R2D2+ trained for 10B environment steps on individual levels (Kapturowski et al., 2019) is also shown for comparison (orange line). (b) Multi-task Atari-57. In the IMPALA experiment, hyperparameters were evolved with PBT. For V-MPO each of the 24 lines represents a set of hyperparameters that were fixed throughout training, and all runs achieved a higher score than the best IMPALA run. Data for IMPALA ("Pixel-PopArt- IMPALA" for DMLab-30 and "PopArt-IMPALA" for Atari-57) was obtained from the authors of Hessel et al. (2018). Each agent step corresponds to 4 environment frames due to the action repeat.
Figure 3: V-MPO trained on single example levels from DMLab-30, compared to IMPALA and more recent results from R2D2+, the larger, DMLab-specific version of R2D2 (Kapturowski et al., 2019). The IMPALA results include hyperparameter evolution with PBT.
Figure 4: Example levels from Atari. In Breakout, V-MPO achieves the maximum score of 864 in every episode. No reward clipping was applied, and the maximum length of an episode was 30 minutes (108,000 frames). Supplementary video for Ms. Pacman: https://bit.ly/2lWQBy5 (a) (b) (c) (d)
Figure 5: (a) Humanoid "run" from full state (Tassa et al., 2018) and (b) humanoid "gaps" from pixel observations (Merel et al., 2019). Purple curves are the same runs but without parametric KL constraints. Det. eval.: deterministic evaluation. Supplementary video for humanoid gaps: https://bit.ly/2L9KZdS. (c)-(d) Example OpenAI Gym tasks. See also Fig. 11 in the Appendix for Gym Humanoid-V1.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep reinforcement learning (RL) with neural network function approximators has achieved superhu- man performance in several challenging domains ( Mnih et al., 2015 ;  Silver et al., 2016 ; 2018). Some of the most successful recent applications of deep RL to difficult environments such as Dota 2 ( Ope- nAI, 2018a ), Capture the Flag ( Jaderberg et al., 2019 ), Starcraft II ( Vinyals et al., 2019 ), and dexterous object manipulation ( OpenAI, 2018b ) have used policy gradient-based methods such as Proximal Policy Optimization (PPO) ( Schulman et al., 2017 ) and the Importance-Weighted Actor-Learner Architecture (IMPALA) ( Espeholt et al., 2018 ), both in the approximately on-policy setting. Policy gradients, however, can suffer from large variance that may limit performance, especially for high-dimensional action spaces ( Wu et al., 2018 ). In practice, moreover, policy gradient methods typically employ carefully tuned entropy regularization in order to prevent policy collapse. As an alternative to policy gradient-based algorithms, in this work we introduce an approximate policy iteration algorithm that adapts Maximum a Posteriori Policy Optimization (MPO) ( Abdolmaleki et al., 2018a ;b) to the on-policy setting. The modified algorithm, V-MPO, relies on a learned state-value function V (s) instead of the state-action value function used in MPO. Like MPO, rather than directly updating the parameters in the direction of the policy gradient, V-MPO first constructs a target distribution for the policy update subject to a sample-based KL constraint, then calculates the gradient that partially moves the parameters toward that target, again subject to a KL constraint.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 As we are particularly interested in scalable RL algorithms that can be applied to multi-task settings where a single agent must perform a wide variety of tasks, we show for the case of discrete actions that the proposed algorithm surpasses previously reported performance in the multi-task setting for both the Atari-57 ( Bellemare et al., 2012 ) and DMLab-30 ( Beattie et al., 2016 ) benchmark suites, and does so reliably without population-based tuning of hyperparameters ( Jaderberg et al., 2017a ). For a few individual levels in DMLab and Atari we also show that V-MPO can achieve scores that are substantially higher than has previously been reported in the single-task setting, especially in the challenging Ms. Pacman. V-MPO is also applicable to problems with high-dimensional, continuous action spaces. We demon- strate this in the context of learning to control both a 22-dimensional simulated humanoid from full state observations-where V-MPO reliably achieves higher asymptotic performance than previous algorithms-and a 56-dimensional simulated humanoid from pixel observations ( Tassa et al., 2018 ;  Merel et al., 2019 ). In addition, for several OpenAI Gym tasks ( Brockman et al., 2016 ) we show that V-MPO achieves higher asymptotic performance than has previously been reported.

Section Title: BACKGROUND AND SETTING
  BACKGROUND AND SETTING We consider the discounted RL setting, where we seek to optimize a policy π for a Markov Decision Process described by states s, actions a, initial state distribution ρ env 0 (s 0 ), transition probabilities P env (s t+1 |s t , a t ), reward function r(s t , a t ), and discount factor γ ∈ (0, 1). In deep RL, the policy π θ (a t |s t ), which specifies the probability that the agent takes action a t in state s t at time t, is described by a neural network with parameters θ. We consider problems where both the states s and actions a may be discrete or continuous. Two functions play a central role in RL: the state-value function V π (s t ) = E at,st+1,at+1,... In the usual formulation of the RL problem, the goal is to find a policy π that maximizes the expected return given by J(π) = E s0,a0,s1,a1,... ∞ t=0 γ t r(s t , a t ) . In policy gradient algorithms ( Williams, 1992 ;  Sutton et al., 2000 ;  Mnih et al., 2016 ), for example, this objective is directly optimized by estimating the gradient of the expected return. An alternative approach to finding optimal policies derives from research that treats RL as a problem in probabilistic inference, including Maximum a Posteriori Policy Optimization (MPO) ( Levine, 2018 ;  Abdolmaleki et al., 2018a ;b). Here our objective is subtly different, namely, given a suitable criterion for what are good actions to take in a certain state, how do we find a policy that achieves this goal? As was the case for the original MPO algorithm, the following derivation is valid for any such criterion. However, the policy improvement theorem ( Sutton & Barto, 1998 ) tells us that a policy update performed by exact policy iteration, π(s) = arg max a [Q π (s, a) − V π (s)], can improve the policy if there is at least one state-action pair with a positive advantage and nonzero probability of visiting the state. Motivated by this classic result, in this work we specifically choose an exponential function of the advantages A π (s, a) = Q π (s, a) − V π (s).

Section Title: Notation
  Notation In the following we use s,a to indicate both discrete and continuous sums (i.e., integrals) over states s and actions a depending on the setting. A sum with indices only, such as s,a , denotes a sum over all possible states and actions, while s,a∼D , for example, denotes a sum over sample states and actions from a batch of trajectories (the "dataset") D.

Section Title: RELATED WORK
  RELATED WORK V-MPO shares many similarities, and thus relevant related work, with the original MPO algorithm ( Ab- dolmaleki et al., 2018a ;b). In particular, the general idea of using KL constraints to limit the size of policy updates is present in both Trust Region Policy Optimization (TRPO;  Schulman et al., 2015 ) and Proximal Policy Optimization (PPO) ( Schulman et al., 2017 ); we note, however, that this corresponds to the E-step constraint in V-MPO. It is worth noting here the following main differences with MPO, which is conceptually quite similar to V-MPO. MPO is primarily designed to be a sample-efficient off-policy algorithm in which the Published as a conference paper at ICLR 2020 E-step constructs a conditional target distribution q(a|s), which requires a state-action value function Q(s, a) that can evaluate multiple sampled actions for a given state. In contrast, V-MPO is primarily (though not exclusively) designed to be an on-policy algorithm in which the E-step constructs a joint distribution ψ(s, a), and in the absence of a learned Q-function only one action per state is used. In this regard V-MPO can also be compared to Fitted Q-iteration by Advantage Weighted Regression ( Neumann & Peters, 2009 ), which learns a Q-function but uses only one action per state. V-MPO can also be related to Relative Entropy Policy Search (REPS) ( Peters et al., 2008 ). Two distinguishing features of V-MPO from REPS are the introduction of the M-step KL constraint and the use of top-k advantages. Moreover, in REPS the value function is a linear function of a learned feature representation whose parameters are trained by matching the feature distributions under the policy's stationary state distribution. In V-MPO, the nonlinear neural network value function is instead learned directly from n-step returns. Interestingly, previous attempts to use REPS with neural network function approximators reported very poor performance, being particularly prone to local optima ( Duan et al., 2016 ). In contrast, we find that the principles of EM-style policy optimization, when combined with this learned value function and appropriate constraints, can reliably train powerful neural networks, including transformers, for RL tasks. Like V-MPO, Supervised Policy Update (SPU) ( Vuong et al., 2019 ) seeks to exactly solve an optimization problem and fit the parametric policy to this solution. As we argue in Appendix D, however, SPU uses this nonparametric distribution quite differently from V-MPO; as a result, the final algorithm is closer to a policy gradient algorithm such as PPO.

Section Title: METHOD
  METHOD V-MPO is an approximate policy iteration ( Sutton & Barto, 1998 ) algorithm with a specific prescrip- tion for the policy improvement step. In general, policy iteration uses the fact that the true state-value function V π corresponding to policy π can be used to obtain an improved policy π . Thus we can 1. Generate trajectories τ from an old policy π θold (a|s) whose parameters θ old are fixed. To control the amount of data generated by a particular policy, we use a target network which is fixed for T target learning steps (Fig. 1a). 2. Evaluate the policy π θold (a|s) by learning the value function V π θ old (s) from empirical returns and estimating the corresponding advantages A π θ old (s, a) for the actions that were taken (Section 4.1). 3. Based on A π θ old (s, a), estimate an improved policy π θ (a|s) which we call the "online" policy to distinguish it from the fixed target network (Section 4.2). The first two steps are standard, and describing V-MPO's approach to step 3 is the essential contribu- tion of this work. At a high level, our strategy is to first construct a nonparametric target distribution for the policy update, then partially move the parametric policy towards this distribution subject to a KL constraint. We first review policy evaluation (step 2) in Section 4.1, then derive the V-MPO policy improvement (step 3) in Section 4.2. Ultimately, we use gradient descent to optimize a single, relatively simple loss, which is given in Eq. 10 following the derivation. A summary of the full algorithm is also presented in Algorithm 1.

Section Title: POLICY EVALUATION
  POLICY EVALUATION In the present setting, policy evaluation means learning an approximate state-value function V π (s) given a policy π(a|s), which we keep fixed for T target learning steps (i.e., batches of trajectories). We note that the value function corresponding to the target policy is instantiated in the "online" network receiving gradient updates; bootstrapping uses the online value function, as it is the best available estimate of the value function for the target policy. Thus in this section π refers to π θold , while the value function update is performed on the current φ, which may share parameters with the current θ. We fit a parametric value function V π φ (s) with parameters φ by minimizing the squared loss L V (φ) = 1 2|D| st∼D V π φ (s t ) − G (n) t 2 , (1) where G (n) t is the standard n-step target for the value function at state s t at time t ( Sutton & Barto, 1998 ). This return uses the actual rewards in the trajectory and bootstraps from the value function for the rest: for each = t, . . . , t + n − 1 in an unroll, G (n) = t+n−1 k= γ k− r k + γ t+n− V π φ (s t+n ). The advantages, which are the key quantity of interest for the policy improvement step in V-MPO, are then given by A π (s t , a t ) = G (n) t − V π φ (s t ) for each s t , a t in the batch of trajectories.

Section Title: PopArt normalization
  PopArt normalization As we are interested in the multi-task setting where a single agent must learn a large number of tasks with differing reward scales, we used PopArt ( van Hasselt et al., 2016 ;  Hessel et al., 2018 ) for the value function, even when training on a single task. We observed benefits in using PopArt even in the single-task setting, partly due to the fact that we do not tune the relative weighting of the policy evaluation and policy improvement losses despite sharing most parameters for the policy and value networks. Specifically, the value function outputs a separate value for each task in normalized space, which is converted to actual returns by a shift and scaling operation, the statistics of which are learned during training. We used a scale lower bound of 10 −2 , scale upper bound of 10 6 , and learning rate of 10 −4 for the statistics. The lower bound guards against numerical issues when rewards are extremely sparse.

Section Title: Importance-weighting for off-policy data
  Importance-weighting for off-policy data It is possible to importance-weight the samples using V-trace to correct for off-policy data ( Espeholt et al., 2018 ), for example when data is taken from a replay buffer. For simplicity, however, no importance-weighting was used for the experiments presented in this work, which were mostly on-policy.

Section Title: POLICY IMPROVEMENT IN V-MPO
  POLICY IMPROVEMENT IN V-MPO In this section we show how, given the advantage function A π θ old (s, a) for the state-action distribution p θold (s, a) = π θold (a|s)p(s) induced by the old policy π θold (a|s), we can estimate an improved policy π θ (a|s). More formally, let I denote the binary event that the new policy is an improvement (in a sense to be defined below) over the previous policy: I = 1 if the policy is successfully improved and 0 otherwise. Then we would like to find the mode of the posterior distribution over parameters θ conditioned on this event, i.e., we seek the maximum a posteriori (MAP) estimate Published as a conference paper at ICLR 2020 where we have written p(I = 1|θ) as p θ (I = 1) to emphasize the parametric nature of the dependence on θ. We use the well-known identity log p(X) = E ψ(Z) log p(X,Z) ψ(Z) + D KL ψ(Z) p(Z|X) for any latent distribution ψ(Z), where D KL (ψ(Z) p(Z|X)) is the Kullback-Leibler divergence between ψ(Z) and p(Z|X) with respect to Z, and the first term is a lower bound because the KL divergence is always non-negative. Then considering s, a as latent variables, Policy improvement in V-MPO consists of the following two steps which have direct correspondences to the expectation maximization (EM) algorithm ( Neal & Hinton, 1998 ): In the expectation (E) step, we choose the variational distribution ψ(s, a) such that the lower bound on log p θ (I = 1) is as tight as possible, by minimizing the KL term. In the maximization (M) step we then find parameters θ that maximize the corresponding lower bound, together with the prior term in Eq. 2.

Section Title: E-STEP
  E-STEP In the E-step, our goal is to choose the variational distribution ψ(s, a) such that the lower bound on log p θ (I = 1) is as tight as possible, which is the case when the KL term in Eq. 3 is zero. Given the old parameters θ old , this simply leads to ψ(s, a) = p θold (s, a|I = 1), or Intuitively, this solution weights the probability of each state-action pair with its relative improvement probability p θold (I = 1|s, a). We now choose a distribution p θold (I = 1|s, a) that leads to our desired outcome. As we prefer actions that lead to a higher advantage in each state, we suppose that this probability is given by p θold (I = 1|s, a) ∝ exp A π θ old (s, a) η (5) for some temperature η > 0, from which we obtain the equation on the right in Eq. 12. This probability depends on the old parameters θ old and not on the new parameters θ. Meanwhile, the value of η allows us to control the diversity of actions that contribute to the weighting, but at the moment is arbitrary. It turns out, however, that we can tune η as part of the optimization, which is desirable since the optimal value of η changes across iterations. The convex loss that achieves this, Eq. 13, is derived in Appendix A by minimizing the KL term in Eq. 3 subject to a hard constraint on ψ(s, a).

Section Title: Top-k advantages
  Top-k advantages We found that learning improves substantially if we take only the samples corresponding to the highest 50% of advantages in each batch for the E-step, corresponding to the use ofD rather than D in Eqs. 12, 13. Importantly, these must be consistent between the maximum likelihood weights in Eq. 12 and the temperature loss in Eq. 13, since, mathematically, this corresponds to a specific choice of the policy improvement probability in Eq. 5 to only use the top half of the advantages. This is similar to the technique used in the Cross Entropy Method (CEM) ( Mannor et al., 2003 ) and Covariance Matrix Adaptation - Evolutionary Strategy (CMA- ES) ( Hansen et al., 1997 ;  Abdolmaleki et al., 2017 ), and is a special case of the more general feature that any rank-preserving transformation is allowed under this formalism. For example, in Fig. 8 of the Appendix we show an example of an agent trained with uniform weights given to the top-k samples, instead of optimizing the temperature. Other choices are possible, and in future work we will investigate the suitability of different choices for specific applications.

Section Title: Importance weighting for off-policy corrections
  Importance weighting for off-policy corrections As for the value function, importance weights can be used in the policy improvement step to correct for off-policy data. While not used for the experiments presented in this work, details for how to carry out this correction are given in Appendix E.

Section Title: M-STEP: CONSTRAINED SUPERVISED LEARNING OF THE PARAMETRIC POLICY
  M-STEP: CONSTRAINED SUPERVISED LEARNING OF THE PARAMETRIC POLICY In the E-step we found the nonparametric variational state-action distribution ψ(s, a), Eq. 4, that gives the tightest lower bound to p θ (I = 1) in Eq. 3. In the M-step we maximize this lower bound Published as a conference paper at ICLR 2020 together with the prior term log p(θ) with respect to the parameters θ, which effectively leads to a constrained weighted maximum likelihood problem. Thus the introduction of the nonparametric distribution in Eq. 4 separates the RL procedure from the neural network fitting. We would like to find new parameters θ that minimize Note, however, that so far we have worked with the joint state-action distribution ψ(s, a) while we are in fact optimizing for the policy, which is the conditional distribution π θ (a|s). Writing p θ (s, a) = π θ (a|s)p(s) since only the policy is parametrized by θ and dropping terms that are not parametrized by θ, the first term of Eq. 6 is seen to be the weighted maximum likelihood policy loss In the sample-based computation of this loss, we assume that any state-action pairs not in the batch of trajectories have zero weight, leading to the normalization in Eq. 12. As in the original MPO algorithm, a useful prior is to keep the new policy π θ (a|s) close to the old policy π θold (a|s): log p(θ) ≈ −αE s∼p(s) D KL π θold (a|s) π θ (a|s) . While intuitive, we motivate this more formally in Appendix B. It is again more convenient to specify a bound on the KL divergence instead of tuning α directly, so we solve the constrained optimization problem Intuitively, the constraint in the E-step expressed by Eq. 18 in Appendix A for tuning the temperature only constrains the nonparametric distribution; it is the constraint in Eq. 8 that directly limits the change in the parametric policy, in particular for states and actions that were not in the batch of samples and which rely on the generalization capabilities of the neural network function approximator. To make the constrained optimization problem amenable to gradient descent, we use Lagrangian relaxation to write the unconstrained objective as J (θ, α) = L π (θ) + α α − E s∼p(s) D KL π θold (a|s) π θ (a|s) , (9) which we can optimize by following a coordinate-descent strategy, alternating between the opti- mization over θ and α. Since η and α are Lagrange multipliers that must be positive, after each gradient update we project the resulting η and α to a small positive value which we choose to be η min = α min = 10 −8 throughout the results presented below. KL constraints in both the E-step and M-step are generally well satisfied, especially for the E-step since the temperature optimization is convex. Fig. 7 in the Appendix shows an example of how the KL constraints behave in the Atari Seaquest experiment presented below. We note, in particular, that it is desirable for the bounds to not just be satisfied but saturated.

Section Title: FULL LOSS FUNCTION
  FULL LOSS FUNCTION In this section we provide the full loss function used to implement V-MPO, which is perhaps simpler than is suggested by the derivation. Consider a batch of data D consisting of a number of trajectories, with |D| total state-action samples. Each trajectory consists of an unroll of length n of the form τ = (s t , a t , r t+1 ), . . . , (s t+n−1 , a t+n−1 , r t+n ), s t+n including the bootstrapped state s t+n , where r t+1 = r(s t , a t ). The total loss is the sum of a policy evaluation loss and a policy improvement loss, L(φ, θ, η, α) = L V (φ) + L V-MPO (θ, η, α), (10) where φ are the parameters of the value network, θ the parameters of the policy network, and η and α are Lagrange multipliers. In practice, the policy and value networks share most of their parameters in the form of a shared convolutional network (a ResNet) and recurrent LSTM core, and are optimized together (Fig. 1b) ( Mnih et al., 2016 ). We note, however, that the value network parameters φ are considered fixed for the policy improvement loss, and gradients are not propagated.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 The policy evaluation loss for the value function, L V (φ), is the standard regression to n-step returns and is given by Eq. 1 above. The policy improvement loss L V-MPO (θ, η, α) is given by Here the policy loss is the weighted maximum likelihood loss L π (θ) = − s,a∼D ψ(s, a) log π θ (a|s), ψ(s, a) = exp A target (s,a) η s,a∼D exp A target (s,a) η , (12) where the advantages A target (s, a) for the target network policy π θtarget (a|s) are estimated according to the standard method described above. The tilde over the dataset,D, indicates that we take samples corresponding to the top half advantages in the batch of data. The η, or "temperature", loss is We perform the alternating optimization over θ and α while keeping a single loss function by alternately applying a "stop-gradient" to the Lagrange multiplier and KL term. Then the KL constraint, which can be viewed as a form of trust-region loss, is given by L α (θ, α) = 1 |D| s∈D α α − sg D KL π θtarget (a|s) π θ (a|s) + sg[[α]]D KL π θtarget (a|s) π θ (a|s) , (14) where sg[[·]] indicates a stop gradient, i.e., that the enclosed term is assumed constant with respect to all variables. Note that here we use the full batch D, notD. For continuous action spaces parametrized by Gaussian distributions, we use decoupled KL constraints for the M-step in Eq. 14 as in  Abdolmaleki et al. (2018b) ; the precise form is given in Appendix C. We used the Adam optimizer ( Kingma & Ba, 2015 ) with default TensorFlow hyperparameters to optimize the total loss in Eq. 10. In particular, the learning rate was fixed at 10 −4 for all experiments. Use policy π θtarget to act in the environment and collect B trajectories τ of length n. Update θ online , φ online , η, α using Adam to minimize the total loss in Eq. 10. η ← max(η, η min ) α ← max(α, α min ) end for until Fixed number of steps.

Section Title: EXPERIMENTS
  EXPERIMENTS Details on the network architecture and hyperparameters used for each task are given in Appendix F.

Section Title: DISCRETE ACTIONS: DMLAB, ATARI
  DISCRETE ACTIONS: DMLAB, ATARI DMLab. DMLab-30 ( Beattie et al., 2016 ) is a collection of visually rich, partially observable 3D environments played from the first-person point of view. Like IMPALA, for DMLab we used pixel control as an auxiliary loss for representation learning ( Jaderberg et al., 2017b ;  Hessel et al., 2018 ). However, we did not employ the optimistic asymmetric reward scaling used by previous IMPALA (a) Multi-task DMLab-30. (b) Multi-task Atari-57. experiments to aid exploration on a subset of the DMLab levels, by weighting positive rewards more than negative rewards ( Espeholt et al., 2018 ;  Hessel et al., 2018 ;  Kapturowski et al., 2019 ). Unlike in  Hessel et al. (2018)  we also did not use population-based training (PBT) ( Jaderberg et al., 2017a ). Additional details for the settings used in DMLab can be found in Table 5 of the Appendix. Fig. 2a shows the results for multi-task DMLab-30, comparing the V-MPO learning curves to data obtained from  Hessel et al. (2018)  for the PopArt IMPALA agent with pixel control. We note that the result for V-MPO at 10B environment frames across all levels matches the result for the Recurrent Replay Distributed DQN (R2D2) agent ( Kapturowski et al., 2019 ) trained on individual levels for 10B environment steps per level.  Fig. 3  shows example individual levels in DMLab where V-MPO achieves scores that are substantially higher than has previously been reported, for both R2D2 and IMPALA. The pixel-control IMPALA agents shown here were carefully tuned for DMLab and are similar to the "experts" used in  Schmitt et al. (2018) ; in all cases these results match or exceed previously published results for IMPALA ( Espeholt et al., 2018 ;  Kapturowski et al., 2019 ).

Section Title: Atari
  Atari The Atari Learning Environment (ALE) ( Bellemare et al., 2012 ) is a collection of 57 Atari 2600 games that has served as an important benchmark for recent deep RL methods. We used the standard preprocessing scheme and a maximum episode length of 30 minutes (108,000 frames), see Table 6 in the Appendix. For the multi-task setting we followed  Hessel et al. (2018)  in setting the discount to zero on loss of life; for the example single tasks we did not employ this trick, since it can prevent the agent from achieving the highest score possible by sacrificing lives. Similarly, while in the multi-task setting we followed previous work in clipping the maximum reward to 1.0, no such clipping was applied in the single-task setting in order to preserve the original reward structure. Additional details for the settings used in Atari can be found in Table 6 in the Appendix. Fig. 2b shows the results for multi-task Atari-57, demonstrating that it is possible for a single agent to achieve "superhuman" median performance on Atari-57 in approximately 4 billion (∼70 million per level) environment frames. Again, while we did not employ PBT in order to demonstrate that individual V-MPO runs can exceed the performance of a population of IMPALA agents, Fig. 6 shows that with population-based tuning of hyperparameters even higher performance is possible. We also compare the performance of V-MPO on a few individual Atari levels to R2D2 ( Kapturowski et al., 2019 ), which previously achieved some of the highest scores reported for Atari. Again, V- MPO can match or exceed previously reported scores while requiring fewer interactions with the environment. In Ms. Pacman, the final performance approaches 300,000 with a 30-minute timeout (and the maximum 1M without). Inspired by the argument in  Kapturowski et al. (2019)  that in a fully observable environment LSTMs enable the agent to utilize more useful representations than is available in the immediate observation, for the single-task setting we used a Transformer-XL (TrXL) ( Dai et al., 2019 ) to replace the LSTM core. Unlike previous work for single Atari levels, we did not employ any reward clipping ( Mnih et al., 2015 ;  Espeholt et al., 2018 ) or nonlinear value function rescaling ( Kapturowski et al., 2019 ).

Section Title: CONTINUOUS CONTROL
  CONTINUOUS CONTROL To demonstrate V-MPO's effectiveness in high-dimensional, continuous action spaces, here we present examples of learning to control both a simulated humanoid with 22 degrees of freedom from full state observations and one with 56 degrees of freedom from pixel observations ( Tassa et al., 2018 ;  Merel et al., 2019 ). As shown in Fig. 5a, for the 22-dimensional humanoid V-MPO reliably achieves higher asymptotic returns than has previously been reported, including for Deep Deterministic Policy Gradients (DDPG) ( Lillicrap et al., 2015 ), Stochastic Value Gradients (SVG) ( Heess et al., 2015 ), and MPO. These algorithms are far more sample-efficient but reach a lower final performance. In the "gaps" task the 56-dimensional humanoid must run forward to match a target velocity of 4 m/s and jump over the gaps between platforms by learning to actuate joints with position-control ( Merel et al., 2019 ). Previously, only an agent operating in the space of pre-learned motor primitives was able to solve the task from pixel observations ( Merel et al., 2018 ; 2019); here we show that V-MPO can learn a challenging visuomotor task from scratch (Fig. 5b). For this task we also demonstrate the importance of the parametric KL constraint, without which the agent learns poorly. In Figs. 5c-d we also show that V-MPO achieves the highest asymptotic performance reported for two OpenAI Gym tasks ( Brockman et al., 2016 ). Again, MPO and Stochastic Actor-Critic ( Haarnoja et al., 2018 ) are far more sample-efficient but reach a lower final performance. These experiments are presented to demonstrate the existence of higher-return solutions than have previously been reported, and an algorithm, V-MPO, that can reliably converge to these solutions. However, in the future we desire algorithms that can do so while using fewer interactions with the environment.

Section Title: CONCLUSION
  CONCLUSION In this work we have introduced a scalable on-policy deep reinforcement learning algorithm, V-MPO, that is applicable to both discrete and continuous control domains. For the results presented in this work neither importance weighting nor entropy regularization was used; moreover, since the size of neural network parameter updates is limited by KL constraints, we were also able to use the same learning rate for all experiments. This suggests that a scalable, performant RL algorithm may not require some of the tricks that have been developed over the past several years. Interestingly, both the original MPO algorithm for replay-based off-policy learning ( Abdolmaleki et al., 2018a ;b) and V-MPO for on-policy learning are derived from similar principles, providing evidence for the benefits of this approach as an alternative to popular policy gradient-based methods.

```
