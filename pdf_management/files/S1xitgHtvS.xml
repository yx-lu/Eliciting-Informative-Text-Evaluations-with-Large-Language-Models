<article article-type="research-article"><front><article-meta><title-group><article-title>Published as a conference paper at ICLR 2020 MAKING SENSE OF REINFORCEMENT LEARNING AND PROBABILISTIC INFERENCE</article-title></title-group><contrib-group content-type="author"><contrib contrib-type="person"><name><surname>O'Donoghue</surname><given-names>Brendan</given-names></name></contrib><contrib contrib-type="person"><name><surname>Osband</surname><given-names>Ian</given-names></name></contrib><contrib contrib-type="person"><name><surname>Ionescu</surname><given-names>Catalin</given-names></name></contrib></contrib-group><abstract><p>Reinforcement learning (RL) combines a control problem with statistical estima- tion: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts 'RL as inference' and suggests a partic- ular framework to generalize the RL problem as probabilistic inference. Our pa- per surfaces a key shortcoming in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: The exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must re- sort to approximation. We demonstrate that the popular 'RL as inference' approx- imation can perform poorly in even very basic problems. However, we show that with a small modification the framework does yield algorithms that can provably perform well, and we show that the resulting algorithm is equivalent to the recently proposed K-learning, which we further connect with Thompson sampling.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Probabilistic inference is a procedure of making sense of uncertain data using Bayes' rule. The optimal control problem is to take actions in a known system in order to maximize the cumulative rewards through time. Probabilistic graphical models (PGMs) offer a coherent and flexible language to specify causal relationships, for which a rich literature of learning and inference techniques have developed (<xref ref-type="bibr" rid="b0">Koller &amp; Friedman, 2009</xref>). Although control dynamics might also be encoded as a PGM, the relationship between action planning and probabilistic inference is not immediately clear. For inference, it is typically enough to specify the system and pose the question, and the objectives for learning emerge automatically. In control, the system and objectives are known, but the question of how to approach a solution may remain extremely complex (<xref ref-type="bibr" rid="b2">Bertsekas, 2005</xref>).</p><p>Perhaps surprisingly, there is a deep sense in which inference and control can represent a dual view of the same problem. This relationship is most clearly stated in the case of linear quadratic systems, where the Ricatti equations relate the optimal control policy in terms of the system dynamics (<xref ref-type="bibr" rid="b6">Welch et al., 1995</xref>). In fact, this connection extends to a wide range of systems, where control tasks can be related to a dual inference problem through rewards as exponentiated probabilities in a distinct, but coupled, PGM (<xref ref-type="bibr" rid="b0">Todorov, 2007</xref>; 2008). A great benefit of this connection is that it can allow the tools of inference to make progress in control problems, and vice-versa. In both cases the connections provide new insights, inspire new algorithms and enrich our understanding (<xref ref-type="bibr" rid="b0">Toussaint &amp; Storkey, 2006</xref>; <xref ref-type="bibr" rid="b6">Ziebart et al., 2008</xref>; <xref ref-type="bibr" rid="b6">Kappen et al., 2012</xref>).</p><p>Reinforcement learning (RL) is the problem of learning to control an unknown system (<xref ref-type="bibr" rid="b0">Sutton &amp; Barto, 2018</xref>). Like the control setting, an RL agent should take actions to maximize its cumulative rewards through time. Like the inference problem, the agent is initially uncertain of the system dynamics, but can learn through the transitions it observes. This leads to a fundamental tradeoff: The agent may be able to improve its understanding through exploring poorly-understood states and actions, but it may be able to attain higher immediate reward through exploiting its existing knowledge (<xref ref-type="bibr" rid="b0">Kearns &amp; Singh, 2002</xref>). In many ways, RL combines control and inference into a general framework for decision making under uncertainty. Although there has been ongoing research Published as a conference paper at ICLR 2020 in this area for many decades, there has been a recent explosion of interest as RL techniques have made high-profile breakthroughs in grand challenges of artificial intelligence research (<xref ref-type="bibr" rid="b6">Mnih et al., 2013</xref>; <xref ref-type="bibr" rid="b12">Silver et al., 2016</xref>).</p><p>A popular line of research has sought to cast 'RL as inference', mirroring the dual relationship for control in known systems. This approach is most clearly stated in the tutorial and review of <xref ref-type="bibr" rid="b7">Levine (2018)</xref>, and provides a key reference for research in this field. It suggests that a generalization of the RL problem can be cast as probabilistic inference through inference over exponentiated rewards, in a continuation of previous work in optimal control (<xref ref-type="bibr" rid="b0">Todorov, 2009</xref>). This perspective promises several benefits: A probabilistic perspective on rewards, the ability to apply powerful inference algorithms to solve RL problems and a natural exploration strategy. In this paper we will outline an important way in which this perspective is incomplete. This shortcoming ultimately results in algorithms that can perform poorly in even very simple decision problems. Importantly, these are not simply technical issues that show up in some edge cases, but fundamental failures of this approach that arise in even the most simple decision problems.</p><p>In this paper we revisit an alternative framing of 'RL as inference'. In fact, we show that the orig- inal RL problem was already an inference problem all along. 1 Importantly, this inference problem includes inference over the agent's future actions and observations. Of course, this perspective is not new, and has long been known as simply the Bayes-optimal solution, see, e.g., <xref ref-type="bibr" rid="b10">Ghavamzadeh et al. (2015)</xref>. The problem is that, due to the exponential lookahead, this inference problem is fundamentally intractable for all but the simplest problems (<xref ref-type="bibr" rid="b11">Gittins, 1979</xref>). For this reason, RL re- search focuses on computationally efficient approaches that maintain a level of statistical efficiency (<xref ref-type="bibr" rid="b9">Furmston &amp; Barber, 2010</xref>; <xref ref-type="bibr" rid="b6">Osband et al., 2017</xref>).</p><p>We provide a review of the RL problem in Section 2, together with a simple and coherent framing of RL as probabilistic inference. In Section 3 we present three approximations to the intractable Bayes-optimal policy. We begin with the celebrated Thompson sampling algorithm, then we review the popular 'RL as inference' framing, as presented by <xref ref-type="bibr" rid="b7">Levine (2018)</xref>, and highlight a clear and simple shortcoming in this approach. Finally, we review K-learning (<xref ref-type="bibr" rid="b0">O'Donoghue, 2018</xref>), which we re-interpret as a modification to the RL as inference framework that provides a principled approach to the statistical inference problem, as well as a presenting a relationship with Thompson sampling.</p><p>In Section 4 we present computational studies that support our claims.</p></sec><sec><title>REINFORCEMENT LEARNING</title><p>We consider the problem of an agent taking actions in an unknown environment in order to maximize cumulative rewards through time. For simplicity, this paper will model the environment as a finite horizon, discrete Markov Decision Process (MDP) M = (S, A, R, P, H, &#961;). 2 Here S = {1, .., S} is the state space, A = {1, .., A} is the action space and each episode is of fixed length H &#8712; N. Each episode &#8712; N begins with state s 0 &#8764; &#961; then for timesteps h = 0, .., H &#8722; 1 the agent selects action a h , observes transition s h+1 with probability P(s h+1 , s h , a h ) &#8712; [0, 1] and receives reward r h+1 &#8764; R(s h , a h ), where we denote by &#181;(s h , a h ) = Er h+1 the mean reward. We define a policy &#960; to be a mapping from S to probability distributions over A and write &#928; for the space of all policies. For any timestep t = ( , h), we define F t = (s 0 0 , a 0 0 , r 0 1 , .., s h&#8722;1 , a h&#8722;1 , r h ) to be the sequence of observations made before time t. An RL algorithm maps histories to policies &#960; t = alg(S, A, F t ).</p><p>Our goal in the design of RL algorithms is to obtain good performance (cumulative rewards) for an unknown M &#8712; M, where M is some family of possible environments. Note that this is a different problem from typical 'optimal control', that seeks to optimize performance for one particular known MDP M ; although you might still fruitfully apply an RL algorithm to solve problems of that type. For any environment M and any policy &#960; we can define the action-value function,</p><p>Where the expectation in (1) is taken with respect to the action selection a j for j &gt; h from the policy &#960; and evolution of the fixed MDP M . We define the value function V M,&#960; h (s) = E &#945;&#8764;&#960; Q M,&#960; h (s, &#945;) and write Q M, h (s, a) = max &#960;&#8712;&#928; Q M,&#960; h (s, a) for the optimal Q-values over policies, and the optimal value function is given by V M,</p><p>In order to compare algorithm performance across different environments, it is natural to normalize in terms of the regret, or shortfall in cumulative rewards relative to the optimal value,</p><p>This quantity depends on the unknown MDP M , which is fixed from the start and kept the same throughout, but the expectations are taken with respect to the dynamics of M and the learning algorithm alg. For any particular MDP M , the optimal regret of zero can be attained by the non- learning algorithm alg M that returns the optimal policy for M .</p><p>In order to assess the quality of a reinforcement learning algorithm, which is designed to work across some family of M &#8712; M, we need some method to condense performance over a set to a single number. There are two main approaches to this:</p><p>WorstCaseRegret(M, alg, L) = max M &#8712;M Regret(M, alg, L), (4) where &#966; is a prior over the family M. These differing objectives are often framed as Bayesian (average-case) (3) and frequentist (worst-case) (4) RL 3 . Although these two settings are typically studied in isolation, it should be clear that they are intimately related through the choice of M and &#966;. Our next section will investigate what it would mean to 'solve' the RL problem. Importantly, we show that both frequentist and Bayesian perspectives already amount to a problem in probabilistic inference, without the need for additional re-interpretation.</p></sec><sec><title>SOLVING THE RL PROBLEM THROUGH PROBABILISTIC INFERENCE</title><p>If you want to 'solve' the RL problem, then formally the objective is clear: find the RL algorithm that minimizes your chosen objective, (3) or (4). To anchor our discussion, we introduce a simple decision problem designed to highlight some key aspects of reinforcement learning. We will revisit this problem setting as we discuss approximations to the optimal policy. Problem 1 (One unknown action). Fix N &#8712; N &#8805; 3, &gt; 0 and define M N, = {M + N, , M &#8722; N, }. Both M + and M &#8722; share S = {1}, H = 1 and A = {1, .., N }; they only differ through their rewards:</p><p>Where R(a) = x &#8712; R is a shorthand for deterministic reward of x when choosing action a. Problem 1 is extremely simple, it involves no generalization and no long-term consequences: It is an independent bandit problem with only one unknown action. For known M + , M &#8722; the optimal policy is trivial: Choose a t = 2 in M + and a t = 1 in M &#8722; for all t. An RL agent faced with unknown M &#8712; M should attempt to optimize the RL objectives (3) or (4). Unusually, and only because Problem 1 is so simple, we can actually compute the optimal solutions to both in terms of L (the total number of episodes) and &#966; = (p + , p &#8722; ) where p + = P(M = M + ), the probability of being in M + .</p><p>For L &gt; 3 an optimal minimax (minimizing the worst-case regret) RL algorithm is to first choose a 0 = 2 and observe r 1 . If r 1 = 2 then you know you are in M + so pick a t = 2 for all t = 1, 2.., for Regret(L) = 0. If r 1 = &#8722;2 then you know you are in M &#8722; so pick a t = 1 for all t = 1, 2.., for Regret(L) = 3. The worst-case regret of this algorithm is 3, which cannot be bested by any algorithm.</p></sec><sec><title>Published as a conference paper at ICLR 2020</title><p>Actually, the same RL algorithm is also Bayes-optimal for any &#966; = (p + , p &#8722; ) provided p + L &gt; 3. This relationship is not a coincidence. All admissible solutions to the worst-case problem (4) are given by solutions to the average-case (3) for some 'worst-case' prior&#966; (<xref ref-type="bibr" rid="b0">Wald, 1950</xref>). As such, for ease of exposition, our discussion will focus on the Bayesian (or average-case) setting. However, readers should understand that the same arguments apply to the worst-case objective.</p><p>In Problem 1, the key probabilistic inference the agent must consider is the effects of it own ac- tions upon the future rewards, i.e., whether it has chosen action 2. Slightly more generally, where actions are independent and episode length H = 1, the optimal RL algorithm can be computed via Gittins indices, but these problems are very much the exception (<xref ref-type="bibr" rid="b11">Gittins, 1979</xref>). In problems with generalization or long-term consequences, computing the Bayes-optimal solution is computation- ally intractable. One example of an algorithm that converges to Bayes-optimal solution in the limit of infinite computation is given by Bayes-adaptive Monte-Carlo Planning (<xref ref-type="bibr" rid="b12">Guez et al., 2012</xref>). The problem is that, even for very simple problems, the lookahead tree of interactions between actions, observations and algorithmic updates grows exponentially in the search depth (<xref ref-type="bibr" rid="b6">Strehl et al., 2006</xref>). Worse still, direct computational approximations to the Bayes-optimal solution can fail exponen- tially badly should they fall short of the required computation (<xref ref-type="bibr" rid="b0">Munos, 2014</xref>). As a result, research in reinforcement learning amounts to trying to find computationally tractable approximations to the Bayes-optimal policy that maintain some degree of statistical efficiency.</p></sec><sec><title>APPROXIMATIONS FOR COMPUTATIONAL AND STATISTICAL EFFICIENCY</title><p>The exponential explosion of future actions and observations means solving for the Bayes-optimal solution is computationally intractable. To counter this, most computationally efficient approaches to RL simplify the problem at time t to only consider inference over the data F t that has been gath- ered prior to time t. The most common family of these algorithms are 'certainty equivalent' (under an identity utility): They take a point estimate for their best guess of the environmentM , and try to optimize their control given these estimates VM , . Typically, these algorithms are used in con- junction with some dithering scheme for random action selection (e.g., epsilon-greedy), to mitigate premature and suboptimal convergence (<xref ref-type="bibr" rid="b0">Watkins, 1989</xref>). However, since these algorithms do not prioritize their exploration, they may take exponentially long to find the optimal policy (<xref ref-type="bibr" rid="b6">Osband et al., 2014</xref>).</p><p>In order for an RL algorithm to be statistically efficient, it must consider the value of information. To do this, an agent must first maintain some notion of epistemic uncertainty, so that it can direct its ex- ploration towards states and actions that it does not understand well (<xref ref-type="bibr" rid="b0">O'Donoghue et al., 2018</xref>). Here again, probabilistic inference finds a natural home in RL: We should build up posterior estimates for the unknown problem parameters, and use this distribution to drive efficient exploration.</p></sec><sec><title>THOMPSON SAMPLING</title><p>One of the oldest heuristics for balancing exploration with exploitation is given by Thompson sam- pling, or probability matching (<xref ref-type="bibr" rid="b5">Thompson, 1933</xref>). Each episode, Thompson sampling (TS) ran- domly selects a policy according to the probability it is the optimal policy, conditioned upon the data seen prior to that episode. Thompson sampling is a simple and effective method that success- fully balances exploration with exploitation (<xref ref-type="bibr" rid="b6">Russo et al., 2018</xref>).</p><p>Implementing Thompson sampling amounts to an inference problem at each episode. For each s, a, h define the binary random variable O h (s, a) where O h (s, a) = 1 denotes the event that action a is optimal for state s in timestep h. 5 The TS policy for episode is thus given by the inference problem, &#960; TS &#8764; P(O | F ), (5) where P(O | F ) is the joint probability over all the binary optimality variables (hereafter we shall suppress the dependence on F ). To understand how Thompson sampling guides exploration let us consider its performance in Problem 1 when implemented with a uniform prior &#966; = ( 1 2 , 1 2 ). In the Published as a conference paper at ICLR 2020 first timestep the agent samples M 0 &#8764; &#966;. If it samples M + it will choose action a 0 = 2 and learn the true system dynamics, choosing the optimal arm thereafter. If it samples M &#8722; it will choose action a 0 = 1 and repeat the identical decision in the next timestep. Note that this procedure achieves BayesRegret 2.5 according to &#966;, but also worst-case regret 3, which matches the optimal minimax performance despite its uniform prior.</p><p>Recent interest in TS was kindled by strong empirical performance in bandit tasks (<xref ref-type="bibr" rid="b5">Chapelle &amp; Li, 2011</xref>). Following work has shown that this algorithm satisfies strong Bayesian regret bounds close to the known lower bounds for MDPs, under certain assumptions (<xref ref-type="bibr" rid="b0">Osband &amp; Van Roy, 2017</xref>; 2016). However, although much simpler than the Bayes-optimal solution, the inference problem in (5) can still be prohibitively expensive. <xref ref-type="table" rid="tab_0">Table 1</xref> describes one approach to performing the sampling required in (5) implicitly, by maintaining an explicit model over MDP parameters. This algorithm can be computationally intractable as the MDP becomes large and so attempts to scale Thompson sampling to complex systems have focused on approximate posterior samples via randomized value functions, but it is not yet clear under which settings these approximations should be expected to perform well (<xref ref-type="bibr" rid="b6">Osband et al., 2017</xref>). As we look for practical, scalable approaches to posterior inference one promising (and popular) approach is known commonly as 'RL as inference'.</p></sec><sec><title>THE 'RL AS INFERENCE' FRAMEWORK AND ITS LIMITATIONS</title><p>The computational challenges of Thompson sampling suggest an approximate algorithm that re- places (5) with a parametric distribution suitable for expedient computation. It is possible to view the algorithms of the 'RL as inference' approach in this light (<xref ref-type="bibr" rid="b6">Rawlik et al., 2013</xref>; <xref ref-type="bibr" rid="b0">Todorov, 2009</xref>; <xref ref-type="bibr" rid="b0">Toussaint, 2009</xref>; <xref ref-type="bibr" rid="b6">Deisenroth et al., 2013</xref>; <xref ref-type="bibr" rid="b8">Fellows et al., 2019</xref>); see <xref ref-type="bibr" rid="b7">Levine (2018)</xref> for a recent survey. These algorithms choose to model the probability of optimality according to, P(O h (s, a)|&#964; h (s, a)) &#8733; exp &#63723; &#63725; (s ,a )&#8712;&#964; h (s,a) &#946;E &#181;(s , a ) &#63734; &#63736; . (6) for some &#946; &gt; 0, where &#964; h (s, a) is a trajectory (a sequence of state-action pairs) starting from (s, a) at timestep h, and where E denotes the expectation under the posterior at episode . With this potential in place one can perform Bayesian inference over the unobserved 'optimality' variables, obtaining posteriors over the policy or other variables of interest. This presentation of the RL as inference framework is slightly closer to the one in <xref ref-type="bibr" rid="b6">Deisenroth et al. (2013</xref>, &#167;2.4.2.2) than to <xref ref-type="bibr" rid="b7">Levine (2018)</xref>, but ultimately it produces the same family of algorithms. We provide such a derivation in the appendix for completeness.</p><p>Applying inference procedures to (6) leads naturally to RL algorithms with some 'soft' Bellman updates, and added entropy regularization. We describe the general structure of these algorithms in <xref ref-type="table" rid="tab_1">Table 2</xref>. These algorithmic connections can help reveal connections to policy gradient, actor-critic, and maximum entropy RL methods (<xref ref-type="bibr" rid="b6">Mnih et al., 2016</xref>; <xref ref-type="bibr" rid="b0">O'Donoghue et al., 2017</xref>; <xref ref-type="bibr" rid="b13">Haarnoja et al., 2017</xref>; 2018; <xref ref-type="bibr" rid="b7">Eysenbach et al., 2018</xref>). The problem is that this resultant 'posterior' derived using (6) does not generally bear any close relationship to the agent's epistemic probability that (s, a, h) is optimal.</p><p>To understand how 'RL as inference' guides decision making, let us consider its performance in Problem 1. Practical implementations of 'RL as inference' estimate E &#181; through observations. For N large, and without prior guidance, the agent is then extremely unlikely to select action a t = 2 and so resolve its epistemic uncertainty. Even for an informed prior &#966; = ( 1 2 , 1 2 ) action selection according to the exploration strategy of Boltzmann dithering is unlikely to sample action 2 for which E &#181;(2) = 0 (<xref ref-type="bibr" rid="b7">Levine, 2018</xref>; <xref ref-type="bibr" rid="b4">Cesa-Bianchi et al., 2017</xref>). This is because the N &#8722; 1 'distractor' actions with E &#181; &#8805; 1 &#8722; are much more probable under the Boltzmann policy.</p><p>This problem is the same problem that afflicts most dithering approaches to exploration. 'RL as inference' as a framework does not incorporate an agents epistemic uncertainty, and so can lead to poor policies for even simple problems. While (6) allows the construction of a dual 'posterior distribution', this distribution does not generally bear any relation to the typical posterior an agent should compute conditioned upon the data it has gathered, e.g., equation (5). Despite this short- coming RL as inference has inspired many interesting and novel techniques, as well as delivered algorithms with good performance on problems where exploration is not the bottleneck (<xref ref-type="bibr" rid="b7">Eysenbach et al., 2018</xref>). However, due to the use of language about 'optimality' and 'posterior inference' etc., it may come as a surprise to some that this framework does not truly tackle the Bayesian RL problem. Indeed, algorithms using 'RL as inference' can perform very poorly on problems where accurate uncertainty quantification is crucial to performance. We hope that this paper sheds some light on the topic.</p></sec><sec><title>MAKING SENSE OF 'RL AS INFERENCE' VIA K-LEARNING</title><p>In this section we suggest a subtle alteration to the 'RL as inference' framework that develops a coherent notion of optimality. The K-learning algorithm was originally introduced through a risk- seeking exponential utility (<xref ref-type="bibr" rid="b0">O'Donoghue, 2018</xref>). In this paper we re-derive this algorithm as a principled approximate inference procedure with clear connections to Thompson sampling, and we highlight its similarities to the 'RL as inference' framework. We believe that this may offer a road towards combining the respective strengths of Thompson sampling and the 'RL as inference' frame- works. First, consider the following approximate conditional optimality probability at (s, a, h): P(O h (s, a)|Q M, h (s, a)) &#8733; exp &#946;Q M, h (s, a), (7) for some &#946; &gt; 0, and note that this is conditioned on the random variable Q M, h (s, a). We can marginalize over possible Q-values yielding P(O h (s, a)) = P (O h (s, a)|Q M, h (s, a))dP(Q M, h (s, a)) &#8733; exp G Q h (s, a, &#946;), (8) where G Q h (s, a, &#183;) denotes the cumulant generating function of the random variable Q M, h (s, a) (<xref ref-type="bibr" rid="b0">Kendall, 1946</xref>). Clearly K-learning and the 'RL as inference' framework are similar, since equa- tions (6) and (7) are closedly linked, but there is a crucial difference. Notice that the integral per- formed in (8) is with respect to the posterior over Q M, h (s, a), which includes the epistemic uncer- tainty explicitly.</p><p>Given the approximation to the posterior probability of optimality in (8) we could sample actions from it as our policy, as done by Thompson sampling (5). However, that requires computation of the cumulant generating function G Q h (s, a, &#946;), which is non-trivial. It was shown in (<xref ref-type="bibr" rid="b0">O'Donoghue, 2018</xref>) that an upper bound to the cumulant generating function could be computed by solving a particular 'soft' Bellman equation. The resulting K-values, denoted K h (s, a) at (s, a, h), are also optimistic for the expected optimal Q-values. Specifically, for any sequence {&#946; } the following holds</p><p>Following a Boltzmann policy over these K-values satisfies a Bayesian regret bound which matches the current best bound for Thompson sampling up to logarithmic factors under the same set of assumptions. We summarize the K-learning algorithm in Table (3), where &#946; &gt; 0 is a constant and and n (s, a) is the visitation count of (s, a) before episode , i.e., the number of times the agent has taken action a at state s, and &#963; &gt; 0 is a constant. The uncertainty in the transition function is incorporated into the constant &#963;, which is a technical detail we omit here for clarity, see (<xref ref-type="bibr" rid="b0">O'Donoghue, 2018</xref>) for details. In this way the agent is given a reward signal that includes a bonus which is higher for states and actions that the agent has visited less frequently.</p><p>Comparing <xref ref-type="table" rid="tab_1">Tables 2</xref> and 3 it is clear that soft Q-learning and K-learning share some similarities: They both solve a 'soft' value function and use Boltzmann policies. However, the differences are important. Firstly, K-learning has an explicit schedule for the inverse temperature parameter &#946; , and secondly it adds a bonus based on visitation count to the expected reward. These two relatively small changes make K-learning a principled exploration and inference strategy.</p><p>To understand how K-learning drives exploration, consider its performance on Problem 1. Since this is a bandit problem we can compute the cumulant generating functions for each arm and then use the policy given by (8). For any non-trivial prior and choice of &#946; &gt; 0 the cumulant generating function is optimistic for arm 2 which results in the policy selecting arm 2 more frequently, thereby resolving its epistemic uncertainty. As &#946; &#8594; &#8734; K-learning converges to the policy of pulling arm 2 deterministically. This is in contrast to soft Q-learning where arm 2 is exponentially unlikely to be selected as the exploration parameter &#946; grows.</p></sec><sec><title>CONNECTIONS BETWEEN K-LEARNING AND THOMPSON SAMPLING</title><p>Since K-learning can be viewed as approximating the posterior probability of optimality of each action it is natural to ask how close an approximation it is. A natural way to measure this similarity is the Kullback-Leibler (KL) divergence between the distributions, D KL (P(O h (s)) || &#960; K h (s)) = a P(O h (s, a)) log(P(O h (s, a))/&#960; K h (s, a)), where we are using the notation O h (s) = O h (s, &#183;) and &#960; K h (s) = &#960; K h (s, &#183;). This is different to the usual notion of distance that is taken in variational Bayesian methods, which would typically reverse the order of the arguments in the KL divergence (<xref ref-type="bibr" rid="b3">Blundell et al., 2015</xref>). However, in RL that 'direction' is not appropriate: a distribution minimizing D KL (&#960; h (s) || P(O h (s))) may put zero probability on regions of support of P(O h (s)). This means an action with non-zero probability of be- ing optimal might never be taken. On the other hand a policy minimizing D KL (P(O h (s)) || &#960; h (s)) must assign a non-zero probability to every action that has a non-zero probability of being optimal, or incur an infinite KL divergence penalty. With this characterization in mind, and noting that the h (s) = P(O h (s)), our next result links the policies of K-learning to Thompson sampling.</p><p>Theorem 1. The K-learning value function V K and policy &#960; K defined in <xref ref-type="table" rid="tab_2">Table 3</xref> satisfy the follow- ing bound at every state s &#8712; S and h = 0, . . . H:</p><p>We defer the proof to Appendix 5.2. This theorem tells us that the distance between the true proba- bility of optimality and the K-learning policy is bounded for any choice of &#946; &lt; &#8734;. In other words, if there is an action that might be optimal then K-learning will eventually take that action.</p></sec><sec><title>WHY IS 'RL AS INFERENCE' SO POPULAR?</title><p>The sections above outline some surprising ways that the 'RL as inference' framework can drive suboptimal behaviour in even simple domains. The question remains, why do so many popular and effective algorithms lie within this class? The first, and most important point, is that these algorithms can perform extremely well in domains where efficient exploration is not a bottleneck. Furthermore, they are often easy to implement and amenable to function approximation (<xref ref-type="bibr" rid="b6">Peters et al., 2010</xref>; <xref ref-type="bibr" rid="b0">Kober &amp; Peters, 2009</xref>; <xref ref-type="bibr" rid="b0">Abdolmaleki et al., 2018</xref>). Our discussion of K-learning in Section 3.3 shows that a relatively simple fix to this problem formulation can result in a framing of RL as inference that maintains a coherent notion of optimality. Computational results show that, in tabular domains, K- learning can be competitive with, or even outperform Thompson sampling strategies, but extending these results to large-scale domains with generalization is an open question (<xref ref-type="bibr" rid="b0">O'Donoghue, 2018</xref>; <xref ref-type="bibr" rid="b6">Osband et al., 2017</xref>).</p><p>The other observation is that the 'RL as inference' can provide useful insights to the structure of particular algorithms for RL. It is valid to note that, under certain conditions, following policy gradient is equivalent to a dual inference problem where the 'probabilities' play the role of dummy variables, but are not supposed to represent the probability of optimality in the RL problem. In this light, <xref ref-type="bibr" rid="b7">Levine (2018)</xref> presents the inference framework as a way to generalize a wide range of state of the art RL algorithms. However, when taking this view, you should remember that this inference duality is limited to certain RL algorithms, and without some modifications (e.g. Section 3.3) this perspective is in danger of overlooking important aspects of the RL problem.</p></sec><sec><title>COMPUTATIONAL EXPERIMENTS</title></sec><sec><title>ONE UNKNOWN ACTION (PROBLEM 1)</title><p>Consider the environment of Problem 1 with uniform prior &#966; = ( 1 2 , 1 2 ). We fix = 1e &#8722; 3 and consider how the Bayesian regret varies with N &gt; 3. <xref ref-type="fig" rid="fig_0">Figure 1</xref> compares how the regret scales for Bayes-optimal (1.5), Thompson sampling (2.5), K-learning (&#8804; 2.2) and soft Q-learning (which grows linearly in N for the optimal &#946; &#8594; 0, but would typically grow exponentially for &#946; &gt; 0). This highlights that, even in a simple problem, there can be great value in considering the value of information.</p></sec><sec><title>'DEEPSEA' EXPLORATION</title><p>Our next set of experiments considers the 'DeepSea' MDPs introduced by <xref ref-type="bibr" rid="b6">Osband et al. (2017)</xref>. At a high level this problem represents a 'needle in a haystack', designed to require efficient exploration, the complexity of which grows with the problem size N &#8712; N. DeepSea (<xref ref-type="fig" rid="fig_1">Figure 2</xref>) is a scalable variant of the 'chain MDPs' popular in exploration research (<xref ref-type="bibr" rid="b6">Jaksch et al., 2010</xref>). 6 The agent begins each episode in the top-left state in an N &#215; N grid. At each timestep the agent can move left or right one column, and falls one row. There is a small negative reward for heading right, and zero reward for left. There is only one rewarding state, at the bottom right cell. The only way the agent can receive positive reward is to choose to go right in each timestep. Algorithms that do not perform deep exploration will take an exponential number of episodes to learn the optimal policy, but those that prioritize informative states and actions can learn much faster. Figure 3a shows the 'time to learn' for tabular implementations of K-learning (Section 3.3), soft Q- learning (Section 3.2) and Thompson sampling (Section 3.1). We implement each of the algorithms with a N (0, 1) prior for rewards and Dirichlet(1/N ) prior for transitions. Since these problems are small and tabular, we can use conjugate prior updates and exact MDP planning via value iteration. As expected, Thompson sampling and K-learning scale gracefully to large domains but soft Q- learning does not.</p></sec><sec><title>BEHAVIOUR SUITE FOR REINFORCEMENT LEARNING</title><p>So far our experiments have been confined to the tabular setting, but the main focus of 'RL as inference' is for scalable algorithms that work with generalization. In this section we show that the same insights we built in the tabular setting extend to the setting of deep RL. To do this we implement variants of Deep Q-Networks with a single layer, 50-unit MLP (<xref ref-type="bibr" rid="b6">Mnih et al., 2013</xref>). To adapt K-learning and Thompson sampling to this deep RL setting we use an ensemble of size 20 with randomized prior functions to approximate the posterior distribution over neural network Q-values (<xref ref-type="bibr" rid="b6">Osband et al., 2018</xref>) (full experimental details are included in Appendix 5.4). We then evaluate all of the algorithms on bsuite: A suite of benchmark tasks designed to highlight key issues in RL (<xref ref-type="bibr" rid="b6">Osband et al., 2019</xref>).</p><p>In particular, bsuite includes an evaluation on the DeepSea problems but with a one-hot pixel representation of the agent position. In Figure 3b we see that the results for these deep RL im- plementations closely match the observed scaling for the tabular setting. In particular, the algo- rithms motivated by Thompson sampling and K-learning both scale gracefully to large problem sizes, where soft Q-learning is unable to drive deep exploration. Our bsuite evaluation includes many more experiments that can be fit into this paper, but we provide a link to the complete results at bit.ly/rl-inference-bsuite. In general, the results for Thompson sampling and K- learning are similar, with soft Q-learning performing significantly worse on 'exploration' tasks. We push a summary of these results to Appendix 6.</p></sec><sec><title>CONCLUSION</title><p>This paper aims to make sense of reinforcement learning and probabilistic inference. We review the reinforcement learning problem and show that this problem of optimal learning already com- bined the problems of control and inference. As we highlight this connection, we also clarify some potentially confusing details in the popular 'RL as inference' framework. We show that, since this problem formulation ignores the role of epistemic uncertainty, that algorithms derived from that framework can perform poorly on even simple tasks. Importantly, we also offer a way forward, to reconcile the views of RL and inference in a way that maintains the best pieces of both. In partic- ular, we show that a simple variant to the RL as inference framework (K-learning) can incorporate uncertainty estimates to drive efficient exploration. We support our claims with a series of simple di- dactic experiments. We leave the crucial questions of how to scale these insights up to large complex domains for future work.</p></sec><sec id="figures"><title>Figures</title><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Model-based Thompson sampling.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Soft Q-learning.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_2"><label>Table 3:</label><caption><title>Table 3:</title><p>K-learning.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Regret scaling on Problem 1. Soft Q-learning does not scale gracefully with N .</p></caption><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>DeepSea exploration: A simple example where deep exploration is critical.</p></caption><graphic /><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Learning times for DeepSea experiments. Dashed line represents 2 N .</p></caption><graphic /><graphic /><graphic /><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Maximum a posteriori policy optimisation</article-title><source>International Conference on Learning Representations (ICLR)</source><year>2018</year><person-group person-group-type="author"><name><surname>Abdolmaleki</surname><given-names>Abbas</given-names></name><name><surname>Springenberg</surname><given-names>Jost Tobias</given-names></name><name><surname>Tassa</surname><given-names>Yuval</given-names></name><name><surname>Heess</surname><given-names>Remi Munos Nicolas</given-names></name><name><surname>Riedmiller</surname><given-names>Martin</given-names></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><source>Stochastic simulation: Algorithms and analysis</source><year>2007</year><volume>57</volume><person-group person-group-type="author"><name><surname>Asmussen</surname><given-names>S&#248;ren</given-names></name><name><surname>Peter W Glynn</surname><given-names /></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><source>Dynamic programming and optimal control</source><year>2005</year><volume>1</volume><person-group person-group-type="author"><name><surname>Dimitri</surname><given-names>P</given-names></name><name><surname>Bertsekas</surname><given-names /></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Weight uncertainty in neural networks</article-title><source>arXiv preprint arXiv:1505.05424</source><year>2015</year><person-group person-group-type="author"><name><surname>Blundell</surname><given-names>Charles</given-names></name><name><surname>Cornebise</surname><given-names>Julien</given-names></name><name><surname>Kavukcuoglu</surname><given-names>Koray</given-names></name><name><surname>Wierstra</surname><given-names>Daan</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Boltzmann exploration done right</article-title><source>Advances in Neural Information Processing Systems</source><year>2017</year><fpage>6287</fpage><lpage>6296</lpage><person-group person-group-type="author"><name><surname>Cesa-Bianchi</surname><given-names>Nicol&#242;</given-names></name><name><surname>Gentile</surname><given-names>Claudio</given-names></name><name><surname>Neu</surname><given-names>Gergely</given-names></name><name><surname>Lugosi</surname><given-names>Gabor</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>An empirical evaluation of thompson sampling</article-title><source>Advances in neural information processing systems</source><year>2011</year><fpage>2249</fpage><lpage>2257</lpage><person-group person-group-type="author"><name><surname>Chapelle</surname><given-names>Olivier</given-names></name><name><surname>Li</surname><given-names>Lihong</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>A survey on policy search for robotics</article-title><source>Foundations and Trends R in Robotics</source><year>2013</year><volume>2</volume><issue>1-2</issue><fpage>1</fpage><lpage>142</lpage><person-group person-group-type="author"><name><surname>Peter Deisenroth</surname><given-names>Marc</given-names></name><name><surname>Neumann</surname><given-names>Gerhard</given-names></name><name><surname>Peters</surname><given-names>Jan</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Diversity is all you need: Learning skills without a reward function</article-title><source>arXiv preprint arXiv:1802.06070</source><year>2018</year><person-group person-group-type="author"><name><surname>Eysenbach</surname><given-names>Benjamin</given-names></name><name><surname>Gupta</surname><given-names>Abhishek</given-names></name><name><surname>Ibarz</surname><given-names>Julian</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Virel: A variational inference framework for reinforcement learning</article-title><source>Advances in Neural Information Processing Systems</source><year>2019</year><fpage>7120</fpage><lpage>7134</lpage><person-group person-group-type="author"><name><surname>Fellows</surname><given-names>Matthew</given-names></name><name><surname>Mahajan</surname><given-names>Anuj</given-names></name><name><surname>Tim</surname><given-names>G J</given-names></name><name><surname>Rudner</surname><given-names>Shimon Whiteson</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Variational methods for reinforcement learning</article-title><source>Pro- ceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</source><year>2010</year><fpage>241</fpage><lpage>248</lpage><person-group person-group-type="author"><name><surname>Furmston</surname><given-names>Thomas</given-names></name><name><surname>Barber</surname><given-names>David</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Bayesian reinforcement learning: A survey</article-title><source>Foundations and Trends R in Machine Learning</source><year>2015</year><volume>8</volume><issue>5-6</issue><fpage>359</fpage><lpage>483</lpage><person-group person-group-type="author"><name><surname>Ghavamzadeh</surname><given-names>Mohammad</given-names></name><name><surname>Mannor</surname><given-names>Shie</given-names></name><name><surname>Pineau</surname><given-names>Joelle</given-names></name><name><surname>Tamar</surname><given-names>Aviv</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Bandit processes and dynamic allocation indices</article-title><source>Journal of the Royal Statistical Society. Series B (Methodological)</source><year>1979</year><fpage>148</fpage><lpage>177</lpage><person-group person-group-type="author"><name><surname>John</surname><given-names>C</given-names></name><name><surname>Gittins</surname><given-names /></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Efficient Bayes-adaptive reinforcement learning using sample-based search</article-title><source>Advances in Neural Information Processing Systems</source><year>2012</year><fpage>1025</fpage><lpage>1033</lpage><person-group person-group-type="author"><name><surname>Guez</surname><given-names>Arthur</given-names></name><name><surname>Silver</surname><given-names>David</given-names></name><name><surname>Dayan</surname><given-names>Peter</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Reinforcement learning with deep energy-based policies</article-title><source>Proceedings of the 34th International Conference on Machine Learning (ICML)</source><year>2017</year><person-group person-group-type="author"><name><surname>Haarnoja</surname><given-names>Tuomas</given-names></name><name><surname>Tang</surname><given-names>Haoran</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref></ref-list></back></article>