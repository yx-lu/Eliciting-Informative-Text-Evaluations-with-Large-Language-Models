Title:
```
Under review as a conference paper at ICLR 2020 UNSUPERVISED LEARNING OF NODE EMBEDDINGS BY DETECTING COMMUNITIES
```
Abstract:
```
We present Deep MinCut (DMC), an unsupervised approach to learn node em- beddings for graph-structured data. It derives node representations based on their membership in communities. As such, the embeddings directly provide interest- ing insights into the graph structure, so that the separate node clustering step of existing methods is no longer needed. DMC learns both, node embeddings and communities, simultaneously by minimizing the mincut loss, which captures the number of connections between communities. Striving for high scalability, we also propose a training process for DMC based on minibatches. We provide em- pirical evidence that the communities learned by DMC are meaningful and that the node embeddings are competitive in different node classification benchmarks.
```

Figures/Tables Captions:
```
Figure 1: Learning embeddings by detecting communities.
Figure 2: Effect of batch size
Figure 3: Embedding size vs. #communities
Figure 4: Hierarchy of words/topics
Table 1: Statistics of the datasets
Table 2: Node classification results.
Table 3: DMC vs. DGI on cluster quality
Table 4: Test Precision
Table 5: Top-5 words for the first five dimensions
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Graphs are a natural representation of relations between entities in complex systems, such as social networks or information networks. To enable inference on graphs, a graph embedding may be learned. It comprises node embeddings, each being a vector-based representation of a graph's node that incorporates its relations to other nodes ( Goyal & Ferrara, 2018 ;  Hamilton et al., 2017b ). While supervised node embeddings have received a lot of attention, most real-world graphs are not labelled, which calls for unsupervised learning techniques. The main principle in unsupervised learning of node embeddings is that "similar" nodes have close embeddings in the embedding space. The similarity of nodes is often defined based on their distance in a graph, e.g., based on their co-occurrence probability in a random walk ( Goyal & Ferrara, 2018 ;  Perozzi et al., 2014 ;  Grover & Leskovec, 2016 ). Recently, it was also argued that two nodes should be similar, if they are similar to a graph summary representation ( Veličković et al., 2018 ). In this work, we argue that node embeddings shall not only be of high quality for inference tasks, but shall also be meaningful. That is, they shall directly provide insights into interesting structures in a graph in order to avoid a potentially biased post-analysis step, e.g., through clustering of the embeddings. We therefore assess node similarity from the perspective of node communities, where the dimensions of embeddings are some unknown communities instead of some unknown latent features as in traditional techniques. Considering a community as a set of densely connected nodes with sparse connections to outside nodes, the homophily principle is restated as follows: Nodes with similar community membership characteristics shall have close embeddings. Specifically, for each node, we incorporate membership information as a probability distribution over a set of communi- ties. Then, nodes are similar, if they are both likely and unlikely to be part of the same communities. Since communities are generally unknown, we propose to minimize the mincut loss for unsupervised learning of communities and node embeddings simultaneously. Mincut loss leverages the principle that communities are well-separated if there are few connections between them ( Fortunato, 2010 ). It is theoretically motivated as its optimal closed-form solution can be found, while its variant, the normalized cut, is a well-studied problem. Aiming at a realisation of the above idea, we propose Deep MinCut (DMC), a neural network ap- proach to minimize mincut loss. We learn node embeddings to sample one-hot vectors that represent the assignment of nodes to communities. The vectors are drawn from distributions parameterized by continuous node embeddings using Gumbel-Softmax ( Jang et al., 2016 ;  Maddison et al., 2016 ). This renders the process differentiable and, thus, enables joint learning of embeddings and communities.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 We demonstrate the applicability of DMC in various applications. In node classification, our node embeddings turn out to outperform traditional embedding techniques ( Grover & Leskovec, 2016 ;  Perozzi et al., 2014 ;  Veličković et al., 2018 ), while also revealing the graph's community structure. In community detection, by stacking mincut losses, we are able to learn a hierarchy of communities, e.g., when generating word embeddings, we can link words to topics, and topics to abstract themes.

Section Title: RELATED WORK
  RELATED WORK Graph embedding constructs a low-dimensional model of the nodes of a graph that incorporates its structure ( Hamilton et al., 2017b ;  Goyal & Ferrara, 2018 ). Embedding techniques can be classified into shallow ( Perozzi et al., 2014 ;  Grover & Leskovec, 2016 ) and deep approaches ( Hamilton et al., 2017a ;  Kipf & Welling, 2016 ;  Wu et al., 2019 ). Shallow approaches rely on an embedding lookup table to map nodes to embeddings. On the other hand, deep models construct a node's embedding by performing aggregation of its neighbours' embeddings.

Section Title: Unsupervised node embeddings
  Unsupervised node embeddings While different models may be employed to embed nodes ( Per- ozzi et al., 2014 ;  Grover & Leskovec, 2016 ), the majority of unsupervised learning techniques leverage a contrastive loss function, such as skipgram loss ( Hamilton et al., 2017b ) or infomax loss ( Veličković et al., 2017 ). The encoder is trained such that, given a scoring function, a high score is given to positive samples, whereas negative samples receive a low score. For skipgram loss, the positive samples are nodes that are close in a random walk, while the negative samples are randomly selected from other graph nodes. A drawback of the random-walk objective is that it can only capture local information around a node ( Perozzi et al., 2014 ;  Grover & Leskovec, 2016 ). For infomax loss, positive samples are nodes in the original graph, whereas negative samples are nodes in randomly corrupted graphs. While infomax loss can capture the global structure, its perfor- mance is highly dependent on the corruption strategy ( Veličković et al., 2018 ). Since embeddings also need to be learned for the corrupted graphs, it further suffers from high training time. While our proposed method also considers the global graph structure, it differs in that our approach is non- constrastive, i.e., it does not require unnecessary learning of negative samples. Moreover, due to the nature of our loss function, we are able to learn the node embeddings as well as their clusters. Community detection is a well-studied problem with many applications ( Fortunato, 2010 ;  Fortu- nato & Hric, 2016 ). While numerous techniques for community detection have been proposed, we focus on those that generate node embeddings, such as spectral methods ( Newman, 2006b ;a;  White & Smyth, 2005 ). Spectral methods operate either on the modularity matrix ( Newman, 2006b ) or the Laplacian matrix ( White & Smyth, 2005 ). While embeddings may be learned by reconstructing these matrices (Wang et al., 2016), existing methods leverage matrix factorization, which does not scale to large graphs. Closest to our work is ( Nazi et al., 2019 ), which proposes a partition loss function for graph partitioning. As the work focuses on graph partitioning, the loss function aims for balanced partitions based on the number of nodes and, therefore, is not applicable in our setting.

Section Title: EMBEDDINGS AND COMMUNITY DETECTION
  EMBEDDINGS AND COMMUNITY DETECTION

Section Title: Graphs
  Graphs We consider a directed, weighted graph G = {V, F} with nodes V = {v i } and edges F = {(v i , v j ) | v i ∈ V ∧ v j ∈ V}, each edge (v i , v j ) being assigned a weight s (vi,vj ) ∈ R. Such a graph can also be represented by its adjacency matrix A of size n × n, where each row and column represents a node in G and a cell A ij denotes the edge weight. Note that we allow self-loops in the graph, but not multi-edges between nodes. Also, edge weights can be negative. We also consider attributed graphs in which nodes have features. We denote the node features matrix as F ∈ R n×D .

Section Title: Communities
  Communities We denote by C = {C 1 , C 2 , · · · , C k } the set of k disjoint communities of graph G, where k i=1 C i = V and ∀ C i = C j , C i ∩ C j = ∅. The assignment of nodes to communities is captured by a membership matrix P ∈ {0, 1} n×k with rows representing nodes in G and columns representing communities in C. As each node is only assigned to one community, the rows of P are one-hot vectors, where P ij = 1 if node v i is assigned to community C j . Assuming the membership matrix P is already known, the number of cross-connections between communities C i and C j can be captured by the non-diagonal elements of the adjacency matrix C of Under review as a conference paper at ICLR 2020 the quotient graph, where the nodes are communities: On the other hand, elements C ii capture the number of connections within community C i .

Section Title: Mincut loss
  Mincut loss While community detection is a well-studied problem, there is no consensus on the precise notion of a community ( Fortunato, 2010 ). A common principle is that well-separated com- munities have more connections inside than across communities. Hence, communities are detected by minimizing the number of connections between them, as captured by the following loss function: L P (A) = − k i C ii = −T r(P T AP) (2) where T r(X) is the trace of matrix X. We call the loss function in Equation 2 mincut loss, as it aims to minimize the number of connections between communities.

Section Title: Degenerated cases
  Degenerated cases Minimizing Equation 2 may lead to degenerated cases, where all nodes are assigned to one community while the others are empty ( Fortunato, 2010 ). In practice, there are two solutions to this problem. If there is prior knowledge on the communities (e.g., they shall have equal size), a respective constraint is added to the mincut loss. Another approach is to minimize the normalized cut ( Shi & Malik, 2000 ;  Zhang & Rohe, 2018 ), which is defined as follows: where C −i denotes the set of communities except C i and assoc(C i , C −i ) is the total degree of nodes in community C i . Then, the normalized cut (normcut) loss can be captured as follows: L P (A) = T r( P T L sym P P T P ) (3) where L sym = I − D −1/2 AD 1/2 is the symmetric Laplacian matrix with D be the degree matrix of A and and L = D − A is the Laplacian matrix.

Section Title: Graph-like data
  Graph-like data For ease of presentation, mincut loss is formulated based on graphs. Yet, it can be applied to any problem comprising a set T of items and a kernel function k : T × T → R that assigns weights to item pairs. This creates a kernel matrix K that can be considered as the adjacency matrix of the items. Mincut loss is designed to separate items into subsets such that the connection strength between every pair of subsets is minimized, which also means the coherence of each subset is maximized. Hence, mincut loss is applicable to a wide range of unsupervised problems.

Section Title: DEEP MINCUT
  DEEP MINCUT We first discuss the spectral approach to find optimal solutions for normcut and mincut loss, as it provides a baseline technique for comparison. We then introduce Deep MinCut along with an efficient training process based on minibatches.

Section Title: SPECTRAL APPROACH
  SPECTRAL APPROACH Since P is a binary matrix, optimizing the normcut loss to find P is an NP-hard problem ( Fortunato, 2010 ). Following traditional approaches in community detection ( Fortunato, 2010 ;  White & Smyth, 2005 ), we relax P from a binary matrix to a real matrix H ∈ R n×k . However, for the relaxed matrix H to be meaningful, it needs to retain the following semantic constraint from matrix P that each node belongs to only one community: h i h T j = 0 where h i is the i-th column of the matrix H. The matrix H that minimizes Equation 3 can be found by eigendecomposition of the adjacency matrix L sym . This is captured by the following theorem. H T H ) is minimized when the i-th column vector of H is parallel with the i-th eigenvector, i.e., h i ↑↑ q i . It is worth noting that closed-form solutions for the mincut loss can be found in a similar manner where the matrix H that minimizes the mincut loss can be constructed from the largest eigenvectors of the adjacency matrix A. The proofs of these theorems can be found in Appendix C.

Section Title: DEEP MINCUT - A NEURAL NETWORK APPROACH
  DEEP MINCUT - A NEURAL NETWORK APPROACH Although analytical solutions to Equation 2-3 can be found by eigendecomposition, such approach to minimizing the normcut loss is infeasible for large graphs. We therefore propose a neural network approach called Deep MinCut to learn node embeddings and communities at the same time. Our framework is illustrated in  Figure 1 , which we explain in detail in the remainder.

Section Title: Learning the membership matrix
  Learning the membership matrix To detect communities, we want to learn the membership matrix P that minimizes the normalized cut L P (A) = T r( P T L sym P P T P ). Recall that P captures the assignment of nodes to communities. Intuitively, this assignment is based on a node's role in the graph and the graph structure, which is also the information that shall be encoded in a node embedding ( Grover & Leskovec, 2016 ;  Hamilton et al., 2017a ). Hence, we propose to compute the membership matrix P based on node embeddings. An embedding matrix H, which contains all the node embeddings, is derived by an encoder E θ : V → R d . The encoder E θ encodes every node in V to a d-dimensional space, where θ denote parameters. Any existing node embedding techniques such as graph convolutional encoders ( Hamil- ton et al., 2017b ;  Wu et al., 2019 ) or shallow encoders ( Grover & Leskovec, 2016 ;  Perozzi et al., 2014 ) can be used to realize E θ , since the parameters θ can be learned during optimization of the normalized cut. The node embeddings in H can also be used for downstream tasks such as node classification or link prediction.

Section Title: Differential sampling
  Differential sampling To obtain P from H, we sample a one-hot vector p of P from the corre- sponding node embedding h of H. Intuitively, H ij shall capture the likelihood that the i-th node is assigned to the j-th community. As such, we consider the elements h 1 , . . . , h k of h to be the unnor- malized class probabilities of a k-dimensional categorical distribution. Then, by sampling from this distribution, we are able to obtain the one-hot vector p. Sampling from this distribution can be done using the Gumbel-max trick ( Gu et al., 2018 ;  Niu et al., 2019 ) where the non-zero element of the one-hot vector p = (p 1 , . . . , p k ) is found as follows: p i = 1, if i = arg max j (h j + g) 0, otherwise where g ∼ Gumbel(0, 1) is a sample from the standard Gumbel distribution. Note that the sampling process is still non-differentiable as the arg max operation is discontinuous since the Gumbel-max trick only makes sampling from a categorical distribution an optimization problem. By replacing the arg max function with the differentiable softmax , however, we render Under review as a conference paper at ICLR 2020 Algorithm 1: Computation of normcut loss. where x i = h i + g and τ is a temperature hyperparameter. Gumbel-Softmax (GS) not only allows to sample discrete values from a continuous distribution, but it is also differentiable. The latter is important for learning the parameters of E θ using backpropagation. As the temperature τ → 0, the row p approaches the one-hot vector. The whole computation process of the normcut loss from an adjacency matrix and the node embeddings is shown in Algorithm 1.

Section Title: Straight-through Gumbel-Softmax
  Straight-through Gumbel-Softmax Several mincut losses may be stacked to learn a hierarchy of items. For instance, another mincut loss can be applied to the adjacency matrix C to learn super- communities. Then, communities and super-communities can be learned in an end-to-end manner, similar to the hierarchical pooling framework in ( Ying et al., 2018 ). However, our approach is unsupervised and may thus be used in applications where labels are not available.

Section Title: MINIBATCH TRAINING
  MINIBATCH TRAINING

Section Title: Sampling method
  Sampling method To improve scalability of DMC, parameters shall be learned with batches of nodes, instead of the whole adjacency matrix. This is equivalent to approximating mincut loss with sampled subgraphs. However, a simple random sampling of nodes to construct a subgraph is not sufficient as the subgraph may not be connected. Hence, optimization of normcut loss is non-trivial. Against this background, we propose to construct an ego-network for each node in the graph and use this ego-network as the subgraph. This sampling procedure is similar to the neighbourhood sampling method proposed by  Hamilton et al. (2017a) . Given a node v ∈ V, its ego-network of depth d is the induced subgraph obtained from a sample of all nodes with a distance of at most d to v. Sampling is done at each level, with replacement of a fixed amount of neighbours. This is to make the subgraphs to have equal size. To create a batch of size b, we create b such ego-networks.

Section Title: Theoretical motivation
  Theoretical motivation We provide a theoretical motivation on why it is possible to approximate the normcut loss function with sampled subgraphs. Let K ∈ R m×k be the embedding matrix of the subgraph S of size m. We also denote the adjacency matrix of this subgraph as A, its degree matrix as D, and its Laplacian matrix as L. The following theorem shows that we can approximate normcut loss to a certain degree with high probability using the subgraph of size m < n where n is the number of nodes in the original graph. Theorem 2 (proof in Appendix C) states that we can choose a batch size such that the difference in the approximated loss and the true loss is small with high probability. Other factors that affect this probability are the bounds a, b of its adjacency matrix and the embedding size k. Finally, the probability also depends on how accurately we approximate the loss function, as controlled by ε.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: NODE EMBEDDING EXPERIMENTS
  NODE EMBEDDING EXPERIMENTS

Section Title: Setup
  Setup We evaluate the quality of our embeddings for node classification on four datasets, see  Table 1 . Cora, Citeseer and Pubmed are paper citation networks where a label represents the domain of a paper. Wiki is a word adjacency graph with the word labels being their POS tags. In this experiment, we use a one-layer Graph Convolutional Network ( Kipf & Welling, 2016 ) as the encoder for DMC and also report the results obtained with the spectral approach. We compare DMC with several baselines. First, we compare against community detection techniques that generate node embeddings, such as DANMF ( Ye et al., 2018 ), M-NMF ( Wang et al., 2017 ) and GAP ( Nazi et al., 2019 ). Second, we include unsupervised node embedding methods that use contrastive loss, such as DeepWalk ( Perozzi et al., 2014 ) and DGI ( Veličković et al., 2018 ). For all methods that involve randomization, we train three models with different seeds. The node embedding size is consistently set to 128. The obtained node embeddings are used to learn a logistic classifier. Instead of a fixed train/test split, we use 50 random splits and report the mean accuracy and standard deviation as suggested by  Shchur et al. (2018) . Results.  Table 2  highlights the benefits of generating embeddings by community detection for node classification. Our technique outperforms the baseline methods in three out of four benchmarks. The largest gap is observed for the Wiki dataset, which can be explained by the non-attributed nature of this dataset. DMC considers the whole structure of the graph, whereas most baseline techniques consider only the neighbourhood surrounding a node. While DGI is able to incorporate the whole graph, it relies more on node features, which is less beneficial for non-attributed graphs. The spectral approach underperforms significantly on the bibliographic datasets as it only uses the structure information in the graph. In addition, the optimal solution to normcut loss may not be the best embeddings for node classification, as the node labels are more correlated to node features. In addition to node embeddings, DMC also learns how to cluster the embeddings. We compare the cluster quality obtained by our approach with the best baseline for node classification, which is DGI. For DGI, we use k-means to cluster the node embeddings into several clusters where the number of clusters is the number of classes. Then, we compare the cluster quality obtained using DMC and DGI on two metrics: Normalized Mutual Information (NMI) and Homogeneity (HG).  Table 3  shows that the cluster quality obtained by DMC is significantly higher than the one by DGI. For instance, the NMI scores with DMC are 7× better than those with DGI on the Cora dataset. This illustrates the benefits of jointly learning node embeddings and their clusters. This is particularly important in unsupervised settings, where the learned embeddings are fed into a downstream task, such as node classification or graph analysis through clustering.

Section Title: COMMUNITY DETECTION EVALUATION
  COMMUNITY DETECTION EVALUATION

Section Title: Setup
  Setup where #w i is the number of times word w i appears in the corpus, while #(w i , w j ) is the number of times the words appear together. The adjacency matrix obtained using the above function is the PPMI matrix, a well-established concept in NLP ( Levy & Goldberg, 2014 ). Following  Yin & Shen (2018) , we construct a word corpus of 10000 words that appear >100 times in the Text8 corpus ( Mahoney, 2011 ). Words are said to appear together if they are within a window of five. We further analyse the quality of the communities constructed by DMC. As those are represented by the dimensions of the node embeddings (in this case, word embeddings), high-quality communities correspond to explainable word topics. We evaluate explainability by a word intrusion test. We create a set of five words for each dimension by selecting the top-4 words and a single low-ranked word. Human workers on MTurk are asked to detect one word per dimension that does not belong to the respective set. The detection precision then measures explainability. We compare DMC against explainable word embeddings techniques, OIWE ( Luo et al., 2015 ), Sparse Coding (SC) ( Faruqui et al., 2015 ), and Non-Negative Sparse Coding (NNSC) ( Faruqui et al., 2015 ). For a qualitative analysis, we also report the words with the largest embedding values along exemplary dimensions.

Section Title: Results
  Results   Table 4  shows that DMC outperforms state-of-the-art methods in precision of the word intrusion test. Note that NNSC, SC, and OIWE require additional data such as existing word em- beddings as input, whereas our methods learn explainable word embeddings directly on a word corpus.  Table 5  shows that the top-ranked words indeed assign a meaning to each dimension (here, the first three dimensions concern medieval literature, DC comics, and transportation). By stacking two normcut losses, DMC is also able to learn a hierarchy of words and topics.  Figure 4  gives an example, where super-topic #18 captures IT-related words, while supertopic #3 is related to religion.

Section Title: EFFECTS OF MINIBATCH TRAINING
  EFFECTS OF MINIBATCH TRAINING Setup. We evaluate the effects of minibatch training on the classification accuracy on three citation networks by varying the batch size from 20 to 500.

Section Title: Results
  Results Confirming our theoretical analysis,  Figure 2  shows that the difference between the approx- imated loss and the true loss depends on the batch size. With increasing batch sizes, the accuracy on the Cora and Citeseer datasets increases as well. However, after an initial sharp increase, the differences become smaller. This shows the robustness of our approach to the batch size. Moreover, we observe a reversed trend on the Pubmed dataset, which is the largest among the citation graphs. We believe that this can be attributed to the stochasticity of minibatch training, which renders outlier nodes to be less important as the subgraphs can only cover parts of the whole graph.

Section Title: RELATION BETWEEN EMBEDDING SIZE AND NUMBER OF COMMUNITIES
  RELATION BETWEEN EMBEDDING SIZE AND NUMBER OF COMMUNITIES

Section Title: Setup
  Setup The aim of this experiment is two-fold. First, we aim to show the merit of the mincut loss. While the normcut prevents degenerate cases, the mincut loss is useful if we have prior knowledge about the graph structure. Second, we want to analyze the effect of the embedding size w.r.t the number of communities. For this experiment, we need a ground truth number of communities, which is why we rely on the Stochastic Block Model (SBM), a well-established benchmark for community detection ( Chen et al., 2019 ;  Fortunato & Hric, 2016 ;  Fortunato, 2010 ). Using SBM, a graph with a known number of communities (#com) is generated. Then, we construct node embeddings with varying dimensionality using the spectral approach for mincut loss with an additional balancing constraint on the community sizes. The parameters of SBM are discussed in detail in Appendix B. Node embeddings are assessed for link prediction with the ROC metric (avg over 100 runs). We chose two values for parameter p, the probability of an edge between two nodes in a community. Results.  Figure 3  confirms that "there is a sweet spot for the dimensionality, [..] neither too small, nor too large" ( Arora et al., 2016 ). Our results provide one possible explanation for the dimen- sionality trade-off. If the embedding size is smaller than the number of communities, unrelated communities are combined, lowering the ROC. If the embedding size is larger than the number of communities, communities are split up further. This also decreases the ROC, but not as drastically, since the model has higher capacity. In practice, the embedding quality tends to increase and then stabilize, with increasing embedding sizes, due to the communities often having different sizes.

Section Title: CONCLUSION
  CONCLUSION We presented a novel perspective on unsupervised learning of node embeddings. Following the idea of community detection, we proposed Deep MinCut (DMC), an approach to minimize the mincut loss function to learn node embeddings and communities simultaneously. DMC learns node embed- dings that are not only of high quality, but are also meaningful as they capture the graph's structure. We demonstrated the value of node embeddings learned with mincut loss in diverse experiments.

```
