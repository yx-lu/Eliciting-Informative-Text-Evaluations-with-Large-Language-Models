Title:
```
Published as a conference paper at ICLR 2020 BIOLOGICALLY INSPIRED SLEEP ALGORITHM FOR IN- CREASED GENERALIZATION AND ADVERSARIAL RO- BUSTNESS IN DEEP NEURAL NETWORKS
```
Abstract:
```
Current artificial neural networks (ANNs) can perform and excel at a variety of tasks ranging from image classification to spam detection through training on large datasets of labeled data. While the trained network may perform well on similar testing data, inputs that differ even slightly from the training data may trigger un- predictable behavior. Due to this limitation, it is possible to design inputs with very small perturbations that can result in misclassification. These adversarial at- tacks present a security risk to deployed ANNs and indicate a divergence between how ANNs and humans perform classification. Humans are robust at behaving in the presence of noise and are capable of correctly classifying objects that are noisy, blurred, or otherwise distorted. It has been hypothesized that sleep promotes gen- eralization of knowledge and improves robustness against noise in animals and humans. In this work, we utilize a biologically inspired sleep phase in ANNs and demonstrate the benefit of sleep on defending against adversarial attacks as well as in increasing ANN classification robustness. We compare the sleep al- gorithm's performance on various robustness tasks with two previously proposed adversarial defenses - defensive distillation and fine-tuning. We report an increase in robustness after sleep phase to adversarial attacks as well as to general image distortions for three datasets: MNIST, CUB200, and a toy dataset. Overall, these results demonstrate the potential for biologically inspired solutions to solve ex- isting problems in ANNs and guide the development of more robust, human-like ANNs.
```

Figures/Tables Captions:
```
Figure 1: FGSM classification accuracy as a function of noise added (Epsilon) for the three datasets tested: A) Patches B) MNIST C) CUB-200.
Figure 2: Sleep increases robustness to general distortions. A) Generalization classification accuracy for 5 networks for noise and blur on the MNIST dataset. B) Generalization classification accuracy for 4 networks for the noisy CUB-200 task. Note that there are only 4 networks because there is no blur task here.
Table 1: Adversarial Attack Scores (Best defense scores are bolded, lowest attack success rates are in blue)
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Although artificial neural networks (ANNs) have recently begun to rival human performance on var- ious tasks, ranging from complex games ( Silver et al. (2016) ) to image classification ( Krizhevsky et al. (2012) ), ANNs have been shown to underperform when the testing data differs in specific ways even by a small amount from the training data ( Geirhos et al. (2018) ). This lack of generalization presents two issues when ANNs are utilized in the real world. First, ANNs are often trained on cu- rated datasets of images designed to best capture the image content, whereas in real-world scenarios, they may be tested on disturbed or noisy inputs, not observed during training. Second, ANNs are susceptible to adversarial attacks, or the deliberate creation of inputs designed to fool ANNs that may be imperceptibly different from correctly classified inputs ( Szegedy et al. (2013) ). These two issues limit ANNs applicability in the real world and present potential security risks when deployed.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 There have been two main approaches for investigating ANN robustness: adversarial machine learn- ing and training data manipulation ( Ford et al. (2019) ). Adversarial machine learning aims to de- velop novel attack methods which perturb the input minimally while changing the ANN's classifica- tion outcome ( Moosavi-Dezfooli et al. (2016) ;  Carlini & Wagner (2017) ;  Goodfellow et al. (2014) ;  Athalye et al. (2017) ;  Nguyen et al. (2015) ) as well as to design defense mechanisms which prevent these attacks from affecting ANN behavior ( Papernot et al. (2016b) ;  Goodfellow et al. (2014) ;  Huang et al. (2015) , see  Yuan et al. (2019)  for review). Training data manipulation research typically ex- amines the impact of changing the input distribution during testing and observing the effect on ANN performance.  Geirhos et al. (2018)  showed that ANNs trained on images with one type of distortion may not perform well when tested on other types of distortions, even if images with both distortions appear identical to the human eye. Likewise, ANNs trained on unperturbed images exhibit reduced performance when images in the test set are distorted, for example, through horizontal translations, blurring, or the addition of compression artifacts ( Dodge & Karam (2016) ;  Vasiljevic et al. (2016) ;  Zhou et al. (2017) ). Although it has been proposed that adversarial and manipulation robustness can be increased through various mechanisms during the training phase, such as fine-tuning, recent research has shown that these methods are mostly ineffective or their effectiveness is inconclusive ( Geirhos et al. (2018) ;  Uesato et al. (2018) ;  Athalye et al. (2018) ). It has been hypothesized that in the mammalian brain sleep helps to create generalized represen- tations of an input learned during the awake state ( Stickgold & Walker (2013) ;  Lewis & Durrant (2011) ). Sleep has been identified as being critical for memory consolidation - a process of convert- ing recent memories into long-tern storage ( Rasch & Born (2013) ). During sleep, there is reacti- vation of neurons involved in previously learned activity ( Stickgold (2005) ) and this reactivation is likely to invoke the same spatio-temporal pattern of neuronal firing as the pattern observed during training in the awake state ( Wilson & McNaughton, 1994 ). Sleep reactivation, or replay, serves to strengthen synapses involved in a learned task through local synaptic plasticity, such as Spike Time Dependent Plasticity (STDP). Plastic changes during sleep can increase a subject's ability to form connections between memories and to generalize knowledge learned during the awake state ( Payne et al. (2009) ). In one study ( Wamsley et al. (2010) ), subjects learned to find an exit to a maze in a virtual 3D environment. Subjects who were allowed to sleep exhibited a more complex under- standing of the overall shape of the maze ( Wamsley et al. (2010) ). Using biophysical model of a cortical network  Gonzalez et al. (2019)  and  Wei et al. (2018)  showed that sleep dynamics promotes reactivation and helps to create distinct representations for unique memories by devoting synapses to specific memory traces. This body of neuroscience work suggests that a sleep-like activity may be applied to ANNs to enable the network to extract the gist of the training data without being con- strained by the statistics of a specific training data set. Our specific hypothesis is that sleep phase could aid in reducing a neural network's susceptibility to adversarial attacks and to increase gener- alization performance by reducing the impact that imperceptible input changes can have on the task output. In this new work, we propose a sleep-inspired algorithm to defend against adversarial attacks as well as to increase ANN robustness to noise. We utilize the notion of sleep from biology and apply an off-line unsupervised "sleep" phase to modify the parameters of a fully connected ANN. We demonstrate a number of performance improvements over existing defense algorithms, such as fine- tuning or adversarial retraining and defensive distillation, on both adversarial and noise robustness. The contributions are summarized below: • We analyze how robust the proposed sleep algorithm is to four different types of adversarial attacks on three different datasets (MNIST, CUB200, and a toy dataset). For most condi- tions (MNIST, toy dataset), after sleep phase was applied, the attacks consistently resulted in adversarial examples that were more distinct from the original input compared to the adversarial examples designed for the original (before sleep) network. • We illustrate that the sleep algorithm creates a more robust network whereby performance on noisy and blurred inputs is higher compared to control or defensively distilled network and is more robust to the other types of distortions compared to ANNs that are fine-tuned on a single distortion. • We analyze the impact of the sleep algorithm on task representation and demonstrate that the algorithm creates decision boundaries that more closely resemble the true classes, ef- fectively extracting the gist of the data.

Section Title: ADVERSARIAL ATTACKS AND DISTORTIONS
  ADVERSARIAL ATTACKS AND DISTORTIONS Adversarial attacks aim to create minimal perturbations that, while imperceptible to the human eye, fool ANNs. These attacks range from white-box to black-box attacks, based on how much infor- mation they assume the attacker to possess about the network. White-box attacks assume that the attacker has access to the network architecture, training data and weights. These attacks can range from absolute information, such as gradient-based attacks which compute the gradient of the loss with respect to the input ( Brendel et al. (2017) ), to score-based attacks which only utilize predicted scores of the model. Black-box attacks, which assume no knowledge about the network, solely rely on the decision made in order to craft adversarial examples. Attacks can be (a) targeted such that the attacker aims to create an adversarial example that the network would predict as a certain class or (b) untargeted where the attacker's goal is simply to cause any kind of misclassification ( Biggio & Roli (2018) ). In this work we consider four types of adversarial attacks ranging from white-box to black-box attacks. We assume that the attacker solely wants to cause a misclassification, with no respect to the output class. We present a brief description of each of the four attacks below (see Appendix for examples of images created by these attacks). Fast Gradient Sign Method (FGSM). FGSM ( Goodfellow et al. (2014) ) computes the sign of the gradient of the loss function (J) with respect to the original input x using the weights θ of the network and the target labels y. This represents the direction to change each pixel in the original input in order to increase the loss function. Based on the value of , the corresponding perturbation to the original image can range from small to large. Thus, in this work we use the average of the smallest values of needed to create an adversarial example x (misclassified input) for each input in the testing set. DeepFool. DeepFool ( Moosavi-Dezfooli et al. (2016) ) is an iterative method which approximates the nearest decision boundary to the input at time t and moves the input x t in that direction to compute x t+1 . This process is repeated until a misclassification is produced or the runtime of the simulation is exceeded. For this attack, we measure the L2-norm between the original input x and the adversarial input x . Thus, successful defenses should result in a high L2-norm for this algorithm. Jacobian-based Saliency Map (JSMA). JSMA ( Papernot et al. (2016a) ) aims to craft adversarial examples that minimize the L0-norm of x − x by reducing the number of pixels that are altered. In summary, the algorithm computes the gradient, as done in FGSM but for all possible classes. These gradient values represent how changing each pixel contributes to the overall loss function, with large values indicating a significant effect on the loss. These values are used to create a saliency map, where each pixel's impact on the loss is modelled. The algorithm utilizes this saliency map to alter individual pixels, repeating the gradient and saliency map computation until an adversarial example is created. For this type of attack, we utilize the L2-norm to determine defense success.

Section Title: Boundary Attack
  Boundary Attack The Boundary Attack ( Brendel et al. (2017) ) is a black-box attack which relies solely on the decision of the ANN to craft an adversarial example. Given an input x, a random input x 0 is chosen such that f (x) = f (x 0 ), where f (x) is the label produced by the ANN. In our work, x 0 is chosen from a uniform distribution. The attack starts by moving x 0 toward x until it reaches the point where f (x) = f (x 0 ), or the decision boundary in between f (x) and f (x 0 ). From here, the attack consists of two steps: an orthogonal perturbation and a forward perturbation. During the orthogonal perturbation, random points along the hypersphere around f (x t ) are sampled. Those that are adversarial and closer to x than before are added to the queue for forward perturbation. During the forward perturbation, a small step is taken from x t to x as long as f (x) = f (x t ). This process is repeated until a convergence criterion is met. For this attack, we utilize the L2-norm to define defense success.

Section Title: Distortions
  Distortions Although not specifically designed to attack an ANN, distortions negatively impact ANN performance. In this work we consider two simple distortion techniques: blurring and Gaus- sian noise. In first case, we perform 2-D Gaussian filtering with a blur kernel of varying standard deviation in order to blur the images. In second case, we add Gaussian noise with mean 0 and standard deviation σ. These distortions are tested in the networks implementing the proposed sleep algorithm as well as using the adversarial defenses discussed below.

Section Title: ADVERSARIAL DEFENSES
  ADVERSARIAL DEFENSES We compare our sleep algorithm with two existing adversarial defenses: defensive distillation and fine-tuning, or adversarial retraining. Defensive distillation ( Papernot et al. (2016b) ) utilizes two training sessions in order to create a distilled network. First, an initial network is trained on (X, Y ), where X is the training data, and Y is the one-hot encoded training labels. The activation function of this network is changed such that the softmax function of the output layer is computed using a temperature term T as follows: A higher T forces the ANN to produce larger probability values for each class, whereas lower T values support a similar representation as the one-hot encoded labels. After the first network is trained, the output of the network (probability values) is used to train a distilled network with the same softmax-temperature function. Previous work has shown this approach can be successful at preventing some types of attacks ( Papernot et al. (2016b) ). However, others have shown that it is not successful at defending against modified versions of those attacks or novel attacks in general ( Carlini & Wagner (2016 ; 2017)). Based on the previous work which found that temperature values between 20 and 100 effectively prevent adversarial attacks ( Papernot et al. (2016b) ), we use a temperature value of T = 50 in our implementation of defensive distillation. Adversarial retraining aims to fine-tune the network on adversarial examples with the correct labels as a form of regularization. Previous work has shown that adversarial retraining can mitigate the effectiveness of some adversarial attacks.  Goodfellow et al. (2014)  showed that adversarial retrain- ing can reduce the error rate on MNIST, demonstrating greater ANN robustness after fine-tuning. Likewise,  Moosavi-Dezfooli et al. (2016)  showed that fine-tuning on DeepFool attacks can reduce the effectiveness of their attacks. However, they observed that fine-tuning on FGSM attacks has negative results, actually increasing the strength of the attack. This suggests that fine-tuning may overfit the network to certain attacks, while failing to extrapolate to other attacks, similar to results shown for generalization in ANNs ( Geirhos et al. (2018) ). For the adversarial retraining procedure presented here, we train the network on the original input and then fine-tune the network on various adversarial attacks with a reduced learning rate.

Section Title: SLEEP ALGORITHM
  SLEEP ALGORITHM The basic intuition behind the sleep algorithm is that a period of offline activity, whereby network weights are modified according to an unsupervised learning algorithm, allows the parameters of the network to become more reflective of the underlying statistics of the task at hand, while not overfitting the statistics of the training data. The pseudocode is presented in Algorithm 1. In short, an ANN is trained using stochastic gradient descent and the standard backpropagation algorithm (exact parameters used for each of the datasets are shown in Table 2). After training, the network structure is converted into a spiking neural network (SNN). After building the SNN, we run a sleep phase which modifies the network connectivity based on spike-timing dependent plasticity (STDP). After the sleep phase, the SNN network is converted back into the ANN and testing is performed.

Section Title: SPIKING NEURAL NETWORKS
  SPIKING NEURAL NETWORKS SNNs seek to model closely temporal brain dynamics. In short, SNNs are composed of spiking neurons and model the information transformation and the dependence on exact timing of spikes that occurs in biological networks ( Ghosh-Dastidar & Adeli (2009) ). Individual neuron models can range from simple integrate-and-fire type neurons which sum their inputs and produce an output (spike) if this exceeds some firing threshold to more complex Hodgkin-Huxley type neurons which model sodium-, potassium-, and chloride-channel kinetics ( Abbott & Kepler (1990) ). Recent work has shown that a near loss-less conversion between ANNs and SNNs can be achieved by propagating activity through a spiking neural network for a given input and counting the number of times that each output neuron fires ( Diehl et al. (2015) ).

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 To convert an ANN to SNN (Lines 1-3 of pseudocode), we assume the ANN utilizes ReLU neurons with no bias. This assumption is made so that the output neuron's activation can be treated as a firing rate, either zero or positive, and that the thresholds of all neurons in a given layer are of the same scale. The weights from the ANN are directly mapped to the SNN. In our analysis, each unit in the SNN is modelled as an integrate-and-fire type neuron, computing the following equation: Here, τ m represents the decay constant of the membrane potential, v is the voltage at a given time, w i is the weight connecting from neuron i, and s(i) is the spiking activity of neuron i, either 1 or 0.

Section Title: PLASTICITY AND SLEEP
  PLASTICITY AND SLEEP The key advantage of using a SNN is that biologically inspired training rules can be applied while the network is driven by noisy input. Empirical data suggest that the brain uses spike-timing dependent plasticity (STDP) ( Song et al., 2000 ), where weight updates depend on the relative timing of pre- and post-synaptic spikes. It has been shown that STDP results in balanced activity, where all neurons fire in equal proportions ( Song et al. (2000) ). Here we utilize a modified version of STDP: if a pre- synaptic spike induces a post-synaptic spike, then the weight between these neurons is increased. If a post-synaptic spike occurs, but the pre-synaptic neuron does not spike, then the corresponding weight is decreased (in this case postsynaptic spiking may occur because of spiking in other neurons connecting to that post-synaptic neuron). The sleep training phase we propose here can be described as following. First, inputs to each neu- ron of the input layer must be presented as spiking activity in order to propagate activity from the input layer to the hidden layers of the network. We convert inputs (real-valued pixel intensities or features) to spikes by defining a maximum firing rate f max with units spikes sec and computing a Poisson-distributed spike raster, such that inputs with higher values (i.e. brighter pixels) will have higher rate than inputs with lower values, with no spike rates exceeding f max . Next, activity is prop- agated through the network as spikes and the STDP rule is applied to update weights. In biological networks, increase of synaptic strength during slow-wave sleep leads to characteristic patterns of ac- tivity with repetitive periods of elevated firing (Up-states), when previously learned memory traces are spontaneously replayed. To simulate this dynamics, synaptic weights in SNN are up-scaled to induce high firing rates in later layers. Other important parameters include the threshold for each layer and the length of sleep. The parameters used for each dataset are presented in Table 3.

Section Title: EXPERIMENTS AND DATASETS
  EXPERIMENTS AND DATASETS Below, we describe the general experimental setup as well as the datasets tested. First, we trained a control ANN using the training set for each of the main datasets used in this study. Next, we created a defensively distilled network using T = 50 for the temperature parameter to create the second test network. Then, we fine-tuned the control ANN on a specific attack or distortion method to create the third test network. Finally, we converted the control ANN to an SNN and applied the sleep algorithm as described above to create the fourth test network. We created adversarial examples for each of these four networks using the attacks we described above (fine-tuned networks are tested on the attacks they were fine-tuned on). Then, we analyze how successful each attack is to fool each of the four networks using the metrics defined above. For generalization (blur and noise), we performed the same setup as above creating four different networks. We then tested each network on varying levels of distortion. We tested networks fine-tuned on blurred and noisy images to measure how performance generalizes across distortion methods. We averaged performance across a minimum of three networks for each attack and distortion. We used three datasets to compare performance: Patches (a toy dataset created simply for analysis), MNIST ( LeCun et al. (1998) ), and CUB-200 ( Welinder et al. (2010) ). Patches consists of four binary images arranged in a 10x10 square. Each image has its own label (1-4), and consists of 25 bright pixels (value set to 1) and 75 dark pixels. The overlap of bright pixels among the four images (see Appendix] Appendix) is chosen such that the task is not trivial. The MNIST dataset consists of 70,000 28x28 greyscale images of handwritten digits, with 60,000 in the training set and 10,000 in the testing set. CUB-200 is a high resolution dataset of images of birds with 200 bird species, Published as a conference paper at ICLR 2020

Section Title: RESULTS
  RESULTS We evaluate the sleep algorithm in two settings: (1) Adversarial attacks designed to fool neural networks and (2) generalization distortions designed to reflect imperfect viewing conditions or other types of noise. For adversarial attacks (other than FGSM), we utilize the following metric to evaluate the success of each defense. Let x i be the adversarial example created for input x i . The total score S A for an attack is the median squared L2-distance for all samples, where N is the dimension of the space: For FGSM, we define the following metric which computes the median minimum noise level needed to produce a misclassification across all samples: For MNIST and CUB-200, we evaluate the attacks on all examples in the testing set. Examples that the networks get wrong before the attack was implemented are discarded from the analysis (in these cases x i − x 2 k = 0 and i = 0 for all attacks). For FGSM and distortions, we also include plots of classification accuracy as a function of noise level. For DeepFool and JSMA, we report adversarial attack accuracy (number of examples where f (x) = y and f (x ) = f (x), where y is the correct label, over number of examples tested). Note that these algorithms would always produce an adversarial example if allowed to run forever. However, due to computational limitations, we included a run-time limits on the number of iterations for these algorithms (see Appendix). Thus, a lower adversarial attack accuracy indicates that the attack would need more iterations to run to reach 100% accuracy. This is a similar measure as distance since more iterations would result in more distinct adversaries for all attacks implemented and the updates at each iteration have the same magnitude for each defense.

Section Title: ADVERSARIAL ATTACKS
  ADVERSARIAL ATTACKS Here we report the scores for all different attacks and for the all datasets. For the FGSM attack, the sleep algorithm increases the median minimum noise needed for misclassification for all three datasets compared to the control network (also see  Figure 1 ). For MNIST dataset, the amount of noise needed to fool the network after the sleep algorithm was almost double of that needed for either the fine-tuning or defensive distillation approaches. For the Patches dataset, both defensive distillation and fine-tuning increase the robustness of the network. However, on CUB-200, only fine-tuning and sleep were able to defend, albeit marginally, against the FGSM attack. Looking at the classification accuracy of the network as a function of noise added ( ,  Figure 1 ), we observe that in the Patches and CUB-200 dataset, sleep tends to have higher classification accuracy than the other methods for epsilon < 0.1. After this point, sleep tends to have equal classification accuracies as compared to the other methods. For MNIST, the baseline classification accuracy on the original test set decreases slightly compared to the other methods (80% after sleep). However, the performance remains high longer than for the other defense methods on images that were correctly classified. We observed that performance continued to drop after a sufficiently large amount of noise was added. This is biologically plausible as adding more noise to an image should result in image degradation and misclassifications. In sum, these results indicate that a sleep phase can successfully mitigate FGSM, more so than a control network. For DeepFool, sleep has a significant effect on the defense score on the MNIST dataset, both re- ducing the attack success rate and increasing the distance between the adversarial example and the original input by an order of magnitude. For Patches and CUB-200 this effect is less pronounced, with fine-tuning or the control network performing better. We hypothesize that sleep was ineffective in preventing the DeepFool attack in tasks with very few exemplars per class (Patches) or a large number of classes (CUB-200). In CUB-200, there is a large number of classes so the distance be- tween the input and the nearest decision boundary is smaller (this is supported by the fact that JSMA, an L0 attack, does worse than DeepFool for CUB-200 and vice versa for MNIST, control networks). In this case, sleep is unable to move the decision boundary of one class without impinging on the decision space of another class. In MNIST, where the decision space for one class is presumably larger, sleep can alter decision boundaries in a way that has a minimal effect on other classes. Sleep successfully increases the network's robustness to the JSMA attacks on MNIST and Patches, reducing the attack success rate in the case of MNIST and increasing the distance needed to create an adversary for Patches. On CUB-200, there is a marginal reduction in the adversarial attack accuracy compared to the control network. Defensive distillation and fine-tuning also reduce JSMA's effectiveness. However, for these two defenses, in the case of MNIST, the networks were capable of finding an adversary for a higher percentage of the testing set. Thus, the effect of changing a small number of important pixels is mitigated after running the sleep algorithm. For the Boundary Attack, we found that no defense mechanism helps compared to the control in decreasing the attack's effectiveness on the MNIST dataset. However, for CUB-200 and Patches, the sleep algorithm results in a higher defense score than that for the control network. This lends support to the idea that sleep favorably alters decision boundaries so that it becomes harder to find an adversarial example that is close to the original image after the sleep phase. This also suggests that sleep is not simply obfuscating gradients, which has been a common criticism of several adversarial defenses ( Athalye et al. (2018) ), which are tested on white-box attacks. In fact, given the long run-time for convergence of this algorithm, if we define a threshold for adversarial attack success (L2-norm > 1), then sleep successfully defends against this attack on the MNIST dataset (see Table 4). Why does sleep phase help? It has been shown that sleep tends to promote an increase in stronger weights while pruning weaker weights, thus increasing the width of the weights' distribution ( Gon- zalez et al., 2019 ). This results in the consolidation of strong memories at the cost of diminishing weak memories. From this point of view, a memory is a subspace or abstraction in the decision space corresponding to a given class. Sleep may result in enlarging the subspace the network allocates to a stronger category while shrinking weaker ones (Figure 5A). The process of strengthening the strong memory also results in making it robust and noise invariant, as seen in Figure 5B where the first 8 categories (numbers 0-7) are strengthened and become more invariant to the FGSM attack, while the last two digits are essentially forgotten and the network cannot confidently predict exemplars from these classes (Figure 5C). If the noise is less targeted, as in the case of random noise or blurring, sleep does not need to alter the decision space as much to produce better generalization and can maintain a high baseline accuracy, as we demonstrate in the next section.

Section Title: GENERALIZATION
  GENERALIZATION   Figure 2  shows the network performance for noisy and blurry distortions of data for MNIST (A) as well as noisy distortions for the CUB-200 feature embeddings (B, see Figure 3 for results on Patches). Overall, fine-tuning on an image distortion results in the best performance for that specific distortion. However, as was noted ( Geirhos et al. (2018) ), fine-tuning on a specific distortion does not extend to other types of distortions. In our analysis, fine-tuning the network on blurred MNIST images results in high performance (> 80%) on blurred images. However, for noisy images, this performance was only marginally above the control network. The sleep algorithm increased perfor- mance for both distortion methods, since this approach is not tailored to any one representation of the training set. Finally we tested how sleep increases robustness on blur and noise distortions. In biological systems, sleep increases generalization through replay of memories learned during awake which leads to changes in synaptic weights. These changes entail both an increase in synaptic weights associated with a specific task and pruning of synapses involved in other tasks (Gonzalez et al., 2019;  Tononi & Cirelli, 2006 ). Figures 9 and 10 show that correlations among like digits in the hidden layers of the network are greater after applying sleep than before for noisy and blurred images. Likewise, pairs of different digits usually become decorrelated after sleep, suggesting synaptic pruning. We also show that both normalized spiking activity and activations of digit-specific neurons are higher after sleep than before (Figures 11 and 12, see Appendix for details). These results suggest that the sleep algorithm increases robustness through biologically plausible learning mechanisms involving replay of relevant activity during sleep phase.

Section Title: CONCLUSIONS AND FUTURE DIRECTIONS
  CONCLUSIONS AND FUTURE DIRECTIONS In this work, we show that a biologically inspired sleep algorithm can increase an ANN's robust- ness to both adversarial attacks and general image distortions. The algorithm augments the normal (e.g., back-propagation based) training phase of an ANN with an unsupervised learning phase in the equivalent SNN modelled after how the biological brain utilises sleep to improve learning. We hypothesize that the unsupervised sleep phase creates more natural feature representations which in turn lead to more natural decision boundaries, thus increasing the robustness of the network. Al- though this robustness may come at a cost of overall accuracy, it has been shown that robustness may have multiple important benefits, such as more salient feature representations as well as invariance to input modifications ( Tsipras et al. (2018) ). We also show that the trade-off between robustness and accuracy does not always occur, particularly for image distortions such as noise or blur. Future work includes converting the sleep algorithm into a regularization technique to be applied in more Published as a conference paper at ICLR 2020 standardized machine learning frameworks as well as understanding the theoretical basis for the beneficial role of spike based plasticity rules in increasing network robustness.

Section Title: ACKNOWLEDGEMENTS
  ACKNOWLEDGEMENTS

```
