Title:
```
Under review as a conference paper at ICLR 2020 BEHAVIOR-GUIDED REINFORCEMENT LEARNING
```
Abstract:
```
We introduce a new approach for comparing reinforcement learning policies, using Wasserstein distances (WDs) in a newly defined latent behavioral space. We show that by utilizing the dual formulation of the WD, we can learn score functions over trajectories that can be in turn used to lead policy optimization towards (or away from) (un)desired behaviors. Combined with smoothed WDs, the dual formulation allows us to devise efficient algorithms that take stochastic gradient descent steps through WD regularizers. We incorporate these regularizers into two novel on-policy algorithms, Behavior-Guided Policy Gradient and Behavior-Guided Evolution Strategies, which we demonstrate can outperform existing methods in a variety of challenging environments. We also provide an open source demo 1 .
```

Figures/Tables Captions:
```
Figure 1: Behavioral Embedding Maps (BEMs) map trajectories to points in the behavior embedding space E. Two trajec- tories may map to the same point in E.
Figure 2: Behavioral embedding func- tions corresponding to two policies π1 (green) and π2 (blue) whose BEMs map trajectories to points in the real line.
Figure 3: BGPG vs. TRPO: We compare BGPG and TRPO (KL divergence) on several continuous control tasks. As a baseline we also include results without a trust region (β = 0 in Algorithm 2). Plots show the mean ± std across 5 random seeds. BGPG consistently outperforms other methods.
Figure 4: The clock-time comparison (in sec) of BGPG (alternating optimization) with particle approximation.
Figure 5: Efficient Exploration. On the left we show a visualization of the simulated environment, with the deceptive barrier between the (quadruped) agent and the goal. On the right, we show two plots with the median curve across five seeds, with the IQR shaded for the quadruped and point environment respectively.
Figure 6: Escaping Local Maxima. A comparison of BGES with those using different distances on PPEs.
Figure 7: Imitation Learning.
Figure 8: Choice of BEM
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION One of the key challenges in reinforcement learning (RL) is to efficiently incorporate the behavioral characteristics of learned policies into optimization algorithms (Lee & Popovic, 2010; Meyerson et al., 2016; Conti et al., 2018). The fundamental question we aim to shed light on in this paper is: What is the right measure of similarity between two policies acting on the same underlying MDP and how can we devise algorithms to leverage this information for reinforcement learning? In simple terms, the main thesis motivating the methods we propose is that: Two policies may perform similar actions at a local level but result in very different global behaviors. We propose to define behaviors via so-called Behavioral Embedding Maps (BEMs), which are functions mapping trajectories (realizations of policies) into latent behavioral spaces representing trajectories in a compact way. BEMs enable us to identify policies with their Probabilistic Policy Embeddings (PPEs), which we define as the pushforward distributions over trajectory embeddings as a result of applying a BEM to a policy's trajectories. Importantly, two policies with distinct distributions over trajectories may result in the same probabilistic embedding. PPEs provide us a way to rigorously define dissimilarity between policies. We do this by equipping them with metrics defined on the manifold of probabilistic measures, namely a class of Wasserstein distances (WDs, Villani (2008)). There are several reasons for choosing WDs: • Flexibility. We can use any cost function between embeddings of trajectories, allowing the distance between PPEs to arise organically from an interpretable distance between embedding points. • Non-injective BEMs. Different trajectories may be mapped to the same embedding point (for example in the case of the last-state embedding). This precludes the use of likelihood-based distances such as the KL divergence (Kullback & Leibler, 1951), which we discuss in Section 6. • Behavioral Test Functions. Solving the dual formulation of the WD objective yields a pair of test functions over the space of embeddings that can be used to score trajectories. The behavioral test functions underpin all our algorithms, directing optimization towards desired behaviors. To learn them it suffices to define the BEM and the cost function between points in the PPE space. To mitigate the computational burden of computing WDs, we rely on their entropy-regularized formulations. This allows us to update the learned test functions in a computationally efficient manner Under review as a conference paper at ICLR 2020 via stochastic gradient descent (SGD) on a Reproducing Kernel Hilbert Space (RKHS). We develop a novel method for stochastic optimal transport based on random feature maps (Rahimi & Recht, 2008) to produce compact and memory-efficient representations of learned behavioral test functions. Finally, having laid the groundwork for comparing trajectories via behavior-driven trajectory scores, we address our core question by introducing two new on-policy RL algorithms: • Behavior Guided Policy Gradients (BGPG): We propose to replace the KL-based trust region from Schulman et al. (2015) with a WD-based trust region in the PPE space. • Behavior Guided Evolution Strategies (BGES): Inspired by the NSR-ES algorithm from Conti et al. (2018), BGES jointly optimizes for reward and novelty using the WD in the PPE space. In addition, we also demonstrate a way to harness our methodology for imitation learning (Section 7.3) and repulsion learning (Section 9.4), and we believe there may be many more potential applications in the future.

Section Title: MOTIVATING BEHAVIOR-GUIDED REINFORCEMENT LEARNING
  MOTIVATING BEHAVIOR-GUIDED REINFORCEMENT LEARNING Throughout this paper we prompt the reader to think of a policy as a distribution over its trajectories, induced by the policy's (possibly stochastic) map from state to actions and the unknown environment dynamics. We care about summarizing (or embedding) trajectories into succinct representations that can be compared with each other (via a cost/metric). These comparisons arise naturally when answering questions such as: Has a given trajectory achieved a certain level of reward? Has it visited a certain part of the state space? We think of these summaries or embeddings as characterizing the behavior of the trajectory. We formalize these notions in Section 3. We show that by identifying policies with the embedding distributions that result of applying the embedding function (summary) to their trajectories, and combining this with the provided cost metric, we can induce a topology over the space of policies given by the WD over their embedding distributions. The methods we propose can be thought of as ways to leverage this "behavior" geometry for a variety of downstream applications such as policy optimization and imitation learning. This topology emerges naturally from the sole definition of an embedding map (behavioral summary) and a cost function. Crucially these choices occur in the semantic space of behaviors as opposed to parameters or visitation frequencies 2 . One of the advantages of choosing a Wasserstein geometry is that non-surjective trajectory embedding maps are allowed. This is not possible with a KL induced one (in non-surjective cases, computing the likelihood ratios in the KL definition is in general intractable). In Sections 4 and 5 we show that in order to get a handle on this geometry we can use the dual formulation of the Wasserstein distance to learn functions (Behavioral Test Functions) that can provide scores on trajectories which then can be added to the reward signal (in policy optimization) or used as a reward (in Imitation Learning). In summary, by defining an embedding map of trajectories into a behavior embedding space equipped with a metric 3 , our framework allows us to learn "reward" signals (Behavioral Test Functions) that can serve to steer policy search algorithms through the "behavior geometry" either in conjunction with a task specific reward (policy optimization) or on their own (e.g. Imitation Learning). We develop versions of on policy RL algorithms which we call Behavior Guided Policy Gradient (BGPG) and Behavior Guided Evolution Strategies (BGES) that enhance their baseline versions by the use of learned Behavioral Test Functions. Our experiments in Section 7 show this modification is useful. We also show how to use Behavioral Test Functions in Imitation Learning, where we only need access to an expert's embedding. Although our framework also has obvious applications to safety, (learning policies that avoid undesirable or dangerous behaviors) we leave this for future work. We also consider simple heuristics for the embeddings (inspired by other existing use cases), but believe future work on learned embeddings could be a significant enhancement.

Section Title: DEFINING BEHAVIOR IN REINFORCEMENT LEARNING
  DEFINING BEHAVIOR IN REINFORCEMENT LEARNING A Markov Decision Process (MDP) is a tuple (S, A, P, R). Here S and A stand for the sets of states and actions respectively, such that for s, s ∈ S and a ∈ A: P(s |a, s) is the probability that the system/agent transitions from s to s given action a and R(s , a, s) is a reward obtained by an agent transitioning from s to s via a. A policy π θ : S → A is a (possibly randomized) mapping (parameterized by θ ∈ R d ) from S to A. Let Γ = {τ = s 0 , a 0 , r 0 , · · · s H , a H , r H s.t. s i ∈ S, a i ∈ A, r i ∈ R} be the set of possible trajectories enriched by sequences of partial rewards under some policy π. The undiscounted reward function R : Γ → R (which expectation is to be maximized by optimizing θ) satisfies R(τ ) = H i=0 r i , where r i = R(s i+1 , a i , s i ). We start with a Behavioral Embeddng Space (BES) which we denote as E and a Behavioral Embedding Map (BEM), Φ : Γ → E, mapping trajectories to embeddings in E ( Fig. 1 ). Importantly, the mapping does not need to be surjective. We will provide examples of BESs and BEMs at the end of the section. Given a policy π, we let P π denote the distribution induced over the spaces of trajectories Γ and by P Φ π the corresponding pushforward distribution on E induced by Φ. We call P Φ π the Probabilistic Policy Embedding (PPE) of a policy π. A policy π can be fully characterized by the distribution P π . Additionally, we require the BES E to be equipped with a metric (or cost function) C : E × E → R. Given two trajectories τ 1 , τ 2 in Γ, C(Φ(τ 1 ), Φ(τ 2 )) measures how different these trajectories are in the behavior space. The following are examples of BEMs (with the corresponding BESs) categorized into three main types (we will use examples from all three types in our experiments in Section 7): 1. State-based: the final state Φ 1 (τ ) = s H , the visiting frequency of a fixed state Φ s 2 (τ ) = H t=0 1(s t = s), the frequency vector of visited states Φ 3 (τ ) = H t=0 e st (where e s ∈ R |S| is the one-hot vector corresponding to state s); see also Section 7.2. 2. Action-based: the concatenation of actions Φ 4 (τ ) = [a 0 , ..., a H ]; see also Section 7.1. 3. Reward-based: the total reward Φ 5 (τ ) = H t=0 r t , reward-to-go vector Φ 6 (τ ) = H t=0 r t t i=0 e i (where e i ∈ R H+1 is a one-hot vector corresponding to i and with dimensions indexed from 0 to H); see also Section 7.1 and Section 7.3. For instance, P Φ3 π is the frequency with which different states are visited under π. Note that some of the above embeddings are only for the tabular case (|S|, |A| < ∞) while others are universal.

Section Title: WASSERSTEIN DISTANCE & OPTIMAL TRANSPORT PROBLEM
  WASSERSTEIN DISTANCE & OPTIMAL TRANSPORT PROBLEM Let µ, ν be (Radon) probability measures over domains X ⊆ R m , Y ⊆ R n and let C : X × Y → R be a cost function. For γ > 0, a smoothed Wasserstein Distance is defined as: where Π(µ, ν) is the space of couplings (joint distributions) over X × Y with marginal distributions µ and ν, KL(·|·) denotes the KL divergence between distributions π and ρ with support X × Y defined as: KL(π|ρ) = X ×Y log dπ dξ (x, y) dπ(x, y) and ξ is a reference measure over X × Y. When the cost is an p distance and γ = 0, WD γ is also known as the Earth mover's distance and the corresponding optimization problem is known as the optimal transport problem (OTP).

Section Title: WASSERSTEIN DISTANCE: DUAL FORMULATION
  WASSERSTEIN DISTANCE: DUAL FORMULATION We will use smoothed WDs to derive efficient regularizers for RL algorithms. To arrive at this goal, we first need to consider the dual form of Equation 1. Under the subspace topology (Bourbaki, Under review as a conference paper at ICLR 2020 Algorithm 1 Random Features Wasserstein SGD Input: kernels κ, over X , Y respectively with corresponding random feature maps φ κ , φ , smooth- ing parameter γ, gradient step size α, number of optimization rounds M , initial dual vectors p µ 0 , p ν 0 . Return: p µ M , p ν M . 1966) for X and Y, let C(X ) denote the space of continuous functions on X and let C(Y) denote the space of continuous functions over Y. The choice of the subspace topology ensures our discussion encompasses the discrete case. Let C : X × Y → R be a cost function, interpreted as the "ground cost" to move a unit of mass from x to y. Define I as the (0, ∞) indicator function, where the value 0 denotes set membership. Using Fenchel duality, we can obtain the following dual formulation of the problem in Eq. 1: where E C (λ µ , λ ν ) is defined as: We will set dξ(x, y) ∝ 1 for discrete domains and dξ(x, y) = dµ(x)dν(y) otherwise. If λ * µ , λ * ν are the functions achieving the maximum in Eq. 2, and γ is sufficiently small then In this case the difference between E µ [λ * (x)] and E µ [λ * (y)] equals the WD. In other words, the function λ * gives higher scores to regions of the space X where µ has more mass. This observation is key to the success of our algorithms in guiding optimization towards desired behaviors. We combine several techniques to make the optimization of objective from Eq. 2 tractable. First, we replace X and Y with the functions from a RKHS corresponding to universal kernels (Micchelli et al., 2006). This is justified since those function classes are dense in the set of continuous functions of their ambient spaces. In this paper we choose the Gaussian kernel and approximate it using random Fourier feature maps (Rahimi & Recht, 2008) to increase efficiency. Consequently, the functions λ learned by our algorithms have the following form: λ(x) = (p λ ) φ(x), where φ is a random feature map with m standing for the number of random features and p λ ∈ R m . For the Gaussian kernel, φ is defined as follows: φ(z) = 1 √ m cos(Gz + b) for z ∈ R d , where G ∈ R m×d is Gaussian with iid entries taken from N (0, 1), b ∈ R m with iid b i s such that b i ∼ Unif[0, 2π] and the cos function acts elementwise. Henceforth, when we refer to optimization over λ, we mean optimizing over corresponding dual vectors p λ associated with λ. We can solve for the optimal dual functions by performing SGD over the dual objective in Eq. 2. Algorithm 1 is the random features equivalent of Algorithm 3 in Genevay et al. (2016) and will be a prominent subroutine of our methods. An explanation and proof of why this is the right stochastic gradient is in Lemma 10.2 in the Appendix. Under review as a conference paper at ICLR 2020 If p µ * , p ν * are the optimal dual vectors and (x 1 , y 1 ), · · · , (x k , y k ) i.i.d ∼ µ ν, then Algorithm 1 can be used to get an estimator of WD γ (µ, ν) as follows:

Section Title: BEHAVIOR-GUIDED REINFORCEMENT LEARNING
  BEHAVIOR-GUIDED REINFORCEMENT LEARNING Here we introduce the framework which allows us to incorporate our behavioral approach to rein- forcement learning into practical on-policy algorithms. Denote by π θ a policy parameterized by θ ∈ R d . The goal of policy optimization algorithms is to find a policy maximizing, as a function of the policy parameters, the expected total reward L(θ) := E τ ∼Pπ θ [R(τ )].

Section Title: BEHAVIORAL TEST FUNCTIONS
  BEHAVIORAL TEST FUNCTIONS If C : E × E → R is a cost function defined over behavior space E, and π 1 , π 2 are two policies, then: Γ → R define score functions over the space of trajectories. If γ is close to zero, the score function s i gives higher scores to trajectories from π i whose behavioral embedding is common under π i but rarely appears under π j for j = i ( Fig. 2 ).

Section Title: ALGORITHMS
  ALGORITHMS We propose to solve a WD-regularized objective to tackle behavior-guided policy optimization. All of our algorithms hinge on trying to maximize an objective of the form: F (θ) = L(θ) + βWD γ (P Φ π θ , P Φ b ), (6) where P Φ b is a base distribution over behavioral embeddings (possibly dependent on θ) and β ∈ R could be positive or negative. Although the base distribution P Φ b could be arbitrary, our algorithms will instantiate P Φ b = 1 |S| ∪ π ∈S P Φ π for some family of policies S (possibly satisfying |S| = 1) we want the optimization to attract to / repel from. In order to compute approximate gradients for F , we rely on the dual formulation of the WD. After substituting the composition maps resulting from Eq. 5 into Eq. 2, we obtain: F (θ) ≈ E τ ∼Pπ θ [R(τ ) + βs 1 (τ )] − βE φ∼P Φ b [λ * 2 (φ)] , (7) where s 1 : Γ → R equals s 1 = λ * 1 •Φ, the Behavioral Test Function of policy π θ and λ * 2 is the optimal dual function of embedding distribution P Φ b . Consequently ∇ θ F (θ) ≈ ∇ θ E τ ∼Pπ θ [R(τ ) + βs 1 (τ )]. We learn a score function s 1 over trajectories that can guide our optimization by favoring those trajectories that show desired global behaviors. Eq. 7 is an approximation to the true objective from Eq. 2 whenever γ > 0. In practice, the entropy regularization requires a damping term as defined in Equation 3. If ξ(P Φ π θ , P Φ b ) is the joint distribution of choice then F (θ) = L(θ) + βV for When the embedding space E is not discrete All of our methods perform a version of alternating SGD optimization: we take certain number of SGD steps over the internal dual Wasserstein objective, followed by more SGD steps over the outer objective having fixed the current dual functions. Although in practice the different components Under review as a conference paper at ICLR 2020 that make up the optimization objectives we consider here could be highly nonconvex, in the cases these functions satisfy some convexity assumptions, we can provide a sharp characterization for the convergence rates of our algorithms. Details are given in Section 10 in the Appendix. We consider two distinct approaches to optimizing this objective, by exploring in the action space and backpropagating, as in policy gradient methods (Schulman et al., 2015; 2017), and by considering a black-box optimization problem as in Evolution Strategies (ES, Salimans et al. (2017)). These two different approaches lead to two new algorithms: Behavior-Guided Policy Gradient (BGPG) and Behavior-Guided Evolution Strategies (BGES), that we discuss next.

Section Title: BEHAVIOR-GUIDED POLICY GRADIENT (BGPG)
  BEHAVIOR-GUIDED POLICY GRADIENT (BGPG) Our first algorithm seeks to solve the optimization problem in Section 5.2 with policy gradients. We refer to this method as the Behavior-Guided Policy Gradient (BGPG) algorithm (see Algorithm 2 below). 1. Run π t−1 in the environment to get advantage values A πt−1 (s, a) and trajectories {τ (t) i } M i=1 2. Update policy and test functions via several alternating gradient steps over the objective: c. Use samples fromP πt−1,π θ and Algorithm 1 to update λ 1 , λ 2 . Specifically, we maintain a stochastic policy π θ and compute policy gradients as in prior work (Schulman et al., 2015). To optimize the Wasserstein distance WD γ , we approximate the gradient of this term via the random-feature Wasserstein SGD . Importantly, this stochastic gradient can be approximated by samples collected from the policy π θ . In its simplest form, the∇ θF in Step b. in Algorithm 2 can be computed by the vanilla policy gradient over the advantage component and using the reinforce estimator through the components involving Behavioral Test Functions acting on trajectories from P π θ . We explain in Appendix 8.1 a lower-variance gradient estimator alternative. BGPG can be thought of as a variant of Trust Region Policy Optimization with a Wasserstein penalty. As opposed to vanilla TRPO, the optimization path of BGPG flows through policy parameter space while encouraging it to follow a smooth trajectory through the geometry of the PPE space. We proceed to show that given the right embedding and cost function, we can prove a monotonic improvement theorem for BGPG, showing that our methods satisfy at least similar guarantees as TRPO. For a given policy π, we denote as: V π , Q π and A π (s, a) = Q π (s, a) − V π (s) the: value function, Q-function and advantage function (see Appendix: Section 10.5). Furthermore, let V (π) be the expected reward of policy π and ρ π (s) = E τ ∼Pπ T t=0 1(s t = s) be the visitation measure. Two distinct policies π andπ can be related via the equation (see: Sutton et al. (1998)) V (π) = V (π)+ S ρπ(s) Aπ (a|s)A π (s, a)da ds and the linear approximations to V around π via: L(π) = Under review as a conference paper at ICLR 2020 V (π) + S ρ π (s) Aπ (a|s)A π (s, a)da ds (see: Kakade & Langford (2002)). Let S be a finite set. Consider the following embedding Φ s : Γ → R |S| defined by (Φ(τ )) s = T t=0 1(s t = s) and related cost function defined as: C(v, w) = v − w 1 . Then WD 0 (P Φ s π , P Φ s π ) is related to visitation frequencies since WD 0 (P Φ s π , P Φ s π ) ≥ s∈S |ρ π (s) − ρπ(s)| (see Section 10.5 for the proof). These observations enable us to prove an analogue of Theorem 1 from Schulman et al. (2015), namely: As in Schulman et al. (2015), Theorem 5.1 implies a policy improvement guarantee for BGPG.

Section Title: BEHAVIOR GUIDED EVOLUTION STRATEGIES (BGES)
  BEHAVIOR GUIDED EVOLUTION STRATEGIES (BGES) ES takes a black-box optimization approach to RL, by considering a rollout of a policy, parameterized by θ as a black-box function F . This approach has gained in popularity recently (Salimans et al., 2017; Mania et al., 2018; Choromanski et al., 2019). If we take this approach to optimizing the objective in Eq. 2, the result is a black-box optimization algorithm which seeks to maximize the reward and simultaneously maximizes or minimizes the difference in behavior from the base embedding distribution P Φ b . We call this method the Behavior-Guided Evolution Strategies (BGES) algorithm (see Algorithm 3 below). 1. Sample 1 , · · · , n independently from N (0, I). 2. Evaluate policies {π k t } n k=1 parameterized by {θ t + σ k } n k=1 to return rewards R k and trajecto- ries τ k for all k. 3. Use BEM to map trajectories τ k to produce empirical PPEsP Φ π k t for all k. 4. Update λ 1 and λ 2 using Algorithm 1, where µ = 1 n ∪ n k=1P Φ π k t−1 and ν = 1 n ∪ n k=1P Φ π k t are the uniform distribution over the set of PPEs from 3 for t − 1 and t. 5. Approximate WDγ(P Φ π k t , P Φ πt ) plugging in λ 1 , λ 2 into Eq. 4 for each perturbed policy π k 6. Update Policy: θ t+1 = θ t + η∇ ES F , where: When β > 0, and we take P Φ b = P Φ πt−1 , BGES resembles the NSR-ES algorithm from Conti et al. (2018), an instantiation of novelty search (Lehman & Stanley, 2008). The positive weight on the WD-term enforces newly constructed policies to be behaviorally different from the previous ones (improving exploration) while the R−term drives the optimization to achieve its main objective, i.e., maximize the reward. The key difference in our approach is the probabilistic embedding map, with WD rather than Euclidean distance. We show in Section 7.2 that BGES outperforms NSR-ES for challenging exploration tasks. The approximation introduced by Step 5 bypasses the need of computing a different pair of behavioral test functions λ 1 , λ 2 for each perturbed policy π k . If we take β < 0, and assume P Φ b = P Φ π to correspond to embedded trajectories from an oracle or expert policy, we can perform imitation learning. Despite not accessing the expert's policy (just the trajectories it generates), we show in Section 7.3 that this approach dramatically improves learning.

Section Title: RELATED WORK
  RELATED WORK Our work is related to research in multiple areas in neuroevolution and machine learning:

Section Title: Behavior Characterizations
  Behavior Characterizations The idea of directly optimizing for behavioral diversity was intro- duced by Lehman & Stanley (2008) and Lehman (2012), who proposed to search directly for novelty, Under review as a conference paper at ICLR 2020 rather than simply assuming it would naturally arise in the process of optimizing an objective function. This approach has been applied to deep RL (Conti et al., 2018) and meta-learning (Gajewski et al., 2019). In all of this work, the policy is represented via a behavioral characterization (BC), typically chosen with knowledge of the environment, for example the final (x,y) coordinate for a locomotion task. Additionally, in most cases these BCs are considered to be deterministic, with Euclidean distances used to compare BCs. In our setting, we move from deterministic BCs to stochastic PPEs, thus requiring the use of metrics capable of comparing probabilistic distributions.

Section Title: Distance Metrics
  Distance Metrics WDs have been used in many different applications in machine learning where guarantees based on distributional similarity are required (Jiang et al., 2019; Arjovsky et al., 2017). We make use of WDs in our setting for a variety of reasons. First and foremost, the dual formulation of the WD allows us to recover Behavioral Test Functions, thus providing us with behavior-driven trajectory scores. In contrast to KL divergences, WDs are sensitive to user-defined costs between pairs of samples instead of relying only on likelihood ratios. Furthermore, as opposed to KL divergences, it is possible to take SGD steps using entropy-regularized Wasserstein objectives. Computing an estimator of the KL divergence is hard without a density model. Since in our framework multiple unknown trajectories may map to the same behavioral embedding, the likelihood ratio between two embedding distributions may be ill-defined.

Section Title: WDs for RL
  WDs for RL We are not the first to propose using WDs in RL. Zhang et al. (2018) have recently introduced Wasserstein Gradient Flows (WGFs) for finding efficient RL policies. This approach casts policy optimization as gradient descent flow on the manifold of corresponding probability measures, where geodesic lengths are given as second-order WDs. We note that computing WGFs is a nontrivial task. In Zhang et al. (2018) this is done via particle approximation methods. We show in Section 7 that RL algorithms using these techniques are substantially slower than our methods. The WD has also been employed to replace KL terms in standard Trust Region Policy Optimization (Richemond & Maginnis, 2017). This is a very special case of our more generic framework (cf. Section 5.2). In Richemond & Maginnis (2017) it is suggested to solve the corresponding RL problems via Fokker- Planck equations and diffusion processes, yet no empirical evidence of the feasibility of this approach is provided. We propose general practical algorithms and provide extensive empirical evaluation. Distributional RL Distributional RL (DRL, Bellemare et al. (2017)) expands on traditional off- policy methods (Mnih et al., 2013) by attempting to learn a distribution of the return from a given state, rather than just the expected value. These approaches have impressive experimental results (Bellemare et al., 2017; Dabney et al., 2018), with a growing body of theory (Rowland et al., 2018; Qu et al., 2019; Bellemare et al., 2019; Rowland et al., 2019). Superficially it may seem that learning a distribution of returns is similar to our approach to PPEs, when the BEM is a distribution over rewards. Indeed, reward-driven embeddings used in DRL can be thought of as special cases of the general class of BEMs. We note two key differences: 1) DRL methods are off-policy whereas our BGES and BGPG algorithms are on-policy, and 2) DRL is typically designed for discrete domains, since Q-Learning with continuous action spaces is generally much harder. Furthermore, we note that while the WD is used in DRL, it is only for the convergence analysis of the DRL algorithm-the algorithm itself does not use WDs (Bellemare et al., 2017).

Section Title: EXPERIMENTS
  EXPERIMENTS Here we seek to test whether our behavior-guided approach to RL translates to performance gains for simulated environments. We individually evaluate our two proposed algorithms, BGPG and BGES, versus their respective baselines for a range of benchmark tasks. While in some cases the results may not be state of the art, we believe the improvement vs. popular RL algorithms (in particular TRPO and ES) are exciting results which could stimulate future work. We also include a study of using our method for imitation learning. For each subsection we provide additional details in the Appendix.

Section Title: BEHAVIOR-GUIDED POLICY GRADIENT
  BEHAVIOR-GUIDED POLICY GRADIENT Our key question is whether BGPG can outperform baseline TRPO methods using KL divergence. In  Fig. 3 , we see this is clearly the case for four continuous control tasks: Pendulum from OpenAI Gym and Hopper: Stand, Hooper: Hop and Walker: Stand from the DeepMind Control Suite (Tassa et al., 2018). For the BEM, we use the concatenation-of-actions (as used already in TRPO). We also Under review as a conference paper at ICLR 2020 confirm results from (Schulman et al., 2015) that a trust region greatly improves performance, as we see the black curve (without one) often fails to learn.

Section Title: Wall Clock Time
  Wall Clock Time To illustrate computational benefits of alternating optimization (AO) of WD in BGPG, we compare it to the particle approximation (PA) method introduced in Zhang et al. (2018) in  Fig. 4 . In practice, the WD across different state samples can be optimized in a batched manner using AO (see Appendix for details). We see that AO is substantially faster than PA.

Section Title: BEHAVIOR-GUIDED EVOLUTION STRATEGIES
  BEHAVIOR-GUIDED EVOLUTION STRATEGIES As a novelty-search method, BGES is designed to actively explore the environment by behaving differently for previous policies. With that in mind, we seek to evaluate the ability to solve two key challenges in exploration for RL: deceptive rewards and local maxima. Deceptive Rewards A common challenge in model-free RL is deceptive rewards. These arise since agents can only learn from data gathered via exploration in the environment. To test BGES in this setting, we created two intentionally deceptive environments where agents may easily be fooled into learning suboptimal policies. In both cases the agent is penalized at each time step for being far away from a goal. The deception comes from a wall situated in the middle, which means that initially positive rewards from moving directly forward will lead to a suboptimal policy.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 We consider two types of agents-a two-dimensional point and a much larger quadruped. Details are provided in the Appendix (Section 9). We compare with state-of-the-art on-policy methods for efficient exploration: NSR-ES from (Conti et al., 2018), which assumes the BEM is deterministic and uses the Euclidean distance to compare policies, and NoisyNet-TRPO from Fortunato et al. (2018). Results are presented on  Fig. 5 . Policies avoiding the wall correspond to rewards: R > −5000 and R > −800 for the quadruped and point respectively. In the prior case an agent needs to first learn how to walk and the presence of the wall is enough to prohibit vanilla ES from even learning forward locomotion. We note that BGES is the only method that drives the agent to the goal in both settings. For the quadruped the BEM is the reward-to-go while for the point we used the final state.

Section Title: Escaping Local Maxima
  Escaping Local Maxima In  Fig. 6  we compare our methods with methods using regularizers based on other distances or divergences (specifically, Hellinger, Jensen-Shannon (JS), KL and Total Variation (TV) distances), as well as vanilla ES (i.e., with no distance regularizer). Experiments were performed on a Swimmer environment from OpenAI Gym (Brockman et al., 2016), where the number of samples of the ES optimizer was drastically reduced. BGES is the only one that manages to obtain good policies which also proves that the benefits come here not just from introducing the regularizer, but from its particular form. As discussed in Section 5.3, we can also utilize the BGES algorithm for imitation learning, by setting β < 0, and using an expert's tra- jectories for the PPE. For this experiment we use the reward-to-go BEM (Section 5). In  Fig. 7 , we show that this approach significantly outperforms vanilla ES on the Swimmer task. Although conceptually simple, we believe this could be a powerful approach with potential extensions, for example in designing safer algorithms.

Section Title: HYPERPARAMETER SELECTION
  HYPERPARAMETER SELECTION Our approach includes several new hyperparameters, such as the kernel for the Behavioral Test Functions and the choice of BEM. For our experiments we did not perform any hyperparameter optimization. We only considered the rbf kernel, and only varied the BEM for BGES. For BGPG we chose the concatenation of actions, since this is the same as used in the KL divergence for TRPO. For BGES, we demonstrated several different BEMs, and we show an ablation study for the point agent in  Fig. 8  where we see that both the reward-to-go (RTG) and Final State (SF) worked, but the vector of all states (SV) did not (for 5 seeds). We leave learned BEMs as exciting future work.

Section Title: CONCLUSION AND FUTURE WORK
  CONCLUSION AND FUTURE WORK In this paper we proposed a new paradigm for on-policy learning in RL, where policies are em- bedded into expressive latent behavioral spaces and the optimization is conducted by utilizing the repelling/attraction signals in the corresponding probabilistic distribution spaces. The use of Wasser- stein distances (WDs) guarantees flexibility in choosing cost funtions between embedded policy trajectories, enables stochastic gradient steps through corresponding regularized objectives (as op- posed to KL divergence methods) and provides an elegant method, via their dual formulations, to quantify behaviorial difference of policies through the behavioral test functions. Furthermore, the dual formulations give rise to efficient algorithms optimizing RL objectives regularized with WDs. We also believe the presented methods shed new light on several other challenging problems of modern RL, including: learning with safety guarantees (a repelling signal can be used to enforce behaviors away from dangerous ones) or anomaly detection for reinforcement learning agents (via the above score functions). We are also excited by the possibility of scaling this approach to a population setting, learning the behavioral embedding maps from data, or adapting the degree of repulsion/attraction during optimization (parameter β).

```
