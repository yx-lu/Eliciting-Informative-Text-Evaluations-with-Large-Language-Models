Title:
```
Under review as a conference paper at ICLR 2020 TOWARDS UNDERSTANDING THE REGULARIZATION OF ADVERSARIAL ROBUSTNESS ON NEURAL NET- WORKS
```
Abstract:
```
The problem of adversarial examples has shown that modern Neural Network (NN) models could be rather fragile. Among the most promising techniques to solve the problem, one is to require the model to be -adversarially robust (AR); that is, to require the model not to change predicted labels when any given input examples are perturbed within a certain range. However, it is observed that such methods would lead to standard performance degradation, i.e., the degradation on natural examples. In this work, we study the degradation through the regularization perspective. We identify quantities from generalization analysis of NNs; with the identified quantities we empirically find that AR is achieved by regularizing/biasing NNs towards less confident solutions by making the changes in the feature space (induced by changes in the instance space) of most layers smoother uniformly in all directions; so to a certain extent, it prevents sudden change in prediction w.r.t. perturbations. However, the end result of such smoothing concentrates samples around decision boundaries, resulting in less confident solutions, and leads to worse standard performance. Our studies suggest that one might consider ways that build AR into NNs in a gentler way to avoid the problematic regularization.
```

Figures/Tables Captions:
```
Figure 1: Experiment results on ResNet56 (He et al., 2016) trained on the CIFAR10 dataset. For the details of the experiments, refer to section 5. (a) The standard deviation of singular values of each layer of NNs with adversarial robustness (AR) strength 4, 16 (AR strength 8 is dropped for clarity of the plot). To emphasize, the x-axis is the layer index - overall 56 layers are involved. (b) The probability distribution of margins of NNs with AR strength 4, 8, 16. (c) The standard and adversarial accuracy of NNs with AR 4, 8, 16.
Figure 2: (a) Illustration of the regularization effect of adversarial robustness. If a NN T is - adversarially robust, for a given example x (drawn as filled squares or circles) and points x in the yellow ball {x | ρ(x, x ) ≤ } around x, the predicted labels of x, x should be the same, and the loss variation is potentially bigger as x moves from the center to the edge, as shown as intenser yellow color at the edge of a ball. Collectively, the adversarial robustness of each example requires an instance-space margin (IM) to exist for the decision boundary, shown as the shaded cyan margin. As normally known, margin is related to generalization ability that shrinks the hypothesis space. In this case, the IM required by adversarial robustness would weed out hypotheses that do not have an adequate IM, such as the red dashed line shown in the illustration. (b) Illustration of lemma 4.1. Given a NN with ReLU activation function, the feature map I l at layer l is divided into regions where I l (x) is piecewise linear w.r.t. x. The induced linear map W q 1 is given by diag(τ 1 (q))W 1 , where diag(τ l (q)) is a diagonal matrix whose diagonal entries are given by a vector τ 1 (q) that has 0-1 values. For example, in region p, I 1 = W p 1 x and distance between instances x are vertical elongated, while in region q, I 1 = W q 1 x and distance are horizontally elongated. Thus given x, x , the difference ||I l (x) − I l (x )|| between I l (x) and I l (x ) is the length of the transformed line segment x − x drawn, of which each segment is linearly transformed in a different way.
Figure 3: Experiment results on CIFAR10/100, and Tiny-ImageNet. Net A, B are ResNet-56 and ResNet-110 (He et al., 2016) respectively. The unit of x-axis is the adversarial robustness (AR) strength of NNs, c.f. the beginning of section 5. (a) Plots of loss gap (and error rate gap) between training and test datasets v.s. AR strength. (b) Plots of losses (and error rates) on training and test datasets v.s. AR strength.
Figure 4: Margin distributions of NNs with AR strength 4, 8, 16 on Training and Test sets of CIFAR10/100.
Figure 5: (a)(b) are histograms of estimated probabilities and losses respectively of the test set sample of NNs trained AR strength 4, 8, 16. We plot a subplot of a narrower range inside the plot of the full range to show the histograms of examples that are around the middle values to show the change induced by AR that induces more middle valued confidence predictions. (c)(d) are standard deviations of singular values of weight matrices of NNs at each layer trained on CIFAR10/100 with AR strength 4, 16. The AR strength 8 is dropped for clarity.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Despite the remarkable performance (Krizhevsky et al., 2012) of Deep Neural Networks (NNs), they are found to be rather fragile and easily fooled by adversarial examples (Szegedy et al., 2014). More intriguingly, these adversarial examples are generated by adding imperceptible noise to normal examples, and thus are indistinguishable for humans. NNs that are more robust to adversarial examples tend to have lower standard accuracy (Su et al., 2018), i.e., the accuracy measured on natural examples. The trade-off between robustness and accuracy has been observed (Kurakin et al., 2017; Madry et al., 2018; Tsipras et al., 2019). To understand such a phenomenon, Tsipras et al. (2019) show that for linear models, if examples are closed to decision boundaries, robustness provably conflicts with accuracy, though the proof seems unlikely to generalize to NNs. Zhang et al. (2019) show that a gap exists between surrogate risk gap and 0-1 risk gap if many examples are close to decision boundaries, and better robustness can be achieved by pushing examples away from decision boundaries. But pushing examples away again degrades NN performance in their experiments. A more established remedy is developed to require NNs to be -adversarially robust (AR), e.g., via Adversarial Training (Madry et al., 2018), Lipschitz-Margin Training (Tsuzuku et al., 2018); that is, they require the model not to change predicted labels when any given input examples are perturbed within a certain range. Note that such hard requirement is different from penalties on the risk function employed by Lyu et al. (2015) and Miyato et al. (2018), which is not our subject of investigation (more discussion in appendix A). In practice, hard-requirement methods are found to lead to worse performance measured in standard classification accuracy. We aim to study this branch of methods. We investigate how adversarial robustness influence the behaviors of NNs to make them more robust but have lower performance. In an earlier time (Szegedy et al., 2014), adversarial training has been suggested as a form of regularization: it augments the training of NNs with adversarial examples, thus might improve the generalization of the end models. How does a possible improvement in generalization end up degrading performance? It prompts us to analyze the regularization effects of AR on NNs. A successful regularization technique is expected to improve test performance, Under review as a conference paper at ICLR 2020 (a) STD of Singular Values (b) Margin Distribution (c) Accuracy but an improved performance is only one of the possible outcomes of improved generalization. Technically, improved generalization implies the reduction in gap between training errors and test errors. Regularization achieves the gap reduction by reducing the size of the hypothesis space, which reduces the variance, but meanwhile increases the bias of prediction made - a constant classifier can have zero generalization errors, but also have low test performance. Thus, when a hypothesis space is improperly reduced, another possible outcome is biased poorly performing models with reduced generalization gaps.

Section Title: Key results
  Key results Through a series of theoretically motivated experiments, we find that AR is achieved by regularizing/biasing NNs towards less confident solutions by making the changes in the feature space of most layers (which are induced by changes in the instance space) smoother uniformly in all directions; so to a certain extent, it prevents sudden change in prediction w.r.t. perturbations. However, the end result of such smoothing concentrates examples around decision boundaries and leads to worse standard performance. We elaborate the above statement in details shortly in section 1.1. Overall, the investigation of generalization behaviors of NNs points out possible directions where we might go if we are to resolve the issue of the test performance degradation done by AR. The main result shows that the hypothesis space of NNs is improperly reduced, thus we might investigate how to avoid it when enforcing AR. Though beyond the scope of this work, we conjecture that the improper reduction comes from the indistinguishability of the change induced in the intermediate layers of NNs by adversarial noise and that by inter-class difference. To guarantee AR, NNs are asked to smoothe out difference uniformly in all directions in a high dimensional space, and thus are biased towards diffident solutions that make similar/concentrated predictions. We leave the investigation of the conjecture as future works.

Section Title: AR LEADS TO DIFFIDENT NNS WITH MORE INDECISIVE MISCLASSIFICATIONS
  AR LEADS TO DIFFIDENT NNS WITH MORE INDECISIVE MISCLASSIFICATIONS This section elaborates the key results we briefly present previously. 1. AR reduces the variance of (norms of) the activation/outputs (compared with NNs with different AR strength) at most layers that are emitted/induced by feeding perturbations (of any directions) to that layer from the previous layer. Through a series of theoretically motivated experiments, the results prompt us to look at the singular value distributions of the weight matrix of each layer of the NNs. Shown in fig. 1a, we find that overall the standard deviation (STD) of singular values associated with a layer of the NN trained with lower AR strength 4 is larger than that of the NN with higher AR strength 16 1 - the green dots are mostly below the red dots. Note that given a matrix W and an example x, singular values of W determine how the norm ||W x|| is changed Under review as a conference paper at ICLR 2020 when compared with ||x||. More specifically, let σ min , σ max be the maximal and minimal singular values, if x is not in the null space of W , then we have ||W x|| ∈ [σ min ||x||, σ max ||x||], where || · || denotes 2-norm. This applies to norm ||δx|| of a perturbation as well; that is, given possible changes δx of x of the same norm ||δx|| = c, where c is a constant, the variance of σ(W ) roughly determines the variance of ||W δx||, where σ(W ) denotes all singular values {σ i } of W . In more details, note that by SVD decomposition, W δx = i σ i u i v T i δx, thus σ i determines how the component v T i δx in the direction of v i is amplified. To see an example, suppose that σ min = σ max = σ 0 , then the variance of σ(W ) is zero, and ||W δx|| = σ 0 ||δx||. In this case, the variance of ||W δx|| (given an ensemble of perturbations δx of the same norm c) is zero as well. The conclusion holds as well for ReLU(W δx), where W here is a weight matrix of a layer of a NN, and ReLU denotes Rectifier Linear Unit activation function (proved by applying Cauchy interlacing law by row deletion (Chafai) to lemma 4.1). Consequently, by reducing the variance of singular values of weight matrix of a layer of the NN, AR reduces the norm variance of layer activations induced by input perturbations. 2. The reduced norm variance induced by example perturbations concentrates examples, and it empirically concentrates them around decision boundaries; that is, predictions are more diffident. The reduced variance implies that the outputs of each layer of the NN are more concentrated, but it does not tell where they are concentrated. Note that in the previous paragraph, the variance relationship discussed between ||W δx|| and ||δx|| equally applies to ||W x|| and ||x||, where x is an actual example instead of perturbations. Thus, to find out the concentration of perturbations, we can look at the concentration of samples. Technically, we look at margins of examples. In a multi-class setting, suppose a NN computes a score function f : R d → R L , where L is the number of classes; a way to convert this to a classifier is to select the output coordinate with the largest magnitude, meaning x → arg max i f i (x). The confidence of such a classifier could be quantified by margins. It measures the gap between the output for the correct label and other labels, meaning f y (x) − max i =y f i (x). Margin piece-wise linearly depends on the scores, thus the variance of margins is also in a piece-wise linear relationship with the variance of the scores, which are computed linearly from the activation of a NN layer. Thus, the consequence of concentration of activation discussed in the previous paragraph can be observed in the distribution of margins. More details of the connection between singular values and margins are discussed in section 5.2.2, after we present lemma 4.1. A zero margin implies that a classifier has equal propensity to classify an example to two classes, and the example is on the decision boundary. We plot the margin distribution of the test set of CIFAR10 in fig. 1b, and find that margins are increasingly concentrated around zero - that is, the decision boundaries - as AR strength grows. 3. The sample concentration around decision boundaries smoothes sudden changes induced per- turbations, but also increases indecisive misclassification. The concentration of test set margins implies that the induced change in margins by the perturbation in the instance space is reduced by AR. The statement may not be immediately obvious, so we explain in details as follows. Given two examples x, x from the test set, δx = x − x can be taken as a significant perturbation that changes the example x to x . The concentration of overall margins implies the change induced by δx is smaller statistically in NNs with higher AR strength. Thus, for an adversarial perturba- tion applied on x, statistically the change of margins is smaller as well - experimentally it is reflected in the increased adversarial robustness of the network, as shown in the increasing curve in fig. 1c. That is, the sudden changes of margins originally induced by adversarial perturbations are smoothed (to change slowly). However, the cost of such smoothness is lower confidence in prediction, and more test examples are slightly/indecisively moved to the wrong sides of the decision boundaries - incurring lower accuracy, as shown in the decreasing curve in fig. 1c. Lastly, we note that experiments in this section are used to illustrate our main arguments in this section. Further consistent quality results are reported in section 5 by conducting experiments on CIFAR10/100 and Tiny-ImageNet with networks of varied capacity.

Section Title: OUTLINE AND CONTRIBUTIONS
  OUTLINE AND CONTRIBUTIONS As briefly discussed at the beginning, this work carries out generalization analysis on NNs with AR. The quantities we investigate in the previous section are identified by the generalization errors (GE) upper bound we establish at theorem 4.1, which characterizes the regularization of AR on NNs. The key result is actually obtained at the end of a series of analysis, thus we present the outline of the analysis here. Under review as a conference paper at ICLR 2020 Outline. After presenting some preliminaries in section 3, we proceed to analyze the regularization of AR on NNs, and establish a GE upper bound in section 4. The bound prompts us to look at the GE gaps in experiments. In section 5.1, we find that for NNs trained with higher AR strength, the surrogate risk gaps (GE gaps) decrease for a range of datasets, i.e., CIFAR10/100 and Tiny-ImageNet (ImageNet, 2018). It implies AR effectively regularizes NNs. We go further to study the finer behavior change of NNs that might lead to such a gap reduction. Again, we follow the guidance of theorem 4.1. We look at the margins in section 5.2.1, then at the singular value distribution in section 5.2.2, and discover the main results described in section 1.1. More corroborative experiments are run in appendix B.4 to show that such phenomenon exists in a broad range of NNs with varied capacity, and more complementary results are present in appendix B.3 to explain some seemingly abnormal observations. More related works are present in section 2.

Section Title: Contributions
  Contributions Overall, the core contribution in this work is to show that adversarial robustness (AR) regularizes NNs in a way that hurts its capacity to learn to perform in test. More specifically: • We establish a generalization error (GE) bound that characterizes the regularization of AR on NNs. The bound connects margin with adversarial robustness radius via singular values of weight matrices of NNs, thus suggesting the two quantities that guide us to investigate the regularization effects of AR empirically. • Our empirical analysis tells that AR effectively regularizes NNs to reduce the GE gaps. To understand how reduced GE gaps turns out to degrade test performance, we study variance of singular values of layer-wise weight matrices of NNs and distributions of margins of samples, when different strength of AR are applied on NNs. • The study shows that AR is achieved by regularizing/biasing NNs towards less confident solutions by making the changes in the feature space of most layers (which are induced by changes in the instance space) smoother uniformly in all directions; so to a certain extent, it prevents sudden change in prediction w.r.t. perturbations. However, the end result of such smoothing concentrates samples around decision boundaries and leads to worse standard performance.

Section Title: RELATED WORKS
  RELATED WORKS Robustness in machine learning models is a large field. We review some more works that analyze robustness from the statistical perspective. The majority of works that study adversarial robustness from the generalization perspective study the generalization behaviors of machine learning models under adversarial risk. The works that study adversarial risk include Attias et al. (2018); Schmidt et al. (2018); Cullina et al. (2018); Yin and Bartlett (2018); Khim and Loh (2018); Sinha et al. (2018). The bounds obtained under the setting of adversarial risk characterize the risk gap introduced by adversarial examples, thus, it is intuitive that a larger risk gap would be obtained for a larger allowed perturbation limit , which is roughly among the conclusions obtained in those bounds. That is to say, the conclusion normally leads to a larger generalization error as an algorithm is asked to handle more adversarial examples, for that it focuses on characterizing the error of adversarial examples, not that of natural examples. However, adversarial risk is not our focus. In this paper, we study when a classifier needs to accommodate adversarial examples, what is the influence that the accommodation has on generalization behaviors of empirical risk of natural data.

Section Title: PRELIMINARIES
  PRELIMINARIES Assume an instance space Z = X × Y, where X is the space of input data, and Y is the label space. Z := (X, Y ) are the random variables with an unknown distribution µ, from which we draw samples. We use S m = {z i = (x i , y i )} m i=1 to denote the training set of size m whose examples are drawn independently and identically distributed (i.i.d.) by sampling Z. Given a loss function l, the goal of learning is to identify a function T : X → Y in a hypothesis space (a class T of functions) that minimizes the expected risk Since µ is unknown, the observable quantity serving as the proxy to the expected risk R is the empirical risk

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Our goal is to study the discrepancy between R and R m , which is termed as generalization error - it is also sometimes termed as generalization gap in the literature Definition 1 (Covering number). Given a metric space (S, ρ), and a subsetS ⊂ S, we say that a subsetŜ ofS is a -cover ofS, if ∀s ∈S, ∃ŝ ∈Ŝ such that ρ(s,ŝ) ≤ . The -covering number ofS is N (S, ρ) = min{|Ŝ| :Ŝ is an -covering ofS}. Various notions of adversarial robustness have been studied in existing works (Madry et al., 2018; Tsipras et al., 2019; Zhang et al., 2019). They are conceptually similar; in this work, we formalize its definition to make clear the object for study. Definition 2 ((ρ, )-adversarial robustness). Given a multi-class classifier f : X → R L , and a metric ρ on X , where L is the number of classes, f is said to be adversarially robust w.r.t. adversarial perturbation of strength , if there exists an > 0 such that ∀z = (x, y) ∈ Z and δx ∈ {ρ(δx) ≤ }, we have fŷ(x + δx) − f i (x + δx) ≥ 0, whereŷ = arg max j f j (x) and i =ŷ ∈ Y. is called adversarial robustness radius. When the metric used is clear, we also refer (ρ, )-adversarial robustness as -adversarial robustness. Note that the definition is an example-wise one; that is, it requires each example to have a guarding area, in which all examples are of the same class. Also note that the robustness is w.r.t. the predicted class, since ground-truth label is unknown for a x in test. We characterize the GE with ramp risk, which is a typical risk to undertake theoretical analysis (Bartlett et al., 2017; Neyshabur et al., 2018b). Definition 3 (Margin Operator). A margin operator M : R L × {1, . . . , L} → R is defined as M(s, y) := s y − max i =y s i Definition 4 (Ramp Loss). The ramp loss l γ : R → R + is defined as Definition 5 (Ramp Risk). Given a classifier f , ramp risk is the risk defined as R γ (f ) := E(l γ (−M(f (X), Y ))), where X, Y are random variables in the instance space Z previously. We will use a different notion of margin in theorem 4.1, and formalize its definition as follows. We reserve the unqualified word "margin" specifically for the margin discussed previously - the output of margin operator for classification. We call this margin to-be-introduced instance-space margin (IM). Definition 6 (Smallest Instance-space Margin). Given an element z = (x, y) ∈ Z, let v(x) be the distance from x to its closest point on the decision boundary, i.e., the instance-space margin (IM) of example x. Given an -covering of Z, let v min = min x∈{x∈X | ||x−xi||2≤ ,∀xi∈Sm} v(x). (2) v min is the smallest instance-space margin of elements in the covering balls that contain training examples.

Section Title: THEORETICAL INSTRUMENTS FOR EMPIRICAL STUDIES ON AR
  THEORETICAL INSTRUMENTS FOR EMPIRICAL STUDIES ON AR In this section, we rigorously establish the bound mentioned in the introduction. We study the map T defined in section 3 as a NN (though technically, T now is a map from X to R L , instead of to Y, such an abuse of notation should be clear in the context). To begin with, we introduce an assumption, before we state the generalization error bound guaranteed by adversarial robustness. Assumption 4.1 (Monotony). Given a point x ∈ X , let x be the point on the decision boundary of a NN T that is closest to x. Then, for all x on the line segment x + t(x − x), t ∈ [0, 1], the margin M(T x , y) decreases monotonously. The assumption is a regularity condition on the classifier that rules out undesired oscillation between x and x . To see how, notice that the margin defined in definition 3 reflects how confident the decision is made. Since x is on the decision boundary, it means the classifier is unsure how it should be classified. Thus, when the difference x − x is gradually added to x, ideally we want the confidence that we have on classifying x to decrease in a consistent way to reflect the uncertainty. Theorem 4.1. Let T denote a NN with ReLU and MaxPooling nonlinear activation functions (a definition is put at eq. (6) for readers' convenience), l γ the ramp loss defined at definition 4, and Z the instance space assumed in section 4. Assume that Z is a k-dimensional regular manifold that accepts an -covering with covering number ( C X ) k , and assumption 4.1 holds. If T is 0 -adversarially robust (defined at definition 2), ≤ 0 , and denote v min the smallest IM margin in the covering balls that contain training examples (defined at definition 6), σ i min the smallest singular values of weight matrices W i , i = 1, . . . , L − 1 of a NN, {w i } i=1,...,|Y| the set of vectors made up with ith rows of W L (the last layer's weight matrix), then given an i.i.d. training sample S m = {z i = (x i , y i )} m i=1 drawn from Z, its generalization error GE(l • T ) (defined at eq. (1)) satisfies that, for any η > 0, with probability at least 1 − η is a lower bound of margins of examples in covering balls that contain training samples.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 The proof of theorem 4.1 is in appendix C. The bound identifies quantities that would be studied experimentally in section 5 to understand the regularization of AR on NNs. The first term in eq. (3) in theorem 4.1 suggests that quantities related to the lower bound of margin u min might be useful to study how -adversarial robustness ( -AR) regularizes NNs. However, -AR is guaranteed in the instance space that determines the smallest instance-space margin v min . To relate GE bound with -AR, we characterize in eq. (4) the relationship between margin with IM, via smallest singular values of NNs' weight matrices, suggesting that quantities related to singular values of NNs' weight matrices might be useful to study how AR regularizes NNs as well. An illustration on how AR could influence generalization of NNs through IM is also given in fig. 2a. The rightmost term in eq. (3) is a standard term in robust framework (Xu and Mannor, 2012) in learning theory, and is not very relevant to the discussion. The remaining of this paper are empirical studies that are based on the quantities, e.g., margin distributions and singular values of NNs' weight matrices, that are related to the identified quantities, i.e., u min , σ i min . These studies aim to illuminate with empirical evidence on the phenomena that AR regularizes NNs, reduces GE gaps, but degrades test performance. 2 Before turning into empirical study, we further present a lemma to illustrate the relation characterized in eq. (4) without the need to jump into proof of theorem 4.1. It would motivate our experiments later in section 5.2.2. We state the following lemma that relates distances between elements in the instance space with those in the feature space of any intermediate network layers. Lemma 4.1. Given two instances x, x ∈ X , let I l (x) be the activation g(W l g(W l−1 . . . g(W 1 x))) at layer l of x (c.f. definition of NNs at appendix C.2), then there exist n ∈ N sets of matrices {W qj i } i=1...l , j = 1 . . . n, that each of the matrix W qj i is obtained by setting some rows of W i to zero, and {q j } j=1...n are arbitrary distinctive symbols indexed by j that index W qj i , such that ||I l (x) − I l (x )|| = n j=1 ej sj l i=1 W qj i dt(x − x ) where s 1 = 0, s j+1 = e j , e n = 1, s j , e j ∈ [0, 1] - each [s j , e j ] is a segment in the line segment parameterized by t that connects x and x . Its proof is in appendix C, and an illustration is given in fig. 2b. Essentially, it states that difference in the feature space of a NN, induced by the difference between elements in the instance space, is a summation of the norms of the linear transformation ( l i=1 W qj i ) applied on segments of the line segment that connects x, x in the instance space. Since W qj i is obtained by setting rows of W i to zero, the singular values of these induced matrices are intimately related to weight matrices W i of NN by Cauchy interlacing law by row deletion (Chafai). Since the margin of an example x is a linear transform of the difference between I L−1 (x) and the I L−1 (x ) of an element x on the decision boundary, singular values of {W i } i=1...L−1 determine the amplification/shrinkage of the IM x − x .

Section Title: EMPIRICAL STUDIES ON REGULARIZATION OF ADVERSARIAL ROBUSTNESS
  EMPIRICAL STUDIES ON REGULARIZATION OF ADVERSARIAL ROBUSTNESS In this section, guided by theorem 4.1, we undertake empirical studies to explore AR's regularization effects on NNs. We first investigate the behaviors of off-the-shelf architectures of fixed capacity on various datasets in section 5.1 and 5.2. More corroborative controlled studies that explore the regularization effects of AR on NNs with varied capacity are present in appendix B.4.

Section Title: ADVERSARIAL ROBUSTNESS EFFECTIVELY REGULARIZES NNS ON VARIOUS DATASETS
  ADVERSARIAL ROBUSTNESS EFFECTIVELY REGULARIZES NNS ON VARIOUS DATASETS This section aims to explore whether AR can effectively reduce generalization errors - more specifically, the surrogate risk gaps. We use adversarial training (Madry et al., 2018) to build 2 Note that in the previous paragraph, though we identifies quantities umin and σ i min related to the upper bound of GE, the quantities we actually would study empirically are margin distribution and all singular values that characterize the GE of all samples, not just the extreme case (upper bound). The analytic characterization of the GE of all samples is not possible since we do not have enough information (at least we do not know the true distribution of samples). That's why to arrive at close-form analytic characterization of GE, we resort to the extreme non-asymptotic large-sample behaviors. The analytic form is a neat way to present how relevant quantities influence GE. In the rest of the paper, we would carry on empirical study on the distributions of margins and singular values mostly to investigate AR's influence on GE of all samples. adversarial robustness into NNs. The AR strength is characterized by the maximally allowed l ∞ norm of adversarial examples that are used to train the NNs. Details on the technique to build adversarial robustness into NNs is given in appendix B.1. Our experiments are conducted on CIFAR10, CIFAR100, and Tiny-ImageNet (ImageNet, 2018) that represent learning tasks of increased difficulties. We use ResNet-56 and ResNet-110 (He et al., 2016) for CIFAR10/100, and Wide ResNet (WRN-50-2-bottleneck) (Zagoruyko and Komodakis, 2016) for Tiny-ImageNet (ImageNet, 2018). These networks are trained with increasing AR strength. Results are plotted in  fig. 3 , where Net A stands for ResNet56, and Net B for ResNet-110.

Section Title: Regularization of AR on NNs
  Regularization of AR on NNs We observe in fig. 3a (shown as blue lines marked by circles) that GE gaps (the gaps between training and test losses) decrease as strength of AR increase; we also observe in fig. 3a that training losses increase as AR strength increase; these results (and more results in subsequent fig. 6) imply that AR does regularize training of NNs by reducing their capacities to fit training samples. Interestingly, in the CIFAR10/100 results in fig. 3b, the test losses show a decreasing trend even when test error rates increase. It suggests that the network actually performs better measured in test loss as contrast to the performance measured in test error rates. This phenomenon results from that more diffident wrong predictions are made by NNs thanks to adversarial training, which will be explained in details in section 5.2, when we carry on finer analysis. We note that on Tiny-ImageNet, the test loss does not decrease as those on CIFAR10/100. It is likely because the task is considerably harder, and regularization hurts NNs even measured in test loss.

Section Title: Trade-off between regularization of AR and test error rates
  Trade-off between regularization of AR and test error rates The error rate curves in fig. 3b also tell that the end result of AR regularization leads to biased-performing NNs that achieve degraded test performance. These results are consistent across datasets and networks.

Section Title: Seemingly abnormal phenomenon
  Seemingly abnormal phenomenon An seemingly abnormal phenomenon in CIFAR10 observed in fig. 3a is that the error rate gap actually increases. It results from the same underlying behaviors of NNs, which we would introduce in section 5.2, and an overfitting phenomenon that AR cannot control. Since it would be a digress to explain, it is put in appendix B.3. We finally note that the adversarial robustness training reproduced is relevant, of which the defense effect is comparable with existing works. One may refer to fig. 11 in appendix D.2 for the details. We can see from it that similar adversarial robustness to Madry et al. (2018) and Li et al. (2018) is achieved for CIFAR10/100, Tiny-ImageNet in the NNs we reproduce.

Section Title: REFINED ANALYSIS THROUGH MARGINS AND SINGULAR VALUES
  REFINED ANALYSIS THROUGH MARGINS AND SINGULAR VALUES The experiments in the previous sections confirm that AR reduces GE, but decreases accuracy. We study the underlying behaviors of NNs to analyze what have led to it here. More specifically, we show that adversarial training implements -adversarial robustness by making NNs biased towards less confident solutions; that is, the key finding we present in section 1.1 that explains both the prevented sudden change in prediction w.r.t. sample perturbation (i.e., the achieved AR), and the reduced test accuracy.

Section Title: MARGINS THAT CONCENTRATE MORE AROUND ZERO LEAD TO REDUCED GE GAP
  MARGINS THAT CONCENTRATE MORE AROUND ZERO LEAD TO REDUCED GE GAP To study how GE gaps are reduced, theorem 4.1 suggests we first look at the margins of examples - a lower bound of margins is u min in eq. (4). The analysis on margins has been a widely used tool in learning theory (Bartlett et al., 2017). It reflects the confidence that a classifier has on an example, which after being transformed by a loss function, is the surrogate loss. Thus, the loss difference between examples are intuitively reflected in the difference in confidence characterized by margins. To study how AR influences generalization of NNs, we plot in  fig. 4  the margin distributions of samples which are obtained by training ResNet-56 on CIFAR10 and CIFAR100 with increased AR strength (the same setting as for  fig. 3 ). Applying the same network of ResNet-56 respectively on CIFAR-10 and CIFAR-100 of different learning difficulties creates learning settings of larger- and smaller-capacity NNs.

Section Title: Concentration and reduced accuracy
  Concentration and reduced accuracy In  fig. 4 , we can see that in both CIFAR10/100, the distributions of margins become more concentrated around zero as AR grows. The concentration moves the mode of margin distribution towards zero and more examples slightly across the decision boundaries, where the margins are zero, which explains the reduced accuracy.

Section Title: Concentration and reduced loss/GE gap
  Concentration and reduced loss/GE gap The concentration has different consequences on train- ing and test losses. Before describing the consequences, to directly relate the concentration to loss gap, we further introduce estimated probabilities of examples. This is because though we use ramp loss in theoretical analysis, in the experiments, we explore the behaviors of more practically used cross entropy loss. The loss maps one-to-one to estimated probability, but not to margin, though they both serve as a measure of confidence. Suppose p(x) is the output of the softmax function of dimension L (L is the number of target classes), and y is the target label. The estimated probability of x would be the y-th dimension of (p(x)), i.e., (p(x)) y . On the training sets, since the NNs are optimized to perform well on the sets, only a tiny fraction of them are classified wrongly. To concentrate the margin distribution more around zero, is to make almost all of predictions that are correct more diffident. Thus, a higher expected training loss ensues. On the test sets, the estimated probabilities of the target class concentrate more around middle values, resulting from lower confidence/margins in predictions made by NNs, as shown in fig. 5a (but the majority of values are still at the ends). Note that wrong predictions away from decision boundaries (with large negative margins) map to large loss values in the surrogate loss function. Thus, though NNs with larger AR strength have lower accuracy, they give more predictions whose estimated probabilities are at the middle (compared with NNs with smaller AR strength). These predictions, even if relatively more of them are wrong, maps to smaller loss values, as shown in fig. 5b, where we plot the histogram of loss values of test samples. In the end, it results in expected test losses that are lower, or increase in a lower rate than the training Under review as a conference paper at ICLR 2020 losses on CIFAR10/100, Tiny-ImageNet, as shown in fig. 3b. The reduced GE gap results from the increased training losses, and decreased or less increased test losses. The observation in section 5.2.1 shows that AR make NNs just less confident by reducing the variance of predictions made and concentrate margins more around zero. In this section, we study the underlying factors of AR that make NNs become less confident. To begin with, we show that the singular values of the weight matrix of each layer determine the perturbation in margins of samples induced by perturbations in the instance space. Such a connection between singular values and the perturbation of outputs of a single layer, i.e., ReLU(W δx), has been discussed in section 1.1. In the following, with lemma 4.1, we describe how the relatively more complex connection between margins and singular values of each weight matrix of layers of NNs holds. Observe that margins are obtained by applying a piece-wise linear mapping (c.f. the margin operator in definition 3) to the activation of the last layer of a NN. It implies the perturbations in activation of the last layer induce changes in margins in a piece-wise linear way. Meanwhile, the perturbation in the activation of the last layer (induced by perturbation in the instance space) is determined by the weight matrix's singular values of each layer of NNs. More specifically, this is explained as follows. Lemma 4.1 shows that the perturbation δI induced by δx, is given by n j=1 ej sj l i=1 W qj i δxdt . Note that for each i, W qi i is a matrix. By Cauchy interlacing law by row deletion (Chafai), the singular values of W i , the weight matrix of layer i, determine the singular values of W qj i . Thus, suppose l = 1, we have the change (measured in norm) induced by perturbation as n j=1 ej sj W qj 1 δxdt . The singular values of W 1 would determine the variance (of norms) of activation change induced by perturbations δx, similarly as explained in section 1.1 except that the norm change now is obtained by a summation of n terms W qj 1 δxdt (each of which is the exact form discussed in section 1.1) weighted by 1/(e j − s j ). Similarly, for the case where l = 2 . . . L − 1, the singular values of W l determine the variance of changes induced by the perturbation of the previous layer (induced by perturbation from further previous layer recursively) of layer l. Consequently, we choose to study these singular values. We plot the standard deviation of singular values of each layer of ResNet56 trained on CIFAR10/100 earlier, shown in fig. 5c 5d. Overall, we can see that the standard deviation of singular values associated with a layer of the NN trained with AR strength 4 is mostly larger than that of the NN with AR strength 16. The STD reduction in CIFAR100 is relatively smaller than CIFAR10, since as observed in fig. 4b, the AR induced concentration effect of margin distributions is also relatively less obvious than that in fig. 4a. More quantitative analysis is given in appendix B.2. This leads us to our key results described in section 1.1.

```
