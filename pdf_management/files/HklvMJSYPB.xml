<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 ADAPTIVE ADVERSARIAL IMITATION LEARNING</article-title></title-group><abstract><p>We present the ADaptive Adversarial Imitation Learning (ADAIL) algorithm for learning adaptive policies that can be transferred between environments of vary- ing dynamics, by imitating a small number of demonstrations collected from a single source domain. This is an important problem in robotic learning because in real world scenarios 1) reward functions are hard to obtain, 2) learned policies from one domain are difficult to deploy in another due to varying source to target domain statistics, 3) collecting expert demonstrations in multiple environments where the dynamics are known and controlled is often infeasible. We address these constraints by building upon recent advances in adversarial imitation learn- ing; we condition our policy on a learned dynamics embedding and we employ a domain-adversarial loss to learn a dynamics-invariant discriminator. The effec- tiveness of our method is demonstrated on simulated control tasks with varying environment dynamics and the learned adaptive agent outperforms several recent baselines.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Humans and animals can learn complex behaviors via imitation. Inspired by these learning mecha- nisms, Imitation Learning (IL) has long been a popular method for training autonomous agents from human-provided demonstrations. However, human and animal imitation differs markedly from com- monly used approaches in machine learning. Firstly, humans and animals tend to imitate the goal of the task rather than the particular motions of the demonstrator (<xref ref-type="bibr" rid="b1">Baker et al., 2007</xref>). Secondly, humans and animals can easily handle imitation scenarios where there is a shift in embodiment and dynamics between themselves and a demonstrator. The first feature of human IL can be represented within the framework of Inverse Reinforcement Learning (IRL) (<xref ref-type="bibr" rid="b0">Ng et al., 2000</xref>; <xref ref-type="bibr" rid="b0">Abbeel &amp; Ng, 2004</xref>; <xref ref-type="bibr" rid="b5">Ziebart et al., 2008</xref>), which at a high level casts the problem of imitation as one of matching outcomes rather than actions. Recent work in adversarial imitation learning (<xref ref-type="bibr" rid="b6">Ho &amp; Ermon, 2016</xref>; <xref ref-type="bibr" rid="b7">Finn et al., 2016</xref>) has accomplished this by using a discriminator to judge whether a given behavior is from an expert or imitator, and then a policy is trained using the discriminator expert likelihood as a reward. While successful in multiple problem domains, this approach makes it difficult to accom- modate the second feature of human learning: imitation across shifts in embodiment and dynamics. This is because in the presence of such shifts, the discriminator may either simply use the embod- iment or dynamics to infer whether it is evaluating expert behavior, and as a consequence fails to provide a meaningful reward signal.</p><p>In this paper we are concerned with the problem of learning adaptive policies that can be transferred to environments with varying dynamics, by imitating a small number of expert demonstrations col- lected from a single source domain. This problem is important in robotic learning because it is better aligned with real world constraints: 1) reward functions are hard to obtain, 2) learned policies from one domain are hard to deploy to different domains due to varying source to target domain statistics, and 3) the target domain dynamics oftentimes changes while executing the learned policy. As such, this work assumes ground truth rewards are not available, and furthermore we assume that expert demonstrations come from only a single domain (i.e. an instance of an environment where dynamics cannot be exactly replicated by the policy at training time). To the best of our knowledge, this is the first work to tackle this challenging problem formulation.</p><p>Our proposed method solves the above problem by building upon the GAIL (<xref ref-type="bibr" rid="b6">Ho &amp; Ermon, 2016</xref>; <xref ref-type="bibr" rid="b7">Finn et al., 2016</xref>) framework, by firstly conditioning the policy on a learned dynamics embedding ("context variable" in policy search literature (<xref ref-type="bibr" rid="b5">Deisenroth et al., 2013</xref>)). We propose two embedding Under review as a conference paper at ICLR 2020 approaches on which the policy is conditioned, namely, a direct supervised learning approach and a variational autoencoder (VAE) (<xref ref-type="bibr" rid="b12">Kingma &amp; Welling, 2013</xref>) based unsupervised approach. Secondly, to prevent the discriminator from inferring whether it is evaluating the expert behavior or imitator behavior purely through the dynamics, we propose using a Gradient Reversal Layer (GRL) to learn a dynamics-invariant discriminator. We demonstrate the effectiveness of the proposed algorithm on benchmark Mujoco simulated control tasks.</p><p>The main contributions of our work include: 1) present a general and novel problem formulation that is well aligned with real world scenarios in comparison to recent literature 2) devise a conceptually simple architecture that is capable of learning an adaptive policy from a small number of expert demonstrations (order of 10s) collected from only one source environment, 3) design an adversarial loss for addressing the covariate shift issue in discriminator learning.</p></sec><sec><title>RELATED WORK</title><p>Historically, two main avenues have been heavily studied for imitation learning: 1) Behavioral Cloning (BC) and 2) Inverse Reinforcement Learning (IRL). Though conceptually simple, BC suf- fers from compound errors caused by covariate shift, and subsequently, often requires a large quan- tity of demonstrations (<xref ref-type="bibr" rid="b0">Pomerleau, 1989</xref>), or access to the expert policy (<xref ref-type="bibr" rid="b5">Ross et al., 2011</xref>) in order to recover a stable policy.</p><p>Recent advancements in imitation learning (<xref ref-type="bibr" rid="b6">Ho &amp; Ermon, 2016</xref>; <xref ref-type="bibr" rid="b7">Finn et al., 2016</xref>) have adopted an adversarial formation that interleaves between 1) discriminating the generated policy against the expert demonstrations and 2) a policy improvement step where the policy aims to fool the learned discriminator.</p><p>Dynamics randomization (<xref ref-type="bibr" rid="b5">Tobin et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Sadeghi &amp; Levine, 2016</xref>; <xref ref-type="bibr" rid="b14">Mandlekar et al., 2017</xref>; <xref ref-type="bibr" rid="b5">Tan et al., 2018</xref>; <xref ref-type="bibr" rid="b5">Pinto et al., 2017</xref>; <xref ref-type="bibr" rid="b17">Peng et al., 2018</xref>; <xref ref-type="bibr" rid="b3">Chebotar et al., 2018</xref>; <xref ref-type="bibr" rid="b5">Rajeswaran et al., 2016</xref>) has been one of the prevailing vehicles for addressing varying simulation to real-world domain statistics. This avenue of methods typically involves perturbing the environment dynamics (often times adversarially) in simulation in order to learn an adaptive policy that is robust enough to bridge the "Reality Gap".</p><p>While dynamics randomization has been explored for learning robust policies in an RL setting, it has a critical limitation in the imitation learning context: large domain shifts might result in direc- tional differences in dynamics, therefore, the demonstrated actions might no longer be admissible for solving the task in the target domain. Our method (<xref ref-type="fig" rid="fig_1">Figure 2</xref>) also involves training in a vari- ety of environments with different dynamics. However, we propose conditioning the policy on an explicitly learned dynamics embedding to enable adaptive policies based on online system ID. <xref ref-type="bibr" rid="b5">Yu et al. (2017)</xref> adopted a similar approach towards building adaptive policies. They learn an online system identification model and condition the policy on the predicted model parameters in an RL setting. In comparison to their work, we do not assume access to the ground truth reward signals or the ground truth physics parameters at evaluation time, which makes this work's problem for- mulation a harder learning problem, but with greater potential for real-world applications. We will compare our method with <xref ref-type="bibr" rid="b5">Yu et al. (2017)</xref> in the experimental section.</p><p>Third person imitation learning (<xref ref-type="bibr" rid="b6">Stadie et al., 2017</xref>) also employs a GRL (<xref ref-type="bibr" rid="b9">Ganin &amp; Lempitsky, 2014</xref>) under a GAIL-like formulation with the goal of learning expert behaviors in a new domain. In comparison, our method also enables learning adaptive policies by employing an online dynamics identification component, so that the policies can be transferred to a class of domains, as opposed to one domain. In addition, learned policies using our proposed method can handle online dynamics perturbations.</p><p>Meta learning (<xref ref-type="bibr" rid="b7">Finn et al., 2017</xref>) has also been applied to address varying source to target domain dynamics (<xref ref-type="bibr" rid="b6">Duan et al., 2017</xref>; <xref ref-type="bibr" rid="b15">Nagabandi et al., 2018</xref>). The idea behind meta learning in the context of robotic learning is to learn a meta policy that is "initialized" for a variety of tasks in simulation, and then fine-tune the policy in the real-world setting given a specific goal. After the meta-learning phase, the agent requires significantly fewer environment interactions to obtain a policy that solves the task. In comparison to meta learning based approaches, fine-tuning on the test environment is Under review as a conference paper at ICLR 2020 not required in our method, with the caveat being that this is true only within the target domain where the dynamics posterior is effective.</p></sec><sec><title>BACKGROUND</title><p>In this section we will briefly review GAIL (<xref ref-type="bibr" rid="b6">Ho &amp; Ermon, 2016</xref>). Inspired by GANs, the GAIL objective is defined as:</p><p>Where &#960; E denotes the expert policy that generated the demonstrations; &#960; &#952; is the policy to imitate the expert; D is a discriminator that learns to distinguish between &#960; &#952; and &#960; E with generated state- action pairs. In comparison to GAN optimization, the GAIL objective is rarely differentiable since differentiation through the environment step is often intractable. Optimization is instead achieved via RL-based policy gradient algorithms, e.g., PPO (<xref ref-type="bibr" rid="b5">Schulman et al., 2017</xref>) or off policy methods, e.g., TD3 (<xref ref-type="bibr" rid="b13">Kostrikov et al., 2018</xref>). Without an explicit reward function, GAIL relies on reward signals provided by the learned discriminator, where a common reward formulation is r &#969; (s, a) = &#8722; log(1 &#8722; D &#969; (s, a)).</p></sec><sec><title>ADAPTIVE ADVERSARIAL IMITATION LEARNING (ADAIL)</title></sec><sec><title>PROBLEM DEFINITION</title><p>Suppose we are given a class E of environments with different dynamics but similar goals, a domain generator g(c) which takes in a code c and generates an environment e c &#8712; E, and a set of expert demonstrations {&#964; exp } collected from one source environment e exp &#8712; E. In adaptive imitation learning, one attempts to learn an adaptive policy &#960; &#952; that can generalize across environments within E. We assume that the ground truth dynamics parameters c, which are used to generate the simulated environments, are given (or manually sampled) during the training phase.</p></sec><sec><title>ALGORITHM OVERVIEW</title><p>We allow the agent to interact with a class of similar simulated environments with varying dynamics parameters, which we call "adaptive training". To be able to capture high-level goals from a small set of demonstrations, we adopt a approach similar to GAIL. To provide consistent feedback signals during training across environments with different dynamics, the discriminator should be dynamics- invariant. We enable this desirable feature by learning a dynamics-invariant feature layer for the discriminator by 1) adding another head D R (c|s, a) to the discriminator to predict the dynamics parameters, and 2) inserting a GRL in-between D R and the dynamics-invariant feature layer. The discriminator design is illustrated in <xref ref-type="fig" rid="fig_0">Figure 1</xref>. In addition, to enable adaptive policies, we introduced a dynamics posterior that takes a roll-out trajectory and outputs an embedding, on which the policy is conditioned. Intuitively, explicit dynamics latent variable learning endows the agent with the ability to identify the system and act differently against changes in dynamics. Note that a policy can learn to infer dynamics implicitly, without the need for an external dynamics embedding. However, we find experimentally that policies conditioned explicitly on the environment parameters outperform those that do not. The overall architecture is illustrated in <xref ref-type="fig" rid="fig_1">Figure 2</xref>. We call the algorithm Adaptive Adversarial Imitation Learning (ADAIL), with the following objective (note that for brevity, we for now omit the GRL term discussed in Section 3.4):</p><p>Where c is a learned latent dynamics representation that is associated with the rollout environment in each gradient step; &#964; is a roll-out trajectory using &#960; &#952; (&#183;|c) in the corresponding environment; Q(c|&#964; ) is a "dynamics posterior" for inferring the dynamics during test time; The last term in the objective, E &#964; &#8764;&#960; &#952; (&#183;|c) [log Q &#966; (c|&#964; )], is a general form of the expected log likelihood of c given &#964; . Note that, the posterior training is on-policy, meaning that the rollouts are collected online using the current Under review as a conference paper at ICLR 2020</p></sec><sec><title>Algorithm 1 ADAIL</title><p>1: Inputs:</p><p>2: An environment class E.</p><p>3: Initial parameters of policy &#952;, discriminator &#969;, and posterior &#966;.</p><p>4: A set of expert demonstrations {&#964;exp} on one of the environment eexp &#8712; E. An environment generator g(c) that takes a code and generates an environment ec &#8712; E. A prior distribution of p(c). policy, thereby the last term of the objective is dependent on &#952;. One can employ various supervised and unsupervised methods towards optimizing this term. We will explore a few methods in the following subsections.</p><p>The algorithm is outlined in Algorithm 1.</p></sec><sec><title>ADAPTIVE TRAINING</title><p>Adaptive training is achieved through 1) allowing the agent to interact with a class of similar simu- lated environments within class E, and 2) learning a dynamics posterior for predicting the dynamics based on rollouts. The environment class E is defined as a set of parameterized environments with n degrees of freedom, where n is the total number of latent dynamics parameters that we can change. We assume that we have access to an environment generator g(c) that takes in a sample of the dynamics parameters c and generates an environment. At each time when an on-policy rollout is initiated, we re-sample the dynamics parameters c based on a predefined prior distribution p(c).</p></sec><sec><title>LEARNING A DYNAMICS-INVARIANT DISCRIMINATOR</title><p>GAIL learns from the expert demonstrations by matching an implicit state-action occupancy mea- sure. However, this formulation might be problematic in our training setting, where on-policy rollouts are collected from environments with varying dynamics. In non-source environments, the discriminator can no longer provide canonical feedback signals. This motivates us to learn a dynamics-invariant feature space, where, the behavior-oriented features are preserved but dynamics- identifiable features are removed. We approach this problem by assuming that the behavior-oriented characteristics and dynamics-identifiable characteristics are loosely coupled and thereby we can learn a dynamics-invariant representation for the discriminator. In particular, we employ a tech- nique called a Gradient Reversal Layer (GRL) (<xref ref-type="bibr" rid="b9">Ganin &amp; Lempitsky, 2014</xref>), which is widely used in image domain adaptation (<xref ref-type="bibr" rid="b2">Bousmalis et al., 2016</xref>). The dynamics-invariant features layer is shared with the original discriminator classification head, illustrated in <xref ref-type="fig" rid="fig_0">Figure 1</xref>.</p></sec><sec><title>DIRECT SUPERVISED DYNAMICS LATENT VARIABLE LEARNING</title><p>Perhaps one of the best latent representations of the dynamics is the ground truth physics param- eterization (gravity, friction, limb length, etc). In this section we explore supervised learning for inferring dynamics. A neural network is employed to represent the dynamics posterior, which is learned via supervised learning by regressing to the ground truth physics parameters given a replay buffer of policy rollouts. We update the regression network using a Huber loss to match environment dynamics labels. Details about the Huber loss can be found in appendix A.1. During training, we condition the learned policy on the ground truth physics parameters. During evaluation, on the other hand, the policy is conditioned on the predicted physics parameters from the posterior.</p><p>We use (state, action, next state) as the posterior's input, i.e., Q &#966; (c|s, a, s ), and a 3-layer fully- connected neural network to output the N-dimensional environment parameters. Note that one can use a recurrent neural network and longer rollout history for modeling complex dynamic structures, however we found that this was not necessary for the chosen evaluation environments.</p></sec><sec><title>VAE-BASED UNSUPERVISED DYNAMICS LATENT VARIABLE LEARNING</title><p>For many cases, the number of varying latent parameters of the environment is high, one might not know the set of latent parameters that will vary in a real world laboratory setting, or the latent parameters are oftentimes strongly correlated (e.g., gravity and friction) in terms of their effect on environment dynamics. In this case, predicting the exact latent parameterization is hard. The policy is mainly concerned with the end effector of the latent parameters. This motivates us to use a unsupervised tool to extract a latent dynamics embedding. In this section, we explore a VAE-based unsupervised approach similar to conditional VAE (<xref ref-type="bibr" rid="b5">Sohn et al., 2015</xref>) with an additional contrastive regularization loss, for learning the dynamics without ground truth labels.</p><p>With the goal of capturing the underlying dynamics, we avoid directly reconstructing the (state, action, next state) tuple, (s, a, s ). Otherwise, the VAE would likely capture the latent structure of the state space. Instead, the decoder is modified to take-in the state-action pair, (s, a), and a latent code, c, and outputs the next state, s . The decoder now becomes a forward dynamics predictive model. The unsupervised dynamics latent variable learning method is illustrated in <xref ref-type="fig" rid="fig_2">Figure 3</xref>.</p><p>Where Q(c|s, a, s ) is the dynamics posterior (encoder); P (s |s, a, c) is a forward dynamics pre- dictive model (decoder); P (c) is a Gaussian prior over the latent code c. Similar to Davis et al. (2007) and <xref ref-type="bibr" rid="b11">Hsu &amp; Kira (2015)</xref>, to avoid the encoder learning an identity mapping on s , we add the following KL-based contrastive regularization to the loss:</p><p>Where (s 0 , a 0 , s 0 ) and (s 1 , a 1 , s 1 ) are sampled from the same roll-out trajectory; (s 2 , a 2 , s 2 ) and (s 3 , a 3 , s 3 ) are sampled from different roll-out trajectories. D 0 is a constant. We use this regular- ization to introduce additional supervision in order to improve the robustness of the latent posterior. The overall objective for the dynamics learner is min &#966;,&#968; &#8722;ELBO + &#955;L contrastive (4) where &#955; is a scalar to control the relative strength of the regularization term. The learned poste- rior (encoder) infers the latent dynamics, which is used for conditioning the policy. The modified algorithm can be found in the appendix (Algorithm 2).</p></sec><sec><title>EXPERIMENTS</title></sec><sec><title>ENVIRONMENTS</title><p>To evaluate the proposed algorithm we consider 4 simulated environments: CartPole, Hopper, HalfCheetah and Ant. The chosen dynamics parameters are specified in table 3, and an example of one such parameter (HalfCheetah gravity component x) is shown in <xref ref-type="fig" rid="fig_3">Figure 4</xref>. During training the parameters are sampled uniformly from the chosen range. Source domain parameters are also given in the table 3. For each source domain, we collect 16 expert demonstrations.</p><p>Gym CartPole-V0: We vary the force magnitude in continuous range [&#8722;1, 1] in our training setting. Note that the force magnitude can take negative values, which flips the force direction.</p><p>3 Mujoco Environments: Hopper, HalfCheetah, and Ant: With these three environments, we vary 2d dynamics parameters: gravity x-component and friction.</p></sec><sec><title>ADAIL ON SIMULATED CONTROL TASKS</title></sec><sec><title>Is the dynamics posterior component effective under large dynamics shifts?</title><p>We first demonstrate the effectiveness of the dynamics posterior under large dynamics shifts on a toy Gym environment, Cartpole, by varying 1d force magnitude. As the direction of the force changes, blindly mimicking the demonstrations collected from the source domain (F m = 1.0) would not work on target domains with F m &lt; 0.0. This result is evident when comparing ADAIL to GAIL with dynamics randomization. As shown in Figure 5a, GAIL with Dynamics Randomization failed to generalize to F m &lt; 0.0, whereas, ADAIL is able to achieve the same performance as F m &gt; 0.0. We also put a comparison with ADAIL-rand, where the policy is conditioned on uniformly random values of the dynamics parameters, which completely breaks the performance across the domains.</p></sec><sec><title>How does the GRL help improve the robustness of performance across domains?</title><p>To demonstrate the effectiveness of GRL in the adversarial imitation learning formulation, we do a comparative study with and without GRL on GAIL with dynamics randomization in the Hopper environment. The results are shown in Figure 5b.</p></sec><sec><title>How does the overall algorithm work in comparison with baseline methods?</title><p>We demonstrate the overall performance of ADAIL by applying it to three Mujoco control tasks: HalfCheetah, Ant and Hopper. For each of the Mujoco environments, we vary 2 continuous dynam- ics parameters and we compare the performance of ADAIL with a few baseline methods, including 1) the PPO expert which was used to collect demonstrations; 2) the UP-true algorithm of <xref ref-type="bibr" rid="b5">Yu et al. (2017)</xref>, which is essentially a PPO policy conditioned on ground truth physics parameters; and 3) GAIL with dynamics randomization, which is unmodified GAIL training on a variety of environ- ments with varying dynamics. The results of this experiment are show in in <xref ref-type="fig" rid="fig_5">Figure 6</xref>.</p></sec><sec><title>HalfCheetah</title><p>The experiments show that 1) as expected the PPO expert (Plot 6a) has limited adapt- ability to unseen dynamics. 2) UP-true (Plot 6b) achieves similar performance across test envi- ronments. Note that since UP-true has access to the ground truth reward signals and the policy is conditioned on ground truth dynamics parameters, the Plot 6b shows an approximate expected up- per bound for our proposed method since we do not assume access to reward signals during policy training, or to ground truth physics parameters at policy evaluation time. 3) GAIL with dynamics randomization (Plot ??) can generalize to some extent, but failed to achieve the demonstrated per- formance in the source environment (gravity x = 0.0, friction = 0.5) 4) Plots 9f 9g show evaluation of the proposed method ADAIL with policy conditioned on ground truth physics parameters and predicted physics parameters respectively; ADAIL matches the expert performance in the source environment (gravity x = 0.0, friction = 0.5) and generalizes to unseen dynamics. In particular, when the environment dynamics favors the task, the adaptive agent was able to obtain even higher performance (around friction = 1.2, gravity = 2).</p></sec><sec><title>Ant and Hopper</title><p>We again show favorable performance on both Ant and Hopper in <xref ref-type="fig" rid="fig_5">Figure 6</xref>. To understand how ADAIL generalizes to environments not sampled at training time, we do a suite of studies in which the agent is only allowed to interact in a limited set of environments. <xref ref-type="fig" rid="fig_6">Figure 7</xref> shows the performance of ADAIL on different settings, where a 5&#215;5 region of environment parame- ters including the expert source environment are "blacked-out". This case is particularly challenging since the policy is not allowed access the domain from which the expert demonstrations were col- lected, and so our dynamics-invariant discriminator is essential. For additional held out experiments see Appendix A.5.</p><p>The experiments show that, 1) without training on the source environment, ADAIL with the ground truth parameters tends to have performance drops on the blackout region but largely is able to gener- alize (Figure 7a); 2) the posterior's RMSE raises on the blackout region (Figure 7c); 3) consequently ADAIL with the predicted dynamics parameters suffers from the posterior error on the blackout re- gion (Figure 7b).</p></sec><sec><title>How does unsupervised version of the algorithm perform?</title><p>VAE-ADAIL on HalfCheetah. With the goal of understanding the characteristics of the learned dy- namics latent embedding through the unsupervised method and its impact on the overall algorithm, as a proof of concept we apply VAE-ADAIL to HalfCheetah environment varying a 1D continuous dynamics, friction. The performance is shown in <xref ref-type="fig" rid="fig_7">Figure 8</xref>.</p></sec><sec><title>CONCLUSION</title><p>In this work we proposed the ADaptive Adversarial Imitation Learning (ADAIL) algorithm for learn- ing adaptive control policies from a limited number of expert demonstrations. We demonstrated the effectiveness of ADAIL on two challenging MuJoCo test suites and compared against recent SoTA. We showed that ADAIL extends the generalization capacities of policies to unseen environments, and we proposed a variant of our algorithm, VAE-ADAIL, that does not require environment dy- namics labels at training time. We will release the code to aid in reproduction upon publication. Under review as a conference paper at ICLR 2020</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Discriminator with Gradients Reversal Layer (GRL). The red layer is the GRL which reverses the gradients during backprop. The yellow layer is a dynamics-invariant layer that is shared with the classification task.</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>The ADAIL architecture. "Environment" is sampled from a population of environments with varying dynamics, "Demonstrations" are collected from one environment within the environ- ment distribution, "Posterior" is the dynamics predictor, Q(c|&#964; ); Latent code "c" represents the ground truth or learned dynamics parameters; The policy input is extended to include the latent dynamics embedding c.</p></caption><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>VAE-based unsupervised dynamics latent variable learning.</p></caption><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Vary x-component of gravity in HalfCheetah environment. The red arrows in the picture show the gravity directions.</p></caption><graphic /><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Environments. F m = Force magnitude; Gx=Gravity x-component; F r = Friction. For each environment, we collect 16 expert demonstrations from the source domain.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>(a): ADAIL on CartPole-V0. Blue: PPO Expert; green: GAIL with Dynamics Ran- domization; red: ADAIL with latent parameters from the dynamics posterior; light blue: ADAIL with uniformly random latent parameters. (b): GAIL with Dynamics Randomization without (left, 2024.89 &#177; 669.39) or with (right, 2453.63 &#177; 430.51) GRL on Hopper</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_5"><object-id>fig_5</object-id><label>Figure 6:</label><caption><title>Figure 6:</title><p>Comparing ADAIL with baselines on Mujoco tasks. Each plot is a heatmap that demon- strates the performance of an algorithm in environments with different dynamics. Each cell of the plot shows 10 episodes averaged cumulative rewards on a particular 2D range of dynamics. Note that to aid visualization, we render plots for Ant in log scale.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_6"><object-id>fig_6</object-id><label>Figure 7:</label><caption><title>Figure 7:</title><p>Generalization of our policy to held out parameters on the HalfCheetah environment. The red rectangles in plots show the blackout regions not seen during policy training.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_7"><object-id>fig_7</object-id><label>Figure 8:</label><caption><title>Figure 8:</title><p>VAE-ADAIL performance on HalfCheetah</p></caption><graphic /><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Apprenticeship learning via inverse reinforcement learning</article-title><source>Proceedings of the twenty-first international conference on Machine learning</source><year>2004</year><fpage>1</fpage><lpage>1</lpage><person-group person-group-type="author"><name><surname>Abbeel</surname><given-names>Pieter</given-names></name><name><surname>Andrew</surname><given-names>Y</given-names></name><name><surname>Ng</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Goal inference as inverse planning</article-title><source>Proceedings of the Annual Meeting of the Cognitive Science Society</source><year>2007</year><volume>29</volume><person-group person-group-type="author"><name><surname>Chris</surname><given-names>L</given-names></name><name><surname>Baker</surname><given-names>Joshua B</given-names></name><name><surname>Tenenbaum</surname><given-names>Rebecca R</given-names></name><name><surname>Saxe</surname><given-names /></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Domain separation networks</article-title><source>Advances in neural information processing systems</source><year>2016</year><fpage>343</fpage><lpage>351</lpage><person-group person-group-type="author"><name><surname>Bousmalis</surname><given-names>Konstantinos</given-names></name><name><surname>Trigeorgis</surname><given-names>George</given-names></name><name><surname>Silberman</surname><given-names>Nathan</given-names></name><name><surname>Krishnan</surname><given-names>Dilip</given-names></name><name><surname>Erhan</surname><given-names>Dumitru</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Closing the sim-to-real loop: Adapting simulation randomization with real world experience</article-title><source>arXiv preprint arXiv:1810.05687</source><year>2018</year><person-group person-group-type="author"><name><surname>Chebotar</surname><given-names>Yevgen</given-names></name><name><surname>Handa</surname><given-names>Ankur</given-names></name><name><surname>Makoviychuk</surname><given-names>Viktor</given-names></name><name><surname>Macklin</surname><given-names>Miles</given-names></name><name><surname>Issac</surname><given-names>Jan</given-names></name><name><surname>Ratliff</surname><given-names>Nathan</given-names></name><name><surname>Fox</surname><given-names>Dieter</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Information-theoretic metric learning</article-title><source>Proceedings of the 24th international conference on Machine learning</source><year>2007</year><fpage>209</fpage><lpage>216</lpage><person-group person-group-type="author"><name><surname>Jason</surname><given-names>V</given-names></name><name><surname>Davis</surname><given-names>Brian</given-names></name><name><surname>Kulis</surname><given-names>Prateek</given-names></name><name><surname>Jain</surname><given-names>Suvrit</given-names></name><name><surname>Sra</surname><given-names>Inderjit S</given-names></name><name><surname>Dhillon</surname><given-names /></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>A survey on policy search for robotics</article-title><source>Foundations and Trends R in Robotics</source><year>2013</year><volume>2</volume><issue>1-2</issue><fpage>1</fpage><lpage>142</lpage><person-group person-group-type="author"><name><surname>Peter Deisenroth</surname><given-names>Marc</given-names></name><name><surname>Neumann</surname><given-names>Gerhard</given-names></name><name><surname>Peters</surname><given-names>Jan</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>One-shot imitation learning</article-title><source>Advances in neural information processing systems</source><year>2017</year><fpage>1087</fpage><lpage>1098</lpage><person-group person-group-type="author"><name><surname>Duan</surname><given-names>Yan</given-names></name><name><surname>Andrychowicz</surname><given-names>Marcin</given-names></name><name><surname>Stadie</surname><given-names>Bradly</given-names></name><name><surname>Openai</surname><given-names>Jonathan</given-names></name><name><surname>Ho</surname><given-names>Jonas</given-names></name><name><surname>Schneider</surname><given-names>Ilya</given-names></name><name><surname>Sutskever</surname><given-names>Pieter</given-names></name><name><surname>Abbeel</surname><given-names>Wojciech</given-names></name><name><surname>Zaremba</surname><given-names /></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Guided cost learning: Deep inverse optimal control via policy optimization</article-title><source>International Conference on Machine Learning</source><year>2016</year><fpage>49</fpage><lpage>58</lpage><person-group person-group-type="author"><name><surname>Finn</surname><given-names>Chelsea</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Model-agnostic meta-learning for fast adaptation of deep networks</article-title><year>2017</year><volume>70</volume><fpage>1126</fpage><lpage>1135</lpage><person-group person-group-type="author"><name><surname>Finn</surname><given-names>Chelsea</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Unsupervised domain adaptation by backpropagation</article-title><source>arXiv preprint arXiv:1409.7495</source><year>2014</year><person-group person-group-type="author"><name><surname>Ganin</surname><given-names>Yaroslav</given-names></name><name><surname>Lempitsky</surname><given-names>Victor</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Generative adversarial imitation learning</article-title><source>Advances in Neural Information Processing Systems</source><year>2016</year><fpage>4565</fpage><lpage>4573</lpage><person-group person-group-type="author"><name><surname>Ho</surname><given-names>Jonathan</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Neural network-based clustering using pairwise constraints</article-title><source>arXiv preprint arXiv:1511.06321</source><year>2015</year><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>Yen-Chang</given-names></name><name><surname>Kira</surname><given-names>Zsolt</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Auto-encoding variational bayes</article-title><source>arXiv preprint arXiv:1312.6114</source><year>2013</year><person-group person-group-type="author"><name><surname>Diederik</surname><given-names>P</given-names></name><name><surname>Kingma</surname><given-names>Max</given-names></name><name><surname>Welling</surname><given-names /></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Addressing sample inefficiency and reward bias in inverse reinforcement learning</article-title><source>ICLR</source><year>2018</year><person-group person-group-type="author"><name><surname>Kostrikov</surname><given-names>Ilya</given-names></name><name><surname>Kumar Krishna Agrawal</surname><given-names>Sergey</given-names></name><name><surname>Levine</surname><given-names>Jonathan</given-names></name><name><surname>Tompson</surname><given-names /></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Adversarially robust policy learning: Active construction of physically-plausible perturbations</article-title><source>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><year>2017</year><fpage>3932</fpage><lpage>3939</lpage><person-group person-group-type="author"><name><surname>Mandlekar</surname><given-names>Ajay</given-names></name><name><surname>Zhu</surname><given-names>Yuke</given-names></name><name><surname>Garg</surname><given-names>Animesh</given-names></name><name><surname>Fei-Fei</surname><given-names>Li</given-names></name><name><surname>Savarese</surname><given-names>Silvio</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Learning to adapt in dynamic, real-world environments through meta- reinforcement learning</article-title><source>arXiv preprint arXiv:1803.11347</source><year>2018</year><person-group person-group-type="author"><name><surname>Nagabandi</surname><given-names>Anusha</given-names></name><name><surname>Clavera</surname><given-names>Ignasi</given-names></name><name><surname>Liu</surname><given-names>Simin</given-names></name><name><surname>Ronald</surname><given-names>S</given-names></name><name><surname>Fearing</surname><given-names>Pieter</given-names></name><name><surname>Abbeel</surname><given-names>Sergey</given-names></name><name><surname>Levine</surname><given-names>Chelsea</given-names></name><name><surname>Finn</surname><given-names /></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>Algorithms for inverse reinforcement learning</article-title><source>Icml</source><year>2000</year><volume>1</volume><fpage>2</fpage><lpage>2</lpage><person-group person-group-type="author"><name><surname>Andrew</surname><given-names>Y</given-names></name><name><surname>Ng</surname><given-names /></name><name><surname>Stuart</surname><given-names>J</given-names></name><name><surname>Russell</surname><given-names /></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Sim-to-real transfer of robotic control with dynamics randomization</article-title><source>IEEE International Conference on Robotics and Automation (ICRA)</source><year>2018</year><fpage>1</fpage><lpage>8</lpage><person-group person-group-type="author"><name><surname>Xue Bin Peng</surname><given-names>Marcin</given-names></name><name><surname>Andrychowicz</surname><given-names>Wojciech</given-names></name><name><surname>Zaremba</surname><given-names>Pieter</given-names></name><name><surname>Abbeel</surname><given-names /></name></person-group></element-citation></ref></ref-list></back></article>