Title:
```
Under review as a conference paper at ICLR 2020 A BOOLEAN TASK ALGEBRA FOR REINFORCEMENT LEARNING
```
Abstract:
```
We propose a framework for defining a Boolean algebra over the space of tasks. This allows us to formulate new tasks in terms of the negation, disjunction and conjunction of a set of base tasks. We then show that by learning goal-oriented value functions and restricting the transition dynamics of the tasks, an agent can solve these new tasks with no further learning. We prove that by composing these value functions in specific ways, we immediately recover the optimal policies for all tasks expressible under the Boolean algebra. We verify our approach in two domains-including a high-dimensional video game environment requiring function approximation-where an agent first learns a set of base skills, and then composes them to solve a super-exponential number of new tasks.
```

Figures/Tables Captions:
```
Figure 1: Consider two tasks, M LEFT and M DOWN , in which an agent must navigate to the left and bottom regions of an xy-plane respectively. From left to right we plot the reward for entering a region of the state space for the individual tasks, the negation of M LEFT , and the union (disjunction) and intersection (conjunction) of tasks. For reference, we also plot the average reward function, which has been used in previous work to approximate the conjunction operator (Haarnoja et al., 2018; Hunt et al., 2019; Van Niekerk et al., 2019). Note that by averaging reward, terminal states that are not in the intersection are erroneously given rewards.
Figure 2: An example of zero-shot Boolean algebraic composition using the learned extended value functions. Arrows represent the optimal action in a given state. (a-b) The learned optimal goal oriented value functions for the base tasks. (c) Zero-shot disjunctive composition. (d) Zero-shot conjunctive composition. (e) Combining operators to model exclusive-or composition. (f) Compo- sition that produces logical nor. Note that the resulting optimal value function can attain a goal not explicitly represented by the base tasks.
Figure 3: Results in comparison to the disjunctive composition of Van Niekerk et al. (2019). (a) The number of samples required to learn the extended value function is greater than learning a standard value function. However, both scale linearly and differ only by a constant factor. (b) The extended value functions allow us to solve exponentially more tasks than the disjunctive approach without further learning. (c) In the modified task with 40 goals, we need to learn only 7 base tasks, as opposed to 40 for the disjunctive case.
Figure 4: By composing extended value functions from the base tasks (collecting blue objects, and collecting squares), we can act optimally in new tasks with no further learning. To generate the value functions, we place the agent at every location and compute the maximum output of the network over all goals and actions. We then interpolate between the points to smooth the graph. Any error in the visualisation is due to the use of non-linear function approximation.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) has achieved recent success in a number of difficult, high-dimensional environments ( Mnih et al., 2015 ;  Levine et al., 2016 ;  Lillicrap et al., 2016 ;  Silver et al., 2017 ). However, these methods generally require millions of samples from the environment to learn op- timal behaviours, limiting their real-world applicability. A major challenge is thus in designing sample-efficient agents that can transfer their existing knowledge to solve new tasks quickly. This is particularly important for agents in a multitask or lifelong setting, since learning to solve complex tasks from scratch is typically infeasible. One approach to transfer is composition (Todorov, 2009), which allows an agent to leverage existing skills to build complex, novel behaviours. These newly-formed skills can then be used to solve or speed up learning in a new task. In this work, we focus on concurrent composition, where existing base skills are combined to produce new skills (Todorov, 2009;  Saxe et al., 2017 ;  Haarnoja et al., 2018 ;  Van Niekerk et al., 2019 ;  Hunt et al., 2019 ;  Peng et al., 2019 ). This differs from other forms of composition, such as options ( Sutton et al., 1999 ) and hierarchical RL ( Bacon et al., 2017 ), where actions and skills are chained in a temporal sequence. In this work, we define a Boolean algebra over the space of tasks and optimal value functions. This extends previous composition results to encompass all Boolean operators: conjunction, disjunction, and negation. We then prove that there exists a homomorphism between the task and value function algebras. Given a set of base tasks that have been previously solved by the agent, any new task written as a Boolean expression can immediately be solved without further learning, resulting in a zero-shot super-exponential explosion in the agent's abilities. We illustrate our approach in a simple domain, where an agent first learns to reach a number of rooms, after which it can then optimally solve any task expressible in the Boolean algebra. We then demonstrate composition in high-dimensional video game environments, where an agent first learns to collect different objects, and then compose these abilities to solve complex tasks immediately. Our results show that, even when function approximation is required, an agent can leverage its existing skills to solve new tasks without further learning.

Section Title: PRELIMINARIES
  PRELIMINARIES We consider tasks modelled by Markov Decision Processes (MDPs). An MDP is defined by the tuple (S, A, ρ, r), where (i) S is the state space, (ii) A is the action space, (iii) ρ is a Markov transition Under review as a conference paper at ICLR 2020 kernel (s, a) → ρ (s,a) from S × A to S, and (iv) r is the real-valued reward function bounded by [r MIN , r MAX ]. In this work, we focus on stochastic shortest path problems ( Bertsekas & Tsitsiklis, 1991 ), which model tasks in which an agent must reach some goal. We therefore consider the class of undiscounted MDPs with an absorbing set G ⊆ S. The goal of the agent is to compute a Markov policy π from S to A that optimally solves a given task. A given policy π is characterised by a value function V π (s) = E π [ ∞ t=0 r(s t , a t )], specifying the expected return obtained under π starting from state s. 1 The optimal policy π * is the policy that obtains the greatest expected return at each state: V π * (s) = V * (s) = max π V π (s) for all s in S. A related quantity is the Q-value function, Q π (s, a), which defines the expected return obtained by executing a from s, and thereafter following π. Similarly, the optimal Q-value function is given by Q * (s, a) = max π Q π (s, a) for all s in S and a in A. Finally, we denote a proper policy to be a policy that is guaranteed to eventually reach the absorbing set G ( James & Collins, 2006 ;  Van Niekerk et al., 2019 ). We assume the value functions for improper policies-those that never reach absorbing states-are unbounded below.

Section Title: BOOLEAN ALGEBRAS FOR TASKS AND VALUE FUNCTIONS
  BOOLEAN ALGEBRAS FOR TASKS AND VALUE FUNCTIONS In this section, we develop the notion of a Boolean task algebra, allowing us to perform logical operations-conjunction (∧), disjunction (∨) and negation (¬)-over the space of tasks. We then show that, having solved a series of base tasks, an agent can use its knowledge to solve tasks ex- pressible as a Boolean expression over those tasks, without any further learning. We consider a family of related MDPs M restricted by the following assumptions: Assumption 1. For all tasks in a set of tasks M, (i) the tasks share the same state space, action space and transition dynamics, (ii) the transition dynamics are deterministic, (iii) reward functions between tasks differ only on the absorbing set G, and (iv) the set of possible terminal rewards consists of only two values. That is, for all (g, a) in G × A, we have that r(g, a) ∈ {r ∅ , r U } ⊂ R with r ∅ ≤ r U . For all non-terminal states, we denote the reward r s,a to emphasise that it is constant across tasks. Assumption 2. For all tasks in a set of tasks M which adhere to Assumption 1, the set of possible terminal rewards consists of only two values. That is, for all (g, a) in G × A, we have that r(g, a) ∈ {r ∅ , r U } ⊂ R with r ∅ ≤ r U . For all non-terminal states, we denote the reward r s,a to emphasise that it is constant across tasks. Assumption 1 is similar to that of  Todorov (2007)  and identical to  Van Niekerk et al. (2019) , and imply that each task can be uniquely specified by its reward function. Furthermore, we note that Assumption 2 is only necessary to formally define the Boolean algebra. Although we have placed restrictions on the reward functions, the above formulation still allows for a large number of tasks to be represented. Importantly, sparse rewards can be formulated under these restrictions.

Section Title: A BOOLEAN ALGEBRA FOR TASKS
  A BOOLEAN ALGEBRA FOR TASKS An abstract Boolean algebra is a set B equipped with operators ¬, ∨, ∧ that satisfy the Boolean axioms of (i) idempotence, (ii) commutativity, (iii) associativity, (iv) absorption, (v) distributivity, (vi) identity, and (vii) complements. 2 Given the above definitions and the restrictions placed on the set of tasks we consider, we can now define a Boolean algebra over a set of tasks. Theorem 1. Let M be a set of tasks. Define M U , M ∅ ∈ M to be tasks with the respective reward functions

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Then M forms a Boolean algebra with universal bounds M ∅ and M U when equipped with the following operators: Proof. See Appendix. Theorem 1 allows us to compose existing tasks together to create new tasks in a principled way.  Figure 1  illustrates the semantics for each of the Boolean operators in a simple environment.

Section Title: EXTENDED VALUE FUNCTIONS
  EXTENDED VALUE FUNCTIONS The reward and value functions described in Section 2 are insufficient to solve tasks specified by the Boolean algebra above. We therefore extend these to define goal-oriented versions of the reward and value function, given by the following two definitions: Definition 1. The extended reward functionr : S × G × A → R is given by the mapping (s, g, a) → N if g = s ∈ G r(s, a) otherwise, (1) where N ≤ min{r MIN , (r MIN − r MAX )D}, and D is the diameter of the MDP ( Jaksch et al., 2010 ). To understand why standard value functions are insufficient, consider two tasks that have multiple different goals, but at least one common goal. Clearly, there is a meaningful conjunction between them-namely, achieving the common goal. Now consider an agent that learns standard value func- tions for both tasks, and which is then required to solve their conjunction without further learning. Note that this is impossible in general, since the regular value function for each task only represents the value of each state with respect to the nearest goal. That is, for all states where the nearest goal for each task is not the common goal, the agent has no information about that common goal. Conversely, by learning extended value functions, the agent is able to learn the value of achieving all goals, and not simply the nearest one. Because we require that tasks share the same transition dynamics, we also require that the absorbing set of states is shared. Thus the extended reward function adds the extra constraint that, if the agent enters a terminal state for a different task, it should receive the largest penalty possible. In practice, we can simply set N to be the lowest finite value representable by the data type used for rewards. Definition 2. The extended Q-value functionQ : S × G × A → R is given by the mapping (s, g, a) →r(s, g, a) + SVπ (s , g)ρ (s,a) (ds ), (2) whereVπ(s, g) = Eπ [ ∞ t=0r (s t , g, a t )]. The extended Q-value function is similar to universal value function approximators (UVFAs) ( Schaul et al., 2015 ), but differs in that it uses the extended reward function definition. It is also similar to DG functions ( Kaelbling, 1993 ), except here we use task-dependent reward functions, as opposed to measuring distance between states. The standard reward functions and value functions can be recovered from their extended versions through the following lemma. (ii): Each g in G can be thought of as defining an MDP M g := (S, A, ρ, r Mg ) with reward function In the same way, we can also recover the optimal policy from these extended value functions by first applying Lemma 1, and acting greedily with respect to the resulting value function. Proof. See Appendix. Combining Lemmas 1 and 2, we can extract the greedy action from the extended value func- tion by first maximising over goals, and then selecting the maximising action: π * (s) ∈ arg max a∈A max g∈GQ * (s, g, a). If we consider the extended value function to be a set of standard value functions (one for each goal), then this is equivalent to first performing generalised policy improvement ( Barreto et al., 2017 ), and then selecting the greedy action. Finally, much like the regular definition of value functions, the extended Q-value function can be written as the sum of rewards received by the agent until first encountering a terminal state. Proof. This follows directly from Lemma 2. Since all tasks M ∈ M share the same optimal policy π * g up to (but not including) the goal state g ∈ G, their return G π * g T −1 = T −1 t=0 r M (s t , π * g (s t )) is the same up to (but not including) g.

Section Title: A BOOLEAN ALGEBRA FOR VALUE FUNCTIONS
  A BOOLEAN ALGEBRA FOR VALUE FUNCTIONS In the same manner we constructed a Boolean algebra over a set of tasks, we can also do so for a set of optimal extended Q-value functions for the corresponding tasks. Theorem 2. LetQ * be the set of optimal extendedQ-value functions for tasks in M. Definē Q * ∅ ,Q * U ∈Q * to be the optimalQ-functions for the tasks M ∅ , M U ∈ M. ThenQ * forms a Boolean algebra when equipped with the following operators: Proof. See Appendix.

Section Title: BETWEEN TASK AND VALUE FUNCTION ALGEBRAS
  BETWEEN TASK AND VALUE FUNCTION ALGEBRAS Having established a Boolean algebra over tasks and extended value function, we finally show that there exists an equivalence between the two. As a result, if we can write down a task under the Boolean algebra, we can immediately write down the optimal value function for the task. Theorem 3. Let F : M →Q * be any map from M toQ * such that F(M ) =Q * M for all M in M. Then F is a homomorphism. Proof. See Appendix.

Section Title: ZERO-SHOT TRANSFER THROUGH COMPOSITION
  ZERO-SHOT TRANSFER THROUGH COMPOSITION We can use the theory developed in the previous sections to perform zero-shot transfer by first learning extended value functions for a set of base tasks, and then composing them to solve new tasks expressible under the Boolean algebra. To demonstrate this, we conduct a series of experiments in a Four Rooms domain ( Sutton et al., 1999 ), where an agent must navigate in a grid world to a particular location. The agent can move in any of the four cardinal directions at each timestep, but colliding with a wall leaves the agent in the same location. The transition dynamics are deterministic, and rewards are −0.1 for all non-terminal states, and 1 at the goal.

Section Title: LEARNING BASE TASKS
  LEARNING BASE TASKS We use a modified version of Q-learning ( Watkins, 1989 ) to learn extended Q-value functions de- scribed previously. Our algorithm differs in a number of ways from standard Q-learning: we keep track of the set of terminating states seen so far, and at each timestep we update the extended Q-value function with respect to both the current state and action, as well as all goals encountered so far. We also use the definition of the extended reward function, and so if the agent encounters a terminal state of a different task, it receives reward N . The full pseudocode is listed in the Appendix. If we know the set of goals (and hence potential base tasks) upfront, then it is easy to select a minimal set of base tasks that can be composed to produce the largest number of composite tasks. We first assign a Boolean label to each goal in a table, and then use the columns of the table as base tasks. The goals for each base task are then those goals with value 1 according to the table. In this domain, the two base tasks we select are M T , which requires that the agent visit either of the top two rooms, and M L , which requires visiting the two left rooms. We illustrate this selection procedure in the Appendix.

Section Title: BOOLEAN COMPOSITION
  BOOLEAN COMPOSITION Having learned the optimal extended value functions for our base tasks, we can now leverage The- orems 1-3 to solve new tasks with no further learning.  Figure 2  illustrates this composition, where an agent is able to immediately solve complex tasks such as exclusive-or. We illustrate a few com- posite tasks here, but note that in general, if we have K base tasks, then a Boolean algebra allows for 2 2 K new tasks to be constructed. Thus having trained on only two tasks, our agent has enough information to solve a total of 16 composite tasks. By learning extended value functions, an agent can subsequently solve a massive number of tasks; however, the upfront cost of learning is likely to be higher. We investigate the trade-off between the two approaches by investigating how the sample complexity scales with the number of tasks. We compare to  Van Niekerk et al. (2019) , who used regular value functions to demonstrate optimal disjunctive composition. We note that while the upfront learning cost is therefore lower, the number of tasks expressible using only disjunction is 2 K −1, which is significantly less than the full Boolean algebra. We also test using an extended version of the Four Rooms domain, where additional goals are placed along the sides of all walls, resulting in a total of 40 goals. Empirical results are illustrated by  Figure 3 . Our results show that while additional samples are needed to learn an extended value function, the agent is able to expand the tasks it can solve super-exponentially. Furthermore, the number of base tasks we need to solve is only logarithmic in the number of goal states. For an environment with K goals, we need to learn only log 2 K + 1 base tasks, as opposed to the disjunctive approach which requires K base tasks. Thus by sacrificing sample efficiency initially, we achieve an exponential increase in abilities compared to previous work ( Van Niekerk et al., 2019 ).

Section Title: COMPOSITION WITH FUNCTION APPROXIMATION
  COMPOSITION WITH FUNCTION APPROXIMATION Finally, we demonstrate that our compositional approach can also be used to tackle high-dimensional domains where function approximation is required. We use the same video game environment as  Van Niekerk et al. (2019) , where an agent must navigate a 2D world and collect objects of different shapes and colours. The state space is an 84×84 RGB image, and the agent is able to move in any of the four cardinal directions. The agent also possesses a pick-up action, which allows it to collect an object when standing on top of it. There are two shapes (squares and circles) and three colours (blue, beige and purple) for a total of six unique objects. The position of the agent is randomised at the start of each episode. We modify deep Q-learning ( Mnih et al., 2015 ) to learn extended action-value functions. 4 Our approach differs in that the network takes a goal state as additional input (again specified as an RGB image). Additionally, when a terminal state is encountered, it is added to the collection of goals seen so far, and when learning updates occur, these goals are sampled randomly from a replay buffer. We first learn to solve two base tasks: collecting blue objects, and collecting squares, which can then be composed to solve new tasks immediately. We demonstrate composition characterised by (i) disjunction, (ii) conjunction and (iii) exclusive- or. This corresponds to tasks where the target items are: (i) blue or square, (ii) blue squares, and (iii) blue or squares, but not blue squares.  Figure 4  illustrates sample trajectories, as well as the subsequent composed value functions, for the respective tasks.

Section Title: RELATED WORK
  RELATED WORK The ability to compose value functions was first demonstrated using the linearly-solvable MDP framework ( Todorov, 2007 ), where value functions could be composed to solve tasks similar to the disjunctive case (Todorov, 2009). Van  Niekerk et al. (2019)  show that the same kind of composition can be achieved using entropy-regularised RL ( Fox et al., 2016 ), and extend the results to the stan- dard RL setting, where agents can optimally solve the disjunctive case. Using entropy-regularised RL,  Haarnoja et al. (2018)  approximates the conjunction of tasks by averaging their reward func- tions, and demonstrates that by averaging the optimal value functions of the respective tasks, the agent can achieve performance close to optimal.  Hunt et al. (2019)  extends this result by composing value functions to solve the average reward task exactly, which approximates the true conjunctive case. More recently,  Peng et al. (2019)  introduce a few-shot learning approach to compose policies Under review as a conference paper at ICLR 2020 (a) Trajectories for disjunctive composition. multiplicatively. Although lacking theoretical foundations, results show that an agent can learn a weighted composition of existing base skills to solve a new complex task. By contrast, we show that zero-shot optimal composition can be achieved for all Boolean operators.

Section Title: CONCLUSION
  CONCLUSION We have shown how to compose tasks using the standard Boolean algebra operators. These com- posite tasks can be immediately solved by first learning goal-oriented value functions, and then composing them in a similar manner. Finally, we note that there is much room for improvement in learning the extended value functions for the base tasks. In our experiments, we learned each extended value function from scratch, but it is likely that having learned one for the first task, we could use it to initialise the extended value function for the second task to improve convergence times. One area for improvement lies in efficiently learning the extended value functions, as well as developing better algorithms for solving tasks with sparse rewards. For example, it is likely that approaches such as hindsight experience replay ( Andrychowicz et al., 2017 ) could reduce the num- ber of samples required to learn extended value functions, while  Mirowski et al. (2017)  provides a method for learning complex tasks with sparse rewards using auxiliary tasks. We leave incorporating these approaches to future work. Our proposed approach is a step towards both interpretable RL- since both the tasks and optimal value functions can be specified using Boolean operators-and the ultimate goal of lifelong learning agents, which are able to solve combinatorially many tasks in a sample-efficient manner. Under review as a conference paper at ICLR 2020

```
