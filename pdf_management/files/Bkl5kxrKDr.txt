Title:
```
Published as a conference paper at ICLR 2020 A GENERALIZED TRAINING APPROACH FOR MULTIAGENT LEARNING
```
Abstract:
```
This paper investigates a population-based training regime based on game-theoretic principles called Policy-Spaced Response Oracles (PSRO). PSRO is general in the sense that it (1) encompasses well-known algorithms such as fictitious play and double oracle as special cases, and (2) in principle applies to general-sum, many- player games. Despite this, prior studies of PSRO have been focused on two-player zero-sum games, a regime wherein Nash equilibria are tractably computable. In moving from two-player zero-sum games to more general settings, computation of Nash equilibria quickly becomes infeasible. Here, we extend the theoretical underpinnings of PSRO by considering an alternative solution concept, α-Rank, which is unique (thus faces no equilibrium selection issues, unlike Nash) and applies readily to general-sum, many-player settings. We establish convergence guarantees in several games classes, and identify links between Nash equilibria and α-Rank. We demonstrate the competitive performance of α-Rank-based PSRO against an exact Nash solver-based PSRO in 2-player Kuhn and Leduc Poker. We then go beyond the reach of prior PSRO applications by considering 3- to 5-player poker games, yielding instances where α-Rank achieves faster convergence than approximate Nash solvers, thus establishing it as a favorable general games solver. We also carry out an initial empirical validation in MuJoCo soccer, illustrating the feasibility of the proposed approach in another complex domain.
```

Figures/Tables Captions:
```
Figure 2: Oracle comparisons for randomly-generated games with varying player strategy space sizes |S k |. Top and bottom rows, respectively, correspond to 4- and 5-player games.
Figure 3: Results for 2-player poker domains.
Figure 4: Results for poker domains with more than 2 players.
Table 1: Theory overview. SP and MP, resp., denote single and multi-population games. BR and PBR, resp., denote best response and preference-based best response. † Defined in the noted propositions.
Table 2: Symmetric zero-sum game used to analyze the behavior of PSRO in Example 1.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Creating agents that learn to interact in large-scale systems is a key challenge in artificial intelligence. Impressive results have been recently achieved in restricted settings (e.g., zero-sum, two-player games) using game-theoretic principles such as iterative best response computation ( Lanctot et al., 2017 ), self-play ( Silver et al., 2018 ), and evolution-based training ( Jaderberg et al., 2019 ;  Liu et al., 2019 ). A key principle underlying these approaches is to iteratively train a growing population of player policies, with population evolution informed by heuristic skill ratings (e.g., Elo ( Elo, 1978 )) or game-theoretic solution concepts such as Nash equilibria. A general application of this principle is embodied by the Policy-Space Response Oracles (PSRO) algorithm and its related extensions ( Lanctot et al., 2017 ;  Balduzzi et al., 2019 ). Given a game (e.g., poker), PSRO constructs a higher- level meta-game by simulating outcomes for all match-ups of a population of players' policies. It then trains new policies for each player (via an oracle) against a distribution over the existing meta-game Published as a conference paper at ICLR 2020 policies (typically an approximate Nash equilibrium, obtained via a meta-solver 1 ), appends these new policies to the meta-game population, and iterates. In two-player zero sum games, fictitious play ( Brown, 1951 ), double oracle ( McMahan et al., 2003 ), and independent reinforcement learning can all be considered instances of PSRO, demonstrating its representative power ( Lanctot et al., 2017 ). Prior applications of PSRO have used Nash equilibria as the policy-selection distribution ( Lanctot et al., 2017 ;  Balduzzi et al., 2019 ), which limits the scalability of PSRO to general games: Nash equilibria are intractable to compute in general ( Daskalakis et al., 2009 ); computing approximate Nash equilibria is also intractable, even for some classes of two-player games ( Daskalakis, 2013 ); finally, when they can be computed, Nash equilibria suffer from a selection problem ( Harsanyi et al., 1988 ;  Goldberg et al., 2013 ). It is, thus, evident that the reliance of PSRO on the Nash equilibrium as the driver of population growth is a key limitation, preventing its application to general games. Recent work has proposed a scalable alternative to the Nash equilibrium, called α-Rank, which applies readily to general games ( Omidshafiei et al., 2019 ), making it a promising candidate for population-based training. Given that the formal study of PSRO has only been conducted under the restricted settings determined by the limitations of Nash equilibria, establishing its theoretical and empirical behaviors under alternative meta-solvers remains an important and open research problem. We study several PSRO variants in the context of general-sum, many-player games, providing convergence guarantees in several classes of such games for PSRO instances that use α-Rank as a meta-solver. We also establish connections between Nash and α-Rank in specific classes of games, and identify links between α-Rank and the Projected Replicator Dynamics employed in prior PSRO instances ( Lanctot et al., 2017 ). We develop a new notion of best response that guarantees convergence to the α-Rank distribution in several classes of games, verifying this empirically in randomly-generated general-sum games. We conduct empirical evaluations in Kuhn and Leduc Poker, first establishing our approach as a competitive alternative to Nash-based PSRO by focusing on two-player variants of these games that have been investigated in these prior works. We subsequently demonstrate empirical results extending beyond the reach of PSRO with Nash as a meta-solver by evaluating training in 3- to 5-player games. Finally, we conduct preliminary evaluations in MuJoCo soccer ( Liu et al., 2019 ), another complex domain wherein we use reinforcement learning agents as oracles in our proposed PSRO variants, illustrating the feasibility of the approach.

Section Title: PRELIMINARIES
  PRELIMINARIES

Section Title: Games
  Games We consider K-player games, where each player k ∈ [K] has a finite set of pure strategies S k . Let S = k S k denote the space of pure strategy profiles. Denote by S −k = l =k S l the set of pure strategy profiles excluding those of player k. Let M (s) = (M 1 (s), . . . , M K (s)) ∈ R K denote the vector of expected player payoffs for each s ∈ S. A game is said to be zero-sum if k M k (s) = 0 for all s ∈ S. A game is said to be symmetric if all players have identical strategy sets S k , and for any permutation ρ, strategy profile (s 1 , . . . , s K ) ∈ S, and index k ∈ [K], one has M k (s 1 , . . . , s K ) = M ρ(k) (s ρ(1) , . . . , s ρ(K) ). A mixed strategy profile is defined as π ∈ ∆ S , a tuple representing the probability distribution over pure strategy profiles s ∈ S. The expected payoff to player k under a mixed strategy profile π is given by M k (π) = s∈S π(s)M k (s). Nash Equilibrium (NE) Given a mixed profile π, the best response for a player k is de- fined BR k (π) = arg max ν∈∆ S k [M k (ν, π −k )]. A factorized mixed profile π(s) = k π k (s k ) is a Nash equilibrium (NE) if π k ∈ BR k (π) for all k ∈ [K]. Define NASHCONV(π) = k M k (BR k (π), π −k ) − M k (π); roughly speaking, this measures "distance" from an NE ( Lanctot et al., 2017 ). In prior PSRO instances ( Lanctot et al., 2017 ), a variant of the replicator dynamics ( Taylor and Jonker, 1978 ;  Maynard Smith and Price, 1973 ), called the Projected Replicator Dynamics (PRD), has been used as an approximate Nash meta-solver (see Appendix E for details on PRD).

Section Title: Oracles
  Oracles We define an oracle O as an abstract computational entity that, given a game, computes policies with precise associated properties. For instance, a best-response oracle O k (π) = BR k (π) computes the best-response policy for any player k, given a profile π. One may also consider approximate-best-response oracles that, e.g., use reinforcement learning to train a player k's policy against a fixed distribution over the other players' policies, π −k . Oracles play a key role in population- based training, as they compute the policies that are incrementally added to players' growing policy populations ( McMahan et al., 2003 ;  Lanctot et al., 2017 ;  Balduzzi et al., 2019 ). The choice of oracle O also affects the training convergence rate and final equilibrium reached (e.g., Nash or α-Rank). Empirical Game-theoretic Analysis PSRO relies on principles from empirical game-theoretic analysis (EGTA) ( Walsh et al., 2002 ;  Phelps et al., 2004 ;  Wellman, 2006 ). Given a game (e.g., poker), EGTA operates via construction of a higher-level 'meta-game', where strategies s correspond to policies (e.g., 'play defensively' in poker) rather than atomic actions (e.g., 'fold'). A meta-payoff table M is then constructed by simulating games for all joint policy combinations, with entries corresponding to the players' expected utilities under these policies. Game-theoretic analysis can then be conducted on the meta-game in a manner analogous to the lower-level game, albeit in a much more scalable manner. As the theoretical discussion hereafter pertains to the meta-game, we use s, M , and π to respectively refer to policies, payoffs, and distributions at the meta-level, rather than the underlying low-level game. In our analysis, it will be important to distinguish between SSCCs of the underlying game, and of the meta-game constructed by PSRO; we refer to the latter as meta-SSCCs.

Section Title: POLICY-SPACE RESPONSE ORACLES: NASH AND BEYOND
  POLICY-SPACE RESPONSE ORACLES: NASH AND BEYOND We first overview Policy-Space Response Oracles (PSRO) prior to presenting our findings. Given an underlying game (e.g., Poker), PSRO first initializes the policy space S using randomly-generated policies, then expands the players' policy populations in three iterated phases: complete, solve, and Published as a conference paper at ICLR 2020 expand (see Algorithm 1 and Fig. 1). In the complete phase, a meta-game consisting of all match-ups of these joint policies is synthesized, with missing payoff entries in M completed through game simulations. Next, in the solve phase, a meta-solver M computes a profile π over the player policies (e.g., Nash, α-Rank, or uniform distributions). Finally, in the expand phase, an oracle O computes at least one new policy s k for each player k ∈ [K], given profile π. As other players' policy spaces S −k and profile π −k are fixed, this phase involves solving a single-player optimization problem. The new policies are appended to the respective players' policy sets, and the algorithm iterates. We use PSRO(M, O) to refer to the PSRO instance using meta-solver M and oracle O. Notably, PSRO-based training for two-player symmetric games can be conducted using a single population of policies that is shared by all players (i.e., S k is identical for all k). Thus, we henceforth refer to two- player symmetric games as 'single-population games', and more generally refer to games that require player-specific policy populations as 'multi-population games'. Recent investigations of PSRO have solely focused on Nash-based meta-solvers and best-response-based oracles ( Lanctot et al., 2017 ;  Balduzzi et al., 2019 ), with theory focused around the two-player zero-sum case. Unfortunately, these guarantees do not hold in games beyond this regime, making investigation of alternative meta-solvers and oracles critical for further establishing PSRO's generalizability.

Section Title: GENERALIZING PSRO THEORY
  GENERALIZING PSRO THEORY This section establishes theoretical properties of PSRO for several useful classes of general games. We summarize our results in  Table 1 , giving a full exposition below. It is well-known that PSRO(Nash, BR) will eventually return an NE in two-player zero-sum games ( McMahan et al., 2003 ). In more general games, where Nash faces the issues outlined earlier, α-Rank appears a promis- ing meta-solver candidate as it applies to many-player, general-sum games and has no selection problem. How- ever, open questions remain regarding convergence guarantees of PSRO when using α-Rank, and whether standard BR oracles suffice for ensuring these guaran- tees. We investigate these theoretical questions, namely, whether particular variants of PSRO can converge to the α-Rank distribution for the underlying game. A first attempt to establish convergence to α-Rank might involve running PSRO to convergence (until the oracle returns a strategy already in the convex hull of the known strategies), using α-Rank as the meta-solver, and a standard best response oracle. However, the following example shows that this will not work in general for the single-population case (see Fig. A.5 for a step-by-step illustration). Example 1. Consider the symmetric zero-sum game specified in  Table 2 . As X is the sole sink component of the game's response graph (as illustrated in Fig. A.5a), the single-population α-Rank distribution for this game puts unit mass on X. We now show that a PSRO algorithm that computes best responses to the α-Rank distribution over the current strategy set need not recover strategy X, by computing directly the strategy sets of the algorithm initialized with the set {C}. 1. The initial strategy space consists only of the strategy C; the best response against C is D. 2. The α-Rank distribution over {C, D} puts all mass on D; the best response against D is A. 3. The α-Rank distribution over {C, D, A} puts all mass on A; the best response against A is B. 4. The α-Rank distribution over {C, D, A, B} puts mass ( 1 /3, 1 /3, 1 /6, 1 /6) on (A, B, C, D) respec- tively. For φ sufficiently large, the payoff that C receives against B dominates all others, and since B has higher mass than C in the α-Rank distribution, the best response is C. Thus, PSRO(α-Rank, BR) leads to the algorithm terminating with strategy set {A, B, C, D} and not discovering strategy X in the sink strongly-connected component. This conclusion also holds in the multi-population case, as the following counterexample shows. Example 2. Consider the game in  Table 2 , treating it now as a multi-population problem. It is readily verified that the multi-population α-Rank distributions obtained by PSRO with initial strategy sets consisting solely of C for each player are: (i) a Dirac delta at the joint strategy (C, C), leading to best responses of D for both players; (ii) a Dirac delta at (D, D) leading to best responses of A for both players; (iii) a Dirac delta at (A, A), leading to best responses of B for both players; and finally (iv) a distribution over joint strategies of the 4×4 subgame induced by strategies A, B, C, D that leads to a best response not equal to X; thus, the full α-Rank distribution is again not recovered.

Section Title: A NEW RESPONSE ORACLE
  A NEW RESPONSE ORACLE The previous examples indicate that the use of standard best responses in PSRO may be the root cause of the incompatibility with the α-Rank solution concept. Thus, we define the Preference-based Best Response (PBR) oracle, which is more closely aligned with the dynamics defining α-Rank, and which enables us to establish desired PSRO guarantees with respect to α-Rank.

Section Title: Consider first the single-population case
  Consider first the single-population case where the arg max returns the set of policies optimizing the objective, and the optimization is over pure strategies in the underlying game. The intuition for the definition of PBR is that we would like the oracle to return strategies that will receive high mass under α-Rank when added to the population; objective (1) essentially encodes the probability flux that the vertex corresponding to σ would receive in the random walk over the α-Rank response graph (see Section 2 or Appendix D for further details). We demonstrate below that the use of the PBR resolves the issue highlighted in Example 1 (see Fig. A.6 in Appendix A for an accompanying visual). Example 3. Steps 1 to 3 of correspond exactly to those of Example 1. In step 4, the α-Rank distribution over {C, D, A, B} puts mass ( 1 /3, 1 /3, 1 /6, 1 /6) on (A, B, C, D) respectively. A beats C and D, thus its PBR score is 1 /3. B beats A and D, thus its PBR score is 1 /2. C beats B, its PBR score is thus 1 /3. D beats C, its PBR score is thus 1 /6. Finally, X beats every other strategy, and its PBR score is thus 1. Thus, there is only one strategy maximizing PBR, X, which is then chosen, thereby recovering the SSCC of the game and the correct α-Rank distribution at the next timestep. In the multi-population case, consider a population of N strategy profiles {s 1 , . . . , s N } and corre- sponding meta-solver distribution (π i ) N i=1 . Several meta-SSCCs may exist in the multi-population α-Rank response graph. In this case, we run the PBR oracle for each meta-SSCC separately, as follows. Suppose there are meta-SSCCs, and denote by π ( ) the distribution π restricted to the th meta-SSCC, for all 1 ≤ ≤ L. The PBR for player k on the th meta-SSCC is then defined by Thus, the PBR oracle generates one new strategy for each player for every meta-SSCC in the α-Rank response graph; we return this full set of strategies and append to the policy space accordingly, as in Line 5 of Algorithm 1. Intuitively, this leads to a diversification of strategies introduced by the oracle, as each new strategy need only perform well against a subset of prior strategies. This hints at interesting links with the recently-introduced concept of rectified-Nash BR ( Balduzzi et al., 2019 ), which also attempts to improve diversity in PSRO, albeit only in two-player zero-sum games. We henceforth denote PSRO(α-Rank, PBR) as α-PSRO for brevity. We next define α-CONV, an approximate measure of convergence to α-Rank. We restrict discussion to the multi-population case here, describing the single-population case in Appendix A.4. With the notation introduced above, we define PBR-SCORE k (σ; π, S) = i π ( ) i 1 M k (σ, s −k i ) > M k (s k i , s −k i ) , and α-CONV = k max σ PBR-SCORE k (σ) − max s∈S k PBR-SCORE k (s) , where max σ is taken over the pure strategies of the underlying game. Unfortunately, in the multi-population case, a PBR-SCORE of 0 does not necessarily imply α-partial con- vergence. We thus introduce a further measure, PCS-SCORE, defined by PCS-SCORE = # of α-PSRO strategy profiles in the underlying game's SSCCs # of α-PSRO strategy profiles in meta-SSCCs , which assesses the quality of the α-PSRO popula- tion. We refer readers to Appendix C.3 for pseudocode detailing how to implement these measures in practice.

Section Title: α-PSRO: THEORY, PRACTICE, AND CONNECTIONS TO NASH
  α-PSRO: THEORY, PRACTICE, AND CONNECTIONS TO NASH We next study the theoretical properties of PSRO(α-Rank, PBR), which we henceforth refer to as α-PSRO for brevity. We consider that α-PSRO has converged if no new strategy has been returned by PBR for any player at the end of an iteration. Proofs of all results are provided in Appendix B. Definition 1. A PSRO algorithm is said to converge α-fully (resp., α-partially) to an SSCC of the underlying game if its strategy population contains the full SSCC (resp., a sub-cycle of the SSCC, denoted a 'sub-SSCC') after convergence. Definition 2. We also adapt PBR to be what we call novelty-bound by restricting the arg max in Equation (1) to be over strategies not already included in the population with PBR-SCORE > 0. In particular, the novelty-bound version of the PBR oracle is given by restricting the arg max appearing in (2) to only be over strategies not already present in the population. These definitions enable the following results for α-PSRO in the single- and multi-population cases. Intuitively, the lack of convergence without a novelty-bound oracle can occur due to intransitivities in the game (i.e., cycles in the game can otherwise trap the oracle). An example demonstrating this issue is shown in Fig. B.7, with an accompanying step-by-step walkthrough in Appendix B.4. Specifically, SSCCs may be hidden by "intermediate" strategies that, while not receiving as high a payoff as current population-pool members, can actually lead to well-performing strategies outside the population. As these "intermediate" strategies are avoided, SSCCs are consequently not found. Note also that this is related to the common problem of action/equilibrium shadowing, as detailed in  Matignon et al. (2012) . In Section 5, we further investigate convergence behavior beyond the conditions studied above. In practice, we demonstrate that despite the negative result of Proposition 4, α-PSRO does significantly increase the probability of converging to an SSCC, in contrast to PSRO(Nash, BR). Overall, we have shown that for general-sum multi-player games, it is possible to give theoretical guarantees Published as a conference paper at ICLR 2020 for a version of PSRO driven by α-Rank in several circumstances. By contrast, using exact NE in PSRO is intractable in general. In prior work, this motivated the use of approximate Nash solvers generally based on the simulation of dynamical systems or regret minimization algorithms, both of which generally require specification of several hyperparameters (e.g., simulation iterations, window sizes for computing time-average policies, and entropy-injection rates), and a greater computational burden than α-Rank to carry out the simulation in the first place.

Section Title: Implementing the PBR Oracle
  Implementing the PBR Oracle Recall from Section 3 that the BR oracle inherently solves a single- player optimization problem, permitting use of a single-agent RL algorithm as a BR approximator, which is useful in practice. As noted in Section 4.1, however, there exist games where the BR and PBR objectives are seemingly incompatible, preventing the use of standard RL agents for PBR approximation. While exact PBR is computable in small-scale (e.g., normal-form) games, we next consider more general games classes where PBR can also be approximated using standard RL agents. Definition 3. Objective A is 'compatible' with objective B if any solution to A is a solution to B. Proposition 5. A constant-sum game is denoted as win-loss if M k (s) ∈ {0, 1} for all k ∈ [K] and s ∈ S. BR is compatible with PBR in win-loss games in the two-player single-population case. Proposition 6. A symmetric two-player game is denoted monotonic if there exists a function f : S → R and a non-decreasing function σ : R → R such that M 1 (s, ν) = σ(f (s) − f (ν)). BR is compatible with PBR in monotonic games in the single-population case. Finally, we next demonstrate that under certain conditions, there are strong connections between the PBR objective defined above and the broader field of preference-based RL ( Wirth et al., 2017 ). Proposition 7. Consider symmetric win-loss games where outcomes between deterministic strategies are deterministic. A preference-based RL agent (i.e., an agent aiming to maximize its probability of winning against a distribution π of strategies {s 1 , . . . , s N }) optimizes exactly the PBR objective (1). Given this insight, we believe an important subject of future work will involve the use of preference- based RL algorithms in implementing the PBR oracle for more general classes of games. We conclude this section with some indicative results of the relationship between α-Rank and NE. Proposition 8. For symmetric two-player zero-sum games where off-diagonal payoffs have equal magnitude, all NE have support contained within that of the single-population α-Rank distribution. Proposition 9. In a symmetric two-player zero-sum game, there exists an NE with support contained within that of the α-Rank distribution. For more general games, the link between α-Rank and Nash equilibria will likely require a more complex description. We leave this for future work, providing additional discussion in Appendix A.3.

Section Title: EVALUATION
  EVALUATION We conduct evaluations on games of increasing complexity, extending beyond prior PSRO applications that have focused on two-player zero-sum games. For experimental procedures, see Appendix C.

Section Title: Oracle comparisons
  Oracle comparisons We evaluate here the performance of the BR and PBR oracles in games where PBR can be exactly computed. We consider randomly generated, K-player, general-sum games with increasing strategy space sizes, |S k |.  Figure 2  reports these results for the 4- and 5-player instances (see Appendix C.4 for 2-3 player results). The asymmetric nature of these games, in combination with the number of players and strategies involved, makes them inherently, and perhaps surprisingly, large in scale. For example, the largest considered game in  Fig. 2  involves 5 players with 30 strategies each, making for a total of more than 24 million strategy profiles in total. For each combination of K and |S k |, we generate 1e6 random games. We conduct 10 trials per game, in each trial running the BR and PBR oracles starting from a random strategy in the corresponding response graph, then iteratively expanding the population space until convergence. Importantly, this implies that the starting strategy may not even be in an SSCC. As mentioned in Section 4.2, α-CONV and PCS-SCORE jointly characterize the oracle behaviors in these multi-population settings. Figure 2a plots α-CONV for both oracles, demonstrating that PBR outperforms BR in the sense that it captures more of the game SSCCs. Figures 2b and 2c, respectively, Published as a conference paper at ICLR 2020 plot the PCS-SCORE for BR and PBR over all game instances. The PCS-SCORE here is typically either (a) greater than 95%, or (b) less than 5%, and otherwise rarely between 5% to 95%. For all values of |S k |, PBR consistently discovers a larger proportion of the α-Rank support in contrast to BR, serving as useful validation of the theoretical results of Section 4.3.

Section Title: Meta-solver comparisons
  Meta-solver comparisons We consider next the standard benchmarks of Kuhn and Leduc poker ( Kuhn, 1950 ;  Southey et al., 2005 ;  Lanctot et al., 2019 ). We detail these domains in Appendix C.2, noting here that both are K-player, although Leduc is significantly more complex than Kuhn. We first consider two-player instances of these poker domains, permitting use of an exact Nash meta-solver.  Figure 3  compares the NASHCONV of PSRO(M, BR) for various meta-solver M choices. Note that the x axis of  Figure 3  and  Figure 4  is the Total Pool Length (The sum of the length of each player's pool in PSRO) instead of the number of iterations of PSRO, since Rectified solvers can add more than one policy to the pool at each PSRO iteration (Possibly doubling pool size at every PSRO iteration). It is therefore more pertinent to compare exploitabilities at the same pool sizes rather than at the same number of PSRO iterations. In Kuhn poker (Fig. 3a), the α-Rank, Nash, and the Projected Replicator Dynamics (PRD) meta- solvers converge essentially at the same rate towards zero NASHCONV, in contrast to the slower rate of the Uniform meta-solver, the very slow rate of the Rectified PRD solver, and the seemingly constant NASHCONV of the Rectified Nash solver. We provide in Appendix C.5 a walkthrough of the first steps of the Rectified Nash results to more precisely determine the cause of its plateauing NASHCONV. A high level explanation thereof is that it is caused by Rectified Nash cycling through the same policies, effectively not discovering new policies. We posit these characteristics, antipodal to the motivation behind Rectified Nash, come from the important fact that Rectified Nash was designed to work only in symmetric games, and is therefore not inherently well-suited for the Kuhn and Leduc poker domains investigated here, as they are both asymmetric games. We did not add the Rectified PRD results the other, greater-than-2 players experiments, as its performance remained non-competitive. As noted in  Lanctot et al. (2017) , PSRO(Uniform, BR) corresponds to Fictitious Play ( Brown, 1951 ) and is thus guaranteed to find an NE in such instances of two-player zero-sum games. Its slower convergence rate is explained by the assignment of uniform mass across all policies s ∈ S, implying that PSRO essentially wastes resources on training the oracle to beat even poor-performing strategies. While α-Rank does not seek to find an approximation of Nash, it nonetheless reduces the NASHCONV yielding competitive results in comparison to an exact-Nash solver in these instances. Notably, the similar performance of α-Rank and Nash serves as empirical evidence that α-Rank can be applied competitively even in the two-player zero-sum setting, while also showing great promise to be deployed in broader settings where Nash is no longer tractable. We next consider significantly larger variants of Kuhn and Leduc Poker involving more than two players, extending beyond the reach of prior PSRO results ( Lanctot et al., 2017 ).  Figure 4  visualizes the NASHCONV of PSRO using the various meta-solvers (with the exception of an exact Nash solver, due to its intractability in these instances). In all instances of Kuhn Poker, α-Rank and PRD show competitive convergence rates. In 3-player Leduc poker, however, α-Rank shows fastest convergence, with Uniform following throughout most of training and PRD eventually reaching a similar NASHCONV. Several key insights can be made here. First, computation of an approximate Nash via PRD involves simulation of the associated replicator dynamics, which can be chaotic ( Palaiopanos et al., 2017 ) even in two-player two-strategy games, making it challenging to determine when PRD has suitably converged. Second, the addition of the projection step in PRD severs its connection with NE; the theoretical properties of PRD were left open in  Lanctot et al. (2017) , leaving it without any guarantees. These limitations go beyond theoretical, manifesting in practice, e.g., in Fig. 4d, where PRD is outperformed by even the uniform meta-solver for many iterations. Given these issues, we take a first (and informal) step towards analyzing PRD in Appendix E. For α-Rank, by contrast, we both establish theoretical properties in Section 4, and face no simulation-related challenges as its computation involves solving of a linear system, even in the general-sum many-player case ( Omidshafiei et al., 2019 ), thus establishing it as a favorable and general PSRO meta-solver.

Section Title: MuJoCo Soccer
  MuJoCo Soccer While the key objective of this paper is to take a first step in establishing a theoretically-grounded framework for PSRO-based training of agents in many-player settings, an exciting question regards the behaviors of the proposed α-Rank-based PSRO algorithm in complex domains where function-approximation-based policies need to be relied upon. In Appendix F, we take a first step towards conducting this investigation in the MuJoCo soccer domain introduced in  Liu et al. (2019) . We remark that these results, albeit interesting, are primarily intended to lay the foundation for use of α-Rank as a meta-solver in complex many agent domains where RL agents serve as useful oracles, warranting additional research and analysis to make conclusive insights.

Section Title: RELATED WORK
  RELATED WORK We discuss the most closely related work along two axes. We start with PSRO-based research and some multiagent deep RL work that focuses on training of networks in various multiagent settings. Then we continue with related work that uses evolutionary dynamics (α-Rank and replicator dynamics) as a solution concept to examine underlying behavior of multiagent interactions using meta-games. Policy-space response oracles ( Lanctot et al., 2017 ) unify many existing approaches to multiagent learning. Notable examples include fictitious play ( Brown, 1951 ;  Robinson, 1951 ), independent reinforcement learning ( Matignon et al., 2012 ) and the double oracle algorithm ( McMahan et al., 2003 ). PSRO also relies, fundamentally, on principles from empirical game-theoretic analysis (EGTA) ( Walsh et al., 2002 ;  Phelps et al., 2004 ;  Tuyls et al., 2018 ;  Wellman, 2006 ;  Vorobeychik, 2010 ;  Wiedenbeck and Wellman, 2012 ;  Wiedenbeck et al., 2014 ). The related Parallel Nash Memory (PNM) algorithm ( Oliehoek et al., 2006 ), which can also be seen as a generalization of the double oracle algorithm, incrementally grows the space of strategies, though using a search heuristic rather than exact best responses. PNMs have been successfully applied to games settings utilizing func- tion approximation, notably to address exploitability issues when training Generative Adversarial Networks (GANs) ( Oliehoek et al., 2019 ). PSRO allows the multiagent learning problem to be decomposed into a sequence of single-agent learning problems. A wide variety of other approaches that deal with the multiagent learning problem without this reduction are also available, such as Multiagent Deep Deterministic Policy Gradients (MADDPG) ( Lowe et al., 2017 ), Counterfactual Multiagent Policy Gradients (COMA) ( Foerster et al., 2018 ), Differentiable Inter-Agent Learning (DIAL) ( Foerster et al., 2016 ), Hysteretic Deep Recurrent Q-learning ( Omidshafiei et al., 2017 ), and lenient Multiagent Deep Reinforcement Learning ( Palmer et al., 2018 ). Several notable contributions have also been made in addressing multiagent learning challenges in continuous-control settings, most recently including the approaches of  Iqbal and Sha (2019) ;  Gupta et al. (2017) ;  Wei et al. (2018) ;  Peng et al. (2017) ;  Khadka et al. (2019) . We refer interested readers to the following survey of recent deep multiagent RL approaches  Hernandez-Leal et al. (2019) . α-Rank was introduced by  Omidshafiei et al. (2019)  as a scalable dynamic alternative to Nash equilibria that can be applied in general-sum, many-player games and is capable of capturing the underlying multiagent evolutionary dynamics. Concepts from evolutionary dynamics have long been used in analysis of multiagent interactions from a meta-game standpoint ( Walsh et al., 2002 ;  Tuyls and Parsons, 2007 ;  Hennes et al., 2013 ;  Bloembergen et al., 2015 ;  Tuyls et al., 2018 ).

Section Title: DISCUSSION
  DISCUSSION This paper studied variants of PSRO using α-Rank as a meta-solver, which were shown to be competitive with Nash-based PSRO in zero-sum games, and scale effortlessly to general-sum many- player games, in contrast to Nash-based PSRO. We believe there are many interesting directions for future work, including how uncertainty in the meta-solver distribution, informed by recent developments in dealing with incomplete information in games ( Reeves and Wellman, 2004 ;  Walsh et al., 2003 ;  Rowland et al., 2019 ), can be used to inform the selection of new strategies to be added to populations. In summary, we strongly believe that the theoretical and empirical results established in this paper will play a key role in scaling up multiagent training in general settings.
  1 A meta-solver is a method that computes, or approximates, the solution concept that is being deployed.

```
