Title:
```
Under review as a conference paper at ICLR 2020 ACCELERATED VARIANCE REDUCED STOCHASTIC EXTRAGRADIENT METHOD FOR SPARSE MACHINE LEARNING PROBLEMS
```
Abstract:
```
Recently, many stochastic gradient descent algorithms with variance reduction have been proposed. Moreover, their proximal variants such as Prox-SVRG can effectively solve non-smooth problems, which makes that they are widely applied in many machine learning problems. However, the introduction of proximal oper- ator will result in the error of the optimal value. In order to address this issue, we introduce the idea of extragradient and propose a novel accelerated variance re- duced stochastic extragradient descent (AVR-SExtraGD) algorithm, which inher- its the advantages of Prox-SVRG and momentum acceleration techniques. More- over, our theoretical analysis shows that AVR-SExtraGD enjoys the best-known convergence rates and oracle complexities of stochastic first-order algorithms such as Katyusha for both strongly convex and non-strongly convex problems. Finally, our experimental results show that for ERM problems and robust face recognition via sparse representation, our AVR-SExtraGD can yield better performance than state-of-the-art algorithms such as Prox-SVRG and Katyusha. The asynchronous variant of AVR-SExtraGD outperforms KroMagnon and ASAGA, which are the asynchronous variants of SVRG and SAGA, respectively.
```

Figures/Tables Captions:
```
Figure 1: Comparison of experimental results of different algorithms for Lasso (top) and Elastic-Net (bottom) problems on different data sets. The y-axis represents the gap between the objective value and the minimum, and the x-axis corresponds to running time.
Figure 2: Comparison of experimental results of different algorithms for Lasso (the first two columns) and Elastic-Net (the latter two columns) problems on different data sets. The y-axis repre- sents the gap of objective value, and the x-axis corresponds to the number of effective passes (top) or running time (bottom).
Figure 3: Comparison of experimental results about different choices of K in AVR-SExtraGD for Lasso on different data sets. The y-axis represents the gap of objective value, and the x-axis corre- sponds to the number of effective passes ((a.1) and (b.1)) or running time ((a.2) and (b.2)).
Table 1: Comparison of Recognition Rates on the AR and Yale Datasets.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION In this paper, we mainly consider the following composite convex optimization problem: where F (x) : R d → R is the average of smooth convex component functions f i (x), and R(x) is a relatively simple convex function (but may not be differentiable). In this paper, we use · to denote the standard Euclidean norm, and · 1 to denote the 1 -norm. Moreover, we use P * to denote the real optimal value of P (·), and P * to denote the optimal value obtained by algorithms. This form of optimization problems often appears in machine learning, signal processing, data sci- ence, statistics and operations research, and has a wide range of applications such as regularized empirical risk minimization (ERM), sparse coding for image and video recovery, and represen- tation learning for object recognition. Specifically, for a collection of given training examples {(a 1 , b 1 ), ..., (a n , b n )}, where a i ∈ R d , b i ∈ R (i = 1, 2, ..., n) and a i is a feature vector, while b i is the desired response. When f i (x) = 1 2 (a T i x−b i ) 2 , we can obtain the ridge regression problem by setting R(x) = λ 2 x 2 . We also get the Lasso or Elastic-Net problems by setting R(x) = λ x 1 or R(x) = λ2 2 x 2 +λ 1 x 1 , respectively. Moreover, if we set f i (x) = log(1+exp(−b i x T a i )), we will get the regularized logistic regression problem.

Section Title: RECENT RESEARCH PROGRESS
  RECENT RESEARCH PROGRESS The proximal gradient descent (PGD) method is a standard and effective method for Problem (1), and can achieve linear convergence for strongly convex problems. Its accelerated algorithms, e.g., accelerated proximal gradient (APG) (Tseng (2008);  Beck & Teboulle (2009) ), attain the conver- gence rate of O(1/T 2 ) for non-strongly convex problems, where T denotes the number of iterations.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 In recent years, stochastic gradient descent (SGD) has been successfully applied to many large-scale learning problems, such as training for deep networks and linear prediction ( Tong (2004) ), because of its significantly lower per-iteration complexity than deterministic methods, i.e., O(d) vs. O(nd). Besides, many tricks for SGD have also been proposed, such as  Loshchilov & Hutter (2016) . How- ever, the variance of the stochastic gradient may be large due to random sampling ( Johnson & Tong (2013) ), which leads that the algorithm requires a gradually reduced step size, thus it will converge slow. Even under the strongly convex condition, SGD only achieves a sub-linear convergence rate O(1/T ). Recently, many SGD methods with variance reduction have been proposed. For the case of R(x) = 0,  Roux et al. (2012)  developed a stochastic average gradient descent (SAG) method, which is a randomized variant of the incremental aggregated gradient method proposed by  Blatt et al. (2007) . Then stochastic variance reduced gradient (SVRG) ( Johnson & Tong (2013) ) was proposed, and has been widely introduced into various subsequent optimization algorithms, due to its lower storage space (i.e., O(d)) than that of SAG (i.e., O(nd)). SVRG reduced the variance effectively by changing the estimation of stochastic gradients. The introduction of a snapshot point x mainly has the effect of correcting the direction of gradient descent, and reduces the variance. Later,  Konečný & Richtárik (2013)  proposed the semi-stochastic gradient descent methods as well as their mini-batch version ( Konečný et al. (2014) ). And their asynchronous distributed variant ( Ruiliang et al. (2016) ) is also been proposed later. More recently,  Lin & Tong (2014)  proposed the Prox-SVRG method, which introduced the proximal operator, and then applied the idea of SVRG to solve the non-smooth optimization problems. However, Prox-SVRG can only be used to solve the strongly convex optimization problems. In order to solve the non-strongly convex problems,  Zeyuan & Yuan (2016)  proposed the SVRG++ algorithm. Besides, to accelerate the algorithm and reducing the complexity, by combining the main ideas of APG and  Prox-SVRG, Nitanda (2014)  proposed an accelerated variance reduction proximal stochastic gradient descent (Acc-Prox-SVRG) method, which can effectively reduce the complexity of the algorithm compared to the two basic algorithms. Very recently,  Zeyuan (2017)  developed a novel Katyusha algorithm which introduced the Katyusha momentum to accelerate the algorithm. With the development of parallel and distributed computing which can effectively reduce computing time and improve performance,  Ryu & Wotao (2017)  came up with an algorithm called Proximal Proximal Gradient, which combined the proximal gradient method and ADMM ( Gabay & Mercier (1976) ). Furthermore, it is easy to implement in parallel and distributed environments because of its innovative algorithm structure.

Section Title: OUR MAIN CONTRIBUTIONS
  OUR MAIN CONTRIBUTIONS We find that due to the introduction of proximal operator, there is a gap between P * and P * , and its theoretical derivation can be seen in Appendix A. To address this issue,  Nguyen et al. (2017)  proposed the idea of extragradient which can be seen as a guide during the process, and introduced it into the optimization problems. Intuitively, this additional iteration allows us to examine the geometry of the problem and consider its curvature information, which is one of the most important bottlenecks for first order methods. By using the idea of extragradient, we can get a better result in each inner-iteration. Therefore, the idea of extragradient is our main motivation. In this paper, we propose a novel algorithm for solving non-smooth optimization problems. The main contributions of this paper are summarized as follows. • In order to improve the result of the gap between P * and P * , and achieve fast convergence, a novel algorithm, which combines the idea of extragradient, Prox-SVRG and the trick of momen- tum acceleration, is proposed, called accelerated variance reduced stochastic extragradient descent (AVR-SExtraGD). • We provide the convergence analysis of our algorithm, which shows that AVR-SExtraGD achieves linear convergence for strongly convex problems, and the convergence condition in the non-strongly convex case is also given. According to the convergence rate, we can know that AVR-SExtraGD has the same excellent result as the best-known algorithms, such as Katyusha. • Finally, we show by experiments that the performance of AVR-SExtraGD (as well as VR- SExtraGD, which is the basic algorithm of AVR-SExtraGD) is obviously better than the popular algorithm, Prox-SVRG, which confirms the advantage of extragradient. For the widely used accel- erated algorithm, Katyusha, the performance of our algorithm is still improved.

Section Title: BASIC ASSUMPTIONS
  BASIC ASSUMPTIONS We first make the following assumptions to solve the problem (1): Assumption 1 (Smoothness). The convex function F (·) is L-smooth, i.e., there exists a constant L > 0 such that for any x, y ∈ R d , ∇F (x)−∇F (y) ≤ L x−y .

Section Title: Assumption 2 (Lower Semi-continuity)
  Assumption 2 (Lower Semi-continuity) where G ∈ ∂R(y) which is the set of sub-gradient of R(·) at y.

Section Title: PROX-SVRG AND EXTRAGRADIENT DESCENT METHODS
  PROX-SVRG AND EXTRAGRADIENT DESCENT METHODS An effective method for solving Problem (1) is Prox-SVRG which improved Prox-FG ( Lions & Mercier (1979) ) and Prox-SG ( Langford et al. (2009) ) by introducing the stochastic gradient and combining the idea of SVRG, respectively. For strongly convex problems, Prox-SVRG can reach linear convergence with a constant step size, and its main update rules arẽ wherex is the snapshot point used in SVRG,∇f i k (x k−1 ) is the variance reduced stochastic gradient estimator, and Prox R η (·) is the proximal operator. Although Prox-SVRG can converge fast, because of proximal operator, the final solution has the deviation, which makes the solution inaccurate, thus Prox-SVRG still needs to be further improved, which is our important motivation. The extragradient method was first proposed by  Korpelevič (1976) . It is a classical method for solving variational inequality problems, and it generates an estimation sequence by using two pro- jection gradients in each iteration. By combining this idea with some first-order descent methods,  Nguyen et al. (2017)  proposed an extended extragradient method (EEG) which can effectively solve the problem (1), and can also solve relatively more general problems as follows: min x∈R d P (x) def = F (x) + R(x) where F (x) is not necessarily composed by multiple functions f i (x). Unlike the classical extragra- dient method, EEG uses proximal gradient instead of orthogonal projection in each iteration. The main update rules of EEG are where s k and α k are two step sizes. From the update rules of EEG, we can see that in each iteration, EEG needs to calculate two gradients, which will definitely slow down the algorithm. Therefore, the algorithm needs to be further accelerated by an efficient technique.

Section Title: MOMENTUM ACCELERATION AND MIG
  MOMENTUM ACCELERATION AND MIG Firstly, we introduce the momentum acceleration technique whose main update rules are where dw is the gradient of the objective function at w, β is a parameter, and α is a step size. The update rules take not only the gradient of the current position, but also the gradient of the past position into account, which makes the final descent direction of w t after using momentum Under review as a conference paper at ICLR 2020

Section Title: Algorithm 1 AVR-SExtraGD
  Algorithm 1 AVR-SExtraGD Input: Initial vector x 0 , the number of epochs S, the number of iterations m per epoch, the step sizes η 1 , η 2 , momentum parameter β, and the set K. reduce the oscillation of descent, thus this method can effectively accelerate the convergence of the algorithm. According to the Nesterov's momentum, lots of accelerated algorithms were proposed, such as APG and Acc-Prox-SVRG.  Later, Zeyuan (2017)  proposed Katyusha to further accelerate the algorithm, and MiG ( Kaiwen et al. (2018) ) was proposed to simplify the structure of Katyusha, and the mo- mentum acceleration of MiG is embodied in each iteration as follows: Moreover, it is easy to get that the oracle complexity of MiG is less than that of Prox-SVRG and APG, which means that MiG can effectively accelerate the original Prox-SVRG algorithm. There- fore, we can also use this acceleration technique to accelerate our algorithm and address the issue of slow convergence due to the calculations of two different gradients.

Section Title: OUR AVR-SEXTRAGD METHOD
  OUR AVR-SEXTRAGD METHOD We note that EEG requires computing two full gradients in each iteration, which will take a lot of time for large-scale machine learning problems. Therefore, we first consider and propose the stochastic variant of the algorithm, namely stochastic extragradient descent (SExtraGD), to re- duce the per-iteration computational complexity, and further propose an efficient variance reduced stochastic extragradient descent (VR-SExtraGD) algorithm. Their main update rules and the de- tailed algorithm of VR-SExtraGD can be found in Appendix C. On the basis of VR-SExtraGD, we refer to the momentum acceleration technique proposed in MiG, and propose an innovative accelerated variance reduced stochastic extragradient descent algorithm, called AVR-SExtraGD. It is used to solve non-smooth (both SC and non-SC) optimization prob- lems. To further accelerate the algorithm and address the issue of slow convergence speed caused by two gradients in each inner-iteration, only part of the iterations are updated by extragradient descent. Our AVR-SExtraGD algorithm is outlined in Algorithm 1. Firstly, we give some explanation for Algorithm 1. For our AVR-SExtraGD, we need to compute a full gradient of F (x). And Step 3 in Algorithm 1 is the selection of momentum parameter. Step 6 to Step 8 are the update rule of AVR-SExtraGD, and Step 9 is the update rule of MiG. Here we only use AVR-SExtraGD in the set K. Step 13 is the formulation of the snapshot point. Finally, we Under review as a conference paper at ICLR 2020 give the set of the start point of next inner-iteration in Step 14. Moreover, our output is the snapshot point of the last outer iteration. And for our AVR-SExtraGD algorithm, we have the following remarks. • Following the requirement of step sizes in EEG, the step sizes in our algorithms also need to satisfy similar conditions. After combining all the conditions, we get the conditions: • In AVR-SExtraGD, we use one more trick to speed up the algorithm, that is, only part of the iterations (i.e., when k ∈ K) are updated by extragradient descent, and the rest of the iterations are still updated by the update rules of MiG. For different problems and different data sets, we manually adjust the choice of K, and the details can be seen in Section 5.3. • For the momentum parameter β, when P (·) is a strongly convex function, we can set β as a constant which is generally set as 0.9. And β is also set as 1− √ µη2 1+ √ µη2 in Acc-Prox-SVRG, while we set β = 0.9 in our AVR-SExtraGD. However, when P (·) is non-strongly convex, the value of β in each iteration is no longer fixed. We set β s as a decreasing sequence, which satisfies 1 Particularly, in AVR-SExtraGD, we set β s = 2 s+4 , which satisfies the inequality defined above. As we all know, for the general GD method, the iterate x k in each iteration eventually converges to the real optimal point of the function, so there is no error in the final optimal value. Therefore, the proximal operator will introduce a bad result in convergence. To adress this issue, we introduce the idea of extragradient, which takes one more proximal operator than Prox-SVRG. And according to the idea of extragradient, we know that the update structure of EEG can make use of the curvature information of the objective function. Although we change the original EEG into a stochastic version, the advantage of the extragradient structure is still retained to some degree, and thus our algorithm can get a better result than the algorithm without extragra- dient.That is, although the method of extragradient can not directly reduce the gap of the optimal value, it can improve the bad result brought by the gap, and obtain a better result after every inner loop, Thus, AVR-SExtraGD can improve the accuracy of the algorithm. In summary, our AVR-SExtraGD method combines the advantage of Prox-SG for solving non- smooth optimization problems, the advantage of EEG, and the trick of SVRG to reduce the variance of stochastic gradient. And it is further accelerated by introducing the momentum acceleration used in MiG. Therefore, our algorithm has more advantages than the basic algorithms mentioned above.

Section Title: CONVERGENCE ANALYSIS
  CONVERGENCE ANALYSIS In this section, we analyze the convergence properties of AVR-SExtraGD under strongly convex and non-strongly convex conditions. For convenience analysis, we use∇ i k F (·) to denote∇f i k (·), that defined in (3) in the analysis of AVR-SExtraGD. We give some key lemmas, which are important to prove the convergence of AVR-SExtraGD in Appendix B, and all the proofs of our lemmas and theorems in this section are also given in Appendix B.

Section Title: FOR SC PROBLEMS
  FOR SC PROBLEMS For strongly convex problems, the linear convergence of AVR-SExtraGD can be guaranteed by the following theorem. Theorem 1 (Strongly Convex). Suppose that Assumptions 1, 2 and 3 hold, and let x * = arg min x P (x). In addition, assume η 1 = η 2 = η > 0 and Lβ + Lβ 1−β ≤ 1 η . Then, by appropri- ately choosing η, β and m = Θ(n), Algorithm 1 achieves an -additive error with following oracle complexities in expectation: which also means that for SC problems, the oracle complexity of Algorithm 1 is

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 This result means that for strongly convex problems, AVR-SExtraGD achieves linear convergence and enjoys the best-known oracle complexity of stochastic first-order algorithms, such as Katyusha.

Section Title: FOR NON-SC PROBLEMS
  FOR NON-SC PROBLEMS The convergence of AVR-SExtraGD for solving non-SC problems can be guaranteed by the follow- ing theorem. which also means that when we choose m = Θ(n), Algorithm 1 achieves the following oracle com- plexity in expectation: The result shows that AVR-SExtraGD enjoys the same oracle complexity as Katyusha and MiG, which is close to the best-known complexity in this case (i.e., O(n log 1 + nL )). In addition, we also analyze the convergence of VR-SExtraGD and give and prove the related lemmas and theorems to guarantee its convergence, which can be found in Appendix D.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we evaluate the performance of AVR-SExtraGD and compare it with its counterparts including Prox-SVRG and Katyusha on real-world data sets, whose information is shown as Table 2 in Appendix E. Besides, for these real-world data sets, we consider the two common problem models: Lasso and Elastic-Net. We also apply our algorithm to face recognition tasks and compare it with the compared algorithms. Next, we give the setup of the related parameters as follows: • Regularization Parameters: The regularization parameters for real-world datasets are shown in Table 2. • The Number of Inner-Iteration: The number of inner-iterations of Katyusha and Prox-SVRG is usually set as m = 2n. Our algorithm adds one more gradient calculation in each inner-iteration than Prox-SVRG, and for an equal complexity of each epoch, we set m = n in AVR-SExtraGD, so that in each epoch, all the three algorithms require calculating 3n stochastic gradients. What's more, the reasonableness of such a setting can be found in  Sebbouh et al. (2019) . • Step Sizes: We set our step sizes as: η 1 = 2 5L , η 2 = 3 5L . We note that the selected step sizes do not satisfy the conditions requested in the remark of Section 3. Nevertheless, we can see from the ex- perimental results that our algorithm still converges well, which means that in practice experiments, we can choose larger step sizes to improve the convergence speed. For fair comparison, we implemented all the methods in C++ with a Matlab interface, and performed all the experiments on a PC with an Intel i7-7700 CPU and 32GB RAM.

Section Title: RESULTS OF LASSO, ELASTIC-NET AND LOGISTIC REGRESSION
  RESULTS OF LASSO, ELASTIC-NET AND LOGISTIC REGRESSION In this part, we consider three common problems, including Lasso, Elastic-Net and the 1 -norm regularized logistic regression, whose models can be found in Appendix E.  Figure 1  shows the performance of all the algorithms for Lasso and Elastic-Net on all the data sets. For running time, our AVR-SExtraGD obviously outperforms Prox-SVRG and Kayusha, which shows the faster convergence speed of AVR-SExtraGD than Katyusha, and justifies that the extra- gradient and the momentum acceleration are able to improve Prox-SVRG efficiently. Moreover, for Under review as a conference paper at ICLR 2020 the two problems, we propose the asynchronous sparse variant of AVR-SExtraGD by bringing our algorithm into a sparse asynchronous framework and compare its performance with KroMagnon ( Mania et al. (2015) ) and ASAGA ( Leblond et al. (2016) ) on rcv1 and real-sim, as shown in Table 2. The results are shown in  Figure 2 , which verify that the asynchronous variant of AVR-SExtraGD significantly outperforms the variants of SVRG (i.e., KroMagnon) and SAGA ( Defazio et al. (2014) ) (i.e., ASAGA) in terms of iterations and running time. Then, for a more comprehensive compari- son, we compare the performance of more algorithms for Lasso and the 1 -norm regularized logistic regression on a9a and Covtype, and the results are shown as Figure 4 and Figure 5 in Appendix E.

Section Title: RESULTS ON FACE RECOGNITION
  RESULTS ON FACE RECOGNITION We also apply our AVR-SExtraGD as well as Prox-SVRG and Katyusha to robust face recogni- tion via sparse representation ( John et al. (2009) ) on the AR and Yale. We set the loss function Under review as a conference paper at ICLR 2020 in the training process as the same function as the Lasso and Elastic-Net problems. For approxi- mately equal time, the number of outer loops is 200 for Prox-SVRG and AVR-SExtraGD, and 50 for Katyusha. In order to compare the results reasonably, we implement all the algorithms for 20 times and get the average and standard deviation of recognition rates, as shown in  Table 1 . The results in  Table 1  show that the recognition rate of AVR-SExtraGD is significantly higher than other algorithms on both the AR and Yale data sets. This means that our AVR-SExtraGD can learn a more efficient representation for face recognition.

Section Title: THE SELECTION OF K IN AVR-SEXTRAGD
  THE SELECTION OF K IN AVR-SEXTRAGD For the selection of K, we do some relevant experiments as examples. We choose different K to solve Lasso problem by our algorithm, and get the results as shown in  Figure 3 . where, K = n means that the extragradient is calculated every integer multiple of n (n ∈ {1, 8, 25, 75, 250}). For a9a, when the extragradient is used every time, the function value decreases faster with respect to the number of iterations, but the result is not good for running time. Thus, for a9a, we choose K = 25. As for Covtype, obviously, K = 1 is the best choice.

Section Title: CONCLUSIONS AND FUTURE WORK
  CONCLUSIONS AND FUTURE WORK In this paper, we mainly considered the non-smooth optimization problem in large-scale and high- dimensional settings. By introducing the idea of extragradient and momentum acceleration, we improved the classical Prox-SVRG and then proposed a novel algorithm, called AVR-SExtraGD. From our theoretical analysis, we can know that AVR-SExtraGD attains linear convergence for SC problems, and achieves the same oracle complexity as Katyusha, which is the best-known one of stochastic first-order algorithms in both SC and non-SC cases. Finally, the experimental results showed that AVR-SExtraGD improved the result of the gap of the optimal value introduced by proximal operator, and thus improved the accuracy of solutions and convergence speed, which con- firmed the efficiency of extragradient and momentum acceleration. For future work, we can extend the ideas introduced in this paper to many existing proximal algorithms, including Prox-AFG ( Beck & Teboulle (2009) ), Prox-SAG ( Schmidt et al. (2017) ) and Prox-SDCA ( Shalev-Shwartz & Tong (2012) ;  Shalev-shwartz & Tong (2014) ) which is a proximal variant of SDCA ( Shalev-Shwartz & Tong (2013) ), and it will certainly improve the performance of these algorithms. Moreover, we can also rewrite our algorithm into the form of mini-batch, whose computation of gradient evaluations can be parallelized ( Agarwal & Duchi (2011) ;  Dekel et al. (2012) ). Under review as a conference paper at ICLR 2020

```
