Title:
```
Published as a conference paper at ICLR 2020 LEARNING THE ARROW OF TIME FOR PROBLEMS IN REINFORCEMENT LEARNING
```
Abstract:
```
We humans have an innate understanding of the asymmetric progression of time, which we use to efficiently and safely perceive and manipulate our environment. Drawing inspiration from that, we approach the problem of learning an arrow of time in a Markov (Decision) Process. We illustrate how a learned arrow of time can capture salient information about the environment, which in turn can be used to measure reachability, detect side-effects and to obtain an intrinsic reward signal. Finally, we propose a simple yet effective algorithm to parameterize the problem at hand and learn an arrow of time with a function approximator (here, a deep neural network). Our empirical results span a selection of discrete and continuous environments, and demonstrate for a class of stochastic processes that the learned arrow of time agrees reasonably well with a well known notion of an arrow of time due to Jordan, Kinderlehrer, and Otto (1998).
```

Figures/Tables Captions:
```
Figure 1: The agent (in orange) is tasked with reaching its goal, the checkered flag (middle frame). It may take the shorter path (right frame), which entails breaking the vases in its way, or it may prefer the safer path (left frame) which is longer but keeps the vases intact. The former path is irreversible, and the initial state is unreachable from the final state (red arrow). On the contrary, the latter path is completely reversible, and the initial state remains reachable from the final state. Now, an arrow of time (pink) measures the disorder, which might help a safe agent decide which path to take.
 
Figure 2: The potential difference (i.e. change in h- potential) between consecutive states along a trajec- tory on the Vaseworld (2D world) environment. The dashed vertical lines denote when a vase is broken. Gist: the h-potential increases step-wise when the agent irreversibly breaks a vase (corresponding to the spikes), but remains constant as it reversibly moves about. Further, the spikes are all of roughly the same height, indicating that the h-potential has learned to measure irreversibility by counting the number of de- stroyed vases.
Figure 3: The h-potential along a trajectory from a random policy, annotated with the corresponding state images on the Sokoban (2D world) environment. The white sprite corresponds to the agent, orange to a wall, blue to a box and green to a goal. Gist: the h- potential increases sharply as the agent pushes a box against the wall. While it may decrease for a given trajectory (in this case because the agent manages to move a box away from the wall), it increases in ex- pectation over all trajectories (cf. Fig 14 in Appendix C.1.3).
Figure 4: The h-potential (for Mountain Car) at zero-velocity plotted against position. Also plot- ted (orange) is the height profile of the mountain. Gist: the h-potential approximately recovers the height-profile of the mountain with just trajecto- ries from a random policy.
Figure 5: The h-potential as a function of state (position and velocity) for (continuous) Mountain-Car with and without friction. The overlay shows random trajectories (emanating from the dots). Gist: with friction, we find that the state with largest h is one where the car is stationary at the bottom of the valley. Without friction, there is no dissipation and the car oscillates up and down the valley. Consequently, we observe that the h-potential is constant (up-to edge effects) and thereby uninformative.
Figure 6: The true arrow of time (the Free- Energy functional, in blue) plotted against the learned arrow of time (the H-functional, i.e. the negative spatial expectation of the h-potential; plotted in orange) after linear scaling and shifting. Gist: we find the H-functional to be in good (al- beit not perfect) agreement with the Free-Energy functional, where the latter is a known notion of an arrow of time.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The asymmetric progression of time has a profound effect on how we, as agents, perceive, process and manipulate our environment. Given a sequence of observations of our familiar surroundings (e.g. as video frames), we possess the innate ability to predict whether the said observations are ordered correctly. We use this ability not just to perceive, but also to act: for instance, we know to be cautious about dropping a vase, guided by the intuition that the act of breaking a vase cannot be undone. This profound intuition reflects some fundamental properties of the world in which we dwell, and in this work we ask whether and how these properties can be exploited to learn a representation that functionally mimics our understanding of the asymmetric nature of time. The term Arrow of Time was coined by the British astronomer  Eddington (1929)  to denote this inherent asymmetry, which he attributed to the non-decreasing nature of the total thermodynamic entropy of an isolated system, as required by the second law of thermodynamics. Since then, the notion of an arrow of time has been formalized and explored in various contexts, spanning not only physics, but also algorithmic information theory (Zurek, 1989), causal inference ( Janzing et al., 2016 ) and time-series analysis ( Janzing, 2010 ;  Bauer et al., 2016 ). Broadly, an arrow of time can be thought of as a function that monotonously increases as a system evolves in time. Expectedly, the notion of irreversibility plays a central role in the discourse. In sta- tistical physics, it is posited that the arrow of time (i.e. entropy production) is driven by irreversible processes ( Prigogine, 1978 ; Seifert, 2012). To understand how a notion of an arrow of time can be useful in the reinforcement learning context, consider the example of a cleaning robot tasked with moving a box across a room ( Amodei et al., 2016 ). The optimal way of successfully completing the task might involve the robot doing something disruptive, like knocking a vase over ( Fig 1 ). Now on the one hand, such disruptions - or side-effects - might be difficult to recover from. In the extreme case, they might be virtually irreversible - say when the vase is broken. On the other hand, irre- versibility implies that states with a larger number of broken vases tend to occur in the future, and one should therefore expect an arrow of time (as a scalar function of the state) to assign larger values Disorder Time Disorder Time Disorder Time Disorder Time to states with larger number of broken vases. An arrow of time should therefore quantify the amount of disorder in the environment, analogous to the entropy for isolated thermodynamical systems. Now, one possible application could be to detect and preempt such side-effects, for instance by penalizing policies that significantly increment the arrow of time by executing difficult-to-reverse transitions. But the utility of an arrow of time is more general: it serves as a directed measure of reachability. This can be seen by observing that it is more difficult to obtain order from disorder: it is, after all, difficult to reach a state with a vase intact from one with it broken, rather than vice versa. In this sense, we may say that a state is relatively unreachable from another state if an arrow of time assigns a lower value to the former. Further, a directed measure of reachability afforded by an arrow of time can be utilized for deriving an intrinsic reward signal to enable agents to learn complex skills in the absence of external rewards. To see how, consider that an agent tasked with reversing the arrow of time (by creating order from disorder) must in general learn complex skills to achieve its goal. Indeed, gluing together a broken vase will require the agent to learn an array of complex planning and motor skills, which is the ultimate goal of such intrinsic rewards.

Section Title: THE h-POTENTIAL
  THE h-POTENTIAL Motivated by the preceding discussion, our goal is to learn a function that quantifies the amount of disorder in a given environment state, where we say that irreversible state transitions increase disorder. In this sense, we seek a function (of the state) that is constant in expectation along fully reversible state transitions, but increase in expectation along state transitions that are less reversible. To that end, we begin by formally introducing this function, which we call the h-potential, as the solution to a functional optimization problem. Subsequently, we critically discuss a few conceptual roadblocks that must be cleared before such a function can be useful in the RL setting.

Section Title: FORMALISM
  FORMALISM Consider a Markov Decision Process (a MDP, i.e. environment), and let S and A be its state and action spaces respectively. A policy π is a mapping from the state space to the space of distributions over actions. Given a state s ∈ S sampled from some initial state distribution p 0 , we may sample Published as a conference paper at ICLR 2020 an action a ∈ A from the policy π(a|s), which in turn can be used to sample another state s ∈ S from the environment dynamics p(s |a, s). Iterating N more times for a fixed π, one obtains a sequence of states (s 0 , ..., s t , ..., s N ), which is a realization of the Markov chain (a trajectory) with transition probabilities p π (s t+1 |s t ) = a∈A p(s t+1 |s t , a)π(a|s t ). We may now define a function h π : S → R as the solution to the following functional objective: J π [ĥ] = E t∼U ({0,...,N −1}) E st E st+1|st [ĥ(s t+1 ) −ĥ(s t )|s t ] + λT [ĥ]; h π = arg max h J π [ĥ] (1) where U (A) is the uniform distribution over any set A, E t E st E st+1|st is the expectation over all state transitions, λ is a scalar coefficient and T [ĥ] is a regularizing term that preventsĥ from diverging within a finite domain. In words: the first term on the right hand side of the first equation above encourages h π to increase in expectation along the sampled trajectories, whereas the second term controls this increase; the two terms are balanced with a coefficient λ. Informally: if a state transition s → s is fully reversible, the probability of sampling it equals that of sampling the corresponding reverse transition, s → s. For such transitions, the pressure on h π to increase along the forward transition (s → s ) is compensated by the counter-pressure for it to increase along the reverse transition (s → s), or equivalently, decrease along the forward transition. Along such transitions, we should therefore expect h π to remain constant (in expectation). Accordingly, if the forward transition were to be more likely (i.e. if the transition is not fully reversible), we should expect h π to increase (in expectation) in order to satisfy its objective. The regularizer T must be chosen to suit the problem at hand, and different choices result in solutions that have different characteristics 1 . Possible choices for T include (any combination of) the negative of L 2 norm − ĥ 2 , and/or the following trajectory regularizer: Intuitively: while the solution h π is required to increase in expectation along trajectories, the trajec- tory regularizer acts as an contrastive term by penalizing h π for changing at all. With some effort, the problem defined in Eqn 1 can be approached analytically for toy Markov chains (interested readers may refer to App A for a technical discussion). However, such analytical treatment becomes infeasible for more complex and larger-scale environments with unknown tran- sition probabilities. To tackle such environments, we will cast the functional optimization problem in Eqn 1 to an optimization problem over the parameters of a deep neural network and solve it for a variety of discrete and continuous environments.

Section Title: SUBTLETIES
  SUBTLETIES In this section, we discuss two conceptually rich subtleties that determine the conditions under which the learned arrow of time (h-potential) can be useful in practice. The Role of a Policy. The first subtlety is rooted in the observation that the trajectories (s 0 , ..., s N ) are collected by a given but arbitrary policy. However, there may exist policies for which the re- sulting arrow of time is unnatural, perhaps even misleading. Consider for instance the actions of a practitioner of Kintsugi, the ancient Japanese art of repairing broken pottery. The corresponding policy 2 might cause the environment to transition from a state where the vase is broken to one where it is not. If we learn the h-potential on such trajectories, it might be the case that counter to our in- tuition, states with a larger number of broken vases are assigned smaller values (and the vice versa). Now, one may choose to resolve this conundrum by defining: J [h] = E π∼U (Π) J π [h] (3) where Π is the set of all policies defined on S, and U (Π) denotes a uniform distribution over Π. The resulting function h * = arg max{J [h] + λT [h]} would characterize the arrow of time with respect to all possible policies, and one would expect that for a vast majority of such policies, the transition from broken vase to a intact vase is rather unlikely and/or requires highly specialized policies. Unfortunately, determining h * is not feasible for most interesting applications, given the outer ex- pectation over all possible policies. As a compromise, we use (uniformly) random actions to gather trajectories. The simplicity of the corresponding random policy justifies its adoption, since one would expect a policy resembling (say) a Kintsugi artist to be rather complex and not implementable with random actions. In this sense, we ensure that the learned arrow of time characterizes the un- derlying dynamics of the environment, and not the peculiarities of a particular agent 3 . The price we pay is the lack of adequate exploration in complex enough environments, although this problem plagues most model-based reinforcement learning approaches  4  (cf.  Ha & Schmidhuber (2018) ). In the following, we assume π to be uniformly random and use h π interchangeably with h.

Section Title: Dissipative Environments
  Dissipative Environments The second subtlety concerns what we require of environments in which the arrow of time is informative. To illustrate the matter, consider the class of systems  5  , a typical instance of which could be a billiard ball moving on a frictionless arena and bouncing (elastically) off the edges ( Bunimovich, 2007 ). The state space comprises the ball's velocity and its position constrained to a billiard table (without holes!), where the ball is initialized at a random position on the table. For such a system, it can be seen by time-reversal symmetry that when averaged over a large number of trajectories, the state transition s → s is just as likely as the reverse transition s → s. In this case, recall that the arrow of time is expected to remain constant. A similar argument can be made for systems that identically follow closed trajectories in their respective state space (e.g. a frictionless and undriven pendulum). It follows that the h-potential must remain constant along the trajectory and that the arrow of time is uninformative. However, for so-called dissipative systems, the notion of an arrow of time is pronounced and well studied (Willems, 1972;  Prigogine, 1978 ). In MDPs, dissipative behaviour may arise in situations where certain transitions are irreversible by design (e.g. bricks disappearing in Atari Breakout), or due to partial observability, e.g. for a damped pendulum, the state space does not track the microscopic processes that give rise to friction 6 . Therefore, a central premise underlying the practical utility of learning the arrow of time is that the considered MDP is indeed dissipative, which we shall assume in the following; in Sec 5 (Fig 5b), we will empirically investigate the case where this assumption is violated.

Section Title: APPLICATIONS WITH RELATED WORK
  APPLICATIONS WITH RELATED WORK In this section, we discuss a few applications of the arrow of time, and illustrate how the h-potential provides a common framework to unify the notions of reachability, safety and curiosity.

Section Title: MEASURING REACHABILITY
  MEASURING REACHABILITY Given two states s and s in S, the reachability of s from s measures how difficult it is for an agent at state s to reach state s . The prospect of learning reachability from state-transition trajectories has been explored: in  Savinov et al. (2018) , the approach taken involves learning a logistic regressor network g θ : S × S → [0, 1] to predict the probability of states s and s being reachable to one another within a certain number of steps (of a random policy), in which case g(s, s ) ≈ 1. However, the model g is not directed: it does not learn whether s is more likely to follow s, or the vice versa. Instead, our proposal is to derive a directed measure of reachability from h-potential by defining a function η : S × S → R such that η(s, s ) ≡ η(s → s ) := h(s ) − h(s), where η(s → s ) measures the reachability of state s from state s. This inductive bias on η (in form of a functional constraint) induces the following useful properties. First, consider the case where the transition between states s and s is fully reversible, i.e. when state s is exactly as reachable from state s as is s from s; we denote such transitions with s ↔ s . Now, in expectation, we obtain that h(s ) = h(s) and consequently, η(s → s ) = η(s → s) = 0. But if instead the state s is more likely to follow state s than the vice versa (in expectation over trajectories), we say s is more reachable from s than the vice versa. It follows in expectation that Published as a conference paper at ICLR 2020 h(s ) > h(s), and consequently, η(s → s ) > 0. Now the inductive bias on η as a difference of h-potentials automatically implies η(s → s) = −η(s → s ) < 0. Second, observe that the reachability measure implemented by η is additive by construction: given a trajectory s 0 → s 1 → s 2 , we have that η(s 0 → s 2 ) = η(s 0 → s 1 ) + η(s 1 → s 2 ). As a special case, if we have that s 0 ↔ s 1 and s 1 ↔ s 2 - i.e. if η(s 0 → s 1 ) = η(s 1 → s 2 ) = 0 - it identically follows that s 0 ↔ s 2 , i.e. η(s 0 → s 2 ) = 0. In this case, the inductive bias enables η to generalize to the transition s 0 ↔ s 2 even if it is never explicitly sampled by the policy. Third, η allows for a soft measure of reachability. It measures not only whether a state s is reachable from another state s, but also quantifies how reachable the former is from the latter. As an example, consider a trajectory s 0 → s 1 → ... → s 100 , where the agent breaks one vase at every state transition. If the h-potential increases in constant increments for every vase broken (which we confirm it does in Sec 5), we obtain due to the inductive bias that η(s 0 → s 100 ) = 100 · η(s 0 → s 1 ). This behaviour is sought-after in the context of AI-Safety ( Krakovna et al., 2018 ;  Leike et al., 2017 ). Nonetheless, one should be careful when interpreting η. While the above implies that η(s → s) = η(s → s ) if the transition between states s and s is fully reversible, the converse can only be guaranteed if the Markov process admits a trajectory between s and s in either direction, i.e. if there exists a trajectory that visits both s and s (in any order). Observe that this condition much weaker than ergodicity, which requires that the Markov process admit a trajectory from any given state s to all other states s . In fact, the discrete environments we investigate in Sec 5 are non-ergodic.

Section Title: DETECTING AND PENALIZING SIDE EFFECTS FOR SAFE EXPLORATION
  DETECTING AND PENALIZING SIDE EFFECTS FOR SAFE EXPLORATION The problem of detecting and avoiding side-effects is well known and crucially important for safe exploration ( Moldovan & Abbeel, 2012 ;  Eysenbach et al., 2017 ;  Krakovna et al., 2018 ;  Armstrong & Levinstein, 2017 ). Broadly, the problem involves detecting and avoiding state transitions that permanently and irreversibly damage the agent or the environment ( Leike et al., 2017 ). As such, it is fundamentally related to reachability, as in the agent is prohibited from taking actions that drastically reduce the reachability between the resulting state and some predefined safe state. In  Eysenbach et al. (2017) , the authors learn a reset policy responsible for resetting the environment to some initial state after the agent has completed its trajectory. The resulting value function of the reset policy indicates when the actual (forward) policy executes an irreversible state transition, but at the cost of the added complexity of training a reset policy. In contrast,  Krakovna et al. (2018)  propose to attack the problem by measuring reachability relative to a safe baseline policy - namely by evaluating the reduction in reachability of all environment states from the current state with respect to that from a baseline state, where the latter is defined as the state that system would have (counterfactually) been in had the agent acted according to the corresponding baseline policy. However, determining the counterfactual baseline state requires a causal model of the environment, which cannot always assumed to be known. We propose to directly use the reachability measure η defined in Section 3.1 to derive a reward term for safe-exploration. Let r t be some external reward at time-step t. The augmented reward is given by:r t = r t − β · max{η(s t−1 → s t ), 0} (4) where β is a scaling coefficient. In practice, one may replace η with σ(η), where σ is a monotonically increasing transfer function (e.g. a step function). Intuitively, transitions s → s that are less reversible cause the h-potential to increase, and the resulting reachability measure η(s → s ) is positive in expectation. This incurs a penalty (due to the negative sign), which is reflected in the value function of the agent. Conversely, transitions that are reversible should have the property that η(s → s ) = 0 (also in expectation), thereby incurring no penalty.

Section Title: REWARDING CURIOUS BEHAVIOUR
  REWARDING CURIOUS BEHAVIOUR In most reinforcement learning applications, the reward function is assumed to be given; however, shaping a good reward function can often prove to be a challenging endeavour. It is in this context that the notion of curiosity comes to play an important role ( Schmidhuber, 2010 ;  Chentanez et al., 2005 ;  Pathak et al., 2017 ;  Burda et al., 2018 ;  Savinov et al., 2018 ). One typical approach towards encouraging curious behaviour is to seek novel states that surprise the agent ( Schmidhuber, 2010 ;  Published as a conference paper at ICLR 2020 Pathak et al., 2017 ;  Burda et al., 2018 ) and use the error in the agent's prediction of future states is used as a curiosity reward. This approach is however known to be susceptible to the so-called noisy-TV problem, wherein an uninteresting source of entropy like a noisy-TV can induce a large curiosity bonus because the agent cannot predict its future state.  Savinov et al. (2018)  propose to circumvent the noisy-TV problem by defining novelty in terms of (undirected) reachability, wherein states that are easily reachable from the current state are considered less novel. The h-potential and the corresponding reachability measure η affords another way of defining a curiosity reward. Say an agent's policy samples a trajectory from state s to s . Now, recall that η(s → s ) takes a positive value if state s is reachable from s (with respect to a simple reference policy); we therefore encourage the agent policy to sample trajectories where the η(s → s ) is negative, i.e. where s is less reachable from s. In doing so, we encourage the agent to seek states that are otherwise difficult to reach just by chance, and possibly learn useful skills in the process. In other words, we reward the agent for reversing the arrow of time (recall that η(s → s ) < 0 implies h(s ) < h(s)). The general form of the corresponding reward is given by: While the above is independent of the external reward function defined by the environment, the lat- ter might often align with the former: in many environments, the task at hand is to reach the least reachable state. This is readily recognized in classical control tasks like Pendulum, Cartpole and Mountain-Car, where the goal state is often the least reachable. However, if the environment's spec- ified task requires the agent to inadvertently execute irreversible trajectories, it is possible that our proposed reward is less applicable. Furthermore, while the proposed curiosity reward encourages the agent to reach for difficult-to-reach states, it need not provide an incentive to seek out diverse states. In other words: an agent optimizing the proposed reward may seek out the most difficult-to-reach states, but ignore other interesting but less difficult-to-reach states in the process (cf. App C.3). To summarize, we used the h-potential to define a directed measure of reachability (Sec 3.1), which then naturally lead to two applications. In the first (Sec 3.2), we obtained a safety penalty by essen- tially discouraging the agent from increasing the h-potential by executing difficult-to-reverse transi- tions. In the second (Sec 3.3), we argued that encouraging the agent to decrease the h-potential can provide an useful curiosity (intrinsic) reward signal in the absence of external rewards. In this sense, we have illustrated how the framework of a learned arrow of time (i.e. the h-potential) unifies the notions of reachability, safety, and curiosity.

Section Title: ALGORITHM
  ALGORITHM In Sec 2, we proposed a general functional objective, and defined the h-potential as the solution to the corresponding functional optimization problem. While the problem could be approached analytically with some effort for certain toy Markov chains (see App A), complex environments with unspecified dynamics require a fundamentally different approach. We therefore convert the functional optimization problem in Eqn 1 (right) to one over the parameters θ of a deep neural networkĥ θ to obtain the following surrogate problem: θ * = arg max θ E t∼U ({0,...,N −1}) E st E st+1|st [ĥ θ (s t+1 ) −ĥ θ (s t )|s t ] + λT [ĥ θ ] (6) where π is a reference policy, i.e. uniform random, and we denote the solutionĥ θ * by h. To train the network, the expectations are replaced by their sample estimates. As for the regularizer, recall that its purpose was to prevent h from diverging within a finite domain - this can be achieved by a loss term T (like the trajectory regularizer in Eqn 2), or by a training constraint like early stopping. The training algorithm is rather straightforward and can be summarized as follows (please refer to App B for the full algorithm). We first use an offline reference policy (uniform random, in our experiments) to sample trajectories from the environment. Next, we sample a batch of uniformly random state transitions and evaluate the objective in Eqn 6 (by replacing expectations by their sample estimates). We regularize the either by adding the trajectory regularizer to the objective or by using early stopping to terminate the training after a fixed number of iterations. Finally, we optimize the parameters θ ofĥ θ to maximize the objective at hand.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we empirically investigate the h-potential that we obtain with the training procedure described in the previous section. First, we show in a 2D-world environment that the h-potential learns to measure reachability. Second, we show that the h-potential can be used to detect side- effects in the challenging game of Sokoban ( Leike et al., 2017 ). Third, we show on the game of Mountain Car with Friction that the h-potential can learn to capture sailent features of the environ- ment, which can be used to formulate an intrinsic reward. We also demonstrate how the h-potential fails if the environment is not dissipative, i.e. if the friction is turned off. Finally, we show for a par- ticle undergoing Brownian motion under a potential that in expectation over states, the h-potential agrees reasonably well with the Free Energy functional, wherein the latter is known to be an arrow of time ( Jordan et al., 1998 ). Moreover in App C, we show results on three additional environments.

Section Title: Measuring Irreversibility
  Measuring Irreversibility The environment con- sidered is a 7 × 7 2D world, where cells can be oc- cupied by the agent, the goal and/or a vase (their respective positions are randomly sampled in each episode). If the agent enters a cell with a vase in it, the vase disappears without compromising the agent. In  Fig 2 , we plot the change in h-potential (recall that η(s t → s t+1 ) = h(s t+1 ) − h(s t )) to find that the breaking of a vase (irreversible) corresponds to the h-potential increasing in steps of roughly constant size (observe that the spikes attain similar heights), whereas the agent moving around (reversible) does not result in it increasing. This indicates that the h-potential has learned to quantify irreversibility in- stead of merely detecting it by counting the number of broken vases. In App C.1.1, (a) we further inves- tigate the effect of adding temporally-correlated and TV (uncorrelated) noise to the state and find that the h-potential is fairly robust to the latter but might get distracted by the former and (b) verify that an agent trained with the safety penalty in Eqn 4 breaks fewer vases (than without). against a wall. Moreover, the task of even determining whether a move is irreversible might be non- trivial, making the problem a good test-bed for detecting side-effects ( Leike et al., 2017 ). In  Fig 3 , we see that the h-potential increases if a box is pushed against a wall (irreversible side-effect) but remains constant if the agent moves about (reversible, even when the agent pushes a box around), demonstrating that the h-potential has indeed learned to detect side-effects. For experimental details and additional plots, please refer to App C.1.3.

Section Title: Obtaining Intrinsic Reward and the Importance of Dissipativity
  Obtaining Intrinsic Reward and the Importance of Dissipativity The environment considered shares its dynamics with the well known (continuous) Mountain-Car environment (Sutton & Barto, 2011), but with a crucial amendment: the car is subject to friction. Friction is required to make the environment dissipative and thereby induce an arrow of time (cf. Sec 2.2). Moreover, we initialize the system in a uniform-randomly sampled state to avoid exploration issues (cf. App C.3). In  Fig 4 , we see that the learned h-potential roughly recovers the terrain from random trajectories (i.e. without external rewards), which can now be used to obtain an intrinsic reward signal. Further, Fig 5b illustrates the importance of dissipation (in this case, induced via friction). Details in App C.2.2.

Section Title: Comparison with the Free-Energy Functional.
  Comparison with the Free-Energy Functional. The setting considered is that of a particle (a random-walker) undergoing Brownian motion un- der the influence of a potential field Ψ(x) (where x denotes the spatial position). We denote the proba- bility of finding the particle at position x at time t by ρ(x, t). Now, the dynamics of the corresponding time-dependent random variable (i.e. stochastic pro- cess) X(t) is governed by the stochastic differential equation: dX(t) = −∇Ψ(X(t))dt + 2β −1 dW(t) (7) where W(t) is the standard Wiener process (i.e. dW(t) is white-noise) and β −1 is a temperature pa- rameter. The Free-Energy functional F is now de- fined as: where the first expectation of the RHS is the energy functional, and the second expectation is the negative entropy. A celebrated result due to  Jordan, Kinderlehrer, and Otto (1998)  is that the Free-Energy is a Lyapunov functional of the dynamics, i.e. it can only decrease with time, thereby defining a notion of an arrow of time. Now, to find out how well our learned arrow of time agrees with the Free-Energy functional, we train it with realizations of the stochastic process X(t) in two-dimensions.  Fig 6  plots the Free-Energy functional F against a linearly adjusted H-functional, defined as: H[ρ(·, t)] = −E x∼ρ(·,t) [h(x)]. Indeed, we find that up to a linear transform, the H-functional (and the corresponding h-potential) agrees reasonably well with the true arrow of time given by the Free-Energy functional F . Crucially, the H-functional is also a Lyapunov functional of the dynamics - implying that in expectation over states, the h-potential functions as an arrow of time. Details can be found in App C.4.

Section Title: CONCLUSION
  CONCLUSION In this work, we approached the problem of learning an arrow of time in a Markov (Decision) Processes. We defined the arrow of time (h-potential) as a solution to an optimization problem and laid out the conceptual roadblocks that must be cleared before it can be useful in the RL context. But once these roadblocks have been cleared, we demonstrated how the notions of reachability, safety and curiosity can be bridged by a common framework of a learned arrow of time. Finally, we empirically investigated the strengths and shortcomings of our method on a selection of discrete and continuous environments. Future work could draw connections to algorithmic independence of cause and mechanism ( Janzing et al., 2016 ) and explore applications in causal inference ( Janzing, 2010 ;  Peters et al., 2017 ).
  In particular, observe that a dissipative system may or may not be ergodic.

```
