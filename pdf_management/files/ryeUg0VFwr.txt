Title:
```
Under review as a conference paper at ICLR 2020 STRIVING FOR SIMPLICITY IN OFF-POLICY DEEP REINFORCEMENT LEARNING
```
Abstract:
```
This paper advocates the use of offline (batch) reinforcement learning (RL) to help (1) isolate the contributions of exploitation vs. exploration in off-policy deep RL, (2) improve reproducibility of deep RL research, and (3) facilitate the design of simpler deep RL algorithms. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on all of the replay data of an online DQN agent, can outperform online DQN on Atari 2600 games. We also present Random Ensemble Mixture (REM), a simple Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Using the DQN replay dataset, offline REM surpasses the gains from online C51 and outperforms offline QR-DQN. Furthermore, REM performs comparably to QR-DQN in the online RL setting on Atari 2600 games. The DQN replay dataset can serve as an offline RL benchmark and will be released.
```

Figures/Tables Captions:
```
Figure 1: Median normalized online evaluation scores averaged over 5 runs (shown as traces) across stochastic version of 60 Atari 2600 games of (a) offline RL agents trained using the DQN replay dataset containing 200 million frames. Offline REM outperforms offline QR-DQN and surpasses gains from online C51. (b) online RL agents trained for 200 million frames (standard protocol). Online REM with 4 Q-networks performs comparably to online QR-DQN. Each iteration corresponds to 1 million frames.
Figure 2: Normalized performance improvement (in %) over online DQN (Nature), per game, of (a) offline DQN (Nature) and (b) offline QR-DQN trained using the DQN replay dataset for same number of gradient updates as online DQN. The normalized online score for each game is 0.0 and 1.0 for the worse and better performing agent among fully trained online DQN and random agents respectively.
Figure 3: Neural network architectures for DQN, QR-DQN and the proposed expected RL variants, i.e., Ensemble-DQN and REM, with the same multi-head architecture as QR-DQN. The individual Q-heads share all of the neural network layers except the final fully connected layer.
Figure 4: Offline REM vs. Baselines. Normalized scores averaged over 5 runs (shown as traces) across 60 stochastic Atari 2600 games of offline agents trained using the DQN replay dataset trained for 5X gradient steps.
 
Figure 5: Effect of Dataset Size. Normalized scores (averaged over 5 runs) of QR-DQN and multi-head REM trained offline on stochastic version of 5 Atari 2600 games for 5X gradient steps using only a fraction of the entire DQN replay dataset (200M frames) obtained via randomly subsampling trajectories.
Figure 6: Effect of Dataset Quality. Normalized scores (averaged over 3 runs) of QR-DQN and multi-head REM trained offline on stochastic version of 5 Atari 2600 games for 5X gradient steps using logged data from online DQN trained only for 20M frames (20 iterations). The horizontal line shows the performance of best policy found during DQN training for 20M frames which is significantly worse than fully-trained DQN.
Table 1: Median normalized best average eval- uation scores (averaged over 5 runs) across 60 stochastic Atari 2600 games, measured as percent- ages and number of games where an offline agent achieves better scores than a fully trained online DQN (Nature) agent.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep neural networks have become a critical component of modern reinforcement learning (RL) ( Sut- ton and Barto, 2018 ). The seminal work of  Mnih et al. (2013 ; 2015) on deep Q-networks (DQN) has demonstrated that it is possible to train neural networks using Q-learning ( Watkins and Dayan, 1992 ) to achieve human-level performance in playing Atari 2600 games ( Bellemare et al., 2013 ) directly from raw pixels. Recent progress in mastering Go ( Silver et al., 2016 ) and advances in robotic control ( Levine et al., 2016 ;  OpenAI et al., 2018 ;  Kalashnikov et al., 2018 ) present additional supporting evidence for the enormous potential of deep RL. Off-policy RL algorithms such as Q-learning are attractive because they disentangle data collection and policy optimization and offer more sample efficient solutions than on-policy algorithms ( Sutton et al., 2000 ;  Schulman et al., 2015 ;  Mnih et al., 2016 ). Importantly, off-policy techniques can leverage the vast amount of existing offline logged data for real-world applications such as digital advertis- ing ( Strehl et al., 2010 ;  Bottou et al., 2013 ), education ( Mandel et al., 2014 ), and healthcare ( Shortreed et al., 2011 ). Since online RL is often unsafe to deploy in the real world, offline RL algorithms are the only feasible solution for many practical decision making problems ( Dulac-Arnold et al., 2019 ). Nevertheless, off-policy RL algorithms when combined with neural networks can be unstable or even divergent ( Baird, 1995 ;  Boyan and Moore, 1995 ;  Tsitsiklis and Van Roy, 1997 ). In the absence of theoretical guarantees for off-policy deep RL, recent advances (see  Hessel et al. (2018)  for an overview) are largely governed by empirical results on a popular benchmark suite of Atari 2600 games ( Bellemare et al., 2013 ). Unfortunately, these empirical results are difficult to reproduce ( Henderson et al., 2018 ) and the contribution of novel RL algorithms is often conflated with many other design choices ( Clary et al., 2019 ;  Khetarpal et al., 2018 ). Given the empirical nature of deep off-policy RL, it is crucial to come up with simpler and reproducible experimental settings and study the relative importance of different components of deep RL algorithms. It is also important to strive for finding successful RL algorithms that are as simple as possible. This paper advocates the use of offline (batch) RL to help (1) isolate the contributions of exploitation vs. exploration in off-policy deep RL, (2) improve reproducibility of deep RL research, and (3) facilitate the design of simpler off-policy deep RL algorithms. To this end, we propose a new benchmark for offline optimization of RL agents on Atari 2600 games using all of the replay data of a DQN agent ( Mnih et al., 2015 ). Using this benchmark, we investigate the following questions: 1. Is it possible to train successful Atari agents based solely on offline data? 2. Can one design simple and effective alternatives to intricate RL algorithms in the offline setting? 3. Are the insights gained from the offline setting useful for developing effective online algorithms? The contributions of this paper can be summarized as: • An offline RL benchmark is proposed for evaluating and designing RL algorithms on Atari 2600 games without exploration, based on the logged replay data of a DQN agent comprising 50 million (observation, action, reward, next observation) tuples per game. This reduces the computation cost of the experiments considerably and helps improve reproducibility of deep RL research by standardizing training using a fixed offline dataset. The replay dataset used in our experiments will be released to enable offline optimization of RL algorithms on a common ground. • Contrary to recent work ( Zhang and Sutton, 2017 ;  Fujimoto et al., 2019 ), we find that the logged DQN data is sufficient for optimizing strong Atari agents offline without any environment interaction. For instance, QR-DQN ( Dabney et al., 2018b ) trained offline on the DQN replay dataset significantly outperforms online DQN ( Mnih et al., 2015 ). • A simple and novel Q-learning algorithm called Random Ensemble Mixture (REM) is presented, which enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM, trained using the DQN replay dataset, outperforms more complex RL algorithms such as offline QR-DQN and surpasses the gains from online C51 ( Bellemare et al., 2017 ) over online DQN (Figure 1a). • We use the insights gained from the offline experiments to develop an online variant of REM, which performs comparably with online QR-DQN (Figure 1b).

Section Title: OFF-POLICY REINFORCEMENT LEARNING
  OFF-POLICY REINFORCEMENT LEARNING An interactive environment in reinforcement learning (RL) is typically described as a Markov decision process (MDP) (S, A, R, P, γ) ( Puterman, 1994 ), with a state space S, an action space A, a stochastic reward function R(s, a), transition dynamics P (s |s, a) and a discount factor γ ∈ [0, 1). A stochastic policy π(· | s) maps each state s ∈ S to a distribution (density) over actions. For an agent following the policy π, the action-value function, denoted Q π (s, a), is defined as the expectation of cumulative discounted future rewards, i.e., The goal of RL is to find an optimal policy π * that attains maximum expected return, for which Q π * (s, a) ≥ Q π (s, a) for all π, s, a. The Bellman optimality equations ( Bellman, 1957 ) characterize the optimal policy in terms of the optimal Q-values, denoted Q * = Q π * , via: To learn a policy from interaction with the environment, Q-learning ( Watkins and Dayan, 1992 ) iteratively improves an approximate estimate of Q * , denoted Q θ , by repeatedly regressing the LHS of (2) to target values defined by samples from the RHS of (2). For large and complex state Under review as a conference paper at ICLR 2020 spaces, approximate Q-values are obtained using a neural network as the function approximator. To further stabilize optimization, a target network Q θ with frozen parameters may be used for computing the learning target ( Mnih et al., 2013 ). The target network parameters θ are updated to the current Q-network parameters θ after a fixed number of time steps. DQN ( Mnih et al., 2013 ; 2015) parameterizes Q θ with a convolutional neural network ( LeCun et al., 1998 ) and uses Q-learning with a target network while following an -greedy policy with respect to Q θ for data collection. DQN minimizes the TD error using the loss L(θ) on mini-batches of agent's past experience tuples, (s, a, r, s ), sampled from an experience replay buffer D ( Lin, 1992 ) collected during training: Q-learning is an off-policy algorithm ( Sutton and Barto, 2018 ) since the learning target can be computed without any consideration of how the experience was generated. In offline (batch) RL ( Ernst et al., 2005 ;  Riedmiller, 2005 ;  Lange et al., 2012 ), we assume access to a fixed offline dataset of experiences D, without any further interaction with the environment. A family of recent off-policy deep RL algorithms, which serve as a strong baseline in this paper, include Distributional RL ( Bellemare et al., 2017 ;  Jaquette, 1973 ) methods. Such algorithms estimate a density over returns for each state-action pair, denoted Z π (s, a), instead of directly estimating the mean Q π (s, a). Accordingly, one can express a form of distributional Bellman optimality as Z * (s, a) D = r + γZ * (s , argmax a ∈A Q * (s , a )), where r ∼ R(s, a), s ∼ P (· | s, a), (4) and D = denotes distributional equivalence and Q * (s , a ) is estimated by taking an expectation with respect to Z * (s , a ). C51 ( Bellemare et al., 2017 ) approximates Z * (s, a) by using a categorical distribution over a set of pre-specified anchor points, and distributional QR-DQN ( Dabney et al., 2018b ) approximates the return density by using a uniform mixture of K Dirac delta functions, i.e., QR-DQN trains each θ i for 1 ≤ i ≤ K to match the 2i−1 2K -quantile of the target return density (RHS of (4)) using the Huber quantile regression loss ( Koenker, 2005 ;  Dabney et al., 2018b ). QR-DQN, albeit complex, outperforms C51 and DQN and obtains state-of-the-art results on Atari 2600 games, among agents that do not exploit n-step updates ( Sutton, 1988 ) and prioritized replay ( Schaul et al., 2016 ). This paper avoids using n-step updates and prioritized replay to keep the empirical study simple and focused on deep Q-learning algorithms.

Section Title: OFFLINE OPTIMIZATION OF REINFORCEMENT LEARNING AGENTS
  OFFLINE OPTIMIZATION OF REINFORCEMENT LEARNING AGENTS Modern off-policy deep RL algorithms perform remarkably well on common benchmarks such as the Atari 2600 games ( Bellemare et al., 2013 ) and continuous control MuJoCo tasks ( Todorov et al., 2012 ). Such off-policy algorithms ( Mnih et al., 2015 ;  Lillicrap et al., 2015 ) are considered "online" because they alternate between optimizing a policy and using that policy to collect more data. Typically, these algorithms keep a sliding window of most recent experiences in a finite replay buffer ( Lin, 1992 ), throwing away stale data to incorporate most fresh (on-policy) experiences. In principle, off-policy algorithms can learn from data collected by any policy, however,  Zhang and Sutton (2017)  assert that using a large replay buffer can significantly hurt the performance of Q-learning algorithms, since it can delay rare on-policy experiences that are important for policy learning. This paper revisits offline off-policy RL and investigates whether off-policy deep RL agents trained solely on offline data can be successful. We advocate the use of offline RL to help isolate an RL algorithm's ability to exploit experience and generalize vs. its ability to explore effectively by collecting interesting new data. The offline RL setting removes design choices related to the replay buffer and exploration; therefore, it is much simpler to experiment with and reproduce than the typical online off-policy learning. Also, in the offline setting, we optimize an objective (e.g., (3)) over a fixed dataset as opposed to a changing replay buffer of an online agent. In this work, we mainly experiment Under review as a conference paper at ICLR 2020 with large and diverse datasets motivated by real-world RL problems where we already have access to (usually continually growing) diverse datasets of logged experiences, e.g., historical data from recommender systems ( Bottou et al., 2013 ), database of robotic experiences ( Dasari et al., 2019 ) etc. To facilitate a study of offline RL with large and diverse datasets, we train several instances of DQN (Nature) agents ( Mnih et al., 2015 ) on stochastic version of 60 Atari 2600 games for 200 million frames each, with a frame skip of 4 (standard protocol) and sticky actions enabled ( Machado et al., 2018 ). On each game, we train 5 different agents with random initialization, and store all of the tuples of (observation, action, reward, next observation) encountered during training into 5 replay datasets of 50 million tuples each. These replay datasets are used for training standard off-policy RL agents, offline, without any interaction with the environment during training. We perform an online evaluation of the agents in the intervals of 1 million training frames, and report the best evaluation score for each agent, averaged over 5 runs.

Section Title: EXPERIMENTS AND RESULTS
  EXPERIMENTS AND RESULTS Given the logged replay data of the DQN agent, it is natural to ask whether training an offline variant of DQN on this data will recover the performance of online DQN. Furthermore, whether more recent off-policy algorithms are able to exploit the DQN replay dataset more effectively than DQN. To investigate these questions, we train DQN (Nature) and distributional QR-DQN ( Dabney et al., 2018b ) agents, offline, on the DQN replay dataset for the same number of gradient updates as online DQN. We use the hyperparameters provided in the Dopamine baselines ( Castro et al., 2018 ) for a standardized comparison (Section A.4) and report scores using a normalized scale (Section A.3). We note that the replay datasets include samples from all of the intermediate policies seen during the optimization of the DQN (Nature) agent. Accordingly, we compare the performance of offline agents against the best performing policy among the mixture of data collecting policies.

Section Title: Results
  Results We find that offline DQN underperforms online DQN on all except a few games while offline QR-DQN outperforms offline DQN and online DQN on most of the games ( Figure 2 ). C51 ( Bellemare et al., 2017 ) trained offline using the DQN replay dataset also considerably improves upon offline DQN (Figure A.4). These results suggest that DQN (Nature) is somewhat ineffective at exploiting off-policy data. However, offline QR-DQN's impressive performance demonstrates that it is possible to optimize strong Atari agents completely offline (see Figure A.6 for learning curves).

Section Title: OFFLINE CONTINUOUS CONTROL EXPERIMENTS
  OFFLINE CONTINUOUS CONTROL EXPERIMENTS In contrast to our offline results on Atari 2600 games with discrete action spaces, discussed above,  Fujimoto et al. (2019)  find that standard off-policy deep RL agents are not effective on continuous control problems when trained offline, even when large and diverse replay datasets are used. The results of  Fujimoto et al. (2019)  are based on the evaluation of a standard continuous control agent, called DDPG ( Lillicrap et al., 2015 ), and other more recent continuous control algorithms such as TD3 ( Fujimoto et al., 2018 ) and SAC ( Haarnoja et al., 2018 ) are not considered in their study. Motivated by the so-called final buffer setting in  Fujimoto et al. (2019)  (Section A.2), we train a DDPG agent on continuous control MuJoCo tasks ( Todorov et al., 2012 ) for 1 million time steps and store all of the experienced transitions. Using this dataset, we train standard off-policy agents including Under review as a conference paper at ICLR 2020 TD3 and DDPG completely offline. Consistent with our offline results on Atari games, offline TD3 significantly outperforms the data collecting DDPG agent and offline DDPG (Figure A.1).

Section Title: SEEKING SIMPLER OFF-POLICY RL ALGORITHMS
  SEEKING SIMPLER OFF-POLICY RL ALGORITHMS

Section Title: Section
  Section where l δ is the Huber loss. While Bootstrapped-DQN uses one of the Q-value estimates in each episode to improve exploration, in the offline setting, we are only concerned with the ability of Ensemble-DQN to exploit better and use the mean of the Q-value estimates for evaluation.

Section Title: RANDOM ENSEMBLE MIXTURE (REM)
  RANDOM ENSEMBLE MIXTURE (REM) Increasing the number of models used for ensembling typically improves the performance of su- pervised learning models ( Shazeer et al., 2017 ). This raises the question whether one can use an ensemble over an exponential number of Q-estimates in a computationally efficient manner. Inspired by dropout ( Srivastava et al., 2014 ), we propose Random Ensemble Mixture for RL. Random Ensemble Mixture (REM) uses multiple parameterized Q-functions to estimate the Q-values, similar to Ensemble-DQN. The key insight behind REM is that one can think of a convex combination of multiple Q-value estimates as a valid Q-value estimate itself. This is especially true at the fixed point, where all of the Q-value estimates have converged to an identical Q-function. Using this insight, we train a family of Q-function approximators defined by mixing probabilities on a (K − 1)-simplex. Specifically, for each mini-batch, we randomly draw a categorical distribution α, which defines a convex combination of the K estimates to approximate the optimal Q-function. This approximator is trained against its corresponding target to minimize the TD error. The loss L(θ) takes the form, REM considers Q-learning as a constraint satisfaction problem based on Bellman optimality con- straints (2) and L(θ) can be viewed as an infinite set of constraints corresponding to different mixture probability distributions. For action selection, we use the average of the K value estimates as the Q-function, i.e., Q(s, a) = k Q k θ (s, a)/K. REM is easy to implement and analyze (see Proposition 1), and can be viewed as a simple regularization technique for value-based RL. In our experiments, we use a very simple distribution P ∆ : we first draw a set of K values i. i. d. from Uniform (0, 1) and normalize them to get a valid categorical distribution, i.e., α k ∼ U (0, 1) followed by α k = α k / α i .

Section Title: EXPERIMENTS AND RESULTS
  EXPERIMENTS AND RESULTS

Section Title: Asymptotic performance of offline agents
  Asymptotic performance of offline agents In supervised learning, asymptotic performance matters much more than performance within a fixed budget of gradient updates. Similarly, for a given sample complexity, we prefer RL algorithms that perform the best as long as the number of gradient updates is feasible. Since the sample efficiency of the offline agents for a given dataset is fixed, we train them on the DQN replay dataset for five times as many gradient updates as online DQN. Comparison with QR-DQN. QR-DQN modifies the DQN (Nature) architecture to output K values for each action using a multi-head Q-network and replaces RMSProp ( Tieleman and Hinton, 2012 ) with Adam ( Kingma and Ba, 2015 ) for optimization. To ensure a fair comparison with QR-DQN, we use the same multi-head Q-network as QR-DQN with K = 200 heads ( Figure 3 ), where each head represents a Q-value estimate for REM and Ensemble-DQN. We also use Adam for optimization.

Section Title: Additional Baselines
  Additional Baselines baseline determines whether the random combinations of REM provide any significant benefit over simply using an ensemble of predictors to stabilize the Bellman target. REM with separate Q-networks. To investigate whether the effectiveness of REM is linked to ensembling techniques and dropout, we compare offline REM with K Q-value estimates computed using K separate Q-networks vs. a multi-head Q-network for K ∈ {4, 16}.

Section Title: Results
  Results Figure 4a shows the comparison of the additional baselines with REM and Ensemble-DQN and  Table 1  summarizes the asymptotic performance results. DQN with Adam noticeably bridges the gap in asymptotic performance between QR-DQN and DQN (Nature) in the offline setting. Offline Ensemble-DQN does not improve upon this strong DQN baseline showing that its naive ensembling approach is inadequate. Furthermore, Averaged Ensemble-DQN performs only slightly better than Ensemble-DQN. In contrast, REM exploits offline data more effectively than other agents, including QR-DQN, when trained for more gradient updates. Surprisingly, using the DQN replay dataset, offline REM surpasses the gains from online C51, a huge improvement over online DQN (Figure 1a). Based on above empirical results, we hypothesize that gains from REM are due to the noise from randomly ensembling Q-value estimates leading to more robust training analogous to dropout. Furthermore, REM with separate Q-networks performs better asymptotically and learns faster than REM with a multi-head Q-network in the offline setting (Figure A.5). This consolidates our belief about effectiveness of REM being linked to dropout as separate Q-networks result in more variation in individual Q-estimates leading to more robust training.

Section Title: FROM OFFLINE TO ONLINE REINFORCEMENT LEARNING
  FROM OFFLINE TO ONLINE REINFORCEMENT LEARNING Can we combine the insights gained from the offline setting with appropriate design choices (e.g., ex- ploration, replay buffer) to create effective online methods? In online RL, learning and data generation are tightly coupled, i.e., an agent that learns faster also collects more relevant training data. We ran online REM with K separate Q-networks (with K = 4 for computational efficiency) because of the improved convergence speed over multi-head REM in the offline setting. For data collection, we use -greedy with a randomly sampled Q-estimate from the simplex for each episode, similar to Bootstrapped DQN. To estimate the gains from the REM objective (7) in the online setting, we also evaluate Bootstrapped-DQN with identical modifications (e.g., separate Q-networks) as online REM. Figure 1b show that REM performs on par with QR-DQN and considerably outperforms Bootstrapped-DQN (see Figure A.8 for learning curves). Based on these results, we conclude that the offline setting can help facilitate the design of effective online off-policy RL algorithms.

Section Title: OFFLINE RL: DATASET SIZE AND COMPOSITION
  OFFLINE RL: DATASET SIZE AND COMPOSITION Our offline learning results indicate that 50 million logged experience tuples per game from DQN (Na- ture) are sufficient to obtain good online performance on most of the Atari 2600 games. One may argue that the size of the fixed replay and its composition play a key role in our empirical re- sults ( De Bruin et al., 2015 ). To study the role of the replay buffer size in the success of the offline agents, we perform an ablation experiment with variable replay buffer size. We conducted offline experiments with reduced data obtained via randomly subsampling entire trajectories from the logged DQN experiences, thereby maintaining the same data distribution.  Figure 5  presents the performance of the offline REM and QR-DQN agents with N % of the tuples in the DQN replay dataset where N ∈ {1, 10, 20, 50, 100}. As expected, performance tends to increase Under review as a conference paper at ICLR 2020 as the fraction of data increases. With N ≥ 10%, REM and QR-DQN still perform comparably to online DQN on most of these games. However, the performance deteriorates drastically for N = 1%. To see the effect of quality of offline dataset, we perform another ablation where we train offline REM and QR-DQN on the first 20 million frames in the DQN replay dataset which approximate exploration data with suboptimal returns. Similar to our offline results with the entire replay dataset, on most Atari games, offline REM and QR-DQN perform comparably and outperform the best policy amongst the mixture of policies which collected the first 20 million frames ( Figure 6  and Figure A.9).

Section Title: DISCUSSION AND FUTURE WORK
  DISCUSSION AND FUTURE WORK Since the (observation, action, next observation, reward) tuples in DQN replay dataset are stored in the order they were experienced by online DQN during training, various data collection strategies for benchmarking offline RL can be induced by subsampling the replay dataset containing 200 million frames. For example, the first k million frames from the DQN replay dataset emulate exploration data with suboptimal returns (e.g.,  Figure 6 ) while the last k million frames are analogous to near- expert data with stochasticity. Another option is to randomly subsample the entire dataset to create smaller offline datasets with sufficient diversity (e.g.,  Figure 5 ). Based on the popularity and ease of experimentation on Atari games, we hope that the DQN replay dataset can be used for benchmarking offline RL in addition to continuous control setups  Fujimoto et al. (2019) ;  Kumar et al. (2019) . Offline RL tackles the challenge of squeezing the most performance gains out of a fixed batch of data. This indicates the utility of offline RL for creating more sample-efficient RL agents as an online agent should exploit the data collected prior to any given time during online training as much as possible before further collecting new data via exploration. The proposed offline benchmark can be utilized for developing better RL algorithms for various data regimes shown in  Figure 5 . In our offline RL setup, the agents exhibit overfitting on some games ( Figure 6 ), even with 50 million tuples (Figure A.7), i.e., after a sufficiently large number of gradient updates, their performance degrades. We also observe divergence w.r.t. Huber loss in our reduced data experiments with N = 1%. Based on these findings, we posit that our offline benchmark can be used for designing techniques akin to regularization for stable off-policy deep RL algorithms with reasonable performance throughout learning ( Garcıa and Fernández, 2015 ), a requirement for many practical RL applications. Finally, investigating Rainbow components, e.g., prioritized replay ( Schaul et al., 2016 ), n-step updates ( Sutton, 1988 ), double Q-learning ( Van Hasselt et al., 2016 ), etc., in the offline RL setting and incorporating them with REM are interesting directions. Experience replay-based algorithms can be more sample efficient than model-based approaches ( Van Hasselt et al., 2019 ), and using the DQN replay dataset on Atari 2600 games for designing non-parametric replay models ( Pan et al., 2018 ) and parametric world models ( Kaiser et al., 2019 ) is quite promising for improving sample-efficiency. We also leave further investigation of the exploitation ability of distributional RL to future work.

Section Title: RELATED WORK
  RELATED WORK Our work is motivated by whether the complexity of recent off-policy approaches (e.g., Rainbow ( Hes- sel et al., 2018 )) is necessary and is similar in spirit to work by  Rajeswaran et al. (2017) , which attempts to demystify the complexity of on-policy deep RL for continuous control. Our work is mainly related to two subareas of RL known as batch RL and distributional RL. For the sake of clarity, we refer to batch RL as offline RL in other sections of the paper.

Section Title: Batch Reinforcement Learning
  Batch Reinforcement Learning While there has been increasing interest in batch RL ( Lange et al., 2012 ) over the last few years ( Jiang and Li, 2016 ;  Farajtabar et al., 2018 ;  Irpan et al., 2019 ), much of this focussed on off-policy policy evaluation, where the goal is to estimate the performance of a given fixed policy. Similar to ( Ernst et al., 2005 ;  Kalyanakrishnan and Stone, 2007 ;  Jaques et al., 2019 ), we investigate batch off-policy learning, which requires learning a good policy given a fixed dataset. In our offline setup, we only assume access to samples from the behavior policy and focus on Q-learning methods without any importance sampling correction as opposed to ( Swaminathan and Joachims, 2015 ;  Liu et al., 2019 ). Recent work ( Fujimoto et al., 2019 ;  Kumar et al., 2019 ) documents that standard off-policy methods with fixed datasets fail on continuous control MuJoCo tasks. Opposed to  Kumar et al. (2019) , we focus on the offline setting with data collected from a diverse mixture of policies rather than a single Markovian behavior policy. Furthermore, in contrast to  Fujimoto et al. (2019) , our batch learning results on MuJoCo tasks and Atari 2600 games (Section 3) demonstrate that standard off-policy deep RL algorithms (e.g., TD3 ( Fujimoto et al., 2018 ), QR-DQN ( Dabney et al., 2018b )) are quite effective when learning truly off-policy from large and diverse datasets.  Zhang and Sutton (2017)  show that large replay buffers can hurt the performance of simple Q-learning methods with weak function approximators; they attribute this to the "off-policyness" of a large buffer which might delay important transitions for learning. However, our results reveal that even with uniform sampling, effective deep Q-learning algorithms (e.g., REM) can exploit completely off-policy logged DQN data on Atari 2600 games, given sufficient number of gradient updates.

Section Title: Distributional Reinforcement Learning
  Distributional Reinforcement Learning Recently, distributional RL algorithms (e.g., C51 ( Belle- mare et al., 2017 ), QR-DQN ( Dabney et al., 2018b ), IQN ( Dabney et al., 2018a )) gained popularity due to their strong performance on Atari 2600 games. We observe that distributional algorithms (e.g., QR- DQN, C51) are quite effective at exploitation and develop much simpler REM (Section 4.2), which is as successful as QR-DQN. In contrast to distributional RL, much of the theoretical results of Q-learning naturally extends to REM due to its similarity to Q-learning, e.g., in the tabular setting, REM converges to Q * , while popular distributional methods (e.g., QR-DQN, C51) need not learn true return statistics corresponding to the underlying MDP ( Rowland et al., 2019 ).

Section Title: CONCLUSIONS
  CONCLUSIONS This work investigates off-policy deep RL using an offline RL benchmark on Atari 2600 games based on logged experiences of a DQN agent. We demonstrate that it is possible to learn policies with high returns that significantly outperform the policies used to collect replay data. We develop REM, a simple variant of ensemble Q-learning, that can effectively exploit large-scale off-policy data and shares much of the success of the more complicated distributional RL algorithms on Atari 2600 games. The proposed benchmark can serve as a testbed for designing simple, stable and sample-efficient RL algorithms and enable the research community to evaluate off-policy methods on a common ground. As mentioned by  Sutton and Barto (2018 , Chapter 11.10): "The potential for off-policy learning remains tantalizing, the best way to achieve it still a mystery."

```
