Title:
```
Under review as a conference paper at ICLR 2020 CONSTRAINED MARKOV DECISION PROCESSES VIA BACKWARD VALUE FUNCTIONS
```
Abstract:
```
Although Reinforcement Learning (RL) algorithms have found tremendous success in simulated domains, they often cannot directly be applied to physical systems, especially in cases where there are hard constraints to satisfy (e.g. on safety or resources). In standard RL, the agent is incentivized to explore any behavior as long as it maximizes rewards, but in the real world undesired behavior can damage either the system or the agent in a way that breaks the learning process itself. In this work, we model the problem of learning with constraints as a Constrained Markov Decision Process, and provide a new on-policy formulation for solving it. A key contribution of our approach is to translate cumulative cost constraints into state-based constraints. Through this, we define a safe policy improvement method which maximizes returns while ensuring that the constraints are satisfied at every step. We provide theoretical guarantees under which the agent converges while ensuring safety over the course of training. We also highlight computational advantages of this approach. The effectiveness of our approach is demonstrated on safe navigation tasks and in safety-constrained versions of MuJoCo environments, with deep neural networks.
```

Figures/Tables Captions:
```
Figure 1: (a) Example of a gridworld environment. (b,c) Performance over the training for Unconstrained (red), Lyapunov-based (green), and our method (blue) all trained with n-step SARSA on 2D GridWorld task over 20 random seeds. The x-axis is the number of episodes in thousands. The dotted black line in (c) denotes the constraint threshold, d0. The bold line represents mean, and the shaded region denotes 80% confidence-intervals.
Figure 2: A2C Performance over the training for Unconstrained (red), Lyapunov-based (green), and our method (blue) all trained with A2C on MuJoCo tasks over 10 random seeds. The x-axis is the number of episodes in thousands. The dotted black line denotes d0. The bold line represents the mean, and the shaded region denotes the 80% confidence-intervals.
Figure 3: PPO Performance over the training for Unconstrained (red), Lyapunov-based (green), and our method (blue) all trained with PPO on MuJoCo tasks over 10 random seeds. The x-axis is the number of episodes in thousands, and y-axis denotes the undiscounted accumulated returns. The dotted black line denotes d0. The bold line represents the mean, and the shaded region denotes the 80% confidence-intervals.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement Learning (RL) provides a sound decision-theoretic framework to optimize the behavior of learning agents in an interactive setting ( Sutton & Barto, 2018 ). Recently, the field of RL has found success in many high-dimensional domains, like video games, Go, robot locomotion and navigation. However, most of the success of RL algorithms has been limited to simulators, where the learning algorithm has the ability to reset the simulator. In the physical world, an agent will need to avoid harmful behavior (e.g. damaging the environment or the agent's hardware) while learning to explore behaviors that maximize the reward. A few popular approaches for avoiding undesired behaviors for high-dimensional systems include reward-shaping ( Moldovan & Abbeel, 2012 ), reachability-preserving algorithms ( Mitchell, 2003 ;  Eysenbach et al., 2017 ), state-level surrogate constraint satisfaction algorithms ( Dalal et al., 2018 ), risk-sensitive algorithms ( Tamar et al., 2013 ;  Chow et al., 2015 ) and apprenticeship learning (Abbeel & Ng, 2004). There also exists model-based Bayesian approaches that are focused on imposing the constraints via the dynamics (such as classifying parts of state space as unsafe) and then using model predictive control to incorporate the constraints in the policy optimization and planning (Turchetta et al., 2016;  Berkenkamp et al., 2017 ;  Wachi et al., 2018 ;  Koller et al., 2018 ). A natural way to model safety is via constraint satisfaction. A standard formulation for adding constraints to RL problems is the Constrained Markov Decision Process (CMDP) framework ( Altman, 1999 ), wherein the environment is extended to also provide feedback on constraint costs. The agent must then attempt to maximize its expected return while also satisfying cumulative constraints. A few algorithms have been proposed to solve CMDPs for high-dimensional domains with continuous action spaces - however they come with their own caveats. Reward Constrained Policy Optimization ( Tessler et al., 2018 ) and Primal Dual Policy Optimization ( Chow et al., 2015 ) do not guarantee constraint satisfaction during the learning procedure, only on the final policy. Constrained Policy Optimization ( Achiam et al., 2017 ) provides monotonic policy improvement but is computationally expensive due to requiring a backtracking line-search procedure and conjugate gradient algorithm for approximating the Fisher Information Matrix. Lyapunov-based Safe Policy Optimization (Chow Under review as a conference paper at  ICLR 2020 et al., 2019 ) requires solving a Linear Program (LP) at every step of policy evaluation, although they show that there exists heuristics which can be substituted for the LP at the expense of theoretical guarantees. In this work, we propose an alternate formulation for solving CMDPs that transforms trajectory-level constraints into localized state-dependent constraints, through which a safe policy improvement step can be defined. In our approach, we define a notion of Backward Value Functions, which act as an estimator of the expected cost collected by the agent so far and can be learned via standard RL bootstrap techniques. We provide conditions under which this new formulation is able to solve CMDPs without violating the constraints during the learning process. Our formulation allows us to define state-level constraints without explicitly solving a LP or the Dual problem at every iteration. Our method is implemented as a reduction to any model-free on-policy bootstrap based RL algorithm, both for deterministic and stochastic policies, and discrete and continuous action spaces. We provide the empirical evidence of our approach with Deep RL methods on various safety benchmarks, including 2D navigation grid worlds ( Leike et al., 2017 ;  Chow et al., 2018 ), and MuJoCo tasks ( Achiam et al., 2017 ;  Chow et al., 2019 ).

Section Title: CONSTRAINED MARKOV DECISION PROCESSES
  CONSTRAINED MARKOV DECISION PROCESSES We write P(Y ) for the set of probability distributions on a space Y . A Markov Decision Process (MDP) ( Puterman, 2014 ) is a tuple (X , A, P, r, x 0 ), where X is a set of states, A is a set of actions, r : X × A → [0, R M AX ] is a reward function, P : X × A → P(X ) is a transition probability function, and x 0 is a fixed starting state. For simplicity we assume a deterministic reward function and starting state, but our results generalize. A Constrained Markov Decision Process (CMDP) ( Altman, 1999 ) is a MDP with additional con- straints that restrict the set of permissible policies for the MDP. Formally, a CMDP is a tuple (X , A, P, r, x 0 , d, d 0 ), where d : X → [0, D M AX ] is the cost function 1 and d 0 ∈ R ≥0 is the maxi- mum allowed cumulative cost. The set of feasible policies that satisfy the CMDP is the subset of stationary policies Π D := {π : X → P(A) E[ T t=0 d(x t ) | x 0 , π] ≤ d 0 }. We consider a finite time horizon T after which the episode terminates. The expected sum of rewards following a policy π from an initial state x is given by the value function V π (x) = E[ T t=0 r(x t , a t ) | π, x]. Analogously, the expected sum of costs is given by the cost value function V π D (x) = E[ T t=0 d(x t ) | π, x]. The RL problem in the CMDP is to find the feasible policy which maximizes expected returns from the initial state x 0 , i.e. An important point to note about CMDPs is that, in the original formulation, the cost function depends on immediate states but the constraint is cumulative and thus depends on the entire trajectory. In the case of MDPs, where a model of the environment is not known or is not easily obtained, it is still possible for the agent to find the optimal policy using Temporal Difference (TD) methods ( Sutton, 1988 ). Broadly, these methods update the estimates of the value functions via bootstraps of previous estimates on sampled transitions (we refer the reader to  Sutton & Barto (2018)  for more information). In the on-policy setting, we alternate between estimating the state-action value function Q π for a given π and updating the policy to be greedy with respect to the value function.

Section Title: SAFE POLICY ITERATION VIA BACKWARD VALUE FUNCTIONS
  SAFE POLICY ITERATION VIA BACKWARD VALUE FUNCTIONS Our approach proposes to convert the trajectory-level constraints of the CMDP into single-step state-wise constraints in such a way that satisfying the state-wise formulation will entail satisfying the original trajectory-level problem. The advantages of this approach are twofold: i) working with single-step state-wise constraints allows us to obtain analytical solutions to the optimization problem, and ii) the state-wise constraints can be defined via value-function-like quantities and can thus be estimated with well-studied value-based methods. The state-wise constraints are defined via Backward Value Functions, in Section 3.2, and in Section 3.3 we provide a safe policy iteration procedure which satisfies said constraints (and thus the original problem).

Section Title: BACKWARD MARKOV CHAIN
  BACKWARD MARKOV CHAIN Unlike in traditional RL, in the CMDP setting the agent needs to take into account the constraints which it has accumulated so far in order to plan accordingly for the future. Intuitively, the accumulated cost so far can be estimated via the cost value function V D running "backward in time". Before giving the details of our approach and formally introducing the Backward Value Functions, we review the main ideas, which are built upon the work of  Morimura et al. (2010) , who also considered time-reversed Markov chains but from the standpoint of estimating the gradient of the log stationary distribution; we extend these ideas to TD methods. Assumption 3.1 (Stationarity). The MDP is ergodic for any policy π, i.e., the Markov chain charac- terized by the transition probability P π (x t+1 |x t ) = at∈A P(x t+1 |x t , a t )π(a t |x t ) is irreducible and aperiodic. Let M(π) denote the Markov chain characterized by transition probability P π (x t+1 |x t ). The above assumption implies that there exists a unique stationary distribution η π associated with π, such that it satisfies: η π (x t+1 ) = xt∈X P π (x t+1 |x t )η π (x t ). We abuse the notation and denote According to Bayes' rule, the probability q(x t−1 , a t−1 |x t ) of a previous state-action pair (x t−1 , a t−1 ) leading to the current state x t is given by: The forward Markov chain, characterized by the transition matrix P π (x t+1 |x t ), runs forward in time, i.e., it gives the probability of the next state in which the agent will end up. Analogously, a backward Markov chain is denoted by the transition matrix # » P π (x t−1 |x t ) = at−1∈A # » P π (x t−1 , a t−1 |x t ), and describes the state and action the agent took to reach the current state. Definition 3.1 (Backward Markov Chain). A backward Markov chain associated with M(π) is denoted by # » B (π) and is characterized by the transition probability

Section Title: BACKWARD VALUE FUNCTION
  BACKWARD VALUE FUNCTION We define the Backward Value Function (BVF) to be a value function running on the backward Markov chain # » B (π). A BVF is the expected sum of returns or costs collected by the agent so far. We are mainly interested in maintaining estimates of the cumulative cost incurred at a state in order to express the total constraint state-wise. We note that, since every Markov chain M(π) is ergodic by Assumption 3.1, the corresponding backward Markov chain B(π) is also ergodic ( Morimura et al., 2010 , Prop. B.1). In particular, every policy π can reach the initial state via some path in the transition graph of the backward Markov chain. Thus, the backwards Markov chain are also finite-horizon for some T B , with x 0 corresponding to the terminal state. We define a finite-horizon Backward Value Function for cost as: Proposition 3.1 (Sampling). Samples from the forward Markov chain M(π) can be used directly to estimate the statistics of the backward Markov chain # » B (π) (or the Backward Value Function). We Under review as a conference paper at ICLR 2020 have: are expectations over the forward and backward chains respectively. The Equation (3) holds true even in the limit K → ∞. The proof is given in Appendix B.1. Using the above proposition, we get an interchangeability property that removes the need to sample from the backward chain. We can use the traditional RL setting and draw samples from the forward chain and still estimate the BVFs. Equation (2) can be written recursively as: In operator form, the above equation can also be written as: The proof is given in Appendix B.2. The above proposition allows us to soundly extend the RL methods based on Bellman operators for the estimation of BVFs.

Section Title: SAFE POLICY IMPROVEMENT VIA BVF-BASED CONSTRAINTS
  SAFE POLICY IMPROVEMENT VIA BVF-BASED CONSTRAINTS With the Backward Value Function framework, the trajectory-level optimization problem associated with a CMDP can be rewritten in state-wise form. Recall that a feasible policy must satisfy the constraint: Alternatively, for each timestep t ∈ [0, T ] of a trajectory: Via the identities E[ T k=t d(x k ) | x 0 , π] ≤ E xt∼δx 0 (P π ) t [V π D (x t )] and E[ t k=0 d(x k ) | x 0 , π] ≤ E x k ∼δx 0 (P π ) t [ # » V π D (x t )](derived in Appendix C) 2 , we remark that the quantity on the LHS is less than the expectation over k-step trajectories of # » V π D (x t ) + V π D (x t ) − d(x t ). In other words, for each t ∈ [0, T ] : These are the state-wise constraints that should hold at each step in a given trajectory - we refer to them as the value-based constraints. Satisfying the value-based constraints will automatically satisfy the given CMDP constraints. 2 δx 0 is a Dirac distribution at x0, and δx 0 (P π ) t is the distribution of states at time t.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 This formulation allows us to introduce a policy improvement step, which maintains a safe feasible policy at every iteration by using the previous estimates of the forward and backward value functions 3 . The policy improvement step is defined by a linear program, which performs a greedy update with respect to the current state-action value function subject to the value-based constraints: Our first result is that the policies obtained by the policy improvement step will satisfy the safety constraints. We write TV(·, ·) for the total variation metric between distributions. Theorem 3.1 (Consistent Feasibility). Assume that successive policies are updated sufficiently slowly, i.e. TV(π k+1 (·|x), π k (·|x)) ≤ d0−V π k D (x0) 2DMAXT 2 . 4 Then the policy iteration step given by (SPI) is consistently feasible, i.e. if π k is feasible at x 0 then so is π k+1 . It is also possible to consider larger neighbourhoods for updates of successive policies, but at the cost of everywhere-feasibility. For want of space, we present that result in Appendix D. Next we show that the policy iteration step given by (SPI) leads to monotonic improvement. Theorem 3.2 (Policy Improvement). Let π n and π n+1 be successive policies generated by the policy iteration step of (SPI). Then V πn+1 (x) ≥ V πn (x) ∀x ∈ X . In particular, the sequence of value functions {V πn } n≥0 given by (SPI) monotonically converges. Proofs for Theorems 3.1 and 3.2 are given in Appendix D. Finding the sub-optimality gap (if any) remains an interesting question left for future work.

Section Title: PRACTICAL IMPLEMENTATION CONSIDERATIONS
  PRACTICAL IMPLEMENTATION CONSIDERATIONS

Section Title: DISCRETE ACTION SPACE
  DISCRETE ACTION SPACE In discrete action spaces, the problem in (SPI) can be solved exactly as a Linear Programming problem. It is possible to approximate its analytical solution by casting it into the corresponding entropy-regularized counterpart ( Neu et al., 2017 ;  Chow et al., 2018 ). The details of the closed form solution can be found in Appendix E. Furthermore, if we restrict the set of policies to be deterministic, then it is possible to have an in-graph solution as well. The procedure then closely resembles the Action Elimination Procedure ( Puterman, 2014 , Chapter 6), where non-optimal actions are identified as being those which violate the constraints.

Section Title: EXTENSION TO CONTINUOUS CONTROL
  EXTENSION TO CONTINUOUS CONTROL For MDPs with only state-dependent costs,  Dalal et al. (2018)  proposed the use of safety layers, a constraint projection approach, that enables action correction at each step. At any given state, an unconstrained action is selected and is passed to the safety layer, which projects the action to the nearest action (in Euclidean norm) satisfying the necessary constraints. We extend this approach to stochastic policies to handle the corrections for the actions generated by stochastic policies. When the policy is parameterized with a Gaussian distribution, then the safety-layer can still be used by projecting both the mean and standard-deviation vector to the constraint-satisfying hyper-plane 5 . In most cases, the standard-deviation vector is kept fixed or independent of the state ( Kostrikov, 2018 ;  Dhariwal et al., 2017 ), which allows us to formulate the problem as solving the following L2-projection of the mean of the Gaussian in Euclidean space. For µ π (.; θ), at any given state x ∈ X , Under review as a conference paper at ICLR 2020 the safety layer solves the following projection problem: As shown in  Dalal et al. (2018) ;  Chow et al. (2019) , if the constraints have linear nature then an analytical solution exists. In order to get a linearized version of the constraints (and simplify the projection), we can approximate the constraint with its first-order Taylor series at µ = µ π (x): The above objective function is positive-definite and quadratic, and the constraints are linear. Though this problem can be solved by an in-graph QP solver, there exists an analytical solution (see Ap- pendix G): Proposition 4.1. At a given state x ∈ X , the solution to the Eq. (5), µ * is:

Section Title: RELATED WORK
  RELATED WORK

Section Title: Lagrangian-based methods
  Lagrangian-based methods Initially introduced in  Altman (1999) , more scalable versions of the Lagrangian based methods have been proposed over the years ( Moldovan & Abbeel, 2012 ;  Tessler et al., 2018 ;  Chow et al., 2015 ). The general form of the Lagrangian methods is to convert the problem to an unconstrained problem via Langrange multipliers. If the policy pa- rameters are denoted by θ, then Lagrangian formulation becomes: min λ≥0 max θ (L(θ, λ) = min λ≥0 max θ [V π θ (x 0 ) − λ(V π θ D (x 0 ) − d 0 ))] , where L is the Lagrangian and λ is the Lagrange multiplier (penalty coefficient). The main problems of the Lagrangian methods are that the Lagrangian multiplier is either a hyper-parameter (without much intuition), or is solved on a lower time-scale. That makes the unconstrained RL problem a three time-scale 6 problem, which makes it very difficult to optimize in practice. Another problem is that during the optimization, this procedure can violate the constraints. Ideally, we want a method that can respect the constraint throughout the training and not just at the final optimal policy.

Section Title: Lyapunov-based methods
  Lyapunov-based methods In control theory, the stability of the system under a fixed policy is computed using Lyapunov functions ( Khalil, 1996 ). A Lyapunov function is a type of scalar potential function that keeps track of the energy that a system continually dissipates. Recently,  Chow et al. (2018 ;  2019 ) provide a method of constructing the Lyapunov functions to guarantee global safety of a behavior policy using a set of local linear constraints. Their method requires the knowledge of T V (π, π * ) to guarantee the theoretical claims. They substitute the ideally required Lyapunov function with an approximate solution that requires solving a LP problem at every iteration. For the practical scalable versions, they use a heuristic, a constant Lyapunov function for all states that only depends on the initial state and the horizon. While our methods also constructs state-wise constraints, there are two notable differences: a) our assumption only rely on the current policy candidate and the baseline policy, instead of the baseline and the optimal policy, b) our method does not require solving an LP at every update step to construct the constraint and as such the only approximation error that is introduced comes from the function approximation.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Conservative Policy Improvement: Constrained Policy Optimization (CPO) ( Achiam et al., 2017 ) extends the trust-region policy optimization ( Schulman et al., 2015 ) algorithm to satisfy constraints during training as well as after convergence. CPO is computationally expensive as it uses an approximation to the Fisher Information Matrix which requires many steps of conjugate gradient descent (n cg steps) followed by a backtracking line-search procedure (n ls steps) for each iteration, so it is more expensive by O(n cg + n ls ) per update. Furthermore, accurately estimating the curvature requires a large number of samples in each batch ( Wu et al., 2017 ).

Section Title: EXPERIMENTS
  EXPERIMENTS We empirically validate our approach on RL benchmarks to measure the performance of the agent with respect to the accumulated return and cost during training in the presence of neural-networks based function approximators. We compare our approach with the respective Unconstrained versions, and the Lyapunov-based approach ( Chow et al., 2018 ;  2019 ) in each setting. Even though our formulation is based on the undiscounted case, we use discounting with γ = 0.99 for estimating the value functions in order to be consistent with the baselines.

Section Title: STOCHASTIC GRID WORLD
  STOCHASTIC GRID WORLD Motivated by the safety in navigation tasks, we first consider a stochastic 2D grid world ( Leike et al., 2017 ;  Chow et al., 2018 ). The agent (green cell in Fig. 1a) starts in the bottom-right corner, the safe region, and the objective is to move to the goal on the other side of the grid (blue cell). The agent can only move in the adjoining cells in the cardinal directions. It gets a reward of +1000 on reaching the goal, and a penalty of −1 at every timestep. Thus, the task is to reach the goal in the shortest amount of time. There are a number of pits in the terrain (red cells) that represent the safety constraint and the agent gets a cost of 10 on passing through any pit cell. Occasionally, with probability p = 0.05, a random action will be executed instead of the one selected by the agent. Thus, the task is to reach to the goal in the shortest amount of time, while passing through the red grids at most d 0 /10 times. The size of the grid is 12 × 12 cells, and the pits are randomly generated for each grid with probability ρ = 0.3. The agent starts at (12, 12) and the goal is selected uniformly on (α, 0), where α ∼ U (0, 12). The threshold d 0 = 20 implies the agent can pass at most two pits. The maximum horizon is 200 steps, after which the episode terminates. We use the action elimination procedure described in Sec 4.1 in combination with n-step SARSA ( Rummery & Niranjan, 1994 ;  Peng & Williams, 1994 ) using neural networks and multiple syn- chronous agents as in  Mnih et al. (2016) . We use -greedy exploration. The results are shown in  Fig. 1  (more experimental details can be found in Appendix H). We observe that the agent is able to respect the safety constraints more adequately than the Lyapunov-based method, albeit at the expense of some decrease in return, which is the expected trade-off for satisfying the constraints.

Section Title: MUJOCO BENCHMARKS
  MUJOCO BENCHMARKS Based on the safety experiments in  Achiam et al. (2017) ;  Chow et al. (2019) , we design three simulated robot locomotion continuous control tasks using the MuJoCo simulator ( Todorov et al., 2012 ) and OpenAI Gym ( Brockman et al., 2016 ): (1) Point-Gather: A point-mass agent (S ⊆ R 9 , A ⊆ R 2 ) is rewarded for collecting the green apples and constrained to avoid the red bombs; (2) Safe-Cheetah: A bi-pedal agent (S ⊆ R 18 , A ⊆ R 6 ) is rewarded for running at high speed, but at the same time constrained by a speed limit; (3) Point-Circle: The point-mass agent (S ⊆ R 9 , A ⊆ R 2 ) is rewarded for running along the circumference of a circle in counter-clockwise direction, but is constrained to stay within a safe region smaller than the radius of the circle. We integrate our method on top of the A2C algorithms ( Mnih et al., 2016 ) and PPO ( Schulman et al., 2017 ), using the procedure described in Section 4.2. More details about the tasks and network architecture can be found in the Appendix I. Algorithmic details can be found in Appendix J. The results with A2C are shown in  Fig. 2  and the results with PPO are shown in  Fig. 3 . We observe that 7 In practice, the starting states in the episode are unlikely to be distributed according to the stationary distribution η π . We still use the initial trajectories to update the estimates nonetheless in our experiments, but we use n-step updates. Under review as a conference paper at ICLR 2020 our Safe method is able to respect the safety constraint throughout most of the learning, and with much greater degree of compliance than the Lyapunov-based method, especially when combined with A2C. The one case where the Safe method fails to respect the constraint is in Point-Circle with PPO (Fig. 3(c)). Upon further examination, we note that the training in this scenario has one of two outcomes: some runs end with the learner in an infeasible set of states from which it cannot recover; other runs end in a good policy that respects the constraint. We discuss solutions to overcome this in the final section.

Section Title: DISCUSSION
  DISCUSSION We present a method for solving constrained MDPs that respects trajectory-level constraints by converting them into state dependent value-based constraints, and show how the method can be used to handle safety limitations in both discrete and continuous spaces. The main advantage of our approach is that the optimization problem is more easily solved with value-based constraints, while providing similar guarantees and requiring less approximations. The empirical results presented show that our approach is able to solve the tasks with good performance while maintaining safety throughout training. It is important to note that there is a fundamental trade-off between exploration and safety. It is impossible to be 100% safe without some knowledge; in cases where that knowledge is not provided a priori, it must be acquired through exploration. We see this in some of our results (Gridworld, Point-Circle) where our safe policy goes above the constraint in the very early phases of training (all our experiments started from a random policy). We note that the other methods also suffer from this shortcoming. An open question is how to provide initial conditions or a priori knowledge, to avoid this burn-in phase. Another complementary strategy to explore is for cases where an agent is stuck in an unsafe or infeasible policy space, where a recovery method (trained by purely minimizing the constraints) could be useful to help the agent recover ( Achiam et al., 2017 ;  Chow et al., 2019 ).
    Konda & Tsitsiklis, 2000 ), and adding a learning schedule over the Lagrangian makes it three time scale.

```
