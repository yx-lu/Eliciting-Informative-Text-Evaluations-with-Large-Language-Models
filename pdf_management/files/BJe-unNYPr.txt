Title:
```
Under review as a conference paper at ICLR 2020 ACCELERATED INFORMATION GRADIENT FLOW
```
Abstract:
```
We present a systematic framework for the Nesterov's accelerated gradient flows in the spaces of probabilities embedded with information metrics. Here two met- rics are considered, including both the Fisher-Rao metric and the Wasserstein-2 metric. For the Wasserstein-2 metric case, we prove the convergence properties of the accelerated gradient flows, and introduce their formulations in Gaussian families. Furthermore, we propose a practical discrete-time algorithm in parti- cle implementations with an adaptive restart technique. We formulate a novel bandwidth selection method, which learns the Wasserstein-2 gradient direction from Brownian-motion samples. Experimental results including Bayesian infer- ence show the strength of the current method compared with the state-of-the-art.
```

Figures/Tables Captions:
```
Figure 1: The effect of the BM method. Samples are plotted as blue dots. Left to right: MCMC, MED, HE and BM. All methods are run for 200 iterations with the same initialization.
Figure 2: The acceleration effect of W-AIG flow and the strength of adaptive restart (ODE level). The target density is a Gaussian distribution with zero mean on R 100 . Left: L = 1, κ ≈ 3.8 × 10 3 . Right: β = 1, κ ≈ 4.0 × 10 3 .
Figure 3: The acceleration effect of W-AIG flow and the strength of adaptive restart (particle level). The setting of target densities is identical to the ones in Figure 2.
Figure 4: Comparison of different methods on Bayesian logistic regression, averaged over 10 inde- pendent trials. The shaded areas show the variance over 10 trials. Top: BM; Bottom: MED. Left: Test accuracy; Right: Test log-likelihood.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Recently, optimization problems on the space of probability and probability models attract increas- ing attentions from machine learning communities. These problems include variational inference (Blei et al., 2017), Bayesian inference (Liu & Wang, 2016), Generative Adversary Networks (GAN, Goodfellow et al. (2014)), and policy optimizations (Zhang et al., 2018), etc. For instance, vari- ational inference methods approximate a target density by minimizing the Kullback-Leibler (KL) divergence as the loss (objective) function. Gradient descent methods with sampling efficient properties play essential roles to solve these opti- mization problems. Here the gradient descent direction often relies on the information metric over the probability space. This direction naturally reflects the change of the loss function with respect to the metric. In literature, two important information metrics, such as the Fisher-Rao metric and the Wasserstein-2 (in short, Wasserstein) metric, are of great interests (Amari, 1998; Otto, 2001; Laf- ferty, 1988). For the Fisher-Rao gradient, classical results including Adam (Kingma & Ba, 2014) and K-FAC (Martens & Grosse, 2015) demonstrate its effectiveness in probability models. For the Wasserstein gradient, many classical methods such as Markov chain Monte Carlo (MCMC) meth- ods (Geman & Geman, 1987; Neal et al., 2011; Welling & Teh, 2011) and particle-based variational inference (ParVI) methods (Liu & Wang, 2016; Chen & Zhang, 2017; Chen et al., 2018) are based on this framework in the probability space. The strength of using the Wasserstein gradient is also shown in probability models such as GANs. (Arjovsky et al., 2017; Lin et al., 2018; Li et al., 2019). The Nesterov's accelerated method (Nesterov, 1983) is widely applied in accelerating the vanilla gradient descent under the Euclidean metric. It corresponds to a damped Hamiltonian flow, known as the accelerated gradient flow (Su et al., 2016). A natural question is whether there exists a counterpart of the accelerated gradient flow in the probability space under information metrics. For optimization problems on a Riemannian manifold, the accelerated gradient methods are studied by Liu et al. (2017); Zhang & Sra (2018). The probability space embedded with information metric can be viewed as a Riemannian manifold. Several previous works explore accelerated methods in this manifold under the Wasserstein metric. Liu et al. (2018; 2019) propose an acceleration framework of ParVI methods based on manifold optimization. Taghvaei & Mehta (2019) introduce the accelerated flow from an optimal control perspective. On the other hand, Cheng et al. (2017); Ma et al. (2019) explore and analyze the acceleration on MCMC, based on the underdamped Langevin dynamics. In this paper, we present a unified framework of accelerated gradient flows in the probability space embedded with information metrics, named Accelerated Information Gradient (AIG) flows. From an information-differential-geometry perspective, we derive AIG flows by damping Hamiltonian flows, concerning both the Fisher-Rao metric and the Wasserstein metric. Then we focus on the Wasser- Under review as a conference paper at ICLR 2020 stein metric with the KL divergence loss function. In Gaussian families, we verify the existence of the solution to AIG flows. Here we show that the AIG flow corresponds to a well-posed ODE system in the space of symmetric positive definite matrices. We rigorously prove the convergence rate of AIG flows based on the geodesic convexity of the loss function. Here we note that our proof removes the unnecessary technical assumption in (Taghvaei & Mehta, 2019, Theorem 1). Besides, we handle two difficulties in numerical implementations of AIG flows. On the one hand, as pointed out by Taghvaei & Mehta (2019); Liu et al. (2019), the logarithm of density term (Wasser- stein gradient of KL divergence) is hard to approximate in particle formulations. We propose a novel kernel selection method, whose bandwidth is learned by sampling from Brownian motions. We call it the BM method. On the other hand, we notice that the AIG flow can be a numerically stiff system, especially in high-dimensional sample spaces. This is because the solution of AIG flows can be close to the boundary of the probability space. To handle this issue, we propose an adaptive restart technique, which accelerates and stabilizes the AIG algorithm. Numerical results in toy examples, Gaussian measures and Bayesian Logistic regression indicate the validity of the BM method and the acceleration effects of the proposed AIG flow. This paper is organized as follows. Section 2 briefly reviews the information metrics and their corre- sponding gradient flows and Hamiltonian flows in the probability space. In Section 3, we formulate various forms of AIG flows and analyze W-AIG flows in Gaussian measures. We theoretically prove the convergence rate of W-AIG flows in Section 4. Section 5 presents the discrete-time algorithm for W-AIG flows, including the BM method and the adaptive restart technique. Section 6 provides numerical experiments.

Section Title: METRIC AND FLOWS IN THE PROBABILITY SPACE
  METRIC AND FLOWS IN THE PROBABILITY SPACE Suppose that Ω is a region in R n . Let F(Ω) denote the set of smooth functions on Ω. ·, · and · are the Euclidean inner product and norm in R n . ∇, ∇· and ∆ represent the gradient, divergence and Laplacian operators in R n . Denote the set of probability density The tangent space at ρ ∈ P(Ω) follows T ρ (Ω) = σ ∈ F(Ω) : σdx = 0. . The cotangent space at ρ, T * ρ P(Ω), can be treated as the quotient space F(Ω)/R, which are functions in F(Ω) defined up to addition of constants. Definition 1 (Metric in the probability space) A metric tensor G(ρ) : T ρ P(Ω) → T * ρ P(Ω) is an invertible mapping from the tangent space at ρ to the cotangent space at ρ. This metric tensor defines the metric (inner product) on the tangent space T ρ P(Ω). Namely, for σ 1 , σ 2 ∈ T ρ P(Ω), we define We present two important examples of metrics on the probability space P(Ω): the Fisher-Rao metric from information geometry and the Wasserstein metric from optimal transport. Example 1 (Fisher-Rao metric) The inverse of the Fisher-Rao metric tensor is defined by The Fisher-Rao metric on the tangent space is given by where Φ i is the solution to σ i = ρ Φ i − Φ i ρdx , i = 1, 2. Example 2 (Wasserstein metric) The inverse of the Wasserstein metric tensor is defined by Under review as a conference paper at ICLR 2020 The Wasserstein metric on the tangent space is given by where Φ i is the solution to σ i = −∇ · (ρ∇Φ i ), i = 1, 2.

Section Title: GRADIENT FLOWS
  GRADIENT FLOWS In learning, many problems can be formulated as the optimization problem in the probability space, min ρ∈P(Ω) E(ρ). Here E(ρ) is a divergence or metric loss functional between ρ and a target density ρ * ∈ P(Ω). One typical example of E(ρ) is the KL divergence from ρ to ρ * , Another example is the maximum mean discrepancy (MMD, Gretton et al. (2012)), where K(x, y) is a given kernel function. The gradient flow for E(ρ) in (P(Ω), g ρ ) takes the form Here δE(ρt) δρt is the L 2 first variation w.r.t. ρ t . We formulate the gradient flow under either the Fisher-Rao metric or the Wasserstein metric. Example 3 (Fisher-Rao gradient flow) The Fisher-Rao gradient flow is given by Example 4 (Wasserstein gradient flow) The Wasserstein gradient flow writes

Section Title: HAMILTONIAN FLOW
  HAMILTONIAN FLOW In this subsection, we briefly review the Hamiltonian flow in the probability space. By using the metric g ρ in the probability space, we can define a Lagrangian by The Euler-Lagrange equation for the Lagrangian follows where Φ t is up to a spatially-constant function shrift. Here (2) is the Hamiltonian flow Under review as a conference paper at ICLR 2020 Similarly, we can write the Hamiltonian flow under the Fisher-Rao metric or the Wasserstein metric. Example 5 (Fisher-Rao Hamiltonian flow) The Fisher-Rao Hamiltonian flow follows Example 6 (Wasserstein Hamiltonian flow) The Wasserstein Hamiltonian flow writes The corresponding Hamiltonian is H W (ρ t , Φ t ) = 1 2 ∇Φ t 2 ρ t dx + E(ρ t ). This is identical to the Wasserstein Hamiltonian flow introduced by Chow et al. (2019).

Section Title: ACCELERATED INFORMATION GRADIENT FLOW
  ACCELERATED INFORMATION GRADIENT FLOW Let α t ≥ 0 be a scalar function of t. We add a damping term α t Φ t to the Hamiltonian flow (3). This renders the Accelerated Information Gradient (AIG) flow with initial values ρ 0 = ρ 0 and Φ 0 = 0. The choice of α t depends on the geodesic convexity of E(ρ), which has an equivalent definition as follows. Definition 2 For a functional E(ρ) defined on the probability space, we say that E(ρ) has a β- positive Hessian (in short, Hess(β)) w.r.t. the metric g ρ if there exists a constant β ≥ 0 such that for any ρ ∈ P(Ω) and any σ ∈ T ρ P(Ω), we have Here Hess is the Hessian operator w.r.t. g ρ . Remark 1 The Nesterov's accelerated method (Nesterov, 1983) is a first-order method to optimize f (x) in the Euclidean space. The corresponding accelerated gradient flow by Su et al. (2016) is equivalent to a damped Hamiltonian system with initial values x(0) = x 0 and p(0) = 0. The choice of α t depends on the property of f (x). If f (x) is β-strongly convex, then α t = 2 √ β; if f (x) is convex, then α t = 3/t. We apply this Hamiltonian flow interpretation to construct (AIG) in the probability space with information metrics. We give examples of AIG flows under either the Fisher-Rao metric or the Wasserstein metric. Example 7 (Fisher-Rao AIG flow) The Fisher-Rao AIG flow writes Under review as a conference paper at ICLR 2020 Example 8 (Wasserstein AIG flow) The Wasserstein AIG flow writes For the rest of this paper, we mainly focus on the Wasserstein metric. Here the AIG flow (Eulerian formulation in fluid dynamics) has a counterpart in the particle level (Lagrangian formulation). Proposition 2 Suppose that X t ∼ ρ t and V t = ∇Φ t (X t ) are the position and the velocity of a particle at time t. Then, the differential equation of the particle system corresponding to (W-AIG) writes Remark 2 The ∇ log ρ t (X t )dt term cannot be simply replaced by a Brownian motion dB t because ρ t is the marginal distribution on X t . Several previous works have studied the accelerated gradient flow of KL divergence in the probability space under the Wasserstein metric. Taghvaei & Mehta (2019) construct the accelerated gradient flow in the probability space based on Wibisono et al. (2016)'s variational formulation on the Nesterov's accelerated method. Their flows coincide with (W-AIG-P-KL) with α t = 3/t after rescaling. The underdamped Langevin dynamics in (Cheng et al., 2017; Ma et al., 2019) damps the Hamiltonian flow of the particles, which is different from (W-AIG-P) as shown in (Taghvaei & Mehta, 2019). Liu et al. (2018; 2019) give the discrete-time accelerated algorithm from the perspective of manifold optimization.

Section Title: WASSERSTEIN METRIC RESTRICTED TO GAUSSIAN
  WASSERSTEIN METRIC RESTRICTED TO GAUSSIAN In this subsection, we demonstrate that (W-AIG) has an ODE formulation in Gaussian family. De- note N 0 n to the multivariate Gaussian densities with zero means. Namely, if ρ 0 , ρ * ∈ N 0 n , then we show that (W-AIG) has a solution (ρ t , Φ t ) and ρ t ∈ N 0 n . Let P n and S n represent symmetric positive definite matrix and symmetric matrix with size n × n respectively. Each ρ ∈ N 0 n can be uniquely expressed by its covariance matrix Σ ∈ P n by ρ(x; Σ) = (2π) −n/2 √ det(Σ) exp − 1 2 x T Σ −1 x . The Wasserstein metric on P(R n ) induces a metric on N 0 n , which is a totally-geodesic submanifold in P(R n ), see (Takatsu, 2008; Modin, 2016; Malagò et al., 2018). So there exists a Wasserstein metric on P n , also known as the Bures metric. For Σ ∈ P n , the tangent space and cotangent space follow T Σ P n T * Σ P n S n . Definition 3 (Wasserstein metric in Gaussian) For Σ ∈ P n , the metric tensor G(Σ) : S n → S n is defined by G(Σ) −1 S = 2(ΣS + SΣ). The Wasserstein metric on S n is g Σ (A 1 , A 2 ) = tr(A 1 G(Σ)A 2 ) = tr(S 1 ΣS 2 ), where S i ∈ S n is the solution to A i = ΣS i + S i Σ, i = 1, 2. Proposition 3 Given an energy function E(Σ), the Wasserstein gradient flow in Gaussian writeṡ Here ∇ Σt is the standard matrix derivative. The Hamiltonian flow is a system of (Σ t , S t ), i.e., The corresponding Hamiltonian writes H(Σ t , S t ) = tr(S t Σ t S t ) + 2E(Σ t ). Therefore, by adding the damping term α t S t , we obtain the Wasserstein AIG flow in Gaussian. Under review as a conference paper at ICLR 2020 with initial values Σ 0 = Σ 0 and S 0 = 0. For now, we consider E(Σ) to be the KL divergence.

Section Title: CONVERGENCE RATE ANALYSIS ON W-AIG FLOWS
  CONVERGENCE RATE ANALYSIS ON W-AIG FLOWS In this section, we prove the convergence rate of (W-AIG). If E(ρ) only satisfies Hess(0), then the solution ρ t to (W-AIG) with α t = 3/t satisfies Here the constants C 0 , C 0 only depend on ρ 0 . Remark 3 Here Hess(β) is equivalent to the β-geodesic convexity in the probability space w.r.t. g ρ . For the Wasserstein metric, it is also known as β-displacement convexity; see (Villani, 2008, Chap 16). Consider the case where E(ρ) is the KL divergence and the target density takes the form ρ * ∝ exp(−f (x)). A sufficient condition for Hess(β) is that f (x) is β-strongly convex, see (Otto & Villani, 2000; Bakry &Émery, 1985). If E(ρ) satisfies Hess(β) for β > 0, then the classical analysis indicates that the solution to the Wasserstein gradient flow has an O(e −2βt ) convergence rate. The W-AIG flow improves the convergence rate to O(e − √ βt ), especially when β is close to 0. Here we provide a sketch in the proof of Theorem 2. Given ρ t , we can find the optimal transport plan T t from ρ t to ρ * . Let T #ρ denote the push-forward density from ρ by the mapping T . The following proposition characterizes the inverse of the exponential map in probability space with the Wasserstein metric. Proposition 4 Denote the geodesic curve γ(s) that connects ρ t and ρ * by γ(s) = (sT t + (1 − s) Id)#ρ t , s ∈ [0, 1]. Here Id is the identity mapping from R n to itself. Then,γ(0) corresponds to a tangent vector −∇ · (ρ t (x)(T t (x) − x)) ∈ T ρt P(Ω). We first consider the case where E(ρ) satisfies Hess(β) for β > 0. Motivated by the Lyapunov function for Nesterov's ODE in the Euclidean case, we construct the following Lyapunov function. Under review as a conference paper at ICLR 2020 Note that E(0) only depends on ρ 0 . This proves the first part of Theorem 2. We now consider the case where E(ρ) satisfies Hess(0). We construct the following Lyapunov function. Proposition 6 Suppose that E(ρ) satisfies Hess(0). ρ t is the solution to (W-AIG) with α t = 3/t. Then, E(t) defined in (8) satisfiesĖ(t) ≤ 0. As a result, Because E(0) only depends on ρ 0 , we complete the proof. Remark 4 For the Hess(0) case, we obtain the same result in (Taghvaei & Mehta, 2019, Theorem 1). Their proof comes from the Lagrangian formulation (W-AIG-P) and our proof is based on the Eulerian formulation (W-AIG). However, their technical assumption E X t + e −γt Y t − T ρ∞ ρt (X t ) · d dt T ρ∞ ρt (X t ) = 0 is only valid in 1-dimensional case. In Appendix C.4, we prove that this quantity is non-negative. This is due to the Hodge decomposition behind the optimal transport, see Lemma 1 in Appendix C.3.

Section Title: DISCRETE-TIME ALGORITHM FOR W-AIG FLOWS
  DISCRETE-TIME ALGORITHM FOR W-AIG FLOWS In this section, we present the discrete-time implementation of (W-AIG-P-KL). This implementation is simpler and more stable than the one in (Taghvaei & Mehta, 2019). Suppose that initial positions of a particle system {X i 0 } N i=1 are given and V i 0 = 0. The time parameter t is related to the step size √ τ via t = √ τ k. The update rule follows We review two common choices of ξ k as follows. If X i k follows a Gaussian distribution, then ξ k (x) = −Σ −1 k (x − m k ), (10) where m k and Σ k are the mean and the covariance matrix of {X i k } N i=1 . For the non-Gaussian case, we use the kernel density estimation (KDE, Singh (1977)),ρ k (x) = 1 N N i=1 K(x, X i k ) to approximate ρ t (x). Here K(x, y) is a kernel function. Then, ξ k writes A common choice of K(x, y) is a Gaussian kernel with the bandwidth h, K G (x, y; h) = (2πh) −n/2 exp x − y 2 /(2h) . There are two difficulties in the discretization. For one thing, the bandwidth h strongly affects the estimation of ∇ log ρ t , so we propose the BM method to adaptively learn the bandwidth from samples. For another, the second equation in (W-AIG) is the Hamilton- Jacobi equation, which usually has strong stiffness. We propose an adaptive restart technique to deal with this problem. Remark 5 Our numerical implementations of W-AIG flows can be viewed as a special case of ParVI methods. Compared to traditional MCMC methods, ParVI methods are more sample-efficient because make full use of a finite number of particles by taking particle interaction into account.

Section Title: LEARN THE BANDWIDTH VIA BROWNIAN MOTION
  LEARN THE BANDWIDTH VIA BROWNIAN MOTION SVGD uses a median (MED) method to choose the bandwidth, i.e., Under review as a conference paper at ICLR 2020 Liu et al. (2018) propose a Heat Equation (HE) method to adaptively adjust bandwidth. Motivated by the HE method, we introduce the Brownian motion (BM) method to adaptively learn the bandwidth. Given the bandwidth h, {X i } N i=1 and a step size s, we can compute two particle systems Y i k (h) = X i k − sξ k (x; h), Z i k = X i k + √ 2sB i , i = 1, . . . N, where B i is the standard Brownian motion. We want to minimize MMD(ρ Y ,ρ Z ), the MMD be- tween the empirical distributionρ Y (x) = N i=1 δ(x − Y i k (h)) andρ Z (x) = N i=1 δ(x − Z i k ). Here, the kernel K(x, y) for the MMD is the Gaussian kernel with a bandwidth of 1. So we optimize over h to minimize MMD(ρ Y ,ρ Z ), using the bandwidth h k−1 from the last iteration as the initialization. For simplicity we denote BM(h k−1 , {X i k } N i=1 , s) as the output of the BM method. Remark 6 Besides KDE, there are other methods that approximate the ∇ log ρ t (x) (compute ξ k ) via a kernel function, such as the blob method (Carrillo et al., 2019) and the diffusion map (Taghvaei & Mehta, 2019). The BM method can also select the kernel bandwidth for these methods.

Section Title: ADAPTIVE RESTART
  ADAPTIVE RESTART To enhance the practical performance, we introduce an adaptive restart technique, which shares the same idea of gradient restart in (Odonoghue & Candes, 2015; Wang et al., 2019) under the Euclidean case. Consider If ϕ k < 0, then we restart the algorithm with initial values X i 0 = X i k and V i 0 = 0. This essentially keeps ∂ t E(ρ t ) negative along the trajectory. The numerical results show that the adaptive restart accelerates and stabilizes the discrete-time algorithm. The overall algorithm is summarized below.

Section Title: NUMERICAL EXPERIMENTS
  NUMERICAL EXPERIMENTS In this section, we present several numerical experiments to demonstrate the validity of the BM method, the acceleration effect of the Wasserstein AIG flow, and the strength of the adaptive restart technique. Implementation details can be found in Appendix D.

Section Title: TOY EXAMPLE
  TOY EXAMPLE We first investigate the validity of the BM method in selecting the bandwidth. The target density ρ * is a toy bimodal distribution (Rezende & Mohamed, 2015). We compare two types of particle implementations of the Wasserstein gradient flow over the KL divergence:

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Here B i k ∼ N (0, 1) is the standard Brownian motion and ξ k takes the form (11). The first method is known as the MCMC method and the second method is called the ParVI method. For the second method, the bandwidth h is selected by MED/HE/BM respectively.  Figure 1  shows the distribution of 200 samples based on different methods. Samples from MCMC match the target distribution in a stochastic way; samples from MED collapse; samples from HE align tidily around the contour lines; samples from BM arrange neatly and are closer to samples from MCMC. This indicates that the BM method makes the particle system behave similar to MCMC, though in a deterministic way.

Section Title: GAUSSIAN MEASURES
  GAUSSIAN MEASURES Next, we explore the effectiveness of (W-AIG) flow compared to the Wasserstein gradient flow and demonstrate the strength of the adaptive restart. The target density ρ * is a Gaussian distribution with zero mean on R 100 , the covariance matrix of ρ * is Σ * and W * = (Σ * ) −1 . Let L and β be the largest/smallest eigenvalue of W * . E(ρ) satisfies Hess(β) and the step size is τ = 1/(4L). The condition number of W * is defined as κ = L/β. The large L indicates Σ * is close to be singular. We first demonstrate the effectiveness of (W-AIG-G) in the ODE level. Detailed discretization is left in Appendix D.2. The initial value is set to be Σ 0 = I. For now, WGF denotes the discretization of the Wasserstein gradient flow; AIG-(r)(s) denotes the discretization of the Wasserstein AIG flow. For letters in the parentheses, 'r' denotes using the adaptive restart and 's' denotes utilizing β.  Figure 2  presents the convergence of the KL divergence on two target distributions with small/large L. We observe that AIG converges faster compared to WGF, which verifies Theorem 2. The adaptive restart also accelerates the algorithm. Then, we demonstrate the results in the particle level. The setting of ρ * is same as the previous experiment. The initial distribution of samples follows N (0, I) and the number of samples is N = 600. For a particle system {X i k } N i=1 , we record the KL divergence E(Σ k ) (6) using the empirical covariance matrixΣ k . The left part of  Figure 3  (small L) is almost identical to  Figure 2 , which verifies the acceleration effect of AIG flows. It also indicates that the adaptive restart helps to accelerate the convergence. From the right part of  Figure 3  (large L), AIG and AIG-s diverge because of the ill target distribution, and the adaptive restart solves this problem.

Section Title: BAYESIAN LOGISTIC REGRESSION
  BAYESIAN LOGISTIC REGRESSION We perform the standard Bayesian logistic regression experiment on the Covertype dataset, follow- ing the same settings as Liu & Wang (2016). We compare our methods with MCMC, SVGD (Liu & Wang, 2016), WNAG (Liu et al., 2018) and WNes (Liu et al., 2019). We select the bandwidth using either the MED method or the proposed BM method.  Figure 4  indicates that the BM method accelerates and stabilizes the performance of WGF and AIG. The performance of MCMC and WGF are similar and they achieve the best log-likelihood. In test accuracy, AIG-r converges faster than other methods and is more stable. The adaptive restart improves the overall performance of AIG.

Section Title: CONCLUSION
  CONCLUSION In summary, we propose the framework of AIG flows by damping Hamiltonian flows with respect to certain information metrics in the probability space. AIG flows have been carefully studied in Gaussian families. Theoretically, we establish the convergence rate of W-AIG flows. Numerically, we propose the discrete-time algorithm and the adaptive restart technique to overcome the numerical stiffness of W-AIG flows. We introduce a novel kernel selection method by learning from Brownian- motion samples. The numerical experiments verify the acceleration effect of AIG flows and the strength of the adaptive restart. In future works, we intend to systematically explain the stiffness of AIG flows and the effect of the adaptive restart. We shall apply our results to general information metrics, especially for the Fisher-Rao metric. We expect to study the related sampling efficient optimization methods and discrete-time algorithms on general probability models.

```
