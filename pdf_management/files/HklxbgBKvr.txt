Title:
```
LOGICAL SEQUENCE DESIGN Christof Angermueller Google Research {christofa}@google.com
```
Abstract:
```
The ability to design biological structures such as DNA or proteins would have considerable medical and industrial impact. Doing so presents a challenging black-box optimization problem characterized by the large-batch, low round set- ting due to the need for labor-intensive wet lab evaluations. In response, we pro- pose using reinforcement learning (RL) based on proximal-policy optimization (PPO) for biological sequence design. RL provides a flexible framework for opti- mization generative sequence models to achieve specific criteria, such as diversity among the high-quality sequences discovered. We propose a model-based vari- ant of PPO, DyNA PPO, to improve sample efficiency, where the policy for a new round is trained offline using a simulator fit on functional measurements from prior rounds. To accommodate the growing number of observations across rounds, the simulator model is automatically selected at each round from a pool of diverse models of varying capacity. On the tasks of designing DNA transcription factor binding sites, designing antimicrobial proteins, and optimizing the energy of Ising models based on protein structure, we find that DyNA PPO performs significantly better than existing methods in settings in which modeling is feasible, while still not performing worse in situations in which a reliable model cannot be learned.
```

Figures/Tables Captions:
```
Figure 1: Comparison of methods on optimizing the energy of protein contact Ising models. Left: the cu- mulative maximum reward depending on the number of rounds for one selected protein target (1A3N). Right: the mean cumulative maximum relative to Random for alternative protein targets. Since f (x) can be well- approximated by a model trained on few examples, model-based training (DyNA PPO) results in a clear im- provement over model-free training (PPO).
Figure 2: Analysis of the performance of DyNA PPO on the Ising model. Left: Performance of DyNA PPO depending on the number of inner policy optimization rounds using the surrogate model. Using 0 rounds corresponds to PPO training. Since the surrogate model is sufficiently accurate, it is useful to perform many inner loop optimization rounds before querying f (x) again. Right: the R 2 of the surrogate model. Since it is always above the threshold for model-based training (0.5; dashed line), it is always used for training.
Figure 3: Comparison of methods on optimization transcription factor binding sites. Left: maximum cumulative reward f (x) as a function of samples. Right: fraction of local optima found. DyNA PPO optimizes f (x) faster and finds more optima than PPO and baseline methods. Results are shown for one representative transcription factor target (SIX6 REF R1).
Figure 4: Analysis of the progression of model-based training on the transcription factor task. Left: the mean R 2 model score averaged across replicas as a function of the number of training samples. The horizontal dashed line indicates the minimum threshold (0.5) for model-based training. Right: the fraction of replicates that performed model-based training based on this threshold. Shows that models tend to be inaccurate in early rounds and are therefore not used for model-based training. This explains the relatively small improvement of DyNA PPO over PPO in Figure 3.
Figure 5: Comparison of the proposed exploration bonus vs. entropy regularization on the transcription factor task. Left: performance with exploration bonus as a function of the density penalty λ (Section 2.4). Right: performance of entropy regularization as a function of the regularization strength. The top row shows that PPO finds about 80% of local optima with a relatively mild density penalty of λ = 0.1, whereas only about 45% local optima are found when using entropy regularization. The bottom row shows that varying the density penalty enables to control the sequence diversity quantified by the mean pairwise hamming distance between sequences.
Figure 6: Comparison of methods on the AMP design task. Left: Model-based training using DyNA PPO and model-free PPO clearly outperform the other methods in terms of the maximum cumulative reward. Right: The mean pairwise hamming distance between sequences proposed at each round, which is lower for DyNA PPO and PPO but does not converge to zero due to the density-based exploration bonus (
Table 1: Mean rank of methods across transcription factor binding targets. Mean rank of methods across all 41 hold-out transcription factor targets. Ranks were computed within each target using the average of metrics across optimization rounds, and then averaged across target. The higher the rank the better. 7 is the maximum rank. DyNA PPO outperforms the other methods on both optimization of f (x) and its ability to identify multiple well-separated local optima.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Driven by real-world obstacles in health and disease requiring new drugs, treatments, and assays, the goal of biological sequence design is to identify new discrete sequences x which optimize some oracle, typically an experimentally-measured functional property f (x). This is a difficult black-box optimization problem over a combinatorially large search space in which function evaluation relies on slow and expensive wet-lab experiments. The setting induces unusual constraints in black-box optimization and reinforcement learning: large synchronous batches with few rounds total. The current gold standard for biomolecular design is directed evolution, which was recently rec- ognized with a Nobel prize (Arnold, 1998) and is a form of randomized local search. Despite its impact, directed evolution is sample inefficient and relies on greedy hillclimbing to the optimal se- quences. Recent work has demonstrated that machine-learning-guided optimization (Section 3) can find better sequences faster. Reinforcement learning (RL) provides a flexible framework for black-box optimization that can har- ness modern deep generative sequence models. This paper proposes a simple method for improving the sample efficiency of policy gradient methods such as PPO (Schulman et al., 2017) for black-box optimization by using surrogate models that are trained online to approximate f (x). Our method updates the policy's parameters using sequences x generated by the current policy π θ (x), but evalu- ated using a learned surrogate f (x), instead of the true, but unknown, oracle reward function f (x). We learn the parameters of the reward model, w, simultaneously with the parameters of the policy. This is similar to other model-based RL methods, but simpler, since in the context of sequence opti- mization, the state-transition model is deterministic and known. Initially the learned reward model, f (x), is unreliable, so we rely entirely on f (x) to assess sequences and update the policy. This allows a graceful fallback to PPO when the model is not effective. Over time, the reward model becomes more reliable and can be used as a cheap surrogate, similar to Bayesian optimization meth- ods (Shahriari et al., 2015). We show empirically that cross-validation is an effective heuristic for assessing the model quality, which is simpler than the inference required by Bayesian optimization. We rigorously evaluate our method on three in-silico sequence design tasks that draw on experi- mental data to construct functions f (x) characteristic of real-world design problems: optimizing binding affinity of DNA sequences of length 8 (search space size 4 8 ); optimizing anti-microbial peptide sequences (search space size 20 50 ), and optimizing binary sequences where f (x) is defined by the energy of an Ising model for protein structure (search space size 20 50 ). These do not rely on wet lab experiments, and thus allow for large-scale benchmarking across a range of methods. We show that our DyNA PPO method achieves higher cumulative reward for a given budget (measured in terms of number of calls to f (x)) than existing methods, such as standard PPO, various forms of the cross-entropy method, Bayesian optimization, and evolutionary search. In summary, our contributions are as follows: • We provide a model-based RL algorthm, DyNA PPO, and demonstrate its effectiveness in performing sample efficient batched black-box function optimization. • We address model bias by quantifying the reliability and automatically selecting models of appropriate complexity via cross-validation. • We propose a visitation-based exploration bonus and show that it is more effective than entropy-regularization in identifying multiple local optima. • We present a new optimization task for benchmarking methods for biological sequence design based on protein energy Ising models.

Section Title: METHODS
  METHODS Let f (x) be the function that we want to optimize and x ∈ V T a sequence of length T over a vocabulary V such as DNA nucleotides (|V | = 4) or amino acids (|V | = 20). We assume N experimental rounds and that B sequences can be measured per round. Let D n = {(x, f (x))} be the data acquired in round n with |D n | = B. For simplicity, we assume that the sequence length T is constant, but our approach based on generating sequences autoregressively easily generalizes to variable-length sequences.

Section Title: MARKOV DECISION PROCESS
  MARKOV DECISION PROCESS We formulate the design of a single sequence x as a Markov decision process M = (S, A, p, r) with state space S, action space A, transition function p, and reward function r. The state space S = ∪ t=1...T V t is the set of all possible sequence prefixes and A corresponds to the vocabulary V . A sequence is generated left to right. At time step t, the state s t = a 0 , ..., a t−1 corresponds to the t last tokens and the action a t ∈ A to the next token. The transition function p(s t + 1|s t ) = s t a t is deterministic and corresponds to appending a t to s t . The reward r(s t , a t ) is zero except at the last step T , where it corresponds to the functional measurement f (s T −1 ). For generating variable-length sequences, we extend the vocabulary by a special end-of-sequence token and terminate sequence generation when this token is selected.

Section Title: Algorithm 1: DyNA PPO
  Algorithm 1: DyNA PPO 1: Input: Number of experiment rounds N 2: Input: Number of model-based training rounds M 3: Input: Set of candidate models S = {f } 4: Input: Minimum model score τ for model-based training 5: Input: Policy π θ with initial parameters θ

Section Title: POLICY OPTIMIZATION
  POLICY OPTIMIZATION We train a policy π θ (a t |s t ) to optimize the expected sum of rewards : We use proximal policy optimization (PPO) with KL trust-region constraint (Schulman et al., 2017), which we have found to be more stable and sample efficient than REINFORCE (Williams, 1992). We have also considered off-policy deep Q-learning (DQN) (Mnih et al., 2015), and categorical dis- tributional deep Q-learning (CatDQN) (Bellemare et al., 2017), which are in principle more sample- efficient than on-policy learning using PPO since they can reuse samples multiple times. However, they performed worse than PPO in our experiments (Appendix C). We implement algorithms using the TF-Agents RL library (Guadarrama et al., 2018). We employ autoregressive models with one fully-connected layer as policy and value networks since they are faster to train and outperformed recurrent networks in our experiments. At time step t, the network takes as input the W last characters a t−W , ..., a t−1 that are one-hot encoded, where the context window size W is a hyper-parameter. To provide the network with information about the current position of the context window, it also receives the time step t, which is embedded using a sinusoidal positional encoding (Vaswani et al., 2017), and concatenated with the one-hot characters. The policy network outputs a distribution π θ (a t |s t ) over next the token a t . The value network V (s t ), which approximates the expected future reward for being in state s t , is used as a baseline to reduce the variance of stochastic estimates of equation 1 (Schulman et al., 2017).

Section Title: MODEL-BASED POLICY OPTIMIZATION
  MODEL-BASED POLICY OPTIMIZATION Model-based RL learns a model of the environment that is used as a simulator to provide additional pseudo-observations. While model-free RL has been successful in domains where interaction with the environment is cheap, such as those where the environment is defined by a software program, its high sample complexity may be unrealistic for biological sequence design. In model-based RL, the MDP M = (S, A, p, r) is approximated by a model M = (S, A, p , r ) with the same state space S and action space A as M (Sutton & Barto, 2018, Ch. 8). Since the transition function p is deterministic in our case, only the reward function r(s t , a t ) needs to be approximated by r (s t , a t ). Since r(s T , a T ) is non-zero at the last step T and then corresponds to f (x) with x == s T −1 , the problem reduces to approximating f (x). This can be done by supervised regression by fitting a regressor f (x) on the data ∪ n <=n D n collected so far. We then use the resulting model to collect additional observations (x, f (x)) and update the policy in a simulation phase, instead of only using observations (x, f (x)) from the the true environment, which are expensive to collect. We call our method DyNA PPO since it is similar to the DYNA architecture (Sutton (1991); Peng et al. (2018)) and since can be used for DNA sequence design. Model-based RL provides the promise of improved sample efficiency when the model is accurate, but it can reduce performance if insufficient data are available for training a trustworthy model. In this case, the policy is prone to exploit regions where the model is inaccurate (Janner et al., 2019). To reap the benefit of model-based RL when the model is accurate and avoid reduced performance when it is not, we (i) automatically select the model from a set of candidate models of varying complexity, (ii) only use the selected model if it is accurate, and iii) stop model-based training as soon the the model uncertainty increases by a certain threshold. After each round of experiment, we fit a set of candidate models on all available data to estimate f (x) via supervised regression. We quantify the accuracy of each candidate model by the R 2 score, which we estimate by five-fold cross-validation. See Appendix G for a discussion of different data splitting strategies to select models using cross- validation. If the R 2 score of all candidate model is below a pre-specified threshold τ , we do not perform model-based training in that round. Otherwise, we build an ensemble model that includes all models with a score greater or equal than τ , and use the average prediction as reward for training the policy. We considered τ as a tunable hyper-parameter, were we found τ = 0.5 to be optimal for all problems (see Figure 14. By ignoring the model if it is inaccurate, we aim to prevent the policy from exploiting deficiencies of the model (Janner et al., 2019). We perform up to M model-based optimization rounds (see Algorithm 1) and stop as soon as the model uncertainty increased by a certain factor relative to the model uncertainty at the first round (m = 1). This is motivated by our observation that the model uncertainty is strongly correlated with the unknown model error, and prevents from training the policy with inaccurate model predictions (see Figure 12, 13) as soon as the model starts to explore regions on which the model was not trained on. For models, we consider nearest neighbor regression, Bayesian ridge regression, random forests, gradient boosting trees, Gaussian processes, and ensemble of deep neural networks. Within each model family, we additionally use cross-validation for tuning hyper-parameters, such as the number of trees, tree depth, kernels and kernel parameters, or the number of hidden layers and units (see Appendix A.7 for details). By testing and optimizing the hyper-parameters of different models automatically, the model capacity can dynamically increase as data becomes available. In Bayesian optimization, non-parametric models such as Gaussian processes are popular regres- sors, and they also automatically grow model capacity as more data arrives (Shahriari et al., 2015). However, with Bayesian optimization there is no opportunity to ignore the regressor entirely if it is unreliable. Furthermore, Bayesian optimization relies on performing (approximate) Bayesian infer- ence, which in practice is sensitive to the choice of hyper-parameter (Snoek et al., 2012). Overall, our method combines the positive attributes of both generative and discriminative ap- proaches to sequence design. Our experiments do not compare to prior work on model-based RL, since these methods primarily focus on estimating a dynamics model for state transitions.

Section Title: DIVERSITY-PROMOTING REWARD FUNCTION
  DIVERSITY-PROMOTING REWARD FUNCTION Learning policies to generate diverse sequences is important because of several reasons. In many ap- plications, f (x) is an in-vitro (taking place outside a living organism) surrogate for an in-vivo taking place inside a living organism) functional measurement that is even more expensive to evaluate than f (x). The in-vivo measurement may depend on properties that are correlated with f (x) and others that are not captured at all in-vitro, such as off-target effects or toxicity. To improve the chance that a sequence satisfying the ultimate in-vivo criteria is found, it is therefore desirable for the optimiza- tion procedure to discover a diverse set of candidate optima. Here, diversity is a downstream metric, for which training the policy π θ (x) to maximize equation 1 will not necessarily yield good perfor- mance. For example, a high-quality policy can learn to always generate the same sequence x with a high value of f (x), and will therefore result in zero diversity. An additional reason that diversity matters is that it yields a good exploration strategy, even for scenarios where optimizing equation 1 is sufficient. Finally, use of strategies that reward high-diversity policies can reduce the policies' tendency to generate exact duplicates. To increase sequence diversity, we employ a simple exploration reward bonus based on the den- sity of proposed sequences, similar to existing exploration techniques based on state visitation fre- quency (Bellemare et al., 2016). Specifically, we define the final reward as r T = f (x)−λ·dens (x), where dens (x) ∈ N + is the weighted number of sequences that have been proposed in previous rounds with a distance of less than away from x, where the weight decays linearly with the dis- tance. This reward penalizes proposing similar sequences multiple times, where the strength of the penalty is controlled by λ. As a result, the policy learns not to generate related sequences and hence explores the search space more effectively. We used the edit distance as distance metric and tuned the distance radius , where setting > 0 improved exploration on high-dimensional problems (see Figure 11). We also considered an alternative penalty based on the nearest neighbor distance of the proposed sequence to past sequences, which we found to be less effective (see Figure 9).

Section Title: RELATED WORK
  RELATED WORK Recently, machine learning approaches have been shown to be effective in optimizing real-world DNA and protein sequences (Wang et al., 2019; Chhibbar & Joshi, 2019; de Jongh et al., 2019; Liu et al., 2019; Sample et al., 2019; Wu et al., 2019). Existing methods for biological sequence design fall into three broad categories: evolutionary search, optimization using discriminative models (e.g. Bayesian optimization), and optimization using generative models (e.g. the cross entropy method). Evolutionary approaches perform direct local search in the space of sequences. They include the aforementioned directed evolution and derivatives with application-specific mutation and recombi- nation steps. Evolutionary approaches are appealing since they are simple and can easily incorporate human intuition into the design process, but generally suffer from low sample efficiency. Optimization methods based on discriminative models alternate between two steps: (i) using the data that have been collected so far to fit a regressor f (x) to approximate f (x), and (ii) using f (x) to define an acquisition function that is optimized to select the next batch of sequences. Recently, such an approach was used to optimize the binding affinity of IgG antibodies (Liu et al., 2019), where a neural network ensemble was used for f (x). In general, optimizing the acquisition func- tion is a non-trivial combinatorial optimization problem. Liu et al. (2019) employed activation maximization, where gradient-based optimization is performed on a continuous relaxation of the discrete search space. However, this requires f (x) to be differentiable and optimization of a con- tinuous relaxation is vulnerable to leaving the data manifold (cf. deep dream (Mordvintsev et al., 2015)). Bayesian optimization defines an acquisition function such as the expected improvement (Mockus et al., 2014) based on the uncertainty of f (x), which enables balancing exploration and exploita- tion (overview provided in Shahriari et al. (2015)). Gaussian processes (GPs) are commonly used for Bayesian black-box optimization since they provide calibrated uncertainty estimates. Unfortunately, GPs are hard to scale to large, high-dimensional datasets and are sensitive to the choice of hyper- parameters. In response, recent work has performed continuous black-box optimization in the latent space of a deep generative model (Gómez-Bombarelli et al., 2018). However, this approach requires a pre-trained model such as a variational autoencoder to obtain the latent embeddings. Our model- based reinforcement learning approach is similar to these approaches in that we train a reinforcement learning policy to optimize a model f (x). However, our policy is also trained directly on observa- tions of f (x) and is able to resort to model-free training by automatically identifying if the model f (x) is too inaccurate to be used as surrogate of f (x). Janner et al. (2019) investigated conditions in which an estimate of model generalization (their analysis uses validation accuracy) could justify model usage in such model-based policy optimization settings. Hashimoto et al. (2018) proposed using a cascade of classifiers, one per round, to guide sampling progressively better candidates. Optimization methods based on generative models seek to learn a distribution p θ (x) parameterized by θ that maximizes the expected value of f (x): E x∼P θ (x) [f (x)]. We note that this is the same form as variational optimization objectives, which allow the use of parameter-space evolutionary strategies (Staines & Barber, 2013; Wierstra et al., 2014; Salimans et al., 2017). Variants of the cross entropy method (De Boer et al., 2005; Brookes et al., 2019a) optimize θ, by alternating two steps: (i) sampling x ∼ p θ (x) and evaluating f(x), and (ii) updating θ to maximize this expecta- tion. Methods differ in how step (ii) is performed. For example, hillclimb-MLE (Neil et al., 2018) performs maximum-likelihood training on the top k sequences from step (i). Similarly, Feedback GAN (FBGAN) uses samples whose target function value f (x) exceeds a fixed threshold for train- ing a generative adversarial network (Gupta & Zou, 2018). Design by Adaptive Sampling (DbAs) performs weighted MLE of variational autoencoders (Kingma & Welling, 2014), where a sample's weight corresponds to the probability that f (x) is greater than a quantile cutoff under an noise Published as a conference paper at ICLR 2020 model (Brookes & Listgarten, 2018). In Brookes et al. (2019b), p θ (x) is further restricted to stay close to a prior distribution over sequences. An alternative approach for optimizing the above expectation is RL. While RL has been used for generating natural text (Bahdanau et al., 2016), small molecules (Zhou et al., 2019), and RNA se- quences that fold into a particular structure (Runge et al., 2018), we are not aware of applications of RL to optimizing DNA and protein sequences. DyNA PPO is related to existing work on model-based RL for sample efficient control (Deisenroth & Rasmussen, 2011; Kurutach et al., 2018; Peng et al., 2018; Kaiser et al., 2019; Janner et al., 2019), with the key difference that the state transition function is known and the reward function is unknown in our work, whereas most existing model-based RL approaches seek to model the state-transition function and consider the reward function as known. Prior work on sequence generation incorporates non-differentiable rewards, like BLEU in machine translation, via weighted maximum likelihood (MLE). Norouzi et al. (2016) introduce reward aug- mented MLE, while Bahdanau et al. (2016) fine tune an MLE-pretrained model using actor-critic methods. Reinforcement learning has also been applied to solving combinatorial optimization prob- lems (Bello et al., 2016; Bengio et al., 2018; Dai et al., 2017; Kool et al., 2018). In this setting sample complexity is less important because evaluating f (x) only involves a fast software program. Recent work has proposed generative models of protein structures (Sabban & Markovsky, 2019) or generative models of amino acids conditional on protein structure (Ingraham et al., 2019). Such methods are outside of the scope of this paper's experiments, since they could only be used in experimental settings where protein structures, which are expensive to measure, are available. Finally, DNA and protein design differs from small molecule design (Griffiths & Hernández-Lobato, 2017; Kusner et al., 2017; Gómez-Bombarelli et al., 2018; Jin et al., 2018; Sanchez-Lengeling & Aspuru-Guzik, 2018; Korovina et al., 2019) in the following points: (i) the number of sequences measured in parallel in the lab is typically higher (hundred or thousands vs. dozens) due to the maturity of DNA synthesis and sequencing technology, (ii) the search space is a set of sequences instead of molecular graphs, which require specialized network architectures for both discriminative and generative models, and (iii) molecules must be optimized subject to the constraint that there is a set of reactions to synthesize them, whereas practically all DNA or protein sequences are synthesiz- able.

Section Title: EXPERIMENTS
  EXPERIMENTS In the next three sections, we compare DyNA PPO to existing methods on three in-silico optimiza- tion problems that we designed in collaboration with life scientists to faithfully simulate the behavior of real wet-lab experiments, which would be cost prohibitive for a comprehensive methodological evaluation. Along the way, we present ablation experiments to help to better understand the behavior of DyNA PPO. We compare the performance of model-free policy optimization (PPO) and model-based optimiza- tion (DyNA PPO) with the following methods that we discussed in Section 3. Further details for each method can be found in Appendix A: • RegEvolution: Local search based on regularized evolution (Real et al., 2019), which has performed well on other black-box optimization tasks and can be seen as an instance of directed evolution. • DbAs: Cross-entropy optimization using variational autoencoders (Brookes & Listgarten, 2018). • FBGAN: Cross entropy optimization using generative adversarial networks (Gupta & Zou, 2018). • Bayesopt GP: Bayesian optimization using a Gaussian process regressor and activation maximization as acquisition function solver. • Bayesopt ENN Bayesian optimization using an ensemble of neural network regressors and activation maximization as acquisition function solver. Published as a conference paper at ICLR 2020 • Random: Guessing sequences uniformly at random. We quantify optimization performance by the cumulative maximum reward f (x) for sequences pro- posed up to a given round, and we use the area under the cumulative maximum reward curve to summarize one optimization trajectory as a single number. We quantify sequence diversity (Sec- tion 2.4) in terms of the mean pairwise hamming distance between the sequences proposed at each round. For problems with known optima, we also report the fraction of global optima found. We replicate experiments with 50 random seeds.

Section Title: OPTIMIZATION OF PROTEIN CONTACT ISING MODELS
  OPTIMIZATION OF PROTEIN CONTACT ISING MODELS We first consider synthetic black-box optimization problems based on the 3D structure of naturally- occurring proteins. Ising models fit on sets of evolutionary-related protein sequences have been shown to be accurate predictors for proteins' 3D structure (Shakhnovich & Gutin, 1993; Weigt et al., 2009; Marks et al., 2011; Sułkowska et al., 2012). We consider the inverse problem: given a protein, we seek to find the amino acid sequence that minimizes the energy of the Ising model parameterized by its structure. Optimizers are given a budget of 10 rounds with batch size 1000 and we consider sequences of length 50 (search space size 20 50 ). The functional form of the energy function is given in Appendix B.1. On the left of  Figure 1  we consider the optimization trajectory for a representative protein and on the right we compare the best f (x) found for each method across a range of proteins. We find that DyNA PPO considerably outperforms the other methods. We expect that this is because this synthetic Published as a conference paper at ICLR 2020 reward landscape can be well-described by a model fit using few examples, which also explains the good performance of Bayesian optimization. On the left of  Figure 2  we vary the number of inner-loop policy optimization rounds with observations from the model-based environment, where using 0 rounds corresponds to performing standard PPO. Since the surrogate model is of sufficient accuracy already at the beginning (right plot), performing more inner policy optimization rounds increases performance and enables DyNA PPO to generate high-quality sequences using very few evaluations of f (x).

Section Title: OPTIMIZATION OF TRANSCRIPTION FACTOR BINDING SITES
  OPTIMIZATION OF TRANSCRIPTION FACTOR BINDING SITES Transcription factors are protein sequences that bind to DNA sequences and regulate their activ- ity. Barrera et al. (2016) measured the binding affinity of numerous transcription factors against all possible length-8 DNA sequences (V = 4). The resulting dataset defines 158 different discrete opti- mization tasks, where the goal of each task is to find a DNA sequence of length eight that maximizes the affinity towards one of the transcription factors. It is well suited for in-silico benchmarking since (i) it is exhaustive and thereby does not require estimating missing f (x) and (ii) the distinct local optima of all tasks are known and can be used to quantify exploration (see Appendix B.2 for details). The optimization methods are given a budget of 10 rounds with a batch size of B = 100 sequences. The search space size is 4 8 . We use one task (CRX REF R1) for optimizing the hyper-parameters of all methods, and test performance on 41 heterogeneous hold-out tasks.  Figure 3  plots the performance of methods on a single representative binding target (SIX REF R1) as a function of the total number of sequences measured so far. We find that DyNA PPO and PPO outperform all other methods in terms of both the cumulative maximum f (x) found as well as the fraction of local optima discovered. We also find that the diversity of proposed sequences quantified by the fraction of global optima found is high compared to other generative approaches. This shows that our method continues to explore the search space by proposing novel sequences instead of converging to a single sequence or a handful of sequences-a desired property as discussed in Section 2.4. Across all tasks DyNA PPO and PPO rank highest compared with other methods ( Table 1 ). In  Figure 4  and 5, we analyze the effects two key design decisions of DyNA PPO: model-based train- ing and promoting exploration. We find that automated model selection automatically increases the complexity of the model, but that the models are not always accurate enough to be used for model- based training. This explains the relatively small improvement of DyNA PPO over PPO. We also find that the exploration bonus outlined in Section 2.4 is more effective than entropy regularization in finding multiple local optima and promoting sequence diversity.

Section Title: OPTIMIZATION OF ANTI-MICROBIAL PEPTIDES
  OPTIMIZATION OF ANTI-MICROBIAL PEPTIDES Next, we seek to design antimicrobial peptides (AMPs). AMPs are relatively short (8 - 75 amino acids) protein sequences (|V | = 20 amino acids), which are promising candidates against multi- resistant pathogens due to their wide range of antimicrobial activities. We use the dataset proposed by Witten & Witten (2019), which contains 6,760 unique AMP sequences and their antimicrobial ac- tivity towards multiple pathogens. We follow Witten & Witten (2019) for preprocessing the dataset and generating non-AMP sequences as negative training samples. Unlike the transcription factor binding site dataset, we do not have wet-lab experiments for every sequence in the search space. Therefore, we fit random forest classifiers to predict if a sequence is antimicrobial towards a certain pathogen in the dataset (see Section B.3), and use the predicted probability as the functional mea- surement f (x) to optimize. Given the high accuracy of the classifiers (cross-validated AUC 0.94 and 0.99), we expect that the reward landscape of f (x) is of realistic difficulty. We perform 8 rounds with a batch size 250 and restrict the sequence length to at most 50 characters (search space size 20 50 ). Figure 11).  Figure 6  compares methods on C. alibicani. We find that model-based optimization using DyNA PPO enables finding high reward sequences in early rounds, though model-free PPO slightly sur- passes the performance of DyNA PPO later on. Both DyNA PPO and PPO considerably outperform the other methods in terms of the maximum f (x) found. The density based exploration bonus prevents PPO and DyNA PPO from generating non-unique sequences (Figure 11). Stopping model- based training as soon as the model uncertainty increased by a certain factor prevents DyNA PPO from converging to a sub-optimal solution when performing many model-based optimization rounds (Figure 12,13).

Section Title: CONCLUSION
  CONCLUSION We have shown that RL is an attractive alternative to existing methods for designing DNA and protein sequences. We have proposed DyNA PPO, a model-based extension of PPO (Schulman et al., 2017) with automatic model selection that improves sample efficiency, and incorporates a reward function that promotes exploration by penalizing identical sequences. By approximating an expensive wet-lab experiment with a surrogate model, we can perform many rounds of optimization in simulation. While this work has been focused on showing the benefit of DyNA PPO for biological sequence design, we believe that the large-batch, low-round optimization setting described here may well be of general interest, and that model-based RL may be applicable in other scientific and economic domain.

```
