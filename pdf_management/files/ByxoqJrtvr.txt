Title:
```
None
```
Abstract:
```
Imitation learning algorithms provide a simple and straightforward approach for training control policies via standard supervised learning methods. By maximiz- ing the likelihood of good actions provided by an expert demonstrator, supervised imitation learning can produce effective policies without the algorithmic complex- ities and optimization challenges of reinforcement learning, at the cost of requiring an expert demonstrator - typically a person - to provide the demonstrations. In this paper, we ask: can we use imitation learning to train effective policies without any expert demonstrations? The key observation that makes this possible is that, in the multi-task setting, trajectories that are generated by a suboptimal policy can still serve as optimal examples for other tasks. In particular, in the setting where the tasks correspond to different goals, every trajectory is a successful demon- stration for the state that it actually reaches. Informed by this observation, we propose a very simple algorithm for learning behaviors without any demonstra- tions, user-provided reward functions, or complex reinforcement learning meth- ods. Our method simply maximizes the likelihood of actions the agent actually took in its own previous rollouts, conditioned on the goal being the state that it actually reached. Although related variants of this approach have been proposed previously in imitation learning settings with example demonstrations, we present the first instance of this approach as a method for learning goal-reaching policies entirely from scratch. We present a theoretical result linking self-supervised im- itation learning and reinforcement learning, and empirical results showing that it performs competitively with more complex reinforcement learning methods on a range of challenging goal reaching problems.
```

Figures/Tables Captions:
```
Figure 1: Goal conditioned supervised learning: We can learn how to reach goals by simply sam- pling trajectories, relabeling them to be optimal in hindsight and treating them as expert data, and then performing supervised learning via behavior cloning.
Figure 2: Evaluation Tasks: For each of the following tasks, we evaluate our algorithm using low- dimensional sensory data and pixel observations: (Left) 2D Navigation, (Center) robotic pushing, and (Right) Lunar Lander.
Figure 3: State-based tasks: GCSL is competitive with state-of-the-art off-policy value function RL algorithms for goal-reaching from low-dimensional sensory observations. Shaded regions denote the standard deviation across 3 random seeds (lower is better).
Figure 4: Image-based tasks: On three tasks with image observations, GSCL achieves similar performance to a state-of-the-art baseline, TD3, while being substantially simpler. Shaded regions denote the standard deviation across 3 random seeds (lower is better). Under review as a conference paper at ICLR 2020 rameter and architectural choices, are presented in Appendix A.2. Videos and further details can be found at https://sites.google.com/view/gcsl/.
Figure 5: Performance across variations of GCSL (Section 5.3) for the pushing domain. Plots for other domains in Ap- pendix A.4
Figure 6: Initializing from Demon- strations: GCSL is more amenable to initializing using expert demonstrations than value-function RL methods.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) algorithms hold the promise of providing a broadly-applicable tool for automating control, and the combination of high-capacity deep neural network models with RL extends their applicability to settings with complex observations and that require intricate policies. However, RL with function approximation, including deep RL, presents a challenging optimization problem. Despite years of research, current deep RL methods are far from a turnkey solution: most popular methods lack convergence guarantees ( Baird, 1995 ;  Tsitsiklis & Van Roy, 1997 ) or require prohibitive numbers of samples (Schulman et al., 2015;  Lillicrap et al., 2015 ). Moreover, in practice, many commonly used algorithms are extremely sensitive to hyperparameters ( Henderson et al., 2018 ). Besides the optimization challenges, another usability challenge of RL is reward function design: although RL automatically determines how to solve the task, the task itself must be specified in a form that the RL algorithm can interpret and optimize. These challenges prompt us to consider whether there might exist a general method for learning behaviors without the need for complex, deep RL algorithms. Imitation learning is an alternative paradigm to RL that provides a simple and straightforward ap- proach for training control policies via standard supervised learning methods. By maximizing the likelihood of good actions provided by an expert demonstrator, supervised imitation learning can produce effective policies without the algorithmic complexities and optimization challenges of RL. Supervised learning algorithms in deep learning have matured to the point of being robust and reli- able, and imitation learning algorithms have demonstrated success in acquiring behaviors robustly and reliably from high-dimensional sensory data such as images ( Rajeswaran et al., 2017 ;  Lynch Under review as a conference paper at ICLR 2020 et al., 2019 ). The catch is that imitation learning methods require an expert demonstrator - typically a human - to provide a number of demonstrations of optimal behavior. Obtaining expert demon- strations can be challenging; the large number of demonstrations required limits the scalability of such algorithms. In this paper, we ask: can we use ideas from imitation learning to train effective policies without any expert demonstrations, retaining the benefits of imitation learning, but making it possible to learn goal-directed behavior autonomously from scratch? The key observation for making progress on this problem is that, in the multi-task setting, trajectories that are generated by a suboptimal policy can serve as optimal examples for other tasks. In particular, in the setting where the tasks correspond to reaching different goal states, every trajectory is a successful demonstration for the state that it actually reaches. Similar observations have been made in prior works as well ( Kaelbling, 1993 ;  Andrychowicz et al., 2017 ;  Nair et al., 2018 ;  Mavrin et al., 2019 ;  Savinov et al., 2018 ), but have been used to motivate data reuse in off-policy RL or semi- parametric methods. Our approach will leverage this idea to obtain near-optimal goal-conditioned policies without RL or reward functions. The algorithm that we study is, at its core, very simple: at each iteration, we run our latest goal- conditioned policy, collect data, and then use this data to train a policy with supervised learning. Supervision is obtained by noting that each action that is taken is a good action for reaching the states that actually occurred in future time steps along the same trajectory. This algorithm resembles imitation learning, but is self-supervised. This procedure combines the benefits of goal-conditioned policies with the simplicity of supervised learning, and we theoretically show that this algorithm cor- responds to a convergent policy learning procedure. While several prior works have proposed train- ing goal-conditioned policies via imitation learning based on a superficially similar algorithm ( Ding et al., 2019 ;  Lynch et al., 2019 ), to our knowledge no prior work proposes a complete policy learning algorithm based on this idea that learns from scratch, without expert demonstrations. This proce- dure reaps the benefits of off-policy data re-use without the need for learning complex Q functions or value functions. Moreover, we can bootstrap our algorithm with a small number of expert demon- strations, such that it can continue to improve its behavior self supervised, without dealing with the challenges of combining imitation learning with off-policy RL. The main contribution of our work is a complete algorithm for learning policies from scratch via goal-conditioned imitation learning, and to show that this algorithm can successfully train goal- conditioned policies. Our theoretical analysis of self-supervised goal-conditioned imitation learning shows that this method optimizes a lower bound on the probability that the agent reaches the desired goal. Empirically, we show that our proposed algorithm is able to learn goal reaching behaviors from scratch without the need for an explicit reward function or expert demonstrations.

Section Title: RELATED WORK
  RELATED WORK Our work addresses the same problem statement as goal conditioned reinforcement learning (RL) ( Andrychowicz et al., 2017 ;  Held et al., 2018 ;  Kaelbling, 1993 ;  Nair et al., 2018 ), where we aim to learn a policy via RL that can reach different goals. Learning goal-conditioned policies is quite challenging, especially when provided only sparse rewards. This challenge can be partially mitigated by hindsight relabeling approaches that relabel goals retroactively ( Kaelbling, 1993 ;  Schaul et al., 2015 ;  Pong et al., 2018 ;  Andrychowicz et al., 2017 ). However, even with relabelling, the goal- conditioned optimization problem still uses unstable off-policy RL methods. In this work, we take a different approach and leverage ideas from supervised learning and data relabeling to build off- policy goal reaching algorithms which do not require any explicit RL. This allows GCSL to inherit the benefits of supervised learning without the pitfalls of off-policy RL. While, in theory, on-policy algorithms might be used to solve goal reaching problem as well, their inefficient use of data makes it challenging to apply these approaches to real-world settings. Our algorithm is based on ideas from imitation learning ( Billard et al., 2008 ;  Hussein et al., 2017 ) via behavioral cloning ( Pomerleau, 1989 ) but it is not an imitation learning method. While it is built on top of ideas from supervised learning, we are not trying to imitate externally provided expert demonstrations. Instead, we build an algorithm which can learn to reach goals from scratch, without explicit rewards. A related line of work ( Hester et al., 2018 ;  Brown et al., 2019 ) has explored how agents can leverage expert demonstrations to bootstrap the process of reinforcement learning. While GCSL is an algorithm to learn goal-reaching policies from scratch, it lends itself naturally Under review as a conference paper at ICLR 2020 to bootstrapping from demonstrations. As we show in Section 5.4, GCSL can easily incorporate demonstrations into off-policy learning and continue improving, avoiding many of the challenges described in  Kumar et al. (2019b) . Recent imitation learning algorithms propose methods that are closely related to GCSL.  Lynch et al. (2019)  aim to learn general goal conditioned policies from "play" data collected by a hu- man demonstrator, and  Ding et al. (2019)  perform goal-conditioned imitation learning where expert goal-directed demonstrations are relabeled for imitation learning. However, neither of these meth- ods are iterative, and both require human-provided expert demonstrations. Our method instead iteratively performs goal-conditioned behavioral cloning, starting from scratch. Our analysis shows that performing such iterated imitation learning on the policy's own sampled data actually optimizes a lower bound on the probability of successfully reaching goals, without the need for any expert demonstrations. The cross-entropy method ( Mannor et al., 2003 ), self-imitation learning ( Oh et al., 2018 ), reward- weighted regression ( Peters & Schaal, 2007 ), path-integral policy improvement (Theodorou et al., 2010), reward-augmented maximum likelihood ( Norouzi et al., 2016 ;  Nachum et al., 2016 ), and proportional cross-entropy method ( Goschin et al., 2013 ) selectively weight policies or trajectories by their performance during learning, as measured by then environment's reward function. While these may appear procedurally similar to GCSL, our method is fully self-supervised, as it does not require a reward function, and is applicable in the goal-conditioned setting. Additionally, our algorithm continues to perform well in the purely off-policy setting, where no new data is collected, a key difference from other algorithms ( Lynch et al., 2019 ;  Ding et al., 2019 ). A few works similar to ours in spirit study the problem of learning goal-conditioned policy without external supervision. Zero-shot visual imitation uses an inverse model with forward consistency to learn from novelty seeking behavior, but lacks convergence guarantees and requires learning a complex inverse model Pathak et al. (2018). Semi-parametric methods ( Savinov et al., 2018 ;  Eysenbach et al., 2019 ) learn a policy similar to ours but do so by building a connectivity graph over the visited states in order to navigate environments, which requires large memory storage and computation time that increases with the number of states.

Section Title: PRELIMINARIES
  PRELIMINARIES

Section Title: Goal Reaching
  Goal Reaching We consider the goal reaching in an environment defined by the tuple S, A, T , ρ(s 0 ), T, p(g) . S and A correspond to the state and action spaces respectively, T (s |s, a) to the transition kernel, ρ(s 0 ) to the initial state distribution, T the horizon length, and p(g) to the distribution over goal states g ∈ S. We aim to find a time-varying goal-conditioned policy π(·|s, g, h): S × S × [T ] → ∆(A), where ∆(A) is the probability simplex over the action space A and h is the remaining horizon. We will say that a policy is optimal if it maximizes the probability the specified goal is reached at the end of the episode: It is important to note here that we are not considering the notion of optimality to be finding the shortest path to the goal, but merely saying that the trajectory must reach the goal at the end of T time-steps. This problem can equivalently be cast in reinforcement learning. The modified state space S = S × S × [T ] contains the current state, goal, and the remaining horizon; the modified transition kernel T ((s , g , h ) | (s, g, h), a) = T (s | s, a) · 1(d = d , h = h − 1) appropriately handles the modified state space; and the reward function r((s, g, h)) = 1(s = g, h = 0) depends on both the goal and the time step. Because of the special structure of this formulation, off-policy RL methods can relabel an observed transition ((s, g, h), a, (s , g, h − 1)) to that of a different goal g and different horizon h like ((s, g , h ), a, (s , g , h − 1)). A common approach is to relabel trajectories with the goal they actually reached instead of the commanded goal, and often referred to as hindsight experience replay ( Andrychowicz et al., 2017 ;  Nair et al., 2018 ).

Section Title: Imitation Learning
  Imitation Learning We consider algorithms for goal-reaching that use behavior cloning, a stan- dard method for imitation learning. In behavior cloning for goal-conditioned policies, an expert policy provides demonstrations for reaching some target goals at the very last timestep, and we aim to find a policy that best predicts the expert actions from the observations. More formally, Under review as a conference paper at ICLR 2020 given a dataset Goal-conditioned imitation learning ( Lynch et al., 2019 ) investigate a similar formalism that makes an additional assumption on the quality of the expert demonstrations: that the expert is optimal not just for reaching s T , but also optimal for reaching all the states s 1 , . . . s T −1 preceding it. This corresponding policy is Note that  Lynch et al. (2019)  implement a special case of this objective where the policy is inde- pendent of the horizon. In the next section, we will discuss how repeatedly alternating between data collection and goal-conditioned imitation learning can be used to learn a goal-reaching policy. Perhaps surprisingly, this procedure optimizes the objective in Equation 1, without relying on expert demonstrations.

Section Title: LEARNING GOAL-CONDITIONED POLICIES WITH SELF-IMITATION
  LEARNING GOAL-CONDITIONED POLICIES WITH SELF-IMITATION The goal-conditioned imitation learning results in prior work show that expert demonstrations can provide supervision not only for the task the expert was aiming for, but also for reaching any state along the expert's trajectory ( Lynch et al., 2019 ;  Ding et al., 2019 ). Can we design a procedure that uses goal-conditioned behavior cloning as a subroutine, that does not need any expert demonstra- tions, but that nonetheless optimizes a well-defined reward function? In this work, we show how the idea of imitation learning with data relabeling can be re-purposed to construct a learning algorithm that is able to learn how to reach goals from scratch without any expert demonstrations. We shed light on the reasons that imitation learning with data relabeling is so powerful, and how building an iterative algorithm out of this procedure gives rise to a method that optimizes a lower bound on a reinforcement learning objective, while providing a number of benefits over standard RL algorithms. It is important to note here that we are not proposing an imitation learning algorithm, but an algorithm for learning how to reach goals from scratch without any expert demonstrations. We are simply leveraging ideas from imitation learning to build such a goal reaching algorithm.

Section Title: GOAL REACHING VIA ITERATED IMITATION LEARNING
  GOAL REACHING VIA ITERATED IMITATION LEARNING First, consider goal conditioned imitation learning via behavioral cloning with demonstrations (Equation 2). This scheme works well given expert data D, but expert data is unavailable when we are learning to reach goals from scratch. Can we use goal conditioned behavior cloning to learn how to reach goals from scratch, without the need for any expert demonstrations?

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 To leverage behavior cloning when learning from scratch, we use the following insight: while an arbitrary trajectory from a sub-optimal policy may be suboptimal for reaching the intended goal, it may be optimal for reaching some other goal. In the goal-reaching formalism defined in Equation 1, recall a policy is optimal if it maximizes the probability that the goal is reached at the last timestep of an episode. This notion of optimality doesn't have to take the direct or shortest path to the goal, it simply has to eventually reach the goal. Under this notion of optimality, we can use a simple data relabeling scheme to construct an expert dataset from an arbitrary set of trajectories. Consider a trajectory τ = {s 1 , a 1 , s 2 , a 2 , . . . , s T , a T } obtained by commanding the policy π θ (a|s, g, h) to reach some goal g. Although the actions may be suboptimal for reaching the commanded goal g, they do succeed at reaching the states s t+1 , s t+2 , . . . that occur later in the observed trajectory. More precisely, for any timestep t and horizon h, the action a t in state s t is likely to be a good action for reaching s t+h in h timesteps, and thus useful supervision for π θ (·|s t , s t+h , h). This step of autonomous relabeling allows us to convert suboptimal trajectories into optimal goal reaching trajectories for different goals, without the need for any human intervention. To obtain a concrete algorithm, we can relabel all such timesteps and horizons in a trajectoryto create an expert dataset according to D τ = {(s t , a t , s t+h , h) : t, h > 0, t + h ≤ T }. Because the relabelling procedure is valid for any horizon h ≤ T , we can relabel every such combination to create T 2 optimal tuples of (s, a, g, h) from a single trajectory. This relabeled dataset can then be used to perform goal-conditioned behavioral cloning to update the policy π θ . While performing one iteration of goal conditioned behavioral cloning on the relabeled dataset is not immediately sufficient to reach all desired goals, we will show that this procedure does in fact optimize a lower bound on a well-defined reinforcement learning objective. As described concretely in Algorithm 1, the algorithm proceeds as follows: (1) Sample a goal from a target goal distribution p(g). (2) Execute the current policy π(a|s, g, h) for T steps in the environment to collect a potentially suboptimal trajectory τ . (3) Relabel the trajectory according to the previous paragraph to add T 2 new expert tuples (s t , a t , s t+h , h) to the training dataset. (4) Perform supervised learning on the entire dataset to update the policy π(a|s, g, h) via maximum likelihood. We term this iterative procedure of sampling trajectories, relabelling them, and training a policy until convergence goal- conditioned supervised learning (GCSL). The GCSL algorithm (described above) provides us with an algorithm that can learn to reach goals from the target distribution p(g) simply using iterated behavioral cloning. The resultant goal reach- ing algorithm is off-policy, uses low variance gradients, and is simple to implement and tune without the need for any explicit reward function engineering or demonstrations. Additionally, since this algorithm is off-policy and does not require a value function estimator, it is substantially easier to bootstrap from demonstrations when real demonstrations are available, as our experiments will show in Section 5.4.

Section Title: THEORETICAL JUSTIFICATION
  THEORETICAL JUSTIFICATION While the GCSL algorithm is simple to implement, does this algorithm actually solve a well-defined policy learning problem? In this section, we argue that GCSL maximizes a lower bound on the probability for a policy to reach commanded goals. We start by writing the probability that policy π conditioned on goal g produces trajectory τ as π(τ |g) = p(s 0 ) T h=0 π(a t |s t , g, h)T (s t+1 |s t , a t ). We define G(τ ) = s T as final state of a trajec- Under review as a conference paper at ICLR 2020 tory. Recalling Equation 1, the target goal-reaching objective we wish to maximize is the probability of reaching a commanded goal: In the language of reinforcement learning, we are optimizing a multi-task problem where the reward in each task is an indicator that a goal was reached. The distribution over tasks (goals) of interest is assumed to be pre-specified as p(g). In the on-policy setting, GCSL performs imitation learning on trajectories commanded by the goals that were reached by the current policy, an objective that can be written as Here, π old corresponds to a copy of π through which gradients do not propagate, following the notation of  Schulman et al. (2015) . Our main result, which is derived in the on-policy data collection setting, shows that optimizing J GCSL (π) optimizes a lower bound on the desired objective, J(π) (proof in Appendix B): Theorem 4.1. Let J GCSL and J be as defined above. Then, J(π) ≥ J GCSL (π) + C, Where C is a constant that does not depend on π. Note that to prevent J(π) and J GCSL (π) from being zero, the probability of reaching a goal under π must be nonzero - in scenarios where such a condition does not hold, the bound remains true, albeit vacuous. The tightness of this bound can be controlled by the effective error in the GCSL objective - we present this, alongside a technical analysis of the bound in Appendix B.1. This indicates that in the regime with expressive policies where the loss function can be minimized well, GCSL will improve the expected reward.

Section Title: EXPERIMENTS
  EXPERIMENTS In our experimental evaluation, we aim to answer the following questions: 1. Does GCSL effectively learn goal-conditioned policies from scratch? 2. Does the performance of GCSL improve over successive iterations? 3. Can GCSL learn goal-conditioned policies from high-dimensional image observations? 4. Can GCSL incorporate demonstration data more effectively than standard RL algorithms?

Section Title: EXPERIMENTAL DETAILS
  EXPERIMENTAL DETAILS We consider a number of simulated control environments: 2-D room navigation, object pushing with a robotic arm, and the classic Lunar Lander game, shown in  Figure 2 . The tasks allow us to study the performance of our method under a variety of system dynamics, both low-dimensional state inputs and high-dimensional image observations, and in settings with both easy and difficult exploration. For each task, the target goal distribution corresponds to a uniform distribution over all reachable configurations. Performance of a method is quantified by the distance of the agent to the goal at the last timestep (not by the number of time-steps to the goal as is sometimes considered in goal reaching). We present full details about the environments, evaluation protocol, and hyperparameter choices in Appendix A. For the practical implementation of GCSL, we parametrize the policy as a neural network that takes in state, goal, and horizon as input and outputs a parametrized action distribution. We find that omitting the horizon from the input to the policy still provides good results, despite the formulation suggesting that the optimal policy is most likely non-Markovian. We speculate that this is due to optimal actions changing only mildly with different horizons in our tasks. Full details about the implementation for GCSL are presented in Appendix A.1.

Section Title: LEARNING GOAL-CONDITIONED POLICIES
  LEARNING GOAL-CONDITIONED POLICIES We evaluate the effectiveness of GCSL for reaching goals on the domains visualized in  Figure 2 , both from low-dimensional proprioception and from images. To better understand the performance of our algorithm, we compare to two families of reinforcement learning algorithms for solving goal- conditioned tasks. First, we consider off-policy temporal-difference RL algorithms, particular TD3- HER ( Eysenbach et al., 2019 ;  Held et al., 2018 ), which uses hindsight experience replay to more efficiently learn goal-conditioned value functions. TD3-HER requires significantly more machinery than our simple procedure: it maintains a policy, a value function, a target policy, and a target value function, all which are required to prevent degradation of the learning procedure. We also com- pare with on-policy reinforcement learning algorithms such as TRPO (Schulman et al., 2015) that cannot leverage data relabeling, but often provide more stable optimization than off-policy meth- ods. Because these methods cannot relabel data, we provide an epsilon-ball reward corresponding to reaching the goal. Details for the training procedure for these comparisons, along with hyperpa- We first investigate the learning performance of these algorithms from low-dimensional sensor ob- servations, as shown in  Figure 3 . We find that on the pushing and lunar lander domains, GCSL is able to reach a larger set of goals consistently than either RL algorithm. Although TD3 is able to fully solve the navigation task, on the other domains which require synthesis of more challenging control behavior, the algorithm makes slow, if any, learning progress. Given a limited amount of data, TRPO performs poorly as it cannot relabel or reuse data, and so cannot match the performance of the other two algorithms. When scaling these algorithms to image-based domains, which we evaluate in  Figure 4 , we find that GCSL is still able to learn goal-reaching behaviors on several of these tasks. albeit slower than from state. For most tasks, from both state and images, GCSL is able to reach within 80% of the desired goals and learn at a rate comparable to or better than previously proposed off-policy RL methods. This evaluation demonstrates that simple iterative self-imitation is a competitive scheme for reaching goals in challenging environments which scales favorably with dimensionality of state and complexity of behaviors.

Section Title: ANALYSIS OF LEARNING PROGRESS AND LEARNED BEHAVIORS
  ANALYSIS OF LEARNING PROGRESS AND LEARNED BEHAVIORS To better understand the learning behaviors of the algorithm, we investigate how GCSL performs as we vary the quality and quantity of data, the policy class we optimize over, and the relabelling technique ( Figure 5 ). Full details for these scenarios can be found in Appendix A.4. First, we consider how varying the policy class can af- fect the performance of GCSL. In Section 5.1, we hy- pothesized that optimizing over a Markovian policy class would be performant over maintaining a non-Markovian policy. We find that allowing policies to be time-varying ("Time-Varying Policy" in  Figure 5 ) can drastically speed up training on small domains, as these non-Markovian optimal policies can be fit more closely. However, on domains with active exploration challenges such as the Pusher, exploration using time-varying policies is inef- fective, and degrades performance. We investigate how the quality of the data in the dataset used to train the policy affects the learned policy. We consider two variations of GCSL: one which collects data using a fixed policy ("Fixed Data Collection" in  Figure 5 ) and another which limits the size of the dataset to be small, forcing all the data to be on-policy ("On-Policy" in  Figure 5 ). When collecting data using a fixed policy, the learning progress of the algorithm demonstratedly decreases, which indicates that the iterative loop of collecting data and training the policy is crucial for converging to a performant solution. By forcing the data to be all on-policy, the algorithm cannot utilize the full set of expe- riences seen thus far and must discard data. Although this on-policy process remains effective on simple domains, the technique leads to slower learning progress on tasks requiring more challenging control.

Section Title: INITIALIZING WITH DEMONSTRATIONS
  INITIALIZING WITH DEMONSTRATIONS Because GCSL can perform self-imitation from arbitrary data sources, the algorithm is amenable to initialization from prior exploration or from demonstrations. In this section, we study how GCSL performs when incorporating expert demonstrations as initializations. Our results comparing GCSL and TD3 in this setting corroborate the existing hypothesis that off-policy value function RL algo- rithms are challenging to integrate with initialization from demonstrations  Kumar et al. (2019a) . We consider the setting where an expert provides a set of demonstration trajectories, each for reach- ing a different goal. GCSL requires no modifications to incorporate these demonstrations - it simply adds the expert data to the initial dataset, and begins the training procedure. In contrast, multiple prior works have proposed additional algorithmic changes to off-policy TD-based methods to in- Under review as a conference paper at ICLR 2020 corporate data from expert demonstrations ( Kumar et al., 2019a ). We compare the performance of GCSL to one such variant of TD3-HER in incorporating expert demonstrations on the robotic push- ing environment in  Figure 6  (Details in Appendix A.5). Although TD3 achieves better performance with the demonstrations than when learning from scratch, it drops in performance at the beginning of training, which means TD3 regresses from the initial behavior-cloned policy, an undesirable char- acteristic for initializing from demonstrations. In contrast, GCSL scales favorably, learns faster than from scratch, and effectively incorporates the expert demonstrations. We believe this benefit largely comes from not needing to train an explicit critic, which can be unstable when trained using highly off-policy data such as demonstrations ( Kumar et al., 2019b ).

Section Title: DISCUSSION AND FUTURE WORK
  DISCUSSION AND FUTURE WORK In this work, we proposed GCSL, a simple algorithm for learning goal-conditioned policies that uses imitation learning, while still learning autonomously from scratch. This method is exceptionally simple, relying entirely on supervised learning to learn policies by relabeling its own previously collected data. This method can easily utilize off-policy data, seamlessly incorporate expert demonstra- tions when they are available, and can learn directly from image observations. Although several prior works have explored similar algorithm designs in an imitation learn- ing setting ( Ding et al., 2019 ;  Lynch et al., 2019 ), to our knowledge our work is the first to derive a complete it- erated algorithm based on this principle for learning from scratch, and the first to theoretically show that this method optimizes a lower bound on a well-defined reinforcement learning objective. While our proposed method is simple, scalable, and readily applicable, it does have a number of limitations. The current instantiation of this approach provides limited facilities for effective explo- ration, relying entirely on random noise during the rollouts to explore. More sophisticated explo- ration methods, such as exploration bonuses ( Mohamed & Rezende, 2015 ; Storck et al., 1995), are difficult to apply to our method, since there is no explicit reward function that is used during learning. However, a promising direction for future work would be to reweight the sampled rollouts based on novelty to effectively incorporate a novelty-seeking exploration procedure. A further direction for future work is to study whether the simplicity and scalability of our method can make it possible to perform goal-conditioned reinforcement learning on substantially larger and more varied datasets. This can in principle enable wider generalization, and realize a central goal in goal-conditioned reinforcement learning - universal policies that can succeed at a wide range of tasks in diverse environments.

```
