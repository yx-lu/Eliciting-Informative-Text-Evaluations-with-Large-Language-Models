Title:
```
Published as a conference paper at ICLR 2020 OPTION DISCOVERY USING DEEP SKILL CHAINING
```
Abstract:
```
Autonomously discovering temporally extended actions, or skills, is a longstand- ing goal of hierarchical reinforcement learning. We propose a new algorithm that combines skill chaining with deep neural networks to autonomously discover skills in high-dimensional, continuous domains. The resulting algorithm, deep skill chaining, constructs skills with the property that executing one enables the agent to execute another. We demonstrate that deep skill chaining significantly outperforms both non-hierarchical agents and other state-of-the-art skill discov- ery techniques in challenging continuous control tasks. 1 2 1 Video of learned policies: https://youtu.be/MGvvPmm6JQg 2 Code: https://github.com/deep-skill-chaining/deep-skill-chaining
```

Figures/Tables Captions:
```
Figure 1: (a) Learning curves comparing deep skill chaining (DSC), a flat agent (DDPG) and Option- Critic. (b) Comparison with Hierarchical Actor Critic (HAC). (c) the continuous control tasks cor- responding to the learning curves in (a) and (b). Solid lines represent median reward per episode, with error bands denoting one standard deviation. Our algorithm remains the same between (a) and (b). All curves are averaged over 20 runs, except for Ant Maze which was averaged over 5 runs.
Figure 2: Initiation sets of options learned in the Lock and Key task. Blue sphere in top-right room represents the key, red sphere in top-left room represents the lock. Red regions represent states inside the initiation classifier of learned skills, whereas blue/gray regions represent states outside of it. Each column represents an option - the top row corresponding to the initiation set when has key is false and the bottom row corresponding to the initiation set when has key is true.
Figure 3: Solution trajectories found by deep skill chaining. Sub-figure (d) shows two trajectories corresponding to the two possible initial locations in this task. Black points denote states in which π O chose primitive actions, other colors denote temporally extended option executions.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Hierarchical reinforcement learning ( Barto & Mahadevan, 2003 ) is a promising approach for solving long-horizon sequential decision making problems. Hierarchical methods lower the decision mak- ing burden on the agent through the use of problem specific action abstractions ( Konidaris, 2019 ). While the use of temporally extended actions, or options ( Sutton et al., 1999 ), has been shown to accelerate learning ( McGovern & Sutton, 1998 ), there remains the question of skill discovery: how can agents autonomously construct useful skills via interaction with the environment? While a large body of work has sought to answer this question in small discrete domains, skill discovery in high-dimensional continuous spaces remains an open problem. An early approach to skill discovery in continuous-state environments was skill chaining ( Konidaris & Barto, 2009b ), where an agent constructs a sequence of options that target a salient event in the MDP (for example, the goal state). The skills are constructed so that successful execution of each option in the chain allows the agent to execute another option, which brings it closer still to its eventual goal. While skill chaining was capable of discovering skills in continuous state spaces, it could only be applied to relatively low-dimensional state-spaces with discrete actions. We introduce a new algorithm that combines the core insights of skill chaining with recent advances in using non-linear function approximation in reinforcement learning. The new algorithm, deep skill chaining, scales to high-dimensional problems with continuous state and action spaces. Through a series of experiments on five challenging domains in the MuJoCo physics simulator ( Todorov et al., 2012 ), we show that deep skill chaining can solve tasks that otherwise cannot be solved by non- hierarchical agents in a reasonable amount of time. Furthermore, the new algorithm outperforms state-of-the-art deep skill discovery algorithms ( Bacon et al., 2017 ;  Levy et al., 2019 ) in these tasks.

Section Title: BACKGROUND AND RELATED WORK
  BACKGROUND AND RELATED WORK Sequential decision making problems can be formalized as Markov Decision Processes (MDPs). We consider goal-oriented episodic MDPs, where S denotes the state space, A is the action space, R is the reward function, T is the transition function, γ is the discount factor and g ∈ S is the terminating goal state ( Sutton & Barto, 2018 ). Unlike goal-conditioned algorithms ( Sutton et al., 2011 ;  Schaul et al., 2015 ), we do not require that g be known; instead we assume access to an indicator function 1 g : s ∈ S − → {0, 1} which the agent can query to determine if it has reached the MDP's goal.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 One way to learn a policy in an MDP is to first learn an action-value function. The action-value func- tion Q π (s t , a t ) is defined as the expected sum of discounted future rewards if the agent takes action a t from s t and then follows policy π thereafter: Q π (s t , a t ) = E π [r t + γ max at+1 Q π (s t+1 , a t+1 )]. Q-learning ( Watkins & Dayan, 1992 ) is a commonly used off-policy algorithm that uses the action- value function for control through a greedy policy π(s t ) = arg max at Q(s t , a t ). Inspired by recent success in scaling Q-learning to high-dimensional spaces ( Mnih et al., 2015 ;  Van Hasselt et al., 2016 ;  Lillicrap et al., 2015 ;  Tesauro, 1994 ), we learn the action-value function Q π φ (s t , a t ) using non-linear function approximators parameterized by φ, by minimizing the loss L(φ) = E π [(Q φ (s t , a t ) − y t ) 2 ] where the Q-learning target y t is given by the following equation ( Van Hasselt et al., 2016 ): Deep Q-Learning (DQN) ( Mnih et al., 2015 ) casts minimizing L(φ) as a standard regression problem by using target networks (parameterized by φ ) and experience replay ( Lin, 1993 ).

Section Title: THE OPTIONS FRAMEWORK
  THE OPTIONS FRAMEWORK The options framework ( Sutton et al., 1999 ) models skills as options. An option o consists of three components: (a) its initiation condition, I o (s), which determines whether o can be executed in state s, (b) its termination condition, β o (s), which determines whether option execution must terminate in state s and (c) its closed-loop control policy, π o (s), which maps state s to a low level action a ∈ A. Augmenting the set of available actions with options results in a Semi-Markov Decision Process (SMDP) ( Sutton et al., 1999 ) where the next state depends on the current state, action and time.

Section Title: SKILL DISCOVERY ALGORITHMS
  SKILL DISCOVERY ALGORITHMS Skill discovery has been studied extensively in small discrete domains ( McGovern & Sutton, 1998 ; Ş imşek &  Barto, 2004 ;  Ş imşek et al., 2005 ;  Bakker & Schmidhuber, 2004 ;  Schmidhuber, 1991 ;  Pickett & Barto, 2002 ;  Dietterich, 2000 ). Recently however, there has been a significant body of work aimed at discovering skills in continuous spaces.

Section Title: Option-critic methods
  Option-critic methods Option-Critic ( Bacon et al., 2017 ) uses an end-to-end gradient based algo- rithm to learn options in high-dimensional continuous spaces. Option-Critic was a substantial step forward in skill discovery and led to a family of related methods ( Klissarov et al., 2017 ;  Tiwari & Thomas, 2019 ;  Riemer et al., 2018 ;  Liu et al., 2017 ;  Jain et al., 2018 ). Proximal Policy Option Critic (PPOC) ( Klissarov et al., 2017 ) extends Option-Critic to continuous action spaces and is the version of Option-Critic that we compare against in this paper. Our method bypasses two fundamental short- comings of the Option-Critic framework: (a) unlike Option-Critic, we explicitly learn initiation sets of options and thus do not assume that all options are executable from everywhere, and (b) we do not treat the number of skills required to solve a task as a fixed and costly hyperparameter. Instead, our algorithm flexibly discovers as many skills as it needs to solve the given problem.

Section Title: Feudal methods
  Feudal methods An alternative to the options framework is Feudal RL ( Dayan & Hinton, 1993 ), which creates a hierarchy in which managers learn to assign subgoals to workers; workers take a subgoal state as input and learn to reach it. Feudal Networks (FuN) ( Vezhnevets et al., 2017 ) used neural networks to scale the Feudal-RL framework to high-dimensional continuous spaces; it was extended and outperformed by HIRO ( Nachum et al., 2018 ) in a series of control tasks in the MuJoCo simulator. More recently, Hierarchical Actor-Critic (HAC) ( Levy et al., 2019 ) outperformed HIRO in a similar suite of continuous control problems. While HIRO relies on having a dense "distance- to-goal" based reward function to train both levels of their feudal hierarchy, HAC's use of Hindsight Experience Replay (HER) ( Andrychowicz et al., 2017 ) allows it to work in the more general sparse- reward setting. Given its strong performance in continuous control problems and its ability to learn effectively in sparse-reward settings, we compare against HAC as a representative feudal method.

Section Title: Learning backward from the goal
  Learning backward from the goal The idea of sequencing locally applicable controllers is well es- tablished in robotics and control theory in the form of pre-image backchaining ( Kaelbling & Lozano- Pérez, 2017 ) and LQR-Trees ( Tedrake, 2009 ). Such methods either require individually engineered control loops or a model of the system dynamics. Our work fits in the model-free RL setting and Published as a conference paper at ICLR 2020 thus requires neither. More recently, reverse curriculum learning ( Florensa et al., 2017 ) also learns backward from the goal. However, they define a curriculum of start states to learn a single policy, rather than learning skills. Relay Networks ( Kumar et al., 2018 ) segment the value function back- ward from the goal using a thresholding scheme, which makes their method reliant on the accurate estimation of the value function. By contrast, our algorithm is agnostic to errors in value estimation, which are unavoidable when using function approximation in high-dimensional spaces.

Section Title: Planning with learned skills
  Planning with learned skills Options have been shown to empirically speed up planning in several domains ( Silver & Ciosek, 2012 ;  Jinnai et al., 2019 ;  James et al., 2018 ;  Francis & Ram, 1993 ;  Konidaris, 2016 ;  Sharma et al., 2019 ). However,  Konidaris et al. (2018)  show that for resulting plans to be provably feasible, skills must be executable sequentially. While they assume that such skills are given, we show that they can be autonomously discovered in high-dimensional spaces.

Section Title: DEEP SKILL CHAINING
  DEEP SKILL CHAINING Deep skill chaining (DSC) is based on the intuition that it is easier to solve a long-horizon task from states in the local neighborhood of the goal. This intuition informs the first step of the algorithm: create an option that initiates near the goal and reliably takes the agent to the goal. Once such an option is learned, we create another option whose goal is to take the agent to a state from which it can successfully execute the first option. Skills are chained backward in this fashion until the start state of the MDP lies inside the initiation set of some option. The inductive bias of creating sequentially executable skills guarantees that as long as the agent successfully executes each skill in its chain, it will solve the original task. More formally, skill chaining amounts to learning options such that the termination condition β oi (s t ) of an option o i is the initiation condition I oi−1 (s t ) of the option that precedes it in its chain. Our algorithm proceeds as follows: at time t, the policy over options π O : s t ∈ S − → o ∈ O determines which option to execute (Section 3.2). Control is then handed over to the selected option o i 's internal policy π oi : s ∈ S − → a t ∈ R |A| . π oi outputs joint torques until it either reaches its goal (β oi := I oi−1 ) or times out at its predetermined budget T (Section 3.1). At this point, π O chooses another option to execute. If at any point the agent reaches the goal state of the MDP or the initiation condition of a previously learned option, it creates a new option to target such a salient event. The machinery for learning the initiation condition of this new option is described in Section 3.3. We now detail the components of our architecture and how they are learned. Readers may also refer to Figures 4 & 7 and the pseudo-code in Appendix A.5 to gain greater intuition about our algorithm.

Section Title: INTRA-OPTION POLICY
  INTRA-OPTION POLICY Each option o maintains its own policy π o : s − → a t ∈ R |A| , which is parameterized by its own neural networks θ o . To train π o (s; θ o ), we must define o's internal reward function. In sparse reward problems, o is given a subgoal reward when it triggers β o ; otherwise it is given a step penalty. In the dense reward setting, we can compute the distance to the parent option's initiation set classifier and use that to define o's internal reward function. We can now treat learning the intra-option policy (π o ) as a standard RL problem and use an off-the-shelf algorithm to learn this policy. Since in this work we solve tasks with continuous action spaces, we use Deep Deterministic Policy Gradient (DDPG) ( Lillicrap et al., 2015 ) to learn option policies over real-valued actions.

Section Title: POLICY OVER OPTIONS
  POLICY OVER OPTIONS Initially, the policy over options (π O ) only possesses one option that operates over a single time step (T = 1). We call this option the global option (o G ) since its initiation condition is true everywhere in the state space and its termination condition is true only at the goal state of the MDP (i.e, I o G (s) = 1∀s and β o G = 1 g ). Using o G , π O can select primitive actions. At first the agent continually calls upon o G , which uses its internal option policy π o G to output exactly one primitive action. Once o G triggers the MDP's goal state N times, DSC creates its first temporally extended option, the goal option (o g ), whose termination condition is also set to be the goal state of the MDP, i.e, β og = 1 g . As the agent discovers new skills, it adds them to its option repertoire and relies on π O to deter- mine which option (including o G ) it must execute at each state. Unlike o G , learned options will be Published as a conference paper at ICLR 2020 temporally extended, i.e, they will operate over T > 1 time steps. If in state s t the agent chooses to execute option o i , then o i will execute its own closed-loop control policy (for τ steps) until its termination condition is met (τ < T ) or it has timed out at τ = T time steps. At this point, control is handed back to π O , which must now choose a new option at state s t+τ .

Section Title: Option selection
  Option selection To select an option in state s t , π O first constructs a set of admissible options given by Equation 2. π O then chooses the admissible option that maximizes its option-value function, as shown in Equation 3. Since the agent must choose from a discrete set of options at any time, we learn its option-value function using Deep Q-learning (DQN) ( Mnih et al., 2015 ). Learning the option-value function: Given an SMDP transition (s t , o t , r t:t+τ , s t+τ ), we update the value of taking option o t in state s t according to SMDP Q-learning update ( Bradtke & Duff, 1995 ). Since the agent learns Q-values for different state-option pairs, it may choose to ignore learned options in favor of primitive actions in certain parts of the state-space (in the interest of maximizing its expected future sum of discounted rewards). The Q-value target for learning the weights φ of the DQN is given by: Adding new options to the policy over options: Equations 2, 3 and 4 show how we can learn the option-value function and use it for selecting options. However, we must still incrementally add new skills to the network during the agent's lifetime. After the agent has learned a new option o's initiation set classifier I o (we will discuss how this happens in Section 3.3), it performs the following steps before it can add o to its option repertoire: • To initialize o's internal policy π o , the parameters of its DDPG (θ o ) are set to the parameters of the global agent's DDPG (θ o G ). Subsequently, their neural networks are trained independently. This provides a good starting point for optimizing π o , while allowing it to learn sub-problem specific abstractions. • To begin predicting Q-values for o, we add a new output node to final layer of the DQN parame- terizing π O . • We must assign appropriate initial values to Q φ (s, o). We follow  Konidaris & Barto (2009b)  and collect all the transitions that triggered β o and use the max over these Q-values to optimistically initialize the new output node of our DQN. 3 This is done by setting the bias of this new node, which ensures that the Q-value predictions corresponding to the other options remain unchanged.

Section Title: INITIATION SET CLASSIFIER
  INITIATION SET CLASSIFIER Central to the idea of learning skills is the ability to learn the set of states from which they can be executed. First, we must learn the initiation set classifier for o g , the option used to trigger the MDP's goal state. While acting in the environment, the agent's global DDPG will trigger the goal state N times (also referred to as the gestation period of the option by  Konidaris & Barto (2009b)  and  Niekum & Barto (2011) ). We collect these N successful trajectories, segment the last K states from each trajectory and learn a one-class classifier around the segmented states. Once initialized, it may be necessary to refine the option's initiation set based on its policy. We do so by executing the option and collecting data to train a two-class classifier. States from which option execution was successful are labeled as positive examples. States from which option execution timed out are labeled as negative examples. We continue this process of refining the option's initiation set classifier for a fixed number of episodes, which we call the initiation period of the option.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 At the end of the initiation period, we fix the option's initiation set classifier and add it to the list of salient events in the MDP. We then construct a new option whose termination condition is the initiation classifier of the option we just learned. We continue adding to our chain of options in this fashion until a learned initiation set classifier contains the start state of the MDP.

Section Title: GENERALIZING TO SKILL TREES
  GENERALIZING TO SKILL TREES Our discussion so far has been focused on learning skill chains that extend from the goal to the start state of the MDP. However, such a chain is not sufficient if the agent has multiple start states or if we want the agent to learn multiple ways of solving the same problem. To permit such behavior, our algorithm can be used to learn skills that organize more generally in the form of trees ( Konidaris & Barto, 2009b ;  Konidaris et al., 2012 ). This generalization requires some additional care while learning initiation set classifiers, the details of which can be found in Section A.1 of the Appendix. To demonstrate our ability to construct such skill trees (and their usefulness), we consider a maze navigation task, E-Maze, with distinct start states in Section 4.

Section Title: OPTIMALITY OF DISCOVERED SOLUTIONS
  OPTIMALITY OF DISCOVERED SOLUTIONS Each option o's internal policy π o is is given a subgoal reward only when it triggers its termination condition β o . As a result, π o is trained to find the optimal trajectory for entering its own goal region. Naively executing learned skills would thus yield a recursively optimal solution to the MDP ( Barto & Mahadevan, 2003 ). However, since the policy over options π O does not see subgoal rewards and is trained using extrinsic rewards only, it can combine learned skills and primitive actions to discover a flat optimal solution π * to the MDP ( Barto & Mahadevan, 2003 ). Indeed, our algorithm allows π O to employ discovered skills to quickly and reliably find feasible paths to the goal, which over time can be refined into optimal solutions. It is worth noting that our ability to recover π * in the limit is in contrast to feudal methods such as HAC ( Levy et al., 2019 ) in which higher levels of the hierarchy are rewarded for choosing feasible subgoals, not optimal ones. To summarize, our algorithm proceeds as follows: (1) Collect trajectories that trigger new option o k 's termination condition β o k . (2) Train o k 's option policy π o k . (3) Learn o k 's initiation set clas- sifier I o k . (4) Add o k to the agent's option repertoire. (5) Create a new option o k+1 such that β o k+1 = I o k . (6) Train policy over options π O . Steps 1, 3, 4 and 5 continue until the MDP's start state is inside some option's initiation set. Continue steps 2 and 6 indefinitely.

Section Title: EXPERIMENTS
  EXPERIMENTS We test our algorithm in five tasks that exhibit a strong hierarchical structure: (1) Point-Maze ( Duan et al., 2016 ), (2) Four Rooms with Lock and Key, (3) Reacher ( Brockman et al., 2016 ), (4) Point E-Maze and (5) Ant-Maze ( Duan et al., 2016 ;  Brockman et al., 2016 ). Since tasks 1, 3 and 5 appear frequently in the literature, details of their setup can be found in Appendix A.3.

Section Title: Four Rooms with Lock and Key
  Four Rooms with Lock and Key In this task, a point agent ( Duan et al., 2016 ) is placed in the Four Rooms environment ( Sutton et al., 1999 ). It must pick up the key (blue sphere in the top-right room in Figure 1(c), row 2) and then navigate to the lock (red sphere in the top-left room). The agent's state space consists of its position, orientation, linear velocity, rotational velocity and a has key indicator variable. If it reaches the lock with the key in its possession, its episode terminates with a sparse reward of 0; otherwise it gets a step penalty of −1. If we wish to autonomously discover the importance of the key, (i.e, without any corresponding extrinsic rewards) a distance-based dense reward such as that used in related work ( Nachum et al., 2018 ) would be infeasible.

Section Title: Point E-Maze
  Point E-Maze This task extends the benchmark U-shaped Point-Maze task ( Duan et al., 2016 ) so that the agent has two possible start locations - on the top and bottom rungs of the E-shaped maze respectively. We include this task to demonstrate our algorithm's ability to construct skill trees.

Section Title: COMPARATIVE ANALYSES
  COMPARATIVE ANALYSES We compared the performance of our algorithm to DDPG, Option-Critic and Hierarchical Actor- Critic (HAC), in the conditions most similar to those in which they were originally evaluated. For Published as a conference paper at ICLR 2020 (a) (b) (c) instance, in the Ant-Maze task we compare against Option-Critic under a dense-reward formulation of the problem while comparing to HAC under a sparse-reward version of the same task. As a result, we show the learning curves comparing against them on different plots (columns (a) and (b) in  Figure 1  respectively) to emphasize the difference between the algorithms, the settings in which they are applicable, and the way they are evaluated. Comparison with DDPG and Option-Critic: Figure 1(a) shows the results of comparing our pro- posed algorithm (DSC) with a flat RL agent (DDPG) and the version of Option-Critic designed for continuous action spaces (PPOC). 4 Deep skill chaining comfortably outperforms both base- lines. Both DSC and DDPG use the same exploration strategy in which a t = π θ (s t ) + η t where η t ∼ N (0, t ). Option-Critic, on the other hand, learns a stochastic policy π θ (a t |s t ) and thus has baked-in exploration ( Sutton & Barto, 2018 ,  Ch. 13 ), precluding the need for additive noise during action selection. We hypothesize that this difference in exploration strategies is the reason Option- Critic initially performs better than both DDPG and DSC in the Reacher and Point E-Maze tasks.

Section Title: Comparison with Hierarchical Actor-Critic
  Comparison with Hierarchical Actor-Critic We compare our algorithm to Hierarchical Actor- Critic (HAC) ( Levy et al., 2019 ), which has recently outperformed other hierarchical reinforcement learning methods ( Nachum et al., 2018 ;  Vezhnevets et al., 2017 ) on a wide variety of tasks. 5 A noteworthy property of the HAC agent is that it may prematurely terminate its training episodes to prevent flooding its replay buffer with uninformative transitions. The length of each training episode in DSC however, is fixed and determined by the test environment. Unless the agent reaches the goal state, its episode lasts for the entirety of its episodic budget (e.g, this would be 1000 timesteps in the Point-Maze environment). Thus, to compare the two algorithms, we perform periodic test rollouts wherein all networks are frozen and both algorithms have the same time budget to solve the given task. Furthermore, since both DSC and HAC learn deterministic policies, we set t = 0 during these test rollouts. When comparing to HAC, we perform 1 test rollout after each training episode in all tasks except for Ant-Maze, where we average performance over 5 test rollouts every 10 episodes. Figure 1(b) shows that DSC outperforms HAC in all environments except for Four Rooms with a Lock and Key, where their performance is similar, even though DSC does not use Hindsight Experience Replay ( Andrychowicz et al., 2017 ) to deal with the sparse reward nature of this task.

Section Title: INTERPRETING LEARNED SKILLS
  INTERPRETING LEARNED SKILLS   Figure 2  visualizes the initiation set classifiers of options discovered by DSC in Four Rooms with a Lock and Key. Despite not getting any extrinsic reward for picking up the key, DSC discovers the following skill chain: the options shown in  Figure 2  columns (c) and (d) bring the agent to the room with the key. The option shown in column (b) then picks up the key (top row) and then takes the agent to the room with the lock (bottom row). Finally, the option in column (a) solves the overall problem by navigating to the lock with the key. Similar visualizations of learned initiation set classifiers in the E-Maze task can be found in the Figure 6 in the Appendix.  Figure 3  shows that DSC is able to learn options that induce simple, efficient policies along different segments of the state-space. Furthermore, it illustrates that in some states, the policy over options prefers primitive actions (shown in black) over learned skills. This suggests that DSC is robust to situations in which it constructs poor options or is unable to learn a good option policy in certain portions of the state-space. In particular,  Figure 3 (d)  shows how DSC constructs a skill tree to solve a problem with two distinct start states. It learns a common option near the goal (shown in blue), which then branches off into two different chains leading to its two different start states respectively.

Section Title: DISCUSSION AND CONCLUSION
  DISCUSSION AND CONCLUSION Deep skill chaining breaks complex long-horizon problems into a series of sub-problems and learns policies that solve those sub-problems. By doing so, it provides a significant performance boost when compared to a flat learning agent in all of the tasks considered in Section 4. We show superior performance when compared to Option-Critic, the leading framework for option discovery in continuous domains. A significant drawback of Option-Critic is that it assumes that all options are executable from everywhere in the state-space. By contrast, deep skill chaining explicitly learns initiation set classifiers. As a result, learned skills specialize in different regions of the state- space and do not have to bear the burden of learning representations for states that lie far outside of their initiation region. Furthermore, each option in the Option-Critic architecture leverages the same state-abstraction to learn option-specific value functions and policies, while deep skill chaining permits each skill to construct its own skill-specific state-abstraction ( Konidaris & Barto, 2009a ). An advantage of using Option-Critic over DSC is that it is not confined to goal-oriented tasks and can work in tasks which require continually maximizing non-sparse rewards. Section 4 also shows that deep skill chaining outperforms HAC in four out of five domains, while achieving comparable performance in one. We note that even though HAC was designed to work in the multi-goal setting, we test it here in the more constrained single-goal setting. Consequently, we argue that in problems which permit a stationary set of target events (like the ones considered here), deep skill chaining provides a favorable alternative to HAC. Furthermore, HAC depends on Hind- sight Experience Replay (HER) to train the different layers of their hierarchy. Deep skill chaining shows the benefits of using hierarchies even in the absence of such data augmentation techniques but including them should yield additional performance benefits in sparse-reward tasks. A drawback of deep skill chaining is that, because it builds skills backward from the goal, its per- formance in large state-spaces is dependent on a good exploration algorithm. We used the naive exploration strategy of adding Gaussian noise to chosen actions ( Lillicrap et al., 2015 ;  Fujimoto et al., 2018 ) since the exploration question is orthogonal to the ideas presented here. The lack of a sophisticated exploration algorithm also explains the higher variance in performance in the Point- Maze task in  Figure 1 . Combining effective exploration ( Machado et al., 2018 ;  Jinnai et al., 2020 ) with DSC's high reliability of triggering target events is a promising avenue for future work. We presented a new skill discovery algorithm that can solve high-dimensional goal-oriented tasks far more reliably than flat RL agents and other popular hierarchical methods. To our knowledge, DSC is the first deep option discovery algorithm that does not treat the number of options as a fixed and costly hyperparameter. Furthermore, where other deep option discovery techniques have struggled to show consistent improvements over baseline flat agents in the single task setting ( Zhang & Whiteson, 2019 ;  Smith et al., 2018 ;  Harb et al., 2018 ;  Klissarov et al., 2017 ), we unequivocally show the necessity for hierarchies for solving challenging problems.

Section Title: ACKNOWLEDGEMENTS
  ACKNOWLEDGEMENTS
  Using the mean Q-value is equivalent to performing Monte Carlo rollouts. Instead, we follow the principle of optimism under uncertainty ( Brafman & Tennenholtz, 2002 ) to select the max over the Q-values.

```
