<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 DEEPSFM: STRUCTURE FROM MOTION VIA DEEP BUNDLE ADJUSTMENT</article-title></title-group><abstract><p>Structure from motion (SfM) is an essential computer vision problem which has not been well handled by deep learning. One of the promising trends is to apply explicit structural constraint, e.g. 3D cost volume, into the network. In this work, we design a physical driven architecture, namely DeepSFM, inspired by tradi- tional Bundle Adjustment (BA), which consists of two cost volume based archi- tectures for depth and pose estimation respectively, iteratively running to improve both. In each cost volume, we encode not only photo-metric consistency across multiple input images, but also geometric consistency to ensure that depths from multiple views agree with each other. The explicit constraints on both depth (struc- ture) and pose (motion), when combined with the learning components, bring the merit from both traditional BA and emerging deep learning technology. Extensive experiments on various datasets show that our model achieves the state-of-the-art performance on both depth and pose estimation with superior robustness against less number of inputs and the noise in initialization.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Structure from motion (SfM) is a fundamental human vision functionality which recovers 3D struc- tures from the projected retinal images of moving objects or scenes. It enables machines to sense and understand with the 3D world and is critical in achieving real-world artificial intelligence. Over decades of researches, there has been a lot of great success on SfM; however, the performance is far from perfect.</p><p>Conventional SfM approaches (<xref ref-type="bibr" rid="b0">Agarwal et al., 2011</xref>; <xref ref-type="bibr" rid="b2">Wu et al., 2011a</xref>; <xref ref-type="bibr" rid="b6">Engel et al., 2017</xref>; <xref ref-type="bibr" rid="b3">Delaunoy &amp; Pollefeys, 2014</xref>) heavily rely on Bundle-Adjustment (BA) (<xref ref-type="bibr" rid="b2">Triggs et al., 1999</xref>; <xref ref-type="bibr" rid="b0">Agarwal et al., 2010</xref>), in which 3D structures and camera motions of each view are jointly optimized via Levenberg- Marquardt (LM) algorithm (<xref ref-type="bibr" rid="b0">Nocedal &amp; Wright, 2006</xref>) according to the cross-view correspondence. Though successful in certain scenarios, conventional SfM based approaches are fundamentally re- stricted by the coverage of the provided multiple views and the overlaps among them. They also typically fail to reconstruct textureless or non-lambertian (e.g. reflective or transparent) surfaces due to the missing of correspondence across views. As a result, selecting sufficiently good input views and the right scene requires excessive caution and is usually non-trivial to even experienced user. Recent researches resort to deep learning to deal with the typical weakness of conventional SfM. Early effort utilizes deep neural network as a powerful mapping function that directly regresses the structures and motions (<xref ref-type="bibr" rid="b2">Ummenhofer et al., 2017</xref>; <xref ref-type="bibr" rid="b2">Vijayanarasimhan et al., 2017</xref>; <xref ref-type="bibr" rid="b2">Zhou et al., 2017</xref>; <xref ref-type="bibr" rid="b8">Wang et al., 2017</xref>). Since the geometric constraints of structures and motions are not explicitly en- forced, the network does not learn the underlying physics and prone to overfitting. Consequently, they do not perform as accurate as conventional SfM approaches and suffer from extremely poor generalization capability. Most recently, the 3D cost volume (<xref ref-type="bibr" rid="b0">Teed &amp; Deng, 2018</xref>) has been intro- duced to explicit leveraging photo-consistency in a differentiable way, which significantly boosts the performance of deep learning based 3D reconstruction. However, the camera motion usually has to be known (<xref ref-type="bibr" rid="b2">Yao et al., 2018</xref>; <xref ref-type="bibr" rid="b17">Im et al., 2019</xref>) or predicted via direct regression (<xref ref-type="bibr" rid="b2">Ummenhofer et al., 2017</xref>; <xref ref-type="bibr" rid="b2">Zhou et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Teed &amp; Deng, 2018</xref>), which still suffer from generalization issue.</p><p>In this paper, we explicitly enforce photo-consistency, geometric-consistency, and camera motion constraints in a unified deep learning framework. In particular, our network includes a depth based cost volume (D-CV) and a pose based cost volume (P-CV). D-CV optimizes per-pixel depth values Under review as a conference paper at ICLR 2020 with the current camera poses, while P-CV optimizes camera poses with the current depth esti- mations. Conventional 3D cost volume enforces photo-consistency by unprojecting pixels into the discrete camera fronto-parallel planes and computing the photometric (i.e. image feature) difference as the cost. In addition to that, our D-CV further enforces geometric-consistency among cameras with their current depth estimations by adding the geometric (i.e. depth) difference to the cost. Note that the initial depth estimation can be obtained using the conventional 3D cost volume. For pose es- timation, rather than direct regression, our P-CV discretizes around the current camera positions, and also computes the photometric and/or geometric differences by hypothetically moving the camera into the discretized position. Note that the initial camera pose can be obtained by a rough estimation from the direct regression methods such as (<xref ref-type="bibr" rid="b2">Ummenhofer et al., 2017</xref>). Our framework bridges the gap between the conventional and deep learning based SfM by incorporating explicit constraints of photo-consistency, geometric-consistency and camera motions all in the deep network.</p><p>The closest work in the literature is the recently proposed BA-Net (<xref ref-type="bibr" rid="b0">Tang &amp; Tan, 2018</xref>), which also aims to explicitly incorporate multi-view geometric constraints in a deep learning framework. They achieve this goal by integrating the LM optimization into the network. However, the LM iterations are unrolled with few iterations due to the memory and computational inefficiency, and thus it may lead to non-optimal solutions. In contrast, our method does not have a restriction on the number of it- erations and achieves empirically better performance. Furthermore, LM in SfM originally optimizes point and camera positions, and thus direct integration of LM still requires good correspondences. To evade the correspondence issue in typical SfM, their models employ a direct regressor to predict depth at the front end, which heavily relies on prior in the training data. In contrast, our model is a fully physical-driven architecture that less suffers from over-fitting issue for both depth and pose estimation.</p><p>To demonstrate the superiority of our method, we conduct extensive experiments on DeMoN datasets, ScanNet and ETH3D. The experiments show that our approach outperforms the state-of- the-art <xref ref-type="bibr" rid="b0">Schonberger &amp; Frahm (2016)</xref>; <xref ref-type="bibr" rid="b2">Ummenhofer et al. (2017)</xref>; <xref ref-type="bibr" rid="b0">Tang &amp; Tan (2018)</xref>.</p></sec><sec><title>RELATED WORK</title><p>There is a large body of work that focuses on inferring depth or motion from color images, ranging from single view, multiple views and monocular video. We discuss them in the context of our work.</p></sec><sec><title>Single-view Depth Estimation</title><p>While ill-posed, the emerging of deep learning technology enables the estimation of depth from a single color image. The early work directly formulates this into a per-pixel regression problem (<xref ref-type="bibr" rid="b4">Eigen et al., 2014</xref>), and follow-up works improve the performance by introducing multi-scale network architectures (<xref ref-type="bibr" rid="b4">Eigen et al., 2014</xref>; <xref ref-type="bibr" rid="b4">Eigen &amp; Fergus, 2015</xref>), skip- connections (<xref ref-type="bibr" rid="b8">Wang et al., 2015</xref>; <xref ref-type="bibr" rid="b2">Liu et al., 2016</xref>), powerful decoder and post process (<xref ref-type="bibr" rid="b11">Garg et al., 2016</xref>; <xref ref-type="bibr" rid="b2">Laina et al., 2016</xref>; <xref ref-type="bibr" rid="b2">Kuznietsov et al., 2017</xref>; <xref ref-type="bibr" rid="b8">Wang et al., 2015</xref>; <xref ref-type="bibr" rid="b2">Liu et al., 2016</xref>), and new loss functions (<xref ref-type="bibr" rid="b8">Fu et al., 2018</xref>). Even though single view based methods generate plausible results, the models usually resort heavily to the prior in the training data and suffer from generalization capability. Nevertheless, these methods still act as an important component in some multi-view systems (<xref ref-type="bibr" rid="b0">Tang &amp; Tan, 2018)</xref> Traditional Structure-from-Motion Simultaneously estimating 3d structure and camera motion is a well studied problem which has a traditional tool-chain of techniques (<xref ref-type="bibr" rid="b1">Furukawa et al., 2010</xref>; <xref ref-type="bibr" rid="b0">New- combe et al., 2011</xref>; <xref ref-type="bibr" rid="b2">Wu et al., 2011b</xref>). Structure from Motion(SfM) has made great progress in many aspects. <xref ref-type="bibr" rid="b0">Lowe (2004)</xref>; <xref ref-type="bibr" rid="b13">Han et al. (2015)</xref> aim at improving features and <xref ref-type="bibr" rid="b0">Snavely (2011)</xref> introduce new optimization techniques. More robust structures and data representations are introduced by <xref ref-type="bibr" rid="b12">Gherardi et al. (2010)</xref>; <xref ref-type="bibr" rid="b0">Schonberger &amp; Frahm (2016)</xref>. Simultaneous Localization and Sapping(SLAM) sys- tems track the motion of the camera and build 3D structure from video sequence (<xref ref-type="bibr" rid="b2">Newcombe et al., 2011</xref>; <xref ref-type="bibr" rid="b6">Engel et al., 2014</xref>; <xref ref-type="bibr" rid="b0">Mur-Artal et al., 2015</xref>; <xref ref-type="bibr" rid="b0">Mur-Artal &amp; Tard&#243;s, 2017</xref>). <xref ref-type="bibr" rid="b6">Engel et al. (2014)</xref> propose the photometric bundle adjustment algorithm to directly minimize the photometric error of aligned pixels. However, traditional SfM and SLAM methods are sensitive to low texture region, occlusions, moving objects and lighting changes, which limit the performance and stability.</p><p>Deep Learning for Structure-from-Motion Deep neural networks have shown great success in stereo matching and Structure-from-Motion problems. <xref ref-type="bibr" rid="b2">Ummenhofer et al. (2017)</xref>; <xref ref-type="bibr" rid="b8">Wang et al. (2017)</xref>; <xref ref-type="bibr" rid="b2">Vijayanarasimhan et al. (2017)</xref>; <xref ref-type="bibr" rid="b2">Zhou et al. (2017)</xref> regress depth map and camera pose di- Under review as a conference paper at ICLR 2020 rectly in a supervised manner or by introducing photometric constraints between depth and motion as a self-supervision signal. Such methods solve the camera motion as a regression problem, and the relation between camera motion and depth prediction is neglected.</p><p>Recently, some methods exploit multi-view photometric or feature-metric constraints to enforce the relationship between dense depth map and the camera pose in network. The SE3 transformer layer is introduced by <xref ref-type="bibr" rid="b0">Teed &amp; Deng (2018)</xref>, which uses geometry to map flow and depth into a camera pose update. <xref ref-type="bibr" rid="b8">Wang et al. (2018)</xref> propose the differentiable camera motion estimator based on the Direct Visual Odometry (<xref ref-type="bibr" rid="b2">Steinbr&#252;cker et al., 2011</xref>). <xref ref-type="bibr" rid="b2">Clark et al. (2018)</xref> using a LSTM-RNN (<xref ref-type="bibr" rid="b16">Hochreiter et al., 2001</xref>) as the optimizer to solve nonlinear least squares in two-view SfM. <xref ref-type="bibr" rid="b0">Tang &amp; Tan (2018)</xref> train a network to generate a set of basis depth maps and optimize depth and camera poses in a BA-layer by minimizing a feature-metric error.</p></sec><sec><title>ARCHITECTURE</title><p>Our framework receives frames of a scene from different viewpoints, and produces photo-metrically and geometrically consistent depth maps across all frames and the corresponding camera poses. Similar to BA, we also assume initial structures (i.e depth maps) and motions (i.e. camera poses) are given. Note that the initialization is not necessary to be super accurate for the good perfor- mance using our framework and thus can be easily obtained from some direct regression methods (<xref ref-type="bibr" rid="b2">Ummenhofer et al., 2017</xref>).</p><p>Now we introduce the detail of our model - DeepSFM. Without loss of generality, we describe our model taking two images as input, namely the reference image and the source image, as an example, and all the technical components can be extended for multiple images straightforward. As shown in <xref ref-type="fig" rid="fig_0">Figure 1</xref>, we first extract feature maps from input images through a shared encoder. We then sample the solution space for depth uniformly in the inverse-depth space between a predefined minimum and maximum range and camera pose around the initialization respectively. After that, we build cost volumes accordingly to reason the confidence of each hypothesis. This is achieved by validating the consistency between the feature of the reference view and the ones warped from the source image. Besides photo-metric consistency that measures the color image similarity, we also take into account the geometric consistency across warped depth maps. Note that depth and pose require different designs of cost volume to efficiently sample the hypothesis space. Gradients can back-propagate through cost volumes, and cost-volume construction does not affect any trainable parameters. The cost volumes are then fed into 3D CNN to regress new depth and pose. These updated value can be used to create new cost volumes, and the model improves the prediction iteratively.</p><p>For notations, we denote {I i } n i=1 as the image sequences in one scene, {D i } n i=1 as the corresponding ground truth depth maps, {K i } n i=1 as the camera intrinsics, {R i , t i } n i=1 as the ground truth rotations Under review as a conference paper at ICLR 2020 and translations of camera, {D * i } n i=1 and {R * i , t * i } n i=1 as initial depth maps and camera pose param- eters for constructing cost volumes, where n is the number of image samples.</p></sec><sec><title>2D FEATURE EXTRACTION</title><p>Given the input sequences {I i } n i=1 , we extract the 2D CNN feature {F i } n i=1 for each frame. Firstly, a 7 layers' CNN with kernel size 3&#215;3 is applied to extract low contextual information. Then we adopt a spatial pyramid pooling (SPP) (<xref ref-type="bibr" rid="b18">Kaiming et al., 2014</xref>) module, which can extract hierarchical multi- scale features through 4 average pooling blocks with different pooling kernel size (4 &#215; 4, 8 &#215; 8, 16 &#215; 16, 32 &#215; 32). Finally, we pass the concatenated features through 2D CNNs to get the 32-channel image features after upsampling these multi-scale features into the same resolution. These image sequence features are used by the building of both our depth based and pose based cost volumes.</p></sec><sec><title>DEPTH BASED COST VOLUME (D-CV)</title><p>Traditional plane sweep cost volume aims to back-project the source images onto successive virtual planes in the 3D space and measure photo-consistency error among the warped image features and reference image features for each pixel. Different from the cost volume used in previous multi- view and structure-from-motion methods, we construct a D-CV to further utilize the local geometric consistency constraints introduced by depth maps. Inspired by the traditional plane sweep cost volumes, our D-CV is a concatenation of three components: the reference image features, the warped source image features and the homogeneous depth consistency maps.</p></sec><sec><title>Hypothesis Sampling</title><p>To back-project the features and depth maps from source viewpoint to the 3D space in reference viewpoint, we uniformly sample a set of L virtual planes {d l } L l=1 in the inverse-depth space which are perpendicular to the forward direction (z-axis) of the reference view- point. These planes serve as the hypothesis of the output depth map, and the cost volume can be built upon them.</p></sec><sec><title>Feature warping</title><p>To construct our D-CV, we first warp source image features F i (of size CHannel &#215; W idth &#215; Height ) to each of the hypothetical depth map planes d l using camera intrinsic matrix K and initial camera poses {R * i , t * i }, according to: F il (u) = F i (&#361; l ) ,&#361; l &#8764; K [R * i |t * i ] K &#8722;1 u d l 1 (1) where u and&#361; l are the homogeneous coordinates of each pixel in the reference view and the pro- jected coordinates onto the corresponding source view.F il (u) denotes the warped feature of the source image through the l-th virtual depth plane. Note that the projected homogeneous coordinates u l are floating numbers, and we adopt a differentiable bilinear interpolation to generate the warped feature mapF il . The pixels with no source view coverage are assigned with zeros. <xref ref-type="bibr" rid="b17">Following Im et al. (2019)</xref>, we concatenate the reference feature and the warped reference feature together and obtain a 2CH &#215; L &#215; W &#215; H 4D feature volume.</p></sec><sec><title>Depth consistency</title><p>In addition to photometric consistency, to exploit geometric consistency and promote the quality of depth prediction, we add two more channels on each virtual plane: the warped initial depth maps from the source view and the depth map of the virtual plane from the perspective of the source view. Note that the former is the same as image feature warping, while the latter requires a coordinate transformation from the reference camera to the source camera.</p><p>In particular, the first channel is computed as follows. The initial depth map of source image is first down-sampled and then warped to hypothetical depth planes based on initial camera pose similarly to the image feature warping:D * il (u) = D * i (&#361; l ) (2) where the coordinates u and&#361; l are defined in Eq. 1 andD * il (u) represents the warped one-channel depth map on the l-th depth plane. One distinction between depth warping and feature warping is that we adopt nearest neighbor sampling for depth warping, instead of bilinear interpolation. A comparison between the two methods are provided in Appendix C.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>The second channel contains the depth values of the virtual planes in the reference view by seeing them from the source view. To transform the virtual planes to the source view coordinate system, we apply a T function on each virtual plane d l in the following:</p><p>We stack the warped initial depth maps and the transformed depth planes together, and get a depth volume of size 2 &#215; L &#215; W &#215; H.</p><p>By concatenating the feature volume and depth volume together, we obtain a 4D cost tensor of size (2CH + 2) &#215; L &#215; W &#215; H. Given the 4D cost volume, our network learns a cost volume of size L &#215; W &#215; H using several 3D convolutional layers with kernel size 3 &#215; 3 &#215; 3. When there is more than one source image, we get the final cost volume by averaging over multiple input source views.</p></sec><sec><title>POSE BASED COST VOLUME (P-CV)</title><p>In addition to the construction of D-CV, we also propose a P-CV, aiming at optimizing initial camera poses through both photometric and geometric consistency. Instead of building a cost volume based on hypothetical depth map planes, our novel P-CV is constructed based on a set of assumptive camera poses. Similar to D-CV, P-CV is also concatenated by three components: the reference image features, the warped source image features and the homogeneous depth consistency maps. Given initial camera pose parameters {R * i , t * i }, we uniformly sample a batch of discrete candidate camera poses around. Since jointly sampling camera rotation and translation along 6-DoF is costly, we shift rotation and translation separately by keeping one frozen while sampling the other one. In the end, a group of P virtual camera poses noted as {R * ip |t * ip } P p=1 around input pose are obtained for cost volume construction.</p><p>The posed-based cost volume is also constructed by concatenating image features and homogeneous depth maps. However, source view features and depth maps are warped based on sampled camera poses. For feature warping, we compute&#361; p as following equations: u p &#8764; K R * ip |t * ip K &#8722;1 u D * i 1 (4) where D * i is the initial reference view depth. Similar to D-CV, we get warped source feature map F ip after bilinear sampling and concatenate it with reference view feature map. We also transform the initial reference view depth and source view depth into one homogeneous coordinate system, which enhances the geometric consistency between camera pose and multi view depth maps. After concatenating the above feature maps and depth maps together, we again build a 4D cost volume of size (2CH + 2) &#215; P &#215; W &#215; H, where W and H are the width and height of feature map, CH is the number of channels. We get output of size 1 &#215; P &#215; 1 &#215; 1 from the above 4-D tensor after eight 3D convolutional layers with kernel size 3 &#215; 3 &#215; 3, three 3D average pooling layers with stride size 2 &#215; 2 &#215; 1 and one global average pooling at the end.</p></sec><sec><title>COST AGGREGATION AND REGRESSION</title><p>For depth prediction, we follow the cost aggregation technique introduced by <xref ref-type="bibr" rid="b17">Im et al. (2019)</xref>. We adopt a context network, which takes reference image features and each slice of the coarse cost volume after 3D convolution as input and produce the refined cost slice. The final aggregated depth based volume is obtained by adding coarse and refined cost slices together. The last step to get depth prediction of reference image is depth regression. We pass each slice of D-CV through a soft-max function to get the probability of every depth value l. Then the weighted sum of all hypothetical depth values is regarded as predicted depth map; this operation is called soft-argmax. We can also get the predicted coarse depth map by the same way using coarse D-CV. For camera poses prediction, we also apply a soft-argmax function on pose cost volume and get the estimated output rotation and translation vectors.</p></sec><sec><title>TRAINING</title><p>The DeepSFM learns the feature extractor, cost aggregation, and the regression layers in a supervised way. We denoteR i andt i as predicted rotation angles and translation vectors of camera pose. Then the pose loss function is defined as the L1 distance between prediction and groundtruth: L rotation = R i &#8722; R i and L translation = t i &#8722; t i . We denoteD 0 i andD i as predicted coarse depth map and refined depth map for the i-th image, then the depth loss function is defined as following equation: L depth = i &#955;H(D 0 i , D i ) + H(D i , D i ) (5) where &#955; is weight parameter and function H is Huber loss.</p><p>Our final objective becomes</p><p>We follow two rules to set &#955; r , &#955; t and &#955; d : 1) the loss term provides gradient on the same order of numerical value range, such that no single loss term could dominate the training process, since accuracy in depth and camera pose are both important to reach a good consensus. 2) we found in practice the camera rotation has higher impact on the accuracy of the depth but not the opposite. To encourage better performance of pose, we set a relatively large &#955; r . In practice, the weight parameter &#955; to balance loss objective is set to 0.7, while &#955; r = 0.8, &#955; t = 0.1 and &#955; d = 0.1.</p><p>The RGB sequences, corresponding ground-truth depth maps and camera intrinsics and extrinsics are fed as input samples. We initialize the 2D feature extraction layers with pre-trained DPSNet weight. The initial depth maps and camera poses {D * i } n i=1 and {R * i , t * i } n i=1 are obtained from De- MoN. To keep correct scale, we multiply translation vectors and depth maps by the norm of the ground truth camera translation vector. The whole training and testing procedure are performed as four iterations. During each iteration, we take the predicted depth maps and camera poses of previous iteration as new {D * i } n i=1 and {R * i , t * i } n i=1 for cost volume construction.</p><p>We implement our system using PyTorch framework. The training procedure takes 6 days on 3 NVIDIA TITAN GPUs on all 160k training sequences. The training batch size is set to 4, and the Adam optimizer (&#946; 1 = 0.9, &#946; 2 = 0.999) is used with learning rate 2 &#215; 10 &#8722;4 , which decreases to 4 &#215; 10 &#8722;5 after 2 epochs. Within the first two epochs, the parameters in 2D CNN feature extraction module are frozen, and the ground truth depth maps for source images are used to construct D-CV and P-CV, which are replaced with predicted depth maps from network in latter epochs. During training process, the length of input sequences is 2 (one reference image and one source image). The L for D-CV is set to 64 and the N for P-CV is 10. The range of both cost volumes is adapted during training and testing.</p></sec><sec><title>EXPERIMENTS</title></sec><sec><title>DATASETS</title><p>We evaluate DeepSFM on widely used datasets and compare to state-of-the-art methods on accuracy and generalization capability.</p><p>DeMoN Datasets Proposed in DeMoN (<xref ref-type="bibr" rid="b2">Ummenhofer et al., 2017</xref>), this dataset contains data from various sources, including SUN3D (<xref ref-type="bibr" rid="b2">Xiao et al., 2013</xref>), RGB-D SLAM (<xref ref-type="bibr" rid="b2">Sturm et al., 2012</xref>), and Scenes11 (<xref ref-type="bibr" rid="b2">Chang et al., 2015</xref>). To test the generalization capability, we also evaluate on MVS (<xref ref-type="bibr" rid="b9">Fuhrmann et al., 2014</xref>) dataset but not use it for the training. In all four datasets, RGB image sequences and the ground truth depth maps are provided with the camera intrinsics and camera poses. Note that those datasets together provide a diverse set of both indoor and outdoor, synthetic and real-world scenes. Specifically, Scenes11 consists of synthetic images rendered from random scenes, on which ground truth camera poses and depth are perfect, but objects are lack of reality in scale and semantics. For training and testing, we use the same setting as DeMoN.</p><p>ETH3D Dataset ETH3D dataset provides a variety of indoor and outdoor scenes with high- precision ground truth 3D points captured by laser scanners, which is a more solid benchmark Under review as a conference paper at ICLR 2020 dataset. Ground truth depth maps are obtained by projecting the point clouds to each camera view. Raw images are in high resolution but resized to 810 &#215; 540 pixels for evaluation due to memory constraint. Again, all the models are trained on DeMoN and tested here.</p></sec><sec><title>DeMoN Datasets</title><p>Our results on DeMoN datasets and the comparison to other methods are shown in <xref ref-type="table" rid="tab_0">Table 1</xref>. We cite results of some strong baseline methods from DeMoN paper, named as Base-Oracle, Base-SIFT, Base-FF and Base-Matlab respectively (<xref ref-type="bibr" rid="b2">Ummenhofer et al., 2017</xref>). Base-Oracle estimate depth with the ground truth camera motion using SGM (<xref ref-type="bibr" rid="b15">Hirschmuller, 2005</xref>). Base-SIFT, Base- FF and Base-Matlab solve camera motion and depth using feature, optical flow, and KLT tracking correspondence from 8-pt algorithm (<xref ref-type="bibr" rid="b14">Hartley, 1997</xref>). We also compare to some most recent state- of-the-art methods LS-Net (<xref ref-type="bibr" rid="b2">Clark et al., 2018</xref>) and BA-Net (<xref ref-type="bibr" rid="b0">Tang &amp; Tan, 2018</xref>). LS-Net introduces the learned LSTM-RNN optimizer to minimizing photometric error for stereo reconstruction. BA- Net is the most recent work that minimizes the feature-metric error between multi-view via the differentiable Levenberg-Marquardt (<xref ref-type="bibr" rid="b0">Lourakis &amp; Argyros, 2005</xref>) algorithm.</p><p>To make a fair comparison, we adopt the same error metrics as DeMoN for depth and camera pose evaluation. L1-inv computes the disparity map errors, and sc-inv is a scale-invariant error metric. L1-rel measures the depth errors relative to the ground truth depth, which emphasize depth estima- tion of close range in the scene. For camera poses evaluation, the angles between the prediction and the ground truth rotation and translation are shown as Rot and Trans respectively.</p><p>Our method outperforms all traditional baseline methods and DeMoN on both depth and camera poses. When compared to more recent LS-Net and BA-Net, our method produces better results in most metrics of the four datasets. On RGB-D dataset, our performance is comparable to the state- of-the-art due to relatively higher noise in the RGB-D ground truth. LS-Net trains an initialization network which regresses depth and motion directly before adding the LSTM-RNN optimizer. The performance of the RNN optimizer is highly affected by the accuracy of the regressed initializa- tion. The depth results of LS-Net are consistently poorer than BA-Net and our method, despite better rotation parameters are estimated by LS-Net on RGB-D and Sun3D datasets with very good initialization. Our method is slightly inferior to BA-Net on the L1-rel metric, which is probably due to that we sample 64 virtual planes uniformly as the hypothetical depth set, while BA-Net op- timizes depth prediction based on a set of 128-channel estimated basis depth maps that are more memory consuming but have more fine-grained results empirically. Despite all that, it is shown that our learned cost volumes with geometric consistency work better than the photometric bundle ad- justment (e.g. used in BA-Net) in most scenes. In particular, we improve mostly on the Scenes11 dataset, where the ground truth is perfect but the input images contain a lot of texture-less regions, which are challenging to photo-consistency based methods.</p></sec><sec><title>ETH3D</title><p>We further test the generalization capability on ETH3D. We provide comparisons to COLMAP (<xref ref-type="bibr" rid="b0">Schonberger &amp; Frahm, 2016</xref>) and DeMoN on ETH3D. COLMAP is a state-of-the-art Structure-from-Motion method, while DeMoN introduces a classical deep network architecture that directly regress depth and motion in a supervised manner. In the accuracy metric, the error &#948; s de- fined as max( y * i yi , yi y * i ), and the thresholds are typically set as [1.25, 1.25 2 , 1.25 3 ]. In <xref ref-type="table" rid="tab_1">Table 2</xref>, our method shows the best performance overall among all the comparison methods. Our method pro- duces better results than DeMoN consistently, since we impose geometric and physical constraints onto network rather than learning to regress directly. When compared with COLMAP, our method performs better on most metrics. COLMAP behaves well in the accuracy metric (i.e. abs_diff). However, the presence of outliers is often observed in the predictions of COLMAP, which leads to poor performance in other metrics such as abs_rel and sq_rel, since those metrics are sensitive to outliers. We put more qualitative comparisons with COLMAP in Appendix C. For more comparison on generalization, another experiment on ScanNet is provided in Appendix B.</p></sec><sec><title>MODEL ANALYSIS</title><p>In this section, we analyze our model on several aspects to verify the optimality and show advantages over previous methods.</p></sec><sec><title>Iterative Improvement</title><p>Our model can run iteratively to reduce the prediction error. <xref ref-type="fig" rid="fig_1">Figure 2</xref> (solid lines) shows our performance over iterations when initialized with the prediction from DeMoN. As can be seen, our model effectively reduces both depth and pose errors upon the DeMoN output. Under review as a conference paper at ICLR 2020 Throughout the iterations, better depth and pose benefit each other by building more accurate cost volume, and both are consistently improved. The whole process is similar to coordinate descent algorithm, and finally converges at iteration 4.</p></sec><sec><title>Effect of P-CV</title><p>We compare DeepSFM to a baseline method for our P-CV. In this baseline, the depth prediction is the same as DeepSFM, but the pose prediction network is replaced by a direct visual odometry model <xref ref-type="bibr" rid="b2">Steinbr&#252;cker et al. (2011)</xref>, which updates camera parameters by minimizing pixel-wise photometric error between image features. Both methods are initialized with DeMoN results. As provided in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, DeepSFM consistently produces lower errors on both depth and pose over all the iterations. This shows that our P-CV predicts more accurate pose and performs more robust against noise depth at early stages. View Number DeepSFM works still reasonably well with fewer views due to the free from opti- mization based components. To show this, we compare to COLMAP with respect to the number of input views on ETH3D. As depicted in <xref ref-type="fig" rid="fig_2">Figure 3</xref>, more images yield better results for both meth- ods as expected. However, our performance drops significantly slower than COLMAP with fewer number of inputs. Numerically, DeepSFM cuts the depth error by half under the same number of views as COLMAP, or achieves similar error with half number of views required by COLMAP. This clearly demonstrates that DeepSFM is more robust when fewer inputs are available.</p></sec><sec><title>CONCLUSIONS</title><p>We present a deep learning framework for Structure-from-Motion, which explicitly enforces photo- metric consistency, geometric consistency and camera motion constraints all in the deep network. This is achieved by two key components - namely D-CV and P-CV. Both cost volumes measure the photo-metric errors and geometric errors but hypothetically move reconstructed scene points (structure) or camera (motion) respectively. Our deep network can be considered as an enhanced learning based BA algorithm, which takes the best benefits from both learnable priors and geometric rules. Consequently, our method outperforms conventional BA and state-of-the-art deep learning based methods for SfM.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Overview of our method. 2D CNN is used to extract photometric feature to construct cost volumes. Initial source depth maps are used to introduce geometry consistency. A series of 3D CNN layers are applied for both pose based cost volume and D-CV. Then a context network and depth regression operation are applied to produce predicted depth map of reference image.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Results on MVS, SUN3D, RGBD and Scenes11, the best results are noted by Bold.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Results on ETH3D (Bold: best; &#945; = 1.25). abs_rel, abs_diff, sq_rel, rms, and log_rms, are absolute relative error, absolute difference, square relative difference, root mean square and log root mean square, respectively.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Comparison with baseline during iterations. Our work converges at a better position. (a) abs relative error and log RMSE. (b) rotation and translation degree error.</p></caption><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Depth map results w.r.t. the number of images.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><source>European conference on computer vision</source><year>2010</year><fpage>29</fpage><lpage>42</lpage><person-group person-group-type="author"><name><surname>References Sameer Agarwal</surname><given-names>Noah</given-names></name><name><surname>Snavely</surname><given-names /></name><name><surname>Steven</surname><given-names>M</given-names></name><name><surname>Seitz</surname><given-names>Richard</given-names></name><name><surname>Szeliski</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Building rome in a day</article-title><source>Communications of the ACM</source><year>2011</year><volume>54</volume><issue>10</issue><fpage>105</fpage><lpage>112</lpage><person-group person-group-type="author"><name><surname>Agarwal</surname><given-names>Sameer</given-names></name><name><surname>Furukawa</surname><given-names>Yasutaka</given-names></name><name><surname>Snavely</surname><given-names>Noah</given-names></name><name><surname>Simon</surname><given-names>Ian</given-names></name><name><surname>Curless</surname><given-names>Brian</given-names></name><name><surname>Steven</surname><given-names>M</given-names></name><name><surname>Seitz</surname><given-names>Richard</given-names></name><name><surname>Szeliski</surname><given-names /></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. Under review as a conference paper at ICLR 2020 Ronald Clark, Michael Bloesch, Jan Czarnowski, Stefan Leutenegger, and Andrew J Davison. Learning to solve nonlinear least squares for monocular stereo</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><year>2018</year><fpage>284</fpage><lpage>299</lpage><person-group person-group-type="author"><name><surname>Angel</surname><given-names>X</given-names></name><name><surname>Chang</surname><given-names>Thomas</given-names></name><name><surname>Funkhouser</surname><given-names>Leonidas</given-names></name><name><surname>Guibas</surname><given-names>Pat</given-names></name><name><surname>Hanrahan</surname><given-names>Qixing</given-names></name><name><surname>Huang</surname><given-names>Zimo</given-names></name><name><surname>Li</surname><given-names>Silvio</given-names></name><name><surname>Savarese</surname><given-names>Manolis</given-names></name><name><surname>Savva</surname><given-names>Shuran</given-names></name><name><surname>Song</surname><given-names>Hao</given-names></name><name><surname>Su</surname><given-names /></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Photometric bundle adjustment for dense multi-view 3d mod- eling</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2014</year><fpage>1486</fpage><lpage>1493</lpage><person-group person-group-type="author"><name><surname>Delaunoy</surname><given-names>Ama&#235;l</given-names></name><name><surname>Pollefeys</surname><given-names>Marc</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</article-title><source>The IEEE International Conference on Computer Vision (ICCV)</source><year>2015</year><person-group person-group-type="author"><name><surname>Eigen</surname><given-names>David</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Depth map prediction from a single image using a multi-scale deep network</article-title><source>Advances in neural information processing systems</source><year>2014</year><fpage>2366</fpage><lpage>2374</lpage><person-group person-group-type="author"><name><surname>Eigen</surname><given-names>David</given-names></name><name><surname>Puhrsch</surname><given-names>Christian</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><source>European conference on computer vision</source><year>2014</year><fpage>834</fpage><lpage>849</lpage><person-group person-group-type="author"><name><surname>Engel</surname><given-names>Jakob</given-names></name><name><surname>Sch&#246;ps</surname><given-names>Thomas</given-names></name><name><surname>Cremers</surname><given-names>Daniel</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Direct sparse odometry</article-title><source>IEEE transactions on pattern analysis and machine intelligence</source><year>2017</year><volume>40</volume><issue>3</issue><fpage>611</fpage><lpage>625</lpage><person-group person-group-type="author"><name><surname>Engel</surname><given-names>Jakob</given-names></name><name><surname>Koltun</surname><given-names>Vladlen</given-names></name><name><surname>Cremers</surname><given-names>Daniel</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Deep ordinal regression network for monocular depth estimation</article-title><source>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><year>2018</year><person-group person-group-type="author"><name><surname>Fu</surname><given-names>Huan</given-names></name><name><surname>Gong</surname><given-names>Mingming</given-names></name><name><surname>Wang</surname><given-names>Chaohui</given-names></name><name><surname>Batmanghelich</surname><given-names>Kayhan</given-names></name><name><surname>Tao</surname><given-names>Dacheng</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Mve-a multi-view reconstruction envi- ronment</article-title><source>GCH</source><year>2014</year><fpage>11</fpage><lpage>18</lpage><person-group person-group-type="author"><name><surname>Fuhrmann</surname><given-names>Simon</given-names></name><name><surname>Langguth</surname><given-names>Fabian</given-names></name><name><surname>Goesele</surname><given-names>Michael</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><source>IEEE computer society conference on computer vision and pattern recognition</source><year>2010</year><fpage>1434</fpage><lpage>1441</lpage><person-group person-group-type="author"><name><surname>Furukawa</surname><given-names>Yasutaka</given-names></name><name><surname>Curless</surname><given-names>Brian</given-names></name><name><surname>Steven</surname><given-names>M</given-names></name><name><surname>Seitz</surname><given-names>Richard</given-names></name><name><surname>Szeliski</surname><given-names /></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><source>European Conference on Computer Vision (ECCV)</source><year>2016</year><fpage>740</fpage><lpage>756</lpage><person-group person-group-type="author"><name><surname>Garg</surname><given-names>Ravi</given-names></name><name><surname>Kumar</surname><given-names>Vijay</given-names></name><name><surname>Carneiro</surname><given-names>Gustavo</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Improving the efficiency of hierarchi- cal structure-and-motion</article-title><source>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</source><year>2010</year><fpage>1594</fpage><lpage>1600</lpage><person-group person-group-type="author"><name><surname>Gherardi</surname><given-names>Riccardo</given-names></name><name><surname>Farenzena</surname><given-names>Michela</given-names></name><name><surname>Fusiello</surname><given-names>Andrea</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Matchnet: Unifying feature and metric learning for patch-based matching</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2015</year><fpage>3279</fpage><lpage>3286</lpage><person-group person-group-type="author"><name><surname>Han</surname><given-names>Xufeng</given-names></name><name><surname>Leung</surname><given-names>Thomas</given-names></name><name><surname>Jia</surname><given-names>Yangqing</given-names></name><name><surname>Sukthankar</surname><given-names>Rahul</given-names></name><name><surname>Berg</surname><given-names>Alexander C</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>In defense of the eight-point algorithm</article-title><source>IEEE Transactions on pattern analysis and machine intelligence</source><year>1997</year><volume>19</volume><issue>6</issue><fpage>580</fpage><lpage>593</lpage><person-group person-group-type="author"><name><surname>Hartley</surname><given-names>Richard I</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Accurate and efficient stereo processing by semi-global matching and mu- tual information</article-title><source>IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)</source><year>2005</year><volume>2</volume><fpage>807</fpage><lpage>814</lpage><person-group person-group-type="author"><name><surname>Hirschmuller</surname><given-names>Heiko</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><source>International Conference on Artificial Neural Networks</source><year>2001</year><fpage>87</fpage><lpage>94</lpage><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>Sepp</given-names></name><name><surname>Younger</surname><given-names>Steven</given-names></name><name><surname>Conwell</surname><given-names>Peter R</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Dpsnet: End-to-end deep plane sweep stereo</article-title><source>International Conference on Learning Representations</source><year>2019</year><person-group person-group-type="author"><name><surname>Im</surname><given-names>Sunghoon</given-names></name><name><surname>Jeon</surname><given-names>Hae-Gon</given-names></name><name><surname>Lin</surname><given-names>Stephen</given-names></name><name><surname>Kweon</surname><given-names>In So</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Spatial pyramid pooling in deep convo- lutional networks for visual recognition</article-title><source>European Conference on Computer Vision (ECCV)</source><year>2014</year><person-group person-group-type="author"><name><surname>Kaiming</surname><given-names>He</given-names></name><name><surname>Xiangyu</surname><given-names>Zhang</given-names></name><name><surname>Shaoqing</surname><given-names>Ren</given-names></name><name><surname>Sun</surname><given-names>Jian</given-names></name></person-group></element-citation></ref></ref-list></back></article>