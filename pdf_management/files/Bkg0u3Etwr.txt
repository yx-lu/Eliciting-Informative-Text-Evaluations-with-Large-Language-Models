Title:
```
Published as a conference paper at ICLR 2020 MAXMIN Q-LEARNING: CONTROLLING THE ESTIMATION BIAS OF Q-LEARNING
```
Abstract:
```
Q-learning suffers from overestimation bias, because it approximates the maximum action value using the maximum estimated action value. Algorithms have been proposed to reduce overestimation bias, but we lack an understanding of how bias interacts with performance, and the extent to which existing algorithms mitigate bias. In this paper, we 1) highlight that the effect of overestimation bias on learning efficiency is environment-dependent; 2) propose a generalization of Q-learning, called Maxmin Q-learning, which provides a parameter to flexibly control bias; 3) show theoretically that there exists a parameter choice for Maxmin Q-learning that leads to unbiased estimation with a lower approximation variance than Q-learning; and 4) prove the convergence of our algorithm in the tabular case, as well as convergence of several previous Q-learning variants, using a novel Generalized Q-learning framework. We empirically verify that our algorithm better controls estimation bias in toy environments, and that it achieves superior performance on several benchmark problems. 1 1 Code is available at https://github.com/qlan3/Explorer
```

Figures/Tables Captions:
```
Figure 1: A simple episodic MDP, adapted from Figure 6.5 in Sutton & Barto (2018) which is used to highlight the difference between Double Q-learning and Q-learning. This MDP has two non- terminal states A and B. Every episode starts from A which has two actions: Left and Right. The Right action transitions to a terminal state with reward 0. The Left action transitions to state B with reward 0. From state B, there are 8 actions that all transition to a terminal state with a reward µ + ξ, where ξ is drawn from a uniform distribution U (−1, 1). When µ > 0, the optimal action in state A is Left; when µ < 0, it is Right.
Figure 2: Comparison of three algorithms using the simple MDP in Figure 1 with different values of µ, and thus different expected rewards. For µ = +0.1, shown in (a), the optimal -greedy policy is to take the Left action with 95% probability. For µ = −0.1, shown in in (b), the optimal policy is to take the Left action with 5% probability. The reported distance is the absolute difference between the probability of taking the Left action under the learned policy compared to the optimal -greedy policy. All results were averaged over 5, 000 runs.
Figure 3: Comparison of four algorithms on Mountain Car under different reward variances. The lines in (a) show the average number of steps taken in the last episode with one standard error. The lines in (b) show the number of steps to reach the goal position during training when the reward variance σ 2 = 10. All results were averaged across 100 runs, with standard errors. Additional experiments with further elevated σ 2 can be found in Appendix C.2.
Figure 4: Learning curves on the seven benchmark environments. The depicted return is averaged over the last 100 episodes, and the curves are smoothed using an exponential average, to match previous reported results (Young & Tian, 2019). The results were averaged over 20 runs, with the shaded area representing one standard error. Plots (h) and (i) show the performance of Maxmin DQN on Pixelcopter and Asterix, with different N , highlighting that larger N seems to result in slower early learning but better final performance in both environments.
 
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Q-learning ( Watkins, 1989 ) is one of the most popular reinforcement learning algorithms. One of the reasons for this widespread adoption is the simplicity of the update. On each step, the agent updates its action value estimates towards the observed reward and the estimated value of the maximal action in the next state. This target represents the highest value the agent thinks it could obtain from the current state and action, given the observed reward. Unfortunately, this simple update rule has been shown to suffer from overestimation bias ( Thrun & Schwartz, 1993 ;  van Hasselt, 2010 ). The agent updates with the maximum over action values might be large because an action's value actually is high, or it can be misleadingly high simply because of the stochasticity or errors in the estimator. With many actions, there is a higher probability that one of the estimates is large simply due to stochasticity and the agent will overestimate the value. This issue is particularly problematic under function approximation, and can significant impede the quality of the learned policy ( Thrun & Schwartz, 1993 ;  Szita & Lőrincz, 2008 ;  Strehl et al., 2009 ) or even lead to failures of Q-learning ( Thrun & Schwartz, 1993 ). More recently, experiments across several domains suggest that this overestimation problem is common (Hado van Hasselt et al., 2016). Double Q-learning ( van Hasselt, 2010 ) is introduced to instead ensure underestimation bias. The idea is to maintain two unbiased independent estimators of the action values. The expected action value of estimator one is selected for the maximal action from estimator two, which is guaranteed not to overestimate the true maximum action value. Double DQN (Hado van Hasselt et al., 2016), the extension of this idea to Q-learning with neural networks, has been shown to significantly improve performance over Q-learning. However, this is not a complete answer to this problem, because trading overestimation bias for underestimation bias is not always desirable, as we show in our experiments.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 Several other methods have been introduced to reduce overestimation bias, without fully moving towards underestimation. Weighted Double Q-learning ( Zhang et al., 2017 ) uses a weighted combination of the Double Q-learning estimate, which likely has underestimation bias, and the Q-learning estimate, which likely has overestimation bias. Bias-corrected Q-Learning ( Lee et al., 2013 ) reduces the overestimation bias through a bias correction term. Ensemble Q-learning and Averaged Q-learning ( Anschel et al., 2017 ) take averages of multiple action values, to both reduce the overestimation bias and the estimation variance. However, with a finite number of action- value functions, the average operation in these two algorithms will never completely remove the overestimation bias, as the average of several overestimation biases is always positive. Further, these strategies do not guide how strongly we should correct for overestimation bias, nor how to determine-or control-the level of bias. The overestimation bias also appears in the actor-critic setting (Fujimoto et al., 2018;  Haarnoja et al., 2018 ). For example,  Fujimoto et al. (2018)  propose the Twin Delayed Deep Deterministic policy gradient algorithm (TD3) which reduces the overestimation bias by taking the minimum value between two critics. However, they do not provide a rigorous theoretical analysis for the effect of applying the minimum operator. There is also no theoretical guide for choosing the number of estimators such that the overestimation bias can be reduced to 0. In this paper, we study the effects of overestimation and underestimation bias on learning performance, and use them to motivate a generalization of Q-learning called Maxmin Q-learning. Maxmin Q-learning directly mitigates the overestimation bias by using a minimization over multiple action-value estimates. Moreover, it is able to control the estimation bias varying from positive to negative which helps improve learning efficiency as we will show in next sections. We prove that, theoretically, with an appropriate number of action-value estimators, we are able to acquire an unbiased estimator with a lower approximation variance than Q-learning. We empirically verify our claims on several benchmarks. We study the convergence properties of our algorithm within a novel Generalized Q-learning framework, which is suitable for studying several of the recently proposed Q-learning variants. We also combine deep neural networks with Maxmin Q-learning (Maxmin DQN) and demonstrate its effectiveness in several benchmark domains.

Section Title: PROBLEM SETTING
  PROBLEM SETTING We formalize the problem as a Markov Decision Process (MDP), (S, A, P, r, γ), where S is the state space, A is the action space, P : S ×A×S → [0, 1] is the transition probabilities, r : S ×A×S → R is the reward mapping, and γ ∈ [0, 1] is the discount factor. At each time step t, the agent observes a state S t ∈ S and takes an action A t ∈ A and then transitions to a new state S t+1 ∈ S according to the transition probabilities P and receives a scalar reward R t+1 = r(S t , A t , S t+1 ) ∈ R. The goal of the agent is to find a policy π : S × A → [0, 1] that maximizes the expected return starting from some initial state. Q-learning is an off-policy algorithm which attempts to learn the state-action values Q : S × A → R for the optimal policy. It tries to solve for The optimal policy is to act greedily with respect to these action values: from each s select a from arg max a∈A Q * (s, a). The update rule for an approximation Q for a sampled transition s t , a t , r t+1 , s t+1 is: where α is the step-size. The transition can be generated off-policy, from any behaviour that sufficiently covers the state space. This algorithm is known to converge in the tabular setting (Tsitsiklis, 1994), with some limited results for the function approximation setting ( Melo & Ribeiro, 2007 ).

Section Title: UNDERSTANDING WHEN OVERESTIMATION BIAS HELPS AND HURTS
  UNDERSTANDING WHEN OVERESTIMATION BIAS HELPS AND HURTS In this section, we briefly discuss the estimation bias issue, and empirically show that both overestimation and underestimation bias may improve learning performance, depending on the Published as a conference paper at ICLR 2020 environment. This motivates our Maxmin Q-learning algorithm described in the next section, which allows us to flexibly control the estimation bias and reduce the estimation variance. The overestimation bias occurs since the target max a ∈A Q(s t+1 , a ) is used in the Q-learning update. Because Q is an approximation, it is probable that the approximation is higher than the true value for one or more of the actions. The maximum over these estimators, then, is likely to be skewed towards an overestimate. For example, even unbiased estimates Q(s t+1 , a ) for all a , will vary due to stochasticity. Q(s t+1 , a ) = Q * (s t+1 , a ) + e a , and for some actions, e a will be positive. As a result, E[max a ∈A Q(s t+1 , a )] ≥ max a ∈A E[Q(s t+1 , a )] = max a ∈A Q * (s t+1 , a ). This overestimation bias, however, may not always be detrimental. And, further, in some cases, erring towards an underestimation bias can be harmful. Overestimation bias can help encourage exploration for overestimated actions, whereas underestimation bias might discourage exploration. In particular, we expect more overestimation bias in highly stochastic areas of the world; if those highly stochastic areas correspond to high-value regions, then encouraging exploration there might be beneficial. An underestimation bias might actually prevent an agent from learning that a region is high-value. Alternatively, if highly stochastic areas also have low values, overestimation bias might cause an agent to over-explore a low-value region. We show this effect in the simple MDP, shown in  Figure 1 . The MDP for state A has only two actions: Left and Right. It has a deterministic neutral reward for both the Left action and the Right action. The Left action transitions to state B where there are eight actions transitions to a terminate state with a highly stochastic reward. The mean of this stochastic reward is µ. By selecting µ > 0, the stochastic region becomes high-value, and we expect overestimation bias to help and underestimation bias to hurt. By selecting µ < 0, the stochastic region becomes low-value, and we expect overestimation bias to hurt and underestimation bias to help. We test Q-learning, Double Q-learning and our new algorithm Maxmin Q-learning in this environment. Maxmin Q-learning (described fully in the next section) uses N estimates of the action values in the targets. For N = 1, it corresponds to Q-learning; otherwise, it progresses from overestimation bias at N = 1 towards underestimation bias with increasing N . In the experiment, we used a discount factor γ = 1; a replay buffer with size 100; an -greedy behaviour with = 0.1; tabular action-values, initialized with a Gaussian distribution N (0, 0.01); and a step-size of 0.01 for all algorithms. The results in  Figure 2  verify our hypotheses for when overestimation and underestimation bias help and hurt. Double Q-learning underestimates too much for µ = +1, and converges to a suboptimal policy. Q-learning learns the optimal policy the fastest, though for all values of N = 2, 4, 6, 8, Maxmin Q-learning does progress towards the optimal policy. All methods get to the optimal policy for µ = −1, but now Double Q-learning reaches the optimal policy the fastest, and followed by Maxmin Q-learning with larger N .

Section Title: MAXMIN Q-LEARNING
  MAXMIN Q-LEARNING In this section, we develop Maxmin Q-learning, a simple generalization of Q-learning designed to control the estimation bias, as well as reduce the estimation variance of action values. The idea is Published as a conference paper at ICLR 2020 to maintain N estimates of the action values, Q i , and use the minimum of these estimates in the Q-learning target: max a min i∈{1,...,N } Q i (s , a ). For N = 1, the update is simply Q-learning, and so likely has overestimation bias. As N increase, the overestimation decreases; for some N > 1, this maxmin estimator switches from an overestimate, in expectation, to an underestimate. We characterize the relationship between N and the expected estimation bias below in Theorem 1. Note that Maxmin Q-learning uses a different mechanism to reduce overestimation bias than Double Q- learning; Maxmin Q-learning with N = 2 is not Double Q-learning. The full algorithm is summarized in Algorithm 1, and is a simple modification of Q-learning with experience replay. We use random subsamples of the observed data for each of the N estimators, to make them nearly independent. To do this training online, we keep a replay buffer. On each step, a random estimator i is chosen and updated using a mini-batch from the buffer. Multiple such updates can be performed on each step, just like in experience replay, meaning multiple estimators can be updated per step using different random mini-batches. In our experiments, to better match DQN, we simply do one update per step. Finally, it is also straightforward to incorporate target networks to get Maxmin DQN, by maintaining a target network for each estimator. We now characterize the relation between the number of action-value functions used in Maxmin Q-learning and the estimation bias of action values. For compactness, we write Q i sa instead of Q i (s, a). Each Q i sa has random approximation error e i sa Q i sa = Q * sa + e i sa . We assume that e i sa is a uniform random variable U (−τ, τ ) for some τ > 0. The uniform random assumption was used by  Thrun & Schwartz (1993)  to demonstrate bias in Q-learning, and reflects that non-negligible positive and negative e i sa are possible. Notice that for N estimators with n sa samples, the τ will be proportional to some function of n sa /N , because the data will be shared amongst the N estimators. For the general theorem, we use a generic τ , and in the following corollary provide a specific form for τ in terms of N and n sa . Recall that M is the number of actions applicable at state s . Define the estimation bias Z M N for transition s, a, r, s to be We now show how the expected estimation bias E[Z M N ] and the variance of Q min sa are related to the number of action-value functions N in Maxmin Q-learning. Theorem 1 is a generalization of the first lemma in  Thrun & Schwartz (1993) ; we provide the proof in Appendix A as well as a visualization of the expected bias for varying M and N . This theorem shows that the average estimation bias E[Z M N ], decreases as N increases. Thus, we can control the bias by changing the number of estimators in Maxmin Q-learning. Specifically, the average estimation bias can be reduced from positive to negative as N increases. Notice that E[Z M N ] = 0 when t M N = 1 2 . This suggests that by choosing N such that t M N ≈ 1 2 , we can reduce the bias to near 0. Furthermore, V ar[Q min sa ] decreases as N increases. This indicates that we can control the estimation variance of target action value through N . We show just this in the following Corollary. The subtlety is that with increasing N , each estimator will receive less data. The fair comparison is to compare the variance of a single estimator that uses all of the data, as compared to the maxmin estimator which shares the samples across N estimators. We show that there is an N such that the variance is lower, which arises largely due to the fact that the variance of each estimator decreases linearly in n, but the τ parameter for each estimator only decreases at a square root rate in the number of samples.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we first investigate robustness to reward variance, in a simple environment (Mountain Car) in which we can perform more exhaustive experiments. Then, we investigate performance in seven benchmark environments. Robustness under increasing reward variance in Mountain Car Mountain Car ( Sutton & Barto, 2018 ) is a classic testbed in Reinforcement Learning, where the agent receives a reward of −1 per step with γ = 1, until the car reaches the goal position and the episode ends. In our experiment, we modify the rewards to be stochastic with the same mean value: the reward signal is sampled from a Gaussian distribution N (−1, σ 2 ) on each time step. An agent should learn to reach the goal position in as few steps as possible. The experimental setup is as follows. We trained each algorithm with 1, 000 episodes. The number of steps to reach the goal position in the last training episode was used as the performance measure. The fewer steps, the better performance. All experimental results were averaged over 100 runs. The key algorithm settings included the function approximator, step-sizes, exploration parameter and replay buffer size. All algorithm used -greedy with = 0.1 and a buffer size of 100. For each algorithm, the best step-size was chosen from {0.005, 0.01, 0.02, 0.04, 0.08}, separately for each reward setting. Tile-coding was used to approximate the action-value function, where we used 8 tilings with each tile covering 1/8th of the bounded distance in each dimension. For Maxmin Q-learning, we randomly chose one action-value function to update at each step. As shown in  Figure 3 , when the reward variance is small, the performance of Q-learning, Double Q- learning, Averaged Q-learning, and Maxmin Q-learning are comparable. However, as the variance increases, Q-learning, Double Q-learning, and Averaged Q-learning became much less stable than Maxmin Q-learning. In fact, when the variance was very high (σ = 50, see Appendix C.2), Q- learning and Averaged Q-learning failed to reach the goal position in 5, 000 steps, and Double Q- learning produced runs > 400 steps, even after many episodes.

Section Title: Results on Benchmark Environments
  Results on Benchmark Environments To evaluate Maxmin DQN, we choose seven games from Gym ( Brockman et al., 2016 ), PyGame Learning Environment (PLE) ( Tasfi, 2016 ), and MinAtar ( Young & Tian, 2019 ): Lunarlander, Catcher, Pixelcopter, Asterix, Seaquest, Breakout, and Space Invaders. For games in MinAtar (i.e. Asterix, Seaquest, Breakout, and Space Invaders), we reused the hyper-parameters and settings of neural networks in ( Young & Tian, 2019 ). And the step-size was chosen from [3 * 10 −3 , 10 −3 , 3 * 10 −4 , 10 −4 , 3 * 10 −5 ]. For Lunarlander, Catcher, and Pixelcopter, the neural network was a multi-layer perceptron with hidden layers fixed to [64, 64]. The discount factor was 0.99. The size of the replay buffer was 10, 000. The weights of neural networks were optimized by RMSprop with gradient clip 5. The batch size was 32. The target network was updated every 200 frames. -greedy was applied as the exploration strategy with decreasing linearly from 1.0 to 0.01 in 1, 000 steps. After 1, 000 steps, was fixed to 0.01. For Lunarlander, the best step-size was chosen from [3 * 10 −3 , 10 −3 , 3 * 10 −4 , 10 −4 , 3 * 10 −5 ]. For Catcher and Pixelcopter, the best step-size was chosen from [10 −3 , 3 * 10 −4 , 10 −4 , 3 * 10 −5 , 10 −5 ]. For both Maxmin DQN and Averaged DQN, the number of target networks N was chosen from [2, 3, 4,  5 ,  6 , 7, 8, 9]. And we randomly chose one action-value function to update at each step. We first trained each algorithm in a game for certain number of steps. After that, each algorithm was tested by running 100 test episodes with -greedy where = 0.01. Results were averaged over 20 runs for each algorithm, with learning curves shown for the best hyper-parameter setting (see Appendix C.3 for the parameter sensitivity curves). We see from  Figure 4  that Maxmin DQN performs as well as or better than other algorithms. In environments where final performance is noticeably better--Pixelcopter, Lunarlander and Asterix-the initial learning is slower. A possible explanation for this is that the Maxmin agent more extensively explored early on, promoting better final performance. We additionally show on Pixelcopter and Asterix that for smaller N , Maxmin DQN learns faster but reaches suboptimal performance-behaving more like Q-learning-and for larger N learns more slowly but reaches better final performance.

Section Title: CONVERGENCE ANALYSIS OF MAXMIN Q-LEARNING
  CONVERGENCE ANALYSIS OF MAXMIN Q-LEARNING In this section, we show Maxmin Q-learning is convergent in the tabular setting. We do so by providing a more general result for what we call Generalized Q-learning: Q-learning where the bootstrap target uses a function G of N action values. The main condition on G is that it maintains relative maximum values, as stated in Assumption 1. We use this more general result to prove Maxmin Q-learning is convergent, and then discuss how it provides convergence results for Q- learning, Ensemble Q-learning, Averaged Q-learning and Historical Best Q-learning as special cases. Many variants of Q-learning have been proposed, including Double Q-learning ( van Hasselt, 2010 ), Weighted Double Q-learning ( Zhang et al., 2017 ), Ensemble Q-learning ( Anschel et al., 2017 ), Averaged Q-learning ( Anschel et al., 2017 ), and Historical Best Q-learning ( Yu et al., 2018 ). These algorithms differ in their estimate of the one-step bootstrap target. To encompass all variants, the target action-value of Generalized Q-learning Y GQ is defined based on action-value estimates from both dimensions: Y GQ = r + γQ GQ s (t − 1) (2) where t is the current time step and the action-value function Q GQ s (t) is a function of For simplicity, the vector (Q GQ sa (t)) a∈A is denoted as Q GQ s (t), same for Q i s (t). The corresponding update rule is We first introduce Assumption 1 for function G in Generalized Q-learning, and then state the theorem. The proof can be found in Appendix B. We can verify that Assumption 1 holds for Maxmin Q-learning. Set K = 1 and set N to be a positive integer. Let Q s = (Q 1 s , . . . , Q N s ) and define G M Q (Q s ) = max a∈A min i∈{1,...,N } Q i sa . It is easy to check that part (i) of Assumption 1 is satisfied. Part (ii) is also satisfied because

Section Title: CONCLUSION
  CONCLUSION Overestimation bias is a byproduct of Q-learning, stemming from the selection of a maximal value to estimate the expected maximal value. In practice, overestimation bias leads to poor performance in a variety of settings. Though multiple Q-learning variants have been proposed, Maxmin Q- learning is the first solution that allows for a flexible control of bias, allowing for overestimation or underestimation determined by the choice of N and the environment. We showed theoretically that we can decrease the estimation bias and the estimation variance by choosing an appropriate number N of action-value functions. We empirically showed that advantages of Maxmin Q-learning, both on toy problems where we investigated the effect of reward noise and on several benchmark environments. Finally, we introduced a new Generalized Q-learning framework which we used to prove the convergence of Maxmin Q-learning as well as several other Q-learning variants that use N action-value estimates.

```
