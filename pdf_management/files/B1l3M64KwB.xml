<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 HOW MANY WEIGHTS ARE ENOUGH : CAN TENSOR FACTORIZATION LEARN EFFICIENT POLICIES ?</article-title></title-group><abstract><p>Deep reinforcement learning requires a heavy price in terms of sample efficiency and overparameterization in the neural networks used for function approximation. In this work, we employ tensor factorization in order to learn more compact rep- resentations for reinforcement learning policies. We show empirically that in the low-data regime, it is possible to learn online policies with 2 to 10 times less to- tal coefficients, with little to no loss of performance. We also leverage progress in second order optimization, and use the theory of wavelet scattering to further reduce the number of learned coefficients, by foregoing learning the topmost con- volutional layer filters altogether. We evaluate our results on the Atari suite against recent baseline algorithms that represent the state-of-the-art in data efficiency, and get comparable results with an order of magnitude gain in weight parsimony.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>The successes of reinforcement learning (thereafter 'RL'), and specifically deep RL, come at a heavy computational price. It is well known that achieving human-level performance in domains such as Atari (<xref ref-type="bibr" rid="b32">Sutton &amp; Barto, 2018</xref>; <xref ref-type="bibr" rid="b10">Mnih et al., 2013</xref>; <xref ref-type="bibr" rid="b13">Hessel et al., 2017</xref>) requires hundreds of millions of frames of environment interaction. As such, the problem of sample efficiency in RL is of critical importance. Several tracks of concurrent research are being investigated, and have reduced by orders of magnitude the number of environment interactions required for good performance beyond the previous benchmark of biologically-inspired episodic control methods (<xref ref-type="bibr" rid="b2">Blundell et al., 2016</xref>; <xref ref-type="bibr" rid="b2">Pritzel et al., 2017</xref>) to a couple hours of human gameplay time (<xref ref-type="bibr" rid="b13">van Hasselt et al., 2019</xref>; <xref ref-type="bibr" rid="b17">Kaiser et al., 2019</xref>). However, while the data-efficiency of RL methods has seen recent drastic performance gains, the function approximators they use still require millions of learned weights, potentially still leaving them heavily overparameterized. Independently motivated by biological facts like the behavioural readiness of newborn animals, several authors (<xref ref-type="bibr" rid="b11">Gaier &amp; Ha, 2019</xref>; <xref ref-type="bibr" rid="b7">Cuccu et al., 2018</xref>; <xref ref-type="bibr" rid="b35">Wang et al., 2019</xref>) have recently looked at doing away with learning so many weights for RL tasks. Smaller networks not only train faster, but may yet offer another avenue for gains in the form of better gener- alization (<xref ref-type="bibr" rid="b1">Zhang et al., 2016</xref>). Recent work from <xref ref-type="bibr" rid="b11">Gaier &amp; Ha (2019)</xref> studies the effect of inductive bias of neural architectures in RL ; they forego training altogether, but transfer networks that only obtain 'better than chance performance on MNIST'. In similar fashion, <xref ref-type="bibr" rid="b35">Wang et al. (2019)</xref> investi- gate the effect of random projections in the restricted setting of imitation learning. Finally, <xref ref-type="bibr" rid="b7">Cuccu et al. (2018)</xref> manage human-level performance on the Atari suite using a separate dictionary learning procedure for their features, bypassing the usual end-to-end learning paradigm. The perspective of neural architecture search applied to RL appears difficult, if not computationally inextricable. Concurrently, the study of biologically-inspired models of learning has exhibited two mathematical characterizations that might be critical in explaining how biological learning takes place so effi- ciently. First, the low-rank properties of learned perceptual manifolds (<xref ref-type="bibr" rid="b4">Chung et al., 2018</xref>; <xref ref-type="bibr" rid="b2">2016</xref>) are giving rise to a rich theory borrowing from statistical physics. Second, another well known line of work has identified Gabor filters, and more generally wavelet filter-like structures, in the actual visual cortex of animals (<xref ref-type="bibr" rid="b16">Jones &amp; Palmer, 1987</xref>), and linked those to sparsity-promoting methods and dictionary learning (<xref ref-type="bibr" rid="b25">Olshausen &amp; Field, 1996</xref>; <xref ref-type="bibr" rid="b26">1997</xref>; <xref ref-type="bibr" rid="b14">Hyv&#228;rinen &amp; Hoyer, 2001</xref>). But these breakthroughs have not, so far, been reflected as inductive priors in the shape of modifications in deep RL neural networks architectures, which remain fairly fixed on the Atari domain. Therefore the following questions remain: how parsimonious do function approximators in RL need to be, in order to maintain good performance? And can we be at once sample-efficient and weight- efficient ? In this work, we turn to the mathematical theories of tensor factorization (<xref ref-type="bibr" rid="b6">Cichocki et al., 2009</xref>), second-order optimization (<xref ref-type="bibr" rid="b0">Amari, 1998</xref>; <xref ref-type="bibr" rid="b23">Martens &amp; Grosse, 2015</xref>) and wavelet scattering (<xref ref-type="bibr" rid="b1">Mallat, 2011</xref>) to answer this question positively and empirically, in a model-free setting. We propose to use these methods in order to save weights and therefore favour convergence of policies:</p><p>&#8226; We replace dense, fully-connected layers with tensor regression layers.</p><p>&#8226; Optionally, we replace the topmost layer in the convolutional architecture with a scattering layer; the deeper convolutional layers are left untouched.</p><p>&#8226; The (positive) impact of second-order optimization is also evaluated.</p><p>To the best of our knowledge, this is the first time those fields have been combined together in this context, and that tensor factorization is applied to deep RL.</p></sec><sec><title>BACKGROUND &amp; RELATED WORK</title></sec><sec><title>DEEP REINFORCEMENT LEARNING</title><p>We consider the standard Markov Decision Process framework as in <xref ref-type="bibr" rid="b32">Sutton &amp; Barto (2018)</xref>. This setting is characterised by a tuple S, A, T, R, &#947; , where S is a set of states, A a set of actions, R a reward function that is the immediate, intrinsic desirability of a certain state, T a transition dynamics and &#947; &#8712; [0, 1] a discount factor. The purpose of the RL problem is to to find a policy &#960;, which represents a mapping from states to a probability distribution over actions, that is optimal, i.e., that maximizes the expected cumulative discounted return &#8734; k=0 &#947; k R t+k+1 at each state s t &#8712; S.</p><p>In Q-learning, the policy is given implicitly by acting greedily or -greedily with respect to learned action-value functions q &#960; (s, a), that are learned following the Bellman equation. In deep Q-learning, q &#952; becomes parameterized by the weights &#952; of a neural network and one minimizes the expected Bellman loss :</p><p>In practice, this is implemented stochastically via uniform sampling of transitions in an experience replay buffer, as is done in the seminal paper <xref ref-type="bibr" rid="b10">Mnih et al. (2013)</xref>. Several algorithmic refinements to that approach exist. First, Double Q-learning (<xref ref-type="bibr" rid="b13">van Hasselt et al., 2015</xref>) proposes to decouple learning between two networks in order to alleviate the Q-value overestimation problem. Second, dueling Q-networks (<xref ref-type="bibr" rid="b35">Wang et al., 2015</xref>) explicitly decompose the learning of an action-value func- tion q &#952; (s, a) as the sum of an action-independent state-value, much like what is traditionally done in policy gradient methods (<xref ref-type="bibr" rid="b32">Sutton &amp; Barto, 2018</xref>), implemented via a two-headed neural network architecture. Finally, prioritized RL (<xref ref-type="bibr" rid="b13">Schaul et al., 2015</xref>) proposes to replace the uniform sampling of transitions in the experience replay buffer with importance sampling, by prioritizing those transi- tions that present the most Bellman error (those transitions that are deemed the most 'surprising' by the agent). <xref ref-type="bibr" rid="b10">Fortunato et al. (2017)</xref> use extra weights to learn the variance of the exploration noise in a granular fashion, while <xref ref-type="bibr" rid="b1">Bellemare et al. (2017)</xref> propose to learn a full distribution of action-values for each action and state. Combined, those methods form the basis of the Rainbow algorithm in <xref ref-type="bibr" rid="b13">Hessel et al. (2017)</xref>.</p></sec><sec><title>TENSOR FACTORIZATION</title><p>Here we introduce notations and concepts from the tensor factorization literature. An intuition is that the two main decompositions below, CP and Tucker decompositions, can be understood as multilinear algebra analogues of SVD or eigendecomposition.</p></sec><sec><title>CP decomposition</title><p>A tensor X &#8712; R I1&#215;I2&#215;&#183;&#183;&#183;&#215;I N , can be decomposed into a sum of R rank-1 tensors, known as the Canonical-Polyadic decomposition, where R is the rank of the decomposition. Its purpose is to find vectors u (1) k , u (2) k , &#183; &#183; &#183; , u (N ) k , for k = [1 . . . R], as well as a vector of weights Under review as a conference paper at ICLR 2020 &#955; &#8712; R R such that:</p><p>Tucker decomposition. A tensor X &#8712; R I1&#215;I2&#215;&#183;&#183;&#183;&#215;I N , can be decomposed into a low rank approx- imation including a core G &#8712; R R1&#215;R2&#215;&#183;&#183;&#183;&#215;R N and a set of projection factors U (0) , &#183; &#183; &#183; , U (N &#8722;1) , with U (k) &#8712; R R k ,&#206; k , k &#8712; (0, &#183; &#183; &#183; , N &#8722; 1) that, when projected along the corresponding dimension of the core, reconstruct the full tensor X . The tensor in its decomposed form can be written:</p><p>Tensor regression layer. For two tensors X &#8712; R K1&#215;&#183;&#183;&#183;&#215;Kx&#215;I1&#215;&#183;&#183;&#183;&#215;I N and Y &#8712; R I1&#215;&#183;&#183;&#183;&#215;I N &#215;L1&#215;&#183;&#183;&#183;&#215;Ly , we denote by X , Y N &#8712; R K1&#215;&#183;&#183;&#183;&#215;Kx&#215;L1&#215;&#183;&#183;&#183;&#215;Ly the contraction of X by Y along their N last modes; their generalized inner product is</p><p>This makes it possible to define a tensor regression layer (<xref ref-type="bibr" rid="b18">Kossaifi et al., 2017b</xref>) that is differentiable and learnable end-to-end by gradient descent. Let us denote by X &#8712; R I1&#215;I2&#215;&#183;&#183;&#183;&#215;I N the input activation tensor for a sample and y &#8712; R I N the label vector. A tensor regression layer estimates the regression weight tensor W &#8712; R I1&#215;I2&#215;&#183;&#183;&#183;&#215;I N under a low-rank decomposition. In the case of a Tucker decomposition (as per our experiments) with ranks (R 1 , &#183; &#183; &#183; , R N ), we have :</p></sec><sec><title>WAVELET SCATTERING</title><p>The wavelet scattering transform was originally introduced by <xref ref-type="bibr" rid="b1">Mallat (2011)</xref> and <xref ref-type="bibr" rid="b1">Bruna &amp; Mallat (2012)</xref> as a non-linear extension to the classical wavelet filter bank decomposition. Its principle is as follows. Denoting by x y[n] the 2-dimensional, circular convolution of two signals x[n] and y[n], let us assume that we have pre-defined two wavelet filter banks available &#182; &#968; (1) &#955;1 [n] &#169; &#955;1&#8712;&#923;1 &#182; &#968; (2) &#955;2 [n] &#169; &#955;2&#8712;&#923;2 , with &#955; 1 and &#955; 2 two frequency indices. These wavelet filters correspond to high frequencies, so we also give ourselves the data of a lowpass filter &#966; J [n]. Finally, and by opposition to traditional linear wavelet transforms, we also assume a given nonlinearity &#961;(t). Then the scattering transform is given by coefficients of order 0,1, and 2, respectively :</p><p>This can effectively be understood and implemented as a two-layer convolutional neural network whose weights are not learned but rather frozen and given by the coefficients of wavelets &#968; and &#966; (with Gabor filters as a special case (<xref ref-type="bibr" rid="b1">Mallat, 1998</xref>)). The difference with traditional filter banks comes from the iterated modulus/nonlinear activation function applied at each stage, much like in traditional deep learning convolutional neural networks. The generic mathematical definition involves order n iterated scatterings, in the vein of S i x above, but sometimes restricts nonlinearity &#961; to be a modulus function | &#183; |. In practice, the potential of scattering transforms to accelerate learning by providing ready-made convolutional layers has been investigated in <xref ref-type="bibr" rid="b1">Oyallon et al. (2013)</xref> and <xref ref-type="bibr" rid="b1">Oyallon et al. (2018)</xref> and is a subject of active ongoing research. Scattering will be the second of our weight-saving methods.</p></sec><sec><title>SECOND ORDER OPTIMIZATION WITH K-FAC</title><p>While stochastic gradient descent is usually performed purely from gradient observations derived from auto-differentiation, faster, second order optimization methods first multiply the weights' &#952; gradient vector &#8711; &#952; by a preconditioning matrix, yielding the weight update &#952; n+1 &#8592; &#952; n &#8722; &#951;G &#8722;1 n &#8711; &#952; , with &#951; a step size. In the case of second order methods, the matrix G &#8722;1 n is chosen to act as a tractable iterative approximation to the inverse Hessian or Empirical Fisher Information Matrix (<xref ref-type="bibr" rid="b0">Amari, 1998</xref>) of the neural network model in question. Kronecker-factored approximate curvature or K-FAC (<xref ref-type="bibr" rid="b23">Martens &amp; Grosse, 2015</xref>) enforces a Kronecker decomposition of the type G = A &#8855; B, with A and B being smaller, architecture-dependent matrices. Unlike the above methods, K-FAC has been applied as a plug-in in the deep RL literature and shown to promote both anytime convergence properties as well as terminal accuracies (<xref ref-type="bibr" rid="b37">Wu et al., 2017</xref>).</p></sec><sec><title>OBSERVATIONS AND METHODS</title></sec><sec><title>EXPLORING TRAINED AGENTS</title><p>Stability of trained dense layers eigenvalues. In order to assess experimentally if tensor factor- ization can make sense in RL, we investigate the eigenvalues of the dense layers of a deep RL agent. Unlike the traditional supervised learning setting, the input data distribution to RL function approximators shifts as the agent explores its environment; as such, concentration properties of the eigenvalues of the linear layers cannot be guaranteed all the way throughout training. Since condi- tioning techniques such as batch normalization (<xref ref-type="bibr" rid="b15">Ioffe &amp; Szegedy, 2015</xref>) are rarely used in deep RL, this is all the more important. Our experiments (see <xref ref-type="fig" rid="fig_0">figure 1</xref>) show that the distribution of eigen- values does not seem to widen significantly, at least during the initial phases of training we care about. Furthermore and interestingly, it does not seem to deviate significantly from the one observed at initialization. All together, this suggests there might be some merit in learning low-rank policies.</p></sec><sec><title>ARCHITECTURAL MODIFICATIONS</title></sec><sec><title>Baseline</title><p>We then proceed to build upon the well-known Rainbow algorithm (<xref ref-type="bibr" rid="b13">Hessel et al., 2017</xref>). Rainbow uses a fairly standard shallow convolutional architecture like the seminal DQN paper of <xref ref-type="bibr" rid="b10">Mnih et al. (2013)</xref>, and is an oft-cited baseline on the Atari suite. In spite of its performance, Rain- bow often requires dozens of millions of a single game's frames in order to perform well. Very re- cently, a 'data-efficient' efficient version of Rainbow has been proposed by <xref ref-type="bibr" rid="b13">van Hasselt et al. (2019)</xref>, with a view to match or beat the latest state-of-the-art results achieved by model-based RL methods. This is achieved with no major change in network architecture, but via a selection of mildly hand- tuned hyperparameters favouring efficiency against wall-clock running time (see appendix). We do take this as a baseline. Changes. We modify the architecture of the neural network function approximators used, in accor- dance with the principles described above, combining them to reflect inductive biases promoting fewer learnable parameters:</p><p>&#8226; We replace the fully-connected, linear layers used in Rainbow and data-efficient Rainbow with tensor regression layers (<xref ref-type="bibr" rid="b18">Kossaifi et al., 2017b</xref>) in order to learn low-rank policies (ranks in appendix).</p><p>&#8226; We use either the K-FAC second order stochastic optimizer, or the standard ADAM opti- mizer (<xref ref-type="bibr" rid="b17">Kingma &amp; Ba, 2014</xref>). Optimization with K-FAC yields better results ceteris paribus and therefore works to counter performance loss due to using fewer weights.</p><p>&#8226; We combine the two methods with various rank (and therefore weight compression) ratios, targetting sensible compression ratios guided by deep learning intuition; and evaluate those on the same subset of Atari games as both <xref ref-type="bibr" rid="b13">van Hasselt et al. (2019)</xref>; <xref ref-type="bibr" rid="b17">Kaiser et al. (2019)</xref>.</p><p>&#8226; When possible, we replace the first convolutional layer in the approximating neural network with a scattering layer for further gains in terms of learnable weights. In that way, we inves- tigate the impact of not actually learning one of the convolutional layer weights. Deep RL, paradoxically, tends to use shallow network architectures with two or three convolutional layers only, which makes it very fit for scattering methods. But since learned convolutional features discriminate well between high and low rewards, the promise of fully unsupervised scattering layers seems remote, without resorting to further ad-hoc methods like dictionary learning. Therefore we limit ourselves to one-step (one single layer) scatterings. This is illustrated in <xref ref-type="fig" rid="fig_1">figure 2</xref>. We then proceed to evaluate the merit of these changes.</p></sec><sec><title>EXPERIMENTAL RESULTS</title></sec><sec><title>PRIORITIZED TENSORIZED DQN</title><p>Proof of concept. We begin with showing proof of concept on the simple Pong Atari game. Our experimental setup consists in our own implementation of prioritized double DQN as a baseline (<xref ref-type="bibr" rid="b13">Schaul et al., 2015</xref>; <xref ref-type="bibr" rid="b13">van Hasselt et al., 2015</xref>). We replaced the densely connected layer of the original DQN architecture with a tensor regression layer implementing Tucker decomposition for different Tucker ranks, yielding different network compression factors.</p></sec><sec><title>Qualitative behaviour</title><p>First results, both in terms of learning performance and compression factor, can be seen in <xref ref-type="fig" rid="fig_2">figure 3</xref>. Our two main findings are that first, the final performance of the agent remains unaffected by the tensor factorization, even with high compression rates - with respect to all network weights - of five times. In line with intuition, larger compression rates do however cause more delays in learning. Second, tensor factorization negatively affects stability during training - in tough compression regimes, the plateauing phases of learning curves feature occasional noisy drawdowns, illustrating the increased difficulty of learning, as seen in <xref ref-type="fig" rid="fig_3">figure 4</xref>. Interestingly, approx- imation errors incurred by tensor regression noise do sometimes have poor consequences illustrated by those drawdowns, but overall seem to behave as additional exploration noise.</p></sec><sec><title>DATA-EFFICIENT RAINBOW ON ATARI</title></sec><sec><title>Evaluation protocol</title><p>For all our Atari experiments, we used OpenAI Gym (<xref ref-type="bibr" rid="b2">Brockman et al., 2016</xref>), and a combination of PyTorch (<xref ref-type="bibr" rid="b29">Paszke et al., 2017</xref>), TensorLy (<xref ref-type="bibr" rid="b18">Kossaifi et al., 2016</xref>) and Kymatio (<xref ref-type="bibr" rid="b1">Andreux et al., 2018</xref>) for auto-differentiation. We evaluated our agents in the low-data regime of Under review as a conference paper at ICLR 2020 100,000 steps, on half the games, with 3 different random seeds for reproducibility (<xref ref-type="bibr" rid="b12">Henderson et al., 2017</xref>), taking the data-efficient Rainbow agent (<xref ref-type="bibr" rid="b13">van Hasselt et al., 2019</xref>) as our baseline. Our specific hyperparameters are described in appendix. We report our results in <xref ref-type="table" rid="tab_0">tables 1</xref> and 2. <xref ref-type="table" rid="tab_0">Table 1</xref> shows proof of concept of the online learning of low-rank policies, with a loss of final performance varying in proportion to the compression in the low-rank linear layers, very much like in the deep learning literature (<xref ref-type="bibr" rid="b18">Kossaifi et al., 2017a</xref>;b). The number of coefficients in the original data-efficient Rainbow is of the order of magnitude of 1M and varies depending on the environment and its action-space size. The corresponding tensor regression layer ranks are in appendix, and chosen to target 400k, 200k and 100k coefficients respectively. While individual game results tend to decrease monotonously with increasing compression, we observe that they are noisy due to the nature of exploration in RL, and average scores reported correspond to the intuition that performance seems to decrease fast after a certain overparameterization threshold is crossed. To take this noisy character into account, we take care to be conservative and report the average of the final three episodes of the learned policy after 80k, 90k and 100k steps, respectively.</p></sec><sec><title>Denoised baseline</title><p>So as to not muddy the discussion and provide fair baselines, we do report on the NoisyNet (<xref ref-type="bibr" rid="b10">Fortunato et al., 2017</xref>) ablation of Rainbow ('Denoised' columns), as the NoisyLinear layer doubles up the number of coefficients required and actually performs worse in our experiments. Its principle is that the variance of exploration noise represents a criticial tradeoff for performance (too little and one stalls, too much and one risks catastrophic updates), so it is sensible to treat it as a parameter to learn. However, in order to decouple both factors of overparametrization and exploration in the discussion of deep RL performance, we simply use a fixed exploration schedule. This denoised exploration baseline is an ablation we can then compare our tensorized methods to. Under review as a conference paper at ICLR 2020 Second-order optimization. We then proceed to assess the impact of second-order optimization in architecture by substituting ADAM optimization for K-FAC, and introducing scattering, in <xref ref-type="table" rid="tab_1">table 2</xref>. In spite of our conservative reporting, the efficiency boost from using a second order scheme more than makes up for low-rank approximation error (109% performance) with five times less co- efficients than <xref ref-type="bibr" rid="b13">van Hasselt et al. (2019)</xref>, and learning with a full order of magnitude less coefficients (98% performance) is made possible by the combination of K-FAC and TRL. The results however do show a sharp drop in average performance when scattering is added. Interestingly enough some games perform relatively very well with that method, simultaneously showing proof of viability and of additional work required.</p></sec><sec><title>CONCLUSION</title><p>We have demonstrated that in the low-data regime, it is possible to leverage biologically plausible characterizations of experience data (namely low-rank properties and wavelet scattering separability) to exhibit architectures that learn policies with an order of magnitude less weights than current state-of-the-art baselines, essentially without loss of performance, and in an online fashion. In particular, this provides a compelling alternative to methods like policy distillation (<xref ref-type="bibr" rid="b31">Rusu et al., 2015</xref>; <xref ref-type="bibr" rid="b8">Czarnecki et al., 2019</xref>). We do hope that this will lead to even further progress towards sample efficiency and speedy exploration methods. Further work will, first, focus on thorough evaluation and research of scattering architectures in order to achieve further gains, and second investigate additional, orthogonal biologically-friendly research directions such as promoting sparsity via, for instance, L 1 regularization. Finally, we are excited by the potential of tensor factorization to offer shared core tensors for policies in multi-task and meta-learning.</p><p>Under review as a conference paper at ICLR 2020</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Eigenvalue histograms of the value-based linear layer during training. 50 agent runs data- efficient Rainbow (van Hasselt et al., 2019), of 100,000 steps on the Atari game Road Runner.</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Our architectural approach consists in replacing hidden layers in deep RL agents with tensor regression (top). Optionally we substitute the topmost convolutional layer with scattering (middle), and combine both methods (bottom).</p></caption><graphic /><graphic /><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Prioritized tensorized DQN on Atari Pong. Original learning curve versus several learning curves for five different Tucker ranks factorizations and therefore parameter compression rates (3 different random seeds each, with a 30 episodes moving average for legibility). Best viewed in colour.</p></caption><graphic /><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Focus on a typical single run of the tensorized DQN learning (score vs. number of thou- sand episodes). The overall shape of the typical learning curve is preserved, but drawdowns in the plateauing phase do appear.</p></caption><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Mean episode returns as reported in baselines SimPLe (Kaiser et al., 2019) and data-efficient Rainbow (van Hasselt et al., 2019), versus our agents, on 26 Atari games. 'Denoised' is the NoisyNet ablation of Rainbow; 'TRL' shows the performance of the data-efficient Rainbow with tensor regres- sion layers substituted for linear ones.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Mean episode returns of our low-rank agents with second-order optimization and scattering. The Scattering column also includes KFAC optimization and TRL 5x, resulting in around 10x total weights efficiency gains.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Natural gradient works efficiently in learning</article-title><source>Neural Computation</source><year>1998</year><volume>10</volume><fpage>251</fpage><lpage>276</lpage><person-group person-group-type="author"><name><surname>References Shunichi Amari</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Kymatio: Scattering Transforms in Python</article-title><source>arXiv e-prints</source><year>2018</year><person-group person-group-type="author"><name><surname>Andreux</surname><given-names>M</given-names></name><name><surname>Angles</surname><given-names>T</given-names></name><name><surname>Exarchakis</surname><given-names>G</given-names></name><name><surname>Leonarduzzi</surname><given-names>R</given-names></name><name><surname>Rochette</surname><given-names>G</given-names></name><name><surname>Thiry</surname><given-names>L</given-names></name><name><surname>Zarka</surname><given-names>J</given-names></name><name><surname>Mallat</surname><given-names>S</given-names></name><name><surname>And&#233;n</surname><given-names>J</given-names></name><name><surname>Belilovsky</surname><given-names>E</given-names></name><name><surname>Bruna</surname><given-names>J</given-names></name><name><surname>Lostanlen</surname><given-names>V</given-names></name><name><surname>Hirn</surname><given-names>M J</given-names></name><name><surname>Oyallon</surname><given-names>E</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Cella</surname><given-names>C</given-names></name><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Bellemare</surname><given-names>M G</given-names></name><name><surname>Dabney</surname><given-names>W</given-names></name><name><surname>Munos</surname><given-names>R</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><source>Model-Free Episodic Control. arXiv e-prints</source><year>2016</year><person-group person-group-type="author"><name><surname>Blundell</surname><given-names>C</given-names></name><name><surname>Uria</surname><given-names>B</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Ruderman</surname><given-names>A</given-names></name><name><surname>Leibo</surname><given-names>J</given-names></name><name><surname>Rae</surname><given-names>J</given-names></name><name><surname>Wierstra</surname><given-names>D</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Invariant Scattering Convolution Networks</article-title><source>arXiv e-prints, March</source><year>2012</year><person-group person-group-type="author"><name><surname>Bruna</surname><given-names>J</given-names></name><name><surname>Mallat</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><source>Linear readout of object manifolds</source><year>2016</year><volume>93</volume><issue>6</issue><fpage>060301</fpage><lpage>060301</lpage><person-group person-group-type="author"><name><surname>Chung</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>D D</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Classification and Geometry of General Perceptual Manifolds</article-title><source>Physical Review X</source><year>2018</year><volume>8</volume><issue>3</issue><fpage>031003</fpage><lpage>031003</lpage><person-group person-group-type="author"><name><surname>Chung</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>D D</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Nonnegative Matrix and Tensor Factorizations - Applications to Exploratory Multi-way Data Analysis and Blind Source Separa- tion</article-title><year>2009</year><person-group person-group-type="author"><name><surname>Cichocki</surname><given-names>Andrzej</given-names></name><name><surname>Zdunek</surname><given-names>Rafal</given-names></name><name><surname>Phan</surname><given-names>Anh Huy</given-names></name><name><surname>Amari</surname><given-names>Sh</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Playing Atari with Six Neurons</article-title><source>arXiv e-prints</source><year>2018</year><person-group person-group-type="author"><name><surname>Cuccu</surname><given-names>G</given-names></name><name><surname>Togelius</surname><given-names>J</given-names></name><name><surname>Cudre-Mauroux</surname><given-names>P</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Dis- tilling Policy Distillation</article-title><source>arXiv e-prints</source><year>2019</year><person-group person-group-type="author"><name><surname>Czarnecki</surname><given-names>W M</given-names></name><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Osindero</surname><given-names>S</given-names></name><name><surname>Jayakumar</surname><given-names>S M</given-names></name><name><surname>Swirszcz</surname><given-names>G</given-names></name><name><surname>Jaderberg</surname><given-names>M</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Solid harmonic wavelet scattering for predictions of molecule properties</article-title><year>2018</year><volume>148</volume><issue>24</issue><fpage>241732</fpage><lpage>241732</lpage><person-group person-group-type="author"><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Exarchakis</surname><given-names>G</given-names></name><name><surname>Hirn</surname><given-names>M</given-names></name><name><surname>Mallat</surname><given-names>S</given-names></name><name><surname>Thiry</surname><given-names>L</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Noisy Networks for Exploration</article-title><source>arXiv e-prints</source><year>2017</year><person-group person-group-type="author"><name><surname>Fortunato</surname><given-names>M</given-names></name><name><surname>Azar</surname><given-names>M</given-names></name><name><surname>Piot</surname><given-names>B</given-names></name><name><surname>Menick</surname><given-names>J</given-names></name><name><surname>Osband</surname><given-names>I</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Munos</surname><given-names>R</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Pietquin</surname><given-names>O</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name><name><surname>Legg</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Weight Agnostic Neural Networks</article-title><source>arXiv e-prints</source><year>2019</year><person-group person-group-type="author"><name><surname>Gaier</surname><given-names>A</given-names></name><name><surname>Ha</surname><given-names>D</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Deep Reinforcement Learning that Matters</article-title><source>arXiv e-prints</source><year>2017</year><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>P</given-names></name><name><surname>Islam</surname><given-names>R</given-names></name><name><surname>Bachman</surname><given-names>P</given-names></name><name><surname>Pineau</surname><given-names>J</given-names></name><name><surname>Precup</surname><given-names>D</given-names></name><name><surname>Meger</surname><given-names>D</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Rainbow: Combining Improvements in Deep Reinforcement Learning</article-title><source>arXiv e-prints, October</source><year>2017</year><person-group person-group-type="author"><name><surname>Hessel</surname><given-names>M</given-names></name><name><surname>Modayil</surname><given-names>J</given-names></name><name><surname>Van Hasselt</surname><given-names>H</given-names></name><name><surname>Schaul</surname><given-names>T</given-names></name><name><surname>Ostrovski</surname><given-names>G</given-names></name><name><surname>Dabney</surname><given-names>W</given-names></name><name><surname>Horgan</surname><given-names>D</given-names></name><name><surname>Piot</surname><given-names>B</given-names></name><name><surname>Azar</surname><given-names>M</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>A two-layer sparse coding model learns simple and complex cell receptive fields and topography from natural images</article-title><source>Vision Research</source><year>2001</year><volume>41</volume><fpage>2413</fpage><lpage>2423</lpage><person-group person-group-type="author"><name><surname>Hyv&#228;rinen</surname><given-names>Aapo</given-names></name><name><surname>Hoyer</surname><given-names>Patrik O</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</article-title><source>arXiv e-prints</source><year>2015</year><person-group person-group-type="author"><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>An evaluation of the two-dimensional gabor filter model of simple receptive fields in cat striate cortex</article-title><source>Journal of neurophysiology</source><year>1987</year><volume>6</volume><fpage>1233</fpage><lpage>1258</lpage><person-group person-group-type="author"><name><surname>Jeffrey</surname><given-names>P</given-names></name><name><surname>Jones</surname><given-names /></name><name><surname>Larry</surname><given-names>A</given-names></name><name><surname>Palmer</surname><given-names /></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Model- Based Reinforcement Learning for Atari. arXiv e-prints, March 2019. Under review as a conference paper at ICLR 2020 D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization</article-title><source>arXiv e-prints</source><year>2014</year><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>L</given-names></name><name><surname>Babaeizadeh</surname><given-names>M</given-names></name><name><surname>Milos</surname><given-names>P</given-names></name><name><surname>Osinski</surname><given-names>B</given-names></name><name><surname>Campbell</surname><given-names>R</given-names></name><name><surname>Czechowski</surname><given-names>K</given-names></name><name><surname>Erhan</surname><given-names>D</given-names></name><name><surname>Finn</surname><given-names>C</given-names></name><name><surname>Kozakowski</surname><given-names>P</given-names></name><name><surname>Levine</surname><given-names>S</given-names></name><name><surname>Mohiuddin</surname><given-names>A</given-names></name><name><surname>Sepassi</surname><given-names>R</given-names></name><name><surname>Tucker</surname><given-names>G</given-names></name><name><surname>Michalewski</surname><given-names>H</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>TensorLy: Tensor Learning in Python</article-title><source>arXiv e-prints, October</source><year>2016</year><person-group person-group-type="author"><name><surname>Kossaifi</surname><given-names>J</given-names></name><name><surname>Panagakis</surname><given-names>Y</given-names></name><name><surname>Anandkumar</surname><given-names>A</given-names></name><name><surname>Pantic</surname><given-names>M</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Tensor Contraction Layers for Parsimonious Deep Nets</article-title><source>arXiv e-prints</source><year>2017</year><person-group person-group-type="author"><name><surname>Kossaifi</surname><given-names>J</given-names></name><name><surname>Khanna</surname><given-names>A</given-names></name><name><surname>Lipton</surname><given-names>Z C</given-names></name><name><surname>Furlanello</surname><given-names>T</given-names></name><name><surname>Anandkumar</surname><given-names>A</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Tensor Regression Net- works</article-title><source>arXiv e-prints</source><year>2017</year><person-group person-group-type="author"><name><surname>Kossaifi</surname><given-names>J</given-names></name><name><surname>Lipton</surname><given-names>Z C</given-names></name><name><surname>Khanna</surname><given-names>A</given-names></name><name><surname>Furlanello</surname><given-names>T</given-names></name><name><surname>Anandkumar</surname><given-names>A</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>Group Invariant Scattering</article-title><source>arXiv e-prints</source><year>2011</year><person-group person-group-type="author"><name><surname>Mallat</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><source>A Wavelet Tour of Signal Processing</source><year>1998</year></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>Optimizing neural networks with kronecker-factored approxi- mate curvature</article-title><source>ArXiv</source><year>2015</year><person-group person-group-type="author"><name><surname>Martens</surname><given-names>James</given-names></name><name><surname>Roger</surname><given-names>B</given-names></name><name><surname>Grosse</surname><given-names /></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>Playing Atari with Deep Reinforcement Learning</article-title><source>arXiv e-prints</source><year>2013</year><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>Wierstra</surname><given-names>D</given-names></name><name><surname>Riedmiller</surname><given-names>M</given-names></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><year>1996</year><volume>381</volume><fpage>607</fpage><lpage>609</lpage><person-group person-group-type="author"><name><surname>Bruno</surname><given-names>A</given-names></name><name><surname>Olshausen</surname><given-names /></name><name><surname>David</surname><given-names>J</given-names></name><name><surname>Field</surname><given-names /></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>Sparse coding with an overcomplete basis set: A strategy employed by v1?</article-title><source>Vision Research</source><year>1997</year><volume>37</volume><fpage>3311</fpage><lpage>3325</lpage><person-group person-group-type="author"><name><surname>Bruno</surname><given-names>A</given-names></name><name><surname>Olshausen</surname><given-names /></name><name><surname>David</surname><given-names>J</given-names></name><name><surname>Field</surname><given-names /></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><article-title>Generic Deep Networks with Wavelet Scattering</article-title><source>arXiv e-prints</source><year>2013</year><person-group person-group-type="author"><name><surname>Oyallon</surname><given-names>E</given-names></name><name><surname>Mallat</surname><given-names>S</given-names></name><name><surname>Sifre</surname><given-names>L</given-names></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>Scattering Networks for Hybrid Representation Learning</article-title><source>arXiv e-prints</source><person-group person-group-type="author"><name><surname>Oyallon</surname><given-names>E</given-names></name><name><surname>Zagoruyko</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>G</given-names></name><name><surname>Komodakis</surname><given-names>N</given-names></name><name><surname>Lacoste-Julien</surname><given-names>S</given-names></name><name><surname>Blaschko</surname><given-names>M</given-names></name><name><surname>Belilovsky</surname><given-names>E</given-names></name></person-group></element-citation></ref><ref id="b29"><element-citation publication-type="journal"><source>Automatic differentiation in pytorch</source><year>2017</year><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>Adam</given-names></name><name><surname>Gross</surname><given-names>Sam</given-names></name><name><surname>Chintala</surname><given-names>Soumith</given-names></name><name><surname>Chanan</surname><given-names>Gregory</given-names></name><name><surname>Yang</surname><given-names>Edward</given-names></name><name><surname>Devito</surname><given-names>Zachary</given-names></name><name><surname>Lin</surname><given-names>Zeming</given-names></name><name><surname>Desmaison</surname><given-names>Alban</given-names></name><name><surname>Antiga</surname><given-names>Luca</given-names></name><name><surname>Lerer</surname><given-names>Adam</given-names></name></person-group></element-citation></ref><ref id="b30"><element-citation publication-type="journal"><source>Neural Episodic Control. arXiv e-prints, March</source><year>2017</year><person-group person-group-type="author"><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Uria</surname><given-names>B</given-names></name><name><surname>Srinivasan</surname><given-names>S</given-names></name><name><surname>Puigdom&#232;nech</surname><given-names>A</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Wierstra</surname><given-names>D</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name></person-group></element-citation></ref><ref id="b31"><element-citation publication-type="journal"><article-title>Policy Distillation. arXiv e-prints</article-title><source>Prioritized Experience Replay. arXiv e-prints</source><year>2015</year><person-group person-group-type="author"><name><surname>Rusu</surname><given-names>A A</given-names></name><name><surname>Gomez Colmenarejo</surname><given-names>S</given-names></name><name><surname>Gulcehre</surname><given-names>C</given-names></name><name><surname>Desjardins</surname><given-names>G</given-names></name><name><surname>Kirkpatrick</surname><given-names>J</given-names></name><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Hadsell</surname><given-names>R</given-names></name><name><surname>Schaul</surname><given-names>T</given-names></name><name><surname>Quan</surname><given-names>J</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name></person-group></element-citation></ref><ref id="b32"><element-citation publication-type="journal"><source>Reinforcement Learning: An Introduction</source><year>2018</year><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>Richard S</given-names></name><name><surname>Andrew</surname><given-names>G</given-names></name><name><surname>Barto</surname><given-names /></name></person-group></element-citation></ref><ref id="b33"><element-citation publication-type="journal"><article-title>Deep Reinforcement Learning with Double Q-learning</article-title><source>arXiv e-prints</source><year>2015</year><person-group person-group-type="author"><name><surname>Van Hasselt</surname><given-names>H</given-names></name><name><surname>Guez</surname><given-names>A</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name></person-group></element-citation></ref><ref id="b34"><element-citation publication-type="journal"><article-title>When to use parametric models in reinforcement learning? arXiv e-prints</article-title><year>2019</year><person-group person-group-type="author"><name><surname>Van Hasselt</surname><given-names>H</given-names></name><name><surname>Hessel</surname><given-names>M</given-names></name><name><surname>Aslanides</surname><given-names>J</given-names></name></person-group></element-citation></ref><ref id="b35"><element-citation publication-type="journal"><article-title>Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation</article-title><source>arXiv e-prints</source><year>2019</year><person-group person-group-type="author"><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Ciliberto</surname><given-names>C</given-names></name><name><surname>Amadori</surname><given-names>P</given-names></name><name><surname>Demiris</surname><given-names>Y</given-names></name></person-group></element-citation></ref><ref id="b36"><element-citation publication-type="journal"><article-title>Dueling Network Architectures for Deep Reinforcement Learning</article-title><source>arXiv e-prints</source><year>2015</year><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Schaul</surname><given-names>T</given-names></name><name><surname>Hessel</surname><given-names>M</given-names></name><name><surname>Van Hasselt</surname><given-names>H</given-names></name><name><surname>Lanctot</surname><given-names>M</given-names></name><name><surname>De Freitas</surname><given-names>N</given-names></name></person-group></element-citation></ref><ref id="b37"><element-citation publication-type="journal"><article-title>Scalable trust-region method for deep reinforce- ment learning using Kronecker-factored approximation</article-title><source>arXiv e-prints</source><year>2017</year><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Mansimov</surname><given-names>E</given-names></name><name><surname>Liao</surname><given-names>S</given-names></name><name><surname>Grosse</surname><given-names>R</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group></element-citation></ref><ref id="b38"><element-citation publication-type="journal"><article-title>Understanding deep learning requires rethinking generalization</article-title><source>arXiv e-prints</source><year>2016</year><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Bengio</surname><given-names>S</given-names></name><name><surname>Hardt</surname><given-names>M</given-names></name><name><surname>Recht</surname><given-names>B</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name></person-group></element-citation></ref></ref-list></back></article>