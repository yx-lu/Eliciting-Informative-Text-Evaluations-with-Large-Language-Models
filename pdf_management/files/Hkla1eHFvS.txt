Title:
```
Under review as a conference paper at ICLR 2020 EFFICIENT EXPLORATION VIA STATE MARGINAL MATCHING
```
Abstract:
```
Reinforcement learning agents need to explore their unknown environments to solve the tasks given to them. The Bayes optimal solution to exploration is in- tractable for complex environments, and while several exploration methods have been proposed as approximations, it remains unclear what underlying objective is being optimized by existing exploration methods, or how they can be altered to incorporate prior knowledge about the task. Moreover, it is unclear how to acquire a single exploration strategy that will be useful for solving multiple down- stream tasks. We address these shortcomings by learning a single exploration policy that can quickly solve a suite of downstream tasks in a multi-task setting, amortizing the cost of learning to explore. We recast exploration as a problem of State Marginal Matching (SMM), where we aim to learn a policy for which the state marginal distribution matches a given target state distribution, which can incorporate prior knowledge about the task. We optimize the objective by reducing it to a two-player, zero-sum game between a state density model and a parametric policy. Our theoretical analysis of this approach suggests that prior exploration methods do not learn a policy that does distribution matching, but acquire a replay buffer that performs distribution matching, an observation that potentially explains prior methods' success in single-task settings. On both simulated and real-world tasks, we demonstrate that our algorithm explores faster and adapts more quickly than prior methods. 1
```

Figures/Tables Captions:
```
Figure 1: State Marginal Matching: (Left) Our goal is to learn a policy whose distribution over states (blue histogram) matches some target density (black line). Our algorithm iteratively increases the reward on states visited too infrequently (green arrow) and decreases the reward on states visited too frequently (red arrow). (Center) At convergence, these two distributions are equal. (Right) For complex target distributions, we use a mixture of policies ρπ(s) = ρπ z (s)p(z)dz. (See Appendix B.)
Figure 2: Exploration in State Space (SMM) vs. Action Space (SAC) for Navigation: (a): A point-mass agent is spawned at the center of m long hallways that extend radially outward, and the target state distribution places uniform probability mass 1 m at the end of each hallway. We can vary the length of the hallway and the number of hallways to control the task difficulty. (b) A heatmap showing states visited by SAC and SMM during training illustrates that SMM explores a wider range of states. (c) SMM reaches more goals than the MaxEnt baseline. SM4 is an extension of SMM that incorporates mixture modelling with n > 1 skills (see Appendix B), and further improves exploration of SMM.
Figure 3: Exploration for Manipulation. (a) Task: The robot agent controls a single gripper arm to move a block object to a goal location on the table surface. The goal is not observed by the robot, thus requiring the robot to explore by moving the block to different locations on the table. (b) Test-Time Exploration: At test-time, we sample goal locations uniformly on the table, and plot the percentage of goals found within N episodes. SMM and its mixture-model variant SM4 (Algorithm 2) both explore faster than the baselines, allowing it to successfully find the goal in fewer episodes. (c) State Entropy: After training, we rollout the policy for 1e3 epochs, and record the entropy of the object and gripper positions. SMM achieves higher state entropy than the other methods. Historical averaging also improves the exploration of prior methods. (d) Non-Uniform Exploration: We measure the discrepancy between the state marginal distribution, ρπ(s), and a non-uniform target distribution. SMM matches the target distribution better than SAC and is on par with Count. Error bars show std. dev. across 4 random seeds.
Figure 4: Real-World Exploration: (a) D'Claw is a 9-DoF robotic hand (Ahn et al., 2019) that is trained to turn a valve object. (b) Sim2Real: We trained each algorithm in simulation, and then measured how far the trained policy rotated the knob on the hardware robot. We also measured the maximum angle that the agent turned the knob in the clockwise and counter-clockwise directions within one episode. (c) Training on Hardware: We trained SAC and SMM on the real robot for 1e5 environment steps (about 9 hours in real time), and measured the maximum angle turned throughout training. We see that SMM moves the knob more and visits a wider range of states than SAC. All results are averaged over 4-5 seeds.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) algorithms must be equipped with exploration mechanisms to effectively solve tasks with limited reward signals. These tasks arise in many real-world applications where providing human supervision is expensive. The inability of current RL algorithms to adequately explore limits their applicability to long-horizon control tasks. A wealth of prior work has studied exploration for RL. While, in theory, the Bayes-optimal exploration strategy is optimal, it is intractable to compute exactly, motivating work on tractable heuristics for exploration. Exploration methods based on random actions have limited ability to cover a wide range of states. More sophisticated techniques, such as intrinsic motivation, accelerate learning in the single-task setting. However, these methods have two limitations. First, they do not explicitly define an objective to quantify "good exploration," but rather argue that exploration arises implicitly through some iterative procedure. Lacking a well-defined optimization objective, it remains challenging to understand what these methods are doing and why they work. Similarly, the lack of a metric to quantify exploration, even if only for evaluation, makes it challenging to compare exploration methods and assess progress in this area. The second limitation is that these methods target the single-task setting. Because these methods aim to converge to the optimal policy for a particular task, it is challenging to repurpose these methods to solve multiple tasks. We address these shortcomings by considering a multi-task setting, where many different reward functions can be provided for the same set of states and dynamics. Rather than exploring from scratch for each task, we aim to learn a single, task-agnostic exploration policy that can be adapted to many possible downstream reward functions, amortizing the cost of learning to explore. This exploration Under review as a conference paper at ICLR 2020 policy can be viewed as a prior on the policy for solving downstream tasks. Learning will consist of two phases: during training, we acquire this task-agnostic exploration policy; during testing, we use this exploration policy to quickly explore and maximize the task reward. Learning a single exploration policy is considerably more difficult than doing exploration throughout the course of learning a single task. The latter is done by intrinsic motivation ( Pathak et al., 2017 ;  Tang et al., 2017 ;  Oudeyer et al., 2007 ) and count-based exploration methods ( Bellemare et al., 2016 ), which can effectively explore to find states with high reward, at which point the agent can decrease exploration and increase exploitation of those high-reward states. While these methods perform efficient exploration for learning a single task, the policy at any particular iteration is not a good exploration policy. For example, the final policy at convergence would only visit the high-reward states discovered for the current task. What objective should be optimized to obtain a good exploration policy? We recast exploration as a problem of State Marginal Matching: given a desired state distribution, we learn a mixture of policies for which the state marginal distribution matches this desired distribution. Without any prior information, this objective reduces to maximizing the marginal state entropy H[s], which encourages the policy to visit as many states as possible. The distribution matching objective also provides a convenient mechanism to incorporate prior knowledge about the task, whether in the form of safety constraints that the agent should obey; preferences for some states over other states; reward shaping; or the relative importance of each state dimension for a particular task. We also propose an algorithm to optimize the State Marginal Matching (SMM) objective. First, we reduce the problem of SMM to a two-player, zero-sum game between a policy player and a density player. We find a Nash Equilibrium for this game using fictitious play ( Brown, 1951 ), a classic procedure from game theory. Our resulting algorithm iteratively fits a state density model and then updates the policy to visit states with low density under this model. Our analysis of this approach sheds light on prior work on exploration. In particular, while the policy learned by existing exploration algorithms does not perform distribution matching, the replay buffer does, an observation that potentially explains the success of prior methods. On both simulated and real-world tasks, we demonstrate that our algorithm explores more effectively and adapts more quickly to new tasks than state-of-the-art baselines.

Section Title: RELATED WORK
  RELATED WORK Most prior work on exploration has looked at exploration bonuses and intrinsic motivation. One class of exploration methods uses prediction error of some auxiliary task as an exploration bonus, which provides high (intrinsic) reward in states where the predictive model performs poorly ( Pathak et al., 2017 ;  Oudeyer et al., 2007 ;  Schmidhuber, 1991 ;  Houthooft et al., 2016 ;  Burda et al., 2018 ). Another set of approaches ( Tang et al., 2017 ;  Bellemare et al., 2016 ;  Schmidhuber, 2010 ) directly encourage the agent to visit novel states. While all methods effectively explore during the course of solving a single task ( Taïga et al., 2019 ), the policy obtained at convergence is often not a good exploration policy (see Section 4). In contrast, our method converges to a highly-exploratory policy by maximizing state entropy in the training objective (Eq. 2). Many exploration algorithms can be classified by whether they explore in the space of actions, policy parameters, goals, or states. Common exploration strategies including -greedy and Orn- stein-Uhlenbeck noise ( Lillicrap et al., 2015 ), as well as standard MaxEnt RL algorithms ( Ziebart, 2010 ;  Haarnoja et al., 2018 ), explore in the action space. Recent work ( Fortunato et al., 2017 ;  Plappert et al., 2017 ) shows that adding noise to the parameters of the policy can result in good exploration. Most closely related to our work are methods that perform exploration in the space of states or goals ( Colas et al., 2018 ;  Held et al., 2017 ;  Nair et al., 2018 ;  Pong et al., 2019 ;  Hazan et al., 2018 ). In fact,  Hazan et al. (2018)  consider the same State Marginal Matching objective that we examine and propose a similar algorithm. In relation to  Hazan et al. (2018) , our main contributions are (1) empirically showing that exploration based on state-entropy is competitive with existing state-of-the-art exploration methods, and (2) explaining how existing exploration methods based on prediction error are implicitly maximizing this state-entropy objective. In Appendix C.1, we also discuss how goal-conditioned RL ( Kaelbling, 1993 ;  Schaul et al., 2015 ) can be viewed as a special case of State Marginal Matching when the goal-sampling distribution is learned jointly with the policy.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 The problems of exploration and meta-reinforcement learning are tightly coupled. Meta-reinforcement learning algorithms ( Duan et al., 2016 ;  Finn et al., 2017 ;  Rakelly et al., 2019 ;  Mishra et al., 2017 ) must perform effective exploration if they hope to solve a downstream task. Some prior work has explicitly looked at the problem of learning to explore ( Gupta et al., 2018 ;  Xu et al., 2018 ). Our problem statement is similar to meta-learning, in that we also aim to learn a policy as a prior for solving downstream tasks. However, whereas meta-RL requires a distribution of task reward functions, our method will require only a single target state marginal distribution. Due to the simpler problem assumptions and training procedure, our method may be easier to apply in real-world domains. Related to our approach are standard maximum action entropy algorithms ( Haarnoja et al., 2018 ;  Kappen et al., 2012 ;  Rawlik et al., 2013 ;  Ziebart et al., 2008 ;  Theodorou & Todorov, 2012 ). While these algorithms are referred to as MaxEnt RL, they are maximizing entropy over actions, not states. These algorithms can be viewed as performing inference on a graphical model where the likelihood of a trajectory is given by its exponentiated reward ( Toussaint & Storkey, 2006 ;  Levine, 2018 ;  Abdolmaleki et al., 2018 ). While distributions over trajectories induce distributions over states, computing the exact relationship requires integrating over all possible trajectories, an intractable problem for most MDPs. A related but distinct class of relative entropy methods use a similar entropy-based objective to limit the size of policy updates ( Peters et al., 2010 ;  Schulman et al., 2015 ). Finally, the idea of distribution matching has been employed successfully in imitation learning ( Ziebart et al., 2008 ;  Ho & Ermon, 2016 ;  Finn et al., 2016 ;  Fu et al., 2017 ). Similar to some inverse RL algorithms ( Ho & Ermon, 2016 ;  Fu et al., 2018 ), our method iterates between learning a policy and learning a reward function, though our reward function is obtained via a density model instead of a discriminator. While inverse RL algorithms assume access to expert trajectories, we instead assume access to the density of the target state marginal distribution. In many realistic settings, such as robotic control with many degrees of freedom, providing fully-specified trajectories may be much more challenging than defining a target state marginal distribution. The latter only requires some aggregate statistics about expert behavior, and does not even need to be realizable by any policy. In summary, our work unifies prior exploration methods as performing approximate distribution matching, and explains how state distribution matching can be performed properly. This perspective provides a clearer picture of exploration, and this observation is useful particularly because many of the underlying ingredients, such as adversarial games and density estimation, have seen recent progress and therefore might be adopted to improve exploration methods.

Section Title: STATE MARGINAL MATCHING
  STATE MARGINAL MATCHING In this section, we propose the State Marginal Matching problem as a principled objective for learning to explore, and offer an algorithm for optimizing it. We consider a parametric policy π θ ∈ Π {π θ | θ ∈ Θ} that chooses actions a ∈ A in a Markov Decision Process (MDP) M with fixed episode lengths T , dynamics distribution p(s t+1 | s t , a t ), and initial state distribution p 0 (s). The MDP M together with the policy π θ form an implicit generative model over states. We define the state marginal distribution ρ π (s) as the probability that the policy visits state s: We emphasize that ρ π (s) is not a distribution over trajectories, and is not the stationary distribution of the policy after infinitely many steps, but rather the distribution over states visited in a finite-length episode. 2 We also note that any trajectory distribution matching problem can be reduced to a state marginal matching problem by augmenting the current state to include all previous states. We assume that we are given a target distribution p * (s) over states s ∈ S that encodes our belief about the tasks we may be given at test-time. For example, a roboticist might assign small values of p * (s) to states that are dangerous, regardless of the desired task. Alternatively, we might also learn p * (s) from data about human preferences ( Christiano et al., 2017 ). For goal-reaching tasks, we can analytically derive the optimal target distribution (Appendix C). Given p * (s), our goal is to find a 2 ρπ(s) approaches the policy's stationary distribution in the limit as the episodic horizon T → ∞. parametric policy that is "closest" to this target distribution, where we measure discrepancy using the Kullback-Leibler (KL) divergence: This is the same objective as in  Hazan et al. (2018) . Note that we use the reverse-KL ( Bishop, 2006 ), which is mode-seeking (i.e., exploratory). We show in Appendix C that the policies obtained via State Marginal Matching provide an optimal exploration strategy for a particular distribution over reward functions. To gain intuition for the State Marginal Matching objective, we decomposed it in two ways. In Equation 2, we see that State Marginal Matching is equivalent to maximizing the reward function r(s) log p * (s) while simultaneously maximizing the entropy of states. Note that, unlike traditional MaxEnt RL algorithms ( Ziebart et al., 2008 ;  Haarnoja et al., 2018 ), we regularize the entropy of the state distribution, not the conditional distribution of actions given states, which results in exploration in the space of states rather than in actions. Moreover, Equation 1 suggests that State Marginal Matching maximizes a pseudo-reward r(s) log p * (s) − log ρ π (s), which assigns positive utility to states that the agent visits too infrequently and negative utility to states visited too frequently (see  Figure 1 ). We emphasize that maximizing this pseudo-reward is not a RL problem because the pseudo-reward depends on the policy.

Section Title: OPTIMIZING THE STATE MARGINAL MATCHING OBJECTIVE
  OPTIMIZING THE STATE MARGINAL MATCHING OBJECTIVE Optimizing Equation 1 to obtain a single exploration policy is more challenging than standard RL because the reward function itself depends on the policy. To break this cyclic dependency, we introduce a parametric state density model q ψ (s) ∈ Q {q ψ | ψ ∈ Ψ} to approximate the policy's state marginal distribution, ρ π (s). We assume that the class of density models Q is sufficiently expressive to represent every policy: Assumption 1. For every policy π ∈ Π, there exists q ∈ Q such that D KL (ρ π (s) q(s)) = 0. Under this assumption, optimizing the policy w.r.t. this approximate distribution q(s) will yield the same solution as Equation 1 (see Appendix A for the proof): Proposition 3.1. Let policies Π and density models Q satisfying Assumption 1 be given. For any target distribution p * , the following optimization problems are equivalent: Solving the new max-min optimization problem is equivalent to finding the Nash equilibrium of a two-player, zero-sum game: a policy player chooses the policy π while the density player chooses the density model q. To avoid confusion, we use actions to refer to controls a ∈ A output by the policy π in the traditional RL problem and strategies to refer to the decisions π ∈ Π of the policy player and decisions q ∈ Q of the density player. The Nash existence theorem ( Nash, 1951 ) proves that such a stationary point always exists for such a two-player, zero-sum game. One common approach to saddle point games is to alternate between updating player A w.r.t. player B, and updating player B w.r.t. player A. However, games such as Rock-Paper-Scissors illustrate that such a greedy approach is not guaranteed to converge to a stationary point. A slight variant, Under review as a conference paper at ICLR 2020

Section Title: Algorithm 1 Learning to Explore via Fictitious Play
  Algorithm 1 Learning to Explore via Fictitious Play Algorithm 1: An algorithm for optimizing the State Marginal Matching objective (Equation 1). The algorithm iterates between (1) fitting a density model q (m) and (2) training the policy π (m) with a RL objective to optimize the expected return w.r.t. the updated reward function r(s). The algorithm returns the collection of policies from each iteration, which do distribution matching in aggregate. fictitious play ( Brown, 1951 ) does converge to a Nash equilibrium in finite time ( Robinson, 1951 ;  Daskalakis & Pan, 2014 ). At each iteration, each player chooses their best strategy in response to the historical average of the opponent's strategies. In our setting, fictitious play alternates between fitting the density model to the historical average of policies (Equation 4), and updating the policy with RL to minimize the log-density of the state, using a historical average of the density models (Equation 5): Crucially, the exploration policy is not the last policy, π m+1 , but rather the historical average policy: Definition 3.1. A historical average policyπ(a | s), parametrized by a collection of policies π 1 , · · · , π m , is a policy that randomly samples one of the policy iterates π i ∼ Unif[π 1 , · · · , π m ] at the start of each episode and takes actions according to that policy for each step in the episode. A new policy is sampled for the next episode. We summarize the resulting algorithm in Algorithm 1. In practice, we can efficiently implement Equation 4 and avoid storing the policy parameters from every iteration by instead storing sampled states from each iteration. 3 We cannot perform the same trick for Equation 5, and instead resort to approximating the historical average of density models with the most recent iterate. Algorithm 1 looks similar to prior exploration methods based on prediction-error, suggesting that we might use SMM to understand how these prior methods work.

Section Title: WHY DOES PREDICTION-ERROR EXPLORATION WORK?
  WHY DOES PREDICTION-ERROR EXPLORATION WORK? Exploration methods based on prediction error ( Burda et al., 2018 ;  Stadie et al., 2015 ;  Pathak et al., 2017 ;  Schmidhuber, 1991 ;  Chentanez et al., 2005 ) do not converge to an exploratory policy, even in the absence of extrinsic reward. For example, consider the asymptotic behavior of ICM ( Pathak et al., 2017 ) in a deterministic MDP, such as the Atari games where it was evaluated. At convergence, the predictive model will have zero error in all states, so the exploration bonus is zero - the ICM objective has no effect on the policy at convergence. Similarly, consider the exploration bonus in Pseudocounts ( Bellemare et al., 2016 ): 1/n(s), wheren(s) is the (estimated) number of times that state s has been visited. In the infinite limit, each state has been visited infinitely many times, so the Pseudocount exploration bonus also goes to zero - Pseudocounts has no effect at convergence. Similar reasoning can be applied to other methods based on prediction error ( Burda et al., 2018 ;  Stadie et al., 2015 ). More broadly, we can extend this analysis to stochastic MDPs, where we consider an abstract exploration algorithm that alternates between computing some intrinsic reward and performing RL (to convergence) on that intrinsic reward. Existing prediction-error exploration Under review as a conference paper at ICLR 2020 methods are all special cases. At each iteration, the RL step solves a fully-observed MDP, which always admits a deterministic policy as a solution ( Puterman, 2014 ). Thus, any exploration algorithm in this class cannot converge to a single, exploratory policy. Despite these observations, prior methods do excel at solving hard exploration tasks. We draw an analogy to fictitious play to explain their success. While these methods never acquire an exploratory policy, over the course of training they will eventually visit all states. In other words, the historical average over policies will visit a wide range of states. Since the replay buffer exactly corresponds to this historical average over states, these methods will obtain a replay buffer with a diverse range of experience, possibly explaining why they succeed at solving hard exploration tasks. Moreover, this analysis suggests a surprisingly simple method for obtaining an exploration from these prior methods: use a mixture of the policy iterates throughout training. The following section will not only compare SMM against prior exploration methods, but also show that this historical averaging trick can be used to improve existing exploration methods.

Section Title: SIMULATED EXPERIMENTS
  SIMULATED EXPERIMENTS We used simulated control tasks to determine if SMM learns an exploratory policy, to compare SMM to prior exploration methods, and to study the effect of historical averaging. More details can be found in Appendix D, and code will be released upon publication.

Section Title: Baselines and Implementation Details
  Baselines and Implementation Details We compare to a state-of-the-art off-policy MaxEnt RL algorithm, Soft Actor-Critic (SAC) ( Haarnoja et al., 2018 ); an inverse RL algorithm, Generative Adversarial Imitation Learning (GAIL) ( Ho & Ermon, 2016 ); and three exploration methods: • Count-based Exploration (C), which discretizes states and uses − logπ(s) as an exploration bonus. • Pseudo-counts (PC) ( Bellemare et al., 2016 ), which uses the recoding probability as a bonus. • Intrinsic Curiosity Module (ICM) ( Pathak et al., 2017 ), which uses prediction error as a bonus. We used SAC as the base RL algorithm for all exploration methods (SMM, C, PC, ICM). To implement SMM, we define the target distribution in terms of the extrinsic environment reward: p * (s) ∝ exp(r env (s)). We use a variational autoencoder (VAE) to model the density q(s) for both SMM and Pseudocounts (PC). For the GAIL baseline, we generated synthetic expert data by sampling expert states from the target distribution p * (s) (see Appendix D.2 for details). Results for all experiments are averaged over 4-5 random seeds. We start with a sanity check: Is exploration in state space (as done by SMM) better than exploration in action space (as done by MaxEnt RL, e.g., SAC)? To study this question, we implemented a 2D Navigation environment, shown in Figure 2a. To evaluate each method, we counted the number of hallways that the agent fully explored (i.e., reached the end) during training. Figure 2b shows the state visitations for the three hallway environment, illustrating that SAC only explores one hallway, whereas SMM explores all three. Figure 2c also shows that SMM consistently explores 60% of hallways, whereas SAC rarely visits more than 20% of hallways. The remaining simulated experiments used the Manipulation environment ( Plappert et al., 2018 ), shown in Figure 3a. Our first experiment evaluates whether the exploration policy acquired by SMM allows us to solve downstream tasks more quickly. We defined the target distribution to be uniform over the entire state space (joint + block configuration), with the constraints that we put Under review as a conference paper at ICLR 2020 (a) 0 50 100 Episode # 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Success Rate SM4 SMM SMM w/o HA ICM SAC PC (b) (c) 0 20 40 SAC Count SMM KL Divergence 0 2 4 SAC Count SMM TV Distance (d) low probability mass on states where the block has fallen off the table; that actions should be small; and that the arm should be close to the object. As shown in Figure 3b, SMM adapts substantially more quickly than other exploration methods, achieving a success rate 20% higher than the next best method, and reaching the same level of performance of the next baseline (ICM) in 4x fewer episodes. SMM without historical averaging attains similar performance as the next best baseline (ICM), suggesting that historical averaging is the key ingredient, while the particular choice of prediction error or VAE is less important. We provide further ablation studies of SMM in Appendix B.2. While historical averaging is necessary to guarantee convergence (§ 3.1), most prior exploration methods do not employ historical averaging, raising the question of whether it is necessary in practice. To answer this question, we compare SMM to three exploration methods. In Figure 3c, we compare the policy obtained at convergence with the historical average of policy iterates over training for each method. We measure how well each method explores by computing the marginal state entropy, which we compute by discretizing the state space. 4 The results show that SMM maximizes state entropy at least as effectively as prior methods, if not better. While this comparison is somewhat unfair, as we measure exploration using the objective that SMM maximizes, none of the methods we compare against propose metrics for exploration that we could use instead. Furthermore, we see that historical averaging is not only beneficial to SMM, but also improves the exploration of prior methods. In our final simulated experiment, we check whether prior knowledge injected via the target distribu- tion is reflected in the policy obtained from State Marginal Matching. Using the same Manipulation environment as above, we modified the target distribution to assign larger probability to states where the block was on the left half of the table than on the right half. In Figure 3d, we measure whether SMM is able to achieve the target distribution by measuring the discrepancy between the block's horizontal coordinate and the target distribution. Compared to the SAC baseline, SMM and the Count baseline are half the distance to the target distribution. No method achieves zero discrepancy, suggesting that future methods could be better at matching state marginals.

Section Title: REAL-WORLD EXPERIMENTS
  REAL-WORLD EXPERIMENTS While almost all research on exploration focus on simulated domains, attributes of the real world such as partial observability, nonstationarity, and stochasticity may make the exploration more challenging. The aim of this section is to see if SMM explores effectively on a real-world robotic control task. We used the D'Claw ( Ahn et al., 2019 ) robotic manipulator, which is a 3-fingered hand positioned vertically above a handle that it can turn. For all experiments on the D'Claw, we used a target distribution that places uniform mass over all object angles [−180 • , 180 • ]. In a first experiment, we trained SMM and other baselines in simulation, and then evaluated the acquired exploration policy on the real robot using two metrics: the total number of rotations (in either direction), and the maximum radians turned (in both directions). For each method, we computed the average metric across 100 evaluation episodes. We repeated this process for 5 independent training runs. Figure 4b shows that SMM turns the knob more than the baselines, and it turns the knob to a wider range of angles. To test for statistical significance, we used a 1-sided Student's t-test to test the hypothesis that SMM turned the knob more and to a wider range of angles than SAC. The p-values were all less than 0.05: p = 0.046 for number of rotations, p = 0.019 for maximum clockwise angle, and p = 0.001 for maximum counter-clockwise angle. In our second experiment, we investigated whether it was possible to learn an exploration policy directly in the real world, without the need for a simulator. Learning to explore in the real world is quite important, as building faithful simulators of complex systems is challenging. The physical constraints of the real robot make data efficiency paramount, suggesting that learning to explore will require an effective exploration strategy. In Figure 4c, we plot the range of angles that the policy explores throughout training. Not only does SMM explore a wider range of angles than SAC, but its ability to explore increases throughout training, suggesting that the SMM objective is correlated with real-world metrics of exploration. In summary, the results in this section suggests that exploration techniques may actually be useful in the real world, which may encourage future work to study exploration methods on real-world tasks.

Section Title: DISCUSSION
  DISCUSSION In this paper, we introduced a formal objective for exploration. While it is often unclear what existing exploration algorithms will converge to, our State Marginal Matching objective has a clear solution: at convergence, the policy should visit states in proportion to their density under a target distribution. Not only does this objective encourage exploration, it also provides human users with a flexible mechanism to bias exploration towards states they prefer and away from dangerous states. Upon convergence, the resulting policy can thereafter be used as a prior in a multi-task setting, amortizing exploration and enabling faster adaptation to new, potentially sparse, reward functions. The algorithm we proposed looks quite similar to previous exploration methods based on prediction error, suggesting that those methods are also performing some form of distribution matching. However, by deriving our method from first principles, we note that these prior methods omit a crucial historical averaging step, without which the algorithm is not guaranteed to converge. Experiments on both simulated and real-world tasks demonstrated how SMM learns to explore, enabling an agent to efficiently explore in new tasks provided at test time. In future work, we aim to study connections between inverse RL, MaxEnt RL and state marginal matching, all of which perform some form of distribution matching. Empirically, we aim to scale to more complex tasks by parallelizing the training of all mixture components simultaneously. Broadly, we expect the state distribution matching problem formulation to enable the development of more effective and principled RL methods that reason about distributions rather than individual states. Under review as a conference paper at ICLR 2020
  One approach is to maintain an infinite-sized replay buffer, and fit the density to the replay buffer at each iteration. Alternatively, we can replace older samples in a fixed-size replay buffer less frequently such that sampling from B is uniform over iterations.

```
