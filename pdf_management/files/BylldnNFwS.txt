Title:
```
Under review as a conference paper at ICLR 2020 ON THE DECISION BOUNDARIES OF DEEP NEURAL NETWORKS: A TROPICAL GEOMETRY PERSPECTIVE
```
Abstract:
```
This work tackles the problem of characterizing and understanding the decision boundaries of neural networks with piecewise linear non-linearity activations. We use tropical geometry, a new development in the area of algebraic geometry, to provide a characterization of the decision boundaries of a simple neural network of the form (Affine, ReLU, Affine). Specifically, we show that the decision bound- aries are a subset of a tropical hypersurface, which is intimately related to a poly- tope formed by the convex hull of two zonotopes. The generators of the zonotopes are precise functions of the neural network parameters. We utilize this geometric characterization to shed lights on new perspectives of three tasks. In doing so, we propose a new tropical perspective for the lottery ticket hypothesis, where we see the effect of different initializations on the tropical geometric representation of the decision boundaries. Also, we leverage this characterization as a new set of trop- ical regularizers, which deal directly with the decision boundaries of a network. We investigate the use of these regularizers in neural network pruning (removing network parameters that do not contribute to the tropical geometric representation of the decision boundaries) and in generating adversarial input attacks (with input perturbations explicitly perturbing the decision boundaries geometry to change the network prediction of the input).
```

Figures/Tables Captions:
```
Figure 1: Decision Boundaries as Geometric Structures. The decision boundaries B (in red) comprise two linear pieces separating classes C1 and C2. As per Theorem 2, the dual subdivision of this single hidden neural network is the convex hull between the zonotopes Z G 1 and Z G 2 . The normals to the dual subdivison δ(R(x)) are in one-to-one correspondence to the tropical hypersurface T (R(x)), which is a superset to the decision boundaries B. Note that some of the normals to δ(R(x)) (in red) are parallel to the decision boundaries.
Figure 2: Effect of Different Initializations on the Decision Boundaries Polytope. From left to right:
Figure 3: Tropical Pruning Pipeline. Pruning the 4 th node, or equivalently removing the two yellow vertices of zonotope ZG 2 does not affect the decision boundaries polytope which will not lead to any change in accuracy.
Figure 4: Results of Tropical Pruning. Pruning-accuracy plots for AlexNet (top) and VGG16 (bottom) trained on SVHN, CIFAR10, and CIFAR100, pruned with our tropical method and three other pruning methods.
Figure 5: Dual View of Tropical Adversarial Attacks. We show the effects of tropical adversarial attacks on a synthetic binary dataset at two different input points (in black). From left to right: the decision regions of the original and perturbed models, and decision boundaries polytopes (green for original and blue for perturbed).
Figure 6: Effect of Tropical Adversarial Attacks on MNIST Dataset. We show qualitative examples of adversarial attacks, produced by solving Problem (5), on two digits (8,9) from MNIST. From left to right, images are classified as [8,7,5,4] and [9,7,5,4] respectively.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep Neural Networks (DNNs) have demonstrated outstanding performance across several research domains, including computer vision ( Krizhevsky et al., 2012 ), speech recognition ( Hinton et al., 2012 ), natural language processing ( Bahdanau et al., 2015 ;  Devlin et al., 2018 ), quantum chemistry ( Schütt et al., 2017 ), and healthcare ( Ardila et al., 2019 ;  Zhou et al., 2019 ) to name a few ( Le- Cun et al., 2015 ). Nevertheless, a rigorous interpretation of their success remains evasive ( Shalev- Shwartz & Ben-David, 2014 ). For instance, and in an attempt to uncover the expressive power of DNNs,  Montufar et al. (2014)  studied the complexity of functions computable by DNNs that have piecewise linear activations. They derived a lower bound on the maximum number of linear regions. Several other works have followed to improve such estimates under certain assumptions ( Arora et al., 2018 ). In addition, and in attempt to understand some of the subtle behaviours DNNs exhibit, e.g. the sensitive reaction of DNNs to small input perturbations, several works directly investigated the decision boundaries induced by a DNN used for classification. The work of  Seyed- Mohsen Moosavi-Dezfooli (2019)  showed that the smoothness of these decision boundaries and their curvature can play a vital role in network robustness. Moreover,  He et al. (2018a)  studied the expressiveness of these decision boundaries at perturbed inputs and showed that these boundaries do not resemble the boundaries around benign inputs.  Li et al. (2018)  showed that under certain assumptions, the decision boundaries of the last fully connected layer of DNNs will converge to a linear SVM. Also,  Beise et al. (2018)  showed that the decision regions of DNNs with width smaller than the input dimension are unbounded. More recently, and due to the popularity of the piecewise linear ReLU as an activation function, there has been a surge in the number of works that study this class of DNNs in particular. As a result, this has incited significant interest in new mathematical tools that help analyze piecewise linear functions, such as tropical geometry. While tropical geometry has shown its potential in many applications such as dynamic programming ( Joswig & Schröter, 2019 ), linear programming (Allamigeon et al., 2015), multi-objective discrete optimization ( Joswig & Loho, 2019 ), enumerative geometry ( Mikhalkin, 2004 ), economics ( Akian et al., 2009 ;  Mai Tran & Yu, 2015 ), it has only been Under review as a conference paper at ICLR 2020 recently used to analyze DNNs. For instance,  Zhang et al. (2018)  showed an equivalency between the family of DNNs with piecewise linear activations and integer weight matrices and the family of tropical rational maps, i.e. ratio between two multi-variate polynomials in tropical algebra. The work of  Zhang et al. (2018)  was mostly concerned about characterizing the complexity of a DNN and specifically counting the number of linear regions, into which the function represented by the DNN can divide the input space, by counting the number of vertices of some polytope representation. This novel approach recovered the results of  Montufar et al. (2014)  with a much simpler analysis.

Section Title: Contributions
  Contributions In this paper, we take the results of  Zhang et al. (2018)  some steps further and present a novel perspective on the decision boundaries of DNNs using tropical geometry. To that end, our contributions are three-fold. (i) We derive a geometric representation (convex hull between two zonotopes) for a super set to the decision boundaries of a DNN in the form (Affine, ReLU, Affine). (ii) We demonstrate support for the lottery ticket hypothesis ( Frankle & Carbin, 2019 ) using a geometric perspective. (iii) We leverage the geometrical representation of the decision boundaries (the decision boundaries polytope) in two interesting applications: network purning and adversarial attacks. In regards to tropical pruning, we provide a new geometric perspective in which one can directly compress the decision boundaries polytope efficiently resulting in only minor perturbations to the decision boundaries. We conduct extensive experiments on AlexNet ( Krizhevsky et al., 2012 ) and VGG16 ( Simonyan & Zisserman, 2014 ) on SVHN ( Netzer et al., 2011 ), CIFAR10, and CI- FAR 100 ( Krizhevsky & Hinton, 2009 ) datasets, in which 90% pruning rate can be achieved with a marginal drop in testing accuracy. As for tropical adversarial attack, we show that one can con- struct input adversaries that can change network predictions by perturbing the decision boundaries polytope. We conduct extensive experiments on MNIST ( LeCun, 1998 ).

Section Title: PRELIMINARIES TO TROPICAL GEOMETRY
  PRELIMINARIES TO TROPICAL GEOMETRY We provide here some preliminaries to tropical geometry. For a thorough detailed review, we refer the reader to the work of  Itenberg et al. (2009) ;  Maclagan & Sturmfels (2015) . Definition 1. (Tropical Semiring 1 ) The tropical semiring T is the triplet {R ∪ {−∞}, ⊕, }, where ⊕ and define tropical addition and tropical multiplication, respectively. They are denoted as: It can be readily shown that −∞ is the additive identity and 0 is the multiplicative identity. Given the previous definition, a tropical power can be formulated as x a = x x · · · x = a.x, for x ∈ T, a ∈ N, where a.x is standard multiplication. Moreover, the tropical quotient can be defined as: x y = x − y where x − y is the standard subtraction. For ease of notation, we write x a as x a . Now, we are in a position to define tropical polynomials, their solution sets and tropical rationals. Definition 2. (Tropical Polynomials) For x ∈ T d , c i ∈ R and a i ∈ N d , a d-variable tropical polynomial with n monomials. f : T d → T d can be expressed as: We use the more compact vector notation x a = x a1 1 x a2 2 · · · x a d d where x, a ∈ R d . Moreover and for ease of notation, we will denote c i x ai as c i x ai throughout the paper. Definition 3. (Tropical Rational Functions) A tropical rational function is a standard difference or equivalently, a tropical quotient of two tropical polynomials: f (x) − g(x) = f (x) g(x). Algebraic curves or hypersurfaces in algebraic geometry, which are the solution sets to polynomials, can be analogously extended to tropical polynomials too. Definition 4. (Tropical Hypersurfaces) A tropical hypersurface of a tropical polynomial f (x) = c 1 x a1 ⊕ · · · ⊕ c n x an is the set of points x where f is attained by two or more monomials in f , i.e. Tropical hypersurfaces divide the domain of f into convex regions, where f is linear in each region. Moreover, every tropical polynomial can be associated with a Newton polytope. Under review as a conference paper at ICLR 2020 Definition 5. (Newton Polytopes) The Newton polytope of a tropical polynomial f (x) = c 1 x a1 ⊕ · · · ⊕ c n x an is the convex hull of the exponents a i ∈ N d regarded as points in R d , i.e. A tropical polynomial determines a dual subdivision, which can thus be constructed by projecting the collection of upper faces (UF) in P(f ) := ConvHull{(a i , c i ) ∈ R d × R : i = 1, . . . , n} to R d . That is to say, the dual subdivision determined by f is given as δ(f ) := {π(p) ⊂ R d : p ∈ UF(P(f ))} where π : R d × R → R d is the projection that drops the last coordinate. It has been shown by  Maclagan & Sturmfels (2015)  that the tropical hypersurface T (f ) is the (d-1)-skeleton of the polyhedral complex dual to δ(f ). So, each vertex of δ(f ) corresponds to one region in R d where f is linear.  Zhang et al. (2018)  showed an equivalency between tropical rational maps and any neural network f : R n → R k with piecewise linear activations and integer weights through the following theorem. Theorem 1. ( Tropical Characterization of Neural Networks, Zhang et al. (2018) ). A feedforward neural network with integer weights and real biases with piecewise linear activation functions is a function f : R n → R k , whose coordinates are tropical rational functions of the input, i.e., f (x) = H(x) Q(x) = H(x) − Q(x), where H and Q are tropical polynomials. While this result is new in the context of tropical geometry, it is not surprising, since any piecewise linear function can be represented as a difference of two max functions over a set of hyperplanes  Melzer (1986) . Mathematically, that is to say if f is a piecewise linear function, it can be written as f (x) = max i∈[m] {a i x} − max j∈[n] {b j x}, where [m] = {1, . . . , m} and [n] = {1, . . . , n}. Thus, it is clear that each of the two maxima above is a tropical polynomial recovering Theorem 1.

Section Title: DECISION BOUNDARIES OF DEEP NEURAL NETWORKS AS POLYTOPES
  DECISION BOUNDARIES OF DEEP NEURAL NETWORKS AS POLYTOPES In this section, we analyze the decision boundaries of a network in the form (Affine, ReLU, Affine) using tropical geometry. For ease, we use ReLUs as the non-linear activation, but any other piecewise linear function can also be used. The functional form of this network is: f (x) = Bmax (Ax + c 1 , 0) + c 2 , where max(.) is an element-wise operator. The outputs of the network f are the logit scores. Throughout this section, we assume 2 that A ∈ Z p×n , B ∈ Z 2×p , c 1 ∈ R p and c 2 ∈ R 2 . For ease of notation, we only consider networks with two outputs, i.e. B 2×p , where the extension to a multi-class output follows naturally and it is discussed in the appendix. Now, since f is a piecewise linear function, each output can be expressed as a tropical rational as per Theorem 1. If f 1 and f 2 refer to the first and second outputs respectively, we have f 1 (x) = H 1 (x) Q 1 (x) and f 2 (x) = H 2 (x) Q 2 (x), where H 1 , H 2 , Q 1 and Q 2 are tropical polynomials. In what follows and for ease of presentation, we present our main results where the network f has no biases, i.e. c 1 = 0 and c 2 = 0, and we leave the generalization to the appendix. Theorem 2. For a bias-free neural network in the form of f (x) : R n → R 2 where A ∈ Z p×n and B ∈ Z 2×p , let R(x) = H 1 (x) Q 2 (x) ⊕ H 2 (x) Q 1 (x) be a tropical polynomial. Then: • Let B = {x ∈ R n : f 1 (x) = f 2 (x)} defines the decision boundaries of f , then B ⊆ T (R(x)). The proof for Theorem 2 is left for the appendix. Digesting Theorem 2. Theorem 2 can be broken into two major results. The first, which is on the algebra side, i.e. finding the solution set to tropical polynomials, states that the decision boundaries Under review as a conference paper at ICLR 2020 B is a subset of the tropical hypersurface of the tropical polynomial R(x), i.e. T (R(x)). The second result, which is on the geometry side, of Theorem 2 relates the tropical polynomial R(x) to the geometric representation of the solution set to R(x), i.e. T (R(x)), referred to as the dual subdivision, i.e. δ(R(x)). In particular, Theorem 2 states that the dual subdivision for a network f is the convex hull of two zonotopes denoted as Z G1 and Z G2 . Note that this dual subdivision is a function of only the network parameters A and B. Theorem 2 bridges the gap between the behaviour of the decision boundaries B, through the super-set T (R(x)), and the polytope δ (R(x)), which is the convex hull of two zonotopes. It is worthwhile to mention that  Zhang et al. (2018)  discussed a special case of the first part of Theorem 2 for a neural network with a single output and a score function s(x) to classify the output. To the best of our knowledge, this work is the first to propose a tropical geometric formulation of a super-set containing the decision boundaries of a multi-class classification neural network. In particular, the first result of Theorem 2 states that one can alter the network, e.g. by pruning network parameters, while preserving the decision boundaries B, if one preserves the tropical hypersurface of R(x) or T (R(x)). While preserving the tropical hypersurfaces can be equally difficult to preserving the decision boundaries directly, the second result of Theorem 2 comes in handy. For a bias free network, π becomes an identity mapping with δ(R(x)) = ∆(R(x)), and thus the dual subdivision δ(R(x)), which is the Newton polytope ∆(R(x)) in this case, becomes a well structured geometric object that can be exploited to preserve decision boundaries. Since  Maclagan & Sturmfels (2015)  (Proposition 3.1.6) showed that the tropical hypersurface is the skeleton of the dual to δ(R(x)), the normal lines to the edges of the polytope δ(R(x)) are in one-to-one correspondence with the tropical hypersurface T (R(x)).  Figure 1  details this intimate relation between the decision boundaries, tropical hypersurface T (R(x)), and normals to δ (R(x)). Before any further discussion, we recap the definition of zonotopes. Another common definition for zonotopes is the Minkowski sum (refer to appendix A for the def- inition of the Minkowski sum) of a set of line segments that start from the origin with end points u 1 , . . . , u p ∈ R n . It is also well known that the number of vertices of a zonotope is polynomial in the number of line segments. That is to say, |vert (Z U ) | ≤ 2 n−1 i=0 p−1 i = O p n−1 ( Gritzmann & Sturmfels, 1993 ). While Theorem 2 presents a strong relation between a polytope (convex hull of two zonotopes) and the decision boundaries, it remains unclear how such a polytope can be efficiently constructed. Although the number of vertices of a zonotope is polynomial in the number of its generating line segments, fast algorithms for enumerating these vertices are still restricted to zonotopes with line segments starting at the origin ( Stinson et al., 2016 ). Since the line segments generating the zono- topes in Theorem 2 have arbitrary end points, we present the next result that transforms these line segments into a generator matrix of line segments starting from the origin, as prescribed in Definition 6. This result is essential for the efficient computation of the zonotopes in Theorem 2. training dataset, decision boundaries polytope of original network followed by the decision boundaries polytope during several iterations of pruning with different initializations. The proof is left for the appendix. As per Proposition 1, the generator matrices of zonotopes Z G1 , Z G2 in Theorem 2 can be defined as In what follows, we show several applications for Theorem 2. We begin by leveraging the geometric structure to help in reaffirming the behaviour of the lottery ticket hypothesis.

Section Title: TROPICAL VIEW TO THE LOTTERY TICKET HYPOTHESIS
  TROPICAL VIEW TO THE LOTTERY TICKET HYPOTHESIS The lottery ticket hypothesis was recently proposed by  Frankle & Carbin (2019) , in which the au- thors surmise the existence of sparse trainable sub-networks of dense, randomly-initialized, feed- forward networks that-when trained in isolation-perform as well as the original network in a similar number of iterations. To find such sub-networks,  Frankle & Carbin (2019)  propose the fol- lowing simple algorithm: perform standard network pruning, initialize the pruned network with the same initialization that was used in the original training setting, and train with the same number of epochs. They hypothesize that this should result in a smaller network with a similar accuracy to the larger dense network. In other words, a subnetwork can have similar decision boundaries to the original network. While in this section we do not provide a theoretical reason for why this proposed pruning algorithm performs favorably, we utilize the geometric structure that arises from Theorem 2 to reaffirm such behaviour. In particular, we show that the orientation of the decision boundaries polytope δ(R(x)), known to be a superset to the decision boundaries T (R(x)), is preserved after pruning with the proposed initialization algorithm of  Frankle & Carbin (2019) . On the other hand, pruning routines with a different initialization at each pruning iteration will result in a severe vari- ation in the orientation of the decision boundaries polytope. This leads to a large change in the orientation of the decision boundaries, which tends to hinder accuracy. To this end, we train a neural network with 2 inputs (n = 2), 2 outputs, and a single hidden layer with 40 nodes (p = 40). We then prune the network by removing the smallest x% of the weights. The pruned network is then trained using different initializations: (i) the same initialization as the original network ( Frankle & Carbin, 2019 ), (ii) Xavier ( Glorot & Bengio, 2010 ), (iii) standard Gaus- sian and (iv) zero mean Gaussian with variance of 0.1.  Figure 2  shows the evolution of the decision boundaries polytope, i.e. δ(R(x)), as we perform more pruning (increasing the x%) with different initializations. It is to be observed that the orientation of the polytopes δ(R(x)) vary much more for all different initialization schemes as compared to the lottery ticket initialization. This gives an indi- cation that lottery ticket initialization indeed preserves the decision boundaries throughout the evo- lution of pruning. Another approach to investigate the lottery ticket could be by observing the poly- topes representing the functional form of the network directly, i.e. δ(H {1,2} (x)) and δ(Q {1,2} (x)), in lieu of the decision boundaries polytopes. However, this does not provide conclusive answers to the lottery ticket, since there can exist multiple functional forms, and correspondingly multiple polytopes δ(H {1,2} (x)) and δ(Q {1,2} (x)), for networks with the same decision boundaries. This is why we explicitly focus our analysis on δ(R(x)), which is directly related to the decision boundaries of the network. Further discussions and experiments are left for the appendix.

Section Title: TROPICAL NETWORK PRUNING
  TROPICAL NETWORK PRUNING Network pruning has been identified as an effective approach for reducing the computational cost and memory usage during network inference time. While pruning dates back to the work of  LeCun et al. (1990)  and Hassibi & Stork (1993), it has recently gained more attention. This is due to the fact that most neural networks over-parameterize commonly used datasets. In network pruning, the task is to find a smaller subset of the network parameters, such that the resulting smaller network has sim- ilar decision boundaries (and thus supposedly similar accuracy) to the original over-parameterized network. In this section, we show a new geometric approach towards network pruning. In particu- Under review as a conference paper at ICLR 2020 lar, as indicated by Theorem 2, preserving the polytope δ(R(x)) preserves a superset to the decision boundaries T (R(x)), and thus supposedly the decision boundaries themselves.

Section Title: Motivational Insight
  Motivational Insight For a single hidden layer neural network, the dual subdivision to the decision boundaries is the polytope that is the convex hull of two zonotopes, where each is formed by taking the Minkowski sum of line segments (Theorem 2).  Figure 3  shows an example where pruning a neuron in the neural network has no effect on the dual subdivision polytope and equivalently no effect on the accuracy, since the decision boundaries of both networks remain the same.

Section Title: Problem Formulation
  Problem Formulation Given the motivational insight, a natural question arises: Given an over- parameterized binary neural network f (x) = B max (Ax, 0), can one construct a new neural network, parameterized by some sparser weight matricesÃ andB, such that this smaller network has a dual subdivision δ(R(x)) that preserves the decision boundaries of the original network? In order to address this question, we propose the following general optimization problem The function d(.) defines a distance between two geometric objects. Since the generatorsG 1 and G 2 are functions ofÃ andB (as per Theorem 2), this optimization problem can be challenging to solve. However, for pruning purposes, one can observe from Theorem 2 that if the generatorsG 1 andG 2 had fewer number of line segments (rows), this corresponds to a fewer number of rows in the weight matrixÃ (sparser weights). To this end, we observe that ifG 1 ≈ G 1 andG 2 ≈ G 2 , thenδ(R(x)) ≈ δ(R(x)), and thus the decision boundaries tend to be preserved as a consequence. Therefore, we propose the following optimization problem as a surrogate to Problem (1) The matrix mixed norm for C ∈ R n×k is defined as C 2,1 = n i=1 C(i, :) 2 , which encourages the matrix C to be row sparse, i.e. complete rows of C are zero. Note thatG 1 = Diag[ReLU(B(1, : ))+ReLU(−B(2, :))]Ã,G 2 = Diag[ReLU(B(2, :))+ReLU(−B(1, :))]Ã, and Diag(v) rearranges the elements of vector v in a diagonal matrix. We solve the aforementioned problem with alternating optimization over the variablesÃ andB, where each sub-problem is solved in closed form. Details of the optimization and the extension to multi-class case are left for the appendix.

Section Title: Extension to Deeper Networks
  Extension to Deeper Networks For deeper networks, one can still apply the aforementioned op- timization for consecutive blocks. In particular, we prune each consecutive block of the form (Affine,ReLU,Affine) starting from the input and ending at the output of the network.

Section Title: Experiments on Tropical Pruning
  Experiments on Tropical Pruning Here, we evaluate the performance of the proposed pruning approach as compared to several classical approaches on several architectures and datasets. In par- ticular, we compare our tropical pruning approach against Class Blind (CB), Class Uniform (CU) and Class Distribution (CD)  Han et al. (2015) ;  See et al. (2016) . In Class Blind, all the parameters across all nodes of a layer are sorted by magnitude where x% with smallest magnitudes are pruned. Similar to Class Blind, Class Uniform prunes the parameters with smallest x% magnitudes per node in a layer as opposed to sorting all parameters in all nodes as in Class Blind. Lastly, Class Distri- bution performs pruning of all parameters for each node in the layer, just as in Class Uniform, but the parameters are pruned based on the standard deviation σ c of the magnitude of the parameters per node. Since fully connected layers in deep neural networks tend to have much higher memory complexity than convolutional layers, we restrict our focus to pruning fully connected layers. We Under review as a conference paper at ICLR 2020 train AlexNet and VGG16 on SVHN , CIFAR10, and CIFAR 100 datasets. We observe that we can prune more than 90% of the classifier parameters for both networks without affecting the accuracy. Moreover, we can boost the pruning ratio using our method without affecting the accuracy by simply retraining the network biases only.

Section Title: Setup
  Setup We adapt the architectures of AlexNet and VGG16, since they were originally trained on Ima- geNet ( Deng et al., 2009 ), to account for the discrepancy in the input resolution. The fully connected layers of AlexNet and VGG16 have sizes of (256,512,10) and (512,512,10), respectively on SVHN and CIFAR100 with the last layer replaced to 100 for CIFAR100. All networks were trained to baseline test accuracy of (92%,74%,43%) for AlexNet on SVHN, CIFAR10 and CIFAR100, respec- tively and (92%,92%,70%) for VGG16. To evaluate the performance of pruning, following previous works ( Han et al., 2015 ), we report the area under the curve (AUC) of the pruning-accuracy plot. The higher the AUC is, the better the trade-off is between pruning rate and accuracy. For efficiency purposes, we run the optimization in Problem (2) for a single alternating iteration to identify the rows inÃ and elements ofB that will be pruned, since an exact pruning solution might not be necessary. The algorithm and the parameters setup to solving (2) is left for the appendix. Results.  Figure 4  shows the pruning comparison between our tropical approach and the three afore- mentioned popular pruning schemes on both AlexNet and VGG16 over the different datasets. Our proposed approach can indeed prune out as much as 90% of the parameters of the classifier without sacrificing much of the accuracy. For AlexNet, we achieve much better performance in pruning as compared to other methods. In particular, we are better in AUC by 3%, 3%, and 2% over other pruning methods on SVHN, CIFAR10 and CIFAR100, respectively. This indicates that the decision boundaries can indeed be preserved by preserving the dual subdivision polytope. For VGG16, we perform similarly well on both SVHN and CIFAR10 and slightly worse on CIFAR100. While the performance achieved here is comparable to the other pruning schemes, if not better, we emphasize that our contribution does not lie in outperforming state-of-the-art pruning methods, but rather in giving a new geometry based perspective to network pruning. We conduct more experiments, where only the biases of the network or the biases of the classifier are fine tuned after pruning . Retrain- ing biases can be sufficient as they do not contribute to the orientation of the decision boundaries polytope, thereafter the decision boundaries, but only a translation. Discussion on biases and more results are left for the appendix.

Section Title: TROPICAL ADVERSARIAL ATTACKS
  TROPICAL ADVERSARIAL ATTACKS DNNs are notoriously known to be susceptible to adversarial attacks. In fact, adding small im- perceptible noise, referred to as adversarial attacks, at the input of these networks can hinder their performance. Several works investigated the decision boundaries of neural networks in the presence of adversarial attacks. For instance,  Khoury & Hadfield-Menell (2018)  analyzed high dimensional geometry of adversarial examples by the means of manifold reconstruction. Also,  He et al. (2018b)  crafted adversarial attacks by estimating the distance to the decision boundaries using random search directions. In this work, we provide a tropical geometric view to this problem. where we show how Theorem 2 can be leveraged to construct a tropical geometric based targeted adversarial attack. Dual View to Adversarial Attacks. For a classifier f : R n → R k and input x 0 that is classified as c, a standard formulation for targeted adversarial attacks flips the classifier prediction to a particular class t and it is usually defined as follows This objective aims at computing the lowest energy input noise η (measured by D) such that the the new sample (x 0 + η) crosses the decision boundaries of f to a new classification region. Here, we present a dual view to adversarial attacks. Instead of designing a sample noise η such that (x 0 + η) belongs to a new decision region, one can instead fix x 0 and perturb the network parameters to move the decision boundaries in a way that x 0 appears in a new classification region. In particular, let A 1 be the first linear layer of f , such that f (x 0 ) = g(A 1 x 0 ). One can now perturb A 1 to alter the decision boundaries and relate the perturbation to the input perturbation as follows From this dual view, we observe that traditional adversarial attacks are intimately related to per- turbing the parameters of the first linear layer through the linear system: A 1 η = ξ A1 x 0 . To this end, Theorem 2 provides explicit means to geometrically construct adversarial attacks by means of perturbing decision boundaries. In particular, since the normals to the dual subdivision polytope δ(R(x)) of a given neural network represent the tropical hypersurface set T (R(x)) which is, as per Theorem 2, a superset to the decision boundaries set B, ξ A1 can be designed to result in a minimal perturbation to the dual subdivision that is sufficient to change the network prediction of x 0 to the targeted class t. Based on this observation, we formulate the problem as follows The loss is the standard cross-entropy loss. The first row of constraints ensures that the network prediction is the desired target class t when the input x 0 is perturbed by η, and equivalently by perturbing the first linear layer A 1 by ξ A1 . This is identical to f 1 as proposed by Carlini & Wagner (2016). Moreover, the third and fourth constraints guarantee that the perturbed input is feasible and that the perturbation is bounded, respectively. The fifth constraint is to limit the maximum perturbation on the first linear layer, while the last constraint enforces the dual equivalence between input perturbation and parameter perturbation. The function D 2 captures the perturbation of the dual subdivision polytope upon perturbing the first linear layer by ξ A1 . For a single hidden layer neural network parameterized as (A 1 + ξ A1 ) ∈ R p×n and B ∈ R 2×p for the 1 st and 2 nd layers respectively, D 2 can capture the perturbations in each of the two zonotopes discussed in Theorem 2. The derivation, discussion, and extension of (6) to multi-class neural networks is left for the ap- pendix. We solve Problem (5) with a penalty method on the linear equality constraints, A 1 η = ξ A1 x 0 , where each penalty step is solved with ADMM ( Boyd et al., 2011 ) in a similar fashion to the work of  Xu et al. (2018) . The details of the algorithm are left for the appendix. Motivational Insight to the Dual View. This intuition is presented in  Figure 5 . We train a single hidden layer neural network where the size of the input is 2 with 50 hidden nodes and 2 outputs on a simple dataset as shown in  Figure 5 . We then solve Problem 5 for a given x 0 shown in black. We show the decision boundaries for the network with and without the perturbation at the first lin- ear layer ξ A1 .  Figure 5  shows that indeed perturbing an edge of the dual subdivision polytope, by perturbing the first linear layer, corresponds to perturbing the decision boundaries and results in miss-classifying x 0 . Interestingly and as expected, perturbing different decision boundaries corre- sponds to perturbing different edges of the dual subdivision. In particular, one can see from  Figure 5  Under review as a conference paper at ICLR 2020 that altering the decision boundaries, by altering the dual subdivision polytope through perturbations in the first linear layer, can result in miss-classifying a previously correctly classified input x 0 .

Section Title: MNIST Experiment
  MNIST Experiment Here, we design perturbations to misclassify MNIST images. Figure 7 shows several adversarial examples that change the network prediction for digits 8 and 9 to digits 7, 5, and 4, respectively. In some cases, the perturbation η is as small as = 0.1, where x 0 ∈ [0, 1] n . Several other adversarial results are left for the appendix. We again emphasize that our approach is not meant to be compared with (or beat) state of the art adversarial attacks, but rather to provide a novel geometrically inspired perspective that can shed new light in this field.

Section Title: CONCLUSION
  CONCLUSION In this paper, we leverage tropical geometry to characterize the decision boundaries of neural net- works in the form (Affine, ReLU, Affine) and relate it to well-studied geometric objects such as zonotopes and polytopes. We leaverage this representation in providing a tropical perspective to support the lottery ticket hypothesis, network pruning and designing adversarial attacks. One natu- ral extension for this work is a compact derivation for the characterization of the decision boundaries of convolutional neural networks (CNNs) and graphical convolutional networks (GCNs).
  .

```
