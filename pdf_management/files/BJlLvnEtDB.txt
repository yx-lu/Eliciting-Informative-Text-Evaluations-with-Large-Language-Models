Title:
```
Under review as a conference paper at ICLR 2020 ANALYSIS AND INTERPRETATION OF DEEP CNN REP- RESENTATIONS AS PERCEPTUAL QUALITY FEATURES
```
Abstract:
```
Pre-trained Deep Convolutional Neural Network (CNN) features have popularly been used as full-reference perceptual quality features for CNN based image quality assessment, super-resolution, image restoration and a variety of image- to-image translation problems. In this paper, to get more insight, we link basic human visual perception to characteristics of learned deep CNN representations as a novel and first attempt to interpret them. We characterize the frequency and orientation tuning of channels in trained image classification deep CNNs (e.g., VGG-16) by applying grating stimuli of different spatial frequencies and orienta- tions as input. We observe that the behavior of CNN channels as spatial frequency and orientation selective filters can be used to link basic human visual perception models to their characteristics. Doing so, we develop a theory to get more in- sight into deep CNN representations as perceptual quality features. We conclude that sensitivity to spatial frequencies that have lower contrast masking thresholds in human visual perception and a definite and strong orientation selectivity are important attributes of deep CNN channels that deliver better perceptual quality features.
```

Figures/Tables Captions:
```
Figure 1: Experimental Setup. The network is stimulated by gratings of varying spatial frequency. The responses of different feature maps are recorded as activation vs spatial frequency data. To quantify orientation tuning, the network is stimulated by gratings of fixed spatial frequency and varying orientations to record mean activation vs orientation data.
Figure 2: Characterizing spatial frequency and orientation tuning in channels across different layers of the pre-trained VGG-16.
Figure 3: Two different feature maps may have different sensitivities to important visual frequencies.
Figure 4: Correlation of metric scores in Eq. 1 with human subjective DMOS for the 'fire2 ReLU exp2x2' layer of the 'SqueezeNet'. It can be seen that the metric in Eq. 1 with the channel subset H-10 has a much better correlation with DMOS, compared to Eq. 1 with the L-10 subset of channels.
Table 1: Objective Quality Assessment Test. The correlation of metric scores delivered by Eq. 1 (for different feature subsets) with human subjective assessment of perceptual quality, quantified by DMOS.
Table 2: 2AFC Similarity Test. How well metric decisions conform with human assessment of image triplets .
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Quantifying human perception of image quality has been a subject of significant research for quite some time. Full-reference objective metrics such as the PSNR (Peak Signal to Noise Ratio) and SSIM (Structural Similarity Index) ( Wang et al. (2004) ), being fair metrics of distortion between two images, are not a satisfactory metrics to measure differences in perceptual quality. Considering the recent interest in the applications of deep CNNs in perception-oriented problems such as super- resolution, image-restoration, frame-interpolation and style-transfer etc, research into effective loss metrics that quantify perceptual quality and help train CNNs in delivering better perceptual quality has become paramount. The perceptual loss proposed by  Johnson et al. (2016)  was one of the first to demonstrate how ef- fective the feature representations of pre-trained image classification CNNs could be as features of full-reference perceptual quality, especially when incorporated into loss functions for image restora- tion. The perceptual loss is now popularly adopted in many image restoration problems such as super-resolution, style transfer, denoising etc. ( Ledig et al. (2017) , Wang et al. (2018) , Gatys et al. (2016) ).  Zhang et al. (2018)  and  Blau & Michaeli (2018)  further demonstrate how effective deep CNN representations can be as features of perceptual quality, but without any analysis into their characteristics. More recently,  Mechrez et al. (2018)  proposed a variation of the perceptual loss called the contextual loss, which still employs deep CNN features as perceptual quality features but uses an approximation of the KL-divergence to quantify distance. The contextual loss has been demonstrated to be quite effective in maintaining natural image statistics during SISR (Single-Image Super-Resolution). The recent PIRM Super-Resolution Challenge Report ( Blau et al. (2018) ) clearly iterates that the perceptual loss and the contextual loss are the most widely used loss functions for CNN based perceptual image Super-Resolution. Nevertheless, like most applications of deep learning, there has been little or no effort to understand and interpret the role of deep CNN representations as effective perceptual quality features. This is quite understandable, as it is difficult to find a direction to approach this problem from. Neural networks are non-linear, which makes a tractable analysis tricky. Furthermore, human perception of Under review as a conference paper at ICLR 2020 quality is also something that is still not understood completely. Most of our basic understanding of human visual perception of quality is in the frequency domain, with models such as the Contrast Sensitivity Function (CSF) ( de Faria et al. (1998) ). To make a connection between deep CNN features and human perception, it is important to realize that deep CNN channels are essentially complex spatial frequency and orientation selective filters. We stimulate pre-trained image classification CNNs with sinusoidal grating stimuli, record the re- sponse in the form of mean activation of each channel as function of spatial frequency/orientation of input grating, thus quantifying the frequency and orientation selectivity of different channels. This approach makes it significantly easier to establish a connection between perception models such as the CSF with learned deep feature representations. We hypothesize that two attributes are im- portant for deep CNN channels that are good perceptual quality features. The attributes are based on visual masking in human visual perception, which refers to human ability to perceive distor- tions and contrast in visual stimulus. The first attribute is sensitivity to spatial frequencies at which there is minimal contrast masking in human visual perception ( Nadenau et al. (2000) ), making the CNN channel sensitive to highly perceivable distortions. The second attribute being a definite and strong orientation selectivity, which helps the channel respond better to image regions with less pat- tern complexity, where there is less masking for distortions from a perceptual standpoint ( Wu et al. (2017) ). We verify our hypothesis by designing an Objective Quality Assessment (OQA) experiment ( Sheikh et al. (2006) ). OQA experiments correlate the performance of any quality metric with human per- ception of quality, which is an accepted and standard experimental technique. We group the set of channels in different CNN layers into subsets on the basis of our hypothesis and demonstrate that the group which has channels with our described attributes, delivers a much better as a set of per- ceptual quality features. We repeat our experiment across multiple layers of many pre-trained image classification networks such as the VGG-16 ( Simonyan & Zisserman (2014) ), AlexNet ( Krizhevsky & Hinton (2012) ), ShuffleNet ( Zhang et al. (2017) ) and SqueezeNet ( Iandola et al. (2017) ).

Section Title: DEEP CNN REPRESENTATIONS AS PERCEPTUAL QUALITY FEATURES
  DEEP CNN REPRESENTATIONS AS PERCEPTUAL QUALITY FEATURES The main motivation behind using pre-trained image classification deep CNN representations as perceptual quality features is that instead of a distance measure between two images being a good FR metric, computing distance after non-linear transformation of images into a high dimensional manifold, might result in a better perceptual quality measure. The high dimensional manifold in this case is the manifold of pre-trained CNN features. The general form for the perceptual loss ( Johnson et al. (2016) ) is given by Eq. (1) Where 'Φ k m ' is the feature map corresponding to the 'm th ' channel in the 'k th ' layer which as 'M' number of total channels with feature map dimensions 'H·W'. As mentioned before, applying pre- trained deep CNN representations as perceptual quality features has proven to be quite effective in FR-IQA methods ( Bosse et al. (2017) ), image restoration ( Wang et al. (2018) ) and style transfer ( Gatys et al. (2016) ) problems, as iterated by  Blau et al. (2018) . However, little else is known of the ability and characteristics of deep CNN representations as per- ceptual quality features. In this work, using basic human perception models, we aim to get more insight into the role of pre-trained deep representations as perceptual quality features.

Section Title: PROBLEM FORMULATION
  PROBLEM FORMULATION Section. 2 iterates the motivation and wide spread use of pre-trained deep CNN representations as features of full-reference perceptual quality. However, there has been no effort to explain and interpret the role of deep representations as perceptual features. We consider a CNN convolution layer as collection of channels which deliver perceptual quality features. For example, the relu3 2 layer of the VGG-16 has 256 channels. Are all of the channels equally effective in delivering good Under review as a conference paper at ICLR 2020 perceptual quality features? Are some channels better than others and if so, what attributes make them better? The problem in question is important in explaining the role of deep CNN representations as percep- tual quality features, but it is somewhat difficult to approach because of the 'black-box' nature of neural networks. In section. 4.1, we will introduce a methodology to quantify the spatial frequency and orientation tuning of channels in pre-trained CNNs. Using this formulation, we will interpret and explain deep CNN features as perceptual quality features by making use of basic human visual perception models, which rely on spatial frequency and orientation characteristics of input stimuli. In essence, the formulation in Section. 4.1 will act as a bridge to link attributes of deep representa- tions and basic visual perception.

Section Title: FREQUENCY/ORIENTATION TUNING QUANTIFICATION
  FREQUENCY/ORIENTATION TUNING QUANTIFICATION Our experimental method is inspired by the grating stimulus experiments used by neuro-scientists to study human visual perception characteristics ( Kulikowski et al. (1982) ). We aim to quantify both the spatial frequency and orientation tuning of different channels in the pre-trained CNN. To quantify the spatial frequency tuning, we generate concentric sinusoidal gratings of a fixed con- trast and varying spatial frequencies (cycles per degree), use them to stimulate pre-trained image classification CNNs and record the responses of the feature maps in the form of mean activation ver- sus spatial frequency for each channel.  Fig. 1  illustrates the overall scheme of measuring the spatial frequency responses of channels in various convolution layers of the trained VGG-16 network. The reason we are using a concentric pattern is to eliminate the factor of orientation selectivity from this part of the analysis. Some concentric grating stimulus patterns are shown as input to the trained VGG-16 network in Fig. 2.(a). To quantify orientation selectivity at low contrast masking thresholds, we stimulate the pre-trained network with linear pattern sinusoidal gratings with different orientations. The gratings have a fixed spatial frequency, which corresponds to the peak of the Contrast Sensitivity Function (CSF). Some sample grating patterns are shown in  Fig. 1 . Sample observations of orientation selectivity for channels in different layers of the pre-trained VGG-16 are shown in Fig. 2.(b).

Section Title: VISUAL FREQUENCY SENSITIVITY
  VISUAL FREQUENCY SENSITIVITY In this section, we will use the spatial frequency selectivity quantification in section. 4.1 to intro- duce the concept of visual frequency sensitivity. Human perception of images is largely dependent on attributes of input stimulus. A significant proportion of neuro-science research advocates the role of the early visual cortex as a spatial frequency analyzer ( Maffei & Fiorentini (1973) ). Hu- man perception characteristics are dependent on spatial frequency and one of the most significant models that quantifies this characteristic is called the Contrast Sensitivity Function (CSF). The CSF depicts human ability to perceive contrast changes as a function of spatial frequency. The spatial frequencies where the CSF has a higher value, correspond to lower contrast masking thresholds in perception (ease in perceiving distortions and contrast changes). In essence, this corresponds to a higher probability of perceiving distortions at high CSF valued spatial frequencies. Considering the presented analysis on the spatial frequency selective behavior of deep feature maps. Our hypothesis is that the deep representations that are more sensitive to high CSF valued spatial frequencies, can be better features of perceptual quality.  Fig. 3  plots mean activation of two channels versus spatial frequency of the input grating. Feature Map-2 can be seen to have a higher sensitivity compared to Feature Map-1 at high CSF valued spatial frequencies, making Feature Map-2 more sensitive to distortions corresponding to low contrast masking threshold regions in input images. We model this attribute quantitatively as µ 1 defined in Eq. 2 µ 1 (k, m) = f CSF (f ). ∂a k m ∂f (2) where 'k' is the index for the convolution layer, 'm' is the feature map index in each convolution layer, 'CSF' is the contrast sensitivity function (CSF), 'a' is the mean activation of the feature map and 'f ' is the spatial frequency in cycles per degree. µ 1 quantifies the average sensitivity of a CNN channel weighted by the CSF over different spatial frequencies. The channels having higher µ 1 values should deliver better perceptual features according to our hypothesis, because they can be more sensitive to visually perceivable distortions in input images.

Section Title: ORIENTATION SELECTIVITY
  ORIENTATION SELECTIVITY In addition to the underlying spatial frequency, orientation also plays an important part in human per- ception of visual stimulus. Neuro-science research indicates that the HVS (Human Visual System) is highly adapted to extract repeated patterns for visual content representation ( Wu et al. (2017) ). The complexity of a visual pattern has an effect in its perception. If a pattern is regular, the visual Under review as a conference paper at ICLR 2020 masking for such a pattern is weak, and distortions are easily perceivable. For complex and irregular image patches, the visual system presents a stronger masking effect. We have quantified orientation selectivity of different channels in a pre-trained image classification CNN (VGG-16) in Fig. 2.(b). We observe that a significant proportion of channels show a definite orientation selective tuning, such as the ones represented in Fig. 2(b)(a), Fig. 2(b)(b), Fig. 2(b)(j) etc. There channels should in theory be more sensitive in responding to simple patterns. However, quite a few channels show weaker orientation sensitivity such as the ones represented in Fig. 2(b)(c), Fig. 2(b)(k), Fig. 2(b)(n) and Fig. 2(b)(o) etc. We hypothesize the channels that show strong and definite orientation selective tuning, respond better to regular image patterns, which have lower masking thresholds, making these channels deliver better perceptual quality features. Suppose that within some layer 'k', a m θ be the mean activation of a feature map corresponding to channel 'm' to the input grating of orientation 'θ'. Let the maximum mean activation beâ m = max θ a m θ . We model our orientation selectivity attribute for a channel as µ 2 in Eq. (3). Considering our hypothesis, channels with higher µ 2 should deliver relatively better features of perceptual quality.

Section Title: PERCEPTUAL EFFICACY SCORE (PE)
  PERCEPTUAL EFFICACY SCORE (PE) Based on our defined attributes, we devise a quantification for the efficacy of channels in pre-trained deep CNNs to deliver good features for perceptual quality, called the Perceptual Efficacy (PE). The perceptual efficacy of a channel with index 'm' in layer 'k' is defined as the product of normalized µ 2 and µ 2 .

Section Title: EXPERIMENTAL SETUP
  EXPERIMENTAL SETUP We devise an experimental methodology to verify our hypotheses that deep CNN representations that have a higher PE are better perceptual quality features. Let F k be the set of all channels within a layer 'k' of a pre-trained CNN (e.g VGG-16). We constitute subsets of channels from F k based on the quantification of our proposed attributes (PE). For example, if there are 128 channels in the relu2 2 layer of the VGG-16, we can group the top 15% (19 channels) of the total 128 with the highest PE as Under review as a conference paper at ICLR 2020 Similarly, the bottom 15% channels with the lowest PE can be represented as L-15 = {Φ k 0 , Φ k 1 , . . . , Φ k m } (7) where H-x,L-x ⊆ F k and x ∈ (0, 100]. For x = 100, the subsets become the complete set of channels F k . To validate our hypotheses, it is necessary to demonstrate that subsets containing higher PE val- ued channels deliver better perceptual quality features compared to subsets with lower PE valued channels.

Section Title: OBJECTIVE QUALITY ASSESSMENT (OQA) TESTS
  OBJECTIVE QUALITY ASSESSMENT (OQA) TESTS OQA tests correlate the performance of any quality metric, with human subjective assessment and perception of quality ( Sheikh et al. (2006) ). Human assessment of perceptual image quality is quan- tified using the Differential Mean Opinion Score (DMOS) over images with varying levels of distor- tion. DMOS is a quantitative representation of how human observers are able to perceive perceptual differences between natural and distorted images. Metrics that have higher correlation with DMOS scores after regression, measured using statistical indicators such as the RMSE (Root Mean Square Error), LCC (Linear Correlation Coefficient) and the SROCC (Spearman Rank Order Correlation Coefficient), are regarded as better quality metrics ( Sheikh et al. (2006) ) . The higher the correlation metrics (LCC and SROCC), the better a metric correlates with human ability to perceive perceptual differences. In our problem setting, we will use Eq. 1 with the different subsets of channels, as defined in Section 5. We demonstrate that for use with Eq. 1, within different CNN layers, channels having higher PE, give much better correlation with DMOS compared to channels with lower PE. Essentially, we demonstrate that CNN channels with our pre-described attributes are indeed better perceptual quallity features. We use images and DMOS scores from both the LIVE image quality dataset ( Sheikh et al. (2006) ) and multiple distortion dataset ( Jayaraman et al. (2012) ) which collectively include images with Gaussian Blur, JPEG compression, JPEG2000, White Noise as well as images which have been corrupted with multiple types of distortions (such as white noise, Gaussian blur and camera noise) simultaneously. We will repeat our experiment accross multiple layers of several pre-trained image classification CNNs such as AlexNet, ShuffleNet, SqueezeNet and VGG-16.

Section Title: 2AFC SIMILARITY TESTS
  2AFC SIMILARITY TESTS In the 2AFC test, two distorted images are shown to an observer and he/she is asked to rate which one is closer to the ground truth, in perceptual appearance. This process is repeated for multiple image triplets and observers per-triplet to construct a data-set called the Berkley-Adobe Perceptual Patch Similarity Data-set (BAPPS) ( Zhang et al. (2018) ). Objective metrics such as the one in Eq. 1 are thereafter evaluated to see how well they conform to the pair-wise human judgment. For example, in an image triplet, let x 0 and x 1 be two distorted versions of the ground truth image x g that are shown to 5 human observers, 4 of which judge x 0 to be closer to x g , as opposed to x 1 being closer to x g . If an objective metric evaluates x 0 to be closer to the ground truth, it will get an 80% credit which in the opposite case would be 20%. The BAPPS data-set contains images with distortions such as super-resolution, frame-interpolation and deblurring, which do not have subjective DMOS data-sets available online. Therefore, as a secondary experiment, we perform a 2AFC test on super-resolution, frame-interpolation and video- deblurred frame images in the BAPPS data-set with Eq. 1 for different channel subsets defined in Section. 5. In order to verify our hypothesis, we will show that subsets that contain channels with higher PE, deliver better perceptual quality features.

Section Title: RESULTS AND DISCUSSIONS
  RESULTS AND DISCUSSIONS   Table 1  quantifies the correlation of Eq. 1 with DMOS for different subsets of channels, constructed on the basis of our described attributes, as explained in Section 5.  Table 1  validates our hypothesis that within a CNN layer, channels which have higher PE (Eq. 4) deliver better perceptual quality features. It must be reminded that higher LCC and SROCC indicate better correlation of Eq. 1 with human ability to discern perceptual differences (DMOS). It can be observed that very small proportions (2%-10% of total) of channels with the highest PE, deliver better perceptual quality features compared to a much higher proportion (50%-90% of total) of channels having lower PE. Furthermore, in a majority of cases, a small proportion of channels that have our described attributes (higher PE), perform even better than the complete set of channels in the layer. This implies that our proposed attributes are indeed important characteristics that make learned deep CNN representations good perceptual quality features.  Table. 2  shows the results of our secondary 2AFC similarity test on the super-resolution, frame- interpolation and video-deblurring distorted images in the BAPPS data-set. It can been seen that yet again, similar to the conclusion in the primary QQA experiment, the subsets with channels having higher PE are better perceptual quality features compared to even much larger subsets having channels with lower PE.

Section Title: FUTURE WORK
  FUTURE WORK We have proposed a model to explain and interpret which channels in pre-trained image classification CNNs deliver better perceptual quality features. The model may be used to improve the use of deep representations as perceptual quality features by helping in feature selection for IQA methods such as ( Bosse et al. (2017) ) and maybe designing channel attentive mechanism to improve the perceptual loss ( Johnson et al. (2016) ). The model may also be reference for learning better perceptual quality feature representations which may benefit a wide variety of applications. Furthermore, the model may be enhanced to include more psychophysical factors such as eccentricity etc. Another possible application may be CNN-based image compression where prior knowledge of the potential efficacy of different channels may help efficient perceptual compression of redundant image data.

Section Title: CONCLUSIONS
  CONCLUSIONS Deep CNN representations of pre-trained image classifications CNNs have been popularly used as perceptual quality features for perception orientated applications such as CNN based quality assess- ment, image/video super-resolution and many image-to-image translation problems. In this paper, as a novel and first effort, we have linked basic human visual perception models to pre-trained deep CNN representations in order to explain and interpret them as perceptual quality features. Based on Under review as a conference paper at ICLR 2020 masking characteristics in human visual perception, we formulate attributes of channels in different layers of pre-trained networks, and experimentally demonstrate that the attributes are important char- acteristics that make some deep CNN representations better perceptual quality features compared to others.

Section Title: A GRATING GENERATION
  A GRATING GENERATION In this section, we present details behind the generation of sinusoidal gratings of different spatial frequencies. The contrast sensitivity function is expressed on the domain of spatial frequency in cycles per degree (cyc/deg). The cycles per degree express the number of sine cycles captured by the observer per unit degree of observation. Obviously, the distance of viewing and dimensions of the screen play an important part in this measurement. We essentially generate gratings in the computer simulation in cycles per pixel. Let the display screen being used in the experiment have a height 'h' inches and resolution 'r' pixels per inch. The optimal viewing distance in psychovisual experiments should satisfy a function called the PVD (42 (2002)). The PVD is a function that expresses the optimal ratio of viewing distance to the height of the display screen. The optimal viewing distance 'd' for the screen with height 'h' can be calculated using the PVD. The transformation between cycles/degree and cycles/pixel is cycles pixel = cycles degree × degrees pixel (8)

```
