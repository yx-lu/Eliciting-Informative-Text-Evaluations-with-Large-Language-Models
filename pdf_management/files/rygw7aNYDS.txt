Title:
```
Under review as a conference paper at ICLR 2020 Efficient Inference and Exploration for Rein- forcement Learning
```
Abstract:
```
Despite an ever growing literature on reinforcement learning algorithms and ap- plications, much less is known about their statistical inference. In this paper, we investigate the large-sample behaviors of the Q-value estimates with closed-form characterizations of the asymptotic variances. This allows us to efficiently construct confidence regions for Q-value and optimal value functions, and to develop policies to minimize their estimation errors. This also leads to a policy exploration strategy that relies on estimating the relative discrepancies among the Q estimates. Numeri- cal experiments show superior performances of our exploration strategy than other benchmark approaches.
```

Figures/Tables Captions:
```
Figure 1: RiverSwim Problem
Table 1: Exact tabular update
Table 2: Approximate value iteration
Table 3: Probability of correct selection for different exploration policies, n = 10 3
Table 4: Probability of correct selection for different exploration policies, n = 10 4
```

Main Content:
```

Section Title: Introduction
  Introduction We consider the classical reinforcement learning (RL) problem where the agent interacts with a random environment and aims to maximize the accumulated discounted reward over time. The environment is formulated as a Markov decision process (MDP) and the agent is uncertain about the true dynamics to start with. As the agent interacts with the environment, data about the system dynamics are collected and the agent becomes increasingly confident about her decision. With finite data, however, the potential reward from each decision is estimated with errors and the agent may be led to a suboptimal decision. Our focus in this paper is on statistically efficient methodologies to quantify these errors and uncertainties, and to demonstrate their use in obtaining better policies. More precisely, we investigate the large-sample behaviors of estimated Q-value, optimal value function, and their associated policies. Our results are in the form of asymptotic convergence to an explicitly identified and computable Gaussian (or other) distribution, as the collected data sizes increase. The motivation of our investigation is three-fold. First, these precise asymptotic statements allow us to construct accurate confidence regions for quantities related to the optimal policy, and, like classical statistical inference, they can assess the reliability of the current estimates with respect to the data noises. Second, our results complement some finite-sample error bounds developed in the literature ( Kearns & Singh, 1998 ;  Kakade, 2003 ;  Munos & Szepesvári, 2008 ), by supplementing a closed-form asymptotic variance that often shows up in the first-order terms in these bounds. Our third and most important motivation is to design good exploration policies by directly using our tight error estimates. Motivated by recent autonomous-driving and other applications (e.g.,  Kalashnikov et al. (2018) ), we consider the pure exploration setting where an agent is first assigned an initial period to collect as much experience as possible, and then, with the optimal policy trained offline, starts deployment to gain reward. We propose an efficient strategy to explore by optimizing the worst-case estimated relative discrepancy among the Q-values (ratio of mean squared difference to variance), which provides a proxy for the probability of selecting the best policy. Similar criteria have appeared in the so-called optimal computing budget allocation (OCBA) Under review as a conference paper at ICLR 2020 procedure in simulation-based optimization ( Chen & Lee, 2011 ) (a problem closely related to best-arm identification (Audibert & Bubeck, 2010) in online learning). In this approach, one divides computation (or observation) budget into stages in which one sequentially updates mean and variance estimates, and optimizes next-stage budget allocations according to the worst-case relative discrepancy criterion. Our proposed procedure, which we term Q-OCBA, follows this idea with a crucial use of our Q-value estimates and randomized policies to achieve the optimal allocation. We demonstrate how this idea consistently outperforms other benchmark exploration policies, both in terms of the probability in selecting the best policy and generating the tightest confidence bounds for value estimates at the end of the exploration period. Regarding the problem of constructing tight error estimates in RL, the closest work to ours is  Mannor et al. (2004 ;  2007 ), which studies the bias and variance in value function estimates with a fixed policy. Our technique resolves a main technical challenge in  Mannor et al. (2004 ;  2007 ), which allows us to substantially generalize their variance results to Q-values, optimal value functions and asymptotic distributional statements. The derivation in  Mannor et al. (2004 ;  2007 ) hinges on an expansion of the value function in terms of the perturbation of the transition matrix, which (as pointed out by the authors) is not easily extendable from a fixed-policy to the optimal value function. In contrast, our results utilize an implicit function theorem applied to the Bellman equation that can be verified to be sufficiently smooth. This idea turns out to allow us to obtain gradients for Q-values, translate to the optimal value function, and furthermore generalize to similar results for constrained MDP and approximate value iterations. We also relate our work to the line of studies on dynamic treatment regimes (DTR) ( Laber et al., 2014 ) applied commonly in medical decision-making, which focuses on the statistical properties of polices on finite horizon (such as two-period). Our infinite-horizon results on the optimal value and Q-value distinguishes our developments from the DTR literature. Moreover, our result on the non-unique policy case can be demonstrated to correspond to the "non-regularity" concept in DTR, where the true parameters are very close to the decision "boundaries" that switch the optimal policy (motivated by situations of small treatment effects), thus making the obtained policy highly sensitive to estimation noises. In the rest of this paper, we first describe our MDP setup and notations (Section 2). Then we present our results on large-sample behaviors (Section 3), demonstrate their use in exploration strategies (Section 4), and finally substantiate our findings with experimental results (Section 5). In the Appendix, we first present generalizations of our theoretical results to constrained MDP (A.1) and problems using approximate value iteration (A.2). Then we include more numerical experiments (B), followed by all the proofs (C).

Section Title: Problem Setup
  Problem Setup Consider an infinite horizon discounted reward MDP, M = (S, A, R, P, γ, ρ), where S is the state space, A is the action space, R(s, a) denotes the random reward when the agent is in state s ∈ S and selects action a ∈ A, P (s |s, a) is the probability of transitioning to state s in the next epoch given current state s and taken action a, γ is the discount factor, and ρ is the initial state distribution. The distribution of the reward R and the transition probability P are unknown to the agent. We assume both S and A are finite sets. Without loss of generality, we denote S = {1, 2, . . . , m s } and A = {1, 2, . . . , m a }. Finally, we make the following stochasticity assumption: Assumption 1. R(s, a) has finite mean µ R (s, a) and finite variance σ 2 R (s, a) ∀ s ∈ S, a ∈ A. For any given s ∈ S and a ∈ A, R(s, a) and S ∼ P (·|s, a) are all independent random variables. A policy π is a mapping from each state s ∈ S to a probability measure over actions a ∈ A. Specifically, we write π(a|s) as the probability of taking action a when the agent is in state s and π(·|s) as the m a - dimensional vector of action probabilities at state s. For convenience, we sometimes write π(s) as the Under review as a conference paper at ICLR 2020 realized action given the current state is s. The value function associated with a policy π is defined as V π (s) = E[ ∞ t=0 γ t R(s t , π(s t ))|s 0 = s] with s t+1 ∼ P (.|s t , π(s t )). The expected value function, under the initial distribution ρ, is denoted by χ π = s ρ(s)V π (s). A policy π * is said to be optimal if V π * (s) = max π V π (s) for all s ∈ S. For convenience, we denote V * = V π * and χ * = s ρ(s)V * (s). The Q-value, denoted by Q(s, a), is defined as Q(s, a) = µ R (s, a) + γE[V * (S )|s, a]. Correspondingly, V * (s) = max a Q(s, a) and the Bellman equation for Q takes the form Q(s, a) = µ R (s, a) + γE max a Q(s , a )|s, a , (1) for any (s, a) ∈ S × A. Denoting the Bellman operator as T µ R ,P (·), Q is a fixed point associated with T µ R ,P , i.e. Q = T µ R ,P (Q). For the most part of this paper we make the following assumption about Q: Assumption 2. For any state s ∈ S, arg max a∈A Q(s, a) is unique. Under Assumption 2, the optimal policy π * is unique and deterministic. Let a * (s) = arg max a∈A Q(s, a). Then π * (a|s) = 1 (a = a * (s)), where 1(·) denotes the indicator function. We next introduce some statistical quantities arising from data. Suppose we have n ob- servations (whose collection mechanism will be made precise later), which we denote as {(s t , a t , r t (s t , a t ), s t (s t , a t )) : 1 ≤ t ≤ n}, where r t (s t , a t ) is the realized reward at time t and s t (s t , a t ) = s t+1 . We define the sample meanμ R,n and the sample varianceσ 2 R,n of the reward aŝ and its m s × m s sampling covariance matrix Σ Ps,a (with one sample point of 1(s t = s, a t = a)) as With the data, we construct our estimate of Q, calledQ n , which is the empirical fixed point of We shall focus on the empirical errors due to noises of the collected data, and assume the MDP or Q-value evaluation can be done off-line so that the fixed point equation forQ n can be solved exactly. .

Section Title: Quantifying Asymptotic Estimation Errors
  Quantifying Asymptotic Estimation Errors We present an array of results regarding the asymptotic behaviors ofQ n andV * n . To prepare, we first make an assumption on our exploration policy π to gather data. Define the extended transition probabilityP π asP π (s , a |s, a) = P (s |s, a)π(a |s ). We make the assumption: Under review as a conference paper at ICLR 2020 Assumption 3. The Markov chain with transition probabilityP π is positive recurrent. Under Assumption 3,P π has a unique stationary distribution, denoted w, equal to the long run frequency in visiting each state-action pair, i.e. w(s, a) = lim n→∞ 1 n 1≤t≤n 1(s t = i, a t = j), where all w(s, a)'s are positive. Note that Assumption 3 is satisfied if for any two states s, s , there exists a sequence of actions such that s is attainable from s under P , and, moreover, if π is sufficiently mixed, e.g., π satisfies π(a |s ) > 0 for all s , a . Our results in the sequel uses the following further notations. We denote "⇒" as "convergence in distribution", and N (µ, Σ) as a multivariate Gaussian distribution with mean vector µ and covariance matrix Σ. We write I as the identity matrix, and e i as the i-th unit vector. The dimension of N (µ, Σ), I and e i should be clear from the context. When not specified, all the vectors are column vectors. Let N = m s m a . In our algebraic derivations, we need to re-arrange µ R , Q and w as N -dimensional vectors. We thus define the following indexing rule: (s = i, a = j) is re-indexed as (i − 1)m a + j, e.g. µ R (i, j) = µ R ((i − 1)m a + j). We also need to re-arrangeP π as an N × N matrix following the same indexing rule, i.e.P π (i , j |i, j) =P π ((i − 1)m a + j, (i − 1)m a + j ).

Section Title: Limit Theorems under Sufficient Exploration
  Limit Theorems under Sufficient Exploration We first establish the asymptotic normality ofQ n under exploration policy π: Theorem 1. Under Assumptions 1 and 2, if the data is collected according to π satisfying Assumption 3, thenQ n is a strongly consistent estimator of Q, i.e.Q n → Q almost surely as n → ∞. Moreover, W , D R and D Q are N × N diagonal matrices with In addition to the asymptotic Gaussian behavior, a key element of Theorem 1 is the explicit form of the asymptotic variance Σ. This is derived from the delta method ( Serfling, 2009 ) and, intuitively, is the product of the sensitivities (i.e., gradient) of Q with respect to its parameters and the variances of the parameter estimates. Here the parameters are µ R and P , with corresponding gradients (I − γP π * ) −1 and (I − γP π * ) −1 V * . The variances of these parameter estimates (i.e., (2) and (4)) involve σ 2 R (i, j) and Σ Pi,j , and the sample size allocated to estimate each parameter, which is proportional to w(i, j).

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 In the Appendix we also prove, using the same technique as above, a result on the large-sample behavior of the value function for a fixed policy (Corollary 2), which essentially recovers Corollary 4.1 in  Mannor et al. (2007) . Different from  Mannor et al. (2007) , we derive our results by using an implicit function theorem on the corresponding Bellman equation to obtain the gradient of Q, viewing the latter as the solution to the equation and as a function of µ R , P . This approach is able to generalize the results for fixed policies in  Mannor et al. (2007)  to the optimal value functions, and also provide distributional statements as Theorem 1 and Corollary 1 above. We also note that another potential route to obtain our results is to conduct perturbation analysis on the linear program (LP) representation of the MDP, which would also give gradient information of V * (and hence also Q), but using the implicit function theorem here seems sufficient.

Section Title: Non-Unique Optimal Policy
  Non-Unique Optimal Policy Suppose the optimal policy for the MDP M is not unique, i.e., Assumption 2 does not hold. In this situation, the estimatedQ n andV * n may "jump" around different optimal actions, leading to a more complicated large-sample behavior as described below: Theorem 2. Suppose Assumptions 1 and 3 hold but there is no unique optimal policy. Then there exists In the case that K > 1 in Theorem 2, the limit distribution becomes non-Gaussian. This arises because the sensitivity to P or µ R can be very different depending on the perturbation direction, which is a consequence of solution non-uniqueness that can be formalized as a non-degeneracy in the LP representation of the MDP. We note that this phenomenon is analogous to the "non-regularity" concept in DTR that arises because the "true" parameters in these problems are very close to the decision "boundaries", which makes the obtained policy highly sensitive to estimation noises and incurs a 1/ √ n-order bias behavior. Our case of non-unique optimal policy here captures precisely this same behavior, where we see in Theorem 2 that when K > 1 the asymptotic limit no longer has mean zero and consequently a 1/ √ n-order bias arises. We also develop two other generalizations of large-sample results, for constrained MDP and approxi- mate value iteration respectively (see Appendices A.1 and A.2).

Section Title: Efficient Exploration Policy
  Efficient Exploration Policy We utilize our results in Section 3 to design exploration policies. We focus on the setting where an agent is assigned a period to collect data by running the state transition with an exploration policy. The goal is to obtain the best policy at the end of the period in a probabilistic sense, i.e., minimize the probability of selecting a suboptimal policy for the accumulated reward. We propose a strategy that maximizes the worst-case relative discrepancy among all Q-value estimates. More precisely, we define, for i ∈ S, j ∈ A and j = a * (i), the relative discrepancy as h ij = (Q(i, a * (i)) − Q(i, j)) 2 /σ 2 ∆Q (i, a * (i), j), where σ 2 ∆Q (i, a * (i), j) is defined in (6). Our procedure attempts to maximize the minimum of h ij 's, max w∈Wη min i∈S min j∈A,j =a * (i) h ij , (7) where w denotes the proportions of visits on the state-action pairs, within some allocation set W η (which we will explain). Intuitively, h ij captures the relative "difficulty" in obtaining the optimal policy given the estimation errors of Q's. If the Q-values are far apart, or if the estimation variance is small, then h ij is large which signifies an "easy" problem, and vice versa. Criterion (7) thus aims to make the problem the "easiest". Alternatively, one can also interpret (7) from a large deviations view ( Glynn & Juneja, 2004 ;  Dong & Zhu, 2016 ). Suppose the Q-values for state i between two different actions a * (i) and j are very close. Then, one can show that the probability of suboptimal selection between the two has roughly an exponential decay rate controlled by h ij . Obviously, there can be many more comparisons to consider, but the exponential form dictates that the smallest decay rate dominates the calculation, thus leading to the inner min's in (7). Criterion like (7) is motivated from the OCBA procedure in simulation optimization (which historically has considered simple mean-value alternatives ( Chen & Lee, 2011 )). Here, we consider the Q-values. For convenience, we call our procedure Q-OCBA. Implementing criterion (7) requires two additional considerations. First, solving (7) needs the model primitives Q, P and σ 2 R that appear in the expression of h ij . These quantities are unknown a priori, but as we collect data they can be sequentially estimated. This leads to a multi-stage optimization plus parameter update scheme. Second, since data are collected through running a Markov chain on the exploration actions, not all allocation w is admissible, i.e., realizable as the stationary distribution of the MDP. To resolve this latter issue, we will derive a convenient characterization for admissibility. Call π(·|s) admissible if the Markov Chain with transition probabilityP π , defined for Assump- tion 3, is positive recurrent, and denote w π as its stationary distribution. Define the set The following provides a characteriza- tion of the set of admissible π: Lemma 1. For any admission policy π, w π ∈ W. For any w ∈ W, π w with π w (a = j|s = i) = w((i − 1)m a + j)/ ( ma k=1 w((i − 1)m a + k)) is an admissible policy. In other words, optimizing over the set of admissible policies is equivalent to optimizing over the set of stationary distributions. The latter is much more tractable thanks to the linear structure of W. In practice, we will use W η = W ∩ {w ≥ η} for some small η > 0 to ensure closedness of the set (our experiments use η = 10 −6 ). Algorithm 1 describes Q-OCBA. In our experiments shown next, we simply use two stages, i.e., K = 2. Finally, we also note that criterion like (7) can be modified according to the decision goal.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 For example, if one is interested in obtaining the best estimate of χ * , then it would be more beneficial to consider min w∈Wη σ 2 χ . We showcase this with additional experiments in the Appendix. Input: Number of iterations K, length of each batch {B k } 1≤k≤K , initial exploration policy π 0 ; Algorithm 1: Q-OCBA sequential updating rule for exploration Note that (7) is equivalent to min w max i∈S max j∈A,j =a * (i) s,a c ij (s, a)/w s,a subject to w ∈ W η , where c ij (s, a)'s are non-negative coefficients. Based on the closed-form characterization of Σ in Theorem 1, c ij (s, a)'s can be estimated with plug-in estimators using data collected in earlier stages.

Section Title: Numerical Experiments
  Numerical Experiments We conduct several numerical experiments to support our large-sample results in Sections 3 and demonstrate the performance of Q-OCBA against some benchmark methods. We use the RiverSwim problem in ( Osband et al., 2013 ) with m s states and two actions at each state: swim left (0) or swim right (1) (see  Figure 1 ). The triplet above each arc represents i) the action, 0 or 1, ii) the transition probability to the next state given the current state and action, iii) the reward under the current state and action. Note that, in this problem, rewards are given only at the left and right boundary states (where the value of r L will be varied). We consider the infinite horizon setting with γ = 0.95 and ρ = [1/m s , . . . , 1/m s ] T . We first demonstrate the validity of our large-sample results. We use a policy that swims right with probability 0.8 at each state, i.e. π(1|s) = 0.8.  Tables 1  and 2 show the coverage rates of the constructed 95% CIs, for a small m s = 6 (using Theorem 1 and Corollary 1) and a large m s = 31 (using Theorem 4 in the Appendix) respectively. The latter case uses a linear interpolation with S 0 = {1, 4, . . . , 28, 31}. All coverage rates are estimated using 10 3 independent experimental repetitions (the bracketed numbers in the tables show the half-widths of 95% CI for the coverage estimates). For the Q-values, we report the average coverage rate over all (s, a) pairs. When the number of observations n is large enough (≥ 3 × 10 4 for exact update and ≥ 10 5 for interpolation), we see highly accurate CI coverages, i.e., close to 95%. Next we investigate the efficiency of our exploration policy. We compare Q-OCBA with K = 2 to four benchmark policies: i) -greedy with different values of , ii) random exploration (RE) with different values of π(1|s), iii) UCRL2 (a variant of UCRL) with δ = 0.05 ( Jaksch et al., 2010 ), iv) PSRL with different posterior updating frequencies ( Osband et al., 2013 ), i.e., PSRL(x) means PSRL is implemented with x episodes. We use m s = 6 and vary r L from 1 to 3. To ensure fairness, we use a two-stage implementation for all policies, with 30% of iterations first dedicated to RE (with π(1|s) = 0.6) as a warm start, i.e., the data are used to estimate the parameters needed for the second stage. To give enough benefit of the doubt, we notice the probabilities of correct selection for both UCRL2 and PSRL are much worse without the warm start.  Tables 3  and 4 compare the probabilities of obtaining the optimal policy (based on the estimated Q n 's). For -greedy, RE, and PSRL, we report the results with the parameters that give the best performances in our numerical experiments. The probability of correct selection is estimated using 10 3 replications of the procedure. We observe that Q-OCBA substantially outperforms the other methods, both with a small data size (n = 10 3 in  Table 3 ) and a larger one (n = 10 4 in  Table 4 ). Generally, these benchmark policies perform worse for larger values of r L . This is because for small r L , the (s, a) pairs that need to be explored more also tend to have larger Q-values. However, as r L increase, there is a misalignment between the Q-values and the (s, a) pairs that need more exploration. The superiority of our Q-OCBA in these experiments come as no surprise to us. The benchmark methods like UCRL2 and PSRL are designed to minimize regret which involves balancing the exploration-exploitation trade-off. On the other hand, Q-OCBA focuses on efficient exploration only, i.e., our goal is to minimize the probability of incorrect policy selection, and this is achieved by carefully utilizing the variance information gathered from the first stage that is made possible by our derived asymptotic formulas. We provide additional numerical results in Appendix B.

Section Title: A Additional Theoretical Results
  A Additional Theoretical Results In this section, we present additional results on large-sample behaviors for constrained MDPs and also estimations based on approximation value iteration.

Section Title: A.1 Constrained Problems
  A.1 Constrained Problems We consider the constrained MDP setting for budgeted decision-making ( Boutilier & Lu, 2016 ) and more recently safety-critical applications ( Achiam et al., 2017 ;  Chow et al., 2017 ). Suppose now we aim to maximize the long-run accumulated discounted reward, V π (s) = E[ ∞ t=0 γ t R(s t , π(s t ))|s 0 = s], while at the same time want to ensure that a long-run accumulated discounted cost, denoted as L π (s) = E[ ∞ t=0 γ t C(s t , π(s t ))|s 0 = s] which we call the loss function, is constrained by some given value η, i.e., We assume data coming in like before and, in addition, that we have observations on the incurred cost at each sample of (s, a). Call the empirical estimate of the costμ C,n . We follow our paradigm to solve the empirical counterpart of the problem, namely to find a policyπ * n that solves (8) by usinĝ V π n (s) andL π n (s) instead of V π (s) and L π (s), whereV π n (s)'s andL π n (s)'s are the value functions and loss functions evaluated using the empirical estimatesμ R,n ,μ C,n ,P n . We focus on the estimation error of the optimal value (instead of the feasibility, which could also be important but not pursued here). To understand the error, we first utilize an optimality characterization of constrained MDPs. In general, an optimal policy for (8) is a "split" policy ( Feinberg & Rothblum, 2012 ), namely, a policy that is deterministic except that at one particular state a randomization between two different actions is allowed. This characterization can be deduced from the associated LP using occupancy measures ( Altman, 1999 ). We call the randomization probability the mixing parameter α * , i.e., whenever this particular state, say s r , is visited, action a * 1 (s r ) is chosen with probability α * and action a * 2 (s r ) is chosen with probability 1 − α * . We then have the following result: Theorem 3. Suppose Assumptions 1 and 3 hold and there is a unique optimal policy. Moreover, assume that there is no deterministic policy π that satisfies s ρ(s)L π (s) = η. Then we have √ n(V * n − V * ) ⇒ N (0, Σ) as n → ∞, where one of the following cases hold:

```
