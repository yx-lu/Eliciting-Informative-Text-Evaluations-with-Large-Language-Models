Title:
```
Under review as a conference paper at ICLR 2020 PROVABLE REPRESENTATION LEARNING FOR IMITA- TION LEARNING VIA BI-LEVEL OPTIMIZATION
```
Abstract:
```
A common strategy in modern learning systems is to learn a representation which is useful for many tasks, a.k.a. representation learning. We study this strategy in the imitation learning setting for Markov decision processes (MDPs) where mul- tiple experts' trajectories are available. We formulate representation learning as a bi-level optimization problem where the "outer" optimization tries to learn the joint representation and the "inner" optimization encodes the imitation learning setup and tries to learn task-specific parameters. We instantiate this framework for the imitation learning settings of behavior cloning and observation-alone. Theo- retically, we provably show using our framework that representation learning can reduce the sample complexity of imitation learning in both settings. We also pro- vide proof-of-concept experiments to verify our theoretical findings.
```

Figures/Tables Captions:
```
Figure 1: Experiments for verifying theory. Left: validation loss on DirectedSwimmer. Right: average return on NoisyCombinationLock
Figure 2: Experiments on policy Optimization with representation trained by imitation learning Left: average return on the DirectedSwimmer. Right: average return on the NoisyCombinationLock.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Humans can often learn from experts quickly and with a few demonstrations and we would like our artificial agents to do the same. However, even for simple imitation learning tasks, the current state-of-the-art methods require thousand of demonstrations. Humans do not learn new skills from scratch. We can summarize learned skills, distill them and build a common ground, a.k.a, represen- tation that is useful for learning future skills. Can we build an agent to do the same? The current paper studies how to apply representation learning to imitation learning. Specifically, we want to build an agent that is able learn a representation from multiple experts' demonstrations, where the experts aim to solve different Markov decision processes (MDPs) that share the same state and action spaces but can differ in the transition and reward functions. The agent can use this representation to reduce the number of demonstrations required for a new imitation learning task. While several methods have been proposed ( Duan et al., 2017 ;  Finn et al., 2017b ; James et al., 2018) to build agents that can adapt quickly to new tasks, none of them, to our knowledge, give provable guarantees showing the benefit of using past experience. Furthermore, they do not focus on learning a representation. See Section 2 for more discussions. In this paper, we propose a framework to formulate this problem and analyze the statistical gains of representation learning. The main idea is to use bi-level optimization formulation where the "outer" optimization tries to learn the joint representation and the "inner" optimization encodes the imitation learning setup and tries to learn task-specific parameters. In particular, the inner optimization is flexible enough to allow the agent to interact with the environment. This framework allows us to do a rigorous analysis to show provable benefits of representation learning for imitation learning. With this framework at hand, we make the following concrete contributions: • We first instantiate our framework in the setting where the agent can observe experts' actions and tries to find a policy that matches the expert's policy, a.k.a, behavior cloning. This setting can be viewed as a straightforward extension of multi-task representation learning for supervised learning ( Maurer et al., 2016 ). We show in this setting that with sufficient number of experts (possibly optimizing for different reward functions), the agent can learn a representation that provably reduces the sample complexity for a new target imitation learning task. • Next, we consider a more challenging setting where the agent cannot observe experts' actions but only their states, a.k.a., the observation-alone setting. We set the inner optimization as a min- max problem inspired by  Sun et al. (2019) . Notably, this min-max problem requires the agent to interact with the environment to collect samples. We again show that with sufficient number of Under review as a conference paper at ICLR 2020 experts, the agent can learn a representation that provably reduces the sample complexity for a target task where the agent cannot observe actions from either source experts or the target expert. • We conduct experiments to verify our theoretical insights by learning a representation from multi- ple tasks using our framework and testing it using both behavior cloning and policy optimization. In these settings, we observe that by learning representations the agent can learn a good policy with fewer samples than needed to learn a policy from scratch. The key contribution is to connect existing literature on multi-task representation learning that deals with supervised learning ( Maurer et al., 2016 ) to single task imitation learning methods with guar- antees ( Syed & Schapire, 2010 ;  Ross et al., 2011 ;  Sun et al., 2019 ). To our knowledge, this is the first work showing such guarantees for general losses that are not necessarily convex.

Section Title: RELATED WORK
  RELATED WORK Representation learning has shown its great power in various domains. See  Bengio et al. (2013)  for a survey. Theoretically,  Maurer et al. (2016)  gave analysis showing representation can provably reduce the sample complexity in the multi-task supervised learning setting. Recently,  Arora et al. (2019)  analyzed the benefit of representation learning via contrastive learning. These papers all build representations for the agent / learner. We remark that researchers also try to build representations about the environment / physical world ( Wu et al., 2017 ). Imitation learning can help with sample efficiency of many problems ( Ross & Bagnell, 2010 ;  Sun et al., 2017 ;  Daumé et al., 2009 ;  Chang et al., 2015 ;  Pan et al., 2018 ). Most existing work con- sider the setting where the learner can observe expert's action. A general strategy is use supervised learning to learn a policy that maps the state to action that matches expert's behaviors. The most straightforward one is behavior cloning ( Pomerleau, 1991 ), which we also study in our paper. More advanced approaches have also been proposed ( Ross et al., 2011 ;  Ross & Bagnell, 2014 ;  Sun et al., 2018 ). These approaches, including behavior cloning, often enjoy sound theoretical guarantees in the single task case. Our paper extends the theoretical guarantees of behavior cloning to the multi- task representation learning setting. This paper also considers a more challenging setting, imitation learning from observation alone. Though some model-based methods have been proposed ( Torabi et al., 2018 ;  Edwards et al., 2018 ), these methods lack theoretical guarantees. Another line of work learns a policy that minimizes the difference between the state distributions induced by it and the expert policy, under a certain distri- butional metric ( Ho & Ermon, 2016 ).  Sun et al. (2019)  gave a theoretical analysis to characterize the sample complexity of this approach and our method for this setting is inspired by their approach. A line of work uses meta-learning for imitation learning ( Duan et al., 2017 ;  Finn et al., 2017b ; James et al., 2018). Our work is different from theirs as we want to explicitly learn a representation that is useful across all tasks whereas these work try to learn a meta-algorithm that can quickly adapt to a new task. For example,  Finn et al. (2017b)  used a gradient based method for adaptation. Recently  Raghu et al. (2019)  argued that most of the power of MAML ( Finn et al., 2017a ) like approaches comes from learning a shared representation. On the theoretical side of meta-learning and multi-task learning,  Baxter (2000)  performed the first theoretical analysis and gave sample complexity bounds using covering numbers.  Bullins et al. (2019)  provides an efficient algorithm that generalizes to new unseen tasks, but for linear repre- sentations. Another recent line of work analyzes gradient based meta-learning methods, similar to MAML ( Finn et al., 2017a ). Existing work on the sample complexity and regret of these meth- ods ( Denevi et al., 2019 ;  Finn et al., 2019 ;  Khodak et al., 2019 ) show guarantees for convex losses by leveraging tools from online convex optimization. In contrast, our analysis works for arbitrary function classes and the bounds depend on the gaussian averages of these classes. Recent work ( Rajeswaran et al., 2019 ) uses a bi-level optimization framework for meta-learning and improves computation (not statistical) aspects of meta-learning through implicit differentiation.

Section Title: PRELIMINARIES
  PRELIMINARIES Markov Decision Processes (MDPs): Let M = (S, A, P, C, ν) be an MDP, where S is the state space, A is the finite action space with |A| = K, H ∈ Z + is the planning horizon, P : S × A → Under review as a conference paper at ICLR 2020 (S) is the transition function, C : S × A → R is the cost function and ν ∈ (S) is the initial state distribution. We assume that cost is bounded by 1, i.e. C(s, a) ≤ 1, ∀s ∈ S, a ∈ A. This is a standard regularity condition used in many theoretical reinforcement learning work. A (stochastic) policy is defined as π = (π 1 , . . . , π H ), where π h : S → (A) prescribes a distribution over action for each state at level h ∈ [H]. For a stationary policy, we have π 1 = · · · = π H = π. A policy π induces a random trajectory s 1 , a 1 , s 2 , a 2 , . . . , s H , a H where s 1 ∼ ν, a 1 ∼ π 1 (s), s 2 ∼ P s1,a1 etc. Let ν π h denote the distribution over S induced at level h by policy π. The value function V π h : S → R is defined as The goal is to learn a policy π that minimizes the expected cost J(π) = E s1∼ν V π 1 (s 1 ). We define the Bellman operator at level h for any policy π as Γ π h : R S → R S , where for s ∈ S and g ∈ R S ,

Section Title: Multi-task Imitation learning
  Multi-task Imitation learning We formally describe the problem we want to study. We as- sume there are multiple tasks (MDPs) sampled i.i.d. from a distribution η. A task µ ∼ η is an MDP M µ = (S, A, H, P µ , C µ , ν µ ); all tasks share everything except the cost function, initial state distribution and transition function. For simplicity of presentation, we will assume a common tran- sition function P for all tasks; proofs remain exactly the same even otherwise. For every task µ, π * µ = (π * 1,µ , . . . , π * H,µ ) is an expert policy that the learner has access to in the form of trajectories induced by that policy. The trajectories may or may not contain expert's actions. These correspond to two settings that we discuss in more detail in Section 5 and Section 6. The distributions of states induced by this policy at different levels are denoted by {ν * 1,µ , . . . , ν * H,µ } and the average state dis- tribution as ν * µ = 1 H H h=1 ν * h,µ . We define V * h,µ to be the value function of π * µ and J µ to be the expected cost function for task µ. We will drop the subscript µ whenever the task at hand is clear from context. Of interest is also the special case where the expert policy π * µ is stationary.

Section Title: Representation learning
  Representation learning In this work, we wish to learn policies from a function class of the form Π = F • Φ, where Φ ⊆ {φ : S → R d | φ(s) 2 ≤ R} is a class of bounded norm representation functions mapping states to vectors and F ⊆ {f : R d → ∆(A)} is a class of functions mapping state representations to distribution over actions. We will be using linear functions, i.e. F = {x → softmax(W x) | W ∈ R K×d , W F ≤ 1}. We denote a policy parametrized by φ ∈ Φ and f ∈ F by π φ,f , where π φ,f (a|s) = f (φ(s)) a . In some cases, we may also use the policy π φ,f (a|s) = I{a = arg max a ∈A f (φ(s)) a } 1 . Denote Π φ = {π φ,f : f ∈ F } to be the class of policies that use φ as the representation function. Given demonstrations from expert policies for T tasks sampled independently from η, we wish to first learn representation functions (φ 1 , . . . ,φ H ) so that we can use a few demonstrations from an expert policy π * for new task µ ∼ η and learn a policy π = (π 1 , . . . , π H ) that uses the learned representations, i.e. π h ∈ Πφ h , such that has average cost of π is not too far away from π * . In the case of stationary policies, we need to learn a single φ by using tasks and learn π ∈ Π φ for a new task. The hope is that data from multiple tasks can be used to learn a complicated function φ ∈ Φ first, thus requiring only a few samples for a new task to learn a linear policy from the class Π φ .

Section Title: Gaussian complexity
  Gaussian complexity As in  Maurer et al. (2016) , we measure the complexity of a function class H ⊆ {h : X → R d } on a set X = (X 1 , . . . , X n ) ∈ X n by using the following Gaussian average G(H(X)) = E   sup h∈H d,n i=1 j=1 γ ij h i (X j ) | X j    (2) where γ ij are independent standard normal variables.  Bartlett & Mendelson (2003)  also used Gaus- sian averages to show some generalization bounds.

Section Title: BI-LEVEL OPTIMIZATION FRAMEWORK
  BI-LEVEL OPTIMIZATION FRAMEWORK In this section we introduce our framework and give a high-level description of the conditions under which this framework gives us statistical guarantees. Our main idea is to phrase learning represen- tations for imitation learning as the following bi-level optimization Here µ is the inner loss function that penalizes π being different from π * µ for the task µ. In general, one can use any loss µ that is used for single task imitation learning, e.g. for the behavioral cloning setting (cf. Section 5), µ is a classification like loss that penalizes the mismatch between predictions by π * and π, while for the observation-alone setting (cf. Section 6) it is some measure of distance between the state visitation distributions induced by π and π * . The outer loss function is over the representation φ. The use of bi-level optimization framework naturally enforces policies in the inner optimization to share the same representation. While Equation 3 is formulated in terms of the distribution η, in practice we only have access to few samples for T tasks; let x (1) , . . . , x (T ) denote samples from tasks µ (1) , . . . , µ (T ) sampled i.i.d. from η. We thus learn the representationφ by minimizing empirical versionL of Equation 3. L(φ) = 1 T T i=1 min π∈Π φ x (i) (π) = 1 T T i=1 x (i) (π φ,x (i) ) where x is the empirical loss on samples x and π φ,x = arg min π∈Π φ x (π) corresponds to a task specific policy that uses a fixed representation φ. Our goal then is to show that for a new task µ ∼ η, the policy πφ ,x learned by using samples x from the task µ has low expected cost J µ , i.e., Informal Theorem 4.1. With high probability over the sampling of train task data and with suffi- cient number of tasks and samples per task, At a high level, in order to prove such a theorem for a particular choice of µ , we would need to prove the following three properties about µ and x : 1. x (π) concentrates to µ (π) simultaneously for all π ∈ Π φ (for a fixed φ), with sample complex- ity depending on some complexity measure of Π φ rather than being polynomial in |S|; 2. a small value of µ (π) implies a small value for J µ (π) − J µ (π * µ ); 3. if φ and φ induce "similar" representations then min π∈Π φ µ (π) and min π∈Π φ µ (π) are close. The first property ensures that learning a policy for a single task by fixing the representation is sample efficient, thus making representation learning a useful problem to solve. The second property ensures that matching the behavior of the expert as measured by the loss µ ensures low average cost i.e., µ is meaningful for the average cost; any standard imitation learning loss will satisfy this. The third property is specific to representation learning and requires µ to use representations in a smooth way. This ensures that the empirical loss for T tasks is a good estimate for the average loss on tasks sampled from η. We prove these three properties for the cases where µ is the either behavioral cloning loss or observation-alone loss, with natural choices for the empirical loss x . However the general proof recipe can be used for potentially many other settings and loss functions. In the next section, we will describe representation learning for behavioral cloning as an instantiation of the above framework and describe the various components of the framework. Furthermore we will describe the results and give a proof sketch to show how the aforementioned properties help us show our final guarantees. The guarantees for this setting follow almost directly from results in  Maurer et al. (2016)  and  Ross et al. (2011) . Later in Section 6 we describe the same for the observations alone setting which is more non-trivial.

Section Title: REPRESENTATION LEARNING FOR BEHAVIORAL CLONING
  REPRESENTATION LEARNING FOR BEHAVIORAL CLONING Choice of µ : We first specify the inner loss function in the bi-level optimization framework. In the single task setting, the goal of behavioral cloning ( Syed & Schapire, 2010 ;  Ross et al., 2011 ) Under review as a conference paper at ICLR 2020 is to use expert trajectories of the form τ = (s 1 , a 1 , . . . , s H , a H ) to learn a stationary policy 2 that tries to mimic the decisions of the expert policy on the states visited by the expert. For a task µ, this reduces to a supervised classification problem that minimizes a surrogate to the following loss µ 0−1 (π) = E s∼ν * µ ,a∼π * µ (s) I{π(s) = a}. We abuse notation and denote this distribution over (s, a) for task µ as µ; so (s, a) ∼ µ is the same as s ∼ ν * µ , a ∼ π * µ (s). Prior work (Syed  & Schapire, 2010 ;  Ross et al., 2011 ) have shown that a small value of µ 0−1 (π) implies a small difference J(π)−J(π * ). Thus for our setting, we choose µ to be of the following form µ (π) = E s∼ν * µ ,a∼π * µ (s) (π(s), a) = E (s,a)∼µ (π(s), a) (4) where is any surrogate to 0-1 loss I{a = arg max a ∈A π(s) a } that is Lipschitz in φ(s). In this work we consider the logistic loss (π(s), a) = − log(π(s) a ). Learning φ from samples: Given expert trajectories for T tasks µ (1) , . . . , µ (T ) we construct a dataset X = {x (1) , . . . , x (T ) }, where x (t) = {(s t j , a t j )} n j=1 ∼ (µ (t) ) n is the dataset for task t. Details of the dataset construction are provided in Section C.1. Let S denote the set of states {s t j }. Instantiating our framework, we learn a good representation by solvingφ = arg min φ∈ΦL (φ), wherê Evaluating representationφ: A learned representationφ is tested on a new task µ ∼ η as follows: draw samples x ∼ µ n using trajectories from π * µ and solve πφ ,x = arg min π∈Πφˆ x (π). Does πφ ,x have expected cost J µ (πφ ,x ) not much larger than J µ (π * µ )? The following theorem answers this question. We make the following two assumptions to prove the theorem. The first assumption holds if π * µ is aiming to maximize some cost function. The second assumption is for representation learning to make sense: we need to assume the existence of a common repre- sentation φ * that can approximate all expert policies and γ measures this expressiveness of Φ. Now we present our first main result. Theorem 5.1. Letφ ∈ arg min φ∈ΦL (φ). Under Assumptions 5.1,5.2, with probability 1 − δ over the sampling of dataset X, we have To gain intuition for what the above bound means, we give a PAC-style guarantee for the special case where the class of representation functions Φ is finite. This follows directly from the above theorem and the use of Massart's lemma. Corollary 5.1. In the same setting as Theorem 5.1, suppose Φ is finite. If number of tasks satis- fies T ≥ c 1 max H 4 R 2 log(|Φ|) 2 , H 4 ln(4/δ) 2 , and number of samples (expert trajectories) per task satisfies n ≥ c 2 H 4 R 2 K 2 for small constants c 1 , c 2 , then with probability 1 − δ,

Section Title: Discussion
  Discussion The above bound says that as long as we have enough tasks to learn a representation from Φ and sufficient samples per task to learn a linear policy, the learned policy will have small average cost on a new task from η. The first term H 2 γ is small if the representation class Φ is expressive enough to approximate the expert policies (see Assumption 5.2). The results says that if we have access to data from T = O H 4 R 2 log(|Φ|) 2 tasks sampled from η, we can use them to learn a representation such that for a new task we only need n = O H 4 R 2 K 2 samples (expert demonstrations) to learn a linear policy with good performance. In contrast, without access to tasks, we would need n = O max H 4 R 2 log(|Φ|) 2 , H 4 R 2 K 2 samples from the task to learn a good policy π ∈ Π from scratch. Thus if the complexity of the representation function class Φ is much more than number of actions (log(|Φ|) K in this case), then multi-task representation learning might be much more sample efficient 4 . Note that the dependence of sample complexity on H comes from the error propagation when going from µ to J µ ; this is also observed in single task imitation learning ( Ross et al., 2011 ;  Sun et al., 2019 ). We give a proof sketch for Theorem 5.1 below, while the full proof is deferred to Appendix A.

Section Title: PROOF SKETCH
  PROOF SKETCH The proof has two main steps. In the first step we bound the error due to use of samples. The policy π φ,x that is learned on samples x ∼ µ n is evaluated on the distribution µ and the average loss incurred by representation φ across tasks isL(φ) = E µ∼η E x∼µ n µ (π φ,x ). On the other hand, if the learner had complete access to the distribution η and distributions µ for ev- ery task, then the loss minimizer would be φ * = arg min φ∈Φ L(φ), where L(φ) := E µ∼η min π∈Π φ µ (π). Using results from  Maurer et al. (2016) , we can prove the following aboutφ Lemma 5.2. With probability 1 − δ over the choice of X,φ ∈ arg min The proof of this lemma is provided in the appendix for completeness. The second step of the proof is connecting the lossL(φ) and the average cost J µ of the policies induced by φ for tasks µ ∼ η. This can obtained by using the connection between the surrogate 0-1 loss µ and the cost J µ that has been established in prior work ( Ross et al., 2011 ;  Syed & Schapire, 2010 ). The following lemma uses the result for deterministic expert policies from  Ross et al. (2011) . Lemma 5.3. Given a representation φ withL(φ) ≤ . Let x ∼ µ n be samples for a new task µ ∼ η. Let π φ,x be the policy learned by behavioral cloning on the samples, then under Assumption 5.1 This suggests that makingL small is good enough. A simple implication of Assumption 5.2 that min φ∈Φ L(φ) ≤ L(φ * ) ≤ γ, along with the above two lemmas completes the proof.

Section Title: REPRESENTATION LEARNING FOR OBSERVATION-ALONE SETTING
  REPRESENTATION LEARNING FOR OBSERVATION-ALONE SETTING Now we consider the setting where we cannot observe experts' actions but only their states. As in  Sun et al. (2019) , we also solve a problem at each level; consider a level h ∈ [H]. Choice of µ h : Let π * µ = {π * 1,µ , . . . , π * H,µ } be the sequence of expert policies (possibly stochastic) at different levels for the task µ. Let ν * h,µ be the distribution induced on the states at level h by the expert policy π * µ . The goal in imitation learning with observations alone ( Sun et al., 2019 ) is Under review as a conference paper at ICLR 2020 to learn a policy π = (π 1 , . . . , π H ) that matches the distributions ν π h with ν * h for every h, w.r.t. a discriminator class G 5 that contains the true value functions V * 1 , . . . , V * H and is approximately closed under the Bellman operator of π * . Instead, in this work we learn π that matches the distributions π h · ν * h and ν * h+1 for every h w.r.t. to a class G ⊆ {g : S → R, |g| ∞ ≤ 1} that contains the value functions and has a stronger Bellman operator closure property. For every task µ, µ h is defined as µ h (π) = max g∈G [ E s∼ν * h,µ E a∼π(s) s∼Ps,a g(s) − Ē s∼ν * h+1,µ g(s)] (6) = max g∈G [ E s∼ν * h,µ E a∼U (A) s∼Ps,a Kπ(a|s)g(s) − Ē s∼ν * h+1,µ g(s)] where we rewrite µ h by importance sampling in the second equation; this will be useful to get an empirical estimate. While our definition of µ h differs slightly from the one used in  Sun et al. (2019) , using similar techniques, we will show that small values for µ h (π h ) for every h ∈ [H] will ensure that the policy π = (π 1 , . . . , π H ) will have expected cost J µ (π) close to J µ (π * µ ). We abuse notation, and for a task µ we denote µ = (µ 1 , . . . , µ H ) where µ h is the distribution of (s, a,s,s) Learning φ h from samples: We assume, 1) access to 2n expert trajectories for T independent train tasks, 2) ability to reset the environment at any state s and sample from the transition P (·|s, a) for any a ∈ A. The second condition is satisfied in many problems equipped with simulators. Using the sampled trajectories for the T tasks {µ (1) , . . . , µ (T ) } and doing some interaction with environment, we get the following dataset X = {X 1 , . . . , X H } where X h is the dataset for level h. Specifically, X h = {x (1) h , . . . , x (T ) H } where x (i) h = {(s i j , a i j ,s i j ,s i j )} n j=1 ∼ (µ (i) ) n . Additionally we denote S h = {s i j } T,n i=1,j=1 to be all the s-states in X h ,S h andS h are similarly defined. Details about how this dataset is constructed from expert trajectories and interactions with environment is provided in Section C.2. We learn the representationφ h = arg min φ∈ΦL h (φ), wherê L h (φ) = 1 T T i=1 min π∈Π φ max g∈G 1 n n j=1 [Kπ(a i j |s i j )g(s i j ) − g(s i j )] = 1 T T i=1 min π∈Π φˆ x (i) h (π) where for dataset x = {(s j , a j ,s j ,s j )} n j=1 ,ˆ x h (π) := max g∈G 1 n n j=1 [Kπ(a j |s j )g(s j ) − g(s j )]. Note that because of the max g∈G ,ˆ x h is no longer an unbiased estimator of µ h when x ∼ µ n h . However we can still show generalization bounds. Evaluating representationsφ 1 , . . . ,φ H : Learned representations are tested on a new task µ ∼ η as follows: get samples x = (x 1 , . . . , x H ) 6 for all levels using trajectories from π * µ , where x h ∼ µ n h . For each level h, learn πφ h ,x h = arg min π∈Πφˆ x h h (π) and consider the policy πφ ,x = (πφ 1,x1 , . . . , πφ H ,x H ). Before presenting the guarantee for πφ ,x , we introduce a notion of Bellman error that will show up in our results. For a policy π = (π 1 , . . . , π H ) and an expert policy π * = (π * 1 , . . . , π * H ), we define the inherent Bellman error We make the following two assumptions for the subsequent theorem. These are standard assump- tions in theoretical reinforcement learning literature.

Section Title: Assumption 6.1 (Value function realizability
  Assumption 6.1 (Value function realizability Now we present our main theorem for the observation-alone setting. We again give a PAC-style guarantee for the special case where the class of representation functions Φ and value function class G are finite. It follows from the above theorem and Massart's lemma.

Section Title: Discussion
  Discussion As in the previous section, the number of samples required for a new task after learn- ing a representation is independent of the class Φ but depends only on the value function class G and number of actions. Thus representation learning is very useful when the class Φ is much more complicated than G, i.e. R 2 log(|Φ|) max{log(|G|), R 2 K}. In the above bounds, φ be is a Bell- man error term. This type of error terms occur commonly in the analysis of policy iteration type algorithms ( Munos, 2005 ;  Munos & Szepesvári, 2008 ). We remark that unlike in  Sun et al. (2019) , our Bellman error is based on the Bellman operator of the learned policy rather than the optimal policy.  Le et al. (2019)  used a similar notion that they call inherent Bellman evaluation error. The proof of Theorem 6.1 follows a similar outline to that of behavioral cloning. However we cannot use the results from  Maurer et al. (2016)  directly since we are solving a min-max game for each task. We provide the proof in Appendix B.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section we present experimental results on the DirectedSwimmer environment (modified from the Swimmer environment from OpenAI gym ( Brockman et al., 2016 )) with  Todorov et al. (2012)  simulator and a NoisyCombinationLock environment designed by ourself. These experiments have two aims: 1) verify the benefit of representation learning predicted by our theory, 2) test the power of representations learned via our framework in a broader context: we learn a policy for a new task by using the representation and doing policy optimization instead of imitation learning. In our ex- periments we learn representations using Equation 5. Experiment details are deferred to Section D.

Section Title: Our method
  Our method Given access to a dataset X = {(s t j , a t j )} n j=1 of n state-action pairs each for T tasks, we learn aφ according to Equation 8. For any new task we learn a linear policy π from the class Πφ. Baseline: For a task we learn a policy π from the class Π without learning a representation first.

Section Title: Verification of theory
  Verification of theory In  Figure 1  we verify our theoretical findings. On the left, we test on the DirectedSwimmer environment and report the logistic loss on the validation, which measures how close the trained policy is to the target expert policy. We find that learning representations, even with a few experts, can significantly reduce the sample complexity. On the right, we report the average Under review as a conference paper at ICLR 2020 reward of the trained policies on the environment. Here we see a different phenomenon: when the number of experts is small (4 or 16), the baseline method can beat policies trained using represen- tation learning, though the baseline method requires more samples to do so. When the number of experts is large (64), we see the policy trained using representation learning can significantly out- perform the baseline method. This behavior is expected as when the number of experts is small, we may learn a sub-optimal representation and because we fix this representation for training the policy, more samples for the test task cannot make this policy better, whereas more samples always make the baseline method better. Nevertheless, when the number of experts is large, we can significantly reduce the sample complexity. With 60 samples, the base line method is still far behind the policy trained using representation learning with 64 experts.

Section Title: Policy optimization with representations trained by imitation learning
  Policy optimization with representations trained by imitation learning We next test the utility of representations learned via our framework for RL. After training a representation, we use a simpli- fied proximal policy optimization method that learns a linear policy over the learned representation. Results are reported in  Figure 2 . For DirectedSwimmer and NoisyCombinationLock, we observe a common pattern. When the number of experts to learn the representation is small, the baseline method enjoys better performance than the policies trained using representation learning. As the number of experts to learn the representation increases, we see the policy trained using represen- tation learning can initially outperform baseline, sometime significantly. However, unsurprisingly, the baseline method performs very well with a large number of samples, since it is allowed to learn a representation from scratch. This experiment suggests that representations trained via imitation learning can be useful beyond imitation learning, especially when the target task has few samples.

Section Title: CONCLUSION
  CONCLUSION The current paper proposes a bi-level optimization framework to formulate and analyze representa- tion learning for imitation learning using multiple demonstrators. Theoretical guarantees are pro- vided to justify the statistical benefit of representation learning. Some preliminary experiments verify the effectiveness of the proposed framework. In particular, in experiments, we find the rep- resentation learned via imitation learning is also useful for policy optimization in the reinforcement learning setting. We believe it is an interesting theoretical question to explain this phenomenon. Ad- ditionally, extending this bi-level optimization framework to incorporate methods beyond imitation learning is an interesting future direction. Under review as a conference paper at ICLR 2020
  These statements are qualitative since we are comparing upper bounds.

```
