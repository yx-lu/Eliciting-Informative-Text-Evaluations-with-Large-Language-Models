Title:
```
Under review as a conference paper at ICLR 2020 DOUBLEMARGIN: EFFICIENT TRAINING OF ROBUST AND VERIFIABLE NEURAL NETWORKS
```
Abstract:
```
Recent works have developed several methods of defending neural networks against adversarial attacks with certified guarantees. We propose that many common certified defenses can be viewed under a unified framework of regularization. This unified framework provides a technique for comparing different certified defenses with respect to robust generalization. In addition, we develop a new regularizer that is both more efficient than existing certified defenses and can be used to train networks with higher certified accuracy. Our regularizer also extends to an 0 threat model and ensemble models. Through experiments on MNIST, CIFAR-10 and GTSRB, we demonstrate improvements in training speed and certified accuracy compared to state-of-the-art certified defenses.
```

Figures/Tables Captions:
```
Figure 1: CNN-Cert certified accuracy against perturbation size for models trained normally and with several robust training methods including Bounded, IBP and our method, Double Margin. Results are shown for different values of train . (a) shows results on MNIST and (b) on CIFAR. Refer to Table 2 for additional baselines including MMR and TRADES.
Table 1: u i and l i are layer-wise bounds found using interval bounds propagation, and u i c and l i c are layer-wise bounds using convex outer bounds. denotes element-wise multiplication. || · || op,∞ denote the ∞ induced operator norm. g is a classification-calibrated loss function. d B and d D are quantities relating to lower bounds on the minimum adversarial distortion. Expectations are taken over training data. The training cost is compared to the standard training.
Table 2: CNN-Cert certified accuracies and training times. std in (·). * denotes a different setting of regularization parameter λ, decreased from 1 2 to 1 4 . MMR is trained for 4 epochs due to its long training time. Our methods or variants are in blue.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Although deep neural networks (DNNs) have achieved tremendous success in various applications, it becomes widely-known that they are vulnerable to adversarial examples (also known as adversarial attacks), namely, crafted examples with human-imperceptible perturbations to cause misclassification ( Goodfellow et al., 2015 ;  Szegedy et al., 2013 ). Many attack generation methods have been proposed in order to find the possible minimum adversarial perturbation, commonly evaluated by its p norm for p ∈ {0, 1, 2, ∞} ( Papernot et al., 2016 ;  Carlini & Wagner, 2017 ; Athalye & Sutskever, 2017;  Su et al., 2019 ;  Xu et al., 2018 ;  Chen et al., 2018 ). Meanwhile, various defense methods were proposed to enhance the robustness of DNNs against adversarial attacks. However, many of them are built on heuristic strategies, which are thus easily bypassed by stronger adversaries ( Athalye et al., 2018 ). The work  Madry et al. (2018)  proposed a stronger defense method, adversarial training, which minimizes the worst-case training loss under adversarial perturbations. However, it is restricted to a specific type of adversarial perturbations and not generalized to other types of perturbations ( Tramèr & Boneh, 2019 ). Motivated by the limitation of heuristic defense, another line of research (known as verified/certified robustness) aims to provide provable robust guarantees of DNNs against an input with arbitrary perturbation within a certain p ball region ( Katz et al., 2017 ). Here the former uses expensive computation methods, e.g., mixed-integer programming (MIP), to find the exact robustness bound, and the latter considers a relaxed verification problem by convexifying the adversarial polytope but significantly improves the computation efficiency compared to the exact method. Besides resorting to approximate verification method, the recent work  Xiao et al. (2019)  proposed the principle of co-design between training and verification, and showed that the exact verification method can be accelerated by imposing weight sparsity and activation stability (so-called ReLU stability) on trainable network models. On the other hand, it was shown in  Kolter & Wong (2018) ;  Raghunathan et al. (2018) ;  Gowal et al. (2018) ;  Mirman et al. (2018) ;  Dvijotham et al. (2018a)  that by incorporating the relaxed but computationally efficient verification methods into the training process, the learnt model yields strengthened robustness with certificate. Following this line of research, in this work: Under review as a conference paper at ICLR 2020 • We develop a unified framework of certified defense and propose that many common certified defenses can be incorporated as different types of regularization. We theoretically analyze general regularizations and provide guidelines on how to select regularization. • We propose an efficient regularizer that better quantifies adversarial sensitivity during the training process to yield more certifiable models. Through experiments on six different architectures, we demonstrate our method's computational efficiency and higher certified accuracies on large perturbations compared to all current state-of-the-art certification based training methods. In particular, we demonstrate up to a 55% increase in certified accuracy at = 0.2 on MNIST, up to a 20.5% increase at = 2/255 on CIFAR, and up to a 24% increase at = 8/255 on GTSRB. In addition, on all architectures and datasets we achieve faster training than any other certification based training methods. • We extend our regularizer to an 0 threat model and model ensembles. We demonstrate empirically that our regularizer achieves high certified accuracy on this threat model. We also empirically show that model ensembling of models trained with our regularizer achieve higher certified accuracies.

Section Title: BACKGROUND AND RELATED WORK
  BACKGROUND AND RELATED WORK

Section Title: VERIFICATIONS
  VERIFICATIONS Assuming a norm-bounded threat model, finding the minimum adversarial distortion exactly is an NP- complete problem, making it computationally infeasible ( Katz et al., 2017 ). Fortunately, finding lower bounds on the minimum adversarial distortion is computationally tractable. Several techniques find these lower bounds only as a function of model weights ( Szegedy et al., 2013 ;  Peck et al., 2017 ;  Hein & Andriushchenko, 2017 ;  Raghunathan et al., 2018 ), but these methods typically provide very loose bounds for neural networks with more than 2 layers. Using an input-specific certification method, it is possible to find non-trivial bounds for fully connected ReLU networks ( Kolter & Wong, 2018 ;  Weng et al., 2018 ;  Wang et al., 2018 ), as well as networks with general activation functions ( Zhang et al., 2018 ) and architectures ( Singh et al., 2018 ). An "any-time" certifier has also been developed that allows for a trade-off between certification time and bound quality ( Dvijotham et al., 2018b ).

Section Title: CERTIFIED DEFENSES
  CERTIFIED DEFENSES Recent works have also developed methods of defending against adversarial attacks. One line of work uses adversarial training with adversarial attacks and empirically demonstrates high resistance to attacks ( Madry et al., 2018 ;  Sinha et al., 2018 ). However, adversarial training is not targeted towards verification or certification methods, and we therefore do not consider it a certified defense. One step towards certified defenses is natural regularization such as sparsity-inducing weight magnitude penalization. This method combined with adversarial training yields highly verifiable models ( Xiao et al., 2019 ). Using an additional ReLU stability regularizer to enhance ease-of-verification allows for even more verifiable models. Other defenses specifically target certifiers or use certification methods as part of the training procedure. We note that these "certified" defenses are not truly certified since they cannot ensure robustness to unseen points without using a certifier on these points. These defenses instead produce models that are empirically more certifiable on unseen test points. Using convex outer bounds to bound the adversarial loss function has been shown to be effective at producing certifiable models, although training is relatively slow ( Kolter & Wong, 2018 ;  Wong et al., 2018 ). Using interval bounds propagation (IBP) to bound the adversarial loss is much cheaper to train ( Gowal et al., 2018 ) and has surprisingly become the state-of-the-art certifiably robust training method ( Gowal et al., 2018 ;  Salman et al., 2019 ) despite IBP performing much more poorly than the convex outer bounds ( Kolter & Wong, 2018 ;  Wong et al., 2018 ) in certifications. In this paper, we unify certified defenses under a unified framework of regularization. We also develop an efficient certified defense that has similar computation overhead as IBP-based defense ( Gowal et al., 2018 ) while yielding better performance than existing methods.

Section Title: CERTIFICATION METHODS
  CERTIFICATION METHODS

Section Title: Threat model
  Threat model In this paper, we will use the notation of fully-connected neural networks for exposition, but our method works for general convolutional neural networks including residual networks. Appendix B Table 3 includes descriptions of the main notations used in this paper. Consider an n layer neural network f (x) with input x where the first layer of the network z 0 is set to x. Given weights W i , biases b i and an activation function σ, for i = 0, . . . , n − 1, subsequent layers are defined as: z i+1 = W i+1 σ(z i ) + b i+1 , (1) with f (x) = z n . With this expression, the first layer of the network is defined by using the identity activation at the first layer. We assume the following threat model: a nominal input x nom is perturbed by perturbation δ to produce a perturbed input x = x nom + δ, where ||δ|| p ≤ and || · || p represents an p norm. Suppose the correct classification is given by c. Then the minimum distortion * is the minimal ∈ R + satisfying: max j =c z n j − z n c > 0.

Section Title: Interval Bounds Propagation
  Interval Bounds Propagation There exist several methods to efficiently find certified lower bounds on the minimum distortion necessary for misclassification. One such method is interval bounds propagation (IBP) ( Gowal et al., 2018 ;  Gehr et al., 2018 ) which bounds each layer in a network with a fixed upper and lower bound. These bounds are then propagated at each layer of the network using the previous layer's bounds. Specifically, given layer-wise bounds where l i ≤ z i ≤ u i , the next layer's bounds are found as: u i+1 = W i+1 + σ(u i ) + W i+1 − σ(l i ) + b i+1 , (2) where W + and W − denote the positive and negative components of W respectively with other entries being zeros otherwise. Lower bounds are found similarly. Intuitively, IBP finds a box bounding each layer, which can result in very loose bounds for general network as demonstrated in ( Kolter & Wong, 2018 ;  Gehr et al., 2018 ).

Section Title: Linear Bounding Framework
  Linear Bounding Framework Certified bounds can also be found using a linear bounding frame- work as first proposed in Fast-Lin ( Weng et al., 2018 ) and later in the Neurify ( Wang et al., 2018 ) and DeepZ ( Singh et al., 2018 ) frameworks. This approach typically finds tighter bounds on minimum distortion than IBP. This framework bounds each activation layer σ(z i ) as follows: α i z i + β i L ≤ σ(z i ) ≤ α i z i + β i U , where α i represents the slopes of linear bounds on the activation and β i L ,β i U representing intercepts of linear bounds on the activation. When σ is ReLU activation, Fast-Lin sets the coefficients to be: When both bounds are positive or negative, the bound on the activation is exactly the linear component on the corresponding side (i.e. when z i l,j > 0 for example, α i j =1, β i L,j = β i U,j = 0). Using these layer-wise bounds, Fast-Lin finds a pair of linear bounds on the network: Ax + b L ≤ f (x) ≤ Ax + b U . Then Fast-Lin bounds the network output over all possible adversarial distortion measured by - p ball by: Ax nom + b L − ||A|| :,q ≤ f (x) ≤ Ax nom + b U + ||A|| :,q , where || · || :,q denotes a row-wise q norm, dual to the norm p of the assumed attack threat model. is the assumed attack norm size. Intuitively, Fast-Lin finds linear upper and lower bounds on the entire network to analyze the output layer. Because Fast-Lin finds linear bounds on the network as an intermediate step to finding output bounds z u , z l , the bounds are tighter than the corresponding IBP bounds u, l. Fast-Lin is equivalent to using convex outer bounds to bound the set of possible values at each layer of the network. Fast-Lin has been extended to general activation functions and asymmetric upper and lower bounds with different values of α i in CROWN ( Zhang et al., 2018 ), and has been extended to general network architectures in CNN-Cert ( Boopathy et al., 2019 ).

Section Title: UNIFIED FRAMEWORK OF CERTIFIED DEFENSE
  UNIFIED FRAMEWORK OF CERTIFIED DEFENSE We propose a unified framework of certified defense under which many common certified defenses can be viewed as regularizations of a particular form. We first use our framework to provide analysis of Under review as a conference paper at ICLR 2020 general certified defenses. Under this framework, we then propose our regularizer which outperforms other current certified defense methods as suggested by the experiments. Because IBP has been shown to be the current state-of-the-art in efficiently training certifiable models, we consider it as our main baseline and highlight our method's advantages. Finally, we discuss extensions of our regularization.

Section Title: ANALYSIS OF REGULARIZERS
  ANALYSIS OF REGULARIZERS A Unified Regularizer Framework for Certified Defense Many different training methods can often be formulated as different regularizations added to a standard loss function. Given model param- eters θ, a loss function L(θ) and a regularizer R(θ), the total regularized loss with a regularization parameter λ is given by: Note that we denote a model's parameters in general as θ, and we use calligraphic symbols to denote functions of general parameters θ and standard symbols to denote functions varying at different data points. Standard regularizations chosen to enhance generalization include weight-based regularization where the regularizer is chosen to be i ||W i || l with l being some matrix norm. In fact, many certification-based robust training methods can also be viewed as different regularizations R(θ) as seen in  Table 1 . Therefore, both standard regularizations and many robust training methods can be viewed under a unified framework. Training methods can be designed by selecting a particular regularizer according to the designer's objective.

Section Title: Regularizer Construction
  Regularizer Construction Certified defenses such as IBP construct a regularizer by using bounds on the last layer of the network u n and l n to find an upper bound on the adversarial loss. Suppose a standard loss function L(θ) of model parameters θ is defined as a function of the last layer of the network z n : L(θ) = E[L(z n )], where the expectation is taken over the training set. For many loss functions L, including softmax cross-entropy loss and squared loss, it is possible to decompose the loss into an increasing and decreasing component: L(z n ) = L + (z n ) + L − (z n ), (3) where L + (z n ) increases element-wise with z n and L − (z n ) decreases element-wise with z n . For example in the case of softmax cross-entropy loss: where e x is element-wise exponential function and y are one-hot encoded labels, the first term can be mapped to L + (z n ) while the second corresponds to L − (z n ) in (3). IBP then constructs the following regularizer: R(θ) = E[L + (u n ) + L − (l n ) − L(z n nom )], where u n and l n are defined by (2) and z n nom is the value of layer z n with unperturbed input x nom . When this regularizer is added to the standard loss function L(θ) computed with unperturbed input x nom , the resulting quantity L(θ) + R(θ) is an upper bound on the adversarial loss since: L(z n nom )+(L + (u n )+L − (l n )−L(z n nom )) ≥ L(z n ), where z n is a function of perturbed input x, obtained by monotonicity of L + and L − and l n ≤ z n ≤ u n .

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Note that the adversarial loss L(z n ) is unknown, but the upper bound constructed here depends only on the IBP bounds u n and l n which are known. Typically, the model is trained with objective L(θ) + λR(θ) with λ < 1 instead of λ = 1 in order to ensure stable training.

Section Title: Connecting regularization to robustness and generalization
  Connecting regularization to robustness and generalization A regularized loss function often can be seen as an upper bound on a particular robustness or generalization objective. As shown previously, adding a robust training regularizer such as IBP is equivalent to minimizing an upper bound on the adversarial loss: 1 m i max x:||x−xi||p≤ L(f (x)) ≤ L(θ) + λR(θ), for sufficiently large λ where there are m points x i in the training set. Similarly 1 and 2 regularization can be seen as minimizing a probabilistic upper bound on the test set loss: L test (θ) ≤ L(θ) + λR(θ). This is because many generalization error bounds ( Bartlett & Mendelson, 2002 ;  Neyshabur et al., 2015 ) are statements of the following form: with high probability, L test (θ) ≤ L(θ) + K j ||W j || + C √ m , (4) where L test (θ) is the test set error, || · || is either a Frobenius or ∞ operator norm, m represents the number of training set points, and K and C are constants depending on the network architecture. It is possible to analyze the effect of general regularizations under the assumption that λ is close to zero. This corresponds to close to zero for robustness-based regularizers or m very large for generalization-based regularizers such as 1 or 2 weight regularization that can be interpreted as minimizing a generalization bound of the form in Equation 4. Proposition 1. Given a classifier with parameters θ and loss L(θ) with a local minimumθ, then for λ near 0, the local minimum of L(θ) + λR(θ) is approximately: θ − λH(L(θ)) −1 ∇ θ R(θ), (5) where H(L(θ)) is the Hessian of the loss at the original local minimumθ. This statement provides a method of relating a regularizer to its effect on the classifier. Please refer to Appendix E for the proof and additional intuition. Proposition 2. Given a classifier with parameters θ and loss L(θ) with a local minimumθ. For λ near 0, suppose θ * is the corresponding local minimum of a regularized loss L(θ) + λR 1 (θ). Then, for λ near 0, L(θ * ) + λR 2 (θ * ) < L(θ) + λR 2 (θ) if and only if: This statement can be used to provide guidelines on regularization selection. Intuitively, the condition implies that selecting regularizers with close gradient to a particular "optimal" regularizer R 2 (θ * ) is advantageous where distance is defined using the left hand side of the Equation 6. Please refer to Appendix F for the proof and additional intuition.

Section Title: OUR REGULARIZER
  OUR REGULARIZER Double Margin: Motivation and Rationale IBP is one way to propagate the adversarial sensitivity at different layers of the neural network and construct a robust loss function, but there exist other methods of quantifying sensitivity in the network. Different from IBP that bounds the input of each layer z through u and l in (2), we introduce double margins s and v such that [z nom − s − v, z nom + s + v] represents the approximate range of values each layer z can take, where z nom represents the value of the layer for unperturbed input x nom . In the similar spirit of IBP, we define the s term to approximate error margins at each layer of the network, but this margin s only requires propagation of one quantity while IBP requires two quantities u and l. Existing certifiers such as Fast-Lin ( Weng et al., 2018 ) and Reluplex ( Katz et al., 2017 ) rely on activations to be locally linear in order to achieve tight bounds. However, using the s term by itself does not necessarily result in local linearity at activations. Thus, we also include the v term which penalizes a finite difference approximation of the second derivative of the activation function. This Under review as a conference paper at ICLR 2020 margin has the property that when the network is locally linear in the sense that σ(z i j ) ≈ az i j + b for some a, b at all layers i, all margins v i ≈ 0. Intuitively, the s term can be seen as measuring a 1st order, linear level of adversarial sensitivity, while the v term measures 2nd order, nonlinear sensitivity (see Appendix C Figure 2 for a visual illustration). The inclusion of the v margin is the key qualitative difference between our approach and IBP. Because our approach explicitly penalizes non-linearity, it results in tighter certification and therefore more certifiable networks compared to IBP. Double Margin: Formal Definition Assuming a known p perturbation size , our margins s i and v i at layer i have the same dimensionality as z i , whether they are vectors in fully connected layers or tensors in convolutional layers. Only the margins at the second layer depend on p. These margins are initialized as: s 0 = 1, v 0 = 0. For i = 0, . . . , n − 1, subsequent layer margins are defined as: For general p norms p = ∞, the second layer margins are modified so that s 1 is exactly half the range of values z 1 can take. To motivate the form of Equation (7), note that IBP can be reparameterized in terms of the half bound gap h = (u − l)/2 and bound average m = (u + l)/2. Then the half bound gap h is propagated as: The s margin is propagated similarly, with the difference being that IBP uses (u i + l i )/2 instead of z i as the centering point. This supports the intuition that the s margin by itself approximates the IBP bounds. The v margin is propagated the same way, with an additional term that uses a finite difference approximation of the second derivative of σ(·) at z i . Specifically, note that given a 2nd order Taylor expansion of σ(·) at a specific scalar point where γ > 0 is small. Applying this approximation to the second term in Equation (8): where all operators are applied element-wise. By approximating the second derivative of the activation, this term penalizes a level of non-linearity. To motivate our use of a finite difference instead of a second derivative directly, note that in the case of ReLU, using a second derivative would not work since ReLU has second derivative zero almost everywhere. A finite difference approximation also quantifies nonlinearity over a larger neighborhood; see Eq. 1 in  Moosavi-Dezfooli et al. (2018)  for a similar justification. Because we add two additional margins s and v in addition to the original network, we call our method double margin. Although the margins are defined for a fully connected network, the margins can be computed analogously for special cases or extensions such as convolutional neural networks or residual networks. Our regularizer is defined implicitly as a function of the model parameters θ: We note that our regularizer when added to the standard loss function is not a true upper bound on the adversarial loss since the margins do not provide exact bounds on each layer. However, as shown by Proposition 2, the gradient of a regularizer rather than its bound validity determines its certified test loss. Therefore, and as we also show experimentally, using an upper bound on the adversarial loss is not necessary to train certifiable models.

Section Title: Training Procedure
  Training Procedure Training proceeds by using standard optimizers on the regularized loss. See Appendix D Algorithm 1 for the full procedure. Note that using a value of λ = 0 corresponds to Under review as a conference paper at ICLR 2020 standard training while using λ = 1 corresponds to using only the regularizer (i.e. robust loss). In practice, robust training methods such as IBP typically increase the value of λ during training process. IBP specifically uses a warm-up period where λ = 0 first, then linearly increases λ until it reaches λ = 0.5. Note that the value of λ is separate from the value of used during training which parameterizes the regularizer R(θ). A value of = 0 also corresponds to regular training, and higher values of correspond to defending against larger perturbation attacks. In addition to increasing λ during training, robust training methods also typically increase during training. IBP uses a piece-wise linear schedule for with warm-up and ramp-up periods coinciding with the schedule for λ. We use a similar piece-wise linear schedule as IBP.

Section Title: EXTENSIONS
  EXTENSIONS We extend our method to an 0 threat model by modifying the double margin propagation at the second layer as done for general p perturbations. We also extend our method to ensembles of models trained with our regularizer. Additional details are provided in Appendix H.

Section Title: EXPERIMENTS
  EXPERIMENTS We conduct experiments comparing our regularizer to other robust training methods including 1 weight regularization, adversarial training, IBP, ReLU stability and convex outer bounds (Bounded).

Section Title: SETUP, MODELS, DATASET
  SETUP, MODELS, DATASET Implementation, Architectures, Training Parameters Training methods are implemented in Python with Tensorflow ( Abadi et al., 2016 ) and training is conducted on a NVIDIA Tesla P100 GPU. We evaluate the training methods on networks trained on the MNIST, CIFAR-10 and GTSRB ( Stal- lkamp et al., 2012 ) datasets. We consider 6 model architectures listed roughly in increasing model size: (i) Small CNN, (ii) Pureconv CNN, (iii) Resnet CNN, (iv) Pooling CNN, (v) Medium CNN, (vi) Large CNN. For a fair comparison among training methods, we used fixed training parameters. Architecture and hyperparameter details are deferred to Appendix G.

Section Title: Comparative Methods
  Comparative Methods We train networks with (i) normal training, (ii) 1 weight regularization, (iii) adversarial training, (iv) IBP, (v) ReLU stability, (vi) Bounded, (vii) MMR ( Croce et al., 2019 ), (viii) TRADES ( Zhang et al., 2019 ) and (ix) our method (Double Margin). Due to the computational cost of running Bounded training, we train this method only on the small CNN architecture. For training Bounded models, we use the technique of random projections as proposed by ( Wong et al., 2018 ), using 50 random projections. For the small CNN architecture, we try additional robust training methods including (x) combining adversarial training with the ReLU stability loss and weight pruning (Adv+RS+Pruning), and (xi) combining 1 weight regularization with ReLU stability and weight pruning (L1+RS+Pruning). We also (xii) add 1 weight regularization to our regularizer (Double Margin + L1).

Section Title: Evaluation
  Evaluation Although our regularizer applies to a general p threat model, we primarily conduct experiments assuming a ∞ threat model. The networks are evaluated on certified accuracy computed using CNN-Cert-ReLU, which efficiently finds a lower bound on the worst-case adversarial accuracy under norm bounded perturbations. We use CNN-Cert due to the high computational cost of using other methods such as Mixed Integer Optimization (MIO) ( Tjeng & Tedrake, 2019 ), even with using a optimization timeout as done by  Gowal et al. (2018) . In particular, we found that the initialization stage of the MIO verifier itself typically took longer than CNN-Cert. Baseline certified accuracies are generally higher in prior works because they are computed using slower certifiers. Certified accuracies are computed for 200 test set points (unless otherwise noted) over a range of perturbation sizes in [0, 0.4] for MNIST and [0, 9/255] for CIFAR and GTSRB. In addition, for selected networks we compute certified accuracies using IBP. For all certified accuracy comparisons, we use the same randomly selected set of test points. For two selected networks, we also compute certified accuracies on the entire test set and verify that the certified accuracies are similar to ensure that our randomly chosen test points are representative (see Appendix I).

Section Title: RESULTS
  RESULTS

Section Title: Baselines Comparison
  Baselines Comparison In all CNN-Cert comparison tables, the best methods in each column are highlighted, excluding normal training and standard weight regularization.  Table 2  provides a summary of the comparative experiments. We first highlight that over all tested datasets, our training methods train in less time than IBP, Bounded, TRADES and MMR. On MNIST, our methods, in bold, outperform ReLU stability based methods and adversarial training for = 0.01 and larger, and outperform IBP and Bounded for large perturbation sizes beyond = 0.07. We note that adding 1 weight regularization to our regularizer further enhances our certified accuracies. On CIFAR, our method outperforms ReLU stability based methods and adversarial training for = 1/255 and larger, and outperforms IBP and Bounded for perturbation sizes beyond = 0.5/255. On MNIST and CIFAR, we observe that due to high hyperparameter sensitivity, Bounded and ReLU stability sometimes have low clean accuracy and TRADES achieves low certified accuracy. Certified accuracy results for selected small MNIST and CIFAR networks are also presented visually in  Figure 1 . On GTSRB, our method outperforms IBP and MMR for = 3/255 and larger. Additional results on MNIST, CIFAR and GTSRB are shown in Appendix J, Tables 8, 9 and 6 respectively. In brief, all the experiments show that our method achieves higher CNN-Cert accuracy for large than any prior work.

Section Title: Ablation Experiments and Hyperparameter Analysis
  Ablation Experiments and Hyperparameter Analysis In addition, we separately analyze the effect of s and v margins in Double Margin by running the following comparisons: training with only the s margin (Variation 1: 's only'), and training Double Margin with IBP bounds u, l replacing s (Variation 2: 'IBP+v').  Table 2  shows that Variation 1 performs comparably to or outperforms IBP while requiring only fewer propagation during training, supporting the intuition that the s margin by itself approximates IBP. Variation 2 performs comparably to Double Margin but it requires one additional propagation during training (10-40% increased training time). These observations suggest that the 'v' margin is primarily responsible for Double Margin's improved certifiable robustness for large perturbations, but at the expense of lower clean accuracy compared to IBP. In order to improve the clean accuracy of networks trained with our method, we also train under a different hyperparameter setting where λ is halved during training, reducing its final value from 1 2 to 1 4 . As seen in  Table 2  Double Margin still outperforms IBP on the largest (60.4 vs 0.0% on MNIST and 46.4 vs 26.8% on GTSRB) while achieving a higher clean accuracy (91.1% on MNIST and 75.6% on GTSRB). These results suggest that fine-tuning hyperparameters can improve clean accuracy to be more comparable to prior work while maintaining our better certified accuracy on large .

Section Title: Extensions
  Extensions We find that our method's high certified accuracy extends to an 0 norm threat model. In addition, we find that using model ensembles with our method can be used to enhance certified accuracy. Results are shown in Appendix I.

Section Title: CONCLUSION
  CONCLUSION In this paper, we propose a unified framework of certified defense as regularization. We introduce a new efficient regularizer under this framework, and extend it to a 0 threat model and model ensembles. Through comparative experiments on several model architectures, we demonstrate that our method outperforms state-of-the-art certification based training methods in terms of training time and certified accuracy.

```
