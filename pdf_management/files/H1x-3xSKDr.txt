Title:
```
Under review as a conference paper at ICLR 2020 BATCH NORMALIZATION INCREASES ADVERSARIAL VULNERABILITY
```
Abstract:
```
Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this does not fully explain the vulnerability we observe, given that it occurs for a single BN layer. We argue that the main cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the numerical stability constant of BN acts as an important meta-parameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly for linear models but find that it still holds for nonlinear models.
```

Figures/Tables Captions:
```
Figure 1: A dataset with one task-relevant (α = 1) and one task-irrelevant dimension (α = 2). Nor- malization aligns the decision boundary with the Bayes solution (indicated by arrows in "BNGD"), but this minimizes the averaged distance between the points and the boundary, maximizing adver- sarial vulnerability. Compared with the decision boundary of a linear model (θ ≈ 0 • ), the batch- normalized model has θ = 66.7 • . On the right is the dataset seen by the BNGD classifier. We use Σ 11 = 1, Σ 22 = 0.01, Σ 12 = Σ 21 = 0.05, ν 0 = [−5, 0], and ν 1 = [5, 0].
Figure 2: We extend the experiment of Yang et al. (2019) by training fully-connected nets of depth L and constant-width (N l = 384) ReLU layers by SGD, batch size B, and learning rate η = 10 −5 B on MNIST. The BN parameters γ and β were left as default, momentum disabled, and c = 10 −3 . The dashed line is the theoretical maximum trainable depth of batch-normalized networks as a function of the batch size. We report the clean test accuracy, and that for additive Gaussian noise and BIM perturbations. The batch-normalized models were trained for 10 epochs, while the unnormalized ones were trained for 40 epochs as they took longer to converge. The 40 epoch batch-normalized plot was qualitatively similar with dark blue bands for BIM for shallow and deep variants. The dark blue patch for 55 and 60 layer unnormalized models at large batch sizes depicts a total failure to train. These networks were trainable by reducing η, but for consistency we keep η the same in both cases.
Table 1: As predicted by the theory, batch-normalized gradient descent (BNGD) yields a tilted decision boundary w.r.t. the nearest-centroid classifier, regardless of the affine parameters being learned or fixed. We report the tilting angle (θ) and accuracies of linear models trained on MNIST 3 vs. 7 for vanilla GD, GD with L2 weight decay "WD"(λ = 0.1), and BNGD. Affine = "F" indicates γ = 1 and β = 0, whereas "T" means they are randomly initialized and learnable. AWGN = N (0, 1), FGSM used with = 1 /10. Entries are the mean and its standard error over five random seeds.
Table 2: (Small learning rate) Test accuracies of VGG8 and WideResNet-28-10 on CIFAR-10 and CIFAR-10.1 (v6) in several variants: clean, noisy, and PGD perturbed. We evaluate models achieving the highest validation accuracy after training for 150 epochs.
Table 3: (Large initial learning rate) Robustness of VGG models of increasing depth on CIFAR-10, with and without BN. See text for meta-parameters.
Table 4: Robustness of three modern convolutional neural network architectures with and without BN on the CIFAR-10-C common "noise" corruptions (Hendrycks & Dietterich, 2019). We use "F" to denote the Fixup variant of WRN. Values were averaged over five intensity levels for each corruption.
Table 5: Robustness of pre-trained ImageNet mod- els with and without BN. Note: The numeric suffix indicates number of layers, or the spatial patch width in pixels (of 224) for BagNet.
Table 6: Evaluating the robustness of a MLP with and without batch norm. See text for architecture. We observe a 61 ± 1% reduction in test accuracy due to batch norm for √ d = 84 compared to √ d = 28.
Table 7: Evaluating the robustness of a MLP with 2 weight decay (same λ as for linear model, see Table 13 of Appendix F). See text for architecture. Adding batch norm degrades all accuracies.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION BN is a standard component of modern deep neural networks, and tends to make the training process less sensitive to the choice of hyperparameters in many cases ( Ioffe & Szegedy, 2015 ). While ease of training is desirable for model developers, an important concern among stakeholders is that of model robustness during deployment to plausible, previously unseen inputs. The adversarial examples phenomenon has exposed unstable predictions across state-of-the-art models ( Szegedy et al., 2014 ). This has led to a variety of methods that aim to improve robustness, but doing so effectively remains a challenge ( Athalye et al., 2018 ;  Schott et al., 2019 ;  Hendrycks & Dietterich, 2019 ;  Jacobsen et al., 2019a ). We believe that a prerequisite to developing methods that increase robustness is an understanding of factors that reduce it. Approaches for improving robustness often begin with existing neural network architectures-that use BN-and patch them against specific attacks, e.g., through inclusion of adversarial examples during training ( Szegedy et al., 2014 ;  Goodfellow et al., 2015 ;  Kurakin et al., 2017 ;  Madry et al., 2018 ). An implicit assumption is that BN itself does not reduce robustness, however, recent initialization-time analyses have shown that it causes exploding gradients, and increased sensitivity to input perturbations as the network depth increases (Yang et al., 2019;  Labatie, 2019 ). In this work, we consider the impact of BN in practical scenarios in terms of robustness to common corruptions ( Hendrycks & Dietterich, 2019 ) and adversarial examples ( Szegedy et al., 2014 ), finding that BN induced sensitivity remains a concern even in cases where its use appears benign on the basis of clean test accuracy, and when only one BN layer is used. The frequently made observation that adversarial vulnerability can scale with the input dimen- sion ( Goodfellow et al., 2015 ;  Gilmer et al., 2018 ;  Simon-Gabriel et al., 2019 ) highlights the importance of identifying regularizers as more than merely a way to improve test accuracy. In particular, BN was a confounding factor in  Simon-Gabriel et al. (2019) , making the results of their initialization-time analysis hold after training. By adding 2 regularization and removing BN, we show that there is no inherent relationship between adversarial vulnerability and the input dimension.

Section Title: BATCH NORMALIZATION
  BATCH NORMALIZATION We briefly review how BN modifies the hidden layers' pre-activations h of a neural network. We use the notation of Yang et al. (2019), where α is an index for units in a layer l, and i for a mini-batch of B samples from the dataset; N l denotes the number of units in layer l, W l is the matrix of weights and b l is the vector of biases that parametrize layer l. The batch mean is defined as μ α = 1 B i h αi , and the variance is σ 2 α = 1 B i (h αi − μ α ) 2 . In the BN procedure, the mean μ α is subtracted from the pre-activation of each unit h l αi (consistent with  Ioffe & Szegedy (2015) ), the result is divided by the standard deviation σ α plus a small constant c to prevent division by zero, then scaled and shifted by the learned parameters γ α and β α , respectively. This is described in equation 1, where a per-unit nonlinearity φ, e.g., ReLU, is applied after the normalization. This procedure introduces complications, however. Consider two mini-batches that differ by only a single example: due to the induced batch-wise nonlinearity, they will have different representations of all examples. These differences are amplified by stacking BN layers, and were shown to cause exploding gradients at initialization (Yang et al., 2019). Conversely, normalization of intermediate representations for two different training inputs impairs the ability to distinguish definite examples that ought to be classified with a large prediction margin (as judged by an "oracle"), from more ambiguous instances. The last layer of a discriminative neural network, in particular, is typically a linear decoding of class label-homogeneous clusters, and thus makes use of information contained in the mean and variance at this stage for classification. In light of these observations, we begin in our analysis by adding a single BN layer to models trained by gradient descent (GD). This is the most favorable scenario according to the analysis of Yang et al. (2019), where more layers and a smaller mini-batch size exacerbate the exploding gradients.

Section Title: BOUNDARY TILTING
  BOUNDARY TILTING   Tanay & Griffin (2016)  relate the adversarial vulnerability of linear classifiers to the tilting angle θ of the decision boundary w.r.t. the nearest-centroid classifier. Following their setup, we examine how BN affects this angle in a simple linear model, and then show that increasing model complexity cannot "undo" this vulnerability. Consider the binary classification task of iden- tifying two different types of input x subject to Gaussian noise with a linear classifier w x + b. This can be modeled by the class-conditional distribution p(x|y = j) = N (ν j , Σ) with la- bel y ∼ Ber(0.5). The Bayes-optimal so- lution to this problem is given by the weight vector w = Σ −1 ν 0 − ν 1 , and b = 1 2 (ν 1 + ν 0 ) Σ −1 (ν 1 − ν 0 ) + log p(y=0) p(y=1) , where p(y) denotes the marginal probability for the label y (see e.g. ( Jordan, 1995 )), while the nearest- centroid classifier is defined by w * = ν 0 − ν 1 . We analyze the effect of batch-normalizing the input to the classifier for this problem (i.e., h αi = x αi ), first in the simplest setting where γ α = 1, β α = 0 ∀α. We select the class distribution means ν j to be symmetric around zero, so that the batch mean computed by BN is μ α = 0 ∀α. The batch-normalized linear clas- sifier is thus defined as: f (x) = w x+b √ σ 2 +c . By construction of our synthetic dataset, the vari- ance of the batch can be deduced from the data Under review as a conference paper at ICLR 2020 We depict simulations of the toy model in  Figure 1 . We use constant learning rate GD, which is known to converge to the max-margin solution-equivalent to the nearest centroid classifier in this case-for linear models on separable data ( Soudry et al., 2018 ). Batch-normalized GD (BNGD) converges for arbitrary learning rates for linear models ( Cai et al., 2019 ); we use a value of 0.1 for 1000 epochs. Next, we train linear models on the MNIST 3 vs. 7 dataset with 5000 training samples (drawn uniformly per class) using a learning rate of 0.1 for 50 epochs. We compute the angle θ w.r.t. the nearest-centroid classifier, which is obtained by subtracting the "average 3" from the "average 7" of the full training set. Although this may seem like a crude reference point, the nearest-centroid classifier is much more robust than the linear model of  Goodfellow et al. (2015) , achieving 40% accuracy for the fast gradient sign method (FGSM) at = 1 /4 vs. ≈ 0%. Results consistent with the boundary tilting theory are shown in  Table 1 , which not only shows that BN causes tilting, but that this is unaffected by the parameters γ and β. Post-normalization, there is no signal to γ and β about the variances of the original dataset. This is consistent with other works that observe γ and β do not influence the studied effect (van Laarhoven, 2017;  Zhang et al., 2019a ; Yang et al., 2019) Increasing the numerical stability constant c increases robustness in terms of absolute test accuracy for additive white Gaussian noise (AWGN) on MNIST and CIFAR-10 datasets by 33% and 41% respectively (at the cost of standard accuracy). We defer the details of this experiment to Appendix A. This loss of accuracy, and the effect of c are consistent with  Labatie (2019) , which remarks that under BN, directions of high signal variance are dampened, while directions of low signal variance are amplified. This preferential exploration of low signal directions reduces the signal-to-noise ratio and increases sensitivity w.r.t. the input. Increasing c reduces the sensitivity along "low signal directions".

Section Title: EMPIRICAL RESULTS
  EMPIRICAL RESULTS For the main practical results on MNIST, SVHN, CIFAR-10, and ImageNet, we evaluate the robustness-measured as a drop in test accuracy under various input perturbations-of convolutional networks with and without BN. 1 As a white-box adversarial attack we use projected gradient descent (PGD) in ∞ - and 2 -norm variants, for its simplicity and ability to degrade performance with little change to the input ( Madry et al., 2018 ). The PGD implementation details are provided in Appendix B. We report the test Under review as a conference paper at ICLR 2020 Standard meta-parameters from the literature were used to train models with and without BN from scratch over several random seeds. All uncertainties are the standard error of the mean accuracy. 2 For SVHN and CIFAR-10, we examine two different learning rate schemes, given that it has been suggested that one of the primary mechanisms of BN is to facilitate training with a larger learning rate ( Ioffe & Szegedy, 2015 ;  Bjorck et al., 2018 ): 1. A fixed "small" learning rate of 1e-2 (SVHN, CIFAR-10). 2. An initial "large" learning rate of 1e-1, with subsequent drops by a factor of ten (CIFAR-10). In the SVHN experiment, VGG variants ( Simonyan & Zisserman, 2015 ) are trained using using five random seeds. BN increased clean test accuracy by 1.86 ± 0.05%, but reduced test accuracy for additive noise by 5.5 ± 0.6%, for PGD- ∞ by 17 ± 1%, and for PGD- 2 by 20 ± 1%. We defer the full meta-parameters and results to Appendix E. For the CIFAR-10 experiments we trained models with a similar procedure as for SVHN, but with random 32 × 32 crops using four-pixel padding, and horizontal flips. We evaluate two families of contemporary models: one without skip connections (VGG) and a WideResNets (WRN) using "Fixup" initialization ( Zhang et al., 2019b ) to reduce the use of BN. Results of the first experiment are summarized in  Table 2 . In this case, inclusion of BN for VGG reduces the clean generalization gap (difference between training and test accuracy) by 1.1 ± 0.2%. For additive noise, test accuracy drops by 6 ± 1%, and for PGD perturbations by 17.3 ± 0.7% and 5.9 ± 0.4% for ∞ and 2 variants, respectively. Very similar results are obtained on a new test set, CIFAR-10.1 v6 ( Recht et al., 2018 ): BN slightly improves the clean test accuracy (by 2.0 ± 0.3%), but leads to a consider- able drop in test accuracy of 6 ± 1% for the case with additive noise, and 15 ± 1% and 3.4 ± 0.6% respectively for ∞ and 2 PGD variants (PGD absolute values omitted for CIFAR-10.1 in  Table 2  for brevity). In the second "large" learning rate exper- iment summarize in  Table 3 , we prolong training for up to 350 epochs, and drop the learning rate at epoch 150 and 250 in both cases. This increases clean test accuracy rel- ative to results in  Table 2 . The deepest model that could be trained without BN using this 2 Each experiment has a unique uncertainty, hence the number of decimal places varies. trained bag-of-local-feature models (BagNets) on ImageNet with an architecture that discards spatial information between patches and is thus considered to make extensive use of texture patterns for classification ( Brendel & Bethge, 2019 ). For patch sizes {9, 17, 33}, the top-5 accuracies of the BagNets are reduced to just 1.25%, 5.09%, and 14.62% for AWGN, respectively. Compared with  Table 5 , where all models obtain over 40%, these figures suggest that robustness to Gaussian noise is a good proxy for the use of texture for ImageNet classification. These results support the hypothesis that BN may be exacerbating this tendency to use superficial texture-like information. Next, we evaluate the robustness of pre-trained ImageNet models from the torchvision.models repository, which conveniently provides models with and without BN. 4 Results are shown in  Table 5 , where BN improves top-5 accuracy on noise in some cases, but consistently reduces it by 8.54% to 11.00% (absolute) for PGD. The trends are the same for top-1 accuracy, only the absolute values are smaller; the degradation varies from 2.38% to 4.17%. Given the discrepancy between noise and PGD for ImageNet, we include a black-box transfer analysis in the Appendix E.2 that is consistent with the white-box analysis. Finally, we explore the role of batch size and depth in  Figure 2 . We find that BN limits the maximum trainable depth, which increases with the batch size, but quickly plateaus as predicted by Theorem 3.10 of (Yang et al., 2019). Robustness decreases with the batch size for depths that maintain a reasonable test accuracy, at around 25 or fewer layers. This tension between clean accuracy and robustness as a function of the batch size is not observed in unnormalized networks.

Section Title: VULNERABILITY AND INPUT DIMENSION
  VULNERABILITY AND INPUT DIMENSION A recent work  Simon-Gabriel et al. (2019)  analyzes adversarial vulnerability of batch-normalized networks at initialization time and conjectures based on a scaling analysis that, under the com- monly used  He et al. (2015)  initialization scheme, adversarial vulnerability scales as ∼ √ d. They also show in experiments that indepen- dence between vulnerability and the input dimension can be approximately recovered through adversarial training by projected gra- dient descent (PGD) ( Madry et al., 2018 ), with a modest trade-off of clean accuracy. Intuitively, the input dimension should be ir- relevant as it does not affect the image com- plexity ( Shafahi et al., 2019 ). We show that this can be achieved by sim- pler means and with little to no trade-off through 2 weight decay, where the regular- ization constant λ corrects the loss scaling as the norm of the input increases with d. We increase the MNIST image width √ d from 28 to 56, 84, and 112 pixels. The loss L is predicted to grow like √ d for -sized attacks by Thm. 4 of  Simon-Gabriel et al. (2019) . We confirm that without regulariza- tion the loss does scale roughly as predicted (see Table 13 of Appendix F). Training with 2 weight decay, however, we obtain adversarial test accuracy ratios of 0.98±0.01, 0.96±0.04, and 1.00±0.03 and clean accuracy ratios of 0.999±0.002, 0.996 ± 0.003, and 0.987 ± 0.004 for √ d of 56, 84, and 112, respectively, relative to the original √ d = 28 dataset. A more detailed explanation and results are provided in Appendix F. Next, we repeat this experiment with a two- hidden-layer ReLU MLP, with the number of hidden units equal to the half the input dimension, and optionally use one hidden layer with batch norm. 5 To evaluate robust- ness, 100 iterations of BIM- ∞ were used with a step size of 1e-3, and ∞ = 0.1. We also report test accuracy with additive Gaus- sian noise of zero mean and unit variance, the same first two moments as the clean im- ages. 6 Despite a difference in clean accuracy of only 0.08 ± 0.05%,  Table 6  shows that for the original image resolution, batch norm reduced accuracy for noise by 16.4 ± 0.4%, and for BIM- ∞ by 43.8±0.5%. Robustness keeps decreasing as the image size increases, with the batch-normalized network having ∼ 40% less robustness to BIM and 13 − 16% less to noise at all sizes. We then apply the 2 regularization constants tuned for the respective input dimensions on the linear model to the ReLU MLP with no further adjustments.  Table 7  shows that by adding sufficient 2 regularization (λ = 0.01) to recover the original ( √ d = 28, no BN) accuracy for BIM of ≈ 66% when using batch norm, we induce a test error increase of 1.69 ± 0.01%, which is substantial on Under review as a conference paper at ICLR 2020 MNIST. Furthermore, using the same regularization constant and no batch norm increases clean test accuracy by 1.39 ± 0.04%, and for the BIM- ∞ perturbation by 21.7 ± 0.4%. Finally, following the guidance in the original work on batch norm ( Ioffe & Szegedy, 2015 ) to the extreme (λ = 0): when we reduce weight decay when using batch norm, accuracy for the ∞ = 0.1 perturbation is degraded by 79.3 ± 0.3% for √ d = 56, and 81.2 ± 0.2% for √ d = 84.

Section Title: In all cases, using batch norm greatly reduced test accuracy for noisy and adversarially perturbed inputs, while weight decay increased accuracy for such inputs.
  In all cases, using batch norm greatly reduced test accuracy for noisy and adversarially perturbed inputs, while weight decay increased accuracy for such inputs. As supplementary evidence, we contrast the "Fooling images" ( Nguyen et al., 2015 ) and  Carlini & Wagner (2017)  examples of BN vs. L2-regularized models on MNIST and SVHN in Appendix J.

Section Title: RELATED WORK
  RELATED WORK Our work examines the effect of batch norm on model robustness at test time. References with an immediate connection to our work were discussed in the previous sections; here we briefly mention other works that do not have a direct relationship to our experiments, but are relevant to batch norm in general. The original work  Ioffe & Szegedy (2015)  that introduced batch norm as a technique for improving neural network training and test performance motivated it in terms of "internal covariate shift" - referring to the changing distribution of layer outputs, an effect that requires subsequent layers to steadily adapt to the new distribution and thus slows down the training process. Several follow-up works started from the empirical observation that batch norm usually accelerates and stabilizes training, and attempted to clarify the mechanism behind this effect. One argument is that batch- normalized networks have a smoother optimization landscape due to smaller gradients immediately before the batch-normalized layer ( Santurkar et al., 2018 ). However, Yang et al. (2019) study the effect of stacking many batch-normalized layers and prove that this causes gradient explosion that is exponential in network depth for networks without skip connections and holds for any non-linearity. In practice, relatively shallow batch-normalized networks seem to benefit from the "helpful smoothing" of the loss surface property  Santurkar et al. (2018) , while very deep networks are not trainable (Yang et al., 2019). In our work, we found that a single batch-normalized layer suffices to induce severe adversarial vulnerability. A concurrent submission suggests that BN induced vulnerability may be due to a mismatch between the tracked mean and variance values at training versus test time (Anonymous, 2020). We investigate this hypothesis in Appendix I and find that the use of tracked statistics can play a similar role as tuning the numerical stability constant c, thus this does not completely account for the vulnerability. Weight decay's loss scaling mechanism is complementary to other mechanisms identified in the literature, for instance that it increases the effective learning rate (van Laarhoven, 2017;  Zhang et al., 2019a ). Our results are consistent with these works in that weight decay reduces the generalization gap (between training and test error), even in batch-normalized networks where it is presumed to have no effect. Given that batch norm is not typically used on all layers, the loss scaling mechanism persists, although to a lesser degree in this case.  Shafahi et al. (2019)  performed similar input dimension scaling experiments as in this work and came to a similar conclusion that the input dimension is irrelevant to adversarial vulnerability. However, like  Simon-Gabriel et al. (2019) , they use PGD rather than weight decay to prevent vulnerability from increasing with input dimension. Although it can be shown that robust optimization is equivalent to parameter norm regularization for linear models if we allow the -ball (aka disturbance δ) to vary with each example (Xu et al., 2009), we maintain that the latter is a more efficient approach.

Section Title: CONCLUSION
  CONCLUSION We found that there is no free lunch with batch norm when model robustness is a concern: the accelerated training properties and occasionally higher clean test accuracy come at the cost of increased vulnerability, both to additive noise and for adversarial perturbations. We have shown that there is no inherent relationship between the input dimension and vulnerability. Our results highlight the importance of identifying the disparate mechanisms of regularization techniques. Under review as a conference paper at ICLR 2020
  We first apply the noise to the original 28×28 pixel images, then resize them to preserve the appearance of the noise.

```
