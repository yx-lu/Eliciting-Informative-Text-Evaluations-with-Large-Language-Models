Title:
```
Under review as a conference paper at ICLR 2020 HIERARCHICAL DISENTANGLE NETWORK FOR OB- JECT REPRESENTATION LEARNING
```
Abstract:
```
An object can be described as the combination of primary visual attributes. Dis- entangling such underlying primitives is the long-term objective of representation learning. It is observed that categories have the natural hierarchical characteristic- s, i.e. any two objects can share some common primitives in a particular category level while they may possess their unique ones in another. However, previous works usually operate in a flat manner (i.e. in a particular level) to disentangle the representations of objects. Though they may obtain the primitives to constitute objects as the categories in that level, their results are obviously not efficient and complete. In this paper, we propose the hierarchical disentangle network (HDN) to exploit the rich hierarchical characteristics among categories to divide the dis- entangling process in a coarse-to-fine manner, such that each level only focuses on learning the specific representations and finally the common and unique represen- tations in all levels jointly constitute the raw object. Specifically, HDN is designed based on an encoder-decoder architecture. To simultaneously ensure the disentan- glement and interpretability of the encoded representations, a novel hierarchical generative adversarial network (GAN) is elaborately designed. Quantitative and qualitative evaluations on four object datasets validate the effectiveness of our method.
```

Figures/Tables Captions:
```
Figure 1: An illustration of a hierarchical structure (a) and extracting the hierarchical features that constitute a leaf-level image (b). In (b), the common features that only contain the information of its being the root category are first extracted. By tracing from the root to leaf, the unique features that contain additional information of its being the finer-grained category are further extracted.
Figure 2: An illustration of the framework of our method. Given images belonging to a hierarchy, the common feature maps of their being the root category and the unique features in different non- root levels that can further distinguish them as finer-grained categories are extracted by the upper residual and bottom 1*1 convolutional branches, respectively. The unique features are transformed as the parameters of Adaptive Instance Normalization (AdaIN) and thus be aggregated into the com- mon feature maps to obtain comprehensive representations of images. To ensure disentanglement of the unique features, different levels of them are randomly combined and then reconstructed in the image space where adversarial loss and hierarchical classification loss are elaborately designed.
Figure 3: Semantic translation results of the source images controlled by hierarchically disentangled features of the targets on CelebA. Different columns denote results of using F 1 , {R l } L l=2 or their combinations disentangled from the target images to replace the corresponding levels of the sources. Ground truths of R 2 , R 3 , R 4 are gender, smile and hair color, respectively.
Figure 4: 2D tSNE of disentangled F l on test set of CelebA for different levels. For easy understand, M and F mean male and female, S and N mean Smile and Neural, Bl, G and Br mean Black, Golden and Brown hair respectively.
Figure 5: Interpolations of disentangled R l between the source (first columns) and target (last columns) images (other levels unchanged for the source images).
Figure 6: Top-5 returned images of a retrieval case using different parts of features (i.e. different R l of HDN, and different bit parts of SSDH). Green and red boxes are correct and false samples judged by the hierarchy.
Figure 7: Semantic translation results between seen and unseen objects. Here we replace the source images with all levels of R l of the targets, i.e. the right most case in Fig.3.
Table 1: Accuracy of hierarchical classifications for test and generated (Gen.) images.
Table 2: mAP results of retrieval for compared methods in different semantic levels.
Table 3: Hierarchical prediction performance for seen test set and unseen leaf-level categories.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Representation learning, as one basic and hot topic in machine learning and computer vision com- munity, has achieved significant progress in recent years on different tasks such as recognition ( Rus- sakovsky et al., 2015 ), detection ( Ren et al., 2015 ;  Redmon et al., 2016 ;  Liu et al., 2016b ) and gen- eration ( Goodfellow et al., 2014 ), benefiting from the rapid development of representation learned by deep neural networks. Considering the strong capacity of deep representation, in this paper, we mainly focus on the deep representation learning framework. Despite great success the deep representations have achieved as mentioned above, two important problems are still unresolved or less considered, i.e. the interpretability and the disentanglement of the learned representations. In the past decades, various works have been developed to reveal the black box of deep learning ( Zeiler & Fergus, 2014 ;  Dosovitskiy & Brox, 2016b ;  Bau et al., 2017 ;  Simonyan et al., 2013 ;  Stock & Cissé, 2017 ;  Zhang et al., 2017 ) and move us closer to the goal of disentangling the variations within data ( Reed et al., 2014 ;  Mathieu et al., 2016 ;  Rifai et al., 2012 ;  Tran et al., 2017 ;  Gonzalez-Garcia et al., 2018 ;  Huang et al., 2018 ;  Chen et al., 2016 ). Even though they have brought great insights to us, they still have some limitations. For instance, ( Chen et al., 2016 ;  Xie et al., 2017 ;  Zhao et al., 2017 ) learn to disentangle variation factors within each category using generative models, instead of investigating the similarities and differences among categories, leading to poor discriminability. Therefore, the learned representations would not well conform to human perception. Though ( Gonzalez-Garcia et al., 2018 ;  Huang et al., 2018 ) try to obtain the domain-invariant and domain-specific knowledge, they can only handle two categories one time, which is not that efficient. In this paper, we attempt to learn disentangled representations in a more natural and efficient manner. Let us first discuss how humans understand an object. Generally speaking, an object can be regarded as the combination of many semantic attributes. Hundreds of thousands of objects in the world can be clustered and recognized by humans just because we can figure out the common and unique Under review as a conference paper at ICLR 2020 attributes of an object compared to others. Besides, a man who never play the billiards can only recognize a table in an image, while a sports fan may regard it as a billiard table. Both of them are right since categories have natural hierarchical structure. As shown in Fig. 1(a), given six leaf-level categories, they can be organized in a three-level hierarchy considering the common and different features they have. Each child category in the hierarchy is a special case of its parent category since it inherits all features from its parent category and has extra features that are not present in its parent category. From another perspective, each parent category is the abstraction of all its child categories considering it contains the attributes that are present in all its child categories. Then we come back to the task of disentangling representation learning. It aims to learn the representation encoding useful information that can be applied in other tasks (e.g. building classifiers and predictors) ( Bengio et al., 2013 ). Taking the hierarchical nature of categories into account, if we only learn the representations of an object in a flat manner for a specific category level as previous works do, it will not be scalable and comprehensive for the machine to be qualified for various tasks in the real world. Our work aims to exploit the natural hierarchical characteristics among categories to divide the rep- resentation learning in a coarse-to-fine manner, such that each level only focuses on learning the specific representations. For instance, given a billiard table image in Fig. 1(b), it tangles the in- formation of being a furniture, a table and a billiard table. We first extract the features that only contain the information of furniture from the image. By tracing from the root to leaf level, more and more information is extracted until we can recognize its belonging categories in all hierarchi- cal levels. By doing so, the disentangled representations are expected to find wide and promising applications. For example, one can transfer the semantics in a specific category level from one ob- ject to another while keep information of other levels unchanged. Besides, it would help for the hierarchical image compression task using different levels of the disentangled representations. To achieve the objective of hierarchical disentangling and simultaneously interpreting the results so that humans can understand, we propose the hierarchical disentangle network (HDN), which draws lessons from hierarchical classification and the recent proposed generative adversarial nets ( Good- fellow et al., 2014 ). Extensive experiments are conducted on four popular object datasets to validate the effectiveness of our method.

Section Title: RELATED WORKS
  RELATED WORKS

Section Title: Disentangling Deep Representations
  Disentangling Deep Representations The goal of disentangling representation learning is to dis- cover factors of variation within data ( Bengio et al., 2013 ). Recent years have witnessed a substantial interest on such research area ( Tenenbaum & Freeman, 1996 ), including works based on deep learn- ing ( Reed et al., 2014 ;  Mathieu et al., 2016 ;  Rifai et al., 2012 ;  Wang et al., 2017 ;  Tran et al., 2017 ;  Gonzalez-Garcia et al., 2018 ;  Huang et al., 2018 ;  Chen et al., 2016 ). ( Rifai et al., 2012 ) is proba- bly the earliest to learn disentangled representations using deep networks for the task of emotion recognition. ( Reed et al., 2014 ) is based on a higher-order Boltzmann machine and regards each factor variation of the manifold as its sub-manifold. ( Mathieu et al., 2016 ) and ( Chen et al., 2016 ) leverage the generative adversarial nets (GAN) to learn factors of variation. Recently, cross-domain translation methods ( Gonzalez-Garcia et al., 2018 ;  Huang et al., 2018 ) learn the domain-invariant and domain-specific representations. These works ignore the existing natural and inherent hierarchy relationships among categories, with which we can conduct the disentangling in a coarse-to-fine manner such that each level only focuses on learning the specific representations.

Section Title: Network Interpretability
  Network Interpretability Network interpretability aims to learn how the network works via visu- alizing it from the perspective that humans can understand. These methods can be briefly divided into two groups according to whether the visualization is involved in the network during training, i.e. the off-line methods and online methods. The off-line methods make attempts to visualize patterns in image space that activate each convolutional filter ( Zeiler & Fergus, 2014 ;  Dosovitskiy & Brox, 2016b ;a;  Bau et al., 2017 ; 2019) or to interpret the area in an image that is responsible for the net- work prediction ( Simonyan et al., 2013 ;  Fong & Vedaldi, 2017 ;  Zintgraf et al., 2017 ;  Abbasi-Asl & Yu, 2017 ;  Stock & Cissé, 2017 ;  Palacio et al., 2018 ;  Geirhos et al., 2019 ). While such methods can explain what has already been learned by the model, they cannot improve the model interpretability in return. Instead, the online works propose to directly learn interpretable representations during training ( Li et al., 2018 ;  Zhang et al., 2017 ). However, these methods mainly focus on figuring out the running mechanism of networks while paying less attention to dissecting variations of the features among categories, which cannot make models really understand their inputs.

Section Title: Hierarchy-regularized Learning
  Hierarchy-regularized Learning Semantic hierarchies have been explored in object classification task for accelerating recognition ( Griffin & Perona, 2008 ;  Marszalek & Schmid, 2008 ), obtaining a sequence of predictions ( Deng et al., 2012 ;  Ordonez et al., 2013 ), making use of category relation graphs ( Deng et al., 2014 ;  Ding et al., 2015 ), and improving recognition performance as additional supervision ( Zhao et al., 2011 ;  Srivastava & Salakhutdinov, 2013 ;  Hwang & Sigal, 2014 ;  Yan et al., 2015 ;  Goo et al., 2016 ;  Ahmed et al., 2016 ). While these discriminative classification works have achieved their expected goals, they usually lack interpretability. To address such issues, ( Xie et al., 2017 ;  Zhao et al., 2017 ) propose to use generative models to disentangle the factors from low-level representations to high-level ones that can construct a specific object. ( Singh et al., 2019 ) uses an unsupervised generative framework to hierarchically disentangle the background, object shape and appearance from an image. However, they either deal with each category in isolation or ignore the discriminability of learned features, and thus cannot accurately disentangle the differences and similarities among categories.

Section Title: HIERARCHICAL REPRESENTATION LEARNING
  HIERARCHICAL REPRESENTATION LEARNING Supposing that a category hierarchy is given in the form shown in Fig. 1(a), we use l = 1, ..., L to denote the level of hierarchy (L for the leaf level and 1 for the root level), K l to denote the number of nodes at level l, n k l to denote the k-th node at level l, and C k l to denote the number of children for n k l . As illustrated in Fig. 1(b), given an original object image denoted as I o , our goal is to extract the feature F l in the l-th level. Generally speaking, an object O can be described as the combination of a group of visual attributes: Under review as a conference paper at ICLR 2020 where ∆(O) represents currently undefined attributes existing on O. As we have discussed, humans classify O in a particular category level according to a subset of the whole attribute set in Eqn.(1). Take the object in Fig. 1(b) for example, it can be regarded as a furniture since it contains the attribute subset {A 1 + ... + A i }, and be classified to a table in terms of the attribute subset {A 1 + ... + A i + A i+1 + ... + A j } present in it. Therefore, the disentangled feature F l for our objectives in Fig. 1(b) is actually the reflection of the attribute subset formulated in Eqn.(1). Moreover, since the hierarchical correlations (i.e. the inherited relationship) among categories in different hierarchies, obviously the subset {A 1 + ... + A i + A i+1 + ... + A j } includes the subset {A 1 + ... + A i }, naturally leading to the disentangled F l−1 being the proper subset of F l . Taking these into consideration, we design the hierarchical disentangle network (HDN) based on the autoencoder architecture in  Fig. 2 . The encoder E dissects the hierarchical representations given a semantic hierarchy. The decoder G plays the role of an interpreter to reflect the variations of semantic in the image space for different hierarchical levels guided by the hierarchical discriminator D adv and classifiers D cls (they share most network architecture except the output layers).

Section Title: TOP-DOWN LEARNING OF HIERARCHICAL REPRESENTATIONS
  TOP-DOWN LEARNING OF HIERARCHICAL REPRESENTATIONS Since F l−1 is the proper subset of F l , once F l−1 is obtained, only the difference R l (1 < l ≤ L) between F l and F l−1 needs to be encoded. Considering these, we devise a top-down representation extraction scheme. Given F l−1 and R l , we aggregate them together to obtain the whole representation in the l-th level. Such procedure can be formulated as: F l = F l−1 ⊕ R l (2) where ⊕ means information aggregation. In summary, for hierarchical disentanglement, the com- mon feature F 1 in the root level and the unique ones {R l } L l=2 in deeper levels need to be encoded. To further interpret the semantics of these features to humans, the decoder reconstructs them in the image space. The semantics of F 1 are shared among all its offspring which can be regarded as the invariant content of the object, while those of {R l } L l=2 are unique for different levels which plays the role of the variant style of the object. Therefore, F 1 and {R l } L l=2 are processed in the upper and bottom branches respectively to make them play different roles during the reconstruction, as shown in  Fig. 2 .

Section Title: CONSTRAINTS FOR THE LEARNING PROCESS
  CONSTRAINTS FOR THE LEARNING PROCESS The basic constraints of hierarchical disentanglement are making features in different levels perform their own duties. For an object O, the encoded F 1 and {R l } L l=2 are complementary, as the constraints of F l being the proper subset of F l+1 . F 1 should encode just right information to describes its being the root category. Progressively using R l , one can distinguish it from other categories in l-th level. Apart from disentanglement, visualization of features in the image space is also one of our objec- tives. We turn to the popular conditional generative adversarial nets (cGANs) ( Mirza & Osindero, 2014 ) which can control generated images given conditions. Our HDN leverages the disentangled features F 1 and {R l } L l=2 to control the variations of reconstructed images in different category levels. To ensure F 1 , {R l } L l=2 be well disentangled, we propose a random combination strategy for different levels of features from different objects and control the generated images through these combined features, as shown in Fig.2. Specifically, given F 1 1 , {R 1 l } L l=2 and F 2 1 , {R 2 l } L l=2 from arbitrary two objects, we obtain the newly combined features F 1 and {R l } L l=2 , where ∀1 ≤ l ≤ L, R l (F 1 if l = 1) come from either the first or second object. The newly combined features are aggregated together as the input for the decoder G to generate a new object image I g . Such image should satisfy the following losses: - Hierarchical classification loss. For each level, I g should be classified to the category that R i l reflects (root level F 1 only contains one category), defined as: Under review as a conference paper at ICLR 2020 where J cls is cross-entropy loss among local categories in each level which have a common parent node k such as the dashed rectangled categories in the bottom right corner of Fig.2. p(G) denotes distribution of generated images G(F i 1 , {R i l } L l=2 ). D cls (I g ) c l is probabilistic prediction on the c-th local category, and y c l is the ground truth local label of the generated object in the l-th level. Please note that we only focus on the local brother categories instead of all categories in that level. It makes the disentanglement more flexible. On one hand, the classification in each level can thus only focus on the unique features that are just discriminative among those local brother categories. On the other hand, the duties of different levels can be well disentangled, since if the semantic information encoded in different levels is tangled, after the random combination and image reconstruction, the hierarchical classifiers would be quite confused. - Adversarial loss. We employ GANs to match the distribution of reconstructed images to the real data distribution. Specifically, the LS-GAN ( Mao et al., 2017 ) loss is adopted in light of its stable training, defined as: - Image reconstruction loss. As for F 1 and {R 1 l } L l=2 from one same object, we should be able to reconstruct it as close to the input as possible. J I recon = E I r ∼p (G) [||I r − I o || 1 ] (5) where p (G) is the distribution of generations taking F 1 , {R l } L l=2 from the same objects as inputs. - Feature reconstruction loss. Apart from the image reconstruction loss, the feature reconstruction loss is added to HDN to stabilize the training process. where p(E) is the distribution of encoded hierarchical features E(I o ). Now we combine the four loss functions defined in Eqn.(3), Eqn.(4), Eqn.(5) and Eqn.(6) into one comprehensive loss function for supervising the disentangling of E and visualization of G: J(E, G) = J cls + J GAN + αJ I recon + βJ F,R recon (7) where α and β are the hyper-parameters to balance the weights of the four terms. As for the update of discriminator and hierarchical classifiers, we use the following loss:

Section Title: RELATIONSHIP WITH PREVIOUS WORK
  RELATIONSHIP WITH PREVIOUS WORK It is noted that the recent work DTLC-GAN ( Kaneko et al., 2018 ) has similarities with our method on motivations of learning hierarchical representations. Nevertheless, DTLC-GAN is indeed differ- ent from ours. Specifically, the detailed goals of leveraging hierarchical relationship are different. DTLC-GAN aims to maximize the mutual information between conditioned representation and data in each level, i.e. study how the appearance of data varies with more and more specific conditions and thus synthesize data with more fine-grained details. Our method focuses more on how humans distinguish objects from categories in different hierarchical levels and wishes such manner of un- derstanding objects can be applied to the machine, i.e. learn the commonality and individuality of categories in nature. Therefore, the disentangled features of our method are mainly served for down- stream discriminative tasks such as semantic retrieval, open world unseen category recognition as we have attempted in the following experiments. Besides, thanks to the disentangled commonality, our method can further realize the semantic translations between images by exchanging the individual parts, which has been a popular application in real world.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: Datasets
  Datasets We conduct experiments on hierarchical annotated data from four datasets, typical ex- amples in the hierarchy are shown in Fig.8, Fig.9 and Fig.10 in Appendix 1 . The first is CelebA dataset ( Liu et al., 2015 ). It provides more than 200K face images with 40 attribute annotation- s. Following the official train/test protocol, we define a four-level hierarchical structure which has explicit attribute difference between any two levels. Specifically, all faces (root category) are first divided into two categories based on gender. Such initial categories are further classified according to the smile expression and hair color in the next two levels. With such ground-truth hierarchical annotations, we can validate our method more easily. The second dataset named Fashion-MNIST ( Xiao et al., 2017 ) is proposed as a direct drop-in re- placement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same train/test split with MNIST. Since such dataset does not provide any hierarchical structure, we cluster T-shirt, coat, pullover as one super category and trouser, dress as another super one to construct a three-level hierarchy (root is fashion) according to their appearance similarities. The other two datasets are 3D data, CADCars ( Fidler et al., 2012 ) and ShapeNet ( Chang et al., 2015 ). CADCars contains 183 3D Car models and ShapeNet is constitutive of 51,300 3D models covering 55 common and 205 finer-grained categories. Using the provided tools, we generated 24 2D images with 6 pose and 4 illumination variations for CADCars. These 2D data are clustered into four super categories, i.e. minibus, sedan, sports and SUV, and are further divided into 6 finer-grained categories for each super one based on pose annotations, which defines a three-level hierarchy. On ShapeNet, 12 2D images with pose variation are obtained for each 3D model. One three-level category-pose hierarchy named as ShapeNet-P similar to CADCars and one three-level hierarchy named as ShapeNet-C as in Fig.1.(a) are defined. Ratio of train/test split is 4:1.

Section Title: Implementation Details
  Implementation Details Our HDN is implemented with Pytorch platform 2 . Design of the back- bone follows recent proposed image generation ( Karras et al., 2018 ) and translation works ( Huang et al., 2018 ). Images are resized to 128*128 resolution for all datasets except Fashion-MNIST which is resized to 28*28. As shown in Fig.2, to match it with our task, we increase the number of 1*1 convolution branches such that originally one representation is disentangled into multiple hierar- chical levels. We also equip the residual blocks with Adaptive Instance Normalization (AdaIN) whose parameters are dynamically generated by a multi-layer perception (MLP) from the disentan- gled latent codes. Besides, D has L output branches, one for real/fake predictions and the others for hierarchical classifications. More training details are given in the Appendix.

Section Title: DISENTANGLED RESULTS
  DISENTANGLED RESULTS Firstly, we replace one or several levels of disentangled features for an image with those of another image, and then observe the visual changes of generated image to validate the semantic consistence with pre-defined hierarchical structure. Fig.3 (and Fig.12, Fig.13 in the Appendix) shows such semantic translation results. It is observed that different level of features perform their own duties, i.e. they carry just enough information to control the variations within that level (e.g. gender, smile and hair color for CelebA we specially predefined on CelebA), but would not involve more belongs to other level. For instance, in Fig.3 change features of an image in arbitrary one, two or all levels to those of another image, the semantics would be changed correspondingly. Apart from {R l } L l=2 , the common feature F 1 also encodes information that is not discriminative among its offspring categories but is necessary to construct the object (e.g. the identity, pose and even the background information of a face image). To give a more intuitive feeling about the ability of disentangled features, we investigate the discriminabilites of them via the popular tSNE tool ( Maaten & Hinton, 2008 ). As shown in Fig.4 (and Fig.14, Fig.15, Fig.16, Fig.17 in the Appendix), with only the common feature F 1 , samples are mixed together. When progressively be combined with features of deeper levels R l , samples are better separated and almost consistent with the hierarchical structure. Apart from direct visual edit, we also show that one can transform the source image smoothly by linear interpolation (with 5 equally spaced interpolation coefficients from 0.1 to 0.9) of disentangled features between the source and target. Such examples are shown in Fig.5. We can see that genders, expressions, hair colors and their combinations of the source images (first columns in each case) can be changed smoothly towards those of the targets (last columns of each case). Learning a smooth feature space with continuous variations is a significant issue for representation learning task, which can ensure the generalization ability for unseen similar objects. We have investigated this in Sec.4.3. At the end of this section, a quantitative evaluation of these results is conducted. To be specific, we use the learned hierarchical classifier D to evaluate whether the semantics are correctly disentangled and decoded into the randomly translated images as in Fig.3. To ensure D is reliable, the accuracy of hierarchical classifications on the test data is given as a reference.  Table.1  gives the evaluation results. Firstly, it can be seen that the semantics of translated images with changing different levels are recognized correctly by the corresponding classifiers. Secondly, the deeper of the level, the more difficult of the translation, since the criteria for distinguish one category from others in the deeper level would become more and more complicated (summation of all criteria above this level). Finally, it becomes difficult to transfer the unique features and generate images when that information is difficult to be described and disentangled such as in the leaf-level of Fashion-MNIST and ShapeNet-C, which deserves to make more efforts.

Section Title: APPLICATION TO IMAGE RETRIEVAL
  APPLICATION TO IMAGE RETRIEVAL One of the objectives of learned representations is to be applied in real-world applications. Semantic image retrieval has been studied for years. Hashing is one effective and space-time efficient solution for this task. However, the semantic of target images that users expect are not always consistent just due to the tangled information of objects in different hierarchical levels. In this section, we conduct retrieval in different levels on CelebA. We compare three competing deep hashing methods, i.e. DSH ( Liu et al., 2016a ), HashNet ( Cao et al., 2017 ) and SSDH ( Yang et al., 2018 ), and two strong pre-trained GAN baselines, i.e. supervised StarGAN ( Choi et al., 2018 ) and unsupervised StyleGAN  Karras et al. (2018) . The backbones of hashing methods are same with the bottom branch of encoder E of HDN and pretrained on CASIA WebFace dataset ( Yi et al., 2014 ). In l-th level, a model with bit-length as same as the dimension of the concatenation of {R l } l 2 is trained. As for StarGAN and StyleGAN, the latent features before the last layer of discriminator are used as the representations of samples. To make a fair comparison with hashing methods, we also binarize the disentangled features via Sigmoid activation fucntion, which we named as HDN-B. Images of the test set are used to retrieve the training set.  Table.2  gives the mAP results in different semantic levels. First, our method achieves the best performance, though we did not impose specific metric objectives on features while the flat StarGAN and StyleGAN perform not well. Second, HDN is more efficient since it only needs one model owing to disentanglement while hashing methods have to train a model in each level. We also tried to use only one model trained in the leaf-level to evaluate in high levels (methods with postfix of "-S"), but the results are inferior to those independently trained for each level. Third, HDN-B is better than HDN, which mainly due to the increased non-linear ability of features. Finally, the retrieval of HDN is more interpretable. As shown in Fig.6, with different parts of features, the returned images satisfy different semantic requirement, while for general method like SSDH one can not interpret the meanings of different code parts.

Section Title: UNSEEN CATEGORY PREDICTION AND SEMANTIC EDIT
  UNSEEN CATEGORY PREDICTION AND SEMANTIC EDIT Recognition of unseen categories is a challenging task for deep learning models, which has high requirements for the generalization ability of models. As our HDN learns features in different hier- archy levels, it can obtain sequenced category predictions for an object. Therefore, if unseen objects share similarities with seen ones, one should still obtain the right predictions in those levels, and the predictions among seen categories in levels where unseen objects have their own features should be confused. For levels with similarities and with their own features, accuracy and entropy of predic- tions of a linear hierarchical classifier trained with the disentangled features are used as evaluation metrics respectively. In this section, we test HDN on certain unseen leaf-level categories, i.e. bald and gray hair of CelebA, kinds of new tables and sofas of ShapeNet-C and objects with other poses of ShapeNet-P (typical examples of these data are shown in Fig.11 in the Appendix).  Table.3  shows the quantitative results. Two conclusions can be reached. 1). In levels where seen and unseen objects share similarities (i.e. gender, smile levels on CelebA, Sofa/Table level on ShapeNet- C, Loveseat/Club chair/Work table/Billiards on ShapeNet-P), most objects can be correctly classi- fied. 2). In the leaf-level, unseen objects have the unique unseen features, leading to the prediction entropy increase obviously compared with that of seen objects. Besides, it is found that the unseen objects are more likely classified to similar seen categories in leaf-level. For instance, about 30% and 56% bald faces are recognized as black and golden hair respectively, 50% and 50% leather couches are predicted as loveseat and L-couch respectively, 44% and 50% of the frontal sofa/table are classified as the right 30 • offset of frontal and left 30 • offset of frontal. The semantic translations in Fig.7 between seen and unseen images also verify such results. The semantics of non-leaf levels can be transferred as usual, but the unseen unique features are not. Bald may be disentangled as golden or black hair due to the skin color. The material of leather is ignored in ShapeNet-C since model focus more on shape to distinguish seen objects rather than material during training. The translations to frontal pose are also confused as can be found in the cases of ShapeNet-P. Through this study, we think that disentangling the visual primitives of objects as learned knowledge is one of the most promising solutions for the ability of open-world recognition.

Section Title: CONCLUSIONS
  CONCLUSIONS We propose the hierarchical disentangle network (HDN) which exploits the natural characteristic- s among categories to divide the representation learning in a coarse-to-fine manner. Our model achieves promising disentangle results. We also show the applications of such disentangled features on semantic translation, retrieval and even unseen objects prediction. However, our work is just an early step towards the long goal of disentangled representation learning, limited by the capacity of generative models on large scale and heavily tangled categories, the performance of HDN is not quite well such as on the ImageNet dataset which deserves to make more efforts.

```
