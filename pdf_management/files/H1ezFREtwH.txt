Title:
```
Published as a conference paper at ICLR 2020 COMPOSING TASK-AGNOSTIC POLICIES WITH DEEP REINFORCEMENT LEARNING
```
Abstract:
```
The composition of elementary behaviors to solve challenging transfer learning problems is one of the key elements in building intelligent machines. To date, there has been plenty of work on learning task-specific policies or skills but almost no focus on composing necessary, task-agnostic skills to find a solution to new problems. In this paper, we propose a novel deep reinforcement learning-based skill transfer and composition method that takes the agent's primitive policies to solve unseen tasks. We evaluate our method in difficult cases where training policy through standard reinforcement learning (RL) or even hierarchical RL is either not feasible or exhibits high sample complexity. We show that our method not only transfers skills to new problem settings but also solves the challenging environ- ments requiring both task planning and motion control with high data efficiency.
```

Figures/Tables Captions:
```
Figure 1: Policy ensemble composition model that takes the state information s t and a set of primi- tive policies' output {â i } N i=0 to compute a composite action a t .
Figure 2: Benchmark control and manipulation tasks requiring an agent to reach or move the object to the given targets (shown in red for pusher and green for rest).
Figure 3: Comparison results of our method against several standard RL methods averaged over ten trials in a set of difficult tasks. The vertical and horizontal axis represents the distance of the agent/object from the target and environment steps in millions, respectively. Note that our compo- sition framework learns to solve the task with high samples efficiency, whereas other benchmark methods either fail or perform poorly.
Figure 4: Performance comparison of our composition model trained with HIRO on a standard Ant (150 units torque limit) against three variants of standard HIRO formulation in three challenging environments. The pretrained HIRO model undergoes 4 million steps of extra training to counter for the training time utilised by premitive skills of our composition model. The low-torque-Ant has 30 units toruqe limit. We report mean and standard error, over ten trials, of agent's final distances from the given goals, normalized by their initial distance, over 10 million steps.
Figure 5: Ablative Study: Performance comparison, averaged over ten trials, of our composite model against its ablated variations that lack attention model, bidirectional-RNN (BRNN) or both attention and BRNN (AttBRNN) in three different environments.
Figure 6: Each path corresponds to its adja- cent attention weight mapping. The weighting "strength" of each primitive policy is depicted for each step (i.e. up (U), down (D), left (L), and right (R)). Each path begins at the origin and ends when the point-mass is within one unit of a goal. The plot contours represent the position cost.
Table 1: Performance comparison of our model against SAC (Haarnoja et al., 2018b), TRPO (Schul- man et al., 2015), and PPO (Schulman et al., 2017) on benchmark control tasks in terms of distance (lower the better) of an agent from the given target. The mean final distances with standard devia- tions over ten trials are reported. We also normalize the reported values by the agent initial distance from the goal so values close to 1 or higher show failure. It can be seen that our method (shown in bold) accomplishes the tasks by reaching goals whereas other methods fail except for SAC in simple Pusher and Ant Random Goal environments.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Compositionality is the integration of primitive functions into new complex functions that can fur- ther be composed into even more complex functions to solve novel problems ( Kaelbling & Lozano- Pérez, 2017 ). Evidence from neuroscience and behavioral biology research shows that humans and animals have the innate ability to transfer their basic skills to new domains and compose them hi- erarchically into complex behaviors ( Rizzolatti et al., 2001 ). In robotics, the primary focus is on acquiring new behaviors rather than composing and re-using the already acquired skills to solve novel, unseen tasks ( Lake et al., 2017 ). In this paper, we propose a novel policy ensemble composition method 1 that takes the basic, task- agnostic robot policies, transfers them to new complex problems, and efficiently learns a composite model through standard- or hierarchical-RL ( Schulman et al., 2015 ; 2017;  Haarnoja et al., 2018b ;  Vezhnevets et al., 2017 ;  Florensa et al., 2017 ;  Nachum et al., 2018 ). Our model has an encoder- decoder architecture. The encoder is a bidirectional recurrent neural network that embeds the given skill set into latent states. The decoder is a feed-forward neural network that takes the given task information and latent encodings of the skills to output the mixture weights for skill set compo- sition. We show that our composition framework can combine the given skills both concurrently (and -operation) and sequentially (or -operation) as per the need of the given task. We evalu- ate our method in challenging scenarios including problems with sparse rewards and benchmark it against the state-of-the-art standard- and hierarchical- RL methods. Our results show that the proposed composition framework is able to solve extremely hard RL-problems where standard- and hierarchical-RL methods are sample inefficient and either fail or yield unsatisfactory results.

Section Title: RELATED WORK
  RELATED WORK In the past, robotics research has been primarily focused on acquiring new skills such as Dynamic Movement Primitives (DMPs) ( Schaal et al., 2005 ) or standard reinforcement learning policies. A lot of research in DMPs revolves around learning compact, parameterized, and modular representa- tions of robot skills ( Schaal et al., 2005 ;  Ijspeert et al., 2013 ;  Paraschos et al., 2013 ;  Matsubara et al., 2011 ). However, there have been quite a few approaches that address the challenge of composing DMPs in an efficient, scalable manner. To date, DMPs are usually combined through human-defined heuristics, imitation learning or planning ( Konidaris et al., 2012 ;  Muelling et al., 2010 ;  Arie et al., 2012 ;  Veeraraghavan & Veloso, 2008 ). Likewise, RL ( Sutton & Barto, 2018 ) research is also central- ized around learning new policies (? Schulman et al., 2015 ; 2017;  Haarnoja et al., 2018b ) for complex decision-making tasks by maximizing human-defined rewards or intrinsic motivations ( Silver et al., 2016 ;  Qureshi et al., 2017 ; 2018;  Levine et al., 2016 ). To the best of the authors' knowledge, there hardly exists approaches that simultaneously combine and transfer past skills into new skills for solving new complicated problems. For instance,  Todorov (2009) ,  Haarnoja et al. (2018a)  and  Sahni et al. (2017)  require humans to decompose high-level tasks into intermediate objectives for which either Q-functions or policies are obtained via learning. The high-level task is then solved by merely maximizing the average intermediate Q-functions or combining intermediate policies through temporal-logic. Note that these approaches do not combine task-agnostic skills thus lack generalizability and the ability to transfer skills to the new domains. A recent and similar work to ours is a multiplicative composition policies (MCP) framework ( Peng et al., 2019 ). MCP comprises i) a set of trainable Gaussian primitive policies that take the given state and proposes the corresponding set of action distributions and ii) a gating function that takes the extra goal information together with the state and outputs the mixture weights for composition. The primitive policies and a gating function trained concurrently using reinforcement learning. In their transfer learning tasks( Peng et al., 2019 ), the primitive polices parameters are kept fixed, and the gating function is trained to output the mixture weights according to the new goal information. In our ablation studies, we show that training an MCP like-gating function that directly outputs the mixture weights without conditioning on the latent encoding of primitive actions gives inferior performance compared to our method. Our method utilizes all information (states, goals, and primitive skills) in a structured way through attention framework, and therefore, leads to better performance. Recent advancements lead to Hierarchical RL (HRL) that automatically decomposes the complex tasks into subtasks and sequentially solves them by optimizing the given objective function (? Vezh- nevets et al., 2017 ;  Nachum et al., 2018 ). In a similar vein, the options framework ( Sutton et al., 1999 ;  Precup, 2000 ) is proposed that solves the given task through temporal abstraction. Recent methods such as option-critic algorithm ( Bacon et al., 2017 ) simultaneously learns a set of sub-level policies (options), their termination functions, and a high-level policy over options to solve the given problem. Despite being an exciting step, the option-critic algorithm is hard to train and requires reg- ularization ( Vezhnevets et al., 2016 ;  Harb et al., 2018 ), or else it ends up discovering options for every time step or a single option for the entire task. In practice, the sub-level options or objectives obtained via HRL are inherently task-specific and therefore cannot be transferred to new domains.

Section Title: BACKGROUND
  BACKGROUND We consider a standard RL formulation based on Markov Decision Process (MDP) defined by a tuple {S, A, P, R}, where S and A represent the state and action space, P is the set of transition probabilities, and R denotes the reward function. At time t ≥ 0, the agent observes a state s t ∈ S and performs an action a t ∈ A. The agent's action a t transitions the environment state from s t ∈ S to s t+1 ∈ S with respect to the transition probability P(s t+1 |s t , a t ) and leads to a reward r t ∈ R. For compositionality, we extend the standard RL framework by assuming that the agent has ac- cess to the finite set of primitive policies Π = {π i } N i=0 that could correspond to agent's skills, controller, or motor-primitives. Our composition model is agnostic to the structure of primitive policy functions, but for the sake of this work, we assume that each of the sub-policies {π i } N i=0 solves the MDP defined by a tuple {Ŝ, A,P,R i }. Therefore,Ŝ,P andR i are the state-space, transition probabilities and rewards of the primitive policy π i , respectively. Each of the primitive policies π i :Ŝ × A → [0, 1], ∀i ∈ [0, 1, · · · , N ], takes a stateŝ ∈Ŝ and outputs a distribu- Published as a conference paper at ICLR 2020 tion over the agent's action space A. We define our composition model as a composite policy π c θ : S × A N +1 × A → [0, 1], parameterize by θ, that outputs a distribution over the action space conditioned on the environment's current state s ∈ S and the primitive policies {â i ∈ A} N i=0 ∼ Π. The state space of the composite model is S = [Ŝ, G]. The space G could include any task spe- cific information such as target locations. Hence, in our framework, the state inputs to the primitive policies Π and composite policy π c θ need not to be the same. In remainder of this section, we show that our composition model solves an MDP problem. To avoid clutter, we assume that both primitive policy ensemble and composite policy have the same state space S, i.e., G = ∅. The composition model samples an action from a distribution parameterized by the actions of sub-level policies and the state s ∈ S of the environment. Therefore, we can augment the given state space S as S c : A N +1 × S, where A N +1 : {A i } N i=0 are the outputs of sub-level policies. Hence, compositional MDP is defined as {S c , A, P c , R} where S c = A N × S is the new composite state-space, A is the action-space, P c : S c × S c × A → [0, 1] is the transition probability function, and R is the reward function for the given task.

Section Title: POLICY ENSEMBLE COMPOSITION
  POLICY ENSEMBLE COMPOSITION In this section, we present our policy ensemble composition framework, shown in  Fig. 1 . Our composition model consists of i) the encoder network that takes the outputs of primitive policies and embeds them into latent spaces; ii) the decoder network that takes current state s t of the environment and the latent embeddings from the encoder network to parameterize the attention network; iii) the attention network that outputs the probability distribution over the primitive low-level policies representing their mixture weights. The remainder of the section explains the individual models of our composition framework and the overall training procedure.

Section Title: ENCODER NETWORK
  ENCODER NETWORK Our encoder is a bidirectional recurrent neural network (BRNN) that consists of Long Short-Term Memory units ( Hochreiter & Schmidhuber, 1997 ). The encoder takes the outputs of the policy ensemble {â i } N i=0 and transform them into latent states of forward and backward RNN, denoted as {h f i } N +1 i=0 and {h b i } N +1 i=0 , respectively, where h f i , h b i ∈ R d ; ∀i ∈ [0, 1, · · · , N + 1]. The N + 1 states of forward and backward RNN corresponds to their last hidden states denoted as h f and h b , respectively, in  Fig. 1 .

Section Title: DECODER NETWORK
  DECODER NETWORK Our decoder is a simple feed-forward neural network that takes the last hidden states of the forward and backward encoder network, i.e., {h f , h b }, and the current state of the environment s to map them into a latent space h ∈ R d . The state input to the decoder network is defined as s : [ŝ, g], Published as a conference paper at ICLR 2020 whereŝ ∈Ŝ is the state input to the low-level policy ensemble and g ∈ G could be any additional information related to the given task, e.g., goal position of the target to be reached by the agent.

Section Title: ATTENTION NETWORK
  ATTENTION NETWORK The composition weights (see  Fig. 1 ) {w i ∈ [0, 1]} N i=0 are determined by the attention network as follows: where W f , W b , W d ∈ R d×d and W ∈ R d . The weights {w i } N i=0 for the composite policy are computed using gumbel-softmax denoted as softmax(q/T), where T is the temperature term ( Jang et al., 2016 ).

Section Title: COMPOSITE POLICY
  COMPOSITE POLICY Given the primitive policy ensemble Π = {π i } N i=0 , the composite action is the weighted sum of all primitive policies outputs, i.e., π c θ = N i w i π i . Since, we consider the primitive policies to be Gaussian distributions, the output of each primitive policy is parameterized by mean µ and variance σ, i.e., {â i ∼ N (µ i , σ i )} N i=0 ← {π i } N i=0 . Hence, the composite policy can be represented as π c θ = N i w i N (µ i , σ i ), where N (·) denotes Gaussian distribution, and i w i = 1. Given the mixture weights, other types of primitive policies, such as DMPs ( Schaal et al., 2005 ), can also be composed together by the weighted combination of their normalized outputs.

Section Title: COMPOSITE MODEL TRAINING OBJECTIVE
  COMPOSITE MODEL TRAINING OBJECTIVE The general objective of RL methods is to maximize the cumulative expected reward, i.e., J(π c θ ) = E π c θ [ ∞ t=0 γ t r t ], where γ : (0, 1] is a discount factor. We consider the policy gradient methods to update the parameters θ of our composite model, i.e., θ ← θ + η θ J(π c θ ), where η is the learning rate. We show that our composite policy can be trained through standard RL and HRL methods, described as follow.

Section Title: STANDARD REINFORCEMENT LEARNING
  STANDARD REINFORCEMENT LEARNING In standard RL, the policy gradients are determined by either on-policy or off-policy updates (? Schulman et al., 2015 ; 2017;  Haarnoja et al., 2018b ) and any of them could be used to train our composite model. However, in this paper, we consider off-policy soft-actor critic (SAC) method ( Haarnoja et al., 2018b ) for the training of our policy function. SAC maximizes the expected entropy H(·) in addition to the expected reward, i.e., J(π c θ ) = T t=0 E π c θ [r(s t , a t ) + λH(π c θ (·|s t ))] (2) where λ is a hyperparameter. We use SAC as it motivates exploration and has been shown to capture the underlying multiple modes of an optimal behavior. Since there is no direct method to estimate a low-variance gradient of Eq (2), we use off-policy value function-based optimization algorithm (for details refer to Appendix A.1 of supplementary material).

Section Title: HIERARCHICAL REINFORCEMENT LEARNING
  HIERARCHICAL REINFORCEMENT LEARNING In HRL, there are currently two streams - task decomposition through sub-goals ( Nachum et al., 2018 ) and option framework ( Bacon et al., 2017 ) that learns temporal abstractions. In the options framework, the options can be composite policies that are acquired with their termination functions. In task decomposition methods that generate sub-goal through high-level policy, the low-level policy can be replaced with our composite policy. In our work, we use the latter approach ( Nachum et al., 2018 ), known as HIRO algorithm, to train our policy function. Like, standard HIRO, we use two level policy structure. At each time step t, the high-level policy π hi θ , with parameters θ , observes a state s t and takes an action by generating a goal g t ∈ S in the state-space S for the composite low-level policy π c:low θ to achieve. The π c:low θ takes the state s t , the goal g t , and the primitive actions {â i } N 0 to predict a composite action a t through which an agent interacts with the environment. The high-level policy is trained to maximize the expected task rewards given by the environment whereas the composite low-level policy is trained to maximize the expected intrinsic reward defined as the negative of distance between current and goal states, i.e., s t + g t − s t+1 2 . To conform with HIRO settings, we perform off-policy correction of the high- level policy experiences and we train both high- and low-level policies via TD3 algorithm ( Fujimoto et al., 2018 ) (for details refer to Appendix A.2 of supplementary material).

Section Title: EXPERIMENTS AND RESULTS
  EXPERIMENTS AND RESULTS We evaluate and compare our method against standard RL, and HRL approaches in challenging environments (shown in  Fig. 2 ) that requires complex task planning and motion control. The imple- mentation details of all presented methods and environment settings are provided in Appendix C of supplementary material. We also do an ablative study in which we take away different components of our composite model to highlight their importance. Furthermore, we depict attention weights of our model in a navigation task to highlight its ability of concurrent and sequential composition. We consider the following seven environments for our analysis: (1) Pusher: A simple manipulator has to push an object to a given target location. (2) Ant Random Goal: In this environment, a quadruped-Ant is trained to reach the randomly sampled goal location in the confined circular region. (3) Ant Cross Maze: The cross-maze contains three target locations. The task for a quadruped Ant is to reach any of the three given targets by navigating through a 3D maze without collision. (4) HalfCheetah Hurdle: In this problem, the task for a halfcheetah is to run and jump over the three barriers to reach the given target location. (5) Ant Maze: A ⊃-shaped maze poses a challenging navigation task for a quadruped-Ant. In this task, the agent is given random targets all along the maze to reach while training. However, during the evaluation, we test the agent for reaching the farthest end of the maze. (6) Ant Push: A challenging environment that requires both task and motion planning. The environment contains a movable block, and the goal region is located behind that block. The task for an agent is to reach the target by first moving to the left of the maze so that it can move up and right to push the block out of the way for reaching the target. (7) Ant Fall: A navigation task where the target is located across the rift in front of the agent's initial position. There also happen to be a moveable block, so the agent has to move to the right, push the block forward, fill the gap, walk across, and move to the left to reach the target location. (8) Multi-goal Point Mass: In this scenario, the task is to navigate a point-mass to one of the four goals located diagonally to agent initial position. In all tasks, we also acquire primitive skills of the agent for our composite policy. For Ant, we use four basic policies for moving left, right, up, and down. The pusher uses two primitive policies that are to push an object to the left and down. In HalfCheetah hurdle environment, the low-level policies include jumping and running forward. Finally fot the point-mass robot, the composition model takes Published as a conference paper at ICLR four policies for moving in the up, down, left and right directions. Furthermore, in all environments, except pusher, the primitive policies were agnostic of high-level tasks ( such as target locations) that were therefore provided separately to our composite model via decoder network. This highlights the ability of our model to transfer basic robot skills to novel problems.

Section Title: COMPARATIVE STUDY
  COMPARATIVE STUDY In our comparative studies, we divide our test environments into two groups. The first group includes Pusher, Random Goal Ant, Ant Cross Maze, and HalfCheetah-Hurdle environments, whereas the second group comprises the remaining environments that require task and motion planning under weak reward signals. In the first group of settings, we compare our composite model trained with SAC ( Haarnoja et al., 2018b ) against the standard Gaussian policies obtained using SAC ( Haarnoja et al., 2018b ), PPO ( Schulman et al., 2017 ), and TRPO ( Schulman et al., 2015 ). We exclude HRL methods in these cases as the environment rewards sufficiently represent the underlying task, whereas HRL approaches are applicable in cases that have a weak reward signal or require task and motion planning.  Table 1  presents the mean and standard deviation of the agent's final distance from the given targets after the end of an evaluation rollout over the ten trials.  Fig. 3  shows the mean learning performance over all trials during the three million training steps. In these set of problems, TRPO and PPO entirely fail to reach the goal, and SAC performs reasonably well but only in simple Ant Random Goal and Pusher environments as it fails in other cases. Our composite policy obtained using SAC successfully solves all tasks and exhibit high data-efficiency by learning in merely a few thousand training steps. In our second group of environments, we use distance-based rewards that are weak signals as greed- ily following them does not lead to solving the problem. Furthermore, in these environments, poli- cies trained with standard RL, including our composite policy, failed to solve the problem even after 20 million training steps. Therefore, we trained our composite policy with HIRO ( Nachum et al., 2018 ) and compared its performance against standard HIRO formulation ( Nachum et al., 2018 ). We also tried to include option-critic framework ( Bacon et al., 2017 ), but we were unable to get any considerable results with their online implementation despite several attempts with the parameter tuning. One of the reasons option-critic fails is because it relies purely on task rewards to learn, which makes them inapplicable for cases with weak reward signals ( Nachum et al., 2018 ).  Nachum et al. (2018)  use a modified Ant in their paper that has 30 units joint torque limit (low- torque-Ant). For our composition method, we use Mujoco standard Ant that has a torque limit of 150 units which makes the learning even harder as the Ant is now more prone to instability than a low-torque-Ant. For standard HIRO formulation, we trained three variants i) HIRO trained on standard Mujoco Ant, ii) HIRO trained on low-torque Ant (as in ( Nachum et al., 2018 )); iii) HIRO pretrained for the equal number of training steps as used to train the primitive policies of our composition method on a low-torque-Ant (For more details, refer to Appendix C.1).  Fig. 4  shows the learning performance, averaged over ten trials, during 10 million steps. It can be seen that the composite policy with HIRO outperforms standard HIRO ( Nachum et al., 2018 ) by a significant margin. It also certifies the utility of solving RL tasks using composition by leveraging basic pre-acquired skills. Furthermore, HIRO performs poorly with standard Ant, even pretraining did not improve the performance, as it imposes a harder control problem.

Section Title: ABLATIVE STUDY
  ABLATIVE STUDY We remove bidirectional RNN (BRNN) (similar to ( Peng et al., 2019 )), attention-network, and both attention-network and BRNN (AttBRNN) from our composition model to highlight their importance in the proposed architecture in solving complex problems. We train all models with SAC ( Haarnoja et al., 2018b ). The first model is our composite policy without attention in which the decoder network takes the state information and last hidden states of the encoder (BRNN) to directly output actions rather than mixture weights. The second model is without attention network and BRNN, it is a feed-forward neural network that takes the state information and the primitive actions and predicts the action to interact with the environment. The third model is without BRNN, it is a feed- forward neural network that takes the state and goal information, and output the mixture weights. The mixture weights are then used to combine the actions from primitive policies. This setting is same as ( Peng et al., 2019 ) for their transfer learning problems.  Fig. 5  shows the mean performance comparison, over ten trials, of our composite model against its ablated versions on a Ant Random Goal, Cross Maze Ant, and Pusher environment. We exclude remaining test environments in this study as ablated models completely failed to perform or show any progress. Note that the better performance of our method compared to ablated versions highlight the mer- its of our architecture design. Intuitively, BRNN allows the dynamic encoding of a skill set that could be of variable lengths 2 . And our decoder network uses the encoded skills together with given state and goal information (states, goals, and primitive skills) in a structured way using at- tention framework and provides significantly better performance than other ablated models, includ- ing ( Peng et al., 2019 ).Furthermore, another merit of using the attention network is that it bypasses the complex transformation of action embeddings (composition-without-attention) or actions and state-information (composition-without-AttBRNN) directly to action space. Hence, the proposed architecture is crucial for the composition of task-agnostic sub-level policies to solve new problems.

Section Title: DEPICTION OF ATTENTION WEIGHTS
  DEPICTION OF ATTENTION WEIGHTS In order to further assess the merit of utilizing an attention network, we apply our model to a simple 2D multi-goal point-mass environment as shown in  Fig. 6 . The point-mass is initial- ized around the origin (with variance σ 2 = 0.1) and must randomly choose one of four goals to reach. For this experiment we use dense re- wards with both a positional and actuation cost. Primitive policies of up (+y), down (−y), left (−x), and right (+x) were trained and com- posed to reach goals, represented here as red dots, in the "diagonal" directions where a com- bination of two or more primitive policies are required to reach each goal. The four mappings in the figure give us insight into how the attention network is utilizing the given primitives to achieve the desired task. At each step in a given path, the weights {w i } N i=0 for each low-level policy are assigned and com- posed together to move the point-mass in the desired direction. We see here that even with some noise and short-term error, the attention weights are strongest for primitive policies that move the point-mass to its chosen goal. We also see that multiple policies are activated at once to achieve more direct movements toward the goal, as op- posed to "stair-stepping" where only one primitive is activated at a time. Both of these observations point to the concurrent and sequential nature of this composition model. Please refer to Appendix D for depiction of attention weights in other complicated environments.

Section Title: CONCLUSIONS AND FUTURE WORK
  CONCLUSIONS AND FUTURE WORK We present a novel policy ensemble composition method that combines a set of independent and task-agnostic primitive policies through reinforcement learning to solve the given tasks. We show that our method can transfer the given skills to novel problems and can compose them both sequen- tially (or -operation) and concurrently (and -operation) to find a solution for the task in hand. Our experiments highlight that composition is vital for solving problems requiring complex motion skills and decision-making where standard reinforcement learning and hierarchical reinforcement learn- ing methods either fail or need a massive number of interactive experiences to achieve the desired results. In our future work, we plan to extend our method to automatically acquire the missing skills in the given skillset that are necessary to solve the specific problems (refer to Appendix B for prelimi- 2 However, in this paper we only consider a primitive skill set of fixed size Published as a conference paper at ICLR 2020 nary results). We also aim towards a system that learns the hierarchies of composition models by combining primitive policies into complex policies that would further be composed together for a combinatorial outburst in the agent's skillset.

```
