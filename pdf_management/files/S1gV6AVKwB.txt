Title:
```
Under review as a conference paper at ICLR 2020 CROSS DOMAIN IMITATION LEARNING
```
Abstract:
```
We study the question of how to imitate tasks across domains with discrepancies such as embodiment and viewpoint mismatch. Many prior works require paired, aligned demonstrations and an additional RL procedure for the task. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Cross Domain Imitation Learning (CDIL) problem, which encom- passes imitation learning in the presence of viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsu- pervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from unpaired, unaligned demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when CDIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in both embodiment and viewpoint mismatch scenarios where aligned demonstrations don't exist and show the effectiveness of our approach.
```

Figures/Tables Captions:
```
Figure 1: (a). Illustration of paired, aligned vs unpaired, unaligned demonstrations in the alignment task set D x,y (b). Alignment: we learn state, action maps f, g between the self (x) and expert (y) domain from unpaired, unaligned demonstrations by minimizing a distribution matching loss and an imitation loss. (c) Adaptation: adapt the expert domain policy ⇡ y,T or demonstrations to obtain a self domain policy⇡ x,T a simple training algorithm to learn MDP reductions. In section 6, we experimentally evaluate GAMA and find that meaningful state correspondences between various domains are learned from unpaired, unaligned demonstrations. We then compare the CDIL performance of GAMA against several baselines in both embodiment and viewpoint mismatch scenarios and show the effectiveness of our approach.
Figure 2: Example MDP reduction from M x to M y . , are state and action maps
Figure 3: Illustration of MDP alignment problem
Figure 4: Visualization of the learned state maps for pen$pen (Top Left), pen$cart (Top Right), snake4$snake3 (Bottom Left), reach2$reach3 (Bottom Right). GAMA is able to recover MDP reduc- tions (Top Left/Right) and finds interpretable correspondences between domains that are not perfectly alignable, yet intuitively share structure (Bottom Left/Right). Baselines fail in most cases
Figure 5: CDIL performance. Alignment complexity (Left), Adaptation complexity (Middle), and transferability (Right) for W2C/R2W on the top/bottom rows, respectively. GAMA outperforms baselines in all metrics. Notably, adaptation complexity of GAMA is close to that of the self-demo baseline. Error bars/regions show the standard deviation over 5 runs.
Table 1: Quantitative evaluation of learned state maps. GAMA reliably finds MDP permutations while baselines incur 10⇥ larger deviation loss from the ground truth permutation map. Error bars/regions show the standard deviation over 5 runs.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Humans possess an astonishing ability to recognize latent structural similarities between behaviors in related but distinct domains, and learn new skills from cross domain demonstrations alone. Not only are we capable of learning from third person observations that have no obvious correspondence to our internal self representations ( Stadie et al., 2017 ;  Liu et al., 2018 ;  Sermanet et al., 2018 ), but we also are capable of imitating agents with different embodiments ( Gupta et al., 2017 ;  Rizzolatti & Craighero, 2004 ) as can be observed in an infant's learning of visuomotor skills from adults with different biomechanics and physical capabilities ( Jones, 2009 ). Previous work in neuroscience ( Marshall & Meltzoff, 2015 ) and robotics ( Kuniyoshi & Inoue, 1993 ;  Kuniyoshi et al., 1994 ) have recognized the pitfalls of exact behavioral cloning in the presence of domain discrepancies and posited that the effectiveness of the human imitation learning mechanism hinges, crucially, on the capability to learn structure preserving domain correspondences. These correspondences enable the learner to internalize the expert demonstrations and produce a reconstruction of the behavior in the self domain. Consider a young child that has learned to associate his internal body map with the limbs of an adult. When the adult demonstrates running, the child is able to imagine himself running, and reproduce the behavior. Recently, separate solutions have been proposed for imitation learning across two kinds of domain discrepancies: embodiment ( Gupta et al., 2017 ) and viewpoint ( Liu et al., 2018 ;  Sermanet et al., 2018 ) mismatch. These works ( Liu et al., 2018 ;  Sermanet et al., 2018 ;  Gupta et al., 2017 ) require paired, time-aligned demonstrations to obtain state correspondences and an extra RL step with a proxy reward. However, paired, aligned demonstrations are seldom obtainable and RL loops are expensive. In this work we formalize the Cross Domain Imitation Learning (CDIL) problem which encompasses prior work in imitation learning across domains with viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally in a self domain, given demonstrations of the task in a distinct expert domain. We propose a two-step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state, action maps from unpaired, unaligned demonstrations. In the adaptation step we leverage the learned state, action maps to zero-shot imitate tasks across domains without an additional RL step. To shed light on when CDIL can be solved by alignment and adaptation, we first introduce a class of structure preserving maps, called MDP reductions, that adapts optimal policies between MDPs (section 3). We further characterize a family of MDP pairs that share reductions, formally state the MDP alignment problem, and elucidate its connection to CDIL. In section 4, 5 we derive GAMA, Under review as a conference paper at ICLR 2020

Section Title: CROSS DOMAIN IMITATION LEARNING PROBLEM STATEMENT
  CROSS DOMAIN IMITATION LEARNING PROBLEM STATEMENT An infinite horizon Markov Decision Process (MDP) M 2 ⌦ with deterministic dynamics is a tuple (S, A, P, ⌘, R) where ⌦ is the set of all MDPs, S is the state space, A is the action space, P : S ⇥ A ! S is a (deterministic) transition function, R : S ⇥ A ! R is the reward function, and ⌘ is the initial state distribution. A domain is an MDP without the reward, i.e (S, A, P, ⌘). Intuitively, a domain fully characterizes the embodied agent and the environment dynamics, but not the desired behavior. A task T is a label for an MDP corresponding to the high level description of optimal behavior, such as "walking". T is analogous to category labels for images. An MDP with domain x for task T is denoted by M x,T = (S x , A x , P x , ⌘ x , R x,T ), where R x,T is a reward function encapsulating the behavior labeled by T . For example, different reward functions are needed to realize the "walking" behavior in two morphologically different humanoids. A (stationary) policy for M x,T is a map ⇡ x,T : S x ! B(A x ) where B is the set of probability measures on A x and an optimal policy ⇡ ⇤ x,T = arg max ⇡x J(⇡ x ) achieves the highest policy performance J(⇡ x ) = E ⇡x [ P 1 t=0 t R x,T (s (t) x , a (t) x )] where 0 < < 1 is a discount factor. A demonstration of length H is a sequence of state, action tuples ⌧ Mx ,T = {(s (t) x , a (t) x )} H t=1 sampled from an optimal policy and D Mx ,T = {⌧ (k) Mx ,T } K k=1 is a set of demonstrations for M x,T Let M x,T , M y ,T be self and expert MDPs for a target task T . Given expert domain demonstrations D My ,T , Cross Domain Imitation Learning (CDIL) aims to determine an optimal self domain policy ⇡ ⇤ x,T without access to the reward function R x,T . In this work we propose to first solve an MDP alignment problem and then leverage the alignments to zero-shot imitate expert domain demonstrations. Like prior work ( Gupta et al., 2017 ;  Liu et al., 2018 ;  Sermanet et al., 2018 ), we assume the availability of an alignment task set D x,y = {(D Mx ,T i , D My ,T i )} N i=1 containing demonstrations for N tasks {T i } N i=1 from both the self and expert domain. D x,y could, for example, contain both robot (x) and human (y) demonstrations for a set primitive tasks such as walking, running, and jumping. Unlike prior work, demonstrations are unpaired and unaligned, i.e (s (t) x , s (t) y ) may not be a valid state correspondence. (see Figure 1(a)) Paired, time-aligned cross domain data is expensive and may not even exist when task execution rates differ or there exists systematic embodiment mismatch between the domains. For example, a child can imitate an adult running, but not achieve the same speed. Our set up emulates a natural setting in which humans compare how they perform tasks to how other agents perform the same tasks in order to find structural similarities and identify domain correspondences. We now proceed to introduce a theoretical framework that explains how and when the CDIL problem can be solved by MDP alignment followed by adaptation.

Section Title: ALIGNABLE MDPS
  ALIGNABLE MDPS Let ⇧ ⇤ M be the set of all optimal policies for MDP M. We define an occupancy measure ( Syed et al., 2008 ) Definition 1. An optimality function O Mx : S x ⇥ A x ! {0, 1} for an MDP M x satisfies: In words, Eq. 1 states that only optimal state, action pairs in x can be mapped to optimal state, action pairs in y and Eq. 2 states that r must be surjective on the set of optimal state, action pairs in y. Eq. 3 states that a reduction must preserve (deterministic) dynamics. We use the notation M x , M y to denote that ( , ) is a reduction from M x to M y , and the shorthand M x M y to denote that M x reduces to M y . To gain an intuitive understanding of MDP reductions, picture the execution trace of an optimal policy as a directed graph with colored edges in which the nodes correspond to states visited by an optimal policy, and the colored edges correspond to actions taken. An MDP reduction from M x to M y homomorphs the execution graph of an optimal policy in M x to a execution graph of an optimal policy in M y .  Figure 2  shows an example of a valid reduction from M x to M y : states 1, 2 in S x are mapped (merged) to state a in S y and the blue, green actions in A x are mapped to the brown action in A y . Intuitively, if M x , M y , then ( , ) compresses M x by merging all optimal state, action pairs that have identical dynamics properties. Definition 3. Two MDPs M x , M y are alignable if and only if M x M y or M y M x . Definition 3 states that MDPs are alignable if reductions exists between them, meaning that they share structure. We use (M x , M y ) = {( , )|M x , M y } to denote the set of all valid reductions from M x to M y . Reductions have a particularly useful property which is that they adapt policies across alignable MDPs. Consider a state map f : S x ! S y , an inverse action map g : A y ! A x , and a composite policy⇡ x = g ⇡ y f (see Figure 1(b)). In words,⇡ x maps a self state to an expert state via f , simulates the expert's action choice for the mapped state via ⇡ y , then chooses a self action that corresponds to the simulated expert action with g. The following lemma holds for⇡ x . Lemma 1 states that the state, action maps (f, g 1 ) chosen to be a reduction can adapt optimal policies between alignable MDPs. Here onwards we interchangeably refer to (f, g) as "alignments". We now show how the CDIL problem can be solved by first solving an MDP alignment problem followed by an adaptation step. In words, two MDP pairs are joint alignable if there exists a shared reduction. We define an equiva- lence class [(M x , M y )] ⇠ = {(M x 0 , M y 0 ) | (M x 0 , M y 0 ) ⇠ (M x , M y )} of MDP pairs that share reductions. Overloading notation, ({(M x i , M y i )} N i=1 ) = {( , ) | ( , ) 2 (M x 1 , M x 1 ) \ · · · \ (M x N , M x N )}. We now formally state the MDP alignment problem: Let (M x,T , M y ,T ) be an MDP pair for a target task T . Given an alignment task set We can then use ( , ) for CDIL: given cross domain demonstrations D My ,T for the target task T , learn an expert domain policy ⇡ y,T , and adapt it into the self domain using ( , ) according to Lemma 1. We can now assess when domains with embodiment and viewpoint mismatch have meaningful state correspondences, i.e MDP reductions, thus allowing for cross domain imitation. The states of a human expert with more degrees of freedom than a robot imitator can be merged into the robot states if the task only requires the robot's degrees of freedom and the execution traces share structure, e.g traces are both cycles. However, if the task requires all degrees of freedom possessed only by the human, the robot cannot find meaningful correspondences, and also cannot imitate the task. Two MDPs for different viewpoints of an agent performing a task are MDP permutations since there is a one-to-one correspondence between state, actions at same timestep in the execution trace of an optimal policy.

Section Title: LEARNING MDP REDUCTIONS
  LEARNING MDP REDUCTIONS We now derive objectives that can be optimized to learn MDP reductions. We propose distribution matching and policy performance maximization. We first define the distributions to be matched. Definition 5. Let M x , M y be two MDPs and⇡ x = g ⇡ y f for f : S x ! S y , g : A y ! A x and policy ⇡ y . P = {ŝ (t) y ,â (t) y } t 0 is the co-domain policy execution process realized by running⇡ x , i.e: The target distribution y ⇡y is over transitions uniformly sampled from execution traces of ⇡ y and the proxy distribution x!ŷ ⇡x is over cross domain transitions uniformly sampled from realizations of P. We now propose three concrete objectives: 1.⇡ x is optimal, 2. x!ŷ ⇡x = y ⇡y , 3. g is injective. In other words, we seek to learn f, g that matches distributions over transition tuples in domain y while maximizing policy performance in domain x. The former captures the dynamics preservation property from Eq. 3 and the latter captures the optimal policy preservation property from Eq. 1, 2. The following theorem uncovers the connection between our objectives and MDP reductions. Theorem 1. Let M x , M y be MDPs satisfying Assumption 1 (see Supp Materials). If M x M y , then 9f : S x ! S y , g : A y ! A x , and an optimal covering policy ⇡ y (Supp Materials, Def 6) that satisfy objectives 1, 2. Conversely, if 9f : S x ! S y , g : A y ! A x and an optimal covering policy ⇡ y satisfying objectives 1, 2, 3, then M x M y and 9( , ) 2 (M x , M y ) s.t f = and g(a y ) = a y , 8a y 2 A y . Theorem 1 states that if two MDP are alignable, then objectives 1, 2 can be satisfied. Conversely, if objectives 1, 2, 3 can be satisfied for two MDPs, then they must be alignable and all solutions (f, g) are MDP reductions. While Theorem 1 requires alignable MDPs to guarantee identifiability, our experiments will also run on MDPs that are not perfectly alignable, i.e. Eq. 1, 2, 3 do not hold exactly, but intuitively share structure. In the next section, we propose a simple algorithm to learn MDP reductions.

Section Title: GENERATIVE ADVERSARIAL MDP ALIGNMENT
  GENERATIVE ADVERSARIAL MDP ALIGNMENT Building on Theorem 1, we propose the following general form training objective for aligning MDPs: min f,g J(⇡ x ) + d( x!ŷ ⇡x , y ⇡y ) (7) where J(⇡ x ) is the performance of⇡ x , d is a distance metric between distributions, and > 0 is a Lagrange multiplier. In practice, we found that injectivity of g is unnecessary to enforce in continuous domains. We now present an instantiation of this framework: Generative Adversarial MDP Alignment (GAMA). Recall that we are given an alignment task set D x,y = {(D Mx ,T i , D My ,T i )} N i=1 . In the alignment step, we learn ⇡ ⇤ y,Ti , 8T i and parameterized state, action maps f ✓ f : S x ! S y , g ✓g : A y ! A x that compose⇡ x,Ti = g ✓g ⇡ ⇤ y,Ti f ✓ f . To match x!ŷ ⇡x , y ⇡y , we employ adversarial training (Goodfellow Under review as a conference paper at ICLR 2020 et al., 2014) in which separate discriminators D ✓ i D per task are trained to distinguish between "real" transitions (s y , a y , s 0 y ) ⇠ ⇡ ⇤ y,Ti and "fake" transitions (ŝ y ,â y ,ŝ 0 y ) ⇠⇡ x,Ti , whereŝ y = f ✓ f (s x ),â y = ⇡ y (ŝ y ),ŝ 0 y = f ✓ f (P x ✓ P (s x , g(â y ))), and P x ✓ P is a fitted model of the x domain dynamics. (see Figure 1(b)) The generator, consisting of f ✓ f , g ✓g , is trained to fool the discriminator while maximizing policy performance. The distribution matching gradients are back propagated through the learned dynamics, ⇡ ⇤ y,Ti is learned by Imitation Learing (IL) on D My ,T i , and the policy performance objective on⇡ x,Ti is achieved by IL on D Mx ,T i . In this work we use behavioral cloning ( Pomerleau, 1991 ) for IL. We thus seek to find a saddle point {f, g} [ {D ✓ i D } N i=1 of the following objective: where D KL is the KL-divergence. We provide the full execution flow of GAMA in Algorithm 1 In the adaptation step, we are given expert demonstrations D My ,T of a new target task T , from which we fit an expert domain policy ⇡ y,T which are composed with the learned alignments to construct an adapted self policy⇡ x,T = g ✓g ⇡ y,T f ✓ f . We also experiment with a demonstration adaptation method which additionally trains an inverse state map f 1 : S y ! S x , adapts demonstrations D My ,T into the self domain via f 1 , g, and applies behavioral cloning on the adapted demonstrations. (see Figure 1(c)) Notably, our entire procedure does not require paired, aligned demonstrations nor an RL step.

Section Title: Algorithm 1: Generative Adversarial MDP Alignment (GAMA)
  Algorithm 1: Generative Adversarial MDP Alignment (GAMA) Related Works: Closely related to CDIL, the field of cross domain transfer learning in the context of RL has explored approaches to use state maps to exploit cross domain demonstrations in a pretraining procedure for a new target task for which self domain reward function is available. Canonical Corre- lation Analysis (CCA) (Hotelling, 1936) finds invertible projections into a basis in which data from different domains are maximally correlated. These projections can then be composed to obtain a direct correspondence map between states.  Ammar et al. (2015) ;  Joshi & Chowdhary (2018)  have utilized an unsupervised manifold alignment (UMA) algorithm which finds a linear map between states with similar local geometric properties. UMA assumes the existence of hand crafted features along with a distance metric between them. This family of work commonly uses a linear statemap to define a time-step wise transfer reward and executes an RL step on the new task. Similar to our work, these works use an alignment task set of unpaired, unaligned trajectories to compute the state map. Unlike these works, we learn maps that preserve MDP structure, use deep neural network state, action maps, and achieve zero-shot transfer to the new task without an RL step. More recent work in transfer learning across embodiment ( Gupta et al., 2017 ) and viewpoint ( Liu et al., 2018 ;  Sermanet et al., 2018 ) mismatch obtain state correspondences from an alignment task set comprising paired, time-aligned demonstrations and use them to learn a state map or a state encoder to a domain invariant feature space. In contrast to this family of prior work, our approach learns both state, action maps from unpaired, unaligned demonstrations. Also, we remove the need for additional environment interactions and an expensive RL procedure on the target task by leveraging the action map for zero-shot imitation.  Stadie et al. (2017)  have shown promise in using domain confusion loss and generative adversarial imitation learning ( Ho & Ermon, 2016 ) for learning across small viewpoint mismatch without an alignment task set, but fails in dealing with large viewpoint differences. Unlike  Stadie et al. (2017) , we leverage the alignment task set to succeed in imitating across Under review as a conference paper at ICLR 2020 larger viewpoint mismatch and do not require an RL procedure. MDP homomorphisms ( Ravindran & Barto, 2002 ) have been explored with the aim of compressing state, action spaces to facilitate planning. In similar vein, related works have proposed MDP similarity metrics based on bisimulation methods (Ferns et al., 2004) and Boltzmann machine reconstruction error ( Ammar et al., 2014 ). While conceptually related to our MDP alignability theory, these works have not proposed scalable procedures to discover the homomorphisms and have not drawn connections to cross domain learning.

Section Title: EXPERIMENTS
  EXPERIMENTS Ours experiments were designed to answer the following questions: (1). Can GAMA uncover MDP reductions? (2). Can the learned alignments (f ✓ f , g ✓g ) be leveraged to succeed at CDIL? Note that we include experiments with MDP pairs that are not perfectly alignable, yet intuitively share structure, to show general applicability of GAMA for CDIL. We propose three metrics to evaluate the effectiveness of GAMA. First, alignment complexity which is the number of MDP pairs, i.e number of tasks, in the alignment task set needed to learn alignments that enable zero-shot imitation, given ample cross domain demonstrations for the target tasks. Second, adaptation complexity which is the amount of cross domain demonstrations for the target tasks needed to successfully imitate tasks in the self domain without querying the target task reward function, given a sufficiently large alignment task set. Finally, transferability, which is the environment sample complexity on the target task when using the alignment procedure as weight initialization then running RL with the target task reward function. While we aim to learn optimal self policies without querying the self domain reward function, this metric measures the usefulness of the alignment step even when MDP pairs in the alignment task set are not in the equivalence class of the target MDP pair. We study two ablations of GAMA and compare against the following baselines: GAMA - Policy Adapt (GAMA-PA): learns alignments by Algorithm 1, fits an expert policy ⇡ y,T to D My ,T for a new target task T and zero-shot adapts ⇡ y,T to the self domain via⇡ x,T = g ✓g ⇡ y,T f ✓ f . GAMA - Demonstration Adapt (GAMA-DA): trains f 1 in addition to Algorithm 1, adapts D My ,T into the self domain via (f 1 , g), and fits a self domain policy on the adapted demonstrations. Self Demonstrations (Self-Demo): We behavioral clone on self domain demonstrations for the target task. This baseline provides an "upper bound" on the adapation complexity of CDIL. Canonical Correlation Analysis (Hotelling, 1936) (CCA): finds invertible matrices C x , C y to a basis where domain data are maximally correlated from unpaired, unaligned demonstrations. Unsupervised Manifold Alignment ( Ammar et al., 2015 ) (UMA): finds a map between states that have similar local geometries from unpaired, unaligned demonstrations. Invariant Features ( Gupta et al., 2017 ) (IF): finds invertible projections onto a feature space given state pairings. Dynamic Time Warping ( Muller, 2007 ) is used to obtain the pairings. Imitation from Observation ( Liu et al., 2018 ) (IfO): learns a statemap conditioned on a cross domain observation given state pairings. Dynamic Time Warping ( Muller, 2007 ) is used to obtain the pairings. Third Person Imitation Learning ( Stadie et al., 2017 ) (TPIL): simultaneously learns a domain agnostic feature space and matches distributions in the feature space via GAIL ( Ho & Ermon, 2016 ). We experiment with environments which are extensions of OpenAI Gym ( Brockman et al., 2016 ). pen, cart, reacher2, reacher3, reach2-tp, snake3, and snake4 denotes the pendulum, cartpole, 2-link reacher, 3-link reacher, third person 2-link reacher, 3-link snake, and 4-link snake environments, respectively. (self domain) $ (expert domain) specify an MDP pair in the alignment task set. Model architectures and environment details are further described in the Supp. Materials, section B, C, D.

Section Title: MDP ALIGNMENT EVALUATION
  MDP ALIGNMENT EVALUATION   Figure 4  visualizes the learned state map f ✓ f for several MDP pairs. The pen $ pen alignment task ( Figure 4 , Top Left) and reach$reach-tp task task exemplify scenarios where two MDPs are permutations of each other. Similarly, the pen $ cart alignment task ( Figure 4 , Top Right) has a reduction that maps the pendulum's angle and angular velocity to those of the pole, as the cart's position and velocity are redundant state dimensions once an optimal policy has been learned.  Table 1  presents quantitative evaluations of these simple alignment maps. For pen$pen and reach2$reach2-tp we record the average L2 loss between the learned statemap's outputs and the ground truth permutation map's outputs. As for pen$cart, we do Under review as a conference paper at ICLR 2020 the same on the dimensions that correspond to the angle and angular velocity of the pole. We see from both  Figure 4  and  Table 1  that GAMA is able to learn simple reductions while baselines fail to do so. The key reason behind this performance gap is that most baselines ( Gupta et al., 2017 ;  Liu et al., 2018 ) obtain state maps from time-aligned demonstration data. However, the considered alignment task set contains unaligned demonstrations with diverse starting states, up to 2x differences in demonstration lengths, and varying task execution rates. We see that GAMA also outperforms baselines that learn from unaligned demonstrations (Hotelling, 1936;  Ammar et al., 2015 ) by learning maps that preserve MDP structure with more flexible neural network function approximators. For pen$cart, UMA learns a statemap that outputs out-of-bounds coordinates mainly because the pendulum demonstrations are concentrated around the pole upright state. The optimal UMA embedding matrix in this case is a zero matrix. Then the UMA state map matrix norm is proportional to the inverse embedding matrix norm which is very large. For snake4 $ snake3 and reach2 $ reach3, the MDPs may not be perfectly alignable, yet intuitively share structure. From  Figure 4  (Bottom Left) we see that GAMA identically matches two adjacent joint angles of snake4 to the two joint angles of snake3 and the periodicity of the snake's wiggle is preserved. On reacher2$reacher3, we find that the central pivot angles are matched and further find correspondences between states that have similar extents of contraction.

Section Title: CDIL PERFORMANCE
  CDIL PERFORMANCE Wall2Corner (W2C): The self domain is reacher2 and the expert domain is reacher3. We use the robot's internal state, action representation. The alignment tasks are reaching for 12 goals near the room wall centers and the target tasks are reaching for 12 new goals at the room corners, maximally away from the wall goals. The significant difference between training and test goals makes generalization challenging. Reach2Write (R2W): The self domain is reacher2 and the expert domain is reacher2-tp that has a "third person" state space with a 180 camera angle offset. We use the robot's internal state, action representation. The alignment tasks are reaching for goals and the transfer task is writing letters as fast as possible. The transfer task differs from the alignment tasks in two key aspects: the end effector must draw a straight line from a letter's vertex to vertex and not slow down at the vertices in order to trace the letters fast. Alignment complexity is shown in  Figure 5  (Left). GAMA is able to learn alignments that enable zero-shot imitation on the target task, showing clear gains over a simple pretraining procedure on the self domain MDPs in the alignment task set. Other baselines require an additional RL step and cannot achieve zero-shot imitation.  Figure 5  (Middle) shows the adaptation complexity. Notably, GAMA-DA (blue, dashed) produces adapted demonstrations of similar usefulness as self demonstrations (olive green). Other baselines fail to learn useful alignments from unpaired, unaligned demonstrations and as a result fails at CDIL. Finally,  Figure 5  (Right) shows that the alignment step is useful as weight initialization to accelerate learning of the target task. GAMA (blue) attains optimal performance around 7⇥ faster than all baselines in the W2C experiment, while immediately attaining optimal performance on the R2W task. Baselines fail to learn the writing task as an inaccurate proxy reward function harms performance.

Section Title: CDIL WITH VISUAL INPUTS
  CDIL WITH VISUAL INPUTS The non-visual environment experiments in the previous section demonstrate the limitations of the time-alignment assumptions made in prior work without confounding variables such as the difficulty optimization in high-dimensional space. In this section, we also demonstrate that GAMA scales to higher dimensional, visual environments with 64 ⇥ 64 ⇥ 3 image inputs on the W2C and R2W experiments. Specifically, we train a deep spatial autoencoder on the alignment task set to learn an encoder with the architecture from  Levine et al. (2016) , then apply GAMA on the (learned) latent space. Comparing the dark blue (image input) and light blue curves (internal state input) in  Figure 5 , we see that the adaptation complexity and alignment complexity of GAMA-DA-img, GAMA-PA-img are both similar to that of GAMA-DA, GAMA-PA and better than baselines trained with the robot's internal representation.

Section Title: DISCUSSION AND FUTURE WORK
  DISCUSSION AND FUTURE WORK We've formalized Cross Domain Imitation Learning which encompasses prior work in transfer learning across embodiment ( Gupta et al., 2017 ) and viewpoint differences ( Stadie et al., 2017 ;  Liu et al., 2018 ) along with a practical algorithm that can be applied to both scenarios. We now point out directions future Under review as a conference paper at ICLR 2020 work. Our MDP alignability theory is a first step towards formalizing possible shared structures that enable cross domain imitation. While we've shown that GAMA empirically works well even when MDPs are not perfectly alignable, upcoming works may explore relaxing the conditions for MDP alignability to develop a theory that covers an even wider range of real world MDPs. Future works may also try applying GAMA in the imitation from observations scenario, i.e actions are not available, by aligning observations with GAMA and applying methods from  Sermanet et al. (2018) ;  Liu et al. (2018) . Finally, we hope to see future works develop principled ways design a minimal alignment task set, which is analogous to designing a minimal training set for supervised learning. Under review as a conference paper at ICLR 2020

```
