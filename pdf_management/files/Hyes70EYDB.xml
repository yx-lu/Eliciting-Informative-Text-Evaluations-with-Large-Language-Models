<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 PROPER NETWORK INTERPRETABILITY HELPS AD- VERSARIAL ROBUSTNESS IN CLASSIFICATION</article-title></title-group><abstract><p>Recent works have empirically shown that there exist adversarial examples that can be hidden from neural network interpretability (namely, making network interpretation maps visually similar), and interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust in- terpretation (without resorting to adversarial loss minimization). We show that our defense achieves high classification robustness, outperforming state-of-the-art adversarial training methods against attacks of large perturbation while attaining high interpretation robustness under various settings of adversarial attacks.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>It has become widely known that convolutional neural networks (CNNs) are vulnerable to adver- sarial examples, namely, perturbed inputs with intention to mislead networks' prediction (Szegedy et al., 2014; Goodfellow et al., 2015; Papernot et al., 2016a; Carlini &amp; Wagner, 2017; Chen et al., 2018; Su et al., 2018). The vulnerability of CNNs has spurred extensive research on adversarial attack and defense. To design adversarial attacks, most work has focused on creating either im- perceptible input perturbations (Goodfellow et al., 2015; Papernot et al., 2016a; Carlini &amp; Wagner, 2017; Chen et al., 2018) or adversarial patches robust to the physical environment (Eykholt et al., 2018; Brown et al., 2017; Athalye et al., 2017). Many defense methods have also been developed to prevent CNNs from misclassification when facing adversarial attacks. Examples include defensive distillation (Papernot et al., 2016b), training with adversarial examples (Goodfellow et al., 2015), in- put gradient or curvature regularization (Ross &amp; Doshi-Velez, 2018; Moosavi-Dezfooli et al., 2019), adversarial training via robust optimization (Madry et al., 2018), and TRADES to trade adversarial robustness off against accuracy (Zhang et al., 2019). Besides studying adversarial effects on network prediction decisions, this work explores the connection between adversarial robustness and network interpretability, and provides novel insights on when and how interpretability helps the robustness.</p><p>Having a prediction might not be enough for many real-world machine learning applications. It is also crucial to demystify why CNNs make certain decisions. Thus, the problem of network interpre- tation arises. Various methods have been proposed to understand the mechanism of decision making by CNNs. One category of methods justify a prediction decision by assigning importance values to reflect the influence of individual pixels or image sub-regions on the final classification. Exam- ples include pixel-space sensitivity map methods (Simonyan et al., 2013; Zeiler &amp; Fergus, 2014; Springenberg et al., 2014; Smilkov et al., 2017; Sundararajan et al., 2017) and class-discriminative localization methods (Zhou et al., 2016; Selvaraju et al., 2017; Chattopadhay et al., 2018; Petsiuk et al., 2018), where the former evaluates the sensitivity of a network classification decision to pixel variations at the input, and the latter localizes which parts of an input image were looked at by the network for making a classification decision. Sensitivity map methods include vanilla gradient (Si- monyan et al., 2013), deconvolution (Zeiler &amp; Fergus, 2014), guided backpropagation (Springenberg et al., 2014), SmoothGrad (Smilkov et al., 2017), integrated gradient (IG) (Sundararajan et al., 2017) to name a few. They highlight fine-grained details in the image but are not class-discriminative for visual explanation. By contrast, localization approaches like class activation map (CAM) (Zhou et al., 2016), GradCAM (Selvaraju et al., 2017), GradCAM++ (Chattopadhay et al., 2018) and RISE (Petsiuk et al., 2018) are highly class-discriminative, namely, localizing image sub-regions reasoned Under review as a conference paper at ICLR 2020 for a prediction class. We refer readers to Sec. 2 for some representative interpretation methods. Besides interpreting CNNs via feature importance maps, some methods zoom into the internal re- sponse of neural networks. Examples include network dissection (Bau et al., 2017), which evaluates the alignment between individual hidden units and semantic concepts, and learning perceptually- aligned representations from robust training (Engstrom et al., 2019).</p><p>Very recently, some works (Xu et al., 2019b;a; Zhang et al., 2018; Subramanya et al., 2018; Ghor- bani et al., 2019; Dombrowski et al., 2019; Chen et al., 2019) are beginning to study adversarial robustness by exploring the spectrum between classification accuracy and network interpretability. It was shown in (Xu et al., 2019b;a) that an imperceptible adversarial perturbation to fool classi- fiers can lead to a significant change in a class-specific network interpretability map, e.g., CAM. Thus, it was argued that such an interpretability discrepancy can be used as a helpful metric to differentiate adversarial examples from benign inputs. However, the work (Zhang et al., 2018; Sub- ramanya et al., 2018) showed that under certain conditions, generating an attack (which we call interpretability sneaking attack, ISA) that fools the classifier as well as its coupled interpreter (in terms of keeping interpretability map highly similar to that of benign input) is not significantly more difficult than generating adversarial inputs deceiving the classifier only. Besides investigating ro- bustness in classification through the lens of interpretability, the work (Ghorbani et al., 2019; Dombrowski et al., 2019) studied the robustness in network interpretation maps, showing that which can significantly be manipulated via imperceptible input perturbations but keeping the classifier's decision intact. We call this type of threat model attack against interpretability (AAI). The existing work had no agreement on the relationship between robustness in interpretation and robustness in classification. Spurred by that, we attempt to explore this relationship from both attack and defense perspectives.</p><p>The most relevant work to ours is (Chen et al., 2019), which robustified network interpretation with the aid of integrated gradient (IG), an axiomatic attribution map. It proposed a robust attribution training, which was shown as a principled generalization of previous formulations of robust clas- sification and an effective defense against AAI. In this paper, we first investigate when ISA is possible, and then relate our insights on ISA to robust classification and robust interpretabil- ity. Different from the previous work, our paper contains the following contributions.</p><p>1. We provide an answer to the question of when adversarial examples can bypass in- terpretability discrepancy. We show that enforcing stealthiness of adversarial examples from network interpretation could be challenging. Its difficulty relies on how one measures the interpretability discrepancy caused by input perturbations.</p><p>2. We propose an 1 norm based 2-class interpretability discrepancy measure and theoretically show that constraining it helps adversarial robustness.</p><p>3. We develop an interpretability-aware robust training method and empirically show that interpretability alone can be used to defend adversarial attacks for both misclassifcation and misinterpretation. Compared to the IG-based robust attribution training (Chen et al., 2019), our approach is simpler in implementation, and provides better robustness even as facing a strong adversary.</p></sec><sec><title>PRELIMINARIES AND MOTIVATION: INTERPRETABILITY OF CNNS FOR JUSTIFYING A CLASSIFICATION DECISION</title><p>To explain what and why CNNs predict, we consider two types of network interpretation methods: CAM (Zhou et al., 2016) produces a class-discriminative localization map for CNNs, which performs global averaging pooling over convolutional feature maps prior to the softmax. Let the penultimate layer output K feature maps, each of which is denoted by a vector Under review as a conference paper at ICLR 2020 representation A k &#8712; R u for channel k &#8712; [K]. Here [K] represents the integer set {1, 2, . . . , K}. The ith entry of CAM L CAM (x, c) is given by [LCAM(x, c)]i = 1 u k&#8712;[K] w c k A k,i , i &#8712; [u], (1) where w c k is the linear classification weight that associates the channel k with the class c, and A k,i denotes the ith element of A k . The rationale behind (1) is that the classification score f c (x) can be written as the average of CAM values (Zhou et al., 2016), f c (x) = u i=1 [L CAM (x, c)] i . For visual explanation, L CAM (x, c) is often up-sampled to the input dimension d using bi-linear interpolation. GradCAM (Selvaraju et al., 2017) generalizes CAM for CNNs without the architecture 'global av- erage pooling &#8594; softmax layer' over the final convolutional maps. Specifically, the weight w c k in (1) is given by the gradient of the classification score f c (x) with respect to (w.r.t.) the feature map A k , w c k = 1 u u i=1 &#8706;fc(x) &#8706;A k,i . GradCAM++ (Chattopadhay et al., 2018), a generalized formulation of GradCAM, utilizes a more involved weighted average of the (positive) pixel-wise gradients but provides a better localization map if an image contains multiple occurrences of the same class. In this work, we focus on CAM since it is computationally light and our models used in experiments follow the architecture 'global average pooling &#8594; softmax layer'.</p></sec><sec><title>PSM-type methods</title><p>PSM uses calculations with gradients to assign importance scores to individ- ual pixels toward explaining the classification decision about an input. Examples of commonly-used approaches include vanilla gradient (Simonyan et al., 2013), guided backpropogation (Springenberg et al., 2014), SmoothGrad (Smilkov et al., 2017), and integrated gradient (IG) (Sundararajan et al., 2017). In particular, IG satisfies the completeness attribution axiom that PSM ought to obey. Specif- ically, it averages gradient saliency maps for interpolations between an input x and a baseline image a:</p><p>where m is the number of steps in the Riemman approximation of the integral. The completeness axiom (Sundararajan et al., 2017, Proposition 1) states that d i=1 [L IG (x, c)] i = f c (x) &#8722; f c (a), where the baseline image a is often chosen such that f c (a) &#8776; 0, e.g., the black image. Note that CAM also satisfies the completeness axiom. PSM is able to highlight fine-grained details in the image, but is computationally intensive and not very class-discriminative compared to CAM (Selvaraju et al., 2017).</p></sec><sec><title>Interpretability discrepancy caused by adversarial perturbation</title><p>Let x = x + &#948; represent an adversarial example w.r.t. x, where &#948; denotes an adversarial perturbation. By replacing the input image x with x , the CNN will be fooled from the true label t to the target (incorrect) label t . It was recently shown in (Xu et al., 2019b;a) that the adversary could introduce an evident interpretability discrepancy w.r.t. both the true and the target label in terms of L(x, t) vs. L(x , t), and L(x, t ) vs. L(x , t ). An illustrative example is provided in <xref ref-type="fig" rid="fig_0">Figure 1</xref>. We see that an adversary suppresses the network interpretation w.r.t. the true label but promotes the interpretation w.r.t. the target la- bel. We also observe that compared to IG, CAM and GradCAM++ better localize class-specific discriminative regions. These results reveal two observations on how measuring interpretation discrepancy affects classification robustness: a) interpretability discrepancy may be used to detect adversarial examples, b) interpretability discrepancy itself may be vulnerable to adver- sarial perturbations. In what follows, we explore the spectrum between adversarial robustness and interpretability from a unified perspective considering both the adversarial vulnerability of interpretability discrepancy and the value of interpretability discrepancy in a defense .</p></sec><sec><title>ROBUSTNESS IN CLASSIFICATION VS. INTERPREATION MAP: AN ATTACK PERSPECTIVE</title><p>In this section, we examine two types of threat models, interpretability sneaking attack (ISA) and attack against interpretability (AAI). Since an adversarial example designed for misclassification Under review as a conference paper at ICLR 2020 Input image CAM GradCAM++ IG Original example x Adversarial example x L(&#183;, t) L(&#183;, t ) correlation: 0.4782 correlation: 0.5039 L(&#183;, t) L(&#183;, t ) correlation: 0.5018 correlation: 0.5472 L(&#183;, t) L(&#183;, t ) correlation: 0.4040 correlation: 0.3911 gives rise to interpretability discrepancy (which could then be used as a detector for the adversarial input), the problem of ISA arises: One may wonder whether or not it is easy to generate adversarial examples that mistake classification but keep interpretation intact. Such adversarial vulnerability could have serious consequences when classification and interpretation are jointly used in tasks like medical diagnosis (Subramanya et al., 2018), and call into question the faithfulness of interpretation to network classification. One the other hand, it is suggested from interpretability discrepancy that an interpreter could be quite sensitive to input perturbations. Thus, the problem of AAI arises: One may wonder if perturbed inputs could induce differing explanations but without altering predictions.</p></sec><sec><title>RETHINKING ISA FROM PERSPECTIVE OF INTERPRETABILITY DISCREPANCY</title><p>Previous work (Zhang et al., 2018; Subramanya et al., 2018) showed that it is not difficult to hide adversarial examples from network interpretation when the interpretability discrepancy is measured w.r.t. a single class label (either the true label t or the target label t ). However, we see from <xref ref-type="fig" rid="fig_0">Figure 1</xref> that the adversary (against classification) alters interpretability maps w.r.t. both t and t . This motivates us to rethink whether the single-class interpretability discrepancy measure is proper, and whether ISA is truly easy to bypass an interpretability discrepancy check.</p><p>We consider the following generic form of interpretability discrepancy D x, x = 1 |C| i&#8712;C L(x, i) &#8722; L(x , i) p , (3) where recall that x and x are natural and adversarial examples respectively, L represents an inter- preter, e.g., CAM or IG, C denotes the set of class labels used in L, |C| is the cardinality of C, and we consider p &#8712; {1, 2} in this paper. Clearly, a specification of (3) relies on the choice of C and p. We specify (3) with C = {t, t } and p = 1, leading to 1 2-class interpretability discrepancy measure,</p><p>Rationale behind 1 2-class discrepancy measure. Compared to the previous work (Zhang et al., 2018; Subramanya et al., 2018) using a single class label, we choose C = {t, t } 1 , motivated by the fact that an interpretability discrepancy occurs w.r.t. both t and t (<xref ref-type="fig" rid="fig_0">Figure 1</xref>). Moreover, although Euclidean distance (namely, 2 norm or its square) is arguably one of the most commonly-used dis- crepancy metrics (Zhang et al., 2018), we show in Proposition 1 that the proposed interpretability discrepancy measure D 2, 1 (x, x ) has a non-trivial lower bound for any successful adversarial at- tack. This provides an explanation on why it could be difficult to hide adversarial examples from 1 In addition to the 2-class case, our experiments will also cover the all-class case C = [C]. Under review as a conference paper at ICLR 2020 network interpretability methods. Moreover, 1 is an upper bound of the 2 norm and promotes the sparsity of interpretability discrepancy, which enforces pixels of L to stay intact when facing input perturbations.</p><p>Proposition 1. Given a classifier f (x) &#8712; R C and its interpreter L(x, c) for c &#8712; [C], suppose that the interpreter satisfies the completeness axiom, namely, i [L(x, c)] i = f c (x). For a natural example x and an adversarial example x with prediction t and t respectively, D 2, 1 (x, x ) in (4) has the lower bound</p><p>Proof: See proof in Appendix A.</p><p>Proposition 1 connects D 2, 1 (x, x ) with the classification margin f t (x) &#8722; f t (x). Thus, if a classi- fier has a large classification margin on the natural example x, it will be difficult to find a successful adversarial attack with small interpretability discrepancy. In other words, constraining the inter- pretability discrepancy prevents misclassification since generating adversarial examples becomes infeasible under D 2, 1 (x, x ) &lt; 1 2 (f t (x) &#8722; f t (x)). Also, the completeness condition of L sug- gests specifying (4) with CAM (1) or IG (2). Here we focus on CAM due to its light computation.</p></sec><sec><title>Design of ISA</title><p>where &#955; &gt; 0 is a regularization parameter that strikes a balance between the success of an attack and its resulting interpretability discrepancy, &#964; &#8805; 0 (e.g., 0.1 used in the paper) is a tolerance on the classification margin of a successful attack between the target label t and the non-target top-1 prediction label arg max j =t f j (x + &#948;), D 2, 1 was defined by (4), and &gt; 0 is a (pixel- level) perturbation size. In (6), the first term of the objective corresponds to a C&amp;W-type attack loss (Carlini &amp; Wagner, 2017), which reaches &#8722;&#964; if the attack succeeds in misclassification. To find ISA of minimum interpretability discrepancy, we perform a bisection on &#955; until there exists no successful attack that can be found when &#955; further decreases. Although we focus on &#8734; attack in (6), but a similar formulation applies to 1 and 2 attacks. Attacks are found using PGD, with sub- gradients taken at non-differentiable points. We consider only targeted attacks to better evaluate the effect on interpretability of target classes, although this approach can be extended to an untargeted setting (e.g., by using the target label-free interpretability discrepancy measure introduced in the next section).</p></sec><sec><title>Difficulty of generating ISA versus interpretability discrepancy measure</title><p>Through an illustra- tive example in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, we elaborate on how the choice of interpretability discrepancy measure plays a crucial role on the difficulty of hiding adversarial examples from network interpretation. We refer readers to Sec. 5 for more experimental results. We generate ISA under different specifications of (3) for different values of perturbation size . Compared to 1 / 2 1-class (true class t), 2 2-class, and 1 / 2 all-class, we see that the 1 2-class interpretability discrepancy (4) cannot be easily miti- gated even as the attack power (in terms of ) increases. This is verified by a) its high interpretability discrepancy score and b) its flat slope of discrepancy score against in Figure 2-(a)&amp;(b). Figure 2-(c) further shows CAMs of adversarial examples w.r.t. the true label t and the target label t generated by 1 1/2/all-class ISAs. We note that both 1-class measure and all-class measure could give a false sense of ease of hiding adversarial examples. For 1 1-class ISA, although the interpretability discrepancy w.r.t. t is minimized, the discrepancy w.r.t. t remains large, supported by the observation that 1 1-class ISA even yields a smaller correlation between L(x , t ) and L(x, t ) than PGD attack. Similarly, although the averaged discrepancy over all classes is minimized for 1 all-class ISA, discrepancies w.r.t. specific labels such as t and t do not necessarily re- duce. This illustrates that although ISA may be found, with the proper choice of discrepancy measure, ISA with a low discrepancy becomes quite difficult.</p></sec><sec><title>ATTACK AGAINST INTERPRETABILITY (AAI)</title><p>Different from ISA, AAI produces input perturbations to maximize the interpretability discrepancy while keeping the classification decision intact. Thus, AAI provides a means to evaluate the ad- Under review as a conference paper at ICLR 2020 (a) (b) L(&#183;, t) L(&#183;, t ) original image x 10-step PGD attack x 1 2-class ISA x 1 1-class ISA x 1 all-class ISA x correlation: 0.4782 correlation: 0.5213 correlation: 0.7107 correlation: 0.5342 correlation: 0.5039 correlation: 0.5416 correlation: 0.4129 correlation: 0.5561 (c) versarial robustness in interpretability. Since t = arg max i f i (x) = arg max i f i (x ) = t in AAI, the 2-class interpretability discrepancy measure (4) reduces to its 1-class version. The problem of generating AAI is then cast as minimize &#948; &#955; max{max j =t fj(x + &#948;) &#8722; ft(x + &#948;), 0} &#8722; D1 (x, x + &#948;) subject to &#948; &#8734; &#8804; , (7) where the first term is a hinge loss to enforce f t (x + &#948;) &#8805; max j =t f j (x + &#948;), namely, arg max i f i (x ) = t (unchanged prediction under input perturbations), and D 1 denotes a 1-class interpretability discrepancy measure, e.g., D 1, 1 from (4), or the top-k pixel difference between in- terpretability maps (Ghorbani et al., 2019). Similar to (6), the regularization parameter &#955; in (7) strikes a balance between stealthiness in classification and variation in interpretability. Experiments in Sec. 5 will show that the state-of-the-art defense methods against adversarial examples do not nec- essarily preserve robustness in interpretability as increases, athough the prediction is not altered.</p></sec><sec><title>INTERPRETABILITY-AWARE ROBUST TRAINING: DEFENSE PERSPECTIVE</title><p>We recall from Sec. 3.1 that adversarial examples that intend to fool a classifier could be difficult to evade the 1 2-class interpretability discrepancy. Thus, constraining the interpretability discrepancy helps to prevent misclassification. Motivated by this observation about ISA, we introduce a interpretability based defense method that penalizes interpretability discrepancy to achieve high classification robustness. As an additional benefit, it also robustifies the classifier against AAI, where the adversary maximizes the interpretability discrepancy.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>Target label-free interpretability discrepancy. Different from attack generation, the 1 2-class discrepancy measure (4) cannot directly be used by a defender since the target label t specified by the adversary is not known a priori. To circumvent this issue, we propose to approximate the in- terpretability discrepancy w.r.t. the target label by weighting discrepancies from all non-true classes according to their importance in prediction. This modifies (4) t&#245;</p><p>where the softmax function e f (x ) i i e f (x ) i adjusts the importance of non-true labels according to their classification confidence. Clearly, when x succeeds in misclassification, the top-1 predicted class of x becomes the target label and the resulting interpretability discrepancy is most penalized.</p></sec><sec><title>Interpretability-aware robust training</title><p>where &#952; denotes the model parameters, D train denotes the training dataset, f train is the training loss (e.g., cross-entropy loss), &#947; &gt; 0 is a regularization parameter, and for ease of notation we omit the parameters &#952; and t inD (x, x ). Here &#947; controls the tradeoff between clean accuracy and robustness; see Appendix C for experiments analyzing this tradeoff.</p><p>In problem (9), the inner maximization is only used to evaluate the worst-case interpretability dis- crepancy. Thus, it is different from adversarial training (Madry et al., 2018), where the training loss is replaced with the adversarial loss maximize x : x &#8722;x &#8734;&#8804; f train (&#952;, x ; x, t). The formulation (9) allows us to examine whether or not robust interpretation is directly beneficial to robust classifica- tion. For completeness, we will also provide experiment results on a modified formulation of (9) with the use of the adversarial loss.</p><p>Difference to (Chen et al., 2019). The recent work (Chen et al., 2019) proposed improving adver- sarial robustness by leveraging robust IG attributions. However, different from (Chen et al., 2019), our approach is motivated by the importance of 1 2-class interpretability discrepancy measure. We will show in Sec. 5 that the incorporation of interpretability discrepancy w.r.t. target class labels, namely, the second term in (8), plays a key role in boosting classification and interpretation robust- ness. This is because robust interpretability implies robust classification only when interpre- tation maps are measured with a proper metric. We will also show that our proposed method is sufficient to improve adversarial robustness even in the absence of adversarial loss, while the robust attribution regularization method (Chen et al., 2019) becomes ineffective when the attack becomes stronger. Last but not the least, beyond IG, our proposed theory and method apply to any network interpretation method with completeness axiom. The use of CAM avoids Riemman approximation used in IG and thus simplifies the implementation during robust training.</p></sec><sec><title>EXPERIMENTS</title><p>In this section, we empirically show the effectiveness of our proposed methods in various attack and defense settings. For ISA, we examine how the interpretability discrepancy measure plays a role in hiding adversarial examples from network interpretation. For AAI, we evaluate its attack success rate under the natural and various robust models. For interpretability-aware robust training, we demonstrate its advantages in a) defending against PGD attacks with different steps and pertur- bation sizes (Madry et al., 2018; Athalye et al., 2018), b) defending against unforeseen adversarial attacks (Kang et al., 2019), c) rendering robustness in interpretability, and d) computation efficiency compared to the IG-based robust attribution regularization method (Chen et al., 2019).</p></sec><sec><title>Datasets, CNN models, and experiment setting</title><p>We evaluate networks trained on the MNIST, CIFAR-10 and a restricted ImageNet (R-ImageNet) dataset used in (Engstrom et al., 2019). We consider three models, Small (for MNIST and CIFAR), Pool (for MNIST) and WResnet (for CIFAR and R-ImageNet): 1) a small CNN architecture consisting of three convolutional layers of 16, 32 Under review as a conference paper at ICLR 2020 and 100 filters (Small), 2) a CNN architecture with two convolutional layers of 32 and 64 filters each followed by max-pooling which is adapted from (Madry et al., 2018) (Pool), and 3) a Wide Resnet from (Madry et al., 2018) (WResnet). We refer readers to Appendix B for more details.</p><p>In the following experiments, we consider 5 baselines from the literature: i) standard training (Nor- mal), ii) adversarial training (Adv) (Madry et al., 2018), iii) TRADES (Zhang et al., 2019), iv) IG-Norm that uses IG-based robust attribution regularization (Chen et al., 2019), v) IG-Sum-Norm (namely, IG-Norm with adversarial loss). Additionally, we consider 3 variants of our method: vi) the proposed interpretability-aware robust training method (9) (we call Int), vii) Int using 1 1-class discrepancy (Int-1-class), and viii) Int with adversarial loss (Int-Adv). For interpretability-based methods, the regularization parameter &#947; is set to 0.01. In Appendix C, we explore different settings of the parameter &#947; for our method to demonstrate that we can achieve a range of different points on the robustness-accuracy tradeoff. Evaluating ISA. We evaluate the effect of interpretability discrepancy measure on ease of finding ISAs. Spurred by <xref ref-type="fig" rid="fig_1">Figure 2</xref>, such an effect is quantified by calculating minimum discrepancy required in generating ISAs against different values of perturbation size in (6). We conduct experiments over 4 network interpretation methods: i) CAM, ii) GradCAM++, iii) IG, and iv) internal representation at the penultimate (pre-softmax) layer (denoted by Repr). In order to fairly compare among different interpretation methods, we compute a normalized discrepancy score (NDS) extended from (3) and a normalized slope (NSL) that measures the relative change of NDS for &#8712; [&#711; ,&#710; ]. We refer readers to Appendix D for more details.</p><p>In <xref ref-type="table" rid="tab_0">Table 1</xref>, we present NDS and NSL of ISAs generated under different realizations of interpretabil- ity discrepancy measure (3), each of which is given by a combination of interpretation method (CAM, GradCAM++, IG or Repr), p norm (p &#8712; {1, 2}) and number of interpreted classes. Note that Repr is independent of the number of classes, and thus we report NDS and NSL correspond- ing to Repr in the 2-class column of <xref ref-type="table" rid="tab_0">Table 1</xref>. Given an p norm and an interpretation method, we consistently find that the use of 2-class measure achieves the largest NDS and smallest NSL at the same time. This implies that the 2-class discrepancy measure increases the difficulty of ISA to evade network interpretability check. Moreover, given a class number and an interpretation method, we see that NDS under 1 norm is greater than that under 2 norm, since the former is naturally an upper bound of the latter. Also, the use of 1 norm often yields a smaller value of NSL, implying that the 1 -norm based discrepancy measure is more resistant to ISA. Furthermore, by fixing the combination of 1 norm and 2 classes, we observe that IG is the most resistant to ISA due to their relatively high NDS and low ISA, and Repr yields the worst performance. However, compared to CAM, the computation cost of IG increases dramatically as the input dimension, the model size, and the number of steps in Riemman approximation increase. We find that it becomes infeasible to generate ISA using IG for WResnet under R-ImageNet within 200 hours.</p></sec><sec><title>Evaluating AAI</title><p>We specify problem (7) under the top-k attack setting (Ghorbani et al., 2017), where the top-k intersection between the original and adversarial interpretability maps is minimized. The strength of AAI is then measured by the Kendall's Tau order rank correlation between the aforementioned two interpretability maps (Chen et al., 2019). The higher the correlation is, the more robust the model is against AAI. Reported rank correlations are averaged over a test set. In <xref ref-type="table" rid="tab_1">Table 2</xref>, we present the performance of AAI under multiple perturbation sizes to attack mod- els trained using different training methods. For the model Small under MNIST, we evaluate AAI over 5 baselines (Normal, Adv, TRADES, IG-Norm, IG-Sum-Norm) and 3 variants of our method (Int, Int-1-class and Int-Adv). For a larger model WRes- net under CIFAR-10, the training methods using IG (IG-Norm and IG-Norm-Sum) are excluded due to their prohibitive computation cost. The insights learnt from <xref ref-type="table" rid="tab_1">Table 2</xref> are summarized as below. First, the normally trained model (Normal) does not auto- matically give robustness guarantees in interpretabil- ity, particularly for AAI with &#8805; 0.2. Second, the methods Adv, TRADES, IG-Sum-Norm and Int- Adv that uses adversarial loss offer certain robust- ness against AAI but the performance gets worse as the perturbation size increases. Third, in the ab- sence of adversarial loss, the baseline IG-Norm be- comes less robust as increases. By contrast, our proposed method Int is consistently more robust and its advantage becomes more evident as increases, including when is increased beyond the value used for training.</p></sec><sec><title>Evaluating interpretability-aware robust training</title><p>We previously showed in <xref ref-type="table" rid="tab_1">Table 2</xref> that interpretability-aware robust training (Int, Int-Adv) often achieves more robust interpretability than state-of-the-art adversarial training methods particularly for large perturbation size. We next provide a thorough evaluation on how interpretability helps adversarial robustness against misclassification.</p></sec><sec><title>Robustness versus efficiency</title><p>In <xref ref-type="fig" rid="fig_2">Figure 3</xref>, we present the training time (left y-axis) and the ad- versarial test accuracy (right y-axis) for differ- ent training methods (x-axis), which are ranked in a decreasing order of computation complex- ity.</p><p>Here the adversarial test accuracy (ATA) is measured using 200-step ( &#8734; -norm) perturbation sizes = 0.3 and 0.4 on the Small MNIST model (Madry et al., 2018). Note that all AT-type methods (IG-Norm-Sum, Int-Adv, TRADES and Adv) offer robust classification at = 0.3 with ATA around 80%. Among non-AT but interpretability pro- moted defensive schemes (IG-Norm, Int, Int-1- class), we find that only the proposed Int yields competitive ATA, and outperforms all AT-type methods except Int-Adv at = 0.4. Particularly, IG-Norm degrades significantly in robust classification when PGD attack becomes stronger (also see results in <xref ref-type="table" rid="tab_3">Table 4</xref>). Moreover, the non-robustness of Int-1-class verifies the importance of 2-class interpretability discrepancy measure on rendering robust classification. Last but not the least, Adv, TRADES and Int-based methods have similar computational complexity, but IG-based methods make training time (per epoch) significantly higher, 3 times more than Int-Adv even under the Small MNIST model.</p><p>Int does not cause obfuscated gradients and importance of 2-class measure. It was shown in (Athalye et al., 2018; Carlini, 2019) that some defense methods cause obfuscated gradients, which give a false sense of security. There exist two characteristic behaviors of obfuscated gradients: a) One-step attacks perform better than iterative attacks; b) Increasing distortion bound does not increase success. Motivated by that, we evaluate our interpretability-aware robust training methods under PGD attacks with a) different steps and b) different perturbation sizes. <xref ref-type="table" rid="tab_2">Table 3</xref> reports ATA of interpretability-aware robust training relative to various baselines, where 200-step PGD attacks are conducted for &#8712; {0, 0.05, 0.1, 0.2, 0.3, 0.35, 0.4} for MNIST and &#8712; {0, 2/255, 4/255, 6/255, 8/255, 9/255, 10/255} for CIFAR. We use = 0.3 on MNIST Under review as a conference paper at ICLR 2020 and 8/255 on CIFAR for robust training. As we can see, ATA decreases as increases. Thus, our methods do not exhibit the behavior a) of obfuscated gradients. We also observe that com- pared to Adv and TRADES, Int achieves reasonable but worse ATA as &lt; 0.3 on MNIST and &#8804; 8/255 on CIFAR. As used in PGD attack achieves the value used for robust training, Int achieves ATA 0.790 against ATA 0.890 from Adv on MNIST, but outperforms Adv on CIFAR (0.270 vs 0.170). Interestingly, the advantage of Int becomes more evident as the adversary becomes stronger, i.e., &gt; 0.3 on MNIST and &gt; 8/255 on CIFAR. We highlight that such a robust classification is achieved by promoting robustness of interpretability alone (without using adversarial loss). It is worth mentioning that IG-Norm fails to defend PGD attack with = 0.3 for the Small MNIST model. We further note that Int-1-class performs much worse than Int, supporting the importance of using 2-class discrepancy measure (see Prop. 1). Besides robust classification, IG-Norm and Int-1-class are also not sufficient to render robustness in interpretation (<xref ref-type="table" rid="tab_1">Table 2</xref>). <xref ref-type="table" rid="tab_3">Table 4</xref> shows ATA of interpretability-aware robust training against k-step PGD attacks (examining behavior b) of obfuscated gradients), where k &#8712; {1, 10, 100, 200}. As we can see, ATA decreases as k increases. This also suggests that the high robust accuracy from our methods is not a result of obfuscated gradients. Similar to <xref ref-type="table" rid="tab_2">Table 3</xref>, we see that compared to Int, IG-Norm and Int-1-class are insufficient to defend PGD attacks with k &#8805; 100.</p></sec><sec><title>Beyond &#8734; -norm PGD attacks</title><p>In <xref ref-type="table" rid="tab_4">Table 5</xref>, we present ATA of interpretability-aware robust train- ing and various baselines for defending attacks (Gabor, Snow, JPEG &#8734; , JPEG 2 , and JPEG 1 ) recently proposed in (Kang et al., 2019). These attacks are called 'unforseen attacks' since they are not met by PGD-based robust training and often induce larger perturbations than conventional PGD attacks. For robust training methods without resorting to adversarial loss, we find that Int sig- nificantly outperforms IG-Norm especially under Snow and JPEG p attacks. Int also yields quite competitive robustness compared to AT-type methods on most attacks. In this paper, we investigate the connection be- tween network interpretability and adversarial robustness. We show theoretically and empiri- cally that with the correct choice of discrepancy measure, it is difficult to hide adversarial exam- ples from interpretation. We leverage this dis- crepancy measure to develop a interpretability- aware robust training method that displays 1) high classification robustness in a variety of set- tings and 2) high robustness of interpretation.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Interpretation (L) of benign (x) and adversarial (x ) image from restricted ImageNet (Engstrom et al., 2019) with respect to the true label t='monkey' and the target label t ='fish'. Here the adversarial example is generated by 10-step PGD attack with perturbation size 0.02 (Madry et al., 2018), and we consider three types of interpretation maps, CAM, GradCAM++ and IG. Given an interpretation method, the first column is L(x, t) versus L(x , t), the second column is L(x, t ) versus L(x , t ), and all maps under each category are normalized w.r.t. their largest value. At the bottom of each column, we quantify the resulting interpretability discrepancy by Kendall's Tau order rank correlation (Selvaraju et al., 2017) between every pair of L(x, i) and L(x , i) for i = t or t .</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Seeing the effect of discrepancy measure on hiding adversarial examples from network interpreta- tion. The same benign image in Figure 1 is considered. (a) ISA using CAM-based 1 1/2/all-class discrepancy measure versus perturbation size , (b) ISA using CAM-based squared 2 1/2/all-class discrepancy measure versus , (c) CAM interpretation of example in Figure 1 and its adversarial counterparts from PGD attack and different specifications of ISA. All interpretation maps are normalized w.r.t. their largest value. At the bottom of each interpretation map L(x , &#183;), we quantify the interpretability discrepancy by Kendall's Tau order rank correlation between every pair of L(x , i) and L(x, i) for i &#8712; {t, t }, where x is obtained from PGD attack or each specification of ISA.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>NDS and NSL (format given by NDS/NSL) of successful ISAs generated under different specifi- cations of interpretability discrepancy measure (3) and datasets MNIST, CIFAR-10 and R-ImageNet. Here a discrepancy measure with large NDS and small NSL indicates a strong resistance to ISA.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Performance of AAI for different values of perturbation size in terms of Kendall's Tau or- der rank correlation between the original and ad- versarial interpretability maps. Best robustness re- sults (corresponding to highest correlation values) are highlighted (1st, 2nd, 3rd) under each column of a dataset-model pair.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Computation time per epoch and ad- versarial test accuracy for a Small MNIST model trained with different methods.</p></caption><graphic /><graphic /></fig><table-wrap id="tab_2"><label>Table 3:</label><caption><title>Table 3:</title><p>200-step PGD accuracy for different val- ues of perturbation size . Best ATA results are highlighted (1st, 2nd, 3rd) at each column. Note that ATA with = 0 reduces to natural accuracy.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_3"><label>Table 4:</label><caption><title>Table 4:</title><p>Multi-step PGD accuracy. Best ATA results are highlighted (1st, 2nd, 3rd) at each column.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_4"><label>Table 5:</label><caption><title>Table 5:</title><p>ATA on different unforeseen attacks in (Kang et al., 2019). Best results in each column are high- lighted (1st, 2nd, 3rd)</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap></sec></body><back /></article>