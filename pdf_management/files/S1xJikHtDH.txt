Title:
```
Under review as a conference paper at ICLR 2020 IMPROVING MODEL COMPATIBILITY OF GENERATIVE ADVERSARIAL NETWORKS BY BOUNDARY CALIBRATION
```
Abstract:
```
Generative Adversarial Networks (GANs) is a powerful family of models that learn an underlying distribution to generate synthetic data. Many existing studies of GANs focus on improving the realness of the generated image data for visual applications, and few of them concern about improving the quality of the gener- ated data for training other classifiers-a task known as the model compatibility problem. Literature also show that some GANs often prefer generating 'easier' synthetic data that are far from the boundaries of the classifiers, and refrain from generating near-boundary data, which are known to play an important roles in training the classifiers. To improve GAN in terms of model compatibility, we propose Boundary-Calibration GANs (BCGANs), which leverage the boundary information from a set of pre-trained classifiers using the original data. In par- ticular, we introduce an auxiliary Boundary-Calibration loss (BC-loss) into the generator of GAN to match the statistics between the posterior distributions of original data and generated data with respect to the boundaries of the pre-trained classifiers. The BC-loss is provably unbiased and can be easily coupled with dif- ferent GAN variants to improve their model compatibility. Experimental results demonstrate that BCGANs not only generate realistic images like original GANs but also achieves superior model compatibility than the original GANs.
```

Figures/Tables Captions:
```
Figure 1: A toy dataset generated by different GAN methods. Figure (a) is the original training data and the others are data generated by ACGAN, WGAN and our BWGAN respectively. The background color indicates the decision boundary of a random forest trained on corresponding data. The captions show the test accuracy of the random forest.
Figure 2: 2D visualization of real and generated Sensorless dataset. The mislabeled points are emphasized with border lines. The background color indicates the spaces of each class according to the projection classifier.
Figure 3: Gnerated samples from WGAN and our BWGAN. The images in the same column are in the same category.
Table 1: Summary result of model compatibility evaluate on UCI datasets. The numbers are relative accuracy.
Table 2: Precision at K of feature importance ranking compared to the feature importance ranking obtained from the original dataset
Table 3: F1 score of feature selection by 1 -regularized linear SVM
Table 4: Breakdown results on MNIST dataset.
Table 5: Breakdown results on CIFAR- 10 dataset.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The success of machine learning relies on not only the advances of different models (e.g. deep learning) but also data with sufficient quality and quantity. Nowadays, companies spend tremendous efforts and expense collecting data to build their products. To better solve complicated real-world problems with public or third-party machine learning experts, many companies now needs release some data sets for competitions (e.g. Kaggle) or proof-of-concept purposes. However, considering the costs of collecting data, companies may not be willing to release the dataset if possible. As a result, a technique which can generate synthetic data with properties similar to the original data is in demand. To be specific, we are looking for generating a dataset with the property that machine learning models trained on the generated dataset can exhibit similar performance to ones trained on the original data. This property is called model compatibility ( Park et al., 2018 ) or machine learning efficacy ( Xu et al., 2019 ). The organizations can share the generated data with high model compatibility to the public and enjoy the solution derived from it without leaking the real dataset. When it comes to data generation, generative adversarial networks (GANs,  Goodfellow et al. 2014 ) is the most popular family of generative algorithms because of its impressive performance on gen- erating realistic images ( Karras et al., 2018 ). In GANs, the generator is trained via minimizing a neural network (discriminator) defined probability divergence ( Goodfellow et al., 2014 ;  Arjovsky et al., 2017 ;  Nowozin et al., 2016 ). In addition to image generation, GANs are also widely used in other applications, such as style transfer ( Isola et al., 2017 ;  Zhu et al., 2017 ;  Kim et al., 2017 ) and image processing ( Pathak et al., 2016 ;  Ledig et al., 2017 ;  Chang et al., 2017 ), and generating differ- ent types of data, including time series ( Luo et al., 2018 ;  Chang et al., 2019 ), text ( Yu et al., 2017 ;  Press et al., 2017 ), point clouds ( Li et al., 2018 ), voxels ( Wu et al., 2016 ) and tabular data ( Park et al., 2018 ;  Xu et al., 2019 ).

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Although GANs are versatile as mentioned above, most of their development focus on the metrics such as quality and diversity of the data  Salimans et al. (2016) ;  Heusel et al. (2017) ;  Lucic et al. (2018) . Generating high model compatibility data via GANs is still under explored. The pioneering work ( Xu et al., 2019 ) first shows that data generated from conditional GANs ( Mirza & Osindero, 2014 ) enjoys better model compatibility than VAEs ( Kingma & Welling, 2013 ). So we wonder can we improve the model compatibility if we consider information from the models trained on the original data? For example, Wasserstein GAN ( WGAN, Arjovsky et al. 2017 ) performs a mean- matching between the distribution of real data and generated data. However, only mean-matching is sometimes not enough to learn the whole distribution especially for those boundary cases. Appar- ently, if a GAN knows the boundary between different classes, it may be able to generate instances which are close to the boundary with correct labels. These boundary points will guide a classifier to learn the correct decision boundary. In this work, we try to improve GANs with regards to model compatibility in classification problems. We use a set of pre-trained classifiers to obtain multiple decision boundaries. Then use an auxiliary loss function called Boundary-Calibration loss (BC-loss) to calibrate the generating distribution according to the decision boundaries of these pre-trained classifiers. The main contributions of this work are: • In Section 2, we propose a way to evaluate model compatibility in classification problems. We consider a variety of machine learning algorithms and average the performance to ob- tain a comprehensive metric. • In Section 4, we propose a loss function called Boundary-Calibration loss (BC-loss) which helps typical GANs to learn a distribution with better model compatibility. The loss con- siders the decision boundaries of pre-trained classifiers and minimizes the maximum mean discrepancy ( MMD, Gretton et al. 2012 ) between the original dataset and the generated dataset. In addition, we show that optimizing the BC-loss would not change the optimal solution of the original GAN, but it reduces the feasible set to ensure the model compati- bility. • In Section 5, we demonstrate how BC-loss affects the boundary of the generated data with a two-dimensional toy dataset. We also show that the BC-loss improves model compatibility of the generated data with different types of classifiers and a variety of datasets. Finally, we inspect the feature selection results to examine how the interpretation of machine learning models may be effected. Last, in Section 3, we discuss some works which are similar to our work and describe how does our work differ from them.

Section Title: MODEL COMPATIBILITY IN CLASSIFICATION
  MODEL COMPATIBILITY IN CLASSIFICATION In this work, we focus on generating data for fully-supervised classification learning. Given a dataset D = {(x i , y i )} n i=1 , where x i ∈ X represents features of an instance, y i = f (x i ) ∈ Y represents the corresponding label of x i according f : X → Y, and (x i , y i ) ∼ P D , a learning algorithm A : (X , Y) n → H learns a hypothesis h ∈ H to approximate the mapping function, i.e. A(D) = h ≈ f . Our goal is to obtain a generator G which generates a synthetic dataset D = {(x j , y j )} m j=1 such that A(D ) = h ≈ h. We call this property model compatibility as proposed in  Park et al. (2018) . To measure the model compatibility of a generated dataset quantitatively, we consider the perfor- mance of a classifier trained on the generated dataset comparing to the one trained on the real dataset. We evaluate the accuracy on a separate test dataset to indicate the performance of a given classifier. In addition, we calculate relative accuracy by scaling the test accuracy of the classifier trained on the generated dataset by the accuracy of the classifier trained on the real dataset. The relative accuracy allows us to average the results from different machine learning algorithms more fairly. The final evaluation is : Under review as a conference paper at ICLR 2020 where A is a set of learning algorithms , D (t) = {(x (t) i , y (t) i )} N i=1 is the test dataset, and acc(h, D (t) ) is the accuracy of hypothesis h on test data D (t) . We can determine A as a wide variety of learning algorithms to make the metric provide a more comprehensive measurement of model compatibility.

Section Title: RELATED WORK
  RELATED WORK Research about generating data for classification can be divided into two categories: formulation and architecture. For formulation, Conditional GAN ( CGAN, Mirza & Osindero 2014  ) is an intuitive way to generate instances with corresponding labels. We can learn the distribution of labels by counting and sample the instances from CGAN conditionally. Auxiliary Classifier GAN ( ACGAN, Odena et al. 2017 ) is considered as a better way for conditional generation. It uses an auxiliary classifier to provide information about the boundary between each classes. However, ACGAN has been proved that the objective is biased so it tends to generate data with lower entropy for the auxiliary classifier ( Shu et al., 2017 ). Thus, the lose of instances near the decision boundary may worsen the model compatibility. In this work, we use CGAN along with the proposed BC-loss to generate data with model compatibility. On the other hand, the other line of research focuses on generating data with different network archi- tecture or data processing procedure. Recent works that also consider model compatibility are Table GAN ( Park et al., 2018 ) and Tabular GAN ( Xu et al., 2019 ). Table GAN focuses on the privacy of generated data and thus their is a trade off between privacy and model compatibility. To achieve privacy preserving, they do not improve the model compatibility compared to the original GAN. On the other hand, Tabular GAN puts emphasis on increasing model compatibility of generated data. They propose a framework with a more complicated data processing procedure and use LSTM to better parameterize the target distribution. In contrast to these works, our work focus on the formu- lation of GANs and can be applied to most variants of GANs, including Table-GAN and Tabular GAN. Moreover, while these former works only focus on tabular data, our BC-loss is applicable to generate image datasets as well. Some GAN variants are named similarly to our work but they pay attention to different problems. For example, the boundary described in boundary-seeking GAN ( Hjelm et al., 2017 ) means the de- cision boundary of the discriminator rather than the decision boundary for the supervised labels. To the best of our knowledge, we are the first work trying to improve model compatibility by modifying the formulation of GANs.

Section Title: BOUNDARY-CALIBRATION GAN
  BOUNDARY-CALIBRATION GAN To achieve better model compatibility of GAN, we propose an auxiliary GAN loss which we call boundary-calibration loss (or BC-loss). We assume that we have a set of pre-trained classifiers which are well-trained on the original dataset. The BC-loss helps GANs to calibrate the distribution with respect to the distribution of decision values predicted by pre-trained classifiers. The calibra- tion leads to more accurate data generation near the decision boundary and thus enabling a machine learning algorithm to learn a similar hypothesis to one that learns from the original dataset. To ease the complexity of learning to generate (x, y) jointly, we infer P (y) by counting the proportion of each class in the original dataset and train a conditional generator G such that G(z, y) ∼ P X |y , where P X |y is the conditional data distribution and z ∼ P Z is the initial randomness such as Gaus- sian distribution. Therefore we can generate (x, y) by sampling y ∼ P (y) and G(z, y).

Section Title: BOUNDARY CALIBRATION
  BOUNDARY CALIBRATION Given a pre-trained classifier C, we hope the generated dataset will adopt the same statistics as the original dataset while considering the decision boundary of C. To include the information about the boundary, we calculate posterior P C (y | x i ) from the decision values predicted by the classifier. In practice, we can apply a softmax function to the outputs of a classifier to obtain the posterior. The posterior provides information of an instance from the classifier's aspect. Therefore, given the real dataset X = {x 1 , x 2 , ..., x n }, we can obtain a set of posterior vector C(X) = (P C (y | x 1 ), P C (y | x 2 ), ..., P C (y | x n )). To generate data X with the same distribution of posteriors to the boundary, Under review as a conference paper at ICLR 2020 we match the statistics of C(X) and C(X ) by optimizing a distance M : Here M can be any distance metric which measures the distance between two sets of samples. In statistics, distinguishing whether two sets of samples are from the same distribution is called Two- Sample Test. A classical solution to two-sample test is kernel maximum mean discrepancy (MMD,  Gretton et al. 2012 ). The idea is to compare the statistics between the two sets of samples. If the statistics are similar then these two sets might be sampled from the same distribution. Given two sets of samples X = {x i } n i=1 and Y = {y j } n j=1 , an unbiased estimator of MMD with kernel k is defined as:M In practice, we use Gaussian kernel k(x, x ) = exp( x − x 2 ) in MMD since Gaussian kernel is a characteristic kernel which ensures that the distance is zero if and only if the two distributions are the same ( Gretton et al., 2012 ). The BC-loss can be applied in generator of any GAN variants to improve the model compatibility. In addition, to better fit the real unknown boundary, we can use multiple classifiers to calibrate the distributions from different aspects. As a result, for a loss function of generator L G , we can modify the loss to be:L G = L G + λ |C| C∈CM k (C(X), C(G(Z, Y))) (4) where Z is a set of noises, C is a set of pre-trained classifiers and λ is a hyper-parameter to control the weight of BC-loss.

Section Title: ANALYSIS OF OPTIMAL SOLUTION
  ANALYSIS OF OPTIMAL SOLUTION Next we prove that adding our proposed BC-loss would not change the optimal solution of the original objective. Here we assume the loss of the generator L G achieves its optimal value in the its GAN objectives L G if and only if the distribution of G(z, y) recovers P X |y for all y ∈ Y, which holds for the vanilla GAN ( Goodfellow et al., 2014 ) and most of other GAN variants. Theorem 1 ( Gretton et al. 2012 ). Given a kernel k, if k is a characteristic kernel, then M k (P, Q) = 0 ⇐⇒ P = Q. Theorem 2 (Equivalence of optimal solution). G is an optimal solution of L G ⇐⇒ G is an optimal solution ofL G Proof. (⇒) According to the assumption, G is an optimal solution of L G implies G(Z, y) recovers P X |y for all y ∈ Y. Therefore, P C(X) = P C(G(Z,Y)) and M k (P C(X) , P C(G(Z,Y)) ) = 0 by Theo- rem 1. NowL G = L G + 0 = L G and G is an optimal solution of L G , so G is also an optimal solution ofL G . (⇐) Since L BC ≥ 0, we haveL G = L G + L BC ≥ L G . From above, we knowL G (G) = 0 if G = P X . Thus, for an optimal solution G * , 0 ≥L G (G * ) ≥ L G (G * ) ≥ 0, which implieŝ L G (G * ) = L G (G * ) = 0. Therefore, G * is also an optimal solution of L G . The proof shows that the proposed BC-loss does not change the optimal solution of the original optimization problem. However, we can consider BC-loss as a Lagrangian constraint which restricts the solution to a subspace where the generator owns higher model compatibility .

Section Title: COMPARISON TO MMD GAN
  COMPARISON TO MMD GAN MMD GAN ( Li et al., 2017 ) is a variant of GAN where the generator tries to minimize the MMD between generated data and original data and the discriminator learns a kernel which maximizes the MMD. Though the formulation of MMD GAN and BC-loss are similar, they still do not con- flict because MMD GAN do not known the information about the classifier and the objective of Under review as a conference paper at ICLR 2020 MMD GAN would not lead the discriminator to a classifier. Therefore, BC-loss may still improve MMD GAN by guiding the generator to not generate points across the boundary. To understand the improvement in MMD GAN from BC-loss, we use MMD GAN as one of the baselines in our experiments.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we use a toy dataset to illustrate how the proposed method improves the model compatibility. To be more realistic, we provide more comprehensive results for four different real- world dataset from UCI dataset repository ( Dua & Graff, 2017 ): Adult, Connect-4, Covertype and Sensorless. We then show our method is also applicable in image dataset: MNIST and Cifar10 without losing the image quality. In addition, we investigate the results of feature selections on the generated dataset to see whether the generated data can preserve the interpretation of machine learning models.

Section Title: EXPERIMENTAL SETTINGS
  EXPERIMENTAL SETTINGS

Section Title: EVALUATION
  EVALUATION In this work, we focus on model compatibility of generated datasets. We use a wide variety of machine learning algorithms including linear SVM, decision tree (DT), random forest (RF), and multi-layer perception (MLP) to evaluate the model compatibility. As described in Section 2, we evaluate the relative accuracy for each type of machine learning model, where the relative accuracy is calculated by dividing the accuracy of classifier trained on generated data to the accuracy of classifier trained on original data.

Section Title: COMPARED METHODS
  COMPARED METHODS We take Wasserstein GAN (WGAN) and MMD GAN as our baselines to evaluate the effective- ness of the proposed boundary-calibration technique. We denotes their counterparts with BC-loss as BWGAN and BMMDGAN respectively. All of the methods use gradient penalty to enforce the Lipschitz constraint on the discriminator ( Gulrajani et al., 2017 ;  Li et al., 2017 ). To achieve condi- tional data generation as described in Section 4, we add an embedding layer to learn the embedding vector for each class. The embedding vector is concatenated as additional input features for both generators and discriminators on UCI datasets. For image datasets, the embedding vectors are used as described in  Mirza & Osindero (2014) .

Section Title: 2D TOY DATASET
  2D TOY DATASET We use a 2D toy dataset with two classes to illustrate the results generated by different GAN methods in  Figure 1 . Figure 2a shows the distribution of the original training data. We use these generated data to train a random forest and depict the decision boundary by different background color. From Figure 2b, we can see that although ACGAN can make use of the auxiliary classifier during training, Under review as a conference paper at ICLR 2020 it learns a biased distribution that push the generated data away from the boundary. The large margin between the two clusters brings more uncertainty to the decision boundary and thus leads to worse test accuracy. In Figure 2c, WGAN approximates the original distribution well in the center part of the two cluster, but do not get a clear boundary between the two classes. It generates some ambiguous points near the boundary that would confuse the classifier. Finally, our BWGAN generates points near the boundary more precisely, as shown in Figure 2c.

Section Title: UCI DATASET
  UCI DATASET We evaluate our proposed BC-GAN on four datasets from UCI repository. The attributes of the datasets can be found in Appendix A. Discrete features are processed to one-hot encoding and con- tinuous features are scaled to [0, 1]. For each dataset, we train six multi-layer perceptrons with a random split of half of training data as pre-trained classifiers. In these experiments, generators and discriminators are consist of 3 fully-connected hidden layer with 128 units. A logistic function is applied to the output layer of generators to generate features within 0 and 1. The weight of BC-loss is set to be λ = 100 for all datasets.  Table 1  summarize the comparison between different methods. We calculate the relative accuracy of different machine learning models mentioned in Section 5.1 and average the relative accuracy to indicate the model compatibility of generated data for each dataset. The table shows that the proposed BC-loss improves the accuracy of classifiers generally compared to original WGAN and MMD GAN. Moreover, ACGAN performs worst on three out of four datasets and exhibit a signifi- cant deficiency though it is proved to have the state-of-the-art generation quality. This again prove that the biased objective of ACGAN worsen the model compatibility seriously.The breakdown re- sults and real accuracy are provided in Appendix B. To further investigate the advantage of boundary-calibration, we visualize the generated results of Sensorless in  Figure 2 . We train a fully-connected neural network with a 2-units hidden layer before the output layer to project the generated samples to a 2-dimensional embedding space. The projec- tion classifier is well-trained and achieves over 99% testing accuracy so we can use it to determine whether a sample is generated with incorrect label. The figure shows that there are less mislabeled data generated by BWGAN, especially at the center and the bottom-left region. The fact indicates that boundary-calibration helps GANs generate labeled data more accurately, which may lead to the improvement of classification accuracy.

Section Title: INTERPRETABILITY
  INTERPRETABILITY In addition to accuracy, it is also important that the model trained on generated data should give us the same interpretation of a model trained on the original data. We investigate the interpretability by two common feature selection techniques. First, we train two random forests on the generated and original dataset respectively. Each random forest can provide the importances of the features. We evaluate the consistency of interpretation by calculating precision at Kth, which means how many features ranked top-k in random forest trained on original data are in the top-k importance feature of the random forest trained on generated data. The results are shown in  Table 2 . We provide the results of training a classifier on the same original data with a different random seed as REAL for comparison. The effect of BC-loss is not significant in this aspect. However, the scores of ACGAN drop seriously, which means training a classifier on data generated by ACGAN is somehow dangerous because the meaning of model may be totally different. Another way to select feature is training a linear model with 1 regularization. In  Table 3  we use linear SVM with 1 regularization to select features. Then we calculate the F1 score of features selected by classifiers trained on generated data to known how similar between the two sets of features selected by classifiers trained on original and generated dataset. The results again shows that using boundary-calibration does not has significant effect to feature selection and ACGAN is not proper to generated data for training.

Section Title: IMAGE DATASET
  IMAGE DATASET We further use MNIST and CIFAR-10 dataset to investigate the effectiveness of boundary- calibration on image datasets. For MNIST, we train six 4-layer convolution neural networks (CNN) with random sampling half of training data as pre-trained classifiers, and use the same classifier set in Section 5.1 to evaluate model compatibility. For CIFAR-10, we use ResNet56v2 ( He et al., 2016 ) to obtain three pre-trained classifier and evaluate on CNN and ResNet56v2. In both task, we use DCGAN ( Radford et al., 2016 ) as network structure in all GANs. The weight of BC-loss is set to be λ = 1 for these two datasets.  Table 4  and  Table 5  show the relative accuracy of classifiers trained on generated data. The pro- posed BWGAN still outperforms WGAN with better accuracy in general. The results generated by WGAN and BWGAN are pictured in  Figure 3 . The Inception score and Frechet Inception Distance (FID) for CIFAR-10 are also provided in the caption of  Figure 3 . Though the difference of quality between the images generated from WGAN and BWGAN is not significant in visual, the quantita- tive scores for quality of generated samples of CIFAR-10 are slightly improved. The results indicate that even though our method seems not improve the image quality, it is still able to improve the model compatibility without losing image quality.

Section Title: DISCUSSION
  DISCUSSION We introduce an auxiliary loss in GANs which improves the model compatibility of generated dataset. We prove the new loss is unbiased and is applicable to all variants of GAN to improve model compatibility. We further demonstrate that our method has clear advantages with a variety of machine learning models trained on generated dataset. In addition, we investigate the results of feature selection and found that the BC-loss doesn't effect the interpretation of machine learn- ing models. While this work only focus on classification problem, generating data for regression problem is also worth studying. We hope our work will open the path for GANs with better model compatibility so that synthetic data can be more useful in practice. Under review as a conference paper at ICLR 2020

```
