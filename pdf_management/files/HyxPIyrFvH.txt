Title:
```
Under review as a conference paper at ICLR 2020 WHEN ROBUSTNESS DOESN'T PROMOTE ROBUST-
```
Abstract:
```
We conduct a large experimental comparison of various robustness metrics for image classification. The main question of our study is to what extent current syn- thetic robustness interventions ( p -adversarial examples, noise corruptions, etc.) promote robustness under natural distribution shifts occuring in real data. To this end, we evaluate 147 ImageNet models under 199 different evaluation settings. We find that no current robustness intervention improves robustness on natural dis- tribution shifts beyond a baseline given by standard models without a robustness intervention. The only exception is the use of larger training datasets, which pro- vides a small increase in robustness on one natural distribution shift. Our results indicate that robustness improvements on real data may require new methodology and more evaluations on test sets representing natural distribution shifts.
```

Figures/Tables Captions:
```
Figure 1: Model accuracy on the two natural distribution shifts, ImageNetV2 (left) and Ima- geNetVidRobust (right). Each data point corresponds to one model in our testbed and is shown with 99.5% Clopper-Pearson confidence intervals. The plots demonstrate that the standard test accuracy (x-axis) is an almost perfect predictor for the test accuracy under distribution shift (y-axis). This holds regardless of whether the model was trained with a robustness intervention. Current robust- ness interventions reduce the accuracy drops under these distribution shifts only by a small amount (on ImageNetVidRobust) or not at all (on ImageNetV2). The axes were adjusted using logit scaling and the linear fit was computed in the scaled space. The red shaded region is a 95% confidence region for the linear fit from 100,000 bootstrap samples.
Figure 2: Robustness to synthetic distribution shift (x-axis) vs. effective robustness to the Ima- geNetV2 distribution shift (y-axis). The left plot shows synthetic robustness as measured by an average over image corruptions from (Hendrycks & Dietterich, 2019; Geirhos et al., 2018; 2019). The right plot shows synthetic robustness as measured by PGD attacks for 2 - and ∞ -robustness and two perturbation sizes each. In both cases, synthetic robustness is not predictive of effective robustness under the natural distribution shift. Appendices A.1 and A.2 contains similar plots for individual synthetic robustness measures and the ImageNetVidRobust distribution shift.
Figure 3: The left plot zooms into the range of the ResNet152-ImageNet11k and Instagram models on ImageNetV2 and shows that the models achieve significant effective robustness, i.e., they lie significantly above the linear fit. The right plot zooms into the range of the p -robust models on ImageNetVidRobust with high effective robustness. The dotted line shows the family of models achievable by interpolating between a standard ResNet-152 and a random classifier. It achieves the same robustness trade-off as the p -robust models.
Figure 4: To investigate the impact of training data on robustness, we vary the ILSRVC 2012 data along two axes: the number of images per class (left), and the number of classes (right). Although models trained on more data (e.g., the Instagram and ResNet152-ImageNet11k models) provide improvements in effective robustness, we find that subsampling of the ILSVRC training has no impact on effective robustness.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reliable classification under distribution shift is still out of reach for current machine learning ( Tor- ralba et al., 2011 ;  Recht et al., 2019 ). As a result, the research community has proposed a wide range of evaluation protocols that go beyond a single, static test set. Common examples include adversar- ial examples ( Szegedy et al., 2013 ;  Biggio et al., 2013 ), noise perturbations ( Geirhos et al., 2018 ;  Hendrycks & Dietterich, 2019 ), and spatial transformations ( Fawzi & Frossard, 2015 ;  Engstrom et al., 2019 ). Encouragingly, the past few years have seen substantial progress in robustness to these distribution shifts, e.g., see ( Madry et al., 2018 ;  Zhang et al., 2019 ;  Geirhos et al., 2019 ;  Zhang, 2019 ;  Engstrom et al., 2019 ;  Yang et al., 2019 ) and many others. However, an implicit assumption underlying this research direction is that robustness to such synthetic distribution shifts will lead to models that also perform more reliably on natural distribution shifts. We challenge that assumption. We conduct a large experimental study involving 147 ImageNet mod- els evaluated under 199 different evaluation settings for a total of 29,253 test set evaluations. Our model testbed contains a wide range of standard models and most proposed robustness interventions (adversarial training, various forms of data augmentation, etc.). In order to measure robustness under natural distribution shift, we utilize two recently proposed variants of ImageNet ( Deng et al., 2009 ;  Russakovsky et al., 2015 ) and ImageNetVid ( Berg et al., 2015 ). Both test sets consist entirely of un- perturbed images drawn from the same sources as the original datasets, but also exhibit substantial accuracy drops for all current model architectures ( Recht et al., 2019 ;  Shankar et al., 2019 ).  Figure 1  shows the main result of our evaluation. The plots display model accuracies for the two natural distribution shifts. As noted in prior work, there is a substantial drop in accuracy when going from the original test set to the test set with distribution shift. A priori, one may hope that a more robust model would see a smaller drop than baseline approaches without a robustness intervention. But we find that in both cases, current robustness interventions offer little to no benefit over stan- dard models. In particular, the accuracy on the original test set alone almost perfectly predicts the accuracy on the test sets with distribution shift. Importantly, the same relationship between original and "shifted" accuracy holds for both standard models and models with an explicit robustness in- tervention. This implies that the robustness interventions do not close the gap between original and "shifted" test accuracies any more than standard models with the same accuracy. There are only two significant deviations away from the otherwise universal relationship between original and shifted accuracy. The first is that p -adversarially robust models do offer increased accuracy against the distribution shift on ImageNetVidRobust. However, this increase is in a regime of overall low accuracy. As we will demonstrate in Section 5, interpolating between a standard model and a random classifier yields a comparable robustness increase in this accuracy regime. So the current p -robust models do not provide a truly new robustness trade-off. The second class of outliers are models trained on substantially different data. In particular, the two most pronounced outliers on ImageNetV2 correspond to models trained on 10× to 1,000× more training examples. This shows that adding more training data does indeed increase robustness on ImageNetV2 more than any other currently proposed explicit robustness intervention. However, adding more data also yields only small gains: the accuracy drops of around 10% from ImageNet to ImageNetV2 shrink by 1 - 2%, even when adding 1,000× more data. Overall, our results show that current robustness gains on synthetic distribution shifts do not transfer to improved robustness on the natural distribution shifts presently available as test sets. This suggests that research on reliable machine learning may currently be focusing on interventions that do not promote robustness on natural distribution shifts. Achieving robustness on real data may instead require new methodology and more evaluations on natural distribution shifts. To aid this development, we will release and maintain our robustness testbed as a platform for proposing and evaluating new models and datasets. In total, our testbed includes prediction data for 29,253,000 pairs of models and image inputs. We hope that a comprehensive repository for robust- ness evaluations will simplify the process of comparing proposed models and evaluation settings and enable further analysis of robustness questions. In the next section, we describe our experimental setup in detail. Section 3 introduces our main measure of robustness. The following three sections then investigate our three main questions: • Are synthetic robustness measures predictive of performance on natural distribution shifts? • What robustness interventions are effective for natural distribution shifts? • How does the amount of training data impact robustness?

Section Title: EXPERIMENTAL SETUP
  EXPERIMENTAL SETUP We first formally define our experimental setup. In the standard classification setting, a modelf is first trained on i.i.d. samples from a fixed data distribution. We then testf on another set S 1 of i.i.d. Under review as a conference paper at ICLR 2020 samples from the same distribution to compute the test accuracy acc S1 (f ) = (x,y)∈S1 I[f (x) = y]. We refer to any deviation from this setup as a distribution shift. More specifically, our robustness evaluation involves distribution shifts defined as follows. Instead of using the original test set S 1 , we may also evaluate the accuracy on a new test set S 2 that is collected via a similar but not identical procedure to the original test set S 1 (i.e., S 2 represents a different distribution). Moreover, we allow each data point (either from S 1 or S 2 ) to be pertubed before passing it to the classifierf . We consider a general setup where the perturbation π(x, y,f ) may depend on the modelf to be evaluated. For instance, we can generate ∞ -adversarial examples by setting π(x, y,f ) = arg min x−x ∞≤ε I[f (x ) = y]. Alternatively, we can simply perturb each data point with Gaussian noise, i.e., π(x, y,f ) = x + N (0, σ 2 ) independently of y andf . Combining the choice of test set S and perturbation π, the test accuracy under distribution shift S, π is then defined as Depending on S and π, this "shifted" test accuracy may be higher or lower than the original accuracy acc S1 . The focus of this paper is on distribution shifts where there is reasonable hope to classify the images correctly (e.g., because humans can do so), but standard models achieve only a substantially lower accuracy than on the original test set. Reducing these accuracy gaps between original and shifted accuracy is an important goal of robust machine learning.

Section Title: TYPES OF DISTRIBUTION SHIFTS
  TYPES OF DISTRIBUTION SHIFTS At a high level, we distinguish between two main types of distribution shift. The crucial distinction between the two is whether the distribution shift involves a synthetic intervention at the pixel level. We use the term natural distribution shift for cases that rely only on unmodified images. In contrast, we refer to distribution shifts as synthetic if they involve modifications of existing test images. To be concrete, we now provide an overview of the various distribution shifts in our robustness evaluation.

Section Title: NATURAL DISTRIBUTION SHIFTS
  NATURAL DISTRIBUTION SHIFTS Our testbed includes two distribution shifts that involve only unmodified natural images.

Section Title: ImageNetV2
  ImageNetV2 The first example is the new ImageNetV2 test set recently collected by  Recht et al. (2019) . The goal of this test set was to closely reproduce the original ImageNet dataset creation process, e.g., by matching the data source (Flickr) and the data cleaning process (Mechanical Turk). Nevertheless, the authors found that state-of-the-art models still suffer a performance drop of at least 11% on the new test set. This makes the dataset an interesting example for studying robustness to natural distribution shifts. Testing on ImageNetV2 fits into our evaluation framework by letting S be the new ImageNetV2 test set (denoted by S 2 ) and leaving π to be the identity function.

Section Title: ImageNetVidRobust
  ImageNetVidRobust In contrast to precisely defined, synthetic notions of robustness such as p - adversarial examples, it is currently an open problem to characterize the distribution shift arising in ImageNetV2. To narrow this gap between the real and synthetic robustness challenges,  Shankar et al. (2019)  have introduced a variant of the ImageNetVid dataset ( Berg et al., 2015 ). The authors assembled consecutive video frames that are all highly similar to a human yet cause classifiers to make errors on some of the images. Inspired by p -adversarial examples, the authors propose the following pm k (plus-minus k) metric to evaluate the accuracy of a classifier as pm k,S (f ) = (x,y)∈S min x ∈B k (x) I[f (x ) = y] , where B k (x) contains the 2k frames around the anchor frame x (k frames in each temporal di- rection). This corresponds to a perturbation function π(x, y,f ) = arg min x ∈B k (x) I[f (x ) = y]. Under this metric, the authors find that even the best models see an accuracy drop of at least 13%.

Section Title: SYNTHETIC DISTRIBUTION SHIFTS
  SYNTHETIC DISTRIBUTION SHIFTS The research community has developed a wide range of synthetic robustness notions for image classification over the past five years. In our study, we consider the following classes of synthetic distribution shifts.

Section Title: Adversarial examples
  Adversarial examples One prominent example of synthetic distribution shifts are adversarial examples, which demonstrate that current image classifiers can be fooled by small image perturba- tions that are (almost) imperceptible to a human ( Szegedy et al., 2013 ;  Biggio & Roli, 2018 ). In our robustness evaluation, we focus on untargeted adversarial perturbations bounded in ∞ - or 2 -norm.

Section Title: Image corruptions
  Image corruptions Since p -adversarial examples are unlikely to occur outside a truly worst- case setting, the research community has proposed various synthetic image corruptions as less adversarial distribution shifts. The goal of these corruptions is to test robustness to distribution shifts that are more realistic and hopefully predictive of performance on real data (we will investigate this assumption in Section 4). In our testbed, we include all corruptions from ( Hendrycks & Dietterich, 2019 ) and additionally some corruptions from ( Geirhos et al., 2018 ). These include common examples of image noise (Gaussian, shot noise, etc.), various blurs (Gaussian, motion), simulated weather conditions (fog, snow), and "digital" corruptions such as various JPEG compression levels. We refer the reader to Appendix A.6.3 for a full list of the 38 corruptions.

Section Title: Style transfer
  Style transfer We also include a style transfer version of the ImageNet test set ( Huang & Belongie, 2017 ). This distribution shift was proposed to evaluate whether classification models are relying more on shape or texture features ( Geirhos et al., 2019 ).

Section Title: CLASSIFICATION MODELS
  CLASSIFICATION MODELS Our model testbed includes 147 ImageNet models covering a variety of different architectures and training methodologies. Our goal was to include most relevant pretrained ImageNet models available online. In multiple cases we also contacted paper authors with various degrees of success. At a high level, the models can be divided into the following three categories (see Appendix A.5.1 for a list of all models).

Section Title: Standard models
  Standard models We refer to models trained on the ILSVRC 2012 training set without a specific robustness focus as standard models. This category includes 74 models, ranging from the seminal AlexNet with top-1 accuracy 56.5% through widely used architectures such as VGG, ResNet, and Inception to the state-of-the-art EfficientNet with top-1 accuracy 84.4% ( Krizhevsky et al., 2012 ;  Simonyan & Zisserman, 2014 ;  He et al., 2016 ;  Szegedy et al., 2015 ;  Tan & Le, 2019 ).

Section Title: Robust models
  Robust models This category includes models with an explicit robustness intervention. We subdivide this class further into two types of robustness interventions: • Models with increased adversarial robustness. We include both models trained with projected gradient descent ( Xie et al., 2019 ;  Shafahi et al., 2019 ) and models with evaluation via random- ized smoothing ( Cohen et al., 2019 ;  Salman et al., 2019 ), which yields a total of 14 models. • Further robustness interventions such as data augmentation schemes ( Geirhos et al., 2019 ;  Yun et al., 2019 ) and anti-aliasing ( Zhang, 2019 ). This subset contains 51 models.

Section Title: Models trained on more data
  Models trained on more data Finally, our testbed also contains three types of models that utilized substantially more training data than the aforementioned models. The ResNet152- ImageNet11k model was trained on 12.4 million images for 11,221 classes from the full ImageNet dataset ( Wu, 2016 ). Like all other models, we only evaluate it over the 1,000 classes from ILSVRC 2012. Facebook recently released models trained on 1 billion images from Instagram; we refer to these models as Instagram models ( Mahajan et al., 2018 ). We also include evaluations from the JFT-300M model trained on 300 million images at Google ( Sun et al., 2017 ).

Section Title: MEASURING THE EFFECT OF ROBUSTNESS INTERVENTIONS
  MEASURING THE EFFECT OF ROBUSTNESS INTERVENTIONS When comparing interventions to increase robustness, a crucial question is how to measure their effect in a meaningful way. One approach would be to simply use the absolute accuracy numbers acc S,π under the distribution shift specified by S and π (see Section 2 above). If the goal is to select the best performing model (e.g., for a concrete deployment), this is indeed a relevant criterion.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 On the other hand, relying on absolute accuracy numbers becomes less relevant when comparing various robustness interventions with different unperturbed accuracies acc S1 . As can be seen from  Figure 1 , the accuracy under distribution shift varies substantially with the standard accuracy: mod- els with higher standard accuracy already achieve higher accuracy under distribution shift. So to isolate the effect of a training intervention on the robustness to distribution shift, we would like to measure how much a robustness intervention increases the shifted accuracy beyond what is possible with standard models at the same standard accuracy. To quantify this notion of robustness, we define a "baseline" function β S,π (x) to return the shifted accuracy that a standard model with test accuracy x on the standard test set S 1 is expected to achieve under the distribution shift S, π. So iff is a model without robustness intervention, β should satisfy β S,π (acc S1 (f )) = acc S,π (f ). A priori, it is unclear if such a function β exists. The accuracy of a trained modelf on the dis- tribution shift (S, π) could depend on a variety of properties besides the standard accuracy off (model architecture, data augmentation scheme used during training, etc.). However, both natural distribution shifts we study in our paper have the intriguing property that the standard test accuracy is a nearly perfect predictor of the test accuracy under distribution shift for all standard models in our testbed. In particular, fitting a simple logistic model to the (acc S1 (f ), acc S,π (f )) pairs yields a good linear fit in the logit domain (see  Figure 1 ). This shows that we can obtain an accurate estimatê β from data, which we will rely on in the remainder of the paper. With the baseline robustness function β in place, we can now define the effective robustness ρ(f , S, π) of a trained modelf to distribution shift (S, π) as If a modelf is truly more robust under distribution shift S, π (in particular, more so than standard models without a robustness intervention),f should have effective robustness ρ(f , S, π) substan- tially larger than 0. Graphically, ρ can be interpreted as the vertical deviation away from the linear fit in  Figure 1 . In the following, we will study to what extent various synthetic robustness measures are predictive of the effective robustness on natural distribution shifts, and which models achieve the largest effective robustness on natural distribution shifts.

Section Title: DOES SYNTHETIC ROBUSTNESS PREDICT NATURAL ROBUSTNESS?
  DOES SYNTHETIC ROBUSTNESS PREDICT NATURAL ROBUSTNESS? Given the difficulty of measuring a model's robustness to natural distribution shifts, an important question is whether there are simple synthetic proxies. We therefore study to what extent robustness to synthetic distribution shifts predicts robustness on natural distribution shift. A simple approach would be to directly compare different models' robustness to synthetic distri- bution shift and their robustness to natural distribution shift. However, we run into an important confounding factor. As mentioned earlier, models that have higher accuracy have a smaller accuracy drop on the natural distribution shifts we consider. Moreover, we find that models with higher accu- racy also see smaller accuracy drops under various synthetic distribution shifts. Therefore it would not be methodologically sound to directly compare synthetic robustness to natural robustness and draw any conclusions about transferability between the two. Simply the fact that a model is more accurate will simultaneously predict both that (i) the accuracy drop under natural distribution shift will be smaller, and that (ii) the model will be more robust to synthetic distribution shift. To control for the confounder standard accuracy, we rely on the effective robustness notion intro- duced in Section 3, i.e., the robustness increase that cannot be attributed to an increase in standard accuracy. In  Figure 2 , we analyze the relationship between two synthetic robustness notions and effective robustness on ImageNetV2. The resulting plot shows that the two quantities are almost entirely uncorrelated: a model being more robust to synthetic perturbations does not imply that the model will have a smaller accuracy drop on ImageNetV2. In Appendix A.1, we repeat the above experiment for the ImageNetVidRobust distribution shift and reach similar conclusions.

Section Title: WHAT HELPS WITH NATURAL DISTRIBUTION SHIFT?
  WHAT HELPS WITH NATURAL DISTRIBUTION SHIFT? As can be seen in  Figure 1 , most models did not achieve substantial effective robustness on ImageNetV2 and ImageNetVidRobust. Nevertheless, a few model types stood out with significant effective robustness on one of the two datasets (no model in our testbed that showed non-trivial gains for both natural distrbution shifts). We now discuss the main outliers in more detail.

Section Title: ResNet152-ImageNet11k and Instagram models
  ResNet152-ImageNet11k and Instagram models The left plot in  Figure 3  zooms to the two model families that displayed high effective robustness on ImageNetV2. All of these models are trained on additional data beyond the standard ILSVRC 2012 training set. The models exhibited an effective robustness of 2.0 and 1.4 (median robustness for the four Instagram models), which was substantially larger than any other model with similar accuracy. However, despite their promising performance on ImageNetV2, these models show little to no effective robustness for Ima- geNetVidRobust. Moreover, the effective robustness gain is larger for the ResNet152-ImageNet11k model trained on less data than the Instagram models. To further investigate the effect of data on model robustness, we conduct a series of controlled experiments in Section 6. p -robust models. In the right plot of  Figure 1  we see a cluster of p -adversarially robust classifiers that all exhibit substantial effective robustness for ImageNetVidRobust. However, the models are also in a low accuracy regime, in particular lower than the standard classifiers they were derived from (this trade-off is discussed in ( Tsipras et al., 2019 ;  Raghunathan et al., 2019 ) among others). This trade-off poses a potential confounding factor: although the p -robust models show high effec- tive robustness, it is not clear if the resulting accuracy trade-off is meaningful. To put the trade-off into context, we compare the models to a simple baseline. In particular, consider the family of classifiers given by interpolating between random guessing and the standard classifier the p -robust models are derived from. Since random guessing is not affected by the distribution shift, this interpo- lation can trade off standard accuracy for effective robustness. In the right plot of  Figure 3 , we illus- trate this family of interpolated classifiers (the dotted line) and show that a standard ResNet152 with no robustness interventions can be interpolated with a random classifier to achieve the same trade-off as an p -robust model. So for the purpose of the natural distribution shift on ImageNetVidRobust, the p -robust models offer no benefit beyond interpolating with a random classifier.

Section Title: HOW DOES THE AMOUNT OF TRAINING DATA IMPACT ROBUSTNESS?
  HOW DOES THE AMOUNT OF TRAINING DATA IMPACT ROBUSTNESS? In the previous section we have seen that the ResNet152-ImageNet11k and Instagram models achieve non-zero effective robustness on ImageNetV2. This is a plausible effect since larger training sets provide a more thorough sampling of real world images. However, the JFT-300M model does not display this effect despite being trained on 300× more data than standard models. A possible explanation could be that differences in label diversity or quality play a role in promoting robustness. For example, the Instagram models are trained on a dataset collected to overlap with the ILSVRC categories, and the ResNet152-ImageNet11k model is trained on a superset of the ILSVRC data. Meanwhile, the JFT model is trained on a long-tail, weakly-labeled dataset with 18,291 categories, which, to the best of our knowledge, were not collected to align with ILSVRC categories. To investigate the role of data in more detail, we now conduct two experiments.

Section Title: Varying the number of images per class
  Varying the number of images per class We start by subsampling the number of images per class in the ILSVRC training set by factors of {2, 4, 8, 16, 32} and show the impact on accuracy and robustness on ImageNetV2 in  Figure 4 . As expected, increasing the number of images per class consistently leads to higher accuracy. However, the impact on robustness is exactly as predicted by our fit on standard ImageNet models. This indicates that varying the the size of training sets in an i.i.d. manner affects accuracy but not effective robustness, at least for ImageNetV2.

Section Title: Varying the number of classes
  Varying the number of classes Next, we consider the diversity of the data by varying the set of classes. In particular, we create three successive subsets of the ILSVRC training set with 500, 250, and 125 classes (see Appendix A.6.5 for the list of classes). We then evaluate all models in our testbed on the 125 classes present in all subsets. We present results in  Figure 4 , which shows that varying the number of classes again affects accuracies but not effective robustness. Our experiments suggest that neither growing the number of images per class nor the number of classes in an i.i.d. fashion are effective robustness interventions. Nevertheless, Section 5 shows that larger dataset can provide meaningful robustness improvements. This disparity may be due to limi- tations of emulating changes in the training set by subsampling ILSVRC. For one, our experiments consider i.i.d. subsets of the training images or classes and do not consider growing datasets in a Under review as a conference paper at ICLR 2020 manner that may cover different image distributions, as in the case of Instagram data. Another pos- sibility is that increases in dataset size may only improve robustness after the dataset size is large enough such that the accuracy on the original distribution is close to saturated. Our experiments only observe dataset sizes smaller than ILSVRC, which may fall below this inflection point.

Section Title: RELATED WORK
  RELATED WORK There are several related papers that study the relationship between various synthetic robustness measure, e.g., ( Geirhos et al., 2018 ;  Engstrom et al., 2019 ;  Hendrycks & Dietterich, 2019 ;  Kang et al., 2019 ;  Tramer & Boneh, 2019 ;  Maini et al., 2019 ). To the best of our knowledge, our paper is the first to compare synthetic and natural distribution shifts. Since our findings show that synthetic robustness currently does not promote natural robustness, our paper offers a complementary view to the aforementioned comparisons focused on synthetic robustness. Our work relies on two recent papers introducing datasets with natural distribution shifts ( Recht et al., 2019 ;  Shankar et al., 2019 ).  Recht et al. (2019)  already note that there is a clear linear rela- tionship between original and new test set accuracy but do not evaluate any models with a robustness intervention or models trained on more data.  Shankar et al. (2019)  compare only a small set of robust models. Due to the limited size of their test set and model testbed, their results do not conclusively show that p -robust models offer effective robustness on their dataset. In contrast, our testbed of robustness interventions is substantially larger and aims to encompass all publicly available models. We also include models trained on more data and conduct experiments to investigate the effect of training data on robustness. While our experiments shows that p -robust models do offer effective robustness on ImageNetVidRobust, we also put this improvement in context by demonstrating that it is comparable to interpolating with a random classifier. Moreover, none of the two papers study to what extent synthetic robustness is predictive of robustness under natural distribution shifts.

Section Title: CONCLUSION
  CONCLUSION One aspiration of robustness interventions is to improve performance on real distribution shifts. Since this is a challenging problem, synthetic notions of robustness offer a natural starting point. While it is expected that corresponding interventions will be less effective on real distribution shifts, the hope is that these interventions still convey at least some robustness on real data. Unfortunately, our experiments show that this is not the case for two recent examples of real distribution shifts. On both datasets, models with a robustness intervention perform no better than baselines without such interventions. The fact that progress on synthetic distribution shifts is essentially uncorrelated with progress on real distribution shifts raises the question whether further improvements for synthetic robustness will yield benefits on real data. To deploy machine learning in safety-critical environments, it will be necessary to address this ques- tion and improve robustness under real distribution shifts. Our experiments suggests multiple direc- tions for future work to achieve this goal. One important direction is to assemble more examples of natural distribution shifts in order to test robustness on real data in more detail. While it is possible that other types of natural distribution shifts are more correlated with synthetic robustness, this hypothesis needs to be tested with carefully curated datasets. Our experiments on ImageNetV2 suggest that adding training data can improve robustness. A sim- ilar phenomenon has also been demonstrated for adversarial robustness ( Schmidt et al., 2018 ;  Car- mon et al., 2019 ;  Uesato et al., 2019 ;  Najafi et al., 2019 ;  Zhai et al., 2019 ). However, it is currently unclear what type of data to add: the JFT-300M model trained on 300× more data offers no robust- ness improvement. The ResNet152-ImageNet11k model was trained on roughly 10× more data and offers a robustness gain comparable or larger than the Instagram models trained on 1,000× more data. Understanding what additional training data is most effective could significantly improve ro- bustness to real distribution shifts. Finally, it is important to note that the synthetic robustness interventions do offer robustness on the distribution shifts they are designed for. This raises the hope that accurately characterizing real distribution shifts will allow us to leverage existing techniques for training more robust models under well-specified distribution shifts. The distribution shifts in our comparison and our extensive model testbed offer a starting point for such an investigation into improved training methodology. Under review as a conference paper at ICLR 2020
  We note that this corresponds to the problem of estimating a treatment effect as studied in causal inference. We do not explicitly rely on knowledge about causal inference to make our exposition more accessible but refer the reader to textbooks on causal inference for background ( Pearl et al., 2016 ;  Hernan & Robins, 2019 ).

```
