Title:
```
Published as a conference paper at ICLR 2020 OPTIMISTIC EXPLORATION EVEN WITH A PESSIMISTIC INITIALISATION
```
Abstract:
```
Optimistic initialisation is an effective strategy for efficient exploration in reinforce- ment learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initiali- sation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.
```

Figures/Tables Captions:
```
Figure 1 Consider the simple example with a single state and two actions shown in Figure 1. The left action yields +0.1 reward and the right action yields +1 reward. An agent whose Q-value estimates have been zero-initialised must at the first time step select an action randomly. As both actions are underestimated, this will increase the estimate of the chosen action. Greedy agents always pick the action with the largest Q-value estimate and will select the same action forever, failing to explore the alternative. Whether the agent learns the optimal policy or not is thus decided purely at random based on the initial Q-value estimates. This effect will only be amplified by intrinsic reward.
Figure 2: A simple regression task to illustrate the effect of an optimistic initialisation in neural networks. Left: 10 different networks whose final layer biases are initialised at 3 (shown in green), and the same networks after training on the blue data points (shown in red). Right: One of the trained networks whose output has been augmented with an optimistic bias as in equation 1. The counts were obtained by computing a histogram over the input space [−2, 2] with 50 bins.
Figure 3: Results for the randomised chain environment. Median across 20 seeds is plotted and the 25%-75% quartile is shown shaded. Left: OPIQ outperforms the baselines. Right: OPIQ is more stable than its ablations.
Figure 4: Results for the maze environment comparing OPIQ and baselines. Median across 8 seeds is plotted and the 25%-75% quartile is shown shaded. Left: The episode reward. Right: Number of distinct states visited over training. The total number of states in the environment is shown as a dotted line.
Figure 5: Results for the maze environment comparing OPIQ and ablations. Median across 8 seeds is plotted and the 25%-75% quartile is shown shaded. Left: The episode reward. Right: Number of distinct states visited over training. The total number of states in the environment is shown as a dotted line.
Figure 6: Values used during action selection for each of the 4 actions. The region in blue indicates states that have already been visited. Other colours denote Q-values between 0 (black) and 10 (white). Left: The Q-values used by DQN with pseudocounts. Right: Q + -values used by OPIQ with C action = 100.
Figure 7: Results for Montezuma's Revenge. Median across 4 seeds is plotted and the 25%-75% quartile is shown shaded. Left: The episode reward. Right: The maximum reward achieved during an episode.
Figure 8: Further Results for Montezuma's Revenge showing the number of rooms visited over training comparing OPIQ and baselines. Median across 4 seeds is plotted and the 25%-75% quartile is shown shaded.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION In reinforcement learning (RL), exploration is crucial for gathering sufficient data to infer a good control policy. As environment complexity grows, exploration becomes more challenging and simple randomisation strategies become inefficient. While most provably efficient methods for tabular RL are model-based (Brafman and Tennenholtz, 2002; Strehl and Littman, 2008; Azar et al., 2017), in deep RL, learning models that are useful for planning is notoriously difficult and often more complex (Hafner et al., 2019) than model- free methods. Consequently, model-free approaches have shown the best final performance on large complex tasks (Mnih et al., 2015; 2016; Hessel et al., 2018), especially those requiring hard exploration (Bellemare et al., 2016; Ostrovski et al., 2017). Therefore, in this paper, we focus on how to devise model-free RL algorithms for efficient exploration that scale to large complex state spaces and have strong theoretical underpinnings. Despite taking inspiration from tabular algorithms, current model-free approaches to exploration in deep RL do not employ optimistic initialisation, which is crucial to provably efficient exploration in all model-free tabular algorithms. This is because deep RL algorithms do not pay special attention to the initialisation of the neural networks and instead use common initialisation schemes that yield initial Q-values around zero. In the common case of non-negative rewards, this means Q-values are initialised to their lowest possible values, i.e., a pessimistic initialisation. While initialising a neural network optimistically would be trivial, e.g., by setting the bias of the final layer of the network, the uncontrolled generalisation in neural networks changes this initialisation quickly. Instead, to benefit exploration, we require the Q-values for novel state-action pairs must remain high until they are explored.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 An empirically successful approach to exploration in deep RL, especially when reward is sparse, is intrinsic motivation (Oudeyer and Kaplan, 2009). A popular variant is based on pseudocounts (Bellemare et al., 2016), which derive an intrinsic bonus from approximate visitation counts over states and is inspired by the tabular MBIE-EB algorithm (Strehl and Littman, 2008). However, adding a positive intrinsic bonus to the reward yields optimistic Q-values only for state-action pairs that have already been chosen sufficiently often. Incentives to explore unvisited states rely therefore on the generalisation of the neural network. Exactly how the network generalises to those novel state-action pairs is unknown, and thus it is unclear whether those estimates are optimistic when compared to nearby visited state-action pairs. To ensure optimism in unvisited, novel state-action pairs, we introduce Optimistic Pessimistically Initialised Q-Learning (OPIQ). OPIQ does not rely on an optimistic initialisation to ensure efficient exploration, but instead augments the Q-value estimates with count-based bonuses in the following manner: Q + (s, a) := Q(s, a) + C (N (s, a) + 1) M , (1) where N (s, a) is the number of times a state-action pair has been visited and M, C > 0 are hyperparameters. These Q + -values are then used for both action selection and during bootstrapping, unlike the above methods which only utilise Q-values during these steps. This allows OPIQ to maintain optimism when selecting actions and bootstrapping, since the Q + -values can be optimistic even when the Q-values are not. In the tabular domain, we base OPIQ on UCB-H (Jin et al., 2018), a simple online Q-learning algorithm that uses count-based intrinsic rewards and optimistic initialisation. Instead of optimistically initialising the Q-values, we pessimistically initialise them and use Q + -values during action selection and bootstrapping. Pessimistic initialisation is used to enable a worst case analysis where all of our Q-value estimates underestimate Q * and is not a requirement for OPIQ. We show that these modifications retain the theoretical guarantees of UCB-H. Furthermore, our algorithm easily extends to the Deep RL setting. The primary difficulty lies in obtaining appropriate state-action counts in high-dimensional and/or continuous state spaces, which has been tackled by a variety of approaches (Bellemare et al., 2016; Ostrovski et al., 2017; Tang et al., 2017; Machado et al., 2018a) and is orthogonal to our contributions. We demonstrate clear performance improvements in sparse reward tasks over 1) a baseline DQN that just uses intrinsic motivation derived from the approximate counts, 2) simpler schemes that aim for an optimistic initialisation when using neural networks, and 3) strong exploration baselines. We show the importance of optimism during action selection for ensuring efficient exploration. Visualising the predicted Q + -values shows that they are indeed optimistic for novel state-action pairs.

Section Title: BACKGROUND
  BACKGROUND We consider a Markov Decision Process (MDP) defined as a tuple (S, A, P, R), where S is the state space, A is the discrete action space, P (·|s, a) is the state-transition distribution, R(·|s, a) is the distribution over rewards and γ ∈ [0, 1) is the discount factor. The goal of the agent is then to maximise the expected discounted sum of rewards: E[ ∞ t=0 γ t r t |r t ∼ R(·|s t , a t )], in the discounted episodic setting. A policy π(·|s) is a mapping from states to actions such that it is a valid probability distribution. Deep Q-Network (DQN) (Mnih et al., 2015) uses a nonlinear function approximator (a deep neural network) to estimate the action-value function, Q(s, a; θ) ≈ Q * (s, a), where θ are the parameters of the network. Exploration based on intrinsic rewards (e.g., Bellemare et al., 2016), which uses a DQN agent, additionally augments the observed rewards r t with a bonus β/ N (s t , a t ) based on Published as a conference paper at ICLR 2020 pseudo-visitation-counts N (s t , a t ). The DQN parameters θ are trained by gradient descent on the mean squared regression loss L with bootstrapped 'target' y t : The expectation is estimated with uniform samples from a replay buffer D (Lin, 1992). D stores past transitions (s t , a t , r t , s t+1 ), where the state s t+1 is observed after taking the action a t in state s t and receiving reward r t . To improve stability, DQN uses a target network, parameterised by θ − , which is periodically copied from the regular network and kept fixed for a number of iterations.

Section Title: OPTIMISTIC PESSIMISTICALLY INITIALISED Q-LEARNING
  OPTIMISTIC PESSIMISTICALLY INITIALISED Q-LEARNING Our method Optimistic Pessimistically Initialised Q-Learning (OPIQ) ensures optimism in the Q- value estimates of unvisited, novel state-action pairs in order to drive exploration. This is achieved by augmenting the Q-value estimates in the following manner: Q + (s, a) := Q(s, a) + C (N (s, a) + 1) M , and using these Q + -values during action selection and bootstrapping. In this section, we motivate OPIQ, analyse it in the tabular setting, and describe a deep RL implementation.

Section Title: MOTIVATIONS
  MOTIVATIONS Optimistic initialisation does not work with neural networks. For an optimistic initialisation to benefit exploration, the Q-values must start sufficiently high. More importantly, the values for unseen state-action pairs must remain high, until they are updated. When using a deep neural network to approximate the Q-values, we can initialise the network to output optimistic values, for example, by adjusting the final bias. However, after a small amount of training, the values for novel state-action pairs may not remain high. Furthermore, due to the generalisation of neural networks we cannot know how the values for these unseen state-action pairs compare to the trained state-action pairs.  Figure 2  (left), which illustrates this effect for a simple regression task, shows that different initialisations can lead to dramatically different generalisations. It is therefore prohibitively difficult to use optimistic initialisation of a deep neural network to drive exploration. Instead, we augment our Q-value estimates with an optimistic bonus. Our motivation for the form of the bonus in equation 1, C (N (s,a)+1) M , stems from UCB-H (Jin et al., 2018), where all tabular Q-values are initialised with H and the first update for a state-action pair completely overwrites that value because the learning rate for the update (η 1 ) is 1. One can alternatively view these Q-values as zero-initialised with the additional term Q(s, a) + H · 1{N(s, a) < 1}, where N (s, a) is the visitation count for the state-action pair (s, a). Our approach approximates the discrete indicator Published as a conference paper at ICLR 2020 end end function 1 as (N (s, a) + 1) −M for sufficiently large M . However, since gradient descent cannot completely overwrite the Q-value estimate for a state-action pair after a single update, it is beneficial to have a smaller hyperparameter M that governs how quickly the optimism decays. For a worst case analysis we assume all Q-value estimates are pessimistic. In the common scenario where all rewards are nonnegative, the lowest possible return for an episode is zero. If we then zero-initialise our Q-value estimates, as is common for neural networks, we are starting with a pessimistic initialisation. As shown in  Figure 2 (left), we cannot predict how a neural network will generalise, and thus we cannot predict if the Q-value estimates for unvisited state-action pairs will be optimistic or pessimistic. We thus assume they are pessimistic in order to perform a worst case analysis. However, this is not a requirement: our method works with any initialisation and rewards. In order to then approximate an optimistic initialisation, the scaling parameter C in equation 1 can be chosen to guarantee unseen Q + -values are overestimated, for example, C := H in the undiscounted finite-horizon tabular setting and C := 1/(1 − γ) in the discounted episodic setting (assuming 1 is the maximum reward obtainable at each timestep). However, in some environments it may be beneficial to use a smaller parameter C for faster convergence. These Q + -values are then used both during action selection and during bootstrapping. Note that in the finite horizon setting the counts N , and thus Q + , would depend on the timestep t. Hence, we split the optimistic Q + -values into two parts: a pessimistic Q-value component and an optimistic component based solely on the counts for a state-action pair. This separates our source of optimism from the neural network function approximator, yielding Q + -values that remain high for unvisited state-action pairs, assuming a suitable counting scheme.  Figure 2  (right) shows the effects of adding this optimistic component to a network's outputs. Optimistic Q + -values provide an increased incentive to explore. By using optimistic Q + esti- mates, especially during action selection and bootstrapping, the agent is incentivised to try and visit novel state-action pairs. Being optimistic during action selection in particular encourages the agent to try novel actions that have not yet been visitied. Without an optimistic estimate for novel state-action pairs the agent would have no incentive to try an action it has never taken before at a given state. Being optimistic during bootstrapping ensures the agent is incentivised to return to states in which it has not yet tried every action. This is because the maximum Q + -value will be large due to the optimism bonus. Both of these effects lead to a strong incentive to explore novel state-action pairs.

Section Title: TABULAR REINFORCEMENT LEARNING
  TABULAR REINFORCEMENT LEARNING In order to ensure that OPIQ has a strong theoretical foundation, we must ensure it is provably efficient in the tabular domain. We restrict our analysis to the finite horizon tabular setting and only consider building upon UCB-H (Jin et al., 2018) for simplicity. Achieving a better regret bound using UCB-B (Jin et al., 2018) and extending the analysis to the infinite horizon discounted setting (Dong et al., 2019) are steps for future work. Our algorithm removes the optimistic initialisation of UCB-H, instead using a pessimistic initialisation (all Q-values start at 0). We then use our Q + -values during action selection and bootstrapping. Pseudocode is presented in Algorithm 1. Published as a conference paper at ICLR 2020 Theorem 1. For any p ∈ (0, 1) , with probability at least 1 − p the total regret of Q + is at most The proof is based on that of Theorem 1 from (Jin et al., 2018). Our Q + -values are always greater than or equal to the Q-values that UCB-H would estimate, thus ensuring that our estimates are also greater than or equal to Q * . Our overestimation relative to UCB-H is then governed by the quantity H/(N (s, a) + 1) M , which when summed over all timesteps does not depend on T for M > 1. As M → ∞ we exactly recover UCB-H, and match the asymptotic performance of UCB-H for M ≥ 1. Smaller values of M result in our optimism decaying more slowly, which results in more exploration. The full proof is included in Appendix I. We also show that OPIQ without optimistic action selection or the count-based intrinsic motivation term b T N is not provably efficient by showing it can incur linear regret with high probability on simple MDPs (see Appendices G and H). Our primary motivation for considering a tabular algorithm that pessimistically initialises its Q-values, is to provide a firm theoretical foundation on which to base a deep RL algorithm, which we describe in the next section.

Section Title: DEEP REINFORCEMENT LEARNING
  DEEP REINFORCEMENT LEARNING For deep RL, we base OPIQ on DQN (Mnih et al., 2015), which uses a deep neural network with parameters θ as a function approximator Q θ . During action selection, we use our Q + -values to determine the greedy action: a t = arg max a Q θ (s, a) + C action (N (s, a) + 1) M , (3) where C action is a hyperparameter governing the scale of the optimistic bias during action selection. In practice, we use an -greedy policy. After every timestep, we sample a batch of experiences from our experience replay buffer, and use n-step Q-learning (Mnih et al., 2016). We recompute the counts for each relevant state-action pair, to avoid using stale pseudo-rewards. The network is trained by gradient decent on the loss in equation 2 with the target: y t := n−1 i=0 γ i r(s t+i , a t+i )+ β √ N (st+i,at+i) + γ n max a Q θ − (s t+n , a )+ C bootstrap (N (s t+n , a ) + 1) M . (4) where C bootstrap is a hyperparameter that governs the scale of the optimistic bias during bootstrapping. For our final experiments on Montezuma's Revenge we additionally use the Mixed Monte Carlo (MMC) target (Bellemare et al., 2016; Ostrovski et al., 2017), which mixes the target with the environmental monte carlo return for that episode. Further details are included in Appendix D.4. We use the method of static hashing (Tang et al., 2017) to obtain our pseudocounts on the first 2 of 3 environments we test on. For our experiments on Montezuma's Revenge we count over a downsampled image of the current game frame. More details can be found in Appendix B. A DQN with pseudocount derived intrinsic reward (DQN + PC) (Bellemare et al., 2016) can be seen as a naive extension of UCB-H to the deep RL setting. However, it does not attempt to ensure optimism in the Q-values used during action selection and bootstrapping, which is a crucial component of UCB- H. Furthermore, even if the Q-values were initialised optimistically at the start of training they would not remain optimistic long enough to drive exploration, due to the use of neural networks. OPIQ, on the other hand, is designed with these limitations of neural networks in mind. By augmenting the neural network's Q-value estimates with optimistic bonuses of the form C (N (s,a)+1) M , OPIQ ensures that the Q + -values used during action selection and bootstrapping are optimistic. We can thus consider OPIQ as a deep version of UCB-H. Our results show that optimism during action selection and bootstrapping is extremely important for ensuring efficient exploration.

Section Title: RELATED WORK
  RELATED WORK Tabular Domain: There is a wealth of literature related to provably efficient exploration in the tabular domain. Popular model-based algorithms such as R-MAX (Brafman and Tennenholtz, 2002), Published as a conference paper at ICLR 2020 MBIE (and MBIE-EB) (Strehl and Littman, 2008), UCRL2 (Jaksch et al., 2010) and UCBVI (Azar et al., 2017) are all based on the principle of optimism in the face of uncertainty. Osband and Van Roy (2017) adopt a Bayesian viewpoint and argue that posterior sampling (PSRL) (Strens, 2000) is more practically efficient than approaches that are optimistic in the face of uncertainty, and prove that in Bayesian expectation PSRL matches the performance of any optimistic algorithm up to constant factors. Agrawal and Jia (2017) prove that an optimistic variant of PSRL is provably efficient under a frequentist regret bound. The only provably efficient model-free algorithms to date are delayed Q-learning (Strehl et al., 2006) and UCB-H (and UCB-B) (Jin et al., 2018). Delayed Q-learning optimistically initialises the Q-values that are carefully controlled when they are updated. UCB-H and UCB-B also optimistically initialise the Q-values, but also utilise a count-based intrinsic motivation term and a special learning rate to achieve a favourable regret bound compared to model-based algorithms. In contrast, OPIQ pessimistically initialises the Q-values. Whilst we base our current analysis on UCB-H, the idea of augmenting pessimistically initialised Q-values can be applied to any model-free algorithm.

Section Title: Deep RL Setting
  Deep RL Setting A popular approach to improving exploration in deep RL is to utilise intrinsic motivation (Oudeyer and Kaplan, 2009), which computes a quantity to add to the environmental reward. Most relevant to our work is that of Bellemare et al. (2016), which takes inspiration from MBIE-EB (Strehl and Littman, 2008). Bellemare et al. (2016) utilise the number of times a state has been visited to compute the intrinsic reward. They outline a framework for obtaining approximate counts, dubbed pseudocounts, through a learned density model over the state space. Ostrovski et al. (2017) extend the work to utilise a more expressive PixelCNN (van den Oord et al., 2016) as the density model, whereas Fu et al. (2017) train a neural network as a discriminator to also recover a density model. Machado et al. (2018a) instead use the successor representation to obtain generalised counts. Choi et al. (2019) learn a feature space to count that focusses on regions of the state space the agent can control, and Pathak et al. (2017) learn a similar feature space in order to provide the error of a learned model as intrinsic reward. A simpler and more generic approach to approximate counting is static hashing which projects the state into a lower dimensional space before counting (Tang et al., 2017). None of these approaches attempt to augment or modify the Q-values used for action-selection or bootstrapping, and hence do not attempt to ensure optimistic values for novel state-action pairs. Chen et al. (2017) build upon bootstrapped DQN (Osband et al., 2016) to obtain uncertainty estimates over the Q-values for a given state in order to act optimistically by choosing the action with the largest UCB. However, they do not utilise optimistic estimates during bootstrapping. Osband et al. (2018) also extend bootstrapped DQN to include a prior by extending RLSVI (Osband et al., 2017) to deep RL. Osband et al. (2017) show that RLSVI achieves provably efficient Bayesian expected regret, which requires a prior distribution over MDPs, whereas OPIQ achieves provably efficient worse case regret. Bootstrapped DQN with a prior is thus a model-free algorithm that has strong theoretical support in the tabular setting. Empirically, however, its performance on sparse reward tasks is worse than DQN with pseudocounts. Machado et al. (2015) shift and scale the rewards so that a zero-initialisation is optimistic. When applied to neural networks this approach does not result in optimistic Q-values due to the generali- sation of the networks. Bellemare et al. (2016) empirically show that using a pseudocount intrinsic motivation term performs much better empirically on hard exploration tasks. Choshen et al. (2018) attempt to generalise the notion of a count to include information about the counts of future state-actions pairs in a trajectory, which they use to provide bonuses during action selection. Oh and Iyengar (2018) extend delayed Q-learning to utilise these generalised counts and prove the scheme is PAC-MDP. The generalised counts are obtained through E-values which are learnt using SARSA with a constant 0 reward and E-value estimates initialised at 1. When scaling to the deep RL setting, these E-values are estimated using neural networks that cannot maintain their initialisation for unvisited state-action pairs, which is crucial for providing an incentive to explore. By contrast, OPIQ uses a separate source to generate the optimism necessary to explore the environment.

Section Title: EXPERIMENTAL SETUP
  EXPERIMENTAL SETUP We compare OPIQ against baselines and ablations on three sparse reward environments. The first is a randomized version of the Chain environment proposed by Osband et al. (2016) and used in (Shyam et al., 2019) with a chain of length 100, which we call Randomised Chain. The second is a two-dimensional maze in which the agent starts in the top left corner (white dot) and is only rewarded upon finding the goal (light grey dot). We use an image of the maze as input and randomise the actions similarly to the chain. The third is Montezuma's Revenge from Arcade Learning environment (Bellemare et al., 2013), a notoriously difficult sparse reward environment commonly used as a benchmark to evaluate the performance and scaling of Deep RL exploration algorithms. See Appendix D for further details on the environments, baselines and hyperparameters used. 1

Section Title: ABLATIONS AND BASELINES
  ABLATIONS AND BASELINES We compare OPIQ against a variety of DQN-based approaches that use pseudocount intrinsic rewards, the DORA agent (Choshen et al., 2018) (which generates count-like optimism bonuses using a neural network), and two strong exploration baselines: -greedy DQN: a standard DQN that uses an -greedy policy to encourage exploration. We anneal linearly over a fixed number of timesteps from 1 to 0.01. DQN + PC: we add an intrinsic reward of β/ N (s, a) to the environmental reward based on (Bellemare et al., 2016; Tang et al., 2017). DQN R-Subtract (+PC): we subtract a constant from all environmental rewards received when training, so that a zero-initialisation is optimistic, as described for a DQN in (Bellemare et al., 2016) and based on Machado et al. (2015). DQN Bias (+PC): we initialise the bias of the final layer of the DQN to a positive value at the start of training as a simple method for optimistic initialisation with neural networks. DQN + DORA: we use the generalised counts from (Choshen et al., 2018) as an intrinsic reward. DQN + DORA OA: we additionally use the generalised counts to provide an optimistic bonus during action selection. DQN + RND: we add the RND bonus from (Burda et al., 2018) as an intrinsic reward. BSP: we use Bootstrapped DQN with randomised prior functions (Osband et al., 2018). In order to better understand the importance of each component of our method, we also evaluate the following ablations: Optimistic Action Selection (OPIQ w/o OB): we only use our Q + -values during action selection, and use Q during bootstrapping (without Optimistic Bootstrapping). The intrinsic motivation term remains. Optimistic Action Selection and Bootstrapping (OPIQ w/o PC): we use our Q + -values during action selection and bootstrapping, but do not include an intrinsic motivation term (without Pseudo Counts).

Section Title: RESULTS
  RESULTS

Section Title: RANDOMISED CHAIN
  RANDOMISED CHAIN We first consider the visually simple domain of the randomised chain and compare the count-based methods.  Figure 3  shows the performance of OPIQ compared to the baselines and ablations. OPIQ significantly outperforms the baselines, which do not have any explicit mechanism for optimism during action selection. A DQN with pseudocount derived intrinsic rewards is unable to reliably find the goal state, but setting the final layer's bias to one produces much better performance. For the DQN variant in which a constant is subtracted from all rewards, all of the configurations (including those with pseudocount derived intrinsic bonuses) were unable to find the goal on the right and thus the agents learn quickly to latch on the inferior reward of moving left. Compared to its ablations, OPIQ is more stable in this task. OPIQ without pseudocounts performs similarly to OPIQ but is more varied across seeds, whereas the lack of optimistic bootstrapping results in worse performance and significantly more variance across seeds.

Section Title: MAZE
  MAZE We next consider the harder and more visually complex task of the Maze and compare against all baselines.  Figure 4  shows that only OPIQ is able to find the goal in the sparse reward maze. This indicates that explicitly ensuring optimism during action selection and bootstrapping can have a significant positive impact in sparse reward tasks, and that a naive extension of UCB-H to the deep RL setting (DQN + PC) results in insufficient exploration.  Figure 4  (right) shows that attempting to ensure optimistic Q-values by adjusting the bias of the final layer (DQN Bias + PC), or by subtracting a constant from the reward (DQN R-Subtract + PC) has very little effect. As expected DQN + RND performs poorly on this domain compared to the pseudocount based methods. The visual input does not vary much across the state space, resulting in the RND bonus failing to provide enough intrinsic motivation to ensure efficient exploration. Additionally it does not feature any explicit mechanism for optimism during action selection, and thus  Figure 4  (right) shows it explores the environment relatively slowly. Both DQN+DORA and DQN+DORA OA also perform poorly in this domain since their source of intrinsic motivation disappears quickly. As noted in  Figure 2 , neural networks do not maintain their starting initialisations after training. Thus, the intrinsic reward DORA produces goes to 0 quickly since the network producing its bonuses learns to generalise quickly. BSP is the only exploration baseline we test that does not add an intrinsic reward to the environmental reward, and thus it performs poorly compared to the other baselines on this environment.  Figure 5  shows that OPIQ and all its ablations manage to find the goal in the maze. OPIQ also explores slightly faster than its ablations (right), which shows the benefits of optimism during both action selection and bootstrapping. In addition, the episodic reward for the the ablation without optimistic bootstrapping is noticeably more unstable ( Figure 5 , left). Interestingly, OPIQ without pseudocounts performs significantly worse than the other ablations. This is surprising since the theory suggests that the count-based intrinsic motivation is only required when the reward or transitions of the MDP are stochastic (Jin et al., 2018), which is not the case here. We hypothesise that adding PC-derived intrinsic bonuses to the reward provides an easier learning problem, especially when using n-step Q-Learning, which yields the performance gap. However, our results show that the PC-derived intrinsic bonuses are not enough on their own to ensure sufficient exploration. The large difference in performance between DQN + PC and OPIQ w/o OB is important, since they only differ in the use of optimistic action selection. The results in  Figures 4  and 5 show that optimism during action selection is extremely important in exploring the environment efficiently. Intuitively, this makes sense, since this provides an incentive for the agent to try actions it has never tried before, which is crucial in exploration.  Figure 6  visualises the values used during action selection for a DQN + PC agent and OPIQ, showing the count-based augmentation provides optimism for relatively novel state-action pairs, driving the agent to explore more of the state-action space.

Section Title: MONTEZUMA'S REVENGE
  MONTEZUMA'S REVENGE Finally, we consider Montezuma's Revenge, one of the hardest sparse reward games from the ALE (Bellemare et al., 2013). Note that we only train up to 12.5mil timesteps (50mil frames), a 1/4 of the usual training time (50mil timesteps, 200mil frames).  Figure 7  shows that OPIQ significantly outperforms the baselines in terms of the episodic reward and the maximum episodic reward achieved during training. The higher episode reward and much higher maximum episode reward of OPIQ compared to DQN + PC once again demonstrates the importance of optimism during action selection and bootstrapping. In this environment BSP performs much better than in the Maze, but achieves significantly lower episodic rewards than OPIQ.  Figure 8  shows the distinct number of rooms visited across the training period. We can see that OPIQ manages to reliably explore 12 rooms during the 12.5mil timesteps, significantly more than the other methods, thus demonstrating its improved exploration in this complex environment. Our results on this challenging environment show that OPIQ can scale to high dimensional complex environments and continue to provide significant performance improvements over an agent only using pseudocount based intrinsic rewards.

Section Title: CONCLUSIONS AND FUTURE WORK
  CONCLUSIONS AND FUTURE WORK This paper presented OPIQ, a model-free algorithm that does not rely on an optimistic initialisation to ensure efficient exploration. Instead, OPIQ augments the Q-values estimates with a count-based optimism bonus. We showed that this is provably efficient in the tabular setting by modifying UCB-H to use a pessimistic initialisation and our augmented Q + -values for action selection and bootstrapping. Since our method does not rely on a specific initialisation scheme, it easily scales to deep RL when paired with an appropriate counting scheme. Our results showed the benefits of maintaining optimism both during action selection and bootstrapping for exploration on a number of hard sparse reward environments including Montezuma's Revenge. In future work, we aim to extend OPIQ by integrating it with more expressive counting schemes.

```
