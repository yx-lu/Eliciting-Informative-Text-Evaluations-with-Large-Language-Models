Title:
```
None
```
Abstract:
```
Abstract
```

Figures/Tables Captions:
```
Figure 1: Image-based continuous control tasks from the DeepMind control suite (Tassa et al., 2018) used in our experiments. Each task offers an unique set of challenges, including complex dynamics, sparse rewards, hard exploration, and more. Refer to Appendix A for more information.
Figure 2: Separate β-VAE and policy training with no shared gradients SAC+VAE:pixel (iter, N ), with SAC:state shown as an upper bound. N refers to frequency in environment steps at which the β-VAE updates after initial pretraining. More frequent updates are beneficial for learning better representations, but cannot fully address the gap in performance. Full results in Appendix C.
Figure 3: An unsuccessful attempt to propagate gradients from the actor-critic down to the encoder of the β-VAE to enable end-to-end off-policy training. The learning process of SAC+VAE:pixel exhibits instability together with the subpar performance comparing to the baseline SAC+VAE:pixel (iter, 1), which does not share gradients with the actor-critic. Full results in Appendix D.
Figure 4: Our algorithm (SAC+AE) auguments SAC (Haarnoja et al., 2018) with a regularized autoen- coder (Ghosh et al., 2019) to achieve stable end-to-end training from images in the off-policy regime. The stability comes from switching to a deterministic encoder that is carefully updated with gradients from the reconstruction J(AE) (Equation (3)) and soft Q-learning J(Q) (Equation (1)) objectives.
Figure 5: The main result of our work. Our method demonstrates significantly improved performance over the baseline SAC:pixel. Moreover, it matches the state-of-the-art performance of model-based algorithms, such as PlaNet (Hafner et al., 2018) and SLAC (Lee et al., 2019), as well as a model-free algorithm D4PG (Barth- Maron et al., 2018), that also learns from raw images. Our algorithm exhibits stable learning across ten random seeds and is extremely easy to implement.
Figure 6: Linear projections of latent representation spaces learned by our method (SAC+AE:pixel) and the baseline (SAC:pixel) onto proprioceptive states. We compare ground truth value of each proprioceptive co- ordinate against their reconstructions for cheetah run, and conclude that our method successfully encodes proprioceptive state information. For visual clarity we only plot 2 position (out of 8) and 2 velocity (out of 9) coordinates. Full results in Appendix F.
Figure 7: Encoder pretrained with our method (SAC+AE:pixel) on walker walk is able to generalize to unseen walker stand and walker run tasks. All three tasks share similar image observations, but have quite different reward structure. SAC with a pretrained on walker walk encoder achieves impressive final performance, while the baseline struggles to solve the tasks.
Table 1: A comparison over 6 DMC tasks of SAC from pixels, PlaNet, SLAC, and an upper bound of SAC from proprioceptive state, numbers are averaged over the last 5 episodes across 10 seeds. The large performance gap between SAC:pixel and SAC:state motivates us to address the representation learning bottleneck in model- free off-policy RL. Best performance bolded.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Cameras are a convenient and inexpensive way to acquire state information, especially in complex, unstructured environments, where effective control requires access to the proprioceptive state of the underlying dynamics. Thus, having effective RL approaches that can utilize pixels as input would potentially enable solutions for a wide range of real world problems. The challenge is to efficiently learn a mapping from pixels to an appropriate representation for con- trol using only a sparse reward signal. Although deep convolutional encoders can learn good repre- sentations (upon which a policy can be trained), they require large amounts of training data. As exist- ing reinforcement learning approaches already have poor sample complexity, this makes direct use of pixel-based inputs prohibitively slow. For example, model-free methods on Atari ( Bellemare et al., 2013 ) and DeepMind Control (DMC) ( Tassa et al., 2018 ) take tens of millions of steps ( Mnih et al., 2013 ;  Barth-Maron et al., 2018 ), which is impractical in many applications, especially robotics. A natural solution is to add an auxiliary task with an unsupervised objective to improve sample effi- ciency. The simplest option is an autoencoder with a pixel reconstruction objective. Prior work has attempted to learn state representations from pixels with autoencoders, utilizing a two-step training procedure, where the representation is first trained via the autoencoder, and then either with a policy learned on top of the fixed representation ( Lange & Riedmiller, 2010 ;  Munk et al., 2016 ;  Higgins et al., 2017b ;  Zhang et al., 2018 ;  Nair et al., 2018 ), or with planning ( Mattner et al., 2012 ;  Finn et al., 2015 ). This allows for additional stability in optimization by circumventing dueling training objectives but leads to suboptimal policies. Other work utilizes end-to-end model-free learning with an auxiliary reconstruction signal in an on-policy manner ( Jaderberg et al., 2017 ). We revisit the concept of adding an autoencoder to model-free RL approaches, but with a focus on off-policy algorithms. We perform a sequence of careful experiments to understand why previous approaches did not work well. We found that a pixel reconstruction loss is vital for learning a good representation, specifically when trained end-to-end. Based on these findings, we propose a simple autoencoder-based off-policy method that can be trained end-to-end. Our method is the first model- free off-policy algorithm to successfully train simultaneously both the latent state representation and policy in a stable and sample-efficient manner. Of course, some recent state-of-the-art model-based RL methods ( Hafner et al., 2018 ;  Lee et al., 2019 ) have demonstrated superior sample efficiency to leading model-free approaches on pixel tasks from ( Tassa et al., 2018 ). But we find that our model-free, off-policy, autoencoder-based approach is able to match their performance, closing the gap between model-based and model-free approaches in image-based RL, despite being a far simpler method that does not require a world model. This paper makes three main contributions: (i) a demonstration that adding a simple auxiliary recon- struction loss to a model-free off-policy RL algorithm achieves comparable results to state-of-the-art model-based methods on the suite of continuous control tasks from  Tassa et al. (2018) ; (ii) an under- standing of the issues involved with combining autoencoders with model-free RL in the off-policy setting that guides our algorithm; and (iii) an open-source PyTorch implementation of our simple method for researchers and practitioners to use as a strong baseline that may easily be built upon.

Section Title: RELATED WORK
  RELATED WORK Efficient learning from high-dimensional pixel observations has been a problem of paramount im- portance for model-free RL. While some impressive progress has been made applying model-free RL to domains with simple dynamics and discrete action spaces ( Mnih et al., 2013 ), attempts to scale these approaches to complex continuous control environments have largely been unsuccessful, both in simulation and the real world. A glaring issue is that the RL signal is much sparser than in supervised learning, which leads to sample inefficiency, and higher dimensional observation spaces such as pixels worsens this problem. One approach to alleviate this problem is by training with auxiliary losses. Early work ( Lange & Riedmiller, 2010 ) explores using deep autoencoders to learn feature spaces in visual reinforcement learning, crucially  Lange & Riedmiller (2010)  propose to recompute features for all collected expe- riences after each update of the autoencoder, rendering this approach impractical to scale to more complicated domains. Moreover, this method has been only demonstrated on toy problems. Alter- natively,  Finn et al. (2015)  apply deep autoencoder pretraining to real world robots that does not require iterative re-training, improving upon computational complexity of earlier methods. How- ever, in this work the linear policy is trained separately from the autoencoder, which we find to not perform as well as end-to-end methods.  Shelhamer et al. (2016)  use auxiliary losses in Atari that incorporate forward and inverse dynamics with A3C, an on-policy algorithm. They recommend a multi-task setting and learning dynamics and reward to find a good representation, which relies on the assumption that the dynamics in the task are easy to learn and useful for learning a good policy.  Jaderberg et al. (2017)  propose to use unsupervised auxiliary tasks, both observation-based and reward-based based off of real world in- ductive priors, and show improvements in Atari, again in the on-policy regime, which is much more stable for learning. Unfortunately, this work also relies on inductive biases by designing internal rewards to learn a good representation which is hard to scale to the real world problems.  Higgins et al. (2017b) ;  Nair et al. (2018)  use a beta variational autoencoder (β-VAE) ( Kingma & Welling, 2013 ;  Higgins et al., 2017a ) and attempt to extend unsupervised representation pretraining to the off-policy setting, but find it hard to perform end-to-end training, thus receding to the iterative re- training procedure ( Lange & Riedmiller, 2010 ;  Finn et al., 2015 ). There has been more success in using model-based methods on images, such as  Hafner et al. (2018) ;  Lee et al. (2019) . These methods use a world model ( Ha & Schmidhuber, 2018 ) approach, learning a representation space using a latent dynamics loss and pixel decoder loss to ground on the original observation space. These model-based reinforcement learning methods often show improved sam- ple efficiency, but with the additional complexity of balancing various auxiliary losses, such as a dynamics loss, reward loss, and decoder loss in addition to the original policy and value optimiza- Under review as a conference paper at ICLR 2020 tions. These proposed methods are correspondingly brittle to hyperparameter settings, and difficult to reproduce, as they balance multiple training objectives. To close the gap between model-based and model-free image-based RL in terms of sample efficiency and sidestep the issues of model learning, our goal is to train a model-free off-policy algorithm with auxiliary reconstruction loss in a stable manner.

Section Title: BACKGROUND
  BACKGROUND A fully observable Markov decision process (MDP) is described by tuple S, A, P, R, γ , where S is the state space, A is the action space, P (s t+1 |s t , a t ) is the probability distribution over transitions, R(s t , a t , s t+1 ) is the reward function, and γ is the discount factor ( Bellman, 1957 ). An agent starts in a initial state s 1 sampled from a fixed distribution p(s 1 ), then at each timestep t it takes an action a t ∈ A from a state s t ∈ S and moves to a next state s t+1 ∼ P (·|s t , a t ). After each action the agent receives a reward r t = R(s t , a t , s t+1 ). We consider episodic environments with the length fixed to T . The goal of standard RL is to learn a policy π(a t |s t ) that can maximize the agent's expected cumulative reward T t=1 E (st,at)∼ρπ [r t ], where ρ π is a state-action marginal distribution induced by the policy π(a t |s t ) and transition distribution P (s t+1 |s t , a t ). An important modification ( Ziebart et al., 2008 ) auguments this objective with an entropy term H(π(·|s t )) to encourage exploration and robustness to noise. The resulting maximum entropy objective is then defined as: π * = arg max π T t=1 E (st,at)∼ρπ [r t + αH(π(·|s t ))], where α is a temperature parameter that balances between optimizing for the reward and for the stochasticity of the policy. We build on Soft Actor-Critic (SAC) ( Haarnoja et al., 2018 ), an off-policy actor-critic method that uses the maximum entropy framework to derive soft policy iteration. At each iteration SAC performs a soft policy evaluation step and a soft policy improvement step. The soft policy evaluation step fits a parametric soft Q-function Q(s t , a t ) (critic) by minimizing the soft Bellman residual: J(Q) = E (st,at,rt,st+1)∼D Q(s t , a t ) − r t − γE at+1∼π Q (s t+1 , a t+1 ) − α log π(a t+1 |s t+1 ) 2 , (1) where D is the replay buffer, andQ is the target soft Q-function parametrized by a weight vector obtained using the exponentially moving average of the soft Q-function weights to stabilize training. The soft policy improvement step then attempts to learn a parametric policy π(a t |s t ) (actor) by directly minimizing the KL divergence between the policy and a Boltzmann distribution induced by the current soft Q-function, producing the following objective: The policy π(a t |s t ) is parametrized as a diagonal Gaussian to handle continuous action spaces. When learning from raw images, we deal with the problem of partial observability, which is formal- ized by a partially observable MDP (POMDP). In this setting, instead of getting a low-dimensional state s t ∈ S at time t, the agent receives a high-dimensional observation o t ∈ O, which is a render- ing of potentially incomplete view of the corresponding state s t of the environment ( Kaelbling et al., 1998 ). This complicates applying RL as the agent now needs to also learn a compact latent represen- tation to infer the state. Fitting a high-capacity encoder using only a scarce reward signal is sample inefficient and prone to suboptimal convergence. Following prior work ( Lange & Riedmiller, 2010 ;  Finn et al., 2015 ) we explore unsupervised pretraining via an image-based autoencoder. In practice, the autoencoder is represented as a convolutional encoder f enc that maps an image observation o t to a low-dimensional latent vector z t , and a deconvolutional decoder f dec that reconstructs z t back to the original image o t . The optimization is done by minimizing the standard reconstruction objective: Or in the case of β-VAE ( Kingma & Welling, 2013 ;  Higgins et al., 2017a ), where the variational distribution is parametrized as diagonal Gaussian, the objective is defined as: The latent vector z t is then used by an RL algo- rithm, such as SAC, instead of the unavailable true state s t . To infer temporal statistics, such as velocity and acceleration, it is common practice to stack three consecutive frames to form a sin- gle observation ( Mnih et al., 2013 ). We emphasize that in contrast to model-based methods ( Ha & Schmidhuber, 2018 ;  Hafner et al., 2018 ), we do not predict future states and solely focus on learning representations from the current observation to stay model-free.

Section Title: A DISSECTION OF LEARNING STATE REPRESENTATIONS WITH β-VAE
  A DISSECTION OF LEARNING STATE REPRESENTATIONS WITH β-VAE In this section we explore in a systematic fashion how model-free off-policy RL can be made to train directly from pixel observations. We start by noting a dramatic performance drop when SAC is trained on pixels instead of proprioceptive state (Section 4.2) in the off-policy regime. This result motivates us to explore different ways of employing auxiliary supervision to speed up representation learning. While a wide range of auxiliary objectives could be added to aid effective representation learning, for simplicity we focus our attention on autoencoders. We follow  Lange & Riedmiller (2010) ;  Finn et al. (2015)  and in Section 4.3 try an iterative unsupervised pretraining of an au- toencoder that reconstructs pixels and is parameterized by β-VAE as per  Nair et al. (2018) ;  Higgins et al. (2017a) . Exploring the training procedure used in previous work shows it to be sub-optimal and points towards the need for end-to-end training of the β-VAE with the policy network. Our investiga- tion in Section 4.4 renders this approach useless due to severe instability in training, especially with larger β values. We resolve this by using deterministic forms of the variational autoencoder ( Ghosh et al., 2019 ) and a careful learning procedure. This leads to our algorithm, which is described and evaluated in Section 5.

Section Title: EXPERIMENTAL SETUP
  EXPERIMENTAL SETUP We briefly state our setup here, for more details refer to Appendix B. Throughout the paper we evaluate on 6 image-based challenging continuous control tasks from  Tassa et al. (2018)  depicted in  Figure 1 . For a concise presentation, in some places of the main paper we choose to plot results for reacher easy, ball in cup catch, and walker walk only, while full results are available in the Appendix. An episode for each task results in maximum total reward of 1000 and lasts for exactly 1000 steps. Image observations are represented as 3 × 84 × 84 RGB renderings, where each pixel is scaled down to [0, 1] range. To infer velocity and acceleration we stack 3 consecutive frames following standard practice from  Mnih et al. (2013) . We keep the hyper parameters fixed across all tasks, except for action repeat, which we set only when learning from pixels according to  Hafner et al. (2018)  for a fair comparison to the baselines. If action repeat is used, the number of training observations is only a fraction of the environment steps (e.g. a 1000 steps episode at action repeat 4 will only result in 250 training observations). The exact action repeat settings can be found in Appendix B.3. We evaluate an agent after every 10000 training observation, by computing an average total reward across 10 evaluation episodes. For reliable comparison we run 10 random seeds for each configuration and compute mean and standard deviation of the evaluation reward.

Section Title: MODEL-FREE OFF-POLICY RL WITH NO AUXILIARY TASKS
  MODEL-FREE OFF-POLICY RL WITH NO AUXILIARY TASKS We start with an experiment comparing a model-free and off-policy algorithm SAC ( Haarnoja et al., 2018 ) on pixels, with two state-of-the-art model-based algorithms, PlaNet ( Hafner et al., 2018 ) and SLAC ( Lee et al., 2019 ), and an upper bound of SAC on proprioceptive state ( Table 1 ). We see a large gap between the capability of SAC on pixels (SAC:pixel), versus PlaNet and SLAC, which make use of many auxiliary tasks to learn a better representation, and can achieve performance close to the upper bound of SAC on proprioceptive state (SAC:state). From now, SAC:pixel will be our lower bound on performance as we gradually introduce different auxiliary reconstruction losses in order to close the performance gap.

Section Title: ITERATIVE REPRESENTATION LEARNING WITH β-VAE
  ITERATIVE REPRESENTATION LEARNING WITH β-VAE Following  Lange & Riedmiller (2010) ;  Finn et al. (2015) , we experiment with unsupervised repre- sentation pretraining using a pixel autoencoder, which speeds up representation learning in image- based RL. Taking into account successful results from  Nair et al. (2018) ;  Higgins et al. (2017b)  of using a β-VAE ( Kingma & Welling, 2013 ;  Higgins et al., 2017a ) in the iterative re-training setup, we choose to employ a β-VAE likewise. We then proceed to first learn a representation space by pretraining the f enc , f enc std , and f dec networks of the β-VAE according to the loss J(VAE) Equa- tion (4) on data collected from a random policy. We then learn a control policy on top of the frozen latent representations z t = f enc (o t ). We tune β for best performance, and find large β to be worse, and that very small β ∈ [10 −8 , 10 −6 ] performed best. In  Figure 2  we vary the frequency N at which the representation space is updated, from N = ∞, where the representation is never updated after an initial pretraining period with randomly collected data, to N = 1 where the representation is updated after every policy update. There is a positive correlation between this frequency and the final policy performance. We emphasize that the gradients are never shared between the β-VAE for learning the representation space, and the actor-critic learning the policy. These results suggest that if we can combine the representation pretraining via a β-VAE together with the policy learning in a stable end-to-end procedure, we would expect better performance. However, we note that prior work ( Nair et al., 2018 ;  Higgins et al., 2017a ) has been unable to successfully demonstrate this. Regardless, we next perform such experiment to gain better understanding on what goes wrong.

Section Title: AN ATTEMPT FOR END-TO-END REPRESENTATION LEARNING WITH β-VAE
  AN ATTEMPT FOR END-TO-END REPRESENTATION LEARNING WITH β-VAE Our findings and the results from  Jaderberg et al. (2017)  motivate us to allow gradient propagation to the encoder of the β-VAE from the actor-critic, which in our case is SAC. We enable end-to-end learning by allowing the encoder to not only update with gradients from the J(VAE) loss (Equa- tion (4), as done in Section 4.3, but also with gradients coming from the J(Q) and J(π) (Equa- tions (1) and (2)) losses specified in Section 3. Results in  Figure 3  show that the end-to-end policy learning together with the β-VAE in unstable in the off-policy setting and prone to divergent be- haviours that hurt performance. Our conclusion supports the findings from  Nair et al. (2018) ;  Hig- gins et al. (2017a) , which alleviate the problem by receding to the iterative re-training procedure. We next attempt stabilizing end-to-end training and introduce our method.

Section Title: OUR METHOD: SAC+AE WITH END-TO-END OFF-POLICY TRAINING
  OUR METHOD: SAC+AE WITH END-TO-END OFF-POLICY TRAINING We now seek to design a stable training procedure that can update the pixel autoencoder simulta- neously with policy learning. We build on top of SAC ( Haarnoja et al., 2018 ), a model-free and off-policy actor-critic algorithm. Based on our findings from Section 4, we propose a new, simple algorithm, SAC+AE, that enables end-to-end training. We notice that electing to learn deterministic latent representations, rather than stochastic as in the β-VAE case, has a stabilizing effect on the end-to-end learning in the off-policy regime. We thus use a deterministic autoencoder in a form of the regularized autoencoder (RAE) ( Ghosh et al., 2019 ), that has many structural similarities with β-VAE. We also found it is important to update the convolutional weights in the target critic network faster, than the rest of the parameters. This allows faster learning while preserving the stability of the off-policy actor-critic. Finally, we share the encoder's convolutional weights between the actor and critic networks, but prevent the actor from updating them. Our algorithm is presented in  Figure 4  for visual guidance.

Section Title: PERFORMANCE ON PIXELS
  PERFORMANCE ON PIXELS We now show that our simple method, SAC+AE, achieves stable end-to-end training of an off-policy algorithm from images with an auxiliary reconstruction loss. We test our method on 6 challenging image-based continuous control tasks (see  Figure 1 ) from DMC ( Tassa et al., 2018 ). The RAE consists of a convolutional and deconvolutional trunk of 4 layers of 32 filters each, with 3 × 3 kernel size. The actor and critic networks are 3 layer MLPs with ReLU activations and hidden size of 1024. We update the RAE and actor-critic network at each environment step with a batch of experience sampled from a replay buffer. A comprehensive overview of other hyper paremeters is Appendix B. We perform comparisons against several state-of-the-art model-free and model-based RL algorithms for learning from pixels. In particular: D4PG ( Barth-Maron et al., 2018 ), an off-policy actor-critic algorithm, PlaNet ( Hafner et al., 2018 ), a model-based method that learns a dynamics model with deterministic and stochastic latent variables and employs cross-entropy planning for control, and SLAC ( Lee et al., 2019 ), which combines a purely stochastic latent model together with an model- free soft actor-critic. In addition, we compare against SAC that learns from low-dimensional pro- Under review as a conference paper at ICLR 2020 prioceptive state, as an upper bound on performance. In  Figure 5  we show that SAC+AE:pixel is able to match the state-of-the-art model-based methods such as PlaNet and SLAC, and significantly improve performance over the baseline SAC:pixel. Note that we use 10 random seeds, as recom- mended in  Henderson et al. (2018)  whereas the PlaNet and SLAC numbers shown are only over 4 and 2 seeds, respectively, as per the original publications.

Section Title: ABLATIONS
  ABLATIONS To shed more light on some properties of the latent representation space learned by our algorithm we conduct several ablation studies. In particular, we want to answer the following questions: (i) is our method able to extract a sufficient amount of information from raw images to recover cor- responding proprioceptive states readily? (ii) can our learned latent representation generalize to unseen tasks with similar image observations, but different reward objective, without reconstruction signal? Below, we answer these questions.

Section Title: REPRESENTATION POWER OF THE ENCODER
  REPRESENTATION POWER OF THE ENCODER Given how significantly our method outperforms a variant that does not have access to the image re- construction signal, we hypothesize that the learned representation space encodes a sufficient amount of information about the internal state of the environment from raw images. Moreover, this infor- mation can be easily extracted from the latent state. To test this conjecture, we train SAC+AE:pixel and SAC:pixel until convergence on cheetah run, then fix their encoders. We then train two identical linear projections to map the encoders' latent embedding of image observations into the corresponding proprioceptive states. Finally, we compare ground truth proprioceptive states against their reconstructions on a sample episode. Results in  Figure 6  confirm our hypothesis that the en- coder grounded on pixel observations is powerful enough to almost perfectly restore the internals of the task, whereas SAC without the reconstruction loss cannot. Full results in Appendix F.

Section Title: GENERALIZATION TO UNSEEN TASKS
  GENERALIZATION TO UNSEEN TASKS To verify whether the latent representation space learned by our method is able to generalize to different tasks without additional fine-tuning with the reconstruction signal, we take three tasks walker stand, walker walk, and walker run from DMC, which share similar obser- vational appearance, but have different reward structure. We train an agent using our method (SAC+AE:pixel) on walker walk task until convergence and extract its encoder. Consequently, we train two SAC agents without reconstruction loss on walker stand and walker run tasks from pixels. The encoder of the first agent is initialized with weights from the pretrained walker walk encoder, while the encoder of the second agent is not. Neither of the agents use the reconstruction signal, and only backpropogate gradients from the critic to the encoder (see  Figure 4 ). Results in  Figure 7  suggest that our method learns latent representations that can readily generalize to unseen tasks and help a SAC agent achieve strong performance and solve the tasks.

Section Title: DISCUSSION
  DISCUSSION We have presented the first end-to-end, off-policy, model-free RL algorithm for pixel observations with only reconstruction loss as an auxiliary task. It is competitive with state-of-the-art model-based methods, but much simpler, robust, and without requiring learning a dynamics model. We show through ablations the superiority of end-to-end learning over previous methods that use a two-step training procedure with separated gradients, the necessity of a pixel reconstruction loss over recon- struction to lower-dimensional "correct" representations, and demonstrations of the representation power and generalization ability of our learned representation. We find that deterministic models outperform β-VAEs ( Higgins et al., 2017a ), likely due to the other introduced instabilities, such as bootstrapping, off-policy data, and end-to-end training with auxiliary losses. We hypothesize that deterministic models that perform better even in stochastic environments should be chosen over stochastic ones with the potential to learn probability distri- butions, and argue that determinism has the benefit of added interpretability, through handling of simpler distributions. In the Appendix we provide results across all experiments on the full suite of 6 tasks chosen from DMC (Appendix A), and the full set of hyperparameters used in Appendix B. There are also ad- ditional experiments autoencoder capacity (Appendix E), a look at optimality of the learned latent representation (Appendix H), importance of action repeat (Appendix I), and a set of benchmarks on learning from proprioceptive observation (Appendix J). Finally, we opensource our codebase for the community to spur future research in image-based RL. Under review as a conference paper at ICLR 2020

```
