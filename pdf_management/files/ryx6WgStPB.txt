Title:
```
Published as a conference paper at ICLR 2020 HYPERMODELS FOR EXPLORATION
```
Abstract:
```
We study the use of hypermodels to represent epistemic uncertainty and guide exploration. This generalizes and extends the use of ensembles to approximate Thompson sampling. The computational cost of training an ensemble grows with its size, and as such, prior work has typically been limited to ensembles with tens of elements. We show that alternative hypermodels can enjoy dramatic efficiency gains, enabling behavior that would otherwise require hundreds or thousands of elements, and even succeed in situations where ensemble methods fail to learn re- gardless of size. This allows more accurate approximation of Thompson sampling as well as use of more sophisticated exploration schemes. In particular, we con- sider an approximate form of information-directed sampling and demonstrate per- formance gains relative to Thompson sampling. As alternatives to ensembles, we consider linear and neural network hypermodels, also known as hypernetworks. We prove that, with neural network base models, a linear hypermodel can repre- sent essentially any distribution over functions, and as such, hypernetworks are no more expressive.
```

Figures/Tables Captions:
```
Figure 1: A base model generates an output Y t given parameters θ and input X t , while a hypermodel generates base model parameters g ν (z) given hypermodel parameters ν and an index z.
Figure 2: Computation required for ensemble hypermodels versus diagonal linear hypermodels to perform well on Gaussian bandits with independent arms.
Figure 3: Compare (i) ensemble hypermodels, (ii) linear hypermodels, (iii) annealing -greedy, and (iv) an agent assuming independent actions on a neural network bandit.
Figure 4: Cumulative regret of IDS and TS with with one-sparse models.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Consider the sequential decision problem of an agent interacting with an uncertain environment, aiming to maximize cumulative rewards. Over each time period, the agent must balance between exploiting existing knowledge to accrue immediate reward and investing in exploratory behavior that may increase subsequent rewards. In order to select informative exploratory actions, the agent must have some understanding of what it is uncertain about. As such, an ability to represent and resolve epistemic uncertainty is a core capability required of the intelligent agents. The efficient representation of epistemic uncertainty when estimating complex models like neural networks presents an important research challenge. Techniques include variational inference ( Blun- dell et al., 2015 ), dropout 1 ( Gal & Ghahramani, 2016 ) and MCMC ( Andrieu et al., 2003 ). Another approach has been motivated by the nonparametric bootstrap ( Efron & Tibshirani, 1994 ) and trains an ensemble of neural networks with random perturbations applied to each dataset ( Lu & Van Roy, 2017 ). The spirit is akin to particle filtering, where each element of the ensemble approximates a sample from the posterior and variation between models reflects epistemic uncertainty. Ensem- bles have proved to be relatively effective and to address some shortcomings of alternative posterior approximation schemes ( Osband et al., 2016 ; 2018). When training a single large neural network is computationally intensive, training a large ensem- ble of separate models can be prohibitively expensive. As such, ensembles in deep learning have typically been limited to tens of models ( Riquelme et al., 2018 ). In this paper, we show that this parsimony can severely limit the quality of the posterior approximation and ultimately the quality of the learning system. Further, we consider more general approach based on hypermodels that can realize the benefits of large ensembles without the prohibitive computational requirements. A hypermodel maps an index drawn from a reference distribution to a base model. An ensemble is one type of hypermodel; it maps a uniformly sampled base model index to that independently trained base model. We will consider additional hypermodel classes, including linear hypermodels, which we will use to map a Gaussian-distributed index to base model parameters, and hypernetworks, for which the mapping is a neural network ( Ha et al., 2016 ). Our motivation is that intelligent Published as a conference paper at ICLR 2020 hypermodel design might be able to amortize computation across the entire distribution of base models, and in doing so, offer large gains in computational efficiency. We train our hypermodels to estimate a posterior distribution over base models conditioned on ob- served data, in a spirit similar to that of the Bayesian hypermodel literature (Krueger et al., 2017). Unlike typical variational approximations to Bayesian deep learning, this approach allows com- putationally efficient training with complex multimodal distributions. In this paper, we consider hypermodels trained through stochastic gradient descent on perturbed data (see Section 2.1 for a full description). Training procedures for hypermodels are an important area of research, and it may be possible to improve on this approach, but that is not the focus of this paper. Instead, we aim to understand whether more sophisticated hypermodel architectures can substantially improve explo- ration. To do this we consider bandit problems of varying degrees of complexity, and investigate the computational requirements to achieve low regret over a long horizon. To benchmark the quality of posterior approximations, we compare their efficacy when used for Thompson sampling ( Thompson, 1933 ;  Russo et al., 2018 ). In its ideal form, Thompson sampling (TS) selects each action by sampling a model from the posterior distribution and optimizing over actions. For some simple model classes, this approach is computationally tractable. Hypermodels enable approximate TS in complex systems where exact posterior inference is intractable. Our results address three questions:

Section Title: Q: Can alternative hypermodels outperform ensembles?
  Q: Can alternative hypermodels outperform ensembles? A: Yes. We demonstrate through a simple example that linear hypermodels can offer dramatic im- provements over ensembles in the computational efficiency of approximate TS. Further, we demon- strate that linear hypermodels can be effective in contexts where ensembles fail regardless of ensem- ble size.

Section Title: Q: Can alternative hypermodels enable more intelligent exploration?
  Q: Can alternative hypermodels enable more intelligent exploration? A: Yes. We demonstrate that, with neural network hypermodels, a version of information-directed sampling ( Russo & Van Roy, 2014 ; 2018) substantially outperforms TS. This exploration scheme would be computationally prohibitive with ensemble hypermodels but becomes viable with a hyper- network.

Section Title: Q: Are hypernetworks warranted?
  Q: Are hypernetworks warranted? A: Not clear. We prove a theorem showing that, with neural network base models, linear hypermod- els can already represent essentially any distribution over functions. However, it remains to be seen whether hypernetworks can offer statistical or computational advantages. Variational methods offer an alternative approach to approximating a posterior distribution and sam- pling from it.  O'Donoghue et al. (2018)  consider such an approach for approximating Thompson sampling in reinforcement learning. Approaches to approximating TS and information-directed sampling (IDS) with neural networks base models have been studied in ( Lu & Van Roy, 2017 ;  Riquelme et al., 2018 ) and  Nikolov et al. (2019) , respectively, using ensemble representations of uncertainty. Hypermodels have been a subject of growing interest over recent years.  Ha et al. (2016)  proposed the notion of hypernetworks as a relaxed form of weight-sharing.  Krueger et al. (2017)  proposed Bayesian hypernetworks for estimation of posterior distributions and a training algorithm based on variational Bayesian deep learning. A limitation of this approach is in its requirement that the hypernetwork be invertible.  Karaletsos et al. (2018)  studied Bayesian neural networks with correlated priors, specifically considering prior distributions in which units in the neural network are represented by latent variables and weights between units are drawn conditionally on the values of those latent variables.  Pawlowski et al. (2017)  introduced another variational inference based algorithm that interprets hypernetworks as implicit distributions, i.e. distributions that may have intractable probability density functions but allow for easy sampling.  Hu et al. (2018)  proposes the Stein neural sampler which samples from a given (un-normalized) probability distribution with neural networks trained by minimizing variants of Stein discrepancies.

Section Title: HYPERMODEL ARCHITECTURES AND TRAINING
  HYPERMODEL ARCHITECTURES AND TRAINING We consider base models that are parameterized by an element θ of a parameter space Θ. Given θ ∈ Θ and an input X t ∈ Nx , a base model posits that the conditional expectation of the output Published as a conference paper at ICLR 2020 (a) base model (b) hypermodel Y t+1 ∈ is given by E[Y t+1 |X t , θ] = f θ (X t ), for some class of functions f indexed by θ. Figure 1a depicts this class of parameterized base models. A hypermodel is parameterized by parameters ν, which identify a function g ν : Z → Θ. We will refer to each z ∈ Z as an index, as it identifies a specific instance of the base model. In particular, given hypermodel parameters ν, base model parameters θ can be generated by selecting z ∈ Z and setting θ = g ν (z). This notion of a hypermodel is illustrated in Figure 1b. Along with a hypermodel, in order to represent a distribution over base models, we must specify a reference distribution p z that can be used to sample an element of Z. A hypermodel and reference distribution together represent a distribution over base models through offering a mechanism for sampling them by sampling an index and passing it through the mapping.

Section Title: HYPERMODEL TRAINING
  HYPERMODEL TRAINING Given a set of data pairs {(X t , Y t+1 ) : t = 0, . . . , T −1}, a hypermodel training algorithm computes parameters ν so that the implied distribution over base model parameters approximates its posterior. It is important that training algorithms be incremental. This enables scalability and also allows for ongoing modifications to the data set, as those occurring in the bandit learning context, in which data samples accumulate as time progresses. One approach to incrementally training a hypermodel involves perturbing data by adding noise to response variables, and then iteratively updating parameters via stochastic gradient descent. We will assume here that the reference distribution p z is either an N z -dimensional unit Gaussian or a uniform distribution over the N z -dimensional unit hypersphere. Consider an augmented data set D = {(X t , Y t+1 , A t ) : t = 0, . . . , T − 1}, where each A t ∈ Nz is a random vector that serves to randomize computations carried out by the algorithm. Each vector A t is independently sampled from N (0, I) if p z is uniform over the unit hypersphere. Otherwise, A t is independently sampled from the unit hypersphere. We consider a stochastic gradient descent algorithm that aims to minimize the loss function where ν 0 is the initial vector of hypermodel parameters. Each iteration of the algorithm entails calculating the gradient of terms summed over a minibatch of (x, y, a) tuples and random indices z. Note that σ w a z here represents a random Gaussian perturbation of the response variable y. In particular, in each iteration, a minibatchD is constructed by sampling a subset of D uniformly with replacement, and a setZ of indices is sampled i.i.d. from p z . An approximate loss functioñ is defined based on these sets. Hypermodel parameters are updated according to ν ← ν − α∇ νL (ν,D,Z)/|D| where α, σ 2 w , and σ 2 p are algorithm hyperparameters. In our experiments, we Published as a conference paper at ICLR 2020 will take the step size α to be constant over iterations. It is natural to interpret σ 2 p as a prior variance, as though the prior distribution over base model parameters is N (0, σ 2 p I), and σ 2 w as the standard deviation of noise, as though the error distribution is Y t − f θ (X t )|θ ∼ N (0, σ 2 w ). Note, though, that a hypermodel can be trained on data generated by any process. One can think of the hypermodel and base models as inferential tools in the mind of an agent rather than a perfect reflection of reality.

Section Title: ENSEMBLE HYPERMODELS
  ENSEMBLE HYPERMODELS An ensemble hypermodel is comprised of an ensemble of N ν base models, each identified by a parameter vector in Θ = N θ . Letting indices Z be the set of N ν -dimensional one-hot vectors, we can represent an ensemble in terms of a function g ν : Z → Θ with parameters ν ∈ Θ Nν . In particular, given hypermodel parameters ν ∈ Θ Nν , an index z ∈ Z generates base model parameters g ν (Z) = νZ. For an ensemble hypermodel, the reference distribution p z is taken to be uniform over the N ν elements of Z.

Section Title: LINEAR HYPERMODELS
  LINEAR HYPERMODELS Suppose that Θ = N θ and Z = Nz . Consider a linear hypermodel, defined by g ν (z) = a + Bz, where hypermodel parameters are given by ν = (a ∈ N θ , B ∈ N θ ×Nz ) and z ∈ Z is an index with reference distribution p z taken to be the unit Gaussian N (0, I) over N z -dimensional vectors. Such a hypermodel can be used in conjunction with any base model that is parameterized by a vector of real numbers. The aforementioned linear hypermodel entails a number of parameters that grows with the product of the number N θ of base model parameters and the index dimension N z , since B is a N θ × N z matrix. This can give rise to onerous computational requirements when dealing with neural network base models. For example, suppose that we wish to model neural network weights as a Gaussian random vector. This would require an index of dimension equal to the number of weights, and the number of hypermodel parameters would become quadratic in the number of neural network weights. For a large neural network, storing and updating that many parameters is impractical. As such, it is natural to consider linear hypermodels in which the parameters a and B are linearly constrained. Such linear constraints can, for example, represent independence or conditional independence structure among neural network weights.

Section Title: NEURAL NETWORK HYPERMODELS
  NEURAL NETWORK HYPERMODELS More complex hypermodels are offered by neural networks. In particular, consider the case in which g ν is a neural network with weights ν, taking N z inputs and producing N θ outputs. Such a representation is alternately refered to as a hypernetwork. Let the reference distribution p z be the unit Gaussian N (0, I) over N z -dimensional vectors. As a special case, a neural network hypermodel becomes linear if there are no hidden layers.

Section Title: ADDITIVE PRIOR MODELS
  ADDITIVE PRIOR MODELS In order for our stochastic gradient descent algorithm to operate effectively, it is often important to structure the base model so that it is a sum of a prior model, with parameters fixed at initialization, and a differential model, with parameters that evolve while training. The idea here is for the prior model to represent a sample from a prior distribution and for the differential to learn the difference between prior and posterior as training progresses. This additive decomposition was first introduced in ( Osband et al., 2018 ), which demonstrated its importance in training ensemble hypermodels with neural network base models using stochastic gradient descent. Without this decomposition, to gen- erate neural networks that represent samples from a sufficiently diffuse prior, we would have to initialize with large weights. Stochastic gradient descent tends to train too slowly and thus becomes impractical if initialized in such a way. We will consider a decomposition that uses neural network base models (including linear base mod- els as a special case) though the concept is more general. Consider a neural network model class {fθ :θ ∈Θ} withΘ = Nθ , where the parameter vectorθ includes edge weights and node biases. Let the index set Z be Nz . Let D be a diagonal matrix for which each element is the prior standard Published as a conference paper at ICLR 2020 deviation of corresponding component ofθ. Let B ∈ Nθ×Nz be a random matrix produced at initialization. We will takeθ = DBz to be parameters of the prior (base) model. Note that, given an index z ∈ Z, this generates a prior modelfθ =f DBz . When we wish to completely specify a prior model distribution, we will need to define a distribution for generating the matrix B as well as a reference distribution p z . Given a prior model of the kind we have described, we consider a base model of the form f θ (x) = fθ(x) +fθ(x), where {fθ :θ ∈Θ} is another neural network model class satisfyingf 0 = 0, and θ is the concatenation ofθ andθ. Withθ = DBz, the idea is to compute parametersθ such that f θ =f DBz +fθ approximates a sample from a posterior distribution, conditioned on data. As such,fθ represents a difference between prior and posterior. This decomposition is motivated by the observation that neural network training algorithms are most effective if initialized with small weights and biases. If we initializeθ with small values, the initial values offθ will be small, and in this regime, f θ ≈f DBz , which is appropriate since an untrained base model should represent a sample from a prior distribution. In general,θ is the output of a neural networkĝν taking the same input z as the prior hypermodelθ = DBz. As is discussed above, in the course of training, we will only updateν while keeping D and B fixed.

Section Title: EXPLORATION SCHEMES
  EXPLORATION SCHEMES Our motivation for studying hypermodels stems from their potential role in improving exploration methods. As a context for studying exploration, we consider bandit problems. In particular, we consider the problem faced by an agent making sequential decisions, in each period selecting an action X t ∈ X and observing a response Y t+1 ∈ . Here, the action set X is a finite subset of Nx and Y t+1 is interpreted as a reward, which the agent wishes to maximize. We view the environment as a channel that maps X t to Y t+1 , and conditioned on X t and the en- vironment, Y t+1 is conditionally independent of X 0 , Y 1 , . . . , X t−1 , Y t . In other words, actions do not induce delayed consequences. However, the agent learns about the environment from applying actions and observing outcomes, and as such, its prediction of an outcome Y t+1 is influenced by past observations X 0 , Y 1 , . . . , X t−1 , Y t . A base model serves as a possible realization of the environment, while a hypermodel encodes a belief distribution over possible realizations. We consider an agent that represents beliefs about the environment through a hypermodel, continually updating hypermodel paramerers ν via stochastic gradient descent, as described in Section 2.1, to minimize a loss function based on past actions and observations. At each time t, the agent selects action X t based on the current hypermodel. Its selection should balance between exploring to reduce uncertainty indicated by the hypermodel and exploiting knowledge conveyed by the hypermodel to accrue rewards.

Section Title: THOMPSON SAMPLING
  THOMPSON SAMPLING TS is a simple and often very effective exploration scheme that will serve as a baseline in our experi- ments. With this scheme, each action X t is selected by sampling an index z ∼ p z from the reference distribution and then optimizing the associated base model to obtain X t ∈ arg max x∈X f gν (z) (x). See ( Russo et al., 2018 ) for an understanding when and why TS is effective.

Section Title: INFORMATION-DIRECTED SAMPLING
  INFORMATION-DIRECTED SAMPLING IDS ( Russo & Van Roy, 2014 ; 2018) offers an alternative approach to exploration that aims to more directly quantify and optimize the value of information. There are multiple versions of IDS, and we consider here a sample-based version of variance-IDS ( Russo & Van Roy, 2018 ). In each time period, this entails sampling a new multisetZ i.i.d. from p z . Then, for each action x ∈ X we compute the sample mean of immediate regret Published as a conference paper at ICLR 2020 and a sample variance of reward across possible realizations of the optimal action Here, {Z x * : x * ∈ X } forms a partition ofZ such that, x * is an optimal action for each z ∈Z x * ; that is,Z x * = {z ∈Z|x * ∈ arg max x∈X f gν (z) (x)} Then, a probability vector π * ∈ ∆ X is obtained by solving π * ∈ arg min π∈∆ X x∈X π x r x 2 x∈X π x v x , and action X t sampled from π * . Note that π * x = 0 ifZ x is empty. As established by ( Russo & Van Roy, 2018 ), the minimum over π ∈ ∆ X is always attained by a probability vector that has at most two nonzero components, and this fact can be used to simplify optimization algorithms. Producing reasonable estimates of regret and variance calls for many distinct samples, and the num- ber required scales with the number of actions. An ensemble hypermodel with tens of elements does not suffice, while alternative hypermodels we consider can generate very large numbers of distinct samples.

Section Title: CAN HYPERMODELS OUTPERFORM ENSEMBLES?
  CAN HYPERMODELS OUTPERFORM ENSEMBLES? Because training a large ensemble can be prohibitively expensive, neural network ensembles have typically been limited to tens of models ( Riquelme et al., 2018 ). In this section, we demonstrate that a linear hypermodel can realize the benefits of a much larger ensemble without the onerous computational requirements.

Section Title: GAUSSIAN BANDIT WITH INDEPENDENT ARMS
  GAUSSIAN BANDIT WITH INDEPENDENT ARMS We consider a Gaussian bandit with K independent arms where the mean reward vector θ * ∈ K is drawn from a Gaussian prior N (0, σ 2 p I). During each time period t, the agent selects an action X t and observes a noisy reward Y t+1 = θ * Xt + W t+1 , where W t+1 is i.i.d. N (0, σ 2 w ). We let σ 2 p = 2.25 and σ 2 w = 1, and we fix the time horizon to 10,000 periods. We compare an ensemble hypermodel and a diagonal linear hypermodel trained via SGD with per- turbed data. Our simulation results show that a diagonal linear hypermodel requires about 50 to 100 times less computation than an ensemble hypermodel to achieve our target level of performance. As discussed in Section 2.5, we consider base models of the form f θ (x) =f DBz (x) +fθ(x), wherẽ f DBz (x) is an additive prior model, andfθ(x) is a trainable differential model that aims to learn the difference between prior and posterior. For an independent Gaussian bandit,fθ(x) =fθ(x) =θ x for allθ and x. Although the use of prior models is inessential in this toy example, we include it for consistency and illustration of the approach. The index z ∈ Nz of an ensemble hypermodel is sampled uniformly from the set of N z - dimensional one-hot vectors. Each row of B ∈ K×Nz is sampled from N (0, I), and D = σ p I. The ensemble (differential) hypermodel takes the formĝν(z) =νz, where the parametersν ∈ K×Nz are initialized to i.i.d. N (0, 0.05 2 ). Although initializing to small random numbers instead of zeros is unnecessary for a Gaussian bandit, our intention here is to mimic neural network initialization and treat the ensemble hypermodel as a special case of neural network hypermodels. In a linear hypermodel, to model arms independently, we let z 1 , . . . , z K ∈ m each be drawn in- dependently from N (0, I), and let the index z ∈ Nz be the concatenation of z 1 , . . . , z K , with N z = Km. Let the prior parameters b 1 , . . . , b K ∈ m be sampled uniformly from the m- dimensional hypershpere, and let B ∈ K×Nz be a block matrix with b 1 , . . . , b K on the diago- nal and zero everywhere else. Let D = σ p I. The diagonal linear (differential) hypermodel takes the formĝν(z) = Cz + µ, where µ ∈ K and matrix C ∈ K×Nz has a block diagonal struc- ture C = diag(c 1 , . . . , c K ), with c 1 , . . . , c K ∈ m . The hypermodel parametersν = (C, µ) are initialized to i.i.d. N (0, 0.05 2 ).

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 We train both hypermodels using SGD with perturbed data. For an ensemble hypermodel, the per- turbation of the data point collected at time t is σ w A t z, where A t ∼ N (0, I). For a diagonal linear hypermodel, the perturbation is σ w A t z Xt , where A t is sampled uniformly from the m-dimensional unit hypersphere. We consider an agent to perform well if its average regret over 10,000 periods is below 0.01 √ K. We compare the computational requirements of ensemble and diagonal linear hypermodels across different numbers of actions. As a simple machine-independent definition, we approximate the number of arithmetic operations over each time period: computation = n sgd × n z × n data × n params , where n sgd is the number of SGD steps per time period, n z is the number of index samples per SGD step, n data is the data batch size, and n params is the number of hypermodel parameters involved in each index sample. We fix the data batch size to 1024 for both agents, and sweep over other hyper- parameters separately for each agent. All results are averaged over 100 runs. In  Figure 2 , we plot the computation needed versus number of actions. Using a diagonal linear hypermodel dramatically re- duces the amount of computation needed to perform well relative to using an ensemble hypermodel, with a speed-up of around 50 to 100 times for large numbers of actions.

Section Title: NEURAL NETWORK BANDIT
  NEURAL NETWORK BANDIT In this section we show that linear hypermodels can also be more effective than ensembles in settings that require generalization between actions. We consider a bandit problem with rewards generated by a neural network that takes vector-valued actions as inputs. We consider a finite action set A ⊂ d with d = 20, sampled uniformly from the unit hypersphere. We generate data using a neural network with input dimension 20, 2 hidden layers of size 3, and a scalar output. The output is perturbed by i.i.d. N (0, 1) observation noise. The weights of each layer are sampled independently from N (0, 2.25), N (0, 2.25/3), and N (0, 2.25/3), respectively, with biases from N (0, 1). We compare ensemble hypermodels with 10, 30, 100, and 300 particles, and a linear hypermodel with index dimension 30. Both agents use an additive priorf DBz (x), wheref is a neural network with the same architecture as the one used to generate rewards. For the ensemble hypermodel, each row of B is initialized by sampling independently from N (0, I), and D is diagonal with appropriate prior standard deviations. For the linear hypermodel, we enforce independence of weights across layers by choosing B to be block diagonal with 3 blocks, one for each layer. Each block has width 10. Within each block, each row is initialized by sampling uniformly from the 10-dimensional unit hypersphere. For the trainable differential model, both agents use a neural network architecture with 2 hidden layers of width 10. The parameters of the ensemble hypermodel are initialized to truncated N (0, 0.05 2 ). The weights of the linear hypermodel are initialized using the Glorot uniform initialization, while the biases are initialized to zero. In our simulations, we found that training without data perturbation gives lower regret for both agents. In  Figure 3 , we plot the cumulative regret of agents trained without data perturbation. We see that linear hypermodels achieve the least regret in the long run. The performance of ensemble hy- permodels is comparable when the number of actions is 200. However, there is a large performance gap when the number of actions is greater than 200, which, surprisingly, cannot be compensated by increasing the ensemble size. We suspect that this may have to do with the reliability of neural network regression, and linear hypermodels are somehow able to circumvent this issue. We also compare with an -greedy agent with a tuned annealing rate, and an agent that assumes in- dependent actions and applies TS under the Gaussian prior and Gaussian noise assumption. The gap between the -greedy agent and hypermodel agents grows as the number of actions becomes large, as -greedy explores uniformly and does not write off bad actions. The performance of the agent that assumes independent actions degrades quickly as the number of actions increases, since it does not generalize across actions. In the appendix, we also discuss Bayes by Backprop ( Blundell et al., 2015 ) and dropout ( Gal & Ghahramani, 2016 ) as approximation methods for posterior sampling.

Section Title: CAN HYPERMODELS ENABLE MORE INTELLIGENT EXPLORATION?
  CAN HYPERMODELS ENABLE MORE INTELLIGENT EXPLORATION? IDS, as we described earlier, requires a large number of independent samples from the (approxi- mate) posterior distribution to generate an action. One way to obtain these samples is to maintain an ensemble of models, as is done by  Nikolov et al. (2019) . However, as the number of actions increases, maintaining performance requires a large ensemble, which becomes computationally pro- hibitive. More general hypermodels offer an efficient mechanism for generating the required large number of base model samples. In this section, we present experimental results involving a problem and hypermodel stylized to demonstrate advantages of IDS in a transparent manner. This context is inspired by the one-sparse linear bandit problem constructed by  Russo & Van Roy (2018) . However, the authors of that work do not offer a general computationally practical approach that implements IDS. Hypermodels may serve this need. We generate data according to Y t+1 = X t θ * + W t+1 where θ * ∈ N θ is sampled uniformly from one-hot vectors and W t+1 is i.i.d. N (0, 1) noise. We consider a linear base model f θ (x) = θ x and hypermodel (g ν (z)) m = exp(βν m (z 2 m + α))/ N θ n=1 exp(βν n (z 2 n + α)), where α = 0.01, and β = 10. As a reference distribution we let p z be N (0, I). Let the initial hypermodel parameters ν 0 be the vector with each component equal to one. Note that our hypermodel is designed to allow representation of the prior distribution, as well as uniform distributions over subsets of one-hot vectors. For simplicity, let N θ be a power of two. Let I be the set of indicator vectors for all non- singleton sublists of indices in (1, . . . , N θ ) that can be obtained by bisecting the list one or more times. Note that |I| = N θ − 2. Let the action space X be comprised one hot-vectors and vectors {x/2 : x ∈ I}. As with the one-sparse linear bandit of ( Russo & Van Roy, 2018 ), this problem is designed so that TS will identify the nonzero component of θ * by applying one-hot actions to rule out one component per period, whereas IDS will essentially carry out a bisection search. This difference in behavior stems from the fact that TS will only ever apply actions that have some chance of being optimal, which in this context includes only the one-hot vectors, whereas IDS can apply actions known to be suboptimal if they are sufficiently informative.  Figure 4  plots regret realized by TS and variance-IDS using the aforementioned hypermodel, trained with perturbed SGD. As expected, the difference in performance is dramatic. Each plot is averaged over 500 simulations. We used SGD hyperparameters σ 2 w = 0.01 and σ 2 p = 1/ log N θ . The ex- periments are with N θ = 200, 500 samples are used for computing the variance-based information ratio.

Section Title: ARE HYPERNETWORKS WARRANTED?
  ARE HYPERNETWORKS WARRANTED? Results of the previous sections were generated using ensemble and linear hypermodels. It remains to be seen whether hypernetworks offer substantial benefits. One might believe that hypernetworks can benefit from the computational advantages enjoyed by linear hypermodels while offering the ability to represent a broader range of probability distributions over base models. The following result refutes this possibility by establishing that, with neural network base models, linear hyper- models can represent essentially any probability distribution over functions with finite domain. We denote by L ∞ (X , B) the set of functions f : X → such that f ∞ < B, where X is finite with |X | = K. Theorem 1 Let p z be the unit Gaussian distribution in K . For all > 0, δ > 0, B > 0, and probability measures µ over L ∞ (X , B), there exist a transport map H from p z to µ, a neural network f θ : X → with a linear output node and ReLU hidden nodes, and a linear hypermodel g ν : Z → N θ with form g ν (z) = z T , ν T T such that f gν (z) − f * ∞ ≤ with probability at least 1 − δ, where f * = H(z). This result is established in Appendix A. To digest the result, first suppose that the inequality is satisfied with = 0. Interpret µ as the target probability measure we wish to approximate using the hypermodel. Note that f gν (z) and f * are determined by z ∼ p z , and f * is distributed according to µ, since H is a transport function that maps p z to µ. If f gν (z) − f * ∞ = 0 then f gν (z) is also distributed according to µ, and as such, the hypermodel perfectly represents the distribution. If > 0, the representation becomes approximate with tolerance . Though our result indicates that linear hypermodels suffice to represent essentially all distributions over functions, we do not rule out the possibility of statistical or computational advantages to us- ing hypernetworks. In particular, there could be situations where hypernetworks generalize more accurately given limited data, or where training algorithms operate more effectively with hypernet- works. In supervised learning, deep neural networks offer such advantages even though a single hidden layer suffices to represent essentially any function. Analogous benefits might carry over to hypernetworks, though we leave this question open for future work.

Section Title: CONCLUSION
  CONCLUSION Our results offer initial signs of promise for the role of hypermodels beyond ensembles in improving exploration methods. We have shown that linear hypermodels can offer large gains in computational efficiency, enabling results that would otherwise require ensembles of hundreds or thousands of ele- ments. Further, these efficiency gains enable more sophisticated exploration schemes. In particular, we experiment with a version of IDS sampling and demonstrate benefits over methods based on TS. Finally, we consider the benefits of hypernetworks and establish that, with neural network base models, linear hypermodels are already able to represent essentially any distribution over functions. Hence, to the extent that hypernetworks offer advantages, this would not be in terms of the class of distributions that can be represented.

```
