Title:
```
Under review as a conference paper at ICLR 2020 DS-VIC: UNSUPERVISED DISCOVERY OF DECISION STATES FOR TRANSFER IN RL
```
Abstract:
```
We learn to identify 'decision states', namely the parsimonious set of states where decisions meaningfully affect the future states an agent can reach in an environment. We utilize the VIC framework (Gregor et al., 2016), which maximizes an agent's 'empowerment', i.e. the ability to reliably reach a diverse set of states - and formu- late a sandwich bound on the empowerment objective that allows identification of decision states. Unlike previous work (Goyal et al., 2019), our decision states are discovered without extrinsic rewards - simply by interacting with the world. Our results show that our decision states are: 1) often interpretable, and 2) lead to better exploration on downstream goal-driven tasks in partially observable environments.
```

Figures/Tables Captions:
```
Figure 1: Left: The VIC framework (Gregor et al., 2016) in a navigation context: an agent learns high-level macro-actions (or options) to reach different states in an environment reliably without any extrinsic reward. Right: DS-VIC identifies decision states (in red) associated with options where the agent is empowered to make informed decisions. The rest of the states in a trajectory (dotted lines in blue regions) then show default, option-independent behavior. Identification of decision states leads to improved transfer to novel environments.
Figure 2: Illustration of VIC for 2 timesteps. L: Given a start state S0, VIC samples option ω and follows policy π(at | Ω = ω, st) and infers Ω from the terminating state (S2), optimizing a lower bound on I(S2, Ω | S0). R: DS-VIC considers a particular parameterization of π and imposes a bottleneck on I(At, Ω|St).
Figure 3: Decision state heatmaps plotting I(Ω, Zt|St, S0) at visited states on simple envi- ronments - 4-Room (top) and maze (bottom). First column depicts environment layout, second and third show results for DS-VIC for β = 1e −3 and β = 1 respectively, and the fourth column shows DIAYN. Please refer to Appendix A.4 for details about how I(Ω, Zt|St, S0) is computed. * denotes lower bound (Eq. 1) on empowerment.
Figure 4: We visualize identified decision states for 3 options by DS-VIC on the Mountain Car environment. (a) shows the environment layout, (b) the trajectories corresponding to the 3 options in position-velocity space - if trajectories corresponding to two different options reach the same position and velocity, then they are said to intersect, (c) the final-state distributions in the position-velocity space, and (d) the identified decision states. In (b), (c), (d), x-axis (position) shows the x-coordinate of the car, while y-axis shows the velocity of the car.
Figure 5: Effect of β during DS-VIC pre-training on success with exploration bonus for the goal- conditioned policy. Shaded areas represent stan- dard error of the mean over 10 random seeds.
Figure 6: Transfer results on MultiRoomN6S25 after unsupervised pre-training to identify decision states on MultiRoomN2S6. Shaded regions represent standard errors of the mean over 10 random seeds.
Table 1: Success rate (mean ± standard error) of the goal-conditioned policy when trained with different exploration bonuses in addition to the extrinsic reward Re(t). We report results at ∼5 × 10 5 timesteps for MultiRoomN3S4, MultiRoomN5S4 and at ∼8.2 × 10 6 timesteps for MultiRoomN6S25. We also report the performance of InfoBot for completeness. Note that for rooms of size 4 (MultiRoomN3S4, MultiRoomN5S4), incentivizing to visit corners and doorways (Heuristic Decision States) is equivalent to count-based exploration.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Not all states in a decision making process are created equal. Consider the middle illustration in  Figure 1 , where a robot has four 'options', each representing a goal shape it can potentially reach. Given its spawn location, it can initially proceed straight regardless of the option it chooses until the intersection, at which point it needs to utilise 1 bit of (Shannon) information from the option variable to inform the choice of whether to turn left or right. However, right after it takes the left turn (say), it again does not need the goal information when choosing actions and can follow 'default' or 'option-independent' behaviour. Thus, there is a natural difference in the minimum amount of necessary goal information or 'relevant information' ( Polani et al., 2006 ) needed at different states. Identifying these 'decision states', i.e. states in the environment where a high amount of relevant goal information is needed, leads to better understanding of the environment structure, which has the potential to enable better transfer to novel environments and tasks. This has previously been shown by  Goyal et al. (2019)  in partially-observable, goal-driven (i.e. with extrinsic rewards) 2D navigation settings, where using decision states to guide exploration enables faster learning.

Section Title: Goal-independent Decision States
  Goal-independent Decision States Our key intuition is that decision states exist in an environment independent of extrinsic goals. For example, one can imagine the (middle) red tile at the intersection in  Figure 1  being a useful decision state - even across all possible navigation goals the agent could have. Ofcourse, not all intersections are decision states. For e.g., if the left room was full of lava, where the robot would die, thinking of the middle red tile as a decision state would have limited utility, since the relevant goal information (across all reachable goals) in this case would be 0 bits. Identifying 'unsupervised' (or task-agnostic) decision states is advantageous in scenarios where: 1) rewards are sparse or absent ( Pathak et al., 2017 ), 2) for an agent to learn meaningful behaviour, proxy goals and rewards need to be hand engineered (making it hard to scale), and 3) the notion of a goal might not even be obvious, e.g. in continuous control tasks ( Lillicrap et al., 2015 ). Our DS-VIC method identifies decision states without extrinsic rewards, solely by interacting with partially-observable environments. These decision states are a function of the environment as well as the states an agent can reliably reach. We demonstrate that our decision states generalise to novel environments and tasks, leading to comparable (or better) exploration and performance on downstream tasks compared to  Goyal et al. (2019)  that identifies goal-driven decision states.

Section Title: Decision States in Empowered Agents
  Decision States in Empowered Agents We build on the VIC framework (Gregor et al., 2016) ( Figure 1 ), where the core idea is to maximize the empowerment of an agent. In simple terms, an empowered agent is able to reliably reach a diverse set of states in the environment, while avoiding states that are difficult to reach reliably. We use VIC's formulation to learn options Ω that maximize (a lower bound J V IC on) the mutual information I(S f , Ω) where S f is the final state in a trajectory. We use latent options since we operate in a setting without external goals. To see why this maximizes Under review as a conference paper at ICLR 2020 empowerment, notice that I(S f , Ω) = H(S f ) − H(S f |Ω), where H(.) denotes entropy. Thus, empowerment maximizes the diversity in final states S f while learning options highly predictive of S f ( Salge et al., 2013 ). Using VIC, we identify decision states by attempting to hit the relevant information required at a particular state by minimizing the mutual information I(Ω, A t |S t ) (for every state S t , action A t ). This is similar in spirit to  Goyal et al. (2019) ;  Polani et al. (2006) , with the key difference that we use latent options Ω instead of external goals. Interestingly, we prove that t I(Ω, A t |S t ) is an upper bound on empowerment, which we minimize, in addition to maximizing the VIC lower bound J V IC . This shines a connection to VIC- while they optimize a lower bound, our work optimizes a sandwich bound on the empowerment 1 . To summarize our contributions, • We propose DS-VIC, a novel framework to identify decision states in a task-agnostic manner. These decision states align with our intuitive assessments in various partially-observed scenarios. • We provide an understanding of our proposed objective, proving that our framework optimizes a sandwich bound on the empowerment I(Ω, S f ). • We show that our mechanism to identify decision states is transferable and leads to improved sample-efficiency on goal-driven tasks in novel environments. On a challenging grid-world navigation task, our method outperforms (a re-implementation of)  Goyal et al. (2019) .

Section Title: METHODS
  METHODS

Section Title: NOTATION
  NOTATION We consider a Partially Observable Markov Decision Process (POMDP), defined by the tuple (S, A, P, r), where the (discrete or continuous) state-space S is a set of underlying but unobserved states s ∈ S and A denotes a discrete action space. P : S × S × A denotes an unknown transition function, representing p (s t+1 |s t , a t ) : s t , s t+1 ∈ S, A t ∈ A. Both VIC and DS-VIC train an option (Ω) conditioned policy π(a t |ω, s t ), where ω ∈ {1, · · · , |Ω|}. Following standard practice ( Cover & Thomas, 1991 ), we denote random variables in uppercase (Ω), and items from the sample space of random variables in lowercase (ω).

Section Title: VARIATIONAL INTRINSIC CONTROL (VIC)
  VARIATIONAL INTRINSIC CONTROL (VIC) The option Ω in VIC is a global latent variable underlying a trajectory τ = {S 0 , A 0 , · · · , S f } ( Figure 2 ). To encourage the agent to reach a diverse set of states reliably, VIC proposes maximizing the mutual information ( Cover & Thomas, 1991 ) between Ω and final state S f given s 0 , i.e. I(S f , Ω | S 0 = s 0 ). Informally, this maximizes the empowerment for an agent, i.e. its internal options Ω have a high degree of correspondence to the states of the world S f that it can reach. VIC formulates a variational lower bound on this mutual information. Specifically, let p(ω | s 0 ) = p(ω) be a prior on options (we keep the prior fixed as per  Eysenbach et al. (2018) ), p J (s f | ω, s 0 ) is defined as the (unknown) terminal state distribution achieved when executing the policy π(a t | ω, s t ), and q ν (ω | s f , s 0 ) denote a (parameterized) variational approximation to the true posterior on options

Section Title: DECISON STATE VIC (DS-VIC)
  DECISON STATE VIC (DS-VIC) We identify decision states by employing an information regularizer that finds states with minimum relevant option information. Formally, this means that at every timestep t in the trajectory, we minimize the mutual information I(Ω, A t |S t , S 0 = s) and the resulting states where this mutual information remains high despite the minimization are identified as decision states. Intuitively, this means that on average (across different options), a decision state is a state with higher relevant option information than other states (e.g. the red regions in  Figure 1 ) Overall, our objective is to maximize: J V IC (Ω, S f ; s 0 ) − β t I(Ω, A t | S t , S 0 = s 0 ) (2) where β > 0 is a trade-off parameter. Thus, this is saying that one wants options Ω which allow the agent to have a high empowerment, while utilizing the least relevant information at each step. Interestingly, Equation 2 has a clear, principled interpretation in terms of the empowerment I(Ω, S f |S 0 ) from the VIC model. We state the following lemma (full proof in Appendix): Lemma 2.1 Let A t be the action random variable at timestep t and state S t following an option- conditioned policy π(a t |s t , ω). Then, I(Ω, A t |S t , S 0 ) i.e. the conditional mutual information between the option Ω and action A t when summed over all timesteps in the trajectory, upper bounds the condi- tional mutual information I(Ω, S f |S 0 ) between Ω and the final state S f - namely the empowerment as defined by  Gregor et al. (2016) :

Section Title: Implications
  Implications With this lens, one can view the optimization problem in Equation 2 as a Lagrangian relaxation of the following constrained optimization problem: max J V IC s.t. U DS ≤ R (4) where R > 0 is a constant. Thus, we can essentially target values of R we have in mind by maximizing the lower bound and minimizing the upper bound, by (in practice) sweeping different values of β in eq. (2). This is a result potentially of interest beyond the specific context of decision states ( Achiam et al., 2018 ; Gregor et al., 2016). Specifically, while the VIC objective only allows us to maximize the empowerment I(Ω, S f ) = H(S f ) − H(S f |Ω), imposing an upper bound can help control how 'abstract' the latent option representation is relative to the states S f , by finding solutions that have say a higher entropy H(S f |Ω) on states given options. This might lead to more abstract options that achieve better generalisation depending on downstream tasks. Note that most approaches currently limit the abstraction by constraining the number of discrete options, which (usually) imposes a (tighter) upper bound on H(Ω), since H ≥ 0 in the discrete case. However, this does not hold for the continuous case, where the bound might be more useful. Investigating this is beyond the scope of this current paper, however, as our central aim is to identify decision states, and not to scale the VIC framework to continuous options.

Section Title: ALGORITHMIC DETAILS
  ALGORITHMIC DETAILS Upper Bounds for I(Ω, A t | S t , S 0 ). Inspired by InfoBot ( Goyal et al., 2019 ), we bottleneck the information in a statistic Z t of the state S t and option Ω used to parameterize the policy π(A t | Ω, S t ) ( fig. 2  right). This is justified by the the data-processing inequality (DPI) ( Cover & Thomas, 1991 ) for the markov chain Ω, S t ↔ Z t ↔ A t , which implies I(Ω, A t | S t , S 0 ) ≤ I(Ω, Z t | S t , S 0 ). We can then obtain the following upper bound on I(Ω, Z t | S t , S 0 ) (see appendix for derivation): I(Ω, Z t | S t , S 0 = s) ≤ E Ω∼p(ω),St∼p J (st|Ω,S0=s),Zt∼p(zt|St,Ω) log p(Z t | Ω, S t ) q(Z t ) , (5) where q(z t ) is a fixed variational approximation (set to N (0, I) as in InfoBot), and p φ (z t | ω, s t ) is a parameterized encoder. As explained in section 1 the key difference between eq. (5) and InfoBot is that they construct upper bounds on I(G, A t | S t , S 0 ), while we bottleneck the option-information. We can compute a Monte Carlo estimate of Equation 5 by first sampling an option ω at s 0 and then keeping track of all states visited in trajectory τ . In addition to the VIC term and our bottleneck regularizer, we also include the entropy of the policy over the actions (maximum-entropy RL ( Ziebart, 2010 )) as a bonus to encourage sufficient exploration. Overall, the DS-VIC objective is: max θ,φ,νJ (θ, φ, ν) = E Ω∼p(ω),τ ∼p J (·|Ω,S0),Zt∼p φ (zt|St,Ω) log q ν (Ω | S f , S 0 ) p(Ω) − f −1 t=0 β log p φ (Z t | S t , Ω) q(Z t ) + α log π θ (A t | S t , Z t ) (6) where θ, φ and ν are the parameters of the policy, latent variable decoder and the option inference network respectively. Note that we abuse the notation slightly in Equation 6 by writing τ ∼ p J (· | ω, s 0 ) to denote a trajectory sampled by following π(· | ω, s 0 ). This is a sequential process where Z t ∼ p φ (z t | s t , ω), A t ∼ π θ (a t | s t , z t ) and S t+1 ∼ p(s t+1 | s t , a t ) (state transition function). The first term in the objective ensures reliable control in terms of visitation over a diverse set of final states in the environment while learning options; the second term ensures minimality in using the options sampled to make decisions (taking actions) and the third provides an incentive for exploration. See algorithm 1 in appendix for more details on the entire training pipeline.

Section Title: Handling Partial Observability
  Handling Partial Observability We are generally interested in identifying decision states in partially observable environments (similar to InfoBot  Goyal et al. (2019) ). To formalise how we handle this, let o t be the (partial) observation made by the agent at time t. We set the state s t = g(o 0 , · · · , o t ) in all the equations above, where g is a recurrent neural network (RNN) ( Hochreiter & Schmidhuber, 1997 ) encoding partial observations. We found it important to parameterize the policy ( Figure 2 , right) as: π(a t | ω, s t ) = dz p(z | s t , ω)p(a t | o t , z), i.e. where action distribution p(a t | o t , z) are modelled as 'reactive' (conditioned only on the current observation) but z distributions p(z t | s t , ω) are 'recurrent' (conditioning on the entire history of observations via g(·)). This is because the sequence of observations {o 1 , · · · , o t } might be informative of the option Ω being followed i.e. conditioning p(a t | ·) on S t in addition to z could 'leak' information about the underlying option to the actions, making it the bottleneck penalty on z potentially ineffective. Finally, when maximizing the likelihood of option Ω given the final state S f in learning q(ω | s f ) in VIC (the option inference network), we use the global (x, y) coordinates of the final and initial state of the agent. Note that the agent's policy continues to be based only on observations o t , and it is only at the end of the trajectory that we use the global coordinates to compute intrinsic reward. Please refer to Sec. A.6 for a discussion of the issues when training the model with partial observations in the option inference network. Related Objectives.  Eysenbach et al. (2018)  (DIAYN) attempts to learn skills (similar to options) which can control the states visited by agents while ensuring that states, not actions, are used to distinguish skills. Thus, for an option Ω and every state S t in a trajectory, they maximize t I(Ω, S t ) − I(A t , Ω|S t ) + H(A t |S t ), as opposed to I(Ω, S f ) − β t I(A t , Ω|S t ) + H(A t |S t ) in our objective. With the sum over all timesteps for I(Ω, S t ), the bound in theorem 2.1 no longer holds true, which also means that there is no principled reason (unlike our model) to scale the second term with β - which we find to be crucial for identifying decision states (similar to findings in InfoBot). Experimentally, we show that the decision states with DIAYN are no longer as interpretable, since options in this case are more low level and correspond to trajectories instead of goals.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 The most closely related work to ours is InfoBot, which maximizes t R(t) − βI(A t , G|S t ) for a goal (G) conditioned policy π(a t |S t , G). The key difference is that InfoBot requires extrinsic rewards, while our work is strictly more general and scales even in the absence of extrinsic rewards. Further, in context of both these works, our work provides a principled connection between action- option information reqularization I(A t , Ω|S t ) and empowerment of an agent. The tools from Theo- rem 2.1 might be useful for analysing these previous objectives which both employ this technique.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: TRANSFER TO GOAL-DRIVEN TASKS
  TRANSFER TO GOAL-DRIVEN TASKS Do the unsupervised decision states meaningfully transfer to novel tasks? To address this, we replicate the InfoBot experimental setup, replacing decision state identification (pre-training) with DS-VIC. Exploration-based Transfer.  Goyal et al. (2019)  pretrain their model to identify decision states, and then study if decision states improve exploration when training a new policy π γ (a|s, g) from scratch. Given an environment with reward R e (t), goal G, κ > 0, and state visitation count c(S t ), their reward is: The count-based reward 2 decays with square root of c(S t ) to encourage the model to explore novel states, and the mutual information between goal G and bottleneck variable Z t identifies decision states, and is multiplied with the exploration bonus to encourage visitation of decision states. We use an almost identical setup, replacing their decision-state term from supervised pretraining with DS-VIC intrinsic reward pretraining: I(·) is computed with eq. (5) with a parameter- ized p(z t | ω, s t ) encoder frozen during transfer. See Algorithm 1 in appendix for more details.

Section Title: Environments
  Environments We pre-train and test on grid- worlds from the MiniGrid ( Chevalier-Boisvert et al., 2018 ) environments. We first consider a set of simple environments - 4-Room and Maze (see  Fig. 3 ) followed by the MultiRoomNXSY also used by  Goyal et al. (2019) . The MultiRoomNXSY environments consist of X rooms of size Y, con- nected in random orientations across multiple runs. We also conduct experiments on Mountain-Car ( Brockman et al., 2016 ), a continuous state-space environment with discrete actions. In all pre-training environments, options are executed for a fixed number of steps before terminating. We use Advantage Actor-Critic (A2C) for all experiments. Since code for InfoBot ( Goyal et al., 2019 ) was not public, we report numbers based on a re-implementation of InfoBot, ensuring consistency with their architectural and hyperparameter choices. We will release our code upon publication.

Section Title: RESULTS
  RESULTS

Section Title: Baselines
  Baselines We evaluate the following on quality of exploration and transfer to downstream goal-driven tasks with sparse rewards: 1) InfoBot (our implementation) - which does goal-driven extraction of decision states, 2) DIAYN - whose focus is unsupervised skill acquisition (and not decision Under review as a conference paper at ICLR 2020 states) 3 , but has an I(A t , Ω|S t ) term which can be used for the bonus in Equation 8, 3) count-based exploration which uses visitation counts as exploration incentive (this corresponds to replacing I(Ω, Z t |S t , S 0 ) with 1 in Equation 8), 4) a randomly initialized encoder p(z t | ω, s t ) to check if learning in DS-VIC or InfoBot models improves over random initialization, 5) how different values of β affect performance, and 6) a heuristic baseline that looks for local features like corners and doorways and uses every such occurrence as a decision state. This validates the extent to which relevant goal information is useful in identifying decision states, vs. local, feature-based heuristics.

Section Title: QUALITATIVE RESULTS
  QUALITATIVE RESULTS Grid Worlds:  Figure 3  shows results on 4-Room and maze. We notice that when β = 1, DS-VIC collapses to identifying every state as a decision state and gets poor empowerment: 0.51 (row 1, column 3 in  Figure 3 ), indicating that the regularization strength is too high and hinders empowerment maximization. At lower values of β = 1e-3, we get more discernible decision states. Across both environments, we notice that some corners and intersections are identified as decision states. In general, for DS-VIC on maze with β = 1e-3 (when it gets an empowerment of 2.61), we notice that if an intersection is a decision state, and there is a corner next to it, it is often not a decision state. That is, not every state which looks like an corner needs to be a decision state, if for example the part of the state space in question is only spanned by one option. Finally, for maze we see that for a similar value of empowerment 4 , DS-VIC leads to a sparser decision state distribution than DIAYN. This makes sense, since intuitively, one expects to have to make more decisions when following a particular trajectory (as in DIAYN), as opposed to a terminal state (as in DS-VIC).

Section Title: Mountain Car Environment
  Mountain Car Environment We also use DS-VIC to identify decision states in the Mountain Car ( Brockman et al., 2016 ) environment - a continuous state-space (position and velocity of the car) environment with three discrete actions {+1(right), 0, −1(left)}, which correspond to the direction of the force being applied to the car (see Fig. 4(a)). Fig. 4(b) shows the trajectories and Fig. 4(c) shows the final state(s) corresponding to 3 specific options (out of 8) overlaid on a position-velocity plane. Upon observing decision states in the same position-velocity plane (see Fig. 4(d), point-radius proportional to I(Ω, Z t |s t , s 0 ), we find that decision states occur at points in the state-space where trajectories associated with different options intersect. Intuitively, these are the states where the agent can switch from one option (mode of behavior) to another and therefore, needs to make a decision. Interestingly, there is a dominance of decision states along the velocity = 0 line, which indicates that the model identifies decision states to be points where the cart has velocity = 0 at some position (specific to the option). Finally, decision states often occur, for say, the orange option (Ω 1 ) in the right half of the position-velocity plane and never in the bottom-left quadrant, where it is the only option covering that part of the state-space, and thus default behavior can be practiced.

Section Title: QUANTITATIVE RESULTS
  QUANTITATIVE RESULTS Transfer to Goal-Driven Tasks. Next, we evaluate Equation 8, i.e. whether incentivizing the agent to visit decision states in addition to extrinsic reward can aid in transfer to goal-driven tasks in different environments (see Sec. 3.1). We restrict ourselves to the point-navigation task ( Goyal et al., 2019 ) transfer in the MultiRoomNXSY set of partially-observable environments. In this task, the agent is always spawned in the first room, and has to go to a randomly sampled goal location in the last room, and is rewarded only when it reaches the correct cell.  Goyal et al. (2019)  test the efficacy of different exploration objectives 5 and show that this is a hard setting where efficient exploration is necessary. They show that InfoBot outperforms several state-of-the art exploration methods in this environment. Concretely, we 1) train DS-VIC to identify de- cision states (Equation 2) on MultiRoomN2S6 and transfer to a goal-driven task on MultiRoomN3S4 and MultiRoomN5S4 (similar to  Goyal et al. (2019) ), and 2) train on MultiRoomN2S10 and transfer to MultiRoomN6S25, which is a more challenging transfer task i.e. it has larger rooms (25x25) making efficient exploration critical to find doors quickly. For DS- VIC and DIAYN, we train for 6 million episodes in the decision state identification phase (pre-training) and pick the checkpoints with highest empowerment values across training. Overall Trends.  Table. 1  reports success rate - the % of times the agent reaches the goal on validation. First, our implementation of InfoBot is competitive with  Goyal et al. (2019)   6  . For the MultiRoomN2S6 to N5S4 transfer (middle column), methods which identify decision states outperform count-based exploration. Interestingly, in this setup, a randomly initialized encoder is competitive with both InfoBot and DS-VIC, and serves as an important baseline. In MultiRoomN2S10 to N6S25 transfer, which has a large state space, we find that DS-VIC (at β = 10 −2 ) achieves the best performance, followed by count-based exploration and InfoBot (that are both within standard-error of each other). This shows that decision states identified by DS-VIC effectively guide exploration in sparse-reward settings. More importantly, our model beats a strong heuristic baseline which treats every corner and doorway as a decision state (more details on this are in the appendix). Finally, the randomly initialized encoder as well as DIAYN generalize much worse in this transfer task. β sensitivity. We sweep over β in log-scale from {10 −1 , · · · , 10 −6 }, as shown in  Figure 5  (except β = 10 −1 which does not converge to > 0 empowerment) and also report β = 0 which recovers a no information regularization baseline. We find that 10 −2 works best, followed by 10 −5 and 10 −3 . In general, performance is robust, i.e. across β, we seem to reach ≈ 90% success for most values. More results on N6S25. To better understand Equation 8 compared to other exploration incentives, in addition to success rate and sample-efficiency ( Figure 6 ), we also consider another metric where we count the number of unique rooms reached (other than the starting room 0) as training progresses ( Figure 6 (c) ). Across metrics, DS-VIC performs the best on the challenging N6S25 transfer. Furthermore, the unique room metric reveals that DIAYN gets stuck after exploring the first room.

Section Title: RELATED WORK
  RELATED WORK

Section Title: Intrinsic Control and Intrinsic Motivation
  Intrinsic Control and Intrinsic Motivation Learning how to explore without extrinsic rewards is a foundational problem in Reinforcement Learning ( Machado et al., 2017 ;  Pathak et al., 2017 ; Gregor et al., 2016;  Strehl & Littman, 2008 ;  Schmidhuber, 1990 ). Typical curiosity-driven approaches attempt to visit states that maximize the surprise of an agent ( Pathak et al., 2017 ) or improvement in predictions from a dynamics model ( Stadie et al., 2015 ;  Lopes et al., 2012 ). While curiosity-driven approaches seek out and explore novel states, they typically do not measure how reliably the agent can reach them. In contrast, approaches for intrinsic control ( Eysenbach et al., 2018 ;  Achiam et al., 2018 ; Gregor et al., 2016) explore novel states while ensuring those states are reliably reachable.  Gregor et al. (2016)  maximize the number of final states that can be reliably reached by the policy, while  Eysenbach et al. (2018)  distinguish an option at every state along the trajectory, and  Achiam et al. (2018)  learn options for entire trajectories by encoding the sequence of states at regular intervals. Since decision states are more related to reliably acting in an environment rather than just visiting novel states (without an estimate of reachability), we formulate our regularizer in the intrinsic control framework, specifically building on the work of  Gregor et al. (2016) .

Section Title: Default Behavior and Decision States
  Default Behavior and Decision States Recent work in policy compression has focused on learning a default policy when training on a family of tasks, to be able to re-use behavior across tasks. In ( Galashov et al., 2018 ;  Teh et al., 2017 ), default behavior is learnt using a set of task-specific policies which then regularizes each policy, while  Goyal et al. (2019)  learn a default policy using an information bottleneck on task information and a latent variable the policy conditions on. We devise a similar information regularization objective that learns default behavior shared by all intrinsic options without external rewards so as to reduce learning pressure on option-conditioned policies.

Section Title: Bottleneck states in MDPs
  Bottleneck states in MDPs There is rich literature on identification of bottleneck states in MDPs. The core idea is to either identify all the states that are common to multiple goals in an environment ( McGovern & Barto, 2001 ) or use a diffusion model built using an MDP's transition matrix ( Machado et al., 2017 ; Şimşek & Barto, 2004 ;  Theocharous & Mahadevan, 2002 ). The key distinction between bottleneck states and decision states is that the latter are more closely tied to the information available to the agent and what it can act upon, whereas bottleneck states are more tied to the connectivity structure of an MDP and intrinsic to the environment, representing states which when visited allow access to a novel set of states ( Goyal et al., 2019 ). Concretely, while a corner in a room need not be a bottleneck state, since visiting a corner does not 'open up' new states (the way a doorway would), it is still a useful state for a goal-driven agent with partial observation to visit (since it is a distinct landmark in the state space where meaningful decisions can be made).

Section Title: Information Bottleneck in Machine Learning
  Information Bottleneck in Machine Learning Since the foundational work of  Tishby et al. (1999) ;  Chechik et al. (2005) , there has been a lot of interest in making use of ideas from information bottleneck (IB) for various tasks such as clustering ( Strouse & Schwab, 2017 ;  Still et al., 2004 ), sparse coding ( Chalk et al., 2016 ), classification using deep learning ( Alemi et al., 2016 ;  Achille & Soatto, 2016 ), cognitive science and language ( Zaslavsky et al., 2018 ) and reinforcement learning ( Goyal et al., 2019 ;  Galashov et al., 2018 ;  Strouse et al., 2018 ). We apply an information regularizer to an RL agent that must learn without extrinsic rewards to identify decision states.

Section Title: CONCLUSION
  CONCLUSION We devise a principled approach to identify decision states in an environment without any extrinsic reward supervision using a sandwich bound on the empowerment of  Gregor et al. (2016) . Our approach yields decision states that align with human intuition across environments, and aid directed exploration on external-reward tasks and subsequently lead to better success rate and sample com- plexity in novel environments (competitive to supervised decision states of  Goyal et al. (2019) ). All our code and environments will be made publicly available.
  We found it important to run all models (inlcuding InfoBot) an order of magnitude more steps compared to  Goyal et al. (2019) , but our models also appear to converge to higher success values.

```
