Title:
```
Under review as a conference paper at ICLR 2020 ATTRACTION-REPULSION ACTOR-CRITIC FOR CON- TINUOUS CONTROL REINFORCEMENT LEARNING
```
Abstract:
```
In reinforcement learning, robotic control tasks are often useful for understanding how agents perform in environments with deceptive rewards where the agent can easily become trapped into suboptimal solutions. One way to avoid these local optima is to use a population of agents to ensure coverage of the policy space (a form of exploration), yet learning a population with the "best" coverage is still an open problem. In this work, we present a novel approach to population-based RL in continuous control that leverages properties of normalizing flows to perform attractive and repulsive operations between current members of the population and previously observed policies. Empirical results on the MuJoCo suite demonstrate a high performance gain for our algorithm compared to prior work, including Soft-Actor Critic (SAC).
```

Figures/Tables Captions:
```
Figure 1: a) Augmenting the loss function with AR constraints allows an agent to reach a target policy by following different paths. Attractive and Repulsive policies represent any other agent's policy. b) General flow of the proposed ARAC strategy.
Figure 2: Agent trained to imitate a target while avoiding a repulsive policy using a proactive strategy. Increasing the number of flows leads to more complex policy's shape.
Figure 3: Average return and one standard deviation on 5 random seeds across 7 MuJoCo tasks for ARAC against single SAC agents (with and without NFs). Curves are smoothed using Savitzky-Golay filtering with window size of 7.
Figure 4: Average return and one standard deviation on 5 random seeds across 8 MuJoCo tasks.
Table 1: Maximum average return after 1M (2M for Humanoid (rllab) and 600k for SparseHumanoid-v2) time steps 5 random seeds. Bold: best methods when the gap is less than 100 units. See appendix for average return with standard deviation. Environment short names: HC:
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Many important reinforcement learning (RL) tasks, such as those in robotics and self-driving cars, are challenging due to large action and state spaces ( Lee et al., 2018 ). In particular, environments with large continuous action spaces are prone to deceptive rewards, i.e. fall into local optima in learning ( Conti et al., 2018 ). Applying traditional policy optimization algorithms to these domains often leads to locally optimal, yet globally sub-optimal policies. The agent should then explore the reward landscape more thoroughly in order to avoid falling into these local optima. Not all RL domains that require exploration are suitable for understanding how to train agents that are robust to deceptive rewards. For example, Montezuma's Revenge, a game in the Atari Learning Environment ( Bellemare et al., 2013 ), has sparse rewards; algorithms that perform the best on this task encourage exploration by providing a denser intrinsic reward to the agent to encourage exploration ( Tang et al., 2017 ). On the other hand, many robotic control problems, such as those found in MuJoCo ( Todorov et al., 2012 ), provide the agent with a dense reward signal, yet their high-dimensional action spaces induce a multimodal, often deceptive, reward landscape. For example, in the biped environments, coordinating both arms and legs is crucial for performing well on even simple tasks such as forward motion. However, simply learning to maximize the reward can be detrimental across training: agents will tend to run and fall further away from the start point rather than discovering stable and efficient walking motion. In this setting, exploration serves to provide a more reliable learning signal for the agent by covering more different types of actions during learning. One way to maximize action space coverage is the maximum entropy RL framework ( Ziebart, 2010 ), which prevents variance collapse by adding a policy entropy auxiliary objective. One such prominent algorithm, Soft Actor-Critic (SAC, Haarnoja et al. (2018) ), has been shown to excel in large continuous action spaces. To further improve on exploration properties of SAC, one can maintain a population of agents that cover non-identical sections of the policy space. To prevent premature convergence, a diversity-preserving mechanism is typically put in place; balancing the objective and the diversity term becomes key to converging to a global optimum ( Hong et al., 2018 ). This paper studies a particular family of population-based exploration methods, which conduct coordinated local search in the policy space. Prior work on population-based strategies improves performance on robotic control domains through stochastic perturbation on a single actor's parameter ( Pourchot & Sigaud, 2019 ) or a set of actor's parameters ( Conti et al., 2018 ;  Khadka & Tumer, 2018 ;  Liu et al., 2017 ). We hypothesize that exploring directly in the policy space will be more effective than perturbing Under review as a conference paper at ICLR 2020 the parameters of the policy, as the latter does not guarantee diversity (i.e., different neural network parameterizations can approximately represent the same function). Given a population of RL agents, we enforce local exploration using an Attraction-Repulsion (AR) mechanism. The later consists in adding an auxiliary loss to encourage pairwise attraction or repulsion between members of a population, as measured by a divergence term. We make use of the Kullback- Leibler (KL) divergence because of its desirable statistical properties and its easiness of computation. However, naively maximizing the KL term between two Gaussian policies can be detrimental (e.g. drives both means apart). Because of this, we parametrize the policy with a general family of distributions called Normalizing Flows ( NFs, Rezende & Mohamed, 2015 ); this modification allows to improve upon AR+Gaussian (see Appendix Figure 6). NFs are shown to improve the expressivity of the policies using invertible mappings while maintaining entropy guarantees ( Mazoure et al., 2019 ;  Tang & Agrawal, 2018 ). Nonlinear density estimators have also been previously used for deep RL problems in contexts of distributional RL ( Doan et al., 2018 ) and reward shaping ( Tang et al., 2017 ). The AR objective blends particularly well with SAC, since computing the KL requires stochastic policies with tractable densities for each agent.

Section Title: PRELIMINARIES
  PRELIMINARIES We first formalize the RL setting in a Markov decision process (MDP). A discrete-time, finite-horizon, MDP ( Bellman, 1957 ;  Puterman, 2014 ) is described by a state space S, an action space A, a transition function P : S × A × S → R + , and a reward function r : S × A → R. 1 On each round t, an agent interacting with this MDP observes the current state s t ∈ S, selects an action a t ∈ A, and observes a reward r(s t , a t ) ∈ R upon transitioning to a new state s t+1 ∼ P(s t , a t ). Let γ ∈ [0, 1] be a discount factor. The goal of an agent evolving in a discounted MDP is to learn a policy π : S × A → [0, 1] such as taking action a t ∼ π(·|s t ) would maximize the expected sum of discounted returns, In the following, we use ρ π to denote the trajectory distribution induced by following policy π. If S or A are vector spaces, action and space vectors are respectively denoted by a and s.

Section Title: DISCOVERING NEW SOLUTIONS THROUGH POPULATION-BASED ATTRACTION-REPULSION
  DISCOVERING NEW SOLUTIONS THROUGH POPULATION-BASED ATTRACTION-REPULSION Consider evolving a population of M agents, also called individuals, {π θm } M m=1 , each agent corre- sponding to a policy with its own parameters. In order to discover new solutions, we aim to generate agents that can mimic some target policy while following a path different from those of other policies. Let G denote an archive of policies encountered in previous generations of the population. A natural way of enforcing π to be different from or similar to the policies contained in G is by augmenting the loss of the agent with an Attraction-Repulsion (AR) term: L AR = − E π ∼G β π D KL [π||π ] , (1) where π is an archived policy and β π is a coefficient weighting the relative importance of the Kullback-Leibler (KL) divergence between π and π , which we will choose to be a function of the average reward (see Sec. 3.2 below). Intuitively, Eq. 1 adds to the agent objective a weighted average distance between the current and the archived policies. For β π ≥ 0, the agent tends to move away from the archived policy's behavior (i.e. repulsion, see  Figure 1 ) a). On the other hand, β π < 0 encourages the agent π to imitate π (i.e. attraction).

Section Title: Requirements for AR
  Requirements for AR In order for agents within a population to be trained using the proposed AR-based loss (Eq. 1), we have the following requirements: 1. Their policies should be stochastic, so that the KL-divergence between two policies is well-defined. Under review as a conference paper at ICLR 2020 2. Their policies should have tractable distributions, so that the KL-divergence can be computed easily, either with closed-form solution or Monte Carlo estimation. Several RL algorithms enjoy such properties ( Haarnoja et al., 2018 ;  Schulman et al., 2015 ; 2017). In particular, the soft actor-critic (SAC,  Haarnoja et al., 2018 ) is a straightforward choice, as it currently outperforms other candidates and is off-policy, thus maintains a single critic shared among all agents (instead of one critic per agent), which reduces computation costs.

Section Title: SOFT ACTOR-CRITIC
  SOFT ACTOR-CRITIC SAC ( Haarnoja et al., 2018 ) is an off-policy learning algorithm which finds the information projection of the Boltzmann Q-function onto the set of diagonal Gaussian policies Π: π = arg min π ∈Π D KL π (.|s t ) exp ( 1 α Q πold (s t , .)) Z πold (s t ) , where α ∈ (0, 1) controls the temperature, i.e. the peakedness of the distribution. The policy π, critic Q, and value function V are optimized according to the following loss functions: where B is the replay buffer. The policy used in SAC as introduced in  Haarnoja et al. (2018)  is Gaussian, which is both stochastic and tractable, thus compatible with our AR loss function in Eq. 1. Together with the AR loss in Eq. 1, the final policy loss becomes: However, Gaussian policies are arguably of limited expressibility; we can improve on the family of policy distributions without sacrificing qualities necessary for AR or SAC by using Normalizing Flows ( NFs, Rezende & Mohamed, 2015 ).

Section Title: NORMALIZING FLOWS
  NORMALIZING FLOWS NFs ( Rezende & Mohamed, 2015 ) were introduced as a means of transforming simple distributions into more complex distributions using learnable and invertible functions. Given a random variable z 0 with density q 0 , they define a set of differentiable and invertible functions, {f i } N i=1 , which generate a sequence of d-dimensional random variables, {z i } N i=1 . Because SAC uses explicit, yet simple parametric policies, NFs can be used to transform the SAC policy into a richer one (e.g., multimodal) without risk loss of information. For example,  Mazoure et al. (2019)  enhanced SAC using a family of radial contractions around a point z 0 ∈ R d , f (z) = z + β α + ||z − z 0 || 2 (z − z 0 ) (6) for α ∈ R + and β ∈ R. This results in a rich set of policies comprised of an initial noise sample a 0 , a state-noise embedding h θ (a 0 , s t ), and a flow {f φi } N i=1 of arbitrary length N , parameterized by φ = {φ i } N i=1 . Sampling from the policy π φ,θ (a t |s t ) can be described by the following set of equations:

Section Title: ARAC: ATTRACTION-REPULSION ACTOR-CRITIC
  ARAC: ATTRACTION-REPULSION ACTOR-CRITIC We now detail the general procedure for training a population of agents using the proposed diversity- seeking AR mechanism. More specifically, we consider here SAC agents enhanced with NFs ( Ma- zoure et al., 2019 ).  Figure 1  displays the general flow of the procedure. Algorithm 1 (Appendix) provides the pseudo-code of the proposed ARAC strategy, where sub-procedures for rollout and archive update can be found in the Appendix. Overview ARAC works by evolving a population of M SAC agents {π m φ,θ } M m=1 with radial NFs policies (Eq. 7) and shared critic Q ω , and by maintaining an archive of policies encountered in previous generations of the population. After performing T steps per agent on the environment (Alg. 1 L8-12), individuals are evaluated by performing R rollouts 2 on the environment (Alg. 1 L26-28). This allows to identify the top-K best agents (Alg. 1 L29), also called elites, which will be used to update the critic as they provide the most meaningful feedback (Alg. 1 L13-17). The archive is finally updated in a diversity-seeking fashion using the current population (Alg. 1 L30). The core component of the proposed approach lies within the update of the agents (Alg. 1 L18-25). During this phase, elite individuals are updated using AR operations w.r.t. policies sampled from the archive (Eq. 5), whereas non-elites are updated regularly (Eq. 2).

Section Title: ENHANCING DIVERSITY IN THE ARCHIVE
  ENHANCING DIVERSITY IN THE ARCHIVE Throughout the training process, we maintain an archive G of maximum capacity G, which contains some previously encountered policies. The process goes as follow: until reaching full capacity, the archive saves a copy of the parameters of every individual in the population after the evaluation step. However, by naively adding all individuals as if the archive were just a heap, the archive could end up filled with policies leading to similar rewards, which would result in a loss of diversity ( Mauldin, 1984 ). We mitigate this issue by keeping track of two fitness clusters (low and high) using the partition formed by running a k-means algorithm on the fitness value. Hence, when |G| = G is reached and a new individual is added to the archive, it randomly replaces an archived policy from its respective cluster. This approach, also known as niching, has proved itself effective at maintaining high diversity levels ( Gupta & Ghafir, 2012 ;  Mahfoud, 1995 ).

Section Title: DISCOVERING NEW POLICIES THROUGH ATTRACTION-REPULSION
  DISCOVERING NEW POLICIES THROUGH ATTRACTION-REPULSION The crux of this work lies in the explicit search for diversity in the policy space achieved using the AR mechanism. Since the KL between two base policies (i.e. input of the first flow layer) can be trivially maximized by driving their means apart, we apply attraction-repulsion only on the flow layers, while holding the mean of the base policy constant. This ensures that the KL term doesn't depend on the difference in means and hence controls the magnitude of the AR mechanism. Every time the AR operator is applied (Alg. 1 L20-21), n policies are sampled from the archive and are used for estimating the AR loss (Eq. 1). As in  Hong et al. (2018) , we consider two possible strategies Under review as a conference paper at ICLR 2020 to dictate the value of β π coefficients for policies π ∼ G: where f (π) 3 represents the fitness function of policy π (average reward in our case), and f min and f max are estimated based on the n sampled archived policies. The proactive strategy aims to mimic high reward archived policies, while the reactive strategy is more cautious, only repulsing away the current policy from low fitness archived policies. Using this approach, the current agent policy will be attracted to some sampled policies (β π < 0) and will be repulsed from others (β π ≥ 0) in a more or less aggressive way, depending on the strategy. Unlike  Hong et al. (2018)  who applied proactive and reactive strategies on policies up to 5 timesteps back, we maintain an archive consisting of two clusters seen so far: policies with low and high fitness, respectively. Having this cluster allows to attract/repulse from a set of diverse agents, replacing high-reward policies by policies with similar performance. Indeed, without this process, elements of the archive would collapse on the most frequent policy, from which all agents would attract/repulse. To avoid performing AR against a single "average policy" , we separate low-reward and high-reward agents via clustering.

Section Title: RELATED WORK
  RELATED WORK The challenges of exploration are well studied in the RL literature. Previously proposed approaches for overcoming hard exploration domains tend to either increase the capacity of the state-action value function ( Gal & Ghahramani, 2016 ;  Henderson et al., 2017 ) or the policy expressivity ( Mazoure et al., 2019 ;  Tang & Agrawal, 2018 ;  Touati et al., 2018 ). This work rather tackles exploration from a diverse multi-agent perspective. Unlike prior population-based approaches for exploration ( Conti et al., 2018 ;  Khadka & Tumer, 2018 ;  Pourchot & Sigaud, 2019 ), which seek diversity through the parameters space, we directly promote diversity in the policy space. The current work was inspired by  Hong et al. (2018) , who relied on the KL divergence to at- tract/repulse from a set of previous policies to discover new solutions. However, in their work, the archive is time-based (they restrict themselves to the 5 most recent policies), while our archive is built following a diversity-seeking strategy (i.e., niching and policies come from multiple agents). Notably, ARAC is different of previously discussed works in that it explores the action space in multiple regions simultaneously, a property enforced through the AR mechanism. The proposed approach bears some resemblance with  Liu et al. (2017) , who took advantage of a multi-agent framework in order to perform repulsion operations among agents using of similarity kernels between parameters of the agents. The AR mechanism gives rise to exploration through structured policy rather than randomized policy. This strategy has also been employed in multi-task learning ( Gupta et al., 2018 ), where experience on previous tasks was used to explore on new tasks.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: DIDACTIC EXAMPLE
  DIDACTIC EXAMPLE Consider a 2-dimensional multi-armed bandit problem where the actions lie in the real square [−6, 6] 2 . We illustrate the example of using a proactive strategy where a SAC agent with radial flows policy imitates a desirable (expert) policy while simultaneously repelling from a less desirable policy. The task consists in matching the expert's policy (blue density) while avoiding taking actions from a repulsive policy π (red). We illustrate the properties of radial flows in  Figure 2  by increasing the number of flows (where 0 flow corresponds to a Gaussian distribution). We observe that increasing the number of flows (bottom to top) leads to more complex policy's shapes and multimodality unlike the Gaussian policy which has its variance shrinked (the KL divergence Under review as a conference paper at ICLR 2020 is proportional to the ratio of the two variances, hence maximizing it can lead to a reduction in the variance which can be detrimental for exploration purpose). Details are provided in Appendix.

Section Title: MUJOCO LOCOMOTION BENCHMARKS
  MUJOCO LOCOMOTION BENCHMARKS We now compare ARAC against the CEM-TD3 ( Pourchot & Sigaud, 2019 ), ERL ( Khadka & Tumer, 2018 ) and CERL ( Khadka et al., 2019 ) multi-agent baselines on seven continuous control tasks from the MuJoco suite ( Duan et al., 2016 ): Ant-v2, HalfCheetah-v2, Humanoid-v2, HumanoidStandup-v2, Hopper-v2, Walker2d-v2 and Humanoid (rllab). We also designed a sparse reward environment SparseHumanoid-v2. All algorithms are run over 1M time steps on each environment, except Humanoid (rllab) which gets 2M time steps and SparseHumanoid-v2 on 0.6M time steps. We also include comparison against single-agent baselines. ARAC performs R = 10 rollouts for evaluation steps every 10, 000 interaction steps with the environment. We consider a small population of N = 5 individuals with K = 2 as elites. Every SAC agent has one feedforward hidden layer of 256 units acting as state embedding, followed by a radial flow of length ∈ {3, 4}. A temperature of α = 0.05 or 0.2 is used across all the environments (See appendix for more details). AR operations are carried out by sampling uniformly n = 5 archived Under review as a conference paper at ICLR 2020 policies from G. Parameters details are provided in the Appendix (Table 4). All networks are trained with Adam optimizer ( Kingma & Ba, 2015 ) using a learning rate of 3E −4 . Baselines CEM-TD3 4 , ERL 5 , CERL 6 use the code contained in their respective repositories. HalfCheetah-v2, Hu: Humanoid-v2, Standup: HumanoidStandup-v2  Figure 4  displays the performance of all algorithms on three environments over time steps (see Appendix Figure 7 for all environments). Results are averaged over 5 random seeds.  Table 1  reports the best observed reward for each method. Curves are smoothed using Savitzky-Golay filtering with window size of 7. Small state space environments HalfCheetah-v2, Hopper-v2, and Walker2d-v2 are low-dimensional state space environments (d ≤ 17). Except for HalfCheetah-v2, the proposed approach shows comparable results with its concurrent. Those results match the findings of ( Plappert et al., 2018 ) that some environments with well-structured dynamics require little exploration. Full learning curves can be found in the Appendix.

Section Title: Deceptive reward and Large state space environments
  Deceptive reward and Large state space environments Humanoid-v2, HumanoidStandup-v2 and Humanoid (rllab) belong to bipedal environments with high-dimensional state space (d = 376 and d = 147), and are known to trap algorithms into suboptimal solutions. In addition to the legs, the agent also needs to control the arms, which may influence the walking way and hence induce deceptive rewards ( Conti et al., 2018 ).  Figure 4  shows the learning curves on MuJoCo tasks. We observe that ARAC beats both baselines in performance as well as in convergence rate. Ant-v2 is another high-dimensional state space environment (d ≥ 100). In an unstable setup, a naive algorithm implementing an unbalanced fast walk could still generate high reward, the reward Under review as a conference paper at ICLR 2020 taking into account the distance from start, instead of learning to stand, stabilize, and walk (as expected).

Section Title: Sparse reward environment
  Sparse reward environment To test ARAC in a sparse reward environment, we created SparseHumanoid-v2. The dynamic is the same as Humanoid-v2 but rewards of +1 is granted only given is the center of mass of the agent is above a threshold (set to 0.6 unit in our case). The challenge not only lies in the sparse reward property but also on the complex body dynamic that can make the agent falling down and terminating the episode. As shown in  Figure 4 , ARAC is the only method that can achieve non zero performance. A comparison against single agent methods in the Appendix also shows better performance for ARAC. Sample efficiency compared with single agent methods  Figure 3  (in Appendix) also shows that the sample efficiency of the population-based ARAC compares to a single SAC agent (with and without NFs) and other baselines methods (SAC, TD3). On Humanoid-v2 and Ant-v2 ARAC converges faster, reaching the 6k (4k, respectively) milestone performance after only 1M steps, while a single SAC agent requires 4M (3M, respectively) steps according to ( Haarnoja et al., 2018 ). In general, ARAC achieves competitive results (no flat curves) and makes the most difference (faster convergence and better performance) in the biped environments.

Section Title: Attraction-repulsion ablation study
  Attraction-repulsion ablation study To illustrate the impact of repulsive forces, we introduce a hyperparameter λ in the overall loss (Eq. 5): We ran an ablation analysis on Humanoid-v2 by varying that coefficient. For two random states, we sampled 500 actions from all agents and mapped these actions onto a two-dimensional space (via t-SNE). Appendix Figure 5 shows that without repulsion (λ = 0), actions from all agents are entangled, while repulsion (λ > 0) forces agents to behave differently and hence explore different regions of the action space. The second ablation study is dedicated to highlight the differences between a Gaussian policy (similar to  Hong et al. (2018)  and an NF policy under AR operators. As one can observe in Figure 6, using a Gaussian policy deteriorates the solution as the repulsive KL term drives apart the means of agents and blows up/ shrinks the variance of the Gaussian policy. On the other hand, applying the AR term on the NF layers maximizes the KL conditioned on the mean and variance of both base policies, resulting in a solution which allows sufficient exploration. More details are provided in the Appendix. Finally, through a toy example subject to AR, we characterize the policy's shape when increasing the number of the radial flow policy in  Figure 2  (experimental setup in Appendix). Unlike the diagonal Gaussian policy (SAC) that has symmetry constraints, increasing the number of flows allows the radial policy to adopt more complex shapes (from bottom to top).

Section Title: CONCLUSION
  CONCLUSION In this paper, we addressed the issue of RL domains with deceptive rewards by introducing a population-based search model for optimal policies using attraction-repulsion operators. Our method relies on powerful density estimators (normalizing flows), to let policies exploit the reward landscape under AR constraints. Our ablation studies showed that (1) the strength of AR and (2) the number of flows are the two factors which predominantly affect the shape of the policy. Selecting the correct AR coefficient is therefore important to obtain good performance, while at the same time preventing premature convergence. Empirical results on the MuJoCo suite demonstrate high performance of the proposed method in most settings, including with sparse rewards. Moreover, in biped environments that are known to trap algorithms into suboptimal solutions, ARAC enjoys higher sample efficiency and better performance compared to its competitors which confirms our intuitions on using AR with normalizing flows. As future steps, borrowing from multi-objective optimization literature methods could allow one to combine other diversity metrics with the performance objective, to in turn improve the coverage of the solution space among the individuals by working with the corresponding Pareto front (Horn et al., 1994). Under review as a conference paper at ICLR 2020

```
