Title:
```
Under review as a conference paper at ICLR 2020 QXPLORE: Q-LEARNING EXPLORATION BY MAXIMIZ- ING TEMPORAL DIFFERENCE ERROR
```
Abstract:
```
A major challenge in reinforcement learning is exploration, especially when reward landscapes are sparse. Several recent methods provide an intrinsic motivation to explore by directly encouraging agents to seek novel states. A potential disad- vantage of pure state novelty-seeking behavior is that unknown states are treated equally regardless of their potential for future reward. In this paper, we propose an exploration objective using the temporal difference error experienced on extrinsic rewards as a secondary reward signal for exploration in deep reinforcement learn- ing. Our objective yields novelty-seeking in the absence of extrinsic reward, while accelerating exploration of reward-relevant states in sparse (but nonzero) reward landscapes. We implement the objective with a two-policy Q-learning method in which Q and Q x are the action-value functions for extrinsic and secondary rewards, respectively. Secondary reward is given by the absolute value of the TD-error of Q. Training is off-policy, based on a replay buffer containing a mix of trajectories sampled using Q and Q x . We characterize performance on a set of continuous control benchmark tasks, and demonstrate comparable or faster convergence on all tasks when compared with other state-of-the-art exploration methods.
```

Figures/Tables Captions:
```
Figure 1: Method diagram for QXplore. We define two Q-functions which sample trajectories from their environment and store experiences in separate replay buffers. Q is a standard state-action value- function, whereas Q x 's reward function is the unsigned temporal difference error of the current Q on data sampled from both replay buffers. A policy defined by Q x samples experiences that maximize the TD-error of Q, while a policy defined by Q samples experiences that maximize discounted reward from the environment.
Figure 2: A neural network trained to predict a constant value does not interpolate or extrapolate well outside its training range, which can be exploited for exploration. Predictions of 3-layer MLPs of 256 hidden units per layer trained to imitate f (x) = 0 on R → R with training data sampled uniformly from the range [−0.75, −0.25] ∪ [0.25, 0.75]. Each line is the final response curve of an independently trained network once its training error has converged (MSE < 1e-7).
Figure 3: Performance of QXplore compared with RND and -greedy sampling. QXplore outperforms RND and -greedy on continuous control tasks. QXplore performs better due to efficient exploration sampling by Q x and the separation of the exploration and exploitation objectives. Q indicates the performance of our exploitation Q-function, while Q x indicates the performance of our exploration Q-function, whose objective does not directly maximize reward but which may lead to high reward regardless.
Figure 4: Example trajectories showing Q x 's behavior late in training that is distinctive of TD-error maximization. The corresponding Q network reliably achieves reward at this point. In "fake-out", Q x approaches the reward threshold and suddenly stops itself. In "cross and re-cross", Q x crosses the reward threshold going forward and then goes backwards through the threshold.
Table 1: Number of episodes required to reach mean reward milestones on SparseHalfCheetah for several methods. QXplore outperforms previously published methods. Results marked with "*" are previously published numbers. VIME from Houthooft et al. (2016), EMI and EX2 from Kim et al. (2018), and SimHash from Tang et al. (2017). Results marked with "x" indicate that the mean reward was not achieved. For GEP-PG (Colas et al., 2018) we used the author's implementation, which did not permit easy evaluation of intermediate performance.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep reinforcement learning (RL) has recently achieved impressive results across several challenging domains, such as playing games ( Mnih et al., 2016 ;  Silver et al., 2017 ;  OpenAI, 2018 ;  Baker et al., 2019 ) and controlling robots ( OpenAI et al., 2018 ; Kalashnikov et al., 2018). In many of these tasks, a well-shaped reward function is critical to learning performant policies. On the other hand, deep RL still remains challenging for tasks where the reward function is sparse. In these settings, state-of-the-art RL methods often perform poorly and train very slowly, if at all, due to the low probability of observing improved rewards by following the current optimal policy or with a naive exploration policy such as -greedy sampling. The challenge of learning from sparse rewards is typically framed as a problem of exploration, inspired by the notion that a successful RL agent must efficiently explore the state space of its environment in order to find improved sources of reward. One common exploration paradigm is to directly determine the novelty of states and to encourage the agent to visit states with the highest novelty. In small MDPs this can be achieved through counting how many times each state has been visited. This approach often performs poorly in high-dimensional or continuous state spaces, but recent work ( Tang et al., 2017 ;  Bellemare et al., 2016 ;  Fu et al., 2017 ) using count-like statistics have shown success on benchmark tasks with complex state spaces. Another paradigm for exploration learns a dynamic model of the environment and computes a novelty measure proportional to the error of the model in predicting transitions in the environment. This exploration method relies on the core assumption that well-modeled regions of the state space are similar to previously visited states and thus are less interesting than other regions of state space. Predictions of the transition dynamics can be directly computed ( Pathak et al., 2017 ;  Stadie et al., 2015 ;  Savinov et al., 2019 ;  Burda et al., 2019a ), or related to an information gain objective on the state space, as described in VIME ( Houthooft et al., 2016 ) and EMI ( Kim et al., 2018 ). Several exploration methods have recently been proposed that capitalize on the function approxima- tion properties of neural networks. Random network distillation (RND) trains a function to predict the output of a randomly-initialized neural network from an input state, and uses the approximation Under review as a conference paper at ICLR 2020 error as a reward bonus for a separately-trained RL agent ( Burda et al., 2019b ). Similarly, DORA ( Fox et al., 2018 ) trains a network to predict zero on observed states and deviations from zero are used to indicate unexplored states. An important shortcoming of existing exploration methods is that they only incorporate information about states and therefore assume all unobserved states are equally motivating, regardless of their viability for future reward. The viability of this assumption is highly task dependent: While games like Montezuma's Revenge or Super Mario Bros, where novelty correlates highly with success, can be attacked effectively by state novelty methods alone ( Burda et al., 2019b ;  Pathak et al., 2017 ;  Ecoffet et al., 2019 ;  Kim et al., 2018 ), other tasks such as hide-and-seek or some Atari games where novelty and utility are less correlated tend to frustrate state novelty methods ( Burda et al., 2019b ;  Baker et al., 2019 ;  Burda et al., 2019a ).  Baker et al. (2019)  explored using both RND and a simple state counting baseline to discover skills such as navigation and block-pushing in a hide-and-seek environment. However, the authors found that careful construction of the state representation used for novelty seeking was necessary to discover any such skills, as novelty in the full state space did not correspond to novelty in the intuitive sense ( Baker et al., 2019 ). Instead of focusing on the state-space, this work uses the temporal difference error (TD-error) which provides a signal into novelty in the reward landscape. Past works have also utilized information from the reward landscape as a learning signal to various extents. Schmidhuber et. al. first describe using reward misprediction and model prediction error for exploration ( Schmidhuber, 1991 ;  Thrun & Möller, 1991 ; 1992). However, the work was primarily concerned with model-building and system- identification in small MDPs, and used reward prediction error rather than TD-error.  Later, Gehring & Precup (2013)  used TD-error as a negative signal to constrain exploration to focus on states that are well understood by the value function to avoid common failure modes. Related to maximizing TD-error is maximizing the variance or KL-divergence of a posterior distribution over MDPs or Q-functions, which can be used as a measure of uncertainty ( Osband & Van Roy, 2017 ;  O'Donoghue et al., 2017 ;  Chen et al., 2017 ;  Fox et al., 2018 ;  Osband et al., 2018 ). Posterior uncertainty over Q-functions can be used for information gain in the reward or Q-function space, as opposed to information gain in the state space as described by VIME among others ( Houthooft et al., 2016 ;  Kim et al., 2018 ), though posterior uncertainty methods have thus-far largely been used for local exploration as an alternative to dithering methods such as -greedy sampling, though  Osband et al. (2018)  do apply posterior uncertainty to Montezuma's Revenge. In this paper we propose QXplore, a new exploration formulation that seeks novelty in the predicted reward landscape instead of novelty in the state space. QXplore exploits the inherent reward-space signal from the computation of temporal difference error (TD-error) in value-based RL, and explicitly promotes visiting states where the current understanding of reward dynamics is poor. In the following sections, we describe QXplore and demonstrate its utility for efficient learning on a variety of complex benchmark environments with continuous controls and sparse rewards.

Section Title: PRELIMINARIES
  PRELIMINARIES We consider RL in the terminology of  Sutton & Barto (1998) , in which an agent seeks to maximize reward in a Markov Decision Process (MDP). An MDP consists of states s ∈ S, actions a ∈ A, a state transition function S : S × A × S → [0, 1] giving the probability of moving to state s t+1 after taking action a t from state s t for discrete timesteps t ∈ 0, ..., T . Rewards are sampled from reward function r : S × A → R. An RL agent has a policy π(s t , a t ) = p(a t |s t ) that gives the probability of taking action a t when in state s t . The agent aims to learn a policy to maximize the expectation of the time-decayed sum of reward R π (s 0 ) = T t=0 γ t r(s t , a t ) where a t ∼ π(s t , a t ). A value function V θ (s t ) with parameters θ is a function which computes V θ (s t ) ≈ R π (s t ) for some policy π. Temporal Difference (TD) error δ t measures the bootstrapped error between the value function at the current timestep and the next timestep as A Q-function is a value function of the form Q(s t , a t ), which computes Q(s t , a t ) = r(s t , a t ) + γ · max a Q(s t+1 , a ), the expected future reward assuming the optimal action is taken at each future timestep. An approximation to this optimal Q-function Q θ with some parameters θ may be trained using a mean squared TD-error objective L Q θ = ||Q θ (s t , a t )−(r(s t , a t )+γ ·max a Q θ (s t+1 , a ))|| 2 Under review as a conference paper at ICLR 2020 given some target Q-function Q θ , commonly a time-delayed version of Q θ ( Mnih et al., 2015 ). Extracting a policy π given Q θ amounts to approximating argmax a Q θ (s t , a). Many methods exist for approximating the argmax a operation in both discrete and continuous action spaces ( Lillicrap et al., 2015 ;  Haarnoja et al., 2018 ). Following the convention of  Mnih et al. (2016) , we train Q θ using an off-policy replay buffer of previously visited (s, a, r, s ) tuples, which we sample uniformly. We first provide an overview of the method - a visual representation is depicted in  Figure 1 . At a high level, QXplore is an exploration method that jointly trains two independent agents equipped with their own Q-functions and reward functions: 1. Q: A standard Q-function, that learns a value function on reward provided by the external environment. 2. Q x : A Q-function that learns a value function directly on the TD-error of Q. The policy π Qx that samples Q x and the Q-function Q form an adversarial pair, wherein π Qx seeks to sample state-action pairs that produce large TD-errors while Q's training objective L Q θ attempts to minimize the TD-error for previously sampled state-action pairs. Thus, π Qx achieves reward when the agent ventures into states whose reward dynamics are foreign to Q (i.e. Q under/overestimates reward achieved). Separate replay buffers are maintained for each agent, but each agent receives samples from both buffers at train time. A similar adversarial sampling scheme was used to train an inverse dynamics model by  Hong et al. (2018) , and  Colas et al. (2018)  use separate goal-driven exploration and reward maximization phases for efficient learning, but to our knowledge parallel adversarial sampling policies have not previously been used for exploration.

Section Title: TD-ERROR OBJECTIVE
  TD-ERROR OBJECTIVE We directly treat TD-error as a reward signal and use a Q-function trained on this signal to induce an exploration policy, rather than as a supplementary objective or to compute a confidence bound. Cru- cially, when combined with neural network function approximators, this signal provides meaningful exploration information everywhere as discussed in Section 3.4. For a value function with parameters θ, and TD-error δ t we define our exploration reward function as for some extrinsic reward function r E and target Q-function Q θ . Notably, we use the absolute value of the temporal difference (rather than the squared error) used to compute updates for Q θ to keep the magnitudes of r E and r x comparable and reduce the influence of outlier temporal differences on the gradients of Q x , which we describe below. Intuitively, a policy maximizing the expected sum of r x will sample trajectories where Q θ does not have an accurate estimate of the future rewards it will experience. This is useful for exploration because r x will be large not only for state-action pairs producing unexpected reward, but for all state- action pairs leading to such states, providing a much denser exploration reward function. Further, TD-error-based exploration with a dedicated exploration policy removes the exploration-versus- exploitation tradeoff that state-novelty methods must contend with, where trajectories maximizing state novelty often do not also maximize reward. Separate exploration and exploitation policies allow us to sample trajectories maximizing r x that provide information about the task for Q θ to train on without impacting its ability to maximize reward.

Section Title: Q x : LEARNING A Q-FUNCTION TO MAXIMIZE TD-ERROR
  Q x : LEARNING A Q-FUNCTION TO MAXIMIZE TD-ERROR Next, we will describe how we use the TD-error signal defined in Section 3.2 to define an exploration policy. The reward function r x is generic, and can be maximized by any RL algorithm. However, given its derivation from a bootstrapped Q-function, training a second Q-function to maximize r x allows the entire algorithm to be trained off-policy with two replay buffers that share data between Q θ and the Q-function maximizing r x , which we term Q x . This approach is beneficial for exploration, as it avoids needing to trade off between exploration and exploitation via a weighting hyperparameter, and sharing data between replay buffers improves data efficiency for training both Q-functions. We define a Q-function, Q x,φ (s, a) with parameters φ, whose reward objective is r x . We train Q x,φ using the standard bootstrapped loss function The two Q-functions, Q θ and Q x , are trained off-policy in parallel, sharing replay data so that Q θ can train on sources of reward discovered by Q x and so that Q x can better predict the TD-errors of Q θ . Since the two share data, π Qx acts as an adversarial teacher for Q θ , sampling trajectories that produce high TD-error under Q θ and thus provide novel information about the reward landscape. To avoid off-policy stability issues due to the different reward objectives, we sample a fixed ratio of experiences collected by each policy for each training batch. We use a nonparametric cross-entropy method policy inspired by  Kalashnikov et al. (2018) , previously described as more robust to hyperparameter variance ( Simmons-Edler et al., 2019 ; Kalashnikov et al., 2018). We also experimented with a variant using Under review as a conference paper at ICLR 2020 DDPG-style parametric policies ( Lillicrap et al., 2015 ) for both Q θ and Q x , but found preventing sampling collapse by Q θ 's policy difficult. Our full method is shown in  Figure 1 , and pseudocode in Algorithm 1.

Section Title: STATE NOVELTY FROM NEURAL NETWORK FUNCTION APPROXIMATION ERROR
  STATE NOVELTY FROM NEURAL NETWORK FUNCTION APPROXIMATION ERROR A key question in using TD-error for exploration is: What happens when the reward landscape is flat? Theoretically, in the case that ∀(s, a), r(s, a) = C for some constant C ∈ R, an optimal Q-function which generalizes perfectly to unseen states will, in the infinite time horizon case, simply output ∀(s, a), Q (s, a) = ∞ t=0 Cγ t . This results in a TD-error of 0 everywhere and thus no exploration signal. However, using neural network function approximation, we find that perfect generalization to unseen states-action pairs does not occur, and in fact observe in  Figure 2  that the distance of a new datapoint from the training data manifold correlates with the magnitude of the network output's deviation from ∞ t=1 Cγ t and thus with TD-error. As a result, in the case where the reward landscape is flat TD-error exploration converges to a form of state novelty exploration. This property of neural network function approximation has been used by several previous exploration methods to good effect, including RND ( Burda et al., 2019b ) and DORA ( Fox et al., 2018 ). In particular, the exploration signal used by RND (extrapolation error from fitting the output of a random network) should be analogous to r x (extrapolation error from fitting a constant value), meaning we should expect to perform comparably to RND in the worst case where no extrinsic reward exists.

Section Title: EXPERIMENTS
  EXPERIMENTS We performed several experiments to demonstrate the effectiveness of Q x on continuous control benchmark tasks. We compare QXplore with a related state of the art state novelty-based method, RND ( Burda et al., 2019b ), DORA ( Fox et al., 2018 ), and with -greedy sampling as a simple baseline. Each method is implemented in a shared code base on top of TD3  Fujimoto et al. (2018b)  using a cross entropy method policy as proposed by Qt-Opt Kalashnikov et al. (2018) for hyperparameter stability. We also compare to results from several previous works on SparseHalfCheetah. Finally, we present several ablations to QXplore, as well as analysis of its robustness in response to several hyperparameters. Implementation details and hyperparameters for QXplore, RND, DORA, and -greedy can be found in Appendix A.

Section Title: EXPERIMENTAL SETUP
  EXPERIMENTAL SETUP We benchmark on four continuous control tasks using the MuJoCo physics simulator that each require exploration due to sparse rewards. First, the SparseHalfCheetah task originally proposed by VIME ( Houthooft et al., 2016 ). Next, we benchmark on three OpenAI gym tasks, FetchPush, FetchSlide and FetchPickAndPlace, originally developed for goal-directed exploration methods such as HER ( Andrychowicz et al., 2017 ). We chose these tasks as they are challenging exploration problems that are relatively simple to control, but still involve large continuous state spaces and in the case of the Fetch tasks learning to generalize across random object/goal posi- tions. For consistent reward shaping across tasks we used a reward function in the range [-1-0] for Under review as a conference paper at ICLR 2020 SparseHalfCheetah similar to the Fetch tasks, but results on the original reward function from  Houthooft et al. (2016)  can be found in Appendix E, where we perform comparably. We ran 5 random seeds for each experiment. More details on these environments can be found in Appendix B.

Section Title: EXPLORATION BENCHMARK PERFORMANCE
  EXPLORATION BENCHMARK PERFORMANCE We show the performance of each method on each task in  Figure 3 . QXplore outperforms RND modestly on the SparseHalfCheetah task, but performs much better comparatively on the Fetch tasks- only on FetchPush, the easiest task, did RND find non-random reward. We theorize that this improved performance on the Fetch tasks is because QXplore's TD-error exploration drives the agent to discover the conditional relationship between the changing goal position and the reward function, whereas RND and other state novelty methods are goal-agnostic since the goal is static for the entire episode. While QXplore is not a goal-directed RL method, and does not achieve state-of-the-art performance compared to dedicated goal-directed RL methods, the fact that this relationship is discovered through TD-error exploration is encouraging as to its broader applicability.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 We compare to several other exploration methods is in  Table 1 . The methods from previous work are built on top of TRPO ( Schulman et al., 2015 ), so a comparison in terms of training iterations as in  Figure 3  would not be informative due to TRPO's variable update rule. We instead compare the number of episodes of interaction required to reach a given level of reward, though QXplore was not intended to be performant with respect to this metric. While some decrease in episode efficiency is expected due to differing baseline methods (TRPO ( Schulman et al., 2015 ) versus TD3  Fujimoto et al. (2018b) ), compared to published results for EMI ( Kim et al., 2018 ), EX2 ( Fu et al., 2017 ), VIME ( Houthooft et al., 2016 ), and SimHash ( Tang et al., 2017 ) on the SparseHalfCheetah task, QXplore reaches every reward milestone faster, and achieves a peak reward (300) not achieved by any previous method. We also include here the performance of our implementation of DORA ( Fox et al., 2018 ) on SparseHalfCheetah. DORA performed poorly, possibly because it was not intended for use with continuous action spaces, and thus we did not test it on other tasks. Finally, we compare to GEP-PG ( Colas et al., 2018 ), which used separate exploration and exploitation phases similar to QXplore. We downloaded the author's implementation (built on top of DDPG) and tested it on SparseHalfCheetah using the parameters for the HalfCheetah-v2 task it was originally tested on. The author's implementation did not facilitate evaluating performance midway through training, and thus we report only their final performance number after 4000 episodes, which was 120.2.

Section Title: ROBUSTNESS
  ROBUSTNESS As RL tasks are highly heterogeneous, and good parameterization/performance can be hard to obtain in practice for many methods ( Henderson et al., 2018 ), we performed sweeps over several hyperpa- rameters and introduce several ablations of QXplore on SparseHalfCheetah to demonstrate the method's robustness and validate aspects of the algorithm.

Section Title: Parameter Sweeps
  Parameter Sweeps We swept over the learning rates of Q and Q x , as well as the ratio of self- collected versus other-collected data used to train each function. The results suggest that while the performance of Q is somewhat sensitive to learning rate, keeping learning rates for Q and Q x the same works well. The results also show that while our ratio of 75% self/25% non-self performs best, Q is fairly robust to the on/off-policy data ratio, including when Q is trained entirely off-policy on data collected by Q x . Results are shown in Figures 8 and 9 in Appendix D.

Section Title: Weight Initialization
  Weight Initialization Also, since neural network generalization is key to QXplore, we tested several different network weight initialization schemes, including some that were deliberately poor priors. We found that while the performance of Q is sensitive to initialization scheme, Q x robustly finds reward in all cases. See Figure 12 in Appendix G. The 'Noisy TV' Problem One drawback that naive state novelty exploration methods have is that un- predictable observations (such as from a TV displaying static) act as maxima in the exploration reward function. Naive methods are unavoidably drawn to such states instead of exploring. TD-error driven exploration is not sensitive to unpredictable observations as they do not affect the underlying reward function. To demonstrate this, we tested QXplore with a variant of the SparseHalfCheetah task with noisy observations. We observe that QXplore performs as normal in this case. A description of the task can be found in Appendix F.

Section Title: ABLATIONS
  ABLATIONS There are two features of QXplore that distinguish it from prior work in exploration: the use of a pair of policies that share replay data, the use of unsigned TD-Error to drive exploration. We performed several ablations that assess the impact of aspects of each of these features. Detailed results can be found in Appendix C.

Section Title: Single-Policy QXplore
  Single-Policy QXplore First, we test a single-policy version of QXplore by replacing Q θ (s, a) with a value function V θ (s). We use a value function rather than Q-function in this case to avoid large estimation errors stemming from fully off-policy training such as reported by  Fujimoto et al. (2018a) . We observe in Figure 5 that while the policy is able to find reward quickly and converge faster, the need to satisfy both objectives results in a lower converged reward than the original QXplore method. Under review as a conference paper at ICLR 2020 1-Step Reward Prediction Second, we run an ablation where we replaced Q θ (s, a) with a function that simply predicts the current r(s t , a t ). Using reward error instead of a value function in Q x can still produce the same state novelty fallback behavior in the absence of reward; however, it provides only limited reward-based exploration utility. We tested this variant and observe in Figure 5 that it fails to sample reward. Reward prediction error is not sufficient to allow strong exploration behavior.

Section Title: QXplore with State Novelty Exploration
  QXplore with State Novelty Exploration To assess the importance of TD-error specifically in our two policy algorithm, we replaced the TD-error maximization objective of Q x with the random network prediction error maximization objective of RND, while still performing separate rollouts of each policy. The results are shown in Figure 6. We observe that while the modified Q x function does sample reward, it is too infrequent to guide Q to learn the task, and further that the modified Q x function does not display directional preference in exploration once reward is discovered.

Section Title: QXplore with Signed TD-Error Objective
  QXplore with Signed TD-Error Objective While we used unsigned TD-error to train Q x , we also tested QXplore using signed TD-error. We used the negative signed TD-error −δ t from equation 1 so that better-than-expected rewards result in positive r x values. The results of this experiment are shown in Figure 7. The unsigned TD-error performs better on SparseHalfCheetah.

Section Title: QUALITATIVE BEHAVIORAL ANALYSIS
  QUALITATIVE BEHAVIORAL ANALYSIS Qualitatively, on SparseHalfCheetah we observe interesting behavior from Q x late in training. After initially converging to obtain high reward, Q x appears to get "bored" and will focus on the reward threshold, stopping short or jumping back and forth across it, which results in reduced reward but higher TD-error. This behavior is distinctive of TD-error seeking over state novelty seeking, as such states are not novel compared to moving past the threshold but do result in higher TD-error. Such behavior from Q x motivates Q to explore the state space around the reward boundary. Example sequences of such behaviors are shown in  Figure 4 .

Section Title: DISCUSSION AND CONCLUSIONS
  DISCUSSION AND CONCLUSIONS Here, we have described a new method for using TD-error to explore in reinforcement learning. We instantiate a reward function using TD-error, and show that when combined with neural network approximation, it is sufficient to discover solutions to challenging exploration tasks in fewer training iterations than recent state novelty-based exploration methods. We hope that our results can spur further work on diverse exploration signals in RL. It is also worth noting that there may be additional benefits provided by Q x for Q learning in non- exploration contexts. Maximizing TD-error can be seen as a form of hard example mining, and for complex tasks could result in better generalization behavior and faster transfer to new tasks through efficient trajectory sampling by Q x . One potential future area of investigation is in our method's connection to biological models of dopamine pathways in the brain where levels of dopamine correlate with TD-error in learning trials ( Niv et al., 2005 ), a phenomenon previously described in animals ( Arias-Carrión & Pöppel, 2007 ). Under review as a conference paper at ICLR 2020

```
