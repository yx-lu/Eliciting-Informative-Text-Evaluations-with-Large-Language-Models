Title:
```
Published as a conference paper at ICLR 2020 SEED RL: SCALABLE AND EFFICIENT DEEP-RL WITH ACCELERATED CENTRAL INFERENCE
```
Abstract:
```
We present a modern scalable reinforcement learning agent called SEED (Scal- able, Efficient Deep-RL). By effectively utilizing modern accelerators, we show that it is not only possible to train on millions of frames per second but also to lower the cost of experiments compared to current methods. We achieve this with a simple architecture that features centralized inference and an optimized communication layer. SEED adopts two state of the art distributed algorithms, IMPALA/V-trace (policy gradients) and R2D2 (Q-learning), and is evaluated on Atari-57, DeepMind Lab and Google Research Football. We improve the state of the art on Football and are able to reach state of the art on Atari-57 three times faster in wall-time. For the scenarios we consider, a 40% to 80% cost reduction for running experiments is achieved. The implementation along with experiments is open-sourced so results can be reproduced and novel ideas tried out. Github: http://github.com/google-research/seed_rl.
```

Figures/Tables Captions:
```
Figure 1: Overview of architectures to a shared queue or replay buffer. Asynchronously, the learner reads batches of trajectories from the queue/replay buffer and optimizes the model.
Figure 2: Variants of "near on-policy" when evaluating a policy π while asynchronously optimizing model parameters θ.
Figure 3: Detailed Learner architecture in SEED (with an optional replay buffer).
Figure 4: Comparison of IMPALA and SEED under the exact same conditions (175 actors, same hyperparameters, etc.) The plots show hyperparameter combinations sorted by the final performance across different hyperparameter combinations.
Figure 5: Training on 4 DeepMind Lab tasks. Each curve is the best of the 24 runs based on final return following the evaluation procedure in Espeholt et al. (2018). Sample complexity is maintained up to 8 TPU v3 cores, which leads to 11x faster training than the IMPALA baseline. Top Row: X- axis is per frame (number of frames = 4x number of steps). Bottom Row: X-axis is hours.
Figure 6: Median human-normalized score on Atari-57 for SEED and related agents. SEED was run with 1 seed for each game. All agents use up to 30 random no-ops for evaluation. Left: X-axis is hours Right: X-axis is environment frames (a frame is 1/4th of an environment step due to action repeat). SEED reaches state of the art performance 3.1x faster (wall-time) than R2D2. We evaluate our implementation of R2D2 in SEED architecture on 57 Atari 2600 games from the ALE benchmark. This benchmark has been the testbed for most recent deep reinforcement learning agents because of the diversity of visuals and game mechanics.
Table 1: Performance of SEED, IMPALA and R2D2.
Table 2: Google Research Football "Hard" using two kinds of reward functions. For each reward function, 40 hyperparame- ter sets ran with 3 seeds each which were averaged after 500M frames of training. The table shows the median and maximum of the 40 averaged values. This is a similar setup to Kurach et al. (2019) although we ran 40 hyperparameter sets vs. 100 but did not rerun our best models using 5 seeds.
Table 3: Cost of cloud resources as of Sep. 2019.
Table 4: Training cost on DeepMind Lab for 1 billion frames.
Table 5: Training cost on Google Research Football for 1 billion frames.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The field of reinforcement learning (RL) has recently seen impressive results across a variety of tasks. This has in part been fueled by the introduction of deep learning in RL and the introduction of accelerators such as GPUs. In the very recent history, focus on massive scale has been key to solve a number of complicated games such as AlphaGo ( Silver et al., 2016 ), Dota ( OpenAI, 2018 ) and StarCraft 2 ( Vinyals et al., 2017 ). The sheer amount of environment data needed to solve tasks trivial to humans, makes distributed machine learning unavoidable for fast experiment turnaround time. RL is inherently comprised of heterogeneous tasks: running environments, model inference, model training, replay buffer, etc. and current state-of-the-art distributed algorithms do not efficiently use compute resources for the tasks. The amount of data and inefficient use of resources makes experiments unreasonably expensive. The two main challenges addressed in this paper are scaling of reinforcement learning and optimizing the use of modern accelerators, CPUs and other resources. We introduce SEED (Scalable, Efficient, Deep-RL), a modern RL agent that scales well, is flexible and efficiently utilizes available resources. It is a distributed agent where model inference is done centrally combined with fast streaming RPCs to reduce the overhead of inference calls. We show that with simple methods, one can achieve state-of-the-art results faster on a number of tasks. For optimal performance, we use TPUs (cloud.google.com/tpu/) and TensorFlow 2 ( Abadi et al., 2015 ) to simplify the implementation. The cost of running SEED is analyzed against IMPALA ( Espeholt et al., 2018 ) which is a commonly used state-of-the-art distributed RL algorithm ( Veeriah et al. (2019) ;  Li et al. (2019) ;  Deverett et al. (2019) ;  Omidshafiei et al. (2019) ;  Vezhnevets et al. (2019) ;  Hansen et al. (2019) ;  Schaarschmidt et al.; Tirumala et al. (2019) , ...). We show cost reductions of up to 80% while being significantly faster. When scaling SEED to many accelerators, it can train on millions of frames per second. Finally, the implementation is open-sourced together with examples of running it at scale on Google Cloud (see Appendix A.4 for details) making it easy to reproduce results and try novel ideas.

Section Title: RELATED WORK
  RELATED WORK For value-based methods, an early attempt for scaling DQN was  Nair et al. (2015)  that used asyn- chronous SGD ( Dean et al., 2012 ) together with a distributed setup consisting of actors, replay buffers, parameter servers and learners. Since then, it has been shown that asynchronous SGD leads to poor sample complexity while not being significantly faster ( Chen et al., 2016 ;  Espeholt et al., 2018 ). Along with advances for Q-learning such as prioritized replay ( Schaul et al., 2015 ), dueling networks ( Wang et al., 2016 ), and double-Q learning ( van Hasselt, 2010 ;  Van Hasselt et al., 2016 ) the state-of-the-art distributed Q-learning was improved with Ape-X ( Horgan et al., 2018 ). Recently, R2D2 ( Kapturowski et al., 2018 ) achieved impressive results across all the Arcade Learning Envi- ronment (ALE) ( Bellemare et al., 2013 ) games by incorporating value-function rescaling ( Pohlen et al., 2018 ) and LSTMs ( Hochreiter & Schmidhuber, 1997 ) on top of the advancements of Ape-X. There have also been many approaches for scaling policy gradients methods. A3C ( Mnih et al., 2016 ) introduced asynchronous single-machine training using asynchronous SGD and relied exclu- sively on CPUs. GPUs were later introduced in GA3C ( Mahmood, 2017 ) with improved speed but poor convergence results due to an inherently on-policy method being used in an off-policy setting. This was corrected by V-trace ( Espeholt et al., 2018 ) in the IMPALA agent both for single- machine training and also scaled using a simple actor-learner architecture to more than a thou- sand machines. PPO ( Schulman et al., 2017 ) serves a similar purpose to V-trace and was used in OpenAI Rapid ( Petrov et al., 2018 ) with the actor-learner architecture extended with Redis (redis.io), an in-memory data store, and was scaled to 128,000 CPUs. For inexpensive environments like ALE, a single machine with multiple accelerators can achieve results quickly ( Stooke & Abbeel, 2018 ). This approach was taken a step further by converting ALE to run on a GPU ( Dalton et al., 2019 ). A third class of algorithms is evolutionary algorithms. With simplicity and massive scale, they have achieved impressive results on a number of tasks ( Salimans et al., 2017 ;  Such et al., 2017 ). Besides algorithms, there exist a number of useful libraries and frameworks for reinforcement learn- ing. ELF ( Tian et al., 2017 ) is a framework for efficiently interacting with environments, avoiding Python global-interpreter-lock contention. Dopamine ( Castro et al., 2018 ) is a flexible research focused RL framework with a strong emphasis on reproducibility. It has state of the art agent imple- mentations such as Rainbow ( Hessel et al., 2017 ) but is single-threaded. TF-Agents ( Guadarrama et al., 2018 ) and rlpyt ( Stooke & Abbeel, 2019 ) both have a broader focus with implementations for several classes of algorithms but as of writing, they do not have distributed capability for large- scale RL. RLLib ( Liang et al., 2017 ) provides a number of composable distributed components and a communication abstraction with a number of algorithm implementations such as IMPALA and Ape-X. Concurrent with this work, TorchBeast ( Küttler et al., 2019 ) was released which is an implementation of single-machine IMPALA with remote environments. SEED is closest related to IMPALA, but has a number of key differences that combine the benefits of single-machine training with a scalable architecture. Inference is moved to the learner but envi- ronments run remotely. This is combined with a fast communication layer to mitigate latency issues from the increased number of remote calls. The result is significantly faster training at reduced costs by as much as 80% for the scenarios we consider. Along with a policy gradients (V-trace) implemen- tation we also provide an implementation of state of the art Q-learning (R2D2). In the work we use TPUs but in principle, any modern accelerator could be used in their place. TPUs are particularly well-suited given they high throughput for machine learning applications and the scalability. Up to 2048 cores are connected with a fast interconnect providing 100+ petaflops of compute.

Section Title: ARCHITECTURE
  ARCHITECTURE Before introducing the architecture of SEED, we first analyze the generic actor-learner architecture used by IMPALA, which is also used in various forms in Ape-X, OpenAI Rapid and others. An overview of the architecture is shown in Figure 1a. A large number of actors repeatedly read model parameters from the learner (or parameter servers). Each actor then proceeds the local model to sample actions and generate a full trajectory of observa- tions, actions, policy logits/Q-values. Finally, this trajectory along with recurrent state is transferred Published as a conference paper at ICLR 2020 5. Batch of trajectories 6. Optimize Environment Environment Environment Environment (b) SEED architecture, see detailed replay architecture in  Figure 3 . There are a number of reasons for why this architecture falls short: 1. Using CPUs for neural network inference: The actor machines are usually CPU-based (occasionally GPU-based for expensive environments). CPUs are known to be computa- tionally inefficient for neural networks ( Raina et al., 2009 ). When the computational needs of a model increase, the time spent on inference starts to outweigh the environment step computation. The solution is to increase the number of actors which increases the cost and affects convergence ( Espeholt et al., 2018 ). 2. Inefficient resource utilization: Actors alternate between two tasks: environment steps and inference steps. The compute requirements for the two tasks are often not similar which leads to poor utilization or slow actors. E.g. some environments are inherently single-threading while neural networks are easily parallelizable. 3. Bandwidth requirements: Model parameters, recurrent state and observations are trans- ferred between actors and learners. Relatively to model parameters, the size of the ob- servation trajectory often only accounts for a few percents. 1 Furthermore, memory-based models send large states, increase bandwidth requirements. While single-machine approaches such as GA3C ( Mahmood, 2017 ) and single-machine IMPALA avoid using CPU for inference (1) and do not have network bandwidth requirements (3), they are restricted by resource usage (2) and the scale required for many types of environments. The architecture used in SEED (Figure 1b) solves the problems mentioned above. Inference and trajectory accumulation is moved to the learner which makes it conceptually a single-machine setup with remote environments (besides handling failures). Moving the logic effectively makes the actors a small loop around the environments. For every single environment step, the observations are sent to the learner, which runs the inference and sends actions back to the actors. This introduces a new problem: 4. Latency. To minimize latency, we created a simple framework that uses gRPC (grpc.io) - a high performance RPC library. Specifically, we employ streaming RPCs where the connection from actor to learner is kept open and metadata sent only once. Furthermore, the framework includes a batching module that efficiently batches multiple actor inference calls together. In cases where actors can fit on the same machine as learners, gRPC uses unix domain sockets and thus reduces latency, CPU and syscall overhead. Overall, the end-to-end latency, including network and inference, is faster for a number of the models we consider (see Appendix A.7). The IMPALA and SEED architectures differ in that for SEED, at any point in time, only one copy of the model exists whereas for distributed IMPALA each actor has its own copy. This changes the way the trajectories are off-policy. In IMPALA (Figure 2a), an actor uses the same policy π θt for an entire trajectory. For SEED (Figure 2b), the policy during an unroll of a trajectory may change multiple times with later steps using more recent policies closer to the one used at optimization time. A detailed view of the learner in the SEED architecture is shown on  Figure 3 . Three types of threads are running: 1. Inference 2. Data prefetching and 3. Training. Inference threads receive a batch of observations, rewards and episode termination flags. They load the recurrent states and send the data to the inference TPU core. The sampled actions and new recurrent states are received, and the actions are sent back to the actors while the latest recurrent states are stored. When a trajectory is fully unrolled it is added to a FIFO queue or replay buffer and later sampled by data prefetching threads. Finally, the trajectories are pushed to a device buffer for each of the TPU cores taking part in training. The training thread (the main Python thread) takes the prefetched trajectories, computes gradients using the training TPU cores and applies the gradients on the models of all TPU cores (inference and training) synchronously. The ratio of inference and training cores can be adjusted for maximum throughput and utilization. The architecture scales to a TPU pod (2048 cores) by round- robin assigning actors to TPU host machines, and having separate inference threads for each TPU host. When actors wait for a response from the learner, they are idle so in order to fully utilize the machines, we run multiple environments on a single actor. To summarize, we solve the issues listed previously by: 1. Moving inference to the learner and thus eliminating any neural network related computa- tions from the actors. Increasing the model size in this architecture will not increase the need for more actors (in fact the opposite is true). 2. Batching inference on the learner and having multiple environments on the actor. This fully utilize both the accelerators on the learner and CPUs on the actors. The number of Published as a conference paper at ICLR 2020 TPU cores for inference and training is finely tuned to match the inference and training workloads. All factors help reducing the cost of experiments. 3. Everything involving the model stays on the learner and only observations and actions are sent between the actors and the learner. This reduces bandwidth requirements by as much as 99%. 4. Using streaming gRPC that has minimal latency and minimal overhead and integrating batching into the server module. We provide the following two algorithms implemented in the SEED framework: V-trace and Q- learning.

Section Title: V-TRACE
  V-TRACE One of the algorithms we adapt into the framework is V-trace ( Espeholt et al., 2018 ). We do not include any of the additions that have been proposed on top of IMPALA such as  van den Oord et al. (2018) ;  Gregor et al. (2019) . The additions can also be applied to SEED and since they are more computational expensive, they would benefit from the SEED architecture.

Section Title: Q-LEARNING
  Q-LEARNING We show the versatility of SEED's architecture by fully implementing R2D2 ( Kapturowski et al., 2018 ), a state of the art distributed value-based agent. R2D2 itself builds on a long list of improve- ments over DQN ( Mnih et al., 2015 ): double Q-learning ( van Hasselt, 2010 ;  Van Hasselt et al., 2016 ), multi-step bootstrap targets ( Sutton, 1988 ;  Sutton & Barto, 1998 ;  Mnih et al., 2016 ), du- eling network architecture ( Wang et al., 2016 ), prioritized distributed replay buffer ( Schaul et al., 2015 ;  Horgan et al., 2018 ), value-function rescaling ( Pohlen et al., 2018 ), LSTM's ( Hochreiter & Schmidhuber, 1997 ) and burn-in ( Kapturowski et al., 2018 ). Instead of a distributed replay buffer, we show that it is possible to keep the replay buffer on the learner with a straightforward flexible implementation. This reduces complexity by removing one type of job in the setup. It has the drawback of being limited by the memory of the learner but it was not a problem in our experiments by a large margin: a replay buffer of 10 5 trajectories of length 120 of 84 × 84 uncompressed grayscale observations (following R2D2's hyperparameters) takes 85GBs of RAM, while Google Cloud machines can offer hundreds of GBs. However, nothing prevents the use of a distributed replay buffer together with SEED's central inference, in cases where a much larger replay buffer is needed.

Section Title: EXPERIMENTS
  EXPERIMENTS We evaluate SEED on a number of environments: DeepMind Lab ( Beattie et al., 2016 ), Google Research Football ( Kurach et al., 2019 ) and Arcade Learning Environment ( Bellemare et al., 2013 ).

Section Title: DEEPMIND LAB AND V-TRACE
  DEEPMIND LAB AND V-TRACE DeepMind Lab is a 3D environment based on the Quake 3 engine. It features mazes, laser tag and memory tasks. We evaluate on four commonly used tasks. The action set used is from  Espeholt et al. (2018)  although for some tasks, higher return can be achieved with bigger action sets such as the one introduced in  Hessel et al. (2018) . For all experiments, we used an action repeat of 4 and the number of frames in plots is listed as environment frames (equivalent to 4 times the number of steps). The same set of 24 hyperparameter sets and the same model (ResNet from IMPALA) was used for both agents. More details can be found in Appendix A.1.2.

Section Title: STABILITY
  STABILITY The first experiment evaluates the effect of the change in off-policy behavior described in  Figure 2 . Exactly the same hyperparameters are used for both IMPALA and SEED, including the number of environments used. As is shown in  Figure 4 , the stability across hyperparameters of SEED is slightly better than IMPALA, while achieving slightly higher final returns.

Section Title: SPEED
  SPEED For evaluating performance, we compare IMPALA using an Nvidia P100 with SEED with multiple accelerator setups. They are evaluated on the same set of hyperparameters. We find that SEED is 2.5x faster than IMPALA using 2 TPU v3 cores (see  Table 1 ), while using only 77% more envi- ronments and 41% less CPU (see section 4.4.1). Scaling from 2 to 8 cores results in an additional 4.4x speedup with sample complexity maintained ( Figure 5 ). The speed-up is greater than 4x due to using 6 cores for training and 2 for inference instead of 1 core for each, resulting in better utiliza- tion. A 5.3x speed-up instead of 4.4x can be obtained by increasing the batch size linearly with the number of training cores, but contrary to related research ( You et al., 2017b ;  Goyal et al., 2017 ) we found that increased batch size hurts sample complexity even with methods like warm-up and actor de-correlation ( Stooke & Abbeel, 2018 ). We hypothesize that this is due to the limited actor and environment diversity in DeepMind Lab tasks. In  McCandlish et al. (2018)  they found that Pong scales poorly with batch size but that Dota can be trained effectively with a batch size five orders of magnitude larger. Note, for most models, the effective batch size is batch size · trajectory length. In  Figure 5 , we include a run from a limited sweep on "explore_goal_locations_small" using 64 cores with an almost linear speed-up. Wall-time performance is improved but sample complexity is heavily penalized. When using an Nvidia P100, SEED is 1.58x slower than IMPALA. A slowdown is expected because SEED performs inference on the accelerator. SEED does however use significantly fewer CPUs and is lower cost (see Appendix A.6). The TPU version of SEED has been optimized but it is likely that improvements can be found for SEED with P100.

Section Title: GOOGLE RESEARCH FOOTBALL AND V-TRACE
  GOOGLE RESEARCH FOOTBALL AND V-TRACE Google Research Football is an environment similar to FIFA video games (ea.com). We evaluate SEED on the "Hard" task introduced in  Kurach et al. (2019)  where the baseline IMPALA agent achieved a positive average score after 500M frames using the engineered "checkpoint" reward function but a negative average score using only the score as a reward signal. In all experiments we use the model from  Kurach et al. (2019)  and sweep over 40 hyperparameter sets with 3 seeds each. See all hyperparameters in Appendix A.2.1.

Section Title: SPEED
  SPEED Compared to the baseline IMPALA using 2 Nvidia P100's (and CPUs for inference) in  Kurach et al. (2019)  we find that using 2 TPU v3 cores in SEED improves the speed by 1.6x (see  Table 1 ). Additionally, using 8 cores adds another 4.1x speed-up. A speed-up of 4.5x is achievable if the batch size is increased linearly with the number of training cores (5). However, we found that increasing the batch size, like with DeepMind Lab, hurts sample complexity.

Section Title: INCREASED MAP SIZE
  INCREASED MAP SIZE More compute power allows us to increase the size of the Super Mini Map (SMM) input state. By default its size is 96 x 72 (x 4) and it represents players, opponents, ball and the active player as 2d bit maps. We evaluated three sizes: (1) Default 96 x 72, (2) Medium 120 x 90 and (3) Large 144 x 108. As shown in  Table 1 , switching from Default to Large SMM results in 60% speed reduction. However, increasing the map size improves the final score ( Table 2 ). It may suggest that the bit map representation is not granular enough for the task. For both reward functions, we improve upon the results of  Kurach et al. (2019) . Finally, training on 4B frames improves the results by a significant margin (an average score of 0.46 vs. 4.76 in case of the scoring reward function). We follow the same evaluation pro- cedure as R2D2. In particular, we use the full action set, no loss- of-life-as-episode-end heuristic and start episodes with up to 30 ran- dom no-ops. We use 8 TPU v3 cores and 610 actors to maximize TPU utilization. This achieves 260K environment FPS and per- forms 9.5 network updates per sec- ond. Other hyperparameters are taken from R2D2, and are fully re- produced in appendix A.3.1.  Figure 6  shows the median human- normalized scores for SEED, R2D2, Ape-X and Rainbow. As expected, SEED has similar sample efficiency as R2D2, but it is 3.1x faster (see  Table 1 ). This allows us to reach a median human- normalized score of 1880% in just 1.8 days of training instead of 5, establishing a new wall-time state of the art on Atari-57. With the number of actors in- creased to 1200, a batch size in- creased to 256 and without frame- stacking, we can achieve 440K en- vironment FPS and learn using 16 batches per second. However, this significantly degrades sample efficiency and limits the final me- dian human-normalized score to approximately 1000%.

Section Title: COST COMPARISONS
  COST COMPARISONS With growing complexity of environments as well as size of neural networks used in reinforcement learning, the need of running big experiments increases, making cost reductions important. In this Published as a conference paper at ICLR 2020 section we analyze how increasing complexity of the network impacts training cost for SEED and IMPALA. In our experiments we use the pricing model of Google AI Platform, ML Engine. Our main focus is on obtaining lowest possible cost per step, while maintaining training speed. Distributed exper- iments from  Espeholt et al. (2018)  (IMPALA) used be- tween 150 and 500 CPUs, which translates into $7.125 - $23.75 of actors' cost per hour. The cost of single-GPU learner is $1.46 per hour. Due to the relatively high ex- pense of the actors, our main focus is to reduce number of actors and to obtain high CPU utilization.

Section Title: DEEPMIND LAB
  DEEPMIND LAB Our DeepMind Lab experiment is based on the ResNet model from IMPALA. We evaluate increas- ing the number of filters in the convolutional layers: (1) Default 1x (2) Medium 2x and (3) Large 4x. Experiments are performed on the "explore_goal_locations_small" task. IMPALA uses a single Nvidia Tesla P100 GPU for training while inference is done on CPU by the actors. SEED uses one TPU v3 core for training and one for inference. For IMPALA, actor CPU utilization is close to 100% but in case of SEED, only the environment runs on an actor making CPU idle while waiting for inference step. To improve utilization, a single SEED actor runs multiple environments (between 12 and 16) on a 4-CPU machine. As  Table 4  shows, SEED turns out to be not only faster, but also cheaper to run. The cost ratio between SEED and IMPALA is around 4. Due to high cost of inference on a CPU, IMPALA's cost increases with increasing complexity of the network. In the case of SEED, increased network size has smaller impact on overall costs, as inference accounts for about 30% of the costs (see Table A.5).

Section Title: GOOGLE RESEARCH FOOTBALL
  GOOGLE RESEARCH FOOTBALL We evaluate cost of running experiments with Google Research Football with different sizes of the Super Mini Map representation (the size has virtually no impact on environment's speed.) For IMPALA, two Nvidia P100 GPUs were used for training and SEED used one TPU v3 core for training and one for inference. For IMPALA, we use one core per actor while SEED's actors run multiple environments per actor for better CPU utilization (8 cores, 12 environments). For the default size of the SMM, per experiment training cost differs by only 68%. As the Google Research Football environment is more expensive than DeepMind Lab, training and inference costs 2 TPU cores are sold in multiples of 8, by running many experiments at once we use as many cores per experiment as needed. See cloud.google.com/ml-engine/docs/pricing. have relatively smaller impact on the overall experiment cost. The difference increases when the size of the SMM increases and SEED needing relatively fewer actors.

Section Title: ARCADE LEARNING ENVIRONMENT
  ARCADE LEARNING ENVIRONMENT Due to lack of baseline implementation for R2D2, we do not include cost comparisons for this environment. However, comparison of relative costs between ALE, DeepMind Lab and Football suggests that savings should be even more significant. ALE is the fastest among the three environ- ments making inference proportionally the most expensive. Appendix A.5 presents training cost split between actors and the learner for different setups.

Section Title: CONCLUSION
  CONCLUSION We introduced and analyzed a new reinforcement learning agent architecture that is faster and less costly per environment frame than previous distributed architectures by better utilizing modern ac- celerators. It achieved a 11x wall-time speedup on DeepMind Lab compared to a strong IMPALA baseline while keeping the same sample efficiency, improved on state of the art scores on Google Re- search Football, and achieved state of the art scores on Atari-57 3.1x faster (wall-time) than previous research. The agent is open-sourced and packaged to easily run on Google Cloud. We hope that this will accelerate reinforcement learning research by allowing the community to replicate state-of-the-art results and build on top of them. As a demonstration of the potential of this new agent architecture, we were able to scale it to millions of frames per second in some realistic scenarios (80x speedup compared to previous research). However, this requires increasing the number of environments and using larger batch sizes which hurts sample efficiency in the environments tested. Preserving sample efficiency with larger batch- sizes has been studied to some extent in RL ( Stooke & Abbeel, 2018 ;  McCandlish et al., 2018 ) and in the context of supervised learning ( You et al., 2017b ;a; 2019;  Goyal et al., 2017 ). We believe it is still an open and increasingly important area of research in order to scale up reinforcement learning.

Section Title: Annex Figures
  Annex Figures   fig_6        

```
