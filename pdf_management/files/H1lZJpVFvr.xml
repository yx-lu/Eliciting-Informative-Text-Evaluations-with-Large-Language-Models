<article article-type="research-article"><front><article-meta><title-group><article-title>Published as a conference paper at ICLR 2020 ROBUST LOCAL FEATURES FOR IMPROVING THE GENERALIZATION OF ADVERSARIAL TRAINING</article-title></title-group><contrib-group content-type="author"><contrib contrib-type="person"><name><surname>Song</surname><given-names>Chuanbiao</given-names></name></contrib><contrib contrib-type="person"><name><surname>He</surname><given-names>Kun</given-names></name></contrib><contrib contrib-type="person"><name><surname>Lin</surname><given-names>Jiadong</given-names></name></contrib><contrib contrib-type="person"><name><surname>Wang</surname><given-names>Liwei</given-names></name></contrib><contrib contrib-type="person"><name><surname>Hopcroft</surname><given-names>John E</given-names></name></contrib><contrib contrib-type="person"><xref ref-type="aff" rid="aff0" /></contrib></contrib-group><aff id="aff0"><institution content-type="orgname">School of Computer Science and Technology Huazhong University of Science and Technology Wuhan</institution><country>China</country></aff><aff id="aff1"><country>China</country></aff><aff id="aff2"><institution content-type="orgname">Department of Computer Science Cornell University</institution><country>USA</country></aff><abstract><p>Adversarial training has been demonstrated as one of the most effective methods for training robust models to defend against adversarial examples. However, ad- versarially trained models often lack adversarially robust generalization on unseen testing data. Recent works show that adversarially trained models are more biased towards global structure features. Instead, in this work, we would like to inves- tigate the relationship between the generalization of adversarial training and the robust local features, as the robust local features generalize well for unseen shape variation. To learn the robust local features, we develop a Random Block Shuffle (RBS) transformation to break up the global structure features on normal adver- sarial examples. We continue to propose a new approach called Robust Local Features for Adversarial Training (RLFAT), which first learns the robust local fea- tures by adversarial training on the RBS-transformed adversarial examples, and then transfers the robust local features into the training of normal adversarial ex- amples. To demonstrate the generality of our argument, we implement RLFAT in currently state-of-the-art adversarial training frameworks. Extensive experiments on STL-10, CIFAR-10 and CIFAR-100 show that RLFAT significantly improves both the adversarially robust generalization and the standard generalization of ad- versarial training. Additionally, we demonstrate that our models capture more local features of the object on the images, aligning better with human perception.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Deep learning has achieved a remarkable performance breakthrough on various challenging bench- marks in machine learning fields, such as image classification (<xref ref-type="bibr" rid="b11">Krizhevsky et al., 2012</xref>) and speech recognition (<xref ref-type="bibr" rid="b9">Hinton et al., 2012</xref>). However, recent studies (<xref ref-type="bibr" rid="b8">Szegedy et al., 2014</xref>; <xref ref-type="bibr" rid="b1">Goodfellow et al., 2015</xref>) have revealed that deep neural network models are strikingly susceptible to adversarial ex- amples, in which small perturbations around the input are sufficient to mislead the predictions of the target model. Moreover, such perturbations are almost imperceptible to humans and often transfer across diverse models to achieve black-box attacks (<xref ref-type="bibr" rid="b17">Papernot et al., 2017</xref>; <xref ref-type="bibr" rid="b15">Liu et al., 2017</xref>; <xref ref-type="bibr" rid="b6">Wang et al., 2019</xref>; <xref ref-type="bibr" rid="b14">Lin et al., 2020</xref>).</p><p>Though the emergence of adversarial examples has received significant attention and led to various defend approaches for developing robust models (<xref ref-type="bibr" rid="b10">Madry et al., 2018</xref>; <xref ref-type="bibr" rid="b5">Dhillon et al., 2018</xref>; <xref ref-type="bibr" rid="b6">Wang &amp; Yu, 2019</xref>; Song et al., 2019; <xref ref-type="bibr" rid="b13">Zhang et al., 2019a</xref>), many proposed defense methods provide few benefits for the true robustness but mask the gradients on which most attacks rely (<xref ref-type="bibr" rid="b0">Carlini &amp; Wag- ner, 2017a</xref>; <xref ref-type="bibr" rid="b0">Athalye et al., 2018</xref>; <xref ref-type="bibr" rid="b9">Uesato et al., 2018</xref>; <xref ref-type="bibr" rid="b9">Li et al., 2019</xref>). Currently, one of the best Published as a conference paper at ICLR 2020 techniques to defend against adversarial attacks (<xref ref-type="bibr" rid="b0">Athalye et al., 2018</xref>; <xref ref-type="bibr" rid="b9">Li et al., 2019</xref>) is adversarial training (<xref ref-type="bibr" rid="b10">Madry et al., 2018</xref>; <xref ref-type="bibr" rid="b13">Zhang et al., 2019a</xref>), which improves the adversarial robustness by injecting adversarial examples into the training data.</p><p>Among substantial works of adversarial training, there still remains a big robust generalization gap between the training data and the testing data (<xref ref-type="bibr" rid="b16">Schmidt et al., 2018</xref>; <xref ref-type="bibr" rid="b13">Zhang et al., 2019b</xref>; <xref ref-type="bibr" rid="b6">Ding et al., 2019</xref>; <xref ref-type="bibr" rid="b9">Zhai et al., 2019</xref>). The robustness of adversarial training fails to generalize on unseen testing data. Recent works (<xref ref-type="bibr" rid="b7">Geirhos et al., 2019</xref>; <xref ref-type="bibr" rid="b13">Zhang &amp; Zhu, 2019</xref>) further show that adversarially trained models capture more on global structure features but normally trained models are more biased to- wards local features. In intuition, global structure features tend to be robust against adversarial perturbations but hard to generalize for unseen shape variations, instead, local features generalize well for unseen shape variations but are hard to generalize on adversarial perturbation. It naturally raises an intriguing question for adversarial training:</p><p>For adversarial training, is it possible to learn the robust local features , which have better adver- sarially robust generalization and better standard generalization?</p><p>To address this question, we investigate the relationship between the generalization of adversarial training and the robust local features, and advocate for learning robust local features for adversarial training. Our main contributions are as follows:</p><p>&#8226; To our knowledge, this is the first work that sheds light on the relationship between adversarial training and robust local features. Specifically, we develop a Random Block Shuffle (RBS) transformation to study such relationship by breaking up the global structure features on normal adversarial examples.</p><p>&#8226; We propose a novel method called Robust Local Features for Adversarial Training (RLFAT), which first learns the robust local features, and then transfers the information of robust local features into the training on normal adversarial examples.</p><p>&#8226; To demonstrate the generality of our argument, we implement RLFAT in two currently state- of-the-art adversarial training frameworks, PGD Adversarial Training (PGDAT) (<xref ref-type="bibr" rid="b10">Madry et al., 2018</xref>) and TRADES (<xref ref-type="bibr" rid="b13">Zhang et al., 2019a</xref>). Empirical results show consistent and substantial improvements for both adversarial robustness and standard accuracy on several standard datasets. Moreover, the salience maps of our models on images tend to align better with human perception.</p></sec><sec><title>PRELIMINARIES</title><p>In this section, we introduce some notations and provide a brief description on current advanced methods for adversarial attacks and adversarial training.</p></sec><sec><title>NOTATION</title><p>Let F (x) be a probabilistic classifier based on a neural network with the logits function f (x) and the probability distribution p F (&#183;|x). Let L(F ; x, y) be the cross entropy loss for image classification. The goal of the adversaries is to find an adversarial example x &#8712; B p (x) := {x : x &#8722; x p &#8804; } in the p norm bounded perturbations, where denotes the magnitude of the perturbations. In this paper, we focus on p = &#8734; to align with previous works.</p></sec><sec><title>ADVERSARIAL ATTACKS</title><p>Projected Gradient Descent. Projected Gradient Descent (PGD) (<xref ref-type="bibr" rid="b10">Madry et al., 2018</xref>) is a stronger iterative variant of Fast Gradient Sign Method (FGSM) (<xref ref-type="bibr" rid="b1">Goodfellow et al., 2015</xref>), which iteratively solves the optimization problem max x : x &#8722;x &#8734; &lt; L (F ; x , y) with a step size &#945;:</p><p>where U denotes the uniform distribution, and &#928; B &#8734; (x) indicates the projection of the set B &#8734; (x). Carlini-Wagner attack. Carlini-Wagner attack (<xref ref-type="bibr" rid="b0">CW) (2017b)</xref> is a sophisticated method to directly solve for the adversarial example x adv by using an auxiliary variable w:</p><p>The objective function to optimize the auxiliary variable w is defined as: min w x adv &#8722; x + c &#183; F x adv , (3) where F(x adv ) = max f y true (x adv ) &#8722; max f i (x adv ) : i = y true , &#8722;k . The constant k controls the confidence gap between the adversarial class and the true class.</p><p>N attack. N attack (<xref ref-type="bibr" rid="b9">Li et al., 2019</xref>) is a derivative-free black-box adversarial attack and it breaks many of the defense methods based on gradient masking. The basic idea is to learn a probability density distribution over a small region centered around the clean input, such that a sample drawn from this distribution is likely to be an adversarial example.</p></sec><sec><title>ADVERSARIAL TRAINING</title><p>Despite a wide range of defense methods, <xref ref-type="bibr" rid="b0">Athalye et al. (2018)</xref> and <xref ref-type="bibr" rid="b9">Li et al. (2019)</xref> have broken most previous defense methods (<xref ref-type="bibr" rid="b5">Dhillon et al., 2018</xref>; <xref ref-type="bibr" rid="b1">Buckman et al., 2018</xref>; <xref ref-type="bibr" rid="b6">Wang &amp; Yu, 2019</xref>; <xref ref-type="bibr" rid="b13">Zhang et al., 2019a</xref>), and revealed that adversarial training remains one of the best defense method. The basic idea of adversarial training is to solve the min-max optimization problem, as shown in Eq. (4):</p><p>Here we introduce two currently state-of-the-art adversarial training frameworks.</p><p>PGD adversarial training. PGD Adversarial Training (PGDAT) (<xref ref-type="bibr" rid="b10">Madry et al., 2018</xref>) leverages the PGD attack to generate adversarial examples, and trains only with the adversarial examples. The objective function is formalized as follows: L PGD (F ; x, y) = L(F ; x PGD , y) , (5) where x PGD is obtained via the PGD attack on the cross entropy L(F ; x, y). TRADES. <xref ref-type="bibr" rid="b13">Zhang et al. (2019a)</xref> propose TRADES to specifically maximize the trade-off of adver- sarial training between adversarial robustness and standard accuracy by optimizing the following regularized surrogate loss: L TRADES (F ; x, y) = L(F ; x, y) + &#955;D KL ( p F (&#183;|x) p F (&#183;|x PGD [x]) ) , (6) where x PGD [x] is obtained via the PGD attack on the KL-divergence D KL ( p F (&#183;|x) p F (&#183;|x ) ), and &#955; is a hyper-parameter to control the trade-off between adversarial robustness and standard accuracy.</p></sec><sec><title>ROBUST LOCAL FEATURES FOR ADVERSARIAL TRAINING</title><p>Unlike adversarially trained models, normally trained models are more biased towards the local features but vulnerable to adversarial examples (<xref ref-type="bibr" rid="b7">Geirhos et al., 2019</xref>). It indicates that, in contrast to global structural features, local features seems be more well-generalized but less robust against adversarial perturbation. Based on the basic observation, in this work, we focus on the learning of robust local features on adversarial training, and propose a novel form of adversarial training called RLFAT that learns the robust local features and transfers the robust local features into the training of normal adversarial examples. In this way, our adversarially trained models not only yield strong robustness against adversarial examples but also show great generalization on unseen testing data.</p></sec><sec><title>ROBUST LOCAL FEATURE LEARNING</title><p>It's known that adversarial training tends to capture global structure features so as to increase in- variance against adversarial perturbations (<xref ref-type="bibr" rid="b13">Zhang &amp; Zhu, 2019</xref>; <xref ref-type="bibr" rid="b10">Ilyas et al., 2019</xref>). To advocate for Published as a conference paper at ICLR 2020 the learning of robust local features during adversarial training, we propose a simple and straight- forward image transformation called Random Block Shuffle (RBS) to break up the global structure features of the images, at the same time retaining the local features. Specifically, for an input image, we randomly split the target image into k blocks horizontally and randomly shuffle the blocks, and then we perform the same split-shuffle operation vertically on the resulting image. As illustrated in <xref ref-type="fig" rid="fig_0">Figure 1</xref>, RBS transformation can destroy the global structure features of the images to some extent and retain the local features of the images. Then we apply the RBS transformation on adversarial training. Different from normal adversarial training, we use the RBS-transformed adversarial examples rather than normal adversarial examples as the adversarial information to encourage the models to learn robust local features. Note that we only use the RBS transformation as a tool to learn the robust local features during adversarial training and will not use RBS transformation in the inference phase. we refer to the form of adversarial training as RBS Adversarial Training (RBSAT).</p><p>To demonstrate the generality of our argument, we consider two currently state-of-the-art ad- versarial training frameworks, PGD Adversarial Training (PGDAT) (<xref ref-type="bibr" rid="b10">Madry et al., 2018</xref>) and TRADES (<xref ref-type="bibr" rid="b13">Zhang et al., 2019a</xref>), to demonstrate the effectiveness of the robust local features. We use the following loss function as the alternative to the objective function of PGDAT: L RLFL PGDAT (F ; x, y) = L(F ; RBS(x PGD ), y) , (7) where RBS(&#183;) denotes the RBS transformation; x PGD is obtained via the PGD attack on the cross entropy L(F ; x, y).</p><p>Similarly, we use the following loss function as the alternative to the objective function of TRADES:</p><p>where x PGD [x] is obtained via the PGD attack on the KL-divergence D KL ( p F (&#183;|x) p F (&#183;|x ) ).</p></sec><sec><title>ROBUST LOCAL FEATURE TRANSFER</title><p>Since the type of input images in the training phase and the inference phase is different (RBS trans- formed images for training, versus original images for inference), we consider to transfer the knowl- edge of the robust local features learned by RBSAT to the normal adversarial examples. Specifically, we present a knowledge transfer scheme, called Robust Local Feature Transfer (RLFT). The goal of RLFT is to learn the representation that minimizes the feature shift between the normal adversarial examples and the RBS-transformed adversarial examples.</p><p>In particular, we apply RLFT on the logit layer for high-level feature alignment. Formally, the ob- jective functions of robust local feature transfer for PGDAT and TRADES are formalized as follows, respectively:</p><p>where f (&#183;) denotes the mapping of the logit layer, and &#183; 2 2 denotes the squared Euclidean norm.</p></sec><sec><title>OVERALL OBJECTIVE FUNCTION</title><p>Since the quality of robust local feature transfer depends on the quality of the robust local features learned by RBSAT, we integrate RBSAT and RLFT into an end-to-end training framework, which we refer to as RLFAT (Robust Local Features for Adversarial Training). The general training process of RLFAT is summarized in Algorithm 1. Note that the computational cost of RBS transformation (line 7) is negligible in the total computational cost. Update the parameters of network F through back propagation;</p><p>10: until the training converges.</p><p>We implement RLFAT in two currently state-of-the-art adversarial training frameworks, PGDAT and TRADES, and have new objective functions to learn the robust and well-generalized feature representations, which we call RLFAT P and RLFAT T :</p><p>where &#951; is a hyper-parameter to balance the two terms.</p></sec><sec><title>EXPERIMENTS</title><p>In this section, to validate the effectiveness of RLFAT, we empirically evaluate our two implementa- tions, denoted as RLFAT P and RLFAT T , and show that our models make significant improvement on both robust accuracy and standard accuracy on standard benchmark datasets, which provides strong support for our main hypothesis. Codes are available online 1 .</p></sec><sec><title>EXPERIMENTAL SETUP</title></sec><sec><title>Baselines</title><p>Since most previous defense methods provide few benefit in true adversarially robust- ness (<xref ref-type="bibr" rid="b0">Athalye et al., 2018</xref>; <xref ref-type="bibr" rid="b9">Li et al., 2019</xref>), we compare the proposed methods with state-of-the- art adversarial training defenses, PGD Adversarial Training (PGDAT) (<xref ref-type="bibr" rid="b10">Madry et al., 2018</xref>) and TRADES (<xref ref-type="bibr" rid="b13">Zhang et al., 2019a</xref>).</p></sec><sec><title>Adversarial setting</title><p>We consider two attack settings with the bounded &#8734; norm: the white-box attack setting and the black-box attack setting. For the white-box attack setting, we consider existing strongest white-box attacks: Projected Gradient Descent (PGD) (<xref ref-type="bibr" rid="b10">Madry et al., 2018</xref>) and Carlini- Wagner attack (CW) (<xref ref-type="bibr" rid="b0">Carlini &amp; Wagner, 2017b</xref>). For the black-box attack setting, we perform the powerful black-box attack, N attack (<xref ref-type="bibr" rid="b9">Li et al., 2019</xref>), on a sample of 1,500 test inputs as it is time- consuming.</p></sec><sec><title>Datasets</title><p>We compare the proposed methods with the baselines on widely used benchmark datasets, namely CIFAR-10 and CIFAR-100 (<xref ref-type="bibr" rid="b11">Krizhevsky &amp; Hinton, 2009</xref>). Since adversari- ally robust generalization becomes increasingly hard for high dimensional data and little training data (<xref ref-type="bibr" rid="b16">Schmidt et al., 2018</xref>), we also consider one challenging dataset: STL-10 (<xref ref-type="bibr" rid="b4">Coates et al.</xref>), which contains 5, 000 training images, with 96 &#215; 96 pixels per image.</p></sec><sec><title>Neural networks</title><p>For STL-10, the architecture we use is a wide ResNet 40-2 (<xref ref-type="bibr" rid="b0">Zagoruyko &amp; Ko- modakis, 2016</xref>). For CIFAR-10 and CIFAR-100, we use a wide ResNet w32-10. For all datasets, we scale the input images to the range of [0, 1].</p></sec><sec><title>Hyper-parameters</title><p>To avoid posting much concentrate on optimizing the hyper-parameters, for all datasets, we set the hyper-parameter &#955; in TRADES as 6, set the hyper-parameter &#951; in RLFAT P as 0.5, and set the hyper-parameter &#951; in RLFAT T as 1. For the training jobs of all our models, we set the hyper-parameters k of the RBS transformation as 2. More details about the hyper-parameters are provided in Appendix A.</p></sec><sec><title>EVALUATION RESULTS</title><p>We first validate our main hypothesis: for adversarial training, is it possible to learn the robust local features that have better adversarially robust generalization and better standard generalization? In <xref ref-type="table" rid="tab_0">Table 1</xref>, we compare the accuracy of RLFAT P and RLFAT T with the competing baselines on three standard datasets. The proposed methods lead to consistent and significant improvements on adversarial robustness as well as standard accuracy over the baseline models on all datasets. With the robust local features, RLFAT T achieves better adversarially robust generalization and better standard generalization than TRADES. RLFAT P also works similarly, showing a significant improvement on the robustness against all attacks and standard accuracy than PGDAT. The results demonstrate that, the robust local features can significantly improve both the adver- sarially robust generalization and the standard generalization over the state-of-the-art adversarial training frameworks, and strongly support our hypothesis. That is, for adversarial training, it is possible to learn the robust local features, which have better robust and standard generalization.</p></sec><sec><title>LOSS SENSITIVITY UNDER DISTRIBUTION SHIFT</title><p>Motivation. <xref ref-type="bibr" rid="b6">Ding et al. (2019)</xref> and <xref ref-type="bibr" rid="b13">Zhang et al. (2019b)</xref> found that the effectiveness of adversarial training is highly sensitive to the "semantic-loss" shift of the test data distribution, such as gamma mapping. To further investigate the performance of the proposed methods, we quantify the smooth- ness of the models under the distribution shifts of brightness perturbation and gamma mapping. Loss sensitivity on brightness perturbation. To quantify the smoothness of models on the shift of the brightness perturbation, we propose to estimate the Lipschitz continuity constant F by using the gradients of the loss function with respect to the brightness perturbation of the testing data. We adjust the brightness factor of images in the HSV (hue, saturation, value) color space, which we refer to as x b = V(x, &#945;), where &#945; denotes the magnitude of the brightness adjustment. The lower the value of b F (&#945;) is, the smoother the loss function of the model is:</p><p>Loss sensitivity on gamma mapping. Gamma mapping (<xref ref-type="bibr" rid="b0">Szeliski, 2011</xref>) is a nonlinear element- wise operation used to adjust the exposure of images by applyingx (&#947;) = x &#947; on the original image x. Similarly, we approximate the loss sensitivity under gamma mapping, by using the gradients of the loss function with respect to the gamma mapping of the testing data. A smaller value indicates a smoother loss function.</p></sec><sec><title>Sensitivity analysis</title><p>The results for the loss sensitivity of the adversarially trained models under brightness perturbation are reported in Table 2a, where we adopt various magnitude of brightness adjustment on each testing data. In Table 2b, we report the loss sensitivity of adversarially trained models under various gamma mappings. We observe that RLFAT T provides the smoothest model under the distribution shifts on all the three datasets. The results suggest that, as compared to PGDAT and TRADES, both RLFAT P and RLFAT T show lower gradients of the models on different data distributions, which we can directly attribute to the robust local features.</p></sec><sec><title>ABLATION STUDIES</title><p>To further gain insights on the performance obtained by the robust local features, we perform ab- lation studies to dissect the impact of various components (robust local feature learning and robust local feature transfer). As shown in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, we conduct additional experiments for the ablation studies of RLFAT P and RLFAT T on STL-10, CIFAR-10 and CIFAR-100, where we report the standard accuracy over the clean data and the average robust accuracy over all the attacks for each model.</p></sec><sec><title>Does robust local feature learning help?</title><p>We first analyze that as compared to adversarial training on normal adversarial examples, whether adversarial training on RBS-transformed adversarial ex- amples produces better generalization and more robust features. As shown in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, we observe that Robust Local Features Learning (RLFL) exhibits stable improvements on both standard accu- racy and robust accuracy for RLFAT P and RLFAT T , providing strong support for our hypothesis. Does robust local feature transfer help? We further add Robust Local Feature Transfer (RLFT), the second term in Eq. (10), to get the overall loss of RLFAT. The robust accuracy further increases on all datasets for RLFAT P and RLFAT T . The standard accuracy further increases also, except for RLFAT P on CIFAR-100, but it is still clearly higher than the baseline model PGDAT. It indicates that transferring the robust local features into the training of normal adversarial examples does help promote the standard accuracy and robust accuracy in most cases.</p></sec><sec><title>VISUALIZING THE SALIENCE MAPS</title><p>We would like to investigate the features of the input images that the models are mostly focused on. Following the work of <xref ref-type="bibr" rid="b13">Zhang &amp; Zhu (2019)</xref>, we generate the salience maps using Smooth- Grad (<xref ref-type="bibr" rid="b19">Smilkov et al., 2017</xref>) on STL-10 dataset. The key idea of SmoothGrad is to average the gradients of class activation with respect to noisy copies of an input image. As illustrated in <xref ref-type="fig" rid="fig_2">Fig- ure 3</xref>, all the adversarially trained models basically capture the global structure features of the object on the images. As compared to PGDAT and TRADES, both RLFAT P and RLFAT T capture more local feature information of the object, aligning better with human perception. Note that the images are correctly classified by all these models. For more visualization results, see Appendix B.</p></sec><sec><title>CONCLUSION AND FUTURE WORK</title><p>Differs to existing adversarially trained models that are more biased towards the global structure features of the images, in this work, we hypothesize that robust local features can improve the generalization of adversarial training. To validate this hypothesis, we propose a new stream of adversarial training approach called Robust Local Features for Adversarial Training (RLFAT) and implement it in currently state-of-the-art adversarial training frameworks, PGDAT and TRADES. We provide strong empirical support for our hypothesis and show that the proposed methods based on RLFAT not only yield better standard generalization but also promote the adversarially robust generalization. Furthermore, we show that the salience maps of our models on images tend to align better with human perception, uncovering certain unexpected benefit of the robust local features for adversarial training.</p><p>Our findings open a new avenue for improving adversarial training, whereas there are still a lot to explore along this avenue. First, is it possible to explicitly disentangle the robust local features from the perspective of feature disentanglement? What is the best way to leverage the robust local Published as a conference paper at ICLR 2020 Original PGDAT TRADES RLFATP RLFATT Original PGDAT TRADES RLFATP RLFATT features? Second, from a methodological standpoint, the discovered relationship may also serve as an inspiration for new adversarial defenses, where not only the robust local features but also the global information is taken into account, as the global information is useful for some tasks. These questions are worth investigation in future work, and we hope that our observations on the benefit of robust local features will inspire more future development.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Illustration of the RBS transformation for k = 3. For a better understanding on the RBS transformation, we paint the split image blocks with different colors.</p></caption><graphic /><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>The classification accuracy (%) of defense methods under white-box and black-box attacks on STL-10, CIFAR-10 and CIFAR-100. (a) STL-10. The magnitude of perturbation is 0.03 in &#8734; norm.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>The loss sensitivity of defense methods under different testing data distributions. (a) Loss sensitivity on brightness perturbation for the adversarially trained models.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Ablation studies for RLFAT P and RLFAT T to investigate the impact of Robust Local Feature Learning (RLFL) and Robust Local Feature Transfer (RLFT).</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Salience maps of the four models on sampled images. For each group of images, we have the original image, and the salience maps of the four models sequentially.</p></caption><graphic /><graphic /></fig></sec></body><back><ack /><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</article-title><source>Proceedings of the 35th Interna- tional Conference on Machine Learning, ICML</source><year>2018</year><fpage>274</fpage><lpage>283</lpage><person-group person-group-type="author"><name><surname>References Anish Athalye</surname><given-names>Nicholas</given-names></name><name><surname>Carlini</surname><given-names>David A</given-names></name><name><surname>Wagner</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Thermometer encoding: One hot way to resist adversarial examples</article-title><source>In 6th International Conference on Learning Representations, ICLR</source><year>2018</year><volume>2018</volume><person-group person-group-type="author"><name><surname>Buckman</surname><given-names>Jacob</given-names></name><name><surname>Roy</surname><given-names>Aurko</given-names></name><name><surname>Raffel</surname><given-names>Colin</given-names></name><name><surname>Goodfellow</surname><given-names>Ian J</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Adversarial examples are not easily detected: Bypassing ten detection methods</article-title><source>Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec@CCS</source><year>2017</year><fpage>3</fpage><lpage>14</lpage><person-group person-group-type="author"><name><surname>Carlini</surname><given-names>Nicholas</given-names></name><name><surname>David</surname><given-names>A</given-names></name><name><surname>Wagner</surname><given-names /></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Towards evaluating the robustness of neural networks</article-title><source>IEEE Symposium on Security and Privacy, SP 2017</source><year>2017</year><fpage>39</fpage><lpage>57</lpage><person-group person-group-type="author"><name><surname>Carlini</surname><given-names>Nicholas</given-names></name><name><surname>David</surname><given-names>A</given-names></name><name><surname>Wagner</surname><given-names /></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>An analysis of single-layer networks in unsuper- vised feature learning</article-title><source>Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS</source><year>2011</year><person-group person-group-type="author"><name><surname>Coates</surname><given-names>Adam</given-names></name><name><surname>Ng</surname><given-names>Andrew Y</given-names></name><name><surname>Lee</surname><given-names>Honglak</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Stochastic activation pruning for robust adversarial defense</article-title><source>In 6th International Conference on Learning Representations, ICLR</source><year>2018</year><volume>2018</volume><person-group person-group-type="author"><name><surname>Guneet</surname><given-names>S</given-names></name><name><surname>Dhillon</surname><given-names>Kamyar</given-names></name><name><surname>Azizzadenesheli</surname><given-names>Zachary C</given-names></name><name><surname>Lipton</surname><given-names>Jeremy</given-names></name><name><surname>Bernstein</surname><given-names>Jean</given-names></name><name><surname>Kossaifi</surname><given-names>Aran</given-names></name><name><surname>Khanna</surname><given-names>Animashree</given-names></name><name><surname>Anandkumar</surname><given-names /></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>On the sensitivity of adversarial robustness to input data distributions</article-title><source>In 7th International Conference on Learning Representations, ICLR</source><year>2019</year><volume>2019</volume><person-group person-group-type="author"><name><surname>Gavin Weiguang Ding</surname><given-names>Kry Yik Chau</given-names></name><name><surname>Lui</surname><given-names>Xiaomeng</given-names></name><name><surname>Jin</surname><given-names>Luyu</given-names></name><name><surname>Wang</surname><given-names>Ruitong</given-names></name><name><surname>Huang</surname><given-names /></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</article-title><source>In 7th International Conference on Learning Representations, ICLR</source><year>2019</year><volume>2019</volume><person-group person-group-type="author"><name><surname>Geirhos</surname><given-names>Robert</given-names></name><name><surname>Rubisch</surname><given-names>Patricia</given-names></name><name><surname>Michaelis</surname><given-names>Claudio</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name><name><surname>Wichmann</surname><given-names>Felix A</given-names></name><name><surname>Brendel</surname><given-names>Wieland</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Explaining and harnessing adversarial examples</article-title><source>In 3rd International Conference on Learning Representations, ICLR</source><year>2015</year><volume>2015</volume><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>Ian J</given-names></name><name><surname>Shlens</surname><given-names>Jonathon</given-names></name><name><surname>Szegedy</surname><given-names>Christian</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Deep neural networks for acoustic modeling in speech recognition</article-title><source>IEEE Signal processing magazine</source><year>2012</year><volume>29</volume><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>Geoffrey</given-names></name><name><surname>Deng</surname><given-names>Li</given-names></name><name><surname>Yu</surname><given-names>Dong</given-names></name><name><surname>Dahl</surname><given-names>George</given-names></name><name><surname>Mohamed</surname><given-names>Abdel-Rahman</given-names></name><name><surname>Jaitly</surname><given-names>Navdeep</given-names></name><name><surname>Senior</surname><given-names>An- Drew</given-names></name><name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name><name><surname>Nguyen</surname><given-names>Patrick</given-names></name><name><surname>Kingsbury</surname><given-names>Brian</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Adversarial examples are not bugs, they are features</article-title><source>CoRR</source><year>2019</year><person-group person-group-type="author"><name><surname>Ilyas</surname><given-names>Andrew</given-names></name><name><surname>Santurkar</surname><given-names>Shibani</given-names></name><name><surname>Tsipras</surname><given-names>Dimitris</given-names></name><name><surname>Engstrom</surname><given-names>Logan</given-names></name><name><surname>Tran</surname><given-names>Brandon</given-names></name><name><surname>Madry</surname><given-names>Aleksander</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Learning multiple layers of features from tiny images</article-title><source>Tech- nical report, Citeseer</source><year>2009</year><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>Alex</given-names></name><name><surname>Hinton</surname><given-names>Geoffrey</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Imagenet classification with deep convo- lutional neural networks</article-title><source>Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems</source><year>2012</year><fpage>1106</fpage><lpage>1114</lpage><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>Alex</given-names></name><name><surname>Sutskever</surname><given-names>Ilya</given-names></name><name><surname>Hinton</surname><given-names>Geoffrey E</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>NATTACK: learning the distributions of adversarial examples for an improved black-box attack on deep neural networks</article-title><source>Proceedings of the 36th International Conference on Machine Learning, ICML</source><year>2019</year><fpage>3866</fpage><lpage>3876</lpage><person-group person-group-type="author"><name><surname>Li</surname><given-names>Yandong</given-names></name><name><surname>Li</surname><given-names>Lijun</given-names></name><name><surname>Wang</surname><given-names>Liqiang</given-names></name><name><surname>Zhang</surname><given-names>Tong</given-names></name><name><surname>Gong</surname><given-names>Boqing</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Nesterov accelerated gradient and scale invariance for adversarial attacks</article-title><source>International Conference on Learning Representations</source><year>2020</year><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Jiadong</given-names></name><name><surname>Song</surname><given-names>Chuanbiao</given-names></name><name><surname>He</surname><given-names>Kun</given-names></name><name><surname>Wang</surname><given-names>Liwei</given-names></name><name><surname>Hopcroft</surname><given-names>John E</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Delving into transferable adversarial exam- ples and black-box attacks</article-title><year>2017</year><fpage>24</fpage><lpage>26</lpage><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Yanpei</given-names></name><name><surname>Chen</surname><given-names>Xinyun</given-names></name><name><surname>Liu</surname><given-names>Chang</given-names></name><name><surname>Song</surname><given-names>Dawn</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>Towards deep learning models resistant to adversarial attacks</article-title><source>In 6th International Conference on Learning Representations, ICLR</source><year>2018</year><volume>2018</volume><person-group person-group-type="author"><name><surname>Madry</surname><given-names>Aleksander</given-names></name><name><surname>Makelov</surname><given-names>Aleksandar</given-names></name><name><surname>Schmidt</surname><given-names>Ludwig</given-names></name><name><surname>Tsipras</surname><given-names>Dimitris</given-names></name><name><surname>Vladu</surname><given-names>Adrian</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Practical black-box attacks against machine learning</article-title><source>Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, AsiaCCS</source><year>2017</year><fpage>506</fpage><lpage>519</lpage><person-group person-group-type="author"><name><surname>Papernot</surname><given-names>Nicolas</given-names></name><name><surname>Mcdaniel</surname><given-names>Patrick D</given-names></name><name><surname>Goodfellow</surname><given-names>Ian J</given-names></name><name><surname>Jha</surname><given-names>Somesh</given-names></name><name><surname>Berkay Celik</surname><given-names>Z</given-names></name><name><surname>Swami</surname><given-names>Anan- Thram</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Adver- sarially robust generalization requires more data</article-title><source>Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS</source><year>2018</year><fpage>5019</fpage><lpage>5031</lpage><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>Ludwig</given-names></name><name><surname>Santurkar</surname><given-names>Shibani</given-names></name><name><surname>Tsipras</surname><given-names>Dimitris</given-names></name><name><surname>Talwar</surname><given-names>Kunal</given-names></name><name><surname>Madry</surname><given-names>Aleksander</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Smooth- grad: removing noise by adding noise</article-title><source>CoRR</source><year>2017</year><person-group person-group-type="author"><name><surname>Smilkov</surname><given-names>Daniel</given-names></name><name><surname>Thorat</surname><given-names>Nikhil</given-names></name><name><surname>Kim</surname><given-names>Been</given-names></name><name><surname>Vi&#233;gas</surname><given-names>Fernanda B</given-names></name><name><surname>Wattenberg</surname><given-names>Martin</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Improving the generalization of adversarial training with domain adaptation</article-title><source>In 7th International Conference on Learning Repre- sentations, ICLR</source><year>2019</year><volume>2019</volume><person-group person-group-type="author"><name><surname>Song</surname><given-names>Chuanbiao</given-names></name><name><surname>He</surname><given-names>Kun</given-names></name><name><surname>Wang</surname><given-names>Liwei</given-names></name><name><surname>Hopcroft</surname><given-names>John E</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>Intriguing properties of neural networks</article-title><source>In 2nd International Conference on Learning Representations, ICLR</source><year>2014</year><volume>2014</volume><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>Christian</given-names></name><name><surname>Zaremba</surname><given-names>Wojciech</given-names></name><name><surname>Sutskever</surname><given-names>Ilya</given-names></name><name><surname>Bruna</surname><given-names>Joan</given-names></name><name><surname>Erhan</surname><given-names>Dumitru</given-names></name><name><surname>Goodfel- Low</surname><given-names>Ian J</given-names></name></person-group></element-citation></ref></ref-list></back></article>