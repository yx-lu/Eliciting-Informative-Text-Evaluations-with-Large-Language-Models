Title:
```
Under review as a conference paper at ICLR 2020 POLICY OPTIMIZATION WITH STOCHASTIC MIRROR DESCENT
```
Abstract:
```
Improving sample efficiency has been a longstanding goal in reinforcement learn- ing. In this paper, we propose the VRMPO: a sample efficient policy gradient method with stochastic mirror descent. A novel variance reduced policy gradient estimator is the key of VRMPO to improve sample efficiency. Our VRMPO needs only O( −3 ) sample trajectories to achieve an -approximate first-order stationary point, which matches the best-known sample complexity. We conduct extensive experiments to show our algorithm outperforms state-of-the-art policy gradient methods in vari- ous settings.
```

Figures/Tables Captions:
```
Figure 1: Comparison the performance of MPO with REINFORCE and VPG on the short-corridor grid world domain. In this experiment, we use features φ(s, right) = [1, 0] and φ(s, left) = [0, 1] , s ∈ S.
Figure 2: Comparison of the empirical performance of MPO between non-Euclidean distance (p = 2) and Euclidean distance (p = 2) on standard domains: MountainCar, Acrobot and CartPole.
Figure 3: Learning curves for continuous control tasks. The shaded region represents the standard deviation of the score over the best three trials. Curves are smoothed uniformly for visual clarity.
Table 1: Comparison on complexity required to achieve ∇J(θ) ≤ . Particularly, if ψ(θ) = 1 2 θ 2 2 , then the result (30) of our VRMPO is measured by gradient. Beside, ρ t
Table 2: Max-average return over 500 epochs, where we run 5000 iterations for each epoch. Maxi- mum value for each task is bolded.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) is one of the most wonderful fields of artificial intelligence, and it has achieved great progress recently (Mnih et al., 2015; Silver et al., 2017). To learn the optimal policy from the delayed reward decision system is the fundamental goal of RL. Policy gradient methods (Williams, 1992; Sutton et al., 2000) are powerful algorithms to learn the optimal policy. Despite the successes of policy gradient method, suffering from high sample complexity is still a critical challenge for RL. Many existing popular methods require more samples to be collected for each step to update the parameters (Silver et al., 2014; Lillicrap et al., 2016; Schulman et al., 2015; Mnih et al., 2016; Haarnoja et al., 2018), which partially reduces the effectiveness of the sample. Although all the above existing methods claim it improves sample efficiency, they are all empirical results which lack a strong theory analysis of sample complexity. To improve sample efficiency, in this paper, we explore how to design an efficient and stable algo- rithm with stochastic mirror descent (SMD). Due to its advantage of the simplicity of implemen- tation, low memory requirement, and low computational complexity (Nemirovsky & Yudin, 1983; Beck & Teboulle, 2003; Lei & Tang, 2018), SMD is one of the most widely used methods in machine learning. However, it is not sound to apply SMD to policy optimization directly, and the challenges are two-fold: (I) The objective of policy-based RL is a typical non-convex function, but Ghadimi et al. (2016) show that it may cause instability and even divergence when updating the parameter of a non-convex objective function by SMD via a single batch sample. (II) Besides, the large variance of gradient estimator is the other bottleneck of applying SMD to policy optimization for improving sample efficiency. In fact, in reinforcement learning, the non-stationary sampling process with the environment leads to the large variance of existing methods on the estimate of policy gradient, which results in poor sample efficiency (Papini et al., 2018; Liu et al., 2018).

Section Title: Contributions
  Contributions To address the above two problems correspondingly, in this paper (I) We analyze the theoretical dilemma of applying SMD to policy optimization. Our analysis shows that under the common Assumption 1, for policy-based RL, designing the algorithm via SMD directly can not guarantee the convergence. Hence, we propose the MPO algorithm with a provable convergence guarantee. Designing an efficiently computable, and unbiased gradient estimator by averaging its historical policy gradient is the key to MPO. (II) We propose the VRMPO: a sample efficient policy op- timization algorithm via constructing a variance reduced policy gradient estimator. Specifically, we propose an efficiently computable policy gradient estimator, utilizing fresh information and yielding a more accurate estimation of the gradient w.r.t the objective, which is the key to improve sample efficiency. We prove VRMPO needs O( −3 ) sample trajectories to achieve an -approximate first- order stationary point ( -FOSP) (Nesterov, 2004). To our best knowledge, our VRMPO matches the Under review as a conference paper at ICLR 2020 best-known sample complexity among the existing literature. Besides, we conduct extensive ex- periments, which further show that our algorithm outperforms state-of-the-art bandit algorithms in various settings.

Section Title: BACKGROUND AND NOTATIONS
  BACKGROUND AND NOTATIONS

Section Title: POLICY-BASED REINFORCEMENT LEARNING
  POLICY-BASED REINFORCEMENT LEARNING We consider the Markov decision processes M = (S, A, P, R, ρ 0 , γ), where S is state space, A is action space; At time t, the agent is in a state S t ∈ S and takes an action A t ∈ A, then it receives a feedback R t+1 ; P a ss = P (s |s, a) ∈ P is the probability of the state transition from s to s under taking a ∈ A; The bounded reward function R : S × A → [−R, R], R a s → E[R t+1 |S t = s, A t = a]; ρ 0 : S → [0, 1] is the initial state distribution and γ ∈ (0, 1) is discounted factor. Policy π θ (a|s) is a probability distribution on S × A with the parameter θ ∈ R p . Let τ = {s t , a t , r t+1 } Hτ t=0 be a trajectory, where s 0 ∼ ρ 0 (s 0 ), a t ∼ π θ (·|s t ), r t+1 = R(s t , a t ), s t+1 ∼ P (·|s t , a t ), and H τ is the finite horizon of τ . The expected return J(π θ ) is defined as: J(θ) def = J(π θ ) = τ P (τ |θ)R(τ )dτ = E τ ∼π θ [R(τ )], (1) where P (τ |θ) = ρ 0 (s 0 ) Hτ t=0 P (s t+1 |s t , a t )π θ (a t |s t ) is the probability of generating τ , R(τ ) = Hτ t=0 γ t r t+1 is the accumulated discounted return. Let J (θ) = −J(θ), the central problem of policy-based RL is to solve the problem: Computing the ∇J(θ) analytically, we have For any trajectory τ , let g(τ |θ) = Hτ t=0 ∇ θ log π θ (a t |s t )R(τ ), which is an unbiased estimator of ∇J(θ). Vanilla policy gradient (VPG) is a straightforward way to solve problem (2): θ ← θ + αg(τ |θ), where α is step size. Assumption 1. (Sutton et al., 2000; Papini et al., 2018) For each pair (s, a), any θ ∈ R p , and all components i, j, there exists positive constants G, F s.t., According to the Lemma B.2 of (Papini et al., 2018), Assumption 1 implies ∇J(θ) is L-Lipschiz, i.e., ∇J(θ 1 ) − ∇J(θ 2 ) ≤ L θ 1 − θ 2 , where Besides, Assumption 1 implies the following property of the policy gradient estimator. Lemma 1 (Properties of stochastic differential estimators (Shen et al., 2019)). Under Assumption 1, for any policy π θ and τ ∼ π θ , we have

Section Title: STOCHASTIC MIRROR DESCENT
  STOCHASTIC MIRROR DESCENT Now, we review some basic concepts of SMD; in this section, the notation follows (Nemirovski et al., 2009). Let's consider the stochastic optimization problem, Under review as a conference paper at ICLR 2020 where D θ ∈ R n is a nonempty convex compact set, ξ is a random vector whose probability dis- tribution, µ is supported on Ξ ∈ R d and F : D θ × Ξ → R. We assume that the expectation E[F (θ; ξ)] = Ξ F (θ; ξ)dµ(ξ) is well defined and finite-valued for every θ ∈ D θ . Definition 1 (Proximal Operator (Moreau, 1965)). T is a function defined on a closed convex X , and α > 0. M ψ α,T (z) is the proximal operator of T , which is defined as:

Section Title: Stochastic Mirror Descent
  Stochastic Mirror Descent The SMD solves (7) by generating an iterative solution as follows, If we choose ψ(x) = 1 2 x 2 2 , which implies D ψ (x, y) = 1 2 x − y 2 2 , since then iteration (9) is the proximal gradient (Rockafellar, 1976) view of SGD. Thus, SMD is a generalization of SGD. Convergence Criteria: Bregman Gradient Bregman gradient is a generation of projected gradi- ent (Ghadimi et al., 2016). Recently, Zhang & He (2018); Davis & Grimmer (2019) develop it to measure the convergence of an algorithm for the non-convex optimization problem. Evaluating the difference between each candidate solution x and its proximity is the critical idea of Bregman gra- dient to measure the stationarity of x. Specifically, let X be a closed convex set on R n , α > 0, T (x) is defined on X . The Bregman gradient of T at x ∈ X is: G ψ α,T (x) = α −1 (x − M ψ α,T (x)), (10) where M ψ α,T (·) is defined in Eq.(8). If ψ(x) = 1 2 x 2 2 , then x * is a critical point of T if and only if G ψ α,T (x * ) = ∇T (x * ) = 0 (Bauschke et al. (2011);Theorem 27.1). Thus, Bregman gradient (10) is a generalization of gradient. The following Remark 1 is helpful for us to understand the significance of Bregman gradient, and it gives us some insights to understand this convergence criterion. Remark 1. Let T be a convex function, by the Proposition 5.4.7 of Bertsekas (2009): x * is a sta- tionarity point of T if and only if 0 ∈ ∂(T + δ X )(x * ), (11) where δ X (·) is the indicator function on X . Furthermore, suppose ψ(x) is twice continuously differ- entiable, letx = M ψ α,T (x), by the definition of proximal operator M ψ α,T (·), we have Eq.( ) holds due to the first-order Taylor expansion of ∇ψ(x). By the criteria of (11), if G ψ α,T (x) ≈ 0, Eq.(12) implies the origin point 0 is near the set ∂(T + δ X )(x), i.e.,x is close to a stationary point. In practice, we choose T (θ) = −∇J(θ t ), θ , since then discriminant criterion (12) is suitable to RL problem (2). For the non-convex problem (2), we are satisfied with finding an -approximate First-Order Stationary Point ( -FOSP) (Nesterov, 2004), denoted by θ , such that

Section Title: POLICY OPTIMIZATION WITH STOCHASTIC MIRROR DESCENT
  POLICY OPTIMIZATION WITH STOCHASTIC MIRROR DESCENT In this section, we solve the problem (2) via SMD. Firstly, we analyze the theoretical dilemma of applying SMD directly to policy optimization. Then, we propose a convergent mirror policy optimization algorithm (MPO).

Section Title: THEORETICAL DILEMMA
  THEORETICAL DILEMMA Let T = {τ k } N −1 k=0 be a collection of trajectories, τ k ∼ π θ k , we receive gradient information: where α k > 0 is step-size and other symbols are consistent to previous paragraphs. Due to −J(θ) is non-convex, according to (Ghadimi et al., 2016), a standard strategy for analyzing non-convex optimization methods is to pick up the outputθ N randomly according to the following distribution over {1, 2, · · · , N }: Theorem 1. (Ghadimi et al., 2016) Under Assumption 1, and the total trajectories are {τ k } N k=1 . Consider the sequence {θ k } N k=1 generated by (15), the outputθ N = θ n follows the probability mass distribution of (16). Let 0 < α k < ζ L , (g, u) = g, u , the term L and σ are defined in Eq.(5) and Eq.(6) correspondingly. Then, we have which can not guarantee the convergence of the iteration (15), no matter how the step-size α k is specified. Thus, under the Assumption 1, generating the solution {θ k } N k=1 according to (15) and the output following (16) lack a strong convergence guarantee. An Open Problem The iteration (15) is a very important and general scheme that unifies many ex- isting algorithms. For example, if the mirror map ψ(θ) = 1 2 θ 2 2 , then the update (15) is reduced to policy gradient algorithm (Sutton et al., 2000) which is widely used in modern RL. The update (15) is natural policy gradient (Kakade, 2002; Peters & Schaal, 2008; Thomas et al., 2013) if we choose mirror map ψ(θ) = 1 2 θ F (θ)θ, where F = E τ ∼π θ [∇ θ log π θ (s, a)∇ θ log π θ (s, a) ] is Fisher in- formation matrix. If ψ is Boltzmann-Shannon entropy function (Shannon, 1948), then D ψ is known as KL divergence and update (15) is reduced to relative entropy policy search (Peters et al., 2010; Fox et al., 2016; Chow et al., 2018). Despite the vast body of work around above specific meth- ods, current works are scattered and fragmented in both theoretical and empirical aspects (Agarwal et al., 2019). Thus, it is of great significance to establish the fundamental theoretical convergence properties of iteration (15). Please notice that for the non-convexity of problem (2), the lower bound of (18) holds under As- sumption 1. It is natural to ask: What conditions guarantee the convergence of scheme (15)? This is an open problem. Although, the iteration (15) is intuitively a convergent scheme, as dis- cussed above that particular mirror maps ψ can lead (15) to some popular empirically effective RL algorithms; there is still no generally complete theoretical convergence analysis of (15). Such con- vergence properties not only help us to understand better why those methods work but also inspire us to design novel algorithms with the principled approaches. We leave this open problem and the related questions, e.g., how fast the iteration (15) converges to global optimality or its finite sample analysis, as future works. 4: end for 5: Output:θ N according to (16).

Section Title: AN IMPLEMENTATION WITH CONVERGENT GUARANTEE
  AN IMPLEMENTATION WITH CONVERGENT GUARANTEE In this section, we propose a convergent implementation defined as follows, for each step k: whereĝ k is an arithmetic mean of previous episodes' gradient estimate {g(τ i |θ i )} k i=1 : We present the details of an implementation in Algorithm 1. Notice that Eq.(22) is an incremental implementation of the average (20), thus, (22) enjoys a lower storage cost than (20). For a given episode, the gradient flow (20)/(22) of MPO is slightly different from the traditional VPG, REINFORCE (Williams, 1992), A2C (Mnih et al., 2016) or DPG (Silver et al., 2014) whose gradient estimator follows (14) that is according to the trajectory of current episode, while our MPO uses an arithmetic mean of previous episodes' gradients. The estimator (14) is a natural way to estimate the term −∇J(θ t ) = −E[ Hτ t k=0 ∇ θ log π θ (a k |s k )R(τ t )], i.e. using a single current trajectory to estimate policy gradient. Unfortunately, under Assumption 1, the result of (18) shows using (14) with SMD lacks a guarantee of convergence. This is exactly the reason why we abandon the way (14) and turn to propose (20)/(22) to estimate policy gradient. We provide the convergence analysis of our scheme (20)/(22) in the next Theorem 2. Theorem 2 (Convergence Rate of Algorithm 1). Under Assumption 1, and the total trajectories are {τ k } N k=1 . Consider the sequence {θ k } N k=1 generated by Algorithm 1, and the outputθ N = θ n follows the distribution of Eq.(16). Let 0 < α k < ζ L , (g, u) = g, u , the term L and σ are defined in Eq.(5) and Eq.(6) correspondingly. Letĝ k = 1 Then we have We prove the proof in Appendix A. Let α k = ζ/2L, then, Eq(24) is Our scheme of MPO partially answers the previous open problem through conducting a new policy gradient estimator.

Section Title: VRMPO: A VARIANCE REDUCTION IMPLEMENTATION OF MPO
  VRMPO: A VARIANCE REDUCTION IMPLEMENTATION OF MPO In this section, we propose a variance reduction version of MPO: VRMPO. In optimization commu- nity, variance reduction gradient estimator is a very popular method with provable convergence Under review as a conference paper at ICLR 2020 Algorithm 2 Variance-Reduced Mirror Policy Optimization (VRMPO). 1: Initialize: Policy π θ (a|s) with parameterθ 0 , mirror map ψ,step-size α k > 0, epoch size K,m. Let θ 1 = θ 0 − αG 0 , for each time t ∈ N + , let {τ t j } N j=1 be the trajectories generated by π θt , we define the policy gradient estimate G t and the update rule of parameter as follows, random trajectories. As far as we know, our VRMPO matches the best-known sample complex- ity as the HAPG algorithm (Shen et al., 2019). In fact, according to (Shen et al., 2019), REINFORCE needs O( −4 ) random trajectory trajectories to achieve the -FOSP, and no provable improvement on its complexity has been made so far. The same order of sample complexity of REINFORCE is shown by Xu et al. (2019). With the additional assumptions Var H h=0 π θ0 (a h |s h ) π θt (a h |s h ) ,Var[g(τ |θ)] < +∞, Papini et al. (2018) show that the SVRPG achieves the sample complexity of O( −4 ). Later, under the same assumption as (Papini et al., 2018), Xu et al. (2019) reduce the sample complexity of SVRPG to O( − 10 3 ). We provide more details of the comparison in  Table 1 , from which it is easy to see that our VRMPO matches the best-known sample complexity with least conditions.

Section Title: RELATED WORKS
  RELATED WORKS Stochastic Variance Reduced Gradient in RL Although it has achieved considerable successes in supervised learning, stochastic variance reduced gradient optimization is rarely a matter of choice in RL. To our best knowledge, Du et al. (2017) firstly introduce SVRG (Johnson & Zhang, 2013) to off-policy evaluation. Du et al. (2017) transform the empirical policy evaluation problem into a (quadratic) convex-concave saddle-point problem, then they solve the problem via SVRG straightfor- wardly. Later, to improve sample efficiency for complex RL, Xu et al. (2017) combine SVRG with TRPO (Schulman et al., 2015). Similarly, Yuan et al. (2019) introduce SARAH (Nguyen et al., 2017a) to TRPO to improve sample efficiency. However, the results presented by Xu et al. (2017) and Yuan et al. (2019) are empirical, which lacks a strong theory analysis. Metelli et al. (2018) present a sur- rogate objective function with a Rényi divergence (Rényi et al., 1961) to reduce the variance caused by importance sampling. Recently, Papini et al. (2018) propose a stochastic variance reduced version of policy gradient (SVRPG), and they define the gradient estimator via important sampling, where G t−1 is an unbiased estimator according to the trajectory generated by π θt−1 . Although the above algorithms are practical empirically, their gradient estimates are dependent heavily on impor- tant sampling. This fact partially reduces the effectiveness of variance reduction. Later, Shen et al. (2019) remove the important sampling term, and they construct the gradient estimator as follows, It is different from (Du et al., 2017; Xu et al., 2017; Papini et al., 2018; Shen et al., 2019), the proposed VRMPO admits a stochastic recursive iteration to estimate the policy gradient, see Eq.(28). Our VRMPO exploits the fresh information to improve convergence and reduce variance. Besides, VRMPO reduces the storage cost significantly due to it doesn't require to store the complete historical information. We provide more details of the comparison in  Table 1 . Baseline Methods for Variance Reduction of Policy Gradient Baseline (also also known as con- trol variates (Cheng et al., 2019a) or reward reshaping (Ng et al., 1999; Jie & Abbeel, 2010)) is a widely used technique to reduce the variance (Weaver & Tao, 2001; Greensmith et al., 2004). For example, A2C (Sutton & Barto, 1998; Mnih et al., 2016) introduces the value function as baseline function, Wu et al. (2018) consider action-dependent baseline, and Liu et al. (2018) use the Stein's identity (Stein, 1986) as baseline. Q-Prop (Gu et al., 2017) makes use of both the linear dependent baseline and GAE (Schulman et al., 2016) to reduce variance. Cheng et al. (2019b) present a predictor- corrector framework that can transform a first-order model-free algorithm into a new hybrid method that leverages predictive models to accelerate policy learning. Mao et al. (2019) derive a bias-free, Under review as a conference paper at ICLR 2020 input-dependent baseline to reduce variance, and analytically show its benefits over state-dependent baselines. Recently, Grathwohl et al. (2018); Cheng et al. (2019a) provide a standard explanation for the benefits of such approaches with baseline function. However, the capacity of all the above methods is limited by their choice of baseline function (Liu et al., 2018). In practice, it is troublesome to design a proper baseline function to reduce the variance of policy gradient estimate. Our VRMPO avoids the selection of baseline function, and it uses a current sample trajectory to construct a novel, efficiently computable gradient estimator to reduce variance and speed convergence.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: NUMERICAL ANALYSIS OF MPO
  NUMERICAL ANALYSIS OF MPO In this section, we use an experiment to demonstrate MPO converges faster than VPG/REINFORCE. Then, we test how the mirror map ψ effects the performance of MPO.

Section Title: Performance Comparison
  Performance Comparison We compare the convergence rate of MPO with REINFORCE and VPG em- pirically on the Short Corridor with Switched Actions domain (Chapter 13, Sutton & Barto (2018); We provide some details in Appendix B). The task is to estimate the value function of state s 1 , V (s 1 ) = G 0 ≈ −11.6. The initial pa- rameter θ 0 = U[−0.5, 0.5], where U is uniform distribution. Before we report the experimental results, it is necessary to explain why we only use VPG and REINFORCE as the baseline to compare with our MPO. VPG/REINFORCE is one of the most basic policy gradient methods in RL, and extensive modern policy-based algorithms are derived from VPG/REINFORCE. Our MPO is a novel framework via mirror map to learn the parameter, see (23). Thus, it is natural to compare with VPG and REINFORCE. The result in  Figure 1  shows that MPO converges faster significantly and achieves a better performance than both REINFORCE and MPO. To compare fairly, we use the same random seed, and let p run in [P ] = {1.1, 1.2, · · · , 1.9, 2, 3, 4, 5}. For the non-Euclidean distance case, we only show p = 3, 4, 5, and "best", where p = "best" value is that case it achieves the best performance among the set [P ]. For the limitation of space, we provide more details of experiments in Appendix D.1. The result in  Figure 2  shows that the best method is produced by non-Euclidean distance, not the Euclidean distance. The traditional policy gradient methods such as REINFORCE, VPG, and DPG are all the algorithms update parameters in Euclidean distance. This simple experiment gives us some lights that one can create better algorithms by combining the existing approaches with non-Euclidean distance, which is an interesting direction, and we left it as future work.

Section Title: EVALUATE VRMPO ON CONTINUOUS CONTROL TASKS
  EVALUATE VRMPO ON CONTINUOUS CONTROL TASKS In this section, we compare VRMPO on the MuJoCo continuous control tasks (Todorov et al., 2012) from OpenAI Gym (Brockman et al., 2016). We compare VRMPO with DDPG (Lillicrap et al., 2016), PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and TD3 (Fujimoto et al., 2018). For fairness, all the setups mentioned above share the same network architecture that computes the policy and state value. We run all the algorithms with ten random seeds. The results of max-average epoch return are present in  Table 2 , and return curves are shown in  Figure 3 . For the limitation of space, we present all the details of experiments and some practical tricks for the implementation of VRMPO in Appendix D.2-D.5; in this section, we only offer our experimental results. We evaluate the performance of VRMPO by the following three aspects: score performance, the stability of training, and variance.

Section Title: Score Performance Comparison
  Score Performance Comparison From the results of  Figure 3  and  Table 2 , overall, VRMPO outper- forms the baseline algorithms in both final performance and learning process. Our VRMPO also learns considerably faster with better performance than the popular TD3 on Walker2d-v2, HalfCheetah-v2, Hopper-v2, InvDoublePendulum-v2, and Reacher-v2 domains. On the InvDoublePendulum-v2 task, our VRMPO has only a small advantage over other algorithms. This is because the InvPendulum-v2 task is relatively easy. The advantage of our VRMPO becomes more powerful when the task is more difficult. It is worth noticing that on the HalfCheetah-v2 domain, our VRMPO achieves a significant max-average score 16000+, which outperforms far more than the second-best score 11781.

Section Title: Stability
  Stability The stability of an algorithm is also an important topic in RL. Although DDPG exploits the off-policy sample, which promotes its efficiency in stable environments, DDPG is unstable on the Reacher-v2 task, while our VRMPO learning faster significantly with lower variance. DDPG fails to make any progress on InvDoublePendulum-v2 domain, and the result is corroborated by the work (Dai et al., 2018). Although TD3 takes the minimum value between a pair of critics to limit overes- Under review as a conference paper at ICLR 2020 timation, it learns severely fluctuating in the InvertedDoublePendulum-v2 environment. In contrast, our VRMPO is consistently reliable and effective in different tasks.

Section Title: Variance Comparison
  Variance Comparison As we can see from the results in  Figure 3 , our VRMPO converges with a considerably low variance in the Hopper-v2, InvDoublePendulum-v2, and Reacher-v2. Although the asymptotic variance of VRMPO is slightly larger than other algorithms in HalfCheetah-v2, the final performance of VRMPO outperforms all the baselines significantly. The result in  Figure 3  also implies conducting a proper gradient estimator not only reduce the variance of the score during the learning but speed the convergence of training.

Section Title: CONCLUSION
  CONCLUSION In this paper, we propose the mirror policy optimization (MPO) by estimating the policy gradient via dynamic batch-size of historical gradient information. Results show that making use of historical gradients to estimate policy gradient is more effective to speed convergence. We also propose a variance reduction implementation for MPO: VRMPO, and prove the complexity of VRMPO achieves O( −3 ). To our best knowledge, VRMPO matches the best-known sample complexity so far. Finally, we evaluate the performance of VRMPO on the MuJoCo continuous control tasks, results show that VRMPO outperforms or matches several state-of-art algorithms DDPG, TRPO, PPO, and TD3.

```
