Title:
```
Under review as a conference paper at ICLR 2020 CONFIDENCE SCORES MAKE INSTANCE-DEPENDENT LABEL-NOISE LEARNING POSSIBLE
```
Abstract:
```
Learning with noisy labels has drawn a lot of attention. In this area, most of recent works only consider class-conditional noise, where the label noise is independent of its input features. This noise model may not be faithful to many real-world ap- plications. Instead, few pioneer works have studied instance-dependent noise, but these methods are limited to strong assumptions on noise models. To alleviate this issue, we introduce confidence-scored instance-dependent noise (CSIDN), where each instance-label pair is associated with a confidence score. The confidence scores are sufficient to estimate the noise functions of each instance with minimal assumptions. Moreover, such scores can be easily and cheaply derived during the construction of the dataset through crowdsourcing or automatic annotation. To handle CSIDN, we design a benchmark algorithm termed instance-level forward correction. Empirical results on synthetic and real-world datasets demonstrate the utility of our proposed method.
```

Figures/Tables Captions:
```
Figure 1: Illustration of different noise models. Each color represents an observed classȳ: circles indicateȳ = y, while crosses indicateȳ = y. The size of each point represents the confidence scores in the labelȳ: the bigger the point is, the more confident it is. In the CCN model, the noise function only depends on the label of each instance. In the IDN and CSIDN models, the noise function depends on the observed instance x. To illustrate the IDN model, we show a special case called boundary-consistent noise, i.e., points that lie close to the decision boundary are more likely to be mislabelled. Note the CSIDN model varies from the IDN model via confidence scores (Section 2.3).
Figure 2: The limitation of the small-loss approaches in the IDN model. (a) Clean distribution. (b) instance-dependent noise in the direction w = (0, 1) with an average corruption rate of 40%: points towards the upper region are more likely to be corrupted than points towards the bottom region. (c) Density map of the instances selected by a small-loss approach at epoch 10. The sample selection gets biased towards clean regions. Since the clean and noisy regions have different distributions, selecting most instances from clean regions creates a covariate-shift between the training and test distributions, which can greatly degrades performances.
Figure 3: The test accuracy on synthetic datasets with different levels of IDN noise.
Figure 4: The test accuracy on real-world datasets with different levels of IDN noise.
Table 1: Comparisons between baselines and our work for handling the IDN model. Rate identifia- bility denotes whether the transition matrix is identifiable.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The recent success of deep neural networks has increased the need for high-quality labeled data. However, such a labelling process can be time-consuming and costly. A compromise is to resort to weakly-supervised annotations, using crowdsourcing platforms or trained classifiers that annotate the data automatically. These weakly-supervised annotations tend to be low-quality and noisy, which negatively affects the accuracy of high-capacity models due to memorization effects ( Zhang et al., 2017 ). Thus, learning with noisy labels has often drawn a lot of attention. Early works on noisy labels studied random classification noise (RCN) for binary classification ( Angluin & Laird, 1988 ;  Kearns, 1993 ). In the RCN model, each instance has its label flipped with a fixed noise rate ρ ∈ [0, 1 2 ). A natural extension of RCN is class-conditional noise (CCN) for multi- class classification ( Stempfel & Ralaivola, 2009 ;  Natarajan et al., 2013 ;  Scott et al., 2013 ;  Menon et al., 2015 ;  van Rooyen & Williamson, 2015 ;  Patrini et al., 2016 ) (Appendix A). In the CCN model, each instance from class i has a fixed probability ρ i,j of being assigned to class j. Thus, it is possible to encode some similarity information between classes. For example, we can expect that the image of a "dog" is more likely to be erroneously labelled as "cat" than "boat". To handle the CCN model, a common method is the loss correction, which aims to correct the pre- diction or the loss of the classifier using an estimated noise transition matrix ( Patrini et al., 2017 ;  Sukhbaatar et al., 2015 ;  Goldberger & Ben-Reuven, 2017 ;  Ma et al., 2018 ). Another common ap- proach is the label correction, which aims to improve the label quality during training. For example,  Reed et al. (2015)  introduced a bootstrapping scheme. Similarly,  Tanaka et al. (2018)  proposed to update the weights of a classifier iteratively using noisy labels, and use the updated classifier to yield more high-quality pseudo-labels for the training set. Although these methods have theoretical guarantees, they are unable to cope with real-world noise, e.g., instance-dependent noise (IDN). The IDN model considers a more general noise ( Manwani & Sastry, 2013 ;  Ghosh et al., 2014 ;  Menon et al., 2016 ;  Cheng et al., 2017 ;  Menon et al., 2018 ), where the probability that an instance is mislabeled depends on both its class and features. Intuitively, this noise is quite realistic, as poor- quality or ambiguous instances are more likely to be mislabeled in real-world datasets. However, it is much more complex to formulate the IDN model, since the probability of a mislabeled instance is a function of not only the label space but also the input space that can be very high dimensional. As a result, several pioneer works have considered stronger assumptions on noise functions. How- ever, stronger assumptions tend to restrict the utility of these works ( Table 1 ). For instance, the boundary-consistent noise model considers stronger noise for samples closer to the decision bound- ary of the Bayesian optimal classifier ( Du & Cai, 2015 ;  Menon et al., 2018 ). However, such a model is restricted to binary and cannot estimate noise functions.  Cheng et al. (2017)  recently studied a particular case of the IDN model, where noise functions are upper-bounded. Nonetheless, their method is limited to binary classification and has only been tested on small datasets. Instead of simplifying assumptions on noise functions, we propose to tackle the IDN model from the source, by considering confidence scores to be available for the label of each instance. We term this new setting confidence-scored instance-dependent noise (CSIDN, Figure 1c). The confidence scores denote how likely an instance is to be correctly labeled. Assuming that (i) confidence scores are available for each instance, (ii) transitions probabilities to other classes are independent of the instance conditionally on the assigned label being erroneous and (iii) a set of anchor points is avail- able, we derive an instance-level forward correction algorithm which can fully estimate the transi- tion probability for each instance, and subsequently train a robust classifier with a loss-correction method similarly to  Patrini et al. (2017) . It is noted that confidence scores can be easily and cheaply derived during the construction of the dataset. For example, in crowdsourcing platforms, simply counting how many annotators agree on a given instance can give a notion of how confident a label is. Besides, many real-world datasets are automatically annotated using a trained classifier, such as web-scraped datasets ( Tong Xiao et al., 2015 ) and physiological features inferred from medical records ( Agarwal et al., 2016 ). In these cases, the class-probabilities of the labels assigned by the classifier can be seen as confidence scores, provided that the classifier is well calibrated ( Guo et al., 2017 ). To sum up, we first formulate instance-dependent noise in Section 2.1, and expose its robustness challenge in Section 2.2. Then, we explain our motivation to use confidence scores, and propose the confidence-scored instance-dependent noise (CSIDN) model in Section 2.3. Lastly, to handle this new noise model, we present the first practical algorithm termed instance-level forward correction in Section 3, and validate the proposed algorithm through extensive experiments in Section 4.

Section Title: TACKLING INSTANCE-DEPENDENT NOISE FROM THE SOURCE
  TACKLING INSTANCE-DEPENDENT NOISE FROM THE SOURCE In this section, we present the IDN model along with the limitations of existing approaches, and introduce the CSIDN model as a tractable instance-dependent noise model.

Section Title: NOISE MODELS: FROM CLASS-CONDITIONAL TO INSTANCE-DEPENDENT NOISE
  NOISE MODELS: FROM CLASS-CONDITIONAL TO INSTANCE-DEPENDENT NOISE We formulate the problem of learning with noisy labels in this section. Let D be the distribution of a pair of random variables (X, Y ) ∈ X × Y, where X ∈ R d , Y = {1, 2, . . . , K} and K is the number of classes. In the classification task with noisy labels, we hope to train a classifier while having only access to samples from a noisy distributionD of random variables (X,Ȳ ) ∈ X × Y. Given a point x sampled from X,Ȳ is derived from the random variable Y via a noise transition matrix Each noise function T i,j : X → [0, 1] is defined as T i,j (x) = P (Ȳ = j|Y = i, X = x). In the class-conditional noise (CNN) model (Figure 1a), the transition matrix does not depend on the instance x and the noise is entirely characterized by the K 2 constants T i,j . However, in the instance- dependent noise (IDN) model (Figure 1b), the transition matrix depends on the actual instance. This tremendously complicates the problem, as the noise is now characterized by K 2 functions over the latent space X , which can be very high dimensional (e.g., d ∼ 10 4 -10 6 for an object recognition dataset).

Section Title: CHALLENGES FROM INSTANCE-DEPENDENT NOISE
  CHALLENGES FROM INSTANCE-DEPENDENT NOISE

Section Title: Limitation of existing CCN methods
  Limitation of existing CCN methods Due to the complexity of the IDN model, most recent works in learning with noisy labels have focused on the CCN model (Figure 1a), and the CCN model can be seen as a simplified IDN model (Figure 1b) free of feature information. In addition to loss correction and label correction mentioned before, another method for the CCN model is sample selection, which aims to find reliable samples during training, such as the small- loss approaches ( Jiang et al., 2018 ;  Han et al., 2018 ). Inspired by the memorization in deep learning (Arpit et al., 2017), those methods first run a standard classifier on a noisy dataset, then select the small-loss samples for reliable training. However, all approaches cannot handle the IDN model directly. Specifically, loss correction con- siders the noise model to be characterized by a fixed transition matrix, which does not include any instance-level information. Meanwhile, label correction is vulnerable to the IDN model, since the classifier will be much weaker on noisy regions and labels corrected by the current prediction would likely be erroneous. Similarly, sample selection is easily affected by the IDN model. For example, in the small-loss approaches, instance-dependent noise functions can leave partial re- gions of the input space clean and other regions very noisy (e.g., in an object recognition dataset, poor-quality pictures will tend to receive more noisy labels than high-quality ones). Since clean regions will tend to receive smaller losses than noisy regions, the small-loss approaches, which only trains on points with the smallest-losses, will focus on clean regions and neglect harder noisy re- gions. Then, since the distribution of clean regions will subsequently be different from the global distribution, this will introduce a covariate-shift ( Shimodaira, 2000 ), which greatly degrades perfor- mances. Moreover, it is hard to use importance reweighting ( Sugiyama et al., 2007 ) for alleviate the issue, since importance reweighting would require estimating the clean posterior probability that is intractable for the IDN model. To validate this fact, we generate a 3-class distribution of concentric circles (cf. Figure 2a), with then train a network on the top R(T ) small-loss instances at each epoch T based on the losses of the previous epoch, with R(T ) decreasing in T as described in  Han et al. (2018) . Figure 2c shows the density of the top 50% small-loss instances selected after 10 epochs: since noisy regions are associated to higher losses, the network eventually tends to select instances from the clean region and neglect the noisy region. This leads to covariate-shift, which is associated with decreased per- formances ( Shimodaira, 2000 ).

Section Title: Limitation of pioneer IDN methods
  Limitation of pioneer IDN methods The main challenge of the IDN model is the wide range of possible noise functions included in its formulation. Since each T i,j (·) is a function of the high- dimensional input space X , it is challenging for a model to be flexible enough to fit any real-world noise function while being trainable on corrupted datasets, let alone derive theoretical results. In- stead, various recent works have considered stronger assumptions on noise functions. For instance, boundary-consistent noise (BCN), first introduced by ( Du & Cai, 2015 ) and general- ized in  Menon et al. (2018) , considers stronger noise for samples closer to the decision boundary of the Bayesian optimal classifier. This is a reasonable model for noise from human annotators, since "harder" instances (i.e., instances closer to the decision boundary) are more likely to be cor- rupted. Moreover, it is simple enough to derive some theoretical guarantees, as done in  Menon et al. (2018) . Additionally, an extension of the BCN model was studied in  Bootkrajang & Chaijaruwanich (2018) , where the noise function is a Gaussian mixture of the distance to the Bayesian optimal boundary. However, the BCN model and its extension are restricted to binary classification, and their geometry-based assumption becomes difficult to fathom for high-dimensional input spaces. Furthermore,  Cheng et al. (2017)  recently studied a particular case of the IDN model, where the probabilities that the true labels of samples flip into corrupted ones have upper bounds. They pro- posed a method based on distilled samples, where noisy labels agree with the optimal Bayesian classifier on the clean distribution. However, their method is limited to binary classification and has only been tested on small UCI datasets.  Table 1  summarizes the characteristics of those approaches.

Section Title: CONFIDENCE-SCORED INSTANCE-DEPENDENT NOISE
  CONFIDENCE-SCORED INSTANCE-DEPENDENT NOISE Instead of simplifying assumptions on noise functions, we propose to tackle the IDN model from the source. Namely, we consider that, for each instance, we have access to a measure of confidence in the assigned label. As most of noisy datasets arise from crowdsourcing or automatic annotation, such confidence scores can be easily derived during the dataset construction, often with no extra cost. This allows for a good approximation of noise functions with weaker assumptions.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Before introducing our proposed noise model confidence-scored instance-dependent noise (CSIDN, Figure 1c), we first define what are the confidence scores, and explain why the confidence scores are available in real-world applications.

Section Title: Definition of confidence scores
  Definition of confidence scores For any data point (x,ȳ) sampled from the joint distribution (X,Ȳ ), we define the confidence score r x as follows. Namely, the probability that the assigned label is correct.

Section Title: Availability of confidence scores
  Availability of confidence scores Our rationale is that in tasks involving instance-dependent noise, the confidence information can be easily derived with no extra cost. Firstly, in crowdsourcing platforms, when multiple workers manually annotate datasets, an aggre- gation step is often took to aggregate answers of different workers for each instance (e.g., majority vote). An estimation of r x could then be derived by taking the ratio of votes for the assigned label on the total number of workers. Moreover, since this estimation would of course be less reliable as the number of workers decreases, an alternative could be to directly ask workers for self-reported confidence scores of their responses ( Cosmides & Tooby, 1996 ;  Oyama et al., 2013 ). Secondly, the confidence information can also be available in automatic annotation via a softmax output layer of deep neural networks. This layer outputs an estimation of the probability that each class is the true label: when a model outputs a given class with probability 0.9, we expect the pre- dicted class to be true 9 times out of 10 on average. A model that estimates the accurate probability is well-calibrated. Therefore, in the case of labels generated by a well-calibrated model, the softmax probability of the assigned label can be directly interpreted as a confidence measure that the label is correct. Even though  Guo et al. (2017)  showed that recent deep neural networks are not usually well-calibrated (whereas early shallower networks were, as shown in  Niculescu-Mizil & Caruana (2005) ), model calibration can be achieved in a relatively straightforward way at the validation time, e.g., using temperature scaling (Section 4.2 in  Guo et al. (2017) ).

Section Title: CSIDN: a tractable instance-dependent noise model
  CSIDN: a tractable instance-dependent noise model Recall the intrinsic difficulty of the IDN model: to fully characterize this noise, one would need to estimate K 2 functions T i,j (·) over the input space X . This is of course intractable with a finite noisy dataset. This is why pioneer solutions to the IDN model have been so far limited by very strong assumptions. However, considering additional confidence scores, one can wonder whether such information would make the IDN model tractable with less restrictive assumptions. Hence, we introduce a new and tractable instance-dependent noise model: confidence-scored instance-dependent noise (CSIDN, Figure 1c). In this noise model, the training data takes the form S := {(x i ,ȳ i , r xi ), i = 1, . . . , N }, where {(x i ,ȳ i )} i i.i.d. ∼D and r xi = P (Y =ȳ i |Ȳ =ȳ i , X = x i ) is the previously defined confidence scores in the assigned label of a given instance (Eq. (2)). The confidence infor- mation r x is decisive for robustness to instance-dependent noise, as it provides a proxy for the noise functions T i,j of the training data that are often intractable otherwise.

Section Title: BENCHMARK SOLUTION FOR HANDLING THE CSIDN MODEL
  BENCHMARK SOLUTION FOR HANDLING THE CSIDN MODEL To tackle the CSIDN model, we propose a benchmark solution. Inspired by forward correction ( Patrini et al., 2017 ) for the CCN model, we want to correct each prediction P (ȳ|x) with the noise transition matrix T (x). However, the transition matrix for the CSIDN model is instance-dependent, and has to be estimated for each instance x. We term our solution instance-level forward correction.

Section Title: ESTIMATING INSTANCE-DEPENDENT TRANSITION MATRIX
  ESTIMATING INSTANCE-DEPENDENT TRANSITION MATRIX Using the confidence scores, we will first estimate the diagonal terms (T i,i (·)) K i=1 of the transition matrix, and then estimate the non-diagonal ones.

Section Title: Diagonal terms
  Diagonal terms The diagonal terms of the transition matrix correspond to the probabilities that assigned labels are equal to true labels. However, the confidence scores available are only relevant Under review as a conference paper at ICLR 2020 to the class corresponding to the observed label. Therefore, we need to proceed differently whether the confidence scores are available for the considered class or not. First, note that for each sample (x,ȳ, r x ) ∈ S i := {(x,ȳ, r x ) ∈ S|ȳ = i}, T i,i (x) can be derived for the most part from the confidence scores alone: In practice, we use an iterative procedure to estimate in turn β i (·) and T i,i (·) (see Section 3.2 for details). Then, for the rest of samples (x,ȳ, r x ) ∈ S\S i , r x does not give any direct information on T i,i (·). Hence, we simply set each function T i,i (·) as its empirical mean µ i estimated using samples from S i at the current epoch: where |S| denotes the cardinality of S. Non-diagonal terms. For non-diagonal terms, we have: In Eq. (4), α i,j (x) refers to the probability that an instance x with true label i has an observed label j, once we know that the observed label is different from the true one. Then, a reasonable assumption is that ∀i = j, ∀x ∈ X , α i,j (x) = α i,j : conditionally on the observed label being erroneous, the class transitions are not influenced by the instance x. In other words, the dependence in x of the noise function only impacts the "magnitude" of the noise and not the class transitions. To illustrate this assumption, consider a crowdsourcing task of object recognition with adjacent classes which annotators can only differentiate with details that can be more or less visible depending on the instance. For example, objects from a given class may have distinctive traits, but those can be more or less visible in the pictures. When those traits are present, the annotators can confidently predict the right class. Otherwise, they will make errors towards adjacent classes. In this case, the probability that the assigned label is wrong highly depends the instance (with distinctive traits being visible or not). Nonetheless, conditionally on the instance being corrupted, i.e., because those traits were not visible enough on the image, the transition probabilities to the adjacent classes are not influenced by the instance itself. With the previous assumption, we obtain ∀i = j, ∀x ∈ X , T i,j (x) = α i,j (1 − T i,i (x)) with α i,j ∈ [0, 1]. This allows us to estimate the K(K −1) constants (α i,j ) i =j once, and derive the non-diagonal noise functions of T (x) directly from our estimates of the diagonal noise functions (Eq. (5)).

Section Title: OVERALL ALGORITHM: INSTANCE-LEVEL FORWARD CORRECTION
  OVERALL ALGORITHM: INSTANCE-LEVEL FORWARD CORRECTION Estimating T i,i and β i . To train a classifier h with the instance-level forward correction method, we need to estimate both T i,i (x) and β i (x) = P (Ȳ =i|X=x) P (Y =i|X=x) from Eq. (3), for all x ∈ S i . Firstly, the noisy posterior P (Ȳ = i|X = x) can be easily estimated by training a naive classifier on the noisy dataset. Secondly, the true posterior P (Y = i|X = x) can be estimated using the output of the classifier h(x) =P (Y = i|X = x) at the previous epoch.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Therefore, we iteratively updateβ andT with the following steps: 1) ∀x ∈ X , initializeβ i (x) = 1 and train a naive classifier h noisy on the noisy dataD to obtain h noisy (x) =P (Ȳ |X = x). 2) ∀i ∈ [1, K], for each sample (x,ȳ, r x ) ∈ S i , computeT i,i (x) = r xβi (x) and train classifier h for one epoch. 3) ∀i ∈ [1, K], for each sample (x,ȳ, r x ) ∈ S i , updateβ i (x) = hnoisy(x)i h(x)i . Then, we repeat steps 2) and 3) through training. In this way, for every epoch, each function T i,i (·) is estimated for the samples from S i . Lastly, for the rest of samples with noisy label j = i, T i,i (·) is estimated at each epoch using Eq. (4): Computing α i,j . The computation of α i,j boils down to approximating non-diagonal terms of the transition matrix in the CCN model. As ∀i = j, ∀x ∈ X , T i,j (x) = α i,j (1 − T i,i (x)), we have: A simple and reliable way is to use anchor points, i.e., points for which we can know the true class almost surely. These points may be directly available when some training data has been curated, or they can be identified either theoretically as in  Liu & Tao (2015)  or heuristically as in  Patrini et al. (2017) . Having S * i := {(x,ȳ, r x ) ∈ S|P (Y = i|X = x) ≈ 1} a set of class i anchor points, we simply need compute: Two noisy posteriors can be estimated using the same classifier h noisy trained on the noisy distribu- tion h noisy (x) =P (Ȳ |X = x) aforementioned. Thus, α i,j can be estimated as follows: Summary of the training procedure. Given samples S and K sets of anchor points (S * i ) K i=1 , we want to train a classifier h(·) equipped with a loss l. For any loss l : y,ŷ → l(y,ŷ), we define the T -corrected loss as l T : y,ŷ → l(y, Tŷ). The overall procedure is in Algorithm 1 (Appendix C).

Section Title: EXPERIMENTS
  EXPERIMENTS We compare our instance-level forward correction (ILFC) method with four representative baselines: forward correction (FC) ( Patrini et al., 2017 ), mean absolute error (MAE) ( Ghosh et al., 2017 ), L q - norm (LQ) ( Zhang & Sabuncu, 2018 ) and co-teaching (CT) ( Han et al., 2018 ). Details are shown in Appendix D. Note that the pioneer IDN methods cannot work for multi-class cases.

Section Title: SYNTHETIC DATASET
  SYNTHETIC DATASET

Section Title: Generation process
  Generation process We generate a synthetic dataset (Appendix E) consisting in three classes of concentric circles (Figure 6a). We then apply the following instance-dependent noise to each label: P (Ȳ = Y |X = x) = ρ w·x w x + 1 /2 with w = (0, 1) and ρ controlling the mean noise rate. If corrupted, each label is flipped to another class uniformly. Empirical results.  Figure 3  shows the test accuracy of different methods on the synthetic dataset. Each experiment is repeated 5 times and we plot the confidence intervals of each curve. On low-level noise, all methods show good performances (Figure 3a). On mild-level noise, both Co-teaching and ILFC show good performances and outperform other baselines (Figure 3b). On high-level noise, the performance of all the baselines collapse, whereas ILFC constantly maintains good performances (Figures 3c and 3d). More experiments are shown in Appendix B and F.

Section Title: REAL-WORLD DATASET
  REAL-WORLD DATASET

Section Title: Generation process
  Generation process In order to corrupt labels from clean datasets such as SVHN and CIFAR10, we adopt the following procedure: (1) train a classifier h : x → σ(g(x)) on a small subset of the clean dataset; (2) using a small validation set, calibrate the classifier by selecting the temperature t that maximizes the expected calibration error as in  Guo et al. (2017) ; (3) for each instance x, set:ȳ = argmax i h t (x) i and r x = max i h t (x) i . With this process, we attempt to emulate the construction of a real-world dataset (Appendix G). Empirical results. Figures 4a and 4b show the test accuracy on SVHN with 25% and 45% instance-dependent noise, respectively. We can clearly observe that, on both low-level and high- level noise, ILFC shows good performances with a fast convergence rate, and outperforms other baselines. Figures 4c and 4d show the test accuracy on CIFAR10 with 25% and 45% instance- dependent noise, respectively. On low-level noise, all methods show good performances. However, on high-level noise, ILFC shows a fast convergence rate and outperforms other baselines.

Section Title: CONCLUSION
  CONCLUSION In this paper, we give an overview of label-noise learning from class-conditional noise (easier) to instance-dependent noise (harder). We explain why existing approaches cannot handle instance- dependent noise well, and try to address this challenge via confidence scores. Thus, we formally propose the confidence-scored instance-dependent noise (CSIDN) model. To tackle the CSIDN model, we design a practical algorithm termed instance-level forward correction (ILFC). Our ILFC method robustly outperforms existing methods, especially in the case of high-level noise. In future works, we would like to extend label correction and sample selection approaches with the confidence scores from the CSIDN model. Under review as a conference paper at ICLR 2020

```
