Title:
```
Under review as a conference paper at ICLR 2020 MULTIAGENT REINFORCEMENT LEARNING IN GAMES WITH AN ITERATED DOMINANCE SOLUTION
```
Abstract:
```
Multiagent reinforcement learning (MARL) attempts to optimize policies of intelli- gent agents interacting in the same environment. However, it may fail to converge to a Nash equilibrium in some games. We study independent MARL under the more demanding solution concept of iterated elimination of strictly dominated strategies. In dominance solvable games, if players iteratively eliminate strictly dominated strategies until no further strategies can be eliminated, we obtain a single strategy profile. We show that convergence to the iterated dominance solution is guaranteed for several reinforcement learning algorithms (for multiple independent learners). We illustrate an application of our results by studying mechanism design for principal-agent problems, where a principal wishes to incentivize agents to exert costly effort in a joint project when it can only observe whether the project succeeded, but not whether agents actually exerted effort. We show that MARL converges to the desired outcome if the rewards are designed so that exerting effort is the iterated dominance solution, but fails if it is merely a Nash equilibrium.
```

Figures/Tables Captions:
```
Figure 3: Iterated Dominance scheme
Figure 4: Individual agent effort (Iterated Dominance)
Figure 5: Rewards (Iterated Dominance)
Table 1: Reward schemes in our joint project principal-agent environment. Consider agent i who is promised a reward r i and who knows that exactly m of the other agents would exert effort (so the remaining n − m − 1 will not exert effort). If i exerts effort, the project succeeds with probability h m+1 · l n−m−1 , so their expected reward is h m+1 · l n−m−1 · r i − c. If i does not exert effort, the project succeeds with probability h m · l n−m , and their expected reward is h m · l n−m · r i . Agent i would thus exert effort if: h m+1 · l n−m−1 · r i − c > h m · l n−m · r i , or equivalently if r i > c/(h m+1 · l n−m−1 − h m · l n−m ). Observe that the minimal reward to induce i to exert effort decreases in m, and when i assumes no other agents would exert effort (m = 0), the required reward r i is r i > c/(h · l n−1 − l n ). Thus setting r i to r i = c/(h · l n−1 − l n ) + for all agents makes exerting effort a dominant strategy for all agents. In contrast, when i assumes all other agents exert effort (m = n − 1), the required reward r i is r i > c/(h n − h n−1 · l), so setting r i = c/(h n − h n−1 · l) + for all agents makes exerting effort a Nash equilibrium.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Intelligent agents sharing a common environment are affected by the actions taken by their peers. Using reinforcement learning (RL) to derive agent policies becomes challenging since the environment becomes non-stationary for each agent when its peers adapt their behaviour through their learning process. One simple form of multiagent reinforcement learning (MARL) is independent learning, where each agent simply treats its experience as part of the non-stationary environment. Unfortunately, independent MARL fails to converge to a Nash equilibrium in many settings (Bowling, 2000;  Shoham et al., 2003 ). To guarantee convergence to a Nash equilibrium, one must either examine restricted classes of games such as fully cooperative games ( Claus & Boutilier, 1998 ;  Bu et al., 2008 ;  Panait et al., 2006 ;  Matignon et al., 2007 ), or devise specialized algorithms that guarantee convergence ( Hu & Wellman, 2003 ;  Wang & Sandholm, 2003 ). We investigate independent MARL in games that are solvable by iterated elimination of dominated strategies (Moulin, 1979). We say that an action by an agent is dominated by another if the first action offers the agent a strictly lower reward than taking the second action, no matter which actions are taken by the other agents. In iterated elimination of dominated strategy we iteratively examine the actions of every agent, and remove strictly dominated actions, until no further actions can be removed. A game is dominance solvable if only one action profile survives the process of iteratively eliminating strictly dominated strategies. We examine implications of the relation between iterated dominance and RL through applications in mechanism design, a field in economics that studies how to set incentives for rational agents, so as to achieve desired objectives. One key line of work in mechanism design deals with principal-agent problems ( Holmstrom et al., 1979 ) holmstrom1982moral,grossman1992analysis,laffont2009theory, relating to a principal in charge of a joint project, whose success depends on the exertion of effort by multiple agents; the principal wishes to incentivize agents to maximally exert costly effort, but cannot observe how much effort any individual agent exerted.

Section Title: Our contribution
  Our contribution We show that for dominance solvable games, multiagent reinforcement learners converge to the iterated dominance solution for simple and reasonable algorithms; in games with two actions per agent, REINFORCE ( Williams, 1992 ) converges to the solution, and in games with more than two actions Monte-Carlo Policy Improvement ( Sutton & Barto, 2018 ) converges when using importance weighted action value estimators. In contrast to a Nash equilibrium, which exists in Under review as a conference paper at ICLR 2020 any game with a finite action set, not every game is dominance solvable. However, in mechanism design settings we engineer the game in order to achieve certain desired agent behaviors, and can thus construct games that are dominance solvable. We examine mechanism design to illustrate the applications of our work, empirically investigating a principal-agent problem. We show that an incentive scheme based on iterated dominance guarantees that independent reinforcement learners converge to the optimal solution for the principal, whereas under a scheme where exerting effort is only a Nash equilibrium, independent RL typically does not converge to an optimal solution.

Section Title: PRELIMINARIES
  PRELIMINARIES An n-player normal form game is given by a set of players I = {a 1 , . . . , a n }, and for each player a i a (finite) set of pure strategies S i , and a utility function u i : S 1 × S 2 × . . . × S n → R, where u i (s 1 , . . . , s n ) denotes a i 's utility when each player a j plays strategy s j . For brevity, we denote the set of full strategy profiles S = S 1 × S 2 × . . . × S n , and denote items in S as s ∈ S (s = (s 1 , . . . , s n ), where s i ∈ S i ). We also denote S −i = S 1 × . . . × S i−1 × S i+1 × . . . × S n , and given a partial strategy profile s −i = (s 1 , . . . , s i−1 , s i+1 , . . . , s n ) ∈ S −i we denote (s −i , s i ) = (s 1 , . . . , s i−1 , s i , s i+1 , . . . s n ) ∈ S. Given a normal form game G, we say agent a i 's strategy s x ∈ S i strongly dominates s y ∈ S i if a i 's utility is higher when using s x than when using s y , no matter what strategies the other agents use, i.e.: agent a i 's strategy s x ∈ S i strictly dominates s y ∈ S i if for any partial strategy profile s −i ∈ S −i we have u i ((s −i , s x )) > u i ((s −i , s y )). We say player a i 's strategy s x is a i 's dominant strategy if it dominates all other strategies s i ∈ S i . Game-theoretic solutions specify which outcomes are reasonable, under various assumptions of rationality and knowledge. We focus on a prominent procedure called iterated elimination of dominated strategies, and identify conditions under which learning agents converge to this solution. In cases where every agent has a dominant strategy, it seems reasonable to predict that each player would play their dominant strategy. Given a game G, we say a strategy profile s = (s 1 , . . . , s n ) ∈ S is a dominant strategy equilibrium if for any agent a i , strategy s i is a dominant strategy for a i . However, in many games a player may not have a dominant strategy. A less demanding concept is that of a Nash equilibrium, which merely seeks a strategy profile where no player can improve their utility by unilaterally deviating. Given a game G a strategy profile s = (s 1 , . . . , s n ) is a Nash equilibrium if for any player a i and any alternative strategy s x ∈ S i we have u i (s) ≥ u i (s −i , s x ) (i.e. u i (s 1 , . . . , s i−1 , s i , s i+1 , . . . , s n ) ≥ u i (s 1 , . . . , s i−1 , s x , s i+1 , . . . , s n )). A mixed Nash equilibrium exists in games with finite strategy sets ( Nash et al., 1950 ;  Morgenstern & Von Neumann, 1953 ), but many games have multiple Nash equilibria, resulting in an equilibrium selection problem. Another prominent concept is that of iterated dominance ( Osborne & Rubinstein, 1994 ), where we iteratively remove dominated strategies, with eliminated strategies no longer having effect on future dominance relations. Given a game G with players I = {a 1 , . . . , a n }, strategy sets S 1 , . . . , S n and utilities u 1 , . . . , u n , a (strict) domination elimination step d is a triplet d = (i ∈ I, s l , ∈ S i , s h ∈ S i ), where the strategy s h strictly dominates s l for player i. The elimination step d indicates that s l is eliminated from G as it is dominated by s h . Following the elimination step we get the game G d , which is identical to G except the strategy s l is removed from strategy set S i of player i (i.e. the strategy set for i in G d is S i \ {s l }), and the range of the utility function is restricted to this reduced strategy set). A dominance elimination sequence is a sequence (G, d 1 , G d1 , d 2 , G d2 , . . . , G d k−1 , d k−1 , G d k ) where G is an initial game and each d i is an elimination step from the game G i resulting in the game G i+1 . If no more dominance elimination steps can be taken from G k , we say that the strategy profiles in G k survive iterated elimination of (strictly) dominated strategies. Further, if no more dominance elimination steps can be taken from G k and there is only one strategy remaining for each player, the game is called (strict) dominance-solvable. Iteratively eliminating dominated strategies is known to reserve Nash equilibria, and further when removing only strictly dominated strategies the procedure is "path-independent", yielding the same final strategy sets regardless of the order in which the dominated strategies were removed ( Osborne & Rubinstein, 1994 ). Our discussion focuses on normal-form game, but out results extend to temporally extended settings (games with multiple timesteps). We consider MARL in Markov games ( Shapley, 1953 ;  Littman, 1994 ), where in each state agents take actions (possibly given only partial observations of the true world state), with each agent obtaining an individual reward. We consider independent MARL, where agents each learn a behavior policy through their individual experiences interacting with one another in the environment. We discuss MARL in Markov games in Appendix 6.3, along experimental results.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 One motivation for our work comes from mechanism design, a field of economics investigating how incentives should be set up so as to achieve desired outcomes in strategic settings where multiple agents interact. This was studied in settings ranging from government policy and social choice to auctions ( Börgers, 2015 ;  Nisan & Ronen, 2001 ;  Krishna, 2009 ;  Abdulkadiroglu & Sönmez, 2003 ;  Parkes & Singh, 2004 ). We focus on principal-agent problems, where agents take actions on behalf of another entity called the principal, but agents' interests may not align with the principal's (Holm- strom et al., 1979;  Grossman & Hart, 1992 ;  Laffont & Martimort, 2009 ). A key example is efforts in a joint project consisting of multiple tasks, each handled by an agent ( Holmstrom et al., 1979 ;  Holmstrom, 1982 ;  Winter, 2004 ;  Babaioff et al., 2006 ). We discuss this model in Section 4.

Section Title: MULTI-AGENT RL AND DOMINANCE-SOLVABLE GAMES
  MULTI-AGENT RL AND DOMINANCE-SOLVABLE GAMES We consider training multiple independent reinforcement learners in a game G which is strict dominance-solvable. Each agent i takes the role of player i in the game G and its possible actions are the strategies in S i . Given the actions (strategy choices) of all agents we obtain a full strategy profile s ∈ S 1 × . . . × S n , and the reward each agent i obtains is the respective payoff u i (s) in the game. As we consider training general RL agents in a domain that is a normal form game, we intermix game theoretic terminology (strategies and payoffs) and RL terminology (actions and rewards).

Section Title: LEARNING DYNAMICS IN NORMAL FORM GAMES
  LEARNING DYNAMICS IN NORMAL FORM GAMES Given the strategies s −i ∈ S −i of all players except i, agent i faces a single run of a game denoted as b (reflecting the setting induced by the choices s −i ∈ S −i of other players). The possible actions for agent i are S i , and any action a ∈ S i results in a reward r b a = u i (s −i , a) as given by player i's payoff in the game. However, agent i simply selects an action and receives its obtained reward; it plays without ever gaining knowledge of which strategies were used by the other agents.

Section Title: REINFORCE AGENTS IN NORMAL FORM GAMES
  REINFORCE AGENTS IN NORMAL FORM GAMES We consider a REINFORCE ( Williams, 1992 ) agent which maintains a score (logit) per each ac- tion, x = x 1 , . . . , x mi , and applies a softmax operation to transform these scores to the respective probabilities of choosing each action: p x (a) = exp(xa) m i j=1 exp(xj ) . Each agent starts with initial logits for x 1 , . . . , x n . Fixing the choice b of the other agents (relating to their chosen actions in S −i ), denote by J b the expected reward of the target agent, so J b = a p x (a)r b a . The exact REINFORCE update is: x n+1 = x n + α∇ x J b = a r b a ∇ x p x (a). As agents only take a single action each episode, this is typically estimated by substituting ∇ x J b = a r b a ∇ x p x (a) = a r b a p x (a)∇ x log p x (a) = E a∼px r b a ∇ x log p x (a), then selecting a single action a sampled from the distribution p x (parame- terized by x). Given the softmax rule above for setting the action probability distribution p x , and denoting Kronecker delta as δ ij we have: ∂px(i) ∂xj = p x (i)(δ ij − p j ). We examine MARL dynamics in dominance-solvable games, identifying conditions under which learning converges on the (strict) iterated dominance solution. Given the dominance elimination sequence (G, d 1 , G d1 , d 2 , G d2 , . . . , G d k−1 , d k−1 , G d k ) (where the d i s are elimination steps), one may hope the learning dynamics would "follow" the strategy elimination steps in the sequence, first lowering the probability on the dominated strategy of d 1 to (almost) zero, then lowering the probability on dominated strategy of d 2 and so on, until we remain with agents only playing the strategies of G d k . We show that this is indeed the case for MARL using REINFORCE when each agent has at most two strategies. For settings with an arbitrary number of actions per agents, we provide a similar proof for a variant of Monte-Carlo policy iteration given in Section 2.1.2.

Section Title: IMPORTANCE WEIGHTED MONTE-CARLO AGENTS IN NORMAL FORM GAMES
  IMPORTANCE WEIGHTED MONTE-CARLO AGENTS IN NORMAL FORM GAMES Monte-Carlo policy iteration (MCPI) is one of the simplest methods for control. It maintains an estimate of the expected reward for each strategy, updating the estimate after observing the outcome of every run of the game, and follows an -greedy policy based on these estimates to guarantee exploration. To achieve convergence in dominance-solvable games, we use the specific estimator of Algorithm 1. At every step t, it maintains a score x i for every possible action i ∈ S i . The scores are Under review as a conference paper at ICLR 2020 softmaxed to derive a policy distribution P over actions. We denote by P i the probability of choosing action i ∈ S i . Every step, the agent selects an action s i from the current policy P , and depending on the actions b ∈ S −i taken by other agents, it receives a reward r b si . We denote the probability of selecting action i under the policy P at time t as P t,i and the action taken by the agent at time t as A t . As an estimator for the reward when selecting action i, we useR t,i = 1{At=i}r b s i Pt,i (in contrast to standard MCPI whose estimator is the average of past rewards when selecting action i). The score x i is increased by the estimatorR t,i , and the scores x are softmaxed to obtain an improved policy. As with MCPI, to maintain exploration we use an -greedy version of this improved policy ( in addition to the exploration due to the softmax). Algorithm 1 is thus a variant of MCPI with an importance weighted reward estimator, which we study in the context of MARL in dominance-solvable games.

Section Title: CONVERGENCE OF RL TO AN ITERATED DOMINANCE SOLUTION
  CONVERGENCE OF RL TO AN ITERATED DOMINANCE SOLUTION We show that MARL in dominance-solvable games converges to the iterated dominance solution using the above MCPI (Algorithm 1), or under REINFORCE in the two action case. One may consider two training modes. In the serial mode, we cycle through the agents, each time performing RL updates for the policy of the current agent while holding the policies of other agents fixed for many iterations (enough for the policy to converge and eliminating a strategy). As we fix the strategies of others when training each agent, the process "follows" the domination elimination sequence. Another training mode is a parallel mode, where we update the policies of all agents following the experience gained in each episode ( Littman, 1994 ). Our convergence results hold for both modes, but handling the parallel mode requires the more intricate conditional expectation analysis of Theorem 3.4.

Section Title: BINARY ACTION CASE
  BINARY ACTION CASE Consider a dominance-solvable game and MARL using REINFORCE. As discussed in Section 2, given the strategic choices of other agents b = s −i ∈ S −i , agent i faces a run of the game, with reward r b a = u i (s −i , a) depending on i's action a and the strategies b of the other agents. Each agent i performs the REINFORCE updates of Section 2.1.1 based only on its action a and obtained reward r b a , without ever becoming aware of the strategies b = s −i taken by others. A dominance elimination step d = (i, s l , s h ) includes a dominated strategy s l and dominating strategy s h for agent i, where s h strictly dominates s l , so no matter what strategies s −i other players choose, player i obtains a strictly greater utility from s h than s l ; Thus for any s −i , u i (s h , s −i ) > u i (s l , s −i ), or in other words, for any setting b ∈ S −i that agent i may be playing, action s h has a higher payoff than s l , so r b s h > r b s h . Lemma 3.1. Let B be a set of settings with two actions s l and s h , where for any setting b ∈ B the respective rewards for s h is strictly higher than for s l , so r b s

Section Title: Proof
  Proof Thus, regardless of the setting b used, the update increases the probability of the dominating action, at least as much as the update for the minimal gap setting does. Repeatedly applying the update for the minimal gap setting eventually places negligible probability on the dominated action, so this is also the case for any update sequence (of any of the settings). Theorem 3.2. Let G be a dominance-solvable game which has a single strategy profile s ∈ S 1 × . . . × S n surviving iterated elimination of strictly dominated strategies, and where every player has at most 2 strategies, and consider agents trained independently using the REINFORCE update. Then the agents converge to the iterated elimination solution s. Proof. Consider an iterated elimination sequence (G, d 1 , G d1 , d 2 , G d2 , . . . , G d k−1 , d k−1 , G d k ). The first elimination d 1 = (i, s 1 l , s 1 h ) relates to agent i who faces different settings due to other agents playing different strategies, but whose payoff under some s h ∈ S i strictly dominates s l ∈ S i . Lemma 3.1 shows it eventually places negligible mass 1 on the dominated action (for as low 1 as desired). We examine the second elimination step d 2 = (j, s 2 l , s 2 h ). While in the original game j has faced some settings b ∈ S −j where s 2 l ∈ S j got a higher reward than s 2 h ∈ S j , these settings are encountered less and less frequently. Consider a target probability 2 for agent j to select the dominated strategy s 2 l . By Lemma 3.1, there is a number k of steps where if we train agent j for k steps only on settings where s 2 l is dominated by s 2 h , j places a mass of at most 2 on s 2 l . By the union bound, the probability of encountering a "wrong" setting (with s 2 l not dominated by s 2 h ) is at most k 1 ; as 1 is as small as desired, the probability of agent j not reaching the target (a mass of at most 2 on s 2 l ) is also as small as desired. Applying this argument over the elimination sequence, we conclude that agents converge on the single strategy profile s surviving iterated elimination. Our proof of Theorem 3.1 iteratively applies Lemma 3.1, which holds when players have at most two strategies. Section 3.2 provides similar results to Theorem 3.2 for more than two actions, but under the MCPI variant of Algorithm 1. Section 4 backs the theory up through experiments.

Section Title: CONVERGENCE IN DOMINANCE-SOLVABLE GAMES FOR IMPORTANCE WEIGHTED MC
  CONVERGENCE IN DOMINANCE-SOLVABLE GAMES FOR IMPORTANCE WEIGHTED MC We consider agents using Algorithm 1 (IW-MCPI), and show that when an action i dominates action j, IW-MCPI eventually stops choosing the dominated action. We assume rewards are normalized to the range [0, 1]. Denote the IW-MCPI estimator for the reward of action i in time t asR t,i , whereR t,i = 1{At=i}r b A t Pt,i (r b At depends on the agent's action A t , and the actions b taken by others). The reward estimatorsR t = (R t,1 ,R t,2 , . . . ,R t,k ) are then converted to scores per action where S t,i = t j=1R j,i , and the scores are converted to a distribution Q t = (Q t,1 , Q t,2 , . . . , Q t,k ) by taking the softmax: Q t = Sof tmax(S t ). Q encodes a "greedy" policy, which is then converted to an t -greedy policy P t : P t,i = t k + (1 − )Q t (i). We anneal the value of t towards zero over time. Note that over time t, the scores S t,i are a sequence of random variables S 1,i , S 2,i , . . . , S τ,i where each S t,i is dependent on the earlier variables S 1,i , . . . , S t−1,i . We denote the conditional expectation of S t,i given the previous variables as: E t (S t,i ) ∆ = E[S t,i |S 1,i , S 2,i , . . . , S t−1,i ]. In other words, E t denotes the conditional expectation with respect to the observations by player i at the start of round t. Freedman's inequal- ity ( Freedman et al., 1975 ) states that for a sequence of random variables X 1 , . . . , X t (each depending on the previous ones in the sequence), with high probability of at least 1 − δ the following holds: Under review as a conference paper at ICLR 2020 | n t=1 (X t − E t [X t ])| ≤ c n t=1 V ar t [X t ] log 1 δ . Applying Freedman's inequality we obtain: Then by a union bound it follows that with probability at least 1 − δ the following holds for all n: The assumptions that n is nonincreasing and lim n→∞ log(n) n 2 n t=1 1 t = 0 imply that lim n→∞ log(n) n n ≤ lim n→∞ log(n) n 2 2n t=1 1 n = 0. Combining this with (1) shows that with probability one we have: lim n→∞ 1 n | n t=1 X t | = 0 Theorem 3.4. Let G be a dominance-solvable game which has a single strategy profile s ∈ S 1 × . . . × S n surviving iterated elimination of strictly dominated strategies. Consider agents trained independently using Algorithm 1. Provided that lim n→∞ log(n) n 2 n t=1 1 t = 0, the agents converge to the iterated elimination solution s. Proof. By setting t = 1/t p , for any p ∈ (0, 1), the assumptions on t ensure that we can apply Theorem 3.3 (for all i, with probability 1). We show players' strategies converges to the iterated dominant profile. Suppose there exists a round τ 1 after which action i is dominated by action j, which means there exists a g > 0 such that for all t ≥ τ 1 it holds thatR t,i ≤R t,j − g. Then: Taking the limit as n tends to infinity shows there exists a time τ 2 such that for all n ≥ τ 2 we have n t=1R t,j −R t,i ≥ ng/2. Therefore for any n ≥ τ 2 we have: Hence, P n,i = (1 − n )Q n,i + n /k ≤ n + exp (−ng/2). Since lim n→∞ n = 0 by assumption, it follows that lim n→∞ P n,i = 0 almost surely. The previous part shows that if action i is dominated after some round τ 1 , then for any > 0 there exists a round τ 3 such that P n,i ≤ . Choosing sufficiently small and iterating the argument completes the proof in the same way as Theorem 3.2.

Section Title: EMPIRICAL ANALYSIS OF PRINCIPAL-AGENT GAMES
  EMPIRICAL ANALYSIS OF PRINCIPAL-AGENT GAMES Our environment is a simulation of a prominent problem studied by economists, called the principal agent problem ( Holmstrom et al., 1979 ;  Holmstrom, 1982 ;  Winter, 2004 ;  Babaioff et al., 2006 ), through which we show how our results can be used to design mechanisms for reinforcement learners. It considers a project which requires completing multiple tasks, each handled by an agent. Normally each task succeeds with a low (but non-zero) probability, which increases when the handling agent exerts additional effort. The project succeeds only if all its tasks succeed, in which case the principal stands to gain a large monetary amount (Appendix 6.2.1 considers a model where some task failures are allowed). The principal thus wants to make sure as many agents as possible exert effort. A dilemma arises when exerting effort is costly for the agents (i.e. incurs an immediate negative reward); A natural way to compensate for that is for the principal to offer agents a reward based on the effort they exerted. However, in principal-agent settings, the principal only knows whether the entire project succeeded, and is incapable of observing whether any individual agent exerted Under review as a conference paper at ICLR 2020 effort (note tasks succeed with a non-zero probability even without effort). Thus, it can only promise each agent i a reward r i offered only when the entire project is successful. We refer to the promised rewards r = (r 1 , . . . , r n ) as a reward scheme. Each such reward scheme induces a game played by the agents, and the principal gets to design the game, by selecting the reward scheme. On the one hand, the higher the rewards, the more incentive agents have to exert effort. On the other hand, the rewards are costly to the principal, so they want to minimize them. One possible reward scheme is a Nash equilibrium implementation, where the principal sets rewards so that the profile where all agents exert effort is a Nash equilibrium ( Babaioff et al., 2006 ). A Nash scheme may seem tempting to the principal as it offers low rewards. However, independent MARL may not converge to a Nash equilibrium ( Lanctot et al., 2017 ), and there may be multiple equilibria, so agents may converge on an undesired equilibrium. A scheme at the other end of the scale is a dominant strategy scheme, where the principal promises each agent a reward high enough to make exerting effort a dominant strategy, so each agent would rather exert effort no matter what others do. Under this scheme MARL converges on exerting effort, but it is expensive to the principal. We show that an iterated dominance scheme is a good middle ground, guaranteeing convergence to the desired equilibrium at a far cheaper cost. Environment parameters: we simulate a project which depends on five tasks T = {t i } n=5 i=1 , a cost c = 10 for exerting effort, and where any task t i succeeds with probability h = 0.8 if agent i exerts effort and with probability l = 0.1 if they do not. Every episode, each agent i takes one of two actions, either exert effort or not. We sample the success of each task t i as a Bernoulli variable, with a success probability of either h or l, depending on agent i's action. The entire project is successful only if all tasks {t i } n i=1 are successful. The rewards r = (r 1 , . . . , r n ) are the parameters of our environment; an agent who exerts effort incurs a negative reward −c, and if the project is successful they also receive a reward r i .  Table 1  shows the possible reward schemes for these settings. We briefly discuss how these were computed, with full details in the appendix. As  Table 1  shows, even with only five agents, there are huge differences in the principal's expenditure when the project succeeds. We simulate the environment with all three reward schemes, setting = 160 so the reward is just above the minimum threshold ( = 160 is negligible compared to the high reward 142, 857), and use both REINFORCE learners, and Advantage Actor-Critic agents ( Mnih et al., 2016 ) agents. Our results show that under the cheap Nash scheme MARL does not converge to exerting effort (rather, all agents end up not exerting effort). However, MARL does converge on exerting effort for the iterated dominance scheme, which is far cheaper than the dominant scheme. Figure 1 shows the proportion of times where agents select the high effort action over training time under the Nash scheme, indicating that agents do not converge on the Nash equilibrium of all exerting effort. Figure 2 shows the results under the dominant strategy scheme, showing all agents converge on exerting effort.  Figure 3  shows the results for the iterated dominance reward scheme, for different Under review as a conference paper at ICLR 2020 0 50 100 150 200 250 Training iteration x100 0.0 0.2 0.4 values of . It shows that agents indeed learn to all exert effort, at a much lower cost to the principal than in the dominant strategy scheme (roughly 20% of the cost under the dominant strategy scheme).  Figure 4  shows the effort level of individual agents over training in the iterated dominance scheme (measured by the proportion of times where each agent selected the high effort action). It shows that first the highest reward agent learns to exert effort, then the next agent and so on. Interestingly, given the initial effort levels of the other agents, the last agent (with smallest promised reward) initially learns not to exert effort. Only after the other agents exert more effort, this agents learns to exert more effort.  Figure 5  shows the mean agent reward over time in the iterated dominance scheme (similarly to  Figure 3 ). It shows that as agents exert more effort, they improve their overall reward (reaching the reward under the scheme when all agents exert effort). The above figures are for REINFORCE agents, but our experiments with Advantage Actor Critic agents ( Mnih et al., 2016 ) yield very similar results. These results highlight the importance of the iterated dominance concept for multiagent systems comprised of independent reinforcement learners: such systems may not converge to the desired Nash equilibrium, but do converge to an iterated dominance solution. Thus, when designing mechanisms for multiple reinforcement learners, one should strive for an implementation that is based on the stronger iterated dominance solution, rather than on the less demanding Nash equilibrium. We note that our theoretical results hold for REINFORCE only when agents have two actions, however, in the appendix we consider a simulation with three actions (effort levels), and we show that empirically agents do converge to the desired outcome in this case as well.

Section Title: CONCLUSION
  CONCLUSION We have provided convergence results for MARL in iterated dominance solvable games, and discussed their implications to mechanism design for RL agents. Our results show that reward schemes based on iterated dominance are desirable, as MARL with reasonable learning methods is guaranteed to converge to such a solution, in contrast to schemes based on a Nash equilibrium. Several directions are open for future research. First, while we only proved convergence for specific RL algorithms or under some restrictions on the underlying game, we conjecture convergence occurs in wider settings. Could our results be extended to cover other RL algorithms or fewer restrictions on the game? In particular, can one prove convergence for REINFORCE with three or more actions? Second, we have focused on strict dominance - what can one say about weak iterated dominance? Finally, could we theoretically bound the required time to convergence to an iterated dominance solution? Under review as a conference paper at ICLR 2020

```
