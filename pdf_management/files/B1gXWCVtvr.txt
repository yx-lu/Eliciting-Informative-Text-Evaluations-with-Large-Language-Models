Title:
```
Under review as a conference paper at ICLR 2020 ADAPTING BEHAVIOUR FOR LEARNING PROGRESS
```
Abstract:
```
Determining what experience to generate to best facilitate learning (i.e. explo- ration) is one of the distinguishing features and open challenges in reinforcement learning. The advent of distributed agents that interact with parallel instances of the environment has enabled larger scale and greater flexibility, but has not re- moved the need to tune or tailor exploration to the task, because the ideal data for the learning algorithm necessarily depends on its process of learning. We propose to dynamically adapt the data generation by using a non-stationary multi-armed bandit to optimize a proxy of the learning progress. The data distribution is con- trolled via modulating multiple parameters of the policy (such as stochasticity, consistency or optimism) without significant overhead. The adaptation speed of the bandit can be increased by exploiting the factored modulation structure. We demonstrate on a suite of Atari 2600 games how this unified approach produces results comparable to per-task tuning at a fraction of the cost.
```

Figures/Tables Captions:
```
Figure 1: The overall architecture is based on a distributed deep RL agent (black). In addition, modulations z are sampled once per episode and modulate the behaviour policy. A bandit (purple) adapts the distribution over z based on a fitness f t (z) that approximates learning progress (cyan).
Figure 2: LavaWorld experiments, with 31 distinct modulations z. Left: If the candidate behaviours are stationary (not affected by learning), the bandit nearly recovers the performance of the best fixed choice. The factored variant (blue) is more effective than a flat discretization (green). Using the true LP (z) instead of a less informative proxy (red) has a distinct effect: in the stationary sparse-reward case this makes the bandit equivalent to uniform sampling. Right: in a non-stationary setting where the modulated Q-values are learned over time, the dynamics are more complex, and even the bandit with ideal fitness again comes close to the best fixed choice, while uniform or proxy-based variants struggle more (see Appendix B).
Figure 3: The best fixed choices of a modulation z vary per game, and this result holds for multiple classes of modulations. Left: Repeat probabilities ρ. Right: Optimism ω.
Figure 4: No fixed arm always wins. Shown are normalized relative ranks (see text) of different fixed arms (and curated bandit), where ranking is done separately per modulation class (subplot). The score of 1 would be achieved by an arm that performs best across all seeds (0 if it is always the worst). Dashed lines are ranks of an oracle that could pick the best fixed arm in hindsight per game. The gap between the dashed line and the maximum 1 indicates the effect of inter-seed variability, and the gap between the dashed line and the highest bar indicates that different games require different settings. In all four modulation classes, the bandit performance is comparable to one of the best fixed choices.
Figure 5: Tuning exploration by early performance is non-trivial. The bar-plot shows the perfor- mance drop when choosing a fixed z based on initial performance (first 10% of the run) as compared to the best performance in hindsight (scores are normalized between best and worst final outcome, see Appendix D). This works well for some games but not others, and better for some modulation classes than others (colour-coded as in Figure 6), but overall its not a reliable method.
Figure 6: Tuning exploration by choosing a set of modulations is non-trivial. This figure contrasts four settings: the effect of naive ('extended') versus curated z-sets, crossed with uniformly sampling or using the bandit, and for each of the four settings results are given for multiple classes of modu- lations (colour-coded). The length of the bars are normalized relative ranks (higher is better). Note how uniform does well when the set is curated, but performance drops on extended z-sets (except for ω-modulations, which are never catastrophic); however, the bandit recovers a similar performance to the curated set by suppressing harmful modulations (the third group is closer to the first than to the fourth). The 'combo' results are for the combinatorial , ρ, ω space, where the effect is even more pronounced.
Figure 7: A few games cherry-picked to show that the combinatorial , ρ, ω bandit (thick red) can sometimes outperform all the fixed settings of z. The thick black line shows the fixed reference setting = 0.01.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) is a general formalism modelling sequential decision making. It as- pires to be broadly applicable, making minimal assumptions about the task at hand and reducing the need for prior knowledge. By learning behaviour from scratch, it has the potential to surpass human expertise or tackle complex domains where human intuition is not applicable. In practice, however, generality is often traded for performance and efficiency, with RL practitioners tuning algorithms, architectures and hyper-parameters to the task at hand ( Hessel et al., 2019 ). A side-effect of this is that the resulting methods can be brittle, or difficult to reliably reproduce ( Nagarajan et al., 2018 ). Exploration is one of the main aspects commonly designed or tuned specifically for the task be- ing solved. Previous work has shown that large sample-efficiency gains are possible, for example, when the exploratory behaviour's level of stochasticity is adjusted to the environment's hazard rate ( García & Fernández, 2015 ), or when an appropriate prior is used in large action spaces ( Dulac- Arnold et al., 2015 ;  Czarnecki et al., 2018 ;  Vinyals et al., 2019 ). Ideal exploration in the presence of function approximation should be agent-centred. It ought to focus more on generating data that supports the learning of agent at its current parameters θ, rather than making progress on objective measurements of information gathering. A useful notion here is learning progress (LP ), defined as the improvement of the learned policy π θ (Section 3). The agent's source of data is its behaviour policy. Beyond the conventional RL setting of a single stream of experience, distributed agents that interact with parallel copies of the environment can have multiple such data sources ( Horgan et al., 2018 ). In this paper, we restrict ourselves to the setting where all behaviour policies are derived from a single set of learned parameters θ, for ex- ample when θ parameterises an action-value function Q θ . Consequently the behaviour policies are given by π(Q θ , z), where each modulation z leads to meaningfully different behaviour. This can be guaranteed if z is semantic (e.g. degree of stochasticity) and consistent across multiple time-steps. The latter is achieved by holding z fixed throughout each episode (Section 2). We propose to estimate a proxy that is indicative of future learning progress, f (z) (Section 3), separately for each modulation z, and to adapt the distribution over modulations to maximize f , using a non-stationary multi-armed bandit that can exploit the factored structure of the modulations (Section 4).  Figure 1  shows a diagram of all these components. This results in an autonomous adaptation of behaviour to the agent's stage of learning (Section 5), varying across tasks and across time, and reducing the need for hyper-parameter tuning.

Section Title: MODULATED BEHAVIOUR
  MODULATED BEHAVIOUR As usual in RL, the objective of the agent is to find a policy π that maximises the γ-discounted expected return G t . = ∞ i=0 γ i R t+i , where R t is the reward obtained during the transition from time t to t + 1. A common way to address this problem is to use methods that compute the action- value function Q π given by Q π (s, a) . = E[G t |s, a], i.e. the expected return when starting from state s with action a and then following π ( Puterman, 1994 ). A richer representation of Q π that aims to capture more information about the underlying distribu- tion of G t has been proposed by  Bellemare et al. (2017) , and extended by  Dabney et al. (2018) . Instead of approximating only the mean of the return distribution, we approximate a discrete set of n quantile values q ν (where ν ∈ { 1 2n , 3 2n , . . . , 2n−1 2n }) such that P(Q π ≤ q ν ) = ν. Outside the benefits in performance and representation learning ( Such et al., 2018 ), these quantile estimates pro- vide a way of inducing risk-sensitive behaviour. We approximate all q ν using a single deep neural network with parameters θ, and define the evaluation policy as the greedy one with respect to the mean estimate: The behaviour policy is the central element of exploration: it generates exploratory behaviour (and experience therefrom) which is used to learn π θ ; ideally in such a way as to reduce the total amount of experience required to achieve good performance. Instead of a single monolithic behaviour policy, we propose to use a modulated policy to support parameterized variation. Its modulations z should satisfy the following criteria: they need to (i) be impactful, having a direct and meaningful effect on generated behaviour; (ii) have small dimensionality, as to quickly adapt to the needs of the learning algorithm, and interpretable semantics to ease the choice of viable ranges and initialisation; and (iii) be frugal, in the sense that they are relatively simple and computationally inexpensive to apply. In this work, we consider five concrete types of such modulations: Temperature: a Boltzmann softmax policy based on action-logits, modulated by temperature, T . Flat stochasticity: with probability the agent ignores the action distribution produced by the softmax, and samples an action uniformly at random ( -greedy). Per-action biases: action-logit offsets, b, to bias the agent to prefer some actions. Action-repeat probability: with probability ρ, the previous action is repeated ( Machado et al., 2017 ). This produces chains of repeated actions with expected length 1 1−ρ . Optimism: as the value function is represented by quantiles q ν , the aggregate estimate Q ω can be parameterised by an optimism exponent ω, such that ω = 0 recovers the default flat average, while positive values of ω imply optimism and negative ones pessimism. When near risk-neutral, our simple risk measure produces qualitatively similar transforms to those of  Wang (2000) . We combine the above modulations to produce the overall z-modulated policy Under review as a conference paper at ICLR 2020 where z . = (T, , b, ρ, ω), I x is the indicator function, and the optimism-aggregated value is Now that the behaviour policy can be modulated, the following two sections discuss the criteria and mechanisms for choosing modulations z.

Section Title: EXPLORATION & THE EFFECTIVE ACQUISITION OF INFORMATION
  EXPLORATION & THE EFFECTIVE ACQUISITION OF INFORMATION A key component of a successful reinforcement learning algorithm is the ability to acquire expe- rience (information) that allows it to make expeditious progress towards its objective of learning to act in the environment in such a way as to optimise returns over the relevant (potentially dis- counted) horizon. The types of experience that most benefit an agent's ultimate performance may differ qualitatively throughout the course of learning - a behaviour modulation that is beneficial in the beginning of training often enough does not carry over to the end, as illustrated by the analysis in  Figure 5 . However, this analysis was conducted in hindsight, and in general how to generate such experience optimally - optimal exploration in any environment - remains an open problem. One approach is to require exploration to be in service of the agent's future learning progress (LP ), and to optimise this quantity during learning. Although there are multiple ways of defining learning progress, in this work we opted for a task-related measure, namely the improvement of the policy in terms of expected return. This choice of measure corresponds to the local steepness of the learning curve of the evaluation policy π θ , LP t (∆θ) . = E s0 [V π θ t +∆θ (s 0 ) − V π θ t (s 0 )] , (1) where the expectation is over start states s 0 , the value V π (s) = E π [ γ i R i |s 0 = s] is the γ- discounted return one would expected to obtain, starting in state s and following policy π afterwards, and ∆θ is the change in the agent's parameters. Note that this is still a limited criterion, as it is myopic and might be prone to local optima. As prefaced in the last section, our goal here is to define a mechanism that can switch between different behaviour modulations depending on which of them seems most promising at this point in the training process. Thus in order to adapt the distribution over modulations z, we want to assess the expected LP when learning from data generated according to z-modulated behaviour: LP t (z) . = E τ ∼π θ t (z) [LP t (∆θ(τ, t))], with ∆θ(τ, t) the weight-change of learning from trajectory τ at time t. This is a subjective utility measure, quantifying how useful τ is for a particular learning algorithm, at this stage in training.

Section Title: Proxies for learning progress
  Proxies for learning progress While LP (z) is a simple and clear progress metric, it is not readily available during training, so that in practice, a proxy fitness f t (z) ≈ LP t (z) needs to be used. A key practical challenge is to construct f t from inexpensively measurable proxies, in a way that is sufficiently informative to effectively adapt the distribution over z, while being robust to noise, approximation error, state distribution shift and mismatch between the proxies and learning progress. The ideal choice of f (z) is a matter of empirical study, and this paper only scratches the surface on this topic. After some initial experimentation, we opted for the simple proxy of empirical (undiscounted) episodic return: f t (z) = ai∼π(Q θ t ,z) R i . This is trivial to estimate, but it departs from LP (z) in a number of ways. First, it does not contain learner-subjective information, but this is partly mitigated through the joint use of with prioritized replay (see Section 5.1) that over-samples high error experience. Another potential mechanism by which the episodic return can be indicative of future learning is because an improved policy tends to be preceded by some higher-return episodes - in general, there is a lag between best-seen performance and reliably reproducing it. Second, the fitness is based on absolute returns not differences in returns as suggested by Equation 1; this makes no difference to the relative orderings of z (and the resulting probabilities induced by the bandit), but it has the benefit that the non-stationarity takes a different form: a difference-based metric will Under review as a conference paper at ICLR 2020 appear stationary if the policy performance keeps increasing at a steady rate, but such a policy must be changing significantly to achieve that progress, and therefore the selection mechanism should keep revisiting other modulations. In contrast, our absolute fitness naturally has this effect when paired with a non-stationary bandit, as described in the next section.

Section Title: NON-STATIONARY BANDIT TAILORED TO LEARNING PROGRESS
  NON-STATIONARY BANDIT TAILORED TO LEARNING PROGRESS The most effective modulation scheme may differ throughout the course of learning. Instead of applying a single fixed modulation or fixed blend, we propose an adaptive scheme, in which the choice of modulation is dynamically based on learning progress. The adaptation process is based on a non-stationary multi-armed bandit ( Besbes et al., 2014 ;  Raj & Kalyani, 2017 ), where each arm corresponds to a behaviour modulation z. The non-stationarity reflects the nature of the learning progress LP t (z) which depends on the time t in training through the parameters θ t . Because of non-stationarity, the core challenge for such bandit is to identify good modulation arms quickly, while only having access to a noisy, indirect proxy f t (z) of the quantity of interest LP t (z). However, our setting also presents an unusual advantage: the bandit does not need to identify the best z, as in practice it suffices to spread probability among all arms that produce reasonably useful experience for learning. Concretely, our bandit samples a modulation z ∈ {z 1 , . . . , z K } according to the probability that it results in higher than usual fitness (measured as the mean over a recent length-h window): Note that m t depends on the payoffs of the actually sampled modulations z t−h:t−1 , allowing the bandit to become progressively more selective (if m t keeps increasing).

Section Title: Estimation
  Estimation where n(z, h) is the number of times that z was chosen in the corresponding time window. We encode a prior preference of 1 2 in the absence of other evidence, as an additional (fictitious) sample.

Section Title: Adaptive horizon
  Adaptive horizon The choice of h can be tuned as a hyper-parameter, but in order to remove all hyper-parameters from the bandit, we adapt it online instead. The update is based on a regression ac- curacy criterion, weighted by how often the arm is pulled. For the full description, see Appendix A.

Section Title: Factored structure
  Factored structure As we have seen in Section 2, our concrete modulations z have additional factored structure that can be exploited. For that we propose to use a separate sub-bandit (each defined as above) for each dimension j of z. The full modulation z is assembled from the z j independently sampled from the sub-bandits. This way, denoting by K j the number of arms for z j , the total number of arms to model is j K j , which is a significant reduction from the number of arms in the single flattened space j K j . This allows for dramatically faster adaptation in the bandit (see  Figure 2 ). On the other hand, from the perspective of each sub-bandit, there is now another source of non-stationarity due to other sub-bandits shifting their distributions.

Section Title: EXPERIMENTS
  EXPERIMENTS The central claim of this paper is that the best fixed hyper-parameters in hindsight for behaviour differ widely across tasks, and that an adaptive approach obtains similar performance to the best choice without costly per-task tuning. We report a broad collection of empirical results on Atari 2600 ( Bellemare et al., 2013 ) that substantiate this claim, and validate the effectiveness of the pro- posed components. From our results, we distill qualitative descriptions of the adaptation dynamics. To isolate effects, independent experiments may use all or subsets of the dimensions of z. Two initial experiments in a toy grid-world setting are reported in  Figure 2 . They demonstrate that the proposed bandit works well in both stationary and non-stationary settings. Moreover, they highlight the benefits of using the exact learning progress LP (z), and the gap incurred when using less informative proxies f (z). They also indicate that the factored approach can deliver a substantial speed-up. Details of this setting are described in Appendix B.

Section Title: EXPERIMENTAL SETUP: ATARI
  EXPERIMENTAL SETUP: ATARI Our Atari agent is a distributed system inspired by Impala ( Espeholt et al., 2018 ) and Ape-X ( Horgan et al., 2018 ), consisting of one learner (on GPU), multiple actors (on CPUs), and a bandit providing modulations to the actors. On each episode t, an actor queries the bandit for a modulation z t , and the learner for the latest network weights θ t . At episode end, it reports a fitness value f t (z t ) to the bandit, and adds the collected experience to a replay table for the learner. For stability and reliability, we enforce a fixed ratio between experience generated and learning steps, making actors and learner run at the same pace. Our agents learn a policy from 200 million environment frames in 10-12h wall-clock time (compared to a GPU-week for the state-of-art Rainbow agent ( Hessel et al., 2018 )). Besides distributed experience collection (i.e., improved experimental turnaround time), algorithmic elements of the learner are similar to Rainbow: the updates use multi-step double Q-learning, with distributional quantile regression ( Dabney et al., 2018 ) and prioritized experience replay ( Schaul et al., 2015 ). All hyper-parameters (besides those determined by z) are kept fixed across all games and all experiments; these are listed in Appendix C alongside default values of z. These allow us to generate competitive baseline results (118 ± 6% median human-normalised score) with a so-called reference setting (solid black in all learning curves) which sets the exploration parameters to that is most commonly used in the literature ( = 0.01, ω = 0, T = 0, b = 0, ρ = 0). If not mentioned otherwise, all aggregate results are across 15 games listed in Appendix D and at least N = 5 independent runs (seeds). Learning curves shown are evaluations of the greedy pol- icy after the agent has experienced the corresponding number of environment frames. To aggregate scores across these fifteen games we use relative rank, an ordinal statistic that weighs each game equally (despite different score scales) and highlights relative differences between variants. Con- cretely, the performance outcome G(game, seed, variant) is defined as the average return of the greedy policy across the last 10% of the run (20 million frames). All outcomes G(game, ·, ·) are then jointly ranked, and the corresponding ranks are averaged across seeds. The averaged ranks are normalized to fall between 0 and 1, such that a normalized rank of 1 corresponds to all N seeds of a variant being ranked at the top N positions in the joint ranking. Finally, the relative ranks for each variant are averaged across all games. See also Appendix D.

Section Title: QUANTIFYING THE TUNING CHALLENGES
  QUANTIFYING THE TUNING CHALLENGES It is widely appreciated that the best hyper-parameters differ per Atari game.  Figure 3  illustrates this point for multiple classes of modulations (different arms come out on top in different games), while  Figure 4  quantifies this phenomenon across 15 games and 4 modulation classes and finds that this effect holds in general. If early performance were indicative of final performance, the cost of tuning could be reduced. We quantify how much performance would be lost if the best fixed arm were based on the first 10% of the run.  Figure 5  shows that the mismatch is often substantial. This also indicates the best choice is non- stationary: what is good in early learning may not be good later on - an issue sometimes addressed by hand-crafted schedules (e.g., DQN linearly decreases the value of ( Mnih et al., 2015 )). Another approach is to choose not to choose, that is, feed experience from the full set of choices to the learner, an approach taken, e.g., in ( Horgan et al., 2018 ). However, this merely shifts the problem, as it in turn necessitates tuning this set of choices.  Figure 6  shows that the difference between a naive and a carefully curated set can indeed be very large (Table 4 in Appendix C lists all these sets).

Section Title: ADAPTING INSTEAD OF TUNING
  ADAPTING INSTEAD OF TUNING It turns out that adapting the distribution over z as learning progresses effectively addresses the three tuning challenges discussed above (per-task differences, early-late mismatch, handling sets).  Figure 6  shows that the bandit can quickly suppress the choices of harmful elements in a non- curated set; in other words, the set does not need to be carefully tuned. At the same time, a game- Under review as a conference paper at ICLR 2020 0.0 0.5 1.0 zaxxon yars_revenge venture tennis star_gunner space_invaders seaquest qbert private_eye ms_pacman hero frostbite demon_attack breakout asterix Performance drop when picking best fixed z early specific schedule emerges from the non-stationary adaptation, for example recovering an -schedule reminiscent of the hand-crafted one in DQN ( Mnih et al., 2015 ) (see Figure 17 in Appendix E). Finally, the overall performance of the bandit is similar to that of the best fixed choice, and not far from an "oracle" that picks the best fixed z per game in hindsight ( Figure 4 ). A number of other interesting qualitative dynamics emerge in our setting (Appendix E): action biases are used initially and later suppressed (e.g., on SEAQUEST, Figure 19); the usefulness of action Under review as a conference paper at ICLR 2020 repeats varies across training (e.g., on H.E.R.O., Figure 18). Figure 16 looks at additional bandit baselines and finds that addressing the non-stationarity is critical (see Appendix E.3). Finally, our approach generalizes beyond a single class of modulations; all proposed dimensions can adapt simultaneously within a single run, using a factored bandit to handle the combinatorial space. Figure 13 shows this yields similar performance to adapting within one class. In a few games this outperforms the best fixed choice 1 in hindsight; see  Figure 6  ('combo') and  Figure 7 ; presumably because of the added dynamic adaptation to the learning process. On the entire set of 57 Atari games, the bandit achieves similar performance (113 ± 2% median human-normalized score) to our fixed, tuned reference setting (118 ± 6%), despite operating on 60 different combinations of modulations.

Section Title: RELATED WORK
  RELATED WORK Here we focus on two facets of our research: its relation to exploration, and hyper-parameter tuning. First, our work can be seen as building on a rich literature on exploration through intrinsic motivation aimed at maximising learning progress. As the true learning progress is not readily available during training, much of this work targets one of a number of proxies: empirical return ( Jaderberg et al., 2017 ); change in parameters, policy, or value function ( Itti & Baldi, 2006 ); magnitude of training loss ( Mirolli & Baldassarre, 2013 ;  Schmidhuber, 1991 ); error reduction or derivative ( Schmidhuber, 1991 ;  Oudeyer et al., 2007 ); expected accuracy improvement ( Misra et al., 2018 ); compression progress ( Schmidhuber, 2008 ); reduction in uncertainty; improvement of value accuracy; or change in distribution of encountered states. Some of these have the desirable property that if the proxy is zero, so is LP . However, these proxies themselves may only be available in approximated form, and these approximations tend to be highly dependent on the state distribution under which they are evaluated, which is subject to continual shift due to the changes in policy. As a result, direct comparison between different learning algorithms under these proxies tends to be precarious. Second, our adaptive behaviour modulation can be viewed as an alternative to per-task hyper- parameter tuning, or hyper-parameter tuning with cross-task transfer ( Golovin et al., 2017 ), and can be compared to other works attempting to reduce the need for this common practice. (Note that the best-fixed-arm in our experiments is equivalent to explicitly tuning the modulations as hyper- parameters.) Though often performed manually, hyper-parameter tuning can be improved by ran- dom search ( Bergstra et al., 2011 ), but in either case requires many full training cycles, whereas our work optimises the modulations on-the-fly during a single training run. Like our method, Population Based Training (PBT,  Jaderberg et al., 2017 ) and meta-gradient RL ( Andrychowicz et al., 2016 ;  Xu et al., 2018 ) share the property of dynamically adapting hyper- parameters throughout agent training. However, these methods exist in a distinctly different problem setting: PBT assumes the ability to run multiple independent learners in parallel with separate ex- perience. Its cost grows linearly with the population size (typically > 10), but it can tune other hyper-parameters than our approach (such as learning rates). Meta-gradient RL, on the other hand, Under review as a conference paper at ICLR 2020 assumes that the fitness is a differentiable function of the hyper-parameters, which may not generally hold for exploration hyper-parameters. While our method focuses on modulating behaviour in order to shape the experience stream for effective learning, a related but complementary approach is to filter or prioritize the generated ex- perience when sampling from replay. Classically, replay prioritization has been based on TD error, a simple proxy for the learning progress conferred by an experience sample ( Schaul et al., 2015 ). More recently, however, learned and thereby more adaptive prioritization schemes have been pro- posed ( Zha et al., 2019 ), with (approximate) learning progress as the objective function.

Section Title: DISCUSSION & FUTURE WORK
  DISCUSSION & FUTURE WORK Reiterating one of our key observations: the qualitative properties of experience generated by an agent impact its learning, in a way that depends on characteristics of the task, current learning parameters, and the design of the agent and its learning algorithm. We have demonstrated that by adaptively using simple, direct modulations of the way an agent generates experience, we can improve the efficiency of learning by adapting to the dynamics of the learning process and the specific requirements of the task. Our proposed method 2 has the potential to accelerate RL research by reducing the burden of hyper-parameter tuning or the requirement for hand-designed strategies, and does so without incurring the computational overhead of some of the alternatives. The work presented in this paper represents a first stab at exploiting adaptive modulations to the dynamics of learning, and there are many natural ways of extending this work. For instance, such an approach need not be constrained to draw only from experiences generated by the agent; the agent can also leverage demonstrations provided by humans or by other agents. Having an adaptive system control the use of data relieves system designers of the need to curate such data to be of high quality - an adaptive system can learn to simply ignore data sources that are not useful (or which have outlived their usefulness), as our bandit has done in the case of choosing modulations to generate experiences with (e.g., Figures 17, 18, 19). A potential limitation of our proposal is the assumption that a modulation remains fixed for the duration of an episode. This restriction could be lifted, and one can imagine scenarios in which the modulation used might depend on time or the underlying state. For example, an agent might generate more useful exploratory experiences by having low stochasticity in the initial part of an episode, but switching to have higher entropy once it reaches an unexplored region of state space. There is also considerable scope to expand the set of modulations used. A particularly promising avenue might be to consider adding noise in parameter space, and controlling the variance ( Fortunato et al., 2018 ;  Plappert et al., 2018 ). In addition, previous works have shown that agents can learn diverse behaviours conditioned on a latent policy embedding ( Eysenbach et al., 2018 ;  Haarnoja et al., 2018 ), goal ( Ghosh et al., 2018 ;  Nair et al., 2018 ) or task specification ( Borsa et al., 2019 ). A bandit could potentially be exposed to modulating the choices in abstract task space, which could be a powerful driver for more directed exploration.

```
