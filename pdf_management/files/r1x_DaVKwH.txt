Title:
```
HUMAN ON ATARI? LEVELING THE PLAYING FIELD
```
Abstract:
```
Consistent and reproducible evaluation of Deep Reinforcement Learning (DRL) is not straightforward. In the Arcade Learning Environment (ALE), small changes in environment parameters such as stochasticity or the maximum allowed play time can lead to very different performance. In this work, we discuss the difficulties of comparing different agents trained on ALE. In order to take a step further towards reproducible and comparable DRL, we introduce SABER, a Standardized Atari BEnchmark for general Reinforcement learning algorithms. Our methodology extends previous recommendations and contains a complete set of environment parameters as well as train and test procedures. We then use SABER to evaluate the current state of the art, Rainbow. Furthermore, we introduce a human world records baseline, and argue that previous claims of expert or superhuman per- formance of DRL might not be accurate. Finally, we propose Rainbow-IQN by extending Rainbow with Implicit Quantile Networks (IQN) leading to new state- of-the-art performance. Source code is available for reproducibility.
```

Figures/Tables Captions:
```
Figure 1: ALE Space In- vaders As the number of contributions is growing fast, it becomes harder and harder to make a proper comparison between different algorithms. In particular, a relevant difference in the training and evaluation procedures exists between available publications. Those issues are exacerbated by the fact that training DRL agents is very time consuming, resulting in a high barrier for reevaluation of previous work. Specifically, even though ALE is fast at runtime, training an agent on one game takes approxi- mately one week on one GPU and thus the equivalent of more than one year to train on all 61 Atari games. A standardization of the evaluation procedure is needed to make DRL that matters as pointed out by Hen- derson et al. (2018) for the Mujoco benchmark (Todorov et al., 2012): the authors criticize the lack of reproducibility and discuss how to allow for a fair comparison in DRL that is consistent between articles.
Figure 2: World records scores vs. the usual be- ginner human baseline (Mnih et al., 2015) (log scale).
Figure 3: Comparison of Rainbow and Rainbow-IQN on SABER: Median normalized scores with regards to training steps.
Figure 4: Comparison of Rainbow and Rainbow-IQN on SABER: classifying performance of agents relatively to the records baseline (at 200M training frames).
Figure 5: Median performance comparison for DQN, Rainbow and Rainbow-IQN with regards to training frames. Evaluation time is set at 5 minutes to allow a comparison to DQN.
Figure 6: Median normalized scores with regards to training steps averaged over 5 seeds for both Rainbow and Rainbow-IQN. Only the 14 games on which 5 seeds have been conducted were used for this figure.
Table 1: Game parameters of SABER
Table 2: Median and mean human-normalized performance and number of superhuman scores (> 100%). Scores are coming from the original Rainbow and from our re-evaluation of Rainbow following recommendations of Machado et al. (30 minutes evaluation, at 200M training frames).
Table 3: Evolution of performance with evaluation time (mean, median of normalized baseline and number of superhuman agents) for Rainbow and Rainbow-IQN.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Human intelligence is able to solve many tasks of different natures. In pursuit of generality in artificial intelligence, video games have become an important testing ground: they require a wide set of skills such as perception, exploration and control. Reinforcement Learning (RL) is at the forefront of this development, especially when combined with deep neural networks in DRL. One of the first general approaches reaching reasonable performance on many Atari games while using the exact same hyper-parameters and neural network architecture was Deep Q-Network (DQN) ( Mnih et al., 2015 ), a value based DRL algorithm which directly takes the raw image as input. This success sparked a lot of research aiming to create better, faster and more stable general algorithms. The ALE ( Bellemare et al., 2013 ), featuring more than 60 Atari games (see  Figure 1 ), is heavily used in this context. It provides many different tasks ranging from simple paddle control in the ball game Pong to complex labyrinth exploration in Montezuma's Revenge which remains unsolved by general algorithms up to today. In this work, we first discuss current issues in the evaluation procedure of different DRL algorithms on ALE and their impact. We then propose an improved evaluation procedure, extending the rec- ommendations of  Machado et al. (2018) , named SABER : a Standardized Atari BEnchmark for Under review as a conference paper at ICLR 2020 Reinforcement learning. We suggest benchmarking on the world records human baseline and show that RL algorithms are in fact far from solving most of the Atari games. As an illustration of SABER, current state-of-the-art DRL algorithm Rainbow ( Hessel et al., 2018 ) is benchmarked. Finally, we introduce and benchmark on SABER a new state-of-the-art agent: a distributable combination of Rainbow and Implicit Quantiles Network (IQN) ( Dabney et al., 2018 ). The main contributions of this work are : • The proposal, description and justification of the SABER benchmark. • Introduction of a world records human baseline. We argue it is more representative of the human level than the one used in most of previous works. With this metric, we show that the Atari benchmark is in fact a hard task for current general algorithm. • A SABER compliant evaluation of current state-of-the art agent Rainbow. • A new state-of-the-art agent on Atari, Rainbow-IQN, with a comparison on SABER to Rainbow, to give an improvement range for future comparisons. • For reproducibility sake, an open-source implementation of Rainbow, Rainbow-IQN, dis- tributed following the idea from  Horgan et al. (2018) .

Section Title: RELATED WORK
  RELATED WORK Reproducibility and comparison in DRL Deep Reinforcement Learning that matters (Hender- son et al., 2018) is one of the first works to warn about a reproducibility crisis in the field of DRL. This article relies on the MuJoCo ( Todorov et al., 2012 ) benchmark to illustrate how some com- mon practices can bias reported results. As a continuation to the work of  Henderson et al. (2018) , J. Pineau introduced a Machine Learning reproducibility checklist ( Pineau, 2019 ) to allow for re- producibility and fair comparison.  Machado et al. (2018)  deal with the Atari benchmark. They describe the divergence in training and evaluation procedures and how this could lead to difficulties to compare different algorithms. A first set of recommendations to standardize them is introduced, constituting the basis of this work and will be summarized in the next section. Finally, the Github Dopamine ( Castro et al., 2018 ) provides an open-source implementation of some of the current state-of-the-art algorithms on Atari benchmark, including Rainbow and IQN. An evaluation follow- ing almost all guidelines from  Machado et al. (2018)  are provided in  Castro et al. (2018) . However the implementation of Rainbow is partial, and the recommendation of using the full action set is not applied. This is why our work contains a new evaluation of Rainbow. Value based RL DQN ( Mnih et al., 2015 ) is the first value based DRL algorithm benchmarked on all Atari games with the exact same set of hyperparameters (although previous work by  Hausknecht et al. (2014)  already performed such a benchmark with neural networks). This algorithm relies on the well known Q-Learning algorithm (Watkins & Dayan, 1992) and incorporates a neural network. Deep Q-learning is quite unstable and the main success of this work is to introduce practical tricks to make it converge. Mainly, transitions are stored in a replay memory and sampled to avoid correlation in training batch, and a separate target network is used to avoid oscillations. Since then, DQN has been improved and extended to make it more robust, faster and better. Rainbow ( Hessel et al., 2018 ) is the combination of 6 of these improvements ( Van Hasselt et al., 2016 ;  Schaul et al., 2015 ;  Bellemare et al., 2017 ;  Wang et al., 2015 ;  Fortunato et al., 2017 ;  Mnih et al., 2016 ) implemented in a single algorithm. Some ablations studies showed that the most important components were Prioritized Experience Replay (PER) ( Schaul et al., 2015 ) and C51 ( Bellemare et al., 2017 ). The idea behind PER is to sample transitions according to their surprise, i.e. the worse the network is at predicting the Q-value of a specific transition, the more we sample it. C51 is the first algorithm in Distributional RL which predicts the full distribution of the Q-function instead of predicting only the mean of it. Finally, IQN ( Dabney et al., 2018 ) is an improvement over C51. It almost reaches on its own the performance of the full Rainbow with all 6 components. In C51 the distribution of the Q-function is represented as a categorical distribution while in IQN, it is represented by implicit quantiles.

Section Title: CHALLENGES WHEN COMPARING PERFORMANCE ON THE ATARI BENCHMARK
  CHALLENGES WHEN COMPARING PERFORMANCE ON THE ATARI BENCHMARK In this section we discuss several challenges to make a proper comparison between different algo- rithms trained on the Atari benchmark. First, we briefly summarize the initial problems and their solution as proposed by  Machado et al. (2018) . Then we detail a remaining issue not handled by those initial standards, the maximum length time allowed for an episode. Finally, we introduce a readable metric, representative of actual human level and allowing meaningful comparison.

Section Title: REVISITING ALE: AN INITIAL STEP TOWARDS STANDARDIZATION
  REVISITING ALE: AN INITIAL STEP TOWARDS STANDARDIZATION   Machado et al. (2018)  discuss about divergence of training and evaluation procedures on Atari. They show how those divergences are making comparison extremely difficult. They establish recommen- dations that should be used in order to standardize the evaluation process.

Section Title: Stochasticity
  Stochasticity The ALE environment is fully deterministic, i.e. leading to the exact same state if the exact same actions are taken at each state. This is actually an issue for general algorithm evaluation. For example, an algorithm learning by heart good trajectories can actually reach a high score with an open-loop behaviour. To handle this issue,  Machado et al. (2018)  introduce sticky actions: actions coming from the agent are repeated with a given probability ξ, leading to a non deterministic behavior. They show that sticky actions are drastically affecting performance of an algorithm exploiting the environment determinism without hurting algorithms learning more robust policies like DQN ( Mnih et al., 2015 ). We use sticky actions with probability ξ = 0.25 ( Machado et al., 2018 ) in all our experiments. End of an episode: Use actual game over In most of the Atari games the player has multiple lives and the game is actually over when all lives are lost. But some articles, e.g. DQN, Rainbow, IQN, end a training episode after the loss of the first life but still use the standard game over signal while testing. This can in fact help the agent to learn how to avoid death and is an unfair comparison to agents which are not using this game-specific knowledge.  Machado et al. (2018)  recommend to use only the standard game over signal for all games while training.

Section Title: Action set
  Action set Following the recommendation of  Machado et al. (2018)  we do not use the minimal useful action set (the set of actions having an effective impact on the current game) as used by many previous works ( Mnih et al., 2015 ;  Hessel et al., 2018 ). Instead we always use all 18 possible actions on the Atari Console. This removes some specific domain knowledge and reduces the complexity of reproducibility. For some games, the minimal useful action set is different from one version to another of the standard Atari library: an issue to reproduce result on breakout was coming from this ( Graetz, 2018 ).

Section Title: Reporting performance
  Reporting performance As proposed by  Machado et al. (2018) , we report our score while training by averaging k consecutive episodes (we have set k = 100). This gives information about the stability of the training and removes the statistical bias induced when reporting score of the best policy which is today a common practice ( Mnih et al., 2015 ;  Hessel et al., 2018 ).

Section Title: MAXIMUM EPISODE LENGTH
  MAXIMUM EPISODE LENGTH A major parameter is left out of the work of  Machado et al. (2018) : the maximum number of frames allowed per episode. This parameter ends the episode after a fixed number of time steps even if the game is not over. In most of recent works ( Hessel et al., 2018 ;  Dabney et al., 2018 ), this is set to 30 min of game play and only to 5 min in Revisiting ALE ( Machado et al., 2018 ). This means that the reported scores can not be compared fairly. For example, in easy games (e.g. Atlantis, Enduro), the agent never dies and the score is more or less linear to the allowed time: the reported score will be 6 times higher if capped at 30 minutes instead of 5 minutes. We argue that the time cap can make the performance comparison non significant. On many games (e.g. Atlantis, Video Pinball) the scores reported of Ape-X ( Horgan et al., 2018 ), Rainbow ( Hessel et al., 2018 ) and IQN ( Dabney et al., 2018 ) are almost exactly the same. This is because all agents Under review as a conference paper at ICLR 2020 reach the time limit and get the highest possible score in 30 minutes: the difference in scores is due to minor variations, not algorithmic difference. As a consequence, the more successful agents are, the more games are incomparable because they reach the maximum possible score in the time cap. This parameter can also be a source of ambiguity and error. The best score on Atlantis (2,311,815) is reported by Proximal Policy Optimization by  Schulman et al. (2017)  but this score is almost certainly wrong: it seems impossible to reach it in only 30 minutes! The first distributional paper, C51 ( Bellemare et al., 2017 ), also did this mistake and reported wrong results before adding an erratum in a later version on ArXiv. We argue that episodes should not be capped at all. The original ALE article ( Bellemare et al., 2013 , pg.3) states that This functionality is needed for a small number of games to ensure that they always terminate. On some famously hard games like Pitfall and Tennis, random exploration leads to much more negative reward than positive and thus the agent effectively learns to do nothing, e.g. not serving in Tennis. We claim that, even with this constraint, agents still end up learning to do nothing, and the drawback of the cap harms the evaluation of all other games. Moreover, the human high scores for Atari games have been achieved in several hours of play, and would have been unreachable if limited to 30 minutes. To summarize, ideally one would not cap at all length of episode while training and testing. However this makes some limitations of the ALE environment appear, as described in the following paragraph.

Section Title: Glitch and bug in the ALE environment
  Glitch and bug in the ALE environment When setting the maximum length of an episode to infinite time, the agent gets stuck on some games, i.e. the episode never ends, because of a bug in the emulator. In this case, even doing random actions for more than 20 hours neither gives any reward nor end the game. This happens consistently on BattleZone and less frequently on Yar's Revenge. One unmanaged occurrence of this problem is enough to hamper the whole training of the agent. It is important to note that those bugs were discovered by chance and it is probable that this could happen on some other games. We recommend to set the maximum episode length to infinite (in practice, a limit of 100 hours was used). Additionally we suggest a maximum stuck time of 5 minutes. Instead of limiting the allowed time for the agent, we limit the time without receiving any reward. This small trick handles all issues exposed above, and sets all reported scores on the same basis, making comparison to world records possible. Other bugs or particularities harming evalua- tion were encountered while training on the full Atari benchmark: buffer rollover with sudden negative score, influence of a start key for some games, etc. They are detailed and discussed in the supplementary material and we argue that they can have a drastic impact on performance and explain inconsistencies.

Section Title: HUMAN WORLD RECORDS BASELINE
  HUMAN WORLD RECORDS BASELINE A common way to evaluate AI for games is to let agents compete against human world cham- pions. Recent examples for DRL include the victory of AlphaGo versus Lee Sedol for Go ( Silver et al., 2016 ), OpenAI Five on Dota 2 ( OpenAI, 2018 ) or AlphaStar versus Mana for StarCraft 2 ( Vinyals et al., 2019 ). In the same spirit, one of the most used metric for evaluating RL agents on Atari is to compare them to the human baseline introduced by  Mnih et al. (2015) . Previous works use the normalized human score, i.e. 0% is the score of a random player and 100% is the score of the human baseline, which allows to summarize the performance on the whole Atari set in one number, instead of individually comparing raw scores for each of the 61 games. However we argue Under review as a conference paper at ICLR 2020 that this human baseline is far from being representative of the best human player, which means that using it to claim superhuman performance is misleading. The current world records are available online for 58 of the 61 evaluated Atari game 1 . Evaluating these world records scores using the usual human normalized score has a median of 4.4k% and a mean of 99.3k% (see  Figure 2  for details), to be compared to 200% and 800% of original Rainbow ( Hessel et al., 2018 ). As a consequence, we argue that using a normalized human score with the world records will give a much better indication of the performance of the agents and the margin of improvement. Note that 3 games of the ALE (double dunk, elevator action and tennis) do not have a registered world record, so all following experiments contain 58 games.

Section Title: SABER : A STANDARDIZED ATARI BENCHMARK FOR REINFORCEMENT
  SABER : A STANDARDIZED ATARI BENCHMARK FOR REINFORCEMENT

Section Title: LEARNING
  LEARNING In this section we introduce SABER, a set of training and evaluation procedures on the Atari bench- mark allowing for fair comparison and for reproducibility. Moreover, those procedures make it possible to compare with the human world records baseline introduced above and thus to obtain an accurate idea of the gap between general agents and best human players.

Section Title: TRAINING AND EVALUATION PROCEDURES
  TRAINING AND EVALUATION PROCEDURES All recommendations stated in the previous section are summarized in  Table 1  to constitute the SABER benchmark. It is important to note that those procedures must be used at both training and test time. The recent work Go-Explore ( Ecoffet et al., 2019 ) opened a debate on allowing or not stochasticity at training time. They report state-of-the-art performance on the famously hard game Montezuma's Revenge by removing stochasticity at training time. They conclude that we should have benchmarks with and without it ( Ecoffet Adrien & Clune, 2018 ). We choose to use same conditions for training and testing general agents: this is more in line with realistic tasks.

Section Title: REPORTING RESULTS
  REPORTING RESULTS X ( Horgan et al., 2018 ) and based on the implementation of ( Castro et al., 2018 ). IQN is an evolution of the C51 algorithm ( Bellemare et al., 2017 ) which is one of the 6 components of the full Rainbow, so this is a natural upgrade. After the implementation, preliminary tests high- lighted the impact of PER ( Schaul et al., 2015 ): taking the initial hyper-parameters for PER from Rainbow resulted in poor performance. Transitions are sampled from the replay memory propor- tionally to the training loss to the power of priority exponent ω. Reviewing the distribution of the loss shows that it is significantly more spread for Rainbow-IQN than for Rainbow, thus making the training unstable, because some transitions were over-sampled. To handle this issue, 4 values of ω were tested on 5 games: 0.1, 0.15, 0.2, 0.25 instead of 0.5 for original Rainbow, with 0.2 giving the best performance. The 5 games were Alien, Battle Zone, Chopper Command, Gopher and Space Invaders. All other parameters were left as is. Rainbow-IQN is evaluated on SABER and compared to Rainbow in the following section.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we describe the experiments performed on SABER. For all parameters not mentioned in SABER (e.g. the action repeat, the network architecture, the image preprocessing, etc) we care- fully followed the parameters used in Rainbow ( Hessel et al., 2018 ) and IQN ( Dabney et al., 2018 ) papers. Those details and the scores for each agent and individual games can be found in the supple- mentary materials. Training one agent takes slightly less than a week, which makes a full benchmark use around 1 year-GPU. As a consequence, for each algorithm benchmark, trainings were run with only one seed for the full benchmark, and 5 seeds for 14 of the 61 games. Details on the choice of these games and the associated scores can be found in Section 5.3. The combined duration of all experiments conducted for this article is more than 4 years-GPU. Agents are trained using SABER guidelines on the 61 Atari games, and evaluated with the records baseline for 58 games. Scores at both 5 minutes and 30 minutes are kept while training to compare to previous works. Benchmarking Rainbow makes it possible to measure the impact of the guidelines of Machado et al.: sticky actions, ignore life signal and full action set.  Table 2  compares the originally reported performance of Rainbow ( Hessel et al., 2018 ) to an evaluation following the recommendations of Machado et al. The performance is measured with the records baseline, for a 30 minutes evaluation at 200M training frames, to be as close as possible to the conditions of the original Rainbow. The impact of the standardized training procedure is major: as shown in the following paragraph, the difference in median (1.59%) is comparable to the difference between DQN and Rainbow (1.8%, see  Figure 5 ) when both are trained on same training procedures. This demonstrates the importance of explicit and standardized training and evaluation procedures.

Section Title: RAINBOW-IQN: EVALUATION AND COMPARISON
  RAINBOW-IQN: EVALUATION AND COMPARISON Influence of maximum episode length  Table 3  studies the influence of the time limit for the evaluation, by reporting performance for Rainbow and Rainbow-IQN depending on the evaluation time. A significant difference can be seen between 5, 30 minutes and without limiting time of evaluation, which confirms the discussion of Section 2.2.

Section Title: Comparison to Rainbow
  Comparison to Rainbow As introduced in Section 3.2, we compare Rainbow and Rainbow-IQN with median and mean metrics on SABER conditions, and with a classification of the performance of the agents in  Figure 4 .  Figure 3  shows that Rainbow-IQN performance during training is consistently higher than Rainbow. One can notice on  Figure 4  that the majority of agents are in the poor and failing categories, showing the gap that must be crossed to achieve superhuman performance on the ALE. Comparison to DQN  Figure 5  provides a comparison between DQN, Rainbow and Rainbow- IQN. The evaluation time is set at 5 minutes to be consistent with the reported score of DQN by  Machado et al. (2018) . As expected, DQN is outperformed for all training steps. As aforementioned, Under review as a conference paper at ICLR 2020 the difference between DQN and Rainbow is in the same range as the difference coming from divergent training procedures, showing again the necessity for standardization. 5.3 STABILITY OF BOTH RAINBOW AND RAINBOW-IQN  Machado et al. (2018)  use 5 different seeds for training to check that the results are reproducible and stable. For this article, these 5 runs are conducted on both Rainbow and Rainbow-IQN for 14 games (around 25% of the whole benchmark). It would be best to have the whole benchmark on 5 seeds but this was way above our computational resources. Still, these 14 games allow us to make a first step of stability studies. They are chosen according to the results of the first seed, with the idea of prioritizing games on which scores were most notably different between Rainbow and Rainbow-IQN. We also try to choose diverse games from different categories (from failing to superhuman) and we removed the 5 games used for the hyperparameter tuning. Games that were either too hard (such as Montezuma's Revenge or Pitfall) or too simple (such as Pong or Atlantis) are intentionally excluded to make the additional tests as significant as possible. For each game with 5 seeds conducted, we computed the median and mean human-normalized performance averaged over the 5 trials. This way, we can both have a reasonable estimation of the stability of the trainings, and a comparison as fair as possible between Rainbow and Rainbow-IQN.  Figure 6  shows the median averaged over 5 trials for both Rainbow and Rainbow-IQN. We also plot each seed separately and the standard deviation over the 5 seeds. This has been computed only on Under review as a conference paper at ICLR 2020 the 14 games on which we succeeded to conduct 5 runs. This shows that standard deviations are roughly similar for Rainbow and Rainbow-IQN, around 0.2 % on the world record baseline. As these standard deviations are rather small for 25% of the Atari games, we can assume they would be still small on the whole benchmark. We think that this reveals that both Rainbow and Rainbow-IQN are quite stable on Atari and strengthens our confidence on Rainbow-IQN being the new state-of- the-art on the Atari benchmark. In particular, Rainbow-IQN reaches infinite game time on Asteroids on all 5 trials whereas Rainbow fails for each seed.

Section Title: CONCLUSION: WHY IS RL THAT BAD AT ATARI GAMES?
  CONCLUSION: WHY IS RL THAT BAD AT ATARI GAMES? In the current work, we confirm the impact of standardized guidelines for DRL evaluation, and build a consolidated benchmark, SABER. The importance of the play time is highlighted: agents should be trained and evaluated with no time limitation. To provide a more significant comparison, a new baseline is built, based on human world records. Following these recommendations, we show that the state-of-the-art Rainbow agent is in fact far from human world records performance. As a further illustration, we provide an improvement, Rainbow-IQN, and use it to measure the impact of the evaluation time over performance. The striking information from these results is that general DRL algorithms are far from best human performance. The median of world records human normalized score for Rainbow-IQN is 3,1%, meaning that for half of the games, the agent is only 3% of the way from random play to the actual best human play. There are many possible reasons for this failure, which we will briefly discuss here to give an intuition of the current limitations of general DRL algorithm.

Section Title: Reward clipping
  Reward clipping In some games the optimal play for the RL algorithm is not the same as for the human player. Indeed, all rewards are clipped between -1 and 1 so RL agents will prefer to obtain many small rewards over a single large one. This problem is well represented in the game Bowling: the agent learns to avoid striking or sparing. Indeed the actual optimal play is to perform 10 strikes in a row leading to one big reward of 300 (clipped to 1 for the RL agent) but the optimal play for the RL agent is to knock off bowling pins one by one. This shows the need of a better way to handle reward of different magnitude, by using an invertible value function as suggested by  Pohlen et al. (2018)  or using Pop-Art normalization ( van Hasselt et al., 2016 ).

Section Title: Exploration
  Exploration Another common reason for failure is a lack of exploration, resulting in the agent getting stuck in a local minimum. Random exploration or Noisy Networks ( Fortunato et al., 2017 ) are far from being enough to solve most of Atari games. In Kangaroo for example, the agent learns to obtain rewards easily on the first level but never tries to go to the next level. This problem might be exacerbated by the reward clipping: changing level may yield a higher reward, but for the RL algorithm all rewards are the same. Exploration is one of the most studied field in Reinforcement Learning, so possible solutions could rely on curiosity ( Pathak et al., 2017 ) or count-based explo- ration ( Ostrovski et al., 2017 ).

Section Title: Human basic knowledge
  Human basic knowledge Atari games are designed for human players, so they rely on implicit prior knowledge. This will give a human player information on actions that are probably positive, but with no immediate score reward (climbing a ladder, avoiding a skull etc). The most representative example can be seen in Riverraid: shooting a fuel container gives an immediate score reward, but taking it makes it possible to play longer. Current general RL agents do not identify it as a potential bonus, and so die quickly. Even with smart exploration, this remains an open challenge for any general agent.

Section Title: Loop on a sub-optimal policy
  Loop on a sub-optimal policy Finally, we discovered that on some games the agent finds quickly a loop continuously giving a small amount of reward and spends the whole training on this loop. In Bank Heist for example, the agent understood that bonus were respawning when changing level. Therefore the agent learned to just take over and over the same bonus until game timeout, failing to reach a good score. A very similar behaviour was discovered on Elevator Action,Kangaroo, Krull and Tutankham. Under review as a conference paper at ICLR 2020

```
