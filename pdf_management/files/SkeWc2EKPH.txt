Title:
```
Under review as a conference paper at ICLR 2020 MODEL-FREE CONTROL OF NONLINEAR STOCHASTIC SYSTEMS WITH STABILITY GUARANTEE
```
Abstract:
```
Reinforcement learning (RL) offers a principled way to achieve the optimal cu- mulative performance index in discrete-time nonlinear stochastic systems, which are modeled as Markov decision processes. Its integration with deep learning techniques has promoted the field of deep RL with an impressive performance in complicated continuous control tasks. However, from a control-theoretic perspec- tive, the first and most important property of a system to be guaranteed is stability. Unfortunately, stability is rarely assured in RL and remains an open question. In this paper, we propose a stability guaranteed RL framework which simultaneously learns a Lyapunov function along with the controller or policy, both of which are parameterized by deep neural networks, by borrowing the concept of Lyapunov function from control theory. Our framework can not only offer comparable or superior control performance over the state-of-the-art RL algorithms, but also construct a Lyapunov function to validate the closed-loop stability. In the simulated experiments, our approach is evaluated on several well-known examples including classic CartPole balancing, 3-dimensional robot control and control of synthetic biology gene regulatory networks. Compared with RL algorithms without stability guarantee, our approach can enable the system to recover to the operating point when interfered by uncertainties such as unseen disturbances and system paramet- ric variations to a certain extent. (Anonymous code is available to reproduce the experimental results 1 .) 1 https://www.dropbox.com/sh/j9mhvi0vydu7x7c/AACwJbqU5MCLcKPGgcOv0zrHa? dl=0
```

Figures/Tables Captions:
```
Figure 1: Cumulative control performance comparison. The Y-axis indicates the total cost during one episode and the X-axis indicates the total time steps in thousand. The shadowed region shows the 1-SD confidence interval over 10 random seeds. Across all trials of training, LAC converges to stabilizing solution with comparable or superior performance compared with SAC and SPPO. The experiment on Complicated-Repressilator is deferred to Appendix F.
Figure 2: Value of Lagrange multiplier λ during the training of LAC policies. The Y-axis indicates the value of λ and the X-axis indicates the total time steps in thousand. The shadowed region shows the 1-SD confidence interval over 10 random seeds. The value of λ gradually drops and becomes zero at convergence, which implies the satisfaction of stability condition.
Figure 3: State trajectories over time under policies trained by LAC and SAC and tested in the presence of parametric uncertainties and process noise, for CartPole and Repressilator. Solid line indicates the average trajectory and shadowed region for the 1-SD confidence interval. In (a) and (b), the pole length is varied during the inference. In (c) and (d), three parameters are selected to reflect the uncertainties in gene expression. The X-axis indicates the time and Y-axis shows the angle of pole in (a,b) and concentration of protein to be controlled in (c,d), respectively. Dashed line indicates the reference signal. The line in orange indicates the dynamic in original environment. For each curve, only the noted parameter is different with the original setting. We also show the curves in separate zoom-in view in Appendix I.1.
Figure 4: Performance of policies trained by LAC, SAC and SPPO, along with controllers designed by LQR in the presence of persistent disturbances with different magnitudes. X-axis indicates the magnitude of the applied disturbance. For CartPole (a) the Y-axis indicates the probability of falling over and in other three examples (b)-(d) it indicates the total cost. Both policies are evaluated for 100 trials in each setting.
Figure 5: State trajectories under policies trained by LAC and SAC when tracking different reference signals. Solid line indicates the average trajectory and shadowed region for the 1-SD confidence interval. The X-axis indicates the time and Y-axis shows the concentration of protein to be controlled. Dashed lines in different colors are the different reference signals: sinusoid with period of 150 (brown); sinusoid with period of 200 (skyblue);sinusoid with period of 400 (blue); constant reference of 8 (red); constant reference of 16 (green). We also show the curves in separate zoom-in view in Appendix I.2 .
Figure 6: Influence of different Lyapunov function candidates and network structures. In (a) and (c), the Y-axis indicates total cost of policies during training by LAC with Lyapunov function candidates of different length of horizon N and structures, and the X-axis indicates the total time steps in thousand. (b) and (d) shows the death-rate of policies in the presence of instant impulsive force F ranging from 80 to 150 Newton.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Control of discrete-time nonlinear stochastic systems is an important topic in both control theory and reinforcement learning. In the past decades, the advancement of nonlinear control theory in the control community has been successfully applied in aircraft, automobiles, advanced robots and space systems (Slotine et al., 1991; Isidori, 1995). Concurrently, reinforcement learning was developed in the machine learning community to address similar nonlinear control problems (Sutton et al., 1992; Tesauro, 1995; Bertsekas & Tsitsiklis, 1996). Until recently, significant progress has been made by combining advances in deep learning (LeCun et al., 2015) with reinforcement learning. Impressive results are obtained in a series of high-dimensional continuous nonlinear control problems (Duan et al., 2016; Zhang et al., 2016; Zhu et al., 2017; Gu et al., 2017) in which control-theoretic approach is typically difficult to apply. Given a control system, regardless of which controller design method is used, control theory or reinforcement learning, the first and most important property of a system needs to be guaranteed is stability, because an unstable control system is typically useless and potentially dangerous (Slotine et al., 1991). Qualitatively, a system is described as stable if starting the system in the neighborhood of its desired operating point implies that it will stay around the point ever after. For aircraft control systems, a typical stability problem is intuitively related to the following question: will a trajectory perturbation caused by a gust result in a significant deviation in the later flight trajectory? Here, the desired operating point of the system is the flight trajectory in the absence of disturbance. Every control system, whether linear or nonlinear, involves a stability problem which should be carefully studied.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 The most useful and general approach for studying the stability of control systems is Lyapunov method (Lyapunov, 1892), which is dominant in control engineering (Åström & Wittenmark, 1989; Mayne et al., 2000). In Lyapunov method, a scalar "energy-like" function called Lyapunov function is constructed to analyze the stability of the system. For a linear dynamical system, a quadratic function is typically chosen as Lyapunov function in some classic controller design method, such as linear quadratic regulator (LQR) and model predictive control (MPC). Unfortunately, there is no universal method for constructing Lyapunov functions. In this paper, we propose a stability guaranteed reinforcement learning framework to jointly learn the controller or policy 2 and a Lyapunov function both of which are parameterized by deep neural networks, with a focus on stabilization and tracking problems in discrete-time nonlinear stochastic systems modeled by Markov decision process. The contribution of our paper can be summarized as follows: 1) a novel data-based approach for analyzing the stability of the closed-loop system is proposed by constructing a Lyapunov function parameterized by deep neural network; 2) a practical learning algorithm is designed to search the stability guaranteed controller; 3) the learned controller is able to stabilize the system when interfered by uncertainties such as unseen disturbance and system parameters variations of certain extent. In our experiment, we can show that the stability guaranteed controller is more capable of handling uncertainties compared to those without such guarantees in nonlinear control problems including classic CartPole stabilization tasks, control of 3D legged robots and manipulator and reference tracking tasks for synthetic biology gene regulatory networks.

Section Title: RELATED WORKS
  RELATED WORKS In model-free reinforcement learning (RL), stability is rarely addressed due to the formidable chal- lenge of analyzing and designing the closed-loop system dynamics in a model-free manner (Buşoniu et al., 2018), and the associated stability theory in model-free RL remains as an open problem (Buşoniu et al., 2018; Gorges, 2017). Recently, Lyapunov analysis is used in model-free RL to solve control problems with safety constraints (Chow et al., 2018; 2019). In Chow et al. (2018), Lyapunov-based approach for solving constrained Markov decision process is proposed with a novel way of constructing the Lyapunov function through linear programming. In Chow et al. (2019), the above results were further generalized to continuous control tasks. Even though Lyapunov-based methods were adopted in these results, neither of them addressed the stability of the system. As a basic tool in control theory, the construction/learning of Lyapunov function is not a trivial issue and many works are devoted to this problem. In Perkins & Barto (2002), the RL agent controls the switch between controllers designed using Lyapunov domain knowledge, so that any policy is safe and reliable. Petridis & Petridis (2006) proposes a straightforward approach for constructing Lyapunov function for nonlinear systems using neural networks. Richards et al. (2018) proposes a learning-based approach for constructing Lyapunov neural networks with maximized region of attraction. Results on learning and construction of Lyapunov functions are referred to Noroozi et al. (2008); Prokhorov (1994); Serpen (2005); Prokhorov & Feldkamp (1999). Other interesting results on the stability of learning-based control systems are reported in recent years. In Postoyan et al. (2017), an initial result is proposed for the stability analysis of deterministic nonlin- ear systems with optimal controller for infinite-horizon discounted cost, based on the assumption that discount is sufficiently close to 1. In Berkenkamp et al. (2017), a learning model-based safe RL approach with safety guarantee during exploration is introduced but limited to Lipschitz continuous nonlinear systems such as Gaussian process model. In addition, the verification of stability condition requires the discretization of state space, which limits its application to tasks with low-dimensional finite state space.

Section Title: PROBLEM STATEMENT
  PROBLEM STATEMENT We consider discrete-time nonlinear stochastic systems modeled by the Markov decision process (MDP). A MDP is defined by the tuple (S, A, c, P, ρ), where S ⊆ R n is the set of states, A ⊆ R m is the set of actions, c(s, a) : S × A → R + is the cost function, P (s |s, a) is the transition probability function, and ρ(s) is the starting state distribution.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 In this paper, we focus on the stabilization and tracking problems for discrete-time nonlinear stochastic systems modeled by MDP. For both problems, the goal is to find a policy π which can bring the cost c to zero. In stabilization problems, the cost function is defined as the norm of states s where · denotes the Euclidean norm. In tracking problems, we divide the state s into two vectors, s 1 and s 2 , where s 1 is composed of elements of s that are aimed at tracking the reference signal r while s 2 contains the rest. For tracking, s 1 − r is chosen to be the cost function. From a control theoretic perspective, the task of stabilization and tracking could be addressed as ensuring the closed-loop system or error system to be asymptotically stable, i.e., starting from an initial point, the trajectories of state always converge to a single point or the reference trajectory. Let c π (s) E a∼π c(s, a) denote the cost function under policy π, the definition of stability studied in this paper is given as follows.

Section Title: Definition 1
  Definition 1 The stochastic system is said to be stable in mean cost if lim t→∞ E st c π (s t ) = 0 holds for any initial condition s 0 ∈ {s 0 |c π (s 0 ) ≤ b}. If b is arbitrarily large then the stochastic system is globally stable in mean cost. Remark 1 Form of the cost is strictly ruled as the Euclidean norm of state or partial state, while other forms are not considered in this paper. The stability studied in this paper is a type of local asymptotic stochastic stability, which is different to the definition of mean square stability (MSS) that extensively studied on stochastic systems in control theory (Shaikhet, 1997; Huang, 2012). Before proceeding, some notations are to be defined. The closed-loop transition probability is denoted as P π (s |s) A π(a|s)P (s |s, a)da. We also introduce the closed-loop state distribution at certain instant t as P (s|ρ, π, t), which could be defined in an iterative way:

Section Title: MAIN RESULTS
  MAIN RESULTS In this section, we propose the main assumptions and a new theorem.

Section Title: Assumption 1
  Assumption 1 The stationary distribution of state q π (s) lim t→∞ P (s|ρ, π, t) exists.

Section Title: Assumption 2
  Assumption 2 There exists a positive constant b such that ρ(s) > 0, ∀s ∈ {s|c π (s) ≤ b}. Our approach is to construct/find a Lyapunov function which can be used to analyze the stability of the closed-loop system. The Lyapunov method has long been used for stability analysis and controller design in control theory (Boukas & Liu, 2000), but mostly exploited along with a known model, whether deterministic or probabilistic (Corless & Leitmann, 1981; Thowsen, 1983; Huang et al., 2011). The Lyapunov function is a class of semi-positive definite functions L : S → R + . The general idea of exploiting Lyapunov function is to ensure that the difference (or derivative, if the system is in continuous time) of Lyapunov function along the state trajectory is semi-negative definite, so that the state goes in the direction of decreasing the value of Lyapunov function and eventually converges to the origin or a sub-level set of Lyapunov function. Next, we give sufficient conditions for a system to be stable in mean cost in the following. Theorem 1 The stochastic system is stable in mean cost if there exists a function L : S → R + and positive constants α 1 , α 2 and α 3 , such that Due to space limitations, we will include the detailed proof in Appendix A. Eq. (2) is called the energy decreasing condition, i.e., requiring the expectation of Lyapunov function to be decreasing between two consecutive instants. Eq. (1) is the constraint for Lyapunov function, though a rather broad range of parameterization is covered. The sum of quadratic polynomials, e.g., L(s) = s T Qs where Q is a Under review as a conference paper at ICLR 2020 positive definite matrix, are extensively used in the control theory. Such Lyapunov functions can be efficiently discovered by the semi-definite programming solvers and bring in limited conservatism for the control tasks where the cost are also of a quadratic form. In (Richards et al., 2018), a neural network φ θ (·) is designed to construct the Lyapunov function, L(s) = φ θ (s) T φ θ (s). As explored in (Chow et al., 2018) and (Berkenkamp et al., 2017), the value function could be exploited as a Lyapunov function as well. Additionally, the sum of cost over a limited time horizon could also be employed as Lyapunov function, i.e., L(s) = t+N t Ec π (s t ), which is a valid Lyapunov candidate in model predictive control literature (Mayne & Michalska, 1990; Mayne et al., 2000). The choice of Lyapunov function candidate plays an important role in learning a policy. Value function evaluates the infinite time horizon and thus offers a better performance in general, but is rather difficult to approximate because of significant variance and bias (Schulman et al., 2015). On the other hand, the finite horizon sum of cost provides an explicit target for learning a Lyapunov function, thus inherently reduces the bias and enhances the learning process. However, as the model is unknown, predicting the future costs based on the current state and action inevitably introduces variance, which grows as the prediction horizon extends. In principle, for tasks with simple dynamics, the sum-of-cost choice enhances the convergence of learning and robustness of the trained policies, while for complicated systems the choice of value function generally produces better performance. In this paper, we use both value function and sum-of-cost over various horizons as Lyapunov function candidates in different tasks and compare their strength and weakness respectively. Now we would like to give the following two remarks.

Section Title: Remark 2
  Remark 2 This remark is on Assumption 1 and sampling distribution µ π . If an MDP is ergodic then the existence of q π is naturally assured, but all states have to be positive recurrent and aperiodic (Pa- poulis & Pillai, 2002). The existence of sampling distribution µ π (s) is guaranteed by the existence of q π (s). Since the sequence {P (s|ρ, π, t), t ∈ Z + } converges to q π (s) as t approaches ∞, then by the Abelian theorem, the sequence { 1 N N t=0 P (s|ρ, π, t), N ∈ Z + } also converges and µ π (s) = q π (s). Thus we use µ π to approximate the q π since the evaluation of q π requires data to be sampled after infinite instants the episode begins. Even if the sampling period N << ∞, one can still assure that c π converges to a neighborhood of zero, which is related to the initial state distribution and length of Remark 3 This remark is on the connection to previous results concerning the stability of stochastic systems. It should be noted that the stability conditions of Markov chains have been reported in (Shaikhet, 1997; Meyn & Tweedie, 2012), however, of which the validation requires the full knowledge of the model, i.e., the transition probability P (s |s, a). On the contrary, our approach solely depends on data to analyze the stability of the closed-loop system, which further enables the model-free learning algorithms with stability guarantee. However, the validation of stability through a sample-based approach theoretically requires tremendous, if not infinite, amount of samples to thoroughly estimate the distributions, which is the drawback of our approach. We would demonstrate empirically that the algorithm built upon this theorem is reliable though only limited sample is collected.

Section Title: ALGORITHM
  ALGORITHM In this section, we propose an off-policy RL algorithm to learn stability guaranteed policies for discrete-time nonlinear stochastic system modeled by MDP. First, based on the maximum entropy actor-critic framework, we use the Lyapunov function as the critic in the policy gradient formulation. In this algorithm, a Lyapunov critic function L c is needed, which satisfies L(s) = E a∼π L c (s, a). The objective function J(π) is given as follows J(π) = E (s,a,s ,c)∼D [β(log(π θ (f θ ( , s)|s)) + H t ) + λ(L c (s , f θ ( , s )) − L c (s, a) + α 3 c)] (3) where the policy π θ is parameterized by a deep neural network f θ , is an input vector consisted of Gaussian noise. It should be noted that the Lyapunov critic L c (s, a) will be parameterized by the square of a neural network to ensure the semi-positive definiteness of Lyapunov function required in Eq.(1), inspired by the structure explored in Richards et al. (2018). More specifically, L c (s, a) = φ T (s, a)φ(s, a), where φ(s, a) is a multi-layer fully connected neural network. D is the distribution of previously sampled states and actions, or a replay buffer. In the above objective, β Under review as a conference paper at ICLR 2020 and λ are Lagrange multipliers which control the relative importance of policy entropy versus energy decreasing constraint derived from Eq.(2). Similar to Haarnoja et al. (2018), the entropy of policy is expected to remain above the target entropy H t . The parameters of policy network are updated through gradient descent, where the gradient of Eq.(3) is approximated by We use J(L c ) in the following equation as the objective function to update the Lyapunov critic, where L target is the approximation target for L c . If the sum of cost is chosen as Lyapunov function candidate, we have Here, the time horizon N is a hyperparameter to be tuned, of which the influence will be demonstrated in the experiment in Section 5.5. If the value function is chosen as Lyapunov function candidate, L target (s, a) = c + γL c (s , f ( , s )) (7) where L c is the target network parameterized by θ as typically used in the actor-critic meth- ods (Haarnoja et al., 2018; Lillicrap et al., 2015b). L c has the same structure with L c , but the parameter is updated through exponentially moving average of weights of L c controlled by a hyper- parameter τ . In fact, the value function is the discounted sum of cost over infinite time horizon. Later in Section 5, we will show the influence of choosing different Lyapunov function candidates. The value of Lagrange multipliers λ and β are adjusted by the gradient method maximizing the following two objectives respectively, It should be noted that the value of λ and β are clipped to be positive. In addition, to prevent λ from growing unlimitedly causing the algorithm to diverge, we set an upper bound for λ. In our experiments, we found that 1 is a suitable value without much further tuning. Pseudo code of the proposed algorithm is shown in Algorithm 1 in Appendix B. Remark 4 The convergence of the algorithm is composed of the convergence of Lyapunov critic L c and policy π θ respectively. Empirically, the convergence can be judged by the error of Lyapunov function approximation and value of the Lagrange multiplier (close to zero at convergence). In practice, we found that the algorithm converges well in different experiments without much tuning.

Section Title: EXPERIMENT
  EXPERIMENT In this section, we illustrate four simulated examples to demonstrate the general applicability of the proposed method. First of all, the classic control problem of CartPole balancing from control and RL literature (Barto et al., 1983) is illustrated. Then, we consider more complicated high-dimensional continuous control problem of 3D robots, e.g., HalfCheetah and FetchReach, using MuJoCo physics engine (Todorov et al., 2012). Last, we extend our approach to control robots in nanoscale, i.e., molecular robots. Specifically, we consider the problem of reference tracking for a synthetic biology gene regulatory network known as the Repressilator (Elowitz & Leibler, 2000). The proposed method is evaluated for the following aspects: • Convergence: does the proposed training algorithm converge with random parameter initial- ization and does the stability condition (2) hold for the learned policies; • Performance: can the goal of the task be achieved or the cumulative cost be minimized; • Robustness: how do the trained policies perform when faced with uncertainties unseen during training, such as parametric variation and external disturbances; Under review as a conference paper at ICLR 2020 • Generalization: can the trained policies generalize to follow reference signals that are different from the one seen during training. We compare our approach with soft actor-critic (SAC) (Haarnoja et al., 2018), one of the state- of-the-art off-policy actor-critic algorithms that outperform a series of off-policy and on-policy methods such as DDPG (Lillicrap et al., 2015a), PPO (Schulman et al., 2017) on the continuous control benchmarks. The variant of safe proximal policy optimization (SPPO) (Chow et al., 2019), a Lyapunov-based method, is also included in the comparison. The original SPPO is developed to deal with constrained MDP, where safety constraints exist. In our experiments, we modify it to apply the Lyapunov constraints on the MDP tasks and see whether it can achieve the same stability guarantee as LAC. In CartPole example, we also compare with linear quadratic regulator (LQR), a classical model-based optimal control method for stabilization. The outline of this section is as follows. In Section 5.1, a brief introduction will be given on the background and problem description of each example. Then in Section 5.2, the convergence, and performance of the proposed method is demonstrated and compared with SAC. In Section 5.4, the ability of generalization and robustness of the trained policies are evaluated and analyzed. Finally, in Section 5.5, we show the influence of choosing different Lyapunov function candidates upon the performance and robustness of trained policies.

Section Title: BACKGROUND AND PROBLEM DESCRIPTION
  BACKGROUND AND PROBLEM DESCRIPTION In this section, we will give a brief introduction to the examples considered in this paper. Detailed setup information of the first three examples can be found in Appendix C.

Section Title: CARTPOLE
  CARTPOLE This is a classical control problem. The controller is to stabilize the pole vertically at a given position. The cost is determined by the norm of the angular position of the pole and the horizontal position of the cart. The control input is the horizontal force F ∈ [−20, 20] applied in the cart. The agent is dead if the angle θ between pole and vertical position exceeds a threshold, and the episode ends.

Section Title: HALFCHEETAH
  HALFCHEETAH The goal is to control a 17-dimensional 2-legged robot simulated in the MuJoCo simulator. The control task belongs to the reference tracking problem, i.e., to enable the robot to run at the speed of 1m/s in the X-axis direction. The cost is determined by the Euclidean difference between current speed and target speed. The control input is the torque implemented at each joint.

Section Title: FETCHREACH
  FETCHREACH The agent is to control a simulated manipulator to track a randomly generated goal position with its end effector. The cost is determined by the Euclidean distance between end effector and goal. The control input is the torque implemented at each joint. The manipulator is also simulated in the MuJoCo simulator.

Section Title: REPRESSILATOR
  REPRESSILATOR The repressilator is a synthetic biology gene regulatory network with a ring structure pioneered in Elowitz & Leibler (2000), in which each gene represses the other gene cyclically. The dynamics of temporal gene expression exhibit periodic oscillatory behavior. The dynamics of repressilator can be quantitatively described by a set of discrete-time nonlinear difference equations consisting of six states, three mRNAs for transcription and three proteins for translation, based on biochemical kinetic laws. We also include a complicated repressilator example with 4 genes to be controlled, which exhibits an unstable oscillation and is even harder to control. The objective is to force one protein concentrations to follow a priori defined reference trajectories using partially observed states. Detailed setup information of these examples are in Appendix D.

Section Title: MARKOVIAN JUMP SYSTEMS
  MARKOVIAN JUMP SYSTEMS In addition to the systems described above, we introduce two Markovian jump systems (MJS), named MJS1 and MJS2, which contain both discrete switchings (or jumps) and continuous dynamics (Shi & Under review as a conference paper at ICLR 2020 Li, 2015), as test beds for the proposed and baseline methods. The objective is to force the full state to zero. Both MJSs contain unstable subsystems and the dynamics change abruptly and randomly according to the switching signal, and thus are difficult to tackle for the model-free algorithms. Moreover, MJS2 contains an unstable subsystem that is not controllable, which makes it even harder to stabilize. More details on the examples could be found in Appendix E.

Section Title: PERFORMANCE
  PERFORMANCE We parameterize the policy and Lyapunov critic using deep neural networks. For each example,the hyperparameters including time horizon N and DNN architectures selected to construct Lyapunov functions and DNN training parameters can be found in Appendix J. For both algorithms, the hyperparameters are tuned to reach their best performance. In each task, both LAC and SAC are trained for 10 times with random initialization, average total cost and its variance during training are demonstrated in  Figure 1 . In the first three examples (see Figure 1(a)-(c)), SAC and LAC perform comparably in terms of the total cost at convergence and speed of convergence, while SPPO could converge in Cartpole and FetcheReach. In the Repressilator and MJS examples (see Figure 1(d,e,f)), SAC is not always able to find a policy that is capable of completing control objective, resulting in the bad average performance. On the contrary, LAC performs stably regardless of the random initialization. A distinguishing feature of stability assured policy is that it can force and sustain the state or tracking error to zero. This could be intuitively demonstrated by the state trajectories of closed-loop system. We evaluated this property of trained policies in the Repressilator, Complicated-Repressilator and two MJS examples. In our experiments, we found that the LAC agents stabilize the systems well in all tasks. All the state trajectories converge to the reference signal or equilibrium eventually (see Figure 11 (a,c) and Figure 12 (a,c)). On the contrary, without stability guarantee, the state trajectories either diverge (see Figure 11 b and Figure 12 d), or continuously oscillate around the reference trajectory or equilibrium (see Figure 11 d and Figure 12 b). Empirical results are deferred to Appendix F due to space limit.

Section Title: CONVERGENCE
  CONVERGENCE As shown in  Figure 1 , LAC converges stably in all experiments. Moreover, the convergence and validation of stability guarantee could also be checked by observing the value of Lagrange multipliers. When (2) is satisfied, λ will continuously decrease until it becomes zero. Thus by checking the value Under review as a conference paper at ICLR 2020 and variation of λ, the satisfaction of stability condition during training and at convergence could be validated. In  Figure 2 , the value of λ during training is demonstrated. Across all training trials in the experiments, λ converges to zero eventually, which implies that the stability guarantee is valid. A detailed discussion on this is referred to Appendix G.

Section Title: EVALUATION ON ROBUSTNESS AND GENERALIZATION
  EVALUATION ON ROBUSTNESS AND GENERALIZATION It is well-known that over-parameterized policies are prone to become overfitted to a specific training environment. The ability of generalization is the key to the successful implementation of the algorithm in an uncertain real-world environment. In this part, we first evaluate the robustness of policies in the presence of system parametric uncertainties and process noise. Then, we test the robustness of controllers against external disturbances. Finally, we evaluate whether the policy is generalizable by setting different reference signals. To make a fair comparison, we removed the policies that did not converge in SAC and only evaluate the ones that perform well during training. During testing, we found that SPPO appears to be prone to variation in the environment, thus the evaluation results are referred to Appendix H.

Section Title: ROBUSTNESS TO DYNAMIC UNCERTAINTY
  ROBUSTNESS TO DYNAMIC UNCERTAINTY In this part, during the inference, we vary the system parameters in the model/simulator to evaluate the algorithm's robustness against dynamic uncertainty. In the example of CartPole, we vary the length of pole l. In the example of repressilator, we vary the promoter strength a i and dissociation rate K i . Due to stochastic nature in gene expression, we also introduce uniformly distributed noise ranging from [−δ, δ] (we indicate the noise level by δ) to the dynamic of repressilator. The stabilization performance of CartPole and tracking performance of Repressilator by LAC and SAC in the varied environment is demonstrated in  Figure 3 . As shown in Figure 3(a) and (c), the policies trained by LAC are very robust to parametric uncertainties of different values and achieve high tracking precision in each case. On the other hand, though SAC performs well in the original environment (Figure 3(b) and (d)), it fails to track the reference signal in all of the varied environment.

Section Title: ROBUSTNESS TO DISTURBANCES
  ROBUSTNESS TO DISTURBANCES An inherent property of a controller for stabilization is to enable the system to recover to the normal status from perturbations such as external forces and wind. To show this, we introduce persistent external disturbances with different magnitudes in each environment and observe the performance difference between policies trained by LAC and SAC. We also include LQR as the model-based baseline. In CartPole, the agent may fall over when interfered by an external force, ending the episode in advance. Thus in this task, we measure the robustness of controller through the death-rate, i.e., the probability of falling over after being disturbed. For other tasks where the episodes are always of the same length, we measure the robustness of controller by the variation in total cost. Under each disturbance magnitude, the policies are tested for 100 trials and the performance are shown in  Figure 4 . As shown in the  Figure 4 , the controller trained by LAC outperforms SAC and LQR by great extent when faced with external disturbances in CartPole and repressilator (lower death rate and total cost). In the repressilator example, the policies trained by SAC are extremely vulnerable to disturbances, this is potentially due to the existence of an unstable limit cycle in the uncontrolled dynamic (Strelkowa & Barahona, 2010). In HalfCheetah, SAC and LAC are both robust to small external disturbances while LAC is more reliable to larger ones. In FetchReach, SAC and LAC are comparable with maintaining a low cost in a great range of external disturbances. This is perhaps due to the manipulator's inherent robust mechanical design.

Section Title: GENERALIZATION OVER DIFFERENT TRACKING REFERENCES
  GENERALIZATION OVER DIFFERENT TRACKING REFERENCES In this part, we introduce four different reference signals that are unseen during training in the repressilator example: sinusoids with periods of 150 (brown) and 400 (blue), and the constant reference of 8 (red) and 16 (green). We also show the original reference signal used for training (skyblue) as a benchmark. Reference signals are indicated in  Figure 5  by the dashed line in respective colors. Both trained policies are evaluated to track each reference signal for 10 times, and the average dynamics of the target protein concentration are shown in  Figure 5  with the solid line, while the variance of dynamic is indicated by the shadowed area. As shown in  Figure 5 , the policies trained by LAC could generalize well to follow previously unseen reference signals with low deviation (dynamics are very close to the dashed lines), regardless of whether they are in the same mathematical form with the one used for training or not. On the other hand, though SAC tracks the original reference signal well after the unconverged training trials being removed (see the skyblue lines), it is still unable to follow some of the reference signals (see the brown line) and possesses larger variance than LAC when following others.

Section Title: INFLUENCE OF DIFFERENT LYAPUNOV FUNCTION CANDIDATES AND STRUCTURES
  INFLUENCE OF DIFFERENT LYAPUNOV FUNCTION CANDIDATES AND STRUCTURES In this part, we evaluate the influence of choosing different Lyapunov function candidates and network structures. First, we adopt candidates of different time horizon N ∈ {5, 10, 15, 20, ∞} to train policies in the CartPole example, and compare their performance in terms of total cost and robustness. Both of the Lyapunov critics are parameterized by L(s) = φ(s) T φ(s) where φ(s) is a neural network with m dimensional output. Here, N = ∞ implies using value function as Lyapunov candidate. For evaluation of robustness, we apply an impulsive force F at 100 th instant and observe the death-rate of trained policies. The results are demonstrated in  Figure 6 (a,b) . Then we fix the Under review as a conference paper at ICLR 2020 (a) LAC (b) SAC horizon of candidates to be N = 5 but vary the structures of Lyapunov critic, and compare their performance using the same metric as described above. More specifically, these different structures are: As shown in  Figure 6 , in the CartPole environment, both choices of Lyapunov candidates converge fast and achieve comparable total cost at convergence. However, in terms of robustness, the different choices of N play an important role. As observed in  Figure 6 (b) , the robustness of controller decreases as the time horizon N increases. On the other hand, LAC with different structures converge well and possesses similar robustness to impulsive forces. This further proves that our framework allows for a general class of Lyapunov functions, as long as the function is semi-positive definite. Besides, it is interesting to observe that LQR is more robust than SAC when faced with instant impulsive disturbance.

Section Title: CONCLUSIONS
  CONCLUSIONS In this paper, we proposed a model-free approach for analyzing the stability of discrete-time nonlinear stochastic systems modeled by Markov decision process, by employing the Lyapunov function from control theory. Based on the theoretical result, a practical algorithm for designing stability assured controllers for the stabilization and tracking problems. We evaluated the proposed method in various examples and show that our method achieves not only comparable or superior performance compared with the state-of-the-art RL algorithm but also outperforms impressively in terms of robustness to uncertainties and disturbances.
  Controller and policy will be used interchangeably throughout the paper.

```
