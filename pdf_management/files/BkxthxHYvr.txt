Title:
```
Under review as a conference paper at ICLR 2020 CONDITIONAL GENERATION OF MOLECULES FROM DISENTANGLED REPRESENTATIONS
```
Abstract:
```
Though machine learning approaches have shown great success in estimating properties of small molecules, the inverse problem of generating molecules with desired properties remains challenging. This difficulty is in part because the set of molecules which have a given property is structurally very diverse. Treating this inverse problem as a conditional distribution estimation task, we draw upon work in learning disentangled representations to learn a conditional distribution over molecules given a desired property, where the molecular structure is encoded in a continuous latent random variable. By including property information as an input factor independent from the structure representation, one can perform conditional molecule generation via a "style transfer" process, in which we explicitly set the property to a desired value at generation time. In contrast to existing approaches, we disentangle the latent factors from the property factors using a regularization term which constrains the generated molecules to have the property provided to the generation network, no matter how the latent factor changes.
```

Figures/Tables Captions:
```
Figure 1: A demonstration of style transfer Disentangling the latent code z from the property y enables style transfer. This is done by taking an initial x, computing the posterior over the latent variable z, and then generating a new x with the property modified to have a target value y , with p θ (x |y , x) = p θ (x |y , z)p θ (z|x)dz.
Figure 2: Setting and modeling pipeline for conditional generation of molecules, with supervision provided via an external property prediction oracle. Red lines correspond to non-differentiable components, including both a potentially complex sampling process and the property prediction itself. The blue dashed line corresponds to the approximate property predictor, which aims to predict the expected value of the property from a continuous relaxation, marginalized over the sampling process.
Figure 3: Conditional generation given the de- sired logP=-0.5759, row molecules have a logP within a 15% range of the desired one.
Figure 4: Property transfer Before proceeding with the experiments we will give some additional details on how we do conditional generation from p θ (x|y 0 ) given the target property y 0 . Instead of marginalizing over the prior p(z), we mirror the approach taken during training and integrate over an approximation to the marginal inference distribution q φ (z) = 1 N N i=1 q φ (z|x i ) which better characterizes where the mass of the dataset is in the latent space. However, as N is large and we do not wish to keep the entire dataset available at test time, we approximate q φ (z) with an isotropic Gaussian distribution q σ (z) = N (z|0, σ 2 I). We estimate σ for each model by Monte Carlo samples from q φ (z). For the supervised VAE without the soft constraint regularizer this yields 0.053 for QM9 and 0.118 for ZINC. For our model with the soft constraint we get 0.0354 for QM9 and 0.096 for ZINC. We do conditional generation of x given y 0 by sampling from p θ (x|y 0 ) = q σ (z)p θ (x|z, y 0 )dz.
Figure 5: Style transfer. The z of the nine real molecules placed in the x-axis is combined with 11 y property values, sampled in [-4.9, 4.9], the resulting pair is decoded to a molecule.
Figure 6: Property optimization. Given a start molecule (red), we combine its z with 1000 logP values and decode (blue)
Figure 7: A comparison of simulated logP values and Tanimoto similarity to a target on the ZINC dataset. While the stacked LSTM model has high accuracy in terms of matching the desired property, it would require drawing many samples before finding any close matches to any particular desired prototype. The CGD-VAE-3-GRU model represents a middle ground between a standard VAE model which does not condition on the property, and the stacked LSTM model which does not learn a reusable representation.
Table 1: Reconstruction performance and generation quality (Valid, Unique, Novel).
Table 2: Correlation between the desired input property and the obtained property . z ∼q σ (z) corresponding to conditional gen- eration), and x, z ∼ q(z|x) to property trans- fer case.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Conditional molecule generation is far from being solved. The main challenge is the enormous and discrete nature of the molecules space and the fact that molecule properties are highly sensitive to molecular structure ( Kirkpatrick & Ellis, 2004 ). Approaches to conditional generation are typically two-step, either using a model or genetic algorithm to generate candidates which are later filtered, or learning a continuous embedding of the discrete molecules and optimizing in a real-valued representation space. The former is computationally expensive, the latter performs conditional generation only very obliquely. We propose a conditional generative model that produces candidate molecules which targeting a desired property in a single step. This approach builds on work in structured deep generative models ( Kingma et al., 2014 ;  Siddharth et al., 2017 ), which aim to learn a disentangled representation that factors into observed properties we want to control for, and latent factors that account for the remaining features which are either hard to annotate or irrelevant to the properties we wish to optimize. We derive a regularizer for supervised variational autoencoders which exploits property information that we provide as supervision, ensuring that produced molecules adhere to target properties they are conditioned on. We demonstrate the ability of our model to perform accurate conditional molecule generation and a sort of "style transfer" on molecules, where a latent representation for a single molecule can have its target properties perturbed independently of its learnt structural characteristics, allowing direct and efficient generation of candidates for local optimization of molecules.

Section Title: BACKGROUND
  BACKGROUND Molecule discovery tasks come in two flavors. Global optimization seeks to find molecules that have a particular target property. Local optimization starts from some initial molecule and searches for molecules which have a desired property while not straying too far from the prototype. There is some overlap in methods used in the two approaches.

Section Title: DEEP GENERATIVE MODELS FOR MOLECULES
  DEEP GENERATIVE MODELS FOR MOLECULES Virtual screening methods start from a large database of possible molecules and retain the promising ones ( Eckert & Bajorath, 2007 ), as measured by some quality function f (·). Machine learning approaches expand on this by dynamically generating additional candidate molecules;  Segler et al. (2017)  uses a stacked LSTM to produce large numbers of novel molecules which have similar characteristics to an existing database. For properties which are expensive to evaluate, generating large sets of candidate molecules is not particularly useful. More sample-efficient global search can be achieved using Bayesian optimization methods, which use a generative model with a latent space that functions as a continuous repre- sentation of molecules ( Gómez-Bombarelli et al., 2016 ;  Kusner et al., 2017 ). Optimization is then carried out over this continuous representation space to find candidates which are expected to have the desired property. Local gradient-based search can also be applied on continuous latent spaces to optimize the latent representation with respect to a target property ( Jin et al., 2018 ;  Liu et al., 2018 ). A challenge for these latent variable models is to reliably produce valid molecules. Character variational autoencoders (CVAEs) ( Gómez-Bombarelli et al., 2016 ) generate molecules one character at a time, and are prone to syntactic and semantic errors; the grammar-based variational autoencoder (GVAE) ( Kusner et al., 2017 ) and syntax-directed variational autoencoder (SD-VAE) ( Dai et al., 2018 ) instead operate in the space of context-free and attribute grammars, respectively, to ensure syntactic validity. Other work generative models that operates on graph representations ( Simonovsky & Komodakis, 2018 ;  De Cao & Kipf, 2018 ;  Jin et al., 2018 ;  You et al., 2018 ;  Liu et al., 2018 ), largely improving the ability to generate valid molecules. Suppose we are given a training set of pairs D = {(x i , y i )}, i = 1, . . . , N , where x corresponds to molecules and y represents a value of some properties of the molecule x. Assume the molecules represent an i.i.d. sample from some unknown distributionp(x), which assigns high probability to molecules believed to be useful for a given task. Aside from  Segler et al. (2017) , which has no latent space and thus directly trains via maximum likelihood, these latent variable models are trained by optimizing a standard ELBO objective for variational autoencoders (?). This entails learning a stochastic encoder q φ (z|x) which maps molecules into a latent space, and a stochastic decoder p θ (x|z) for reconstructing molecules, by maximizing Notably, the objective is not a function of y: most existing generative models with latent variables do not perform direct conditional generation, and approaches for targeted molecule discovery are bolted on to the learnt model. Some, e.g.  Kusner et al. (2017) , are trained in an "unsupervised" manner, agnostic to any property which later may need to be optimized.  Others, e.g. Gómez-Bombarelli et al. (2016) ;  Liu et al. (2018) , train the autoencoder jointly alongside a function to predict y from z, hoping to guide the latent space to be also good for predicting the desired property. A recent exception is  Assouel et al. (2018) , which learns a deterministic autoencoder where the decoder takes the latent code and the desired property as input, using a mutual information term in training to steer the model towards generating molecules whose target properties match the input.  Guimaraes et al. (2017) ;  De Cao & Kipf (2018) ;  You et al. (2018)  instead learn generation models optimized towards specific metrics, such as drug-likeliness and solubility; the major downside is that these models must be retrained each time for a new property. In contrast, the autoencoder-based methods can be re-used to optimize towards any particular value of the property.

Section Title: STYLE TRANSFER WITH SUPERVISED VAES
  STYLE TRANSFER WITH SUPERVISED VAES While the latent representations learned through standard VAE models perform well on the task of molecule reconstruction they do not necessarily provide interpretable factorised representations. A disentangled representation gives us additional control on the molecule generation process, allowing us to modify a single property leaving the remaining unaffected ( Bengio et al., 2013a ). In many cases important variation in the data is easy to annotate. For example in the case of molecule datasets we have access to different functional descriptors of the molecules obtained by chemoinformatics software such as RDKit (Landrum). Particularly useful to us here are supervised methods for learning disentangled representations ( Kingma et al., 2014 ;  Siddharth et al., 2017 ). These are distinct from Under review as a conference paper at ICLR 2020 unsupervised disentangling approaches such as InfoGAN ( Chen et al., 2016 ) or β-VAE ( Higgins et al., 2017 ), which encourages the latent factor to learn a disentangled representation by modifying the objective to promote component independence. We will learn representations that specifically disentangle molecular properties of interest which we may later want to modify.  Kingma et al. (2014)  demonstrates how disentangling can be used to take two MNIST images of different digits, written in different styles, and independently change the digit while holding the style constant. An analogous operation on molecules would involve holding the physical structure of a molecule (its "style") relatively fixed while modifying a salient property. Unlike (say) the style transfer example for the MNIST digits, the conditional distribution of molecules with a particular value of properties might be very diverse; for example, the QED score attempts to measure the drug-likeness of a molecule, and the set of molecules generated at high values of this score would hopefully have high probability on a large, varied set of molecules. An essential challenge here is that the property only provides a very weak signal as to the overall structure of the molecule. To account for this diversity, we model the conditional distribution with a latent variable z, such that p θ (x|y) = p θ (x|z, y)p(z)dz. Concretely, this involves fitting a joint generative model of the form p θ (x, y, z) = p θ (x|y, z)p(y)p(z), in which y and z are indepen- dent under the prior, and we assume a unit multivariate normal prior p(z). To infer the latent variable z we will use a variational dis- tribution q φ (z|x), which takes the form of a multivariate normal distribution with parameters a nonlinear function of x, to approxi- mate the true posterior p θ (z|x, y). This objective function corresponds to learning a supervised VAE ( Kingma et al., 2014 ), and represents a fairly naïve approach to modeling a conditional distribution.

Section Title: CONDITIONAL GENERATION BY DISENTANGLING
  CONDITIONAL GENERATION BY DISENTANGLING Maximizing this conditional ELBO in Eq (2) will likely yield good reconstructions of molecules from an embedding z (alongside the true property y), but for properties which only weakly inform the generative model there is nothing to enforce that the variable y actually directly has an effect on the generative process. Since the value y is something we know is a derived property of the molecule x, it is completely possible for all information about y to also be encoded in the representation z, in which case there is no guarantee that the learnt likelihood p θ (x|y, z) actually takes into account the value of y - in fact, we know it is possible to fit variational autoencoders where the decoder simply has the form p θ (x|z) - and we are relying on the utility of y in reconstructions to see any sort of disentangling effect.

Section Title: CONSTRAINED ELBO
  CONSTRAINED ELBO In the case of conditional generation of molecules, we often have access to some oracle function f (possibly non-differentiable) which for any given x outputs a property estimate y, for instance, the chemoinformatics software RDKit (Landrum). Since for conditional generation our ultimate goal is to generate a molecule x for any given target property y 0 , which then actually has f (x) = y 0 , we can reframe the problem by introducing hard constraints on the generated values, i.e. if restricting to values of y in the training set, Under review as a conference paper at ICLR 2020 decoder network latent space target property encoder network discrete sampling procedure h : generated molecule f (x) f ! (h) ⇡ E[f (x)] ⇡ y ? g ✓ (z, y ? ) y ? f ! (h) p ✓ (x|z, y ? ) Approximate target property: Non-differentiable property estimation for all i = 1, . . . , N . This is an unreasonably hard constraint, unlikely to be satisfied by any distribution other than one which simply places a point mass on the single training x i associated with y i , but we can relax it by considering that (unlike the molecular space x) the property space y is typically smooth, as many properties are continuous-valued and correspond to a human-interpretable scale. Following  Ma et al. (2018)  and  Hu et al. (2017) , we reframe the constraint as a soft penalty on the ELBO, L(θ, φ) = L ELBO (θ, φ) − λ 1 2 N i=1 Ex ∼p θ (x|yi) f (x) − y i 2 (3) so that they are consistent with the property prediction, i.e., as we have an oracle function f which enable us to access the property of any generated data, we can explicitly add a soft constraint to our loss function to provide explicit guidance for the generative model such that f (x) = y. This constraint is expected to hold for any pair (x, y) we may happen to come across, not just those in the training data. We also show optimizing the relaxed constraint is equivalent to maximizing mutual information with the target y i and generated moleculex; for details see appendix Section 6.1.

Section Title: APPROXIMATING THE PROPERTY PREDICTOR
  APPROXIMATING THE PROPERTY PREDICTOR Introducing the regularizer as in Eq. (3) implicitly guides the reconstruction to take into account the property information, such that the reconstructed data should exhibit properties which match the input properties it is conditioned on. However, existing implementations of f are often non-differentiable or CPU-bound, andx are discrete samples from a categorical distribution, all of which means the gradient of the regularizer can't flow back to the generator. This is outlined in  Figure 2 . To enable the gradient based methods on GPUs during training and avoid discrete sampling, one approach would be to first fit a differentiable approximation to f , and then use either a Gumbel-softmax relaxation ( Jang et al., 2016 ) or tricks like a "straight-through" estimator ( Bengio et al., 2013b ) as a continuous approximation for the discrete samples. Instead, we propose bypassing the discrete sampling step entirely and learning a function f ω that can map from a learned representation of the molecules directly to molecules property ( Hu et al., 2017 ). To do this, we take as input the last hidden layer of the decoder network which parameterizes p θ (x|z, y), denoting this deterministic transformation as g θ (z, y). For the grammar VAE and the syntax-directed VAE, this last layer h = g θ (z, y) is the output of a recurrent layer that generates logits corresponding to unmasked and unnormalized log probabilities for each character at each position in the string; see  Kusner et al. (2017)  and  Dai et al. (2018)  for details on the implementation of the somewhat complex sampling process in the decoder. Ideally, f ω would estimate the property distribution obtained by marginalizing out the discrete sampling step, with f ω (h ≡ g θ (z, y 0 )) ≈ E p θ (x|z,y0) [f (x)], (4) where we condition on z, and y 0 refers to an arbitrary input target property. an expectation over a real-valued variable which does not depend on any of the parameters we are estimating, meaning we can use a simple path estimate of the gradient with respect to θ, ω by exchanging the gradient with the expectation. We thus define a regularization term which can be used as a drop-in replacement for the non-differentiable penalty term in Eq. (3), yielding a candidate objective function

Section Title: LEARNING THE PROPERTY ESTIMATOR JOINTLY WITH GENERATIVE MODEL
  LEARNING THE PROPERTY ESTIMATOR JOINTLY WITH GENERATIVE MODEL While one could imagine attempting to learn f ω jointly with φ, θ by direct optimization of Eq. (6), in practice this is very unstable, as values of g θ (z, y i ) early in training may correspond to very poor generated moleculesx i which may not have properties at all similar to y i . This can be sidestepped by training the property estimator jointly as part of an extended generative model on [x, y]. We note that the property estimator f ω parameterizes a probability distribution p ω (f (x)|z, y 0 ), where x ∼ p θ (x|z, y 0 ) and f is the oracle function that f (x) = y. With a Gaussian distribution over the error, we can consider p ω (f (x)|z, y 0 ) = N (f (x)|f ω (g θ (z, y 0 )), λ −1 2 I) (7) for small, fixed λ 2 . Therefore, we propose defining a new ELBO based on a joint autoencoder for {f (x i ), y i )}, albeit with a factorization such that the input y i bypasses the encoder and is passed directly into the decoder, with a joint likelihood This yields a joint ELBO for the training set of Note that we can rewrite this ELBO as a function of the previous one, with L ELBO (ω, θ, φ) = L ELBO (θ, φ) − λ 2 2 N i=1 E q φ (z|xi) f ω (g θ (z, y i )) − y i 2 2 , (10) where we also see that L ELBO (ω, θ, φ) ≤ L ELBO (θ, φ), allowing us to define an objectivê L(ω, θ, φ) = L ELBO (ω, θ, φ) − L disent (θ, ω), (11) which is a lower bound on Eq. (6). Notice the two terms we have added to the original ELBO are quite similar, differing only in choice of distribution: for learning f ω , we wish to use values of z simulated form the approximate posterior q φ (z|x), whereas for enforcing a constraint across all possible generations we simulate z from the prior p(z).

Section Title: GRADIENT ESTIMATION
  GRADIENT ESTIMATION As the regularizer L disent (θ, ω) encourages disentangling by constraining the molecules generated from y i to have property y i no matter what value z takes, we found that it does not necessarily evaluate at meaningful values of z when sampled randomly from p(z). This roughly corresponds to the notion that not all combinations of "style" and property are physically attainable; ideally for style transfer we would like the generated molecule to stay "close" in structure to the original molecule that we intended to modify. When estimating (gradients of) the soft constraint term L disent (θ, ω), we found it advantageous to use samples of z which correspond to encodings of actual data points, as Under review as a conference paper at ICLR 2020 opposed to random samples from the prior. We approximate expectations with respect to p(x) by looking at the so-called marginal posterior; we note that where the first approximation uses the empirical data distribution as an approximation to the model marginal p θ (x), and the second uses our variational posterior approximation q φ (z|x). We define this quantity as q(z) = 1 N j q φ (z|x j ), a mixture of Gaussians, which we can sample from by drawing random values from our dataset and then drawing from their encoding distributions. When we use this in estimating gradients of the soft constraint, we can use samples from the same minibatch, exactly corresponding to a property transfer task. That is, for any particular y i in the dataset, we can estimate for any uniformly randomly sampled j = i. By sampling z j from q(z j |x j ) where j = i, we make sure that all the label information decoder is receiving comes from the actual y i that is feed to the decoder and z j does not include any information about label. This can be evaluated easily by simply evaluating the penalty term of Eq. (10) twice per minibatch; once as in Eq. (10), and once to approximate L disent (θ, ω) by permuting the properties in the minibatch to be assigned to incorrect molecules. We detail the training algorithm in Section 6.2 of the appendix.

Section Title: EXPERIMENTS
  EXPERIMENTS We experiment with the QM9 dataset ( Ramakrishnan et al., 2014 ), that contains 134k molecules with up to 9 heavy atoms, and the ZINC dataset ( Sterling & Irwin, 2015 ) containing 250k drug- like molecules. Our goal here is two-fold: we would like to understand (1) whether a supervised variational autoencoder is capable of learning suitable conditional distributions over molecules, and (2) to what extent this task is assisted by the additional regularization term corresponding to the soft constraint. We represent molecules using the one-hot encoding of their SMILES production rules ( Kusner et al., 2017 ) and add a semantic constraint ( Dai et al., 2018 ) on the decoder network to avoid generating syntactically correct but semantically invalid molecules. We use 80 production rules to describe molecules and set the maximum SMILES sequence length to 100 for the QM9 dataset and 278 for the Zinc dataset. We experiment with the logP property of the molecules ( Wildman & Crippen, 1999 ). We use the same encoder and decoder network structure as  Dai et al. (2018)  with the only difference that our decoder takes as input the concatenation of y, z. We give the details of the architecture in the appendix section 6.2. We evaluate the reconstruction accuracy and the quality of the molecules generated by our method, which we denote by CGD-VAE (conditional generation with disentangling) and compare against CVAE ( Gómez-Bombarelli et al., 2016 ), GVAE ( Kusner et al., 2017 ), and SD-VAE ( Dai et al., 2018 ). We explore its conditional generation performance in two settings: controlling only the property value and controlling both the property value and the molecule structure to what can be seen as property transfer. We took the results of CVAE, GVAE from the literature. For SD-VAE we used the authors code with the default values to generate results for QM9 since these were not available for QM9. We also implemented supervised VAE versions of SD-VAE which we denote Sup-VAE-X-GRU (X ∈ {1, 3}, denotes GRU layers) and which can do conditional generation. We evaluate reconstruction performance in terms of the correctly reconstructed molecules on test sets of size 10k for QM9 and 5k for ZINC, for the latter we used the default test set. We evaluate the generated molecules' quality by the percentage of valid, unique (i.e. percentage of unique molecules among the generated valid molecules) and novel (i.e. percentage of molecules never seen in the training set among the generated molecules) molecules. We estimate these quantities by sampling 10k (5K for ZINC) z from theq σ (z) and coupling each one of them with a logP value, y, randomly selected from the test set, and we subsequently decode the z, y concatenation. We can see that our model has a better reconstruction performance compared to the baselines while in some cases generating slightly less valid molecules  table 1 . In terms of the three quality measures achieves an excellent performance across all three metrics being always one of the two best performing methods for any metric. To visualise how the conditional generation operates we randomly sample from the test set some molecule and obtain its property value y 0 . We then draw 50 random samples z i fromq σ (z) and decode the [z i , y 0 ] vectors. Among the generated valid molecules we compute the percentage of those that have a property value y i that is within a 15% range from the y 0 property value. In  Figure 3  we present the molecules obtained for a test molecule that had a logP of −0.5759. Out of the 50 generated molecules 46 were valid of which we give the five that were within a 15% range from the y 0 value in  Figure 3 . As we can see we get molecules that are structurally very different from the original one yet they have similar logP value. To quantify the quality of the conditional generations we measure the correlation between the property value we obtain by the conditional generation and the property value on which we conditioned the generation. We randomly sample 1000 y values from the test set and 1000 z values from the approximate learned priorq σ (z). We decode each pair, obtainx ∼ p θ (x|y, z), and then measure the correlation of the original y with theŷ of generatedx. In  Table 2 , we give the correlation estimates for our method and the Sup-VAE baselines. As we can see our method has a considerably higher correlation score between the input and the obtained property than Sup-VAE. Conditional generation seems considerably harder for the ZINC dataset for all methods. To visualise the style transfer behavior of our model we randomly sample two molecules x A , x B from the test set. We then sample z A from the learned posterior q φ (z|x A ). We subsequently decode [z A , y B ], y B is the property of x B , and get a new moleculex AB . Ideally, the obtained moleculê x AB should have a property value (logP) close to the target y B and be similar to x A . In  Figure 4  we give one such example. To put the results into context in Figure 8 in appendix, we give the results of a virtual screening method, where we select from the full dataset five molecules which are structurally Under review as a conference paper at ICLR 2020 similar to x A and have logP values close to y B . As we can see the molecule that our model generates is a new one. To quantify the style transfer performance we proceed in exactly the same manner as we did to quantify the conditional generation performance. However, now instead of sampling z from the approximate learned prior,q σ (z), we first sample some x from the test set and then we sample z from the learned posterior q(z|x). The results are in the second column of  Table 2 . As we can see the correlation values are now lower than the ones we obtained in the simple conditional generation case. This can be explained by the fact that now we are forcing a specific combination of structure, z comes from a real molecule, and property, which might simply be physically infeasible since the molecule space is discrete and not all combinations are possible. In addition, as it was the case for the conditional generation, style transfer is considerably more difficult for the ZINC dataset. We further explore the style transfer and visualize how our model covers the combined space of molecule structure and properties. We sample nine molecules from the QM9 test set, and get their z encodings. For each such encoding we decode the vectors [z, y], y ∈ [−4.9, 4.9], with the y (logP) interval sampled at 11 points. We give in  Figure 5  the resulting valid molecules, each column there corresponds to one of the nine original molecules, the ones surrounded by dotted rectangle, and their decodings with different logP values.For each original molecule we give the generated molecules ordered along the y axis according to the y property that they actually exhibit. The x-axis does not provide an ordering of the original molecules according to z, in fact we have ordered the original molecules by their y property. As we can see not all (z, y) combinations produce a result. These holes can be explained either by the physical infeasibility of the combination and/or a limitation of the learned model. We can use conditional generation to control in a fine manner the value of the desired property, to what can be seen as direct property optimization. We visualise the level of control we have on an experiment with a single molecule (with logP is -1.137), which we randomly sample from the test set. We obtain its z encoding and perform generations with increased logP taking values in 1000 point grid in [−1.137, 4.9]. We then decode [z, y i ] and compute the logP value of the generated molecules. Among the 1000 generated molecules only 19 are unique. We get an increase of logP of a very discrete nature,  Figure 6 . As already discussed not all combinations of structure and properties are possible. The generated molecules themselves are shown in the supplemental material.

Section Title: CONDITIONAL LSTM BASELINE
  CONDITIONAL LSTM BASELINE Finally, we consider a variant of the stacked LSTM model of  Segler et al. (2017) , with no latent space, where the model is modified to take a target logP value as an additional input at each generation step. This model forms a very strong baseline for many distribution matching tasks ( Liu et al., 2018 ; ?), though as best we are aware this has never been used directly for conditional generation given a target property. We use a modification of the implementation provided by (?) with three layers and default settings, and fit the model by maximum likelihood training on p θ (x|y) = T t=1 p θ (x t |x 1:t−1 , y). Training on the ZINC dataset, we find the generated molecules from this model have a very high correlation 0.975 with the target logP value, greatly outperforming any of the latent variable models we consider. This suggests that such a model would be very useful for generating candidates globally, but as the model has no latent variable it is not amenable to style transfer. We observe this in  Figure 7 , which samples 100 candidate molecules from both the stacked LSTM model and for CGD-VAE-3- GRU, conditioning on the property of one randomly-chosen test set example, while computing the Tanimoto similarity (computed using Morgan fingerprints of radius 2) to a second randomly-chosen test set example, across 200 pairs. The VAE has higher Tanimoto similarities as it can condition on the latent variable of the target molecule, representing a trade-off against the better adherence to the target property value of the unconditioned LSTM.

Section Title: CONCLUSION
  CONCLUSION We presented a single step approach for the conditional generation of molecules with desired properties. Our model allows also to condition generation on a prototype molecule with a desired high-level structure. This work thus directly inverts the traditional relationship between molecules and their properties. We found that training the deep generative models conditional on target properties, following a supervised VAE approach, does not appreciably harm the quality of the unconditional Under review as a conference paper at ICLR 2020 generative model as measured by validity, novelty, and uniqueness of samples. Furthermore, we see that the additional act of regularizing the output using an approximate property predictor helps improve both reconstruction accuracy and property correlations in most combinations of tasks and datasets, particularly for the smaller QM9 dataset and for smaller models with fewer RNN layers. We also note that although none of the deep latent variable models are competitive with an LSTM baseline when purely considering generation conditioned on a target property value, the low Tanimoto similarity between randomly sampled candidates and an arbitrary style transfer target makes clear that such a model is not suitable for targeted generation of candidates which are close in structure to a particular prototype. In future work, we want to explore how to further improve the correlation between the desired input properties to the decoder, and the properties of the generated molecules. Moreover, we want also to condition on multiple properties; while this is in principle possible in our framework, we do not explore it empirically here. Modifying a single property while constraining the remaining to be close to the original can further aggravate the infeasibility problem, as not all combinations of molecular properties may even be feasible, perhaps requiring learning a dependency structure between multiple properties.

```
