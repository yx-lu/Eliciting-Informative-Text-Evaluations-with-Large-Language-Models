Title:
```
The University of Texas at Austin ztang@cs.utexas.edu
```
Abstract:
```
Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018a) proposed an approach that substantially reduces the variance of infinite horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing biases due to errors in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to improve accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect. In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.
```

Figures/Tables Captions:
```
Figure 1: Off Policy Evaluation Results on Taxi. Default parameter, discounted factor γ = 0.99, mixed ratio α = β = 1, horizon length H = 600. For (a)-(c) the x-axis is the number of trajectories and y-axis corresponds to MSE, Bias Square and Variance, respectively. For (d) we fix the total number of samples (number of trajectories times horizon length) and change the horizon length as x-axis and observe the MSE. (e) and (f) show the change the mixed ratio of α, β with the change of bias. We repeat each experiment for 1000 runs.
Figure 2: Off Policy Evaluation Results on Puck-Mountain. We set discounted factor γ = 0.995 as default. For (a)-(c) we set the horizon H = 1000 and the x-axis is the number of trajectories for used for evaluation. For (d) we fix the total number of samples and change the horizon length.
Figure 3: Off Policy Evaluation Results on InvertedPendulum-v2. We set discounted factor γ = 0.995 as default. For (a)-(c) we set the horizon H = 1000 and the x-axis is the number of trajectories for used for evaluation. For (d) we fix the total number of samples and change the horizon length.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION A key problem in reinforcement learning (RL) (Sutton & Barto, 1998) is off-policy policy evaluation: given a fixed target policy of interest, estimating the average reward garnered by an agent that follows the policy, by only using data collected from different behavior policies. This problem is widely encountered in many real-life applications (e.g.,  Murphy et al., 2001 ;  Li et al., 2011 ;  Bottou et al., 2013 ;  Thomas et al., 2017 ), where online experiments are expensive and high-quality simulators are difficult to build. It also serves as a key algorithmic component of off-policy policy optimization (e.g.,  Dudík et al., 2011 ;  Jiang & Li, 2016 ;  Thomas & Brunskill, 2016 ;  Liu et al., 2019b ). There are two major families of approaches to policy evaluation. The first approach is to build a simulator that mimics the reward and next-state transitions of the real environment (e.g.,  Fonteneau et al., 2013 ). While straightforward, this approach strongly relies on the model assumptions in building the simulator, which may invalidate evaluation results. The second approach is to use importance sampling to correct the sampling bias in off-policy data, so that an (almost) unbiased estimator can be obtained ( Liu, 2001 ; Strehl et al., 2010;  Bottou et al., 2013 ). A major limitation, however, is that importance sampling can become inaccurate due to high variance. In particular, most existing IS-based estimators compute the weight as the product of the importance ratios of many steps in the trajectory, causing excessively high variance for problems with long or infinite horizon, yielding a curse of horizon ( Liu et al., 2018a ). Recently,  Liu et al. (2018a)  proposes a new estimator for infinite-horizon off-policy evaluation, which presents significant advantages to standard importance sampling methods. Their method directly estimates the density ratio between the state stationary distributions of the target and behavior policies, instead of the trajectories, thus avoiding exponential blowup of variance in the horizon. While Liu et al.'s method shows much promise by significantly reducing the variance, in practice, it may suffer from high bias due to the error or model misspecficiation when estimating the density ratio function.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 In this paper, we develop a doubly robust estimator for infinite horizon off-policy estimation, by integrating Liu et al.'s method with information from an additional value function estimation. This significantly reduces the bias of Liu et al.'s method once either the density ratio, or the value function estimation is accurate (hence doubly robust). Since Liu et al.'s method already promises low variance, our additional bias reduction allows us to achieve significantly better accuracy for practical problems. Technically, our new bias reduction method provides a new angle of double robustness for off-policy evaluation, orthogonal to the existing literature of doubly robust policy evaluation that solely devotes to variance reduction ( Jiang & Li, 2016 ;  Thomas & Brunskill, 2016 ;  Farajtabar et al., 2018 ), mostly based on the idea of control variates (e.g.  Asmussen & Glynn, 2007 ). Our double robustness for bias reduction is significantly different, and instead yields an intriguing connection with the fundamental primal-dual relations between the density (ratio) functions and value functions (e.g.,  Bertsekas, 2000 ; Puterman, 2014). This new perspective can inspire more efficient algorithms for policy evaluation, and lead to unified frameworks for these types of double robustness in future work.

Section Title: BACKGROUND
  BACKGROUND Infinite Horizon Off-Policy Estimation Let M = S, A, r, T , µ 0 be a Markov decision process (MDP) with state space S, action space A, reward function r, transition probability function T , and initial-state distribution µ 0 . A policy π maps states to distributions over A, with π(a|s) being the probability of selecting a given s. The average discounted reward for π, with a given discount γ ∈ (0, 1) 1 , is defined as R π := lim T →∞ E τ ∼π T t=0 γ t r t T t=0 γ t , where τ = {s t , a t , r t } 0≤t≤T is a trajectory with states, actions, and rewards collected by fol- lowing policy π in the MDP, starting from s 0 ∼ µ 0 . Given a set of n trajectories, D = {s (i) t , a (i) t , r (i) t } 1≤i≤n,0≤t≤T , collected under a behavior policy π 0 (a|s), the off-policy evaluation problem aims to estimate the average discounted reward R π for another target policy π(a|s).

Section Title: Estimation via Value Function
  Estimation via Value Function The value function for policy π is defined as the expected accu- mulated discounted future rewards started from a certain state: We use r π (s) = E a∼π(·|s) [r(s, a)] to denote the average reward for state s given policy π. Under the definition, the value function can be seen as a fixed point of the Bellman equation: V π (s) = r π (s) + γP π V π (s), P π V π (s) := E a∼π(·|s),s ∼T (·|s,a) [V π (s )], ∀s ∈ S, (1) where P π V (s) is the average of the next value function given the current state s and policy π; see Appendix A.1 for details. The value function and the expected reward R π is related in the following straightforward way R π = (1 − γ)E s∼µ0 [V π (s)], (2) where the expectation is with respect to the distribution µ 0 (s) of the initial states s 0 at time t. Therefore, given an approximation V of V π , and samples D 0 := {s (i) 0 } 1≤i≤n0 drawn from µ 0 (s), we can estimate R π by Note that this estimator is off-policy in nature, since it requires no samples from the target policy π. Estimation via State Density Function Denote d π,t (·) as average visitation of s t in time step t. The state density function, or the discounted average visitation, is defined as: Published as a conference paper at ICLR 2020 where (1 − γ) can be viewed as the normalization factor introduced by ∞ t=0 γ t . Similar to Bellman equation for value function, the state density function can also be viewed as a fixed point to the following recursive equation ( Liu et al., 2018a , Lemma 3): The operator T π is an adjoint operator of P π used in (1); see Appendix A.1 for a discussion. If the density function d π is known, it provides an alternative way for estimating the expected reward R π , by noting that We can see that both density function d π and value function V π can be used to estimate the expected reward R π . We clarify the connection in detail in Appendix A.1. Off-Policy State Visitation Importance Sampling Equation (4) can not be directly used for off- policy estimation, since it involves expectation under the behavior policy π.  Liu et al. (2018a)  addressed this problem by introducing a change of measures via importance sampling: where w π/π0 (s) is the density ratio function of d π and d π0 . Given an approximation w of w π/π0 , and samples D = {s (i) t , a (i) t , r (i) t } 1≤i≤n,0≤t≤T collected from policy π 0 , we can estimate R π as: where Z is the normalized constant of the importance weights.

Section Title: DOUBLY ROBUST ESTIMATOR
  DOUBLY ROBUST ESTIMATOR Doubly robust estimator is first proposed into reinforcement learning community to solve contextual bandit problem by  Dudík et al. (2011)  as an estimator combining inverse propensity score (IPS) estimator and direct method (DM) estimator.  Jiang & Li (2016)  introduce the idea of doubly robust estimator into off-policy evaluation in rein- forcement learning. It incorporates an approximate value function as a control variate to reduce the variance of importance sampling estimator. Inspired by previous works, we propose a new doubly robust estimator based on our infinite horizon off-policy estimator R π SIS .

Section Title: DOUBLY ROBUST ESTIMATOR FOR INFINITE HORIZON MDP
  DOUBLY ROBUST ESTIMATOR FOR INFINITE HORIZON MDP The value-based estimator R π VAL [ V ] and density-ratio-based estimator R π SIS [ w] are expected to be accurate when V and w are accurate, respectively. Our goal is to combine their advantages, obtaining a doubly robust estimator that is accurate once either V or w or is accurate. To simplify the problem, it is useful to examine the limit of infinite samples, with which R π VAL [ V ] and R π SIS [ w] converge to the following limit of expectations: Here and throughout this work, we assume V and w are fixed pre-defined approximations, and only consider the randomness from the data D. A first observation is that we expect to have r π ≈ V − γP π V by Bellman equation (1), whenV approximates the true value V π . Plugging this into R π SIS [ w] in Equation (7), we obtain the following "bridge estimator", which incorporates information from both w and V : R π bridge [ V , w] = s V (s) − γP π V (s) d π0 (s) w(s), (9) where operator P π is defined in Bellman equation (1). The corresponding empirical estimator is defined by t ) are self- normalized constant of important weights each empirical estimation. However, directly estimating R π using the bridge estimator R π bridge [ V , w] yields a poor estimation, because it includes the errors from both w and V and can be "doubly worse". However, we can construct our "doubly robust" estimator by canceling R π bridge [ V , w] out from R π SIS [ w] and R π VAL [ V ]: And its corresponding empirical estimator can be written as:

Section Title: Doubly Robust Bias Reduction
  Doubly Robust Bias Reduction The double robustness of R π DR [ V , w] is reflected in the following key theorem, which shows that it is accurate once either V or w is accurate. Theorem 3.1 (Doubly Robustness). Let R π DR [ V , w] := lim n0,n,T →∞ R π DR [ V , w] be the limit of R π DR when it has infinite samples. Following the definition above, we have where ε V and ε w are errors of V and w, respective, defined as follows The error ε w of w is measured by the difference with the true density ratio d π (s)/d π0 (s), and the error ε V of V is measured using the Bellman residual. From the theorem we can see that, if V is exact ( V ≡ V π ), we have ε V ≡ 0; if w is exact ( w ≡ d π /d π0 ), we have ε w ≡ 0. Therefore, our estimator is consistent (i.e., lim n,n0→∞ R π DR [ V , w] = R π ) if either V or w are exact. The estimator is thus doubly robust in this sense. In contrast, R π SIS [ w] and R π VAL [ V ] can be more sensitive to the error of w and V , respectively:

Section Title: Variance Analysis
  Variance Analysis Different from the bias reduction, the doubly robust estimator does not guar- antee to the reduce the variance over R π SIS [ w] or R π VAL [ V ] in general. However, as we show in the following result, we can break the variance of R π DR [ V , w] into two parts. The first is the variance of R π VAL [ V ] which is often relatively small. The second is generally no greater than the variance of R π SIS [ w], and can be much smaller when V ≈ V π . Moreover, R π VAL [ V ] and R π SIS [ w] avoid the curse of horizon by design, so their variances tend to be much smaller than the corresponding estimators that apply IPS correction on the trajectories. Therefore, Var Dπ 0 ( R π res [ V , w]) can be small when V is close to the true value V π , or ε V (s) ≈ 0. From the proposition we can see that when V is close to the true value V π , the variance of the residual ε V may be negligibly small compared to the variance of R π SIS [ w]. A further comparison of the variances between R π res [ V , w] and R π SIS [ w] is provided in Appendix A.3. In the case when the TD error εV is negligible, we have Var D0,Dπ 0 R π DR [ V , w] ≈ Var D0 R π VAL [ V ] . Typically, the variance of R π VAL [ V ] is smaller than R π SIS [ w] since the variance of importance sampling methods heavily depends on the effective sample size, which is less controllable compared to R π VAL [ V ]. Therefore, the variance of our doubly robust estimator may even be smaller than that of R π SIS [ w] in practice. The variance in (13) is a sum of two terms because of the assumption that samples from µ 0 and d π0 are independent. In practice, they have dependency but it is possible to couple the samples from µ 0 and d π0 in a certain way to further decrease the variance, which we leave as future work.

Section Title: Proposed Algorithm for Off-Policy Evaluation
  Proposed Algorithm for Off-Policy Evaluation Suppose we have already obtained V and w, as estimations of V π and w π/π0 , respectively, we can directly use equation (11) to estimate R π . A detail procedure is described in Algorithm 1.

Section Title: DOUBLE ROBUSTNESS AND LAGRANGIAN DUALITY
  DOUBLE ROBUSTNESS AND LAGRANGIAN DUALITY In this section, we reveal a surprising connection between our double robustness and Lagrangian duality. We show that our doubly robust estimator is equivalent to the Lagrangian function of a primal-dual formulation of policy evaluation. This connection is of interest in itself, and may provide a foundation for deriving more effective algorithms. We start with the following classic, optimization formulation of policy evaluation (Puterman, 2014): where we find V to maximize its average value, subject to an inequality constraint on the Bellman equation. It can be shown that the solution of (14) is achieved by the true value function V π , hence yielding a true expected reward R π . Introducing a Lagrangian multiplier ρ ≥ 0, we can derive the Lagrangian function L(V, ρ) of (14), Comparing L(V, ρ) with our estimator R π DR [ V , w] in (11), we can see that they are in fact equivalent in expectation. which suggests that L(V, ρ) is "doubly robust" in that it equals R π if either V = V π or ρ = d π . II) The primal problem (14) forms a strong duality with the following dual problem, This shows that the dual problem is equivalent to constraint ρ using the fixed point equation (3) and maximize the average reward given distribution ρ. Since the unique fixed point of (3) is d π (s), the solution of (16) naturally yields the true reward R π , hence forming a zero duality gap with (14). It is natural to intuit the double robustness of the Lagrangian function. From (15), L(V, ρ) can be viewed as estimating the reward R π using value function with a correction of Bellman residual (V − r π − γP π V ). If V = V π , the estimation equals the true reward and the correction equals zero. From the dual problem (16), L(V, ρ) can be viewed as estimating R π using density function ρ, corrected by the residual (ρ − (1 − γ)µ 0 − γT π ρ). We again get the true reward if ρ = d π . It turns out that we can use the primal-dual formula when γ = 1 to obtain the double robust estimator for the average reward case. We clarify it in appendix B.

Section Title: Remark
  Remark The fact that the density function d π forms a dual variable of the value function V π is widely known in the optimal control and reinforcement learning literature (e.g.,  Bertsekas, 2000 ; Puterman, 2014;  de Farias & Van Roy, 2003 ), and has been leveraged in various works for policy optimization. However, it does not seem to be well exploited in the literature of off-policy policy evaluation.

Section Title: RELATED WORK
  RELATED WORK

Section Title: Off-Policy Value Evaluation
  Off-Policy Value Evaluation ) have been proposed to address the high variance issue in the long-horizon problems.  Liu et al. (2018a)  apply importance sampling on the average visitation distribution of state-action pairs, instead of the distribution of the whole trajectories, providing an effective approach to break "the curse of horizon". However, they require to learn a density ratio function over the whole state-action pairs, which may induce large bias. Our work incorporates the density ratio and value function estimation, which significantly reduces the induced bias of two estimators, resulting a doubly robust estimator. Our work is also closely related to DR techniques used in finite horizon problems ( Murphy et al., 2001 ;  Dudík et al., 2011 ;  Jiang & Li, 2016 ;  Thomas & Brunskill, 2016 ;  Farajtabar et al., 2018 ), which incorporate an approximate value function as control variates to IS estimators. Different from existing DR approaches, our work is related to the well known duality between the density and the value function, which reveals the relationship between density (ratio) learning ( Liu et al., 2018a ) and value function learning. Based on this interesting observation, we further obtain the doubly robust estimator for estimating average reward in infinite-horizon problems. A recent work (Xie et al., 2019) has also explored the idea of tracking marginal state distribution shifts at every single time step in an episode, instead of the stationary state distribution across all time steps ( Liu et al., 2018a ). It provides additional benefits for understanding and analyzing problems Published as a conference paper at ICLR 2020 such as short horizon and time-variant MDPs. They call this approach marginalized importance sampling (MIS). Later on,  Kallus & Uehara (2019a)  incorporate the DR technique to improve the MIS estimator. Independent of this work,  Kallus & Uehara (2019b) ; Uehara et al. (2019) extend previous works to propose estimators can be viewed as a variant of ours. Primal-Dual Value Learning Primal-dual optimization techniques have been widely used for off-policy value function learning and policy optimization ( Liu et al., 2015 ;  Chen & Wang, 2016 ;  Dai et al., 2017 ; 2018;  Feng et al., 2019 ). Nevertheless, the duality between density and value function has not been well explored in the literature of off policy value estimation. Our work proposes a new doubly robustness technique for off-policy value estimation, which can be naturally viewed as the Lagrangian function of the primal-dual formulation of policy evaluation, providing an alternative unified view for off policy value evaluation.

Section Title: EXPERIMENT
  EXPERIMENT In this section, we conduct simulation experiments on different environmental settings to compare our new doubly robust estimator with existing methods. We mainly compare with infinite horizon based estimator including state importance sampling estimator ( Liu et al. (2018a) ) and value function estimator. We do not report results on the vanilla trajectory-based importance sampling estimators because of their significant higher variance, but we do compare with the doubly robust version induced by  Thomas & Brunskill (2016)  (self-normalized variant of  Jiang & Li (2016) ). In all experiments we compare with Monte Carlo and naive average as  Liu et al. (2018a)  suggested. The ground truth for each environment is calculated by averaging Monte Carlo estimation with a very large sample size.

Section Title: Taxi Environment
  Taxi Environment We follow  Liu et al. (2018a) 's tabular environment Taxi, which has 2000 states and 6 actions in total. For more experimental details, please check appendix C.1. We pre-train two different V andṼ trained with a small and fairly large size of samples, respectively, whereṼ is very close to true value function V π but V is relatively further from it. Similarly we pre-train ρ andρ ≈ d π . For estimation we use a mixed ratio α, β to control the bias of the input V, ρ, where V = α V + (1 − α)Ṽ and ρ = β ρ + (1 − β)ρ. Figure 1(a)-(c) show results of comparison for different methods as we changing the number of trajectories. We can see that the MSE performance of value function( R π VAL ) and state visitation importance sampling( R π SIS ) estimators are mainly impeded by their large biases, while our method has much less bias thus it can keep decreasing as sample size increase and achieves same performance as on policy estimator. Figure 1(d) shows results if we change the horizon length. Notice that here we keep the number of samples to be the same, so if we increase our horizon length we will decrease the number of trajectories in the same time. We can see that our method alongside with all infinite horizon methods will get better result as horizon length increase. Figure 1(e)-(f) indicate the "double robustness" of our method, where our method benefits from either a better V or a better ρ. Puck-Mountain Puck-Mountain is an environment similar to Mountain-Car, except that the goal of Puck-Mountain is to push the puck as high as possible in a local valley, which has a continuous state space of R 2 and a discrete action space similar to Mountain-Car. We use the softmax functions of an optimal Q-function as both target policy and behavior policy, where the temperature of the behavior policy is higher (encouraging exploration). For more details of constructing policies and training algorithms for density ratio and value functions, please check appendix C.2. Figure 2(a)-(c) show results of comparison for different methods as we changing the number of trajectories. Similar to taxi, we find our method has much lower bias than density ratio and value function estimation, which yields a better MSE. In Figure 2(d) the performance for all infinite horizon estimator will not degenerate as horizon increases, while finite horizon method such as finite weighted horizon doubly robust will suffer from larger variance as horizon increases. InvertedPendulum InvertedPendulum is a pendulum that has its center of mass above its pivot point. We use the implementation of InvertedPendulum from OpenAI gym ( Brockman et al., 2016 ), which is a continuous control task with state space in R 4 and we discrete the action space to be {−1, −0.3, −0.2, 0, 0.2, 0.3, 1}. More experiment details can be found in appendix C.2. In Figure 3(a)-(c) our method again significantly reduces the bias, which yields a better MSE comparing with value and density estimation. Figure 3(d) also shows that our method consistently outperforms all other methods as the horizon increases with a fixed total timesteps.

Section Title: CONCLUSION
  CONCLUSION In this paper, we develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation. Our new proposed doubly robust estimator can be accurate as long as one of the estimators are accurate, which yields a significant advantage comparing to previous estimators. Future directions include deriving more novel optimization algorithms to learn value function and density(ratio) function by using the primal dual framework.
  For average case when γ = 1, the definition of R π is the same. However, the definition of value function is different. We will assume γ < 1 throughout our main paper for simplicity; for average case check appendix B for more details.

```
