Title:
```
Published as a conference paper at ICLR 2020 TARGET-EMBEDDING AUTOENCODERS FOR SUPERVISED REPRESENTATION LEARNING
```
Abstract:
```
Autoencoder-based learning has emerged as a staple for disciplining representations in unsupervised and semi-supervised settings. This paper analyzes a framework for improving generalization in a purely supervised setting, where the target space is high-dimensional. We motivate and formalize the general framework of target- embedding autoencoders (TEA) for supervised prediction, learning intermediate latent representations jointly optimized to be both predictable from features as well as predictive of targets-encoding the prior that variations in targets are driven by a compact set of underlying factors. As our theoretical contribution, we provide a guarantee of generalization for linear TEAs by demonstrating uniform stability, in- terpreting the benefit of the auxiliary reconstruction task as a form of regularization. As our empirical contribution, we extend validation of this approach beyond exist- ing static classification applications to multivariate sequence forecasting, verifying their advantage on both linear and nonlinear recurrent architectures-thereby under- scoring the further generality of this framework beyond feedforward instantiations.
```

Figures/Tables Captions:
```
Figure 1: (a) Feature-embedding and (b) Target-embedding autoencoders. Solid lines correspond to the (prim- ary) prediction task; dashed lines to the (auxiliary) reconstruction task. Shared components are involved in both.
Figure 2: Functions and objectives in (a) FEAs and (b) TEAs. Blue and red identify supervised and representa- tion learning components. FEAs are parameterized by (Φ, W d , Wr) of (φ, d, r), and TEAs by (Θ, Wu, We) of (θ, u, e). Solid lines indicate forward propagation of data; dashed lines indicate backpropagation of gradients.
Figure 3: Sensitivities on 2-regularization coefficient ν, strength-of-prior coefficient λ, and training size N for direct prediction (REG) and with target-embedding (TEA) on linear model with UKCF. For sensitivities on λ, we perform joint training only, so that we isolate the effect of the joint reconstruction task (i.e. removing the confounding effect of staged training). Standard errors are indicated with shaded regions. For full results, see Tables 25-28 for sensitivities on ν, Tables 29-30 for sensitivities on λ, and Tables 31-34 for sensitivites on N .
Table 1: Dataset statistics and input/output dimensions used in experiments
Table 2: Summary results for TEA and comparators on linear model with UKCF (Bold indicates best)
Table 3: Summary results for TEA and comparators on nonlinear (RNN) model (Bold indicates best)
Table 4: Summary source of gain and TEA variants on linear model with UKCF (Bold indicates best)
Table 5: Summary source of gain and TEA variants on nonlinear (RNN) model (Bold indicates best)
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Representation learning deals with uncovering useful underlying structures of data, and autoencoders (Hinton & Salakhutdinov, 2006) have been a staple in a variety of problems. While much research focuses on its use in unsupervised or semi-supervised settings with such diverse objectives as sparsity (Ranzato et al., 2007), generation (Kingma & Welling, 2013), and disentanglement (Chen et al., 2018), autoencoders are also useful in purely supervised settings-in particular, adding an auxiliary feature-reconstruction task to supervised classification problems has been shown to empirically im- prove generalization (Le et al., 2018); in the linear case, the theoretically quantifiable benefit matches that of simplistic norm-based regularization (Bousquet & Elisseeff, 2002; Rosasco & Poggio, 2009). In this paper, we consider the inverse problem setting where the target space Y is high-dimensional; for instance, consider the multi-label classification tasks of object tagging, text annotation, and image segmentation. This is in contrast to the vast majority of works designed to tackle a high-dimensional feature space X (where commonly |X | |Y|, such as in standard classification problems). In this setting, the usual (and universal) strategy of learning to reconstruct features (Weston et al., 2012; Kingma et al., 2014; Le et al., 2018) may not be most useful: learning latent representations that encapsulate the variation within X does not directly address the more challenging problem of mapping back up to a higher-dimensional Y. Instead, we argue for leveraging intermediate representations that are compact and more easily predictable from features, yet simultaneously guaranteed to be predictive of targets. In the process, we provide a unified theoretical perspective on recent applications of autoencoders to label-embedding in static, high-dimensional classification problems (Yu et al., 2014; Girdhar et al., 2016; Yeh et al., 2017). Extending into the temporal setting, we further empirically demonstrate the generality of target-embedding for recurrent, multi-variate sequence forecasting. Our contributions are three-fold. First, we motivate and formalize the target-embedding autoencoder (TEA) framework: a general approach applicable to any underlying architecture. Second, we provide a theoretical learning guarantee in the linear case by demonstrating uniform stability; specifically, we obtain an O(1/N ) bound on instability by analogizing the benefit of the auxiliary reconstruction task to a form of regularization-without incurring additional bias from explicit shrinkage. Finally, we extend empirical validation of this approach beyond the domain of static classification: using the task of multivariate disease trajectory forecasting as case study, we experimentally validate the advantage that TEAs confer on both linear and nonlinear architectures using real-world datasets with both continuous and discrete targets. To the best of our knowledge, we are the first to formalize and quantify the theoretical benefit of autoencoder-based target-representation learning in a purely supervised setting, and to extend its application to the domain of multivariate sequence forecasting.

Section Title: TARGET-EMBEDDING AUTOENCODERS
  TARGET-EMBEDDING AUTOENCODERS Let X and Y be finite-dimensional vector spaces, and consider the supervised learning problem of predicting targets y ∈ Y from features x ∈ X . With a finite batch of N training instances D = {(x n , y n )} N n=1 , the objective is to learn a mapping h : X → Y that generalizes well to new samples from the same distribution. The vast majority of existing work consider the setting-most commonly, classification-where |X | |Y|; under this scenario, autoencoders are often used to first transform the input into some lower-dimensional representation z ∈ Z amenable to the downstream task. Doing so involves adding an auxiliary reconstruction loss r to the primary prediction loss p . Formally, solutions of this form-in supervised and semi-supervised settings alike-consist of a shared forward model φ : X → Z, a reconstruction function r : Z → X , and a prediction function d : Z → Y during training (where notation d reflects the downstream nature of the prediction task). Denotex = r(φ(x)) andŷ = d(φ(x)); then the complete loss function takes the following form, In contrast, we focus on settings where the target space Y is high-dimensional, and where possibly |Y| > |X |. In this case, we argue that learning to reconstruct the input is not necessarily most beneficial. In a simple classification problem, autoencoding inputs leverages the hypothesis that a reconstructive representation is also likely discriminative. In our setting, however, the more immediate problem is the high-dimensional structure of Y; in particular, there is little guarantee that intermediate representations trained to encapsulate x are easily mapped back up to higher-dimensional targets. Our goal is to make use of intermediate representations that are both predictable from features as well as predictive of targets. A target-embedding autoencoder (TEA)-versus what we shall term a feature-embedding autoencoder (FEA)-flips the model architecture around by learning an embedding of target vectors instead, which a predictor then learns a mapping into. This involves an encoder e : Y → Z, an upstream predictor u : X → Z, and a shared forward model θ : Z → Y. Denoteỹ = θ(e(y)) andŷ = θ(u(x)); the complete loss function is now of the following form, Abstractly, the general idea of target space reduction is not new; in particular, it has been present in various solutions in the domain of multi-label classification (see Section 4 and Appendix B for discussions of related work). Here we focus on target-embedding autoencoders; they leverage the assumption that variations in (high-dimensional) target space are driven by a compact and predictable set of factors. By construction, learning to reconstruct directly in output space ensures that latent representations are predictive of targets; at the same time, jointly training with the prediction loss ensures that latent representations are predictable from features. Instead of learning representations for mapping out of (downstream), here we learn representations for mapping into (upstream); the shared forward model handles the rest. See  Figure 1  for high-level diagrams of TEAs versus FEAs. Training and Inference.  Figure 2  gives block diagrams of component functions and objectives in (a) FEAs and (b) TEAs during training (see Algorithm 1 in Appendix C for pseudocode). Training occurs in three stages. First, the autoencoder is trained (to learn representations): the parameters of e and θ are learned on the reconstruction loss. Second, the prediction arm is trained to regress the learned Published as a conference paper at ICLR 2020 embeddings (generated by the encoder): the parameters of u are learned (on the latent loss) while the autoencoder is frozen. Finally, all three components are jointly trained on both prediction and reconstruction losses (Equation 2): parameters of the predictor, embedding, and shared forward model are trained simultaneously. Note that during training, the forward model receives two types of latents as input: encodings of true targets, as well as encodings predicted from features. At inference time, the target-embedding arm is dropped, leaving the learned hypothesis h = θ • u for prediction. Figure 4 (Appendix C) provides step-by-step block diagrams of both training and inference in greater detail. We emphasize that TEAs-as is the case with FEAs-specify a general framework independent of the implementation details of each component. For instance, the solutions to applications in Yu et al. (2014), Yeh et al. (2017), and Girdhar et al. (2016) can be abstractly regarded as linear and nonlinear instances of this framework, with domain-specific architectures (see Section 4 and Appendix B for more detailed discussions). Linear TEAs-which we study in greater detail in Section 3-involve parameterizations (Θ, W u , W e ) of (θ, u, e) consisting of single hidden layers with linear activation.

Section Title: STABILITY-BASED LEARNING GUARANTEE
  STABILITY-BASED LEARNING GUARANTEE Two questions are outstanding. The first is theoretical. We are motivated by the prior that variations in target space are driven by a lower-dimensional set of underlying factors. In this context, can we say something more rigorous about the benefit of TEAs? In this section, we take the first step in showing that jointly learning target representations improves generalization performance in the supervised setting. Specifically, we demonstrate that linear TEAs are characterized by uniform stability, from which theoretical guarantees are known to follow. The second question is empirical. We noted above that certain applications of label-embedding to classification can be interpreted through this framework. Does the benefit extend beyond its static, feedforward instantiations-into the temporal setting for multi-variate sequence forecasting, with both continuous and discrete targets? In Section 5, we first validate our theoretical findings with linear models and sensitivities, as well as extending our empirical analysis to the realm of recurrent, nonlinear models for both regression and classification. Consider a linear TEA, where the upstream predictor is parameterized by W u ∈ R |Z|×|X | , target- embedding by W e ∈ R |Z|×|Y| , and shared forward model by Θ ∈ R |Y|×|Z| , where |Z| < |Y|. The complete loss function is given by L = 1 N N n=1 [ p (ΘW u x n , y n ) + r (ΘW e y n , y n )] following Equation 2. Interpreting the jointly learned autoencoding component as an auxiliary task, we show that the TEA algorithm for learning the shared forward model Θ is uniformly stable with respect to the domain of the supervised prediction task. To establish our notation, first recall the following: denote the empirical risk, where is some loss function. A generalization bound is a probabilistic bound on the defect that takes the following form: R(h D ) −R(h D ) ≤ with some confidence 1 − δ. Uniform stability holds if the minimum value of γ converges to zero as batch size N increases without limit. Uniform stability can be used to derive algorithm-dependent generalization bounds. In particular, Bousquet & Elisseeff (2002) first showed that the defect of a γ-uniformly stable algorithm is is less than O((γ + 1/N ) N log(1/δ)) with probability ≥ 1 − δ. Feldman & Vondrak (2018) recently demonstrated an improved bound of O( (γ + 1/N ) log(1/δ)). Here, we show uniform stability for linear TEAs, where γ is O(1/N )-by which a tight generalization bound follows immediately. Before we begin, we introduce two additional tools: c-strong convexity and σ-admissibility. Note that these conditions are standard and easily satisfied-for instance, by the quadratic loss function; for more context see for example Bousquet & Elisseeff (2002), Liu et al. (2016), and Mohri et al. (2018). To obtain uniform stability, we make two assumptions-both analogous to prior work arguing from the benefit of learning shared models between tasks. Liu et al. (2016) deals with learning multiple tasks in general, and Le et al. (2018) deals with reconstructing inputs in what we describe as FEAs. Now in multi-task learning, the separate tasks are usually chosen due to some prior relationship between them. In the case of Assumption 1 in Liu et al. (2016) and Assumption 5 in Le et al. (2018), this is assumed to come from similarities in feature structures across tasks; hence their assumptions of cross-representability are made in feature space. (Note that this restricts primary and auxiliary features to be elements of the same space). Our setting is contrary: the inputs to primary and auxiliary tasks come from different spaces, but are trained to produce similar labels through a compact, shared latent space; hence our assumption of cross-representability will be made in this latent space instead.

Section Title: Assumption 1 (Representative Vectors
  Assumption 1 (Representative Vectors Remark 1 This assumption is comparatively mild, even for ε = 0. Note that in Liu et al. (2016) the features for separate tasks come from different examples in general, and the similarity of their distri- butions within X is simply assumed. Here, each pair of inputs to the prediction and reconstruction tasks comes from the same instance, and similarity within Z is explicitly enforced through the (joint) training objective. In addition, observe that the assumption will hold with zero error as long as the number of independent latent vectors is at least |Z|. Furthermore, unlike the scenarios in Liu et al. (2016) and Le et al. (2018) we do not require that the input domains of the two tasks be identical. Therefore for ease of exposition, we assume going forward that ε = 0 (see Remark 6 in Appendix A). Remark 2 A comparison with Assumption 4 in Le et al. (2018) sheds additional light on why we expect TEAs to be beneficial where |Y| > |Z|, in contrast with the (more typical) scenario |X | > |Z| |Y|. Critically, the technique in Le et al. (2018) banks on the fact that the prediction arm projects the latent into a lower-dimensional target space. Conversely, Assumption 1 here relies on the fact that the encoding arm maps into the latent space from a higher-dimensional target space (rendering cross-representability therein reasonable). The distinction is crucial: we certainly do not expect any benefit from autoencoding trivially low-dimensional vectors! Note also that here the representative vectors are taken from Y; to take them from X instead would be unreasonable. For any compressive autoencoder, we generally expect if some subset {b 1 , ..., b M } ⊂ {y 1 , ..., y N } spans Y that {W e b 1 , ..., W e b M } then also span Z in order to be maximally reconstructive. The same cannot be said of subsets {c 1 , ..., c M } ⊂ {x 1 , ..., x N } that span X -for instance, take |X | ≤ |Z|. In addition to being representative in terms of latent values, the set of representative points also needs to be representative in terms of the reconstruction error. First, let L denote the counterpart Published as a conference paper at ICLR 2020 to L where the i-th sample (x i , y i ) is replaced by some new instance (x i , y i ) ∈ X × Y; that is, Then, let Θ * , Θ * denote the optimal parameters corresponding to the two losses L and L . That is, the difference in reconstruction error L B r between the two points Θ * , Θ * is upper bounded by some constant factor of the corresponding difference in reconstruction error L r at the two points. Importantly, note that this does not require that the values of the errors L r and L B r themselves be similar, only that their differences be similar. This assumption is identical to Assumption 6 in Le et al. (2018), and plays an identical role: we make use of L B r -which is only dependent on M -to allow the bound to decay with N ; this is in contrast with the generic multi-task analysis of Liu et al. (2016), which-if applied directly to TEAs (as with FEAs)-would give a bound that does not decay with N . Theorem 1 (Uniform Stability) Let p and r be σ p -admissible and σ r -admissible loss functions, and let r be c-strongly convex. Then under Assumptions 1 and 2, the following inequality holds, Corollary 1 (Generalization Bound) Consider the same conditions as in Theorem 1; that is, let p and r be σ p -admissible and σ r -admissible losses, and let r be c-strongly convex. Then under Assumptions 1 and 2, the defect is less than O( (1/N ) log(1/δ)) with probability at least 1 − δ. Proof. Follows immediately from Theorem 1 (above) and either of the following (similar results hold for both): Theorem 1.2 (Feldman & Vondrak, 2018), and Theorem 12 (Bousquet & Elisseeff, 2002). In supervised learning, it is often easy to make an argument-on an intuitive level-for the "regular- izing" effect of additional loss terms. In contrast, this analysis allows us to unambiguously identify and quantify the benefit of the embedding component as a regularizer (see Remark 4 in Appendix A). Remark 3 In the linear label space reduction framework of Yu et al. (2014), uniform convergence is also shown to hold via norm-based regularization. Specifically for uniform stability, a similar bound can also be achieved by adding a strongly convex term to the objective, such as Tikhonov-and 2 -regularization (Bousquet & Elisseeff, 2002; Rosasco & Poggio, 2009; Shalev-Shwartz et al., 2010). Here, however, the joint reconstruction task leverages a different kind of bias-precisely, the assumption that there exist compact and predictable representations of targets. Therefore the signifi- cance of this analysis is that we achieve an equivalent result independent of explicit regularization.

Section Title: RELATED WORK
  RELATED WORK Our work straddles three threads of research: (1) supervised representation learning with autoencod- ers, (2) label space reduction for multi-label classification, and (3) stability-based learning guarantees. Appendix B provides a much expanded treatment, and presents summary tables for additional context.

Section Title: Supervised representation learning
  Supervised representation learning While a great deal of research is devoted to uncovering useful underlying structures of data through autoencoders-with various properties such as sparsity (Ranzato et al., 2007) and disentanglement (Chen et al., 2018), among many others (Tschannen et al., 2018)-the goal of better representations is often for the benefit of downstream tasks. Semi-supervised autoencoders jointly optimized on partially-labeled data can obtain compact representations that improve prediction (Weston et al., 2012; Kingma et al., 2014; Ghifary et al., 2016). Furthermore, auxiliary reconstruction is also useful in a purely supervised setting: rather than focusing on how specific architectures better structure unlabeled data, Le et al. (2018) show the simple benefit of feature-reconstruction on supervised classification-a special case of what we describe as FEAs. In contrast, we focus on target-representation learning in the supervised setting, and analyze its benefit under the prior that high-dimensional targets are driven by a compact and predictable set of factors.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 We take inspiration from the empirical study of Girdhar et al. (2016), where latent representations of 3D objects are jointly trained to be predictable from 2D images. Their setup can be viewed as a specific instance of TEAs with (nonlinear) convolutional components, with a minor variation in training: in the joint stage, predictors continue to regress the learned embeddings, and gradients only backpropagate from latent space (instead of target space). Unlike the symmetry of our losses (which we require for our analysis above), their common decoder is only shared indirectly (and predictions made indirectly). As it turns out, this does not appear to matter for performance (see Section 5). In Mostajabi et al. (2018), a two-stage procedure is used for semantic segmentation-loosely comparable to the first two stages in TEAs; in contrast to our emphasis on joint training, they study the benefit of a frozen embedding branch in parallel with direct prediction. More broadly related to target-embedding, Dalca et al. (2018) build anatomical priors for biomedical segmentation in unsupervised settings.

Section Title: Multi-label classification
  Multi-label classification The general idea of target space dimension reduction has been explored for multi-label classification problems (commonly, annotation based on bags of features). These first derive a reduced label space, then subsequently associate inputs to it; methods include compressed sensing (Hsu et al., 2009), principal components (Tai & Lin, 2010), maximum-margin coding (Zhang & Schneider, 2012), and landmarking (Balasubramanian & Lebanon, 2012). Closer to our theme of joint learning, Chen & Lin (2012) first propose simultaneously minimizing encoding and prediction errors via an SVD formulation. Using generic empirical risk minimization, Yu et al. (2014) formulate the problem as a linear model with a low-rank constraint. While this captures an intuition (similar to ours) of restricted latent factors, their error bounds require norm-based regularization (unlike ours). Recently, Yeh et al. (2017) generalize the label-embedding approach to autoencoders. This flexibly accommodates custom losses to exploit correlations, as well as deep learning for nonlinearities. Our work is related to this line of research, although we operate at a higher level of abstraction, with a significant difference in focus. Their problem is multi-label classification, and their starting point is binary relevance (i.e. label by label). During reduction, they worry about specific losses that capture dependencies within and among spaces. In contrast, we worry about autoencoding at all-that is, we focus on the effect of joint reconstruction on learning the prediction model. Problems can be of any form: classification or regression, and our starting point is direct prediction (i.e. no reconstruction).

Section Title: Stability and learning guarantees
  Stability and learning guarantees Generalizability via hypothesis stability is first studied in Rogers & Wagner (1978) and Devroye & Wagner (1979); unlike arguments based on the complexity of the search space (Vapnik & Chervonenkis, 1971; Pollard, 1984; Koltchinskii, 2001), these account for how the algorithm depends on the data. Bousquet & Elisseeff (2002) first formalize the notion of uniform stability sufficient for learnability, and Feldman & Vondrak (2018) use ideas related to differential privacy (Bassily et al., 2016) for further improvement. Separately, while there is a wealth of research on dimensionality reduction and autoencoders (Singh et al., 2009; Mohri et al., 2015; Gottlieb et al., 2016; Epstein & Meir, 2019), they either operate in the semi-supervised setting, or focus on the benefit of feature representations (not targets) and also do not consider joint learning. The benefit of jointly learning multiple tasks through a common operator (Caruana, 1997) is explored with VC-based (Baxter, 2000) and Rademacher complexity-based (Maurer, 2006; Maurer et al., 2016) analyses. Recently, Liu et al. (2016) show that the algorithm for learning the shared model in a multi-task setting is uniformly stable. While our argument is based on theirs, we are not interested in a generic bound for all tasks; closer to Le et al. (2018), we focus on the primary prediction task, and leverage the auxiliary reconstruction task for stability. Similarly, we arrive at an O(1/N ) on instability without an explicit regularization term as in Bousquet & Elisseeff (2002). Unlike them, however, the fundamental distinction of our setting is that Y is high-dimensional (but where the underlying factors are assumed compact); in this sense our focus is the mirror opposite of theirs.

Section Title: EXPERIMENTS AND DISCUSSION
  EXPERIMENTS AND DISCUSSION So far, we have formalized a general target-autoencoding framework for supervised learning, and quantified the benefit via uniform stability. Our overall goal in this section is to explore this benefit in a simple controlled setting, such that we can identify and isolate its utility on the prediction task, and investigate any sensitivities of interest. By way of preface, we emphasize two observations from above: (1) In the static, multi-label classification setting, the gain from label-embedding has been studied, including the autoencoder approach of Yeh et al. (2017)-which can be viewed as an instantiation of TEAs with sophisticated refinements. (2) The benefit of target-autoencoding is also Published as a conference paper at ICLR 2020 The effective input dimension |X | is computed as the dimension of static data plus the product of the width of the historical window (of temporal information) with its dimension; the effective target dimension |Y| is sim- ilarly computed as the product of the width of the forecast window (of temporal information) with its dimension. demonstrated using nonlinear, convolutional architectures in Girdhar et al. (2016)-which is also an instantiation of TEAs, also noting significant gains. Therefore a natural question of interest is: • Does the utility of target-embedding extend to (nonlinear) recurrent models with sequ- ential data for general, high-dimensional targets (i.e. regression and/or classification)?

Section Title: Disease Trajectories
  Disease Trajectories

Section Title: Datasets
  Datasets We use three datasets in our experiments. The first consists of a cohort of patients enrolled in the UK Cystic Fibrosis registry (UKCF), which records follow-up trajectories for over 10,000 patients. We are interested in forecasting future trajectories for the 11 possible infections and 23 possible comorbidities (all binary variables) recorded at each follow-up, using past trajectories and basic demographics as input. The second consists of patients in the Alzheimer's Disease Neuroimaging Initiative study (ADNI), which tracks disease progression for over 1,700 patients. We are interested in forecasting the evolution of the 8 primary biomarkers and 16 cognitive tests (all continuous variables) measured at each visit, using past measurements and basic demographics as input. The third consists of a cohort of patients in intensive care units from the Medical Information Mart for Intensive Care (MIMIC), which records physiological data streams for over 22,000 patients. Likewise, we are interested in forecasting future trajectories for the 361 most frequently measured variables such as vital signs and lab tests (both binary and continuous variables), again using past measurements and basic demographics as input. See Appendix D for more information on datasets.

Section Title: Experimental Setup
  Experimental Setup In each instance, the prediction input is a precedent window of up to width w x , and the prediction target is the succedent window of width w y . For UKCF (w x , w y ) = (3, 4) at 1-year resolution, for ADNI (4, 8) at 6-month resolution, and for MIMIC (5, 5) at 4-hour (resampled) resolution. All models are implemented in Tensorflow. Linear models consist of a single hidden layer with no nonlinearity; for the nonlinear case, we implement an RNN model for each component using GRUs. See Appendix D for additional detail on model implementation and configuration. For evaluation, we measure the mean squared error (MSE) for continuous targets (averaged across variables), and the area under the precision-recall curve (PRC) and area under the receiver operating characteristic curve (ROC) for binary targets (averaged across variables). We use cross-validation on the training set for hyperparameter tuning, selecting the setting that gives the lowest validation loss averaged across folds. For each model and dataset, we report the average and standard error of each performance metric across 10 different experiment runs, each with a different random train-test split. Note that forecasting high-dimensional disease trajectories is challenging, and input information is deliberately limited (as is often the case in medical practice); the desired targets are similar or higher- dimensional than the inputs (see  Table 1 ). This obviously results in an inherently difficult prediction problem-but which makes for a good candidate setting to test the utility of target-representation learning. RNN autoencoders have previously been proposed for learning representations of inputs (i.e. FEAs instantiated with RNNs) to improve classification (Dai & Le, 2015), prediction (Lyu et al., 2018), generation (Srivastava et al., 2015), and clustering (Baytas et al., 2017); similarly, their mission is not in excessively optimizing specific architectural novelties to match state-of-the-art, but rather in exploring the benefit of the autoencoding framework. Here, we learn representations of targets. The two-sample t-test for difference in means is conducted on the results. An asterisk indicates statistically significant difference in means (p-value < 0.05) relative to the TEA result. PRC and ROC metrics are reported separately for variables representing infections (I) and comorbidities (C). See Tables 9-10 for extended results.

Section Title: MAIN RESULTS
  MAIN RESULTS Overall Benefit. First, we examine the overall utility of TEAs. To verify the linear case first,  Table 2  summarizes the performance of TEA and alternate setups on UKCF. The temporal axis is flattened to simulate ordinary static multi-label classification, and the base case is direct prediction (Base)-that is, absent any auxiliary representation learning or regularization. Next, we allow for 2 -regularization over direct prediction (REG), as well as over all other methods. FEAs differ only by the added feature-reconstruction, and TEAs only by the target-reconstruction; as an additional sensitivity, we also implement a combined approach (F/TEA). More generally, we also wish to examine the benefit of TEA for the nonlinear case:  Table 3  summarizes analogous results where component functions are implemented with GRU networks; results are shown for all datasets. Ceteris paribus, we observe that target-representation learning has a notable positive impact on performance. Interestingly, learning representations of inputs does not yield significant benefit, and the hybrid approach (F/TEA) is worse than TEA; this suggests that forcing intermediate representation to encode both features and targets may be overly constraining. (Note that for the linear model, the instances are restricted to those for which the full input window is available; as a consequence, the results for linear and nonlinear cases are not directly comparable). Figures 4 (Appendix C) and 5 (Appendix D) give training diagrams for all comparators. Additional experiment results (by model, timestep, metric) are in Appendix E.1-2.

Section Title: Source of Gain
  Source of Gain There are two (related) interpretations of TEAs. First, we studied the regularization view in Section 3; this concerns the benefit of joint training using both prediction and reconstruction losses. Ceteris paribus, we expect performance to improve purely by dint of the jointly trained TEA objective. Second, the reduction view says that TEAs decompose the (difficult) prediction problem into two (smaller) tasks: the autoencoder learns a compact representation z of y, and the predictor learns to map x to z. This makes the potential benefit of staged training (Section 2 and Appendix C) intuitively clear, and suggests an alternative-that of simply training the autoencoder and predictor arms in two stages-à la Mostajabi et al. (2018). As a general framework, TEAs is a combination of both ideas: all three components are jointly trained in a third stage-à la Girdhar et al. (2016). We now account for the improvement in performance due to these two sources of benefit;  Table 4  does so for the linear case (on UKCF), and  Table 5  for the more general nonlinear case (on all datasets). The "No Joint" setting isolates the benefit from staged training only. This is analogous to basic unsupervised pretraining (though using targets), and corresponds to omitting the final joint training stage in Algorithm 1. The "No Staged" setting isolates the benefit from joint training only (without pretraining the autoencoder or predictor), and corresponds to omitting the first two training stages in Algorithm 1. The "Neither" setting is equivalent to vanilla prediction (REG) without leveraging either of the advantages. We observe that while both sources of benefit are individually important, neither setting performs quite as well as when both are combined. See Appendix E.1-2 for extended results.

Section Title: Variations
  Variations Having established the utility of target-embedding, we can ask whether variations on the same theme perform similarly. In particular, the embeddings in the empirical studies of Girdhar et al. (2016) and Yeh et al. (2017) are jointly learned via the reconstruction loss r and latent loss z -that is, the prediction arm continues to regress learned embeddings during the joint training stage (Figure 4(d), in Appendix D). The principle is similar, although (as noted in Section 4) the primary task is therefore learned indirectly; this is in contrast to the vanilla TEA setup, where the primary task is learned directly via the prediction loss p .  Tables 4  and 5 also compare the performance of vanilla TEAs with this indirect variant (TEA(L)), as well as a hybrid variant (TEA(LP)) for which both latent and prediction losses are trained jointly with the reconstruction loss (Figure 4(e). Perhaps as expected, we observe that performance across all three variants are more or less identical, affirming the general benefit of target-representation learning. Again, see Appendix E.1-2 for extended results.

Section Title: SENSITIVITIES
  SENSITIVITIES

Section Title: Regularization
  Regularization Of course, target-representation learning is not a replacement for other regulariza- tion strategies; it is an additional tool that can be used in parallel where appropriate. Figure 3(a) shows the performance of TEA and REG with various coefficients ν on 2 -regularization. By itself, introducing 2 -regularization does improve performance up to a certain point, beyond which the additional shrinkage bias incurred begins to be counterproductive; this is not surprising. Interestingly, introducing target-representation learning appears to leverage an orthogonal bias: it consistently improves prediction performance regardless of level of shrinkage. This is a practical result of the the- oretical observation in Remark 3: while prior works obtain stability through explicit 2 -regularization, the benefit from target-embedding relies on a different bias entirely, which allows us to combine them. While increasing the strength of either form of regularization reduces variability in results (see also below), excessive bias of either alone degrades performance. See Appendix E.3 for full results.

Section Title: Strength of Prior
  Strength of Prior Target-embedding attempts to leverage the assumption that there exist compact and predictable representations of targets. Realistically (e.g. due to measurement noise), of course, this will not hold perfectly. In our experiments, we set the ratio of prediction and reconstruction losses to be 1 : 1 for TEA (as well as FEA and F/TEA); that is, the "strength-of-prior" coefficient λ on r is 0.5. In order to isolate the effect of λ during joint training, we observe the performance of TEAs with joint training only (i.e. removing the confounding effect of staged training). For large values of λ, we expect the reconstruction task to dominate in priority, which is (under an imperfect prior) not beneficial for the ultimate prediction task-in general, a hidden representation that is most reconstructive is not necessarily also what is most predictable). For small values of λ, the setup begins to resemble direct prediction. Figure 3(b) verifies our intuition. Note that in the extreme case of λ = 1, predictions are no better than random (see ROC∼ 0.5). See Appendix E.3 for full results. Sample Complexity. Figure 3(c) shows the performance of TEA and REG under various levels of data scarcity. The benefit conferred by TEAs is significant especially when the amount of training data N is limited. Importantly, note that we are operating strictly within the context of supervised learning: unlike in semi-supervised settings, here we are not just restricting access to paired data; we are restricting access to data per se. (Though beyond the scope of this paper, we expect that extending TEAs to semi-supervised learning with additional unpaired data would yield larger gains). Here, without the luxury of learning from unpaired data, we highlight the comparative advantage purely from the addition of target-representation learning. Again, see Appendix E.3 for full results.

Section Title: DISCUSSION
  DISCUSSION By way of conclusion, we emphasize the importance of our central assumption: that there exist compact and predictable representations of the (high-dimensional) targets. This is critical: target- embedding is not useful where this is not true. Now obviously, learning representations of targets is unnecessary if the output dimension is trivially small (e.g. if the target is a single classification label), or if the problem itself is trivially easy (e.g. if direct prediction is already perfect). Also obvious is the situation where representations cannot possibly be compact (e.g. if all output dimensions are independent of each other), in which case any model with a compressive (bottleneck) representation as an intermediate target may make little sense to begin with. Perhaps less obvious is that we cannot assume that the goals of prediction and reconstruction are always aligned. Just as in learning feature- embeddings (for downstream classification), what is most reconstructive may not necessarily encode what is most discriminative; so too in learning target-embeddings (for upstream prediction), what is most reconstructive may not necessarily encode what is most predictable. In the case of disease trajectories, it is medical knowledge that permits this assumption with some confidence. Appendix E.4 gives an extreme (synthetic) counterexample where this prior is outright false-i.e. prediction and reconstruction are directly at odds. While certainly contrived, it serves as a caveat about assumptions. Using the deliberately challenging setting of disease trajectory forecasting with limited information, we have illustrated the nontrivial utility of target-representation learning in a controlled setting with baseline models. While we appreciate that component models in the wild may be more tailored, this setting better allows us to identify and isolate the potential utility of target-autoencoding per se. In addition to verifying our intuitions for the linear case, we have extended empirical validation of target-autoencoding to (nonlinear) sequence-to-sequence recurrent architectures; along the way, we explored the sources of gain from joint and staged training, as well as various sensitivities of interest. Where the prior holds, target-embedding autoencoders are potentially applicable to any high-dimensional prediction task beyond static classification and imaging applications, and exploring its utility for other specific domain-architectures may be a practical direction for future research.

```
