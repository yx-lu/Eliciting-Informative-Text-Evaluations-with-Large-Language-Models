Title:
```
None
```
Abstract:
```
From a young age we learn to use grammatical principles to hierarchically com- bine words into sentences. Action grammars is the parallel idea, that there is an underlying set of rules (a grammar) that govern how we hierarchically combine actions to form new, more complex actions. We introduce the Action Grammar Reinforcement Learning (AG-RL) framework which leverages the concept of ac- tion grammars to consistently improve the sample efficiency of Reinforcement Learning agents. AG-RL works by using a grammar inference algorithm to in- fer the action grammar of an agent midway through training. The agent's action space is then augmented with macro-actions identified by the grammar. We apply this framework to Double Deep Q-Learning (AG-DDQN) and a discrete action version of Soft Actor-Critic (AG-SAC) and find that it improves performance in 8 out of 8 tested Atari games (median +31%, max +668%) and 19 out of 20 tested Atari games (median +96%, maximum +3,756%) respectively without substantive hyperparameter tuning. We also show that AG-SAC beats the model-free state-of- the-art for sample efficiency in 17 out of the 20 tested Atari games (median +62%, maximum +13,140%), again without substantive hyperparameter tuning.
```

Figures/Tables Captions:
```
Figure 1: The high-level steps involved in the AG-RL algorithm. In the first Gather Experience step the base agent interacts as normal with the environment for N1 episodes. Then we feed the experiences of the agent to a grammar calculator which outputs macro-actions that get appended to the action set. The agent starts interacting with the environment again with this updated action set. This process repeats as many times as required, as set through hyperparameters.
Figure 2: Example of a solution to the Towers of Hanoi game. Letters a to f represent the 6 different possible moves. After 7 moves the agent has solved the game and receives +100 reward.
Figure 3: Comparing AG-DDQN to DDQN for 8 Atari games. Graphs show the average evaluation score over 5 random seeds where an evaluation score is calculated every 25,000 steps and averaged over the previous 3 scores. For the evaluation methodology we used the same no-ops condition as in van Hasselt et al. (2015). The shaded area shows ±1 standard deviation.
Figure 4: Comparing AG-SAC to SAC for 20 Atari games. Graphs show the average evaluation score over 5 random seeds where an evaluation score is calculated at the end of 100,000 steps of training. For the evaluation methodology we used the same no-ops condition as in van Hasselt et al. (2015).
Figure 5: Ablation study comparing DDQN to different versions of AG-DDQN for the game Qbert. The dark line is the average of 5 random seeds, the shaded area shows ±1 standard deviation across the seeds.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement Learning (RL) has made great progress in recent years, successfully being applied to settings such as board games ( Silver et al., 2017 ), video games ( Mnih et al., 2015 ) and robot tasks ( OpenAI et al., 2018 ). Some of this advance is due to the use of deep learning techniques to avoid induction biases in the mapping of sensory information to states. However, widespread adoption of RL in real-world domains has remained limited primarily because of its poor sample efficiency, a dominant concern in RL ( Wu et al., 2017 ), and complexity of the training process that need to be managed by various heuristics. Hierarchical Reinforcement Learning (HRL) attempts to improve the sample efficiency of RL agents by making their policies to be hierarchical rather than single level. Using hierarchical policies can lead not only to faster learning, but also ease human understanding of the agent's behaviour - this is because higher-level action representations are easier to understand than low-level ones ( Beyret et al., 2019 ). Identifying the right hierarchical policy structure is, however a non-trivial task ( Osa et al., 2019 ) and so far progress in hierarchical RL has been slow and incomplete, as no truly scalable and successful hierarchical architectures exist ( Vezhnevets et al., 2016 ). Not surprisingly most state- of-the-art RL agents at the moment are not hierarchical. In contrast, humans, use hierarchical grammatical principles to communicate using spoken/written language (Ding et al., 2012) but also for forming meaningful abstractions when interacting with var- ious entities. In the language case, for example, to construct a valid sentence we generally combine a noun phrase with a verb phrase ( Yule, 2015 ). Action grammars are an analogous idea proposing there is an underlying set of rules for how we hierarchically combine actions over time to produce new actions. There is growing neuroscientific evidence that the brain uses similar processing strate- gies for both language and action, meaning that grammatical principles are used in both neural repre- sentations of language and action ( Faisal et al., 2010 ;  Hecht et al., 2015 ;  Pastra & Aloimonos, 2012 ). We hypothesise that using action grammars would allow us to form a hierarchical action represen- tation that we could use to accelerate learning. Additionally, in light of the neuroscientific findings, hierarchical structures of action may also explain the interpretability of hierarchical RL agents, as their representations are structurally more similar to how humans structure tasks. In the following we will explore the use of grammar inference techniques to form a hierarchical representation of actions that agents can operate on. Just like much of Deep RL has focused on forming unbiased sensory representations from data-driven agent experience, here we explore how data-driven agent experience of actions can contribute to forming efficient representations for learning and controlling tasks. Action Grammar Reinforcement Learning (AG-RL) operates by using the observed actions of the agent within a time window to infer an action grammar. We use a grammar inference algorithm to substitute repeated patterns of primitive actions (i.e. words) into temporal abstractions (rules, anal- ogous to a sentence). Similarly, we then replace repeatedly occurring rules of temporal abstractions with higher-level rules of temporal abstractions (analogous to paragraphs), and so forth. These ex- tracted action grammars (the set of rules, rules of rules, rules of rules of rules, etc) is appended to the agent's action set in the form of macro-actions so that agents can choose (and evaluate) primitive actions as well as any of the action grammar rules. We show that AG-RL is able to consistently and significantly improve sample efficiency across a wide range of Atari settings.

Section Title: RELATED WORK
  RELATED WORK Our concept of action grammars that act as efficient representations of actions is both related to and different from existing work in the domain of Hierarchical Control. Hierarchical control of temporally-extended actions allows RL agents to constrain the dimensionality of the temporal credit assignment problem. Instead of having to make an action choice at every tick of the environment, the top-level policy selects a lower-level policy that executes actions for potentially multiple time- steps. Once the lower-level policy finishes execution, it returns control back to top-level policy. Identification of suitable low level sub-policies poses a key challenge to HRL. Current approaches can be grouped into three main pillars: Graph theoretic ( Hengst, 2002 ;  Man- nor et al., 2004 ;  Simsek et al., 2004 ) and visitation-based (Stolle & Precup, 2002;  Simsek et al., 2004 ) approaches aim to identify "bottlenecks" within the state space. Bottlenecks are regions in the state space which characterize successful trajectories. Our work, on the other hand, identi- fies patterns solely in the action space and does not rely on reward-less exploration of the state space. Furthermore, our action grammar framework defines a set of macro-actions as opposed to full option-specific sub-policies. Thereby, it is less expressive but more sample-efficient to infer. Gradient-based approaches, on the other hand, discover parametrized temporally-extended actions by iteratively optimizing an objective function such as the estimated expected value of the log like- lihood of the observed data under the current policy with respect to the latent variables in a proba- bilistic setting ( Daniel et al., 2016 ) or the expected cumulative reward in a policy gradient context ( Bacon et al., 2017 ;  Smith et al., 2018 ). Our action grammar induction, on the other hand, infers pat- terns without supervision solely based on a compression objective. The resulting parse tree provides an interpretable structure for the distilled skill set. Furthermore, recent approaches in macro-action discovery ( Vezhnevets et al., 2017 ;  Florensa et al., 2017 ) attempt to split the goal declaration and goal achievement across different stages and layers of the learned architecture. Thus, while hierarchical construction of goals follows a set of coded rules, our action grammars are inferred entirely data-driven based on agent experience. Usually, the top level of the hierarchy specifies goals in the environment while the lower levels have to achieve them. Again, such architectures lack sample efficiency and easy interpretation. Our context-free grammar-based approach, on the other hand, is a symbolic method that requires few rollout traces and generalizes to more difficult task-settings. Finally, unlike recent work on unifying symbolic and connectionist methods, we do not aim to discover relationships between entities ( Garnelo et al., 2016 ;  Zambaldi et al., 2018 ). Instead our proposed action grammar framework achieves interpretability by extracting hierarchical subroutines associated with sub-goal achievements ( Beyret et al., 2019 ). The Action Grammar Reinforcement Learning framework operates by having the agent repeatedly iterate through two steps (as laid out in  Figure 1 ): During the first Gather Experience step of the game the base RL agent plays normally in the environment for some set number of episodes. The only difference is that during this time we occasionally run an episode of the game with all random exploration turned off and store these experiences separately. We do this because we will later use these experiences to identify the action grammar and so we do not want them to be influenced by noise. See left part of  Figure 1 . After some set number of episodes we pause the agent's interaction with the environment and enter the first Identify Action Grammar stage, see middle part of  Figure 1 . This firstly involves col- lecting the actions used in the best performing of the no-exploration episodes mentioned above. We then feed these actions into a grammar calculator which identifies the action grammar. A simple choice for the grammar calculator is Sequitur ( Nevill-Manning & Witten, 1997 ). Sequitur receives a sequence of actions as input and then iteratively creates new symbols to replace any repeating sub- sequences of actions. These newly created symbols then represent the macro-actions of the action grammar. To minimise the influence of noise on grammar generation, however, we need to regularise the process. A naive regulariser is k-Sequitur ( Stout et al., 2018 ) which is a version of Sequitur that only creates a new symbol if a sub-sequence repeats at least k times (instead of at least two times), where higher k corresponds to stronger regularisation. Here we use a more principled approach and regularise on the basis of an information theoretic criterion: we generate a new symbol if doing so reduces the total amount of information needed to encode the sequence. After we have identified the action grammar we enter our second Gather Experience step. This firstly involves appending the macro-actions in the action grammar to the agent's action set. To do this without destroying what our q-network and/or policy has already learned we use transfer learning. For every new macro-action we add a new node to the final layer of the network, leaving all other nodes and weights unchanged. We also initialise the weights of each new node to the weights of their first primitive action (as it is this action that is most likely to have a similar action value to the macro-action). e.g. if the primitive actions are {a, b} and we are adding macro-action abb then we initialise the weights of the new macro-action to those of a. Then our agent begins interacting with the environment as normal but with an action set that now includes macro-actions and four additional changes: i) In order to maximise information efficiency, when storing experiences from this stage onwards we use a new technique we call Hindsight Action Replay (HAR). It is related to Hindsight Experience Replay which creates new experiences by reimagining the goals the agent was trying to achieve. Instead of reimagining the goals, HAR creates new experiences by reimagining the actions. In particular it reimagines them in two ways: 1. If we play a macro-action then we also store the experiences as if we had played the se- quence of primitive actions individually 2. If we play a sequence of primitive actions that matches an existing macro-action then we also store the experiences as if we had played the macro-action See Appendix A for an example of how HAR is able to more than double the number of collected experiences in some cases. ii) To make sure that the longer macro-actions receive enough attention while learning, we sample experiences from an action balanced replay buffer. This acts as a normal replay buffer except it returns samples of experiences containing equal amounts of each action. iii) To reduce the variance involved in using long macro-actions we develop a new technique called Abandon Ship. During every timestep of conducting a macro-action we calculate how much worse it is for the agent to continue executing its macro-action compared to abandoning the macro-action and picking the highest value primitive action instead. Formally we calculate this value as d = 1 − exp(qm) exp(q highest ) where q m is the action value of the primitive action we are conducting as part of the macro-action and q highest is the action value of the primitive action with the highest action value. We also store the moving average, m(d), and moving standard deviation, std(d), of d. Then each timestep we compare d to threshold t = m(d)+std(d)z where z is the abandon ship hyperparameter that determines how often we will abandon macro-actions. If d > t then we abandon our macro- action and return control back to the policy, otherwise we continue executing the macro-action. iv) When our agent is picking random exploration moves we bias its choices towards macro-actions. For example, when a DQN agent picks a random move (which it does epsilon proportion of the time) we set the probability that it will pick a macro-action, rather than a primitive action, to the higher probability given by the hyperparameter "Macro Action Exploration Bonus". In these cases, we do not use Abandon Ship and instead let the macro-actions fully roll out. The second Gather Experience step then continues until it is time to do another Identify Action Grammar step or until the agent has been trained for long enough and the game ends. Algorithm 1 provides the full AG-RL algorithm.

Section Title: SIMPLE EXAMPLE
  SIMPLE EXAMPLE We now highlight the core aspects of how AG-RL works using the simple game Towers of Hanoi. The game starts with a set of disks placed on a rod in decreasing size order. The objective of the game is to move the entire stack to another rod while obeying the following rules: i) Only one disk can be moved at a time; ii) Each move consists of taking the upper disk from one of the stacks and placing it on top of another stack or on an empty rod; and iii) No larger disk may be placed on top of a smaller disk. The agent only receives a reward when the game is solved, meaning rewards are sparse and that it is difficult for an RL agent to learn the solution.  Figure 2  runs through an example of how the game can be solved with the letters 'a' to 'f ' being used to represent the 6 possible moves in the game. In this game, AG-RL proceeds by having the base agent (which can be any off-policy RL agent) play the game as normal. After some period of time we pause the agent and collect some of the actions taken by the agent, e.g. say the agent played the sequence of actions: "bafbcdbafecfbaf- bcdbcfecdbafbcdb". Then we use a grammar induction algorithm such as Sequitur to create new symbols to represent repeating sub-sequences. In this example, Sequitur would create the 4 new symbols: {G : bc, H : ec, I : baf, J : baf bcd}. We then append these symbols to the agent's action set as macro-actions, so that the action set goes from A = {a, b, c, d, e, f } to: The agent then continues playing in the environment with this new action set which includes macro- actions. Because the macro-actions are of length greater than one it means that their usage effectively reduces the time dimensionality of the problem, making it an easier problem to solve in some cases. 1 We now demonstrate the ability of AG-RL to consistently improve sample efficiency on the much more complicated Atari suite setting.

Section Title: RESULTS
  RESULTS We first evaluate the AG-RL framework using DDQN as the base RL algorithm. We refer to this as AG-DDQN. We compare the performance of AG-DDQN and DDQN after training for 350,000 steps on 8 Atari games chosen a priori to represent a broad range. To accelerate training times for both we set their convolutional layer weights as equal to those of some pre-trained agents 2 and then only train the fully connected layers. For the DDQN-specific hyperparameters of both networks we do no hyperparameter tuning and instead use the hyperparmaeters from  van Hasselt et al. (2015)  or set them manually. For the AG- specific hyperparameters we tried four options for the abandon ship hyperparameter (No Abandon Ship, 1, 2, 3) for the game Qbert and chose the option with the highest score. No other hyperpa- rameters were tuned and all games then used the same hyperparameters which can all be found in Appendix B along with a more detailed description of the experimental setup. We find that AG-DDQN outperforms DDQN in all 8 games with a median final improvement of 31% and a maximum final improvement of 668% -  Figure 3  summarises the results. Next we further evaluate AG-RL by using SAC as the base RL algorithm, leading to AG-SAC. We compare the performance of AG-SAC to SAC. We train both algorithms for 100,000 steps on 20 Atari games chosen a priori to represent a broad range games. As SAC is a much more efficient algorithm than DDQN this time we train both agents from scratch and do not use pre-trained convo- lutional layers. For the SAC-specific hyperparameters of both networks we did no hyperparameter tuning and in- stead used a mixture of the hyperparameters found in  Haarnoja et al. (2018)  and  Kaiser et al. (2019) . For the AG-specific hyperparmaeters again the only tuning we did was amongst 4 options for the abandon ship hyperparameter (No Abandon Ship, 1, 2, 3) on the game Qbert. No other hyperparam- eters were tuned and all games then used the same hyperparameters, details of which can be found in Appendix C. Our results show AG-SAC outperforms SAC in 19 out of 20 games with a median improvement of 96% and a maximum improvement of 3,756% -  Figure 4  summarises the results and Appendix D provides them in more detail. We also find that AG-SAC outperforms Rainbow, which is the model-free state-of-the-art for Atari sample efficiency, in 17 out of 20 games with a median improvement of 62% and maximum improve- ment of 13,140% - see Appendix D for more details. Also note that the Rainbow scores used were taken from  Kaiser et al. (2019)  who explain they were the result of extensive hyperparameter tuning compared to our AG-SAC scores which benefited from very little hyperparameter tuning.

Section Title: DISCUSSION
  DISCUSSION To better understand the results, we first explore what types of macro-actions get identified during the Identify Action Grammar stage, whether the agents use them extensively or not, and to what extent the Abandon Ship technique plays a role. We find that the length of macro-actions can vary greatly from length 2 to over 100. An example of an inferred macro-action was 8888111188881111 from the game Beam Rider where 1 represents Shoot and 8 represents move Down-Right. This macro-action seems particularly useful in this game as the game is about shooting enemies whilst avoiding being hit by them. We also found that the agents made extensive use of the macro-actions. Taking each game's best performing AG-SAC agent, the average attempted move length during evaluation was 20.0. Because of Abandon Ship the average executed move length was significantly lower at 6.9 but still far above the average of 1.0 we would get if the agents were not using their macro-actions. Appendix E gives more details on the differences in move lengths between games. We now conduct an ablation study to investigate the main drivers of AG-DDQN's performance in the game Qbert, with results in  Figure 5  . Firstly we find that HAR was crucial for the improved perfor- mance and without it AG-DDQN performed no better than DDQN. We suspect that this is because without HAR there are much fewer experiences to learn from and so our action value estimates have very high variance. Next we find that using an action balanced replay buffer improved performance somewhat but by a much smaller and potentially insignificant amount. This potentially implies that it may not be necessary to use an action balanced replay and that the technique may work with an ordinary replay buffer. We also see that with our chosen abandon ship hyperparameter of 1.0, performance was higher than when abandon ship was not used. Performance was also similar for the choice of 2.0 which suggests performance was not too sensitive to this choice of hyperparameter. Finally we see improved performance from using transfer learning when appending macro-actions to the agent's action set rather than creating a new network. Lastly, we note that the game in which AG-DDQN and AG-SAC both do relatively best in is Enduro. Enduro is a game with sparse rewards and therefore one where exploration is very important. We therefore speculate that AG-RL does best on this game because using long macro-actions increases the variance of where an agent can end up and therefore helps exploration.

Section Title: CONCLUSION
  CONCLUSION Motivated by the parallels between the hierarchical composition of language and that of actions, we combine techniques from computational linguistics and RL to help develop the Action Grammars Reinforcement Learning framework. The framework expands on two key areas of RL research: Symbolic RL and Hierarchical RL. We ex- tend the ideas of symbolic manipulation in RL ( Garnelo et al., 2016 ;  Garnelo & Shanahan, 2019 ) to the dynamics of sequential action execution. Moreover, while Relational RL approaches ( Zambaldi et al., 2018 ) draw on the complex logic-based framework of inductive programming, we merely observe successful behavioral sequences to induce higher order structures. We provided two implementations of the framework: AG-DDQN and AG-SAC. We showed that AG-DDQN improves on DDQN in 8 out of 8 tested Atari games (median +31%, max +668%) and AG-SAC improves on SAC in 19 out of 20 tested Atari games (median +96%, max +3,756%) all without substantive hyperparameter tuning. We also show that AG-SAC beats the model-free state- of-the-art for 17 out of 20 Atari games (median +62%, max +13,140%) in terms of sample efficiency, again even without substantive hyperparameter tuning. As part of AG-RL we also provided two new and generally applicable techniques: Hindsight Action Replay and Abandon Ship. Hindsight Action Replay can be used to drastically improve information efficiency in any off-policy setting involving macro-actions. Abandon Ship reduces the variance involved when training macro-actions, making it feasible to train algorithms with very long macro- actions (over 100 steps in some cases). Overall, we have demonstrated the power of action grammars to consistently improve the perfor- mance of RL agents. We believe our work is just one of many possible ways of incorporating the concept of action grammars into RL and we look forward to exploring other methods.

```
