Title:
```
Published as a conference paper at ICLR 2020 CROSS-LINGUAL ALIGNMENT VS JOINT TRAINING: A COMPARATIVE STUDY AND A SIMPLE UNIFIED FRAMEWORK
```
Abstract:
```
Learning multilingual representations of text has proven a successful method for many cross-lingual transfer learning tasks. There are two main paradigms for learning such representations: (1) alignment, which maps different independently trained monolingual representations into a shared space, and (2) joint training, which directly learns unified multilingual representations using monolingual and cross-lingual objectives jointly. In this paper, we first conduct direct compar- isons of representations learned using both of these methods across diverse cross- lingual tasks. Our empirical results reveal a set of pros and cons for both methods, and show that the relative performance of alignment versus joint training is task- dependent. Stemming from this analysis, we propose a simple and novel frame- work that combines these two previously mutually-exclusive approaches. Exten- sive experiments demonstrate that our proposed framework alleviates limitations of both approaches, and outperforms existing methods on the MUSE bilingual lexicon induction (BLI) benchmark. We further show that this framework can generalize to contextualized representations such as Multilingual BERT, and pro- duces state-of-the-art results on the CoNLL cross-lingual NER benchmark. 1 * Equal contribution. Ordering determined by dice rolling. 1 As of the time of submission Sept 25, 2019. Source code is available at https://github.com/ thespectrewithin/joint-align. 2 In this paper, "supervision" refers to that provided by a parallel corpus or bilingual dictionaries.
```

Figures/Tables Captions:
```
Figure 1: PCA visualization of English and Spanish embeddings learnt by unsupervised joint train- ing as in Lample et al. (2018b). As shown by plots (a) and (b), most words are shared in the initial embedding space but not well-aligned, hence the oversharing problem. Plots (b) and (c) shows that the vocabulary reallocation step effectively mitigates oversharing while the alignment refinement step further improves the poorly aligned embeddings by projecting them into a close neighborhood.
Figure 2: (a)(b): Results on MT of Align, Joint and our framework for the first 20 training epochs. Results after 20 epochs have similar patterns. (c): BLEU scores for the MT task. Results evaluated on the WMT'14 English-French and WMT'16 German-English. All training settings are the same for each language pair except the embedding initialization. Note that we are not trying to outperform state-of-the-art methods (Song et al., 2019) but rather to observe improvements afforded by embedding initialization. † Results reported by Lample et al. (2018b). Our results are obtained using the official code released by the author. ‡ Duong et al. (2016) is a supervised method that we include for analysis purpose only and is not directly comparable to other results in this table.
Table 1: Precision@1 for the BLI task on the MUSE dataset 6 . Within each category, unsuper- vised methods are listed at the top while supervised methods are at the bottom. The best result for unsupervised methods is underlined while bold signifies the overall best. "IN" refers to iterative normalization proposed in Zhang et al. (2019), "AR" refers to alignment refinement and "VR" refers to vocabulary reallocation.
Table 2: Precision@1 for the BLI task on the MUSE dataset with test pairs of same surface form removed. The best result for unsupervised methods is underlined while bold signifies the overall best.
Table 3: F1 score for the cross- lingual NER task. "Adv" refers to adversarial training. ‡ de- notes results that are not directly comparable due to different re- sources and architectures used. * denotes supervised XLM model trained with MLM and TLM ob- jectives. Its Dutch (nl) result is blank because the model is not pretrained on it. Bold signifies state-of-the-art results. We re- port the average of 5 runs.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Continuous word representations ( Mikolov et al., 2013a ;  Pennington et al., 2014 ;  Bojanowski et al., 2017 ) have become ubiquitous across a wide range of NLP tasks. In particular, methods for cross- lingual word embeddings (CLWE) have proven a powerful tool for cross-lingual transfer for down- stream tasks, such as text classification ( Klementiev et al., 2012 ), dependency parsing ( Ahmad et al., 2019 ), named entity recognition (NER) ( Xie et al., 2018 ;  Chen et al., 2019 ), natural language in- ference ( Conneau et al., 2018b ), language modeling ( Adams et al., 2017 ), and machine translation (MT) ( Zou et al., 2013 ;  Lample et al., 2018a ; Artetxe et al., 2018b;  Lample et al., 2018b ). The goal of these CLWE methods is to learn embeddings in a shared vector space for two or more languages. There are two main paradigms for learning CLWE: cross-lingual alignment and joint training. The most successful approach has been the cross-lingual embedding alignment method ( Mikolov et al., 2013b ), which relies on the assumption that monolingually-trained continuous word embed- ding spaces share similar structure across different languages. The underlying idea is to first inde- pendently train embeddings in different languages using monolingual corpora alone, and then learn a mapping to align them to a shared vector space. Such a mapping can be trained in a supervised fashion using parallel resources such as bilingual lexicons ( Xing et al., 2015 ;  Smith et al., 2017 ;  Joulin et al., 2018b ;  Jawanpuria et al., 2019 ), or even in an unsupervised 2 manner based on distribu- tion matching ( Zhang et al., 2017a ;  Conneau et al., 2018a ; Artetxe et al., 2018a;  Zhou et al., 2019 ). Recently, it has been shown that alignment methods can also be effectively applied to contextualized word representations ( Schuster et al., 2019 ;  Aldarmaki & Diab, 2019 ).

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 Another successful line of research for CLWE considers joint training methods, which optimize a monolingual objective predicting the context of a word in a monolingual corpus along with either a hard or soft cross-lingual constraint. Similar to alignment methods, some early works rely on bilingual dictionaries ( Ammar et al., 2016 ;  Duong et al., 2016 ) or parallel corpora ( Luong et al., 2015 ;  Gouws et al., 2015 ) for direct supervision. More recently, a seemingly naive unsupervised joint training approach has received growing attention due to its simplicity and effectiveness. In par- ticular,  Lample et al. (2018b)  reports that simply training embeddings on concatenated monolingual corpora of two related languages using a shared vocabulary without any cross-lingual resources is able to produce higher accuracy than the more sophisticated alignment methods on unsupervised MT tasks. Besides, for contextualized representations, unsupervised multilingual language model pretraining using a shared vocabulary has produced state-of-the-art results on multiple benchmarks 3 ( Devlin et al., 2019 ;  Artetxe & Schwenk, 2019 ;  Lample & Conneau, 2019 ). Despite a large amount of research on both alignment and joint training, previous work has neither performed a systematic comparison between the two, analyzed their pros and cons, nor elucidated when we may prefer one method over the other. Particularly, it's natural to ask: (1) Does the phenomenon reported in  Lample et al. (2018b)  extend to other cross-lingual tasks? (2) Can we em- ploy alignment methods to further improve unsupervised joint training? (3) If so, how would such a framework compare to supervised joint training methods that exploit equivalent resources, i.e., bilin- gual dictionaries? (4) And lastly, can this framework generalize to contextualized representations? In this work, we attempt to address these questions. Specifically, we first evaluate and compare alignment versus joint training methods across three diverse tasks: BLI, cross-lingual NER, and un- supervised MT. We seek to characterize the conditions under which one approach outperforms the other, and glean insight on the reasons behind these differences. Based on our analysis, we further propose a simple, novel, and highly generic framework that uses unsupervised joint training as ini- tialization and alignment as refinement to combine both paradigms. Our experiments demonstrate that our framework improves over both alignment and joint training baselines, and outperforms exist- ing methods on the MUSE BLI benchmark. Moreover, we show that our framework can generalize to contextualized representations such as Multilingual BERT, producing state-of-the-art results on the CoNLL cross-lingual NER benchmark. To the best of our knowledge, this is the first framework that combines previously mutually-exclusive alignment and joint training methods.

Section Title: BACKGROUND: CROSS-LINGUAL REPRESENTATIONS
  BACKGROUND: CROSS-LINGUAL REPRESENTATIONS

Section Title: Notation
  Notation We assume we have two different languages {L 1 , L 2 } and access to their corresponding training corpora. We use V Li = {w j Li } n L i j=1 to denote the vocabulary set of the ith language where each w j Li represents a unique token, such as a word or subword. The goal is to learn a set of em- beddings E = {x j } m j=1 , with x j ∈ R d , in a shared vector space, where each token w j Li is mapped to a vector in E. Ideally, these vectorial representations should have similar values for tokens with similar meanings or syntactic properties, so they can better facilitate cross-lingual transfer.

Section Title: ALIGNMENT METHODS
  ALIGNMENT METHODS Given the notation, alignment methods consist of the following steps: Step 1: Train an embedding set E 0 = E L1 ∪ E L2 , where each subset E Li = {x j Li } n L i j=1 is trained independently using the ith language corpus and contains an embedding x j Li for each token w j Li . Step 2: Obtain a seed dictionary D = {(w i L1 , w j L2 )} K k=1 , either provided or learnt unsupervised. Step 3: Learn a projection matrix W ∈ R d×d based on D, resulting in a final embedding set E A = (W · E L1 ) ∪ E L2 in a shared vector space. To find the optimal projection matrix W ,  Mikolov et al. (2013b)  proposed to solve the following optimization problem: Published as a conference paper at ICLR 2020 where X L1 and X L2 are matrices of size d × K containing embeddings of the words in D.  Xing et al. (2015)  later showed further improvement could be achieved by restricting W to an orthogonal matrix, which turns the Eq.(1) into the Procrustes problem with the following closed form solution: W * = U V T , (2) with U ΣV T = SVD(X L2 X T L1 ) (3) where W * denotes the optimal solution and SVD(·) stands for the singular value decomposition. As surveyed in Section 5, different methods ( Smith et al., 2017 ;  Conneau et al., 2018a ;  Joulin et al., 2018b ; Artetxe et al., 2018a) differ in the way how they obtain the dictionary D and how they solve for W in step 3. However, most of them still involve solving the Eq.(2) as a crucial step.

Section Title: JOINT TRAINING METHODS
  JOINT TRAINING METHODS Joint training methods in general have the following objective: L J = L 1 + L 2 + R(L 1 , L 2 ) (4) where L 1 and L 2 are monolingual objectives and R(L 1 , L 2 ) is a cross-lingual regularization term. For example,  Klementiev et al. (2012)  use language modeling objectives for L 1 and L 2 . The term R(L 1 , L 2 ) encourages alignment of representations of words that are translations. Training an em- bedding set E J = E L1 ∪ E L2 is usually done by directly optimizing L J . While supervised joint training requires access to parallel resources, recent studies ( Lample et al., 2018b ;  Devlin et al., 2019 ;  Artetxe & Schwenk, 2019 ;  Lample & Conneau, 2019 ) have suggested that unsupervised joint training without such resources is also effective. Specifically, they show that the cross-lingual regularization term R(L 1 , L 2 ) does not require direct cross-lingual supervision to achieve highly competitive results. This is because the shared words between L 1 and L 2 can serve implicitly as anchors by sharing their embeddings to ensure that representations of different languages lie in a shared space. Using our notation, the unsupervised joint training approach takes the following steps: Step 1: Construct a joint vocabulary V J = V L1 ∪ V L2 that is shared across two languages. Step 2: Concatenate the two training corpora and learn an embedding set E J corresponding to V J . The joint vocabulary is composed of three disjoint sets: V 1 J , V 2 J , V s J , where V s J = V L1 ∩ V L2 is the shared vocabulary set and V i J is the set of tokens that appear in the ith language only. Note that a key difference of existing supervised joint training methods is that embeddings corresponding to V s J are not shared between E L1 and E L2 , meaning that they are disjoint, as in alignment methods.

Section Title: DISCUSSION
  DISCUSSION While alignment methods have had great success, there are still some critical downsides, among which we stress the following points: 1. While recent studies in unsupervised joint training have suggested the potential benefits of word sharing, alignment methods rely on two disjoint sets of embeddings. Along with some possible loss of information due to no sharing, one consequence is that finetuning the aligned embeddings on downstream tasks may be sub-optimal due to the lack of cross- lingual constraints at the finetuning stage, whereas shared words can fulfill this role in jointly trained models. 2. A key assumption of alignment methods is the isomorphism of monolingual embedding spaces. However, some recent papers have challenged this assumption, showing that it does not hold for many language pairs ( Søgaard et al., 2018 ;  Patra et al., 2019 ). Also notably,  Ormazabal et al. (2019)  suggests that this limitation results from the fact that the two sets of monolingual embeddings are independently trained. On the other hand, the unsupervised joint training method is much simpler and doesn't share these disadvantages with the alignment methods, but there are also some key limitations: 1. It assumes that all shared words across two languages serve implicitly as anchors and thus need not be aligned to other words. Nonetheless, this assumption is not always true, leading to misalignment. For example, the English word "the" will most likely also appear in the training corpus of Spanish, but preferably it should be paired with Spanish words such as "el" and "la" instead of itself. We refer to this problem as oversharing. 2. It does not utilize any explicit form of seed dictionary as in alignment methods, resulting in potentially less accurate alignments, especially for words that are not shared. Lastly, while the supervised joint training approach does not have the same issues of unsupervised joint training, it shares limitation 1 of the alignment methods. We empirically compare both joint training and alignment approaches in Section 4 and shed light on some of these pros and cons for both paradigms (See Section 4.3.1).

Section Title: PROPOSED FRAMEWORK
  PROPOSED FRAMEWORK Motivated by the pros and cons of both paradigms, we propose a unified framework that first uses unsupervised joint training as a coarse initialization and then applies alignment methods for refine- ment, as demonstrated in  Figure 1 . Specifically, we first build a single set of embeddings with a shared vocabulary through unsupervised joint training, so as to alleviate the limitations of alignment methods. Next, we use a vocabulary reallocation technique to mitigate oversharing, before finally resorting back to alignment methods to further improve the embeddings' quality. Lastly, we show that this framework can generalize to contextualized representations.

Section Title: UNIFYING ALIGNMENT WITH JOINT TRAINING
  UNIFYING ALIGNMENT WITH JOINT TRAINING Our proposed framework mainly involves three components and we discuss each of them as follows.

Section Title: Joint Initialization
  Joint Initialization We use unsupervised joint training ( Lample et al., 2018b ) to train the initial CLWE. As described in Section 2.2, we first obtain a joint vocabulary V J and train its corresponding set of embeddings E J on the concatenated corpora of two languages. This allows us to obtain a single set of embeddings that maximizes sharing across two languages. To train embeddings, we used fastText 4 ( Bojanowski et al., 2017 ) in all our experiments for both word and subword tokens.

Section Title: Vocabulary Reallocation
  Vocabulary Reallocation As discussed in Section 2.3, a key issue of unsupervised joint training is oversharing, which prohibits further refinement as shown in  Figure 1 . To alleviate this drawback, we attempt to "unshare" some of the overshared words, so their embeddings can be better aligned in the next step. Particularly, we perform a vocabulary reallocation step such that words appearing Published as a conference paper at ICLR 2020 mostly exclusively in the ith language are reallocated from the shared vocabulary V s J to V i J , whereas words that appear similarly frequent in both languages stay still in V s J . Formally, for each token w in the shared vocabulary V s J , we use the ratio of counts within each language to determine whether it belongs to the shared vocabulary: r = T L2 T L1 · C L1 (w) C L2 (w) , (5) where C Li (w) is the count of w in the training corpus of the ith language and T Li = w C Li (w) is the total number of tokens. The token w is allocated to the shared vocabulary if 1 − γ γ ≤ r ≤ γ 1 − γ , (6) where γ is a hyper-parameter. Otherwise, we put w into either V 1 J or V 2 J , where it appears mostly frequent. The above process generates three new disjoint vocabulary sets V 1 J , V 2 J , V s J and their corresponding embeddings E 1 J , E 2 J , E s J that are used thereafter. Note that, V J = V J and E J = E J .

Section Title: Alignment Refinement
  Alignment Refinement The unsupervised joint training method does not explicitly utilize any dictionary or form of alignment. Thus, the resulting embedding set is coarse and ill-aligned in the shared vector space, as demonstrated in  Figure 1 . As a final refinement step, we utilize any off-the-shelf alignment method to refine alignments across the non-sharing embedding sets, i.e. mapping E 1 J to E 2 J and leaving E s J untouched. This step could be conducted by either supervised or unsupervised alignment method and we compare both in our experiments.

Section Title: EXTENSION TO CONTEXTUALIZED REPRESENTATIONS
  EXTENSION TO CONTEXTUALIZED REPRESENTATIONS As our framework is highly generic and applicable to any alignment and unsupervised joint train- ing methods, it can naturally generalize to contextualized word representations by aligning the fixed outputs of a multilingual encoder such as multilingual BERT (M-BERT) ( Devlin et al., 2019 ). While our vocab reallocation technique is no longer necessary as contextualized representations are depen- dent on context and thus dynamic, we can still apply alignment refinement on extracted contextual- ized features for further improvement. For instance, as proposed by  Aldarmaki & Diab (2019) , one method to perform alignment on contextualized representations is to first use word alignment pairs extracted from parallel corpora as a dictionary, learn an alignment matrix W based on it, and apply W back to the extracted representations. To obtain W , we can solve Eq.( 1) as described in Sec- tion 2.1, where the embedding matrices X L1 and X L2 now contain contextualized representations of aligned word pairs. Note that this method is applicable to fixed representations but not finetuning.

Section Title: EXPERIMENTS
  EXPERIMENTS We evaluate the proposed approach and compare with alignment and joint training methods on three NLP benchmarks. This evaluation aims to: (1) systematically compare alignment vs. joint training paradigms and reveal their pros and cons discussed in Section 2.3, (2) show that the proposed frame- work can effectively alleviate limitations of both alignment and joint training, and (3) demonstrate the effectiveness of the proposed framework in both non-contextualized and contextualized settings.

Section Title: EVALUATION TASKS
  EVALUATION TASKS

Section Title: Bilingual Lexicon Induction (BLI)
  Bilingual Lexicon Induction (BLI) This task has been the de facto evaluation task for CLWE methods. It considers the problem of retrieving the target language translations of source langauge words. We use bilingual dictionaries complied by  Conneau et al. (2018a)  and test on six diverse language pairs, including Chinese and Russian, which use a different writing script than English. Each test set consists of 1500 queries and we report precision at 1 scores (P@1), following standard evaluation practices ( Conneau et al., 2018a ;  Glavas et al., 2019 ). non-contextualized and contextualized word representations on the CoNLL 2002 and 2003 bench- marks ( Tjong Kim Sang, 2002 ;  Tjong Kim Sang & De Meulder, 2003 ), which contain 4 European languages. To measure the quality of CLWE, we perform zero-shot cross-lingual classification, where we train a model on English and directly apply it to each of the other 3 languages.

Section Title: Unsupervised Machine Translation (UMT)
  Unsupervised Machine Translation (UMT) Lastly, we test our approach using the unsupervised MT task, on which the initialization of CLWE plays a crucial role ( Lample et al., 2018b ). Note that our purpose here is to directly compare with similar studies in  Lample et al. (2018b) , and thus we follow their settings and consider two language pairs, English-French and English-German, and evaluate on the widely used WMT'14 en-fr and WMT'16 en-de benchmarks.

Section Title: EXPERIMENTAL SETUP
  EXPERIMENTAL SETUP For the BLI task, we compare our framework to recent state-of-the-art methods. We obtain numbers from the corresponding papers or  Zhou et al. (2019) , and use the official tools for MUSE ( Conneau et al., 2018a ), GeoMM ( Jawanpuria et al., 2019 ) and RCSLS ( Joulin et al., 2018b ) to obtain miss- ing results. We consider the method of  Duong et al. (2016)  for supervised joint training based on bilingual dictionaries, which is comparable to supervised alignment methods in terms of resources used. For unsupervised joint training, we train uncased joint fastText word vectors of dimension 300 on concatenated Wikipedia corpora of each language pair with default parameters. The hyper- parameter γ is selected from {0.7, 0.8, 0.9, 0.95} on validation sets. For the alignment refinement step in our proposed framework, we use RCSLS and GeoMM to compare with supervised meth- ods, and MUSE for unsupervised methods. In addition, we include an additional baseline of joint training, denoted as Joint - Replace, which is identical to unsupervised joint training except that it utilizes a seed dictionary to randomly replace words with their translations in the training corpus. Following standard practices, we consider the top 200k most frequent words and use the cross- domain similarity local scaling (CSLS) ( Conneau et al., 2018a ) as the retrieval criteria. Note that a concurrent work ( Artetxe et al., 2019 ) proposed a new retrieval method based on MT systems and produced state-of-the-art results. Although their method is applicable to our framework, it has high computational cost and is out of the scope of this work. For the NER task: (1) For non-contextualized representations, we train embeddings the same way as in the BLI task and use a vanilla Bi-LSTM-CRF model ( Lample et al., 2016 ). For all alignment steps, we apply the supervised Procrustes method using dictionaries from the MUSE library for simplicity. (2) For contextualized representations, we use M-BERT, an unsupervised joint training model, as 6 We found that the official evaluation script of MUSE ignores test pairs that are out-of-vocabulary (OOV) on the source or the target sides, which could occur occasionally when evaluating our model due to the proposed vocabulary reallocation step. To ensure fair comparison, we include these OOV pairs. Specifically, if the source word is OOV, we retrieve itself; otherwise, we count the pair as incorrect. Please see appendix B for details. our base model and apply our proposed framework on it by first aligning its extracted features and then feeding them to a task-specific model (M-BERT Feature + Align). Specifically, we use the sum of the last 4 M-BERT layers' outputs as the extracted features. To obtain the alignment matrices, one for each layer, we use 30k parallel sentences from the Europarl corpus for each language pair and follow the procedure of Section 3.2. We feed the extracted features as inputs to a task-specific model with 2 Bi-LSTM layers and a CRF layer (see appendix A). We compare our framework to both finetuning (M-BERT Finetune), which has been studied by previous papers, and feature extraction (M-BERT Feature). Lastly, we also compare against XLM, a supervised joint training model. For the UMT task, we use the exact same data, architecture and parameters released by  Lample et al. (2018b)  7 . We simply use different embeddings trained with the same data as inputs to the model.

Section Title: RESULTS AND ANALYSIS
  RESULTS AND ANALYSIS

Section Title: ALIGNMENT VS. JOINT TRAINING
  ALIGNMENT VS. JOINT TRAINING We compare alignment methods with joint training on all three downstream tasks. As shown in  Table 1  and  Table 3 , we find alignment methods significantly outperform the joint training approach by a large margin in all language pairs for both BLI and NER. However, the unsupervised joint training method is superior than its alignment counterpart on the unsupervised MT task as demonstrated in 2(c). While these results demonstrate that their relative performance is task-dependent, we conduct further analysis to reveal three limitations as discussed in Sec 2.3. First, their poor performance on BLI and NER tasks shows that unsupervised joint training fails to generate high-quality alignments due to the lack of a fine-grained seed dictionary as discussed in its limitation 2. To evaluate accuracy on words that are not shared, we further remove test pairs of the same surface form (e.g. (hate, hate) as a test pair for en-de) of the BLI task and report their results in  Table 2 . We find unsupervised joint training (row 2) to achieve extremely low scores which shows that emebddings of non-shared parts are poorly aligned, consistent with the PCA visualization shown in  Figure 1 . Moreover, we delve into the relative performance of the two paradigms on the MT task by plotting their test BLEU scores of the first 20 epochs in Figure 2(a) and 2(b). We observe that the alignment method actually obtains higher BLEU scores in the first few epochs, but gets surpassed by joint training in later epochs. This shows the importance of parameter sharing as discussed in limitation 1 of alignment methods: shared words can be used as a cross-lingual constraint for unsupervised joint training during fine-tuning but this constraint cannot easily be used in alignment methods. The lack of sharing is also a limitation for the supervised joint training method, which performs poorly on the MT task even with supervision as shown in Figure 2(c). Lastly, we demonstrate that oversharing can be sub-optimal for unsupervised joint training as dis- cussed in its limitation 2. Specifically, we conduct ablation studies for our framework in  Table 1 . Applying alignment refinement on unsupervised joint training without any vocabulary reallocation does not improve its performance (row 13). On the other hand, simple vocabulary reallocation alone boosts the performance by quite a margin (row 11). This shows some words are shared erroneously across languages in unsupervised joint training, thereby hindering its performance. In addition, while utilizing a seed dictionary (row 10) improves the performance of unsupervised joint train- ing, it still suffers from the oversharing problem and performs worse compared to supervised joint training (row 9).

Section Title: EVALUATION OF PROPOSED FRAMEWORK
  EVALUATION OF PROPOSED FRAMEWORK As shown in  Table 1 ,  Table 3 , and  Figure 2 , our proposed framework substantially improves over the alignment and joint training baselines on all three tasks. In particular, it outperforms existing methods on all language pairs for the BLI task (using the CSLS as retrieval metric) and achieves state-of-the-art results on 2 out of 3 language pairs for the NER task. Besides, we show that it alleviates limitations of alignment and joint training methods shown in the previous section. First, the proposed framework largely improves the poor alignment of unsupervised joint training, especially for non-sharing parts. As shown in  Table 1 , the proposed Joint Align framework achieves comparable results to prior methods in the unsupervised case (row 12) and it outperforms previ- ous state-of-the-art methods in the supervised setting (row 15). Specifically, our proposed frame- work can generate well-aligned embeddings after alignment refinement is applied to the initially ill-aligned embeddings, as demonstrated in  Figure 1 . This is further verified by results in  Table 2 , where our proposed framework largely improves accuracy on words not shared between two lan- guages over the unsupervised joint training baseline (row 3 and 6 vs row 2). Besides, our ablation study in  Table 1  further shows the effectiveness of the proposed vocabulary reallocation technique, which alleviates the issue of oversharing. Particularly, we observe no im- provement compared to unsupervised joint training baseline (row 8) when an alignment refinement step is used without vocabulary reallocation (row 13), while a vocabulary reallocation step alone significantly improves the performance (row 11). This is consistent with  Figure 1  and shows that the oversharing is a bottleneck for applying alignment methods to joint training. It also suggests detecting what to share is crucial to achieve better cross-lingual transfer. Lastly, while supervised joint training shares the limitation 1 of alignment methods and performs poorly when finetuned, our proposed framework exploits the same idea of vocabulary sharing used in unsupervised joint training. In the MT tasks, our framework obtains a maximum gain of 2.97 BLEU over baselines we ran and consistently performs better than results reported in  Lample et al. (2018b) . In addition,  Figure 2  shows that Joint Align not only converges faster in earlier training epochs but also consistently outperforms the two baselines thereafter. These empirical findings demonstrate the effectiveness of our proposed methods in the non-contextualized case.

Section Title: CONTEXTUALIZED WORD REPRESENTATIONS
  CONTEXTUALIZED WORD REPRESENTATIONS As can be seen in  Table 3 , when using our framework (M-BERT Feature + Align), we achieve state-of-the-art results on cross-lingual NER on 2 out of 3 languages and the overall average. This shows that our framework can effectively generalize to contextualized representations. Specifically, our framework improves over both the M-BERT feature extraction and finetuning baselines on all three language pairs. However, when compared to non-contextualized results, the gain of using alignment refinement on top of unsupervised joint training is much smaller. This suggests that, as the contextualized unsupervised joint training model performs very well already even without Published as a conference paper at ICLR 2020 (a) fr-en (b) en-de (c) any supervision, it is harder to achieve large improvements. While alignment refinement relies on word alignment, a process that is noisy itself, a better alignment approach may be warranted. Lastly, the reason why a supervised joint training model, XLM, performs worse than its unsupervised counterpart, M-BERT, is likely that XLM uses an uncased vocabulary, where casing information is important for NER tasks.

Section Title: RELATED WORK
  RELATED WORK Word embeddings ( Mikolov et al., 2013a ;  Ruder et al., 2019 ) are a key ingredient to achieving success in monolingual NLP tasks. However, directly using word embeddings independently trained for each language may cause negative transfer ( Wang et al., 2019 ) in cross-lingual transfer tasks. In order to capture the cross-lingual mapping, a rich body of existing work relying on cross-lingual supervision, including bilingual dictionaries ( Mikolov et al., 2013a ;  Faruqui & Dyer, 2014 ;  Artetxe et al., 2016 ;  Xing et al., 2015 ;  Duong et al., 2016 ;  Gouws & Søgaard, 2015 ;  Joulin et al., 2018a ), sentence-aligned corpora ( Kočiskỳ et al., 2014 ;  Hermann & Blunsom, 2014 ;  Gouws et al., 2015 ) and document-aligned corpora ( Vulić & Moens, 2016 ;  Søgaard et al., 2015 ). Besides, unsupervised alignment methods aim to eliminate the requirement for cross-lingual su- pervision. Early work of  Cao et al. (2016)  matches the mean and the standard deviation of two embedding spaces after alignment.  Barone (2016) ;  Zhang et al. (2017a ;b);  Conneau et al. (2018a)  adapted a generative adversarial network (GAN) ( Goodfellow et al., 2014 ) to make the distribu- tions of two word embedding spaces indistinguishable. Follow-up works improve upon GAN-based training for better stability and robustness by introducing Sinkhorn distance ( Xu et al., 2018 ), by stochastic self-training (Artetxe et al., 2018a), or by introducing latent variables ( Dou et al., 2018 ). While alignment methods utilize embeddings trained independently on different languages, joint training methods train word embeddings at the same time.  Klementiev et al. (2012)  train a bilin- gual dictionary-based regularization term jointly with monolingual language model objectives while  Kočiskỳ et al. (2014)  defines the cross-lingual regularization with the parallel corpus. Another branch of methods ( Xiao & Guo, 2014 ;  Gouws & Søgaard, 2015 ;  Ammar et al., 2016 ;  Duong et al., 2016 ) build a pseudo-bilingual corpus by randomly replacing words in monolingual corpus with their translations and use monolingual word embedding algorithms to induce bilingual represen- tations. The unsupervised joint method by  Lample & Conneau (2019)  simply exploit words that share the same surface form as bilingual "supervision" and directly train a shared set of embedding with joint vocabulary. Recently, unsupervised joint training of contextualized word embeddings through the form of multilingual language model pretraining using shared subword vocabularies has produced state-of-the-art results on various benchmarks ( Devlin et al., 2019 ;  Artetxe & Schwenk, 2019 ;  Lample & Conneau, 2019 ;  Pires et al., 2019 ;  Wu & Dredze, 2019 ). A concurrent work by  Ormazabal et al. (2019)  also compares alignment and joint method in the bilingual lexicon induction task. Different from their setup which only tests on supervised settings, we conduct analysis across various tasks and experiment with both supervised and unsupervised conditions. While  Ormazabal et al. (2019)  suggests that the combination of the alignment and joint model could potentially advance the state-of-art of both worlds, we propose such a framework and empirically verify its effectiveness on various tasks and settings.

Section Title: CONCLUSION
  CONCLUSION In this paper, we systematically compare the alignment and joint training methods for CLWE. We point out that the nature of each category of methods leads to certain strengths and limitations. The empirical experiments on extensive benchmark datasets and various NLP tasks verified our analysis. To further improve the state-of-art of CLWE, we propose a simple hybrid framework which combines the strength from both worlds and achieves significantly better performance in the BLI, MT and NER tasks. Our work opens a promising new direction that combines two previously exclusive lines of research. For future work, an interesting direction is to find a more effective word sharing strategy.

```
