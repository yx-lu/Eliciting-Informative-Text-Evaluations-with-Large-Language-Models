Title:
```
Under review as a conference paper at ICLR 2020 SKEW-EXPLORE: LEARN FASTER IN CONTINUOUS SPACES WITH SPARSE REWARDS
```
Abstract:
```
In many reinforcement learning settings, rewards which are extrinsically available to the learning agent are too sparse to train a suitable policy. Beside reward shap- ing which requires human expertise, utilizing better exploration strategies helps to circumvent the problem of policy training with sparse rewards. In this work, we introduce an exploration approach based on maximizing the entropy of the visited states while learning a goal-conditioned policy. The main contribution of this work is to introduce a novel reward function which combined with a goal proposing scheme, increases the entropy of the visited states faster compared to the prior work. This improves the exploration capability of the agent, and there- fore enhances the agent's chance to solve sparse reward problems more efficiently. Our empirical studies demonstrate the superiority of the proposed method to solve different sparse reward problems in comparison to the prior work.
```

Figures/Tables Captions:
```
Figure 1: A sparse-reward task trained by a YuMi robot in simulation and de- ployed to the real hardware. The task consists of (1) opening a door, (2) press- ing a button, and (3) closing the door.
Figure 2: This figure shows a comparison between the state distribution p(s) (dashed blue) and a corresponding novelty frontier distribution skewed from p(s) (red).
Figure 3: Our method, Skew-Explore, aims to obtain uniform state visitation distribution estimated from the history state set. We start by sampling from our history state set, and weighting the states such that less-visited states are assigned with higher weights. We then train a skewed distribution p skewed (s) using the weighted samples as the novelty frontier distribution. Next we sample reference points on the novelty frontier distribution and run our policy to explore around the points.
Figure 4: The PointMaze environment and the DoorOpen environment.
Figure 5: Results of how coverage and entropy changes over iterations in PointMaze (left) and the DoorOpen (right) environments.
Figure 6: Results of how coverage changes over iterations in the PointMaze environment.
Figure 7: (a) Relation between average extrinsic reward per iterations. (b) From up to down, left to right, this figure shows a sequence of the converged policy for the OpenPressClose task, which is a long horizon task with a sparse reward given only at the end.
Table 1: This shows the number of trajectories needed for the algorithm to converge to five uniformly sampled target states in each environment. A successful con- vergence is measured as 90% of the states receive the extrinsic reward with a standard deviation of less then 3%. The two other methods Skew-Fit and RND cannot solve the problem below upper limit 3000 trajectories.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The reward is given only at the end of a successful trial. Reinforcement Learning (RL) is based on performing ex- ploratory actions in a trial-and-error manner and reinforc- ing those actions that result in superior reward outcomes. Exploration plays an important role in solving a given se- quential decision-making problem. A RL agent cannot improve its behaviour without receiving rewards exceed- ing the expectation of the agent, and this happens only as the consequence of properly exploring the environment. In this paper, we propose a method to train a policy which efficiently explores a continuous state space. Our method is particularly well-suited to solve sequential decision- making tasks with sparse terminal rewards, i.e., rewards received at the end of a successful interaction with the environment. We propose to directly maximize the en- tropy of the history states by exploiting the mutual infor- mation between the history states and a number of refer- ence states. To achieve this, we introduce a novel reward function which, given the references, shapes the distribu- tion of the history states. This reward function, combined with goal proposing learning frameworks, maximizes the entropy of the history states. We demonstrate that this way of directly maximizing the state entropy, compared to indirectly maximizing the mutual information ( Warde- Farley et al., 2018 ;  Pong et al., 2019 ) improves the ex- ploration of the state space as well as the convergence speed at solving tasks with sparse terminal rewards. Maximizing the mutual information between the visited states and the goal states, I(S; G), re- sults in a natural exploration of the environment while learning to reach to different goal states ( Warde-Farley et al., 2018 ;  Pong et al., 2019 ). The mutual information can be written as I(S; G) = h(G) − h(G|S); therefore maximizing the mutual information is equivalent to maximizing the en- Under review as a conference paper at ICLR 2020 tropy of the goal state while reducing the conditional entropy (conditioned on the goal state). The first term, encourages the agent to choose its own goal states as diverse as possible, therefore im- proving the exploration, and the second term forces the agent to reach the different goals it has specified for itself, i.e., training a goal-conditioned policy, π(.|s, g). Instead of maximizing the mu- tual information, we propose to maximize the entropy of the visited states directly, i.e., maximizing h(S) = h(Z) + h(S|Z) − h(Z|S), where Z is a random variable that represents the reference points of promising areas for exploration. Therefore, in our formulation, we have an extra term, h(S|Z), which encourages maximizing the entropy of the state conditioned on the reference points. This extra term, implemented by the proposed reward function, helps the agent to explore better at the vicinity of the references. We call our method Skew-Explore, since similar to Skew-Fit introduced by  Pong et al. (2019) , it skews the distribution of the references toward the less visited states, but instead of directly reaching the goals, it explores the surrounding areas of them. We experimentally demonstrate that the new reward function enables an agent to explore the state space more efficiently in terms of covering larger areas in less time compared to the earlier meth- ods. Furthermore, we demonstrate that our RL agent is capable of solving long-term sequential decision-making problems with sparse rewards faster. We apply the method to three simulated tasks, including a problem to find a trajectory of a YuMi robot end-effector, to open a door of a box, pressing a button inside the box and closing the door. In this case, the sparse reward is given only when the button is pressed and the door is closed, i.e., at the end of about one minute of continuous interaction with the environment. To validate appropriateness of the trajectory found in simulation, we deployed it on a real YuMi robot, as shown in  Figure 1 . The main contributions of this paper can be summarized as (1) introducing a novel reward function which increases the entropy of the history states much faster compared to the prior work, and (2) experimentally demonstrating the superi- ority of the proposed algorithm to solve three different sparse reward sequential decision-making problems.

Section Title: RELATED WORK
  RELATED WORK Prior works have studied different algorithms for addressing the exploration problem. In this section, we summarize related works in the domain where rewards from the environment are sparse or absent.

Section Title: Intrinsic Reward
  Intrinsic Reward One way to encourage exploration is to define an intrinsically-motivated reward, including methods that assimilate the definition of curiosity in psychology ( Oudeyer et al., 2007 ;  Pathak et al., 2017 ). These methods have found success in domains like video games ( Ostrovski et al., 2017 ;  Burda et al., 2018 ). In these approaches, the "novelty", "curiosity" or "surprise" of a state is computed as an intrinsic reward using mechanisms such as state-visiting count and prediction error ( Schmidhuber, 1991 ;  Stadie et al., 2015 ;  Achiam & Sastry, 2017 ;  Pathak et al., 2017 ). By considering this information, the agent is encouraged to search for areas that are less visited or have complex dynamics. However, as pointed out by  Ecoffet et al. (2019) , an agent driven by intrinsic reward may suffer from the problem of detaching from the frontiers of high intrinsic reward area. Due to catastrophic forgetting, it may not be able to go back to previous areas that have not yet been fully explored ( Kirkpatrick et al., 2017 ;  Ellefsen et al., 2015 ). Our method is able to keep tracking the novelty frontier and train policy to explore different areas in the frontier.

Section Title: Diverse Skill/Option Discovery
  Diverse Skill/Option Discovery Methods that aim to learn a set of behaviours which are distinct from each other, allow the agent to interact with the environment without rewards for a particular task.  Gregor et al. (2016)  introduced an option discovery technique based on maximizing the mutual information between the options and the final states of the trajectories.  Eysenbach et al. (2018) ;  Florensa et al. (2017a) ;  Savinov et al. (2018)  proposed to learn a fixed set of skills by maximizing the mutual information through an internal objective computed using a discriminator.  Achiam et al. (2018)  extended the prior works by considering the whole trajectories and introduced a curriculum learning approach that gradually increases the number of skills to be learned. In these works, the exploration is encouraged implicitly through learning diverse skills. However, it is difficult to control the direction of exploration. In our method, we maintain a proposing module which tracks the global information of the states we have visited so far, and keep proposing reference points that guide the agent to the more promising areas for exploration.

Section Title: Self-Goal Proposing
  Self-Goal Proposing Self-goal proposing methods are often combined with a goal-conditioned policy ( Kaelbling, 1993 ;  Andrychowicz et al., 2017 ), where a goal (or task) generation model is Under review as a conference paper at ICLR 2020 trained jointly with a goal reaching policy. The agent receives rewards in terms of completing the internal tasks which makes it possible to explore the state space without any supervision from the environment.  Sukhbaatar et al. (2017)  described a scheme with two agents. The first one proposes tasks by performing a sequence of actions and the other repeats the actions in reverse order.  Held et al. (2018)  introduced a method that automatically label and propose goals at the appropriate level of difficulty using adversarial training. Similar works are proposed by  Colas et al. (2018) ;  Veeriah et al. (2018) ;  Florensa et al. (2017b) , where goals are selected based on the learning progress.  Warde- Farley et al. (2018)  trained a goal-conditioned policy by maximizing the mutual information between the goal states and the achieved states. The goals are selected from the agent's recent experience with strategies. Later,  Pong et al. (2019)  applied a similar idea of using mutual information. They maximize the entropy of a goal sampling distribution. The focus of these methods is on learning a policy that can reach diverse goals. Although gradually increasing the scale of the goal proposing network, the agent may eventually cover the entire state space, exploration itself is not efficient. In our work, we adopt the same idea of maximizing the entropy of the goal sampling distribution by  Pong et al. (2019) . However, instead of using the goal-conditioned policy, we introduce a reference point-conditioned policy which greatly increases the efficiency of exploration.

Section Title: SKEW-EXPLORE: SEARCHING FOR THE SPARSE REWARD
  SKEW-EXPLORE: SEARCHING FOR THE SPARSE REWARD We discuss the policy learning problem in continuous state and action spaces, which we model as an infinite-horizon Markov decision process (MDP). The MDP is fully characterized by a tuple (S, A, p a (s, s ), R a (s, s )), where S, the state space, and A, the action space, are subsets of R n , the unknown transition probability p : S × A × S → [0, inf) indicates the probability density function of the next state s given the current state s ∈ S and the action a ∈ A. For each transition, the space associated environment E emits an extrinsic reward according to function R : S × A → R. The objective of the agent is to maximize the discounted return, i.e. return R = ∞ ts=0 γ ts r ts , where γ is a discounted factor and r ts is the reward received at each step t s . In this study, we consider an agent interacting in an environment E with sparse reward. The sparse reward r is modelled as a truncated Gaussian function with a narrow range. From previous interactions, the agent holds an interaction set I t , in which transaction triples (s j , a j , s j+1 ), ∀j ∈ {1, · · · , T − 1} are contained. We also extract the states s j from I t to form a history state set S t , which contains all visited states by the agent until iteration t. The objective of our method is to find an arbitrary external goal in a continuous state space and converge to a policy that maximizes the R as fast as possible. This involves two processes 1) Find the external reward through efficient exploration. 2) Converge to a policy that maximizes R once the external reward is found. We can use the entropy of the history state set as a neutral objective to encourage exploration, since an agent that maximizes this objective should have visited all valid states uniformly. To describe it mathematically, we define a random variable S to represent the history states that the agent has visited. The distribution of S is estimated from the history state set S t . Our goal is to encourage exploration by maximizing the entropy h(S) of the history states. However, using the entropy as the intrinsic reward directly may suffer from problems similar to other intrinsic motivated methods ( Schmidhuber, 1991 ;  Stadie et al., 2015 ;  Achiam & Sastry, 2017 ;  Pathak et al., 2017 ). As the reward of the same state is changing, the agent has the risk of detaching from the frontiers of high intrinsic reward area. We introduce a concept called novelty frontier reference point, which can be sampled from a dis- tribution that represents the novelty frontier ( Ecoffet et al., 2019 ). The novelty frontier defined in our work represents the areas near the states with lower density in distribution p(s). The frontier reference points are sampled after the distribution of the novelty frontier is updated. We define a Z to represent all the history frontier reference points with probability density p(z) estimated from a set Z t that contains all novelty frontier reference points until iteration t. The conditional probability p(s|z) defines the behaviour of the agent with respect to each refer- ence point. In this work, we model this behaviour using a state distribution function K z (s − z) parameterized by the displacement between the state and the reference point. The function K z needs to be chosen carefully as it should satisfy our expectation of the policy behaviour and also, provides an informative reward signal to train the policy. Mathematically, we can rewrite p(s) as p(s) = f (s|z)p(z)dz = K z (s − z)p(z)dz. Generally, K z (·) can be different for different z.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 However, to reduce the complexity of learning, we constrain K z (·) to be consistent for any z, mean- ing K z (s − z) = K(s − z). The definition of K(·) satisfies the definition of a kernel function. Using K(s − z), p(s) can then be further represented as By considering the law of convolution of probability distributions, we obtain S = Z + N, where N is a random variable characterized by a density function K(·). Now with this setup, we are able to to analyze our method's performance using information theory. By considering the entropy's relationship with mutual information h(S) = h(S|Z) + I(S; Z), we receive the final decomposition of our objective under the novelty frontier reference point-conditioned policy framework Eq. 1 indicates that in order to maximize the h(S), we can individually maximize/minimize each term while making other terms fixed. In the following section, we will explain the optimization process in detail. As introduced above, h(Z) is the entropy estimated from the novelty frontier reference points set Z t . To increase h(Z), we need to add a new reference point to Z t such that, the entropy estimated form Z t+1 is larger than the entropy estimated from Z t . In our method, the frontier reference points are sampled from the novelty frontier distribution which represents less history areas according to the current history states.  Pong et al. (2019)  proposed a method to skew the distribution of the history states using importance sampling, such that states with lower density can be proposed more often. In our work, we use a similar way to estimate the novelty frontier distribution. There are three steps in our process. In the first step, we estimate the p(s) from S t using a density estimator e.g. Kernel Density Estimation (KDE). In the second step, we sample Q states {s 0 , · · · , s Q } from p(s), and compute the normalized weight for each state using Eq. 2 where Y α is a normalizing constant. The state with lower p(s) has higher weight and vice versa. Fi- nally, we utilize a generative model training scheme T g (·, ·) (e.g. weighted KDE), together with sam- pled states and weights to get a skewed distribution p skewed (s) = T g ({s 0 , · · · , s n }, {w 0 , · · · , w n }) to represent the novelty frontier distribution. If Q is big enough, by choosing a α appropriately, we are able to expand our frontiers after each iteration. As a consequence, the distribution estimated from Z t will become more and more uniform and its range will become larger and larger, just like annual ring of the tree. The entropy of a continuous uniform function U (p, q) is ln(p − q) and if the distribution has a larger range, the entropy is larger as well.  Fig 2  illustrates the estimated frontier distribution skewed from p(s).

Section Title: MAXIMIZING h(S|Z) − h(Z|S): INCREASING THE EXPLORATION RANGE AROUND
  MAXIMIZING h(S|Z) − h(Z|S): INCREASING THE EXPLORATION RANGE AROUND

Section Title: REFERENCE POINTS
  REFERENCE POINTS The conditional entropy of h(S|Z) and h(Z|S) are highly correlated, maximizing/minimizing them individually are difficult. Therefore, in this section, we consider to maximize h(S|Z) - h(Z|S) Under review as a conference paper at ICLR 2020 as a whole. Using the relation S = Z + N, we rewrite the expression as h(S|Z) − h(Z|S) = h(Z + N|Z) − h(Z|Z + N), which can be further simplified (see Appendix D) as h(Z|S) − h(S|Z) ≥ h(N) − h(Z). This implies that there is a lower bound for the expression h(S|Z) − h(Z|S). For a fixed h(Z), we can maximize the lower bound h(N) − h(Z) by increasing h(N). h(N) is related to the shape and variance of the exploration distribution near the reference point. In our method, we model N as a Gaussian distribution with zero mean. In an ideal case, we would like to have as large variance as possible. However, increasing the variance also results in learning difficulty, as we need a longer trajectory to evaluate the performance and more samples to update the network. Therefore, we use the variance to control the trade-off between exploration efficiency and learning efficiency.

Section Title: DESIGNING THE REWARD FUNCTION
  DESIGNING THE REWARD FUNCTION Our algorithm requires the policy to move around a given reference point, and the distribution of the states in the trajectory should follow a Gaussian distribution centered at the reference point. In this section, we introduce an intrinsic reward function to train such policy by minimizing the the Kullback-Leibler (KL) divergence between the trajectory distribution and the desired Gaussian distribution. For each given reference point z i , we collect a trajectory τ i by running the policy with the given reference point, indicated as π(z i ), for M steps. Then, we estimate the probability density of each state s in τ i , referred to as p τi (s), using a density estimator. Finally, we check the probability density of s in the Gaussian distribution centered at z i , referred to as p zi (s). The KL-divergence between the trajectory distribution p τi and the desired distribution p zi is formulated as follows To minimize the KL-divergence, the intrinsic reward of s with respect to z i is computed as The intrinsic reward function measures the difference between the desired density of s in the trajec- tory and the actual density achieved. The reward is positive when the actual density is smaller than the desired one, when states in the trajectory are too far from the reference point, and the reward is negative when the actual density is larger than the desired one, when the agent stays too long at the reference point. An extrinsic reward r ext (s) is provided by the environment and the total reward of a time step is defined as the weighted sum of the intrinsic and the extrinsic reward. The extrinsic reward should be much greater than the intrinsic reward. The reward of each time step r(s, z i ) is defined as r(s, z i ) = w int · r int (s, z i ) + w ext · r ext (s), (5) where, w int and w ext are respective weights for internal and external rewards. The performance of the policy is closely related to the set Z t , as it records the reference points we used to train the policy until iteration t. As described in section 3.1, while we increase the entropy h(Z) by proposing new reference points form the novelty frontier to train the policy, the policy gradually gain skills to explore different areas. When a state with a large extrinsic reward is discovered, the policy eventually ignores all given reference points and converge to reach the state with the extrinsic reward. Algorithm 1 shows the whole Skew-Explore algorithm using pseudo code and our implementation of the algorithm is available online 1 . In this section, we evaluate our algorithm from three perspectives. 1) How efficient is our algorithm in terms of exploring the entire state space, and how different choice of variance affects the efficiency? 2) Is our algorithm able to converge to a stable solution for tasks with sparse reward? 3) Is our algorithm able to solve a complicated sparse reward task with long horizon? The implementation details of the ex- periments can be found in Appendix E. Two metrics are considered to evaluate the performance. They are the state distribution entropy h(S) and the cov- erage, which are estimated from history state set S t . We describe how we estimate the two metrics and their difference in Appendix A and B. A short video regarding the experiments can be found online 2 .

Section Title: POWER OF EXPLORATION
  POWER OF EXPLORATION In the first experiment, we evaluate our algorithm in term of the efficiency of exploring the state space. We test our algorithm in two simulated environments, the PointMaze and the DoorOpen environments ( Fig. 4 ). In the PointMaze environment, a point agent is controlled to move inside a maze with narrow passages. In the DoorOpen environment, a YuMi robot can open a door by grabbing the door handle. The PointMaze environment was previously used by  Florensa et al. (2017a) ;  Eysenbach et al. (2018) ;  Pong et al. (2019) , whilst environments similar to the DoorOpen environment were used by  Kalakrishnan et al. (2011) ;  Chebotar et al. (2017) ;  Pong et al. (2019) . The objective of the tasks is to explore the entire state space in a minimum amount of time. In order to evaluate the performance, we measure the efficiency as the overall coverage and the entropy of the density estimated from all history states. We compare our algorithm with two baseline algorithms: the random network distillation (RND) proposed by  Burda et al. (2018)  which is an approach using prediction error as the intrinsic reward, and Skew-Fit proposed by  Pong et al. (2019)  which combines a goal proposing network with a goal-conditioned policy. We consider two configurations. The first one is the proximal policy optimization (PPO)  Schulman et al. (2017)  together with a Long Short-Term Memory network (LSTM)  Hochreiter & Schmidhuber (1997) . The second configuration is soft actor-critic (SAC)  Haarnoja et al. (2018)  and hindsight experience replay (HER)  Andrychowicz et al. (2017) . We note here that RND is only tested with PPO and LSTM as it was not designed for off-policy methods. For each configuration, we run Under review as a conference paper at ICLR 2020 12 times and compare the mean and variance.  Fig. 5  shows the results for all 5 configurations. We can see that our method, SAC+HER+Skew-Explore, makes both coverage and entropy increase faster than the other methods. It also increases with relatively small variances.  Fig 6  illustrates how coverage changes for both our method Skew-Explore and Skew-Fit. In this figure, we see how our method is able to cover the state space (area in this case) faster than Skew-Fit. To further analyze how different choices of the variance of N affects the exploration efficiency, several values of variances 0 2 , 1 2 , 2 2 , 3 2 , 4 2 and 6 2 are tested in the PointMaze environment. After 80 iterations, estimated entropy 40.2±1.2, 49.3±0.7, 51.4±0.8, 51.4±0.7, 51.4±0.7 and 48.9±3.3 are received. We observe that while the variance increases, the performance first increases and then decreases.

Section Title: POWER OF SOLVING A SINGLE SPARSE REWARD TASK
  POWER OF SOLVING A SINGLE SPARSE REWARD TASK The power of exploration is an important aspect, but we also want our algorithm to converge to a stable policy that maximizes the extrinsic reward for different sparse re- ward tasks. To this end, we use the same environments ( Fig. 4 ) as in the previous experiment. In each environment, we se- lect five uniformly distributed target points from the area of interest and assign extrin- sic rewards when the agent reaches these points. For each target point, we train an individual policy to reach it. Hypotheti- cally, influenced by the extrinsic reward, the agent eventually ignores the internal goals generated by the goal proposing module and reaches the target points consistently. In or- der to evaluate the performance of our algorithm, we measure how reliably the agent is able to reach each target point. To this end, we collect the final 10 states from the 10 most recent trajectories and define criteria for convergence as the percentage of receiving the extrinsic rewards. If more than 90% of the states receive the extrinsic reward with a standard deviation of less then 3%, we say the agent solved the task successfully. In this experiment, we use the configuration SAC+HER+Skew- Explore which achieves the best performance in the previous experiment.  Table 1  shows how many trajectories the algorithm needs to reach the criteria of convergence. This experiment thus shows that our algorithm is able to solve a sparse reward task, by obtaining a policy with a limited number of trajectories. Additional results can be found in Appendix F.

Section Title: TASK WITH A LONG HORIZON AND REAL WORLD DEMONSTRATION
  TASK WITH A LONG HORIZON AND REAL WORLD DEMONSTRATION In the third experiment, we evaluate the ability of our algorithm in terms of solving a sparse reward task with a long horizon and test the performance of the converged policy using a real world YuMi robot. We increase the complexity of the environment by adding a box and a button to the DoorOpen environment used in the previous two experiments. We design a task called OpenPressClose which needs a long sequence of procedures to be solved. The sequence includes 1) open the box, 2) press the button inside the box and 3) close the box. The extrinsic sparse reward is only given to the agent after all procedures in the sequence are done. This task is exceptionally challenging as each intermediate procedure requires a set of continuous actions in correct order to be achieved and no intermediate reward is provided to guide the search. Therefore this task requires the power of efficient exploration to discover the state that provides an extrinsic reward. If the algorithm fails to explore efficiently, the rewarding state would never be found and no policy will be learned. The results show that the algorithm is able to discover the extrinsic reward and converge to a stable solution. Fig 7a shows the change of average extrinsic reward per step over iterations and Fig 7b shows the converged policy in sequential order. The resulting policy is deployed to a real world YuMi robot as shown in  Fig 1 . A demonstration of the real robot solving the task can be found in the video.

Section Title: CONCLUSION
  CONCLUSION In this work, we propose an algorithm named Skew-Explore, a general framework for continuous state exploration. Inspired by  Skew-Fit Pong et al. (2019) , the main idea of Skew-Explore is to encourage exploration around the novelty frontier reference points proposed by a latent variable proposing network. The algorithm is able to track the global information of entropy of density dis- tribution estimated by the states stored in a history state set, which helps to maximize a corresponded metrics, namely entropy and coverage. Two experiments are conducted to test the power of Skew- Explore on the exploration problem and the single sparse reward problem. In the first experiment, we found that our algorithm Skew-Explore, using SAC and HER together, has the fastest exploration rate. In the second experiment, we found that our algorithm is also able to converge to a stable policy when a single sparse reward is given. As a demonstrator, we used an environment where a robotic manipulator needs to 1) open a door, 2) press a button inside and 3) close the door in a sequence but only with a sparse reward given at the end. We implemented the fully converged policy on a real YuMi robot using policy transfer. Future work will include investigating if we can improve the efficiency of policy convergence by adjusting the proposing network's distribution. Additionally, we will examine whether clustering can increase the efficiency for exploration. Moreover, we will look for a better reward function than KL divergence between Gaussian-based goal distribution and the trajectory distribution. Under review as a conference paper at ICLR 2020

```
