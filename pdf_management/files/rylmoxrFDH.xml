<article article-type="research-article"><front><article-meta><title-group><article-title>Published as a conference paper at ICLR 2020 CRITICAL INITIALISATION IN CONTINUOUS APPROXI- MATIONS OF BINARY NEURAL NETWORKS</article-title></title-group><contrib-group content-type="author"><contrib contrib-type="person"><name><surname>Stamatescu</surname><given-names>George</given-names></name></contrib><contrib contrib-type="person"><name><surname>Fuss</surname><given-names>Ian</given-names></name></contrib><contrib contrib-type="person"><name><surname>White</surname><given-names>Langford B</given-names></name></contrib><contrib contrib-type="person"><name><surname>Gerace</surname><given-names>Federica</given-names></name></contrib><contrib contrib-type="person"><name><surname>Lucibello</surname><given-names>Carlo</given-names></name></contrib><contrib contrib-type="person"><xref ref-type="aff" rid="aff0" /></contrib></contrib-group><aff id="aff0"><institution content-type="orgname">School of Electrical and Electronic Engineering University of Adelaide Adelaide</institution><country>Australia</country></aff><aff id="aff1"><country>Italy</country></aff><abstract><p>The training of stochastic neural network models with binary (&#177;1) weights and activations via continuous surrogate networks is investigated. We derive new sur- rogates using a novel derivation based on writing the stochastic neural network as a Markov chain. This derivation also encompasses existing variants of the surro- gates presented in the literature. Following this, we theoretically study the surro- gates at initialisation. We derive, using mean field theory, a set of scalar equations describing how input signals propagate through the randomly initialised networks. The equations reveal whether so-called critical initialisations exist for each surro- gate network, where the network can be trained to arbitrary depth. Moreover, we predict theoretically and confirm numerically, that common weight initialisa- tion schemes used in standard continuous networks, when applied to the mean values of the stochastic binary weights, yield poor training performance. This study shows that, contrary to common intuition, the means of the stochastic binary weights should be initialised close to &#177;1, for deeper networks to be trainable.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>The problem of learning with low-precision neural networks has seen renewed interest in recent years, in part due to the deployment of neural networks on low-power devices. Currently, deep neural networks are trained and deployed on GPUs, without the memory or power constraints of such devices. Binary neural networks are a promising solution to these problems. If one is interested in addressing memory usage, the precision of the weights of the network should be reduced, with the binary case being the most extreme. In order to address power consumption, networks with both binary weights and neurons can deliver significant gains in processing speed, even making it feasible to run the neural networks on CPUs Rastegari et al. (2016). Of course, introducing discrete variables creates challenges for optimisation, since the networks are not continuous and differentiable.</p><p>Recent work has opted to train binary neural networks directly via backpropagation on a differen- tiable surrogate network, thus leveraging automatic differentiation libraries and GPUs. A key to this approach is in defining an appropriate differentiable surrogate network as an approximation to the discrete model. A principled approach is to consider binary stochastic variables and use this stochasticity to "smooth out" the non-differentiable network. This includes the cases when (i) only weights, and (ii) both weights and neurons are stochastic and binary.</p><p>In this work we study two classes of surrogates, both of which make use of the Gaussian central limit theorem (CLT) at the receptive fields of each neuron. In either case, the surrogates are written Published as a conference paper at ICLR 2020 as differentiable functions of the continuous means of stochastic binary weights, but with more complicated expressions than for standard continuous networks.</p><p>One approximation, based on analytic integration, yields a class of deterministic surrogates Soudry et al. (2014). The other approximation is based on the local reparameterisation trick (LRT) Kingma &amp; Welling (2013), which yields a class of stochastic surrogates Shayer et al. (2017). Previous works have relied on heuristics to deal with binary neurons Peters &amp; Welling (2018), or not backpropa- gated gradients correctly. Moreover, none of these works considered the question of initialisation, potentially limiting performance.</p><p>The seminal papers of Saxe et al. (2013), Poole et al. (2016), Schoenholz et al. (2016) used a mean field formalism to explain the empirically well known impact of initialization on the dynamics of learning in standard networks. From one perspective the formalism studies how signals propagate forward and backward in wide, random neural networks, by measuring how the variance and corre- lation of input signals evolve from layer to layer, knowing the distributions of the weights and biases of the network. By studying these moments the authors in Schoenholz et al. (2016) were able to explain how heuristic initialization schemes avoid the "vanishing and exploding gradients problem" Glorot &amp; Bengio (2010), establishing that for neural networks of arbirary depth to be trainable they must be initialised at "criticality", which corresponds to initial correlation being preserved to any depth.</p><p>The paper makes three contributions. The first contribution is the presentation of new algorithms, with a new derivation able to encompass both surrogates, and all choices of stochastic binary weights, or neurons. The derivation is based on representing the stochastic neural network as a Markov chain, a simplifying and useful development. As an example, using this representation we are easily able to extend the LRT to the case of stochastic binary neurons, which is new. This was not possible in Shayer et al. (2017), who only considered stochastic binary weights. As a second example, the deterministic surrogate of Soudry et al. (2014) is easily derived, without the need for Bayesian message passing arguments. Moreover, unlike Soudry et al. (2014) we correctly backprop- agate through variance terms, as we discuss.</p><p>The second contribution is the theoretical analysis of both classes of surrogate at initialisation, through the prism of signal propagation theory Poole et al. (2016), Schoenholz et al. (2016). This analysis is achieved through novel derivations of the dynamic mean field equations, which hinges on the use of self-averaging arguments Mezard et al. (1987). The results of the theoretical study, which are supported by numerical simulations and experiment, establish that for a surrogate of arbitrary depth to be trainable, it must be randomly initialised at "criticality". In practical terms, critical- ity corresponds to using initialisations that avoid the "vanishing and exploding gradients problem" Glorot &amp; Bengio (2010). We establish the following key results:</p><p>&#8226; For networks with stochastic binary weights and neurons, the deterministic surrogate can achieve criticality, while the LRT cannot.</p><p>&#8226; For networks with stochastic binary weights and continuous neurons, the LRT surrogate can achieve criticality (no deterministic surrogate exists for this case)</p><p>In both cases, the critical initialisation corresponds to randomly initialising the means of the binary weights close to &#177;1, a counter intuitive result.</p><p>A third contribution is the consideration of the signal propagation properties of random binary net- works, in the context of training a differentiable surrogate network. We derive these results, which are partially known, and in order to inform our discussion of the experiments. This paper provides insights into the dynamics and training of the class of binary neural network models. To date, the initialisation of any binary neural network algorithm has not been studied, although the effect of quantization levels has been explored through this perspective Blumenfeld et al. (2019). Currently, the most popular surrogates are based on the so-called "Straight-Through" estimator Bengio et al. (2013), which relies on heuristic definitions of derivatives in order to define a gradient. However, this surrogate typically requires the use of batch normalization, and other heuristics. The contributions in this paper may help shed light on what is holding back the more principled algorithms, by suggesting practical advice on how to initialise, and what to expect during training.</p></sec><sec><title>Published as a conference paper at ICLR 2020</title></sec><sec><title>Paper outline</title><p>In section 2 we present the binary neural network algorithms considered. In sub- section 2.1 we define binary neural networks and subsection 2.2 their stochastic counterparts. In subsection 2.3 we use these definitions to present new and existing surrogates in a coherent frame- work, using the Markov chain representation of a neural network to derive variants of both the deterministic surrogate, and the LRT-based surrogates. We derive the LRT for the case of stochastic binary weights, and both LRT and deterministic surrogates for the case of stochastic binary weights and neurons. In section 3 we derive the signal propagation equations for both the deterministic and stochastic LRT surrogates. This includes deriving the explicit depth scales for trainability, and solv- ing the equations to find the critical initialisations for each surrogate, if they exist. In section 4 we present the numerical simulations of wide random networks, to validate the mean field description, and experimental results to test the trainability claims. In section 5 we summarize the key results, and provide a discussion of the insights they provide.</p></sec><sec><title>BINARY NEURAL NETWORK ALGORITHMS</title></sec><sec><title>CONTINUOUS NEURAL NETWORKS AND BINARY NEURAL NETWORKS</title><p>A neural network model is typically defined as a deterministic non-linear function. We consider a fully connected feedforward model, which is composed of N &#215; N &#8722;1 weight matrices W and bias vectors b in each layer &#8712; {1, . . . , L}, with elements W ij &#8712; R and b i &#8712; R. Given an input vector x 0 &#8712; R N0 , the network is defined in terms of the following recursion, x = &#966; (h ), h = 1 &#8730; N &#8722;1 W x &#8722;1 + b (1) where the pointwise non-linearity is, for example, &#966; (&#183;) = max(0, &#183;). We refer to the input to a neuron, such as h , as the pre-activation field.</p><p>A deterministic binary neural network simply has weights W ij &#8712; {&#177;1} and &#966; (&#183;) = sign(&#183;), and otherwise the same propagation equations. Of course, this is not differentiable, thus we instead consider stochastic binary variables in order to smooth out the non-differentiable network. Ideally, the product of training a surrogate of a stochastic binary network is a deterministic (or stochastic) binary network that is able to generalise from its training set.</p></sec><sec><title>STOCHASTIC BINARY NEURAL NETWORKS</title><p>In stochastic binary neural networks we denote the matrices as S with all weights 1 S ij &#8712; {&#177;1} be- ing independently sampled binary variables with probability is controlled by the mean M ij = ES ij . Neuron activation in this model are also binary random variables, due to pre-activation stochasticity and to inherent noise. We consider parameterised neurons such that the mean activation conditioned on the pre-activation is given by some function taking values in [&#8722;1, 1], i.e. E[x i | h i ] = &#966;(h i ), for example &#966;(&#183;) = tanh(&#183;). We write the propagation rules for the stochastic network as follows:</p><p>Notice that the distribution of x factorizes when conditioning on x &#8722;1 . The form of the neuron's mean function &#966;(&#183;) depends on the underlying noise model. We can express a binary random variable x &#8712; {&#177;1} with x &#8764; p(x; &#952;) via its latent variable formulation x = sign(&#952; + &#945;L). In this form &#952; is referred to as a "natural" parameter, and the term L is a latent random noise, whose cumulative distribution function &#963;(&#183;) determines the form of the non-linearity since &#966;(&#183;) = 2&#963;(&#183;)&#8722;1. In general the form of &#966;(&#183;) will impact on the surrogates' performance, including within and beyond the mean field description presented here. However, a result from the analysis in Section 3 is that choosing a deterministic binary neuron, ie. the sign(&#183;) function, or a stochastic binary neuron, produces the same signal propagation equations, up to a scaling constant.</p></sec><sec><title>DERIVATIONS OF NEW AND EXISTING SURROGATE NETWORKS</title><p>The idea behind several recent papers Soudry et al. (2014), Baldassi et al. (2018), Shayer et al. (2017), Peters &amp; Welling (2018) is to adapt the mean of the binary stochastic weights, with the stochastic model essentially used to "smooth out" the discrete variables and arrive at a differentiable function, open to the application of continuous optimisation techniques. We now derive both the de- terministic surrogate and LRT-based surrogates, in a common framework. We consider a supervised classification task, with training set D = {x &#181; , y &#181; } P &#181;=1 , with y &#181; the label. we define a loss function for our surrogate model via</p><p>For a given input x &#181; and a realization of weights, neuron activations and biases in all layers, denoted by (S, x, b), the stochastic neural network produces a probability distribution over the classes. Expectations over weights and activations are given by the mean values, ES = M and E[x |h ] = &#966;(h ). This objective can be recognised as a (minus) marginal likelihood, thus this method could be described as Type II maximum likelihood, or empirical Bayes.</p><p>The starting point for our derivations comes from rewriting the expectation equation 3 as the marginalization of a Markov chain, with layers indexes corresponding to time indices. Markov chain representation of stochastic neural network:</p><p>where in the second line we dropped from the notation p(S ; M ) the dependence on M for brevity. Therefore, for a stochastic network the forward pass consists in the propagation of the joint distribu- tion of layer activations, p(x |x &#181; ), according to the Markov chain. We drop the explicit dependence on the initial input x &#181; from now on.</p><p>In what follows we will denote with &#966;(h ) the average value of x according to p(x ). The first step to obtaining a differentiable surrogate is to introduce continuous random variables. We take the limit of large layer width and appeal to the central limit theorem to model the field h as Gaussian, with meanh and covariance matrix &#931; .</p></sec><sec><title>Assumption 1: (CLT for stochastic binary networks)</title><p>In the large N limit, under the Lyapunov central limit theorem, the field h = 1</p><p>While this assumption holds true for large enough networks, due to S and x &#8722;1 independency, the Assumption 2 below, is stronger and tipically holds only at initialization.</p></sec><sec><title>Assumption 2: (correlations are zero)</title><p>We assume the independence of the pre-activation field h between any two dimensions. Specifically, we assume the covariance &#931; = Cov(h , h ) to be well approximated by &#931; M F (&#966;(h &#8722;1 )), with MF denoting the mean field (factorized) assumption, where</p><p>This assumption approximately holds assuming the neurons in each layer are not strongly corre- lated. In the first layer this is certainly true, since the input neurons are not random variables 2 . In subsequent layers, since the fields h i and h j share stochastic neurons from the previous layer, this cannot be assumed to be true. We expect this correlation to not play a significant role, since 2 In this case the variance is actually 1</p><p>Published as a conference paper at ICLR 2020 the weights act to decorrelate the fields, and the neurons are independently sampled. However, the choice of surrogate influences the level of dependence. The sampling procedure used within the lo- cal reparametrization trick reduces correlations since variables are sampled, while the deterministic surrogate entirely discards them.</p><p>We obtain either surrogate model by successively approximating the marginal distributions, p(x ) = dh p(x |h ) &#8776;p(x ), starting from the first layer. We can do this by either (i) marginalising over the Gaussian field using analytic integration, or (ii) sampling from the Gaussian. After this, we use the approximationp(x i ) to form the Gaussian approximation for the next layer, and so on.</p></sec><sec><title>Deterministic surrogate</title><p>We perform the analytic integration based on the analytic form of p(x +1 i |h ) = &#963;(x i h i ), with &#963;(&#183;) a sigmoidal function. In the case that &#963;(&#183;) is the Gaussian CDF, we obtainp(x i ) exactly 3 by the Gaussian integral of the Gaussian cumulative distribution function,</p><p>Since we start from the first layer, all random variables are marginalised out, and thush i has no dependence on random h &#8722;1 j via the neuron means &#966;(h ) as in Assumption 1. Instead, we have dependence on meansx = E h E x | h = E h &#966;(h ). Thus it is convenient to define the mean underp(x i ) as &#981; (h, &#963; 2 ) = dh &#966; (h) N (h ;h, &#963; 2 ). In the case that &#963;(&#183;) is the Gaussian CDF, then &#981; (&#183;) is the error function. Finally, the forward pass can be expressed as</p><p>This is a more general formulation than that in Soudry et al. (2014), which considered sign activa- tions, which we obtain in the appendices as a special case. Furthermore, in all implementations we backpropagate through the variance terms &#931; &#8722; 1 2 M F , which were ignored in the previous work of Soudry et al. (2014). Note that the derivation here is simpler as well, not requiring complicated Bayesian message passing arguments, and approximations therein.</p></sec><sec><title>LRT surrogate</title><p>The basic idea here is to rewrite the incoming Gaussian field h &#8764; N (&#181;, &#931;) as h = &#181; + &#8730; &#931; where &#8764; N (0, I). Thus expectations over h can be written as expectations over and approximated by sampling. The resulting network is thus differentiable, albeit not deterministic. The forward propagation equations for this surrogate are</p><p>The local reparameterisation trick (LRT) Kingma &amp; Welling (2013) has been previously used to obtain differentiable surrogates for binary networks. The authors of Shayer et al. (2017) consid- ered only the case of stochastic binary weights, since they did not write the network as a Markov chain. Peters &amp; Welling (2018) considered stochastic binary weights and neurons, but relied on other approximations to deal with the neurons, having not used the Markov chain representation.</p><p>The result of each approximation, applied successively from layer to layer by either propagating means and variances or by, produces a differentiable function of the parameters M ij . It is then possible to perform gradient descent with respect to the M and b. Ideally, at the end of training we obtain a binary network that attains good performance. This network could be a stochastic net- work, where we sample all weights and neurons, or a deterministic binary network. A deterministic network might be chosen taking the most likely weights, therefore setting W ij = sign(M ij ), and replacing the stochastic neurons with sign(&#183;) activations.</p></sec><sec><title>SIGNAL PROPAGATION THEORY FOR CONTINUOUS SURROGATES</title><p>Since all the surrogates still retain the basic neural network structure of layerwise processing, cru- cially applying backpropagation for optimisation, it is reasonable to expect that surrogates are likely Published as a conference paper at ICLR 2020 to inherit similar "training problems" as standard neural networks. In this section we apply this formalism to the surrogates considered, given random initialisation of the means M ij and biases b i . We are able to solve for the conditions of critical initialisation for each surrogate, which essentially allow signal to propagate forwards, and gradients to propagate backwards, without the effects such as neuron saturation. The critical initialisation for the surrogates, the key results of the paper, are provided in Claims 1 and 3.</p></sec><sec><title>FORWARD SIGNAL PROPAGATION FOR STANDARD CONTINUOUS NETWORKS</title><p>We first recount the formalism developed in Poole et al. (2016). Assume the weights of a standard continuous network are initialised with W ij &#8764; N (0, &#963; 2 w ), biases b &#8764; N (0, &#963; 2 b ), and input signal x 0 a has zero mean Ex 0 = 0 and variance E[x 0 a &#183; x 0 a ] = q 0 aa , and with a denoting a particular input pattern. As before, the signal propagates via Equation 1 from layer to layer. We are interested in computing, from layer to layer, the variance q aa = 1 N i (h i;a ) 2 from a par- ticular input x 0 a , and also the covariance between the pre-activations q ab = 1 N i h i;a h i;b , aris- ing from two different inputs x 0 a and x 0 b with given covariance q 0 ab . The mean field approxima- tion used here replaces each element in the pre-activation field h i by a Gaussian random variable whose moments are matched. Assuming also independence within a layer; Eh i;a h j;a = q aa &#948; ij and Eh i;a h j;b = q ab &#948; ij , one can derive recurrence relations from layer to layer,</p><p>ab ) 2 z 2 , and we identify c ab as the corre- lation in layer . The other important quantity is the slope of the correlation recursion equation or mapping from layer to layer, denoted as &#967;, which is given by:</p><p>We denote &#967; at the fixed point c * = 1 as &#967; 1 . As discussed Poole et al. (2016), when &#967; 1 = 1, correlations can propagate to arbitrary depth.</p><p>Definition 1: Critical initialisations are the points (&#963; 2 b , &#963; 2 w ) corresponding to &#967; 1 = 1. Furthermore, &#967; 1 is equivalent to the mean square singular value of the Jacobian matrix for a sin- gle layer J ij = &#8706;h i &#8706;h &#8722;1 j , as explained in Poole et al. (2016). Therefore controlling &#967; 1 will prevent the gradients from either vanishing or growing exponentially with depth. We thus define critical initialisations as follows. This definition also holds for the surrogates which we now study.</p></sec><sec><title>SIGNAL PROPAGATION THEORY FOR DETERMINISTIC SURROGATES</title><p>For the deterministic surrogate model we assume at initialization that the binary weight means M ij are drawn independently and identically from a distribution P (M ), with mean zero and variance of the means given by &#963; 2 m . For instance, a valid distribution could be a clipped Gaussian 4 , or another stochastic binary variable, for example P (M ) = 1 2 &#948;(M + &#963; m ) + 1 2 &#948;(M &#8722; &#963; m ), whose variance is &#963; 2 m . The biases at initialization are distributed as b &#8764; N (0, &#963; 2 b ).</p><p>We show in Appendix B that the stochastic and deterministic binary neuron cases reduce to the same signal propagation equations, up to scaling constants. In light of this, we consider the deterministic Published as a conference paper at ICLR 2020 sign(&#183;) neuron case, since equation for the field is slightly simpler:</p><p>which we can be read from the Eq. 7. As in the continuous case we are interested in computing the variance q aa = 1 N i (h i;a ) 2 and covariance Eh i;a h j;b = q ab &#948; ij , via recursive formulae. The key to the derivation is recognising that the denominator &#931; M F,ii is a self-averaging quantity Mezard et al. (1987). This means it concentrates in probability to its expected value for large N . Therefore we can safely replace it with its expectation. Following this self-averaging argument, we can take expectations more readily as shown in the appendices. We find the variance recursion to be</p><p>Based on this expression, and assuming q aa = q bb , the correlation recursion can be written as</p><p>The slope of the correlation mapping from layer to layer, when the normalized length of each input is at its fixed point q aa = q bb = q * (&#963; m , &#963; b ), denoted as &#967;, is given by:</p><p>where u a and u b are defined exactly as in the continuous case. Refer to the appendices for full details of the derivation.</p></sec><sec><title>CRITICAL INITIALISATION: DETERMINISTIC SURROGATE</title><p>The condition for critical initialisation is &#967; 1 = 1, since this determines the stability of the correlation map fixed point c * = 1. Note that for the deterministic surrogate this is always a fixed point. We can solve for the hyper-parameters (&#963; 2 b , &#963; 2 m ) that satisfy this condition, using the dynamical equations of the network.</p><p>Claim 1: The points (&#963; 2 b , &#963; 2 m ) corresponding to critical initialisation are given by &#963; 2</p><p>This can be established by rearranging Equations 13 and 15. We solve for &#963; 2 b numerically, as shown in Figure 3, for different neuron noise models and hence non-linearities &#981;(&#183;). We find that the critical initialisation for any of these design choices is close to the point (&#963; 2 m , &#963; 2 b ) = (1, 0). However, it is not just the singleton point, as for example in Hayou et al. (2019) for the ReLu case for standard networks. We plot the solutions in the Appendix.</p></sec><sec><title>ASYMPTOTIC EXPANSIONS AND DEPTH SCALES</title><p>The depth scales, as derived in Schoenholz et al. (2016) provide a quantitative indicator to the number of layers correlations will survive for, and thus how trainable a network is. Similar depth scales can be derived for these deterministic surrogates. Asymptotically in network depth , we expect that |q aa &#8722; q * | &#8764; exp(&#8722; &#958;q ) and |c ab &#8722; c * | &#8764; exp(&#8722; &#958;c ), where the terms &#958; q and &#958; c define the depth scales over which the variance and correlations of signals may propagate. We are most interested in the correlation depth scale, since it relates to &#967;. The derivation is identical to that of Schoenholz et al. (2016). One can expand the correlation c ab = c * + , and assuming q aa = q * , it is possible to write</p><p>Published as a conference paper at ICLR 2020</p><p>The depth scale &#958; &#8722;1 c are given by the log ratio log</p><p>We plot this depth scale in <xref ref-type="fig" rid="fig_1">Figure 2</xref>. We derive the variance depth scale in the appendices, since it is different to the standard continuous case, but not of prime practical importance.</p></sec><sec><title>SIGNAL PROPAGATION THEORY FOR LOCAL REPARAMETERIZATION TRICK SURROGATES</title><p>From Equation 8, the pre-activation field for the perturbed surrogate with both stochastic binary weights and neurons is given by,</p><p>where we recall that &#8764; N (0, 1). The non-linearity &#966;(&#183;) can of course be derived from any valid bi- nary stochastic neuron model. Appealing to the same self-averaging arguments used in the previous section, we find the variance map to be</p><p>Interestingly, we see that the variance map does not depend on the variance of the means of the binary weights. This is not immediately obvious from the pre-activation field definition. In the covariance map we do not have such a simplification since the perturbation i,a is uncorrelated between inputs a and b. Thus the correlation map is given by</p></sec><sec><title>CRITICAL INITIALISATION: LRT SURROGATES</title><p>Claim 2: There is no critical initialisation for the local reparameterisation trick based surrogate, for a network with binary weights and neurons.</p><p>Proof: The conditions for a critical initialisation are that c * = 1 to be a fixed point and &#967; 1 = 1. No such fixed point exists. We have a fixed point c * = 1 if and only if &#963; 2 m = 1/E[&#966; 2 (h l&#8722;1 j,a )]. Note that &#963; 2 m &#8804; 1. For any &#966;(z) which is the mean of the stochastic binary neuron, the expectation E[&#966; 2 (z)] &#8804; 1. For example, consider &#966;(z) = tanh(&#954;z) for any finite kappa.</p><p>We also considered the LRT surrogate with continuous (tanh(&#183;)) neurons and stochastic binary weights. The derivations are very similar to the previous case, as we show in the appendix. The variance and correlation maps are given by</p><p>This leads to the following result,</p><p>Claim 3: The critical initialisation for the LRT surrogate, for the case of continuous tanh(&#183;) neurons and stochastic binary weights is the singleton (&#963; 2 b , &#963; 2 m ) = (0, 1).</p><p>Proof: From the correlation map we have a fixed point c * = 1 if and only if &#963; 2 m = 1, by inspection. In turn, the critical initialisation condition &#967; 1 = 1 holds if E[(&#966; (h l&#8722;1 j,a )) 2 ] = 1 &#963; 2 m = 1. Thus, to find the critical initialisation, we need to find a value of q aa = E&#966; 2 (h l&#8722;1 j,a ) + &#963; 2 b that satisfies this final condition. In the case that &#966;(&#183;) = tanh(&#183;), then the function (&#966; (h l&#8722;1 j,a )) 2 &#8804; 1, taking the value 1 at the origin only, this requires q aa &#8594; 0. Thus we have the singleton (&#963; 2 b , &#963; 2 m ) = (0, 1) as the solution.</p></sec><sec><title>NUMERICAL AND EXPERIMENTAL RESULTS</title></sec><sec><title>SIMULATIONS</title><p>We first verify that the theory accurately predicts the average behaviour of randomly initialised networks. We present simulations for the deterministic surrogate in <xref ref-type="fig" rid="fig_0">Figure 1</xref>. We see that the average behaviour of random networks are well predicted by the mean field theory. Estimates of the variance and correlation are plotted, with dotted lines corresponding to empirical means and the shaded area corresponding to one standard deviation. Theoretical predictions are given by solid lines, with strong agreement for even finite networks. Similar plots can be produced for the LRT surrogate. In Appendix D we plot the depth scales as functions of &#963; m and &#963; b .</p></sec><sec><title>TRAINING PERFORMANCE FOR DIFFERENT MEAN INITIALISATION &#963; 2 m</title><p>Here we experimentally test the predictions of the mean field theory by training networks to overfit a dataset in the supervised learning setting, having arbitrary depth and different initialisations. We consider first the performance of the deterministic and LRT surrogates, not their corresponding binary networks.</p><p>We use the MNIST dataset with reduced training set size (50%) and record the training performance (percentage of the training set correctly labeled) after 10 epochs of gradient descent over the training set, for various network depths L &lt; 70 and different mean variances &#963; 2 m &#8712; [0, 1). The optimizer used was SGD with Adam Kingma &amp; Ba (2014) with a learning rate of 2 &#215; 10 &#8722;4 chosen after simple grid search, and a batch size of 64. We see that the experimental results match the correlation depth scale derived, which are overlaid as dotted curves. A proportion of 3&#958; c was found to indicate the maximum attenuation in signal strength before trainability becomes difficult, similarly to previous works Schoenholz et al. (2016).</p><p>A reason we see the trainability not diverging in <xref ref-type="fig" rid="fig_1">Figure 2</xref> is that training time increases with depth, on top of requiring smaller learning rates for deeper networks, as described in Saxe et al. (2013). The experiment here used the same number of epochs regardless of depth, meaning shallower networks actually had an advantage over deeper networks. Note that the theory does not specify for how many steps of training the effects of critical initialisation will persist. Therefore, the number of steps we trained the network for is an arbitrary choice, and thus the experiments validate the theory in a more qualitative way. Results were similar for other optimizers, including SGD, SGD with momentum, and RMSprop. Note that these networks were trained without dropout, batchnorm or any other heuristics.</p><p>In <xref ref-type="fig" rid="fig_1">Figure 2</xref> we present the training performance for the deterministic surrogate and its stochastic bi- nary counterpart. The results for a deterministic binary network were similar to a single Monte Carlo sample. Once again, we test our algorithms on the MNIST dataset and plot results after 5 epochs. We see that the performance of the stochastic network matches more closely the performance of the continuous surrogate as the number of samples increases, from N = 5 to N = 100 samples. We can report that the number of samples necessary to achieve better classification, at least for more shal- low networks, appears to depends on the number of training epochs. This is a sensible relationship, since during the course of training we expect the means of the weights to polarise, moving closer to Published as a conference paper at ICLR 2020 the bounds &#177;1. Likewise, we expect that neurons, which initially have zero mean pre-activations, will also "saturate" during training, becoming either always "on" (+1) or "off" (&#8722;1). A stochastic network being "closer" to deterministic would require fewer samples overall.</p></sec><sec><title>DISCUSSION</title><p>This study of two classes of surrogate networks, and the derivation of their initialisation theories has yielded results of practical significance. Based on the results of Section 3, in particular Claims 1-3, we can offer the following advice. If a practitioner is interested in training networks with binary weights and neurons, one should use the deterministic surrogate, not the LRT surrogate, since the latter has no critical initialisation. If a practitioner is interested in binary weights only,the LRT in this case does have a critical initialisation (and is the only choice from amongst these two classes of surrogate). Furthermore, both networks are critically initialised when &#963; 2 b &#8594; 0 and by setting the means of the weights to &#177;1.</p><p>It was seen that during training, when evaluating the stochastic binary counterparts concurrently with the surrogate, the performance of binary networks was worse than the continuous model, especially as depth increases. We reported that the stochastic binary network, with more samples, outperformed the deterministic binary network, a reasonable result since the objective optimised is the expectation over an ensemble of stochastic binary networks.</p><p>A study of random deterministic binary networks, included in the Appendices, and published re- cently Blumenfeld et al. (2019) for a different problem, reveals unsurprisingly that binary networks are always in a chaotic phase. However a binary network which is trained via some algorithm will of course have different signal propagation behaviour. It makes sense that the closer one is to the early stages of the training process, the closer the signal propagation behaviour is to the randomly initialised case. We might expect that as training progresses the behaviour of the binary counter- parts approaches that of the trained surrogate. Any such difference would not be observed for a heuristic surrogate as used in Courbariaux &amp; Bengio (2016) or Rastegari et al. (2016), which has no continuous forward propagation equations.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Dynamics of the variance and correlation maps, with simulations of a network of width N = 1000, 50 realisations, for various hyperparameter settings: &#963; 2 m &#8712; {0.2, 0.5, 0.99} (blue, green and red respectively). (a) variance evolution, (b) correlation evolution. (c) correlation mapping (c in to c out ), with &#963; 2 b = 0.001</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Top: Training performance of the deterministic surrogate (left) and the LRT surrogate for stochastic binary weights and continuous neurons (right). The vertical axis represents network depth against the variance of the means &#963; 2 m . Both surrogates were trained with &#963; 2 b = 0. Thus, as &#963; 2 m &#8594; 1 we approach criticality in both cases. Overlaid are curves proportional to the correlation depth scale &#958; c . Bottom: Training performance of the deterministic surrogate and its binary counterparts after training on the MNIST dataset for 5 epochs. Left: performance of the continuous surrogate. Centre: the performance of the stochastic binary network, averaged over 5 Monte Carlo samples. Right: 100 Monte Carlo samples. The deterministic binary evaluation is similar to a single Monte Carlo sample, resembling the central figure.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig></sec></body><back><sec><p>We denote random variables with bold font. Also, following physics' jargon, we refer to binary &#177;1 vari- ables as Ising spins or just spins.</p></sec></back></article>