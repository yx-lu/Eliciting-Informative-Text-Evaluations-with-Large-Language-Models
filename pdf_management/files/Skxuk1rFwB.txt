Title:
```
Published as a conference paper at ICLR 2020 TOWARDS STABLE AND EFFICIENT TRAINING OF VERIFIABLY ROBUST NEURAL NETWORKS
```
Abstract:
```
Training neural networks with verifiable robustness guarantees is challenging. Several existing approaches utilize linear relaxation based neural network output bounds under perturbation, but they can slow down training by a factor of hundreds depending on the underlying network architectures. Meanwhile, interval bound propagation (IBP) based training is efficient and significantly outperforms linear relaxation based methods on many tasks, yet it may suffer from stability issues since the bounds are much looser especially at the beginning of training. In this paper, we propose a new certified adversarial training method, CROWN-IBP, by combining the fast IBP bounds in a forward bounding pass and a tight linear relaxation based bound, CROWN, in a backward bounding pass. CROWN-IBP is computationally efficient and consistently outperforms IBP baselines on training verifiably robust neural networks. We conduct large scale experiments on MNIST and CIFAR datasets, and outperform all previous linear relaxation and bound propagation based certified defenses in ∞ robustness. Notably, we achieve 7.02% verified test error on MNIST at = 0.3, and 66.94% on CIFAR-10 with = 8/255. * Work partially done during an internship at DeepMind.
```

Figures/Tables Captions:
```
Figure 1: Verified error and 2nd CNN layer's ∞ induced norm for a model trained using (Wong et al., 2018) and CROWN-IBP. is increased from 0 to 0.3 in 60 epochs.
Figure 2: Standard and verified errors of IBP and CROWN-IBP with differ- ent κ start and κ end values.
Figure 3: Verified error vs. schedule length on 8 medium MNIST models and 8 medium CIFAR-10 models. The solid bars show median values of verified errors. κ start = 1.0 except for the κ = 0 setting. The upper and lower ends of an error bar are the worst and best verified error, respectively. For each schedule length, three color groups represent three different κ settings.
Table 1: IBP trained models have low IBP verified errors but when verified with a typically much tighter bound, including convex adversarial polytope (CAP) (Wong et al., 2018) and CROWN (Zhang et al., 2018), the verified errors increase significantly. CROWN is generally tighter than convex adversarial polytope however the gap between CROWN and IBP is still large, especially at large . We used a 4-layer CNN network for all datasets to compute these bounds. 1 post-activation neuron values. Consider an input example x k with ground-truth label y k , we define a set of S(x k , ) = {x| x − x k ∞ ≤ } and we desire a robust network to have the property y k = argmax j [f (x)] j for all x ∈ S. We define element-wise upper and lower bounds for z (l) and
Table 2: The verified, standard (clean) and PGD attack errors for models trained using IBP and CROWN-IBP on MNIST and CIFAR-10. We only present performance on model DM-large here due to limited space (see Table C for a full comparison). CROWN-IBP outperforms IBP under all κ settings, and achieves state-of-the-art performance on both MNIST and CIFAR datasets for all .
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The success of deep neural networks (DNNs) has motivated their deployment in some safety-critical environments, such as autonomous driving and facial recognition systems. Applications in these areas make understanding the robustness and security of deep neural networks urgently needed, especially their resilience under malicious, finely crafted inputs. Unfortunately, the performance of DNNs are often so brittle that even imperceptibly modified inputs, also known as adversarial examples, are able to completely break the model ( Goodfellow et al., 2015 ;  Szegedy et al., 2013 ). The robustness of DNNs under adversarial examples is well-studied from both attack (crafting powerful adversarial examples) and defence (making the model more robust) perspectives ( Athalye et al., 2018 ;  Carlini & Wagner, 2017a ;b;  Goodfellow et al., 2015 ;  Madry et al., 2018 ;  Papernot et al., 2016 ;  Xiao et al., 2019b ;  2018b ;c;  Eykholt et al., 2018 ;  Chen et al., 2018 ;  Xu et al., 2018 ;  Zhang et al., 2019b ). Recently, it has been shown that defending against adversarial examples is a very difficult task, especially under strong and adaptive attacks. Early defenses such as distillation ( Papernot et al., 2016 ) have been broken by stronger attacks like C&W (Carlini & Wagner, 2017b). Many defense methods have been proposed recently ( Guo et al., 2018 ;  Song et al., 2017 ;  Buckman et al., 2018 ;  Ma et al., 2018 ;  Samangouei et al., 2018 ;  Xiao et al., 2018a ; 2019a), but their robustness improvement cannot be certified - no provable guarantees can be given to verify their robustness. In fact, most of these uncertified defenses become vulnerable under stronger attacks ( Athalye et al., 2018 ;  He et al., 2017 ). Several recent works in the literature seeking to give provable guarantees on the robustness perfor- mance, such as linear relaxations ( Wong & Kolter, 2018 ;  Mirman et al., 2018 ;  Wang et al., 2018a ;  Dvijotham et al., 2018b ;  Weng et al., 2018 ;  Zhang et al., 2018 ), interval bound propagation ( Mirman et al., 2018 ;  Gowal et al., 2018 ), ReLU stability regularization ( Xiao et al., 2019c ), and distributionally Published as a conference paper at ICLR 2020 robust optimization ( Sinha et al., 2018 ) and semidefinite relaxations ( Raghunathan et al., 2018a ;  Dvijotham et al. ). Linear relaxations of neural networks, first proposed by  Wong & Kolter (2018) , is one of the most popular categories among these certified defences. They use the dual of linear programming or several similar approaches to provide a linear relaxation of the network (referred to as a "convex adversarial polytope") and the resulting bounds are tractable for robust optimization. However, these methods are both computationally and memory intensive, and can increase model training time by a factor of hundreds. On the other hand, interval bound propagation (IBP) is a simple and efficient method for training verifiable neural networks ( Gowal et al., 2018 ), which achieved state-of-the-art verified error on many datasets. However, since the IBP bounds are very loose during the initial phase of training, the training procedure can be unstable and sensitive to hyperparameters. In this paper, we first discuss the strengths and weakness of existing linear relaxation based and interval bound propagation based certified robust training methods. Then we propose a new certified robust training method, CROWN-IBP, which marries the efficiency of IBP and the tightness of a linear relaxation based verification bound, CROWN ( Zhang et al., 2018 ). CROWN-IBP bound propagation involves a IBP based fast forward bounding pass, and a tight convex relaxation based backward bounding pass (CROWN) which scales linearly with the size of neural network output and is very efficient for problems with low output dimensions. Additional, CROWN-IBP provides flexibility for exploiting the strengths of both IBP and convex relaxation based verifiable training methods. The efficiency, tightness and flexibility of CROWN-IBP allow it to outperform state-of-the-art methods for training verifiable neural networks with ∞ robustness under all settings on MNIST and CIFAR- 10 datasets. In our experiment, on MNIST dataset we reach 7.02% and 12.06% IBP verified error under ∞ distortions = 0.3 and = 0.4, respectively, outperforming the state-of-the-art baseline results by IBP (8.55% and 15.01%). On CIFAR-10, at = 2 255 , CROWN-IBP decreases the verified error from 55.88% (IBP) to 46.03% and matches convex relaxation based methods; at a larger , CROWN-IBP outperforms all other methods with a noticeable margin.

Section Title: RELATED WORK AND BACKGROUND
  RELATED WORK AND BACKGROUND

Section Title: ROBUSTNESS VERIFICATION AND RELAXATIONS OF NEURAL NETWORKS
  ROBUSTNESS VERIFICATION AND RELAXATIONS OF NEURAL NETWORKS Neural network robustness verification algorithms seek for upper and lower bounds of an output neuron for all possible inputs within a set S, typically a norm bounded perturbation. Most importantly, the margins between the ground-truth class and any other classes determine model robustness. However, it has already been shown that finding the exact output range is a non-convex problem and NP-complete ( Katz et al., 2017 ;  Weng et al., 2018 ). Therefore, recent works resorted to giving relatively tight but computationally tractable bounds of the output range with necessary relaxations of the original problem. Many of these robustness verification approaches are based on linear relaxations of non-linear units in neural networks, including CROWN ( Zhang et al., 2018 ), DeepPoly ( Singh et al., 2019 ), Fast-Lin ( Weng et al., 2018 ), DeepZ ( Singh et al., 2018 ) and Neurify ( Wang et al., 2018b ). We refer the readers to ( Salman et al., 2019b ) for a comprehensive survey on this topic. After linear relaxation, they bound the output of a neural network f i (·) by linear upper/lower hyper-planes: A i,: ∆x + b L ≤ f i (x 0 + ∆x) ≤ A i,: ∆x + b U (1) where a row vector A i,: = W (L) i,: D (L−1) W (L−1) · · · D (1) W (1) is the product of the network weight matrices W (l) and diagonal matrices D (l) reflecting the ReLU relaxations for output neuron i; b L and b U are two bias terms unrelated to ∆x. Additionally,  Dvijotham et al. (2018c ;a);  Qin et al. (2019)  solve the Lagrangian dual of verification problem;  Raghunathan et al. (2018a ;b); Dvijotham et al. propose semidefinite relaxations which are tighter compared to linear relaxation based methods, but computationally expensive. Bounds on neural network local Lipschitz constant can also be used for verification ( Zhang et al., 2019c ;  Hein & Andriushchenko, 2017 ). Besides these deterministic verification approaches, randomized smoothing can be used to certify the robustness of any model in a probabilistic manner (Cohen et al., 2019;  Salman et al., 2019a ;  Lecuyer et al., 2018 ;  Li et al., 2018 ).

Section Title: ROBUST OPTIMIZATION AND VERIFIABLE ADVERSARIAL DEFENSE
  ROBUST OPTIMIZATION AND VERIFIABLE ADVERSARIAL DEFENSE To improve the robustness of neural networks against adversarial perturbations, a natural idea is to generate adversarial examples by attacking the network and then use them to augment the training set ( Kurakin et al., 2017 ). More recently,  Madry et al. (2018)  showed that adversarial training can Published as a conference paper at ICLR 2020 be formulated as solving a minimax robust optimization problem as in (2). Given a model with parameter θ, loss function L, and training data distribution X , the training algorithm aims to minimize the robust loss, which is defined as the maximum loss within a neighborhood {x + δ|δ ∈ S} of each data point x, leading to the following robust optimization problem:   Madry et al. (2018)  proposed to use projected gradient descent (PGD) to approximately solve the inner max and then use the loss on the perturbed example x + δ to update the model. Networks trained by this procedure achieve state-of-the-art test accuracy under strong attacks ( Athalye et al., 2018 ;  Wang et al., 2018a ;  Zheng et al., 2018 ). Despite being robust under strong attacks, models obtained by this PGD-based adversarial training do not have verified error guarantees. Due to the nonconvexity of neural networks, PGD attack can only compute the lower bound of robust loss (the inner maximization problem). Minimizing a lower bound of the inner max cannot guarantee (2) is minimized. In other words, even if PGD-attack cannot find a perturbation with large loss, that does not mean there exists no such perturbation. This becomes problematic in safety-critical applications since those models need certified safety. Verifiable adversarial training methods, on the other hand, aim to obtain a network with good robustness that can be verified efficiently. This can be done by combining adversarial training and robustness verification-instead of using PGD to find a lower bound of inner max, certified adversarial training uses a verification method to find an upper bound of the inner max, and then update the parameters based on this upper bound of robust loss. Minimizing an upper bound of the inner max guarantees to minimize the robust loss. There are two certified robust training methods that are related to our work and we describe them in detail below.

Section Title: Linear Relaxation Based Verifiable Adversarial Training
  Linear Relaxation Based Verifiable Adversarial Training One of the most popular verifiable adversarial training method was proposed in ( Wong & Kolter, 2018 ) using linear relaxations of neural networks to give an upper bound of the inner max. Other similar approaches include  Mirman et al. (2018) ;  Wang et al. (2018a) ;  Dvijotham et al. (2018b) . Since the bound propagation process of a convex adversarial polytope is too expensive, several methods were proposed to improve its efficiency, like Cauchy projection ( Wong et al., 2018 ) and dynamic mixed training ( Wang et al., 2018a ). However, even with these speed-ups, the training process is still slow. Also, this method may significantly reduce a model's standard accuracy (accuracy on natural, unmodified test set). As we will demonstrate shortly, we find that this method tends to over-regularize the network during training, which is harmful for obtaining good accuracy.

Section Title: Interval Bound Propagation (IBP)
  Interval Bound Propagation (IBP) Interval Bound Propagation (IBP) uses a very simple rule to compute the pre-activation outer bounds for each layer of the neural network. Unlike linear relaxation based methods, IBP does not relax ReLU neurons and does not consider the correlations between neurons of different layers, yielding much looser bounds.  Mirman et al. (2018)  proposed a variety of abstract domains to give sound over-approximations for neural networks, including the "Box/Interval Domain" (referred to as IBP in  Gowal et al. (2018) ) and showed that it could scale to much larger networks than other works ( Raghunathan et al., 2018a ) could at the time.  Gowal et al. (2018)  demonstrated that IBP could outperform many state-of-the-art results by a large margin with more precise approximations for the last linear layer and better training schemes. However, IBP can be unstable to use and hard to tune in practice, since the bounds can be very loose especially during the initial phase of training, posing a challenge to the optimizer. To mitigate instability,  Gowal et al. (2018)  use a mixture of regular and minimax robust cross-entropy loss as the model's training loss.

Section Title: Notation
  Notation We define an L-layer feed-forward neural network recursively as: where h (0) (x) = x, n 0 represents input dimension and n L is the number of classes, σ is an element- wise activation function. We use z to represent pre-activation neuron values and h to represent Published as a conference paper at ICLR 2020

Section Title: Verification Specifications
  Verification Specifications Importantly, each element in vector m := Cf (x) ∈ R n L gives us margins between class y and all other classes. We define the lower bound of Cf (x) for all x ∈ S(x k , ) as m(x k , ), which is a very important quantity: when all elements of m(x k , ) > 0, x k is verifiably robust for any perturbation with ∞ norm less than . m(x k , ) can be obtained by a neural network verification algorithm, such as convex adversarial polytope, IBP, or CROWN. Additionally,  Wong & Kolter (2018)  showed that for cross-entropy (CE) loss: (4) gives us the opportunity to solve the robust optimization problem (2) via minimizing this tractable upper bound of inner-max. This guarantees that max x∈S(x k , ) L(f (x), y) is also minimized.

Section Title: ANALYSIS OF IBP AND LINEAR RELAXATION BASED VERIFIABLE TRAINING METHODS
  ANALYSIS OF IBP AND LINEAR RELAXATION BASED VERIFIABLE TRAINING METHODS Interval Bound Propagation (IBP) Interval Bound Propagation (IBP) uses a simple bound propa- gation rule. For the input layer we set x L ≤ x ≤ x U element-wise. For affine layers we have: where |W (l) | takes element-wise absolute value. Note that h (0) = x U and h (0) = x L 2 . And for element-wise monotonic increasing activation functions σ,

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 We found that IBP can be viewed as training a simple augmented ReLU network which is friendly to optimizers (see Appendix A for more discussions). We also found that a network trained using IBP can obtain good verified errors when verified using IBP, but it can get much worse verified errors using linear relaxation based verification methods, including convex adversarial polytope (CAP) by  Wong & Kolter (2018)  (equivalently, Fast-Lin by  Weng et al. (2018) ) and CROWN ( Zhang et al., 2018 ).  Table 1  demonstrates that this gap can be very large on large . However, IBP is a very loose bound during the initial phase of training, which makes training unstable and hard to tune; purely using IBP frequently leads to divergence.  Gowal et al. (2018)  proposed to use a schedule where is gradually increased during training, and a mixture of robust cross-entropy loss with natural cross-entropy loss as the objective to stabilize training: Issues with linear relaxation based training. Since IBP hugely outperforms linear relaxation based methods in the recent work ( Gowal et al., 2018 ) in many settings, we want to under- stand what is going wrong with linear relaxation based methods. We found that, empirically, the norm of the weights in the models produced by linear relaxation based methods such as ( Wong & Kolter, 2018 ) and ( Wong et al., 2018 ) does not change or even decreases during training. In  Figure 1  we train a small 4-layer MNIST model and we linearly increase from 0 to 0.3 in 60 epochs. We plot the ∞ induced norm of the 2nd CNN layer during the training process of CROWN-IBP and ( Wong et al., 2018 ). The norm of weight matrix using ( Wong et al., 2018 ) does not increase. When becomes larger (roughly at = 0.2, epoch 40), the norm even starts to decrease slightly, indicating that the model is forced to learn smaller norm weights. Meanwhile, the verified error also starts to ramp up possibly due to the lack of capacity. We conjecture that linear relaxation based training over-regularizes the model, especially at a larger . However, in CROWN-IBP, the norm of weight matrices keep increasing during the training process, and verifiable error does not significantly increase when reaches 0.3. Another issue with current linear relaxation based training or verification methods is their high computational and memory cost, and poor scalability. For the small network in  Figure 1 , convex adversarial polytope (with 50 random Cauchy projections) is 8 times slower and takes 4 times more memory than CROWN-IBP (without using random projections). Convex adversarial polytope scales even worse for larger networks; see Appendix J for a comparison.

Section Title: THE PROPOSED ALGORITHM: CROWN-IBP
  THE PROPOSED ALGORITHM: CROWN-IBP Overview. We have reviewed IBP and linear relaxation based methods above. As shown in  Gowal et al. (2018) , IBP performs well at large with much smaller verified error, and also efficiently scales to large networks; however, it can be sensitive to hyperparameters due to its very imprecise bound at the beginning phase of training. On the other hand, linear relaxation based methods can give tighter lower bounds at the cost of high computational expenses, but it over-regularizes the network at large and forbids us to achieve good standard and verified accuracy. We propose CROWN-IBP, a new certified defense where we optimize the following problem (θ represents the network parameters): where our lower bound of margin m(x, ) is a combination of two bounds with different natures: IBP, and a CROWN-style bound (which will be detailed below); L is the cross-entropy loss. Note that the combination is inside the loss function and is thus still a valid lower bound; thus (4) still holds and we are within the minimax robust optimization theoretical framework. Similar to IBP and Published as a conference paper at ICLR 2020 TRADES ( Zhang et al., 2019a ), we use a mixture of natural and robust training loss with parameter κ, allowing us to explicitly trade-off between clean accuracy and verified accuracy. In a high level, the computation of the lower bounds of CROWN-IBP (m CROWN-IBP (x, )) consists of IBP bound propagation in a forward bounding pass and CROWN-style bound propagation in a backward bounding pass. We discuss the details of CROWN-IBP algorithm below.

Section Title: Forward Bound Propagation in CROWN-IBP
  Forward Bound Propagation in CROWN-IBP In CROWN-IBP, we first obtain z (l) and z (l) for all layers by applying (5), (6) and (7). Then we will obtain m IBP (x, ) = z (L) (assuming C is merged into W (L) ). The time complexity is comparable to two forward propagation passes of the network. Linear Relaxation of ReLU neurons Given z (l) and z (l) computed in the previous step, we first check if some neurons are always active (z (l) k > 0) or always inactive (z (l) k < 0), since they are effectively linear and no relaxations are needed. For the remaining unstable neurons,  Zhang et al. (2018) ;  Wong & Kolter (2018)  give a linear relaxation for ReLU activation function: α k z (l) k ≤ σ(z (l) k ) ≤ z (l) k z (l) k − z (l) k z (l) k − z (l) k z (l) k z (l) k − z (l) k , for all k ∈ [n l ] and z (l) k < 0 < z (l) k , (10) where 0 ≤ α k ≤ 1;  Zhang et al. (2018)  propose to adaptively select α k = 1 when z (l) k > |z (l) k | and 0 otherwise, which minimizes the relaxation error. Following (10), for an input vector z (l) , we effectively replace the ReLU layer with a linear layer, giving upper or lower bounds of the output: D (l) z (l) ≤ σ(z (l) ) ≤ D (l) z (l) + c (l) d (11) where D (l) and D (l) are two diagonal matrices representing the "weights" of the relaxed ReLU layer. Other general activation functions can be supported similarly. In the following we focus on conceptually presenting the algorithm, while more details of each term can be found in the Appendix.

Section Title: Backward Bound Propagation in CROWN-IBP
  Backward Bound Propagation in CROWN-IBP Unlike IBP, CROWN-style bounds start bound- ing from the last layer, so we refer to it as backward bound propagation (not to be confused with the back-propagation algorithm to obtain gradients). Suppose we want to obtain the lower bound [m CROWN-IBP (x, )] i := z (L) i (we assume the specification matrix C has been merged into W (L) ). The input to layer W (L) is σ(z (L−1) ), which can be bounded linearly by Eq. (11). CROWN-style bounds choose the lower bound of σ(z (L−1) k ) (LHS of (11)) when W (L) i,k is positive, and choose the upper bound otherwise.We then merge W (L) and the linearized ReLU layer together and define: Now we have a lower bound z (L) i = A (L−1) i,: z (L−1) + b (L−1) i ≤ z (L) i where b (L−1) i = k,W (L) i,k <0 W (L) i,k c (l) k + b (L) collects all terms not related to z (L−1) . Note that the diagonal matrix D i,(L−1) implicitly depends on i. Then, we merge A (L−1) i,: with the next linear layer, which is straight forward by plugging in z (L−1) = W (L−1) σ(z (L−2) ) + b (L−1) : Then we continue to unfold the next ReLU layer σ(z (L−2) ) using its linear relaxations, and compute a new A (L−2) ∈ R n L ×n L−2 matrix, with A (L−2) i,: = A (L−1) i,: W (L−1) D i,(L−2) in a similar manner as in (12). Along with the bound propagation process, we need to compute a series of matrices, A (L−1) , · · · , A (0) , where A (l) i,: = A (l+1) i,: W (l+1) D i,(l) ∈ R n L ×n (l) , and A (0) i,: = A (1) i,: W (1) = W (L) i,: D i,(L−1) W (L−2) D i,(L−2) A (L−2) · · · D i,(1) W (1) . At this point, we merged all layers of the network into a linear layer: z (L) i ≥ A (0) i,: x + b, where b collects all terms not related to x. A lower bound for z (L) i with x L ≤ x ≤ x U can then be easily given as For ReLU networks, convex adversarial polytope ( Wong & Kolter, 2018 ) uses a very similar bound propagation procedure. CROWN-style bounds allow an adaptive selection of α i in (10), thus often gives better bounds (e.g., see  Table 1 ). We give details on each term in Appendix L. Computational Cost. Ordinary CROWN ( Zhang et al., 2018 ) and convex adversarial poly- tope ( Wong & Kolter, 2018 ) use (13) to compute all intermediate layer's z (m) i and z (m) i (m ∈ [L]), by considering W (m) as the final layer of the network. For each layer m, we need a different set of m A matrices, defined as A m,(l) , l ∈ {m − 1, · · · , 0}. This causes three computational issues: • Unlike the last layer W (L) , an intermediate layer W (m) typically has a much larger output dimension n m n L thus all A m,(l) ∈ {A m,(m−1) , · · · , A m,(0) } have large dimensions R nm×n l . • Computation of all A m,(l) matrices is expensive. Suppose the network has n neurons for all L − 1 intermediate and input layers and n L n neurons for the output layer (assuming L ≥ 2), the time complexity of ordinary CROWN or convex adversarial polytope is O( L−2 l=1 ln 3 + (L − 1)n L n 2 ) = O((L − 1) 2 n 3 + (L − 1)n L n 2 ) = O(Ln 2 (Ln + n L )). A ordinary forward propagation only takes O(Ln 2 ) time per example, thus ordinary CROWN does not scale up to large networks for training, due to its quadratic dependency in L and extra Ln times overhead. • When both W (l) and W (l−1) represent convolutional layers with small kernel tensors K (l) and K (l−1) , there are no efficient GPU operations to form the matrix W (l) D (l−1) W (l−1) using K (l) and K (l−1) . Existing implementations either unfold at least one of the convolutional kernels to fully connected weights, or use sparse matrices to represent W (l) and W (l−1) . They suffer from poor hardware efficiency on GPUs. In CROWN-IBP, we use IBP to obtain bounds of intermediate layers, which takes only twice the regular forward propagate time (O(Ln 2 )), thus we do not have the first and second issues. The time complexity of the backward bound propagation in CROWN-IBP is O((L − 1)n L n 2 ), only n L times slower than forward propagation and significantly more scalable than ordinary CROWN (which is Ln times slower than forward propagation, where typically n n L ). The third convolution issue is also not a concern, since we start from the last specification layer W (L) which is a small fully connected layer. Suppose we need to compute W (L) D (L−1) W (L−1) and W (L−1) is a convolutional layer with kernel K (L−1) , we can efficiently compute (W (L−1) (D (L−1) W (L) )) on GPUs using the transposed convolution operator with kernel K (L−1) , without unfolding any convoluational layers. Conceptually, the backward pass of CROWN-IBP propagates a small specification matrix W (L) backwards, replacing affine layers with their transposed operators, and activation function layers with a diagonal matrix product. This allows efficient implementation and better scalability. Benefits of CROWN-IBP. Tightness, efficiency and flexibility are unique benefits of CROWN-IBP: • CROWN-IBP is based on CROWN, a tight linear relaxation based lower bound which can greatly improve the quality of bounds obtained by IBP to guide verifiable training and improve stabability; • CROWN-IBP avoids the high computational cost of convex relaxation based methods : the time complexity is reduced from O(Ln 2 (Ln + n L )) to O(Ln 2 n L ), well suited to problems where the output size n L is much smaller than input and intermediate layers' sizes; also, there is no quadratic dependency on L. Thus, CROWN-IBP is efficient on relatively large networks; • The objective (9) is strictly more general than IBP and allows the flexibility to exploit the strength from both IBP (good for large ) and convex relaxation based methods (good for small ). We can slowly decrease β to 0 during training to avoid the over-regularization problem, yet keeping the initial training of IBP more stable by providing a much tighter bound; we can also keep β = 1 which helps to outperform convex relaxation based methods in small regime (e.g., = 2/255 on CIFAR-10).

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: Models and training schedules
  Models and training schedules We evaluate CROWN-IBP on three models that are similar to the models used in ( Gowal et al., 2018 ) on MNIST and CIFAR-10 datasets with different ∞ perturbation norms. Here we denote the small, medium and large models in  Gowal et al. (2018)  as DM-small, DM-medium and DM-large. During training, we first warm up (regular training without robust loss) Published as a conference paper at ICLR 2020 for a fixed number of epochs and then increase from 0 to train using a ramp-up schedule of R epochs. Similar techniques are also used in many other works ( Wong et al., 2018 ;  Wang et al., 2018a ;  Gowal et al., 2018 ). For both IBP and CROWN-IBP, a natural cross-entropy (CE) loss with weight κ (as in Eq (9)) may be added, and κ is scheduled to linearly decrease from κ start to κ end within R ramp-up epochs.  Gowal et al. (2018)  used κ start = 1 and κ end = 0.5. To understand the trade-off between verified accuracy and standard (clean) accuracy, we explore two more settings: κ start = κ end = 0 (without natural CE loss) and κ start = 1, κ end = 0. For β, a linear schedule during the ramp-up period is used, but we always set β start = 1 and β end = 0, except that we set β start = β end = 1 for CIFAR-10 at = 2 255 . Detailed model structures and hyperparameters are in Appendix C. Our training code for IBP and CROWN-IBP, and pre-trained models are publicly available 3 .

Section Title: Metrics
  Metrics Verified error is the percentage of test examples where at least one element in the lower bounds m(x k , ) is < 0. It is an guaranteed upper bound of test error under any ∞ perturbations. We obtain m(x k , ) using IBP or CROWN-IBP (Eq. 13). We also report standard (clean) errors and errors under 200-step PGD attack. PGD errors are lower bounds of test errors under ∞ perturbations. Comparison to IBP.  Table 2  represents the standard, verified and PGD errors under different for each dataset with different κ settings. We test CROWN-IBP on the same model structures in  Table 1  of  Gowal et al. (2018) . These three models' architectures are presented in Table A in the Appendix. Here we only report the DM-large model structure in as it performs best under all setttings; small and medium models are deferred to Table C in the Appendix. When both κ start = κ end = 0, no natural CE loss is added and the model focuses on minimizing verified error, but the lack of natural CE loss may lead to unstable training, especially for IBP; the κ start = 1, κ end = 0.5 setting emphasizes on minimizing standard error, usually at the cost of slightly higher verified error rates. κ start = 1, κ end = 0 typically achieves the best balance. We can observe that under the same κ settings, CROWN-IBP outperforms IBP in both standard error and verified error. The benefits of CROWN-IBP is significant especially when model is large and is large. We highlight that CROWN-IBP reduces the verified error rate obtained by IBP from 8.21% to 7.02% on MNIST at = 0.3 and from 55.88% to 46.03% on CIFAR-10 at = 2/255 (it is the first time that an IBP based method outperforms results from ( Wong et al., 2018 ), and our model also has better standard error). We also note that we are the first to obtain verifiable bound on CIFAR-10 at = 16/255. Trade-off Between Standard Accuracy and Verified Ac- curacy. To show the trade-off between standard and ver- ified accuracy, we evaluate DM-large CIFAR-10 model with test = 8/255 under different κ settings, while keeping all other hyperparameters unchanged. For each κ end = {0.5, 0.25, 0}, we uniformly choose 11 κ start ∈ [1, κ end ] while keeping all other hyper-parameters unchanged. A larger κ start or κ end tends to produce better standard errors, and we can explicitly control the trade-off between standard accuracy and verified accuracy. In  Figure 2  we plot the stan- dard and verified errors of IBP and CROWN-IBP trained models with different κ settings. Each cluster on the figure has 11 points, representing 11 different κ start values. Models with lower verified errors tend to have higher standard er- rors. However, CROWN-IBP clearly outperforms IBP with improvement on both standard and verified accuracy, and pushes the Pareto front towards the lower left corner, indicating overall better performance. To reach the same verified error of 70%, CROWN-IBP can reduce standard error from roughly 55% to 45%.

Section Title: Training Stability
  Training Stability Table 4 of  Gowal et al. (2018)  are evaluated using mixed integer programming (MIP) and linear programming (LP), which are strictly smaller than IBP verified errors but computationally expensive. For a fair comparison, we use the IBP verified errors reported in their Table 3.

Section Title: CONCLUSIONS
  CONCLUSIONS We propose a new certified defense method, CROWN-IBP, by combining the fast interval bound propagation (IBP) bound and a tight linear relaxation based bound, CROWN. Our method enjoys high computational efficiency provided by IBP while facilitating the tight CROWN bound to stabilize training under the robust optimization framework, and provides the flexibility to trade-off between the two. Our experiments show that CROWN-IBP consistently outperforms other IBP baselines in both standard errors and verified errors and achieves state-of-the-art verified test errors for ∞ robustness.
  PyTorch implementation and pre-trained models: https://github.com/huanzhang12/CROWN-IBP

```
