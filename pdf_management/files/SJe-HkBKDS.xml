<article article-type="research-article"><front><article-meta><title-group><article-title>Amharic Text Normalization with Sequence-to-Sequence Models</article-title></title-group><abstract /></article-meta></front><body><sec><title>Introduction</title><p>Text normalization or the transformation of words from the written to the spoken form is an important problem. It is also quite a complex problem, due to the range of different kinds of non-standard words (NSW's), the special processing required for each case, and the propensity for ambiguity among NSW's as a class. Unfortunately, text normalization is not a problem that has received a great deal of attention.</p></sec><sec><title>Method</title><p>We have collected data that includes many types of non-standard words from different Amharic news Media and websites, FBC (Fana Broadcasting Corporate) more than eighty percent, VOA (Voice of America) and BBC (British Broadcasting corporate) with four types of news domain, including political, economic, sport and health, which contributes to the inclusion of most of Amharic NSWs.</p><p>After data gathering and preprocessing, the processed text is an input for our general architecture first component, tokenization, a process of segmentation of collected document into sentence-level and then to word level. Later every token has to pass through a token identification process that identifies its token class type, from which we are going to determine its expansion form for training data preparation. Finally, the training data is the input for our architecture last component, the sequence-to-sequence architecture, which model the whole task as one where we map a sequence of input characters to a sequence of output words.</p></sec><sec><title>Experiment and Results</title><p>We use bidirectional GRU with the size of 250 hidden units both for encoding and decoding layers. The training takes 7 days on the whole 200 thousand sentences training data on 8GB RAM with 2.5 GHz intel core i5 Laptop. Across all experiments, to reduce the input dimension, we choose character-based solution over word-based.</p></sec><sec><title>Conclusion</title><p>In this work, based on our final experiment results, we found 94.8 percent accuracy which is promising, considering that we trained on a small annotated dataset. Besides, on top of the corpus we prepared, we believe the performance of the model can be further improved by designing a more balanced training dataset.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>A Framework for Amharic Text Normalization Detecting the class of the token or the taxonomy is the key part of this task. Once we have determined the class of a token correctly, we expand it based on our rule-based normalizer accordingly. The usage of a token in a sentence determines its class. To determine the class of the token in focus, the surrounding tokens play an important role.</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>The Sequence-to-Sequence Architecture Our architecture performs at a character-level input sequence and word-level output sequence. We used an embedding layer, Bidirectional 3-layer Gated Recurrent Units (GRU) encoder that reads input tokens, and a one-layer GRU decoder that produces word sequences. These are the translation for the architecture inputs and outputs, "&#4768;" for "A", "&#4841;" for "U", "&#4768;&#4850;&#4661;" for "Addis", "&#4768;&#4704;&#4707;" for "Ababa" and "&#4841;&#4754;&#4712;&#4653;&#4658;&#4722;" for "University".</p></caption><graphic /><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Accuracy of Experiment 4</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>A Text Normalisation System for Non-Standard English Words</article-title><source>Proc. 3rd Work. Noisy User-generated Text</source><year>2017</year><volume>2</volume><fpage>107</fpage><lpage>115</lpage><person-group person-group-type="author"><name><surname>References</surname><given-names>E</given-names></name><name><surname>Flint</surname><given-names>E</given-names></name><name><surname>Ford</surname><given-names>O</given-names></name><name><surname>Thomas</surname><given-names>A</given-names></name><name><surname>Caines</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Improving Neural Text Normalization with Data Augmentation at Character-and Morphological Levels</article-title><source>Proc. 8th Int. Jt. Conf. Nat. Lang. Process.</source><year>2017</year><fpage>257</fpage><lpage>262</lpage><person-group person-group-type="author"><name><surname>Saito</surname><given-names>I</given-names></name><name><surname>Suzuki</surname><given-names>J</given-names></name><name><surname>Nishida</surname><given-names>K</given-names></name><name><surname>Sadamitsu</surname><given-names>K</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>DeepNorm-A Deep Learning Approach to Text Normalization</article-title><source>CoRR</source><year>2017</year><fpage>1712</fpage><lpage>1712</lpage><person-group person-group-type="author"><name><surname>Zare</surname><given-names>M</given-names></name><name><surname>Rohatgi</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>RNN Approaches to Text Normalization: A Challenge</article-title><source>Proc. 7th ACM Conf. Recomm. Syst. - RecSys '13</source><year>2016</year><person-group person-group-type="author"><name><surname>Sproat</surname><given-names>R</given-names></name><name><surname>Jaitly</surname><given-names>N</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><source>Text Normalization using Memory Augmented Neural Networks</source><year>2017</year><volume>2018</volume><person-group person-group-type="author"><name><surname>Pramanik</surname><given-names>S</given-names></name><name><surname>Hussain</surname><given-names>A</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><source>Context Tailoring for Text Normalization</source><year>2016</year><fpage>6</fpage><lpage>14</lpage><person-group person-group-type="author"><name><surname>Demir</surname><given-names>S</given-names></name></person-group></element-citation></ref></ref-list></back></article>