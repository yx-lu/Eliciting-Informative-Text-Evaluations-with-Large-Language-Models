Title:
```
Under review as a conference paper at ICLR 2020 A THEORETICAL ANALYSIS OF DEEP Q-LEARNING
```
Abstract:
```
Despite the great empirical success of deep reinforcement learning, its theoretical foundation is less well understood. In this work, we make the first attempt to theoretically understand the deep Q-network (DQN) algorithm (Mnih et al., 2015) from both algorithmic and statistical perspectives. In specific, we focus on the fitted Q iteration (FQI) algorithm with deep neural networks, which is a slight simplification of DQN that captures the tricks of experience replay and target network used in DQN. Under mild assumptions, we establish the algorithmic and statistical rates of convergence for the action-value functions of the iterative policy sequence obtained by FQI. In particular, the statistical error characterizes the bias and variance that arise from approximating the action-value function using deep neural network, while the algorithmic error converges to zero at a geometric rate. As a byproduct, our analysis provides justifications for the techniques of experience replay and target network, which are crucial to the empirical success of DQN. Furthermore, as a simple extension of DQN, we propose the Minimax-DQN algorithm for zero-sum Markov game with two players, which is deferred to the appendix due to space limitations.
```

Figures/Tables Captions:
```

```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) attacks the multi-stage decision-making problems by interacting with the environment and learning from the experiences. With the breakthrough in deep learning, deep reinforcement learning (DRL) demonstrates tremendous success in solving highly challenging problems, such as the game of Go ( Silver et al., 2016 ;  2017 ), robotics (Kober & Peters, 2012), and dialogue systems ( Chen et al., 2017 ). In DRL, the value or policy functions are often represented as deep neural networks and the related deep learning techniques can be readily applied. For example, deep Q-network (DQN) ( Mnih et al., 2015 ), asynchronous advantage actor-critic (A3C) and ( Mnih et al., 2016 ) demonstrate superhuman performance in various applications and become standard algorithms for artificial intelligence. Despite its great empirical success, there exists a gap between the theory and practice of DRL. In particular, most existing theoretical work on reinforcement learning focuses on the tabular case where the state and action spaces are finite, or the case where the value function is linear. Under these restrictive settings, the algorithmic and statistical perspectives of reinforcement learning are well-understood via the tools developed for convex optimization and linear regression. However, in presence of nonlinear function approximators such as deep neural network, the theoretical analysis of reinforcement learning becomes intractable as it involves solving a highly nonconvex statistical optimization problem. To bridge such a gap in DRL, we make the first attempt to theoretically understand DQN, which can be cast as an extension of the classical Q-learning algorithm (Watkins & Dayan, 1992) that uses deep neural network to approximate the action-value function. Although the algorithmic and statistical properties of the classical Q-learning algorithm are well-studied, theoretical analysis of DQN is highly challenging due to its differences in the following two aspects.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 First, in online gradient-based temporal-difference reinforcement learning algorithms, approximating the action-value function often leads to instability.  Baird (1995)  proves that this is the case even with linear function approximation. The key technique to achieve stability in DQN is experience replay (Lin, 1992;  Mnih et al., 2015 ). In specific, a replay memory is used to store the trajectory of the Markov decision process (MDP). At each iteration of DQN, a mini-batch of states, actions, rewards, and next states are sampled from the replay memory as observations to train the Q-network, which approximates the action-value function. The intuition behind experience replay is to achieve stability by breaking the temporal dependency among the observations used in the training of the deep neural network. Second, in addition to the aforementioned Q-network, DQN uses another neural network named target network to obtain an unbiased estimator of the mean-squared Bellman error used in training the Q-network. The target network is synchronized with the Q-network after each period of iterations, which leads to a coupling between the two networks. Moreover, even if we fix the target network and focus on updating the Q-network, the subproblem of training a neural network still remains less well-understood in theory. In this paper, we focus on a slight simplification of DQN, which is amenable to theoretical analysis while fully capturing the above two aspects. In specific, we simplify the technique of experience replay with an independence assumption, and focus on deep neural networks with rectified linear units (ReLU) (Nair & Hinton, 2010) and large batch size. Under this setting, DQN is reduced to the neural fitted Q-iteration (FQI) algorithm (Riedmiller, 2005) and the technique of target network can be cast as the value iteration. More importantly, by adapting the approximation results for ReLU networks to the analysis of Bellman operator, we establish the algorithmic and statistical rates of convergence for the iterative policy sequence obtained by DQN. As shown in the main results in §3, the statistical error characterizes the bias and variance that arise from approximating the action-value function using neural network, while the algorithmic error geometrically decays to zero as the number of iteration goes to infinity. Our contribution is two-fold. First, we establish the algorithmic and statistical errors of the neural FQI algorithm, which can be viewed as a slight simplification of DQN. Under mild assumptions, our results show that the proposed algorithm obtains a sequence of Q-networks that geometrically converges to the optimal action-value function up to an intrinsic statistical error induced by the approximation bias of ReLU network and finite sample size. Second, as a byproduct, our analysis justifies the techniques of experience replay and target network used in DQN, where the latter can be viewed as the value iteration. In addition, we also extend our algorithm to zero-sum Markov games. Due to space limit, we defer these results to the appendix.

Section Title: Notation
  Notation For a measurable space with domain S, we denote by B(S, V ) the set of measurable functions on S that are bounded by V in absolute value. Let P(S) be the set of all probability measures over S. For any ν ∈ P(S) and any measurable function f : S → R, we denote by f p,ν the p -norm of f with respect to measure ν for p ≥ 1. In addition, for simplicity, we write f ν for f 2,ν . In addition, let {f (n), g(n)} n≥1 be two positive series. We write f (n) g(n) if there exists a constant C such that f (n) ≤ C · g(n) for all n larger than some n 0 ∈ N. In addition, we write f (n) g(n) if f (n) g(n) and g(n) f (n).

Section Title: BACKGROUND
  BACKGROUND In this section, we introduce the background. We first lay out the formulation of the reinforcement learning problem, and then define the family of ReLU neural networks.

Section Title: REINFORCEMENT LEARNING
  REINFORCEMENT LEARNING A discounted Markov decision process is defined by a tuple (S, A, P, R, γ). Here S is the set of all states, which can be countable or uncountable, A is the set of all actions, P : S × A → P(S) Under review as a conference paper at ICLR 2020 is the Markov transition kernel, R : S × A → P(R) is the distribution of the immediate reward, and γ ∈ (0, 1) is the discount factor. In specific, upon taking any action a ∈ A at any state s ∈ S, P (· |s, a) defines the probability distribution of the next state and R(· |s, a) is the distribution of the immediate reward. Moreover, for regularity, we further assume that S is a compact subset of R d which can be infinite, A = {a 1 , a 2 , . . . , a M } has finite cardinality M , and the rewards are uniformly bounded by R max , i.e., R(· |s, a) is supported on [−R max , R max ] for any s ∈ S and a ∈ A. A policy π : S → P(A) for the MDP maps any state s ∈ S to a probability distribution π(· | s) over A. For policy π, the corresponding value function V π : S → R is defined as the cumulative discounted reward obtained by when the actions are executed according to π, that is, Similarly, the action-value function Q π : S × A → R is defined as For any given action-value function Q : S × A → R, define the one-step greedy policy π Q as any policy that selects the action with the largest Q-value, that is, for any s ∈ S, π Q (· |s) satisfies Moreover, we define operator P π by (P π Q)(s, a) = E Q(S , A ) S ∼ P (· | s, a), A ∼ π(· | S ) , (2.4) and define the Bellman operator T π by (T π Q)(s, a) = r(s, a) + γ · (P π Q)(s, a), where r(s, a) = rR(dr | s, a) is the expected reward obtained at state s when taking action a. Then it can be verified that Q π is the unique fixed point of T π . The goal of reinforcement learning is to find the optimal policy, which achieves the largest cumulative reward. To characterize optimality, we define optimal action-value function Q * as Q * (s, a) = sup π Q π (s, a), (2.5) where the supremum is taken over all policies. Based on Q * , we define the optimal policy π * as any policy that is greedy with respect to Q * . It can be shown that Q * = Q π * . Finally, we define the Bellman optimality operator T via Then we have Bellman optimality equation T Q * = Q * .

Section Title: DEEP NEURAL NETWORK
  DEEP NEURAL NETWORK We study the performance of DQN with rectified linear unit (ReLU) activation function σ(u) = max(u, 0). For any positive integer L and {d j } L+1 j=0 ⊆ N, a ReLU network f : R d0 → R d L+1 with L hidden layers and width {d j } L+1 j=0 is of form f (x) = W L+1 σ(W L σ(W L−1 . . . σ(W 2 σ(W 1 x + v 1 ) + v 2 ) . . . v L−1 ) + v L ), (2.7) where W ∈ R d ×d −1 and v ∈ R d are the weight matrix and the shift vector in the -th layer, respectively. Here we apply σ to to each entry of its argument in (2.7). In deep learning, the network structure is fixed, and the goal is to learn the network parameters (weights) {W , v } ∈[L+1] with the convention that v L+1 = 0. For deep neural networks, the number of parameters greatly exceeds the input dimension d 0 . To restrict the model class, we focus on the class of ReLU networks where most parameters are zero. Under review as a conference paper at ICLR 2020 Definition 2.1 (Sparse ReLU Network). For any L, s ∈ N, {d j } L+1 j=0 ⊆ N , and V > 0, the family of sparse ReLU networks bounded by V with L hidden layers, network width d, and weight sparsity s is defined as F(L, {d j } L+1 i=0 , s, V ) = f : max ∈[L+1] W ∞ ≤ 1, L+1 =1 W 0 ≤ s, max j∈[d L+1 ] f j ∞ ≤ V , (2.8) where we denote (W , v ) by W . Moreover, f in (2.8) is expressed as in (2.7), and f j is the j-th component of f . Here we focus on functions that are uniformly bounded because the value functions in (2.1) and (2.2) are always bounded by V max = R max /(1 − γ). In the sequel, we write F(L, {d j } L+1 j=0 , s, V max ) as F(L, {d j } L+1 j=0 , s) to simplify the notation. In addition, we restrict the networks weights to be sparse, i.e., s is much smaller compared with the total number of parameters. Such an assumption implies that the network has sparse connections, which are useful for applying deep learning in memory-constrained situations such as mobile devices ( Han et al., 2015 ;  Liu et al., 2015 ). Moreover, we introduce the notion of Hölder smoothness as follows, which is a generalization of Lipschitz continuity, and is widely used to characterize the regularity of functions. where β > 0 and H > 0 are parameters and β is the largest integer no greater than β. In addition, here we use the multi-index notation by letting α = (α 1 , . . . , α r ) ∈ N r , and ∂ α = ∂ α1 . . . ∂ αr . Finally, we conclude this section by defining functions that can be written as a composition of multiple Hölder functions, which captures complex mappings in real-world applications such as multi-level feature extraction.

Section Title: Definition 2.3 (Composition of Hölder Functions)
  Definition 2.3 (Composition of Hölder Functions)

Section Title: UNDERSTANDING DEEP Q-NETWORK
  UNDERSTANDING DEEP Q-NETWORK In the DQN algorithm, a deep neural network Q θ : S × A → R is used to approximate Q * , where θ is the parameter. For completeness, we state the DQN as Algorithm 2 in §A. As shown in the experiments in  Mnih et al. (2015) , two tricks are pivotal for the success of DQN. First, DQN use the trick of experience replay (Lin, 1992). Specifically, at each time t, we store the transition (S t , A t , R t , S t+1 ) into the replay memory M, and then sample a minibatch of independent samples from M to train the neural network via stochastic gradient descent. Since the trajectory of MDP has strong temporal correlation, the goal of experience replay is to obtain uncorrelated samples, which yields accurate gradient estimation for the stochastic optimization problem. Another trick is to use a target network Q θ with parameter θ . Specifically, with independent samples {(s i , a i , r i , s i )} i∈[n] from the replay memory, to update the parameter θ of the Q-network, Under review as a conference paper at ICLR 2020 we compute the target Y i = r i + γ · max a∈A Q θ (s i , a), and update θ by the gradient of Whereas parameter θ is updated once every T target steps by letting θ = θ. That is, the target network is hold fixed for T target steps, and is thus updated in a slower pace. To demystify DQN, it is crucial to understand the role played by these two tricks. For experience replay, in practice, the replay memory size is usually very large. For example, the replay memory size is 10 6 in  Mnih et al. (2015) . Moreover, DQN use the -greedy policy, which enables exploration over S × A. Thus, when the replay memory is large, experience replay is close to sampling independent transitions from an explorative policy. This reduces the variance of the ∇L(θ), which is used to update θ. Thus, experience replay stabilizes the training of DQN, which benefits the algorithm in terms of computation. To understand the statistical property of DQN, we replace the experience replay by sampling independent transitions from a fixed distribution σ ∈ P(S ×A). That is, instead of sampling from the replay memory, we sample i.i.d. observations {(S i , A i )} i∈[n] from σ. Moreover, for any i ∈ [n], let R i and S i be the immediate reward and the next state when taking action A i at state S i . Under this setting, we have E(Y i | S i , A i ) = (T Q θ )(S i , A i ), where Q θ is the target network, which, as we show as follows, is motivated from a statistical consideration. Let us first neglect the target network and set θ = θ. Using bias-variance decomposition, the the expected value of L(θ) in (3.1) is Here the first term in (3.2) is known as the mean-squared Bellman error (MSBE), and the second term is the variance of Y 1 . Whereas L(θ) can be viewed as the empirical version of the MSBE, which has bias E{[Y 1 − (T Q θ )(S 1 , A 1 )] 2 } that also depends on θ. Thus, without the target network, minimizing L(θ) can be drastically different from minimizing the MSBE. To resolve this problem, we use a target network in (3.1), which has expectation E L(θ) = Q θ − T Q θ * 2 σ + E Y 1 − (T Q θ * )(S 1 , A 1 ) 2 , where the variance of Y 1 does not depend on θ. Thus, minimizing L(θ) is close to solving minimize θ∈Θ Q θ − T Q θ 2 σ , (3.3) where Θ is the parameter space. Note that in DQN we hold θ still and update θ for T target steps. When T target is sufficiently large and we neglect the fact that the objective in (3.3) is nonconvex, we would update θ by the minimizer of (3.3) for fixed θ . Therefore, in the ideal case, DQN aims to solve the minimization problem (3.3) with θ fixed, and then update θ by the minimizer θ. Interestingly, this view of DQN offers a statistical interpretation of the target network. In specific, if {Q θ : θ ∈ Θ} is sufficiently large such that it contains T Q θ , then (3.3) has solution Q θ = T Q θ , which can be viewed as one-step of value iteration (Sutton & Barto, 2011) for neural networks. In addition, in the sample setting, Q θ is used to construct {Y i } i∈[n] , which serve as the response in the regression problem defined in (3.1), with (T Q θ ) being the regression function. Furthermore, turning the discussion above into a realizable algorithm, we obtain the neural fitted Q-iteration (FQI) algorithm, which generates a sequence of value functions. Specifically, let F be a class of function defined on S × A. In the k-th iteration of FQI, let Q k be current estimate of Q * . Similar to (3.1) and (3.3), we define Y i = R i + γ · max a∈A Q k (S i , a), and update Q k by Under review as a conference paper at ICLR 2020 This gives the fitted-Q iteration algorithm, which is stated in Algorithm 1. When F is the family of neural networks, Algorithm 1 is known as the neural FQI, which is proposed in Riedmiller (2005). Thus, we can view neural FQI as an modification of DQN, where we replace experience replay by sampling from a fixed distribution σ, so as to understand its the statistical property. As a byproduct, such a modification naturally justifies the trick of target network in DQN. In addition, note that the optimization problem in (3.4) appears in each iteration of FQI, which is nonconvex when neural networks are used. However, since we focus solely on the statistical aspect, we make the assumption that the global optima of (3.4) can be reached, which is also contained F. Interestingly, a recent line of research on deep learning ( Du et al., 2018b ;a;  Zou et al., 2018 ; Chizat & Bach, 2018;  Allen-Zhu et al., 2018a ;b;  Jacot et al., 2018 ; Cao & Gu, 2019;  Arora et al., 2019 ;  Ma et al., 2019 ;  Mei et al., 2019 ; Yehudai & Shamir, 2019) has established global convergence of gradient- based algorithms for empirical risk minimization when the neural networks are overparametrized. We provide more discussions on the computation aspect in §B. Moreover, we make the i.i.d. assumption in Algorithm 1 to simplify the analysis.  Antos et al. (2008b)  study the performance of fitted value iteration with fixed data used in the regression sub-problems repeatedly, where the data is sampled from a single trajectory based on a fixed policy such that the induced Markov chain satisfies certain conditions on the mixing time. Using similar analysis as in  Antos et al. (2008b) , our algorithm can also be extended to handled fixed data that is collected beforehand. Update the action-value function: end for Define policy π K as the greedy policy with respect to Q K . Output: An estimator Q K of Q * and policy π K .

Section Title: THEORETICAL RESULTS
  THEORETICAL RESULTS We establish statistical guarantees for DQN with ReLU networks. Specifically, let Q π K be the action-value function corresponding to π K , which is returned by Algorithm 1. In the following, we obtain an upper bound for Q π K − Q * 1,µ , where µ ∈ P(S × A) is allowed to be different from ν. In addition, we assume that the state space S is a compact subset in R r and the action space A is finite. Without loss of generality, we let S = [0, 1] r hereafter, where r is a fixed integer. To begin with, we first specify the function class F in Algorithm 1. Definition 4.1 (Function Classes). Following Definition 2.1, let F(L, {d j } L+1 j=0 , s) be the family of sparse ReLU networks defined on S with d 0 = r and d L+1 = 1. Then we define F 0 by In addition, let G({p j , t j , β j , H j } j∈[q] ) be set of composition of Hölder smooth functions defined on S ⊆ R r . Similar to F 0 , we define a function class G 0 as Under review as a conference paper at ICLR 2020 By this definition, for any function f ∈ F 0 and any action a ∈ A, f (·, a) is a ReLU network defined on S, which is standard for Q-networks. Moreover, G 0 contains a broad family of smooth functions on S × A. In the following, we make a mild assumption on F 0 and G 0 . Assumption 4.2. We assume that for any f ∈ F 0 , we have T f ∈ G 0 , where T is the Bellman optimality operator defined in (2.6). That is, for any f ∈ F and any a ∈ A, (T f )(s, a) can be written as compositions of Hölder smooth functions as a function of s ∈ S. We remark that this assumption holds when the MDP satisfies some smoothness conditions. For any state-action pair (s, a) ∈ S × A, let P (· | s, a) be the density of the next state. By the definition of the Bellman optimality operator in (2.6), we have For any s ∈ S and a ∈ A, we define functions g 1 , g 2 by letting g 1 (s) = r(s, a) and g 2 (s) = P (s | s, a). Suppose both g 1 and g 2 are Hölder smooth functions on S = [0, 1] r with parameters β and H. Since f ∞ ≤ V max , by changing the order of integration and differentiation with respect to s in (4.3), we obtain that function s → (T f )(s, a) belongs to the Hölder class C r (S, β, H ) with H = H(1 + V max ). Furthermore, in the more general case, suppose for any fixed a ∈ A, we can write P (s | s, a) as h 1 [h 2 (s, a), h 3 (s )], where h 2 : S → R r1 , and h 3 : S → R r2 can be viewed as feature mappings, and h 1 : R r1+r2 → R is a bivariate function. We define function h 4 : R r1 → R by Then by (4.3) we have (T f )(s, a) = g 1 (s) + h 4 • h 2 (s, a). Then Assumption 4.2 holds if h 4 is Hölder smooth and both g 1 and h 2 can be represented as compositions of Hölder functions. Thus, Assumption 4.2 holds if both the reward function and the transition density of the MDP are sufficiently smooth. Moreover, even when the transition density is not smooth, we could also expect Assumption 4.2 to hold. Consider the extreme case where the MDP has deterministic transitions, that is, the next state s is a function of s and a, which is denoted by s = h(s, a). In this case, for any ReLU network f , we have (T f )(s, a) = r(s, a) + γ · max a ∈A f [h(s, a), a ]. Since max a ∈A f (s 1 , a ) − max a ∈A f (s 2 , a ) ≤ max a ∈A f (s 1 , a ) − f (s 2 , a ) for any s 1 , s 2 ∈ S, and network f (·, a) is Lipschitz continuous for any fixed a ∈ A, function m 1 (s) = max a f (s, a ) is Lipschitz on S. Thus, for any fixed a ∈ A, if both g 1 (s) = r(s, a) and m 2 (s) = h(s, a) are compositions of Hölder functions, so is (T f )(s, a) = g 1 (s) + m 1 • m 2 (s). Therefore, even if the MDP has deterministic dynamics, if both the reward function r(s, a) and the transition function h(s, a) are sufficiently nice, Assumption 4.2 still holds true. In the following, we define the concentration coefficients, which measures the similarity between two probability distributions under the MDP. Assumption 4.3 (Concentration Coefficients). Let ν 1 , ν 2 ∈ P(S × A) be two probability measures that are absolutely continuous with respect to the Lebesgue measure on S × A. Let {π t } t≥1 be a sequence of policies. Suppose the initial state-action pair (S 0 , A 0 ) of the MDP has distribution ν 1 , and we take action A t according to policy π t . For any integer m, we denote by P πm P πm−1 · · · P π1 ν 1 the distribution of (S m , A m ). Then we define the m-th concentration coefficient as κ(m; ν 1 , ν 2 ) = sup π1,...,πm E ν2 d(P πm P πm−1 · · · P π1 ν 1 ) dν 2 2 1/2 , (4.4) where the supremum is taken over all possible policies. Furthermore, let σ be the sampling distribution in Algorithm 1 and let µ be a fixed distribution on S × A. We assume that there exists a constant φ µ,σ < ∞ such that

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 By definition, concentration coefficients in (4.4) quantifies the similarity between ν 2 and the dis- tribution of the future states of the MDP when starting from ν 1 . Moreover, (4.5) is a standard assumption in the literature. See, e.g., Munos & Szepesvári (2008);  Lazaric et al. (2016) ;  Scherrer et al. (2015) ;  Farahmand et al. (2010 ; 2016). This assumption holds for large class of systems MDPs and specifically for MDPs whose top-Lyapunov exponent is finite. See  Munos & Szepesvári (2008) ;  Antos et al. (2007)  for more detailed discussions on this assumption. Now we are ready to present the main theorem. Theorem 4.4. Under Assumptions 4.2 and 4.3, let F 0 be defined in (4.1) based on the family of sparse ReLU networks F(L * , {d * j } L * +1 j=0 , s * ) and let G 0 be given in (4.2). Moreover, for any j ∈ [q − 1], we define β * j = β j · =j+1 min(β , 1); let β * q = 1. In addition, let α * = max j∈[q] t j /(2β * j + t j ). For the parameters of G 0 , we assume that there exists a constant ξ > 0 such that For the hyperparameters L * , {d * j } L * +1 j=0 , and s * of the ReLU network, we set d * 0 = 0 and d * L * +1 = 1. Moreover, we set L * (log n) ξ , max j∈[q] {p j+1 · t j } · n α * min i∈[L * ] d * j ≤ max j∈[L * ] d * j n ξ , and s * n α * · (log n) ξ (4.7) for some constant ξ > 0. For any K ∈ N, let Q π K be the action-value function corresponding to policy π K , which is returned by Algorithm 1 based on function class F 0 . Then there exists constants ξ * and C such that This theorem implies that the statistical rate of convergence is the sum of a statistical error and an algorithmic error. The algorithmic error converges to zero in linear rate as the algorithm proceeds, whereas the statistical error reflects the fundamental difficulty of the problem. Thus, when the number of iterations satisfy K ≥ C · log |A| + (1 − α * ) · log n / log(1/γ) iterations, where C is a sufficiently large constant, the algorithmic error is dominated by the statistical error. In this case, if we view both γ and φ µ,σ as constants and ignore the polylogarithmic term, Algorithm 1 achieves error rate |A| · n (α * −1)/2 = |A| · max j∈[q] n −β * j /(2β * j +tj ) , (4.9) which scales linearly with the capacity of the action space, and decays to zero when the n goes to infinity. Furthermore, the rates {n −β * j /(2β * j +tj ) } j∈[q] in (4.9) recovers the statistical rate of nonparametric regression in 2 -norm, whereas our statistical rate n (α * −1)/2 in (4.9) is the fastest among these nonparametric rates, which illustrates the benefit of compositional structure of G 0 . Furthermore, as a concrete example, we assume that both the reward function and the Markov transition kernel are Hölder smooth with smoothness parameter β. As stated below Assumption 4.2, for any f ∈ F 0 , we have (T f )(·, a) ∈ C r (S, β, H ). Then Theorem 4.4 implies that Algorithm 1 achieves error rate |A| · n −β/(2β+r) when K is sufficiently large. Since |A| is finite, this rate achieves the minimax-optimal statistical rate of convergence within the class of Hölder smooth functions defined on [0, 1] d (Stone, 1982) and thus cannot be further improved. In addition, as a simple extension of DQN, we propose the Minimax-DQN algorithm for zero-sum Markov game with two players. Specifically, in this setting, there are two players with action spaces A and B. The action-value function Q * (s, a, b) : S × A × B → R can be similarly defined, which correspond to the value obtained by a pair of policies that constitute the Nash equilibrium. Minimax- DQN differs from the original DQN mainly in the computation of target, which is obtained by solving Under review as a conference paper at ICLR 2020 a zero-sum matrix game via linear programming. Using similar proof technique, we establish both the algorithmic and statistical convergence rates of the action-value functions associated with the sequence of policies returned by the Minimax-DQN algorithm. Due to space limit, we defer the algorithm and its theory to §E.1 in the appendix. Under review as a conference paper at ICLR 2020

```
