Title:
```
Under review as a conference paper at ICLR 2020 ATTACK-RESISTANT FEDERATED LEARNING WITH RESIDUAL-BASED REWEIGHTING
```
Abstract:
```
Federated learning has a variety of applications in multiple domains by utilizing private training data stored on different devices. However, the aggregation process in federated learning is highly vulnerable to adversarial attacks so that the global model may behave abnormally under attacks. To tackle this challenge, we present a novel aggregation algorithm with residual-based reweighting to defend federated learning. Our aggregation algorithm combines repeated median regression with the reweighting scheme in iteratively reweighted least squares. Our experiments show that our aggregation algorithm outperforms other alternative algorithms in the presence of label-flipping, backdoor, and Gaussian noise attacks. We also provide theoretical guarantees for our aggregation algorithm.
```

Figures/Tables Captions:
```
Figure 1: The overview of our aggregation algorithm for attack-resistant federated learning.
Figure 2: Parameter confidence assignment based on the residual which is the distance from a point to the regression line. In the green area, the parameter confidence is 1; in the orange area, the confidence decays from 1 to Î´; in other areas, the confidence is set to 0.
Figure 3: Results of label-flipping attacks on the MNIST dataset. The number of participants is 100, within which 0 to 10 of them are attackers.
Figure 4: Result of backdoor attack success rate on CIFAR-10 under different aggregation algo- rithms. Ours outperforms other baselines.
Table 1: Results of label-flipping attacks on CIFAR-10 dataset with different numbers of attackers. The total number of participants is 10.
Table 2: Results of label-flipping attacks on Amazon Reviews dataset with different numbers of attackers. The total number of participants is 10.
Table 3: Results of backdoor attacks on MNIST and CIFAR-10. There are 10 participants, 1 of whom is an attacker. We denote the accuracy as Acc. and the attack success rate as A.S.R.
Table 4: Results of Gaussian noise attacks on the MNIST dataset when is sampled from a Gaussian distribution with different standard deviations. There are 100 participants where 10 are attackers.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Federated learning is a machine learning methodology for training a global model with decentralized data stored on multiple or even millions of devices ( McMahan et al., 2017 ). In federated learning, private data is stored locally in isolated devices and will not be revealed to other parties during training. Federated learning can enable numerous real-world machine learning applications by utilizing massive training data that are privacy-sensitive and scattered on different devices ( Bonawitz et al., 2017 ). For instance, multiple hospitals can collaborate to train a global model for classifying diseases using X-ray images without compromising patient privacy. Note that these hospitals may possess X-ray images in different quantities and varieties, resulting in the non-IID (independent and identically distributed) data distribution. Federated learning is different from distributed learning in the sense that the training data is often non-IID and we have no control over data distribution in federated learning. The default federated learning aggregation algorithm FedAvg ( McMahan et al., 2017 ) that takes the average of locally updated models is vulnerable to various attacks. We find that federated learning suffers from label-flipping, backdoor, and Gaussian noise attacks in our experiments. When a local model is poisoned, the aggregated global model can also be poisoned and fail to behave correctly. A label-flipping attack ( Biggio et al., 2012 ) happens where an attacker assigns incorrect labels to some data. For example, an attacker can train a local model with cat images mislabelled as dogs and then share the poisoned local model for aggregation. Mitigating attacks in federated learning or distributed learning has been explored in recent research ( Chen et al., 2017 ;  Yin et al., 2018 ; Fung et al., 2018;  Blanchard et al., 2017 ). Although the median or trimmed mean aggregation algorithms ( Yin et al., 2018 ) may seem plausible in distributed learning, their performance degrades in federated learning when data is non-IID. FoolsGold (Fung et al., 2018) is a defense algorithm that identifies participants with similar models as attackers but this strategy may not work when some harmless participants have similar local data. To make federated learning more attack-resistant, we develop an aggregation algorithm that is robust against label-flipping, backdoor, and Gaussian noise attacks in a general non-IID setting. We derive our aggregation algorithm by adopting the repeated median estimator ( Siegel, 1982 ) and the reweighting scheme in iteratively reweighted least squares (IRLS) ( Holland and Welsch, 1977 ;  Rand R, 1997 ). We estimate the confidence of each parameter in the local models and then the weight of each local model can be computed by heuristically accumulating all the parameter confidence in each local model. Our algorithm is straightforward to implement. Furthermore, we provide theoretical guarantees for our aggregation algorithm.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 We compare our proposed algorithm to several baselines by conducting experiments on four datasets, the MNIST dataset ( LeCun et al., 1998 ), CIFAR-10 dataset (Krizhevsky et al., 2009), Amazon Reviews dataset (Ruining and Julian, 2016) and the Lending Club loan dataset ( Kan, 2019 ). Our proposed aggregation significantly mitigates the impact of attacked models in non-IID federated learning and outperforms other baselines in our evaluation.

Section Title: RELATED WORK
  RELATED WORK Adversarial attacks on federated learning. Several attacks have been studied against federated learning ( Wang et al., 2018 ;  Biggio et al., 2012 ; Fung et al., 2018;  Hayes et al., 2019 ;  Hitaj et al., 2017 ;  Melis et al., 2019 ). The label-flipping attack ( Biggio et al., 2012 ) is shown to have great harm to a federated system even with a very small number of attackers (Fung et al., 2018). In this attack, the attacker flips the labels of training data in one class to another class and trains the model accordingly.  Bagdasaryan et al. (2018)  propose a backdoor attack so that the global model behaves incorrectly on adversarial targeted input. In our work, we mainly focus on defending against label-flipping attack, backdoor, and Gaussian noise attacks. Note that an attacker can perform any type of attacks, such as modifying any model values and training the local model on poisoned data for arbitrary epochs.

Section Title: Robust distributed learning
  Robust distributed learning Statistical methods have been studied and applied in robust distributed learning where data is IID ( Feng et al., 2014 ;  Blanchard et al., 2017 ;  Chen et al., 2017 ;  Yin et al., 2018 ;  Alistarh et al., 2018 ). The median method and the trimmed mean method ( Yin et al., 2018 ) are effective approaches in robust distributed learning, but may not be attack-resistant in federated learning where data distribution is non-IID. To tackle the challenge in robust federated learning, we propose a reweighted aggregation algorithm that dynamically assigns weights to the local model based on the residual to a regression line estimated by the repeated median estimator ( Siegel, 1982 ).

Section Title: Defending federated learning
  Defending federated learning Recently, some researchers have proposed some defense strategies for robust federated learning (Fung et al., 2018;  Blanchard et al., 2017 ). FoolsGold (Fung et al., 2018) is a defense mechanism against Sybil attacks by adjusting the learning rates of local models based on contribution similarity. The algorithm identifies grouped actions as Sybil attacks and promotes the diversity of local model update. However, FoolsGold may identify harmless participants as attackers when these participants have similar local data.  Gu et al. (2018)  proposed a model, CalTrain, that represents data with fingerprints to identify poisoned data and models.  Konstantinov and Lampert (2019)  propose to maintain a small reference dataset to justify the quality and accountability of models. While this method is effective, it requires a lot of time to evaluate each model in every single round. Our algorithm does not need an additional reference dataset before or after each aggregation process. Some researchers proposed to improve the privacy preservation of federated learning ( Bonawitz et al., 2017 ;  Geyer et al., 2017 ;  Truex et al., 2018 ;  Thakkar et al., 2019 ).  Bonawitz et al. (2017)  propose a privacy-preserving protocol for model aggregation in federated learning.  Geyer et al. (2017)  introduce differential privacy into federated learning. Instead of enhancing privacy preservation, we focus on the robustness of federated learning so that the global model should behave correctly even when there is a large portion of malicious participants.

Section Title: OUR ALGORITHM
  OUR ALGORITHM In federated learning, there are multiple rounds of communication between participants and a central server for learning a global model. In each round, the global model is shared among the K participants and a local model on each device is trained on its local private data with the shared global model as initialization. Then all the K local models are sent to the central server to update the global model with an aggregation algorithm. The original aggregation scheme uses a simple averaging algorithm to aggregate all the local models ( McMahan et al., 2017 ). Suppose the participant k has a local model M (k) , and we can update the global model M global by taking the (weighted) average of all the K local models.

Section Title: AGGREGATION ALGORITHM
  AGGREGATION ALGORITHM Median is a robust estimator widely used in statistics. However, when the data distribution is non-IID, median neglects a significant amount of information by merely taking a single median value. Hence, in our aggregation algorithm, the global model is designed to be a reweighted average of all the local models where the model weights are estimated robustly. Algorithm 1 summarizes our aggregation algorithm, and a detailed step-by-step description is provided below. We perform a weighted average of all the local models at the model level by assigning a weight to each local model. The weight of each local model is computed by accumulating the parameter confidence in the local model. The parameter confidence is computed based on the residual to a regression line estimated by the repeated median estimator ( Siegel, 1982 ;  Rand R, 1997 ). Inspired by the reweighting scheme in IRLS ( Rand R, 1997 ), we reweight each parameter by its vertical distance (residual) to a robust regression line. For the robust regression line estimation, we use the repeated median estimator ( Siegel, 1982 ) since it has a high breakdown point of 50%. Let y (k) n be the n-th parameter of the k-th local model. We use y n to indicate the list of n-th parameters in all the local models. Let x n be the indices of y n sorted in an ascending order. Then (x n , y n ) is a point set in 2D with increasing values in the y direction.

Section Title: Repeated median
  Repeated median We use the repeated median estimator ( Siegel, 1982 ) to estimate a linear regres- sion line y = Î² n0 + Î² n1 x. The slope Î² n1 and intercept Î² n0 are estimated as follow, Residual computation. We can calculate the residuals of the n-th parameters in all the local models: Since r n can be very different in magnitude for different parameters, we can normalize r n similar to the reweighting scheme in IRLS ( Rand R, 1997 ): where Î³ is a constant. We set Î³ = 1.48 recommended by Wilcox et al. ( Rand R, 1997 ). Then the normalized residuals become Parameter confidence. After obtaining the nor- malized residuals, the parameter confidence can be determined accordingly ( Rand R, 1997 ): We use Î» = 2 in our experiments. Î¨ here acts as a trusted interval and we can expand or shrink the interval by tuning Î». h kk is the k-th diagonal element of matrix in H n :

Section Title: Extreme value correction
  Extreme value correction Extremely large values, even multiplied with a small weight in model aggregation, can damage the global model. We address this issue by involving a a threshold Î´. If a parameter has a confidence value lower than Î´, then it should be corrected as follows, The process of selecting proper Î´ is discussed in the appendix.

Section Title: Model weight
  Model weight To obtain the weight of each local model, we can simply aggregate the parameter confidence in the local model but this is not ideal. Imagine an attacker trains a model honestly, but then alters only 10% of the parameters to some extremely large values. This adversary model still receives about 90% of the parameter confidence. To address this problem, we measure the importance of a parameter by the standard deviation of w n . A confidence assignment with a large standard deviation indicates a great disagreement among this parameter in all models and should be more critical when being accumulated towards model weights: Under review as a conference paper at ICLR 2020 where W (k) is the weight for model k, N is the number of parameters, w n = [w (1) n w (2) n . . . w (K) n ] T . Global model. Finally, we can obtain the updated global model by

Section Title: THEORETICAL GUARANTEE
  THEORETICAL GUARANTEE For simplicity, we consider the bound of the error rate for training a single-parameter model on K devices, each storing S IID samples of data. Suppose that there are K devices and U of them are corrupted. We denote the set of adversarial devices as B and the corruption ratio Î± = U K . We define the parameter of the local model i asÅ· (i) and let Âµ be the optimal model that minimizes the population loss. Based on our algorithm, the residuals can be simplified as (Å· (i) âá»¹), whereá»¹ is the median of {Å· (i) } for i = 1, 2, ..., K. Let |r| be the median of absolute residuals, i.e., |r| = median(|Å· (i) âá»¹|). Then, the normalized residual can be expressed as and the parameter confidence is defined as Then we will prove that the error of the global model M global := 1 K j=1 z (j) K i=1 z (i)Å·(i) is bounded: The details of the proof are presented in the appendix. The bound indicates that when the amount of data and the number of models are sufficiently large, we can achieve a small error rateÃ( 1 â S + 1 â SK + 1 S + 1 â KÎ´ ) with high probability.

Section Title: EXPERIMENTS
  EXPERIMENTS We compare our approach with other aggregation algorithms, including FedAvg ( McMahan et al., 2017 ), the coordinate-wise median method ( Yin et al., 2018 ), the coordinate-wise trimmed mean method ( Yin et al., 2018 ), FoolsGold (Fung et al., 2018), and a coordinate-wise repeated median approach we adopt from ( Siegel, 1982 ). We perform experiments on the MNIST handwritten digit dataset ( LeCun et al., 1998 ), the Amazon Reviews dataset (Ruining and Julian, 2016), the CIFAR-10 dataset (Krizhevsky et al., 2009), and the Lending Club loan dataset ( Kan, 2019 ). We implement attack strategies and defense algorithms in PyTorch ( Paszke et al., 2017 ). We use a two-layer convolutional neural network (CNN) for our MNIST experiments and the network architecture is shown in the appendix. With this simple CNN model, our goal is to evaluate different aggregation algorithms for defending federated learning in the presence of attacks. On the CIFAR-10 dataset, we use ResNet-18 ( He et al., 2015 ) for image classification. The text classification model FastText ( Joulin et al., 2016 ) is adopted for evaluation on the Amazon Reviews dataset. It is a two-layer deep neural network where the first layer is an embedding layer, and the second layer is a fully connected layer. For the Lending Club loan dataset, we use a simple neural network with Under review as a conference paper at ICLR 2020 three fully-connected layers to classify loan status. We use the last two models to demonstrate that our algorithm can be generalized to a natural language processing task and to a real-work financial problem. All the evaluation results are the average of running the same experiments 3 times.

Section Title: DATASETS AND EXPERIMENTAL
  DATASETS AND EXPERIMENTAL

Section Title: MNIST dataset
  MNIST dataset The MNIST dataset contains 70,000 real-world handwritten images with digits from 0 to 9. We evaluate different methods by learning a global model on these training images distributed on multiple devices in a non-IID setting with adversarial attacks.

Section Title: CIFAR-10 dataset
  CIFAR-10 dataset The CIFAR-10 dataset contains 60,000 natural images in ten object classes. The experimental setup is also non-IID on CIFAR-10.

Section Title: Amazon Reviews dataset
  Amazon Reviews dataset The Amazon Reviews dataset (Ruining and Julian, 2016) contains product reviews and ratings collected from the Amazon website. Every review is paired with a sentiment rating from 1 to 5. We categorize comments with rating 1 or 2 as negative and comments with rating 4 or 5 as positive. We discard reviews with rating 3, and we only train a binary classifier. We only use the book reviews from the Kindle Store. 20% of the reviews are used for testing, while the rest is for training. We obtain a training set of 86,164 reviews and a test set of 13,260 reviews.

Section Title: Lending Club Loan dataset
  Lending Club Loan dataset The Lending club dataset LOAN ( Kan, 2019 ) contains financial information such as credit scores and the number of finance inquiries for loan status prediction ("Current", "Late", or "Fully Paid"). There are 1,808,534 data samples in 9 types of loan status. We divide them by US states to simulate the federated learning scenarios with non-IID data distribution where each state represents a participant.

Section Title: RESULTS ON LABEL-FLIPPING ATTACKS
  RESULTS ON LABEL-FLIPPING ATTACKS We evaluate the overall classification performance of different aggregation methods on three datasets under label-flipping attacks, the MNIST dataset ( Biggio et al., 2012 ), the CIFAR-10 dataset (Krizhevsky et al., 2009), and Amazon Reviews dataset (Ruining and Julian, 2016). In label-flipping attacks, attackers flip the labels of training examples in the source class to a target class and train their models accordingly. In the MNIST experiment, we simulate federated learning with 100 participants, within which 0 to 10 of them are attackers. Each participant contains images of two random digits. The attackers are chosen to be some participants with images of digit 1 and another random digit since they are flipping the label of 1 to 7. We run 200 synchronization rounds with Î´ set to 0.01. In each round of federated learning, each participant is supposed to train the local model for 5 epochs, but the attackers can train for arbitrary epochs. The results are shown in  Figure 3  where attackers train the models with 5 more epochs to enhance the attacks. Our algorithm outperforms all other methods and is robust when the number of attackers increases. Median methods (Median and Repeated Median) have relatively low accuracy due to discarding most of the information in model aggregation. Our algorithm, on the other hand, takes the reweighted average of all local models and thus gathers more information in an unsupervised way. We also compare our algorithm with the state-of-the-art algorithm FoolsGold (Fung et al., 2018). Though their algorithm also maintains a low attack success rate, our algorithm is more stable and surpasses FoolsGold by 6% on average. For the CIFAR-10 experiment, following  Bagdasaryan et al. (2018) , we adopt a Dirichlet distribu- tion ( Minka, 2000 ) with a hyperparameter 0.9 to generate non-IID data distribution for totally 10 participants. The experimental setup is the same for the CIFAR-10 and MNIST experiments under backdoor attacks in Section 4.3. The attackers flip the label of "cat" to "dog" since they are the most similar classes in CIFAR-10. The experiment results can be found in  Table 1 . In this experiment, baseline methods perform fairly well because the data distribution is not immensely non-i.i.d., where each user has all the labels but in different amounts. FoolsGold (Fung et al., 2018) fails because they assume that the gradients of honest models are very different because of the non-i.i.d. data, and the gradients of outliers are close because they share the same objective. However, it may not always be the case when the data distribution is not so extremely non-i.i.d, such as the CIFAR-10 case. FoolsGold may think these honest users are close and decide they are outliers. Our algorithm, on the other hand, can adapt to extremely non-i.i.d. cases, such as the MNIST experiments, where all Under review as a conference paper at ICLR 2020 baseline methods fail, and is also suitable for common non-i.i.d. situations such as the CIFAR-10 and the Amazon Review experiments. In the experiment on Amazon Reviews, there are 10 participants, where 0 to 4 participants are attackers who flip all their labels. Attackers also train for 5 more epochs. We run 10 synchronization rounds in the experiment. The result is summarized in  Table 2 . Our algorithm achieves comparable state-of-the-art results, and our performance does not degrade when there are less than 50% attackers.

Section Title: RESULTS ON BACKDOOR ATTACKS
  RESULTS ON BACKDOOR ATTACKS For pixel-pattern backdoor attacks ( Gu et al., 2019 ) in federated learning( Bagdasaryan et al., 2018 ), attackers manipulate their local models so that the learned global model predicts some backdoor target label for any image embedded with certain patterns. An example is shown in Figure 6 in the Appendix. The global model can behave normally for clean data. We choose "bird" in CIFAR-10 and "2" in MNIST as the backdoor target labels. Similarly, for the preprocessed LOAN dataset that contains 92 features, we manipulate 6 features by assigning certain large values to them and change the labels of manipulated data to "Does not meet the credit policy. Status:Fully Paid." The training data is mixed with manipulated data and clean data to fit both the backdoor task and the main task. We compare the performance of aggregation algorithms under two backdoor attack scenarios, which Under review as a conference paper at ICLR 2020 are called the naive approach and the model replacement in  Bagdasaryan et al. (2018) . For the naive approach, an attacker poisons its local model and submits the malicious update in every round. For the model replacement, an attacker only poisons in one round to embed some patterns into the global model, so the attacker needs to scale up its malicious update before submission. In our experiment, the malicious participant attacks in round 6 and scales up its update by 100. We run 200 rounds for MNIST and 100 rounds for CIFAR and LOAN.  Table 3  summarizes the results of backdoor attacks. Our method is the highest in terms of accuracy on MNIST under both backdoor attack scenarios. Moreover, on the more challenging CIFAR-10 dataset, our algorithm is the only one that can defend the naive approach backdoor attack. In  Figure 4 , we plot the attack success rates over time under different aggregation algorithms except for FoolsGold because it completely fails in both main and backdoor tasks. Intuitively, backdoor attacks can easily succeed under FedAvg, and other baselines slow down the process but still reach high attack success rate into over 70% within 100 rounds. Our algorithm effectively defends the attack and remains stable with 9.65% attack success rate when being attacked continuously for 100 rounds. On the LOAN dataset, our method achieves higher accuracy 94.50% (FedAvg 93.65%) and 0.00% attack success rate (FedAvg 99.71%) under the naive approach attack after 100 rounds. Similarly, our method has 95.06% accuracy (FedAvg 94.11%) and 0.00% attack success rate (FedAvg 98.96%) under model replacement attacks after 100 rounds. Other baselines also have 0.00% attack success rate in two attacking scenarios except FoolsGold, whose attack success rate is 99.96% under the naive approach attack. It is also interesting to notice that all the baselines on the CIFAR-10 dataset for the naive backdoor attack approach while they perform fairly well on the MNIST dataset. This is because the model used for CIFAR-10 experiments, ResNet-18, is more complicated and has more parameters (1000Ã) than the simple CNN used for the MNIST dataset. To explore the effect of complex models, we trained a ResNet-18 model on the MNIST dataset and performed the same backdoor attack on it. The results are shown in Figure 7 in the appendix. All the baselines fail to defend but our algorithm remains stable, which is similar with the result of CIFAR-10. Besides, most baseline methods (FedAvg, Median, Trimmed Mean, and Repeated Median) are coordinate-wise operations, while our algorithm accumulates a weight for each model rather than each parameter. We believe the model- wise reweighting scheme preserves the structure of the parameters and can perceive higher-level information.

Section Title: RESULTS ON GAUSSIAN NOISE ATTACKS
  RESULTS ON GAUSSIAN NOISE ATTACKS In Gaussian noise attacks, we try to simulate real-world model corruption with 10 corrupted users at different scales of noise. Gaussian noise attacks are performed by multiplying each parameter by a scale sampled from a Gaussian distribution with a mean of 1. In the experiment on MNIST, we have 100 participants where some may be attackers. Each participant has 300 images from a random digit and other 300 images from another random digit.  Table 4  summarizes the results when is sampled from a Gaussian distribution with different standard deviations and there are 10 attackers. Our aggregation algorithm outperforms other approaches under Gaussian noise at multiple scales.

Section Title: ALTERNATIVE SCHEMES
  ALTERNATIVE SCHEMES Though our algorithm is inspired by the reweighting scheme from IRLS ( Rand R, 1997 ), some alternative linear estimators and weighting schemes can replace the original ones. In this section, we replaced the repeated median estimator in our algorithm with Theil-Sen estimator or median estimator and replaced the weighting scheme with a zero-mean Gaussian function where smaller residual means larger weight. We tuned the Î» (in the Gaussian weighting it is the variance Ï), and the results are shown in table 7 in the appendix. The experiments are performed on the MNIST dataset with label-flipping attacks. Median and Theil-Sen estimators achieve similar results as the Repeated Median estimator, but they can be attacked in a few cases. For example, Î» = 5 and Î´ = 0.01 for Median estimator and Î» = 5 and Î´ = 0.05 for Theil-Sen estimator. Theil-Sen estimator, especially, only has a breakdown point of 29.3%, meaning that it will more easily be broken when the number of attackers increases. The alternative Gaussian weighting scheme is fairly robust against label-flipping attacks. This demonstrates that the robustness comes from our framework rather than intricate hyperparameter tuning.

Section Title: CONCLUSION
  CONCLUSION Federated learning utilizes private data on multiple devices to train a global model, but the simple aggregation algorithm in federated learning is vulnerable to malicious attacks. To tackle this problem, we present a novel aggregation algorithm with residual reweighting. Our experiments on computer vision, natural language processing, and financial data show that our approach is robust to label- flipping, Gaussian noise, and backdoor attacks while prior aggregation methods are not. Our algorithm is easy to implement and readily incorporated into existing federated learning frameworks. We hope our proposed aggregation algorithm can make federated learning more practical and robust in the future. Our source code will be made public. Under review as a conference paper at ICLR 2020

```
