<article article-type="research-article"><front><article-meta><title-group><article-title>Published as a conference paper at ICLR 2020 PROBABILISTIC CONNECTION IMPORTANCE INFER- ENCE AND LOSSLESS COMPRESSION OF DEEP NEU- RAL NETWORKS</article-title></title-group><contrib-group content-type="author"><contrib contrib-type="person"><name><surname>Xing</surname><given-names>Xin</given-names></name></contrib><contrib contrib-type="person"><name><surname>Sha</surname><given-names>Long</given-names></name></contrib><contrib contrib-type="person"><name><surname>Hong</surname><given-names>Pengyu</given-names></name></contrib><contrib contrib-type="person"><name><surname>Shang</surname><given-names>Zuofeng</given-names></name></contrib><contrib contrib-type="person"><name><surname>Liu</surname><given-names>Jun S</given-names></name></contrib><contrib contrib-type="person"><xref ref-type="aff" rid="aff0" /></contrib></contrib-group><aff id="aff0"><institution content-type="orgname">Harvard University</institution></aff><aff id="aff1"><institution content-type="orgname">Brandeis University</institution></aff><aff id="aff2"><institution content-type="orgname">Brandeis University</institution></aff><aff id="aff3"><institution content-type="orgname">New Jersey Institute of Technology</institution></aff><abstract><p>Deep neural networks (DNNs) can be huge in size, requiring a considerable amount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels. We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN's outputs using a nonparemetric scoring test and keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Deep neural networks (DNNs) have many successful applications ranging from computer vision, natural language processing to computational biology. However, large DNNs usually require sig- nificant memory and storage overhead and sometimes a large network bandwidth, which hinges their usages on mobile devices. Running large-size neural networks also consumes a considerable amount of energy, making their deployments on battery-constrained devices difficult. Furthermore, the over-parameterization issue in DNN architectures can impair its performances. Recent works (Han et al. (2015); Ullrich et al. (2017); Louizos et al. (2017) and references therein) show ways to reduce the network complexity by using proper regularization or network pruning to significantly reduce the number of parameters. One way to convert a dense DNN into a sparse one is by applying L 0 /L 1 regularization on the model parameters. The L 1 penalty is computationally efficient, but it also introduces more bias on the large weights and may lead to significant degradation in model performances (Han et al., 2015). In contrast, L 0 regularization shows better performance, but incurs much higher computational complexity due to its combinatorial nature. Pruning, as shown in Han et al. (2015) and Tartaglione et al. (2018), is another promising strategy to sparsify DNNs by only keeping network connections more relevant to the final output. The importance of a network connection (i.e., the connection between two network nodes) can be approximated by the magnitudes or gradients of its weights. However, such an approximation may not be accurate since it does not consider the highly nonlinear relationships between network connections and the final output induced by the multi-layer convolutions of DNNs.</p><p>Some available network compression methods improve the computational performance with moderate to high losses in accuracy, which can be highly undesirable in many critical missions (such as autonomous driving). In order to achieve lossless compression, we need to correctly decipher the relationships between the network connections and the final output. This is a challenging task because the structural nature of DNNs makes the dependence between a network connection and the network output highly nonlinear. In this paper, we propose a probabilistic connection importance inference (PCII) method for testing whether a connection in a DNN is relevant to the DNN output. Specifically, we introduce a new technique called probabilistic tensor product decomposition to decompose the association of two connected network nodes into two components: one related to the DNN output and the other independent of the DNN output. If the strength of the first component is high, we keep the Published as a conference paper at ICLR 2020 network connection. Otherwise, we delete it. The inference is carried out by a nonparametric score test, which is based on modeling the log-transformed joint density of the connected nodes and the final output in a tensor product reproducing kernel Hilbert space (RKHS). We further derive the asymptotic distribution of the proposed test statistics, thus avoiding the computationally intensive resampling step encountered in most nonparametric inference settings. We implemented the method and tested it on image classification tasks, in which our approach achieved the highest lossless compression rates. Section 2 reviews relevant literature; Section 3 introduces the PCII method and algorithm; Section 4 establishes theoretical properties for using the method to infer dependence between a network connection and the DNN output; and Section 5 shows the experimental results and concludes with a short discussion.</p></sec><sec><title>RELATED WORKS</title><p>Zhu &amp; Gupta (2017) found that a DNN can be greatly sparsified with minimal loss in accuracy. One strategy for sparsifying DNNs is to shrink small weights to zero. Along this line of thinking, Louizos et al. (2017) introduced a smoothed version of L 0 regularization aiming to be both computationally feasible and beneficial to generalizability. There are also some regularization methods based on Bayesian formulations. Ullrich et al. (2017) proposed to add a Gaussian mixture prior to the weights. The sparsity is achieved by concentrating weights to cluster centers. Molchanov et al. (2017) proposed variational dropout, which learns individual dropout rate based on the empirical Bayes principle. Also, as shown in Han et al. (2015), the performance of pruning and retraining is related to choice the correct regularization, such as L 1 or L 2 regularization.</p><p>PCII offers a means to prune a DNN by keeping network connections that are the most relevant to the DNN output. This idea goes back to the optimal brain damage work by LeCun et al. (1990). It shows that among the many parameters in the network, many are redundant and do not contribute significantly to the output. Later, Hassibi et al. (1993) proposed the optimal brain surgeon method, which leverages a second-order Taylor expansion to select parameters for deletion. Most recently, Han et al. (2015) proposed a three-step pruning method. Tartaglione et al. (2018) proposed to prune a DNN based on the sensitivity score. There are also several approaches targeting at sparsifying convolution layers. For example, Anwar et al. (2017) proposed to prune feature maps and kernels in convolution layers.</p><p>Comparing with existing pruning methods based on the magnitude or gradient of weights, our approach directly models the relationship between a network connection and the network output in a nonparametric way. In addition, our inference is based on a statistical hypothesis testing, which outputs p-values to quantify the dependence strength of each network connection to the DNN output. The use of p-values allows network connections to be treated in a unified way and alleviates the need of ad hoc weights normalization required in some existing approaches.</p></sec><sec><title>PROBABILISTIC CONNECTION IMPORTANCE INFERENCE</title><p>In this section, we establish a general procedure for building the probabilistic connection structure, in which the connections are inferred by the nonparametric score test in tensor product RKHS. As shown in our experiments (see Section 5), our technique not only sparsifies the network but improves its generalization ability.</p></sec><sec><title>PCII ON FULLY CONNECTED LAYERS</title><p>Without loss of generality, we let a feed-forward acyclic DNN (<xref ref-type="fig" rid="fig_0">Figure 1</xref>) be composed of T layers with X t being the input of the t-th network layer, where t = 0, 1, . . . , T . Let t = 0 and t = T indicate the input and output layers, respectively, and let 0 &lt; t &lt; T index the hidden layers. The collection of all the nodes is denoted as G and the collection of all network connections is denoted as E. We use a pair of nodes to denote a connection in E. For example, (X t,1 , X t+1,1 ) denotes an edge from the 1 st node in the t-th layer to the 1 st node in the (t + 1)-th layer. For simplicity, we use Y to denote the output layer, who takes on categorical values for a classification task, and takes on continuous values for regression. The output of the (t + 1)-th layer can be described as</p><p>where m t denotes the number of nodes in the t-th layer, and g t+1 is the activation function. It should be noted that the magnitude of weights is not necessarily the best indication of the impact of the corresponding network connection. A more preferable way is to directly model the relationship between a network connection and the final output of a DNN. For simplicity, we use (&#945;, &#946;) to denote an arbitrary network connection. Due to the high non-linearity of a DNN, we use the nonparametric function to model the relationship among &#945;, &#946; and Y .</p><p>Let us denote the joint density of &#945;, &#946; and Y as f (&#945;, &#946;, y) and the log-transferred density as &#951;(&#945;, &#946;, y). We assume that &#951;(&#945;, &#946;, y) can be deposed as</p><p>where &#951; &#945; , &#951; &#946; , and &#951; y are marginal effects, &#951; &#945;,y , &#951; &#946;,y , and &#951; &#945;,&#946; are the two-way interaction terms, &#951; &#945;,&#946;,y is the three-way interaction term. Here, we interpret the connection as the interaction of &#945; and &#946;, i.e. &#951; &#945;,&#946; + &#951; &#945;,&#946;,y . Specifically, &#951; &#945;,&#946; is the interaction effect of &#945; and &#946; without the impact of y, and &#951; &#945;,&#946;,y characterizes the interaction of &#945;, &#946; impacted by y. Our aim is to measure the significance of the connection by how much it is related to Y . To model this problem mathematically, we measure the association between the connection and Y by the significance of the three-way interaction &#951; &#945;,&#946;,y . Therefore, inferring whether the connection is related to the final output Y or not is equivalent to testing whether &#951; &#945;,&#946;,y = 0 or not. As shown in <xref ref-type="fig" rid="fig_0">Figure 1</xref>, the connection is important for the network model if and only if the three-way interaction &#951; &#945;,&#946;,y = 0. We propose a score test to qualify the significance of this term. The detailed construction of the statistical test is explained in Section 4.2.</p></sec><sec><title>PCII ON CONVOLUTIONAL LAYERS</title><p>For different activation functions and type of layers, we have modifications to adopt the specific structure. Here, we generalize the proposed PCII test to handle convolutional layers, which are critical in many modern neural network architectures such as VGG16 Simonyan &amp; Zisserman (2014). As demonstrated in Li et al. (2016), sparsifying the filter has little effect on reducing the computation cost. Nevertheless, reducing the volume of filters can greatly increase computing efficiency. For example, the volume of a filter is 3 &#215; 3 &#215; 4. There are four 3 &#215; 3 slices. PCII can be generalized to infer the importance of the slices, which can be treated as the connection between one channel in the current layer and one channel in the next layer. Convolutional filters can be applied to transform channels in the t-th layer to channels in the (t + 1)-th layer. We denote the j-th channel in the t-th layer as X t,j for j = 1, . . . , m t , where m t is the number of channels in t-th layer and t = 0, . . . , T . Each filter connects all channels in the t-th layer to one channel in the t + 1-th layer. Let h i,j t,t+1 denote a convolution operator that connects two channels X t,i and X t+1,j . Then, the filter connected to X t+1,j is {h i,j t,t+1 } mt i=1 where we denote each h i,j t,t+1 as a slice of the filter. For example, when we choose a 3 &#215; 3 &#215; 4 filter and set stride as one, this operator is the filter convolved (slided) across the width and height of the input {X t,i } mt i=1 and offset with the bias. For one slice in the t + 1-th layer, we can write its connection with the slices in the previous layer as X t+1,j = g t+1 ( mt i=1 h i,j t,t+1 (X t,i ) + b j t,t+1 ) where g t+1 is an activation function and b j t,t+1 is the bias for the j-th filter. As illustrated in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, the red arrow from X t,1 to X t+1,mt+1 represents h 1,mt+1 t,t+1 , which is one slice of the filter {h i,mt+1 t,t+1 } mt i=1 connecting the channels in the t-th layer to channel X t+1,mt+1 in the t + 1-th layer. For simplicity, we denote &#945; as one channel in the current layer and &#946; as one channel in the next layer. Since the relationship among the triplet (&#945;, &#946;, Y ) is highly nonlinear, we model its log-transformed joint density as a nonparametric function &#951;(&#945;, &#946;, y). Similar to the fully connected layers, we assume that &#951;(&#945;, &#946;, y) has a decomposition as in (1). As shown in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, the connection between &#945; and &#946; is decomposed to two parts: one unrelated to the output Y , &#951; &#945;,&#946; , and another related to the output Y , &#951; &#945;,&#946;,y . We aim to select connections that have greater contributions to Y , which is mathematically translated into the task of assessing the significance of the three-way interaction term &#951; &#945;,&#946;,y against the null hypothesis &#951; &#945;,&#946;,y = 0.</p></sec><sec><title>ALGORITHM</title><p>In real applications, both fully connected layers and convolutional layers are used in a complex neural network architecture. We integrate the PCII procedure described in the previous two subsections to simultaneously infer the connections in both fully connected and convolutional layers, as summarized in Algorithm 1. We use p (t) ij to denote the p-value for testing the i-th node in t-th layer and j-th node in (t + 1)-th layer for the fully connected layers. For convolutional layers, p (t) ij denotes the p-value for inferring the importance of the filter connecting the i-th slice in the t-th layer and the j-th slice in Published as a conference paper at ICLR 2020 the t + 1-th layer. The calculation of the p-values are given in Section 4 based on our proposed Score test. Step 3: Rank all network connections by their test statistics (or p-values), and select a threshold &#961; f for deleting network connections in fully connected layers and &#961; c in convolutional layers to achieve the desired model compression rate (Strategies for choosing &#961; f and &#961; c are given in Section 5).</p><p>Step 4: Set the same initial value for non-zero weights and filters. Retrain the sparsified DNN.</p></sec><sec><title>SCORE TEST AND THEORETICAL PROPERTIES</title></sec><sec><title>BACKGROUND ON TENSOR PRODUCT RKHS</title><p>Consider two random variables &#945; and &#946; for fully connected layers or two random vectors &#945; and &#946; for convolutional layers. Let Y be a random variable as the final output. The domains for &#945;, &#946; and Y are A, B, and Y, respectively. Here, we assume that the log-transformed joint density function &#951; belongs to a tensor product RKHS H = H &#945; &#8855; H &#946; &#8855; H Y where &#8855; denotes the tensor product of two linear space.</p><p>For marginal RKHS, H l with an inner product &#183;, &#183; H l for l = &#945;, &#946;, Y , there exists a symmetric and square integrable function K l such that f, K l (x, &#183;) H l = f (x), for all f &#8712; H l (2) where K l is called the reproducing kernel of H l for l = &#945;, &#946;, Y . By Mercer's theorem, any continuous kernel has the following decomposition K(x, y) = &#8734; &#957;=0 &#181; &#957; &#966; &#957; (x)&#966; &#957; (y) (3) where the &#181; &#957; 's are non-negative descending eigenvalues and the &#966; &#957; 's are eigen-functions. For the discrete domain {1, . . . , a}, we define the kernel as K(x, y) = 1{x = y} for x, y &#8712; {1, . . . , a}. For a continuous domain, there are different choice of kernels such as Gaussian kernels and Sobolev kernels. Note that the eigenvalues for different kernels on continuous domain have different decay rate. For example, the eigenvalues of the Gaussian kernel have the exponential decay rate, i.e., there exists some c &gt; 0 such that &#181; &#957; exp(&#8722;c&#957;) (Sollich &amp; Williams, 2004); and the eigenvalues of the m-th Sobolev kernels have the polynomial decay rate, i.e., &#181; &#957; &#957; &#8722;2m (Gu, 2013).</p></sec><sec><title>PROBABILISTIC DECOMPOSITION OF TENSOR PRODUCT RKHS</title><p>Next we propose the probabilistic tensor sum decomposition for each marginal RKHS, H l , for l = &#945;, &#946;, Y . We first use Euclidean space as a simple example to illustrate the basic idea of tensor sum decomposition. Recall that the tensor sum decomposition is often called ANOVA decomposition in linear model. For example, for the d-dimensional Euclidean space, we denote f as a vector and let f (x) be the x-th entry of the vector for x = 1, . . . , d. We denote A as an average operator defined as Af (x) = 1 d 1, f . The tensor sum decomposition of the Euclidean space R d is</p><p>Published as a conference paper at ICLR 2020 where the first space is called the grand mean and the second space is called the main effect. Then, we construct the kernel for R d 0 and R d 1 in the following lemma.</p><p>Lemma 1. For a RKHS space H, there corresponds a unique reproducing kernel K, which is non-negative definite. Based on the tensor sum decomposition H = H 0 &#8853; H 1 , where H 0 = {1/d1} and H 1 = {g &#8712; H : E X (g(X)) = 0}, we have the kernel for H 0 as k 0 (x, y) = 1/d (5) and kernel for H 1 as k 1 (x, y) = 1 {x=y} &#8722; 1/d where 1 denotes the indicator function.</p><p>However, in RKHS with infinite dimension, the grand mean is not a single vector. Here, we set the average operator A as A := f &#8594; E x f (x) = E x k x , f H = Ek x , f H where k is the kernel function in H and the first equality is due to the reproducing property. E x k x plays the same role as 1 d 1 in Euclidean space. Then, we have the tensor sum decomposition in a functional space defined as</p><p>Following the same fashion, we call H 0 as the grand mean space and H 1 as the main effect space. Notice that E x k x is also known as the kernel mean embedding which is well established in the statistics literature Berlinet &amp; Thomas-Agnan (2011). Then we introduce the following lemma to construct the kernel function for H 0 and H 1 .</p><p>Lemma 2. For RKHS space H, there corresponds an unique reproducing kernel K, which is non- negative definite. Based on the tensor sum decomposition H = H 0 &#8853; H 1 where H 0 = {E x k x } and H 1 = {g &#8712; H : E x (g(x)) = 0}, we have the kernel for H 0 as</p><p>In neural networks, A, B are in a continuous domain. The final output Y can be either in con- tinuous domain or discrete domain, which depends on the tasks. Here, we construct the tensor sum decomposition for discrete domain and continuous domain based on Lemma 1 and Lemma 2 respectively. Specifically, we have spaces H &#945; , H &#946; and H Y decomposed as tensor sums of subspaces H &#945; = H &#945; 0 &#8853; H &#945; 1 , H &#946; = H &#946; 0 &#8853; H &#946; 1 , H Y = H Y 0 &#8853; H Y 1 . Following Gu (2013), we apply the distributive law and have the decomposition of H as H =(H &#945; 0 &#8853; H &#945; 1 ) &#8855; (H &#946; 0 &#8853; H &#946; 1 ) &#8855; (H Y 0 &#8853; H Y 1 ) &#8801;H 000 &#8853; H 100 &#8853; H 010 &#8853; H 001 &#8853; H 110 &#8853; H 101 &#8853; H 011 &#8853; H 111 . (8) where H ijk = H &#945; i &#8853; H &#946; j &#8853; H Y k . Lemma 3. Suppose K 1 is the reproducing kernel of H 1 on X 1 , and K 2 is the reproducing kernel of H 2 on X 2 . Then the reproducing kernels of</p><p>Lemmas 3 can be easily proved based on Theorems 2.6 in Gu (2013). Lemma 3 implies that the reproducing kernels of the tensor product space is the product of the reproducing kernels. Based on these three lemmas, we can construct kernel for each subspace defined in (8).</p></sec><sec><title>SCORE TEST</title><p>Based on (8), the log-transformed density function &#951; &#8712; H can be correspondingly decomposed as (1). Thus, &#951; &#945;,&#946;,Y = 0 if and only if &#951; * &#8712; H 0 := H 000 &#8853; H 100 &#8853; H 010 &#8853; H 001 &#8853; H 110 &#8853; H 101 &#8853; H 011 where &#951; * is the underlying truth. Hence, we focus on the following hypothesis testing problem:</p><p>Published as a conference paper at ICLR 2020 where H\H 0 denotes set difference of H and H 0 . We next propose a likelihood-ratio based procedure to test (9). Suppose that t = (&#945;, &#946;, y) and t i = (&#945; i , &#946; i , y i ), i = 1, . . . , n, are iid observations generated from T = (A, B, Y). Let LR n be the likelihood ratio functional defined as</p><p>where K is the kernel for H and K 0 is the kernel for H 0 .</p><p>Then we calculate the Fr&#233;chet derivative of the likelihood ratio functional as</p><p>where K 1 is the kernel for H 111 . We define our test statistics as the squared norm of the score function of the likelihood ratio functional as</p><p>By the reproducing property, we can expand the left hand side of (13) as</p><p>We observe an interesting phenomenon that S 2 n has a similar expression with the MMD (Gretton et al., 2012) when Y is a binary variable. When Y &#8712; {0, 1}, the kernel on</p><p>If we replace K &#945;,&#946; 11 with K &#945;,&#946; , i.e., the kernel on the H &#945; &#8855; H &#946; , the right hand side of (15) is the MMD measuring the distance between the joint distribution of &#945; and &#946; in the group with y = 0 and the joint distribution of &#945; and &#946; in the group with y = 1.</p><p>Since we want to infer the importance of the connection between the &#945; and &#946;, we are only interested in comparing whether the interaction effect between &#945; and &#946; changes or not in the two groups. The main effects of &#945; and &#946; only contribute to the importance of the nodes or slices and are not relevant to the connection between &#945; and &#946;.</p></sec><sec><title>CALCULATION OF THE TEST STATISTICS</title><p>We introduce a matrix form of the squared norm of score function for manifesting the computation process. In (14), S 2 n is determined by the kernel on th H &#945; 1 &#8855; H &#946; 1 &#8855; H Y 1 . By Lemma 1 and Lemma 2, we replace the expectation by the sample average and get the kernel function for H l l as</p><p>Published as a conference paper at ICLR 2020 where k l is the kernel function for H l for l = &#945;, &#946;, Y . Some popular choices of the kernel functions are Gaussian kernel, Laplace kernel and polynomial kernel. Let K l be the empirical kernel matrix. We can rewrite (14) as S 2 n = 1 n 2 [(HK &#945; H) &#8226; (HK &#946; H) &#8226; (HK Y H)] ++ (17) where H = I n &#8722; 1 n 11 T , I n is a n &#215; n identity matrix and 1 n is a n &#215; 1 vector of ones, and [A] ++ = n i=1 n j=1 A ij . This test statistics is also related to three-way Lancaster's interaction measure (Lancaster, 2002).</p><p>Fortunately, the computation of (17) only involves the matrix multiplication, the computational procedure can be executed in parallel on GPU. In our experiment, the highly parallelized matrix operation tremendously alleviated the computing time: essentially reducing from quadratic to almost linear. In addition, we can reduce the sample size by sub-sampling r &lt;&lt; n data points. Sub-sampling the dataset is a popular technique to reduce computational burden and can efficiently approximate the full data likelihood in a regression setting (Ma et al., 2015; Drineas et al., 2011). However, for very large data set, the sub-sampling is not efficient. We consider a mini-batch algorithm to calculate the score in each batch and used the averaged score as the final test statistics. This is also related to the divide-and-conquer method which is widely used in kernel-based learning (Zhang et al., 2013; Shang &amp; Cheng, 2017; Liu et al., 2018).</p></sec><sec><title>ASYMPTOTIC DISTRIBUTION</title><p>The asymptotic distribution of S 2 n depends on the decay rate of the kernel of the product of RKHS. Sup- pose that {&#181; &#945; &#957; , &#966; &#945; &#957; } &#8734; &#957;=1 is a series of eigenvalue and eigenfunction pairs for H &#945; 1 , {&#181; &#946; &#957; , &#966; &#946; &#957; } &#8734; &#957;=1 is a sequence of basis for H &#946; 1 . If Y is continuous, we suppose that H Y has the eigensystem, {&#181; Y &#957; , &#966; &#945; &#957; } &#8734; &#957;=1 . If Y is categorical, we suppose that H Y has the eigensystem, {&#181;</p><p>Then we have the eigenvalue eigenfuntion pair for the tensor product space H &#945; 1 &#8855; H &#946; 1 &#8855; H Y 1 as {&#181; &#945; &#957;&#945; &#181; &#946; &#957; &#946; &#181; Y &#957; Y , &#966; &#945; &#957;&#945; &#966; &#946; &#957; &#946; &#966; Y &#957; Y } where &#957; &#945; = 1, . . . , &#8734;, &#957; &#946; = 1, . . . , &#8734;, &#957; Y = 1, . . . , &#8734; (Y is continuous), and &#957; Y = 1, . . . , a &#8722; 1 (Y is categorical with a categories). For simplicity, we order the pairs in the decreasing order of &#181; &#961; , &#961; = 1, . . . , &#8734;. The null hypothesis could be interpreted as factorization hypothesis, i.e., (X, Y ) &#8869; Z &#8744; (X, Z) &#8869; Y &#8744; (Y, Z) &#8869; X or X, Y, Z are mutually independent. Theorem 1. Suppose the kernel on H 111 is square integrable. If Y is continuous variable, then under H 0 , we have nS 2 n d &#8722; &#8594; &#8734; &#961;=1 &#181; &#961; 2 &#961; (18) where &#961; are i.i.d. standard Gaussian random variables.</p><p>The proof of this theorem is shown in the Supplementary Materials A.1. The asymptotic distribution of nS 2 n only depends on the eigenvalues of the kernel. Theorem 1 is related to Wilks' phenomenon demonstrated in the classic nonparametric/semiparametric regression framework (Fan et al., 2001; Shang &amp; Cheng, 2013), i.e., the asymptotic distribution is independent of the nuisance parameters. In practice, we fix the same kernel for the fully connected layers and the same kernel for the convolutional layers. Thus, it provides a unified importance measure for all connections in the same type of layer, which avoids the scaling problem faced by those pruning methods that use magnitudes of weights as an importance measure. In addition, we can use the value of the test statistics as an importance measure for pruning and bypass the effort of calculating the p-values since the order is the same according to either of these two measures.</p></sec><sec><title>RESULTS</title><p>We conducted experiments to test out the PCII method in two supervised image classification tasks: MNIST and CIFAR10. We used TensorFlow to train DNNs. The back-end of PCII was implemented in Fortran and R language. Programming interfaces were implemented to connect the back-end calculations to TensorFlow. The experiments were run on a computer with one Nvidia Titan V GPU and 48 CPU cores.</p></sec><sec><title>Published as a conference paper at ICLR 2020</title><p>PCII offers a convenient way to adjust compression rate by changing the p-value threshold &#961;. For other compression methods in consideration, the compression rate is usually controlled by some hyper-parameters, which need to be tuned via an ad hoc trial and error strategy. Two types of comparisons were carried out. In the first type, we adjust the compression rate of a method while requiring its compressed DNN to achieve the test accuracy of the original uncompressed DNN. We term this the lossless compression rates (LCR). Since it is very time-consuming to obtain an exact test accuracy, we allow a 0.01% deviation from the desired test error rates. In the second type, we compared the minimum testing error (MTE) of the compressed DNNs produced by different model compression methods. MTE shows how a compression method can help increase the generalizability and robustness of a DNN. We only included the results of the tested methods that we could obtain their working codes to reproduce the results reported in their original papers. In addition, we did not include methods that are not able to achieve the LCR of the corresponding test dataset.</p></sec><sec><title>MNIST DATASET</title><p>We tried two network architectures for MNIST (60k training images and 10k test images): a multilayer perceptron (MLP) with two hidden layers of sizes 300 and 100, respectively, and a convolutional neural network LeNet-5 (LeCun et al. (1998)). We trained LeNet-300-100 for 100 epochs to achieve a test error of 1.69%. These approaches include two regularization based methods in Louizos et al. (2017), as well as several pruning based methods in Han et al. (2015), and Guo et al. (2016). The results are summarized in <xref ref-type="table" rid="tab_0">Table 1</xref>. PCII achieved the lowest MTE when the compression rate was 10x. Then, we further increased the compression rate until the error rate reached 1.70%. The results show that PCII not only compressed a medium-sized MLP better than existing methods but also was able to improve generalizability of a MLP via compression (i.e., the MTE is better than the LCR). Interestingly, our inference results can also help in interpreting the fitted neural network. For example, through inferring the importance of the connections between input layer and the the first hidden layer, we can visualize the importance of the features in the input layer. <xref ref-type="fig" rid="fig_3">Figure 3</xref> plots the p-values of the Published as a conference paper at ICLR 2020 associations between the network connections in the first layer and the final output. The heatmap shows that a banded structure repeated 28 times, in which the central region tends to have smaller p-values. The left and right margins of the heatmap show that the connections on the first and last few channels are less relevant to the final output (i.e., have large p-values). This phenomenon is observed because a written digit is usually located in the central part of a image. The LeNet-5 model (https://goo.gl/4yI3dL) is a modified version of LeCun et al. (1998). It includes two convolutional layers with 20 and 50 filters, respectively, and a fully connected layer with 500 nodes. The results are summarized in <xref ref-type="table" rid="tab_1">Table 2</xref>. PCII achieved both the lowest MTE and highest LCR, again, for this model, demonstrating the broad applicability of the PCII strategy for various neural network architectures.</p></sec><sec><title>CIFAR10 DATASET</title><p>To demonstrate the applicability of PCII to complex DNNs, we applied it to VGG16 (Zagoruyko, 2015), which was adapted for the CIFAR-10 dataset (Krizhevsky &amp; Hinton, 2009). The network consists of 13 convolutional and two fully-connected layers. The results are summarized in <xref ref-type="table" rid="tab_2">Table 3</xref>. The test error gradually decreases as the compression rate increases from 1x to 3x, and achieves the MTE at 6.01%. When the compression rate is further increased, the test error begins to increase. As the compression rate reaches 10x, the test error increases to 7.56% that is comparable to the test error of the uncompressed VGG16. For this dataset, we only include the result for PCII due to the limited resources. In fact, we could not obtain the results for other methods in comparison in three days' computing time.</p></sec><sec><title>DISCUSSION</title><p>We propose a statistically principled strategy to directly infer the importance of a connection in a neural network via PCII, a hypothesis testing framework. The proposed PCII test provides a p-value based measure on the importance of the connection through the connection's association with the final output. This type of direct quantification cannot be easily accomplished by the magnitude-based pruning method. Although the two examples are relatively small in size, they demonstrated the broad applicability of the PCII method and its improved power in network compression. Last but not least we note that the PCII testing method can be easily generalized to a broad class of connection types including the skip layer connections in RNN.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Flowchart for probabilistic connection inference. We have two nodes &#945; and &#946; from a fully connected neural network. The connection of &#945; and &#946; is expressed as &#951; &#945;,&#946; + &#951; &#945;,&#946;,y . The importance of the connection is inferred by testing whether the three way interaction &#951; &#945;,&#946;,y is zero or not.</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Flowchart for probabilistic connection inference for convolutional layers. Without loss generality, we use same notation of &#945; and &#946; to denote two channels from a convolutional neural network. The connection between &#945; and &#946; is corresponding to the convolutional operator h 1,mt+1 t,t+1 (shown as one slice with the red border). The importance of this connection is inferred by testing whether the three way interaction &#951; &#945;,&#946;,y is zero or not.</p></caption><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label /><caption><p /></caption><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Experimental results for LeNet-300-100 on MNIST dataset.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>The heatmap showing the PLR test result of the 784 &#215; 300 connections between the input layer and the first FC layer in Lenet-300-100. Each pixel represents a p-value of the corresponding pair. The brighter color representing a smaller p-value. The width and height of the heatmap correspond to the input dimension (784) and the size (300) of the first FC layer.</p></caption><graphic /><graphic /></fig><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Experimental results for LeNet-5-caffe on MNIST dataset.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_2"><label>Table 3:</label><caption><title>Table 3:</title><p>Experimental results for VGG16 on CIFAR10 dataset.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap></sec></body><back /></article>