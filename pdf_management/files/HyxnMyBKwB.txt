Title:
```
Department of Computer Science and Engineering The Chinese University of Hong Kong
```
Abstract:
```
We analyze the Gambler's problem, a simple reinforcement learning problem where the gambler has the chance to double or lose their bets until the target is reached. This is an early example introduced in the reinforcement learning text- book by Sutton & Barto (2018), where they mention an interesting pattern of the optimal value function with high-frequency components and repeating non- smooth points. It is however without further investigation. We provide the exact formula for the optimal value function for both the discrete and the continuous cases. Though simple as it might seem, the value function is pathological: fractal, self-similar, derivative taking either zero or infinity, not smooth on any interval, and not written as elementary functions. It is in fact one of the generalized Cantor functions, where it holds a complexity that has been uncharted thus far. Our anal- yses could lead insights into improving value function approximation, gradient- based algorithms, and Q-learning, in real applications and implementations.
```

Figures/Tables Captions:
```
Figure 1: The optimal state-value function of the discrete Gambler's problem.
Figure 2: The optimal state-value function of the continuous Gambler's problem.
 (s) ≤ 1 for all s, f (s) is continuous on s = 0. (X) Respectively, the unbounded version (Y) of the problem leads to the solutions of the Bellman equa- tion. 0 ≤ γ ≤ 1, p > 0.5. (Y) The results extend for p = 0.5 in general, except an extreme corner case of γ = 1, p = 0.5, where the monotonicity in Lemma 3 will not apply. This case (Z) involves arguments over measurability and the belief of Axiom of Choice, which we will discuss at the end of Section A. γ = 1, p = 0.5, f (s) is unbounded. (Z) We are mostly interested in two settings: the first setting (ABX) and its solution Theorem 12, discuss a set of necessary conditions of f (s) being the optimal value function of the Gambler's problem. As we show later the solution of (ABX) is unique, this solution must be the optimal value function. The second setting (ABY) and its solutions in Proposition 21 and Theorem 22 discuss all the functions that satisfy the Bellman equation. These functions are the optimal points that value iteration and Q-learning algorithms may converge to. (ABZ) is interestingly connected some foundations of mathematics like the belief of axioms, and is discussed in Theorem 27.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION We analytically investigate a deceptively simple problem, the Gambler's problem, introduced in the reinforcement learning textbook by  Sutton & Barto (2018) , on Example 4.3, Chapter 4, page 84. The problem setting is natural and simple enough that little discussion was given in the book apart from an algorithmic solution by value iteration. A close inspection will however show that the problem, as a representative of the entire family of Markov decision processes (MDP), involves a level of complexity and curiosity uncharted in years of reinforcement learning research. The problem discusses a gambler's casino game, where they places multiple rounds of betting. The gambler gains the bet amount if they win a round or loses the bet if they lose the round. The probability of losing each round is p ≥ 0.5, independently. The game ends when the gambler's capital reaches either their goal of N or 0. In each round, the gambler must decide what portion of the capital to stake. In the discrete setting this bet amount must be an integer, but it can be a real number in the continuous setting. To formulate it as an MDP, we denote state s be the current capital and action a the bet amount. The reward is +1 when the state reaches s = N , and zero otherwise. Our goal is to solve the optimal value function of the problem. We first give the solution to the discrete Gambler's problem. Denote N as the target capital, n as the current capital (which is the state in the discrete setting), p > 0.5 as the probability of losing a bet, and γ as the discount factor. The special case of N = 100, γ = 1 corresponds to the original setting in Sutton and Barto's book. The above statement amounts the discrete problem to the continuous problem by a uniform dis- cretization. The rest of the discussion will be on the more general continuous setting. In the setting, the target capital is 1, the state space is [0, 1], and the action space is 0 < a ≤ min{s, 1 − s} at state s, meaning that the bet can be any fraction of the current capital as long as the capital after winning does not exceed 1. We state the optimal value function below and intuitive description of the value function later in this section. Though the description of the Gambler's problem seems natural and simple, Theorem 12 shows that its simpleness is deceptive. The optimal value function is fractal and self-similar, and non-rectifiable (see Corollary 14 and Lemma 8). It is thus not smooth on any interval, which can be unexpected when a significant line of reinforcement learning studies is based on function approximation like discretization and neural networks. The value function (1) can neither be simplified into a formula of elementary functions, which introduces difficulties in understanding it. The function is monoton- ically increasing with v(0) = 0 and v(1) = 1, but its derivative is 0 almost everywhere, which is counterintuitive. This is known as singularity, a famous pathological property of functions. v(s) is continuous almost everywhere but not absolutely continuous. Also when γ is strictly smaller than 1, it is discontinuous on a dense and compact set of infinitely many points. These properties indicate that assumptions like smoothness, continuity, and approximability are not satisfied in this problem. In general, it is reasonable to doubt if these assumptions can be imposed in reinforcement learning. To better understand the pathology of v(s), we analogize it to the Cantor function, which is well known in analysis as a counterexample of many seemingly true statements ( Dovgoshey et al., 2006 ). In fact, v(s) is a generalized Cantor function, where the above descriptions are true for both v(s) and the Cantor function. Intuitive description of v(s). All the statements above require the definition of v(s). In fact, in this paper, v(s) is important enough such that its definition will not change with the context. The function cannot be written as a combination of elementary functions. Nevertheless, we give an intuitive way to understand the function for the original, undiscounted problem. The function can be regarded as generated by the following iterative process. First we fix v(0) = 0 and v(1) = 1, and compute Here, v( 1 2 ) is the weighted average of the two "neighbors" v(0) and v(1) that have been already evaluated. Further, the same operation applies to v( 1 4 ) and v( 3 4 ), where v( 1 4 ) = pv(0) + (1 − p)v( 1 2 ) = (1 − p) 2 and v( 3 4 ) = pv( 1 2 ) + (1 − p)v(1) = (1 − p) + p(1 − p), and so forth to v( 1 8 ), v( 3 8 ), etc. This process evaluates v(s) on the dense and compact set ≥1 G of the dyadic rationals, where G = {k2 − | k ∈ {1, . . . , 2 − 1}}. With the fact that v(s) is monotonically strictly increasing, this dyadic rationals determines the function v(s) uniquely. This iterative process can also be explained from the analytical formula of v(s). Starting with the first bit, a bit of 0 will not change the value, while a bit of 1 will add (1−p) i−1 j=1 ((1−p)+(2p−1)b j ) to the value. This term can also be written as (1 − p)((1 − p) #0 bits · p #1 bits ), where the number of bits is counted over all previous bits. The value (1 − p) #0 bits · p #1 bits decides the gap between two neighbor existing points in the above process, when we insert a new point in the middle. This insertion corresponds to the iteration on G over .

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 We provide high resolution plots of z(n), N = 100 and v(s) in  Figure 1  and  Figure 2 , respectively. The non-smoothness and the self-similar fractal patterns can be clearly observed from the figures. Also, v(s) is continuous when γ = 1 while v(s) is not continuous on infinitely many points when γ < 1. In fact, when γ < 1, the function is discontinuous on the dyadic rationals ≥1 G while continuous on its complement, as we will rigorously show later.

Section Title: Self-similarity
  Self-similarity The function v(s) on [s,s + 2 − ] for anys = 0.b 1 b 2 . . . b (2) , ≥ 1 is self-similar to the function itself on [0, 1]. Let s = 0.b 1 b 2 . . . b . . . (2) ∈ [s,s + 2 − ], this can be observed by The self-similarity can be compared with the Cantor function ( Dovgoshey et al., 2006 ;  Mandelbrot, 1985 ), which uses the ternary of s instead in the formula. The Cantor function is self-similar to itself on [s,s + 3 − ], whens = 0.b 1 b 2 . . . b (3) and b = 1. Both v(s) and the Cantor function can be uniquely described by their self-similarity, the monotonicity, and the boundary conditions.

Section Title: Optimal policies
  Optimal policies It is immediate by Theorem 12 and Lemma 8 that π(s) = min{s, 1 − s} is one of the (Blackwell) optimal policies. Here, Blackwell optimality is defined as the uniform optimality under any 0 ≤ γ ≤ 1. This policy agrees with the intuition that under a game that is in favor of the casino (p > 0.5), the gambler desires to bet the maximum to finish the game in as little cumulative bet as possible. In fact, the probability of reaching the target is the expected amount of capital by the end of the game, which is negative linear to the cumulative bet. The optimality is not unique though, for example, π( 15 32 ) = 1 32 is also optimal (for any γ). Under γ = 1 the original, undiscounted setting, small amount of bets can also be optimal. Namely, when s can be written in finite many bits s = b 1 b 2 . . . b (2) in binary (assume b = 1), π(s) = 2 −l is also an optimal policy. This policy is by repeatedly rounding the capital to carryover the bits, keeping the game to be within at most rounds of bets.

Section Title: PRELIMINARIES
  PRELIMINARIES We use the canonical formulation of the discrete-time Markov decision process (MDP), denoted as the tuple (S, A, T , r, ρ 0 , γ). That includes S the state space, A the action space, T : S × A × S → R + the transition probability function, r : S → R the reward function, ρ 0 : S → R + the initial state distribution, and γ ∈ [0, 1] the unnormalized discount factor. A deterministic policy π : S → A is a map from the state space to the action space. In this problem, T (s, a, s − a) and T (s, a, s + a) are p and 1 − p respectively for s ∈ S, a ∈ A, and T is otherwise 0. Our goal is to solve the optimal value function of the Gambler's problem. In this problem, the state- value function is the probability of the gambler eventually reaching the target capital from a state. The definition of the state-value function of an MDP with respect to state s and policy π is When π * is one of the optimal policies, f π * (s) is the optimal state-value function. Despite that there may exist more than one optimal policies, this optimal state-value function is unique ( Sutton & Barto, 2018 ;  Szepesvári, 2010 ).

Section Title: IMPLICATIONS
  IMPLICATIONS Our results indicate hardness on reinforcement learning ( Papadimitriou & Tsitsiklis, 1987 ;  Littman et al., 1995 ;  Thelen & Smith, 1998 ) and revisions of existing reinforcement learning algorithms. It is worth noting that similar patterns of fractal and self-similarity have been observed empirically, for example in  Chockalingam (2019)  for the Mountain Car problem. With these characterizations being observed in simple problems like Mountain Car and the Gambler's problem, our results are expected to be generalized to a variety of reinforcement learning settings. The first implication is naturally on function value function approximation, which is a developed topic in reinforcement learning ( Lee et al., 2008 ;  Lusena et al., 2001 ). By the fractal property of the optimal value function, that representation of such function must be inexact ( Tikhonov, 2014 ). When discretization is used for value function representation, the approximation error is at least O(1/N ), where N is the number of bins. Proposition 19. When N ∈ N + , N ≥ 4 is a power of 2, letv 1 (s) be piecewise constant on any of the intervals s ∈ (k/N , (k + 1)/N ), k = 0, . . . , N − 1, then Alternatively, when a subclass of L-Lipschitz continuous is used to represent v(s), this error is then at least (1/L) · (1 − p) 2 γ 2 (1 − γ) 2 /(4 − 4pγ) by the discontinuity of v(s) (Proposition 20). It is worth remarking that despite this specific lower bound diminishes when γ is 1, the approximation error is nonzero for an arbitrarily large L under γ = 1, as the derivative of v(s) can be infinite (Fact 16). Notably, neural networks are within this family of functions, where the Lipschitz constant L is determined by the network architecture. By the proposition, it is not possible to obtain the optimal value function when a neural network is used, albeit the universal approximation theorem ( Csáji, 2001 ;  Dovgoshey et al., 2006 ;  Levesley et al., 2007 ). The second implication is by Theorem 12 and Fact 16 that the derivative of v(s) is This imposes that the value function's derivative must not be exactly obtained, as it is 0 almost ev- erywhere, except on the dyadic rationals G , where it has a left derivative of infinity and a right derivative of 0. Algorithms relying on ∂v(s)/∂s and ∂Q(s, a)/∂a ( Lillicrap et al., 2015 ;  Gu et al., 2017 ; Heess et al., 2015;  Fairbank & Alonso, 2012 ;  Fairbank, 2008 ;  Pan et al., 2019 ;  Lim et al., 2018 ), where Q(s, a) is the action-value function ( Sutton & Barto, 2018 ), can suffer from the esti- mation error or even have unpredictable behavior. In practice, the boolean implementation of float numbers can further increase this error, as all points s implemented are in G for some . A precise evaluation requires all these derivatives to be infinity when a Leabague derivative is used (the average of left and right derivatives), which cannot be obtained a computer system. The third implication is on Q-learning ( Mnih et al., 2015 ;  Watkins & Dayan, 1992 ;  Baird, 1995 ), by Theorem 22 and its supporting lemmas. It is proved that when γ = 1, Q-learning has multiple converging points, as the Bellman equation has multiple solutions, namely v(s) and f (0) = 0, f (1) = 1, and f (s) = C for all 0 < s < 1, for some constant C ≥ 1. Therefore, even when the Q-learning algorithm converges, it may not converge to the optimal value function v(s). In fact, as the solution can be either the ground truth of the optimal value function, or a large constant function, it is easier to approximate a constant function than the optimal value function, resulting in a relatively lower Bellman error when converging to the large constant. This challenges Q-learning under γ = 1 when the return (cumulative reward) is unbiased. Though the artificial γ is originally introduced to prevent the return from diverging, it can be also necessary to prevent the algorithm from converging to a large constant in Q-learning, which is not desired.

Section Title: DISCRETE CASE
  DISCRETE CASE The analysis of the discrete case of the Gambler's problem will give an exact solution. It will also explain the reason the plot on the book has a strange pattern of repeating spurious points. The discrete case can be described by the following MDP: The state space is {0, . . . , N }; the action space at n is A(n) = {0 < a ≤ min{n, N − n}}; the transition from state n and action a is n − a and n + a with probability p and 1 − p, respectively; the reward function is r(N ) = 1 and r(n) = 0 for 0 ≤ n ≤ N − 1. The MDP terminates at n ∈ {0, N }. We use a time-discount factor of 0 ≤ γ ≤ 1, where the agent receives γ T r(N ) rewards if the agents reaches the state n = N at time T . Let z(n), n ∈ N, 0 ≤ n ≤ N , be the value function. The exact solution below of the discrete case is relying on Theorem 12, our main theorem which describes the exact solution of the continuous case. This theorem will be discussed and proved later in Section A.1. Proposition 1. Let 0 ≤ γ ≤ 1 and p > 0.5. The optimal value function z(n) is v(n/N ) in the discrete setting of the Gambler's problem, where v(·) is the optimal value function under the continuous case defined in Theorem 12.

Section Title: Proof
  Proof We then show that z(n) = v(n/N ) is the unique function that satisfies the Bellman equation. The proof is similar to the proof of Lemma 2, but the arguments will be relatively easier, as both the state space and the action space are discrete. Let f (n) also satisfy the Bellman equation. We desire to prove that f (n) is identical to z(n). Define δ = max 1≤n≤N −1 f (n) − z(n). This maximum must exists as there are finite many states. Then define the non-empty set S = {n | f (n) − z(n) = δ, 1 ≤ n ≤ N − 1}. For any n ∈ S and a ∈ argmax 1≤n≤min{n ,N −n } pγ f (n − a) + (1 − p)γ f (n + a), we have As the equality holds, by the equality of (♥) we have n − a ∈ S and n + a ∈ S. Now we specify some n 0 ∈ S and a 0 ∈ argmax 1≤n≤min{n0,N −n0} pγ f (n 0 − a) + (1 − p)γ f (n 0 + a). Then, we have n 0 − a 0 ∈ S. Denote n 1 = n 0 − a 0 and, recursively, a t ∈ argmax 1≤n≤min{nt,N −nt} pγ f (n t −a)+(1−p)γ f (n t +a) and n t+1 = n t −a t , t = 1, 2, . . . ; Since a t ≥ 1 and n t ∈ N, there must exist a T such that n T = 0. Therefore, δ = f (n T )−z(n T ) = 0. By the same argumentδ = max 1≤n≤N −1 z(n)−f (n) = 0. Therefore, z(n) and f (n) are identical, as desired. As z(n) is the unique function that satisfies the Bellman equation, it is the optimal value function of the problem. Proposition 1 indicates the discretization of the problems yields the discrete, exact evaluation of the continuous value function at 0, 1/N , . . . , 1. If we omit the learning error, the plots on the book and by the open source implementation ( Zhang, 2019 ) are the evaluation of the fractal v(s) at 0, 1/N , . . . , 1. This explains the strange appearance of the curve in the figures.

Section Title: SETTING
  SETTING We formulate the continuous Gambler's problem as a Markov decision process (MDP) with the state space S = [0, 1] and the action space A(s) = (0, min {s, 1 − s}], s ∈ (0, 1). Here s ∈ S represents the capital the gambler currently possesses and the action a ∈ A(s) denotes the amount of bet. Without loss of generality, we have assumed that the bet amount should be less or equal to 1 − s to avoid the total capital to be more than 1. The consecutive state s transits to s − a and s + a with probability p ≥ 0.5 and 1 − p respectively. The process terminates if s ∈ {0, 1} and the agent receives an episodic reward r = s at the terminal state. Let 0 ≤ γ ≤ 1 be the discount factor. Let f : [0, 1] → R be a real function. For f (s) to be the optimal value function, the Bellman equation for the non-terminal and terminal states are It can be shown (later in Lemma 2 and Lemma 3) that a function satisfying (AB) must be lower bounded by 0. A reasonable upper bound is 1, as the value function is the probability of the gambler eventually reaching the target, which must be between 0 and 1. It is also reasonable to assume the continuity of the value function at s = 0. Otherwise an arbitrary small amount of starting capital will have at least a constant probability of reaching the target 1. 1 Consequently the expectation of capital at the end of the game is greater than the starting capital, which contradicts p ≥ 0.5. The bounded version (X) of the problem leads to the optimal value function.

Section Title: ANALYSIS
  ANALYSIS The analysis section rigorously supports the statements on the Gambler's problem and its Bellman equation with proofs and discussions. It is deferred to the appendix due to the page limit.

Section Title: CONCLUSION AND FUTURE WORKS
  CONCLUSION AND FUTURE WORKS We give a complete solution to the Gambler's problem, a simple and classic problem in reinforce- ment learning, under a variety of settings. We show that its optimal value function is very compli- cated and even pathological. Despite its seeming simpleness, these results are not clearly pointed out in previous studies. Our contributions are the theoretical finding and the implications. It is worthy to bring the cur- rent results to start the discussion among the community. Indicated by the Gambler's problem, the current algorithmic approaches in reinforcement learning might underestimate the complexity. We expect more evidence could be found in the future and new algorithms and implementations could be brought out. It would be interesting to see how these results of the Gambler's problem generalized to other MDPs. Finding these characterizations of MDPs is in general an important step to understand reinforcement learning and sequential decision processes.
  This continuity assumption is only for a better organization of the settings. The more general problem (AB) is solved in Section A.2 without this assumption.

Section Title: A ANALYSIS
  A ANALYSIS

Section Title: A.1 ANALYSIS OF THE GAMBLER'S PROBLEM
  A.1 ANALYSIS OF THE GAMBLER'S PROBLEM In this section we show that v(s) defined below is a unique solution of the system (ABX). Since the optimal state-value function must satisfies the system (ABX), v(s) is the optimal state-value function of the Gambler's problem. This statement is rigorously proved in Theorem 12. for 0 ≤ s < 1, where s = 0.b 1 b 2 . . . b . . . (2) is the binary representation of s. It is obvious that the series converges for any 0 ≤ s < 1. The notation v(s) will always refer to the definition above in this paper and will not change with the context. We show later that this v(s) is the optimal value function of the problem. We use the notation f (s) to denote a general solution of a system, which varies according to the required properties. Let the set of dyadic rationals G = k2 − | k ∈ 1, . . . , 2 − 1 (3) such that G is the set of numbers that can be represented by at most binary bits. The general idea to verify the Bellman equation (AB) is to prove v(s) = max a∈G ∩A(s) (1 − p)γ v(s + a) + pγ v(s − a) for any s ∈ G by induction of = 1, 2, . . . , and generalize this optimality to the entire interval s ∈ (0, 1). It then suffices to show the uniqueness of v(s) that solves the system (ABX). This is proved by assuming the existence of a solution f (s) and derive the identity f (s) = v(s), condition on the Bellman property that v(s) satisfies (AB). For presentation purposes, the uniqueness is discussed first. As an overview, Lemma 2, 3, and 4 describe the system (ABX). Claim 5, Lemma 6, 8, and 9 describe the properties of v(s). Lemma 2 (Uniqueness under existence). Let f (s) : [0, 1] → R be a real function. If v(s) and f (s) both satisfy (ABX), then v(s) = f (s) for all 0 ≤ s ≤ 1.

Section Title: Proof
  Proof

```
