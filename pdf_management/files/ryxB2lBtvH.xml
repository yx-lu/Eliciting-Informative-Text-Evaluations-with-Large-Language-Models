<article article-type="research-article"><front><article-meta><title-group><article-title>VIA SKILL BEHAVIOR DIVERSIFICATION</article-title></title-group><contrib-group content-type="author"><contrib contrib-type="person"><name><surname>Lee</surname><given-names>Youngwoon</given-names></name></contrib><contrib contrib-type="person"><name><surname>Yang</surname><given-names>Jingyun</given-names></name></contrib><contrib contrib-type="person"><name><surname>Lim</surname><given-names>Joseph J</given-names></name></contrib><contrib contrib-type="person"><xref ref-type="aff" rid="aff0" /></contrib></contrib-group><aff id="aff0"><institution content-type="orgname">Department of Computer Science University of Southern California {lee504</institution><city>jingyuny</city></aff><abstract><p>When mastering a complex manipulation task, humans often decompose the task into sub-skills of their body parts, practice the sub-skills independently, and then execute the sub-skills together. Similarly, a robot with multiple end-effectors can perform complex tasks by coordinating sub-skills of each end-effector. To realize temporal and behavioral coordination of skills, we propose a modular framework that first individually trains sub-skills of each end-effector with skill behavior diversification, and then learns to coordinate end-effectors using diverse behaviors of the skills. We demonstrate that our proposed framework is able to efficiently coordinate skills to solve challenging collaborative control tasks such as picking up a long bar, placing a block inside a container while pushing the container with two robot arms, and pushing a box with two ant agents. Videos and code are available at https://clvrai.com/coordination.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Imagine you wish to play Chopin's Fantaisie Impromptu on the piano. With little prior knowledge about the piece, you would first practice playing the piece with each hand separately. After inde- pendently mastering the left and right hand parts, you would move on to practicing with both hands simultaneously. To find the synchronized and non-interfering movements of two hands, you would try variable ways of playing the same melody with each hand, and eventually create a complete piece of music. Through the decomposition of skills into sub-skills of two hands and learning variations of sub-skills, humans make the learning process of manipulation skills much faster than learning everything at once.</p><p>Can autonomous agents efficiently learn complicated tasks with coordination of different skills from multiple end-effectors like humans? Learning to perform collaborative and composite tasks from scratch requires a huge amount of environment interaction and extensive reward engineering, which often results in undesired behaviors (<xref ref-type="bibr" rid="b0">Riedmiller et al., 2018</xref>). Hence, instead of learning a task at once, modular approaches (<xref ref-type="bibr" rid="b0">Andreas et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Oh et al., 2017</xref>; <xref ref-type="bibr" rid="b6">Frans et al., 2018</xref>; <xref ref-type="bibr" rid="b15">Lee et al., 2019</xref>; <xref ref-type="bibr" rid="b8">Peng et al., 2019</xref>; <xref ref-type="bibr" rid="b8">Goyal et al., 2020</xref>) suggest to learn reusable primitive skills and solve more complex tasks by recombining the skills. However, all these approaches either focus on working with single end-effector manipulation or single agent locomotion, and these do not scale to multi-agent problems.</p><p>To this end, we propose a modular framework that learns to coordinate multiple end-effectors with their primitive skills for various robotics tasks, such as bimanual manipulation. The main challenge is that naive simultaneous execution of primitive skills from multiple end-effectors can often cause unintended behaviors (e.g. collisions between end-effectors). Thus, as illustrated in <xref ref-type="fig" rid="fig_0">Figure 1</xref>, our model needs to learn to appropriately coordinate end-effectors; and hence needs a way to obtain, represent, and control detailed behaviors of each primitive skill. Inspired by these intuitions, our method consists of two parts: (1) acquiring primitive skills with diverse behaviors by mutual information maximization, and (2) learning a meta policy that selects a skill for each end-effector and coordinates the chosen skills by controlling the behavior of each skill.</p><p>The main contribution of this paper is a modular and hierarchical approach that tackles cooperative manipulation tasks with multiple end-effectors by (1) learning primitive skills of each end-effector independently with skill behavior diversification and (2) coordinating end-effectors using diverse Pick ( ) Push ( ) Place ( ) Push ( ) behaviors of the skills. Our empirical results indicate that our proposed method is able to efficiently learn primitive skills with diverse behaviors and coordinate these skills to solve challenging collabo- rative control tasks such as picking up a long bar, placing a block inside the container on the right side, and pushing a box with two ant agents. We provide additional qualitative results and code at https://clvrai.com/coordination.</p></sec><sec><title>RELATED WORK</title><p>Deep reinforcement learning (RL) for continuous control is an active research area. However, learning a complex task either from a sparse reward or a heavily engineered reward becomes computationally impractical as the target task becomes complicated. Instead of learning from scratch, complex tasks can be tackled by decomposing the tasks into easier and reusable sub-tasks. Hierarchical reinforcement learning temporally splits a task into a sequence of temporally extended meta actions. It often consists of one meta policy (high-level policy) and a set of low-level policies, such as options framework (<xref ref-type="bibr" rid="b0">Sutton et al., 1999</xref>). The meta policy decides which low-level policy to activate and the chosen low-level policy generates an action sequence until the meta policy switches it to another low- level policy. Options can be discovered without supervision (<xref ref-type="bibr" rid="b0">Schmidhuber, 1990</xref>; <xref ref-type="bibr" rid="b1">Bacon et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Nachum et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Levy et al., 2019</xref>), meta-learned (<xref ref-type="bibr" rid="b6">Frans et al., 2018</xref>), pre-defined (<xref ref-type="bibr" rid="b14">Kulkarni et al., 2016</xref>; <xref ref-type="bibr" rid="b0">Oh et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Merel et al., 2019</xref>; <xref ref-type="bibr" rid="b15">Lee et al., 2019</xref>), or attained from additional supervision signals (<xref ref-type="bibr" rid="b0">Andreas et al., 2017</xref>; <xref ref-type="bibr" rid="b7">Ghosh et al., 2018</xref>). However, option frameworks are not flexible to solve a task that requires simultaneous activation or interpolation of multiple skills since only one skill can be activated at each time step.</p><p>To solve composite tasks multiple policies can be simultaneously activated by adding Q- functions (<xref ref-type="bibr" rid="b10">Haarnoja et al., 2018a</xref>), additive composition (<xref ref-type="bibr" rid="b0">Qureshi et al., 2020</xref>; <xref ref-type="bibr" rid="b8">Goyal et al., 2020</xref>), or multiplicative composition (<xref ref-type="bibr" rid="b8">Peng et al., 2019</xref>). As each policy takes the whole observation as input and controls the whole agent, it is not robust to changes in unrelated parts of the observation. For example, a left arm skill can be affected by the pose change in the right arm, which is not relevant to the left arm skill. Hence, these skill composition approaches can fail when an agent encounters a new combination of skills or a new skill is introduced since the agent will experience unseen observations.</p><p>Instead of having a policy with the full observation and action space, multi-agent reinforcement learning (MARL) suggests to explicitly split the observation and action space according to agents (e.g. robots or end-effectors), which allows efficient low-level policy training as well as flexible skill composition. For cooperative tasks, communication mechanisms (<xref ref-type="bibr" rid="b0">Sukhbaatar et al., 2016</xref>; <xref ref-type="bibr" rid="b8">Peng et al., 2017</xref>; <xref ref-type="bibr" rid="b13">Jiang &amp; Lu, 2018</xref>), sharing policy parameters (<xref ref-type="bibr" rid="b4">Gupta et al., 2017</xref>), and decentralized actor with centralized critic (<xref ref-type="bibr" rid="b0">Lowe et al., 2017</xref>; <xref ref-type="bibr" rid="b5">Foerster et al., 2018</xref>) have been actively used. However, these approaches suffer from the credit assignment problem (<xref ref-type="bibr" rid="b0">Sutton, 1984</xref>) among agents and the lazy agent problem (<xref ref-type="bibr" rid="b0">Sunehag et al., 2018</xref>). As agents have more complicated morphology and larger observation space, learning a policy for a multi-agent system from scratch requires extremely long training time. Moreover, the credit assignment problem becomes more challenging when the complexity of cooperative tasks increases and all agents need to learn completely from scratch. To resolve these issues, we propose to first train reusable skills for each agent in isolation, instead of Published as a conference paper at ICLR 2020 State Next State Action Meta Policy Agent/End-effector 1 Pick Place Push Primitive Skills Behavior Embedding Pick Place Push Selected Skill Index Agent/End-effector 2 Behavior Embedding Pick Place Push Pick Place Push Primitive Skills Selected Skill Index learning primitive skills of multiple agents together. Then, we recombine these skills (<xref ref-type="bibr" rid="b0">Maes &amp; Brooks, 1990</xref>) to complete more complicated tasks with learned coordination of the skills. To coordinate skills from multiple agents, the skills have to be flexible; hence, a skill can be adjusted to collaborate with other agents' skills. Maximum entropy policies (<xref ref-type="bibr" rid="b10">Haarnoja et al., 2017</xref>; <xref ref-type="bibr" rid="b11">2018a</xref>;b) can learn diverse ways to achieve a goal by maximizing not only reward but also entropy of the policy. In addition, <xref ref-type="bibr" rid="b4">Eysenbach et al. (2019)</xref> proposes to discover diverse skills without reward by maximizing entropy as well as mutual information between resulting states and latent representations of skills (i.e. skill embeddings). Our method leverages the maximum entropy policy (<xref ref-type="bibr" rid="b10">Haarnoja et al., 2018b</xref>) with the discriminability objective (<xref ref-type="bibr" rid="b4">Eysenbach et al., 2019</xref>) to learn a primitive skill with diverse behaviors conditioned on a controllable skill embedding. This controllable skill embedding will be later used as a behavior embedding for the meta policy to adjust a primitive skill's behavior for coordination.</p></sec><sec><title>METHOD</title><p>In this paper, we address the problem of solving cooperative manipulation tasks that require collab- oration between multiple end-effectors or agents. Note that we use the terms "end-effector" and "agent" interchangeably in this paper. Instead of learning a multi-agent task from scratch (<xref ref-type="bibr" rid="b0">Lowe et al., 2017</xref>; <xref ref-type="bibr" rid="b4">Gupta et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Sunehag et al., 2018</xref>; <xref ref-type="bibr" rid="b5">Foerster et al., 2018</xref>), modular approaches (<xref ref-type="bibr" rid="b0">Andreas et al., 2017</xref>; <xref ref-type="bibr" rid="b6">Frans et al., 2018</xref>; <xref ref-type="bibr" rid="b8">Peng et al., 2019</xref>) suggest to learn reusable primitive skills and solve more complex tasks by recombining these skills. However, concurrent execution of primitive skills of multiple agents fails when agents never experienced a combination of skills during the pre-training stage, or skills require temporal or behavioral coordination.</p><p>Therefore, we propose a modular and hierarchical framework that learns to coordinate multiple agents with primitive skills to perform a complex task. Moreover, during primitive skill training, we propose to learn a latent behavior embedding, which provides controllability of each primitive skill to the meta policy while coordinating skills. In Section 3.2, we describe our modular framework in detail. Next, in Section 3.3, we elaborate how controllable primitive skills can be acquired. Lastly, we describe how the meta policy learns to coordinate primitive skills in Section 3.4.</p></sec><sec><title>PRELIMINARIES</title><p>We formulate our problem as a Markov decision process defined by a tuple {S, A, T , R, &#961;, &#947;} of states, actions, transition probability, reward, initial state distribution, and discount factor. In our formulation, we assume the environment includes N agents. To promote consistency in our terminology, we use superscripts to denote the index of agent and subscripts to denote time or primitive skill index. Hence, the state space and action space for an agent i can be represented as S i and A i where each element of S i is a subset of the corresponding element in S and A = A 1 &#215;A 2 &#215;&#183; &#183; &#183;&#215;A N , respectively. For each agent i, we provide a set of m i skills, &#928; i = {&#960; i 1 , . . . , &#960; i m i }. A policy of an agent i is represented as</p><p>and receive a single reward r t . The performance is evaluated based on a discounted return R = T &#8722;1 t=0 &#947; t r t , where T is the episode horizon.</p></sec><sec><title>MODULAR FRAMEWORK</title><p>As illustrated in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, our model is composed of two components: a meta policy &#960; meta and a set of primitive skills of N agents &#928; 1 , . . . , &#928; N . Note that each primitive skill &#960; i c i &#8712; &#928; i contains variants of behaviors parameterized by an N z -dimensional latent behavior embedding z i (see Section 3.3). The meta policy selects a skill to execute for each agent, rather than selecting one primitive skill for the entire multi-agent system to execute. Also, we give the meta policy the capability to select which variant of the skill to execute (see Section 3.4). Then, the chosen primitive skills are simultaneously executed for T low time steps.</p><p>The concurrent execution of multiple skills often leads to undesired results and therefore requires coordination between the skills. For example, naively placing a block in the left hand to a container being moved by the right hand can cause collision between the two robot arms. The arms can avoid collision while performing the skills by properly adjusting their skill behaviors (e.g. the left arm leaning to the left side while placing the block and the right arm leaning to the right side while pushing the container) as shown in <xref ref-type="fig" rid="fig_0">Figure 1</xref>. In our method, the meta policy learns to coordinate multiple agents' skills by manipulating the behavior embeddings (i.e. selecting a proper behavior from diverse behaviors of each skill).</p></sec><sec><title>TRAINING AGENT-SPECIFIC PRIMITIVE SKILLS WITH DIVERSE BEHAVIORS</title><p>To adjust a primitive skill to collaborate with other agents' skills in a new environment, the skill needs to support variations of skill behaviors when executed at a given state. Moreover, a behavioral variation of a skill should be controllable by the meta policy for skill coordination. In order to make our primitive skill policies generate diverse behaviors controlled by a latent vector z, we leverage the entropy and mutual information maximization objective introduced in <xref ref-type="bibr" rid="b4">Eysenbach et al. (2019)</xref>. More specifically, a primitive policy of an agent i outputs an action a &#8712; A conditioned on the current state s &#8712; S and a latent behavior embedding z &#8764; p(z), where the prior distribution p(z) is Gaussian (we omit agent i in this section for the simplicity of notations). Diverse behaviors conditioned on a random sample z can be achieved by maximizing the mutual information between behaviors and states M I(s, z), while minimizing the mutual information between behaviors and actions given the state M I(a, z|s), together with maximizing the entropy of the policy H(a|s) to encourage diverse behaviors. The objective can be written as follows (we refer the readers to <xref ref-type="bibr" rid="b4">Eysenbach et al. (2019)</xref> Published as a conference paper at ICLR 2020</p></sec><sec><title>Algorithm 1 ROLLOUT</title><p>1: Input: Meta policy &#960; meta , sets of primitive policies &#928; 1 , ..., &#928; N , and meta horizon T low</p><p>2: Initialize an episode t &#8592; 0 and receive initial state s 0</p><p>3: while episode is not terminated do</p><p>where the learned discriminator q &#966; (z|s) approximates the posterior p(z|s).</p><p>To achieve a primitive skill with diverse behaviors, we augment Equation (3) to the environment reward:</p><p>where &#955; 1 is the entropy coefficient and &#955; 2 is the diversity coefficient which corresponds identifia- bility of behaviors. Maximizing Equation (3) encourages multi-modal exploration strategies while maximizing the reward r t forces to achieve its own goal. Moreover, by maximizing identifiability of behaviors, the latent vector z, named behavior embedding, can represent a variation of the learned policy and thus can be used to control the behavior of the policy. For example, when training a robot to move an object, a policy learns to move the object quickly as well as slowly, and these diverse behaviors map to different latent vectors z. We empirically show that the policies with diverse behaviors achieve better compositionality with other agents in our experiments.</p></sec><sec><title>COMPOSING PRIMITIVE SKILLS WITH META POLICY</title><p>We denote the meta policy as &#960; meta (c 1 , . . . , c N , z 1 , . . . , z N |s t ), where c i &#8712; [1, m i ] represents a skill index of an agent i &#8712; [1, N ] and z i &#8712; R Nz represents a behavior embedding of the skill. Every T low time steps, the meta policy chooses one primitive skill &#960; i c i &#8712; &#928; i for each agent i. Also, the meta policy outputs a set of latent behavior embeddings (z 1 , z 2 , . . . , z N ) and feeds them to the corresponding skills (i.e. &#960; i c i (a i |s i , z i ) for agent i). Once a set of primitive skills {&#960; 1 c 1 , . . . , &#960; N c N } are chosen to be executed, each primitive skill generates an action a i &#8764; &#960; i c i (a i |s i , z i ) based on the current state s i and the latent vector z i . Algorithm 1 illustrates the overall rollout process. Since there are a finite number of skills for each agent to execute, the meta action space for each agent [1, m i ] is discrete, while the behavior embedding space for each agent R Nz is continuous. Thus, the meta policy is modeled as a (2 &#215; N )-head neural network where the first N heads represent m i -way categorical distributions for skill selection and the last N heads represent N z -dimensional Gaussian distributions for behavior control of the chosen skill.</p></sec><sec><title>IMPLEMENTATION</title><p>We model the primitive policies and posterior distributions q &#966; as neural networks. We train the primitive policies using soft actor-critic (<xref ref-type="bibr" rid="b10">Haarnoja et al., 2018b</xref>). When we train a primitive policy, we use a unit Gaussian distribution as the prior distribution of latent variables p(z). We use 5 as the size of latent behavior embedding N z . Each primitive policy outputs the mean and standard Published as a conference paper at ICLR 2020 (a) JACO PICK-PUSH-PLACE (b) JACO BAR-MOVING (c) ANT PUSH deviation of a Gaussian distribution over an action space. For a primitive policy, we apply tanh activation to normalize the action between [&#8722;1, 1]. We model the meta policy as neural network with multiple heads that output the skill index c i and behavior embedding z i for each agent. The meta policy is trained using PPO (<xref ref-type="bibr" rid="b2">Schulman et al., 2017</xref>; 2016; <xref ref-type="bibr" rid="b3">Dhariwal et al., 2017</xref>). All policy networks in this paper consist of 3 fully connected layers of 64 hidden units with ReLU nonlinearities. The discriminator q &#966; in Equation (4) is a 2-layer fully connected network with 64 hidden units.</p></sec><sec><title>EXPERIMENTS</title><p>To demonstrate the effectiveness of our framework, we compare our method to prior methods in the field of multi-agent RL and ablate the components of our framework to understand their importance. We conducted experiments on a set of challenging robot control environments that require coordination of different agents to complete collaborative robotic manipulation and locomotion tasks. Through our experiments, we aim to answer the following questions: (1) can our framework efficiently learn to combine primitive skills to execute a complicated task; (2) can our learned agent exhibit collaborative behaviors during task execution; and (3) can our framework leverage the controllable behavior variations of the primitive skills to achieve better coordination?</p><p>For details about environments and training, please refer to the supplementary material. As the performance of training algorithms varies between runs, we train each method on each task with 6 different random seeds and report mean and standard deviation of each method's success rate.</p></sec><sec><title>BASELINES</title><p>We compare the performance of our method with various single- and multi-agent RL methods illustrated in <xref ref-type="fig" rid="fig_2">Figure 3</xref>:</p><p>Single-agent RL (RL): A vanilla RL method where a single policy takes as input the full observation and outputs all agents' actions.</p><p>Multi-agent RL (MARL): A multi-agent RL method where each of N policies takes as input the observation of the corresponding agent and outputs an action for that agent. All policies share the global critic learned from a single task reward (<xref ref-type="bibr" rid="b0">Lowe et al., 2017</xref>).</p></sec><sec><title>Modular Framework (Modular)</title><p>Single-agent RL with Skill Behavior Diversification (RL-SBD): An RL method augmented with the behavior diversification objective. A meta policy is employed to generate a behavior embedding for a low-level policy, and the low-level policy outputs all agents' actions conditioned on the behavior embedding and the full observation for T low time steps. The meta policy and the low-level policy are jointly trained with the behavior diversification objective described in Equation (4).</p><p>Multi-agent RL with Skill Behavior Diversification (MARL-SBD): A MARL method augmented with the behavior diversification objective. A meta policy generates N behavior embeddings. Then, each low-level policy outputs each agent's action conditioned on its observation and behavior embedding for T low time steps. All policies are jointly trained to maximize Equation (4).</p><p>Modular Framework with Skill Behavior Diversification (Modular-SBD, Ours): Our method which coordinates primitive skills of multiple agents. The modular framework consists of a meta policy and N sets of primitive skills, where each primitive skill is conditioned on a behavior embedding z. The meta policy takes as input the full observation and selects both a primitive skill and a behavior embedding for each agent. Then, each primitive skill outputs action for each agent.</p></sec><sec><title>JACO PICK-PUSH-PLACE</title><p>We developed JACO PICK-PUSH-PLACE and JACO BAR-MOVING environments using two Kinova Jaco arms, where each Jaco arm is a 9 DoF robotic arm with 3 fingers. JACO PICK-PUSH-PLACE starts with a block on the left and a container on the right. The robotic arms need to pick up the block, push the container to the center, and place the block inside the container. For successful completion of the task, the two Jaco arms have to concurrently execute their distinct sets of skills and dynamically adjust their picking, pushing, and placing directions to avoid collision between arms.</p></sec><sec><title>Primitives skills</title><p>There are three primitive skills available to each arm: Picking up, Pushing, and Placing to center (see Figure 4a). Picking up requires a robotic arm to pick up a small block, which is randomly placed on the table. If the block is not picked up after a certain amount of time or the arm drops the block, the agent fails. Pushing learns to push a big container to its opposite side (e.g. from left to the center or from right to center). The agent fails if it cannot place the container to the center. Placing to center requires placing an object in the gripper to the table. The agent only succeeds when it stably places the object at the desired location on the container.</p></sec><sec><title>Composite task</title><p>Our method (Modular-SBD) can successfully perform JACO PICK-PUSH-PLACE task while all baselines fail to compose primitive skills as shown in Figure 5a. The RL and MARL baselines cannot learn the composite task mainly because the agent requires to learn the combinatorial number of skill compositions and to solve the credit assignment problem across multiple agents. Since the composite task requires multiple primitive skills of multiple agents to be performed properly at the same time, a reward signal about a failure case cannot be assigned to the correct agent or skill. By using pre-trained primitive skills, the credit assignment problem is relaxed and all agents can perform their skills concurrently. Therefore, the Modular baseline learns to achieve success but shows significantly lower performance than our method (Modular-SBD). This is because the lack of skill behavior diversification makes it impossible to adjust pushing and placing trajectories during skill composition time, which resulting in frequent end-effector collisions.</p></sec><sec><title>JACO BAR-MOVING</title><p>In JACO BAR-MOVING, two Jaco arms need to pick up a long bar together, move the bar towards a target location while maintaining its rotation, and place it on the table (see Figure 4b). The initial position of the bar is randomly initialized every episode and an agent needs to find appropriate coordination between two arms for each initialization. Compared to JACO PICK-PUSH-PLACE, this task requires that the two arms synchronize their movements and perform more micro-level adjustments to their behaviors.</p></sec><sec><title>Primitives skills</title><p>There are two pre-trained primitive skills available to each arm: Picking up and Placing towards arm. Picking up is same as described in Section 4.2. Placing towards arm learns to move a small block (half size of the block used in the composite task) in the hand towards the robotic arm and then place it on the table. The agent fails if it cannot place the block to the target location.</p></sec><sec><title>Composite task</title><p>The JACO BAR-MOVING task requires the two arms to work very closely together. For example, the Picking up skill of both arms should be synchronized when they start to lift the bar and two arms require to lift the bar while maintaining the relative position between them since they are connected by holding the bar. The modular framework without explicit coordination of skills (Modular) can synchronize the execution of picking, moving, and placing. But the inability to micro-adjust the movement of the other arm causes instability of bar picking and moving. This results in degraded success rates compared to the modular framework with explicit coordination. Meanwhile, all baselines without pre-defined primitive skills fail to learn JACO BAR-MOVING.</p></sec><sec><title>ANT PUSH</title><p>We developed a multi-ant environment, ANT-PUSH, inspired from <xref ref-type="bibr" rid="b0">Nachum et al. (2019)</xref>, simulated in the MuJoCo (<xref ref-type="bibr" rid="b0">Todorov et al., 2012</xref>) physics engine. We use the ant model in OpenAI Gym (<xref ref-type="bibr" rid="b2">Brockman et al., 2016</xref>). In this environment, two ants need to push a large object toward a green target place, collaborating with each other to keep the angle of the object as stable as possible (see Figure 4c).</p></sec><sec><title>Primitives skills</title><p>We train walking skills of an ant agent in 4 directions: up, down, left, and right. During primitive skill training, a block (half size of the block used in the composite task) and an ant agent are randomly placed. Pushing the block gives an additional reward to the agent, which prevents an ant to avoid the block. The learned primitive skills have different speed and trajectories conditioned on the latent behavior embedding.</p></sec><sec><title>Composite task</title><p>Our method achieves 32.3% success rate on ANT PUSH task while all baselines fail to compose primitive skills as shown in Figure 5c and <xref ref-type="table" rid="tab_0">Table 1</xref>. The poor performance of RL, MARL, RL-SBD, and MARL-SBD baselines shows the difficulty of credit assignment between agents, which leads one of the ants moves toward a block and pushes it but another ant does not move. Moreover, the Modular baseline with primitive skills also fails to learn the pushing task. This result illustrates the importance of coordination of agents, which helps synchronizing and controlling the velocities of both ant agents to push the block toward the goal position while maintaining its rotation.</p></sec><sec><title>EFFECT OF DIVERSITY OF PRIMITIVE SKILLS</title><p>To analyze the effect of the diversity of primitive skills, we compare our model with primitive skills trained with different diversity coefficients &#955; 2 = {0.0, 0.05, 0.1, 0.5, 1.0} in Equation (4) on ANT PUSH. <xref ref-type="fig" rid="fig_5">Figure 6</xref> shows that with small diversity coefficients &#955; 2 = {0.05, 0.1}, the agent can control detailed behaviors of primitive skills while primitive skills without diversity (&#955; 2 = 0) cannot be coordinated. The meta policy tries to synchronize two ant agents' positions and velocities by switching primitive skills, but it cannot achieve proper coordination without diversified skills. On the other hand, large diversity coefficients &#955; 2 = {0.5, 1.0} make the primitive skills often focus on demonstrating diverse behaviors and fail to achieve the goals of the skills. Hence, these primitive skills do not have enough functionality to solve the target task. The diversity coefficient needs to be carefully chosen to acquire primitive skills with good performance as well as diverse behaviors.</p></sec><sec><title>EFFECT OF SKILL SELECTION INTERVAL T low</title><p>To analyze the effect of the skill selection interval hyperparameter T low , we compare our method trained with T low = {1, 2, 3, 5, 10} on Jaco environments. The success rate curves in <xref ref-type="fig" rid="fig_6">Figure 7</xref> demonstrate that smaller T low values in range [1, 3] lead to better performance. This can be because the agent can realize more flexible skill coordination by adjusting the behavior embedding frequently.</p><p>In addition to the fixed T low values, we also consider the variation of our method in which the skill behavior embedding is only sampled when the meta policy updates its skill selection. Concretely, we set the value of T low to 1 but update (z 1 t , . . . , z N t ) only if (c 1 t , . . . , c N t ) = (c 1 t&#8722;1 , . . . , c N t&#8722;1 ). We observe that in this setting, the meta policy at times switch back and forth between two skills in two consecutive time steps, leading to slightly worse performance compared to our method with small T low values. This indicates that the meta policy needs to adjust the behavior embedding in order to optimally coordinate skills of the different agents.</p></sec><sec><title>CONCLUSION</title><p>In this paper, we propose a modular framework with skill coordination to tackle challenges of composition of sub-skills with multiple agents. Specifically, we use entropy maximization with mutual information maximization to train controllable primitive skills with diverse behaviors. To coordinate learned primitive skills, the meta policy predicts not only the skill to execute for each agent (end-effector) but also the behavior embedding that controls the chosen primitive skill's behavior. The experimental results on robotic manipulation and locomotion tasks demonstrate that the proposed framework is able to efficiently learn primitive skills with diverse behaviors and coordinate multiple agents (end-effectors) to solve challenging cooperative control tasks. Acquiring skills without supervision and extending our method to a visual domain are exciting directions for future work.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Composing complex skills using multiple agents' primitive skills requires proper coordina- tion between agents since concurrent execution of primitive skills requires temporal and behavioral coordination. For example, to move a block into a container on the other end of the table, the agent needs to not only utilize pick, place, and push primitive skills at the right time but also select the appropriate behaviors for these skills, represented as latent vectors z 1 , z 2 , z 3 , and z 4 above. Naive methods neglecting either temporal or behavioral coordination will produce unintended behaviors, such as collisions between end-effectors.</p></caption><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Our method is composed of two components: a meta policy and a set of agent-specific primitive policies relevant to task completion. The meta policy selects which primitive skill to run for each agent as well as the behavior embedding (i.e. variation in behavior) of the chosen primitive skill. Each selected primitive skill takes as input the agent observation and the behavior embedding and outputs action for that agent.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Different multi-agent architectures. (a) The vanilla RL method considers all agents as a monolithic agent; thus a single policy takes the full observation as input and outputs the full action. (b) The multi-agent RL method (MARL) consists of N policies that operate on the observations and actions of corresponding agents. (c) The modular network consists of N sets of skills for the N agents trained in isolation and a meta policy that selects a skill for each agent. (d-f) The RL, MARL, and modular network methods augmented with skill behavior diversification (SBD) has a meta policy that outputs a skill behavior embedding vector z for each skill.</p></caption><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>The composite tasks pose a challenging combination of object manipulation and locomotion skills, which requires coordination of multiple agents and temporally extended behaviors. (a) The left Jaco arm needs to pick up a block while the right Jaco arm pushes a container, and then it places the block into the container. (b) Two Jaco arms are required to pick and place a bar-shaped block together. (c) Two ants push the red box to the goal location (green circle) together.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>Success rates of our method (Modular-SBD) and baselines. For modular frameworks (Modular and Modular-SBD), we shift the learning curves rightwards the total number of environment steps the agent takes to learn the primitive skills (0.9 M, 1.2 M, and 2.0 M, respectively). Our method substantially improves learning speed and performance on JACO PICK-PUSH-PLACE and ANT PUSH. The shaded areas represent the standard deviation of results from six different seeds. The curves are smoothed using moving average over 10 runs.</p></caption><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Success rates for all tasks, comparing our method against baselines. Each entry in the table represents average success rate and standard deviation over 100 runs. The baselines learning from scratch fail to learn complex tasks with multiple agents.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_5"><object-id>fig_5</object-id><label>Figure 6:</label><caption><title>Figure 6:</title><p>Learning curves of our method with different diversity coefficients &#955; 2 on ANT PUSH.</p></caption><graphic /></fig><fig id="fig_6"><object-id>fig_6</object-id><label>Figure 7:</label><caption><title>Figure 7:</title><p>Success rates of our method with dif- ferent T low coefficients on Jaco environments.</p></caption><graphic /></fig></sec></body><back><ack /><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Modular multitask reinforcement learning with policy sketches</article-title><source>International Conference on Machine Learning</source><year>2017</year><fpage>166</fpage><lpage>175</lpage><person-group person-group-type="author"><name><surname>References Jacob Andreas</surname><given-names>Dan</given-names></name><name><surname>Klein</surname><given-names>Sergey</given-names></name><name><surname>Levine</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>The option-critic architecture</article-title><source>Association for the Advancement of Artificial Intelligence</source><year>2017</year><person-group person-group-type="author"><name><surname>Bacon</surname><given-names>Pierre-Luc</given-names></name><name><surname>Harb</surname><given-names>Jean</given-names></name><name><surname>Precup</surname><given-names>Doina</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Openai gym</article-title><source>arXiv preprint arXiv:1606.01540</source><year>2016</year><person-group person-group-type="author"><name><surname>Brockman</surname><given-names>Greg</given-names></name><name><surname>Cheung</surname><given-names>Vicki</given-names></name><name><surname>Pettersson</surname><given-names>Ludwig</given-names></name><name><surname>Schneider</surname><given-names>Jonas</given-names></name><name><surname>Schulman</surname><given-names>John</given-names></name><name><surname>Tang</surname><given-names>Jie</given-names></name><name><surname>Zaremba</surname><given-names>Wojciech</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><source>Openai baselines</source><year>2017</year><person-group person-group-type="author"><name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name><name><surname>Hesse</surname><given-names>Christopher</given-names></name><name><surname>Klimov</surname><given-names>Oleg</given-names></name><name><surname>Nichol</surname><given-names>Alex</given-names></name><name><surname>Plappert</surname><given-names>Matthias</given-names></name><name><surname>Radford</surname><given-names>Alec</given-names></name><name><surname>Schulman</surname><given-names>John</given-names></name><name><surname>Sidor</surname><given-names>Szymon</given-names></name><name><surname>Wu</surname><given-names>Yuhuai</given-names></name><name><surname>Zhokhov</surname><given-names>Peter</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Diversity is all you need: Learning skills without a reward function</article-title><source>International Conference on Learning Representations</source><year>2019</year><person-group person-group-type="author"><name><surname>Eysenbach</surname><given-names>Benjamin</given-names></name><name><surname>Gupta</surname><given-names>Abhishek</given-names></name><name><surname>Ibarz</surname><given-names>Julian</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Counterfactual multi-agent policy gradients</article-title><source>Association for the Advancement of Artificial Intelligence</source><year>2018</year><person-group person-group-type="author"><name><surname>Jakob</surname><given-names>N</given-names></name><name><surname>Foerster</surname><given-names>Gregory</given-names></name><name><surname>Farquhar</surname><given-names>Triantafyllos</given-names></name><name><surname>Afouras</surname><given-names>Nantas</given-names></name><name><surname>Nardelli</surname><given-names>Shimon Whiteson</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>META LEARNING SHARED HIERARCHIES</article-title><source>International Conference on Learning Representations</source><year>2018</year><person-group person-group-type="author"><name><surname>Frans</surname><given-names>Kevin</given-names></name><name><surname>Ho</surname><given-names>Jonathan</given-names></name><name><surname>Chen</surname><given-names>Xi</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name><name><surname>Schulman</surname><given-names>John</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Divide-and- conquer reinforcement learning</article-title><source>International Conference on Learning Representations</source><year>2018</year><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>Dibya</given-names></name><name><surname>Singh</surname><given-names>Avi</given-names></name><name><surname>Rajeswaran</surname><given-names>Aravind</given-names></name><name><surname>Kumar</surname><given-names>Vikash</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Reinforcement learning with competitive ensembles of information-constrained primitives</article-title><source>International Conference on Learning Representations</source><year>2020</year><person-group person-group-type="author"><name><surname>Goyal</surname><given-names>Anirudh</given-names></name><name><surname>Sodhani</surname><given-names>Shagun</given-names></name><name><surname>Binas</surname><given-names>Jonathan</given-names></name><name><surname>Xue Bin Peng</surname><given-names>Sergey</given-names></name><name><surname>Levine</surname><given-names>Yoshua</given-names></name><name><surname>Bengio</surname><given-names /></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Cooperative multi-agent control using deep reinforcement learning</article-title><source>International Conference on Autonomous Agents and Multi-Agent Systems</source><year>2017</year><fpage>66</fpage><lpage>83</lpage><person-group person-group-type="author"><name><surname>Jayesh</surname><given-names>K</given-names></name><name><surname>Gupta</surname><given-names>Maxim</given-names></name><name><surname>Egorov</surname><given-names>Mykel</given-names></name><name><surname>Kochenderfer</surname><given-names /></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Reinforcement learning with deep energy-based policies</article-title><source>International Conference on Machine Learning</source><year>2017</year><fpage>1352</fpage><lpage>1361</lpage><person-group person-group-type="author"><name><surname>Haarnoja</surname><given-names>Tuomas</given-names></name><name><surname>Tang</surname><given-names>Haoran</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Composable deep reinforcement learning for robotic manipulation</article-title><source>IEEE International Confer- ence on Robotics and Automation</source><year>2018</year><fpage>6244</fpage><lpage>6251</lpage><person-group person-group-type="author"><name><surname>Haarnoja</surname><given-names>Tuomas</given-names></name><name><surname>Pong</surname><given-names>Vitchyr</given-names></name><name><surname>Zhou</surname><given-names>Aurick</given-names></name><name><surname>Dalal</surname><given-names>Murtaza</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</article-title><source>International Conference on Machine Learning</source><year>2018</year><fpage>1856</fpage><lpage>1865</lpage><person-group person-group-type="author"><name><surname>Haarnoja</surname><given-names>Tuomas</given-names></name><name><surname>Zhou</surname><given-names>Aurick</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Learning attentional communication for multi-agent cooperation</article-title><source>Advances in Neural Information Processing Systems</source><year>2018</year><fpage>7254</fpage><lpage>7264</lpage><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Jiechuan</given-names></name><name><surname>Lu</surname><given-names>Zongqing</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation</article-title><source>Advances in Neural Information Processing Systems</source><year>2016</year><fpage>3675</fpage><lpage>3683</lpage><person-group person-group-type="author"><name><surname>Tejas</surname><given-names>D</given-names></name><name><surname>Kulkarni</surname><given-names>Karthik</given-names></name><name><surname>Narasimhan</surname><given-names>Ardavan</given-names></name><name><surname>Saeedi</surname><given-names>Josh</given-names></name><name><surname>Tenenbaum</surname><given-names /></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Com- posing complex skills by learning transition policies</article-title><source>International Conference on Learning Representations</source><year>2019</year><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Youngwoon</given-names></name><name><surname>Shao-Hua</surname><given-names /></name><name><surname>Sun</surname><given-names>Sriram</given-names></name><name><surname>Somasundaram</surname><given-names>Edward</given-names></name><name><surname>Hu</surname><given-names>Joseph J</given-names></name><name><surname>Lim</surname><given-names /></name></person-group></element-citation></ref></ref-list></back></article>