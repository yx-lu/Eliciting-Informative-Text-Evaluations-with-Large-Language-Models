<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 RETHINKING DEEP ACTIVE LEARNING: USING UNLABELED DATA AT MODEL TRAINING</article-title></title-group><abstract><p>Active learning typically focuses on training a model on few labeled examples alone, while unlabeled ones are only used for acquisition. In this work we depart from this setting by using both labeled and unlabeled data during model training across active learning cycles. We do so by using unsupervised feature learning at the beginning of the active learning pipeline and semi-supervised learning at every active learning cycle, on all available data. The former has not been investigated before in active learning, while the study of latter in the context of deep learning is scarce and recent findings are not conclusive with respect to its benefit. Our idea is orthogonal to acquisition strategies by using more data, much like ensemble methods use more models. By systematically evaluating on a number of popular acquisition strategies and datasets, we find that the use of unlabeled data during model training brings a spectacular accuracy improvement in image classification, compared to the differences between acquisition strategies. We thus explore smaller label budgets, even one label per class.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Active learning (<xref ref-type="bibr" rid="b0">Settles, 2009</xref>) is an important pillar of machine learning but it has not been explored much in the context of deep learning until recently (<xref ref-type="bibr" rid="b10">Gal et al., 2017</xref>; <xref ref-type="bibr" rid="b1">Beluch et al., 2018</xref>; <xref ref-type="bibr" rid="b20">Wang et al., 2017</xref>; <xref ref-type="bibr" rid="b11">Geifman &amp; El-Yaniv, 2017</xref>; <xref ref-type="bibr" rid="b0">Sener &amp; Savarese, 2018</xref>). The standard active learning scenario focuses on training a model on few labeled examples alone, while unlabeled data are only used for acquisition, i.e., performing inference and selecting a subset for annotation. This is the opposite of what would normally work well when learning a deep model from scratch, i.e., training on a lot of data with some loss function that may need labels or not. At the same time, evidence is being accumulated that, when training powerful deep models, the difference in performance between acquisition strategies is small (<xref ref-type="bibr" rid="b13">Gissin &amp; Shalev-Shwartz, 2018</xref>; <xref ref-type="bibr" rid="b7">Chitta et al., 2019</xref>; <xref ref-type="bibr" rid="b1">Beluch et al., 2018</xref>).</p><p>In this work, focusing on image classification, we revisit active deep learning with the seminal idea of using all data, whether labeled or not, during model training at each active learning cycle. This departs from the standard scenario in that unlabeled data are now directly contributing to the cost function being minimized and to subsequent parameter updates, rather than just being used to perform inference for acquisition, whereby parameters are fixed. We implement our idea using two principles: unsupervised feature learning and semi-supervised learning. While both are well recognized in deep learning in general, we argue that their value has been unexplored or underestimated in the context of deep active learning.</p><p>Unsupervised feature learning or self-supervised learning is a very active area of research in deep learning, often taking the form of pre-training on artificial tasks with no human supervision for representation learning, followed by supervised fine-tuning on different target tasks like classification or object detection (<xref ref-type="bibr" rid="b8">Doersch et al., 2015</xref>; <xref ref-type="bibr" rid="b20">Wang &amp; Gupta, 2015</xref>; <xref ref-type="bibr" rid="b12">Gidaris et al., 2018</xref>; <xref ref-type="bibr" rid="b4">Caron et al., 2018</xref>). To our knowledge, all deep active learning research so far considers training deep models from scratch. In this work, we perform unsupervised feature learning on all data once at the beginning of the active learning pipeline and use the resulting parameters to initialize the model at each active learning cycle. Relying on <xref ref-type="bibr" rid="b4">Caron et al. (2018)</xref>, we show that such unsupervised pre-training improves accuracy in many cases at little additional cost.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>Semi-supervised learning (<xref ref-type="bibr" rid="b5">Chapelle et al., 2006</xref>) and active learning can be seen as two facets of the same problem: the former focuses on most certain model predictions on unlabeled examples, while the latter on least certain ones. Combined approaches appeared quite early (<xref ref-type="bibr" rid="b0">McCallum &amp; Nigam, 1998</xref>; <xref ref-type="bibr" rid="b21">Zhu et al., 2003</xref>). In the context of deep learning however, such combinations are scarce (<xref ref-type="bibr" rid="b20">Wang et al., 2017</xref>) and have even been found harmful in cases (<xref ref-type="bibr" rid="b9">Ducoffe &amp; Precioso, 2018</xref>). It has also been argued that the two individual approaches have similar performance, while active learning has lower cost (<xref ref-type="bibr" rid="b10">Gal et al., 2017</xref>). In the meantime, research on deep semi-supervised learning is very active, bringing significant progress (<xref ref-type="bibr" rid="b0">Tarvainen &amp; Valpola, 2017</xref>; <xref ref-type="bibr" rid="b17">Laine &amp; Aila, 2017</xref>; <xref ref-type="bibr" rid="b14">Iscen et al., 2019</xref>; <xref ref-type="bibr" rid="b19">Verma et al., 2019</xref>). In this work, we use semi-supervised learning on all data at every active learning cycle, replacing supervised learning on labeled examples alone. Relying on <xref ref-type="bibr" rid="b14">Iscen et al. (2019)</xref>, and contrary to previous findings <xref ref-type="bibr" rid="b20">Wang et al. (2017)</xref>; <xref ref-type="bibr" rid="b10">Gal et al. (2017)</xref>, we show that this consistently brings a dramatic accuracy improvement.</p><p>Since <xref ref-type="bibr" rid="b14">Iscen et al. (2019)</xref> uses label propagation (<xref ref-type="bibr" rid="b19">Zhou et al., 2003a</xref>) to explore the manifold structure of the feature space, an important question is whether it is the manifold similarity or the use of unlabeled data during model training that actually helps. We address this question by introducing a new acquisition strategy that is based on label propagation.</p><p>In summary, we make the following contributions:</p><p>&#8226; We systematically benchmark a number of existing acquisition strategies, as well as a new one, on a number of datasets, evaluating the benefit of unsupervised pre-training and semi-supervised learning in all cases.</p><p>&#8226; Contrary to previous findings, we show that using unlabeled data during model training can yield a dramatic gain compared to differences between acquisition strategies.</p><p>&#8226; Armed with this finding, we explore a smaller budget (fewer labeled examples) than prior work, and we find that the random baseline may actually outperform all other acquisition strategies by a large margin in cases.</p></sec><sec><title>RELATED WORK</title><p>We focus on deep active and semi-supervised learning as well as their combination.</p></sec><sec><title>Active learning</title><p>Geometric methods like core sets (<xref ref-type="bibr" rid="b11">Geifman &amp; El-Yaniv, 2017</xref>; <xref ref-type="bibr" rid="b0">Sener &amp; Savarese, 2018</xref>) select examples based on distances in the feature space. The goal is to select a subset of examples that best approximate the whole unlabeled set. We introduce a similar approach where Euclidean distances are replaced by manifold ranking. There are methods inspired by adversarial learning. For instance, a binary classifier can be trained to discriminate whether an example belongs to the labeled or unlabeled set (<xref ref-type="bibr" rid="b13">Gissin &amp; Shalev-Shwartz, 2018</xref>; <xref ref-type="bibr" rid="b19">Sinha et al., 2019</xref>). Adversarial examples have been used, being matched to the nearest unlabeled example (<xref ref-type="bibr" rid="b0">Mayer &amp; Timofte, 2018</xref>) or added to the labeled pool (<xref ref-type="bibr" rid="b9">Ducoffe &amp; Precioso, 2018</xref>).</p><p>It has been observed however that deep networks can perform similarly regardless of the acquisition function (<xref ref-type="bibr" rid="b13">Gissin &amp; Shalev-Shwartz, 2018</xref>; <xref ref-type="bibr" rid="b7">Chitta et al., 2019</xref>), which we further investigate here. Ensemble and Bayesian methods (<xref ref-type="bibr" rid="b10">Gal et al., 2017</xref>; <xref ref-type="bibr" rid="b1">Beluch et al., 2018</xref>; <xref ref-type="bibr" rid="b7">Chitta et al., 2019</xref>) target representing model uncertainty, which than can be used by different acquisition functions. This idea is orthogonal to acquisition strategies. In fact, <xref ref-type="bibr" rid="b1">Beluch et al. (2018)</xref>; <xref ref-type="bibr" rid="b7">Chitta et al. (2019)</xref> show that the gain of ensemble models is more pronounced than the gain of any acquisition strategy. Of course, ensemble and Bayesian methods are more expensive than single models. Approximations include for instance a single model producing different outputs by dropout (<xref ref-type="bibr" rid="b10">Gal et al., 2017</xref>). Our idea of using all data during model training is also orthogonal to acquisition strategies. It is also more expensive than using labeled data alone, but the gain is spectacular in this case. This allows the use of much smaller label budget for the same accuracy, which is the essence of active learning.</p><p>Semi-supervised active learning has a long history (<xref ref-type="bibr" rid="b0">McCallum &amp; Nigam, 1998</xref>; <xref ref-type="bibr" rid="b19">Muslea et al., 2002</xref>; <xref ref-type="bibr" rid="b21">Zhu et al., 2003</xref>; <xref ref-type="bibr" rid="b19">Zhou et al., 2004</xref>; <xref ref-type="bibr" rid="b11">Long et al., 2008</xref>). A recent deep learning approach acquires the least certain unlabeled examples for labeling and at the same time assigns predicted pseudo-labels to most certain examples (<xref ref-type="bibr" rid="b20">Wang et al., 2017</xref>). This does not always help (<xref ref-type="bibr" rid="b9">Ducoffe &amp; Precioso, 2018</xref>). In some cases, semi-supervised algorithms are incorporated as part of an active learning evaluation (<xref ref-type="bibr" rid="b20">Li et al., 2019</xref>; <xref ref-type="bibr" rid="b0">Sener &amp; Savarese, 2018</xref>). A comparative study suggests that semi-supervised learning Under review as a conference paper at ICLR 2020 does not significantly improve over active learning, despite its additional cost due to training on more data (<xref ref-type="bibr" rid="b10">Gal et al., 2017</xref>). We show that this is clearly not the case, using a state of the art semi-supervised method (<xref ref-type="bibr" rid="b14">Iscen et al., 2019</xref>) that is an inductive version of label propagation (<xref ref-type="bibr" rid="b19">Zhou et al., 2003a</xref>). This is related to <xref ref-type="bibr" rid="b21">Zhu &amp; Ghahramani (2002)</xref>; <xref ref-type="bibr" rid="b21">Zhu et al. (2003)</xref>; <xref ref-type="bibr" rid="b11">Long et al. (2008)</xref>, which however are limited to transductive learning.</p></sec><sec><title>Unsupervised feature learning</title><p>A number of unsupervised feature learning approaches pair match- ing images to learn the representation using a siamese architecture. These pairs can come as fragments of the same image (<xref ref-type="bibr" rid="b8">Doersch et al., 2015</xref>; <xref ref-type="bibr" rid="b0">Noroozi &amp; Favaro, 2016</xref>) or as a result of tracking in video (<xref ref-type="bibr" rid="b20">Wang &amp; Gupta, 2015</xref>). Alternatively, the network is trained on an artificial task like image rotation prediction (<xref ref-type="bibr" rid="b12">Gidaris et al., 2018</xref>) or even matching images to a noisy target (<xref ref-type="bibr" rid="b3">Bojanowski &amp; Joulin, 2017</xref>). The latter is conceptually related to deep clustering (<xref ref-type="bibr" rid="b4">Caron et al., 2018</xref>), the approach we use in this work, where the network learns targets resulting from unsupervised clustering. It is interesting that in the context of semi-supervised learning, unsupervised pre-training has been recently investigated by <xref ref-type="bibr" rid="b19">Rebuffi et al. (2019)</xref>, with results are consistent with ours. However, the use of unsupervised pre-training in deep active learning remains unexplored.</p></sec><sec><title>PROBLEM FORMULATION AND BACKGROUND</title></sec><sec><title>Problem</title><p>We are given a set X := {x i } i&#8712;I &#8834; X of n examples where I := [n] := {1, . . . , n} and, initially, a collection y 0 := (y i ) i&#8712;L0 of b labels y i &#8712; C for i &#8712; L 0 , where C := [c] is a set of c classes and L 0 &#8834; I a set of indices with |L 0 | = b n. The goal of active learning (AL) (<xref ref-type="bibr" rid="b0">Settles, 2009</xref>) is to train a classifier in cycles, where in cycle j = 0, 1, . . . we use a collection y j of labels for training, and then we acquire (or sample) a new batch S j of indices with |S j | = b to label the corresponding examples for the next cycle j + 1. Let L j := L j&#8722;1 &#8746; S j&#8722;1 &#8834; I be the set of indices of labeled examples in cycle j &#8805; 1 and U j := I \ L j the indices of the unlabeled examples for j &#8805; 0. Then y j := (y i ) i&#8712;Lj are the labels in cycle j and S j &#8834; U j is selected from the unlabeled examples. To keep notation simple, we will refer to a single cycle in the following, dropping subscripts j.</p></sec><sec><title>Classifier learning</title><p>The classifier f &#952; : X &#8594; R c with parameters &#952;, maps new examples to a vector of probabilities per class. Given x &#8712; X , its prediction is the class of maximum probability &#960;(p) := arg max k&#8712;C p k , (1) where p k is the k-th element of vector p := f &#952; (x). As a by-product of learning parameters &#952;, we have access to an embedding function &#966; &#952; : X &#8594; R d , mapping an example x &#8712; X to a feature vector &#966; &#952; (x). For instance, f &#952; may be a linear classifier on top of features obtained by &#966; &#952; .</p><p>In a typical AL scenario, given a set of indices L of labeled examples and labels y, the parameters &#952; of the classifier are learned by minimizing the cost function</p><p>Acquisition. Given the set of indices U of unlabeled examples and the parameters &#952; resulting from training, one typically acquires a new batch by initializing S &#8592; &#8709; and then greedily updating by S &#8592; S &#8746; {a(X, L &#8746; S, U \ S, y; &#952;)} (3) until |S| &#8805; b. Here a is an acquisition (or sampling) function, each time selecting one example from U \ S. For each i &#8712; S, the corresponding example x i is then given as query to an oracle (often a human expert), who returns a label y i to be used in the next cycle.</p></sec><sec><title>Geometry</title><p>Given parameters &#952;, a simple acquisition strategy is to use the geometry of examples in the feature space F &#952; := &#966; &#952; (X ), without considering the classifier. Each example x i is represented by the feature vector &#966; &#952; (x i ) for i &#8712; I. One particular example is the function (<xref ref-type="bibr" rid="b11">Geifman &amp; El-Yaniv, 2017</xref>; <xref ref-type="bibr" rid="b0">Sener &amp; Savarese, 2018</xref>)</p><p>Under review as a conference paper at ICLR 2020 each time selecting the unlabeled example in U that is the most distant to its nearest labeled or previously acquired example in L. Such geometric approaches are inherently related to clustering. For instance, k-means++ (<xref ref-type="bibr" rid="b0">Arthur &amp; Vassilvitskii, 2007</xref>) is a probabilistic version of (4).</p></sec><sec><title>Uncertainty</title><p>A common acquisition strategy that considers the classifier is some measure of uncertainty in its prediction. Given a vector of probabilities p, one such measure is the entropy H(p) := &#8722; c k=1 p k log p k , (5) taking values in [0, log c]. Given parameters &#952;, each example x i is represented by the vector of probabilities f &#952; (x i ) for i &#8712; I. Then, acquisition is defined by a(X, L, U, y; &#952;) := arg max i&#8712;U H(f &#952; (x i )), (6) effectively selecting the b most uncertain unlabeled examples for labeling.</p></sec><sec><title>Pseudo-labels</title><p>It is possible to use more data than the labeled examples while learning. In <xref ref-type="bibr" rid="b20">Wang et al. (2017)</xref> for example, given indices L, U of labeled and unlabeled examples respectively and parameters &#952;, one represents example x i by p i := f &#952; (x i ), selects the most certain unlabeled example&#349; L := {i &#8712; U : H(p i ) &#8804; }, (7) and assigns pseudo-label&#375; i := &#960;(p i ) by (1) for i &#8712;L. The same cost function J defined by (2) can now be used by augmenting L to L &#8746;L and y to (y,&#375;), where&#375; := (&#375; i ) i&#8712;L . This augmentation occurs once per cycle in <xref ref-type="bibr" rid="b20">Wang et al. (2017)</xref>. This is an example of active semi-supervised learning. Transductive label propagation (<xref ref-type="bibr" rid="b19">Zhou et al., 2003a</xref>) refers to graph-based, semi-supervised learning. A nearest neighbor graph of the dataset X is used, represented by a symmetric non-negative n &#215; n adjacency matrix W with zero diagonal. This matrix is symmetrically normalized as W := D &#8722;1/2 W D &#8722;1/2 , where D := diag(W 1) is the degree matrix and 1 is the all-ones vector. The given labels y := (y i ) i&#8712;L are represented by a n &#215; c zero-one matrix Y := &#967;(L, y) where row i is a c-vector that is a one-hot encoding of label y i if example x i is labeled and zero otherwise, &#967;(L, y) ik := 1, i &#8712; L &#8743; y i = k, 0, otherwise (8) for i &#8712; I and k &#8712; C. <xref ref-type="bibr" rid="b19">Zhou et al. (2003a)</xref> define the n &#215; c matrix P := &#951;[h(Y )] 1 , where</p><p>I is the n &#215; n identity matrix, and &#945; &#8712; [0, 1) is a parameter. The i-th row p i of P represents a vector of class probabilities of unlabeled example x i , and a prediction can be made by &#960;(p i ) (1) for i &#8712; U . This method is transductive because it cannot make predictions on previously unseen data without access to the original data X.</p></sec><sec><title>Inductive label propagation</title><p>Although the previous methods do not apply to unseen data by themselves, the predictions made on X can again be used as pseudo-labels to train a classifier. This is done in <xref ref-type="bibr" rid="b14">Iscen et al. (2019)</xref>, applied to semi-supervised learning. Like <xref ref-type="bibr" rid="b20">Wang et al. (2017)</xref>, a pseudo-label is generated for unlabeled example x i as&#375; i := &#960;(p i ) by (1), only now p i is the i-th row of the result P of label propagation according to (9) rather than the classifier output f &#952; (x i ). Unlike <xref ref-type="bibr" rid="b20">Wang et al. (2017)</xref>, all unlabeled examples are pseudo-labeled and an additional cost term J w (X, U,&#375;; &#952;) := i&#8712;U w i (f &#952; (x i ),&#375; i ) applies to those examples, where&#375; := (&#375; i ) i&#8712;U and w i := &#946;(p i ) is a weight reflecting the certainty in the prediction of&#375; i :</p><p>Unlike <xref ref-type="bibr" rid="b20">Wang et al. (2017)</xref>, the graph and the pseudo-labels are updated once per epoch during learning in <xref ref-type="bibr" rid="b14">Iscen et al. (2019)</xref>, where there are no cycles.</p><p>Under review as a conference paper at ICLR 2020 Unsupervised feature learning. Finally, it is possible to train an embedding function in an unsuper- vised fashion. A simple method that does not make any assumption on the nature or structure of the data is <xref ref-type="bibr" rid="b4">Caron et al. (2018)</xref>. Simply put, starting by randomly initialized parameters &#952;, the data &#966; &#952; (X) are clustered by k-means, each example is assigned to the nearest centroid, clusters and assignments are treated as classes C and pseudo-labels&#375; respectively, and learning takes place according to J(X, I,&#375;, &#952;) (2). By updating the parameters &#952;, &#966; &#952; (X) is updated too. The method therefore alternates between clustering/pseudo-labeling and feature learning, typically once per epoch.</p></sec><sec><title>TRAINING THE MODEL ON UNLABELED DATA</title><p>We argue that acquiring examples for labeling is not making the best use of unlabeled data: unlabeled data should be used during model training, appearing in the cost function that is being minimized. We choose two ways of doing so: unsupervised feature learning and semi-supervised learning. As outlined in Algorithm 1, we follow the standard active learning setup, adding unsupervised pre- training at the beginning and replacing supervised learning on L by semi-supervised learning on L &#8746; U at each cycle. The individual components are discussed in more detail below.</p><p>Unsupervised pre-training (PRE) takes place at the beginning of the algorithm. We follow <xref ref-type="bibr" rid="b4">Caron et al. (2018)</xref>, randomly initializing &#952; and then alternating between clustering the features &#966; &#952; (X) by k-means and learning on cluster assignment pseudo-labels&#375; of X according to J(X, I,&#375;, &#952;) (2). The result is a set of parameters &#952; 0 used to initialize the classifier at every cycle. Learning per cycle follows inductive label propagation (<xref ref-type="bibr" rid="b14">Iscen et al., 2019</xref>). This consists of supervised learning followed by alternating label propagation and semi-supervised learning on all examples L &#8746; U at every epoch. The supervised learning (SUP) is performed on the labeled examples L only using labels y, according to J(X, L, y, &#952;) (2), where the parameters &#952; are initialized by &#952; 0 . Label propagation (LP) involves a reciprocal k-nearest neighbor graph on features &#966; &#952; (X) (<xref ref-type="bibr" rid="b14">Iscen et al., 2019</xref>). As in <xref ref-type="bibr" rid="b19">Zhou et al. (2003a)</xref>, the resulting affinity matrix W is normalized as W := D &#8722;1/2 W D &#8722;1/2 . Label propagation is then performed according to P = &#951;[h(Y )] (9), by solving the corresponding linear system using the conjugate gradient (CG) method (<xref ref-type="bibr" rid="b14">Iscen et al., 2019</xref>). The label matrix Y := &#967;(L, y) (8) is defined on the true labeled examples L that remain fixed over epochs but grow over cycles. With p i being the i-th row of P , a pseudo-label&#375; i = &#960;(p i ) (1) and a weight w i = &#946;(p i ) (10) are defined for every i &#8712; U (<xref ref-type="bibr" rid="b14">Iscen et al., 2019</xref>).</p><p>Semi-supervised learning (SEMI) takes place on all examples L &#8746; U = I, where examples in L have true labels y and examples in U pseudo-labels&#375; := (&#375; i ) i&#8712;U . Different than <xref ref-type="bibr" rid="b14">Iscen et al. (2019)</xref>, we minimize the standard cost function J(X, L &#8746; U, (y,&#375;), &#952;) (2), but we do take weights w := (w i ) i&#8712;U into account in mini-batch sampling, 1 -normalized as &#951;[w]. In particular, part of each mini-batch is drawn uniformly at random from L, while the other part is drawn with replacement from the discrete distribution &#951;[w] on U : an example may be drawn more than once per epoch or never.</p></sec><sec><title>Discussion</title><p>The above probabilistic weighting decouples the size of the epoch from n and indeed we experiment with epochs smaller than n, accelerating learning compared to <xref ref-type="bibr" rid="b14">Iscen et al. (2019)</xref>. It is similar to importance sampling, which is typically based on loss values (<xref ref-type="bibr" rid="b15">Katharopoulos &amp; Fleuret, 2017</xref>; Cheng et al., 2018) or predicted class probabilities (<xref ref-type="bibr" rid="b19">Yang et al., 2015</xref>). Acceleration is important as training on all examples is more expensive than just the labeled ones, and is repeated at every cycle. On the contrary, unsupervised pre-trained only occurs once at the beginning.</p><p>The particular choice of components is not important: any unsupervised representation learning could replace <xref ref-type="bibr" rid="b4">Caron et al. (2018)</xref> in line 2 and any semi-supervised learning could replace <xref ref-type="bibr" rid="b14">Iscen et al. (2019)</xref> in lines 4-7 of Algorithm 1. We keep the pipeline as simple as possible, facilitating comparisons with more effective choices in the future.</p></sec><sec><title>INVESTIGATING MANIFOLD SIMILARITY IN THE ACQUISITION FUNCTION</title><p>Label propagation (<xref ref-type="bibr" rid="b19">Zhou et al., 2003a</xref>; <xref ref-type="bibr" rid="b14">Iscen et al., 2019</xref>) is based on the manifold structure of the feature space, as captured by the normalized affinity matrix W. Rather than just using this information for propagating labels to unlabeled examples, can we use it in the acquisition function as well? This is important in interpreting the effect of semi-supervised learning in Algorithm 1: is any gain due to the use of manifold similarity, or to training the model on more data? Joint label propagation (jLP), introduced here, is an attempt to answer these questions. It is an acquisition function similar in nature to the geometric approach (4), with Euclidean distance replaced by manifold similarity. In particular, the n-vector Y 1 c , the row-wise sum of Y = &#967;(L, y) (8), can be expressed as Y 1 c = &#948;(L) &#8712; R n , where &#948;(L) i := 1, i &#8712; L, 0, otherwise (11) for i &#8712; I. Hence, in the terminology of manifold ranking (<xref ref-type="bibr" rid="b19">Zhou et al., 2003b</xref>), vector Y 1 c represents a set of queries, one for each example x i for i &#8712; L, and the i-th element of the n-vector h(Y )1 c in (9) expresses the manifold similarity of x i to the queries for i &#8712; I. Similar to (4), we acquire the example in U that is the least similar to examples in L that are labeled or previously acquired:</p><p>This strategy is only geometric and bears similarities to discriminative active learning (<xref ref-type="bibr" rid="b13">Gissin &amp; Shalev-Shwartz, 2018</xref>), which learns a binary classifier to discriminate labeled from unlabeled examples and acquires examples of least confidence in the "labeled" class.</p></sec><sec><title>EXPERIMENTS</title></sec><sec><title>EXPERIMENTAL SETUP</title></sec><sec><title>Datasets</title><p>We conduct experiments on four datasets that are most often used in deep active learning: MNIST (<xref ref-type="bibr" rid="b19">LeCun et al., 1998</xref>), SVHN (<xref ref-type="bibr" rid="b19">Netzer et al., 2011</xref>), CIFAR-10 and CIFAR-100 (<xref ref-type="bibr" rid="b16">Krizhevsky, 2009</xref>). <xref ref-type="table" rid="tab_0">Table 1</xref> presents statistics of the datasets. Following <xref ref-type="bibr" rid="b0">Tarvainen &amp; Valpola (2017)</xref>; <xref ref-type="bibr" rid="b14">Iscen et al. (2019)</xref>, we augment input images by 4 &#215; 4 random translations and random horizontal flips.</p></sec><sec><title>Networks and training</title><p>For all experiments we use a 13-layer convolutional network used previously in <xref ref-type="bibr" rid="b17">Laine &amp; Aila (2016)</xref>. We train the model from scratch at each active learning cycle, using Under review as a conference paper at ICLR 2020 SGD with momentum of 0.9 for 200 epochs. An initial learning rate of 0.2 is decayed by cosine annealing (<xref ref-type="bibr" rid="b22">Loshchilov &amp; Hutter, 2017</xref>), scheduled to reach zero at 210 epochs. The mini-batch size is 32 for standard training and 128 when SEMI is used, except for MNIST where the size of the mini-batch 10 and 64 with SEMI. All other parameters follow <xref ref-type="bibr" rid="b0">Tarvainen &amp; Valpola (2017)</xref>.</p></sec><sec><title>Unsupervised pre-training</title><p>We use k-means as the clustering algorithm and follow the settings of <xref ref-type="bibr" rid="b4">Caron et al. (2018)</xref>. The model is trained for 250 epochs on the respective datasets.</p></sec><sec><title>Semi-supervised learning</title><p>Following <xref ref-type="bibr" rid="b14">Iscen et al. (2019)</xref>, we construct a reciprocal k-nearest neighbor graph on features &#966; &#952; (X), with k = 50 neighbors and similarity function s(u, v) := [&#251; v] 3 + for u, v &#8712; R d , where&#251; is the 2 -normalized counterpart of u, while &#945; = 0.99 in (9). We follow <xref ref-type="bibr" rid="b14">Iscen et al. (2019)</xref> in splitting mini-batches into two parts: 50 examples (10 for MNIST) are labeled and the remaining pseudo-labeled. For the latter, we draw examples using normalized weights as a discrete distribution. The epoch ends when 1 2 |U | pseudo-labels have been drawn, that is the epoch is 50% compared to <xref ref-type="bibr" rid="b14">Iscen et al. (2019)</xref>. Given that |L| |U | in most cases, the labeled examples are typically repeated more than once.</p></sec><sec><title>Acquisition strategies</title><p>We evaluate our new acquisition strategy jLP along with the following baselines: (a) Random; (b) Uncertainty based on entropy (5); (c) CEAL (<xref ref-type="bibr" rid="b20">Wang et al., 2017</xref>), combining entropy with pseudo-labels (7); (d) the greedy version of CoreSet (4) (<xref ref-type="bibr" rid="b0">Sener &amp; Savarese, 2018</xref>; <xref ref-type="bibr" rid="b11">Geifman &amp; El-Yaniv, 2017</xref>).</p></sec><sec><title>Baselines</title><p>For all acquisition strategies, we show results of the complete Algorithm 1 as well as the the standard baseline, that is without pre-training and only fully supervised on labeled examples L, and unsupervised pre-training (PRE) alone without semi-supervised. In some cases, we show semi-supervised (SEMI) alone. For instance, in the scenario of 100 labels per class, the effect of pre-training is small, especially in the presence of semi-supervised. CEAL (<xref ref-type="bibr" rid="b20">Wang et al., 2017</xref>) is a baseline with its own pseudo-labels, so we do not combine it with semi-supervised. The length of the epoch is fixed for Algorithm 1 and increases with each cycle.</p></sec><sec><title>Label budget and cycles</title><p>We consider three different scenarios, as shown in <xref ref-type="table" rid="tab_0">Table 1</xref>. In the first, we use an initial balanced label set L 0 of 10 labels per class, translating into a total of 100 for CIFAR-10 and SVHN and 1000 for CIFAR-100. We use the same values as label budget b for all cycles. In the second, we use initially 100 labels per class in CIFAR-10 with b = 1000 per cycle; this is not interesting for CIFAR-100 as it results in complete labeling of the training set after 4 cycles. Finally, we investigate the use of one label per class both as the initial set and the label budget, on MNIST, translating to 10 labels per cycle. All experiments are carried out for 5 cycles and repeated 5 times using different initial label sets L 0 . We report average accuracy and standard deviation.</p></sec><sec><title>STANDARD BASELINE RESULTS</title><p>We first evaluate acquisition functions without using any unlabeled data. <xref ref-type="fig" rid="fig_0">Figure 1</xref> presents results on SVHN, CIFAR-10 and CIFAR-100. The differences between acquisition functions are not significant, except when compared to Random. On SVHN, Random appears to be considerably better than the other acquisition functions and worse on CIFAR-10 with b = 1000. All the other acquisition functions give near identical results; in particular, there is no clear winner in the case of 10 labels per class on CIFAR-10 and CIFAR-100 (Figure 1(b) and (d), respectively).</p><p>This confirms similar observations made in <xref ref-type="bibr" rid="b13">Gissin &amp; Shalev-Shwartz (2018)</xref> and <xref ref-type="bibr" rid="b7">Chitta et al. (2019)</xref>. Our jLP is no exception, giving similar results to the other acquisition functions. We study this phenomenon in Appendix B. In summary, we find that while the ranks of examples according to different strategies may be uncorrelated, the resulting predictions of label propagation mostly agree. Even in cases of disagreement, the corresponding examples have small weights, hence their contribution to the cost function is small. Since those predictions are used as pseudo-labels in <xref ref-type="bibr" rid="b14">Iscen et al. (2019)</xref>, this can explain why the performance of the learned model is also similar in the presence of semi-supervised learning.</p></sec><sec><title>THE EFFECT OF UNSUPERVISED PRE-TRAINING</title><p>As shown in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, pre-training can be beneficial. PRE by itself brings substantial gain on SVHN and CIFAR-10 with b = 100, up to 6%, while the improvements on CIFAR-100 are moderate. In addition, numerical results in <xref ref-type="table" rid="tab_1">Table 2</xref> for our acquisition strategy jLP show that PRE is beneficial with or without SEMI in most cases. Pre-training provides a relatively easy and cost-effective improvement. It is performed only once at the beginning of the active learning process. While <xref ref-type="bibr" rid="b4">Caron et al. (2018)</xref> was originally tested on large datasets like ImageNet or YFCC100M, we show that it can be beneficial even on smaller datasets like CIFAR-10 or SVHN.</p></sec><sec><title>THE EFFECT OF SEMI-SUPERVISED LEARNING</title><p><xref ref-type="fig" rid="fig_3">Figure 4</xref> shows results on different datasets and acquisition strategies like <xref ref-type="fig" rid="fig_1">Figure 2</xref>, but including both PRE and PRE + SEMI. For the purpose of reproducibility, numeric results, including average and standard deviation measurements, are given in Appendix A for all cycles and datasets. The combination PRE + SEMI yields a further significant improvement over PRE and the standard baseline, on all acquisition functions and datasets. For instance, on CIFAR-10 with a budget of 100, the most noticeable improvement comes from Random, where the improvement of PRE + SEMI is around 15% over the standard baseline at all cycles. The improvement is around 10% in most other cases, which is by far greater than any potential difference between the acquisition methods. Also, Under review as a conference paper at ICLR 2020 noticeably, in the case of SVHN, Random with PRE + SEMI reaches nearly the fully supervised accuracy after just 2 cycles (300 labeled examples in total).</p><p>The gain from semi-supervised learning is dramatic in the few-labels regime of CIFAR-10 with b = 100. A single cycle with PRE + SEMI achieves the accuracy of 4 cycles of the standard baseline in this case, which translates to a significant reduction of cost for human annotation.</p><p>In <xref ref-type="table" rid="tab_1">Table 2</xref> we present the effect of all four combinations: with/without PRE and with/without SEMI. We focus on our jLP acquisition strategy, which has similar performance as all other strategies and uses manifold similarity just like SEMI. In most cases, PRE improves over SEMI alone by around 2%. The use of PRE appears to be particularly beneficial in the first cycles, while its impact decreases as the model performance improves.</p><p>It is worth noting that CEAL, which makes use of pseudo-labels, has a low performance. This has been observed before (<xref ref-type="bibr" rid="b9">Ducoffe &amp; Precioso, 2018</xref>) and can be attributed to the fact that it is using the same set of pseudo-labels in every epoch. By contrast, pseudo-labels are updated in every epoch in our case.</p></sec><sec><title>LABEL PROPAGATION WITH ONE LABEL PER CLASS</title><p>Since PRE and SEMI have a significant gain in classification accuracy, it is reasonable to attempt even fewer labeled examples than in previous work on active learning. We investigate the extreme case of one label per class using MNIST as a benchmark, that is, label budget at each cycle is equal to the number of classes. <xref ref-type="fig" rid="fig_2">Figure 3</xref> shows results on all acquisition strategies with and without SEMI. As in the previous experiments, there is no consistent winner among the selection strategies alone, and accuracy remain below 80% after 5 cycles (50 labels in total) without SEMI. By contrast, Random with SEMI arrives at 90.89% accuracy after two cycles (20 labeled examples), which is 40% better than without SEMI.</p></sec><sec><title>DISCUSSION</title><p>In this work, we have shown the benefit of using both labeled and unlabeled data during model training in deep active learning for image classification. This leads to a more accurate model while requiring less labeled data, which is in itself one of the main objectives of active learning. We have used two particular choices for unsupervised feature learning and semi-supervised learning as components in our pipeline. There are several state of the art methods that could be used for the same purpose, for instance <xref ref-type="bibr" rid="b0">Tarvainen &amp; Valpola (2017)</xref>; <xref ref-type="bibr" rid="b19">Verma et al. (2019)</xref>; <xref ref-type="bibr" rid="b2">Berthelot et al. (2019)</xref>; <xref ref-type="bibr" rid="b19">Rebuffi et al. (2019)</xref> for semi-supervised learning. Our pipeline is as simple as possible, facilitating comparisons with more effective choices, which can only strengthen our results. While the improvement coming from recent acquisition strategies is marginal in many scenarios, an active learning approach that uses unlabeled data for training and not just acquisition appears to be a very Under review as a conference paper at ICLR 2020 good option for deep network models. Our findings can have an impact on how deep active learning is evaluated in the future. For instance, the relative performance of the random baseline to all other acquisition strategies depends strongly on the label budget, the cycle and the presence of pre-training and semi-supervised learning.</p></sec><sec id="figures"><title>Figures</title><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Datasets used in this paper, including the mini-batch sizes used in training with and without SEMI, acquisition size at each active learning step and the total number of labeled images.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Average accuracy vs. cycle on different setups and acquisition strategies.</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Average accuracy vs. cycle on different setups and acquisition strategies with PRE.</p></caption><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Average accuracy vs. cycle on different setups and acquisition strategies.</p></caption><graphic /></fig><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Ablation study. Evalua- tion of results obtained with Ran- dom while adding PRE and/or SEMI.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Average accuracy vs. cycle on different setups and acquisition strategies: Baseline, PRE and PRE +SEMI. PRE and PRE + SEMI scenarios are represented using different dashed lines as presented in the legend. For reference, the full training accuracy is 96.97% for SVHN, 94.84 % for CIFAR-10 and 76.43 % for CIFAR-100.</p></caption><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>K-means++: the advantages of careful seeding</article-title><source>SODA. Society for Industrial and Applied Mathematics</source><year>2007</year><person-group person-group-type="author"><name><surname>Arthur</surname><given-names>References D</given-names></name><name><surname>Vassilvitskii</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>The power of ensembles for active learning in image classification</article-title><source>CVPR</source><year>2018</year><person-group person-group-type="author"><name><surname>William</surname><given-names>H</given-names></name><name><surname>Beluch</surname><given-names>Tim</given-names></name><name><surname>Genewein</surname><given-names>Andreas</given-names></name><name><surname>N&#252;rnberger</surname><given-names>Jan M</given-names></name><name><surname>K&#246;hler</surname><given-names /></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Mixmatch: A holistic approach to semi-supervised learning</article-title><source>arXiv preprint arXiv:1905.02249</source><year>2019</year><person-group person-group-type="author"><name><surname>Berthelot</surname><given-names>David</given-names></name><name><surname>Carlini</surname><given-names>Nicholas</given-names></name><name><surname>Goodfellow</surname><given-names>Ian</given-names></name><name><surname>Papernot</surname><given-names>Nicolas</given-names></name><name><surname>Oliver</surname><given-names>Avital</given-names></name><name><surname>Raffel</surname><given-names>Colin</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Unsupervised learning by predicting noise</article-title><source>ICML</source><year>2017</year><person-group person-group-type="author"><name><surname>Bojanowski</surname><given-names>Piotr</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Deep clustering for unsupervised learning of visual features</article-title><source>arXiv preprint arXiv:1807.05520</source><year>2018</year><person-group person-group-type="author"><name><surname>Caron</surname><given-names>Mathilde</given-names></name><name><surname>Bojanowski</surname><given-names>Piotr</given-names></name><name><surname>Joulin</surname><given-names>Armand</given-names></name><name><surname>Douze</surname><given-names>Matthijs</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><source>Semi-Supervised Learning</source><year>2006</year><person-group person-group-type="author"><name><surname>Chapelle</surname><given-names>Olivier</given-names></name><name><surname>Scholkopf</surname><given-names>Bernhard</given-names></name><name><surname>Zien</surname><given-names>Alexander</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Revisiting pre-training: An efficient training method for image classification</article-title><source>arXiv preprint arXiv:1811.09347</source><year>2018</year><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Bowen</given-names></name><name><surname>Wei</surname><given-names>Yunchao</given-names></name><name><surname>Shi</surname><given-names>Honghui</given-names></name><name><surname>Chang</surname><given-names>Shiyu</given-names></name><name><surname>Xiong</surname><given-names>Jinjun</given-names></name><name><surname>Huang</surname><given-names>Thomas S</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Large-scale visual active learning with deep probabilistic ensembles</article-title><source>arXiv preprint arXiv:1811.03575</source><year>2019</year><person-group person-group-type="author"><name><surname>Chitta</surname><given-names>Kashyap</given-names></name><name><surname>Jose</surname><given-names>M</given-names></name><name><surname>Alvarez</surname><given-names>Adam</given-names></name><name><surname>Lesnikowski</surname><given-names /></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Unsupervised visual representation learning by context prediction</article-title><source>ICCV</source><year>2015</year><person-group person-group-type="author"><name><surname>Doersch</surname><given-names>Carl</given-names></name><name><surname>Gupta</surname><given-names>Abhinav</given-names></name><name><surname>Efros</surname><given-names>Alexei A</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Adversarial active learning for deep networks: a margin based approach</article-title><source>arXiv preprint arXiv:1802.09841</source><year>2018</year><person-group person-group-type="author"><name><surname>Ducoffe</surname><given-names>Melanie</given-names></name><name><surname>Precioso</surname><given-names>Frederic</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Deep bayesian active learning with image data</article-title><source>arXiv preprint arXiv:1703.02910</source><year>2017</year><person-group person-group-type="author"><name><surname>Gal</surname><given-names>Yarin</given-names></name><name><surname>Islam</surname><given-names>Riashat</given-names></name><name><surname>Ghahramani</surname><given-names>Zoubin</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Deep active learning over the long tail</article-title><source>arXiv preprint arXiv:1711.00941</source><year>2017</year><person-group person-group-type="author"><name><surname>Geifman And Ran El-Yaniv</surname><given-names>Yonatan</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Unsupervised representation learning by predicting image rotations</article-title><source>ICLR</source><year>2018</year><person-group person-group-type="author"><name><surname>Gidaris</surname><given-names>Spyros</given-names></name><name><surname>Singh</surname><given-names>Praveer</given-names></name><name><surname>Komodakis</surname><given-names>Nikos</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><source>Discriminative active learning</source><year>2018</year><person-group person-group-type="author"><name><surname>Gissin</surname><given-names>Daniel</given-names></name><name><surname>Shalev-Shwartz</surname><given-names>Shai</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Label propagation for deep semi-supervised learning</article-title><source>CVPR</source><year>2019</year><person-group person-group-type="author"><name><surname>Iscen</surname><given-names>A</given-names></name><name><surname>Tolias</surname><given-names>G</given-names></name><name><surname>Avrithis</surname><given-names>Y</given-names></name><name><surname>Chum</surname><given-names>O</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Biased importance sampling for deep neural network training</article-title><source>arXiv preprint arXiv:1706.00043</source><year>2017</year><person-group person-group-type="author"><name><surname>Katharopoulos</surname><given-names>Angelos</given-names></name><name><surname>Fleuret</surname><given-names>Fran&#231;ois</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><source>Learning multiple layers of features from tiny images</source><year>2009</year><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>Alex</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Temporal ensembling for semi-supervised learning</article-title><source>arXiv preprint arXiv:1610.02242</source><year>2016</year><person-group person-group-type="author"><name><surname>Laine</surname><given-names>Samuli</given-names></name><name><surname>Aila</surname><given-names>Timo</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Temporal ensembling for semi-supervised learning</article-title><source>ICLR</source><year>2017</year><person-group person-group-type="author"><name><surname>Laine</surname><given-names>Samuli</given-names></name><name><surname>Aila</surname><given-names>Timo</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Gradient-based learning applied to document recognition</article-title><source>Proceedings of the IEEE</source><year>1998</year><volume>86</volume><issue>11</issue><fpage>2278</fpage><lpage>2324</lpage><person-group person-group-type="author"><name><surname>Lecun</surname><given-names>Yann</given-names></name><name><surname>Bottou</surname><given-names>L&#233;on</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name><name><surname>Haffner</surname><given-names>Patrick</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Ascent: Active supervision for semi-supervised learning</article-title><source>IEEE Transactions on Knowledge and Data Engineering</source><year>2019</year><person-group person-group-type="author"><name><surname>Li</surname><given-names>Yanchao</given-names></name><name><surname>Li Wang</surname><given-names>Yong</given-names></name><name><surname>Yu</surname><given-names>Dong-Jun</given-names></name><name><surname>Ning</surname><given-names>Ye</given-names></name><name><surname>Hu</surname><given-names>Peng</given-names></name><name><surname>Zhao</surname><given-names>Ruxin</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><source>International Conference on Modeling Decisions for Artificial Intelligence</source><year>2008</year><fpage>179</fpage><lpage>190</lpage><person-group person-group-type="author"><name><surname>Long</surname><given-names>Jun</given-names></name><name><surname>Yin</surname><given-names>Jianping</given-names></name><name><surname>Zhao</surname><given-names>Wentao</given-names></name><name><surname>Zhu</surname><given-names>En</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Sgdr: Stochastic gradient descent with warm restarts</article-title><source>ICLR</source><year>2017</year><person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>Ilya</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group></element-citation></ref></ref-list></back></article>