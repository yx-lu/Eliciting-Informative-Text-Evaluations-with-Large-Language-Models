Title:
```
None
```
Abstract:
```
Transferring knowledge across tasks to improve data-efficiency is one of the open key challenges in the field of global black-box optimization. Readily available algorithms are typically designed to be universal optimizers and, therefore, often suboptimal for specific tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian opti- mization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to meta-train an acquisition func- tion (AF) on a set of related tasks, the proposed method learns to extract implicit structural information and to exploit it for improved data-efficiency. We present experiments on a simulation-to-real transfer task as well as on several synthetic functions and on two hyperparameter search problems. The results show that our algorithm (1) automatically identifies structural properties of objective functions from available source tasks or simulations, (2) performs favourably in settings with both scarse and abundant source data, and (3) falls back to the performance level of general AFs if no particular structure is present.
```

Figures/Tables Captions:
```
Figure 1: Different levels of the MetaBO framework. Left panel: structure of the training loop for meta-learning neural AFs using RL (PPO). Middle panel: the classical BO loop with a neural AF α t,θ . At test time, there is no difference to classical BO, i.e., x t is given by the arg max of the AF output. During training, the AF corresponds to the RL policy evaluated on an adaptive set ξ t ⊂ D. The outputs are interpreted as logits of a categorical distribution from which the actions a t = x t ∈ ξ t are sampled. This sampling procedure is detailed in the right panel. We indicate by the dotted curve and tiny two-headed arrows that α t,θ is a function defined on the whole domain D which can be evaluated at arbitrary points ξ t,n to form the categorical distribution representing the policy π θ .
Figure 2: Performance on three global optimization benchmark functions with random translations sampled uniformly from [−0.1, 0.1] D and scalings from [0.9, 1.1]. To test TAF's performance, we randomly picked M = 50 source tasks from this function class and evaluated both the ranking-based version (TAF-R-50) and the mixture-of-experts version (TAF-ME-50). We trained MetaBO on the same set of source tasks (MetaBO-50). In contrast to TAF, MetaBO can also be trained without man- ually restricting the set of available source tasks. The corresponding results are labelled "MetaBO". MetaBO outperformed EI by clear margin, especially in early stages of the optimization. After few steps used to identify the specific instance of the objective function, MetaBO also outperformed both flavors of TAF over wide ranges of the optimization budget. Results for TAF-20 can be found in App. A.4, Fig. 12.
Figure 3: Performance on a simulation-to-real task (cf. text). MetaBO and TAF used source data from a cheap numerical simulation. (a) Performance on an extended training set in simulation. (b) Transfer to the hardware depicted in (c), averaged over ten BO runs. MetaBO learned robust neural AFs with very strong optimization performance and online adaption to the target objectives, which reliably yielded stabilizing controllers after less than ten BO iterations while TAF-ME-100, TAF-R-100, and EI explore too heavily. Comparing the results for MetaBO and MetaBO-50 in simulation, we observe that MetaBO benefits from its ability to learn from the whole set of available source data, while TAF's applicability is restricted to a comparably small number of source tasks. We move the results for TAF-50 to App. A.4, Fig. 13.
Figure 4: Performance on two 2D hyperparameter optimization tasks (SVM and AdaBoost). We trained MetaBO on precomputed data for 35 randomly chosen datasets and used the same datasets as source tasks for TAF. The remaining 15 datasets were used for this evaluation. MetaBO learned very data-efficient sampling strategies on both experiments, outperforming the benchmark methods by clear margin. Note that the optimization domain is discrete and therefore tasks can be solved exactly, corresponding to zero regret.
Figure 5: Performance of MetaBO trained on D = 3-dimensional objective functions sampled from a GP prior with RBF kernel (upper row) and Matern-5/2 kernel (lower row) with lengthscales drawn randomly from ∈ [0.05, 0.5]. Panels (a, d) show the performance on these training distributions. As we excluded the x-feature from the neural AF inputs during training, the resulting AFs can be applied to functions of different dimensionalities. We evaluated each AF on D = 4 and D = 5 without retraining MetaBO. We report simple regret w.r.t. the best observed function value, determined separately for each function in the test set.
Table 1: The MetaBO setting in the RL framework.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Global optimization of black-box functions is highly relevant for a wide range of real-world tasks. Examples include the tuning of hyperparameters in machine learning, the identification of control parameters, or the optimization of system designs. Such applications oftentimes require the optimiza- tion of relatively low-dimensional ( 10D) functions where each function evaluation is expensive in either time or cost. Furthermore, there is typically no gradient information available. In this context of data-efficient global black-box optimization, Bayesian optimization (BO) has emerged as a powerful solution (Močkus, 1975; Brochu et al., 2010; Snoek et al., 2012; Shahriari et al., 2016). BO's data efficiency originates from a probabilistic surrogate model which is used to generalize over information from individual data points. This model is typically given by a Gaussian process (GP), whose well-calibrated uncertainty prediction allows for an informed exploration- exploitation trade-off during optimization. The exact manner of performing this trade-off, however, is left to be encoded in an acquisition function (AF). There is a wide range of AFs available in the literature which are designed to yield universal optimization strategies and therefore come with minimal assumptions about the class of target objective functions. To achieve optimal data-efficiency on new instances of previously seen tasks, however, it is crucial to incorporate the information obtained from these tasks into the optimization. Therefore, transfer Published as a conference paper at ICLR 2020 learning is an important and active field of research. Indeed, in many practical applications, op- timizations are repeated numerous times in similar settings, underlining the need for specialized optimizers. Examples include hyperparameter optimization which is repeatedly done for the same machine learning model on varying datasets or the optimization of control parameters for a given system with varying physical configurations. Following recent approaches (Swersky et al., 2013; Feurer et al., 2018; Wistuba et al., 2018), we argue that it is beneficial to perform transfer learning for global black-box optimization in the framework of BO to retain the proven generalization capabilities of its underlying GP surrogate model. To not restrict the expressivity of this model, we propose to implicitly encode the task structure in a specialized AF, i.e., in the optimization strategy. We realize this encoding via a novel method which meta-learns a neural AF, i.e., a neural network representing the AF, on a set of source tasks. The meta-training is performed using reinforcement learning, making the proposed approach applicable to the standard BO setting, where we do not assume access to objective function gradients. Our contributions are (1) a novel transfer learning method allowing the incorporation of implicit structural knowledge about a class of objective functions into the framework of BO through learned neural AFs to increase data-efficiency on new task instances, (2) an automatic and practical meta- learning procedure for training such neural AFs which is fully compatible with the black-box optimization setting, i.e, not requiring objective function gradients, and (3) the demonstration of the efficiency and practical applicability of our approach on a challenging simulation-to-real control task, on two hyperparameter optimization problems, as well as on a set of synthetic functions.

Section Title: RELATED WORK
  RELATED WORK The general idea of improving the performance or convergence speed of a learning system on a given set of tasks through experience on similar tasks is known as learning to learn, meta-learning or transfer learning and has attracted a large amount of interest in the past while remaining an active field of research (Schmidhuber, 1987; Hochreiter et al., 2001; Thrun and Pratt, 1998; Lake et al., 2016). In the context of meta-learning optimization, a large body of literature revolves around learning local optimization strategies. One line of work focuses on learning improved optimizers for the training of neural networks, e.g., by directly learning update rules (Bengio et al., 1991; Runarsson and Jonsson, 2000) or by learning controllers for selecting appropriate step sizes for gradient descent (Daniel et al., 2016). Another direction of research considers the more general setting of replacing the gradient descent update step by neural networks which are trained using either reinforcement learning (Li and Malik, 2016; 2017) or in a supervised fashion (Andrychowicz et al., 2016; Metz et al., 2019). Finn et al. (2017), Nichol et al. (2018), and Flennerhag et al. (2019) propose approaches for initializing machine learning models through meta-learning to be able to solve new learning tasks with few gradient steps. We are currently aware of only one work tackling the problem of meta-learning global black-box optimization (Chen et al., 2017). In contrast to our proposed method, the authors assume access to gradient information and choose a supervised learning approach, representing the optimizer as a recurrent neural network operating on the raw input vectors. Based on statistics of the optimization history accumulated in its memory state, this network directly outputs the next query point. In contrast, we consider transfer learning applications where gradients are typically not available. A number of articles address the problem of increasing BO's data-efficiency via transfer learning, i.e., by incorporating information obtained from similar optimizations on source tasks into the current target task. A range of methods accumulate all available source and target data in a single GP and make the data comparable via a ranking algorithm (Bardenet et al., 2013), standardization or multi-kernel GPs (Yogatama and Mann, 2014), multi-task GPs (Swersky et al., 2013), the GP noise model (Theckel Joy et al., 2016), or by regressing on prediction biases (Shilton et al., 2017). These approaches naturally suffer from the cubic scaling behaviour of GPs, which can be tackled for instance by replacing the GP model, e.g., with Bayesian neural networks with task-specific embedding vectors (Springenberg et al., 2016) or with adaptive Bayesian linear regression with basis functions shared across tasks via a neural network (Perrone et al., 2018). Recently, Garnelo et al. (2018) proposed Neural Processes as another interesting alternative for GPs with improved scaling behavior. Other Published as a conference paper at ICLR 2020 approaches retain the GP surrogate model and combine individual GPs for source and target tasks in an ensemble model with the weights adjusted according to the GP uncertainties (Schilling et al., 2016), dataset similarities (Wistuba et al., 2016), or estimates of the GP generalization performance on the target task (Feurer et al., 2018). Similarly, Golovin et al. (2017) form a stack of GPs by iteratively regressing onto the residuals w.r.t. the most recent source task. In contrast to our proposed method, many of these approaches rely on hand-engineered dataset features to measure the relevance of source data for the target task. Such features have also been used to pick promising initial configurations for BO (Feurer et al., 2015a;b). The method being closest in spirit and capability to our approach is proposed by Wistuba et al. (2018). It is similar to the aforementioned ensemble techniques with the important difference that the source and target GPs are not combined via a surrogate model but via a new AF, the so- called transfer acquisition function (TAF). This AF is defined to be a weighted superposition of the predicted improvements according to the source GPs and the expected improvement according to the target GP. Viewed in this context, our method also combines knowledge from source and target tasks in a new AF which we represent as a neural network. Our weighting of source and target data is implicitly determined in a meta-learning phase and is automatically regulated during the optimization on the target task to adapt online to the specific objective function at hand. Furthermore, our method does not store and evaluate many source GPs because the knowledge from the source datasets is encoded directly in the network weights of the learned AF. This allows our method to incorporate large amounts of source data while the applicability of TAF is restricted to a comparably small number of source tasks.

Section Title: PRELIMINARIES
  PRELIMINARIES We are aiming to find a global optimum x * ∈ arg max x∈D f (x) of some unknown objective function f : D → R on the domain D ⊂ R D . The only means of acquiring information about f is via (possibly noisy) evaluations at points in D. Therefore, at each optimization step t ∈ {1, 2, . . . }, the optimizer has to decide for the iterate x t ∈ D solely based on the optimization history H t ≡ {x i , y i } t−1 i=1 with y i = f (x i ) + . Here, ∼ N 0, σ 2 n denotes independent and identically distributed Gaussian noise. In particular, the optimizer does not have access to gradients of f . To assess the performance of global optimization algorithms, it is natural to use the simple regret R t ≡ f (x * ) − f (x + t ) where x + t is the input location corresponding to the best evaluation found by an algorithm up to and including step t. The proposed method relies on the framework of BO and is trained using reinforcement learning. Therefore, we now shortly introduce these frameworks.

Section Title: Bayesian Optimization
  Bayesian Optimization In Bayesian optimization (BO) (Shahriari et al., 2016), one specifies a prior belief about the objective function f and at each step t builds a probabilistic surrogate model conditioned on the current optimization history H t . Typically, a Gaussian process (GP) (Rasmussen and Williams, 2005) is employed as the surrogate model in which case the resulting posterior belief about f (x) follows a Gaussian distribution with mean µ t (x) ≡ E { f (x) | H t } and variance σ 2 t (x) ≡ V { f (x) | H t }, for which closed-form expressions are available. To determine the next iterate x t based on the belief about f given H t , a sampling strategy is defined in terms of an acquisition function (AF) α t ( · | H t ) : D → R. The AF outputs a score value at each point in D such that the next iterate is defined to be given by x t ∈ arg max x∈D α t ( x | H t ). The strength of the resulting optimizer is largely based upon carefully designing the AF to trade-off exploration of unknown versus exploitation of promising areas in D. There is a wide range of general-purpose AFs available in the literature. Popular choices are probability of improvement (PI) (Kushner, 1964), GP-upper confidence bound (GP-UCB) (Srinivas et al., 2010), and expected improvement (EI) (Močkus, 1975). In our experiments, we will use EI as a not pre-informed baseline AF, so we state its definition here, EI t (x) ≡ E f(x) max f (x) − f (x + t−1 ), 0 H t , (1) and note that it can be written in closed form if f (x) follows a Gaussian distribution. To perform transfer learning in the context of BO, Wistuba et al. (2018) introduced the transfer acqusition framework (TAF) which defines a new AF as a weighted superposition of EI on the target Published as a conference paper at ICLR 2020 task and the predicted improvements on the source tasks, i.e., TAF stores separate GP surrogate models for the source and target tasks, with j ∈ {1, . . . , M } indexing the source tasks and j = M + 1 indexing the target task. Therefore, EI M +1 t denotes EI according to the target GP surrogate model and µ j denotes the mean function of the j-th source GP model. y j,max t denotes the maximum of the mean predictions of the j-th source GP model on the set of iterates {x i } t i=1 . The weights w j ∈ R are determined either based on the predicted variances of the source and target GP surrogate models (TAF-ME) or, alternatively, by a pairwise comparison of the predicted performance ranks of the iterates (TAF-R). Reinforcement Learning Reinforcement learning (RL) allows an agent to learn goal-oriented be- havior via trial-and-error interactions with its environment (Sutton and Barto, 1998). This interaction process is formalized as a Markov decision process: at step t the agent senses the environment's state s t ∈ S and uses a policy π : S → P(A) to determine the next action a t ∈ A. Typically, the agent explores the environment by means of a probabilistic policy, i.e., P(A) denotes the probability measures over A. The environment's response to a t is the next state s t+1 , which is drawn from a probability distribution with density p( s t+1 | s t , a t ). The agent's goal is formulated in terms of a scalar reward r t = r(s t , a t , s t+1 ), which the agent receives together with s t+1 . The agent aims to maximize the expected cumulative discounted future reward η(π) when acting according to π and starting from some state s 0 ∈ S, i.e., η(π) ≡ E π T t=1 γ t−1 r t s 0 . Here, T denotes the episode length and γ ∈ (0, 1] is a discount factor.

Section Title: METABO ALGORITHM
  METABO ALGORITHM We devise a global black-box optimization method that is able to automatically identify and exploit structural properties of a given class of objective functions for improved data-efficiency. We stay within the framework of BO, enabling us to exploit the powerful generalization capabilities of a GP surrogate model. The actual optimization strategy which is informed by this GP is classically encoded in a hand-designed AF. Instead, we meta-train on a set of source tasks to replace this AF by a neural network but retain all other elements of the proven BO-loop (middle panel of  Fig. 1 ). To distinguish the learned AF from a classical AF α t , we call such a network a neural acquisition function and denote it by α t,θ , indicating that it is parametrized by a vector θ. We dub the resulting algorithm MetaBO. Let F be the class of objective functions for which we aim to learn a neural acquisition function α t,θ . For instance, F may be the set of objective functions resulting from different physical configurations of a laboratory experiment or from evaluating the loss function of a machine learning model on different data sets. Often, such objective functions share structure which we aim to exploit for data-efficient optimization on further instances from the same function class. In many relevant cases, it is straightforward to obtain approximations to F, i.e., a set of functions F which capture relevant properties of F but are much cheaper to evaluate (e.g., by using numerical simulations or results from previous hyperparameter optimization tasks (Wistuba et al., 2018)). During an offline meta-training phase, MetaBO makes use of such cheap approximations to identify the implicit structure of F and to adapt θ to obtain a data-efficient optimization strategy customized to F. Typically, the minimal set of inputs to AFs in BO is given by the pointwise GP posterior prediction µ t (x) and σ t (x). To perform transfer learning, the AF has to be able to identify relevant structure shared by the objective functions in F. In our setting, this is achieved via extending this basic set of inputs by additional features which enable the neural AF to evaluate sample locations. Therefore, in addition to the mean µ t (x) and variance σ t (x) at potential sample locations, the neural AF also receives the input location x itself. Furthermore, we add to the set of input features the current Published as a conference paper at ICLR 2020 MetaBO Training Loop Neural AF in the BO loop Policy architecture Noisy evaluation of f , GP update optimization step t and the optimization budget T , as these features can be valuable for adjusting the exploration-exploitation trade-off (Srinivas et al., 2010). Therefore, we define This architecture allows learning a scalable neural AF, as we still base our architecture only on the pointwise GP posterior prediction. Furthermore, neural AFs of this form can be used as a plug-in feature in any state-of-the-art BO framework. In particular, if differentiable activation functions are chosen, a neural AF constitutes a differentiable mapping D → R and standard gradient-based optimization strategies can be used to find its maximum in the BO loop during evaluation. We further emphasize that after the training phase the resulting neural AF is fully defined, i.e., there is no need to calibrate any AF-related hyperparameters.

Section Title: Training Procedure
  Training Procedure In the general BO setting, gradients of F are assumed to be unavailable. This is oftentimes also true for the functions in F , for instance, when F comprises numerical simulations or results from previous optimization runs. Therefore, we resort to RL as the meta- algorithm, as it does not require gradients of the objective functions. Specifically, we use the Proximal Policy Optimization (PPO) algorithm as proposed in Schulman et al. (2017).  Tab. 1  translates the MetaBO-setting into RL parlance. We aim to shape the mapping α t,θ (x) during meta-training in such a way that its maximum location corresponds to a promising sampling location x for optimization. The meta-algorithm PPO explores its state space using a parametrized stochastic policy π θ from which the actions a t = x t are sampled depending on the current state s t , i.e., a t ∼ π θ ( · | s t ). As the meta-algorithm requires access to Published as a conference paper at ICLR 2020 the global information contained in the GP posterior prediction, the state s t at optimization step t formally corresponds to the functions µ t and σ t (together with the aforementioned additional input features to the neural AF). To connect the neural AF α t,θ with the policy π θ and to arrive at a practical implementation, we evaluate µ t and σ t on a discrete set of points ξ t ≡ {ξ t,n } N n=1 ⊂ D and feed these evaluations through the neural AF α t,θ one at a time, yielding one scalar output value α t,θ (ξ t,n ) = α t,θ [µ t (ξ t,n ) , σ t (ξ t,n ) , ξ t,n , t, T ] for each point ξ t,n . These outputs are interpreted as the logits of a categorical distribution, i.e., we arrive at the policy architecture cf.  Fig. 1 , right panel. Therefore, the proposed policy evaluates the same neural acquisition function α t,θ at arbitrarily many input locations ξ t,n and preferably samples actions x t ∈ ξ t with high α t,θ (x t ). This incentivizes the meta-algorithm to adjust θ such that promising locations ξ t,n are attributed high values of α t,θ (ξ t,n ). Calculating a sufficiently fine static set ξ of of evaluation points is challenging for higher dimensional settings. Instead, we build on the approach proposed by Snoek et al. (2012) and continuously adapt ξ = ξ t to the current state of α t,θ . At each step t, α t,θ is first evaluated on a static and relatively coarse Sobol grid (Sobol, 1967) ξ global spanning the whole domain D. Subsequently, local maximizations of α t,θ are started from the k points corresponding to the best evaluations. We denote the resulting set of local maxima by ξ local,t . Finally, we define ξ t ≡ ξ local,t ∪ ξ global . The adaptive local part of this set enables the RL agent to exploit what it has learned so far by picking points which look promising according to the current neural AF while the static global part maintains exploration. We refer the reader to App. B.1 for details. The final characteristics of the neural AF are controlled through the choice of reward function. For the presented experiments we emphasized fast convergence to the optimum by using the negative simple regret as the reward signal, i.e., we set r t ≡ −R t . 1 This choice does not penalize explorative evaluations which do not yield an immediate improvement and additionally serves as a normalization of the functions f ∈ F . We emphasize that the knowledge of the true maximum is only required during training and that cases in which it is not known at training time do not limit the applicability of our method, as a cheap approximation (e.g., by evaluating the function on a coarse grid) can also be utilized. The left panel of  Fig. 1  depicts the resulting training loop graphically. The outer loop corresponds to the RL meta-training iterations, each performing a policy update step π θi → π θi+1 . To approximate the gradients of the PPO loss function, we record a batch of episodes in the inner loop, i.e., a set of (s t , a t , r t )-tuples, by rolling out the current policy π θi . At the beginning of each episode, we draw some function f from the training set F and fix an optimization budget T . In each iteration of the inner loop we determine the adaptive set ξ t and feed the state s t through the policy which yields the action a t = x t . We then evaluate f at x t and use the result to compute the reward r t and to update the optimization history: H t → H t+1 = H t ∪ {x t , y t }. Finally, the GP is conditioned on the updated optimization history H t+1 to obtain the next state s t+1 .

Section Title: EXPERIMENTS
  EXPERIMENTS We trained MetaBO on a wide range of function classes and compared the performance of the resulting neural AFs with the general-purpose AF expected improvement (EI) 2 as well as the transfer acquisition function framework (TAF) which proved to be the current state-of-the-art solution for transfer learning in BO in an extensive experimental study (Wistuba et al., 2018). We tested both the ranking-based version (TAF-R) and the mixture-of-experts version (TAF-ME). We refer the reader to App. A for a more detailed experimental investigation of MetaBO's performance. If not stated differently, we report performance in terms of the median simple regret R t over 100 optimization runs on unseen test functions as a function of the optimization step t together with 30%/70% percentiles (shaded areas). We emphasize that all experiments use the same MetaBO Published as a conference paper at ICLR 2020 hyperparameters, making our method easily applicable in practice. Furthermore, MetaBO does not increase evaluation time considerably compared to standard AFs, cf. App. A.2, Tab. 3. In addition, even the most expensive of our experiments (the simulation-to-real task, due to the simulation in the BO loop) required not more than 10h of training time on a moderately complex architecture (10 CPU workers, 1 GPU), which is fully justified for our intended offline transfer learning use-case. To foster reproducibility, we provide a detailed exposition of the experimental settings in App. B and make the source code of MetaBO available online. 3

Section Title: Global Optimization Benchmark Functions
  Global Optimization Benchmark Functions We evaluated our method on a set of synthetic function classes based on the standard global optimization benchmark functions Branin (D = 2), Goldstein-Price (D = 2), and Hartmann-3 (D = 3) (Picheny et al., 2013). To construct the training set F , we applied translations in [−0.1, 0.1] D as well as scalings in [0.9, 1.1]. As TAF stores and evaluates one source GP for each source task, its applicability is restricted to a relatively small amount of source data. For the evaluations of TAF and MetaBO, we therefore picked a random set of M = 50 source tasks from the continuously parametrized family F of available objective functions and spread these tasks uniformly over the whole range of translations and scalings (MetaBO-50, TAF-R-50, TAF-ME-50). We used N TAF = 100 data points for each source GP of TAF. We also tested both flavors of TAF for M = 20 source tasks (with N TAF = 50) and observed that TAF's performance does not necessarily increase with more source data, rendering the choice of suitable source tasks cumbersome.  Fig. 2  shows the performance on unseen functions drawn randomly from F . To avoid clutter, we move the results for TAF-20 to App. A.4, cf. Fig. 12. MetaBO-50 outperformed EI by large margin, in particular at early stages of the optimization, by making use of the structural knowledge about F acquired during the meta-learning phase. Furthermore, MetaBO- 50 outperformed both flavors of TAF-50 over wide ranges of the optimization budget. This is due to its ability to learn sampling strategies which go beyond a combination of a prior over D and a standard AF (as is the case for TAF). Indeed, note that MetaBO spends some initial non-greedy evaluations to identify specific properties of the target objective function, resulting in much more efficient optimization strategies. We investigate this behaviour further on simple toy experiments and using easily interpretable baseline AFs in App. A.1. We further emphasize that MetaBO does not require the user to manually pick a suitable set of source tasks but that it can naturally learn from the whole set F of available source tasks by randomly picking a new task from F at the beginning of each BO iteration and aggregating this information in the neural AF weights. We also trained this full version of MetaBO (labelled "MetaBO") on the Published as a conference paper at ICLR 2020 global optimization benchmark functions, obtaining performance comparable with MetaBO-50. We demonstrate below that for more complex experiments, such as the simulation-to-real task, MetaBO's ability to learn from the full set of available source tasks is crucial for efficient transfer learning. We also investigate the dependence of MetaBO's performance on the number of source tasks in more detail in App. A.2. As a final test on synthetic functions, we evaluated the neural AFs on objective functions outside of the training distribution. This can give interesting insights into the nature of the problems under consideration. We move the results of this experiment to App. A.3.

Section Title: Simulation-to-Real Task
  Simulation-to-Real Task Sample efficiency is of special interest for the optimization of real world systems. In cases where an approximate model of the system can be simulated, the proposed approach can be used to improve the data-efficiency on the real system. To demonstrate this, we evaluated MetaBO on a 4D simulation-to-real experiment. The task was to stabilize a Furuta pendulum (Furuta et al., 1992) for 5 s around the upper equilibrium position using a linear state-feedback controller. We applied BO to tune the four feedback gains of this controller (Fröhlich et al., 2020). To assess the performance of a given controller, we employed a logarithmic quadratic cost function (Bansal et al., 2017). If the controller was not able to stabilize the system or if the voltage applied to the motor exceeded some safety limit, we added a penalty term proportional to the remaining time the pendulum would have had to be stabilized for successfully completing the task. We emphasize that the cost function is rather sensitive to the control gains, resulting in a challenging black-box optimization problem. To meta-learn the neural AF, we employed a fast numerical simulation based on the nonlinear dynamics equations of the Furuta pendulum which only contained the most basic physical effects. In particular, effects like friction and stiction were not modeled. The training distribution was generated by sampling the physical parameters of this simulation (two lengths, two masses), uniformly on a range of 75% - 125% around the measured parameters of the hardware (Quanser QUBE - Servo 2, 4 Fig. 3(c)). We also used this simulation to generate M = 100 source tasks for TAF (N TAF = 200). Fig. 3(a) shows the performance on objective functions from simulation. Again, MetaBO learned a sophisticated sampling strategy which first identifies the target objective function and adapts its optimization strategy accordingly, resulting in very strong optimization performance. In contrast, TAF's superposition of a prior over D obtained from the source tasks with EI on the target task leads to excessive explorative behaviour. We move further experimental results for TAF-50 to App. A.4, Fig. 13. By comparing the performance of MetaBO and MetaBO-50 in simulation, we find that our archi- tecture's ability to incorporate large amounts of source data is indeed beneficial on this complex optimization problem. The results in App. A.2 underline that this task indeed requires large amounts of source data to be solved efficiently. This is substantiated by the results on the hardware, on which we evaluated the full version of MetaBO and the baseline AFs obtained by training on data from simulation without any changes. Fig. 3(b) shows that MetaBO learned a neural AF which generalizes well from the simulated objectives to the hardware task and was thereby able to rapidly adjust to its specific properties. This resulted in very data-efficient optimization on the target system, consistently yielding stabilizing controllers after less than ten BO iterations. In comparison, the benchmark AFs required many samples to identify promising regions of the search space and therefore did not reliably find stabilizing controllers within the budget of 25 optimization steps. As it provides interesting insights into the nature of the studied problem, we investigate MetaBO's generalization performance to functions outside of the training distribution in App. A.3. We empha- size, however, that the intended use case of our method is on unseen functions drawn from the training distribution. Indeed, by measuring the physical parameters of the hardware system and adjusting the ranges from which the parameters are drawn to generate F according to the measurement uncertainty, the training distribution can be modelled in such a way that the true system parameters lie inside of it with high confidence.

Section Title: Hyperparameter Optimization
  Hyperparameter Optimization We tested MetaBO on two 2D-hyperparameter optimization (HPO) problems for RBF-based SVMs and AdaBoost. As proposed in Wistuba et al. (2018), we used precomputed results of training these models on 50 datasets 5 with 144 parameter configurations (RBF kernel parameter, penalty parameter C) for the SVMs and 108 configurations (number of product terms, number of iterations) for AdaBoost. We randomly split these datasets into 35 source datasets used for training MetaBO as well as for TAF and evaluated the resulting optimization strategies on the remaining 15 datasets. To determine when to stop the meta-training of MetaBO, we performed 7-fold cross validation on the training datasets. We emphasize that MetaBO did not use more source data than TAF in this experiment, underlining again its broad applicability in situations with both scarse and abundant source data. The results ( Fig. 4 ) show that MetaBO learned very data-efficient neural AFs which surpassed EI und TAF on both experiments.

Section Title: General Function Classes
  General Function Classes Finally, we evaluated the performance of MetaBO on function classes without any particular structure except a bounded correlation lengthscale. As there is only little structure present in this function class which could be exploited in the transfer learning setting, it is desirable to obtain neural AFs which fall back at least on the performance level of general-purpose AFs such as EI. We performed two different experiments of this type. For the first experiment, we sampled the objective functions from a GP prior with squared-exponential (RBF) kernel with lengthscales drawn uniformly from ∈ [0.05, 0.5]. 6 For the second experiment, we used a GP prior with Matern-5/2 kernel with the same range of lengthscales. For the latter experiment we also used the Matern-5/2 kernel (in contrast to the RBF kernel used in all other experiments) as the kernel of the GP surrogate model to avoid model mismatch. For both types of function classes we trained MetaBO on D = 3 dimensional tasks and excluded the x-feature to study a dimensionality-agnostic version of MetaBO. Indeed, we evaluated the resulting neural AFs without retraining for dimensionalities D ∈ {3, 4, 5}. The results ( Fig. 5 ) show that MetaBO is capable of learning neural AFs which perform better than or at least on on-par with EI on these general function classes.

Section Title: CONCLUSION AND FUTURE WORK
  CONCLUSION AND FUTURE WORK We introduced MetaBO, a novel method for transfer learning in the framework of BO. Via a flexible meta-learning approach, we inject prior knowledge directly into the optimization strategy of BO using neural AFs. The experiments show that our method consistently outperforms existing methods, for instance in simulation-to-real settings or on hyperparameter search tasks. Our approach is broadly applicable to a wide range of practical problems, covering both the cases of scarse and abundant source data. The resulting neural AFs can represent search strategies which go far beyond the abilities of current approaches which often rely on weighted superpositions of priors over the optimization domain obtained from the source data with standard AFs. In future work, we aim to tackle the multi-task multi-fidelity setting (Valkov et al., 2018), where we expect MetaBO's sample efficiency to be of high impact.

```
