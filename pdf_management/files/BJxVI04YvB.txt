Title:
```
Published as a conference paper at ICLR 2020 PAC CONFIDENCE SETS FOR DEEP NEURAL NET- WORKS VIA CALIBRATED PREDICTION
```
Abstract:
```
We propose an algorithm combining calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees-i.e., the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning prob- lem. 1
```

Figures/Tables Captions:
```
Figure 1: Results on ResNet for ImageNet with n = 20000. Default parameters are = 0.01 and δ = 10 −5 . We plot the median and min/max confidence set sizes. (a) Ablation study; C is "calibrated predictor" (i.e., use fφ ,τ instead of fφ), and D is "direct bound" (i.e., use Theorem 1 instead of the VC generalization bound). (b) Restricted to correctly vs. incorrectly labeled images. (c) Varying . (d) Varying δ. g * (x | x, u) mapping a state-action pair (x, u) to a distribution over states x , and consider a known (and fixed) policy π(u | x) mapping a given state x to a distribution over actions u ∈ U ⊆ R d U . Then, we let f * (x | x) = E π(u|x) [g * (x | u)] denote the (unknown) closed-loop dynamics.
Figure 2: Confidence set sizes for an object tracking benchmark (Wu et al., 2013); we use n = 5, 000, = 0.01, and δ = 10 −5 . (a) Ablation study similar to Figure 3. In (b) and (c), we show how the confidence set sizes produced using our algorithm vary with respect to and δ, respectively.
Figure 3: Results on the dynamics model for the half-cheetah with n = 5000. Default parameters are = 0.01 and δ = 10 −5 . (a) Ablation study; A is "accumulated variance" (i.e., for each t ∈ {1, ..., 20}, useΣ t instead of Σ t = Σ(x t−1 )), and C and D are as for ResNet. We plot the median and min/max confidence set sizes (see Section 3.6), averaged across t ∈ {1, ..., 20}. (b) Same ablations, but with per time step size. We plot the average size of the confidence set for the predicted state x t on step t, as a function of t ∈ {1, ..., 20}. (c) Varying , and (d) varying δ. In (a), we compare to two ablations. The labels C and D are as for ResNet; in addition, A refers to using the accumulated varianceΣ t instead of the one-step predicted variances Σ t = Σ(x t−1 ). Thus, A + C + D is our approach. As before, we omit results for the ablation using the VC generalization bound since n is so small that the bound does not hold for any k for the given and δ. In (b), we show the same ablations over the entire trajectory until t = 20. As can be seen, using the calibrated predictor produces a large gain; these gains are most noticeable in the tails. Using the accumulated confidence produces a smaller, but still significant, gain. In (c) and (d), we show how the sizes vary with and δ, respectively. The trends are similar those for ResNet.
Table 1: ImageNet images with varying ResNet confidence set sizes. The confidence set sizes are on the top. The true label is on the left-hand side. Incorrectly labeled images are boxed in red.
Table 2: Confidence sets of ImageNet images with varying ResNet confidence set sizes. The pre- dicted confidence set is shown to the right of the corresponding input image. The true label is shown in red, and the predicted label is shown with a hat. See Table 5 in Appendix D for more examples.
Table 3: Visualization of confidence sets for the tracking dataset (Wu et al., 2013), including the ground truth bounding box (white), the bounding box predicted by the original neural network (Held et al., 2016) (red), and the bounding box produced using our confidence set predictor (green). We have overapproximated the predicted ellipsoid confidence set with a box. Our bounding box contains the ground truth bounding box with high probability. See Table 9 in Appendix D for more examples.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION A key challenge facing deep neural networks is that they do not produce reliable confidence esti- mates, which are important for applications such as safe reinforcement learning (Berkenkamp et al., 2017), guided exploration (Malik et al., 2019), and active learning (Gal et al., 2017). We consider the setting where the test data follows the same distribution as the training data (i.e., we do not consider adversarial examples designed to fool the network (Szegedy et al., 2014)); even in this setting, confidence estimates produced by deep neural networks are notoriously unreliable (Guo et al., 2017). One intuition for this shortcoming is that unlike traditional supervised learning al- gorithms, deep learning models typically overfit the training data (Zhang et al., 2017). As a con- sequence, the confidence estimates of deep neural networks are flawed even for test data from the training distribution since, by construction, they overestimate the likelihood of the training data. A promising approach to addressing this challenge is temperature scaling (Platt, 1999). This ap- proach takes as input a trained neural network fφ(y | x)-i.e., whose parametersφ have already been fit to a training dataset Z train -which produces unreliable probabilities fφ(y | x). Then, this approach rescales these confidence estimates based on a validation dataset to improve their "calibra- tion". More precisely, this approach fits confidence estimates of the form fφ ,τ (y | x) ∝ exp(τ log fφ(y | x)), where τ ∈ R >0 is a temperature scaling parameter that is fit based on the validation dataset. The goal is to choose τ to minimize calibration error, which roughly speaking measures the degree to which the reported error rate differs from the actual error rate. The key insight is that in the temperature scaling approach, only a single parameter τ is fit to the validation data-thus, unlike fitting the original neural network, the temperature scaling algorithm comes with generalization guarantees based on traditional statistical learning theory. Despite the improved generalization guarantees, these confidence estimates still do not come with theoretical guarantees. We are interested in producing confidence sets that satisfy statistical guar- antees while being as small as possible. Given a test input x ∈ X , a confidence set C T (x) ⊆ Y Published as a conference paper at ICLR 2020 (parameterized by T ∈ R) should contain the true label y for at least a 1 − fraction of cases: Since we are fitting a parameter T to based on Z val , we additionally incur a probability of fail- ure due to the randomness in Z val . In other words, given , δ ∈ R >0 , we aim to obtain probably approximately correct (PAC) confidence sets C T (x) ⊆ Y satisfying the guarantee Indeed, techniques from statistical learning theory (Vapnik, 1999) can be used to do so (Vovk, 2013). There are a number of reasons why confidence sets can be useful. First, they can be used to inform safety critical decision making. For example, consider a doctor who uses prediction tools to help perform diagnosis. Having a confidence set would both help the doctor estimate the confidence of the prediction (i.e., smaller confidence sets imply higher confidence), but also give a sense of the set of possible diagnoses. Second, having a confidence set can be useful for reasoning about safety since they contain the true outcome with high probability. For instance, robots may use a confidence set over predicted trajectories to determine whether it is safe to act with high probability. As a concrete example, consider a self-driving car that uses a deep neural network to predict the path that a pedestrian might take. We require that the self-driving car avoid the pedestrian with high probability, which it can do by avoiding all possible paths in the predicted confidence set.

Section Title: Contributions
  Contributions We propose an algorithm combining calibrated prediction and statistical learning theory to construct PAC confidence sets for deep neural networks (Section 3). We propose instantia- tions of this framework in the settings of classification, regression, and learning models for reinforce- ment learning (Section 3.6). Finally, we evaluate our approach on three benchmarks: ResNet (He et al., 2016) for ImageNet (Russakovsky et al., 2015), a model (Held et al., 2016) learned for a vi- sual object tracking benchmark (Wu et al., 2013), and a probabilistic dynamics model (Chua et al., 2018) learned for the half-cheetah environment (Brockman et al., 2016) (Section 4). Examples of ImageNet images with different sized ResNet confidence sets are shown in  Table 1 . As can be seen, our confidence sets become larger and the images become more challenging to classify. In addition, we show predicted confidence sets for ResNet in  Table 2 , as well as predicted confidence sets for the visual object tracking model in  Table 3 .

Section Title: Related work
  Related work There has been work on constructing confidence sets with theoretical guarantees. Oftentimes, these guarantees are asymptotic rather than finite sample (Steinberger & Leeb, 2016; 2018). Alternatively, there has been work focused on predicting confidence sets with a given ex- pected size (Denis & Hebiri, 2017). More relatedly, there has been recent work on obtaining PAC guarantees. For example, there has been some work specific prediction tasks such as binary classification (Lei, 2014; Wang & Qiao, Published as a conference paper at ICLR 2020 The most closely related work is on conformal prediction (Papadopoulos, 2008; Vovk, 2013). Like our approach, this line of work provides a way to construct confidence sets from a given confidence predictor, and provided PAC guarantees for the validity of these confidence sets. Indeed, with some work, our generalization bound Theorem 1 can be shown to be equivalent to Theorem 1 in Vovk (2013). In contrast to their approach, we proposed to use calibrated prediction to construct confi- dence predictors that can suitably be used with deep neural networks. Furthermore, our approach Published as a conference paper at ICLR 2020 makes explicit the connections to temperature scaling and as well as to generalization bounds from statistical learning theory (Vapnik, 1999). In addition, unlike our paper, they do not explicitly pro- vide an efficient algorithm for constructing confidence sets. Finally, we also propose an extension to the case of learning models for reinforcement learning. Finally, we build on a long line of work on calibrated prediction, which aims to construct "cali- brated" probabilities (Murphy, 1972; DeGroot & Fienberg, 1983; Platt, 1999; Zadrozny & Elkan, 2001; 2002; Naeini et al., 2015; Kuleshov & Liang, 2015). Roughly speaking, probabilities are cal- ibrated if events happen at rates equal to the predicted probabilities. This work has recently been applied to obtaining confidence estimates for deep neural networks (Guo et al., 2017; Kuleshov et al., 2018; Pearce et al., 2018), including for learned models for reinforcement learning (Malik et al., 2019). However, these approaches do not come with PAC guarantees.

Section Title: PAC CONFIDENCE SETS
  PAC CONFIDENCE SETS Our goal is to estimate confidence sets that are as small as possible, while simultaneously ensuring that they are probably approximately correct (PAC) (Valiant, 1984). Essentially, a confidence set is correct if it contains the true label. More precisely, let X be the inputs and Y be the labels, and let D be a distribution over Z = X × Y. A confidence set predictor is a function C : X → 2 Y such that C(x) ⊆ Y is a set of labels; we denote the set of all confidence set predictors by C. For a given example (x, y) ∼ D, we say C is correct if y ∈ C(x). Then, the error of C is Finally, consider an algorithm A that takes as input a validation set Z val ⊆ Z consisting of n i.i.d. samples (x, y) ∼ D, and outputs a confidence set predictorĈ. Given , δ ∈ R >0 , we say that A is probably approximately correct (PAC) if Our goal is to design an algorithm A that satisfies (2) while constructing confidence sets C(x) that are as "small in size" as possible on average. The size of C(x) depends on the domain. For classifi- cation, we consider confidence sets that are arbitrary subsets of labels C(x) ⊆ Y = {1, ..., Y }, and we measure the size by |C(x)| ∈ N-i.e., the number of labels in C(x). For regression, we consider confidence sets that are intervals C(x) = [a, b] ⊆ Y = R, and we measure size by b − a-i.e., the length of the predicted interval. Note that there is an intrinsic tradeoff between satisfying (2) and average size of C(x)-larger confidence sets are more likely to satisfy (2).

Section Title: PAC ALGORITHM FOR CONFIDENCE SET CONSTRUCTION
  PAC ALGORITHM FOR CONFIDENCE SET CONSTRUCTION Our algorithm is formulated in the empirical risk framework. Typically, this framework refers to empirical risk minimization. In our setting, such an algorithm would take as input (i) a parametric family of confidence set predictors C = {C θ | θ ∈ Θ}, where Θ is the parameter space, and (ii) a training set Z val ⊆ Z of n i.i.d. samples (x, y) ∼ D, and output the confidence set predictor Cθ, whereθ minimizes the empirical risk: Here, I[φ] ∈ {0, 1} is the indicator function, and the empirical riskL in an estimate of the confidence set error (1) based on the validation set Z val . However, our algorithm does not minimize the empirical risk. Rather, recall that our goal is to minimize the size of the predicted confidence sets given a PAC constraint on the true risk L(θ) based on the given PAC parameters , δ ∈ R >0 and the number of available validation samples n = |Z val |. Thus, the risk shows up as a constraint in the optimization problem, and the objective is instead to minimize the size of the predicted confidence sets: Published as a conference paper at ICLR 2020 At a high level, the value α = α(n, , δ) ∈ R ≥0 is chosen to enforce the PAC constraint, and is based on generalization bounds from statistical learning theory (Valiant, 1984). Furthermore, following the temperature scaling approach (Platt, 1999), the parameter space Θ is chosen to be as small as possible (in particular, one dimensional) to enable good generalization. Finally, our choice of size metric S follows straightforwardly based on our choice of parameter space. In the remainder of this section, we describe the choices of (i) parameter space Θ, (ii) size metric S(θ), and (iii) confidence level α(n, , δ) in more detail, as well as how to solve (3) given these choices.

Section Title: CHOICE OF PARAMETER SPACE Θ
  CHOICE OF PARAMETER SPACE Θ

Section Title: Probability forecasters
  Probability forecasters Our construction of the parameteric family of confidence set predictors C θ assumes given a probability forecaster f : X → P Y , where P Y is a space of probability distributions over Y. Given such an f , we use f (y | x) to denote the probability of label y under distribution f (x). Intuitively, f (y | x) should be the probability (or probability density) that y is the true label for a given input x-i.e., f (y | x) ≈ P (X,Y )∼D [Y = y | X = x]. For example, in classification, we can choose P Y to be the space of categorical distributions over Y, and f may be a neural network whose last layer is a softmax layer with |Y| outputs. Then, f (y | x) = f (x) y . Alternatively, in regression, we can choose P Y to be the space of Gaussian distributions, and f may be a neural network whose last layer outputs the values (µ, σ) ∈ R × R >0 of a Gaussian distribution. Then, f (y | x) = N (x; µ(x), σ(x) 2 ), where (µ(x), σ(x)) = f (x), and N (·; µ, σ 2 ) is the Gaussian density function with mean µ and variance σ 2 .

Section Title: Training a probability forecaster
  Training a probability forecaster To train a probability forecaster, we use a standard approach to calibrated prediction that combines maximum likelihood estimation with temperature scaling. 2 First, we consider a parametric model family F = {f φ | φ ∈ Φ}, where Φ is the parameter space. Note that Φ can be high-dimensional-e.g., the weights of a neural network model. Given a training set Z train ⊆ Z of m i.i.d. samples (x, y) ∼ D, the maximum likelihood estimate (MLE) of φ iŝ We could now use fφ as the probability forecaster. However, the problem with directly usingφ is that becauseφ may be high-dimensional, it often overfits the training data Z train . Thus, the probabilities are typically overconfident compared to what they should be. To reduce their confidence, we use the temperature scaling approach to calibrate the predicted probabilities (Platt, 1999; Guo et al., 2017). Intuitively, this approach is to train an MLE estimate using exactly the same approach used to trainφ, but using a single new parameter τ ∈ R >0 . The key idea is that this time, the model family is based on the parametersφ from (4). In other words, the "shape" of the probabilities forecast by fφ are preserved, but their exact values are shifted. More precisely, consider the model family F = {fφ ,τ | τ ∈ R >0 }, where Then, we have the following MLE for τ : Note thatτ is estimated based on a second training set Z train . Because we are only fitting a single parameter, this training set can be much smaller than the training set Z train used to fitφ.

Section Title: Parametric family of confidence set predictors
  Parametric family of confidence set predictors In other words, C T (x) is the set of y with high probability given x according to f . Considering this scalar parameter space, we denote the minimum of (3) byT .

Section Title: CHOICE OF SIZE METRIC S(T )
  CHOICE OF SIZE METRIC S(T ) To choose the size metric S(T ), we note that for our chosen parametric family of confidence set predictors, smaller values correspond to uniformly smaller confidence sets-i.e., T ≤ T ⇒ ∀x, C T (x) ⊆ C T (x). Thus, we can simply choose the size metric to be This choice minimizes the size of the confidence sets produced by our algorithm.

Section Title: CHOICE OF CONFIDENCE LEVEL α(n, , δ)
  CHOICE OF CONFIDENCE LEVEL α(n, , δ) Naive approach based on VC generalization bound. A naive approach to choosing α(n, , δ) is to do so based on the VC dimension generalization bound (Vapnik, 1999). It is not hard to show that the problem of estimatingT is equivalent to a binary classification problem, and that the VC dimension of Θ for this problem is 1. Thus, the VC dimension bound implies that for all T ∈ Θ, The details of this equivalence are given in Appendix B.2. Then, suppose we choose With this choice, for the solutionT of (3) with α = α(n, , δ), the constraint in (3) ensures that L(CT ; Z val ) ≤ α(n, , δ). Together with the VC generalization bound (7), we have P Zval∼D n L(CT ) > < δ, which is exactly desired the PAC constraint on our predicted confidence sets.

Section Title: Direct generalization bound
  Direct generalization bound In fact, we can get better choices of α by directly bounding general- ization error. For instance, in the realizable setting (i.e., we always haveL(CT ; Z val ) = 0), we can get rates of n =Õ(1/ ) instead of n =Õ(1/ 2 ) (Kearns & Vazirani, 1994); see Appendix A.2 for details. We can achieve these rates by choosing α = 0, but then, the PAC guarantees we obtain may actually be stronger than desired (i.e., for < ). Intuitively, we can directly prove a bound that interpolates between the realizable setting and the VC generalization bound-in particular: Theorem 1. For any ∈ [0, 1], n ∈ N >0 , and k ∈ {0, 1, ..., n}, we have P Z val ∼D n L(CT ) > ≤ k i=0 n i i (1 − ) n−i , whereT is the solution to (3) with α = k/n. 3 We give a proof in Appendix B.2. Based on Theorem 1, we can choose

Section Title: THEORETICAL GUARANTEES
  THEORETICAL GUARANTEES We have the following guarantee, which follows straightforwardly from Theorem 1: Corollary 1. LetT be the solution to (3) for α = α(n, , δ) chosen according to (8). Then, we have In other words, our algorithm is probably approximately correct.

Section Title: PRACTICAL IMPLEMENTATION
  PRACTICAL IMPLEMENTATION Our algorithm for estimating a confidence set predictor CT is summarized in Algorithm 1. The algorithm solves the optimization problem (3) using the choices of Θ, S(T ), and α(n, , δ) described in the preceding sections. There are two key implementation details that we describe here. Computing α(n, , δ). To compute α(n, , δ), we need to solve (8). A straightforward approach is to enumerate all possible choices of k ∈ {0, 1, ..., n}. There are two optimizations. First, the objective is monotone increasing in k, so we can enumerate k in ascending order until the constraint no longer holds. Second, rather than re-compute the left-hand side of the constraint k i=0 n i i (1 − ) n−i , we can accumulate the sum as we iterate over k. We can also incrementally compute n i , i , and (1 − ) n−i . For numerical stability, we perform these computations in log space. Solving (3). To solve (3), note that the constraint in (3) is equivalent to Also, note that k * = n · α(n, , δ) is an integer due to the definition of α(n, , δ) in (8). Thus, we can interpret (9) as saying that E(x, y; T ) = 1 for at most k * of the points (x, y) ∈ Z val . In addition, note that E(x, y; T ) decreases monotonically as fφ ,τ (y | x) becomes larger. Thus, we can sort the points (x, y) ∈ Z val in ascending order of fφ ,τ (y | x), and require that only the first k * points (x, y) in this list satisfy E(x, y; T ) = 1. In particular, letting (x k * +1 , y k * +1 ) be the (k * +1)st point, (9) is equivalent to In other words, this constraint says that T must satisfy y k * +1 ∈ C T (x k * +1 ). Finally, the solutionT to (3) is the smallest T that satisfies (10), which is the T that makes (10) hold with equality-i.e., We have assumed fφ ,τ (y k * +1 | x k * +1 ) > fφ ,τ (y k * | x k * ); if not, we decrement k * until this holds.

Section Title: PROBABILITY FORECASTERS FOR SPECIFIC TASKS
  PROBABILITY FORECASTERS FOR SPECIFIC TASKS We briefly discuss the architectures we use for probability forecasters for various tasks. We give details, including how we measure the sizes of predicted confidence sets C T (x), in Appendix C. We consider three tasks: classification, regression, and model-based reinforcement learning. For classification, we use the standard approach of using a soft-max layer to predict label probabilities f (y | x). For regression, we also use a standard approach where the neural network predicts both the mean µ(x) and covariance Σ(x) of a Gaussian distribution N (µ(x), Σ(x)); then, f (y | x) = N (y; µ(x), Σ(x)) is the probability density of y according to this Gaussian distribution. Finally, for model-based reinforcement learning, our goal is to construct confidence sets over trajectories predicted using a learned model of the dynamics. We consider unknown dynamics Published as a conference paper at ICLR 2020 Next, we consider a forecaster f (x | x) ≈ f * (x | x) of the form f (x | x) = N (x ; µ(x), Σ(x)), and our goal is to construct confidence sets for the predictions of f . However, we want to do so for not just for one-step predictions, but for predictions over a time horizon H ∈ N. In particular, given initial state x 0 ∈ X , we can sample x * 1:H = (x 1 , ..., x H ) ∼ f * by letting x * 0 = x 0 and sequentially sampling x * t+1 ∼ f ( · | x * t ) for each t ∈ {0, 1, ..., H − 1}. Then, our goal is to construct a confidence set that contains x * 1:H ∈ X H with high probability (over both the randomness in an initial state distribution x 0 ∼ d 0 and the randomness in f * ). To do so, we construct and use a forecasterf (x 1:H | x 0 ) based on f . In principle, this task is a special case of multivariate regression, where the inputs are X (i.e., the initial state x 0 ) and the outputs are Y = X H (i.e., a predicted trajectory x 1:H ). However, the variance Σ(x) predicted by our probability forecaster is only for a single step, and does not take into account the fact that x is itself uncertain. Thus, we use a simple heuristic where we accumulate variances over time. More precisely, we construct (i) the predicted meanx 1:H = (x 1 , ...,x H ) byx 0 = x 0 andx t+1 = µ(x t ) for t ∈ {0, 1, ..., H − 1}, and (ii) the predicted variancesΣ 1:H = (Σ 1 , ...,Σ H ) bỹ We use a probability forecasterf (x 1:H | x 0 ) = N (x 1:H ;x 1:H ,Σ 1:H ) to construct confidence sets.

Section Title: EXPERIMENTS
  EXPERIMENTS We describe our experiments on ImageNet (a classification task), a visual object tracking benchmark (a regression task), and the half-cheetah environment (a model-based reinforcement learning task). We give additional results in Appendix D.

Section Title: ResNet for ImageNet
  ResNet for ImageNet We use our algorithm to compute confidence sets for ResNet (He et al., 2016) on ImageNet (Russakovsky et al., 2015), for = 0.01, δ = 10 −5 , and n = 20000 validation images. We show the results in  Figure 1 . In (a), we compare our approach to an ablation. In particular, C refers to performing an initial temperature scaling step to calibrate the neural network predictor (i.e., using fφ instead of fφ ,τ ), and (ii) D refers to using Theorem 1 instead of the VC generalization bound. Thus, C + D refers to our approach. As can be seen, using calibrated predictor produces a noticeable reduction in the maximum confidence set size. We also compared to the ablation C-i.e., using the VC generalization bound. However, we were unable to obtain valid confidence sets for our choice of and δ-i.e., (3) is infeasible. That is, using Theorem 1 outperforms using the VC generalization bound since the VC bound is too loose to satisfy the PAC criterion for our choice of parameters. In addition, in Table 6 in Appendix D, we show results for larger choices of and δ; these results show that our approach substantially outperforms the ablation based on the VC bound even when the VC bound produces valid confidence sets. In (b), we show the confidence set sizes for images correctly vs. incorrectly labeled by ResNet. As expected, the sizes are substantially larger for incorrectly labeled images. Finally, in (c) and (d), we show how the sizes vary with and δ, respectively. As expected, the dependence on is much more pronounced (note that δ is log-scale).

Section Title: Visual object tracking
  Visual object tracking We apply our confidence set prediction algorithm to a 2D visual single- object tracking task, which is a multivariate regression problem. Specifically, the input space X consists of the previous image, the previous bounding box (in R 4 ), and the current image. The output space Y = R 4 is a current bounding box. We use the regression-based tracker from Held et al. (2016), and retrain the regressor neural network to predict the mean and variance of a Gaussian distribution. More precisely, our object tracking model predicts the mean and variance of each bounding box parameter-i.e., (x min , y min , x max , y max ). Given this bounding box forecaster fφ, we calibrate and estimate a confidence set predictor as described in Section 3.6. We use the visual object tracking benchmark from Wu et al. (2013) to train and evaluate our con- fidence set predictor. This benchmark consists of 99 video sequences labeled with ground truth bounding boxes. We randomly split these sequences to form the training set for calibration, valida- tion set for confidence set estimation, and test set for evaluation. For each sequence, a pair of two adjacent frames constitute a single example. Our training dataset contains 20,882 labeled examples, each consisting of of a pair of consecutive images and ground truth bounding boxes. The validation set for confidence set estimation and test set contain 22,761 and 22,761 labeled examples, respec- tively.  Figure 2  shows the sizes of the predicted confidence sets; the sizes are measured as described in Section 3.6 for regression tasks. As for ResNet, we omit results for the VC bound ablation since n is too small to get a bound. The trends are similar to the ones for ResNet.

Section Title: Half-cheetah
  Half-cheetah We use our algorithm to compute confidence sets for a probabilistic neural network dynamics model (Chua et al., 2018) for the half-cheetah environment (Brockman et al., 2016), for = 0.01, δ = 10 −5 , H = 20 time steps, and n = 5000 validation rollouts. When using temperature scaling to calibrate fφ to obtain fφ ,τ , we calibrate each dimension of time steps independently (i.e., we fit H parameters, where H is time horizon). We show the results in  Figure 3 .

Section Title: CONCLUSION
  CONCLUSION We have proposed an algorithm for constructing PAC confidence sets for deep neural networks. Our approach leverages statistical learning theory to obtain theoretical guarantees on the predicted confidence sets. These confidence sets quantify the uncertainty of deep neural networks. For in- stance, they can be used to inform safety-critical decision-making, and to ensure safety with high- probability in robotics control settings that leverage deep neural networks for perception. Future work includes extending these results to more complex tasks (e.g., structured prediction), and han- dling covariate shift (e.g., to handle policy updates in reinforcement learning).
  The theorem statement relies on additional standard technical conditions; see Appendix B.1.

```
