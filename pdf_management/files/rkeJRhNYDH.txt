Title:
```
Published as a conference paper at ICLR 2020 TABFACT: A LARGE-SCALE DATASET FOR TABLE- BASED FACT VERIFICATION
```
Abstract:
```
The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natu- ral language sentences and documents, news, etc), while verification under struc- tured evidence, such as tables, graphs, and databases, remains under-explored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural lan- guage statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different mod- els: Table-BERT and Latent Program Algorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. LPA parses statements into programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far be- hind human performance. We also perform a comprehensive analysis to demon- strate great future opportunities. The data and code of the dataset are provided in https://github.com/wenhuchen/Table-Fact-Checking. 1 https://www.wikipedia.org/
```

Figures/Tables Captions:
```
Figure 1: Examples from the TABFACT dataset. The top table contains the semi-structured knowl- edge facts with caption "United...". The left and right boxes below provide several entailed and refuted statements. The error parts are highlighted with red font.
Figure 2: Proportion of different higher-order operations from the simple/complex channels.
Figure 3: The program synthesis procedure for the table in Figure 1. We link the entity (e.g. demo- cratic, republican), and then composite functions on the fly to return the values from the table.
Figure 4: The diagram of Table-BERT with horizontal scan, two different linearizations are depicted.
Figure 5: The two uniqueness of Table-based fact verification against standard QA problems.
Table 1: Basic statistics of the data collected from the simple/complex channel and the division of Train/Val/Test Split in the dataset, where "Len" denotes the averaged sentence length.
Table 2: The results of different models, the numbers are in percentage. T+F means table followed by fact, while F+T means fact followed by table. NSM is modified from Liang et al. (2017).
Table Question Answering: Another line of research closely related to our task is the table-based
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Verifying whether a textual hypothesis is entailed or refuted by the given evidence is a fundamental problem in natural language understanding ( Katz & Fodor, 1963 ;  Van Benthem et al., 2008 ). It can benefit many downstream applications like misinformation detection, fake news detection, etc. Recently, the first-ever end-to-end fact-checking system has been designed and proposed in  Hassan et al. (2017) . The verification problem has been extensively studied under different natural language tasks such as recognizing textual entailment (RTE) ( Dagan et al., 2005 ), natural language inference (NLI) ( Bowman et al., 2015 ), claim verification ( Popat et al., 2017 ;  Hanselowski et al., 2018 ;  Thorne et al., 2018 ) and multimodal language reasoning (NLVR/NLVR2) ( Suhr et al., 2017 ; 2019). RTE and NLI view a premise sentence as the evidence, claim verification views passage collection like Wikipedia 1 as the evidence, NLVR/NLVR2 views images as the evidence. These problems have been previously addressed using a variety of techniques including logic rules, knowledge bases, and neural networks. Recently large-scale pre-trained language models ( Devlin et al., 2019 ;  Peters et al., 2018 ;  Yang et al., 2019 ;  Liu et al., 2019 ) have surged to dominate the other algorithms to approach human performance on several textual entailment tasks ( Wang et al., 2018 ; 2019). However, existing studies are restricted to dealing with unstructured text as the evidence, which would not generalize to the cases where the evidence has a highly structured format. Since such structured evidence (graphs, tables, or databases) are also ubiquitous in real-world applications like Published as a conference paper at ICLR 2020 District United States House of Representatives Elections, 1972 Entailed Statement Refuted Statement database systems, dialog systems, commercial management systems, social networks, etc, we argue that the fact verification under structured evidence forms is an equivalently important yet under- explored problem. Therefore, in this paper, we are specifically interested in studying fact verification with semi-structured Wikipedia tables ( Bhagavatula et al., 2013 ) 2 as evidence owing to its structured and ubiquitous nature ( Jauhar et al., 2016 ;  Zhong et al., 2017 ;  Pasupat & Liang, 2015 ). To this end, we introduce a large-scale dataset called TABFACT, which consists of 118K manually annotated statements with regard to 16K Wikipedia tables, their relations are classified as ENTAILED and REFUTED 3 . The entailed and refuted statements are both annotated by human workers. With some examples in  Figure 1 , we can clearly observe that unlike the previous verification related problems, TABFACT combines two different forms of reasoning in the statements, (i) Linguistic Reasoning: the verification requires semantic-level understanding. For example, "John J. Mcfall failed to be re-elected though being unopposed." requires understanding over the phrase "lost renomination ..." in the table to correctly classify the entailment relation. Unlike the existing QA datasets ( Zhong et al., 2017 ;  Pasupat & Liang, 2015 ), where the linguistic reasoning is dominated by paraphrasing, TABFACT requires more linguistic inference or common sense. (ii) Symbolic Reasoning: the verifi- cation requires symbolic execution on the table structure. For example, the phrase "There are three Democrats incumbents" requires both condition operation (where condition) and arithmetic oper- ation (count). Unlike question answering, a statement could contain compound facts, all of these facts need to be verified to predict the verdict. For example, the "There are ..." in  Figure 1  requires verifying three QA pairs (total count=5, democratic count=2, republic count=3). The two forms of reasoning are interleaved across the statements making it challenging for existing models. In this paper, we particularly propose two approaches to deal with such mixed-reasoning challenge: (i) Table-BERT, this model views the verification task completely as an NLI problem by linearizing a table as a premise sentence p, and applies state-of-the-art language understanding pre-trained model to encode both the table and statements h into distributed representation for classification. This model excels at linguistic reasoning like paraphrasing and inference but lacks symbolic reasoning skills. (ii) Latent Program Algorithm, this model applies lexical matching to find linked entities and triggers to filter pre-defined APIs (e.g. argmax, argmin, count, etc). We adopt bread-first-search with memorization to construct the potential program candidates, a discriminator is further utilized to select the most "consistent" latent programs. This model excels at the symbolic reasoning aspects by executing database queries, which also provides better interpretability by laying out the decision rationale. We perform extensive experiments to investigate their performances: the best-achieved accuracy of both models are reasonable, but far below human performance. Thus, we believe that the proposed table-based fact verification task can serve as an important new benchmark towards the goal of building powerful AI that can reason over both soft linguistic form and hard symbolic forms. To facilitate future research, we released all the data, code with the intermediate results.

Section Title: TABLE FACT VERIFICATION DATASET
  TABLE FACT VERIFICATION DATASET First, we follow the previous Table-based Q&A datasets ( Pasupat & Liang, 2015 ;  Zhong et al., 2017 ) to extract web tables ( Bhagavatula et al., 2013 ) with captions from WikiTables 4 . Here we filter out overly complicated and huge tables (e.g. multirows, multicolumns, latex symbol) and obtain 18K relatively clean tables with less than 50 rows and 10 columns. For crowd-sourcing jobs, we follow the human subject research protocols 5 to pay Amazon Mechani- cal Turk 6 workers from the native English-speaking countries "US, GB, NZ, CA, AU" with approval rates higher than 95% and more than 500 accepted HITs. Following WikiTableQuestion ( Pasupat & Liang, 2015 ), we provide the annotators with the corresponding table captions to help them better understand the background. To ensure the annotation quality, we develop a pipeline of "positive two-channel annotation" → "negative statement rewriting" → "verification", as described below.

Section Title: POSITIVE TWO-CHANNEL COLLECTION & NEGATIVE REWRITING STRATEGY
  POSITIVE TWO-CHANNEL COLLECTION & NEGATIVE REWRITING STRATEGY To harvest statements of different difficulty levels, we design a two-channel collection process: Low-Reward Simple Channel: the workers are paid 0.45 USD for annotating one Human Intel- ligent Task (HIT) that requires writing five statements. The workers are encouraged to produce plain statements meeting the requirements: (i) corresponding to a single row/record in the table with unary fact without involving compound logical inference. (ii) mention the cell values without dra- matic modification or paraphrasing. The average annotation time of a HIT is 4.2 min. High-Reward Complex Channel: the workers are paid 0.75 USD for annotating a HIT (five state- ments). They are guided to produce more sophisticated statements to meet the requirements: (i) involving multiple rows in the tables with higher-order semantics like argmax, argmin, count, differ- ence, average, summarize, etc. (ii) rephrase the table records to involve more semantic understand- ing. The average annotation time of a HIT is 6.8 min. The data obtained from the complex channel are harder in terms of both linguistic and symbolic reasoning, the goal of the two-channel split is to help us understand the proposed models can reach under different levels of difficulty. As suggested in ( Zellers et al., 2018 ), there might be annotation artifacts and conditional stylistic patterns such as length and word-preference biases, which can allow shallow models (e.g. bag-of- words) to obtain artificially high performance. Therefore, we design a negative rewriting strategy to minimize such linguistic cues or patterns. Instead of letting the annotators write negative statements from scratch, we let them rewrite the collected entailed statements. During the annotation, the workers are explicitly guided to modify the words, phrases or sentence structures but retain the sentence style/length to prevent artificial cues. We disallow naive negations by adding "not, never, etc" to revert the statement polarity in case of obvious linguistic patterns.

Section Title: QUALITY CONTROL
  QUALITY CONTROL To control the quality of the annotation process, we review a randomly sampled statement from each HIT to decide whether the whole annotation job should be rejected during the annotation process. Specifically, a HIT must satisfy the following criteria to be accepted: (i) the statements should contain neither typos nor grammatical errors. (ii) the statements do not contain vague claims like might, few, etc. (iii) the claims should be explicitly supported or contradicted by the table without requiring the additional knowledge, no middle ground is permitted. After the data collection, we re-distribute all the annotated samples to further filter erroneous statements, the workers are paid 0.05 USD per statement to decide whether the statement should be rejected. The criteria we apply are similar: no ambiguity, no typos, explicitly supported or contradictory. Through the post-filtering process, roughly 18% entailed and 27% refuted instances are further abandoned due to poor quality.

Section Title: DATASET STATISTICS
  DATASET STATISTICS Inter-Annotator Agreement: After the data collection pipeline, we merged the instances from two different channels to obtain a diverse yet clean dataset for table-based fact verification. We sample 1000 annotated (table, statement) pairs and re-distribute each to 5 individual workers to re-label them as either ENTAILED or REFUTED. We follow the previous works ( Thorne et al., 2018 ;  Bowman et al., 2015 ) to adopt the Fleiss Kappa ( Fleiss, 1971 ) as an indicator, where Fleiss κ =p c −pe 1−pe is computed from from the observed agreementp c and the agreement by chancep e . We obtain a Fleiss κ = 0.75, which indicates strong inter-annotator agreement and good-quality.

Section Title: Dataset Statistics
  Dataset Statistics As shown in  Table 1 , the amount of data harvested via the complex channel slightly outnumbers the simple channel, the averaged length of both the positive and negative sam- ples are indistinguishable. More specifically, to analyze to which extent the higher-order operations are included in two channels, we group the common higher-order operations into 8 different cate- gories. As shown in  Figure 2 , we sample 200 sentences from two different channels to visualize their distribution. We can see that the complex channel overwhelms the simple channel in terms of the higher-order logic, among which, count and superlatives are the most frequent. We split the whole data roughly with 8:1:1 into train, validation 7 , and test splits and shows their statistics in  Table 1 . Each table with an average of 14 rows and 5-6 columns corresponds to 2-20 different statements, while each cell has an average of 2.1 words. In the training split, the positive instances slightly outnumber the negative instances, while the validation and test split both have rather bal- anced distributions over positive and negative instances.

Section Title: MODELS
  MODELS With the collected dataset, we now formally define the table-based fact verification task: the dataset is comprised of triple instances (T, S, L) consisting of a table T, a natural language statement S = s 1 , · · · , s n and a verification label L ∈ {0, 1}. The table T = {T i,j |i ≤ R T , j ≤ C T } has R T rows and C T columns with the T ij being the content in the (i, j)-th cell. T ij could be a word, a number, a phrase, or even a natural language sentence. The statement S describes a fact to be verified against the content in the table T. If it is entailed by T, then L = 1, otherwise the label L = 0.  Figure 1  shows some entailed and refuted examples. During training, the model and the learning algorithm are presented with K instances like (T, S, L) K k=1 from the training split. In the testing stage, the model is presented with (T, S) K k=1 and supposed to predict the label asL. We measure the performance by the prediction accuracy Acc = 1 K K 1 I(L k = L k ) on the test set. Before building the model, we first perform entity linking to detect all the entities in the statements. Briefly, we first lemmatize the words and search for the longest sub-string matching pairs between statements and table cells/captions, where the matched phrases are denoted as the linked entities. To focus on statement verification against the table, we do not feed the caption to the model and simply Published as a conference paper at ICLR 2020 mask the phrases in the statements which link to the caption with placeholders. The details of the entity linker are listed in the Appendix. We describe our two proposed models as follows.

Section Title: LATENT PROGRAM ALGORITHM (LPA)
  LATENT PROGRAM ALGORITHM (LPA) In this approach, we formulate the table fact verification as a program synthesis problem, where the latent program algorithm is not given in TABFACT. Thus, it can be seen as a weakly supervised learning problem as discussed in  Liang et al. (2017) ;  Lao et al. (2011) . Under such a setting, we propose to break down the verification into two stages: (i) latent program search, (ii) discriminator ranking. In the first program synthesis step, we aim to parse the statement into programs to represent its semantics. We define the plausible API set to include roughly 50 different functions like min, max, count, average, filter, and and realize their interpreter with Python-Pandas. Each API is defined to take arguments of specific types (number, string, bool, and view (e.g sub-table)) to output specific- type variables. During the program execution, we store the generated intermediate variables to different-typed caches N , R, B, V (Num, Str, Bool, View). At each execution step, the program can fetch the intermediate variable from the caches to achieve semantic compositionality. In order to shrink the search space, we follow NSM ( Liang et al., 2017 ) to use trigger words to prune the API set and accelerate the search speed. The definitions of all API, trigger words can be found in the Appendix. The comprehensive the latent program search procedure is summarized in Algorithm 1, Algorithm 1 Latent Program Search with Comments 1: Initialize Number Cache N , String Cache R, Bool Cache B, View Cache V → ∅ 2: Push linked numbers, strings from the given statement S into N , R, and push T into V 3: Initialize the result collector P → ∅ and an empty program trace P = ∅ 4: Initialize the Queue Q = [(P, N , R, B, V)], we use Q to store the intermediate states 5: Use trigger words to find plausible function set F, for example, more will trigger Greater function. 6: while loop over time t = 1 → MAXSTEP do: 25: Return the triple (T, S, P) # Return (Table, Statement, Program Set) and the searching procedure is illustrated in  Figure 3 . After we collected all the potential program candidates P = {(P 1 , A 1 ), · · · , (P n , A n )} for a given statement S (where (P i , A i ) refers to i-th candidate), we need to learn a discriminator to iden- tify the "appropriate" traces from the set from many erroneous and spurious traces. Since we do not have the ground truth label about such discriminator, we use a weakly supervised training al- gorithm by viewing all the label-consistent programs as positive instances {P i |(P i , A i ); A i = L} and the label-inconsistent program as negative instances {P i |(P i , A i ); A i = L} to minimize the cross-entropy of discriminator p θ (S, P ) with the weakly supervised label. Specifically, we build our discriminator with a Transformer-based two-way encoder ( Vaswani et al., 2017 ), where the statement encoder encodes the input statement S as a vector Enc S (S) ∈ R n×D with dimen- sion D, while the program encoder encodes the program P = p 1 , · · · , p m as another vector Enc P (P ) ∈ R m×D , we concatenate these two vectors and feed it into a linear projection layer Published as a conference paper at ICLR 2020 There are more democrats than republicans in the election. In this approach, we view the table verification problem as a two-sequence binary classification problem like NLI or MPRC ( Wang et al., 2018 ) by linearizing a table T into a sequence and treating the statement as another sequence. Since the linearized table can be extremely long surpassing the limit of sequence models like LSTM, Transformers, etc. We propose to shrink the sequence by only retaining the columns containing entities linked to the statement to alleviate such a memory issue. In order to encode such sub-table as a sequence, we propose two different linearization methods, as is depicted in  Figure 4 . (i) Concatenation: we simply concatenate the table cells with [SEP] tokens in between and restart position counter at the cell boundaries; the column name is fed as another type embedding to the input layer. Such design retains the table information in its machine format. (ii) Template: we adopt simple natural language templates to transform a table into a "somewhat natural" sentence. Taking the horizontal scan as an example, we linearize a table as "row one's game is 51; the date is February; ..., the score is 3.4 (ot). row 2 is ...". The isolated cells are connected with punctuations and copula verbs in a language-like format. After obtaining the linearized sub-tableT, we concatenate it with the natural language state- ment S and prefix a [CLS] token to the sentence to obtain the sequence-level representation H = f BERT ([T, S]), with H ∈ R 768 from pre-trained BERT ( Devlin et al., 2019 ). The rep- resentation is further fed into multi-layer perceptron f M LP to obtain the entailment probability p θ (T, S) = σ(f M LP (H)), where σ is the sigmoid function. We finetune the model θ (including the parameters of BERT and MLP) to minimize the binary cross entropy L(p θ (T, S), L) on the training set. At test time, we use the trained BERT model to compute the matching probability between the (table, statement) pair, and classify it as ENTAILED statement when p θ (T, S) is greater than 0.5.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we aim to evaluate the proposed methods on TABFACT. Besides the standard valida- tion and test sets, we also split the test set into a simple and a complex partition based on the channel from which they were collected. This facilitates analyzing how well the model performs under dif- ferent levels of difficulty. Additionally, we also hold out a small test set with 2K samples for human evaluation, where we distribute each (table, statement) pair to 5 different workers to approximate human judgments based on their majority voting, the results are reported in  Table 2 . NSM We follow  Liang et al. (2017)  to modify their approach to fit the setting of TABFACT. Specif- ically, we adopt an LSTM as an encoder and another LSTM with copy mechanism as a decoder to synthesize the program. However, without any ground truth annotation for the intermediate programs, directly training with reinforcement learning is difficult as the binary reward is under- specified, which is listed in  Table 2  as "NSM w/ RL". Further, we use LPA as a teacher to search the top programs for the NSM to bootstrap and then use reinforcement learning to finetune the model, which achieves reasonable performance on our dataset listed as "NSM w/ ML + RL".

Section Title: Preliminary Evaluation
  Preliminary Evaluation In order to test whether our negative rewriting strategy eliminates the ar- tifacts or shallow cues, we also fine-tune a pre-trained BERT ( Devlin et al., 2019 ) to classify the statement S without feeding in table information. The result is reported as "BERT classifier w/o Published as a conference paper at ICLR 2020 Table" in  Table 2 , which is approximately the majority guess and reflects the effectiveness of the rewriting strategy. Before presenting the experiment results, we first perform a preliminary study to evaluate how well the entity linking system, program search, and the statement-program discrimi- nator perform. Since we do not have the ground truth labels for these models, we randomly sample 100 samples from the dev set to perform the human study. For the entity linking, we evaluate its accuracy as the number of correctly linked sentences / total sentences. For the latent program search, we evaluate whether the "true" programs are included in the candidate set P as recall score.

Section Title: Results
  Results We report the performance of different methods as well as human performance in  Table 2 . First of all, we observe that the naive serialized model fails to learn anything effective (same as the Majority Guess). It reveals the importance of template when using the pre-trained BERT ( Devlin et al., 2019 ) model: the "natural" connection words between individual cells is able to unleash the power of the large pre-trained language model and enable it to perform reasoning on the structured table form. Such behavior is understandable given the fact that BERT is pre-trained on purely natural language corpora. In addition, we also observe that the horizontal scan excels in the vertical scan because it better captures the convention of human expression. Among different LPA methods, we found that LPA-Ranking performs the best since it can better suppress the spurious programs than the voting-based algorithm. Overall, the LPA model is on par with Table-BERT on both simple and test split without any pre-training on external corpus, which reflects the effectiveness of LPA to leverage symbolic operations in the verification process. Through our human evaluation, we found that only 58% of sentences have been correctly linked without missing-link or over-link, while the systematic search has a recall of 51% under the cases where the sentence is correctly linked. With that being said, the chance for LPA method to cover the correct program (rationale) is roughly under 30%. After the discriminator's re-ranking step, the probability of selecting these particular oracle program is even much lower. However, we still observe a final overall accuracy of 65%, which indicates that the spurious problem is quite severe in LPA, where the correct label is predicted based on the wrong reason. Through our human evaluation, we also observe that Table-BERT exhibits poor consistency as it can misclassify simple cases but correctly-classify hard cases. These two major weaknesses are yet to be solved in future studies. In contrast, LPA behaves much more consistently and provides a clear latent rationale for its decision. But, such a pipeline system requires laborious handcrafting of API operations and is also very sensitive to the entity linking accuracy. Both methods have pros and cons; how to combine them still remains an open question.

Section Title: Program Annotation
  Program Annotation To further promote the development of different models in our dataset, we collect roughly 1400 human-annotated programs paired with the original statements. These state- ments include the most popular logical operations like superlative, counting, comparison, unique, etc. We provide these annotations in Github 9 , which can either be used to bootstrap the semantic parsers or provide the rationale for NLI models.

Section Title: RELATED WORK
  RELATED WORK Natural Language Inference & Reasoning: Modeling reasoning and inference in human language is a fundamental and challenging problem towards true natural language understanding. There has been extensive research on RTE in the early years ( Dagan et al., 2005 ) and more recently shifted to NLI ( Bowman et al., 2015 ;  Williams et al., 2017 ). NLI seeks to determine whether a natural language hypothesis h can be inferred from a natural language premise p. With the surge of deep learning, there have been many powerful algorithms like the Decomposed Model ( Parikh et al., 2016 ), Enhanced-LSTM ( Chen et al., 2017 ) and BERT ( Devlin et al., 2019 ). Besides the textual ev- idence, NLVR ( Suhr et al., 2017 ) and NLVR2 ( Suhr et al., 2019 ) have been proposed to use images as the evidence for statement verification on multi-modal setting. Our proposed fact verification task is closely related to these inference tasks, where our semi-structured table can be seen as a collection of "premises" exhibited in a semi-structured format. Our proposed problem hence could be viewed as the generalization of NLI under the semi-structured domain. question answering, such as MCQ ( Jauhar et al., 2016 ), WikiTableQuestion ( Pasupat & Liang, 2015 ), Spider ( Yu et al., 2018 ), Sequential Q&A ( Iyyer et al., 2017 ), and WikiSQL ( Zhong et al., 2017 ), for which approaches have been extended to handle large-scale tables from Wikipedia ( Bha- gavatula et al., 2013 ). However, in these Q&A tasks, the question types typically provide strong signals needed for identifying the type of answers, while TABFACT does not provide such speci- ficity. The uniqueness of TABFACT lies in two folds: 1) a given fact is regarded as a false claim as long as any part of the statement contains misinformation. Due to the conjunctive nature of verifica- tion, a fact needs to be broken down into several sub-clauses or (Q, A) pairs to separate evaluate their correctness. Such a compositional nature of the verification problem makes it more challenging than a standard QA setting. On one hand, the model needs to recognize the multiple QA pairs and their relationship. On the other hand, the multiple sub-clauses make the semantic form longer and logic inference harder than the standard QA setting. 2) some facts cannot even be handled using semantic forms, as they are driven by linguistic inference or common sense. In order to verify these state- ments, more inference techniques have to be leveraged to enable robust verification. We visualize the above two characteristics of TABFACT in  Figure 5 .

Section Title: Program Synthesis & Semantic Parsing
  Program Synthesis & Semantic Parsing There have also been great interests in using program synthesis or logic forms to solve different natural language processing problems like question answering ( Liang et al., 2013 ;  Berant et al., 2013 ;  Berant & Liang, 2014 ), visual navigation ( Artzi et al., 2014 ;  Artzi & Zettlemoyer, 2013 ), code generation ( Yin & Neubig, 2017 ;  Dong & Lapata, 2016 ), SQL synthesis ( Yu et al., 2018 ), etc. The traditional semantic parsing papers ( Artzi et al., 2014 ;  Artzi & Zettlemoyer, 2013 ;  Zettlemoyer & Collins, 2005 ;  Liang et al., 2013 ;  Berant et al., 2013 ) greatly rely on rules, lexicon to parse natural language sentences into different forms like lambda calculus, DCS, etc. More recently, researchers strive to propose neural models to directly perform end-to-end formal reasoning like Theory Prover ( Riedel et al., 2017 ;  Rocktäschel & Riedel, 2017 ), Neural Turing Machine ( Graves et al., 2014 ), Neural Programmer ( Neelakantan et al., 2016 ;  2017 ) and Neural-Symbolic Machines ( Liang et al., 2017 ; 2018;  Agarwal et al., 2019 ). The proposed TABFACT serves as a great benchmark to evaluate the reasoning ability of different neural reasoning models. Specifically, TABFACT poses the following challenges: 1) spurious programs (i.e., wrong programs with the true returned answers): since the program output is only a binary label, which can cause serious spurious problems and misguide the reinforcement learning with the under-specified binary rewards. 2) decomposition: the model needs to decompose the statement into sub-clauses and verify the sub-clauses one by one, which normally requires the longer logic in- ference chains to infer the statement verdict. 3) linguistic reasoning like inference and paraphrasing.

Section Title: Fact Checking
  Fact Checking The problem of verifying claims and hypotheses on the web has drawn significant at- tention recently due to its high social influence. Different fact-checking pioneering studies have been performed including LIAR ( Wang, 2017 ), PolitiFact ( Vlachos & Riedel, 2014 ), FEVER ( Thorne et al., 2018 ) and AggChecker ( Jo et al., 2019 ), etc. The former three studies are mainly based on textual evidence on social media or Wikipedia, while AggChecker is closest to ours in using re- lational databases as the evidence. Compared to AggChecker, our paper proposes a much larger dataset to benchmark the progress in this direction.

Section Title: CONCLUSION
  CONCLUSION This paper investigates a very important yet previously under-explored research problem: semi- structured fact verification. We construct a large-scale dataset and proposed two methods, Table- BERT and LPA, based on the state-of-the-art pre-trained natural language inference model and pro- gram synthesis. In the future, we plan to push forward this research direction by inspiring more sophisticated architectures that can perform both linguistic and symbolic reasoning.

```
