Title:
```
Published as a conference paper at ICLR 2020 HIGHER-ORDER FUNCTION NETWORKS FOR LEARN- ING COMPOSABLE 3D OBJECT REPRESENTATIONS
```
Abstract:
```
We present a new approach to 3D object representation where a neural network encodes the geometry of an object directly into the weights and biases of a second 'mapping' network. This mapping network can be used to reconstruct an object by applying its encoded transformation to points randomly sampled from a simple geometric space, such as the unit sphere. We study the effectiveness of our method through various experiments on subsets of the ShapeNet dataset. We find that the proposed approach can reconstruct encoded objects with accuracy equal to or exceeding state-of-the-art methods with orders of magnitude fewer parameters. Our smallest mapping network has only about 7000 parameters and shows reconstruc- tion quality on par with state-of-the-art object decoder architectures with millions of parameters. Further experiments on feature mixing through the composition of learned functions show that the encoding captures a meaningful subspace of objects. ‡
```

Figures/Tables Captions:
```
Figure 1: Top: Overview of HOF. The encoder network g φ encodes the geometry of the object pictured in each input image directly into the parameters of the mapping function f θ , which produces a reconstruction as a transformation of a canonical object (here, the unit sphere). Bottom: We visualize the transformation f θ by showing various subsets of the inputs X and their corresponding mapped locations in red and green, respectively. In each frame, light gray shows the rest of X and dark gray shows the rest of the reconstructed object.
Figure 2: From left to right: Input RGB image, ground truth point cloud, reconstruction from FoldingNet (Yang et al., 2018), reconstruction from DeepSDF (Park et al., 2019), and our method.
Figure 3: Runtime analysis comparing HOF with DeepSDF and FoldingNet architectures. HOF-1 and HOF-3 are HOF with 1 and 3 hidden layers, respectively. Computing environment details are given in Section B.2.
Figure 4: Top Left. An example of inter-class interpolation between two objects by function composition. We show the ground truth objects O A and O B , a single evaluation of their respective decoding functions (giving f A (X) and f B (X)), as well as the possible permutations of compositions, which makes up the leaf nodes in each tree. In f B (f A (X)), we see the wings straighten but remain narrow. In f A (f B (X)), we observe the wings broaden, but they remain angled. Top Right. An example of inter-class interpolation, mixing a table and a rifle. We observe what might be interpreted as a gun with legs in f B (f A (X)) and a table with a single coherent stock in f A (f B (X)). Bottom. An example of intra-class interpolation between two objects with k = 4.
Table 1: Comparing various reconstruction architectures. Reported Chamfer distance values are multiplied by 100 for readability and include standard error in parentheses. HOF-1 and HOF-3 are HOF variants with 1 and 3 hidden layers, respectively.
Table 2: F1/class-weighted F1 score comparison of HOF with methods re- ported in Tatarchenko et al. (2019). Higher is better.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION This paper is primarily concerned with the problem of learning compact 3D object representations and estimating them from images. If we consider an object to be a continuous surface in R 3 , it is not straightforward to directly represent this infinite set of points in memory. In working around this problem, many learning-based approaches to 3D object representation suffer from problems related to memory usage, computational burden, or sampling efficiency. Nonetheless, neural networks with tens of millions of parameters have proven effective tools for learning expressive representations of geometric data. In this work, we show that object geometries can be encoded into neural networks with thousands, rather than millions, of parameters with little or no loss in reconstruction quality. To this end, we propose an object representation that encodes an object as a function that maps points from a canonical space, such as the unit sphere, to the set of points defining the object. In this work, the function is approximated with a small multilayer perceptron. The parameters of this function are estimated by a 'higher order' encoder network, thus motivating the name for our method: Higher-Order Function networks (HOF). This procedure is shown in  Figure 1 . There are two key ideas that distinguish HOF from prior work in 3D object representation learning: fast-weights object encoding and interpolation through function composition. (1) Fast-weights object encoding: 'Fast-weights' in this context generally refers to methods that use network weights and biases that are not fixed; at least some of these parameters are estimated on a per-sample basis. Our fast-weights approach stands in contrast to existing methods which encode objects as vector-valued inputs to a decoder network with fixed weights. Empirically, we find that our approach enables a dramatic reduction (two orders of magnitude) in the size of the mapping network compared to the decoder networks employed by other methods. (2) Interpolation through function composition: Our functional formulation allows for interpolation between inputs by composing the roots of our reconstruction functions. We demonstrate that the functional representation learned by HOF provides a rich latent space in which we can 'interpolate' between objects, producing new, coherent objects sharing properties of the 'parent' objects. In order to position HOF among other methods for 3D reconstruction, we first define a taxonomy of existing work and show that HOF provides a generalization of current best-performing methods. Afterwards, we demonstrate the effectiveness of HOF on the task of 3D reconstruction from an RGB image using a subset of the ShapeNet dataset ( Chang et al., 2015 ). The results, reported in  Tables 1  and 2 and  Figure 2 , show state-of-the-art reconstruction quality using orders of magnitude fewer parameters than other methods.

Section Title: RELATED WORK
  RELATED WORK The selection of object representation is a crucial design choice for methods addressing 3D recon- struction. Voxel-based approaches ( Choy et al., 2016 ;  Häne et al., 2017 ) typically use a uniform discretization of R 3 in order to extend highly successful convolutional neural network (CNN) based approaches to three dimensions. However, the inherent sparsity of surfaces in 3D space make vox- elization inefficient in terms of both memory and computation time. Partition-based approaches such as octrees ( Tatarchenko et al., 2017 ;  Riegler et al., 2017 ) address the space efficiency shortcomings of voxelization, but they are tedious to implement and more computationally demanding to query. Graph-based models such as meshes ( Wang et al., 2018 ;  Gkioxari et al., 2019 ;  Smith et al., 2019 ;  Hanocka et al., 2019 ) provide a compact representation for capturing topology and surface level information, however their irregular structure makes them harder to learn. Point set representations, discrete (and typically finite) subsets of the continuous geometric object, have also gained popularity due to the fact that they retain the simplicity of voxel based methods while eliminating their storage and computational burden ( Qi et al., 2017a ;  Fan et al., 2017 ;  Qi et al., 2017b ;  Yang et al., 2018 ;  Park et al., 2019 ). The PointNet architecture ( Qi et al., 2017a ; b ) was an architectural milestone that made manipulating point sets with deep learning methods a competitive alternative to earlier approaches; however, PointNet is concerned with processing, rather than generating, point clouds. Further, while point clouds are more flexible than voxels in terms of information density, it is still not obvious how to adapt them to the task of producing arbitrary- or varied-resolution predictions. Independently regressing each point in the point set requires additional parameters for each additional point ( Fan et al., 2017 ;  Achlioptas et al., 2018 ), which is an undesirable property if the goal is high-resolution point clouds. Many current approaches to representation and reconstruction follow an encoder-decoder paradigm, where the encoder and decoder both have learned weights that are fixed at the end of training. An image or set of 3D points is encoded as a latent vector 'codeword' either with a learned encoder as in  Yang et al. (2018) ;  Lin et al. (2018) ;  Yan et al. (2016)  or by direct optimization of the latent vector itself with respect to a reconstruction-based objective function as in  Park et al. (2019) . Afterwards, the latent code is decoded by a learned decoder into a reconstruction of the desired object by one of two methods, which we call direct decoding and contextual mapping. Direct decoding methods directly map the latent code into a fixed set of points ( Choy et al., 2016 ;  Fan et al., 2017 ;  Lin et al., 2018 ;  Michalkiewicz et al., 2019 ); contextual mapping methods map the latent code into a function that can be sampled or otherwise manipulated to acquire a reconstruction ( Yang et al., 2018 ;  Park et al., 2019 ;  Michalkiewicz et al., 2019 ;  Mescheder et al., 2019 ). Direct decoding methods generally suffer from the limitation that their predictions are of fixed resolution; they cannot be sampled more or less precisely. With contextual mapping methods, it is possible in principle to sample the object to arbitrarily high resolution with the correct decoder function. However, sampling can provide a significant computational burden for some contextual mapping approaches as those proposed by  Park et al. (2019)  and  Michalkiewicz et al. (2019) . Another hurdle is the need for post-processing such as applying the Marching Cubes algorithm developed by  Lorensen and Cline (1987) . We call contextual mapping approaches that encode context by concatenating a duplicate of a latent context vector with each input latent vector concatenation (LVC) methods. In particular, we compare with LVC architectures used in FoldingNet ( Yang et al., 2018 ) and DeepSDF ( Park et al., 2019 ). HOF is a contextual mapping method that distinguishes itself from other methods within this class through its approach to representing the mapping function: HOF uses one neural network to estimate the weights of another. Conceptually related methods have been previously studied under nomenclature such as the 'fast-weight' paradigm ( Schmidhuber, 1992 ;  De Brabandere et al., 2016 ;  Klein et al., 2015 ;  Riegler et al., 2015 ) and more recently 'hypernetworks' ( Ha et al., 2016 ). However, the work by  Schmidhuber (1992)  deals with encoding memories in sequence learning tasks.  Ha et al. (2016)  suggest that estimating weights of one network with another might lead to improvements in parameter-efficiency. However, this work does not leverage the key insight of using network parameters that are estimated per sample in vision tasks.

Section Title: HIGHER-ORDER FUNCTION NETWORKS
  HIGHER-ORDER FUNCTION NETWORKS HOF is motivated by the independent observations by both  Yang et al. (2018)  and  Park et al. (2019)  that LVC methods do not perform competitively when the context vector is injected by simply concatenating it with each input. In both works, the LVC methods proposed required architectural workarounds to produce sufficient performance on reconstruction tasks, including injecting the latent code multiple times at various layers in the network. HOF does not suffer from these shortcomings due to its richer context encoding (the entire mapping network encodes context) in comparison with LVC. We compare the HOF and LVC regimes more precisely in Section 3.2. Quantitative comparisons of HOF with existing methods can be found in  Table 1 .

Section Title: A FAST-WEIGHTS APPROACH TO 3D OBJECT REPRESENTATION AND RECONSTRUCTION
  A FAST-WEIGHTS APPROACH TO 3D OBJECT REPRESENTATION AND RECONSTRUCTION We consider the task of reconstructing an object point cloud O from an image. We start by training a neural network g φ with parameters φ ( Figure 1 , top-left) to output the parameters θ of a mapping function f θ , which reconstructs the object when applied to a set of points X sampled uniformly from a canonical set such as the unit sphere ( Figure 1 , top-right). We note that the number of samples in X can be increased or decreased to produce higher or lower resolution reconstructions without changing the network architecture or retraining, in contrast with direct decoding methods and some contextual mapping methods which use fixed, non-random samples from X ( Yang et al., 2018 ). The input to g φ is an RGB image I; our implementation takes 64 × 64 × 3 RGB images as input, but our method is general to any input representation for which a corresponding differentiable encoder network can be constructed to estimate θ (e.g. PointNet ( Qi et al., 2017a ) for point cloud completion). Given I, we compute the parameters of the mapping network θ I as That is, the encoder g φ : R 3×64×64 → R d directly regresses the d-dimensional parameters θ I of the mapping network f θ I : R c → R 3 , which maps c-dimensional points in the canonical space X to points in the reconstructionÔ (see  Figure 1 ). We then transform our canonical space X with f θ I in the same manner as other contextual mapping methods: During training, we sample an image I and the corresponding ground truth point cloud model O, where O contains 10,000 points sampled from the surface of the true object. We then obtain the mapping f θ I = g φ (I) and produce an estimated reconstruction of O as in Equation 2. In our training, we only compute f θ I (x) for a sample of 1000 points in X. However, we find that sampling many more points (10-100× as many) at test time still yields high-quality reconstructions. This sample is drawn from a uniform distribution over the set X. We then compute a loss for the predictionÔ using a differentiable set similarity metric such as Chamfer distance or Earth Mover's Distance. We focus on the Chamfer distance as both a training objective and metric for assessing reconstruction quality. The asymmetric Chamfer distance CD(X, Y ) is often used for quantifying the similarity of two point sets X and Y and is given as The Chamfer distance is defined even if sets X and Y have different cardinality. We train g φ to minimize the symmetric objective function (Ô, O) = CD(Ô, O) + CD(O,Ô) as in  Fan et al. (2017) .

Section Title: COMPARISON WITH LVC METHODS
  COMPARISON WITH LVC METHODS We compare our mapping approach with LVC architectures such as DeepSDF ( Park et al., 2019 ) and FoldingNet ( Yang et al., 2018 ). These architectures control the output of the decoder through the concatenation of a latent 'codeword' vector z with each input x i ∈ X. The codeword is estimated by an encoder g φLVC for each image. We consider the case in which the latent vector is only concatenated Published as a conference paper at ICLR 2020 with inputs in the first layer of the decoder network f θ , which we assume to be an MLP. We are interested in analyzing the manner in which the network output with respect to x i may be modulated by varying z. If the vector a i contains the pre-activations of the first layer of f θ given an input point x i , we have a i = W x x i + W z z + b where W x , W z , and b are fixed parameters of the decoder, and only z is a function of I. If we absorb the parameters W z and b into the encoder parameters φ LVC (as W z and b are fixed for all x i ), we can define a new, equivalent latent representation b * = W z z + b = W z g φLVC (I) + b and a new encoder function h with parameters φ LVC ∪ {W z , b} such that h(I) = b * . This gives Thus the LVC approach is equivalent to estimating a fixed subset of the parameters θ of the decoder f θ on a per-sample basis (the first layer bias). From this perspective, HOF is an intuitive generalization: rather than estimating just the first layer bias, we allow our encoder to modulate all of the parameters in the decoder f θ on a per-sample basis. Having demonstrated HOF as a generalization of existing contextual mapping methods, in the next section, we present a novel application of contextual mapping that leverages the compositionality of the estimated mapping functions to aggregate features of multiple objects or multiple viewpoints of the same object.

Section Title: EXTENDING CONTEXTUAL MAPPING METHODS: FEATURE AGGREGATION THROUGH FUNCTION COMPOSITION
  EXTENDING CONTEXTUAL MAPPING METHODS: FEATURE AGGREGATION THROUGH FUNCTION COMPOSITION An advantageous property of methods that use a latent codeword is that they have been empirically shown to learn a meaningful space of object geometries, in which interpolating between object encodings gives new, coherent object encodings ( Fan et al., 2017 ;  Yang et al., 2018 ;  Park et al., 2019 ;  Groueix et al., 2018 ). HOF, on the other hand, does not obviously share this property: interpolating between the mapping function parameters estimated for two different objects need not yield a new, coherent object as the prior work has shown that the solution space of 'good' neural networks is highly non-convex ( Li et al., 2018 ). We demonstrate empirically in Figure 7 that naively interpolating between reconstruction function in the HOF regime does indeed produce meaningless blobs. However, with a small modification to the HOF formulation in Equation 2, we can in fact learn a rich space of functions in which we can interpolate between objects through function composition. We extend the formulation in Equation 2 to one where an object is represented as the k-th power of the mapping f θ I :Ô = {f k θ I (x) : x ∈ X} (4) where f k is defined as the composition of f with itself (k − 1) times: f k (x) = f (f (k−1) (x)) where f 0 (x) x. We call a mapping f θ I whose k-th power reconstructs the object O in image I the k-mapping for O. This modification to Equation 2 adds an additional constraint to the mapping: the domain and codomain must be the same. However, evaluating powers of f leverages the power of weight sharing in neural network architectures; for an MLP mapping architecture with l layers (excluding the input layer), evaluating its k-th power is equivalent to an MLP with l × k layers with shared weights. This formulation also has connections to earlier work on continuous attractor networks as a model for encoding memories in the brain as k becomes large ( Seung, 1998 ). In Section 4.3, we conduct experiments in a setting in which we have acquired RGB images I and J of two objects, O I and O J , respectively. Applying our encoder to these images, we obtain k-mappings f θ I and f θ J , which have parameters θ I = g φ (I) and θ J = g φ (J), respectively. We hypothesize that we can combine the information contained in each mapping function f θi by evaluating any of the 2 k possible functions of the form: f interp = (f θ1 • ... • f θ k ) (5) where the parameters of each mapping f θi are either the parameters of f θ I or f θ J .  Figures 4  and 7 show that interpolation with function composition provides interesting, meaningful outputs in experiments with k = 2 and k = 4.

Section Title: EXPERIMENTAL EVALUATIONS
  EXPERIMENTAL EVALUATIONS We conduct various empirical studies in order to justify two key claims. In Sections 4.1 and 4.2, we compare with other contextual mapping architectures to demonstrate that HOF provides equal or better performance with a significant reduction in parameters and compute time. In Section 4.3 we demonstrate that extending contextual mapping approaches such as HOF with multiple compositions of the mapping function provides a simple and effective approach to object interpolation. Further experimentation, including ablation studies and a simulated robot navigation scenario, can be found in A.7.

Section Title: EVALUATING RECONSTRUCTION QUALITY ON SHAPENET
  EVALUATING RECONSTRUCTION QUALITY ON SHAPENET We test HOF's ability to reconstruct a 3D point cloud of an object given a single RGB image, comparing with other architectures for 3D reconstruction. We conduct two experiments: 1. We compare HOF with LVC architectures to show that HOF is a more parameter-efficient approach to contextual mapping than existing fixed-decoder architectures. 2. We compare HOF to a broader set of state of the art methods on a larger subset of the ShapeNet dataset, demonstrating that it matches or surpasses existing state of the art methods in terms of reconstruction quality. In the first experiment, we compare HOF with other LVC architectures on 13 of the largest classes of ShapeNet ( Yan et al., 2016 ), using the asymmetric Chamfer distance metrics (Equation 3) as reported in  Lin et al. (2018) . In the second experiment, we compare HOF with other methods for 3D reconstruction on a broader selection of 55 classes the ShapeNet dataset, as in  Tatarchenko et al. (2019) . In line with the recommendations in  Tatarchenko et al. (2019) , we report F1 scores for this evaluation.

Section Title: LVC ARCHITECTURE COMPARISON
  LVC ARCHITECTURE COMPARISON For this experiment, we compare HOF with LVC decoder architectures proposed in the literature, specifically those used in DeepSDF ( Park et al., 2019 ) and FoldingNet ( Yang et al., 2018 ), as well as several other baselines. Each architecture maps points from R c to R 3 in order to enable a direct comparison. The dataset contains 31773 ground truth point cloud models for training/validation and 7926 for testing. For each point cloud, there are 24 RGB renderings of the object from a fixed set of 24 camera positions. For both training and testing, each point cloud is shifted so that its bounding box center is at the origin in line with  Fan et al. (2017) . At test time, there is no post-processing performed on the predicted point cloud. The architectures we compare in this experiment are: 1. HOF-1: 1 hidden layer containing 1024 hidden neurons 2. HOF-3: 3 hidden layers containing 128 hidden neurons 3. DeepSDF as described in  Park et al. (2019) , with 8 hidden layers containing 512 neurons each Published as a conference paper at ICLR 2020 4. FoldingNet as described in  Yang et al. (2018) , with 2 successive 'folds', each with a 3-layer MLP with 512 hidden neurons 5. EPCG architecture as reported in  Lin et al. (2018)  6. Point Set Generation network ( Fan et al., 2017 ) as reported in  Lin et al. (2018)  7. 3D-R2N2 ( Choy et al., 2016 ) as reported in  Lin et al. (2018)  Results are reported in  Table 1 . Chamfer Distance scores are scaled by 100 as in line with  Lin et al. (2018) . We find that HOF performs significantly better than that direct decoding baseline of  Lin et al. (2018)  and performs on par with other contextual mapping approaches with 30× fewer parameters. In order to provide a fair comparison with the baseline method, we ensure that ground truth objects are scaled identically to those in  Lin et al. (2018) . We report both 'forward' Chamfer distance CD(Pred, Target) and 'backward' Chamfer distance CD(Target, Pred), again in line with the convention established by  Lin et al. (2018) . Table 7 contains a class-wise breakdown. Qualitative comparisons of the outputs of HOF with state-of-the-art architectures are shown in  Figure 2 .

Section Title: SHAPENET BREADTH COMPARISON
  SHAPENET BREADTH COMPARISON   Tatarchenko et al. (2019)  question the common practice in single-view 3D reconstruction of eval- uating only on the largest classes of ShapeNet. The authors demonstrate that reconstruction methods do not exhibit performance correlated with the size of object classes, and thus evaluating on smaller ShapeNet classes is justified. We use the dataset provided by  Tatarchenko et al. (2019) , which includes 55 classes from the ShapeNet dataset. The authors also suggest using the F1 score metric, defined as the harmonic mean between precision and recall ( Tatarchenko et al., 2019 ). We include comparisons with AtlasNet ( Groueix et al., 2018 ), Octree Generating Networks ( Tatarchenko et al., 2017 ), Matryoshka Networks ( Richter and Roth, 2018 ), and retrieval baselines as reported by  Tatarchenko et al. (2019) . We find that HOF performs competitively with all of these state-of-the-art methods, even surpassing them on many classes. We show summary statistics in  Table 2 . The F1 column contains the average F1 score for each method, uniformly averaging over all classes regardless of how class imbalances in the testing set. The cw-F1 score column averages over class F1 scores weighted by the fraction of the dataset comprised by that class; that is, classes that are over-represented in the testing set are correspondingly over-represented in the cw-F1 score. On the mean F1 metric, HOF outperforms all other methods, including the 'Oracle Nearest-Neighbor' approach described by  Tatarchenko et al. (2019) . The Oracle Nearest Neighbor uses the closest object in the training set as its prediction for each test sample. A complete class- wise performance breakdown is in the Appendix in Table 8. We find that HOF outperforms all 5 comparison methods (including the Oracle) in 23 of the 55 classes. Excluding the oracle, HOF shows the best performance in 28 of the 55 classes.

Section Title: RUNTIME PERFORMANCE COMPARISON
  RUNTIME PERFORMANCE COMPARISON We compare HOF with the decoder architectures proposed in  Park et al. (2019)  and  Yang et al. (2018)  in terms of inference speed.  Figure 3  shows the results of this experiment, comparing how long it takes for each network to map a set of N samples from the canonical space X into the object reconstruction. We ignore the processing time for estimating the latent state z for DeepSDF/FoldingNet and the function parameters θ for HOF; we use the same convolutional neural network architecture with a modified output layer for both. We find that even for medium-resolution reconstructions (N > 1000), the GPU running times for the DeepSDF/FoldingNet architectures and HOF begin to diverge. This difference is even more noticeable in the CPU running time comparison (an almost 100× difference). This performance improvement may be significant for embedded systems that need to efficiently store and reconstruct 3D objects in real time; our representation is small in size, straightforward to sample uniformly (unlike a CAD model), and fast to evaluate.

Section Title: OBJECT INTERPOLATION
  OBJECT INTERPOLATION To demonstrate that our functional representation yields an expressive latent object space, we show that the composition of these functions produces interesting, new objects. The top of  Figure 4  shows in detail the composition procedure. If we have estimated 2-mappings for two objects O A and O B , we demonstrate that f θ A (f θ B (X)) and f θ B (f θ A (X)) both provide interesting mixtures of the two objects and mix the features of the objects in different ways; the functions are not commutative. This approach is conceptually distinct from other object interpolation methods, which decode the interpolation of two different latent vectors. In our formulation, we visualize the outputs of an encoder that has been trained to output 2-mappings in R 3 . In addition, the bottom of  Figure 4  demonstrates a smooth gradient of compositions of the reconstruction functions for two airplanes, when a higher order of mappings (k = 4) is used. To further convey the expressiveness of the composition-based object interpolation, we compare it against a method that performs interpolation in the network parameter space. This latter approach resembles a common way of performing object interpolation in LVC methods: Generate latent codewords from each image, and synthesize new objects by feeding the interpolated latent vectors into the decoder. As a proxy for the latent vector interpolation used in LVC methods, we generate new objects as follows. After outputting the network parameters θ A and θ B for the objects O A and O B , we use the interpolated parameters θ = (θ A + θ B )/2 to represent the mapping function. In Figure 7, we show that our composition-based interpolation is more capable of generating coherent new objects whose geometric features inherited from the source objects are preserved better.

Section Title: CONCLUSION AND FUTURE WORK
  CONCLUSION AND FUTURE WORK We presented Higher Order Function Networks (HOF), which generate a functional representation of an object from an RGB image. The function can be represented as a small MLP with 'fast-weights', or weights that are output by an encoder network g φ with learned, fixed weights. HOF demonstrates state-of-the-art reconstruction quality, as measured by Chamfer distance and F1 score with ground truth models, with far fewer decoder parameters than existing methods. Additionally, we extended contextual mapping methods to allow for interpolation between objects by composing the roots of their corresponding mapping functions. Another advantage of HOF is that points on the surface can be sampled directly, without expensive post-processing methods such as estimating level sets. For future work, we would like to further improve on the parameter-efficiency of HOF, for example with versions of HOF that output only a sparse but flexible subset of the parameters of the mapping function. In addition, connections with other works investigating the properties of 'high-quality' neural network parameters and initializations such as HyperNetworks ( Ha et al., 2016 ), the Lottery Ticket Hypothesis ( Frankle and Carbin, 2018 ), and model-agnostic meta learning ( Finn et al., 2017 ). There are also many interesting applications of HOF in domains such as robotics. A demonstrative application in motion planning can be found in Appendix B.2.2, and  Engin et al. (2020)  explore extensions of HOF for multi-view reconstruction and motion planning. Using functional representa- tions directly for example for manipulation or navigation tasks, rather than generating intermediate 3D point clouds, is also an interesting avenue of future work. We hope that the ideas presented in this paper provides a basis for future developments of efficient 3D object representations and neural network architectures.

```
