Title:
```
CIENT NETWORK DESIGN
```
Abstract:
```
There is growing interest in designing lightweight neural networks for mobile and embedded vision applications. Previous works typically reduce computations from the structure level. For example, group convolution based methods reduce computations by factorizing a vanilla convolution into depth-wise and point-wise convolutions. Pruning based methods prune redundant connections in the network structure. In this paper, we explore the importance of network input for achiev- ing optimal accuracy-efficiency trade-off. Reducing input scale is a simple yet effective way to reduce computational cost. It does not require careful network module design, specific hardware optimization and network retraining after prun- ing. Moreover, different input scales contain different representations to learn. We propose a framework to mutually learn from different input resolutions and network widths 1 . With the shared knowledge, our framework is able to find bet- ter width-resolution balance and capture multi-scale representations. It achieves consistently better ImageNet top-1 accuracy over US-Net (Yu & Huang, 2019) un- der different computation constraints, and outperforms the best compound scale model of EfficientNet (Tan & Le, 2019) by 1.5%. The superiority of our frame- work is also validated on COCO object detection and instance segmentation as well as transfer learning.
```

Figures/Tables Captions:
```
Figure 1: Accuracy-FLOPs curve of US-Net+ and US-Net. US-Net+ means simply applying different resolutions to US-Net during testing.
Figure 2: The training process of our proposed framework. The network width range is [0.25, 1.0], input resolution is chosen from {224, 192, 160, 128}. This can achieve a computation range of [13, 569] MFLOPs on MobileNet v1 backbone. We follow the sandwich rule (Yu & Huang, 2019) to sample 4 networks, i.e., upper-bound full width network (1.0×), lower-bound width net- work (0.25×), and two random width ratios α 1 , α 2 ∈ (0.25, 1). For the full-network, we constantly choose 224×224 resolution. For the other three sub-networks, we randomly select its input resolu- tion. The full-network is optimized with the ground-truth label. Sub-networks are optimized with the prediction of the full-network. Weights are shared among different networks to facilitate mutual learning. CE: Cross Entropy loss. KL: KL Divergence loss.
Figure 3: Illustration of the mutual learning from width and resolution scheme.
Figure 4: Accuracy-FLOPs curves of our proposed framework and US-Net. (a) is based on Mo- bileNet v1 backbone. (b) is based on MobileNet v2 backbone.
Figure 5: The Accuracy-FLOPs curve is based on MobileNet v1 backbone. We highlight the selected resolution under different FLOPs with different colors. For example, the solid green line indicates when the constraint range is [41, 215] MFLOPs, our framework constantly selects input resolution 160 but reduces the width to meet the resource constraint. Best viewed in color.
Figure 6: Accuracy-FLOPs curve of our framework and independently-trained MobileNets. (a) is MobileNet v1 backbone. (b) is MobileNet v2 backbone. The results for different MobileNets configuration are taken from the paper (Howard et al., 2017; Sandler et al., 2018)
Figure 7: Accuracy-FLOPs curves on different transfer learning datasets.
Figure 8: Average Precision - FLOPs curves of our framework and US-Net. (a) is bounding box average precision. (b) is mask average precision.
Table 1: ImageNet Top-1 accuracy on MobileNet v1 backbone.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep neural networks have triumphed over various per- ception tasks such as image classification ( He et al., 2016 ;  Huang et al., 2017b ;  Simonyan & Zisserman, 2014 ), ob- ject detection ( Ren et al., 2015 ;  Redmon et al., 2016 ) and semantic segmentation ( Chen et al., 2017 ). How- ever, deep networks usually require large computational resources, making them hard to deploy on mobile devices and embedded systems. This motivates research in reduc- ing the redundancy in deep neural networks or designing light-weight structures. Specifically, MobileNet ( Howard et al., 2017 ) factorizes a standard 3 × 3 convolution into a 3×3 depth-wise convolution and a 1×1 point-wise convo- lution. ShuffleNet ( Zhang et al., 2018a ) uses 1 × 1 group convolution to further reduce computations and proposes the shuffle operation to help information flow among dif- ferent groups. Another kind of approach is to prune re- dundant connections in the networks. Unstructured prun- ing methods ( Han et al., 2015b ;a) delete network connec- tions which are thought unimportant. Structured pruning methods ( Li et al., 2016 ;  Anwar et al., 2017 ;  Lemaire et al., 2018 ) prune the entire filters and feature maps. Recently, dynamic networks have been introduced which adopt a single model to meet varying computing resource constraints. For example,  Huang et al. (2017a)  proposes a multi-branch structure where early prediction can be made based on the current confidence and resource constraints.  Kim et al. (2018)  builds a network- Under review as a conference paper at ICLR 2020 in-network structure for multiple resources.  Yu et al. (2019)  shares weights among different sub- networks and each sub-network has its own batch normalization layer. US-Net ( Yu & Huang, 2019 ) proposes to compute batch normalization statistics after training, and introduces two training tech- niques to train a network that is executable at any network widths. However, these works approach the problem only from network structure perspective, ignoring the importance of network input. Reducing network input dimension (e.g., lowering image resolution) is a straightforward way to re- duce computational cost. It can be applied to any network structure during testing. Besides, balanc- ing between input resolution and network width can achieve better accuracy-efficiency tradeoffs. For example, to meet a dynamic resource constraint from 13 MFLOPs to 569 MFLOPs on MobileNet v1 backbone, US-Net ( Yu & Huang, 2019 ) needs a network width range of [0.05×, 1.0×] given a 224×224 input resolution. This constraint can also be met via a network width of [0.25×, 1.0×] by adjusting the input resolution from {224, 192, 160, 128} during test time. We denote the latter model as US-Net+. As shown in  Figure 1 , simply combining different resolutions with network widths during testing can already achieve a better accuracy-efficiency tradeoff than US-Net without additional efforts. EfficientNet ( Tan & Le, 2019 ) also acknowledges the importance of balancing be- tween depth, width and resolution. Moreover, reducing input resolution does not necessarily harm the performance, and may sometimes even be beneficial.  Chin et al. (2019)  points out that lower im- age resolution may produce better detection accuracy by reducing focus on redundant details.  Chen et al. (2019b)  claims that different scaled images contain different information.  Sun et al. (2019)  fuses multi-scale features with a multi-branch framework to learn robust representations. Inspired by the observations above, we propose a unified framework to mutually learn from input resolution and network width. Our framework is able to achieve the optimal width-resolution bal- ance under certain resource constraint. Since we share weights among different network widths, each network is able to capture multi-scale representations without any adjustments in the network structure. The whole framework is shown in  Figure 2 . We summarize our contributions as follows. 1. We highlight the importance of input resolution for efficient network design. Previous works either ignore it or treat it independently from network structure. In contrast, we propose a unified framework to mutually learn from input resolution and network width. 2. Our framework is simple and general. It is compatible with any network structures (i.e., network- agnostic) and is as simple as training an independent network. Moreover, it does not change network structure, meaning it can benefit from other add-on structural optimizations. 3. We conduct extensive experiments to verify the effectiveness of our framework. Our frame- work significantly outperforms US-MobileNets ( Yu & Huang, 2019 ) and independently-trained Mo- bileNets on ImageNet classification, as well as COCO object detection and instance segmentation. The experimental results on popular transfer learning datasets also demonstrate the generalization ability of the learned representations of our model.

Section Title: RETHINKING REDUCING COMPUTATIONAL COST
  RETHINKING REDUCING COMPUTATIONAL COST In this section, we explore the importance of input resolution for efficient network design. The computational cost of a vanilla convolution is C 1 × C 2 × K × K × H × W (1) where C 1 and C 2 are the number of input and output channels, K is kernel size, H and W are output feature map size. Most previous works only focus on reducing computational cost from the structure level, that is, reducing the number of channels C 1 × C 2 or reducing kernel size K. MobileNets ( Sandler et al., 2018 ;  Howard et al., 2017 ) decompose the vanilla convolution into a depth-wise convolution and a 1×1 convolution, reducing the cost to (C 1 × K × K + C 1 × C 2 ) × H × W . ShuffleNet ( Zhang et al., 2018a ) further divides 1 × 1 convolution into several groups, shrinking the computational cost to (C 1 × K × K + C 1 × (C 2 /g)) × H × W , where g is the number of groups. However, increasing the number of groups leads to high memory access cost (MAC) ( Ma et al., 2018 ), which makes the network inefficient in practical applications. Pruning based methods aim to prune redundant connections in the network, cutting the computational cost to β 1 × C 1 × β 2 × C 2 × K × K × H × W , where β 1 and β 2 are pruning ratios. Unstructured pruning methods ( Han et al., 2015b ;a) remove the network connections which are thought unimportant. The outcome is a sparsely connected network, which is not efficient on standard libraries and needs Under review as a conference paper at ICLR 2020 Randomly select a resolution from {224, 192, 160, 128} as input to each sub-network Full network (1.0× ) Width (# of channels) Sub-network 2 ( × ) 224x224 192x192 128x128 160x160 224x224 Ground truth CE Sub-network 1 ( ×) Sub-network 3 (0.25×) … KL KL KL 0.25 x (width of the full network) specialized software and hardware optimization (Han et al., 2016). Structured pruning methods ( Li et al., 2016 ;  Anwar et al., 2017 ;  Lemaire et al., 2018 ) prune the entire filters and feature maps. The pruned network maintains structured but needs retraining to retain performance after pruning, which makes it hard to meet the dynamic resource constraints in real-world applications. US-Net ( Yu & Huang, 2019 ) addresses the dynamic constraint problem by sharing weights among different sub- networks and optimizing them simultaneously. But it only considers reducing network width and the performance drops dramatically as computational resource goes down. Instead of only focusing on C 1 × C 2 , we shift our attention to reducing H × W in Eq. 1, i.e., lowering input resolution for the following three reasons. First, reducing H and W can reduce the computational cost without making any adjustments to the network structure. Therefore, it does not require further hardware optimization (Han et al., 2016) or careful structure tuning ( Ma et al., 2018 ). Second, balancing between input resolution and network width will produce better accuracy-efficiency tradeoffs as shown in  Figure 1 . Third, different resolutions contain different information. Lower resolution images may contain more global structures while higher resolution ones may encapsulate more fine-grained patterns. Several works ( Chen et al., 2019b ;  Sun et al., 2019 ;  Ke et al., 2017 ) have explored such multi-scale representation learning. However, they resort to a multi-path structure, which is unfriendly to parallelization ( Ma et al., 2018 ). Motivated by the discussion above, we propose a unified framework to mutually learn from network width and input resolution. Our framework is able to find the optimal width-resolution configuration under certain constraint, and captures multi-scale representations by sharing weights. In addition, it is compatible with any network structures and does not need further optimization and retraining.

Section Title: PRELIMINARY
  PRELIMINARY Our framework leverages the training techniques in US-Net ( Yu & Huang, 2019 ). Therefore, we first introduce these techniques in this section to make this paper self-contained. Sandwich Rule. US-Net trains a network that is executable at any resource constraint. The solu- tion is to randomly sample several network widths for training and accumulate their gradients for optimization. However, the performance of all the sub-networks is bounded by the smallest width (e.g., 0.25×) and the largest width (e.g., 1.0×). Thus, the authors introduce the sandwich rule that is always sampling the smallest and largest width plus two random widths for each training iteration. Under review as a conference paper at ICLR 2020 Inplace Distillation. Knowledge distillation ( Hinton et al., 2015 ) is an effective method to transfer knowledge from a teacher network to a student network. Following the sandwich rule, since the largest network is sampled in each iteration, it is natural to use the largest network as the teacher to guide smaller sub-networks. The largest network is supervised by the ground truth label. This gives a better performance than only training with ground truth labels.

Section Title: Post-Statistics of Batch Normalization
  Post-Statistics of Batch Normalization US-Net proposes that each sub-network needs their own batch normalization (BN) statistics (mean and variance), but it is insufficient to store the statistics of all the sub-networks. As a result, US-Net collects BN statistics for the desired sub-network after training. Experimental results show that 2,000 samples are sufficient to get accurate BN statistics, so this procedure is very efficient.

Section Title: MUTUAL LEARNING FROM WIDTH AND RESOLUTION
  MUTUAL LEARNING FROM WIDTH AND RESOLUTION As discussed in Section 2, different resolutions contain different information. We want to take full advantage of this attribute to learn robust representations. In US-Net, sandwich rule is pro- posed to train a network that is executable at any width. However, we view it as a scheme of mutual learning ( Zhang et al., 2018b ). Since networks with different widths share weights with each other, larger networks can take advantage of the features captured by smaller net- works. Also, smaller networks can benefit from the stronger representation ability of larger net- works. In light of this, we propose to mutually learn from network widths and resolutions. Training framework. Our proposed framework is presented in  Figure 2 . We train a network where its width ranges from 0.25× to 1.0×. We first follow the sandwich rule to sample four sub-networks, i.e., the smallest (0.25×), the largest (1.0×) and two ran- dom ones. Then, unlike traditional ImageNet train- ing with 224×224 input, we resize the input image to four resolutions {224, 196, 160, 128} and feed them into different sub-networks. We denote the weights of a sub-network as W 0:w , where w is the width of the sub-network and 0 : w means the sub-network adopts the first w × 100% weights of each layer of the full network. I R=r represents a r × r input im- age. Then N (W 0:w , I R=r ) represents a sub-network with width w and input resolution r × r. For the largest sub-network (i.e., the full-network in  Figure 2 ), we always train it with the highest resolution (224 × 224) and ground truth label y. The loss for the full network is For the other sub-networks, we randomly pick an input resolution from {224, 196, 160, 128} and train it with the output of the full-network. The loss for the i-th sub-network is loss subi = KLDiv(N (W 0:wi , I R=ri ), N (W 0:1 , I R=224 )), (3) where KLDiv is the Kullback-Leibler divergence. The total loss is the summation of the full- network and sub-networks, i.e., The reason for training the full-network with the highest resolution is that the highest resolution contains more details. Also, the full-network has the strongest learning ability and captures the detailed dicriminatory information from the image data. We experiment with randomly selecting resolutions for all four widths, but it yields worse performance.

Section Title: How mutual learning works
  How mutual learning works In this part, we are going to explain why the proposed framework can mutually learn from different widths and resolutions. For simplicity, we only consider two network widths 0.3× and 0.8×, and two resolutions 128 and 192 in this example. As shown in  Figure 3 , sub-network 0.3× selects input resolution 128, sub-network 0.8× selects input resolution 192. Then we can define the gradients for sub-network 0.3× and 0.8× as grad W0:0.3,,I R=128 and Under review as a conference paper at ICLR 2020 grad W0:0.8,,I R=192 , respectively. Since 0.8× shares weights with 0.3×, we can decompose its gra- dient as grad W0:0.8,,I R=192 = grad W0:0.3,,I R=192 + grad W0.3:0.8,,I R=192 . The gradients of the two networks are accumulated during training, and the total gradients are computed as Therefore, the gradient for sub-network 0.3× is (grad W0:0.3,,I R=128 + grad W0:0.3,,I R=192 ), which enables it to capture multi-scale representations from different input resolutions. Thanks to the random sampling of network width, every sub-network is able to learn multi-scale representations in our framework.

Section Title: Model Inference
  Model Inference The trained model is executable at various width-resolution configurations. The goal is to find the best configuration under a particular resource constraint. A simple way to achieve this is via query table. Specifically, we sample network width from 0.25× to 1.0× with a step- size of 0.05×, and sample network resolution from {224, 192, 160, 128}. We test all these width- resolution configurations on a validation set and choose the best one under a given constraint (FLOPs or latency). Since there is no retraining, the whole process is once for all.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we first present our results on ImageNet classification ( Deng et al., 2009 ). We conduct extensive experiments to illustrate the effectiveness of the proposed framework. Next, we fine-tune the pre-trained model on popular transfer learning datasets to demonstrate the robustness and generalization ability of the learned representations. We further apply our framework to COCO object detection and instance segmentation. To the best of our knowledge, we are the first to benchmark arbitrary-constraint dynamic networks on detection and instance segmentation.

Section Title: IMAGENET CLASSIFICATION
  IMAGENET CLASSIFICATION We compare our framework with US-Net and independently-trained networks on the ImageNet dataset. We evaluated our framework on two popular light-weight structures, MobileNet v1 ( Howard et al., 2017 ) and MobileNet v2 ( Sandler et al., 2018 ). These two networks also represent non-residual and residual structures respectively. We follow the training setting in US-Net ( Yu & Huang, 2019 ). Please refer to the details in Appendix A.1.

Section Title: Compare with US-Net
  Compare with US-Net We first compare our framework with US-Net on MobileNet v1 and Mo- bileNet v2 backbones. The Accuracy-FLOPs curve is shown in  Figure 4 . We can see that our framework consistently outperforms US-Net on both MobileNet v1 and MobileNet v2 backbones. Specifically, we achieve significant improvement under small computational cost. This is because our framework can find a better balance between network width and resolution. For example, if the resource constraint is 150 MFLOPs, US-Net reduces the width to 0.5× but our framework selects a balanced configuration of 0.7× - 160. Moreover, our framework even improves on the full configura- tion (1.0× - 224). This demonstrates that our framework is able to learn multi-scale representations by our mutual learning scheme, which further boosts the performance.

Section Title: Compare with US-Net plus resolution
  Compare with US-Net plus resolution As evident in  Figure 1 , applying different resolutions to US-Net during inference can already achieve improvements over the original US-Net. We denote this method as US-Net+. However, US-Net+ cannot achieve the optimal width-resolution balance due to lack of learning. In  Figure 5 , we plot the Accuracy-FLOPs curves of our framework and US- Net+ based on MobileNet v1 backbone, and highlight the selected input resolutions with different colors. We can see that as we decrease the FLOPs (569 → 468 MFLPs), our framework first reduces network width to meet the constraint while keeping the 224×224 resolution (red color line in  Figure 5 ). After 468 MFLOPs, our framework selects lower input resolution (192) and then continues reducing the width to meet the constraint. On the other hand, US-Net+ cannot find such balance. It always slims the network width and uses the same (224) resolution as the FLOPs decreasing until it goes to really low. This is because US-Net+ does not incorporate input resolution into the learning framework. Simply applying different resolutions during testing cannot achieve the optimal width-resolution balance.

Section Title: Compare with independently trained networks
  Compare with independently trained networks To demonstrate that our framework is able to cap- ture multi-scale representations for each sub-network, we compare with different width-resolution configurations. MobileNets ( Howard et al., 2017 ;  Sandler et al., 2018 ) have reported the results under different configurations. However, they consider width and resolution as independent factors, thus cannot take full advantage of the features contained in different resolutions. We compare the performance of our framework and independently-trained MobileNets in  Figure 6 . For MobileNet v1, widths are selected from {1.0×, 0.75×, 0.5×, 0.25×}, and resolutions are selected from {224, 192, 160, 128}, leading to 16 configurations in total. Similarly, MobileNet v2 selects configura- tions from {1.0×, 0.75×, 0.5×, 0.35×} and {224, 192, 160, 128}. Our framework is executable at any FLOPs between the lower bound and upper bound. From  Figure 6 , our framework consis- tently outperforms MobileNets. Even at the same width-resolution configurations, we can achieve much better performance. This demonstrates that our framework not only finds the optimal width- resolution balance but also learns stronger representations by our mutual learning scheme. To further validate the effectiveness of our mutual learning scheme, we compare our results with the state-of-the-art EfficientNet ( Tan & Le, 2019 ). EfficientNet acknowledges the importance of bal- ancing between network width, depth and resolution. But the authors also treat them as independent factors. They propose to grid search over these three dimensions to find the optimal configuration under certain constraint. Then the network can be scaled up to other constraints following this con- figuration. We compare with the best configuration they found for MobileNet v1 at 2.3 BFLOPs. To meet this constraint, we train our framework with a width range of [1.0×, 2.0×], and select res- olutions from {224, 256, 288, 320}. This makes our framework executable in the range of [0.57, 4.5] BFLOPs. The results are compared in  Table 1 . Our framework finds similar configuration as EfficientNet, except that we arrive at a larger width since depth is not considered. But our frame- work achieves significantly better performance than EfficientNet. We attribute this to the mutual learning from width and resolution scheme.

Section Title: TRANSFER LEARNING
  TRANSFER LEARNING To evaluate the representations learned by our framework, we further conduct experiments on three popular transfer learning datasets, Cifar-100 ( Krizhevsky, 2009 ), Food-101 ( Bossard et al., 2014 ) and MIT-Indoor67 ( Quattoni & Torralba, 2009 ). Cifar-100 is superordinate-level object classifica- tion, Food-101 is fine-grained classification and MIT-Indoor67 is scene classification. Such large variety is suitable to evaluate the robustness of the learned representations. We compare our frame- work with US-Net and independently-trained MobileNet v1. We fine-tune all the models with a batch size of 256, initial learning rate of 0.1 with cosine decay schedule and a total of 100 epochs. Following the setting in ImageNet training, we use width range [0.25×, 1.0×] and resolutions {224, 192, 160, 128}. The results are shown in  Figure 7 . Again, our framework achieves consistently Under review as a conference paper at ICLR 2020 better performance than US-Net and MobileNet. This verifies that our framework is able to learn well-generalized representations.

Section Title: OBJECT DETECTION AND INSTANCE SEGMENTATION
  OBJECT DETECTION AND INSTANCE SEGMENTATION We also apply our framework to COCO object detection and instance segmentation ( Lin et al., 2014 ). Our experiments are based on Mask-RCNN-FPN ( He et al., 2017 ;  Lin et al., 2017 ) and MMDetection ( Chen et al., 2019a ) toolbox. We use VGG-16 ( Simonyan & Zisserman, 2014 ) as backbone network to validate our proposed mutual learning scheme. We first pre-train VGG-16 on ImageNet using US-Net and our framework respectively. The net- work width range is [0.25×, 1.0×] and resolutions are {224, 192, 160, 128}. Then we fine-tune the pre-trained models on COCO. The FPN neck and detection head are shared among different sub-networks. For simplicity, we don't use inplace distillation. Rather, each sub-network is trained with the ground truth. The other training procedures are the same as training ImageNet classifi- cation. Following common settings in object detection, US-Net is trained with image resolution 1000 × 600. Our framework randomly selects resolutions from 1000 × {600, 480, 360, 240}. All models are trained on COCO 2017 training set and tested on COCO 2017 validation set with dif- ferent image resolutions. The Average Precision (AP) results for object detection and instance seg- mentation under different computational cost are presented in  Figure 8 . These results reveal that our framework significantly outperforms US-Net under all resource constraints. Specifically, on the full configuration (1.0×-600), US-Net achieves comparable results with independent VGG-16, while our framework performs much better. This again validates the effectiveness of our width-resolution mutual learning scheme. Please refer to Appendix A.4 for visual comparison results.

Section Title: CONCLUSION AND FUTURE WORK
  CONCLUSION AND FUTURE WORK This paper highlights the importance of simultaneously considering both network width and input resolution for designing efficient network structures. A new framework is defined capable of mutu- ally learning from network width and input resolution. The proposed framework is demonstrated to significantly improve inference performance per FLOP for various datasets and tasks. The simplic- ity and generality of the framework allows it to translate well to generic problem domains. This also make logical extensions readily available by adding other network dimensions, e.g., network depth, to the framework. The framework can also be extended to video input and 3D neural networks, where we can leverage both spatial and temporal information. Under review as a conference paper at ICLR 2020

```
