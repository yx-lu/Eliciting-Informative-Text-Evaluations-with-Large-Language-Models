Title:
```
Published as a conference paper at ICLR 2020 PROJECTION-BASED CONSTRAINED POLICY OPTIMIZATION
```
Abstract:
```
We consider the problem of learning control policies that optimize a reward func- tion while satisfying constraints due to considerations of safety, fairness, or other costs. We propose a new algorithm, Projection-Based Constrained Policy Opti- mization (PCPO). This is an iterative method for optimizing policies in a two-step process: the first step performs a local reward improvement update, while the sec- ond step reconciles any constraint violation by projecting the policy back onto the constraint set. We theoretically analyze PCPO and provide a lower bound on re- ward improvement, and an upper bound on constraint violation, for each policy update. We further characterize the convergence of PCPO based on two different metrics: L 2 norm and Kullback-Leibler divergence. Our empirical results over several control tasks demonstrate that PCPO achieves superior performance, aver- aging more than 3.5 times less constraint violation and around 15% higher reward compared to state-of-the-art methods. 1 1 For code see the project website: https://sites.google.com/view/iclr2020-pcpo
```

Figures/Tables Captions:
```
Figure 1: Update procedures for PCPO. In step one (red arrow), PCPO follows the reward im- provement direction in the trust region (light green). In step two (blue arrow), PCPO projects the policy onto the constraint set (light orange).
Figure 2: Update procedures for CPO (Achiam et al., 2017). CPO computes the update by si- multaneously considering the trust region (light green) and the constraint set (light orange). CPO becomes infeasible when these two sets do not in- tersect.
Figure 3: The gather, circle, grid and bottleneck tasks. (a) Gather task: the agent is rewarded for gathering green apples but is constrained to collect a limited number of red fruit (Achiam et al., 2017). (b) Circle task: the agent is rewarded for moving in a specified wide circle, but is constrained to stay within a safe region smaller than the radius of the circle (Achiam et al., 2017). (c) Grid task: the agent controls the traffic lights in a grid road network and is rewarded for high throughput but constrained to let lights stay red for at most 7 consecutive seconds (Vinitsky et al., 2018). (d) Bottleneck task: the agent controls a set of autonomous vehicles (shown in red) in a traffic merge situation and is rewarded for achieving high throughput but constrained to ensure that human-driven vehicles (shown in white) have low speed for no more than 10 seconds (Vinitsky et al., 2018).
Figure 4: The values of the discounted reward and the undiscounted constraint value (the total number of constraint violation) along policy updates for the tested algorithms and task pairs. The solid line is the mean and the shaded area is the standard deviation, over five runs. The dashed line in the cost constraint plot is the cost constraint threshold h. The curves for baseline oracle, TRPO, indicate the reward and constraint violation values when the constraint is ignored. (Best viewed in color, and the legend is shared across all the figures.) Gaussian
Figure 5: The value of the discounted reward versus the cumulative constraint value for the tested algorithms and task pairs. See the supplemental material for learning curves in the other tasks. PCPO achieves less constraint violation under the same reward improvement compared to the other algorithms.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Recent advances in deep reinforcement learning (RL) have demonstrated excellent performance on several domains ranging from games like Go ( Silver et al., 2017 ) and StarCraft ( AlphaStar, 2019 ) to robotic control ( Levine et al., 2016 ). In these settings, agents are allowed to explore the entire state space and experiment with all possible actions during training. However, in many real- world applications such as self-driving cars and unmanned aerial vehicles, considerations of safety, fairness and other costs prevent the agent from having complete freedom to explore. For instance, an autonomous car, while optimizing its driving policies, must not take any actions that could cause harm to pedestrians or property (including itself). In effect, the agent is constrained to take actions that do not violate a specified set of constraints on state-action pairs. In this work, we address the problem of learning control policies that optimize a reward function while satisfying predefined constraints. The problem of policy learning with constraints is more challenging since directly optimizing for the reward, as in Q-Learning ( Mnih et al., 2013 ) or policy gradient ( Sutton et al., 2000 ), will usu- ally violate the constraints. One approach is to incorporate constraints into the learning process by forming a constrained optimization problem. Then perform policy updates using a conditional gradient descent with line search to ensure constraint satisfaction ( Achiam et al., 2017 ). However, the base optimization problem can become infeasible if the current policy violates the constraints. Another approach is to add a hyperparameter weighted copy of the constraints to the objective func- tion ( Tessler et al., 2018 ). However, this incurs the cost of extensive hyperparameter tuning. To address the above issues, we propose projection-based constrained policy optimization (PCPO). This is an iterative algorithm that performs policy updates in two stages. The first stage maximizes reward using a trust region optimization method (e.g., TRPO ( Schulman et al., 2015a )) without Published as a conference paper at ICLR 2020 constraints. This might result in a new intermediate policy that does not satisfy the constraints. The second stage reconciles the constraint violation (if any) by projecting the policy back onto the constraint set, i.e., choosing the policy in the constraint set that is closest to the selected interme- diate policy. This allows efficient updates to ensure constraint satisfaction without requiring a line search ( Achiam et al., 2017 ) or adjusting a weight ( Tessler et al., 2018 ). Further, due to the projec- tion step, PCPO offers efficient recovery from infeasible (i.e., constraint-violating) states (e.g., due to approximation errors), which existing methods do not handle well. We analyze PCPO theoretically and derive performance bounds for the algorithm. Specifically, based on information geometry and policy optimization theory, we construct a lower bound on reward improvement, and an upper bound on constraint violations for each policy update. We find that with a relatively small step size for each policy update, the worst-case constraint violation and reward degradation are tolerable. We further analyze two distance measures for the projection step onto the constraint set. We find that the convergence of PCPO is affected by the smallest and largest singular values of the Fisher information matrix used during training. By observing these singular values, we can choose the appropriate projection best suited to the problem. Empirically, we compare PCPO with state-of-the-art algorithms on four different control tasks, in- cluding two Mujoco environments with safety constraints introduced by  Achiam et al. (2017)  and two traffic management tasks with fairness constraints introduced by  Vinitsky et al. (2018) . In all cases, the proposed algorithm achieves comparable or superior performance to prior approaches, averaging more reward with fewer cumulative constraint violations. For instance, across the above tasks, PCPO achieves 3.5 times fewer constraint violations and around 15% more reward. This demonstrates the ability of PCPO robustly learn constraint-satisfying policies, and represents a step towards reliable deployment of RL in real problems.

Section Title: PRELIMINARIES
  PRELIMINARIES We frame our policy learning as a constrained Markov Decision Process (CMDP) ( Altman, 1999 ), where policies will direct the agent to maximize the reward while minimizing the cost. We define CMDP as the tuple < S, A, T, R, C >, where S is the set of states, A is the set of actions that the agent can take, T : S × A × S → [0, 1] is the transition probability of the CMDP, R : S × A → R is the reward function, and C : S × A → R is the cost function. Given the agent's current state s, the policy π(a|s) : S → A selects an action a for the agent to take. Based on s and a, the agent transits to the next state (denoted by s ) according to the state transition model T (s |s, a), and receives the reward and pays the cost, denoted by R(s, a) and C(s, a), respectively. We aim to learn a policy π that maximizes a cumulative discounted reward, denoted by J R (π) . = E τ ∼π ∞ t=0 γ t R(s t , a t ) , while satisfying constraints, i.e., making a cumulative discounted cost constraint below a desired threshold h, denoted by J C (π) . = E τ ∼π ∞ t=0 γ t C(s t , a t ) ≤ h, where γ is the discount factor, τ is the trajectory (τ = (s 0 , a 0 , s 1 , · · · )), and τ ∼ π is shorthand for showing that the distribution over the trajectory depends on π : s 0 ∼ µ, a t ∼ π(a t |s t ), s t+1 ∼ T (s t+1 |s t , a t ), where µ is the initial state distribution.  Kakade & Langford (2002)  give an identity to express the performance of policy π in terms of the advantage function over another policy π : J R (π ) − J R (π) = 1 1 − γ E s∼d π a∼π [A π R (s, a)], (1) where d π is the discounted future state distribution, denoted by d π (s) . = (1 − γ) ∞ t=0 γ t P (s t = s|π), and A π R (s, a) is the reward advantage function, denoted by A π R (s, a) . = Q π R (s, a) − V π R (s). Here Q π R (s, a) . = E τ ∼π ∞ t=0 γ t R(s t , a t )|s 0 = s, a 0 = a is the discounted cu- mulative reward obtained by the policy π given the initial state s and action a, and V π R (s) . = Published as a conference paper at ICLR 2020 E τ ∼π ∞ t=0 γ t R(s t , a t )|s 0 = s is the discounted cumulative reward obtained by the pol- icy π given the initial state s. Similarly, we have the cost advantage function

Section Title: PROJECTION-BASED CONSTRAINED POLICY OPTIMIZATION
  PROJECTION-BASED CONSTRAINED POLICY OPTIMIZATION To robustly learn constraint-satisfying policies, we develop PCPO - a trust region method that performs policy updates corresponding to reward improvement, followed by projections onto the constraint set. PCPO, inspired by pro- jected gradient descent, is composed of two steps for each update, a reward improvement step and a projection step (This is illustrated in  Fig. 1 ).

Section Title: Reward Improvement Step
  Reward Improvement Step First, we opti- mize the reward function by maximizing the reward advantage function A π R (s, a) subject to a Kullback-Leibler (KL) divergence constraint. This constraints the intermediate policy π k+ 1 2 to be within a δ-neighbourhood of π k : This update rule with the trust region, {π : E s∼d π k D KL (π||π k )[s] ≤ δ}, is called Trust Region Policy Optimization (TRPO) ( Schulman et al., 2015a ). It constraints the policy changes to a diver- gence neighborhood and guarantees reward improvement.

Section Title: Projection Step
  Projection Step Second, we project the intermediate policy π k+ 1 2 onto the constraint set by mini- mizing a distance measure D between π k+ 1 2 and π: The projection step ensures that the constraint-satisfying policy π k+1 is close to π k+ 1 2 . We consider two distance measures D: L 2 norm and KL divergence. In contrast, using KL divergence projection in the probability distribution space allows us to provide provable guarantees for PCPO.

Section Title: PERFORMANCE BOUND FOR PCPO WITH KL DIVERGENCE PROJECTION
  PERFORMANCE BOUND FOR PCPO WITH KL DIVERGENCE PROJECTION In safety-critical applications such as autonomous cars, one cares about how worse the performance of a system evolves when applying a learning algorithm. To this end, for PCPO with KL divergence projection, we analyze the worst-case performance degradation for each policy update when the current policy π k satisfies the constraint. The following theorem provides a lower bound on reward improvement, and an upper bound on constraint violation for each policy update. Theorem 3.1 (Worst-case Bound on Updating Constraint-satisfying Policies). Define π k+1 R . = max s E a∼π k+1 [A π k R (s, a)] , and π k+1 C . = max s E a∼π k+1 [A π k C (s, a)] . If the current policy π k satis- fies the constraint, then under KL divergence projection, the lower bound on reward improvement, and upper bound on constraint violation for each policy update are where δ is the step size in the reward improvement step. Theorem 3.1 indicates that if δ is small, the worst-case performance degradation is tolerable. Due to approximation errors or the random initialization of policies, PCPO may have a constraint- violating update. Theorem 3.1 does not give the guarantee on updating a constraint-violating policy. Hence we analyze worst-case performance degradation for each policy update when the current policy π k violates the constraint. The following theorem provides a lower bound on reward im- provement, and an upper bound on constraint violation for each policy update. 2a T H −1 a , where a is the gradient of the cost advantage function and H is the Hessian of the KL divergence constraint. If the current policy π k violates the constraint, then under KL divergence projection, the lower bound on reward improvement and the upper bound on constraint violation for each policy update are where δ is the step size in the reward improvement step. Proof. See the supplemental material. Theorem 3.2 indicates that when the policy has greater constraint violation (b + increases), its worst- case performance degradation increases. Note that Theorem 3.2 reduces to Theorem 3.1 if the current policy π k satisfies the constraint (b + = 0). The proofs of Theorem 3.1 and Theorem 3.2 follow from the fact that the projection of the policy is non-expansive, i.e., the distance between the projected policies is smaller than that of the unprojected policies. This allows us to measure it and bound the KL divergence between the current policy and the new policy.

Section Title: PCPO UPDATES
  PCPO UPDATES For a large neural network policy with many parameters, it is impractical to directly solve for the PCPO update in Problem 2 and Problem 3 due to the computational cost. However, with a small step size δ, we can approximate the reward function and constraints with a first order expansion, and approximate the KL divergence constraint in the reward improvement step, and the KL divergence measure in the projection step with a second order expansion. We now make several definitions: g . = ∇ θ E s∼d π k a∼π [A π k R (s, a)] is the gradient of the reward advantage function, a . = ∇ θ E s∼d π k a∼π [A π k C (s, a)] is the gradient of the cost advantage function, H i,j . = ∂ 2 E s∼d π k DKL(π||π k )[s] ∂θj ∂θj is the Hessian of the KL divergence constraint (H is also called the Fisher information matrix. It is symmetric positive semi-definite), b . = J C (π k ) − h is the constraint violation of the policy π k , and θ is the parameter of the policy.

Section Title: Reward Improvement Step
  Reward Improvement Step We linearize the objective function at π k subject to second order approximation of the KL divergence constraint in order to obtain the following updates: Published as a conference paper at ICLR 2020

Section Title: Algorithm 1 Projection-Based Constrained Policy Optimization (PCPO)
  Algorithm 1 Projection-Based Constrained Policy Optimization (PCPO) Projection Step. If the projection is defined in the parameter space, we can directly use L 2 norm projection. On the other hand, if the projection is defined in the probability space, we can use KL divergence projection. This can be approximated through the second order expansion. Again, we linearize the cost constraint at π k . This gives the following update for the projection step: where L = I for L 2 norm projection, and L = H for KL divergence projection. One may argue that using linear approximation to the constraint set is not enough to ensure constraint satisfaction since the real constraint set is maybe non-convex. However, if the step size δ is small, then the linearization of the constraint set is accurate enough to locally approximate it. We solve Problem (4) and Problem (5) using convex programming (See the supplemental material for the derivation). For each policy update, we have We assume that H does not have 0 as an eigenvalue and hence it is invertible. PCPO requires to in- vert H, which is impractical for huge neural network policies. Hence we use the conjugate gradient method ( Schulman et al., 2015a ). Algorithm 1 shows the pseudocode. (See supplemental material for a discussion of the tradeoff between the approximation error and computational efficiency of the conjugate gradient method.)

Section Title: Analysis of PCPO Update Rule
  Analysis of PCPO Update Rule For a problem including multiple constraints, we can extend the update in Eq. (6) by using alternating projections. This approach finds a solution in the intersection of multiple constraint sets by sequentially projecting onto each of the sets. The update rule in Eq. (6) shows that the difference between PCPO with KL divergence and L 2 norm projections is the cost update direction, leading to a difference in reward improvement. These two projections converge to different stationary points with different convergence rates related to the smallest and largest singular values of the Fisher information matrix shown in Theorem 4.1. For our analysis, we make the following assumptions: we minimize the negative reward objective function f : R n → R (We follow the convention of the literature that authors typically minimize the objective function). The function f is L-smooth and twice continuously differentiable over the closed and convex constraint set C. Theorem 4.1 (Reward Improvement Under L 2 Norm and KL Divergence Projections). Let η . = 2δ g T H −1 g in Eq. (6), where δ is the step size for reward improvement, g is the gradient of f, and H is the Fisher information matrix. Let σ max (H) be the largest singular value of H, and a be the gradient of cost advantage function in Eq. (6). Then PCPO with KL divergence projection converges to a stationary point either inside the constraint set or in the boundary of the constraint set. In the latter case, the Lagrangian constraint g = −αa, α ≥ 0 holds. Moreover, at step k + 1 the objective value satisfies PCPO with L 2 norm projection converges to a stationary point either inside the constraint set or in the boundary of the constraint set. In the latter case, the Lagrangian constraint H −1 g = −αa, α ≥ Published as a conference paper at ICLR 2020 0 holds. If σ max (H) ≤ 1, then a step k + 1 objective value satisfies Proof. See the supplemental material. Theorem 4.1 shows that in the stationary point g is a line that points to the opposite direction of a. Further, the improvement of the objective value is affected by the singular value of the Fisher information matrix. Specifically, the objective of KL divergence projection decreases when Lη 2 I ≺ H, implying that σ min (H) > Lη 2 . And the objective of L 2 norm projection decreases when η < 2 L , implying that condition number of H is upper bounded: σmax(H) σmin(H) < 2||g|| 2 2 L 2 δ . Observing the singular values of the Fisher information matrix allows us to adaptively choose the appropriate projection and hence achieve objective improvement. In the supplemental material, we further use an example to compare the optimization trajectories and stationary points of KL divergence and L 2 norm projections.

Section Title: RELATED WORK
  RELATED WORK

Section Title: Policy Learning with Constraints
  Policy Learning with Constraints Learning constraint-satisfying policies has been explored in the context of safe RL ( Garcia & Fernandez, 2015 ). The agent learns policies either by (1) exploration of the environment ( Achiam et al., 2017 ;  Tessler et al., 2018 ;  Chow et al., 2017 ) or (2) through expert demonstrations ( Ross et al., 2011 ;  Rajeswaran et al., 2017 ;  Gao et al., 2018 ). However, using expert demonstrations requires humans to label the constraint-satisfying behavior for every possible situation. The scalability of these rule-based approaches is an issue since many real autonomous systems such as self-driving cars and industrial robots are inherently complex. To overcome this issue, PCPO uses the first approach in which the agent learns by trial and error. To prevent the agent from having constraint-violating behavior during exploring the environment, PCPO uses the projection onto the constraint set to ensure constraint satisfaction throughout learning.

Section Title: Constraint satisfaction by Projections
  Constraint satisfaction by Projections Us- ing a projection onto a constraint set has been explored for general constrained optimization in other contexts. For example,  Akrour et al. (2019)  projects the policy from a parameter space onto the constraint. This ensures the up- dated policy stays close to the previous policy. In contrast, we examine constraints that are de- fined in terms of states and actions. Similarly,  Chow et al. (2019)  proposes θ-projection. This approach projects the policy parameters θ onto the constraint set. However, no provide prov- able guarantees are provided. Moreover, the problem is formulated by adding the weighted constraint to the reward objective function. Since the weight must be tuned, this incurs the cost of hyperparameter tuning. In contrast, PCPO eliminates the cost of the hyperparameter tuning, and provides provable guarantees on learning constraint- satisfying policies. Comparison to CPO ( Achiam et al., 2017 ). Perhaps the closest work to ours is the approach of  Achiam et al. (2017) , who proposes the constrained policy optimization (CPO) algorithm to solve the following: CPO simultaneously considers the trust region and the constraint, and uses the line search to select a step size (This is illustrated in  Fig. 2 ). The update rule of CPO becomes infeasible when the current policy violates the constraint (b > 0). CPO recovers by replacing Problem (7) with an update to purely decrease the constraint value: θ k+1 = θ k − 2δ a T H −1 a H −1 a. This update rule may lead Published as a conference paper at ICLR 2020 (a) Gather (b) Circle (c) Grid (d) Bottleneck to a slow progress in learning constraint-satisfying policies. In contrast, PCPO first optimizes the reward and uses the projection to satisfy the constraint. This ensures a feasible solution, allowing the agent to improve the reward while ensuring constraint satisfaction simultaneously.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: Tasks
  Tasks We compare the proposed algorithm with existing approaches on four control tasks in total: two tasks with safety constraints ((a) and (b) in  Fig. 3 ), and two tasks with fairness constraints ((c) and (d) in  Fig. 3 ). These tasks are briefly described in the caption of  Fig. 3 . The first two tasks - Gather and Circle - are Mujoco environments with state space constraints introduced by  Achiam et al. (2017) . The other two tasks - Grid and Bottleneck - are traffic management problems where the agent controls either a traffic light or a fleet of autonomous vehicles. This is especially challenging since the dimensions of state and action spaces are larger, and the dynamics of the environment are inherently complex.

Section Title: Baselines
  Baselines We compare PCPO with four baselines outlined below. (1) Constrained Policy Optimization (CPO) ( Achiam et al., 2017 ). (2) Primal-dual Optimization (PDO) ( Chow et al., 2017 ). In PDO, the weight (dual variables) is learned based on the current constraint satisfaction. A PDO policy update solves: θ k+1 = arg max θ g T (θ − θ k ) + λ k a T (θ − θ k ), (8) where λ k is updated using λ k+1 = λ k + β(J C (π k ) − h). Here β is a fixed learning rate. (3) Fixed-point Policy Optimization (FPO). A variant of PDO that solves Eq. (8) using a constant λ. (4) Trust Region Policy Optimization (TRPO) ( Schulman et al., 2015a ). The TRPO policy update is an unconstrained one: Note that TRPO ignores any constraints. We include it to serve as an upper bound baseline on the reward performance. Since the main focus is to compare PCPO with the state-of-the-art algorithm, CPO, PDO and FPO are not shown in the ant circle, ant gather, grid and bottleneck tasks for clarity.

Section Title: Experimental Details
  Experimental Details

Section Title: policies
  policies

Section Title: Overall Performance
  Overall Performance The learning curves of the discounted reward and the undiscounted con- straint value (the total number of constraint violation) over policy updates are shown for all tested algorithms and tasks in  Fig. 4 . The dashed line in the constraint figure is the cost constraint threshold h. The curves for baseline oracle, TRPO, indicate the reward and constraint value when the con- straint is ignored. Overall, we find that PCPO is able to improve the reward while having the fastest constraint satisfaction in all tasks. In particular, PCPO is the only algorithm that learns constraint- satisfying policies across all the tasks. Moreover we observe that (1) CPO has more constraint violation than PCPO, (2) PDO is too conservative in optimizing the reward, and (3) FPO requires a significant effort to select a good value of λ. We also observe that in Grid and Bottleneck task, there is slightly more constraint violation than the easier task such as point circle and point gather. This is due to complexity of the policy behavior and non-convexity of the constraint set. However, even with a linear approximation of the constraint set, PCPO still outperforms CPO with 85.15% and 5.42 times less constraint violation in Grid and Bottleneck task, respectively. These observations suggest that projection step in PCPO drives the agent to learn the constraint- satisfying policy within few policy updates, giving PCPO an advantage in applications. To show that PCPO achieves the same reward with less constraint violation, we examine the reward versus the cu- mulative constraint value for the tested algorithms in point circle and point gather task shown in  Fig. 5 . We observe that PCPO outperforms CPO significantly with 66 times and 15 times less constraint violation under the same reward improvement in point circle and point gather tasks, respectively. This observation suggests that PCPO enables the agent to cautiously explore the environment under the constraints.

Section Title: Comparison of PCPO with KL Divergence vs. L 2 Norm Projections
  Comparison of PCPO with KL Divergence vs. L 2 Norm Projections We observe that PCPO with L 2 norm projection is more constraint-satisfying than PCPO with KL divergence projection. In addition, PCPO with L 2 norm projection tends to have reward fluctuation (point circle, ant cir- cle, and ant gather tasks), while with KL divergence projection tends to have more stable reward improvement (all the tasks). The above observations indicate that since the gradient of constraint is not multiplied by the Fisher information matrix, the gradient of the constraint is not aligned with the gradient of the reward. This reduces the reward improvement. However, when the Fisher information matrix is ill-conditioned or not well-estimated, especially in a high dimensional policy space, a bad constraint update direction may hinder constraint satisfaction (ant circle, ant gather, grid and bottleneck tasks). In addition, since the stationary points of KL divergence and L 2 norm projections are different, they converge to policies with different reward (observe that PCPO with L 2 norm projection has higher reward than the one with KL divergence projection around 2250 iterations in ant circle task, and has less reward in point gather task).

Section Title: Discussion of PDO and FPO
  Discussion of PDO and FPO For the PDO baseline, we see that its constraint values fluctuate especially in the point circle task. This phenomena suggests that PDO is not able to adjust the weight λ k quickly enough to meet the constraint threshold, which hinders the efficiency of learning constraint-satisfying policies. If the learning rate β is too big, the agent will be too conservative in improving the reward. For FPO, we also see that it learns near constraint-satisfying policies with slightly larger reward improvement compared to PDO. However, in practice FPO requires a lot of engineering effort to select a good value of λ. Since PCPO requires no hyperparameter tuning, it has the advantage of robustly learning constraint-satisfying policies over PDO and FPO.

Section Title: CONCLUSION
  CONCLUSION We address the problem of finding constraint-satisfying policies. The proposed algorithm - projection-based constrained policy optimization (PCPO) - optimizes for the reward function while using the projections to ensure constraint satisfaction. This update rule allows PCPO to maintain the feasibility of the optimization problem of each update, addressing the issue of state-of-the-art approaches. The algorithm achieves comparable or superior performance to state-of-the-art ap- proaches in terms of reward improvement and constraint satisfaction in all cases. We further analyze the convergence of PCPO, and find that certain tasks may prefer either KL divergence projection or L 2 norm projection. Future work will consider the following: (1) examining the Fisher information matrix to iteratively prescribe the choice of projection for policy update, and hence robustly learn constraint-satisfying policies with more reward improvement, and (2) using expert demonstration or other domain knowledge to reduce the sample complexity.

```
