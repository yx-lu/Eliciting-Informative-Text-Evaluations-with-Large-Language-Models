<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 OUT-OF-DISTRIBUTION DETECTION USING LAYER- WISE UNCERTAINTY IN DEEP NEURAL NETWORKS</article-title></title-group><abstract><p>In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification abil- ity, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experi- ments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification mod- els. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Deep neural networks (DNNs) have achieved high performance in many classification tasks such as image classification (<xref ref-type="bibr" rid="b16">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="b26">Simonyan &amp; Zisserman, 2014</xref>), object detection (<xref ref-type="bibr" rid="b22">Lin et al., 2017</xref>; <xref ref-type="bibr" rid="b25">Redmon &amp; Farhadi, 2018</xref>), and speech recognition (<xref ref-type="bibr" rid="b11">Hinton et al., 2012</xref>; <xref ref-type="bibr" rid="b7">Hannun et al., 2014</xref>). However, DNNs tend to make high confidence predictions even for samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples (<xref ref-type="bibr" rid="b9">Hendrycks &amp; Gimpel, 2016</xref>). Such errors can be harmful to medical diagnosis and automated driving. Because it is not generally possible to control the test data distribution in real-world applications, OOD samples are inevitably included in this distribution. Therefore, detecting OOD samples is important for ensuring the safety of an artificial intelligence system (<xref ref-type="bibr" rid="b2">Amodei et al., 2016</xref>).</p><p>There have been many previous studies (<xref ref-type="bibr" rid="b9">Hendrycks &amp; Gimpel, 2016</xref>; <xref ref-type="bibr" rid="b21">Liang et al., 2017</xref>; <xref ref-type="bibr" rid="b19">Lee et al., 2017</xref>; <xref ref-type="bibr" rid="b5">DeVries &amp; Taylor, 2018</xref>; <xref ref-type="bibr" rid="b19">Lee et al., 2018</xref>; <xref ref-type="bibr" rid="b9">Hendrycks et al., 2018</xref>) that have attempted to solve this problem by regarding samples that are difficult to classify or samples with low classification confidence as OOD examples using DNNs. Their approaches work well and they are computation- ally efficient. The limitation of these studies is that, when using difficult datasets or models with low classification ability, the confidence of inputs will be low, even if the inputs are in-distribution samples. Therefore, these methods incorrectly regard such in-distribution samples as OOD samples, which results in their poor detection performance (<xref ref-type="bibr" rid="b23">Malinin &amp; Gales, 2018</xref>), as shown in <xref ref-type="fig" rid="fig_0">Figure 1</xref>.</p><p>One cause of the abovementioned problem is that their approaches use only the features close to the output layer and the features are strongly related to the classification accuracy. Therefore, we use not only the features close to the output layer but also the features close to the input layer. We hypothesize that the uncertainties of the features close to the input layer are the uncertainties of the feature extraction and are effective for detecting OOD samples. For example, when using convolutional neural networks (CNNs), the filters of the convolutional layer close to the input layer extract features such as edges that are useful for in-distribution classification. In other words, in- distribution samples possess more features that convolutional filters react to than OOD samples. Therefore, the uncertainties of the features will be larger when the inputs are in-distribution samples. Another cause of the abovementioned problem is that their approaches disregard the uncertainty of the features close to the output layer. We hypothesize that the uncertainties of the latent features close Under review as a conference paper at ICLR 2020 Baseline (<xref ref-type="bibr" rid="b9">Hendrycks &amp; Gimpel, 2016</xref>) UFEL (ours) max softmax probability Baseline UFEL (ours) degree of uncertainty to the output layer are the uncertainties of classification and are also effective for detecting OOD samples. For example, in-distribution samples are embedded in the feature space close to the output layer to classify samples. In contrast, OOD samples have no fixed regions for embedding. Therefore, the uncertainties of the features of OOD samples will be larger than those of in-distribution samples. Based on the hypotheses, we propose a method that extracts the Uncertainties of Features in Each Layer (UFEL) and combines them for detecting OOD samples. Each uncertainty is easily estimated after training the discriminative model by computing the mean and the variance of their features using a reparameterization trick such as the variational autoencoder (<xref ref-type="bibr" rid="b14">Kingma &amp; Welling, 2013</xref>) and variational information bottleneck (<xref ref-type="bibr" rid="b0">Alemi et al., 2016</xref>; 2018). Our proposal is agnostic to the model architecture and can be easily combined with any regular architecture with minimum modifications.</p><p>We visualize the maximum values of output probability and the combined uncertainties of the latent features in the feature space of the penultimate layer in <xref ref-type="fig" rid="fig_0">Figure 1</xref>. The combined uncertainties of the features discriminate the in-distribution and OOD images that are difficult to classify. For example, although the images that are surrounded by the red line are in-distribution samples, they have low maximum softmax probabilities and could be regarded as OOD samples in prior work. Meanwhile, their uncertainties are smaller than those of OOD samples and they are regarded as in-distribution samples in our method.</p><p>In experiments, we validate the hypothesis demonstrating that each uncertainty is effective for de- tecting OOD examples. We also demonstrate that UFEL can obtain state-of-the-art performance in several datasets including CIFAR-100, which is difficult to classify, and models including LeNet5 with low classification ability. Moreover, UFEL is robust to hyperparameters such as the number of in-distribution classes and the validation dataset.</p></sec><sec><title>RELATED WORK</title><p>Methods based on the classification confidence <xref ref-type="bibr" rid="b9">Hendrycks &amp; Gimpel (2016)</xref> proposed the base- line method to detect OOD samples without the need to further re-train and change the structure of the model. They define low-maximum softmax probabilities as indicating the low confidence of in-distribution examples and detect OOD samples using the softmax outputs of a pre-trained deep classifier. Building on this work, many models have recently been proposed. <xref ref-type="bibr" rid="b21">Liang et al. (2017)</xref> proposed ODIN, a calibration technique that uses temperature scaling (<xref ref-type="bibr" rid="b6">Guo et al., 2017</xref>) in the Under review as a conference paper at ICLR 2020</p><p>softmax function and adds small controlled perturbations to the inputs to widen the gap between in- distribution and OOD features, which improves the performance of the baseline method. Likewise, <xref ref-type="bibr" rid="b19">Lee et al. (2018)</xref>; DeVries &amp; Taylor (2018); <xref ref-type="bibr" rid="b19">Lee et al. (2017)</xref>; <xref ref-type="bibr" rid="b9">Hendrycks et al. (2018)</xref> also extended the baseline method. Like <xref ref-type="bibr" rid="b9">Hendrycks &amp; Gimpel (2016)</xref>, we use the feature of maximum softmax probability as one of our features.</p><p>Methods based on the uncertainty <xref ref-type="bibr" rid="b23">Malinin &amp; Gales (2018)</xref> attempted to solve the problem of classifying in-distribution samples close to the decision boundary as OOD samples by distinguishing between data uncertainty and distributional uncertainty. Data uncertainty, or aleatoric uncertainty (<xref ref-type="bibr" rid="b13">Kendall &amp; Gal, 2017</xref>), is irreducible uncertainty such as class overlap, whereas distributional un- certainty arises because of the mismatch between training and testing distributions. They argue that the value of distributional uncertainty depends on the difference in the Dirichlet distribution of the categorical parameter. Further, they estimate the parameter of the Dirichlet distribution using a DNN and train the model with in-distribution and OOD datasets. The motivation for our work is similar to that of <xref ref-type="bibr" rid="b23">Malinin &amp; Gales (2018)</xref>. In our work, the distribution of the logit of the categorical pa- rameters is modeled as a Gaussian distribution, which enables us to train the model without an OOD dataset. Furthermore, we estimate the parameters of the Gaussian distribution of latent features close to the input layer.</p></sec><sec><title>PROPOSED METHOD</title><p>In this section, we present UFEL, which extracts the uncertainties of features in each layer and combines them for detecting OOD samples. First, we use the maximum of the softmax output, as in <xref ref-type="bibr" rid="b9">Hendrycks &amp; Gimpel (2016)</xref>, as one of our features. Second, we also use the distribution of the categorical parameter, as in <xref ref-type="bibr" rid="b23">Malinin &amp; Gales (2018)</xref>, using the uncertainty of logits. Furthermore, we use the uncertainty of the feature extraction extracted from the latent space close to the input layer because they will not be relevant to the classification accuracy. We probabilistically model the values of these features, estimate their uncertainties, and combine them.</p><p>Let x &#8712; X be an input, y &#8712; Y = {1, &#183; &#183; &#183; , K} be its label, and l &#8712; {1, &#183; &#183; &#183; , L} be the index of the block layers. The objective function of normal deep classification is as follows: J = E x,y&#8764;p(x,y) [L(f &#966; (x), y)], (1) where p(x, y) is the empirical data distribution, L is a cross entropy loss function, and f &#966; is a DNN. We use the following notation f &#966; = f &#966; L &#8226; f &#966; L&#8722;1 &#8226; &#183; &#183; &#183; &#8226; f &#966;1 as shown in <xref ref-type="fig" rid="fig_1">Figure 2</xref>.</p><p>To extract the uncertainties of features in each layer, we model the lth block layer's output z l as a Gaussian whose parameters depend on the l-1th block layer's output z l&#8722;1 as follows: p(z l |z l&#8722;1 ) = N (z l |f &#181; &#966; l (z l&#8722;1 ), f &#931; &#966; l (z l&#8722;1 )), where f &#966; l is the lth block layer, which outputs both the mean &#181; and covariance matrix &#931;. In this paper, we use a diagonal covariance matrix to reduce the model param- eters. We use the reparameterization trick (<xref ref-type="bibr" rid="b14">Kingma &amp; Welling, 2013</xref>) to write z l = &#181; l + &#963; l , where &#181; l = f &#181; &#966; l (z l&#8722;1 ), &#963; l = f &#931; &#966; l (z l&#8722;1 ), and is the Gaussian noise. Then, our objective function is as follows:</p><p>Under review as a conference paper at ICLR 2020 where z 0 = x. Because of the reparameterization trick, the loss gradient is backpropagated directly through our model, and we can train our model like the regular classification models in Equation 1.</p><p>Next, we explain the two methods of combining the features extracted in each layer. In the first method, we sum the uncertainties of each value of the features in each layer and linearly combine them. Because the feature maps of a convolutional block layer are three dimensional, each element is computed as z l ijk = &#181; l ijk + &#963; l ijk . Moreover, because the output of a fully connected layer is one dimensional, each element is formed as z l i = &#181; l i + &#963; l i . We use a weighted summation of the scale of each feature and the maximum value of the softmax scores as a final feature d LR as follows:</p><p>We choose the parameter &#955; l by training a logistic regression (LR) using in-distribution and OOD validation samples. In the second method, we combine the features directly and nonlinearly using a CNN as follows:</p><p>We train the CNN parameter &#952; with in-distribution and OOD validation samples using binary crossentropy loss. The detailed structures of the CNN are given in Table A.3. We use the values of these feature d(x) to test the performance of detecting OOD samples.</p></sec><sec><title>EXPERIMENTAL SETUP</title><p>In this section, we present the details of the experiments, which includes the datasets, metrics, com- parison methods, and models. Because of space limitations, more details are given in Appendix A.</p></sec><sec><title>Datasets</title><p>We used several standard datasets for detecting OOD samples and classifying in- distribution samples. The SVHN, CIFAR-10, and CIFAR-100 datasets were used as in-distribution datasets, whereas Tiny ImageNet (TIM), LSUN, iSUN, Gaussian noise, and uniform noise were used as OOD datasets. These data were also used in <xref ref-type="bibr" rid="b21">Liang et al. (2017)</xref>; DeVries &amp; Taylor (2018). We applied standard augmentation (cropping and flipping) in all experiments. We used 5,000 validation images split from each training dataset and chose the parameter that can obtain the best accuracy in the validation dataset. We also used 68,257 training images from the SVHN dataset and 45,000 training images from the CIFAR-10 and CIFAR-100 datasets. All the hyperparameters of ODIN and UFEL were tuned on a separate validation set, which consists of 100 OOD images from the test dataset and 1,000 images from the in-distribution validation set. We tuned the parameters of the CNN in Equation 4 using 50 validation training images taken from the 100 validation images. The best parameters were chosen by validating the performance using the rest of 50 validation images. Finally, we tested the models with a test dataset that consisted of 10,000 in-distribution images and 9,900 OOD images.</p></sec><sec><title>Evaluation metrics</title><p>We used several standard metrics for testing the detection of OOD samples and the classification of in-distribution samples. We used TNR at 95% TPR, AUROC, AUPR, and accuracy (ACC), which were also used in <xref ref-type="bibr" rid="b19">Lee et al. (2017</xref>; 2018).</p></sec><sec><title>Comparison method</title><p>We compare UFEL with the baseline (<xref ref-type="bibr" rid="b9">Hendrycks &amp; Gimpel, 2016</xref>) and ODIN (<xref ref-type="bibr" rid="b21">Liang et al., 2017</xref>) methods. For the baseline method, we used max k p(y = k|x) as the detec- tion metric. For ODIN, we used the same detection metric and calibrated it using temperature scaling and small perturbations to the input. The temperature parameter T &#8712; {1, 10, 100, 1000} and the per- turbation parameter &#8712; {0, 0.001, 0.005, 0.01, 0.05, 0.1} were chosen using the in-distribution and OOD validation datasets.</p></sec><sec><title>Model training details</title><p>We adopted LeNet5 (<xref ref-type="bibr" rid="b18">LeCun et al., 1998</xref>) and two state-of-the-art models, WideResNet (<xref ref-type="bibr" rid="b8">He et al., 2016</xref>) and DenseNet (<xref ref-type="bibr" rid="b12">Huang et al., 2017</xref>), in this experiment. In all experi- ments, we used the same model and conditions to compare UFEL with existing methods. Only the structure used to extract the variance parameters differs. For LeNet5, we increased the number of channels of the original LeNet5 to improve accuracy. See Table A.3 for model details. We inserted Under review as a conference paper at ICLR 2020</p></sec><sec><title>EXPERIMENTAL RESULTS</title><p>In this section, we demonstrate the performance of UFEL by conducting five experiments. In the first experiment, we show that UFEL performs better than the baseline (<xref ref-type="bibr" rid="b9">Hendrycks &amp; Gimpel, 2016</xref>) and ODIN (<xref ref-type="bibr" rid="b21">Liang et al., 2017</xref>) methods on several datasets and models. In the second experiment, we confirm that the features of UFEL have almost no relationship with the ACC. In the third experiment, we demonstrate that UFEL has a strong ability to detect OOD data, even if the number of classes of in-distribution data is small. In the fourth experiment, we confirm that UFEL is robust to the number of OOD samples, and in the fifth experiment, we test the performance of UFEL on unseen OOD datasets. The objective of these experiments is to show the uncertainties of the features obtained in each CNN layer distinguish the in-distribution and OOD data. Moreover, we obtain state-of-the-art performance for OOD sample detection by combining these features.</p></sec><sec><title>Detecting OOD samples on several datasets and models</title><p>In this experiment, we evaluate the performance of OOD detection using Equation 3 and Equation 4. In this study, var l is used to denote &#963; l sum , max P is max(z L ), UFEL (LR) denotes d LR in Equation 3, and UFEL (CNN) denotes d CN N in Equation 4. We measured the detection performance using a DenseNet trained on CIFAR- 100 when the iSUN dataset is used to provide the OOD images. <xref ref-type="table" rid="tab_0">Table 1</xref> shows that var 1 and var 3 are strong features that, by themselves, can outperform ODIN. This indicates that the uncertainties of the feature extraction and classification are effective for detecting OOD samples. Moreover, the combination of these features yields state-of-the-art performance.</p><p>In <xref ref-type="table" rid="tab_1">Table 2</xref>, we demonstrate that UFEL outperforms the baseline and ODIN methods on several datasets and models. Furthermore, UFEL is also slightly superior to them with respect to in- distribution accuracy, which indicates that our model is robust to noise because of the reparame- terization trick. Here, we do not report ODIN accuracy because the model of ODIN is the same as that of the baseline. We conducted this experiment three times and used the average of the results. We used the CIFAR-10, CIFAR-100, and SVHN datasets as the in-distribution datasets and the other datasets as the OOD samples. Note that our UFEL outperformed the baseline and ODIN methods by a large margin, especially when using CIFAR-100, which is difficult to classify, or LeNet5 which Under review as a conference paper at ICLR 2020</p></sec><sec><title>Relationship between the performance of detecting OOD samples and in-distribution accuracy</title><p>In this experiment, we show that the features of our method are not related to the in-distribution accuracy. We used CIFAR-10 as the in-distribution dataset and TIM as the OOD dataset. We trained DenseNet-BC for nine epochs and tested the performance at each epoch. As shown in <xref ref-type="fig" rid="fig_2">Figure 3</xref>, each variance (var l) is less related to the accuracy than the baseline and ODIN methods. The var 1 of the feature close to the input layer has the highest ability to detect OOD samples in this experiment. These results also indicate that we can discriminate in-distribution and OOD examples when using a dataset that is difficult to classify.</p></sec><sec><title>Detecting OOD samples while changing the number of in-distribution classes</title><p>In this ex- periment, we show that UFEL is robust to the number of class labels. We used SVHN as in- distribution dataset and changed the number of in-distribution classes in training as {0,1}, {0,1,2},..., {0,1,2,...,9}. We also used TIM, LSUN and iSUN datasets as OOD samples, and LeNet5 as a model. We compared the proposed method with the baseline and ODIN methods, as shown in <xref ref-type="fig" rid="fig_3">Figure 4</xref>. This graph shows the AUROC score of each model when changing the number of training data classes. As this graph shows, UFEL outperforms other methods in all cases and is robust to the number of in-distribution classes, whereas the performance of ODIN drops as the number of class labels decreases. These results suggest that UFEL is effective for small datasets because the number of samples can be decreased to one fifth of the original number when there are two in-distribution classes and the cost of label annotation is reduced.</p></sec><sec><title>Detecting OOD samples while changing the number of OOD samples</title><p>In this experiment, we present the performance of UFEL while changing the number of OOD validation examples. All the hyperparameters of ODIN and UFEL were tuned on a separate validation set, which consists of 30, 50, and 100 OOD images in the test dataset and 1,000 images from the in-distribution validation set. As shown in <xref ref-type="fig" rid="fig_4">Figure 5</xref>, although UFEL (CNN) outperforms other methods including UFEL (LR) in most cases, it performs worse than ODIN in part of the results because some tuning for OOD samples is needed. Meanwhile, UFEL (LR) outperforms prior methods constantly because the number of hyperparameters is small and tuning samples are almost unneeded. Generalization to unseen OOD dataset Because OOD validation samples might not be available in practice, we used only uniform noise as the validation OOD dataset and tested the ability of our model to detect another OOD dataset. We added a binary classification as a comparison method. This method was trained using an in-distribution dataset (positive) and uniform noise (negative). <xref ref-type="table" rid="tab_2">Table 3</xref> shows that UFEL outperforms prior work in all cases and generalize well. <xref ref-type="table" rid="tab_2">Table 3</xref> also indicates that the binary classification method does not generalize well because it cannot distin- guish in-distribution dataset and OOD datasets TIM, LSUN, and iSUN, although it can distinguish Gaussian noise, which is similar to uniform noise.</p></sec><sec><title>CONCLUSION AND FUTURE WORK</title><p>In this paper, we demonstrated that the uncertainties of features extracted in each hidden layer are important for detecting OOD samples. We combined these uncertainties to obtain state-of-the-art OOD detection performance on several models and datasets. The approach proposed in this paper has the potential to increase the safety of many classification systems by improving their ability to detect OOD samples. In future work, our model could be used in an unsupervised model by training it to minimize reconstruction error, which would avoid the need to use in-distribution labels to detect OOD samples. Furthermore, although we compared our model with ODIN, UFEL will perform better if we combine UFEL with ODIN because they are orthogonal methods.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Comparison of existing and proposed methods. We visualized scatter plots of the outputs of the penultimate layer of a CNN that can estimate the uncertainties of latent features using the SVHN dataset (Netzer et al., 2011). We used only classes 0, 1, and 2 for the training data. Classes 0, 1, 2, and OOD, indicated by red, yellow, blue, and black, respectively, were used for the vali- dation data. We plot the contour of the maximum output of the softmax layer of the model. Left: Because the image of "204" includes the digits "2" and "0," the maximum value of the softmax output decreases because the model does not know to which class the image belongs. Right: The sizes of points in the scatter plots indicate the value of the combined uncertainties of features. We can classify the image of "204" as an in-distribution image according to the value of the combined uncertainties.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Network structure of UFEL when using DenseNet. Black arrow: Extracting the variance of latent features using the reparameterization trick. Blue arrow: Combining these features.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Results for the OOD detection test set data for image classification when the in-distribution dataset is CIFAR-100 and the OOD dataset is iSUN. We trained Dense-BC for each method under the same conditions. All results are averaged over three runs &#177; one standard deviation. All values are percentages, and the best results are indicated in bold.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Results for the OOD detection test set data for various situations. All results are averaged over three runs. All values are percentages, and the best results are indicated in bold.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Plot of ACC (x-axis) and AUROC (y-axis). The number on the plot indicates the number of training epochs. We used CIFAR-10 (as in-distribution), TIM (as OOD), and the DenseNet-BC model. This graph shows that the AUROC of UFEL is less related to ACC than those of the baseline and ODIN.</p></caption><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Plot of AUROC (y-axis) when changing the number of in-distribution dataset classes (x- axis). We used SVHN as in-distribution dataset, TIM, LSUN, and iSUN as OOD datasets, and the LeNet5 model. All plots were averaged over three runs and the error bar indicates one standard deviation.</p></caption><graphic /></fig><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>Plot of AUROC (y-axis) when changing the OOD dataset (x-axis). We used CIFAR-10 and CIFAR-100 as the in-distribution dataset. All plots are averaged over three runs and the error bar indicates one standard deviation.</p></caption><graphic /></fig><table-wrap id="tab_2"><label>Table 3:</label><caption><title>Table 3:</title><p>AUROC score for OOD detection test set data. All results are averaged over three runs. All values are percentages and the best results are indicated in bold.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap></sec></body><back><sec><title>A.1 EVALUATION METRIC</title><p>We used several standard metrics for testing the detection of OOD samples and the classification of in-distribution samples.</p><p>TNR at 95% TPR. Let TP, TN, FP, and FN denote the numbers of true positives, true negatives, false positives, and false negatives, respectively. We measure TNR = TN / (FP+TN), when TPR = TP / (TP+FN) is 95%.</p></sec><sec><title>AUROC</title><p>The Area Under the Receiver Operating Characteristic curve (<xref ref-type="bibr" rid="b3">Davis &amp; Goadrich, 2006</xref>) is a threshold-independent metric. The ROC curve depicts the relationship between TPR and FPR.</p></sec><sec><title>AUPR</title><p>We used several standard datasets for detecting OOD samples and classifying in-distribution sam- ples.</p></sec><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Deep variational information bottleneck</article-title><source>arXiv preprint arXiv:1612.00410</source><year>2016</year><person-group person-group-type="author"><name><surname>References Alexander</surname><given-names>A</given-names></name><name><surname>Alemi</surname><given-names>Ian</given-names></name><name><surname>Fischer</surname><given-names>Joshua V</given-names></name><name><surname>Dillon</surname><given-names>Kevin Murphy</given-names></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Uncertainty in the variational information bottleneck</article-title><source>arXiv preprint arXiv:1807.00906</source><year>2018</year><person-group person-group-type="author"><name><surname>Alexander A Alemi</surname><given-names>Ian</given-names></name><name><surname>Fischer</surname><given-names>Joshua V</given-names></name><name><surname>Dillon</surname><given-names /></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Con- crete problems in ai safety</article-title><source>arXiv preprint arXiv:1606.06565</source><year>2016</year><person-group person-group-type="author"><name><surname>Amodei</surname><given-names>Dario</given-names></name><name><surname>Olah</surname><given-names>Chris</given-names></name><name><surname>Steinhardt</surname><given-names>Jacob</given-names></name><name><surname>Christiano</surname><given-names>Paul</given-names></name><name><surname>Schulman</surname><given-names>John</given-names></name><name><surname>Man&#233;</surname><given-names>Dan</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>The relationship between precision-recall and roc curves</article-title><source>Proceedings of the 23rd international conference on Machine learning</source><year>2006</year><fpage>233</fpage><lpage>240</lpage><person-group person-group-type="author"><name><surname>Davis</surname><given-names>Jesse</given-names></name><name><surname>Goadrich</surname><given-names>Mark</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Imagenet: A large-scale hi- erarchical image database</article-title><source>IEEE conference on computer vision and pattern recognition</source><year>2009</year><fpage>248</fpage><lpage>255</lpage><person-group person-group-type="author"><name><surname>Deng</surname><given-names>Jia</given-names></name><name><surname>Dong</surname><given-names>Wei</given-names></name><name><surname>Socher</surname><given-names>Richard</given-names></name><name><surname>Li</surname><given-names>Li-Jia</given-names></name><name><surname>Li</surname><given-names>Kai</given-names></name><name><surname>Fei-Fei</surname><given-names>Li</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Learning confidence for out-of-distribution detection in neural networks</article-title><source>arXiv preprint arXiv:1802.04865</source><year>2018</year><person-group person-group-type="author"><name><surname>Devries</surname><given-names>Terrance</given-names></name><name><surname>Taylor</surname><given-names>Graham W</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>On calibration of modern neural networks</article-title><year>2017</year><volume>70</volume><fpage>1321</fpage><lpage>1330</lpage><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Chuan</given-names></name><name><surname>Pleiss</surname><given-names>Geoff</given-names></name><name><surname>Sun</surname><given-names>Yu</given-names></name><name><surname>Weinberger</surname><given-names>Kilian</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Deep speech: Scaling up end-to-end speech recognition</article-title><source>arXiv preprint arXiv:1412.5567</source><year>2014</year><person-group person-group-type="author"><name><surname>Hannun</surname><given-names>Awni</given-names></name><name><surname>Case</surname><given-names>Carl</given-names></name><name><surname>Casper</surname><given-names>Jared</given-names></name><name><surname>Catanzaro</surname><given-names>Bryan</given-names></name><name><surname>Diamos</surname><given-names>Greg</given-names></name><name><surname>Elsen</surname><given-names>Erich</given-names></name><name><surname>Prenger</surname><given-names>Ryan</given-names></name><name><surname>Satheesh</surname><given-names>Sanjeev</given-names></name><name><surname>Sengupta</surname><given-names>Shubho</given-names></name><name><surname>Coates</surname><given-names>Adam</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Deep residual learning for image recog- nition</article-title><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><year>2016</year><fpage>770</fpage><lpage>778</lpage><person-group person-group-type="author"><name><surname>He</surname><given-names>Kaiming</given-names></name><name><surname>Zhang</surname><given-names>Xiangyu</given-names></name><name><surname>Ren</surname><given-names>Shaoqing</given-names></name><name><surname>Sun</surname><given-names>Jian</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>A baseline for detecting misclassified and out-of-distribution examples in neural networks</article-title><source>arXiv preprint arXiv:1610.02136</source><year>2016</year><person-group person-group-type="author"><name><surname>Hendrycks</surname><given-names>Dan</given-names></name><name><surname>Gimpel</surname><given-names>Kevin</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Deep anomaly detection with outlier exposure</article-title><source>arXiv preprint arXiv:1812.04606</source><year>2018</year><person-group person-group-type="author"><name><surname>Hendrycks</surname><given-names>Dan</given-names></name><name><surname>Mazeika</surname><given-names>Mantas</given-names></name><name><surname>Dietterich</surname><given-names>Thomas G</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Deep neural networks for acoustic modeling in speech recognition</article-title><source>IEEE Signal processing magazine</source><year>2012</year><volume>29</volume><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>Geoffrey</given-names></name><name><surname>Deng</surname><given-names>Li</given-names></name><name><surname>Yu</surname><given-names>Dong</given-names></name><name><surname>Dahl</surname><given-names>George</given-names></name><name><surname>Mohamed</surname><given-names>Abdel-Rahman</given-names></name><name><surname>Jaitly</surname><given-names>Navdeep</given-names></name><name><surname>Senior</surname><given-names>An- Drew</given-names></name><name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name><name><surname>Nguyen</surname><given-names>Patrick</given-names></name><name><surname>Kingsbury</surname><given-names>Brian</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Densely connected convolutional networks</article-title><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><year>2017</year><fpage>4700</fpage><lpage>4708</lpage><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Gao</given-names></name><name><surname>Liu</surname><given-names>Zhuang</given-names></name><name><surname>Van Der Maaten</surname><given-names>Laurens</given-names></name><name><surname>Weinberger</surname><given-names>Kilian</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>What uncertainties do we need in bayesian deep learning for computer vision?</article-title><source>Advances in neural information processing systems</source><year>2017</year><fpage>5574</fpage><lpage>5584</lpage><person-group person-group-type="author"><name><surname>Kendall</surname><given-names>Alex</given-names></name><name><surname>Gal</surname><given-names>Yarin</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv preprint arXiv:1412.6980</source><year>2014</year><person-group person-group-type="author"><name><surname>Diederik</surname><given-names>P</given-names></name><name><surname>Kingma</surname><given-names>Jimmy</given-names></name><name><surname>Ba</surname><given-names /></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Auto-encoding variational bayes</article-title><source>arXiv preprint arXiv:1312.6114</source><year>2013</year><person-group person-group-type="author"><name><surname>Diederik</surname><given-names>P</given-names></name><name><surname>Kingma</surname><given-names>Max</given-names></name><name><surname>Welling</surname><given-names /></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>Imagenet classification with deep convo- lutional neural networks</article-title><source>Advances in neural information processing systems</source><year>2012</year><fpage>1097</fpage><lpage>1105</lpage><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>Alex</given-names></name><name><surname>Sutskever</surname><given-names>Ilya</given-names></name><name><surname>Hinton</surname><given-names>Geoffrey E</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Learning multiple layers of features from tiny images</article-title><source>Technical report, Citeseer</source><year>2009</year><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>Alex</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Gradient-based learning applied to document recognition</article-title><source>Proceedings of the IEEE</source><year>1998</year><volume>86</volume><issue>11</issue><fpage>2278</fpage><lpage>2324</lpage><person-group person-group-type="author"><name><surname>Lecun</surname><given-names>Yann</given-names></name><name><surname>Bottou</surname><given-names>L&#233;on</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name><name><surname>Haffner</surname><given-names>Patrick</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Training confidence-calibrated classifiers for detecting out-of-distribution samples</article-title><source>arXiv preprint arXiv:1711.09325</source><year>2017</year><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Kimin</given-names></name><name><surname>Lee</surname><given-names>Honglak</given-names></name><name><surname>Lee</surname><given-names>Kibok</given-names></name><name><surname>Shin</surname><given-names>Jinwoo</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>A simple unified framework for detecting out-of-distribution samples and adversarial attacks</article-title><source>Advances in Neural Information Process- ing Systems</source><year>2018</year><fpage>7167</fpage><lpage>7177</lpage><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Kimin</given-names></name><name><surname>Lee</surname><given-names>Kibok</given-names></name><name><surname>Lee</surname><given-names>Honglak</given-names></name><name><surname>Shin</surname><given-names>Jinwoo</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>Enhancing the reliability of out-of-distribution image detec- tion in neural networks</article-title><source>arXiv preprint arXiv:1706.02690</source><year>2017</year><person-group person-group-type="author"><name><surname>Liang</surname><given-names>Shiyu</given-names></name><name><surname>Li</surname><given-names>Yixuan</given-names></name><name><surname>Srikant</surname><given-names>R</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Focal loss for dense object detection</article-title><source>Proceedings of the IEEE international conference on computer vision</source><year>2017</year><fpage>2980</fpage><lpage>2988</lpage><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Tsung-Yi</given-names></name><name><surname>Goyal</surname><given-names>Priya</given-names></name><name><surname>Girshick</surname><given-names>Ross</given-names></name><name><surname>He</surname><given-names>Kaiming</given-names></name><name><surname>Doll&#225;r</surname><given-names>Piotr</given-names></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>Predictive uncertainty estimation via prior networks</article-title><source>Advances in Neural Information Processing Systems</source><year>2018</year><fpage>7047</fpage><lpage>7058</lpage><person-group person-group-type="author"><name><surname>Malinin</surname><given-names>Andrey</given-names></name><name><surname>Gales</surname><given-names>Mark</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><source>Reading digits in natural images with unsupervised feature learning</source><year>2011</year><person-group person-group-type="author"><name><surname>Netzer</surname><given-names>Yuval</given-names></name><name><surname>Wang</surname><given-names>Tao</given-names></name><name><surname>Coates</surname><given-names>Adam</given-names></name><name><surname>Bissacco</surname><given-names>Alessandro</given-names></name><name><surname>Wu</surname><given-names>Bo</given-names></name><name><surname>Ng</surname><given-names>Andrew Y</given-names></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><article-title>Yolov3: An incremental improvement</article-title><source>arXiv preprint arXiv:1804.02767</source><year>2018</year><person-group person-group-type="author"><name><surname>Redmon</surname><given-names>Joseph</given-names></name><name><surname>Farhadi</surname><given-names>Ali</given-names></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv preprint arXiv:1409.1556</source><year>2014</year><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>Karen</given-names></name><name><surname>Zisserman</surname><given-names>Andrew</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><article-title>Turkergaze: Crowdsourcing saliency with webcam based eye tracking</article-title><source>arXiv preprint arXiv:1504.06755</source><year>2015</year><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Pingmei</given-names></name><name><surname>Krista</surname><given-names>A</given-names></name><name><surname>Ehinger</surname><given-names>Yinda</given-names></name><name><surname>Zhang</surname><given-names>Adam</given-names></name><name><surname>Finkelstein</surname><given-names /></name><name><surname>Sanjeev</surname><given-names>R</given-names></name><name><surname>Kulkarni</surname><given-names>Jianxiong</given-names></name><name><surname>Xiao</surname><given-names /></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop</article-title><source>arXiv preprint arXiv:1506.03365</source><year>2015</year><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Fisher</given-names></name><name><surname>Seff</surname><given-names>Ari</given-names></name><name><surname>Zhang</surname><given-names>Yinda</given-names></name><name><surname>Song</surname><given-names>Shuran</given-names></name><name><surname>Funkhouser</surname><given-names>Thomas</given-names></name><name><surname>Xiao</surname><given-names>Jianxiong</given-names></name></person-group></element-citation></ref></ref-list></back></article>