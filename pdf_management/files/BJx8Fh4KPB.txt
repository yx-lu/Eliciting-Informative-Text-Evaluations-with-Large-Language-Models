Title:
```
Under review as a conference paper at ICLR 2020 RL-LIM: REINFORCEMENT LEARNING-BASED LOCALLY INTERPRETABLE MODELING
```
Abstract:
```
Understanding black-box machine learning models is important towards their widespread adoption. However, developing globally interpretable models that ex- plain the behavior of the entire model is challenging. An alternative approach is to explain black-box models through explaining individual prediction using a locally interpretable model. In this paper, we propose a novel method for locally inter- pretable modeling - Reinforcement Learning-based Locally Interpretable Model- ing (RL-LIM). RL-LIM employs reinforcement learning to select a small number of samples and distill the black-box model prediction into a low-capacity locally interpretable model. Training is guided with a reward that is obtained directly by measuring agreement of the predictions from the locally interpretable model with the black-box model. RL-LIM near-matches the overall prediction performance of black-box models while yielding human-like interpretability, and significantly outperforms state of the art locally interpretable models in terms of overall pre- diction performance and fidelity.
```

Figures/Tables Captions:
```
Figure 1: The proposed RL-LIM method. White blocks represent fixed (not learnable) models, and grey blocks represent learnable (trainable) models. Stage 0: Black-box model training. Stage 1: Auxiliary dataset construction. Stage 2: Interpretable baseline training. Stage 3: Instance-wise weight estimator training. Stage 4: Interpretable inference.
Figure 2: Synthetic dataset results. Mean absolute weight difference (AWD) with 95% confidence intervals (of 10 independent runs) on three synthetic datasets. X-axis: Distance from the boundary where the local dynamics change, such as X 10 = 0 for Syn1 (in percentile), Y-axis: AWD (the lower, the better). We exclude LIME in these graphs due to its poor performance in terms of AWD (it is higher than 1.6 in all distance regimes for all three synthetic datasets).
Figure 3: Fidelityaverage selection probability of training instances as a function of the number of selected samples on three synthetic datasets. X-axis: λ, Y-axis: LMAE and average selection probability of training instances. LMAE is Local MAE - lower is better.
Figure 4: Qualitative interpretability results. The analyses of feature importance (derived by RL- LIM) for 5 types of subgroups in Adult Income dataset: (a) Age, (b) Gender, (c) Marital status, (d) Race, (e) Education. The color represents the feature importance for each subgroup.
Table 1: Real-world regression dataset results. Overall prediction performance (metric: MAE, lower is better) and fidelity (metric: R 2 score, higher is better) on regression problems with ridge regres- sion as the locally interpretable model. 'Original' is the performance of the original black-box model that the models are approximating. MAE of global ridge regression (RR) can be found below the data name. Red represents performance that is worse than global ridge regression and the negative R 2 scores. Bold represents the best results.
Table 2: Real-world classification dataset results. Overall prediction performance (metric: APR, higher is better) and fidelity (metric: R 2 score, higher is better) on classification problems with shallow regression tree as the locally interpretable model. 'Original' is the performance of the original black-box model that the models are approximating. APR of global decision tree (DT) can be found below the data name.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Artificial Intelligence (AI) is advancing at a rapid pace, particularly with recent advances in deep neural networks and ensemble methods (Goodfellow et al., 2016; He et al., 2016; Chen & Guestrin, 2016; Ke et al., 2017). This progress has been fueled by 'black-box' machine learning models where the decision making is controlled by complex non-linear interactions between many parameters that are difficult for humans to understand and interpret. However, in many real-world applications AI systems are not only expected to perform well but are also required to be interpretable: doctors need to understand why a particular treatment is recommended, and financial institutions need to understand why a loan was declined. Use cases of model interpretability vary across applications: it can provide trust to users by showing rationales behind decisions, enable detection of systematic failure cases, and provide actionable feedback for improving models (Rudin, 2018). Many studies have suggested a trade-off between performance and interpretability (Virág & Nyitrai, 2014; Johansson et al., 2011). This is correct in that globally interpretable models, which attempt to explain the entire model behavior, typically yield considerably worse performance than 'black- box' models (Lipton, 2016). To go beyond the performance limitations of globally interpretable models, another promising direction is locally interpretable models, which instead of explaining the entire model explain a single prediction (Ribeiro et al., 2016). Methodologically, while a globally interpretable model fits a single inherently interpretable model (such as a linear model or a shal- low decision tree) to the entire training set, locally interpretable models aim to fit an inherently interpretable model locally, i.e. for each instance individually, by distilling knowledge from a high performance black-box model. Such locally interpretable models are very useful for real-world AI deployments to provide succinct and human-like explanations to users. They can be used to identify systematic failure cases (e.g. by seeking common trends in input dependence for failure cases), de- tect biases (e.g. by quantifying feature importance for a particular variable), and provide actionable feedback to improve a model (e.g. understand failure cases and what training data to collect). To be useful in practice, locally interpretable models need to maximize two objectives: (i) the overall prediction performance (how well it predicts compared to the ground truth labels) - for the model to be accurate, and (ii) fidelity (how well it approximates the 'black-box' model predictions) - to ensure the model is reliably approximating the black-box model's predictions in the neighborhood Under review as a conference paper at ICLR 2020 of interest (Plumb et al., 2019; Lakkaraju et al., 2019). To this end, a few methods have recently been proposed for locally interpretable modeling: Local Interpretable Model-agnostic Explanations (LIME) (Ribeiro et al., 2016), Supervised Local modeling methods (SILO) (Bloniarz et al., 2016), and Model Agnostic Supervised Local Explanations (MAPLE) (Plumb et al., 2018). LIME in partic- ular has gained notable popularity and has been deployed in many applications due to its simplicity. However, the overall prediction performance and fidelity metrics are not reaching desired levels in many cases (Alvarez-Melis & Jaakkola, 2018; Zhang et al., 2019; Ribeiro et al., 2018; Lakkaraju et al., 2017). Indeed, as we show in our experiments, there are frequent cases where existing locally interpretable models even underperform commonly low-performing globally interpretable models. One of the fundamental challenges to fit a locally interpretable model is the representational capac- ity difference while applying distillation. Black-box machine learning models, such as deep neural networks or ensemble models, have much larger representational capacity than locally interpretable models. This can result in underfitting with conventional distillation techniques, leading to subopti- mal performance (Hinton et al., 2015; Wang et al., 2019). We address this fundamental challenge by proposing a novel Reinforcement Learning-based method to fit Locally Interpretable Models which we call RL-LIM. RL-LIM efficiently utilizes the small representational capacity of locally inter- pretable models by training with a small number of samples that are determined to have the highest value contribution to the fitting of a locally interpretable model. In order to select these highest-value instances, we train instance-wise weight estimators (modeled with deep neural networks) using a re- inforcement signal that quantifies the fidelity metric (i.e. how well does the model approximate the black-box model predictions). The contributions of this paper can be summarized as: 1. We introduce the first method that tackles interpretability through data-weighted training, and show that reinforcement learning is highly effective for end-to-end training of such a model. 2. We show that distillation of a black-box model into a low-capacity interpretable model can be sig- nificantly improved by fitting with a small subset of relevant samples that is controlled efficiently by our method. 3. On various classification and regression datasets, we demonstrate that RL-LIM significantly out- performs alternative models (LIME, SILO and MAPLE) in overall prediction performance and fidelity metrics - in most cases, the overall performance of locally interpretable models obtained by RL-LIM is very similar to complex black-box models.

Section Title: RELATED WORK
  RELATED WORK

Section Title: Locally interpretable models
  Locally interpretable models There are various approaches to interpret black-box models - (Gilpin et al., 2018) provides a good overview. One approach is to directly decompose the predic- tion into feature attributions by considering what-if cases. Shapley values (Štrumbelj & Kononenko, 2014) and their computationally-efficient variants (Lundberg & Lee, 2017) are commonly-used methods in this category. Other notable methods are based on activation differences, e.g. DeepLIFT (Shrikumar et al., 2017), or saliency maps using the gradient flows, e.g. CAM (Zhou et al., 2016) and Grad-CAM (Selvaraju et al., 2017). In this paper, we focus on the direction of locally interpretable modeling - distilling a black-box model into an interpretable model for each input instance. Locally Interpretable Model-agnostic Explanation (LIME) (Ribeiro et al., 2016) is the most popular method for locally interpretable modeling. LIME is based on modifying a data instance by tweaking the feature values and then learning from the impact of the modifications on the output. A funda- mental challenge for LIME is the need for a meaningful distance metric to determine neighborhoods, as simple metrics like Euclidean distance may yield poor fidelity in some cases and the estimation can be highly-sensitive to normalization (Alvarez-Melis & Jaakkola, 2018) especially with categor- ical variables. Supervised Local modeling methods (SILO) (Bloniarz et al., 2016)) aims to improve LIME by determining the neighborhoods for each instance using ad-hoc tree-based ensemble meth- ods. Model Agnostic Supervised Local Explanations (MAPLE) (Plumb et al., 2018) furthers adds a method for feature selection on top of SILO - it utilizes ad-hoc tree-based ensemble methods to determine the weights of training instances for each target instance and uses the weights to opti- mize a locally interpretable model. However, SILO and MAPLE still have shortcomings because the tree-based ensemble methods are optimized independently from the locally interpretable model - lack of joint optimization results in suboptimal fidelity for the locally interpretable model. Over- all, to construct a locally interpretable model, a key problem is how to select the optimal training Under review as a conference paper at ICLR 2020 instances for each testing instance, because the selected training instances mostly determine the constructed locally interpretable model. The number of possibilities for training instance selection is extremely large (exponential in the number of training instances). LIME heuristically utilizes Euclidean distances, whereas SILO and MAPLE use ad-hoc tree-based ensemble methods. Our proposed method, RL-LIM, takes a very different perspective: to properly and efficiently explore the large possible solution space, RL-LIM utilizes reinforcement learning to find the optimal policy that selects the training instances that maximize the fidelity of the locally interpretable model.

Section Title: Data-weighted training
  Data-weighted training Optimal weighing of training data is a paramount problem in machine learning. By upweighting valuable instances and downweighting the low quality or problematic instances, better performance can be obtained in certain learning scenarios, such as imbalanced or noisy labels (Jiang et al., 2018). One approach for data weighting is utilizing Influence Func- tions (Koh & Liang, 2017), that are based on oracle access to gradients and Hessian-vector prod- ucts. Jointly-trained student-teacher methods constitute another approach (Jiang et al., 2018; Bengio et al., 2009) to learn a data-driven curriculum. Using the feedback from the teacher network, training instance-wise weights are learned for the student model. Aligned with our motivations, meta learn- ing is considered for data weighting in Ren et al. (2018). Their proposed method utilizes gradient descent-based meta learning, guided by a small validation set, to maximize the target performance. In this work we consider data-weighted training for a novel purpose: interpretability. Unlike gradi- ent descent-based meta learning, our approach uses reinforcement learning to integrate the reward directly with the fidelity metric. Aforementioned works estimate the same ranking of training in- stances for the entire dataset. Instead, our method yields an instance-wise ranking of training data points, different for each testing instance. This enables efficient distillation of a black-box model prediction into a locally interpretable model.

Section Title: REINFORCEMENT LEARNING-BASED MODELING
  REINFORCEMENT LEARNING-BASED MODELING We consider a training dataset D = {(x i , y i ), i = 1, ..., N } ∼ P for training of a black-box model f , where x i ∈ X is the feature vector in a d-dimensional feature space X and y i ∈ Y is the corresponding label in a label space Y. We also assume that there exists a probe dataset D p = {(x p j , y p j ), j = 1, ..., M } ∼ P where M is the number of probe instances. The probe dataset is used to evaluate the model performance to guide meta-learning as in Ren et al. (2018). If there is no explicit probe dataset, we can randomly partition a subset of the training dataset as the probe dataset and the remainder as the training dataset. RL-LIM is composed of three models: 1. Black-box model f : X → Y - any machine learning model that needs to be explained (e.g. a deep neural network or a decision tree-based ensemble model), 2. Locally interpretable model g θ : X → Y - an inherently interpretable model by design (e.g. a linear model or a shallow decision tree), 3. Instance-wise weight estimation model h φ : X × X × Y → [0, 1] - a function that outputs the instance-wise weights to fit the locally interpretable model. It uses concatenation of a probe feature, a training feature, and a corresponding black-box model prediction on the training feature as its inputs. It can be a complex machine learning model - e.g. here a deep neural network. Our objective is to construct an accurate locally interpretable model g θ such that the predictions made by it are similar to the predictions of the given black-box model f * - i.e. the locally inter- pretable model has high fidelity. We use a loss function, L : Y × Y → R to quantify the fidelity of the locally interpretable model (e.g. mean absolute error, lower the better). In RL-LIM, the three necessary components of an RL framework are as follows: the state is the vector of input features, the action is the selection vector, and the reward is the fidelity which depends on the input fea- tures and the selection vector. The instance-wise weight estimator model is the agent that outputs the actions based on the state (input features). The environment is comprised of the input feature generating process, as well as the black-box model for the target task. The representational capacity difference between the black-box model and the locally interpretable model is the bottleneck we aim to address. Ideally, to avoid underfitting, locally interpretable models should be learned with a minimal number of training instances that are most effective in capturing the model behavior. We propose an instance-wise weight estimation model h φ to estimate the probabil- Under review as a conference paper at ICLR 2020 ities of training instances that should be used for fitting the locally interpretable model. Integrating with the accurate locally interpretable modeling goal, we propose the following objective: where λ ≥ 0 is a hyper-parameter that controls the number of training instances used to fit the locally interpretable model (we study the impact of performance on λ in Section 4.2), and h φ (x p , x, f * (x)) represents the instance-wise weight for each training pair (x, f * (x)) for the probe data x p . L g is the loss function to fit the locally interpretable model, for which we use the mean squared error between predicted values for regression and logits for classification. φ and θ are the trainable parameters, whereas f * (the pre-trained black-box model) is fixed. The first term in the objective function E x p ∼P X L(f * (x p ), g * θ(x p ) (x p )) represents the local predic- tion differences between black-box model and locally interpretable model (referred to as fidelity metric). The second term in the objective function E x p ,x∼P X h φ (x p , x, f * (x)) represents the ex- pected number of selected training points to fit the locally interpretable model. Lastly, the constraint ensures that the locally interpretable model is derived from weighted loss function, where weights are the output of the instance-wise weight estimator h φ . Our formulation does not assume any con- straint on g θ - it could be any inherently interpretable model suitable for the data type of interest. Next, we describe how Eq. (1) can be efficiently addressed with reinforcement learning.

Section Title: TRAINING AND INFERENCE
  TRAINING AND INFERENCE The RL-LIM method, shown in  Fig. 1 , can be thought of as encompassing 5 stages: Under review as a conference paper at ICLR 2020 • Stage 0 - Black-box model training: This stage is the preliminary stage for RL-LIM. Given the training set D, the black-box model f is trained to minimize a loss function (L f ) (e.g. mean squared error for regression or cross-entropy for classification), i.e., f * = arg min f 1 N N i=1 L f (f (x i ), y i ). If the pre-trained black-box model is already saved, we can skip this stage and retrieve the given pre-trained black-box model to f * . • Stage 1 - Auxiliary dataset construction: Using the pre-trained black-box model f * , we create auxiliary training and probe datasets, asD = {(x i ,ŷ i ), i = 1, ..., N } (whereŷ i = f * (x i )) and D p = {(x p j ,ŷ p j ), j = 1, ..., M } (whereŷ p j = f * (x p j )), respectively. These auxiliary datasets (D, D p ) are used for instance-wise weight estimation models and locally interpretable model training. • Stage 2 - Interpretable baseline training: To improve the stability of the instance-wise weight estimator training, a baseline model is observed to be beneficial. As the baseline model g b : X → Y, we use a globally interpretable model (such as a linear model or shallow decision tree) opti- mized to replicate the predictions of the black-box model: • Stage 3 - Instance-wise weight estimator training: We train an instance-wise weight estimator using the auxiliary datasets (D,D p ). To encourage exploration, we consider probabilistic se- lection, with a sampler block that is based on the output of the instance-wise weight estimator - h φ (x p j , x i ,ŷ i ) represents the probability that (x i ,ŷ i ) is selected to train locally interpretable model for the probe instance x p j . Let the binary vector c(x p j ) ∈ {0, 1} N represent the selection opera- tion, such that (x i ,ŷ i ) is selected for training locally interpretable model for x p j when c i (x p j ) = 1. Correspondingly, ρ φ (x p ) is the probability mass function for c(x p j ) given h φ (·): As the original form of the optimization problem in Eq. (1) is intractable due to the expectation operations, we employ approximations: - The sample mean is used as an approximation of the first term of the objective function as - The second term of the objective, which represents the average selection probability, is approximated as the number of selected instances (divided by N ) to have - The constraint term is approximated using the sample mean of the training loss as g * The sampler block yields a non-differential objective, and we cannot train the instance-wise weight estimator using conventional gradient descent-based optimization. There are approxi- mations such as training in expectation (Raffel et al., 2017) or Gumbel-softmax (Jang et al., 2016). Instead, motivated by its many successful applications (Ranzato et al., 2015; Zaremba & Sutskever, 2015; Zhang & Lapata, 2017), we use REINFORCE algorithm (Williams, 1992) such that the selection action is rewarded by the performance of its impact. The loss function for the instance-wise weight estimator l(φ) is expressed as: To apply the REINFORCE algorithm, we directly compute the gradient ∇ φl (φ) as: Using the gradient ∇ φl (φ), we employ the following steps iteratively to update the parameters of the instance-wise weight estimator φ: 1. Estimate instance-wise weights w i (x p j ) = h φ (x p j , x i ,ŷ i ) and instance-wise selection vector c i (x p j ) ∼ Ber(w i (x p j )) for each training and probe instance in a mini-batch. Under review as a conference paper at ICLR 2020 2. Optimize the locally interpretable model with the selection vector for each probe instance: 3. Update the instance-wise weight estimation model parameter φ: where α > 0 is a learning rate and L b (x p j ) = L(f * (x p j ), g * b (x p j )) is the baseline loss against which we benchmark the performance improvement. We repeat the steps above until convergence. • Stage 4 - Interpretable inference: Unlike when training, we use a fixed instance-wise weight estimator (without the sampler and interpretable baseline) and merely fit the locally interpretable model at inference. Given the test instance x t , we obtain the selection probabilities from the instance-wise weight estimator, and using these as the weights, we fit the locally interpretable model via weighted optimization. The outputs of the trained interpretable model are the instance- wise predictions and the corresponding explanations (e.g., local dynamics of the black-box model predictions at x t given by the coefficients of the fitted linear model).

Section Title: COMPUTATIONAL COST
  COMPUTATIONAL COST In this subsection, we analyze the computational cost of RL-LIM for training and inference. As a representative and commonly used example, we assume linear regression as the locally interpretable model, which has a computational complexity of O(d 2 N ) + O(d 3 ) to fit, where d is the number of features and N is the number of training instances. When N >> d (which is often the case in practice), the training computational complexity is approximated as O(d 2 N ) (Tan, 2018).

Section Title: Training
  Training Given a pre-trained black-box model, Stage 1 involves running inference N times and the total complexity depends on the complexity of the black-box model. Unless the black-box model is very complex, the computational complexity of Stage 1 becomes much smaller than Stage 3. Stage 2 has negligible computational overhead. At Stage 3, we iteratively train the instance-wise weight estimator and fit the locally interpretable model from scratch using weighted optimization. Therefore, the computational complexity is O(d 2 N N I ) where N I is the number of iterations in Stage 3 (typically N I < 10, 000 until convergence). Thus, the training complexity scales roughly linearly with the number of training instances.

Section Title: Interpretable inference
  Interpretable inference To infer with the locally interpretable model, we need to fit the locally interpretable model after obtaining the instance-wise weights from the trained instance-wise weight estimator. Thus, for each testing instance, the computational complexity is O(d 2 N ). 1 For instance, on a single NVIDIA V100 GPU, on Facebook Comment dataset (consisting ∼ 600,000 samples), RL-LIM yields a training time of less than 5 hours (including Stage 1, 2 and 3) and an interpretable inference time of less than 10 seconds per a testing instance. On the other hand, LIME results in much longer interpretable inference time (around 30 seconds per a testing instance) due to acquiring a large number of black-box model predictions for the inputs perturbations, whereas SILO and MAPLE are similar to RL-LIM.

Section Title: EXPERIMENTS
  EXPERIMENTS We compare RL-LIM to multiple benchmarks on 3 synthetic datasets and 5 UCI public datasets.

Section Title: Datasets
  Datasets The 3 public datasets for regression problems are: (1) Blog Feedback, (2) Facebook Comment, (3) News Popularity; the other 2 public datasets for classification problems are: (4) Adult Income, (5) Weather. Details of the data descriptions can be found in the hyper-links of each dataset (colored in blue). Data statistics can be found in Table 3 in Appendix A. In this section, we mainly focus on the tabular datasets because the local dynamics are more important and useful to explain for them; however, RL-LIM method can be generalized to other data types in a straightforward way.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Black-box models: We focus on approximating black-box models that are shown to yield competi- tive performance on the target tasks: 3 tree-based ensemble methods (1) XGBoost (Chen & Guestrin, 2016), (2) LightGBM (Ke et al., 2017), (3) Random Forests (RF) (Breiman, 2001); and deep neural networks (4) Multi-layer Perceptron (MLP). Also, we use (5) Ridge Regression (RR) and (6) Re- gression Tree (RT) (for regression) and (7) Logistic Regression (LR) and (8) Decision Tree (DT) (for classification) as globally interpretable models to benchmark. 2 We focus on two types of lo- cally interpretable models: (1) Ridge regression, (2) Shallow regression tree (with a max depth of 3). We report the performance with ridge regression for regression and with shallow regression tree for classification in this section. The results of the other two combinations (with ridge regression for classification and with shallow regression tree for regression) are described in Appendix E.

Section Title: Comparisons to previous work
  Comparisons to previous work We compare the performance of RL-LIM with three competing methods: (1) Local Interpretable Model-agnostic Explanations (LIME) (Ribeiro et al., 2016), (2) Supervised Local modeling methods (SILO) (Bloniarz et al., 2016), (3) Model Agnostic Supervised Local Explanations (MAPLE) (Plumb et al., 2018).

Section Title: Performance metrics
  Performance metrics To evaluate the performance of locally interpretable models using real-world datasets, we quantify the overall prediction performance and its fidelity. We assume a disjoint testing dataset D t = {(x t k , y t k )} L k=1 for evaluation. For the overall prediction performance, we compare the predictions of the locally interpretable models with the ground-truth labels. We use Mean Absolute Error (MAE) for regression and Average Precision Recall (APR) for classification. For fidelity, we compare the outputs (predicted values for regression and logits for classification) of the locally interpretable models and of the black-box model. We consider two metrics: R 2 score (Legates & McCabe, 1999) and Local MAE (LMAE). The details of the metrics are described in Appendix C.

Section Title: Implementation details
  Implementation details We implement instance-wise weight estimator using a multi-layer per- ceptron with tanh activation. The number of hidden units and layers are optimized by the cross- validation. In most cases, 5-layer perceptron with 100 hidden units performs reasonably-well across all datasets. All features are normalized to be between zero and one, using standard minmax scaler. Categorical variables are transformed using one-hot encoding.

Section Title: EXPERIMENTS ON SYNTHETIC DATASETS - RECOVERING LOCAL DYNAMICS
  EXPERIMENTS ON SYNTHETIC DATASETS - RECOVERING LOCAL DYNAMICS On real-world datasets it is challenging to directly evaluate the explanation quality of the locally in- terpretable models due to the absence of ground-truth explanations. Thus we initially focus on syn- thetic datasets (with known ground-truth explanations) to directly evaluate how well the locally in- terpretable models can recover the underlying local dynamics. We construct three synthetic datasets such that the 11-dimensional input features X are sampled from N (0, I) and Y are: All three datasets have different local dynamics in different input regimes. We directly use the ground truth function as the black-box model and focus on how well locally interpretable modeling can capture the local dynamics. We evaluate the performance of capturing local dynamics using Absolute Weight Difference (AWD): AWD = ||w −ŵ||, where w is the ground truth coefficients to generate Y andŵ is the derived coefficient from the locally interpretable models. We use the estimated coefficients of the ridge regression as the derived local dynamics (ŵ). As shown in  Fig. 2 , RL-LIM significantly outperforms other benchmarks in discovering the local dynamics on all three datasets and in different regimes. RL-LIM can actively learn the linear and non-linear decision boundaries for the local dynamics. Note that LIME completely fails to recover the local dynamics as it uses the Euclidean distance uniformly for all features and cannot distinguish the special properties of the features that alter the local dynamics. SILO and MAPLE only use the predictions to discover the local dynamics; thus, it is hard to discover the decision boundary that depends on the other variables which are independent to the predictions. Fig. 5 in Appendix D shows the learning curves of RL-LIM demonstrating the efficiency of reinforcement learning.

Section Title: THE EFFECT OF THE NUMBER OF SELECTED SAMPLES ON FIDELITY
  THE EFFECT OF THE NUMBER OF SELECTED SAMPLES ON FIDELITY In RL-LIM, optimal distillation is enabled by using a small subset of training instances to fit the low-capacity locally interpretable model. The number of selected instances is controlled by λ in our method - if λ is high/low, RL-LIM penalizes more/less on the number of selected instances; thus, less/more instances are selected to construct the locally interpretable model. We analyze the efficacy of λ in controlling the likelihood of selection and the dependency of fidelity on λ. We expect that if we select a too small/large number of training instances, the locally inter- pretable model will overfit/underfit which negatively affects the fidelity in both cases.  Fig. 3  shows that there is a clear relationship between λ and the local fidelity. If λ is too large, RL-LIM selects too small number of instances; thus, the fitted locally interpretable model is less accurate (due to overfitting). On the other hand, if λ is too small, RL-LIM selects too large number of instances and deteriorates fidelity (due to underfitting). To achieve the optimal λ, we conduct cross-validation experiments and select λ which achieves the best validation fidelity (e.g. λ = 0.5 in Syn2).  Fig. 3  shows the average selection probability of the training instances for each λ. As λ increases, the average selection probabilities monotonically decrease due to the higher penalty on the number of selected training instances. Note that even using a small portion of training instances, RL-LIM can accurately distill the predictions of black-box models into locally interpretable models which is crucial to understand and interpret the predictions using the most relevant training instances.

Section Title: EXPERIMENTS ON REAL DATASETS - OVERALL PERFORMANCE AND FIDELITY
  EXPERIMENTS ON REAL DATASETS - OVERALL PERFORMANCE AND FIDELITY On multiple real datasets, we evaluate the overall prediction performance and fidelity. For the re- gression and classification problems, we use ridge regression and shallow regression trees as the locally interpretable model. More results can be found in Appendix E. As can be seen in  Table 1 , the performance of globally interpretable ridge regression (trained on the entire dataset from the scratch) is much worse than other complex non-linear models, implying that modeling non-linear relationships between the features and the labels is important towards high prediction performance. For other locally interpretable modeling methods (LIME, SILO, MAPLE), the performance is far worse than the original black-box model, showing that they fail at efficiently Under review as a conference paper at ICLR 2020 distilling the non-linear black-box models. In some cases (especially on the Facebook dataset), the performance of the benchmarks is even worse than the performance of global ridge regression (highlighted in red), questioning the value of using these locally interpretable models instead of globally interpretable ridge regression. In contrast, RL-LIM achieves similar overall prediction performance to the black-box models and significantly outperforms global ridge regression.  Table 1  also compares the fidelity in terms of R 2 score for regression using ridge regression as the locally interpretable model (LMAE results can be found in Appendix E.3). We observe that R 2 scores for some cases (especially on Facebook dataset and LIME) are negative which represent that the outputs of the locally interpretable models are even worse than the constant mean value estimator. On the other hand, RL-LIM achieves higher and positive R 2 values consistently for all datasets and black-box models than other benchmarks.  Table 2  shows a similar analysis for classification using shallow regression trees (with max depth of 3) as the locally interpretable model (Regression trees are used to model logit outputs for classifica- tion.). The overall prediction performance of four black-box models are significantly better than the globally interpretable decision tree which demonstrates the superior fitting by complex black-box models. Among the locally interpretable models, RL-LIM achieves the best APR and R 2 score for most cases, underlining its strength in distilling the predictions of the black-box model accurately. In some cases, the benchmarks (especially for LIME) achieve lower overall prediction performance than the globally interpretable decision tree (highlighted in red). The overall prediction performance and fidelity metrics of all locally interpretable models seem better for classification problems than regression problems. We expect that the predictions of black-box models are mostly highly confi- dent, i.e. located near 0 or 1; thus, locally interpretable models can easily distill the predictions of the black-box models for classification than regression.

Section Title: QUALITATIVE ANALYSES - INTERPRETATIONS OF RL-LIM ON ADULT INCOME DATASET
  QUALITATIVE ANALYSES - INTERPRETATIONS OF RL-LIM ON ADULT INCOME DATASET In this subsection, we qualitatively analyze the local explanations provided by RL-LIM on the Adult Income dataset. Although RL-LIM is able to provide local explanations for each individual sepa- rately, we analyze its explanations in subgroup granularity for better visualization and understand- ing.  Fig. 4  represents the feature importance (derived by RL-LIM as the local explanations) for Under review as a conference paper at ICLR 2020 five subgroups in predicting the annual income using XGBoost as the black-box model. We use ridge regression as the locally interpretable model and the absolute value of fitted coefficients as the estimated feature importance. As can be observed in  Fig. 4 , for age subgroups, capital gain seems much more important for mature people (older than 25) than young people (younger than 25). For education subgroups, capital gain/loss, occupation, and native countries are more critical for highly-educated people (Doctorate, Prof-school, and Masters graduates) than the others. We do not discover notable biases of black-box models for gender, marital status, and race subgroups.

Section Title: CONCLUSIONS
  CONCLUSIONS We propose a novel method for locally interpretable modeling of pre-trained black-box models. Our proposed method employs reinforcement learning to select a small number of valuable instances and use them to train a low-capacity locally interpretable model. The selection mechanism is guided with a reward obtained from the similarity of predictions of the locally interpretable model and the black- box model. Our approach near-matches the performance of black-box models and significantly outperforms alternative techniques in terms of overall prediction performance and fidelity metrics consistently across various datasets and black-box models.
  We use python packages (including Sklearn and Tensorflow) to implement those predictive models and the details can be found in the hyper-links (colored in blue) of each model and Appendix B.

```
