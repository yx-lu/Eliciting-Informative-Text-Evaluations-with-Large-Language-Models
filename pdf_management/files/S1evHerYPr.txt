Title:
```
Published as a conference paper at ICLR 2020 IMPROVING GENERALIZATION IN META REINFORCE- MENT LEARNING USING LEARNED OBJECTIVES
```
Abstract:
```
Biological evolution has distilled the experiences of many learners into the gen- eral learning algorithms of humans. Our novel meta reinforcement learning algo- rithm MetaGenRL is inspired by this process. MetaGenRL distills the experiences of many complex agents to meta-learn a low-complexity neural objective func- tion that decides how future individuals will learn. Unlike recent meta-RL algo- rithms, MetaGenRL can generalize to new environments that are entirely different from those used for meta-training. In some cases, it even outperforms human- engineered RL algorithms. MetaGenRL uses off-policy second-order gradients during meta-training that greatly increase its sample efficiency.
```

Figures/Tables Captions:
```
Figure 1: A schematic of MetaGenRL. On the left a population of agents (i ∈ 1, . . . , N ), where each member consist of a critic Q (i) θ and a policy π (i) φ that interact with a particular environment e (i) and store collected data in a corresponding replay buffer B (i) . On the right a meta-learned neural objective function L α that is shared across the population. Learning (dotted arrows) proceeds as follows: Each policy is updated by differentiating L α , while the critic is updated using the usual TD-error (not shown). L α is meta-learned by computing second-order gradients that can be obtained by differentiating through the critic.
Figure 2: An overview of L α (τ, x(φ), V ).
Figure 3: Comparing the test-time training behavior of the meta-learned objective functions by MetaGenRL to other (meta) reinforcement learning algorithms. We train randomly initialized agents on (a) environments that were encountered during training, and (b) on significantly different environ- ments that were unseen. Training environments are denoted by † in the legend. All runs are shown with mean and standard deviation computed over multiple random seeds (MetaGenRL: 6 meta-train × 2 meta-test seeds, RL 2 : 6 meta-train × 2 meta-test seeds, EPG: 3 meta-train × 2 meta-test seeds, and 6 seeds for all others).
Figure 4: Meta-training with 20 agents on Cheetah and Lunar. We test the objective function at five stages of meta-training by using it to train three randomly initialized agents on Hopper.
Figure 5: We meta-train MetaGenRL using several alternative parametrizations of L α on a) Lunar and Cheetah, and b) present results of testing on Cheetah. During meta-training a representative ex- ample of a single agent population is shown with shaded regions denoting standard deviation across the population. Meta-test results are reported as per usual across 6 meta-train × 2 meta-test seeds.
Figure 6: We meta-train MetaGenRL on the LunarLander and HalfCheetah environments using one, three, and five inner gradient steps on φ. Meta-test results are reported across 3 meta-train × 2 meta-test seeds.
Table 1: Mean return across multiple seeds (MetaGenRL: 6 meta-train × 2 meta-test seeds, RL 2 : 6 meta-train × 2 meta-test seeds, EPG: 3 meta-train × 2 meta-test seeds) obtained by training randomly initialized agents during meta-test time on previously seen environments (cyan) and on unseen environments (brown). Boldface highlights best meta-learned algorithm. Mean returns (6 seeds) of several human-engineered algorithms are also listed.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The process of evolution has equipped humans with incredibly general learning algorithms. They enable us to solve a wide range of problems, even in the absence of a large number of related prior experiences. The algorithms that give rise to these capabilities are the result of distilling the collec- tive experiences of many learners throughout the course of natural evolution. By essentially learning from learning experiences in this way, the resulting knowledge can be compactly encoded in the ge- netic code of an individual to give rise to the general learning capabilities that we observe today. In contrast, Reinforcement Learning (RL) in artificial agents rarely proceeds in this way. The learn- ing rules that are used to train agents are the result of years of human engineering and design, (e.g. Williams (1992); Wierstra et al. (2008); Mnih et al. (2013); Lillicrap et al. (2016); Schulman et al. (2015a)). Correspondingly, artificial agents are inherently limited by the ability of the designer to incorporate the right inductive biases in order to learn from previous experiences. Several works have proposed an alternative framework based on meta reinforcement learn- ing (Schmidhuber, 1994; Wang et al., 2016; Duan et al., 2016; Finn et al., 2017; Houthooft et al., 2018; Clune, 2019). Meta-RL distinguishes between learning to act in the environment (the rein- forcement learning problem) and learning to learn (the meta-learning problem). Hence, learning itself is now a learning problem, which in principle allows one to leverage prior learning experi- ences to meta-learn general learning rules that surpass human-engineered alternatives. However, while prior work found that learning rules could be meta-learned that generalize to slightly different environments or goals (Finn et al., 2017; Plappert et al., 2018; Houthooft et al., 2018), generalization to entirely different environments remains an open problem. In this paper we present MetaGenRL 1 , a novel meta reinforcement learning algorithm that meta- learns learning rules that generalize to entirely different environments. MetaGenRL is inspired by the process of natural evolution as it distills the experiences of many agents into the parameters of an objective function that decides how future individuals will learn. Similar to Evolved Policy Gradients (EPG; Houthooft et al. (2018)), it meta-learns low complexity neural objective functions that can be used to train complex agents with many parameters. However, unlike EPG, it is able to meta-learn using second-order gradients, which offers several advantages as we will demonstrate. We evaluate MetaGenRL on a variety of continuous control tasks and compare to RL 2 (Wang et al., 2016; Duan et al., 2016) and EPG in addition to several human engineered learning algorithms. Compared to RL 2 we find that MetaGenRL does not overfit and is able to train randomly initialized agents using meta-learned learning rules on entirely different environments. Compared to EPG we find that MetaGenRL is more sample efficient, and outperforms significantly under a fixed budget of environment interactions. The results of an ablation study and additional analysis provide further insight into the benefits of our approach.

Section Title: PRELIMINARIES
  PRELIMINARIES

Section Title: Notation
  Notation We consider the standard MDP Reinforcement Learning setting defined by a tuple e = (S, A, P, ρ 0 , r, γ, T ) consisting of states S, actions A, the transition probability distribution P : S × A × S → R + , an initial state distribution ρ 0 : S → R + , the reward function r : S × A → [−R max , R max ], a discount factor γ, and the episode length T . The objective for the probabilistic policy π φ : S × A → R + parameterized by φ is to maximize the expected discounted return: Human Engineered Gradient Estimators A popular gradient-based approach to maximiz- ing Equation 1 is REINFORCE (Williams, 1992). It directly differentiates Equation 1 with respect to φ using the likelihood ratio trick to derive gradient estimates of the form: Although this basic estimator is rarely used in practice, it has become a building block for an entire class of policy-gradient algorithms of this form. For example, a popular extension from Schulman et al. (2015b) combines REINFORCE with a Generalized Advantage Estimate (GAE) to yield the following policy gradient estimator: where A(τ, V, t) is the GAE and V : S → R is a value function estimate. Several recent other extensions include TRPO (Schulman et al., 2015a), which discourages bad policy updates using trust regions and iterative off-policy updates, or PPO (Schulman et al., 2017), which offers similar benefits using only first order approximations.

Section Title: Parametrized Objective Functions
  Parametrized Objective Functions In this work we note that many of these human engineered policy gradient estimators can be viewed as specific implementations of a general objective function L that is differentiated with respect to the policy parameters: Hence, it becomes natural to consider a generic parametrization of L that, for various choices of parameters α, recovers some of these estimators. In this paper, we will consider neural objective functions where L α is implemented by a neural network. Our goal is then to optimize the parameters α of this neural network in order to give rise to a new learning algorithm that best maximizes Equation 1 on an entire class of (different) environments.

Section Title: META-LEARNING NEURAL OBJECTIVES
  META-LEARNING NEURAL OBJECTIVES In this work we propose MetaGenRL, a novel meta reinforcement learning algorithm that meta- learns neural objective functions of the form L α (τ, π φ , V ). MetaGenRL makes use of value functions and second-order gradients, which makes it more sample efficient compared to prior work (Duan et al., 2016; Wang et al., 2016; Houthooft et al., 2018). More so, as we will demonstrate, MetaGenRL meta-learns objective functions that generalize to vastly different environments. Our key insight is that a differentiable critic Q θ : S × A → R can be used to measure the effect of locally changing the objective function parameters α based on the quality of the corresponding policy gradients. This enables a population of agents to use and improve a single parameterized objective function L α through interacting with a set of (potentially different) environments. During evaluation (meta-test time), the meta-learned objective function can then be used to train a randomly initialized RL agent in a new environment.

Section Title: FROM DDPG TO GRADIENT-BASED META-LEARNING OF NEURAL OBJECTIVES
  FROM DDPG TO GRADIENT-BASED META-LEARNING OF NEURAL OBJECTIVES We will formally introduce MetaGenRL as an extension of the DDPG actor-critic framework (Silver et al., 2014; Lillicrap et al., 2016). In DDPG, a parameterized critic of the form Q θ : S × A → R transforms the non-differentiable RL reward maximization problem into a myopic value maximiza- tion problem for any s t ∈ S. This is done by alternating between optimization of the critic Q θ and the (here deterministic) policy π φ . The critic is trained to minimize the TD-error by following: ∇ θ (st,at,rt,st+1) (Q θ (s t , a t ) − y t ) 2 , where y t = r t + γ · Q θ (s t+1 , π φ (s t+1 )), (5) and the dependence of y t on the parameter vector θ is ignored. The policy π φ is improved to increase the expected return from arbitrary states by following the gradient ∇ φ st Q θ (s t , π φ (s t )). Both gradients can be computed entirely off-policy by sampling trajectories from a replay buffer. MetaGenRL builds on this idea of differentiating the critic Q θ with respect to the policy parameters. It incorporates a parameterized objective function L α that is used to improve the policy (i.e. by following the gradient ∇ φ L α ), which adds one extra level of indirection: The critic Q θ improves L α , while L α improves the policy π φ . By first differentiating with respect to the objective function parameters α, and then with respect to the policy parameters φ, the critic can be used to measure the effect of updating π φ using L α on the estimated return 2 : This constitutes a type of second order gradient ∇ α ∇ φ that can be used to meta-train L α to provide better updates to the policy parameters in the future. In practice we will use batching to optimize Equation 6 over multiple trajectories τ . Similarly to the policy-gradient estimators from Section 2, the objective function L α (τ, x(φ), V ) receives as inputs an episode trajectory τ = (s 0:T −1 , a 0:T −1 , r 0:T −1 ), the value function estimates Compute objective function gradient ∆ i for agent i according to Equation 6 Sum gradients i ∆ i to update L α V , and an auxiliary input x(φ) (previously π φ ) that can be differentiated with respect to the policy parameters. The latter is critical to be able to differentiate with respect to φ and in the simplest case it consists of the action as predicted by the policy. While Equation 6 is used for meta-learning L α , the objective function L α itself is used for policy learning by following ∇ φ L α (τ, x(φ), V ). See  Figure 1  for an overview. MetaGenRL consists of two phases: During meta-training, we alternate between critic updates, objective function updates, and policy updates to meta-learn an objective function L α as described in Algorithm 1. During meta-testing in Algorithm 2, we take the learned objective function L α and keep it fixed while training a randomly initialized policy in a new environment to assess its performance. We note that the inputs to L α are sampled from a replay buffer rather than solely using on-policy data. If L α were to represent a REINFORCE-type objective then it would mean that differentiat- ing L α yields biased policy gradient estimates. In our experiments we will find that the gradients from L α work much better in comparison to a biased off-policy REINFORCE algorithm, and to an importance-sampled unbiased REINFORCE algorithm, while also improving over the popular on-policy REINFORCE and PPO algorithms.

Section Title: PARAMETRIZING THE OBJECTIVE FUNCTION
  PARAMETRIZING THE OBJECTIVE FUNCTION We will implement L α using an LSTM (Gers et al., 2000; Hochreiter & Schmidhuber, 1997) that iterates over τ in reverse order and depends on the current policy action π φ (s t ) (see  Figure 2 ). At every time-step L α receives the reward r t , taken action a t , predicted action by the current policy π φ (s t ), the time t, and value function estimates V t , V t+1 3 . At each step the LSTM outputs the objec- tive value l t , all of which are summed to yield a single scalar output value that can be differentiated with respect to φ. In order to accommodate varying action dimensionalities across different environ- ments, both π φ (s t ) and a t are first convolved and then averaged to obtain an action embedding that does not depend on the action dimensionality. Additional details, including suggestions for more expressive alternatives are available in Appendix B. By presenting the trajectory in reverse order to the LSTM (and L α correspondingly), it is able to assign credit to an action a t based on its future impact on the reward, similar to policy gradient estimators. More so, as a general function approximator using these inputs, the LSTM is in prin- ciple able to learn different variance and bias reduction techniques, akin to advantage estimates, generalized advantage estimates, or importance weights 4 . Due to these properties, we expect the class of objective functions that is supported to somewhat relate to a REINFORCE (Williams, 1992) estimator that uses generalized advantage estimation (Schulman et al., 2015b).

Section Title: GENERALITY AND EFFICIENCY OF METAGENRL
  GENERALITY AND EFFICIENCY OF METAGENRL MetaGenRL offers a general framework for meta-learning objective functions that can represent a wide range of learning algorithms. In particular, it is only required that both π φ and L α can be differentiated w.r.t. to the policy parameters φ. In the present work, we use this flexibility to leverage population-based meta-optimization, increase sample efficiency through off-policy second- order gradients, and to improve the generalization capabilities of meta-learned objective functions. Population-Based A general objective function should be applicable to a wide range of environ- ments and agent parameters. To this extent MetaGenRL is able to leverage the collective experience of multiple agents to perform meta-learning by using a single objective function L α shared among a population of agents that each act in their own (potentially different) environment. Each agent locally computes Equation 6 over a batch of trajectories, and the resulting gradients are combined to update L α . Thus, the relevant learning experience of each individual agent is compressed into the objective function that is available to the entire population at any given time.

Section Title: Sample Efficiency
  Sample Efficiency An alternative to learning neural objective functions using a population of agents is through evolution as in EPG (Houthooft et al., 2018). However, we expect meta-learning using second-order gradients as in MetaGenRL to be much more sample efficient. This is due to off-policy training of the objective function L α and its subsequent off-policy use to improve the policy. Indeed, unlike in evolution there is no need to train multiple randomly initialized agents in their entirety in order to evaluate the objective function, thus speeding up credit assignment. Rather, at any point in time, any information that is deemed useful for future environment interactions can directly be incorporated into the objective function. Finally, using the formulation in Equation 6 one can measure the effects of improving the policy using L α for multiple steps by increasing the corresponding number of gradient steps before applying Q θ , which we will explore in Section 5.2.3.

Section Title: Meta-Generalization
  Meta-Generalization The focus of this work is to learn general learning rules that during test- time can be applied to vastly different environments. A strict separation between the policy and the learning rule, the functional form of the latter, and training across many environments all contribute to this. Regarding the former, a clear separation between the policy and the learning rule as in MetaGenRL is expected to be advantageous for two reasons. Firstly, it allows us to specify the number of parameters of the learning rule independent of the policy and critic parameters. For example, our implementation of L α uses only 15K parameters for the objective function compared to 384K parameters for the policy and critic. Hence, we are able to only use a short description length for the learning rule. A second advantage that is gained is that the meta-learner is unable to directly change the policy and must, therefore, learn to make use of the objective function. This makes it difficult for the meta-learner to overfit to the training environments.

Section Title: RELATED WORK
  RELATED WORK Among the earliest pursuits in meta-learning are meta-hierarchies of genetic algorithms (Schmidhu- ber, 1987) and learning update rules in supervised learning (Bengio et al., 1990). While the former introduced a general framework of entire meta-hierarchies, it relied on discrete non-differentiable programs. The latter introduced local update rules that included free parameters, which could be Published as a conference paper at ICLR 2020 learned using gradients in a supervised setting. Schmidhuber (1993) introduced a differentiable self-referential RNN that could address and modify its own weights, albeit difficult to learn. Hochreiter et al. (2001) introduced differentiable meta-learning using RNNs to scale to larger prob- lem instances. By giving an RNN access to its prediction error, it could implement its own meta- learning algorithm, where the weights are the meta-learned parameters, and the hidden states the subject of learning. This was later extended to the RL setting (Wang et al., 2016; Duan et al., 2016; Santoro et al., 2016; Mishra et al., 2018) (here refered to as RL 2 ). As we show empirically in our pa- per, meta-learning with RL 2 does not generalize well. It lacks a clear separation between policy and objective function, which makes it easy to overfit on training environments. This is exacerbated by the imbalance of O(n 2 ) meta-learned parameters to learn O(n) activations, unlike in MetaGenRL. Many other recent meta-learning algorithms learn a policy parameter initialization that is later fine- tuned using a fixed reinforcement learning algorithm (Finn et al., 2017; Schulman et al., 2017; Grant et al., 2018; Yoon et al., 2018). Different from MetaGenRL, these approaches use second order gradients on the same policy parameter vector instead of using a separate objective function. Albeit in principle general (Finn & Levine, 2018), the mixing of policy and learning algorithm leads to a complicated way of expressing general update rules. Similar to RL 2 , adaptation to related tasks is possible, but generalization is difficult (Houthooft et al., 2018). Objective functions have been learned prior to MetaGenRL. Houthooft et al. (2018) evolve an ob- jective function that is later used to train an agent. Unlike MetaGenRL, this approach is extremely costly in terms of the number of environment interactions required to evaluate and update the ob- jective function. Most recently, Bechtle et al. (2019) introduced learned loss functions for rein- forcement learning that also make use of second-order gradients, but use a policy gradient estimator instead of a Q-function. Similar to other work, their focus is only on narrow task distributions. Learned objective functions have also been used for learning unsupervised representations (Metz et al., 2019), DDPG-like meta-gradients for hyperparameter search (Xu et al., 2018), and learning from human demonstrations (Yu et al., 2018). Concurrent to our work, Alet et al. (2020) uses tech- niques from architecture search to search for viable artificial curiosity objectives that are composed of primitive objective functions. Li & Malik (2016; 2017) and Andrychowicz et al. (2016) conduct meta-learning by learning op- timizers that update parameters φ by modulating the gradient of some fixed objective function L: ∆φ = f α (∇ φ L) where α is learned. They differ from MetaGenRL in that they only modulate the gradient of a fixed objective function L instead of learning L itself. Another connection exists to meta-learned intrinsic reward functions (Schmidhuber, 1991a; Dayan & Hinton, 1993; Wiering & Schmidhuber, 1996; Singh et al., 2004; Niekum et al., 2011; Zheng et al., 2018; Jaderberg et al., 2019). Choosing ∇ φ L α =∇ φ T t=1r t (τ ), wherer t is a meta-learned reward and∇ θ is a gradient estimator (such as a value based or policy gradient based estimator) reveals that meta-learning objective functions includes meta-learning the gradient estimatior∇ itself as long as it is expressible by a gradient ∇ θ on an objective L α . In contrast, for intrinsic reward functions, the gradient estimator∇ is normally fixed. Finally, we note that positive transfer between different tasks (reward functions) as well as envi- ronments (e.g. different Atari games) has been shown previously in the context of transfer learn- ing (Kistler et al., 1997; Parisotto et al., 2015; Rusu et al., 2016; 2019; Nichol et al., 2018) and meta-critic learning across tasks (Sung et al., 2017). In contrast to this work, the approaches that have shown to be successful in this domain rely entirely on human-engineered learning algorithms.

Section Title: EXPERIMENTS
  EXPERIMENTS We investigate the learning and generalization capabilities of MetaGenRL on several continuous control benchmarks including HalfCheetah (Cheetah) and Hopper from MuJoCo (Todorov et al., 2012), and LunarLanderContinuous (Lunar) from OpenAI gym (Brockman et al., 2016). These environments differ significantly in terms of the properties of the underlying system that is to be controlled, and in terms of the dynamics that have to be learned to complete the environment. Hence, by training meta-RL algorithms on one environment and testing on other environments they provide a reasonable measure of out-of-distribution generalization. In our experiments, we will mainly compare to EPG and to RL 2 to evaluate the efficacy of our ap- proach. We will also compare to several fixed model-free RL algorithms to measure how well the algorithms meta-learned by MetaGenRL compare to these handcrafted alternatives. Unless other- wise mentioned, we will meta-train MetaGenRL using 20 agents that are distributed equally over the indicated training environments 5 . Meta-learning uses clipped double-Q learning, delayed policy & objective updates, and target policy smoothing from TD3 (Fujimoto et al., 2018). We will allow for 600K environment interactions per agent during meta-training and then meta-test the objective function for 1M interactions. Further details are available in Appendix B.

Section Title: COMPARISON TO PRIOR WORK
  COMPARISON TO PRIOR WORK

Section Title: Evaluating on previously seen environments
  Evaluating on previously seen environments We meta-train MetaGenRL on Lunar and compare its ability to train a randomly initialized agent at test-time (i.e. using the learned objective function and keeping it fixed) to DDPG, PPO, and on- and off-policy REINFORCE (both using GAE) across multiple seeds. Figure 3a shows that MetaGenRL markedly outperforms both the REINFORCE baselines and PPO. Compared to DDPG, which finds the optimal policy, MetaGenRL performs only slightly worse on average although the presence of outliers increases its variance. In particular, we find that some meta-test agents get 'stuck' for some time before reaching the optimal policy (see Section A.2 for additional analysis). Indeed, when evaluating only the best meta-learned objective function that was obtained during meta-training (MetaGenRL (best objective func) in Figure 3a) we are able to observe a strong reduction in variance and even better performance. We also report results (Figure 3a) when meta-training MetaGenRL on both Lunar and Cheetah, and compare to EPG and RL 2 that were meta-trained on these same environments 6 . For MetaGenRL we were able to obtain similar performance to meta-training on only Lunar in this case. In contrast, for EPG it can be observed that even one billion environment interactions is insufficient to find a good objective function (in Figure 3a quickly dropping below -300). Finally, we find that RL 2 reaches the optimal policy after 100 million meta-training iterations, and that its performance is unaffected by additional steps during testing on Lunar. We note that RL 2 does not separate the policy and the learning rule and indeed in a similar 'within distribution' evaluation, RL 2 was found successful (Wang et al., 2016; Duan et al., 2016). 5 An ablation study in Section A.3 revealed that a large number of agents is indeed required. 6 In order to ensure a good baseline we allowed for a maximum of 100M environment interactions for RL 2  Table 1  provides a similar comparison for two other environments. Here we find that in general MetaGenRL is able to outperform the REINFORCE baselines and PPO, and in most cases (except for Cheetah) performs similar to DDPG 7 . We also find that MetaGenRL consistently outperforms EPG, and often RL 2 . For an analysis of meta-training on more than two environments we refer to Appendix A.

Section Title: Generalization to vastly different environments
  Generalization to vastly different environments We evaluate the same objective functions learned by MetaGenRL, EPG and the recurrent dynamics by RL 2 on Hopper, which is signifi- cantly different compared to the meta-training environments. Figure 3b shows that the learned objective function by MetaGenRL continues to outperform both PPO and our implementations of REINFORCE, while the best performing configuration is even able to outperform DDPG. When comparing to related meta-RL approaches, we find that MetaGenRL is significantly better in this case. The performance of EPG remains poor, which was expected given what was observed on previously seen environments. On the other hand, we now find that the RL 2 baseline fails com- pletely (resulting in a flat low-reward evaluation), suggesting that the learned learning rule that was previously found to be successful is in fact entirely overfitted to the environments that were seen during meta-training. We were able to observe similar results when using different train and test environment splits as reported in  Table 1 , and in Appendix A.

Section Title: ANALYSIS
  ANALYSIS

Section Title: META-TRAINING PROGRESSION OF OBJECTIVE FUNCTIONS
  META-TRAINING PROGRESSION OF OBJECTIVE FUNCTIONS Previously we focused on test-time training randomly initialized agents using an objective function that was meta-trained for a total of 600K steps (corresponding to a total of 12M environment inter- actions across the entire population). We will now investigate the quality of the objective functions during meta-training.  Figure 4  displays the result of evaluating an objective function on Hopper at different intervals dur- ing meta-training on Cheetah and Lunar. Initially (28K steps) it can be seen that due to lack of meta-training there is only a marginal improvement in the return obtained during test time. How- ever, after only meta-training for 86K steps we find (perhaps surprisingly) that the meta-trained Published as a conference paper at ICLR 2020 objective function is already able to make consistent progress in optimizing a randomly initialized agent during test-time. On the other hand, we observe large variances at test-time during this phase of meta-training. Throughout the remaining stages of meta-training we then observe an increase in convergence speed, more stable updates, and a lower variance across seeds.

Section Title: ABLATION STUDY
  ABLATION STUDY We conduct an ablation study of the neural objective function that was described in Section 3.2. In particular, we assess the dependence of L α on the value estimates V t ,V t+1 and on the time compo- nent that could to some extent be learned. Other ablations, including limiting access to the action chosen or to the received reward, are expected to be disastrous for generalization to any other envi- ronment (or reward function) and therefore not explored. Dependence on t We use a parameterized objective function of the form L α (a t , r t , V t , π φ (s t )|t ∈ 0, ..., T − 1) as in  Figure 2  except that it does not receive information about the time-step t at each step. Although information about the current time-step is required in order to learn (for example) a generalized advantage estimate (Schulman et al., 2015b), the LSTM could in principle learn such time tracking on it own, and we expect only minor effects on meta-training and during meta-testing. Indeed in Figure 5b it can be seen that the neural objective function performs well without access to t, although it converges slower on Cheetah during meta-training (Figure 5a). Dependence on V We use a parameterized objective function of the form L α (a t , r t , t, π φ (s t )|t ∈ 0, ..., T − 1) as in  Figure 2  except that it does not receive any information about the value estimates at time-step t. There exist reinforcement learning algorithms that work without value function esti- mates (eg. Williams (1992); Schmidhuber & Zhao (1998)), although in the absence of an alternative baseline these often have a large variance. Similar results are observed for this ablation in Figure 5a Published as a conference paper at ICLR 2020 during meta-training where a possibly large variance appears to affect meta-training. Correspond- ingly during test-time (Figure 5b) we do not find any meaningful training progress to take place. In contrast, we find that we can remove the dependence on one of the value function estimates, i.e. remove V t+1 but keep V t , which during some runs even increases performance.

Section Title: MULTIPLE GRADIENT STEPS
  MULTIPLE GRADIENT STEPS We analyze the effect of making multiple gradient updates to the policy using L α before applying the critic to compute second-order gradients with respect to the objective function parameters as in Equation 6. While in previous experiments we have only considered applying a single update, multiple gradient updates might better capture long term effects of the objective function. At the same time, moving further away from the current policy parameters could reduce the overall quality of the second-order gradients. Indeed, in  Figure 6  it can be observed that using 3 gradient steps already slightly increases the variance during test-time training on Hopper and Cheetah after meta- training on LunarLander and Cheetah. Similarly, we find that further increasing the number of gradient steps to 5 harms performance.

Section Title: CONCLUSION
  CONCLUSION We have presented MetaGenRL, a novel off-policy gradient-based meta reinforcement learning al- gorithm that leverages a population of DDPG-like agents to meta-learn general objective functions. Unlike related methods the meta-learned objective functions do not only generalize in narrow task distributions but show similar performance on entirely different tasks while markedly outperforming REINFORCE and PPO. We have argued that this generality is due to MetaGenRL's explicit sepa- ration of the policy and learning rule, the functional form of the latter, and training across multiple agents and environments. Furthermore, the use of second order gradients increases MetaGenRL's sample efficiency by several orders of magnitude compared to EPG (Houthooft et al., 2018). In future work, we aim to further improve the learning capabilities of the meta-learned objective functions, including better leveraging knowledge from prior experiences. Indeed, in our current implementation, the objective function is unable to observe the environment or the hidden state of the (recurrent) policy. These extensions are especially interesting as they may allow more compli- cated curiosity-based (Schmidhuber, 1991b; 1990; Houthooft et al., 2016; Pathak et al., 2017) or model-based (Schmidhuber, 1990; Weber et al., 2017; Ha & Schmidhuber, 2018) algorithms to be learned. To this extent, it will be important to develop introspection methods that analyze the learned objective function and to scale MetaGenRL to make use of many more environments and agents.
  We emphasize that the neural objective function under consideration is unable to implement DDPG and only uses a constant value estimate (i.e. ∇ φ V = 0 by using gradient stopping) during meta testing.

```
