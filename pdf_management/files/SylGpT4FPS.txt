Title:
```
Under review as a conference paper at ICLR 2020 LAST-ITERATE CONVERGENCE RATES FOR MIN-MAX OPTIMIZATION
```
Abstract:
```
While classic work in convex-concave min-max optimization relies on average- iterate convergence results, the emergence of nonconvex applications such as training Generative Adversarial Networks has led to renewed interest in last-iterate convergence guarantees. Proving last-iterate convergence is challenging because many natural algorithms, such as Simultaneous Gradient Descent/Ascent, provably diverge or cycle even in simple convex-concave min-max settings, and previous work on global last-iterate convergence rates has been limited to the bilinear and convex-strongly concave settings. In this work, we show that the HAMILTONIAN GRADIENT DESCENT (HGD) algorithm achieves linear convergence in a variety of more general settings, including convex-concave problems that satisfy a novel "sufficiently bilinear" condition. We also prove convergence rates for stochastic HGD and for some parameter settings of the Consensus Optimization algorithm of Mescheder et al. (2017).
```

Figures/Tables Captions:
```
Figure 1: HGD converges quickly, while SGDA spirals. This nonconvex-nonconcave objective is in defined in Appendix K.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION In this paper we consider methods to solve smooth unconstrained min-max optimization problems. In the most classical setting, a min-max objective has the form min x1 max x2 g(x 1 , x 2 ) where g : R d × R d → R is a smooth objective function with two inputs. The usual goal in such problems is to find a saddle point, also known as a min-max solution, which is a pair (x * 1 , x * 2 ) ∈ R d × R d that satisfies g(x * 1 , x 2 ) ≤ g(x * 1 , x * 2 ) ≤ g(x 1 , x * 2 ) (1) for every x 1 ∈ R d and x 2 ∈ R d . Min-max problems have a long history, going back at least as far as  Neumann (1928) , which formed the basis of much of modern game theory, and including a great deal of work in the 1950s when algorithms such as fictitious play were explored (Brown, 1951;  Robinson, 1951 ). The convex-concave setting, where we assume g is convex in x 1 and concave in x 2 , is a classic min-max problem that has a number of different applications, such as solving constrained convex optimization problems. While a variety of tools have been developed for this setting, a very popular approach within the machine learning community has been the use of so-called no-regret algorithms ( Cesa-Bianchi & Lugosi, 2006 ;  Hazan, 2016 ). This trick, which was originally developed by  Hannan (1957)  and later emerged in the development of boosting ( Freund & Schapire, 1999 ), provides a simple computational method via repeated play: each of the inputs x 1 and x 2 are updated iteratively according to no-regret learning protocols, and one can prove that the average-iterates (x 1 ,x 2 ) converge to a min-max solution. Recently, interest in min-max optimization has surged due to the enormous popularity of Generative Adversarial Networks (GANs), whose training involves solving a nonconvex min-max problem where x 1 and x 2 correspond to the parameters of two different neural nets ( Goodfellow et al., 2014 ). The fundamentally nonconvex nature of this problem changes two things. First, it is infeasible to find a "global" solution of the min-max objective. Instead, a typical goal in GAN training is to find a local min-max, namely a pair (x * 1 , x * 2 ) that satisfies (1) for all (x 1 , x 2 ) in some neighborhood of (x * 1 , x * 2 ).

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Second, iterate averaging lacks the theoretical guarantees present in the convex-concave setting. This has motivated research on last-iterate convergence guarantees, which are appealing because they more easily carry over from convex to nonconvex settings. Last-iterate convergence guarantees for min-max problems have been challenging to prove since standard analysis of no-regret algorithms says essentially nothing about last-iterate convergence. Widely used no-regret algorithms, such as Simultaneous Gradient Descent/Ascent (SGDA), fail to converge even in the simple bilinear setting where g(x 1 , x 2 ) = x 1 Cx 2 for some arbitrary matrix C. SGDA provably cycles in continuous time and diverges in discrete time (see for example  Daskalakis et al. (2018) ;  Mescheder et al. (2018) ). In fact, the full range of Follow-The-Regularized- Leader (FTRL) algorithms provably do not converge in zero-sum games with interior equilibria ( Mertikopoulos et al., 2018 ). This occurs because the iterates of the FTRL algorithms exhibit cyclic behavior, a phenomenon commonly observed when training GANs in practice as well. Much of the recent research on last-iterate convergence in min-max problems has focused on asymptotic or local convergence ( Mertikopoulos et al., 2019 ;  Mescheder et al., 2017 ;  Daskalakis & Panageas, 2018 ;  Balduzzi et al., 2018 ;  Letcher et al., 2019 ;  Mazumdar et al., 2019 ). While these results are certainly useful, one would ideally like to prove global non-asymptotic last-iterate convergence rates. Provable global convergence rates allow for quantitative comparison of different algorithms and can aid in choosing learning rates and architectures to ensure fast convergence in practice. Yet despite the extensive amount of literature on convergence rates for convex optimization, very few global last-iterate convergence rates have been proved for min-max problems. Existing work on global last-iterate convergence rates has been limited to the bilinear or convex-strongly concave settings ( Tseng, 1995 ;  Liang & Stokes, 2019 ;  Du & Hu, 2019 ; Mokhtari et al., 2019). In particular, the following basic question is still open: "What global last-iterate convergence rates are achievable for convex-concave min-max problems?"

Section Title: Our Contribution
  Our Contribution Understanding global last-iterate rates in the convex-concave setting is an important stepping stone towards provable last-iterate rates in the nonconvex-nonconcave setting. Motivated by this, we prove new linear last-iterate convergence rates in the convex-concave setting for an algorithm called HAMILTONIAN GRADIENT DESCENT (HGD) under weaker assumptions compared to previous results. HGD is gradient descent on the squared norm of the gradient, and it has been mentioned in  Mescheder et al. (2017) ;  Balduzzi et al. (2018) . Our results are the first to show non-asymptotic convergence of an efficient algorithm in settings that not linear or strongly convex in either input. In particular, we introduce a novel "sufficiently bilinear" condition on the second-order derivatives of the objective g and show that this condition is sufficient for HGD to achieve linear convergence in convex-concave settings. The "sufficiently bilinear" condition appears to be a new sufficient condition for linear convergence rates that is distinct from previously known conditions such as the Polyak-Łojasiewicz (PL) condition or pure bilinearity. Our analysis relies on showing that the squared norm of the gradient satisfies the PL condition in various settings. As a corollary of this result, we can leverage  Karimi et al. (2016)  to show that a stochastic version of HGD will have a last-iterate convergence rate of O(1/ √ k) in the "sufficiently bilinear" setting. On the practical side, while vanilla HGD has issues training GANs in practice,  Mescheder et al. (2017)  show that a related algorithm known as Consensus Optimization (CO) can effectively train GANs in a variety of settings, including on CIFAR-10 and celebA. We show that CO can be viewed as a perturbation of HGD, which implies that for some parameter settings, CO converges at the same rate as HGD. We begin in Section 2 with background material and notation, including some of our key assump- tions. In Section 3, we discuss Hamiltonian Gradient Descent (HGD), and we present our linear convergence rates for HGD in various set- tings. In Section 4, we present some of the key technical components used to prove our results from Section 3. Finally, in Section 5, we present our results for Stochastic HGD and Consensus Optimization. The details of our proofs are in Appendix H.

Section Title: BACKGROUND
  BACKGROUND

Section Title: PRELIMINARIES
  PRELIMINARIES In this section, we discuss some key definitions and notation. We will use ||·|| to denote the Euclidean norm for vectors or the operator norm for matrices or tensors. For a symmetric matrix A, we will use λ min (A) and λ max (A) to denote the smallest and largest eigenvalues of A. For a general real matrix A, σ min (A) and σ max (A) denote the smallest and largest singular values of A. Notation Since g is a function of x 1 ∈ R d and x 2 ∈ R d , we will often consider x 1 and x 2 to be components of one vector x = (x 1 , x 2 ). We will use superscripts to denote iterate indices. Following  Balduzzi et al. (2018) , we use ξ = (∇ x1 g, −∇ x2 g) to denote the signed vector of partial derivatives. Under this notation, the Simultaneous Gradient Descent/Ascent (SGDA) update can be written as x (k+1) = x (k) − ηξ(x (k) ). We will use J to denote the Jacobian of ξ, i.e. J ≡ ∇ξ = ∇ 2 x1x1 g ∇ 2 x1x2 g −∇ 2 x2x1 g −∇ 2 x2x2 g . Note that unlike the Hessian in standard optimization, J is not symmetric, due to the negative sign in ξ. When clear from the context, we often omit dependence on x when writing ξ, J, g, H, and other functions. Note that ξ, J, and H are defined for a given objective g - we omit this dependence as well for notational clarity. We will always assume g is sufficiently differentiable whenever we take derivatives. In particular, we assume second-order differentiability in Section 3. We will also use the following non-standard definition for notational convenience:

Section Title: Notions of convergence in min-max problems
  Notions of convergence in min-max problems The convergence rates in this paper will apply to min-max problems where g satisfies the following assumption: Assumption 2.6. All critical points of the objective g are global min-maxes (i.e. they satisfy (1)). In other words, we prove convergence rates to min-maxes in settings where convergence to criti- cal points is necessary and sufficient for convergence to min-maxes. This assumption is true for convex-concave settings, but also holds for some nonconvex-nonconcave settings, as we discuss in Ap- pendix E. This assumption allows us to measure the convergence of our algorithms to -approximate critical points, defined as follows: Definition 2.7. Let ≥ 0. A point x ∈ R d × R d is an -approximate critical point if ||ξ(x)|| ≤ . Convergence to approximate critical points is a necessary condition for convergence to local or global minima, and it is a natural measure of convergence since the value of g at a given point gives no information about how close we are to a min-max. Our main convergence rate results focus on this first-order notion of convergence, which is sufficient given Assumption 2.6. We discuss notions of second-order convergence and ways to adapt our results to the general nonconvex setting in Appendix A.

Section Title: RELATED WORK
  RELATED WORK

Section Title: Asymptotic and local convergence
  Asymptotic and local convergence Several recent papers have given asymptotic or local con- vergence results for min-max problems.  Mertikopoulos et al. (2019)  show that the extragradient (EG) algorithm converges asymptotically in a broad class of problems known as coherent saddle point problems, which include quasiconvex-quasiconcave problems. However, they do not prove convergence rates. For more general smooth nonconvex min-max problems, a number of different papers have given local stability or local asymptotic convergence results for various algorithms, which we discuss in Appendix A. Non-asymptotic convergence rates Work on global non-asymptotic last-iterate convergence rates has been limited to very restrictive settings. A classic result by  Rockafellar (1976)  shows a linear convergence rate for the proximal point method in the bilinear and strongly convex-strongly concave cases. Another classic result, by Tseng (1995), shows a linear convergence rate for the extragradient algorithm in the bilinear case.  Liang & Stokes (2019)  show that a number of algorithms achieve a linear convergence rate in the bilinear case, including Optimistic Mirror Descent (OMD) and Consensus Optimization (CO). They also show that SGDA obtains a linear convergence rate in the strongly convex-strongly concave case.  Mokhtari et al. (2019)  show that OMD and EG obtain a linear rate for the strongly convex-strongly concave case, in addition to proving similar results for generalized versions of both algorithms. Finally,  Du & Hu (2019)  show that SGDA achieves a linear convergence rate for a convex-strongly concave setting with a full column rank linear interaction term. Non-uniform average-iterate convergence A number of recent works have studied the conver- gence of non-uniform averages of iterates, which can be viewed as an interpolation between the standard uniform average-iterate and last-iterate. We discuss these works further in Appendix B.

Section Title: HAMILTONIAN GRADIENT DESCENT
  HAMILTONIAN GRADIENT DESCENT Our main algorithm for finding saddle points of g(x 1 , x 2 ) is called HAMILTONIAN GRADIENT DESCENT (HGD). HGD consists of performing gradient descent on a particular objective function H that we refer to as the Hamiltonian, following the terminology of  Balduzzi et al. (2018) . 2 If we let ξ := ∂g ∂x1 , − ∂g ∂x2 be the vector of (appropriately-signed) partial derivatives, then the Hamiltonian is: Since a critical point occurs when ξ(x) = 0, we can find a (approximate) critical point by finding a (approximate) minimizer of H. Moreover, under Assumption 2.6, finding a critical point is equivalent to finding a saddle point. This motivates the HGD update procedure on x (k) = (x (k) 1 , x (k) 2 ) with step-size η > 0: HGD has been mentioned in  Mescheder et al. (2017) ;  Balduzzi et al. (2018) , and it strongly resembles the Consensus Optimization (CO) approach of  Mescheder et al. (2017) . The HGD update requires a Hessian-vector product because ∇H = ξ J, making HGD a second-order iterative scheme. However, Hessian-vector products are cheap to compute when the objective is defined by a neural net, taking only two gradient oracle calls (Pearlmutter, 1994). This makes the Hessian-vector product oracle a theoretically appealing primitive, and it has been used widely in the nonconvex optimization literature. To the best of our knowledge, previous work on last-iterate convergence rates has only focused on how algorithms perform in three particular cases: (a) when the objective g is bilinear, (b) when g is strongly convex-strongly concave, and (c) when g is convex-strongly concave ( Tseng, 1995 ;  Liang & Stokes, 2019 ;  Du & Hu, 2019 ; Mokhtari et al., 2019). The existence of methods with provable finite-time guarantees for settings beyond the aforementioned has remained an open problem. This work is the first to show that an efficient algorithm, namely HGD, can achieve non-asymptotic convergence in settings that are not strongly convex or linear in either player.

Section Title: CONVERGENCE RATES FOR HGD
  CONVERGENCE RATES FOR HGD We now state our main theorems for this paper, which show convergence to critical points. When Assumption 2.6 holds, we get convergence to min-maxes. All of our main results will use the following multi-part assumption: Assumption 3.1. Let g : R d × R d → R. 1. Assume a critical point for g exists. 2. Assume g is (L 1 , L 2 , L 3 )-Lipschitz and let L H = L 1 L 3 + L 2 2 . Our first theorem shows that HGD converges for the strongly convex-strongly concave case. Although simple, this result will help us demonstrate our analysis techniques. Theorem 3.2. Let Assumption 3.1 hold and let g(x 1 , x 2 ) be α-strongly convex in x 1 and α-strongly concave in x 2 . Then HGD with step-size η = 1/L H starting from some x (0) ∈ R d × R d will have the following convergence rate: Next, we show that HGD converges when g is linear in one of its arguments and the cross-derivative is full rank. This setting allows a slightly tighter analysis compared to Theorem 3.4. Theorem 3.3. Let Assumption 3.1 hold and let g(x 1 , x 2 ) be L-smooth in x 1 and linear in x 2 , and assume the cross derivative ∇ 2 x1,x2 g is full rank with all singular values at least γ > 0 for all x ∈ R d × R d . Then HGD with step-size η = 1/L H starting from some x (0) ∈ R d × R d will have the following convergence rate: Finally, we show our main result, which requires smoothness in both players and a large, well- conditioned cross-derivative. Theorem 3.4. Let Assumption 3.1 hold and let g be L-smooth in x 1 and L-smooth in x 2 . Let µ 2 = min x1,x2 λ min ((∇ 2 x2x2 g(x 1 , x 2 )) 2 ) and ρ 2 = min x1,x2 λ min ((∇ 2 x1x1 g(x 1 , x 2 )) 2 ), and assume the cross derivative ∇ 2 x1x2 g is full rank with all singular values lower bounded by γ > 0 and upper bounded by Γ for all x ∈ R d × R d . Moreover, let the following "sufficiently bilinear" condition hold: Then HGD with step-size η = 1/L H starting from some x (0) ∈ R d × R d will satisfy As discussed above, Theorem 3.4 provides the first last-iterate convergence rate for min-max problems that does not require strong convexity or linearity in either input. For example, the objective g(x 1 , x 2 ) = f (x 1 ) + 3Lx 1 x 2 − h(x 2 ), where f and h are L-smooth convex functions, satisfies the assumptions of Theorem 3.4 and is not strongly convex or linear in either input. We discuss a simple example that is not convex-concave in Appendix E. We also show how our results can be applied to specific settings, such as the Dirac-GAN, in Appendix G. The "sufficiently bilinear" condition (3) is in some sense necessary for our linear convergence rate since linear convergence is impossible in general for convex-concave settings, due to lower bounds on convex optimization ( Agarwal & Hazan, 2018 ; Arjevani et al., 2017). We give some explanations for this condition in the following section. In simple experiments for HGD on convex-concave and nonconvex-nonconcave objectives, the convergence rate speeds up when there is a larger bilinear component, as expected from our theoretical results. We show these experiments in Appendix K.

Section Title: EXPLANATION OF "SUFFICIENTLY BILINEAR" CONDITION
  EXPLANATION OF "SUFFICIENTLY BILINEAR" CONDITION In this section, we explain the "sufficiently bilinear" condition (3). Suppose our objective is g(x 1 , x 2 ) =ĝ(x 1 , x 2 ) + cx 1 x 2 for a smooth functionĝ. Then for sufficiently large values of c (i.e. g has a large enough bilinear term), we see that g satisfies (3). To see this, note that if we have γ 4 > 4L 2 Γ 2 , then condition (3) holds. Let γ and Γ be lower and upper bounds on the singular values of ∇ 2 x1x2ĝ . Then it suffices to have (γ + c) 4 > 4L 2 (Γ + c) 2 , which is true for c = 3 max{L, Γ } (i.e. c = O(L) suffices). This condition is analogous to the case when we use SGDA on the objective g(x 1 , x 2 ) =ĝ(x 1 , x 2 ) + c ||x 1 || 2 − c ||x 2 || 2 for L-smooth convex-concaveĝ. According to  Liang & Stokes (2019) , SGDA will converge at a rate of roughlyL 2 c 2 log(1/ ) forL-smooth and c-strongly convex-strongly concave objectives. 3 For c = 0, SGDA will diverge in the worst case. For c = o(L), we get linear convergence, but it will be slow because L+c c is large (this can be thought of as a large condition number). Finally, for c = Ω(L), we get fast linear convergence, since L+c c = O(1). Thus, to get fast linear convergence it suffices to make the problem "sufficiently strongly convex-strongly concave" (or "sufficiently strongly monotone"). Theorem 3.4 and condition (3) show that there exists another class of settings where we can achieve linear rates in the min-max setting. In our case, if we have an objective g(x 1 , x 2 ) =ĝ(x 1 , x 2 )+cx 1 x 2 for a smooth functionĝ, we will get linear convergence if ∇ 2 x1x2ĝ ≤ δL and c ≥ 3(1 + δ)L, which ensures that the problem is "sufficiently bilinear." Intuitively, it makes sense that the "sufficiently bilinear" setting allows a linear rate because the pure bilinear setting allows a linear rate. Another way to understand condition (3) is that it is a sufficient condition for the existence of a unique critical point in a general class of settings, as we show in the following lemma, which we prove in Appendix F. Lemma 3.5. Let g(x 1 , x 2 ) = f (x 1 ) + cx 1 x 2 − h(x 2 ) where f and h are L-smooth. Moreover, assume that ∇ 2 f (x 1 ) and ∇ 2 h(x 2 ) each have a 0 eigenvalue for some x 1 and x 2 . If (3) holds, then g has a unique critical point.

Section Title: PROOF SKETCHES FOR HGD CONVERGENCE RATE RESULTS
  PROOF SKETCHES FOR HGD CONVERGENCE RATE RESULTS In this section, we go over the key components of the proofs for our convergence rates from Section 3.1. Recall that the intuition behind HGD was that critical points (where ξ(x) = 0) are global minima of H = 1 2 ||ξ|| 2 . On the other hand, there is no guarantee that H is a convex potential function, and a priori, one would not assume gradient descent on this potential would find a critical point. Nonetheless, we are able to show that in a variety of settings, H satisfies the PL condition, which allows HGD to have linear convergence. Proving this requires proving properties about the singular values of J ≡ ∇ξ.

Section Title: THE POLYAK-ŁOJASIEWICZ CONDITION FOR THE HAMILTONIAN
  THE POLYAK-ŁOJASIEWICZ CONDITION FOR THE HAMILTONIAN We begin by recalling the definition of the PL condition. Definition 4.1 (Polyak-Łojasiewicz (PL) condition  Polyak (1963) ;  Lojasiewicz (1963) ). A function The PL condition is well-known to be the weakest condition necessary to obtain linear convergence rate for gradient methods; see for example  Karimi et al. (2016) . We will show that H satisfies the PL condition, which allows us to use the following classic theorem. For completeness, we provide the proof of Theorem 4.2 in Appendix C. All of our results use Assumption 3.1, so we are guaranteed that g has a critical point. This implies that the global minimum of H is 0, which allows us to prove the following key lemma: Lemma 4.3. Assume we have a twice differentiable g(x 1 , x 2 ) with associated ξ, H, J. Let c > 0. If JJ αI for every x, then H satisfies the PL condition with parameter α. Proof. Consider the squared norm of the gradient of the Hamiltonian: The proof is finished by noting that H(x) = 0 when x is a critical point. To use Theorem 4.2, we will also need to show that H is smooth, which holds when g is (L 1 , L 2 , L 3 )- Lipschitz. The proof of Lemma 4.4 is in Appendix H. Lemma 4.4. Consider any g(x 1 , x 2 ) which is (L 1 , L 2 , L 3 )-Lipschitz for constants L 1 , L 2 , L 3 > 0. Then the Hamiltonian H(x) is (L 1 L 3 + L 2 2 )-smooth. To use Lemma 4.3, we will need control over the eigenvalues of JJ , which we achieve with the following linear algebraic lemmas. We provide their proofs in Appendix H. We now proceed to sketch the proofs of our main theorems using the techniques we have described. The following lemma shows it suffices to prove the PL condition for H for the various settings of our theorems: Lemma 4.7. Given g : R d × R d → R, suppose H satisfies the PL condition with parameter α and is L H -smooth. Then if we use HGD starting from some x (0) ∈ R d × R d with step-size η = 1/L H , then we have the following: Proof. Since H satisfies the PL condition with parameter α and H is L H -smooth, we know by Theorem 4.2 that gradient descent on H with step-size 1/L H converges at a rate of H(x (k) ) ≤ (1 − α L H ) k H(x (0) ). Substituting in for H gives the lemma. It remains to show that H satisfies the PL condition in the settings of Theorems 3.2 to 3.4. First, we show the result for the strongly convex-strongly concave setting of Theorem 3.2. Lemma 4.8 (PL for the strongly convex-strongly concave setting). Let g be c-strongly convex in x 1 and c-strongly concave in x 2 . Then H satisfies the PL condition with parameter α = c 2 .

Section Title: Proof
  Proof We apply Lemma 4.5 with H = J. Since g is c-strongly-convex in x 1 and c-strongly concave in x 2 we have M 1 = ∇ 2 x1x1 g cI and M 2 = −∇ 2 x2x2 g cI. Then the magnitude of the eigenvalues of J is at least c. Thus, JJ c 2 I, so by Lemma 4.3, H satisfies the PL condition with parameter c 2 . Next, we show that H satisfies the PL condition for the nonconvex-linear setting of Theorem 3.3. We prove this lemma in Appendix H.4 by using Lemma 4.6. Under review as a conference paper at ICLR 2020 Lemma 4.9 (PL for the smooth nonconvex-linear setting). Let g be L-smooth in x 1 and lin- ear in x 2 . Moreover, for all x ∈ R d × R d , let ∇ 2 x1x2 g(x 1 , x 2 ) be full rank and square with σ min (∇ 2 x1x2 g(x 1 , x 2 )) ≥ γ. Then H satisfies the PL condition with parameter α = γ 4 2γ 2 +L 2 . Finally, we prove that H satisfies the PL condition in the nonconvex-nonconvex setting of Theorem 3.4. The proof for Lemma 4.10 is in Appendix H.5, and it uses Lemma H.2, which is similar to Lemma 4.6. Lemma 4.10 (PL for the smooth nonconvex-nonconvex setting). Let g be L-smooth in x 1 and L-smooth in x 2 . Also, let ∇ 2 x1x2 g be full rank and let all of its singular values be lower bounded by γ and upper bounded by Γ for all x ∈ R d × R d . Let ρ 2 = min x1,x2 λ min ((∇ 2 x1x1 g(x 1 , x 2 )) 2 ) and µ 2 = min x1,x2 λ min ((∇ 2 x2x2 g(x 1 , x 2 )) 2 ). Assume the following condition holds: Then H satisfies the PL condition with parameter Combining Lemmas 4.8 to 4.10 with Lemma 4.7 yields Theorems 3.2 to 3.4.

Section Title: EXTENSIONS OF HGD RESULTS
  EXTENSIONS OF HGD RESULTS

Section Title: Stochastic HGD
  Stochastic HGD Our results above also imply rates for stochastic HGD, where the gradient ∇H in (2), is replaced by a stochastic estimator v of ∇H such that E[v] = ∇H. Since we show that H satisfies the PL condition with parameter α in different settings, we can use Theorem 4 in  Karimi et al. (2016)  to show that stochastic HGD converges at a O(1/ √ k) rate in the settings of Theorems 3.2 to 3.4, including the "sufficiently bilinear" setting. We prove Theorem 5.1 in Appendix I. Theorem 5.1. Let Assumption 3.1 hold and suppose H satisfies the PL condition with parameter α. Suppose we use the update x (k+1) = x (k) − η k v(x (k) ), where v is a stochastic estimate of ∇H such that E[v] = ∇H and E[ v(x (k) ) 2 ] ≤ C 2 for all x (k) . Then if we use η k = 2k+1 2α(k+1) 2 , we have the following convergence rate:

Section Title: Consensus Optimization
  Consensus Optimization The Consensus Optimization (CO) algorithm of  Mescheder et al. (2017)  is as follows: x (k+1) = x (k) − η(ξ(x (k) ) + γ∇H(x (k) )) (5) where γ > 0. This is essentially a weighted combination of SGDA and HGD.  Mescheder et al. (2017)  remark that while HGD has poor performance on nonconvex problems in practice, CO can effectively train GANs in a variety of settings, including on CIFAR-10 and celebA. While they frame CO as SGDA with a small modification, they actually set γ = 10 for several of their experiments, which suggests that one can also view CO as a modified form of HGD. Using this perspective, we prove Theorem 5.2, which implies that we get linear convergence of CO in the same settings as Theorems 3.2 to 3.4 provided that γ is sufficiently large (i.e. the HGD update is large compared to the SGDA update). The key technical component is showing that HGD still performs well even with a certain kind of small arbitrary perturbation. Previously,  Liang & Stokes (2019)  proved that CO achieves linear convergence in the bilinear setting, so our result greatly expands the settings where CO has provable non-asymptotic convergence. We prove Theorem 5.2 in Appendix J. Theorem 5.2. Let Assumption 3.1 hold. Let g be L g smooth and suppose H satisfies the PL condition with parameter α. Then if we update some x (0) ∈ R d × R d using the CO update (5) with step-size η = α 4L H Lg and γ = 4Lg α , we get the following convergence: We also show that CO converges in practice on some simple examples in Appendix K. Under review as a conference paper at ICLR 2020

```
