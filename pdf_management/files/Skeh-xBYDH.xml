<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 ON SYMMETRY AND INITIALIZATION FOR NEURAL NETWORKS</article-title></title-group><abstract><p>This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically ver- ify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Building a theory that can help to understand neural networks and guide their construction is one of the current challenges of machine learning. Here we wish to shed some light on the role sym- metry plays in the construction of neural networks. It is well-known that symmetry can be used to enhance the performance of neural networks. For example, convolutional neural networks (CNNs) (see <xref ref-type="bibr" rid="b22">Lecun et al. (1998)</xref>) use the translational symmetry of images to classify images better than fully connected neural networks. Our focus is on the role of symmetry in the initialization stage. We show that symmetry-based initialization can be the difference between failure and success. On a high-level, the study of neural networks can be partitioned to three different aspects. We study these aspects for the first "non trivial" case of neural networks, networks with one hidden layer. We are mostly interested in the initialization phase. If we take a network with the appropriate architecture, we can always initialize it to the desired function. A standard method (that induces a non trivial learning problem) is using random weights to initialize the network. A different reason- able choice is to require the initialization to be useful for an entire class of functions. We follow the latter option.</p><p>Our focus is on the role of symmetry. We consider the following class of symmetric functions S = S n = n &#8721; i=0 a i &#183; 1 |x|=i : a 1 , . . . , a n &#8712; {&#177;1} , where x &#8712; {0, 1} n and |x| = &#8721; i x i . The functions in this class are invariant under arbitrary permu- tations of the input's coordinates. The parity function &#960;(x) = (&#8722;1) |x| and the majority function are well-known examples of symmetric functions.</p><p>Expressiveness for this class was explored by <xref ref-type="bibr" rid="b27">Minsky and Papert (1988)</xref>. They showed that the parity function cannot be represented using a network with limited "connectivity". Contrastingly, if we use a fully connected network with one hidden layer and a common activation function (like sign, sigmoid, or ReLU) only O(n) neurons are needed. We provide such explicit representations for all functions in S; see Lemmas 1 and 2.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>We also provide useful information on both the training phase and generalization capabilities of the neural network. We show that, with proper initialization, the training process (using standard SGD) efficiently converges to zero empirical error, and that consequently the network has small true error as well.</p><p>Theorem 1. There exists a constant c &gt; 1 so that the following holds. There exists a network with one hidden layer, cn neurons with sigmoid or ReLU activations, and an initialization such that for all distributions D over X = {0, 1} n and all functions f &#8712; S with sample size m &#8805; c(n+log(1/&#948; ))/&#949;, after performing poly(n) SGD updates with a fixed step size h = 1/poly(n) it holds that P x m &#8764;D m S : Pr x&#8764;D (N S (x) = f (x)) &gt; &#949; &lt; &#948; where S = {(x 1 , f (x 1 )), ..., (x m , f (x m ))} and N S (x) is the network after training over S.</p><p>The number of parameters in the network described in Theorem 1 is &#8486;(n 2 ). So in general one could expect overfitting when the sample size is as small as O(n). Nevertheless, the theorem provides generalization guarantees, even for such a small sample size.</p><p>The initialization phase plays an important role in proving Theorem 1. To emphasize this, we report an empirical phenomenon (this is "folklore"). We show that a network cannot learn parity from a random initialization (see Section 5.3). On one hand, if the network size is big, we can bring the empirical error to zero (as suggested in <xref ref-type="bibr" rid="b38">Soudry and Carmon (2016)</xref>), but the true error is close to 1/2. On the other hand, if its size is too small, the network is not even able to achieve small empirical error (see Figure 5). We observe a similar phenomenon also for a random symmetric function. An open question remains: why is it true that a sample of size polynomial in n does not suffice to learn parity (with random initialization)?</p><p>A similar phenomenon was theoretically explained by <xref ref-type="bibr" rid="b14">Shamir (2016)</xref> and <xref ref-type="bibr" rid="b2">Song et al. (2017)</xref>. The parity function belongs to the class of all parities P = P n = {&#960; s (x) = (&#8722;1) s&#183;x : s &#8712; X} where &#183; is the standard inner product. This class is efficiently PAC-learnable with O(n) samples using Gaussian elimination. A continuous version of P was studied by <xref ref-type="bibr" rid="b14">Shamir (2016)</xref> and <xref ref-type="bibr" rid="b2">Song et al. (2017)</xref>. To study the training phase, they used a generalized notion of statistical queries (SQ); see <xref ref-type="bibr" rid="b20">Kearns (1998)</xref>. In this framework, they show that most functions in the class P cannot be efficiently learned (roughly stated, learning the class requires an exponential amount of resources). This framework, however, does not seem to capture actual training of neural networks using SGD. For example, it is not clear if one SGD update corresponds to a single query in this model. In addition, typically one receives a dataset and performs the training by going over it many times, whereas the query model estimates the gradient using a fresh batch of samples in each iteration. The query model also assumes the noise to be adversarial, an assumption that does not necessarily hold in reality. Finally, the SQ-based lower bound holds for every initialization (in particular, for the initialization we use here), so it does not capture the efficient training process Theorem 1 describes. Theorem 1 shows, however, that with symmetry-based initialization, parity can be efficiently learned. So, in a nutshell, parity can not be learned as part of P, but it can be learned as part of S. One could wonder why the hardness proof for P cannot be applied for S as both classes con- sist of many input sensitive functions. The answer lies in the fact that P has a far bigger statistical dimension than S (all functions in P are orthogonal to each other, unlike S).</p><p>The proof of the theorem utilizes the different behavior of the two layers in the network. SGD is performed using a step size h that is polynomially small in n. The analysis shows that in a polynomial number of steps that is independent of the choice of h the following two properties hold: (i) the output neuron reaches a "good" state and (ii) the hidden layer does not change in a "meaningful" way. These two properties hold when h is small enough. In Section 5.2, we experiment with large values of h. We see that, although the training error is zero, the true error becomes large.</p><p>Here is a high level description of the proof. The neurons in the hidden layer define an "embed- ding" of the inputs space X = {0, 1} n into R (a.k.a. the feature map). This embedding changes in time according to the training examples and process. The proof shows that if at any point in time this embedding has good enough margin, then training with standard SGD quickly converges. This is explained in more detail in Section 3. It remains an interesting open problem to understand this phenomenon in greater generality, using a cleaner and more abstract language.</p></sec><sec><title>BACKGROUND</title><p>To better understand the context of our research, we survey previous related works. The expressiveness and limitations of neural networks were studied in several works such as <xref ref-type="bibr" rid="b30">Rahimi and Recht (2008)</xref>; <xref ref-type="bibr" rid="b7">Telgarsky (2016)</xref>; <xref ref-type="bibr" rid="b14">Eldan and Shamir (2016)</xref> and <xref ref-type="bibr" rid="b4">Arora et al. (2016)</xref>. Constructions of small ReLU networks for the parity function appeared in several previous works, such as <xref ref-type="bibr" rid="b0">Wilam- owski et al. (2003)</xref>, <xref ref-type="bibr" rid="b6">Arslanov et al. (2016)</xref>, <xref ref-type="bibr" rid="b6">Arslanov et al. (2002)</xref> and <xref ref-type="bibr" rid="b26">Masato Iyoda et al. (2003)</xref>. Constant depth circuits for the parity function were also studied in the context of computational complexity theory, see for example <xref ref-type="bibr" rid="b16">Furst et al. (1981)</xref>, <xref ref-type="bibr" rid="b1">Ajtai (1983)</xref> and <xref ref-type="bibr" rid="b18">H&#229;stad (1987)</xref>.</p><p>The training phase of neural networks was also studied in many works. Here we list several works that seem most related to ours. <xref ref-type="bibr" rid="b12">Daniely (2017)</xref> analyzed SGD for general neural network architecture and showed that the training error can be nullified, e.g., for the class of bounded degree polynomials (see also <xref ref-type="bibr" rid="b3">Andoni et al. (2014)</xref>). <xref ref-type="bibr" rid="b19">Jacot et al. (2018)</xref> studied neural tangent kernels (NTK), an infinite width analogue of neural networks. <xref ref-type="bibr" rid="b5">Du et al. (2018)</xref> showed that randomly initialized shallow ReLU networks nullify the training error, as long as the number of samples is smaller than the number of neurons in the hidden layer. Their analysis only deals with optimization over the first layer (so that the weights of the output neuron are fixed). <xref ref-type="bibr" rid="b9">Chizat and Bach (2018)</xref> provided another analysis of the latter two works. <xref ref-type="bibr" rid="b2">Allen-Zhu et al. (2018b)</xref> showed that over-parametrized neural networks can achieve zero training error, as as long as the data points are not too close to one another and the weights of the output neuron are fixed. <xref ref-type="bibr" rid="b44">Zou et al. (2018)</xref> provided guarantees for zero training error, assuming the two classes are separated by a positive margin.</p><p>Convergence and generalization guarantees for neural networks were studied in the following works. <xref ref-type="bibr" rid="b8">Brutzkus et al. (2017)</xref> studied linearly separable data. <xref ref-type="bibr" rid="b2">Li and Liang (2018)</xref> studied well separated distributions. <xref ref-type="bibr" rid="b2">Allen-Zhu et al. (2018a)</xref> gave generalization guarantees in expectation for SGD. <xref ref-type="bibr" rid="b4">Arora et al. (2019)</xref> gave data-dependent generalization bounds for GD. All these works optimized only over the hidden layer (the output layer is fixed after initialization).</p><p>Margins play an important role in learning, and we also use it in our proof. <xref ref-type="bibr" rid="b35">Sokolic et al. (2016)</xref>, <xref ref-type="bibr" rid="b35">Sokolic et al. (2017)</xref>, <xref ref-type="bibr" rid="b7">Bartlett et al. (2017)</xref> and <xref ref-type="bibr" rid="b39">Sun et al. (2015)</xref> gave generalization bounds for neural networks that are based on their margin when the training ends. From a practical perspective, <xref ref-type="bibr" rid="b15">Elsayed et al. (2018)</xref>, <xref ref-type="bibr" rid="b31">Romero and Alquezar (2002)</xref> and <xref ref-type="bibr" rid="b25">Liu et al. (2016)</xref> suggested different training algorithms that optimize the margin.</p><p>As discussed above, it seems difficult for neural networks to learn parities. <xref ref-type="bibr" rid="b2">Song et al. (2017)</xref> and <xref ref-type="bibr" rid="b14">Shamir (2016)</xref> demonstrated this using the language statistical queries (SQ). This is a valuable language, but it misses some central aspects of training neural networks. SQ seems to be closely related to GD, but does not seem to capture SGD. SQ also shows that many of the parities functions &#8855; i&#8712;S x i are difficult to learn, but it does not imply that the parity function &#8855; i&#8712;[n] x i is difficult to learn. <xref ref-type="bibr" rid="b0">Abbe and Sandon (2018)</xref> demonstrated a similar phenomenon in a setting that is closer to the "real life" mechanics of neural networks.</p><p>We suggest that taking the symmetries of the learning problem into account can make the difference between failure and success. Several works suggested different neural architectures that take sym- metries into account; see <xref ref-type="bibr" rid="b43">Zaheer et al. (2017)</xref>, <xref ref-type="bibr" rid="b17">Gens and Domingos (2014)</xref>, and <xref ref-type="bibr" rid="b10">Cohen and Welling (2016)</xref>. Here we describe efficient representations for symmetric functions by networks with one hidden layer. These representations are also useful later on, when we study the training process. We study two different activation functions, sigmoid and ReLU (similar statement can be proved for other activations, like arctan). Each activation function requires its own representation, as in the two lemmas below.</p></sec><sec><title>SIGMOID</title><p>We start with the activation &#963; (&#958; ) = 1 1+exp(&#8722;&#958; ) , since it helps to understand the construction for the ReLU activation. The building blocks of the symmetric functions are indicators of |x| = i for Under review as a conference paper at ICLR 2020 A network with one hidden layer of n + 2 neurons with sigmoid activations and one bias neuron is sufficient to represent any function in S. The coefficients of the sigmoid gates are 0, &#177;1 in this representation. The proofs of this lemma and the subsequent lemmas appear in the appendix.</p></sec><sec><title>RELU</title><p>A sigmoid function can be represented using ReLU(&#958; ) = max{0, &#958; } as the difference between two</p><p>Hence, an indicator function can be represented using sign(1 |x|=i &#8722; 0.5) = sign(&#915; i &#8722; 0.5) where</p><p>The lemma shows that a network with one hidden layer of n + 3 ReLU neurons and one bias neuron is sufficient to represent any function in S. The coefficients of the ReLU gates are 0, &#177;1, &#177;2 in this representation.</p></sec><sec><title>TRAINING AND GENERALIZATION</title><p>The goal of this section is to describe a small network with one hidden layer that (when initialized properly) efficiently learns symmetric functions using a small number of examples (the training is done via SGD).</p></sec><sec><title>SPECIFICATIONS</title><p>Here we specify the architecture, initialization and loss function that is implicit in our main result (Theorem 1).</p><p>To guarantee convergence of SGD, we need to start with "good" initial conditions. The initialization we pick depends on the activation function it uses, and is chosen with resemblance to Lemma 2 for Under review as a conference paper at ICLR 2020 ReLU. On a high level, this indicates that understanding the class of functions we wish to study in term of "representation" can be helpful when choosing the architecture of a neural network in a learning context.</p><p>The network we consider has one hidden layer. We denote by w i j the weight between coordinate j of the input and neuron i in the hidden layer. We denote W this matrix of weights. We denote by b i the bias of neuron i of the hidden layer. We denote B this vector of weights. We denote by m i is the weight from neuron i in the hidden layer to the output neuron. We denote M this vector of weights. We denote by b the bias of the output neuron.</p><p>Initialize the network as follows: The dimensions of W are (n + 3) &#215; n. For all 1 &#8804; i &#8804; (n + 3) and 1 &#8804; j &#8804; n, we set w i j = 1 and b i = &#8722;i + 2. We set M = 0 and b = 0.</p><p>To run SGD, we need to choose a loss function. We use the hinge loss, L(x, f ) = max{0, &#8722; f (x)(v x &#183; M + b) + &#946; }, where v x = ReLU(W x + B) is the output of the hidden layer on input x and &#946; &gt; 0 is a parameter of confidence.</p></sec><sec><title>MARGINS</title><p>A key property in the analysis is the 'margin' of the hidden layer with respect to the function being learned.</p><p>A map Y : V &#8594; {&#177;1} over a finite set V &#8834; R d is linearly 1 separable if there exists w &#8712; R d such that sign(w &#183; v) = Y (v) for all v &#8712; V . When the Euclidean norm of w is w = 1, the num- ber marg(w,Y ) = min v&#8712;V Y (v)w &#183; v is the margin of w with respect to Y . The number marg(Y ) = sup w&#8712;R d : w =1 marg(w,Y ) is the margin of Y .</p><p>We are interested in the following set V in R d . Recall that W is the weight matrix between the input layer and the hidden layer, and that B is the relevant bias vector. Given W, B, we are interested in the set V = {v x : x &#8712; X}, where v x = ReLU(W x + B). In words, we think of the neurons in the hidden layer as defining an "embedding" of X in Euclidean space. A similar construction works for other activation functions. We say that Y : V &#8594; {&#177;1} agrees with f &#8712; S if for all x &#8712; X it holds that Y (v x ) = f (x).</p><p>The following lemma bounds from below the margin of the initial V .</p><p>Lemma 3. If Y is a partition that agrees with some function in S for the initialization described above then marg(Y ) &#8805; &#8486;(1/ &#8730; n).</p><p>Proof. By Lemmas 1 and 2, we see that any function in S can be represented with a vector of weights M, b &#8712; [&#8722;1, 1] &#920;(n) of the output neuron together with a bias . These M, b induce a partition Y of V . Namely, Y (v x )M &#183; v x + b &gt; 0.25 for all x &#8712; X. Since (M, b) = O( &#8730; n) we have our desired result.</p></sec><sec><title>FREEZING THE HIDDEN LAYER</title><p>Before analyzing the full behavior of SGD, we make an observation: if the weights of the hidden layer are fixed with the initialization described above, then Theorem 1 holds for SGD with batch size 1. This observation, unfortunately, does not suffice to prove Theorem 1. In the setting we consider, the training of the neural network uses SGD without fixing any weights. This more general case is handled in the next section. The rest of this subsection is devoted for explaining this observation. <xref ref-type="bibr" rid="b29">Novikoff (1962)</xref> showed that that the perceptron algorithm <xref ref-type="bibr" rid="b32">Rosenblatt (1958)</xref> makes a small num- ber of mistakes for linearly separable data with large margin. For a comprehensive survey of the perceptron algorithm and its variants, see <xref ref-type="bibr" rid="b28">Moran et al. (2018)</xref>.</p><p>Novikoff's proof can be generalized to any &#946; &gt; 0 and batches of any size to yield the following theorem; see <xref ref-type="bibr" rid="b11">Collobert and Bengio (2004)</xref>; <xref ref-type="bibr" rid="b21">Krauth and Mezard (1987)</xref> and appendix A. Theorem 2. For Y : V &#8594; {&#177;1} with margin &#947; &gt; 0 and step size h &gt; 0, the modified perceptron algorithm performs at most 2&#946; h+(Rh) 2 (&#947;h) 2 updates and achieves a margin of at least</p><p>So, when the weights of the hidden layer are fixed, Lemma 3 implies that the number of SGD steps is at most polynomial in n.</p></sec><sec><title>STABILITY</title><p>When we run SGD on the entire network, the layers interact. For a ReLU network at time t, the update rule for W is as follows. If the network classifies the input x correctly with confidence more than &#946; , no change is made. Otherwise, we change the weights in M by &#8710;M = yv x h, where y is the true label and h is the step size. If also neuron i of the hidden fired on x, we update its incoming weights by &#8710;W i,: = ym i xh. These update rules define the following dynamical system: (a)</p><p>where H is the Heaviside step function and &#8226; is the Hadamard pointwise product.</p><p>A key observation in the proof is that the weights of the last layer ((4) and (5)) are updated exactly as the modified perceptron algorithm. Another key statement in the proof is that if the network has reached a good representation of the input (i.e., the hidden layer has a large margin), then the interaction between the layers during the continued training does not impair this representation. This is summarized in the following lemma (we are not aware of a similar statement in the literature).</p><p>Lemma 4. Let M = 0, b = 0, and V = {ReLU(W x + B) : x &#8712; X} be a linearly separable embedding of X and with margin &#947; &gt; 0 by the hidden layer of a neural network of depth two with ReLU activation and weights given by W, B. Let R X = max x&#8712;X x , let R = max v&#8712;V v , and 0 &lt; h &#8804; &#947; 5/2 100R 2 R X be the integration step. Assuming R X &gt; 1 and &#947; &#8804; 1, and using &#946; = R 2 h in the loss function, after t SGD iterations the following hold:</p><p>- Each v &#8712; V moves a distance of at most O(R 2 X h 2 Rt 3/2 ).</p><p>- The norm M (t) is at most O(Rh &#8730; t).</p><p>- The training ends in at most O(R 2 /&#947; 2 ) SGD updates.</p><p>Intuitively, this type of lemma can be useful in many other contexts. The high level idea is to identify a "good geometric structure" that the network reaches and enables efficient learning.</p><p>Run SGD for O(n 4 ) steps, where the step size is h = O(1/n 6 ) and the parameter of the loss function is &#946; = R 2 h with R = n 3/2 .</p><p>We claim that it suffices to show that at the end of the training (i) the network correctly classifies all the sample points x 1 , . . . , x m , and (ii) for every x &#8712; X such that there exists 1 &#8804; i &#8804; m with |x| = |x i |, the network outputs y i on x as well. Here is why. The initialization of the network embeds the space X into n + 4 dimensional space (including the bias neuron of the hidden layer). Let V (0) be the initial embedding V (0) = {ReLU(W (0) x + B (0) ) : x &#8712; X}. Although |X| = 2 n , the size of V (0) is n + 1. The VC dimension of all the boolean functions over V (0) is n + 1. Now, m samples suffice to yield &#949; true error for an ERM when the VC dimension is n + 1; see e.g. Theorem 6.7 in <xref ref-type="bibr" rid="b8">Shalev-Shwartz and Ben-David (2014)</xref>. It remains to prove (i) and (ii) above.</p><p>By Lemma 3, at the beginning of the training, the partition of V (0) defined by the target f &#8712; S has a margin of &#947; = &#8486;(1/ &#8730; n). We are interested in the eventual V * = {ReLU(W * x + B * ) : x &#8712; X} embedding of X as well. The modified perceptron algorithm together with Lemma 4 guarantees that after K &#8804; 20R 2 /&#947; 2 = O(n 4 ) updates, (M * , b * ) separates the embedded sample V * S = {ReLU(W * x i + B * ) : 1 &#8804; i &#8804; m} with a margin of at least 0.9&#947;/3.</p><p>It remains to prove (ii). Lemma 4 states that as long as less than K = O(n 4 ) updates were made, the elements in V moved at most O(1/n 2 ). At the end of the training, the embedded sample V S is separated with a margin of at least 0.9&#947;/3 with respect to the hyperplane defined by M * and B * . Each v * x for x &#8712; X moved at most O(1/n 2 ) &lt; &#947;/4. This means that if |x| = |x i | then the network has the same output on x and x i . Since the network has zero empirical error, the output on this x is y i as well.</p><p>A similar proof is available with sigmoid activation (with better convergence rate and larger allowed step size).</p><p>Remark. The generalization part of the above proof can be viewed as a consequence of sample compression (<xref ref-type="bibr" rid="b24">Littlestone and Warmuth (1986)</xref>). Although the eventual network depends on all ex- amples, the proof shows that its functionality depends on at most n + 1 examples. Indeed, after the training, all examples with equal hamming weight have the same label.</p></sec><sec><title>Remark</title><p>The parameter &#946; = R 2 h we chose in the proof may seem odd and negligible. It is a construct in the proof that allows us to bound efficiently the distance that the elements in V have moved during the training. For all practical purposes &#946; = 0 works as well (see Figure 4).</p></sec><sec><title>EXPERIMENTS</title><p>We accompany the theoretical results with some experiments. We used a network with one hidden layer of 4n + 3 neurons, ReLU activation, and the hinge loss with &#946; = n 3 h. In all the experiments, we used SGD with mini-batch of size one and before each epoch we randomized the sample. We ob- served similar behavior for larger mini-batches, other activation functions, and other loss functions. The graphs that appear in the appendix A present the training error and the true error 2 versus the epoch of the training process. In all the comparisons below, we chose a random symmetric function and a random sample from X.</p></sec><sec><title>THE THEORY IN PRACTICE</title><p>Figure 2 demonstrates our theoretical results and also validates the performance of our initialization. In one setting, we trained only the second layer (freezed the weights of the hidden layer) which 2 We deal with high dimensional spaces, so the true error was not calculated exactly but approximated on an independent batch of samples of size 10 4 .</p><p>Under review as a conference paper at ICLR 2020 essentially corresponds to the perceptron algorithm. In the second setting, we trained both layers with a step size h = n &#8722;6 (as the theory suggests). As expected, performance in both cases is similar. We remark that SGD continues to run even after minimizing the empirical error. This happens because of the parameter &#946; &gt; 0.</p></sec><sec><title>OVERSTEPPING THE THEORY</title><p>Here we experiment with two parameters in the proof, the step size h and the confidence parameter &#946; . In Figure 3, we used three different step sizes, two of which much larger than the theory suggests. We see that the training error converges much faster to zero, when the step size is larger. This fast convergence comes at the expense of the true error. For a large step size, generalization cease to hold.</p><p>Setting &#946; = n 3 h is a construct in the proof. Figure 4 shows that setting &#946; = 0 does not impair the performance. The difference between theory (requires &#946; &gt; 0) and practice (allows &#946; = 0) can be explained as follows. The proof bounds the worst-case movement of the hidden layer, whereas in practice an average-case argument suffices.</p></sec><sec><title>HARD TO LEARN PARITY</title><p>Figure 5 shows that even for n = 20, learning parity is hard from a random initialization. When the sample size is small the training error can be nullified but the true error is large. As the sample grows, it becomes much harder for the network to nullify even the training error. With our initializa- tion, both the training error and true error are minimized quickly. Figure 6 demonstrates the same phenomenon for a random symmetric function.</p></sec><sec><title>CORRUPTION OF DATA</title><p>Our initialization also delivers satisfying results when the input data it corrupted. In figure 7, we randomly perturb (with probability p = 1 10 ) the labels and use the same SGD to train the model. In figure 8, we randomly shift every entry of the vectors in the space X by &#949; that is uniformly distributed in [&#8722;0.1, 0.1] n .</p></sec><sec><title>CONCLUSION</title><p>This work demonstrates that symmetries can play a critical role when designing a neural network. We proved that any symmetric function can be learned by a shallow neural network, with proper initialization. We demonstrated by simulations that this neural network is stable under corruption of data, and that the small step size is the proof is necessary.</p><p>We also demonstrated that the parity function or a random symmetric function cannot be learned with random initialization. How to explain this empirical phenomenon is still an open question. The works <xref ref-type="bibr" rid="b14">Shamir (2016)</xref> and <xref ref-type="bibr" rid="b2">Song et al. (2017)</xref> treated parities using the language of SQ. This language obscures the inner mechanism of the network training, so a more concrete explanation is currently missing.</p><p>We proved in a special case that the standard SGD training of a network efficiently produces low true error. The general problem that remains is proving similar results for general neural networks. A suggestion for future works is to try to identify favorable geometric states of the network that guarantee fast convergence and generalization.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Approximations of the symmetric function f A = sign(&#8721; i&#8712;A 1 |x|=i &#8722; 0.5) by sigmoid and ReLU activations for A = {1, 5, 7, 15, 20, 21, 22, 25}.</p></caption><graphic /></fig></sec></body><back><sec><p>1 A standard "lifting" that adds a coordinate with 1 to every vector allows to translate the affine case to the linear case.</p></sec><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><source>Provable limitations of deep learning</source><year>2018</year><person-group person-group-type="author"><name><surname>References</surname><given-names>Emmanuel</given-names></name><name><surname>Abbe</surname><given-names>Colin</given-names></name><name><surname>Sandon</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>&#8721;11-formulae on finite structures</article-title><source>Annals of Pure and Applied Logic</source><year>1983</year><volume>24</volume><issue>1</issue><fpage>1</fpage><lpage>48</lpage><person-group person-group-type="author"><name><surname>Ajtai</surname><given-names>M</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Learning and generalization in overparameterized neural networks, going beyond two layers. CoRR, abs/1811.04918, 2018a. Under review as a conference paper at ICLR 2020 Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over- parameterization</article-title><source>CoRR</source><year>2018</year><person-group person-group-type="author"><name><surname>Allen-Zhu</surname><given-names>Zeyuan</given-names></name><name><surname>Li</surname><given-names>Yuanzhi</given-names></name><name><surname>Liang</surname><given-names>Yingyu</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><year>2014</year><fpage>1908</fpage><lpage>1916</lpage><person-group person-group-type="author"><name><surname>Andoni</surname><given-names>Alexandr</given-names></name><name><surname>Panigrahy</surname><given-names>Rina</given-names></name><name><surname>Valiant</surname><given-names>Gregory</given-names></name><name><surname>Zhang</surname><given-names>Li</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Understanding deep neural networks with rectified linear units</article-title><source>CoRR</source><year>2016</year><person-group person-group-type="author"><name><surname>Arora</surname><given-names>Raman</given-names></name><name><surname>Basu</surname><given-names>Amitabh</given-names></name><name><surname>Mianjy</surname><given-names>Poorya</given-names></name><name><surname>Mukherjee</surname><given-names>Anirbit</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</article-title><source>CoRR</source><year>2019</year><person-group person-group-type="author"><name><surname>Arora</surname><given-names>Sanjeev</given-names></name><name><surname>Du</surname><given-names>Simon S</given-names></name><name><surname>Hu</surname><given-names>Wei</given-names></name><name><surname>Li</surname><given-names>Zhiyuan</given-names></name><name><surname>Wang</surname><given-names>Ruosong</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>N-bit parity neural networks with mini- mum number of threshold neurons</article-title><source>Open Engineering</source><year>2016</year><volume>6</volume><person-group person-group-type="author"><name><surname>Arslanov</surname><given-names>Marat</given-names></name><name><surname>Amirgalieva</surname><given-names>Zhazira E</given-names></name><name><surname>Chingiz</surname><given-names>A</given-names></name><name><surname>Kenshimov</surname><given-names /></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><source>Spectrally-normalized margin bounds for neural networks</source><year>2017</year><person-group person-group-type="author"><name><surname>Bartlett</surname><given-names>Peter</given-names></name><name><surname>Foster</surname><given-names>Dylan J</given-names></name><name><surname>Telgarsky</surname><given-names>Matus</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>SGD learns over-parameterized networks that provably generalize on linearly separable data</article-title><source>ICLR</source><year>2018</year><person-group person-group-type="author"><name><surname>Brutzkus</surname><given-names>Alon</given-names></name><name><surname>Globerson</surname><given-names>Amir</given-names></name><name><surname>Malach</surname><given-names>Eran</given-names></name><name><surname>Shalev-Shwartz</surname><given-names>Shai</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><source>A note on lazy training in supervised differentiable programming</source><year>2018</year><volume>12</volume><person-group person-group-type="author"><name><surname>Chizat</surname><given-names>Lenaic</given-names></name><name><surname>Bach</surname><given-names>Francis</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Cohen and Max Welling</article-title><source>Group equivariant convolutional networks</source><year>2016</year><person-group person-group-type="author"><name><surname>Taco</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Links between perceptrons, mlps and svms</article-title><year>2004</year><person-group person-group-type="author"><name><surname>Collobert</surname><given-names>Ronan</given-names></name><name><surname>Bengio</surname><given-names>Samy</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Sgd learns the conjugate kernel class of the network</article-title><source>Advances in Neural Information Processing Systems</source><year>2017</year><volume>30</volume><fpage>2422</fpage><lpage>2430</lpage><person-group person-group-type="author"><name><surname>Daniely</surname><given-names>Amit</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Gradient descent provably optimizes over- parameterized neural networks</article-title><source>CoRR</source><year>2018</year><person-group person-group-type="author"><name><surname>Du</surname><given-names>Simon S</given-names></name><name><surname>Zhai</surname><given-names>Xiyu</given-names></name><name><surname>P&#243;czos</surname><given-names>Barnab&#225;s</given-names></name><name><surname>Singh</surname><given-names>Aarti</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>The Power of Depth for Feedforward Neural Networks</article-title><source>JMLR</source><year>2016</year><volume>49</volume><fpage>1</fpage><lpage>34</lpage><person-group person-group-type="author"><name><surname>Eldan</surname><given-names>Ronen</given-names></name><name><surname>Shamir</surname><given-names>Ohad</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Large margin deep networks for classification</article-title><source>NIPS</source><year>2018</year><fpage>850</fpage><lpage>860</lpage><person-group person-group-type="author"><name><surname>Gamaleldin</surname><given-names>F</given-names></name><name><surname>Elsayed</surname><given-names>Dilip</given-names></name><name><surname>Krishnan</surname><given-names>Hossein</given-names></name><name><surname>Mobahi</surname><given-names>Kevin</given-names></name><name><surname>Regan</surname><given-names>Samy</given-names></name><name><surname>Bengio</surname><given-names /></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>Parity, circuits, and the polynomial-time hierarchy</article-title><source>FOCS</source><year>1981</year><fpage>260</fpage><lpage>270</lpage><person-group person-group-type="author"><name><surname>Furst</surname><given-names>Merrick</given-names></name><name><surname>Saxe</surname><given-names>James B</given-names></name><name><surname>Sipser</surname><given-names>Michael</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Deep symmetry networks</article-title><source>Advances in Neural Information Processing Systems</source><year>2014</year><volume>27</volume><fpage>2537</fpage><lpage>2545</lpage><person-group person-group-type="author"><name><surname>Gens</surname><given-names>Robert</given-names></name><name><surname>Domingos</surname><given-names>Pedro M</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><source>Computational Limitations of Small-depth Circuits</source><year>1987</year><person-group person-group-type="author"><name><surname>H&#229;stad</surname><given-names>Johan</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Neural tangent kernel: Convergence and generalization in neural networks</article-title><source>NIPS</source><year>2018</year><fpage>8580</fpage><lpage>8589</lpage><person-group person-group-type="author"><name><surname>Jacot</surname><given-names>Arthur</given-names></name><name><surname>Gabriel</surname><given-names>Franck</given-names></name><name><surname>Hongler</surname><given-names>Cl&#233;ment</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Efficient noise-tolerant learning from statistical queries</article-title><source>J. ACM</source><year>1998</year><volume>45</volume><issue>6</issue><fpage>983</fpage><lpage>1006</lpage><person-group person-group-type="author"><name><surname>Kearns</surname><given-names>Michael</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>Learning algorithms with optimal stability in neural networks</article-title><source>J. Phys.</source><year>1987</year><volume>A20</volume><fpage>L745</fpage><lpage>L752</lpage><person-group person-group-type="author"><name><surname>Krauth</surname><given-names>Werner</given-names></name><name><surname>Mezard</surname><given-names>Marc</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Gradient-based learning applied to document recognition</article-title><source>Proceedings of the IEEE</source><year>1998</year><volume>86</volume><issue>11</issue><fpage>2278</fpage><lpage>2324</lpage><person-group person-group-type="author"><name><surname>Lecun</surname><given-names>Y</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Haffner</surname><given-names>P</given-names></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>Learning overparameterized neural networks via stochastic gradient descent on structured data</article-title><year>2018</year><person-group person-group-type="author"><name><surname>Li</surname><given-names>Yuanzhi</given-names></name><name><surname>Liang</surname><given-names>Yingyu</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>Relating data compression and learnability</article-title><source>Technical report</source><year>1986</year><person-group person-group-type="author"><name><surname>Littlestone</surname><given-names>Nick</given-names></name><name><surname>Warmuth</surname><given-names>Manfred K</given-names></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><article-title>Large-margin softmax loss for convolutional neural networks</article-title><source>ICML</source><year>2016</year><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Weiyang</given-names></name><name><surname>Wen</surname><given-names>Yandong</given-names></name><name><surname>Yu</surname><given-names>Zhiding</given-names></name><name><surname>Yang</surname><given-names>Meng Meng</given-names></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>A solution for the n-bit parity problem using a single translated multiplicative neuron</article-title><source>Neural Processing Letters</source><year>2003</year><volume>18</volume><fpage>233</fpage><lpage>238</lpage><person-group person-group-type="author"><name><surname>Masato Iyoda</surname><given-names>Eduardo</given-names></name><name><surname>Nobuhara</surname><given-names>Hajime</given-names></name><name><surname>Hirota</surname><given-names>Kaoru</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><source>Perceptrons: Expanded Edition</source><year>1988</year><person-group person-group-type="author"><name><surname>Marvin</surname><given-names>L</given-names></name><name><surname>Minsky</surname><given-names /></name><name><surname>Seymour</surname><given-names>A</given-names></name><name><surname>Papert</surname><given-names /></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>On the perceptron's compression</article-title><source>CoRR</source><year>2018</year><person-group person-group-type="author"><name><surname>Moran</surname><given-names>Shay</given-names></name><name><surname>Nachum</surname><given-names>Ido</given-names></name><name><surname>Panasoff</surname><given-names>Itai</given-names></name><name><surname>Yehudayoff</surname><given-names>Amir</given-names></name></person-group></element-citation></ref><ref id="b29"><element-citation publication-type="journal"><article-title>On convergence proofs on perceptrons</article-title><source>Proceedings of the Symposium on the Mathe- matical Theory of Automata</source><year>1962</year><volume>12</volume><fpage>615</fpage><lpage>622</lpage><person-group person-group-type="author"><name><surname>Albert</surname><given-names>B J</given-names></name><name><surname>Novikoff</surname><given-names /></name></person-group></element-citation></ref><ref id="b30"><element-citation publication-type="journal"><article-title>Random features for large-scale kernel machines</article-title><source>Advances in Neural Information Processing Systems</source><year>2008</year><volume>20</volume><fpage>1177</fpage><lpage>1184</lpage><person-group person-group-type="author"><name><surname>Rahimi</surname><given-names>Ali</given-names></name><name><surname>Recht</surname><given-names>Benjamin</given-names></name></person-group></element-citation></ref><ref id="b31"><element-citation publication-type="journal"><article-title>Maximizing the margin with feedforward neural networks</article-title><source>Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290)</source><year>2002</year><volume>1</volume><fpage>743</fpage><lpage>748</lpage><person-group person-group-type="author"><name><surname>Romero</surname><given-names>E</given-names></name><name><surname>Alquezar</surname><given-names>R</given-names></name></person-group></element-citation></ref><ref id="b32"><element-citation publication-type="journal"><article-title>The perceptron: A probabilistic model for information storage and organization in the brain</article-title><source>Psychological Review</source><year>1958</year><fpage>65</fpage><lpage>386</lpage><person-group person-group-type="author"><name><surname>Rosenblatt</surname><given-names>F</given-names></name></person-group></element-citation></ref><ref id="b33"><element-citation publication-type="journal"><source>Understanding machine learning: From theory to algorithms</source><year>2014</year><person-group person-group-type="author"><name><surname>Shalev</surname><given-names>Shai</given-names></name><name><surname>Shwartz</surname><given-names>-</given-names></name><name><surname>Ben-David</surname><given-names>Shai</given-names></name></person-group></element-citation></ref><ref id="b34"><element-citation publication-type="journal"><article-title>Distribution-specific hardness of learning neural networks</article-title><source>CoRR</source><year>2016</year><person-group person-group-type="author"><name><surname>Shamir</surname><given-names>Ohad</given-names></name></person-group></element-citation></ref><ref id="b35"><element-citation publication-type="journal"><article-title>Margin preservation of deep neural networks</article-title><source>CoRR</source><year>2016</year><person-group person-group-type="author"><name><surname>Sokolic</surname><given-names>Jure</given-names></name><name><surname>Giryes</surname><given-names>Raja</given-names></name><name><surname>Sapiro</surname><given-names>Guillermo</given-names></name><name><surname>Rodrigues</surname><given-names>Miguel R D</given-names></name></person-group></element-citation></ref><ref id="b36"><element-citation publication-type="journal"><article-title>Robust large margin deep neural networks</article-title><source>IEEE Transactions on Signal Processing</source><year>2017</year><volume>65</volume><fpage>4265</fpage><lpage>4280</lpage><person-group person-group-type="author"><name><surname>Sokolic</surname><given-names>Jure</given-names></name><name><surname>Giryes</surname><given-names>Raja</given-names></name><name><surname>Sapiro</surname><given-names>Guillermo</given-names></name><name><surname>Rodrigues</surname><given-names>Miguel R D</given-names></name></person-group></element-citation></ref><ref id="b37"><element-citation publication-type="journal"><article-title>On the complexity of learning neural networks</article-title><source>CoRR</source><year>2017</year><person-group person-group-type="author"><name><surname>Song</surname><given-names>Le</given-names></name><name><surname>Vempala</surname><given-names>Santosh</given-names></name><name><surname>Wilmes</surname><given-names>John</given-names></name><name><surname>Xie</surname><given-names>Bo</given-names></name></person-group></element-citation></ref><ref id="b38"><element-citation publication-type="journal"><source>No bad local minima: Data independent training error guarantees for multi- layer neural networks</source><year>2016</year><person-group person-group-type="author"><name><surname>Soudry</surname><given-names>Daniel</given-names></name><name><surname>Carmon</surname><given-names>Yair</given-names></name></person-group></element-citation></ref><ref id="b39"><element-citation publication-type="journal"><article-title>Large margin deep neural networks: Theory and algorithms</article-title><source>CoRR</source><year>2015</year><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Shizhao</given-names></name><name><surname>Chen</surname><given-names>Wei</given-names></name><name><surname>Wang</surname><given-names>Liwei</given-names></name><name><surname>Liu</surname><given-names>Tie-Yan</given-names></name></person-group></element-citation></ref><ref id="b40"><element-citation publication-type="journal"><article-title>Representation Benefits of Deep Feedforward Networks</article-title><source>In JMLR</source><year>2016</year><volume>49</volume><fpage>1</fpage><lpage>23</lpage><person-group person-group-type="author"><name><surname>Telgarsky</surname><given-names>Matus</given-names></name></person-group></element-citation></ref><ref id="b41"><element-citation publication-type="journal"><article-title>Solving parity-n problems with feedforward neural networks</article-title><source>IJCNN</source><year>2003</year><fpage>2546</fpage><lpage>2551</lpage><person-group person-group-type="author"><name><surname>Wilamowski</surname><given-names>Bogdan</given-names></name><name><surname>Hunter</surname><given-names>David</given-names></name><name><surname>Malinowski</surname><given-names>Aleksander</given-names></name></person-group></element-citation></ref><ref id="b42"><element-citation publication-type="journal"><article-title>N-bit parity ordered neural networks</article-title><source>Neurocomputing</source><year>2002</year><volume>48</volume><fpage>1053</fpage><lpage>1056</lpage><person-group person-group-type="author"><name><surname>Arslanov</surname><given-names>M Z</given-names></name><name><surname>Ashigaliev</surname><given-names>D U</given-names></name><name><surname>Ismail</surname><given-names>Esraa</given-names></name></person-group></element-citation></ref><ref id="b43"><element-citation publication-type="journal"><source>Deep sets</source><year>2017</year><person-group person-group-type="author"><name><surname>Zaheer</surname><given-names>Manzil</given-names></name><name><surname>Kottur</surname><given-names>Satwik</given-names></name><name><surname>Ravanbakhsh</surname><given-names>Siamak</given-names></name><name><surname>Poczos</surname><given-names>Barnabas</given-names></name><name><surname>Salakhutdinov</surname><given-names>Ruslan</given-names></name><name><surname>Smola</surname><given-names>Alexander</given-names></name></person-group></element-citation></ref><ref id="b44"><element-citation publication-type="journal"><article-title>Stochastic gradient descent optimizes over- parameterized deep relu networks</article-title><source>CoRR</source><year>2018</year><person-group person-group-type="author"><name><surname>Zou</surname><given-names>Difan</given-names></name><name><surname>Cao</surname><given-names>Yuan</given-names></name><name><surname>Zhou</surname><given-names>Dongruo</given-names></name><name><surname>Gu</surname><given-names>Quanquan</given-names></name></person-group></element-citation></ref></ref-list></back></article>