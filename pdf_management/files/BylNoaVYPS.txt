Title:
```
Under review as a conference paper at ICLR 2020 VARIATIONAL AUTOENCODERS FOR OPPONENT MODELING IN MULTI-AGENT SYSTEMS
```
Abstract:
```
Multi-agent systems exhibit complex behaviors that emanate from the interac- tions of multiple agents in a shared environment. In this work, we are interested in controlling one agent in a multi-agent system and successfully learn to inter- act with the other agents that have fixed policies. Modeling the behavior of other agents (opponents) is essential in understanding the interactions of the agents in the system. By taking advantage of recent advances in unsupervised learning, we propose modeling opponents using variational autoencoders. Additionally, many existing methods in the literature assume that the opponent models have access to opponent's observations and actions during both training and execution. To eliminate this assumption, we propose a modification that attempts to identify the underlying opponent model, using only local information of our agent, such as its observations, actions, and rewards. The experiments indicate that our oppo- nent modeling methods achieve equal or greater episodic returns in reinforcement learning tasks against another modeling method.
```

Figures/Tables Captions:
```
Figure 1: Diagram of the proposed VAE architecture
Figure 2: Diagram of the proposed VAE architecture using local information
Figure 3: Payoff matrix, embedding visualization and episodic return during training.
Figure 4: Episodic return during training against the opponents from T (top row) and G (bottom row). Four environments are evaluated; speaker-listener (first column), double speaker-listener (sec- ond column), predator-prey (third column) and spread (fourth column). For the double speaker- listener, predator-prey and spread environments the x-axis is in logarithmic scale.
Figure 5: Ablation on the episodic returns for different inputs in the VAE of SMA2C for weak (top row) and strong (bottom row) generalization in all four environments.
Figure 6: Ablation study on the importance of the discrimination objective. Episodic returns in all four environments for weak (top row) and strong (bottom row) generalization.
Table 1: MI estimations using MINE in the double speaker-listener and the predator-prey environ- ment of the embeddings at the 15th, 20th and 25th time step of the trajectory.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION In recent years, several promising works ( Mnih et al., 2015 ;  Schulman et al., 2015a ;  Mnih et al., 2016 ) have arisen in deep reinforcement learning (RL), leading to fruitful results in single-agent scenarios. In this work, we are interested in using single-agent RL in multi-agent systems, where we control one agent and the other agents (opponents) in the environment have fixed policies. The agent should be able to successfully interact with a diverse set of opponents as well as generalize to new unseen opponents. One effective way to address this problem is opponent modeling. The opponent models output specific characteristics of the opponents based on their trajectories. By successfully modeling the opponents, the agent can reason about opponents' behaviors and goals and adjust its policy to achieve the optimal outcome. There is a rich literature of modeling opponents in the multi- agent systems ( Albrecht & Stone, 2018 ). Several recent works have proposed learning opponent models using deep learning architectures ( He et al., 2016 ;  Raileanu et al., 2018 ;  Grover et al., 2018a ;  Rabinowitz et al., 2018 ). In this work, we focus on learning opponent models using Variational Autoencoders (VAEs) ( Kingma & Welling, 2014 ). This work is, to the best of our knowledge, the first attempt to use VAEs in multi-agent scenarios. VAE are generative models that are commonly used for learning representations of the data, and various works use them in RL for learning representations of the environment ( Igl et al., 2018 ;  Ha & Schmidhuber, 2018 ;  Zintgraf et al., 2019 ). We first propose a VAE for learning opponent representations in multi-agent systems based on the opponent trajectories. A shortcoming of this approach and most opponent modeling methods, as will be presented in Sec- tion 2, is that they require access to opponent's information, such as observations and actions, during training as well as execution. This assumption is too limiting in the majority of scenarios. For exam- ple, consider Poker, where each agent never has access to the opponent's observations. Nevertheless, during Poker, humans can reason about the opponent's behaviors and goals using only their local observations. For example, an increase in the table's pot could mean that the opponent either holds strong cards or is bluffing. Based on the idea that an agent can reason about an opponent's model using its observations, actions, and rewards in a recurrent fashion, we propose a second VAE-based architecture. The encoder of the VAE learns to represent opponents' models conditioned on only local information removing the requirement to access the opponents' information during execution.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 To summarize our contribution, in this work, we explore VAEs for opponent modeling in multi- agent systems. We are not interested in VAEs as generative models but as methods for learning representations. We evaluate our proposed methodology using a toy example and the commonly used Multi-agent Particle Environment ( Mordatch & Abbeel, 2017 ). We evaluate the quality of the learned representations, and the episodic returns that RL algorithms can achieve. The experiments indicate that opponent modeling without opponents' information can perform the same or even better in RL compared to models that access the opponent's information.

Section Title: RELATED WORK
  RELATED WORK

Section Title: Learning Opponent Models
  Learning Opponent Models In this work, we are interested in opponent modeling methods that use neural networks to learn representations of the opponents.  He et al. (2016)  proposed an opponent modeling method that learns a modeling network to reconstruct the opponent's actions given the op- ponent observations.  Raileanu et al. (2018)  developed an algorithm for learning to infer opponents' goals using the policy of the controlled agent.  Grover et al. (2018a)  proposed an encoder-decoder method for modeling the opponent's policy. The encoder learns a point-based representation of dif- ferent opponents' trajectories, and the decoder learns to reconstruct the' opponent's policy given samples from the embedding space. Additionally,  Grover et al. (2018a)  introduce an objective to separate embeddings of different agents into different clusters. d(z + , z − , z) = 1 (1 + e |z−z−|2−|z−z+|2 ) 2 (1) where z + and z are embeddings of the same agent from two different episodes and embedding z − is generated from the episode of a different agent.  Rabinowitz et al. (2018)  proposed the Theory of mind Network (TomNet), which learns embedding-based representations of opponents for meta- learning.  Tacchetti et al. (2018)  proposed RFM to model opponents using graph neural networks. A common assumption among these methods, that this work aims to eliminate, is that access to opponents trajectories is available during execution.

Section Title: Representation Learning in Reinforcement Learning
  Representation Learning in Reinforcement Learning Another topic that has received signifi- cant attention, recently, is representation learning in RL. Using unsupervised learning techniques to learn low dimensional representations of the MDP has led to significant improvement in RL.  Ha & Schmidhuber (2018)  proposed a VAE-based and a forward model to learn state representations of the environment.  Hausman et al. (2018)  learned tasks embeddings and interpolated them to solve harder tasks.  Igl et al. (2018)  used a VAE for learning representation in partially-observable envi- ronments.  Gupta et al. (2018)  proposed MAESN, which learns Gaussian embeddings to represent different tasks during meta-training and manages to quickly adapt to new task during meta-testing. The work of  Zintgraf et al. (2019)  is closely related, where Zintgraf et al. proposed a recurrent VAE model, which receives as input the observation, action, reward of the agent, and learns a variational distribution of tasks.  Rakelly et al. (2019)  used representations from an encoder for off-policy meta- RL. Note that, all these works have been applied for learning representations of tasks or properties of the environments. On the contrary, our approach is focused on learning representations of the opponents.

Section Title: BACKGROUND
  BACKGROUND

Section Title: REINFORCEMENT LEARNING
  REINFORCEMENT LEARNING Markov Decision Processes (MDPs) are commonly used to model decision making problems. An MDP consists of the set of states S, the set of actions A, the transition function, P (s |s, a), which is the probability of the next state, s , given the current state, s, and the action, a, and the reward function, r(s , a, s), that returns a scalar value conditioned on two consecutive states and the intermediate action. A policy function is used to choose an action given a state, which can be stochastic a ∼ π(a|s) or deterministic a = µ(s). Given a policy π, the state value function is defined as V (s t ) = E π [ H i=t γ i−t r t |s = s t ] and the state-action value (Q-value) Q(s t , a t ) = E π [ H i=t γ i−t r t |s = s t , a = a t ], where 0 ≤ γ ≤ 1 is the discount factor and H is the finite horizon of the episode. The goal of RL is to compute the policy that maximizes state value function V , when the transition and the reward functions are unknown.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 There is a large number of RL algorithms; however, in this work, we focus on two actor-critic algorithms; the synchronous Advantage Actor-Critic (A2C) ( Mnih et al., 2016 ;  Dhariwal et al., 2017 ) and the Deep Deterministic Policy Gradient (DDPG) ( Silver et al., 2014 ;  Lillicrap et al., 2015 ). DDPG is an off-policy algorithm, using an experience replay for breaking the correlation between consecutive samples and target networks for stabilizing the training ( Mnih et al., 2015 ). Given an actor network with parameters θ and a critic network with parameter φ, the gradient updates are performed using the following equations. On the other hand, A2C is an on-policy actor-critic algorithm, using parallel environments to break the correlation between consecutive samples. The actor-critic parameters are optimized by: where the advantage term,Â, can be computed using the Generalized Advantage Estimation (GAE) ( Schulman et al., 2015b ).

Section Title: VARIATIONAL AUTOENCODERS
  VARIATIONAL AUTOENCODERS Consider samples from a dataset x ∈ X that are generated from some hidden (latent) random vari- able z based on a generative distribution p u (x|z) with unknown parameter u and a prior distri- bution on the latent variables, which we assume is a Gaussian with 0 mean and unit variance p(z) = N (z; 0, I). We are interested in approximating the true posterior p(z|x) with a varia- tional parametric distribution q w (z|x) = N (z; µ, Σ, w).  Kingma & Welling (2014)  proposed the Variational Autoencoders (VAE) to learn this distribution. Starting from the Kullback-Leibler (KL) divergence from the approximate to the true posterior D KL (q w (z|x) p(z|x)), the lower bound on the evidence log p(x) is derived as: The architecture consists of an encoder which receives a sample x and generates the Gaussian vari- ational distribution p(z|x; w). The decoder receives a sample from the Gaussian variational distri- bution and reconstructs the initial input x. The architecture is trained using the reparameterization trick  Kingma & Welling (2014) .  Higgins et al. (2017)  proposed β-VAE, where a parameter β ≥ 0 is used to control the trade-off between the reconstruction loss and the KL-divergence.

Section Title: APPROACH
  APPROACH

Section Title: PROBLEM FORMULATION
  PROBLEM FORMULATION We consider a modified Markov Game ( Littman, 1994 ), which consists of N agents I = {1, 2, ..., N }, the set of states S, the set of actions A = A 1 × A −1 , the transition function P : S × A × S → R and the reward function r : S × A × S → R N . We consider partially observable settings, where each agent i has access only to its local observation o i and reward r i . Additionally, two sets of pretrained opponents are provided T = {I −1,m } m=M m=1 and G = {I −1,m } m=M m=1 , which are responsible for providing the joint action A −1 . Note that by opponent we refer to I −1,m , which consists of one or more agents, independently from the type of the interactions (cooperative, mixed or competitive). At the beginning of each episode, we sample a pretrained opponent from the set T during training or from G during testing. Our goal is to train the agent 1 using RL, to maximize the average return against opponents from the training set, T, and generalize to opponents sampled from the test G. Note, that when we refer to agent 1 we drop the subscript. We assume a number of K provided episode trajectories for each pretrained opponents j ∈ T, E (j) = {τ (j,k) −1 } k=K−1 k=0 , where τ (j,k) −1 = {o −1,t , a −1,t } t=H t=0 , and o −1,t , a −1,t are the observations and actions of the opponent at the time step t in the trajectory. These trajectories are generated from the opponents in set T, which are represented in the latent space from the variable z and for which we assume there exists an unknown model p u (τ −1 |z). Our goal is to approximate the unknown posterior, p(z|τ −1 ), using a variational Gaussian distribution N (µ, Σ; w) with parameters w. We consider using a β-VAE for the sequential task: We can further subtract the discrimination objective (equation 1) that was proposed by  Grover et al. (2018a) . Since the discrimination objective is always non-negative, we derive and optimize a lower bound as: The discrimination objective receives as input the mean of the variational Gaussian distribution, pro- duced by three different trajectories. Despite the less tight lower bound, the discrimination objective will separate the opponents in the embedding space, which could potentially lead to higher episodic returns. At each time step t, the recurrent encoder network generates a latent sample z t , which is conditioned on the opponent's trajectory τ −1,:t , until this time step. The KL divergence can be written as: The lower bound consist of the reconstruction loss of the trajectory which involves the observation and actions of the opponent. The opponent's action, at each time step depends on its observation and the opponent's policy, which is represented by the latent variable z. We use a decoder that consists of fully-connected layers, however, a recurrent network can be used, if we instead assume that the opponent decides its actions based on the history of its observations. Additionally, the observation at each time step depends only on the dynamics of the environment and the actions of the agents and not on the identity of the opponent. Therefore, the reconstruction loss factorizes as: From the equation above, we observe that the loss is the reconstruction of the opponent's policy given the current observation and a sample from the latent variable. Overall, our proposed VAE takes the form of a Conditional VAE ( Sohn et al., 2015 ).  Figure 1  illustrates the diagram of the VAE. The full pseudocode of the method is provided in the Appendix D. In Sections 1 and 2, it was noted that most agent modeling methods assume access to opponent's observations and actions is available both during training and execution. To eliminate this assump- tion, we propose a VAE that uses a parametric variational distribution which is conditioned on the observation-action-reward triplet of the agent that we control and a variable d indicating whether the episode has terminated; q w (z|τ = (o, a, r, d)). More precisely, our goal is to approximate the true posterior that is conditioned on opponent's information, with a variational distribution that only depends on local information. The use of this local information in a recurrent fashion has been successfully used in meta-RL settings ( Wang et al., 2016 ;  Duan et al., 2016 ). We start by computing the KL divergence between the two distributions: By following the works of  Kingma & Welling (2014)  and  Higgins et al. (2017)  and using the Jensen inequality, the VAE objective can be written as: The reconstruction loss factorizes exactly similar to equation 10. From equation 12, it can be seen that the variational distribution only depends on locally available information. Since during exe- cution, only the encoder is required to generate the opponent's model, this approach removes the assumption that access to the opponent's observations and actions is available during execution.  Figure 2  presents the diagram of the VAE.

Section Title: REINFORCEMENT LEARNING TRAINING
  REINFORCEMENT LEARNING TRAINING We use the latent variable z augmented with the agent's observation to condition the policy of our agent, which is optimized using RL. Consider the augmented observation space O = O × Z, where O is the original observation space of the our agent in the Markov game, and Z is the representation space of the opponent models. The advantage of learning the policy on O compared to O is that the policy can adapt to different z ∈ Z. After training the variational autoencoder that was described in Section 4.2, we use it to train our agent against the opponents in the set T. We use the DDPG ( Lillicrap et al., 2015 ) algorithm for this task. We did not manage to optimize the representation jointly with the policy, neither with DDPG or A2C. At the beginning of each episode, we sample an opponent from the set T. The agent's input is the local observation and a sample from the variational distribution. We refer to this as OMDDPG (Opponent Modeling DDPG), and the full pseudocode is provided in Appendix D. We optimize the second proposed VAE method jointly with the policy of the controlled agent. We use the A2C algorithm, similarly to the meta-learning algorithm RL 2 ( Wang et al., 2016 ;  Duan et al., 2016 ). In the rest of this paper, we refer to this as SMA2C (Self Modeling A2C). The actor's and the critic's input is the local observation and a sample from the latent space. We back-propagate the gradient from both the actor and the critic loss to the parameters of the encoder. Therefore, the encoder's parameters are shaped to maximize both the VAE's objective as well as the discounted sum of rewards. The full pseudocode is provided in Appendix D.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: TOY EXAMPLE
  TOY EXAMPLE We will first provide a toy example to illustrate SMA2C. We consider the classic repeated game of prisoner's dilemma with a constant episode length of 25 time steps. We control one agent, and the other agent is selected randomly between two possible opponent policies. The first opponent always defects, while the second opponent follows a tit-for-tat policy. At the beginning of the episode, one of the two opponents is randomly selected. We train SMA2C against the two possible opponents. The agent that we control has to identify the correct opponent, and the optimal policy, it can achieve, is to defect against opponent one and collaborate with opponent two.  Figure 3  shows the payoff matrix, the embedding space at the last time step of the episode, and the episodic return that SMA2C and A2C achieve during training. Note that, based on the payoff matrix, the optimal average episodic return that can be achieved is 24.5.

Section Title: EXPERIMENTAL FRAMEWORK
  EXPERIMENTAL FRAMEWORK To evaluate the proposed methods in more complex environments, we used the Multi-agent Particle Environment (MPE) ( Mordatch & Abbeel, 2017 ), which provides several different multi-agent en- vironments. The environments have continuous observation, discrete action space, and fixed-length episodes of 25 time steps. Four environments are used for evaluating the proposed methodology; the speaker-listener, the double-speaker listener, the predator-prey, and the spread. In Appendix A, descriptions of the different environments are provided. During the experiments, we evaluated the two proposed algorithms OMDDPG and SMA2C as well as the modeling method of  Grover et al. (2018a)  combined with DDPG ( Lillicrap et al., 2015 ). In all the environments, we pretrain ten different opponents, where five are used for training and five for testing. In the speaker-listener environment, we control the listener, and we create ten dif- ferent speakers using different communication messages for different colors. In the double speaker- listener, which consists of two agents that have to be both listener and speaker simultaneously, we control the first agent. We create a diverse set of opponents that have different communication mes- sages similar to speaker-listener, while they learn to navigate using the MADDPG algorithm ( Lowe et al., 2017 ), with different initial random seeds. In the predator-prey environment, we control the prey and pretrain the three other agents in the environment using MADDPG with different initial parameters. Similarly, in spread, we control one of the agents, while the opponents are pretrained using MADDPG. We use agent generalization graphs ( Grover et al., 2018b ) to evaluate the generalization of the pro- posed methods. We evaluate two types of generalizations in this work. First, we evaluate the episodic returns against the opponents that are used for training, T, which  Grover et al. (2018b)  call "weak generalization". Secondly, we evaluate against unknown opponents from set G, which is called "strong generalization". A figure of an agent generalization graph is provided in Appendix E.

Section Title: EVALUATION OF THE REPRESENTATIONS
  EVALUATION OF THE REPRESENTATIONS To evaluate the representations created from our models we will estimate the Mutual Information (MI) between the variational distribution (q(z|τ ) or q(z|τ −1 )) and the prior on the opponents' identities, which is uniform. This is a common method to estimate the quality of the representation ( Chen et al., 2018 ;  Hjelm et al., 2019 ). To estimate the MI, we use the Mutual Information Neural Estimation (MINE) ( Belghazi et al., 2018 ). Note that, the upper bound of the MI, the entropy of the uniform distribution, in our experiments is 1.61. We gather 200 trajectories against each opponent in T, where 80% of them are used for training and the remaining for testing. The visualization of the embedding space, for the predator-prey environment, is provided in Appendix C. From  Table 1 , we observe that the method of  Grover et al. (2018a)  achieves significantly higher val- ues of MI. We believe that the main reason behind this is the discrimination objective that implicitly increases MI. This is apparent in the MI values of OMDDPG as well. SMA2C manages to create opponent representations, based only on the local information of our agent, that have information about the opponent identities. Additionally, based on  Figure 4 , we observe that the value of MI is not directly related to the episodic returns in RL tasks. In Appendix B, we demonstrate that when we detach the encoder's parameters from the policy optimization, the MI decreases.

Section Title: REINFORCEMENT LEARNING PERFORMANCE
  REINFORCEMENT LEARNING PERFORMANCE We evaluate the proposed opponent modeling methods in RL settings. In  Figure 4 , the episodic returns for the three methods in all four environments are presented. Every line corresponds to the average return over five runs with different initial seeds, and the shad- owed part represents the 95% confidence interval. We evaluate the models every 1000 episodes for Under review as a conference paper at ICLR 2020 100 episodes. During the evaluation, we sample an embedding from the variational distribution at each time step, and the agent follows the greedy policy. The hyperparameters for all the experi- ments in  Figure 4  were optimized on weak generalization scenarios, against opponents from set T. Details about the implementation and hyperparameters that were used for generating the figures are presented in Appendix D. OMDDPG is an upper baseline for SMA2C achieving higher returns in all environments during weak generalization. However, OMDDPG, as well as  Grover et al. (2018a) , tend to overfit and perform poorly during strong generalization in the speaker-listener and double speaker-listener en- vironment. SMA2C achieves higher returns that  Grover et al. (2018a)  in more than half of the scenarios. Below, in the Section 5.5, we perform an ablation study on different inputs in the encoder of SMA2C. In Appendix B, we evaluate whether back-propagating the RL loss to the parameters of the encoder, in SMA2C, affects the episodic returns.

Section Title: ABLATION STUDY ON SMA2C INPUTS
  ABLATION STUDY ON SMA2C INPUTS We perform an ablation study to assess the performance requirements of the SMA2C. Our proposed method utilizes the observation, action, reward, and termination sequence to generate the opponent's model. We use different combinations of these elements in the encoder and compare the average episodic returns. In  Figure 5 , the average episode return is presented for three different cases; SMA2C with all inputs, SMA2C with only observation and action as inputs and SMA2C with only observation as input; for all four environments.

Section Title: ABLATION STUDY ON THE DISCRIMINATION OBJECTIVE
  ABLATION STUDY ON THE DISCRIMINATION OBJECTIVE Another element of this work is the utilization of the discrimination objective of the  Grover et al. (2018a)  in the VAE loss. To better understand how the opponent separation in the embedding space is related to RL performance, below,  Figure 6  shows the episodic return during the training for the OMDDPG with and without the discrimination objective is presented. Using the discrimination objective has a significant impact on the episodic returns in the speaker-listener and the double speaker-listener environment.

Section Title: CONCLUSION
  CONCLUSION To conclude this work, we proposed two methods for opponent modeling in multi-agent systems using variational autoencoders. First, we proposed OMDDPG a VAE-based method that uses the common assumption that access to opponents' information is available during execution. The goal of this work is to motivate opponent modeling without access to opponent's information. The core contribution of this work is SMA2C, which learns representations without requiring access to op- ponent's information during execution. We performed a thorough experimental evaluation of the proposed methodology. We evaluated the quality of the representations produced by our models as well as the episodic return that can achieve in RL tasks. The experiments conclusively indicate that access to the opponent's information is not necessary during execution, eliminating a long-standing assumption of the prior work. Additionally, we provided evidence that the relationship between the MI and the RL performance is not apparent. In the future, we would like to research how these models can be used for non-stationary opponents. Particularly, there are two scenarios worth inves- tigating; the first is multi-agent deep RL, where different agents are learning concurrently leading to non-stationarity in the environment, which prevents the agents from learning optimal policies. Secondly, we would like to explore whether the proposed models can deal with opponents that try to deceive it and exploit the controlled agent ( Ganzfried & Sandholm, 2011 ;  2015 ).

```
