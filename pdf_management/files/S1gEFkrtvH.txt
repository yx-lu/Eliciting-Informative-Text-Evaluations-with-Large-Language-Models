Title:
```
Under review as a conference paper at ICLR 2020 BASISVAE: ORTHOGONAL LATENT SPACE FOR DEEP DISENTANGLED REPRESENTATION
```
Abstract:
```
The variational autoencoder, one of the generative models, defines the latent space for the data representation, and uses variational inference to infer the posterior probability. Several methods have been devised to disentangle the latent space for controlling the generative model easily. However, due to the excessive con- straints, the more disentangled the latent space is, the lower quality the generative model has. A disentangled generative model would allocate a single feature of the generated data to the only single latent variable. In this paper, we propose a method to decompose the latent space into basis, and reconstruct it by linear combination of the latent bases. The proposed model called BasisVAE consists of the encoder that extracts the features of data and estimates the coefficients for linear combination of the latent bases, and the decoder that reconstructs the data with the combined latent bases. In this method, a single latent basis is subject to change in a single generative factor, and relatively invariant to the changes in other factors. It maintains the performance while relaxing the constraint for disentan- glement on a basis, as we no longer need to decompose latent space on a standard basis. Experiments on the well-known benchmark datasets of MNIST, 3DFaces and CelebA demonstrate the efficacy of the proposed method, compared to other state-of-the-art methods. The proposed model not only defines the latent space to be separated by the generative factors, but also shows the better quality of the generated and reconstructed images. The disentangled representation is verified with the generated images and the simple classifier trained on the output of the encoder.
```

Figures/Tables Captions:
```
Figure 1: In the vanilla VAE model, an interpolation experiment results in only changing the single generative factor only for (a) gender and (b) skin color.
Figure 2: The architecture of the proposed model, BasisVAE.
Figure 3: The visualization of (a) the general latent space, (b) the disentangled latent space, and (c) latent space of the proposed model with two coordinates.
Figure 4: (Top) The original images and (bottom) the reconstructed images on (a) MNIST and (b) CelebA datasets. Appendix A shows more generated images.
Figure 5: The generated images of (a) MNIST and (b) CelebA. In the MNIST dataset, the generated data is organized in each row by class.
Figure 6: The images generated from a single basis. The corresponding feature is shown above the image.
Figure 7: The images generated from a single basis. The value of the coefficient is linearly changed along the row. The corresponding characteristics are shown in the left of the images.
Figure 8: The value of the coefficient is linearly changed along the row. The corresponding charac- teristics are shown in the side of the images. Appendix A shows more generated images.
Figure 9: Randomly generated images with the coefficient c i for b i fixed as 1. According to the basis element, the images reflecting the corresponding feature are generated. Appendix A shows more generated images.
Table 1: The results of evaluating the reconstruction performance with SSIM and PSNR.
Table 2: Comparison of image generation quality by FID score.
Table 3: Results of classification using the output of the encoder. The logistic regression model for each class is trained to classify the one class. Appendix B shows more details in the numerical results for each attributes.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The proper choice of data representation is highly correlated with the difficulty of task learning for a given machine learning approach ( Higgins et al., 2017 ;  Kim et al., 2018 ;  Kim & Cho, 2018 ;  2019 ). Using a representation appropriate to specific task and data domain can significantly improve the robustness and successful learning of the model ( Kim et al., 2018 ;  Kim & Cho, 2018 ;  2019 ;  Bengio et al., 2013 ). In particular, disentangled representation is useful when dealing with data with various features, and can be effective for a large variety of domains and tasks ( Bengio et al., 2013 ;  Ridgeway, 2016 ). A latent space is disentangled if single latent units are subject to changes in single generative factors, and relatively invariant to changes in other factors ( Bengio et al., 2013 ). For example, a generative model trained on a dataset of facial images learns independent latent units subject to single independent generative factors such as hair color, gender, and emotion. We define the disentangled representation using equation (1). A change of single generative factor is consistent to the change of single coefficient c i , but not to c j for i = j. z = Σ i c i e i (1) where z ∈ R Z is a latent variable, e i ∈ R Z is a standard unit vector, and c i ∈ R is a coeffi- cient. Disentangled representation can be useful in several machine learning tasks including transfer learning and zero-shot learning ( Lake et al., 2017 ). Moreover, unlike most representation learn- ing algorithms, disentangled representation can be interpreted because they are consistent with the variability of the data ( Dupont, 2018 ). The variational autoencoder (VAE) is used to define the la- tent space by approximating the posterior distribution with approximation as follows ( Kingma & Welling, 2013 ). where D K L is Kullback-Leibler divergence, q φ (z|x) is a posterior distribution inferred by encoder, p θ (z) is a prior distribution, and p(x|z) is a likelihood or decoder. Since the VAEs are powerful to define the latent space, it is often used for disentangled representation learning ( Higgins et al., 2017 ;  Dupont, 2018 ). However, in most cases, the quality of the generated data is relatively low because of the added constraints to the loss function ( Kim & Cho, 2019 ). This is because the scale of the latent variable to represent the generative factor drops from R Z to R 1 when the generative model is f : Z ⊂ R Z → X ⊂ R X , where Z and X are the dimensions of the latent space and data, respectively. Several researchers have studied for disentanglement, but the trade-off with performance has not been considered ( Higgins et al., 2017 ;  Dupont, 2018 ;  Chen et al., 2016 ). In a vanilla VAE, one generative factor changes in the direction of element in a non-standard basis as shown in  Fig. 1 . With this result, if the latent space can be decomposed with basis B = {b 1 , · · · , b n } to denote latent variable z as Σ i c i b i , the single generative factor is associated with the single coefficient. Therefore, disentangled representation learning is achieved by the following two constrains: 1. Each coefficient is subject to change in single generative factor, and relatively invariant to the changes in other factors. 2. The generative model is trained to make the basis of the latent space as a standard basis B 0 = {e 1 , · · · , e n }. In this paper, we focus on the first constraint to formulate disentangled representation without the second constraint. The rest of this paper is organized as follows. In Section 2, we introduce the research for learning disentangled representation. The work we have done in this paper and the proposed model are presented in Section 3 and the evaluation is discussed in Section 4. Section 5 presents a summary and some future works.

Section Title: RELATED WORKS
  RELATED WORKS Many studies have been conducted to learn a data representation. It is used on various applications from feature extraction to dimension reduction. Approaches are divided into two categories: con- ventional methods and deep learning models. Principal component analysis (PCA) or independent component analysis (ICA) are well-known methods to extract features and reduce the size of data ( Smith, 2002 ;  Hyvärinen & Oja, 2000 ). Dictionary learning develops a set (dictionary) of repre- sentative elements from the data such that each datum can be expressed as a weighted sum of the atoms. The elements and weights can be found by minimizing the error with L1 regularization on the weights to enable sparsity ( Mairal et al., 2009 ;  Lee et al., 2007 ;  Aharon et al., 2006 ). They adopted the methods such as basis on linear algebra that defines the materials and mixes them appropriately to represent the data. In another approach, Kingma and Welling proposed auto-encoding variational Bayes to approximate the posterior distribution ( Kingma & Welling, 2013 ). Radford et al. showed that the walking in the latent space resulted in semantic changes ( Radford et al., 2015 ). Oord et al. proposed a vector-quantized VAE to learn a discrete latent representation ( van den Oord & Vinyals, 2017 ). It is not disentangled, but somewhat with general representation to prevent posterior collapse (i.e., violation of the first constraint). Chen et al. presented InfoGAN that learned interpretable rep- Under review as a conference paper at ICLR 2020 resentation by using mutual information ( Chen et al., 2016 ). Higgins et al. introduced an adjustable hyperparameter β that balanced latent channel capacity and independence constraints with recon- struction accuracy (betaVAE) ( Higgins et al., 2017 ). Dupont improved betaVAE by using a joint distribution of continuous and discrete latent variables ( Dupont, 2018 ). Deep learning frameworks showed promise in disentangling factors of variation, but there was a degrade in the quality of the generated data due to the trade-off. In this paper, we propose a method to learn disentangled repre- sentation while maintaining the quality of the generated data by learning materials and weights for data representation like dictionary learning and disentangling factors like deep learning approach.

Section Title: THE PROPOSED METHOD
  THE PROPOSED METHOD The architecture of a proposed model that constructs disentangled representation (i.e., the associ- ation of a single basis element with a single generative factor) with a coefficient of basis element rather than a latent unit is shown in  Fig. 2 . Unlike the conventional VAE that outputs the mean and variance of the latent space expressed as a normal distribution, the encoder of BasisVAE outputs the coefficient f (x) = c associated with elements of the basis B. The latent variable z is sampled from the Gaussian distribution N (M B · f (x), Σ f (x) ), where operator · means matrix multiplication, Σ f (x) is a variance computed by encoder, and M B = [b 1 | · · · |b n ] is a matrix form of bases. The theoretical background, loss function, and algorithms of the proposed model are discussed in detail in the following sections.

Section Title: LATENT SPACE DECOMPOSITION
  LATENT SPACE DECOMPOSITION For the first constraint mentioned in the introduction, it is proved in Theorem 1 that the latent space can be decomposed as a set of single basis elements that are subject to a single generative factor. It is enough to show that the latent variable z in the equation (2) can be decomposed into latent variables z 1 , · · · , z n , called latent basis, associated with a single generative factor, not into latent units, and the evidence lower bound (ELBO) is maintained. Let n x be the number of features that data x has and z 1 , · · · , z nx be the corresponding independent latent variables. Theorem 1. Let the latent variable z in ELBO be decomposed into independent latent variables z 1 , · · · , z nx associated with a single generative factor such that p(z) = Π i p(z i ), then the ELBO with respect to z is equal to the average of values of the ELBO with respect to z i . The q φ (z|x) which the expectation value in equation (1) with respect to should be modified as the form of q φ (z i |x) i . We prove Lemma 1 in order to prove Theorem 1. Lemma 1. If z 1 , · · · , z n are independent and L is a linear operator, E z1,··· ,zn [L(z 1 , · · · z n )] = Σ i L(E zi [z i ]) where a i is a coefficient of z i in L. Proof. We just show it in the case of n = 2. Proof of Theorem 1. Since the latent variable z can be decomposed as independent latent variable z 1 , · · · , z nx , equation (4) is derived from equation (2). By Lemma 1, we can separate the expectation as follows: As a result of equation (8), we can say that the first term of RHS is the reconstruction error, and the second term associates the latent space with the data which has i-feature represented as z i . In the next section, BasisVAE is proposed to maximize the lower bound shown in equation (8), with z 1 , · · · , z nx becoming independent. The proof on the case on n >= 3 in Lemma 1 and the derivations from equation (5) to (6) are more discussed in Appendix C.

Section Title: BASISVAE
  BASISVAE We set n x as the number of features existing in the set X of data and latent variable z as linear com- bination of z 1 , · · · , z nx . By the assumption, z 1 , · · · , z nx are independent, and for any z, z = Σ i c i z i so that the set B = {z 1 , · · · , z nx }is the basis of the latent space. For the sake of convenience, let the elements of B be denoted as b 1 , · · · , b nx . The output of the encoder is coefficients c 1 , · · · , c nx because, otherwise, the model is not different with vanilla VAE and cannot achieve the disentangled representation. The goal of the previous research is to change the latent space from (a) to (b) in  Fig. 3 , but the proposed method changes from (a) to (c). It maintains the area responsible for a single generative factor but achieves disentangled representation using coefficient c i . In this method, the model can learn a disentangled representation with coefficients (constraint 1). Besides, it does not have to define the basis of latent space as standard basis (without constraint 2). The direction of the latent basis b i is not limited to two (the latent unit becomes larger or smaller), but is set in all direc- tions in R Z , thus representing the information in various ways. 1 We train the encoder so that c i = 1 and c j = 0 if the input data has i-feature and no j-feature. The latent variable z is sampled from the normal distribution N (M B · f (x), Σ f (x) ) having the linear combination Σ i c i z i as mean, and Σ f (x) as variance, where M B = [b 1 | · · · |b n ]and f (x) = (c 1 , · · · , c n ). The decoder is trained to reconstruct the data x with z. Algorithm 1 describes the process of defining the latent space through the encoder and reconstructing the data through the decoder. Three losses are defined to train the 1 In R n , as n increases, the number of direction of z ∈ R Z becomes 2 n . end for 12: end for 13: return q φ , p θ , M B latent space in the proposed process: 1) reconstruction loss L recon , 2) inference loss L KL , and 3) basis loss L B as follows. where l is the binary function for measuring the reconstruction error, f (x) is the output of the encoder, and M B = [b 1 | · · · |b nx ] is the basis matrix. Since the elements in M B have to be indepen- dent, i.e., b i · b j = 0 if i = j, and b i · b i = 1, M T B M B should be identity matrix I during training. The total loss of the proposed model is as follows. L = αL recon + βL KL + γL B (12) where α, β, and γ are the hyperparameters for balancing between the losses.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: DATASET AND EXPERIMENTAL SETTINGS
  DATASET AND EXPERIMENTAL SETTINGS To verify the performance of the proposed model, we use the MNIST, 3DFaces and CelebA datasets ( LeCun et al., 1998 ;  Liu et al., 2015 ;  Paysan et al., 2009 ). The CelebA is a dataset with large- scale face attributes. We crop the initial 178×218 size to 138×138 and resize them as 128×128. There are total 202,599 face images and we use 162,769 images as training data and the rest as test data. The pixel values are normalized between 0 and 1. The weights of the model are initialized with the method proposed by Glorot and Bengio ( Glorot & Bengio, 2010 ). The encoder consists of eight convolutional layers whose filter size is 5×5 with LeakyReLU activation function followed by dropout and batch normalization layer ( Maas et al., 2013 ;  Srivastava et al., 2014 ;  Ioffe & Szegedy, 2015 ). The decoder is composed of four convolutional layers and 4 deconvolutional layers with ReLU activation function followed by several layers like encoder ( Nair & Hinton, 2010 ). α, β, and γ are set as 0.0004, 1, and 0.1, respectively. The binary function for measuring the reconstruction error is set as Bojanowski et al. did ( Bojanowski et al., 2017 ). BasisVAE is trained for 100 epochs with 100 batch size. The optimizer used to train the model is Adam proposed by Kingma and Ba ( Kingma & Ba, 2014 ).

Section Title: GENERATED IMAGES
  GENERATED IMAGES To verify the performance of the proposed model, the performance of BasisVAE is compared with the performance of vanilla VAE, betaVAE, and VQ-VAE ( van den Oord & Vinyals, 2017 ), which have the same structure, but different output of encoder to the proposed model, in three aspects: Reconstruction, random generation, and disentanglement.

Section Title: RECONSTRUCTION
  RECONSTRUCTION We evaluate the reconstruction performance of BasisVAE with MNIST and CelebA datasets.  Fig. 4  shows the reconstructed images for the original images. In  Table 1 , we show the structural similarity (SSIM) and peak signal-to-noise ratio (PSNR) values together with the comparison model for the quantitative evaluation of the performance. The experiment is repeated 10 times to compute the SSIM and PSNR values between the actual images and the generated images by the model trained on CelebA dataset. The results of the t-test show that the performance of the BasisVAE is superior to that of the other models statistically.

Section Title: RANDOM GENERATION
  RANDOM GENERATION The generated data by BasisVAE learned with MNIST and CelebA are illustrated in  Fig. 5 . Frechet inception distance is used to evaluate the quality of the generated images (Heusel et al., 2017) as shown in  Table 2 . The p-value obtained from the t-test was less than 0.05, indicating a statistically significant difference in performance.

Section Title: GENERATION FROM BASIS
  GENERATION FROM BASIS We conduct an experiment to verify that the basis learned through BasisVAE has actually influenced the construction of the disentangled representation. BasisVAE generates the images with basis b i by setting the coefficients as c i = 1 and c j for i = j. The feature corresponding to each basis b i is shown on the  Figs. 6  and 7. We also use a 3DFaces dataset as well as CelebA dataset to identify the characteristic change with coefficient size. As shown in  Figs. 8  and 9, we can see the basis element corresponding to azimuth and lighting in 3DFaces dataset and to hair color, bags under eyes, bald, etc. in CelebA dataset. To show that single basis element is subject to a single generative factor, we randomly generate an image with the coefficient c i for b i fixed as 1, as shown in  Fig. 9 . To quantitatively evaluate the disentangled representation, logistic regression is trained to classify the features by inputting the output of the encoder into itself. The more disentangled the latent space is, the higher accuracy the model achieves. We train about 40 binary classifiers for 40 classes of CelebA dataset, and the average accuracy is shown in  Table 3 . Appendix A shows the more generated images and Appendix C shows the distribution of coefficients c i that model learns with examples.

Section Title: CONCLUSION
  CONCLUSION In this paper, we have formulated the disentangled representation learning with two constraints. By proving the Theorem, it is shown that the latent space can be decomposed as independent latent variables associated with single generative factor. We have shown that the proposed BasisVAE constructs disentangled representation without the second constraint by constructing the basis of the latent space. Furthermore, BasisVAE outperforms the vanilla VAE and βVAE in both performance and disentanglement. Since our method can be applied to other VAEs by changing the output of the encoder as coefficients for basis element and adding loss L B , we will verify the versatility and validity by applying it to other models. The performance of the proposed model will be evaluated with other well-known benchmark datasets such as CIFAR10, 3DFaces, and ImageNet. In addition, we will achieve the higher quality of the generated data and interpretability of the latent space by constructing disentangled latent space in generative adversarial network.

```
