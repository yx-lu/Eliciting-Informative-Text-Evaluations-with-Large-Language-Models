Title:
```
Published as a conference paper at ICLR 2020 THINKING WHILE MOVING: DEEP REINFORCEMENT LEARNING WITH CONCURRENT CONTROL
```
Abstract:
```
We study reinforcement learning in settings where sampling an action from the policy must be done concurrently with the time evolution of the controlled system, such as when a robot must decide on the next action while still performing the pre- vious action. Much like a person or an animal, the robot must think and move at the same time, deciding on its next action before the previous one has completed. In order to develop an algorithmic framework for such concurrent control prob- lems, we start with a continuous-time formulation of the Bellman equations, and then discretize them in a way that is aware of system delays. We instantiate this new class of approximate dynamic programming methods via a simple architec- tural extension to existing value-based deep reinforcement learning algorithms. We evaluate our methods on simulated benchmark tasks and a large-scale robotic grasping task where the robot must "think while moving". Videos are available at https://sites.google.com/view/thinkingwhilemoving.
```

Figures/Tables Captions:
```
Figure 1: Shaded nodes represent observed variables and unshaded nodes represent unobserved random variables. (a): In "blocking" MDPs, the environment state does not change while the agent records the current state and selects an action. (b): In "concurrent" MDPs, state and action dynamics are continuous-time stochastic processes s(t) and a i (t). At time t, the agent observes the state of the world s(t), but by the time it selects an action a i (t + t AS ), the previous continuous-time action function a i−1 (t − H + t AS ) has "rolled over" to an unobserved state s(t + t AS ). An agent that concurrently selects actions from old states while in motion may need to interrupt a previous action before it has finished executing its current trajectory.
Figure 2: In concurrent versions of Cartpole and Pendulum, we observe that providing the critic with VTG leads to more robust performance across all hyperparameters. (a) Environment rewards achieved by DQN with different network architectures [either a feedforward network (FNN) or a Long Short-Term Memory (LSTM) network] and different concurrent knowledge features [Uncon- ditioned, Vector-to-go (VTG), or previous action and t AS ] on the concurrent Cartpole task for ev- ery hyperparameter in a sweep, sorted in decreasing order. (b) Environment rewards achieved by DQN with a FNN and different frame-stacking and concurrent knowledge parameters on the con- current Pendulum task for every hyperparameter in a sweep, sorted in decreasing order. Larger area-under-curve implies more robustness to hyperparameter choices. Enlarged figures provided in Appendix A.5.
Figure 3: An overview of the robotic grasping task. A static manipulator arm attempts to grasp objects placed in bins front of it. In simulation, the objects are procedurally generated.
Table 1: Large-Scale Simulated Robotic Grasping Results
Table 2: Real-World Robotic Grasping Results.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION In recent years, Deep Reinforcement Learning (DRL) methods have achieved tremendous success on a variety of diverse environments, including video games ( Mnih et al., 2015 ), zero-sum games ( Sil- ver et al., 2016 ), robotic grasping ( Kalashnikov et al., 2018 ), and in-hand manipulation tasks ( Ope- nAI et al., 2018 ). While impressive, all of these examples use a blocking observe-think-act paradigm: the agent assumes that the environment will remain static while it thinks, so that its actions will be executed on the same states from which they were computed. This assumption breaks in the con- current real world, where the environment state evolves substantially as the agent processes obser- vations and plans its next actions. As an example, consider a dynamic task such as catching a ball: it is not possible to pause the ball mid-air while waiting for the agent to decide on the next control to command. In addition to solving dynamic tasks where blocking models would fail, thinking and acting concurrently can provide benefits such as smoother, human-like motions and the ability to seamlessly plan for next actions while executing the current one. Despite these potential benefits, most DRL approaches are mainly evaluated in blocking simulation environments. Blocking environments make the assumption that the environment state will not change between when the environment state is observed and when the action is executed. This assumption holds true in most simulated environments, which encompass popular domains such as Atari ( Mnih et al., 2013 ) and Gym control benchmarks ( Brockman et al., 2016 ). The system is treated in a sequential manner: the agent observes a state, freezes time while computing an action, and finally applies the action and unfreezes time. However, in dynamic real-time environments such as real-world robotics, the synchronous environment assumption is no longer valid. After observing the state of the environment and computing an action, the agent often finds that when it executes an action, the environment state has evolved from what it had initially observed; we consider this environment a concurrent environment. In this paper, we introduce an algorithmic framework that can handle concurrent environments in the context of DRL. In particular, we derive a modified Bellman operator for concurrent MDPs and Published as a conference paper at ICLR 2020 present the minimal set of information that we must augment state observations with in order to recover blocking performance with Q-learning. We introduce experiments on different simulated environments that incorporate concurrent actions, ranging from common simple control domains to vision-based robotic grasping tasks. Finally, we show an agent that acts concurrently in a real-world robotic grasping task is able to achieve comparable task success to a blocking baseline while acting 49% faster.

Section Title: RELATED WORK
  RELATED WORK

Section Title: Minimizing Concurrent Effects
  Minimizing Concurrent Effects Although real-world robotics systems are inherently concurrent, it is sometimes possible to engineer them into approximately blocking systems. For example, using low-latency hardware ( Abbeel et al., 2006 ) and low-footprint controllers ( Cruz et al., 2017 ) mini- mizes the time spent during state capture and policy inference. Another option is to design actions to be executed to completion via closed-loop feedback controllers and the system velocity is de- celerated to zero before a state is recorded ( Kalashnikov et al., 2018 ). In contrast to these works, we tackle the concurrent action execution directly in the learning algorithm. Our approach can be applied to tasks where it is not possible to wait for the system to come to rest between deciding new actions.

Section Title: Algorithmic Approaches
  Algorithmic Approaches Other works utilize algorithmic modifications to directly overcome the challenges of concurrent control. Previous work in this area can be grouped into five approaches: (1) learning policies that are robust to variable latencies ( Tan et al., 2018 ), (2) including past his- tory such as frame-stacking ( Haarnoja et al., 2018 ), (3) learning dynamics models to predict the future state at which the action will be executed ( Firoiu et al., 2018 ;  Amiranashvili et al., 2018 ), (4) using a time-delayed MDP framework ( Walsh et al., 2007 ;  Firoiu et al., 2018 ;  Schuitema et al., 2010 ), and (5) temporally-aware architectures such as Spiking Neural Networks ( Vasilaki et al., 2009 ; Frémaux et al., 2013), point processes ( Upadhyay et al., 2018 ;  Li et al., 2018 ), and adaptive skip intervals ( Neitz et al., 2018 ). In contrast to these works, our approach is able to (1) optimize for a specific latency regime as opposed to being robust to all of them, (2) consider the properties of the source of latency as opposed to forcing the network to learn them from high-dimensional inputs, (3) avoid learning explicit forward dynamics models in high-dimensional spaces, which can be costly and challenging, (4) consider environments where actions are interrupted as opposed to discrete-time time-delayed environments where multiple actions are queued and each action is exe- cuted until completion. The approaches in (5) show promise in enabling asynchronous agents, but are still active areas of research that have not yet been extended to high-dimensional, image-based robotic tasks.

Section Title: Continuous-time Reinforcement Learning
  Continuous-time Reinforcement Learning While previously mentioned related works largely operate in discrete-time environments, framing concurrent environments as continuous-time sys- tems is a natural framework to apply. In the realm of continuous-time optimal control, path integral solutions ( Kappen, 2005 ;  Theodorou et al., 2010 ) are linked to different noise levels in system dy- namics, which could potentially include latency that results in concurrent properties. Finite differ- ences can approximate the Bellman update in continuous-time stochastic control problems ( Munos & Bourgine, 1998 ) and continuous-time temporal difference learning methods ( Doya, 2000 ) can utilize neural networks as function approximators ( Coulom, 2002 ). The effect of time-discretization (converting continuous-time environments to discrete-time environments) is studied in  Tallec et al. (2019) , where the advantage update is scaled by the time discretization parameter. While these approaches are promising, it is untested how these methods may apply to image-based DRL prob- lems. Nonetheless, we build on top of many of the theoretical formulations in these works, which motivate our applications of deep reinforcement learning methods to more complex, vision-based robotics tasks.

Section Title: VALUE-BASED REINFORCEMENT LEARNING IN CONCURRENT ENVIRONMENTS
  VALUE-BASED REINFORCEMENT LEARNING IN CONCURRENT ENVIRONMENTS In this section, we first introduce the concept of concurrent environments, and then describe the preliminaries necessary for discrete- and continuous-time RL formulations. We then describe the Published as a conference paper at ICLR 2020 MDP modifications sufficient to represent concurrent actions and finally, present value-based RL algorithms that can cope with concurrent environments. The main idea behind our method is simple and can be implemented using small modifications to standard value-based algorithms. It centers around adding additional information to the learning algorithm (in our case, adding extra information about the previous action to a Q-function) that allows it to cope with concurrent actions. Hereby, we provide theoretical justification on why these modifications are necessary and we specify the details of the algorithm in Alg. 1. While concurrent environments affect DRL methods beyond model-free value-based RL, we focus our scope on model-free value-based methods due to their attractive sample-efficiency and off-policy properties for real-world vision-based robotic tasks.

Section Title: CONCURRENT ACTION ENVIRONMENTS
  CONCURRENT ACTION ENVIRONMENTS In blocking environments (Figure 4a in the Appendix), actions are executed in a sequential blocking fashion that assumes the environment state does not change between when state is observed and when actions are executed. This can be understood as state capture and policy inference being viewed as instantaneous from the perspective of the agent. In contrast, concurrent environments (Figure 4b in the Appendix) do not assume a fixed environment during state capture and policy inference, but instead allow the environment to evolve during these time segments.

Section Title: DISCRETE-TIME REINFORCEMENT LEARNING PRELIMINARIES
  DISCRETE-TIME REINFORCEMENT LEARNING PRELIMINARIES We use standard reinforcement learning formulations in both discrete-time and continuous-time settings ( Sutton & Barto, 1998 ). In the discrete-time case, at each time step i, the agent receives state s i from a set of possible states S and selects an action a i from some set of possible actions A according to its policy π, where π is a mapping from S to A. The environment returns the next state s i+1 sampled from a transition distribution p(s i+1 |s i , a i ) and a reward r(s i , a i ). The return for a given trajectory of states and actions is the total discounted return from time step i with discount factor γ ∈ (0, 1]: R i = ∞ k=0 γ k r(s i+k , a i+k ). The goal of the agent is to maximize the expected return from each state s i . The Q-function for a given stationary policy π gives the expected return when selecting action a at state s: Q π (s, a) = E[R i |s i = s, a i = a]. Similarly, the value function gives expected return from state s: V π (s) = E[R i |s i = s]. The default blocking environment formulation is detailed in Figure 1a.

Section Title: VALUE FUNCTIONS AND POLICIES IN CONTINUOUS TIME
  VALUE FUNCTIONS AND POLICIES IN CONTINUOUS TIME For the continuous-time case, we start by formalizing a continuous-time MDP with the differential equation: where S = R d is a set of states, A is a set of actions, F : S × A → S and G : S × A → S describe the stochastic dynamics of the environment, and β is a Wiener process ( Ross et al., 1996 ). In the continuous-time setting, ds(t) is analogous to the discrete-time p, defined in Section 3.2. Continuous-time functions s(t) and a i (t) specify the state and i-th action taken by the agent. The agent interacts with the environment through a state-dependent, deterministic policy function π and the return R of a trajectory τ = (s(t), a(t)) is given by ( Doya, 2000 ): R(τ ) = ∞ t=0 γ t r(s(t), a(t))dt, (2) which leads to a continuous-time value function ( Tallec et al., 2019 ): and similarly, a continuous Q-function: where H is the constant sampling period between state captures (i.e. the duration of an action trajectory) and a refers to the continuous action function that is applied between t and t + H. The expectations are computed with respect to stochastic process p defined in Eq. 1.

Section Title: CONCURRENT ACTION MARKOV DECISION PROCESSES
  CONCURRENT ACTION MARKOV DECISION PROCESSES We consider Markov Decision Processes (MDPs) with concurrent actions, where actions are not executed to full completion. More specifically, concurrent action environments capture system state while the previous action is still executed. After state capture, the policy selects an action that is executed in the environment regardless of whether the previous action has completed, as shown in Figure 4 in the Appendix. In the continuous-time MDP case, concurrent actions can be considered as horizontally translating the action along the time dimension ( Walsh et al., 2007 ), and the effect of concurrent actions is illustrated in Figure 1b. Although we derive Bellman Equations for handling delays in both continuous and discrete-time RL, our experiments extend existing DRL implementa- tions that are based on discrete time.

Section Title: VALUE-BASED CONCURRENT REINFORCEMENT LEARNING ALGORITHMS IN CONTINUOUS AND DISCRETE-TIME
  VALUE-BASED CONCURRENT REINFORCEMENT LEARNING ALGORITHMS IN CONTINUOUS AND DISCRETE-TIME We start our derivation from this continuous-time reinforcement learning standpoint, as it allows us to easily characterize the concurrent nature of the system. We then demonstrate that the conclusions drawn for the continuous case also apply to the more commonly-used discrete setting that we then use in all of our experiments.

Section Title: Continuous Formulation
  Continuous Formulation In order to further analyze the concurrent setting, we introduce the following notation. As shown in Figure 1b, an agent selects N action trajectories during an episode, a 1 , ..., a N , where each a i (t) is a continuous function generating controls as a function of time t. Let t AS be the time duration of state capture, policy inference and any additional communication latencies. At time t, an agent begins computing the i-th trajectory a i (t) from state s(t), while concurrently executing the previous selected trajectory a i−1 (t) over the time interval (t − H + t AS , t + t AS ). At time t + t AS , where t ≤ t + t AS ≤ t + H, the agent switches to executing actions from a i (t). The continuous-time Q-function for the concurrent case from Eq. 4 can be expressed as following: The first two terms correspond to expected discounted returns for executing the action trajectory a i−1 (t) from time (t, t + t AS ) and the trajectory a i (t) from time (t + t AS , t + t AS + H). We can obtain a single-sample Monte Carlo estimatorQ by sampling random functions values p, which simply correspond to policy rollouts: Next, for the continuous-time case, let us define a new concurrent Bellman backup operator: In addition to expanding the Bellman operator to take into account concurrent actions, we demon- strate that this modified operator maintain its contraction properties that are crucial for Q-learning convergence.

Section Title: Discrete Formulation
  Discrete Formulation In order to simplify the notation for the discrete-time case where the dis- tinction between the action function a i (t) and the value of that function at time step t, a i (t), is not necessary, we refer to the current state, current action, and previous action as s t , a t , a t−1 respec- tively, replacing subindex i with t. Following this notation, we define the concurrent Q-function for the discrete-time case: Where t AS is the "spillover duration" for action a t beginning execution at time t + t AS (see Fig- ure 1b). The concurrent Bellman operator, specified by a subscript c, is as follows: Similarly to the continuous-time case, we demonstrate that this Bellman operator is a contraction. We refer the reader to Appendix A.1 for more detailed derivations of the Q-functions and Bellman operators. Crucially, Equation 9 implies that we can extend a conventional discrete-time Q-learning framework to handle MDPs with concurrent actions by providing the Q function with values of t AS and a t−1 , in addition to the standard inputs s t , a t , t.

Section Title: DEEP Q-LEARNING WITH CONCURRENT KNOWLEDGE
  DEEP Q-LEARNING WITH CONCURRENT KNOWLEDGE While we have shown that knowledge of the concurrent system properties (t AS and a t−1 , as defined previously for the discrete-time case) is theoretically sufficient, it is often hard to accurately predict t AS during inference on a complex robotics system. In order to allow practical implementation of our algorithm on a wide range of RL agents, we consider three additional features encapsulating concurrent knowledge used to condition the Q-function: (1) Previous action (a t−1 ), (2) Action selection time (t AS ), and (3) Vector-to-go (V T G), which we define as the remaining action to be executed at the instant the state is measured. We limit our analysis to environments where a t−1 , t AS , and V T G are all obtainable and H is held constant. See Appendix A.3 for details.

Section Title: EXPERIMENTS
  EXPERIMENTS In our experimental evaluation we aim to study the following questions: (1) Is concurrent knowledge defined in Section 3.6, both necessary and sufficient for a Q-function to recover the performance of a blocking unconditioned Q-function, when acting in a concurrent environment? (2) Which representations of concurrent knowledge are most useful for a Q-function to act in a concurrent environment? (3) Can concurrent models improve smoothness and execution speed of a real-robot policy in a realistic, vision-based manipulation task?

Section Title: TOY FIRST-ORDER CONTROL PROBLEMS
  TOY FIRST-ORDER CONTROL PROBLEMS First, we illustrate the effects of a concurrent control paradigm on value-based DRL methods through an ablation study on concurrent versions of the standard Cartpole and Pendulum environments. We use 3D MuJoCo based implementations in DeepMind Control Suite ( Tassa et al., 2018 ) for both tasks. For the baseline learning algorithm implementations, we use the TF-Agents ( Guadarrama et al., 2018 ) implementations of a Deep Q-Network agent, which utilizes a Feed-forward Neural Network (FNN), and a Deep Q-Recurrent Neutral Network agent, which utilizes a Long Short-Term Memory (LSTM) network. To approximate different difficulty levels of latency in concurrent envi- ronments, we utilize different parameter combinations for action execution steps and action selection steps (t AS ). The number of action execution steps is selected from {0ms, 5ms, 25ms, or 50ms} once at environment initialization. t AS is selected from {0ms, 5ms, 10ms, 25ms, or 50ms} either once at environment initialization or repeatedly at every episode reset. In addition to environment parame- ters, we allow trials to vary across model parameters: number of previous actions to store, number of previous states to store, whether to use VTG, whether to use t AS , Q-network architecture, and number of discretized actions. Further details are described in Appendix A.4.1. To estimate the relative importance of different concurrent knowledge representations, we conduct an analysis of the sensitivity of each type of concurrent knowledge representations to combinations of the other hyperparameter values, shown in Figure 2a. While all combinations of concurrent knowledge representations increase learning performance over baselines that do not leverage this information, the clearest difference stems from including VTG. In Figure 2b we conduct a similar analysis but on a Pendulum environment where t AS is fixed every environment; thus, we do not focus on t AS for this analysis but instead compare the importance of VTG with frame-stacking previous actions and observations. While frame-stacking helps nominally, the majority of the performance increase results from utilizing information from VTG.

Section Title: CONCURRENT QT-OPT ON LARGE-SCALE ROBOTIC GRASPING
  CONCURRENT QT-OPT ON LARGE-SCALE ROBOTIC GRASPING Next, we evaluate scalability of our approach to a practical robotic grasping task. We simulate a 7 DoF arm with an over-the-shoulder camera, where a bin in front of the robot is filled with procedurally generated objects to be picked up by the robot. A binary reward is assigned if an object is lifted off a bin at the end of an episode. We train a policy with QT-Opt ( Kalashnikov et al., 2018 ), a deep Q-Learning method that utilizes the cross-entropy method (CEM) to support continuous actions. In the blocking mode, a displacement action is executed until completion: the robot uses a closed-loop controller to fully execute an action, decelerating and coming to rest before observing the next state. In the concurrent mode, an action is triggered and executed without waiting, which means that the next state is observed while the robot remains in motion. Further details of the algorithm and experimental setup are shown in  Figure 3  and explained in Appendix A.4.2.  Table 1  summarizes the performance for blocking and concurrent modes comparing unconditioned models against the concurrent knowledge models described in Section 3.6. Our results indicate that the VTG model acting in concurrent mode is able to recover baseline task performance of the blocking execution unconditioned baseline, while the unconditioned baseline acting in concurrent model suffers some performance loss. In addition to the success rate of the grasping policy, we also evaluate the speed and smoothness of the learned policy behavior. Concurrent knowledge models are able to learn faster trajectories: episode duration, which measures the total amount of wall-time used for an episode, is reduced by 31.3% when comparing concurrent knowledge models with blocking unconditioned models, even those that utilize a shaped timestep penalty that reward faster policies. When switching from blocking execution mode to concurrent execution mode, we see a significantly lower action completion, measured as the ratio from executed gripper displacement to commanded displacement, which expectedly indicates a switch to a concurrent environment. The concurrent knowledge models have higher action completions than the unconditioned model in the concurrent environment, which suggests that the concurrent knowledge models are able to utilize more efficient motions, resulting in smoother trajectories. The qualitative benefits of faster, smoother trajectories are drastically apparent when viewing video playback of learned policies 1 .

Section Title: Real robot results
  Real robot results In addition, we evaluate qualitative policy behaviors of concurrent models compared to blocking models on a real-world robot grasping task, which is shown in Figure 3b. As seen in  Table 2 , the models achieve comparable grasp success, but the concurrent model is 49% faster than the blocking model in terms of policy duration, which measures the total execution time of the policy (this excludes the infrastructure setup and teardown times accounted for in episode duration, which can not be optimized with concurrent actions). In addition, the concurrent VTG model is able to execute smoother and faster trajectories than the blocking unconditioned baseline, which is clear in video playback 1 .

Section Title: DISCUSSION AND FUTURE WORK
  DISCUSSION AND FUTURE WORK We presented a theoretical framework to analyze concurrent systems where an agent must "think while moving".Viewing this formulation through the lens of continuous-time value-based reinforce- Published as a conference paper at ICLR 2020 ment learning, we showed that by considering concurrent knowledge about the time delay t AS and the previous action, the concurrent continuous-time and discrete-time Bellman operators remained contractions and thus maintained Q-learning convergence guarantees. While more information Our theoretical findings were supported by experimental results on Q-learning models acting in simulated control tasks that were engineered to support concurrent action execution. We conducted large-scale ablation studies on toy task concurrent 3D Cartpole and Pendulum environments, across model parameters as well as concurrent environment parameters. Our results indicated that VTG is the least hyperparameter-sensitive representation, and was able to recover blocking learning per- formance in concurrent settings. We extended these results to a complex concurrent large-scale sienvironmentmulated robotic grasping task, where we showed that the concurrent models were able to recover blocking execution baseline model success while acting 31.3% faster. We analyzed the qualitative benefits of concurrent models through a real-world robotic grasping task, where we showed that a concurrent model with comparable grasp success as a blocking baseline was able to learn smoother trajectories that were 49% faster. An interesting topic to explore in future work is the possibility of increased data efficiency when training on off-policy data from various latency regimes. Another natural extension of this work is to evaluate DRL methods beyond value-based algorithms, such as on-policy learning and policy gra- dient approaches. Finally, concurrent methods may allow robotic control in dynamic environments where it is not possible for the robot to stop the environment before computing the action. In these scenarios, robots must truly think and act at the same time.

```
