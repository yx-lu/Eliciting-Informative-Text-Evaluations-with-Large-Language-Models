Title:
```
Under review as a conference paper at ICLR 2020 SAFE POLICY LEARNING FOR CONTINUOUS CONTROL
```
Abstract:
```
We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through safe policies, i.e., policies that keep the agent in desirable situations, both during training and at convergence. We formulate these problems as constrained Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while guaranteeing near-constraint satisfaction for every policy update by projecting either the policy parameter or the selected action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world robot obstacle-avoidance problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction.
```

Figures/Tables Captions:
```
Figure 1: DDPG (red), DDPG-Lagrangian (cyan), SDDPG (blue), SDDPG a-projection (green) on HalfCheetah- Safe and Point-Gather. SDDPG and SDDPG a-projection perform stable and safe learning, although the dynamics and cost functions are unknown, control actions are continuous, and deep function approximations are used. Unit of x-axis is in thousands of episodes. Shaded areas represent the 1-SD confidence intervals (over 10 random seeds). The dashed purple line in the two right figures represents the constraint limit.
Figure 2: PPO (red), PPO-Lagrangian (cyan), SPPO (blue), SPPO a-projection (green) on HalfCheetah-Safe and Point-Gather. SPPO a-projection perform stable and safe learning, when the dynamics and cost functions are unknown, control actions are continuous, and deep function approximation is used.
Figure 3: Robot navigation task details.
Figure 4: DDPG (red), DDPG-Lagrangian (cyan), SDDPG (blue), DDPG a-projection (green) on Robot Navigation. Ours (SDDPG, SDDPG a-projection) balance between reward and constraint learning. Unit of x-axis is in thousands of steps. The shaded areas represent the 1-SD confidence intervals (over 50 runs). The dashed purple line represents the constraint limit.
Figure 5: Navigation routes of two learned policies in the simulator (a) and (b). On-robot experiment (c).
Figure 6: Generalization over success rate (d) and constraint satisfaction (e) on a different environment. The average success rate and cumulative (trajectory- based) constraint cost are over 100 tasks (randomly sampled start and goal robot positions). A task is con- sidered successful if the robot reaches the goal in the map-less navigation task, regardless of the constraint.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The field of reinforcement learning (RL) has witnessed tremendous success in many high-dimensional control problems, including video games ( Mnih et al., 2015 ), board games ( Silver et al., 2016 ), robot locomotion ( Lillicrap et al., 2016 ), manipulation ( Levine et al., 2016 ;  Kalashnikov et al., 2018 ), navigation ( Faust et al., 2018 ), and obstacle avoidance ( Chiang et al., 2019 ). In RL, the ultimate goal is to optimize the expected sum of rewards/costs, and the agent is free to explore any behavior as long as it leads to performance improvement. Although this freedom might be acceptable in many problems, including those involving simulators, and could expedite learning a good policy, it might be harmful in many other problems and could cause damage to the agent (robot) or to the environment (objects or people nearby). In such domains, it is absolutely crucial that while the agent optimizes long-term performance, it only executes safe policies both during training and at convergence. A natural way to incorporate safety is via constraints. A standard model for RL with constraints is constrained Markov decision process (CMDP) ( Altman, 1999 ), where in addition to its standard objective, the agent must satisfy constraints on expectations of auxiliary costs. Although optimal policies for finite CMDPs with known models can be obtained by linear programming ( Altman, 1999 ), there are not many results for solving CMDPs when the model is unknown or the state and/or action spaces are large or infinite. A common approach to solve CMDPs is to use the Lagrangian method ( Altman, 1998 ;  Geibel & Wysotzki, 2005 ), which augments the original objective function with a penalty on constraint violation and computes the saddle-point of the constrained policy optimization via primal-dual methods ( Chow et al., 2017 ). Although safety is ensured when the policy converges asymptotically, a major drawback of this approach is that it makes no guarantee with regards to the safety of the policies generated during training. A few algorithms have been recently proposed to solve CMDPs at scale while remaining safe during training. One such algorithm is constrained policy optimization (CPO) ( Achiam et al., 2017 ). CPO extends the trust-region policy optimization (TRPO) algorithm ( Schulman et al., 2015a ) to handle the constraints in a principled way and has shown promising empirical results in terms scalability, perfor- mance, and constraint satisfaction, both during training and at convergence. Another class of these algorithms is by Chow et al. ( Chow et al., 2018 ). These algorithms use the notion of Lyapunov func- tions that have a long history in control theory to analyze the stability of dynamical systems ( Khalil, 1996 ). Lyapunov functions have been used in RL to guarantee closed-loop stability ( Perkins & Barto, 2002 ;  Faust et al., 2014 ). They also have been used to guarantee that a model-based RL agent can be brought back to a "region of attraction" during exploration ( Berkenkamp et al., 2017 ). Chow et al. ( Chow et al., 2018 ) use the theoretical properties of the Lyapunov functions and propose safe approximate policy and value iteration algorithms. They prove theories for their algorithms when Under review as a conference paper at ICLR 2020 the CMDP is finite with known dynamics, and empirically evaluate them in more general settings. However, their algorithms are value-function-based, and thus are restricted to discrete-action domains. In this paper, we build on the problem formulation and theoretical findings of the Lyapunov-based approach to solve CMDPs, and extend it to tackle continuous action problems that play an important role in control theory and robotics. We propose Lyapunov-based safe RL algorithms that can handle problems with large or infinite action spaces, and return safe policies both during training and at convergence. To do so, there are two major difficulties that need to be addressed: 1) the policy update becomes an optimization problem over the large or continuous action space (similar to standard MDPs with large actions), and 2) the policy update is a constrained optimization problem in which the (Lyapunov) constraints involve integration over the action space, and thus, it is often impossible to have them in closed-form. Since the number of Lyapunov constraints is equal to the number of states, the situation is even more challenging when the problem has a large state space. To address the first difficulty, we switch from value-function-based to policy gradient (PG) algorithms. To address the second difficulty, we propose two approaches to solve our constrained policy optimization problem (a problem with infinite constraints, each involving an integral over the continuous action space) that can work with any standard on-policy (e.g., proximal policy optimization (PPO) ( Schulman et al., 2017 )) and off-policy (e.g., deep deterministic policy gradient (DDPG) ( Lillicrap et al., 2016 )) PG algorithm. Our first approach, which we call policy parameter projection or θ-projection, is a constrained optimization method that combines PG with a projection of the policy parameters onto the set of feasible solutions induced by the Lyapunov constraints. Our second approach, which we call action projection or a-projection, uses the concept of a safety layer introduced by ( Dalal et al., 2018 ) to handle simple single-step constraints, extends this concept to general trajectory- based constraints, solves the constrained policy optimization problem in closed-form using Lyapunov functions, and integrates this closed-form into the policy network via safety-layer augmentation. Since both approaches guarantee safety at every policy update, they manage to maintain safety throughout training (ignoring errors resulting from function approximation), ensuring that all intermediate policies are safe to be deployed. To prevent constraint violations due to function approximation errors, similar to CPO, we offer a safeguard policy update rule that decreases constraint cost and ensures near-constraint satisfaction. Our proposed algorithms have two main advantages over CPO. First, since CPO is closely connected to TRPO, it can only be trivially combined with PG algorithms that are regularized with relative entropy, such as PPO. This restricts CPO to on-policy PG algorithms. On the contrary, our algorithms can work with any on-policy (e.g., PPO) and off-policy (e.g., DDPG) PG algorithm. Having an off-policy implementation is beneficial, since off-policy algorithms are potentially more data-efficient, as they can use the data from the replay buffer. Second, while CPO is not a back-propagatable algorithm, due to the backtracking line-search procedure and the conjugate gradient iterations for computing natural gradient in TRPO, our algorithms can be trained end-to-end, which is crucial for scalable and efficient implementation ( Hafner et al., 2017 ). In fact, we show in Section 3.1 that CPO (minus the line search) can be viewed as a special case of the on-policy version (PPO version) of our θ-projection algorithm, corresponding to a specific approximation of the constraints. We evaluate our algorithms and compare them with CPO and the Lagrangian method on several continuous control (MuJoCo) tasks and a real-world robot navigation problem, in which the robot must satisfy certain constraints, while minimizing its expected cumulative cost. Results show that our algorithms outperform the baselines in terms of balancing the performance and constraint satisfaction (during training), and generalize better to new and more complex environments.

Section Title: PRELIMINARIES
  PRELIMINARIES We consider the RL problem in which the agent's interaction with the environment is modeled as a Markov decision process (MDP). A MDP is a tuple (X , A, γ, c, P, x0), where X and A are the state and action spaces; γ ∈ [0, 1) is a discounting factor; c(x, a) ∈ [0, Cmax] is the immediate cost function; P (·|x, a) is the transition probability distribution; and x0 ∈ X is the initial state. Although we consider deterministic initial state and cost function, our results can be easily generalized to random initial states and costs. We model the RL problems in which there are constraints on the cumulative cost using CMDPs. The CMDP model extends MDP by introducing additional costs and the associated constraints, and is defined by (X , A, γ, c, P, x0, d, d0), where the first six components are the same as in the unconstrained MDP; d(x) ∈ [0, Dmax] is the (state-dependent) immediate constraint cost; and d0 ∈ R ≥0 is an upper-bound on the expected cumulative constraint cost. To formalize the optimization problem associated with CMDPs, let ∆ be the set of Markovian stationary policies, i.e., ∆ = {π : X × A → [0, 1], a π(a|x) = 1}. At each state x ∈ X , we define the generic Bellman operator w.r.t. a policy π ∈ ∆ and a cost function h as T π,h [V ](x) = a∈A π(a|x) [h(x, a) + γ x ∈X P (x |x, a)V (x )]. Given a policy π ∈ ∆, we define the expected Under review as a conference paper at ICLR 2020 cumulative cost and the safety constraint function (expected cumulative constraint cost) as Cπ(x0) := E[ ∞ t=0 γ t c(xt, at) | π, x0] and Dπ(x0) := E[ ∞ t=0 γ t d(xt) | π, x0], respectively. The safety constraint is then defined as Dπ(x0) ≤ d0. The goal in CMDPs is to solve the constrained optimization problem It has been shown that if the feasibility set is non-empty, then there exists an optimal policy in the class of stationary Markovian policies ∆ ( Altman, 1999 , Theorem 3.1).

Section Title: POLICY GRADIENT ALGORITHMS
  POLICY GRADIENT ALGORITHMS Policy gradient (PG) algorithms optimize a policy by computing a sample estimate of the gradient of the expected cumulative cost induced by the policy, and then updating the policy parameter in the gradient direction. In general, stochastic policies that give a probability distribution over actions are parameterized by a κ-dimensional vector θ, so the space of policies can be written as {π θ , θ ∈ Θ ⊂ R κ }. Since in this setting a policy π is uniquely defined by its parameter θ, policy-dependent functions can be written as a function of θ or π interchangeably. DDPG ( Lillicrap et al., 2016 ) and PPO ( Schulman et al., 2017 ) are two PG algorithms that have recently gained popularity in solving continuous control problems. DDPG is an off-policy Q-learning style algorithm that jointly trains a deterministic policy π θ (x) and a Q-value approximator Q(x, a; φ). The Q-value approximator is trained to fit the true Q-value function and the deterministic policy is trained to optimize Q(x, π θ (x); φ) via chain-rule. The PPO algorithm we use in this paper is a penalty form of TRPO ( Schulman et al., 2015a ) with an adaptive rule to tune the DKL penalty weight β k . PPO trains a policy π θ (x) by optimizing a loss function that consists of the standard policy gradient objective and a penalty on the KL-divergence between the current θ and previous θ policies, i.e., DKL(θ, θ ) = E[ t γ t DKL(π θ (·|xt)||π θ (·|xt))|π θ , x0].

Section Title: LAGRANGIAN METHOD
  LAGRANGIAN METHOD Lagrangian method is a straightforward way to address the constraint Dπ θ (x0) ≤ d0 in CMDPs. La- grangian method adds the constraint costs d(x) to the task costs c(x, a) and transform the constrained optimization problem to a penalty form, i.e., The method then jointly optimizes θ and λ to find a saddle-point of the penalized objective. The optimization of θ may be performed by any PG algorithm on the augmented cost c(x, a) + λd(x), while λ is optimized by stochastic gradient descent. As described in Sec. 1, although the Lagrangian approach is easy to implement (see Appendix A for the details), in practice, it often violates the constraints during training. While at each step during training, the objective encourages finding a safe solution, the current value of λ may lead to an unsafe policy. This is why the Lagrangian method may not be suitable for solving problems in which safety is crucial during training.

Section Title: LYAPUNOV FUNCTIONS
  LYAPUNOV FUNCTIONS Since in this paper, we extend the Lyapunov-based approach to CMDPs of ( Chow et al., 2018 ) to PG algorithms, we end this section by introducing some terms and notations from ( Chow et al., 2018 ) that are important in developing our safe PG algorithms. We refer readers to Appendix B for details. We define a set of Lyapunov functions w.r.t. initial state x0 ∈ X and constraint threshold d0 as Lπ B (x0, d0) = {L : X → R ≥0 | T π B ,d [L](x) ≤ L(x), ∀x ∈ X , L(x0) ≤ d0}, where πB is a feasible policy of (1), i.e., Dπ B (x0) ≤ d0. We refer to the constraints in this feasibility set as the Lyapunov constraints. For an arbitrary Lyapunov function L ∈ Lπ B (x0, d0), we denote by FL = π ∈ ∆ : T π,d [L](x) ≤ L(x), ∀x ∈ X , the set of L-induced Markov stationary policies. The contraction property of T π,d , together with L(x0) ≤ d0, imply that any L-induced policy in FL is a feasible policy of (1). However, FL(x) does not always contain an optimal solution of (1), and thus, it is necessary to design a Lyapunov function that provides this guarantee. In other words, the main goal of the Lyapunov approach is to construct a Lyapunov function L ∈ Lπ B (x0, d0), such that FL contains an optimal policy π * , i.e., L(x) ≥ T π * ,d [L](x).  Chow et al. (2018)  show in their Theorem 1 that without loss of optimality, the Lyapunov function that satisfies the above criterion can be expressed as Lπ B , (x) := E ∞ t=0 γ t d(xt) + (xt) | πB, x , in which (x) ≥ 0 is a specific immediate auxiliary constraint cost that keeps track of the maximum constraint budget available for policy improvement (from πB to π * ). They propose ways to construct such , as well as an auxiliary constraint cost surrogate , which is a tight upper-bound on and can be computed more efficiently. They use this construction to propose the safe (approximate) policy and value iteration algorithms, whose objective is to solve the following LP ( Chow et al., 2018 , Eq. 6) during policy improvement: Under review as a conference paper at ICLR 2020 where Vπ B (x) = Tπ B ,c[Vπ B ](x) and QV π B (x, a) = c(x, a) + γ x P (x |x, a)Vπ B (x ) are the value and state-action value functions (w.r.t. the cost function c), and QL π B (x, a) = d(x) + (x) + γ x P (x |x, a)L π B , (x ) is the Lyapunov function. In any iterative policy optimization method, such as those studied in this paper, the feasible policy πB at each iteration can be set to the policy computed at the previous iteration (which is feasible). In LP (2), there are as many constraints as the number of states and each constraint involves an integral over the entire action space. When the state space is large, even if the integral in the constraint has a closed-form (e.g., for finite actions), solving (2) becomes numerically intractable. Chow et al. ( Chow et al., 2018 ) assumed that the number of actions is finite and focused on value-function-based RL algorithms, and addressed the large state issue by policy distillation. Since in this paper, we are interested in problems with large action spaces, solving (2) will be even more challenging. To address this issue, in the next section, we first switch from value-function-based algorithms to PG algorithms, then propose an optimization problem with Lyapunov constraints, analogous to (2), that is suitable for PG, and finally present two methods to solve our proposed optimization problem efficiently.

Section Title: SAFE LYAPUNOV-BASED POLICY GRADIENT
  SAFE LYAPUNOV-BASED POLICY GRADIENT We now present our approach to solve CMDPs in a way that guarantees safety both at convergence and during training. Similar to ( Chow et al., 2018 ), our Lyapunov-based safe PG algorithms solve a constrained optimization problem analogous to (2). In particular, our algorithms consist of two components, a baseline PG algorithm, such as DDPG or PPO, and an effective method to solve the general Lyapunov-based policy optimization problem, the analogous to (2), i.e, In the next two sections, we present two approaches to solve (3) efficiently. We call these approaches 1) θ-projection, a constrained optimization method that combines PG with projecting the policy parameter θ onto the set of feasible solutions induced by the Lyapunov constraints, and 2) a-projection, in which we embed the Lyapunov constraints into the policy network via a safety layer.

Section Title: THE θ-PROJECTION APPROACH
  THE θ-PROJECTION APPROACH The θ-projection approach is based on the minorization-maximization technique in conservative PG ( Kakade & Langford, 2002 ) and Taylor series expansion, and can be applied to both on- policy and off-policy algorithms. Following Theorem 4.1 in ( Kakade & Langford, 2002 ), we first have the following bound for the cumulative cost: −βDKL(θ, θB) ≤ Cπ θ (x0) − Cπ θ B (x0) − Ex∼µ θ B ,x 0 ,a∼π θ [QV θ B (x, a) − V θ B (x)] ≤ βDKL(θ, θB), where µ θ B ,x 0 is the γ-visiting distribution of π θ B starting at the initial state x0, and β is the weight for the entropy-based regularization. 1 Using this result, we denote by C π θ (x0; π θ B ) = Cπ θ B (x0) + βDKL(θ, θB) + Ex∼µ θ B ,x 0 ,a∼π θ [QV θ B (x, a) − V θ B (x)] the surrogate cumulative cost. It has been shown in Eq. 10 of ( Schulman et al., 2015a ) that replacing the objective function Cπ θ (x0) with its surrogate C π θ (x0; π θ B ) in solving (3) will still lead to policy improvement. In order to effectively compute the improved policy parameter θ+, one further approxi- mates the function C π θ (x0; π θ B ) with its Taylor series expansion around θB. In particular, the term Ex∼µ θ B ,x 0 ,a∼π θ [QV θ B (x, a) − V θ B (x)] is approximated up to its first order, and the term DKL(θ, θB) is approximated up to its second order. These altogether allow us to replace the objective function in (3) with Similarly, regarding the constraints in (3), we can use the Taylor series expansion (around θB) to approximate the LHS of the Lyapunov constraints as a∈A (π θ (a|x) − πB(a|x)) QL(x, a) da ≈ (θ − θB), ∇ θ Ea∼π θ [QL θ B (x, a)] | θ=θ B . Using the above approximations, at each iteration, our safe PG algorithm updates the policy by solving the following constrained optimization problem with semi-infinite dimensional Lyapunov constraints: It can be seen that if the errors resulted from the neural network parameterizations of QV θ B and QL θ B , and the Taylor series expansions are small, then an algorithm that updates the policy parameter by solving (4) can ensure safety during training. However, the presence of infinite-dimensional Lyapunov constraints makes solving (4) intractable. A solution to this is to write the Lyapunov constraints in (4) (without loss of optimality) as maxx∈X (θ − θB), ∇ θ Ea∼π θ [QL θ B (x, a)] | θ=θ B − (x) ≤ 0. Since Under review as a conference paper at ICLR 2020 the above max-operator is non-differentiable, this may still lead to numerical instability in gradient descent algorithms. Similar to the surrogate constraint in TRPO (to transform the max D KL constraint to an average DKL constraint, see Eq. 12 in ( Schulman et al., 2015a )), a more numerically stable way is to approximate the Lyapunov constraint using the average constraint surrogate (θ − θB), 1 M M i=1 ∇ θ Ea∼π θ [QL θ B (xi, a)] | θ=θ B ≤ 1 M M i=1 (xi), (5) where M is the number of on-policy sample trajectories of π θ B . In order to effectively compute the gradient of the Lyapunov value function, consider the special case when the auxiliary constraint sur- rogate is chosen as = (1 − γ)(d0 − Dπ θ B (x0)) (see Appendix B for justification). Using the fact that is θ-independent, the gradient term in (5) can be written as a π θ (a|x) ∇ θ log π θ (a|x) QW θ B (xi, a)da, where W θ B (x) = T π B ,d [W θ B ](x) and QW θ B (x, a) = d(x) + γ x P (x |x, a)W θ B (x ) are the con- straint value functions, respectively. Since the integral is equal to E a∼π θ [Q W θ B (x i , a)], the average constraint surrogate (5) can be approximated (approximation is because of the choice of ) by the inequality Dπ θ B (x0) + 1 1−γ (θ − θB), 1 M M i=1 ∇ θ Ea∼π θ [QW θ B (xi, a)]| θ=θ B ≤ d0, which is equivalent to the constraint used in CPO (see Section 6.1 in ( Achiam et al., 2017 )). This shows that CPO (minus the line search) belongs to the class of our Lyapunov-based PG algorithms with θ-projection. We refer to the DDPG and PPO versions of our θ-projection safe PG algorithms as SDDPG and SPPO. Derivation details and the pseudo-code (Algorithm 4) of these algorithms are given in Appendix C.

Section Title: THE a-PROJECTION APPROACH
  THE a-PROJECTION APPROACH The main characteristic of the Lyapunov approach is to break down a trajectory-based constraint into a sequence of single-step state dependent constraints. However, when the state space is infinite, the feasibility set is characterized by infinite dimensional constraints, and thus, it is counter-intuitive to directly enforce these Lyapunov constraints (as opposed to the original trajectory-based constraint) into the policy update optimization. To address this, we leverage the idea of a safety layer from ( Dalal et al., 2018 ), that was applied to simple single-step constraints, and propose a novel approach to embed the set of Lyapunov constraints into the policy network. This way, we reformulate the CMDP problem (1) as an unconstrained optimization problem and optimize its policy parameter θ (of the augmented network) using any standard unconstrained PG algorithm. At every given state, the unconstrained action is first computed and then passed through the safety layer, where a feasible action mapping is constructed by projecting unconstrained actions onto the feasibility set w.r.t. Lyapunov constraints. This constraint projection approach can guarantee safety during training. We now describe how the action mapping (to the set of Lyapunov constraints) works 2 . Recall from the policy improvement problem in (3) that the Lyapunov constraint is imposed at every state x ∈ X . Given a baseline feasible policy πB = π θ B , for any arbitrary policy parameter θ ∈ Θ, we denote by Ξ(πB, θ) = {θ ∈ Θ : QL π B (x, π θ (x)) − QL π B (x, πB(x)) ≤ (x), ∀x ∈ X }, the projection of θ onto the feasibility set induced by the Lyapunov constraints. One way to construct a feasible policy π Ξ(π B ,θ) from a parameter θ is to solve the following 2 -projection problem: We refer to this operation as the Lyapunov safety layer. Intuitively, this projection perturbs the unconstrained action as little as possible in the Euclidean norm in order to satisfy the Lyapunov constraints. Since this projection guarantees safety, if we have access to a closed form of the projection, we may insert it into the policy parameterization and simply solve an unconstrained policy optimization problem, i.e., θ+ ∈ arg min θ∈Θ Cπ Ξ(π B ,θ) (x0), using any standard PG algorithm. To simplify the projection (6), we can approximate the LHS of the Lyapunov constraint with its first-order Taylor series (w.r.t. action a = πB(x)). Thus, at any given state x ∈ X , the safety layer solves the following projection problem: where η(x) ∈ [0, 1) is the mixing parameter that controls the trade-off between projecting on un- constrained policy (for return maximization) and on baseline policy (for safety), and gL π B (x) := ∇aQL π B (x, a) | a=π B (x) is the action-gradient of the state-action Lyapunov function. Similar to the analysis of Section 3.1, if the auxiliary cost is state-independent, one can readily find gL π B (x) by computing the gradient of the constraint action-value function ∇aQW θ B (x, a) | a=π B (x) . Note that the objective function in (7) is positive-definite and quadratic, and the constraint approxima- tion is linear. Therefore, the solution of this (convex) projection problem can be effectively computed by an in-graph QP-solver, such as OPT-Net ( Amos & Kolter, 2017 ). Combined with the above projection procedure, this further implies that the CMDP problem can be effectively solved using an end-to-end PG training pipeline (such as DDPG or PPO). When the CMDP has a single constraint (and thus a single Lyapunov constraint), the policy π Ξ(π B ,θ) (x) has the following analytical solution. Proposition 1. At any given state x ∈ X , the solution to the optimization problem (7) has the form The closed-form solution is essentially a linear projection of the unconstrained action π θ (x) onto the Lyapunov-safe hyper-plane with slope gL π B (x) and intercept (x) = (1 − γ)(d0 − Dπ B (x0)). It is possible to extend this closed-form solution to handle multiple constraints, if there is at most one constraint active at a time (see Proposition 1 in ( Dalal et al., 2018 )).We refer to the DDPG and PPO versions of our a-projection safe Lyapunov-based PG algorithms as SDDPG a-projection and SPPO a-projection. Derivation and pseudo-code (Algorithm 5) of these algorithms are in Appendix C.

Section Title: EXPERIMENTS ON MUJOCO BENCHMARKS
  EXPERIMENTS ON MUJOCO BENCHMARKS We empirically evaluate 3 our Lyapunov-based safe PG algorithms to assess their: (i) performance in terms of cost and safety during training, and (ii) robustness w.r.t. constraint violation. We use three simulated robot locomotion continuous control tasks in the MuJoCo simulator ( Todorov et al., 2012 ). The notion of safety in these tasks is motivated by physical constraints: (i) HalfCheetah-Safe: this is a modification of the MuJoCo HalfCheetah problem in which we impose constraints on the speed of Cheetah in order to force it to run smoothly. The video shows that the policy learned by our algorithm results in slower but much smoother movement of Cheetah compared to the policies learned by PPO and Lagrangian 4 ; (ii) Point-Circle: the agent is rewarded for running in a wide circle, but is constrained to stay within a safe region defined by |x| ≤ x lim ; (iii) Point-Gather & Ant-Gather: the agent is rewarded for collecting target objects in a terrain map, while being constrained to avoid bombs. The last two tasks were first introduced in ( Achiam et al., 2017 ) by adding constraints to the original MuJoCo tasks: Point and Ant. Details of these tasks are given in Appendix D. We compare our algorithms with two state-of-the-art unconstrained algorithms, DDPG and PPO, and two constrained methods, Lagrangian with optimized Lagrange multiplier (Appendix A) and on-policy CPO. We use the CPO algorithm that is based on PPO (unlike the original CPO that is based on TRPO) and coincides with our SPPO algorithm derived in Section 4.1. SPPO preserves the essence of CPO by adding the first-order constraint and relative entropy regularization to the policy optimization problem. The main difference between CPO and SPPO is that the latter does not perform backtracking line-search in learning rate. We compare with SPPO instead of CPO to 1) avoid the additional computational complexity of line-search in TRPO, while maintaining the performance of PG using PPO, 2) have a back-propagatable version of CPO, and 3) have a fair comparison with other back-propagatable safe PG algorithms, such as our DDPG and a-projection based algorithms. Comparison with baselines: Figures 1a, 1b, 2a, 2b, 8a, 8b, 9a, 9b show that our Lyapunov-based PG algorithms are stable in learning and all converge to feasible policies with reasonable performance. Figures 1c, 1d, 2c, 2d, 8c, 8d, 9c, 9b show the algorithms in terms of constraint violation during training. These figures indicate that our algorithms quickly stabilize the constraint cost below the threshold, while the unconstrained DDPG and PPO violate the constraints, and Lagrangian tends to jiggle around the threshold. Moreover, it is worth-noting that the Lagrangian method can be sensitive to the initialization of the Lagrange multiplier λ 0 . If λ 0 is too large, it would make policy updates overly conservative, and if it is too small, then we will have more constraint violation. Without further knowledge about the environment, we treat λ 0 as a hyper-parameter and optimize it via grid-search. See Appendix D for more details and for the experimental results of Ant-Gather and Point-Circle. a-projection vs. θ-projection: The figures indicate that in many cases DDPG and PPO with a- projection converge faster and have lower constraint violation than their θ-projection counterparts (i.e., SDDPG and SPPO). This corroborates with the hypothesis that a-projection is less conservative during policy updates than θ-projection (which is what CPO is based on) and generates smoother gradient updates during end-to-end training.

Section Title: DDPG vs. PPO
  DDPG vs. PPO In most experiments (HalfCheetah, PointGather, and AntGather) the DDPG algo- rithms tend to have faster learning than their PPO counterparts, while the PPO algorithms perform better in terms of constraint satisfaction. The faster learning behavior is due to the improved data- efficiency when using off-policy samples in PG, however, the covariate-shift 5 in off-policy data makes tight constraint control more challenging.

Section Title: SAFE POLICY GRADIENT FOR ROBOT NAVIGATION
  SAFE POLICY GRADIENT FOR ROBOT NAVIGATION We now evaluate safe policy optimization algorithms on a real robot task - a map-less navigation task ( Chiang et al., 2019 ) - where a noisy differential drive robot with limited sensors (Fig. 3a) is required to navigate to a goal outside of its field of view in unseen environments while avoiding collision. The main goal is to learn a policy that drives the robot to goal as efficiently as possible, while limiting the impact energy of collisions, since the collision can damage the robot and environment. Here the CMDP is non-discounting and has a fixed horizon. The agent's observations consist of the relative goal position, agent's velocity, and Lidar measurements (Fig. 3a). The actions are the linear and angular velocity at the robot's center of the mass. 6 The transition probability captures the noisy robot's dynamics, whose exact formulation is unknown to the robot. The robot must navigate Under review as a conference paper at ICLR 2020 (a) Lagrangian (b) SDDPG a- projection (c) SDDPG a- projection to arbitrary goal positions collision-free in a previously unseen environment, and without access to the indoor map and any work-space topology. We reward the agent for reaching the goal, which translates to an immediate cost that measures the relative distance to the goal. To measure the total impact energy of obstacle collisions, we impose an immediate constraint cost to account for the speed during collision, with a constraint threshold d 0 that characterizes the agent's maximum tolerable collision impact energy to any object. Different from the standard approach, where a constraint on collision speed is explicitly imposed to the learning problem at each time step, we emphasize that a CMDP constraint is required here because it allows the robot to lightly brush off the obstacle (such as walls) but prevent it from ramming into any objects. Other use cases of CMDP constraints in robot navigation include collision avoidance ( Pfeiffer et al., 2018 ) or limiting total battery usage of the task.

Section Title: Experimental Results
  Experimental Results We evaluate the learning algorithms on success rate and constraint control averaged over 100 episodes with random initialization. The task is successful if the robot reaches the goal before the constraint threshold (total energy of collision) is exhausted. While all methods converge to policies with reasonable performance, Figure 4a and 4b show that the Lyapunov-based PG algorithms have higher success rates, due to their robust abilities of controlling the total constraint, as well minimizing the distance to goal. Although the unconstrained method often yields a lower distance to goal, it violates the constraint more frequently leading to a lower success rate. Lagrangian approach is less robust to initialization of parameters, and therefore it generally has lower success rate and higher variability than the Lyapunov-based methods. Unfortunately due to function approximation error and stochasticity of the problem, all the algorithms converged pre-maturely with constraints above the threshold, possibly due to the overly conservative constraint threshold (d 0 = 100). Inspection of trajectories shows that the Lagrangian method tends to zigzag and has more collisions, while the SDDPG chooses a safer path to reach the goal (Figures 5a and 5b). Next, we evaluate how well the methods generalize to (i) longer trajectories, and (ii) new environments. The tasks are trained in a 22 by 18 meters environment (Fig. 7) with goals placed within 5 to 10 meters from the robot initial state. In a much larger evaluation environment (60 by 47 meters) with goals placed up to 15 meters away from the goal, the success rate of all methods degrades as the goals are further away (Fig. 6a). The safety methods (a-projection - SL-DDPG, and θ-projection - SG-DDPG) outperform unconstrained and Lagrangian (DDPG and LA-DDPG), while retaining the lower constraints even when the task becomes more difficult (Fig. 6b). Finally, we deployed the SL-DDPG policy onto the real Fetch robot ( Wise et al., 2016 ) in an everyday office environment.  7  Fetch robot weights 150 kilograms, and reaches maximum speed of 7 km/h making the collision force a safety paramount. Figure 5c shows the top down view of the robot log. Robot travelled, through narrow corridors and around people walking through the office, for a total of 500 meters to complete five repetitions of 12 tasks, each averaging about 10 meters to the goal. The robot robustly avoids both static and dynamic (humans) obstacles coming into its path. We observed additional "wobbling" effects, that was not present in simulation. This is likely due to the wheel slippage at the floor that the policy was not trained for. In several occasions when the robot could not find a clear path, the policy instructed the robot to stay put instead of narrowly passing by the obstacle. This is precisely the safety behavior we want to achieve with the Lyapunov-based algorithms.

Section Title: CONCLUSIONS AND FUTURE WORK
  CONCLUSIONS AND FUTURE WORK We used the notion of Lyapunov functions and developed a class of safe RL algorithms for continuous action problems. Each algorithm in this class is a combination of one of our two proposed projections: Under review as a conference paper at ICLR 2020 θ-projection and a-projection, with any on-policy (e.g., PPO) or off-policy (e.g., DDPG) PG algorithm. We evaluated our algorithms on four high-dimensional simulated robot locomotion MuJoCo tasks and compared them with several baselines. To demonstrate the effectiveness of our algorithms in solving real-world problems, we also applied them to an indoor robot navigation problem, to ensure that the robot's path is optimal and collision-free. Our results indicate that our algorithms 1) achieve safe learning, 2) have better data-efficiency, 3) can be more naturally integrated within the standard end-to-end differentiable PG training pipeline, and 4) are scalable to tackle real-world problems. Our work is a step forward in deploying RL to real-world problems in which safety guarantees are of paramount importance. Future work includes 1) extending a-projection to stochastic policies and 2) extensions of the Lyapunov approach to model-based RL and use it for safe exploration.

```
