Title:
```
Published as a conference paper at ICLR 2020 EXPLORATORY NOT EXPLANATORY: COUNTERFACTUAL ANALYSIS OF SALIENCY MAPS FOR DEEP REINFORCEMENT LEARNING
```
Abstract:
```
Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.
```

Figures/Tables Captions:
```
Figure 1: Three examples of perturbation saliency maps generated from the same model on different inputs: (a) a frame taken from an episode of agent play in Breakout, (b) the same frame with the brick pattern reflected across the vertical axis, and (c) the original frame with the ball, paddle and brick pattern reflected across the vertical axis. The blue and red regions represent their importance in action selection and reward estimation from the current state, respectively. The pattern and intensity of saliency around the tunnel is not symmetric in either reflection intervention, indicating that a popular hypothesis (agents learn to aim at tunnels) does not hold for all possible tunnels.
Figure 2: (a) Causal graphical model of the relationships between an RL agent (yellow plate) and an image-based environment (blue plate). The environment maintains some (usually latent) game state. Some function F produces a high-dimensional pixel representation of game state ("Pixels"). The learned network takes this pixel image and produces logits used to select an action. Tempo- rally extended sequences of this action selection procedure result in observed agent behavior."M" represents interventions made by saliency maps. Such interventions are not naturalistic and are in- consistent with the generative process F; (b) conceptual diagram of how a human observer infers explanations. Hypotheses ("Claims") about the semantic features identified by the learned policy are proposed by reasoning backwards about what representation, often latent, might jointly produce the observed saliency pattern and agent behavior.
Figure 3: Interventions on brick configurations in Breakout. (a) saliency after shifting the brick positions by some pixels where shift=0 represents the original frame; (b) saliency after shifting the brick positions, ball, and paddle to the left. The pattern and intensity of saliency around the tunnel is not symmetric in the reflection interventions.
Figure 4: Interventions on displayed score in Amidar. The legend in (b) applies to all figures. (a) re- ward over time for different interventions on displayed score; (b) object saliency on displayed score over time; (c) correlation between the differences in reward and object saliency from the original trajectory. Interventions on displayed score result in differing levels of degraded performance but produce similar saliency maps, suggesting that agent behavior as measured by rewards is underde- termined by salience.
Figure 5: Interventions on enemy location in Amidar. The legend in (b) applies to all figures. (a) the distance-to-player of each enemy in Amidar, observed over time, where saliency intensity is represented by the shaded region around each line; (b) the distance-to-player and saliency, with linear regressions, observed for each enemy; (c) variation in enemy saliency when enemy position is varied by intervention. The plots suggest that there is no substantial correlation and no causal dependence between distance-to-player and object saliency.
Table 1: Categories of interventions on images. Distortion interventions change pixels without changing game state or semantic concepts. Pixel perturbations in adversarial ML add adversarial noise to images to change the output of the network without making human-perceptible changes in the image. Semantics-preserving interventions are manipulations of game state that result in an image that preserves some semantic concept of interest. Reflections across lines of symmetry typically alter aspects of game state, but do not meaningfully change any semantic information about the scene. "Fat-hand" interventions are manipulations intended to measure the effect of some specific treatment, but which unintentionally alter other relevant aspects of the system. The term is drawn from
Table 2: Summary of the survey on usage of saliency maps in deep RL. Columns represent categories of saliency map usage, and rows represent categories of saliency map meth- ods, with each cell denoting the number of claims in those categories. Individual claims may be counted in multiple columns.
Table 3: Numeric results from regression analysis for the observational and interventional results in Figures 5b and c. The results indicate a very small strength of effect (slope) for both observational and interventional data and a small correlation coefficient (r), suggesting that there is, at best, only a very weak causal dependence of saliency on distance-to-player.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Saliency map methods are a popular visualization technique that produce heatmap-like output high- lighting the importance of different regions of some visual input. They are frequently used to explain how deep networks classify images in computer vision applications (Simonyan et al., 2014; Zeiler & Fergus, 2014; Springenberg et al., 2015; Ribeiro et al., 2016; Dabkowski & Gal, 2017; Fong & Vedaldi, 2017; Selvaraju et al., 2017; Shrikumar et al., 2017; Smilkov et al., 2017; Zhang et al., 2018) and to explain how agents choose actions in reinforcement learning (RL) applications (Bog- danovic et al., 2015; Wang et al., 2016; Zahavy et al., 2016; Greydanus et al., 2017; Iyer et al., 2018; Sundar, 2018; Yang et al., 2018; Annasamy & Sycara, 2019). Saliency methods in computer vision and reinforcement learning use similar procedures to generate these maps. However, the temporal and interactive nature of RL systems presents a unique set of opportunities and challenges. Deep models in reinforcement learning select sequential actions whose effects can interact over long time periods. This contrasts strongly with visual classification tasks, in which deep models merely map from images to labels. For RL systems, saliency maps are often used to assess an agent's internal representations and behavior over multiple frames in the environment, rather than to assess the importance of specific pixels in classifying images. Despite their common use to explain agent behavior, it is unclear whether saliency maps provide useful explanations of the behavior of deep RL agents. Some prior work has evaluated the ap- plicability of saliency maps for explaining the behavior of image classifiers (Samek et al., 2017; Adebayo et al., 2018; Kindermans et al., 2019), but there is not a corresponding literature evaluating the applicability of saliency maps for explaining RL agent behavior. In this work, we develop a methodology grounded in counterfactual reasoning to empirically evalu- ate the explanations generated using saliency maps in deep RL. Specifically, we: C1 Survey the ways in which saliency maps have been used as evidence in explanations of deep RL agents. C2 Describe a new interventional method to evaluate the inferences made from saliency maps. C3 Experimentally evaluate how well the pixel-level inferences of saliency maps correspond to the semantic-level inferences of humans.

Section Title: INTERPRETING SALIENCY MAPS IN DEEP RL
  INTERPRETING SALIENCY MAPS IN DEEP RL Consider the saliency maps generated from a deep RL agent trained to play the Atari game Breakout. The goal of Breakout is to use the paddle to keep the ball in play so it hits bricks, eliminating them from the screen. Figure 1a shows a sample frame with its corresponding saliency. Note the high salience on the missing section of bricks ("tunnel") in Figure 1a. Creating a tunnel to target bricks at the top layers is one of the most high-profile examples of agent behavior being explained according to semantic, human-understandable concepts (Mnih et al., 2015). Given the intensity of saliency on the tunnel in 1a, it may seem reasonable to infer that this saliency map provides evidence that the agent has learned to aim at tunnels. If this is the case, mov- ing the horizontal position of the tunnel should lead to similar saliency patterns on the new tunnel. However, Figures 1b and 1c show that the salience pattern is not preserved. Neither the presence of the tunnel, nor the relative positioning of the ball, paddle, and tunnel, are responsible for the intensity of the saliency observed in Figure 1a.

Section Title: SALIENCY MAPS AS INTERVENTIONS
  SALIENCY MAPS AS INTERVENTIONS Examining how some of the technical details of reinforcement learning interact with saliency maps can help explain both the potential utility and the potential pitfalls of interpreting saliency maps. RL methods enable agents to learn how to act effectively within an environment by repeated interaction with that environment. Certain states in the environment give the agent positive or negative reward. The agent learns a policy, a mapping between states and actions according to these reward signals. The goal is to learn a policy that maximizes the discounted sum of rewards received while acting in the environment (Sutton & Barto, 1998). Deep reinforcement learning uses deep neural networks to represent policies. These models enable interaction with environments requiring high-dimensional state inputs (e.g., Atari games). Consider the graphical model in Figure 2a representing the deep RL system for a vision-based game environment. Saliency maps are produced by performing some kind of intervention M on this system and calculating the difference in logits produced by the original and modified images. The interventions used to calculate saliency for deep RL are performed at the pixel level (red node and arrow in Figure 2a). These interventions change the conditional probability distribution of "Pixels" by giving it another parent (Pearl, 2000). Functionally, this can be accomplished through a variety of means, including changing the color of the pixel (Simonyan et al., 2014), adding a gray mask (Zeiler & Fergus, 2014), blurring a small region (Greydanus et al., 2017), or masking objects with the background color (Iyer et al., 2018). The interventions M are used to simulate the effect of the absence of the pixel(s) on the network's output. Note however that these interventions change the image in a way that is inconsistent with the Published as a conference paper at ICLR 2020 generative process F . They are not "naturalistic" interventions. This type of intervention produces images for which the learned network function may not be well-defined.

Section Title: EXPLANATIONS FROM SALIENCY MAPS
  EXPLANATIONS FROM SALIENCY MAPS To form explanations of agent behavior, human observers combine information from saliency maps, agent behavior, and semantic concepts. Figure 2b shows a system diagram of how these components interact. We note that semantic concepts are often identified visually from the pixel output as the game state is typically latent. Counterfactual reasoning has been identified as a particularly effective way to present explanations of the decision boundaries of deep models (Mittelstadt et al., 2019). Humans use counterfactuals to reason about the enabling conditions of particular outcomes, as well as to identify situations where the outcome would have occurred even in the absence of some action or condition (de Graaf & Malle, 2017; Byrne, 2019). Saliency maps provide a kind of pixel-level counterfactual, but if the goal is to explain agent behavior according to semantic concepts, interventions at the pixel level seem unlikely to be sufficient. Since many semantic concepts may map to the same set of pixels, it may be difficult to identify the functional relationship between changes in pixels and changes in network output according to semantic concepts or game state (Chalupka et al., 2015). Researchers may be interpreting differences in network outputs as evidence of differences in semantic concepts. However, changes in pixels do not guarantee changes in semantic concepts or game state. In terms of changes to pixels, semantic concepts, and game state, we distinguish among three classes of interventions: distortion, semantics-preserving, and fat-hand (see  Table 1 ). Semantics-preserving and fat-hand interventions are defined with respect to a specific set of semantic concepts. Fat-hand interventions change game state in such a way that the semantic concepts of interest are also altered. The pixel-level manipulations used to produce saliency maps primarily result in distortion inter- ventions, though some saliency methods (e.g., object-based) may conceivably produce semantics- preserving or fat-hand interventions as well. Pixel-level interventions are not guaranteed to produce changes in semantic concepts, so counterfactual evaluations that apply semantics-preserving inter- ventions may be a more appropriate approach for precisely testing hypotheses of behavior. To assess how saliency maps are typically used to make inferences regarding agent behavior, we surveyed recent conference papers in deep RL. We focused our pool of papers on those that use saliency maps to generate explanations or make claims regarding agent behavior. Our search criteria consisted of examining papers that cited work that first described any of the following four types of saliency maps: Jacobian Saliency. Wang et al. (2016) extend gradient-based saliency maps to deep RL by com- puting the Jacobian of the output logits with respect to a stack of input images. Perturbation Saliency. Greydanus et al. (2017) generate saliency maps by perturbing the origi- nal input image using a Gaussian blur of the image and measure changes in policy from remov- ing information from a region. Object Saliency. Iyer et al. (2018) use template matching, a common computer vision tech- nique (Brunelli, 2009), to detect (template) objects within an input image and measure salience through changes in Q-values for masked and unmasked objects.

Section Title: Attention Saliency
  Attention Saliency Most recently, attention-based saliency mapping methods have been pro- posed to generate interpretable saliency maps (Mott et al., 2019; Nikulin et al., 2019). From a set of 90 papers, we found 46 claims drawn from 11 papers that cited and used saliency maps as evidence in their explanations of agent behavior. The full set of claims are given in Appendix C.

Section Title: SURVEY RESULTS
  SURVEY RESULTS We found three categories of saliency map usage, summarized in  Table 2 . First, all claims interpret salient areas as a proxy for agent focus. For exam- ple, a claim about a Breakout agent notes that the network is focusing on the paddle and little else (Greydanus et al., 2017). Second, 87% of the claims in our survey propose hypotheses about the features of the learned policy by rea- soning backwards about what repre- sentation might jointly produce the observed saliency pattern and agent Published as a conference paper at ICLR 2020 behavior. These types of claims either develop an a priori explanation of behavior and evaluate it using saliency, or they propose an ad hoc explanation after observing saliency to reason about how the agent is using salient areas. One a priori claim notes that the displayed score is the only differing factor between two states and evaluates that claim by noting that saliency focuses on these pixels (Zahavy et al., 2016). An ad hoc claim about a racing game notes that the agent is recogniz- ing a time-of-day cue from the background color and acting to prepare for a new race (Yang et al., 2018). Finally, only 7% (3 out of 46) of the claims drawn from saliency maps are accompanied by additional or more direct experimental evidence. One of these attempts to corroborate the interpreted saliency behavior by obtaining additional saliency samples from multiple runs of the game. The other two attempt to manipulate semantics in the pixel input to assess the agent's response by, for example, adding an additional object to verify a hypothesis about memorization (Annasamy & Sycara, 2019).

Section Title: COMMON PITFALLS IN CURRENT USAGE
  COMMON PITFALLS IN CURRENT USAGE In the course of the survey, we also observed several more qualitative characteristics of how saliency maps are routinely used.

Section Title: Subjectivity
  Subjectivity Recent critiques of machine learning have already noted a worrying tendency to con- flate speculation and explanation (Lipton & Steinhardt, 2018). Saliency methods are not designed to formalize an abstract human-understandable concept such as "aiming" in Breakout, and they do not provide a means to quantitatively compare semantically meaningful consequences of agent behavior. This leads to subjectivity in the conclusions drawn from saliency maps.

Section Title: Unfalsiability
  Unfalsiability One hallmark of a scientific hypothesis or claim is falsifiability (Popper, 1959). If a claim is false, its falsehood should be identifiable from some conceivable experiment or observation. One of the most disconcerting practices identified in the survey is the presentation of unfalsifiable interpretations of saliency map patterns. An example: "A diver is noticed in the saliency map but misunderstood as an enemy and being shot at" (see Appendix C). It is unclear how we might falsify an abstract concept such as "misunderstanding".

Section Title: Cognitive Biases
  Cognitive Biases Current theory and evidence from cognitive science implies that humans learn complex processes, such as video games, by categorizing objects into abstract classes and by in- ferring causal relationships among instances of those classes (Tenenbaum & Niyogi, 2003; Dubey et al., 2018). Our survey suggests that researchers infer that: (1) salient regions map to learned representations of semantic concepts (e.g., ball, paddle), and (2) the relationships among the salient regions map to high-level behaviors (e.g., tunnel-building, aiming). Researchers' expectations im- pose a strong bias on both the existence and nature of these mappings.

Section Title: METHODOLOGY
  METHODOLOGY Our survey indicates that many researchers use saliency maps as an explanatory tool to infer the rep- resentations and processes behind an agent's behavior. However, the extent to which such inferences are valid has not been empirically evaluated under controlled conditions. In this section, we show how to generate falsifiable hypotheses from saliency maps and propose an intervention-based approach to verify the hypotheses generated from saliency maps. We intervene on game state to produce counterfactual semantic conditions. This provides a concrete methodology to assess the relationship between saliency and learned semantic representations.

Section Title: Building Falsifiable Hypotheses from Saliency Maps
  Building Falsifiable Hypotheses from Saliency Maps Though saliency maps may not relate directly to semantic concepts, they may still be an effective tool for exploring hypotheses about agent behavior. As we show schematically in Figure 2b, claims or explanations informed by saliency maps have three components: semantic concepts, saliency, and behavior. Recall that our survey indicates that researchers often attempt to infer aspects of the network's learned representations from saliency patterns. Let X be a subset of the semantic concepts that can be inferred from the input image. Let B represent behavior, or aggregate actions, over temporally extended sequences of frames, and let R be a representation that is a function of some pixels that the agent learns during training.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 To create scientific claims from saliency maps, we recommend using a relatively standard pattern which facilitates objectivity and falsifiability: {concept set X} is salient =⇒ agent has learned {representation R} resulting in {behavior B}. Consider the Breakout brick reflection example presented in Section 2. The hypothesis introduced ("the agent has learned to aim at tunnels") can be reformulated as: bricks are salient =⇒ agent has learned to identify a partially complete tunnel resulting in maneuvering the paddle to hit the ball toward that region. Stating hypotheses in this format implies falsifiable claims amenable to empirical analysis.

Section Title: Counterfactual Evaluation of Claims
  Counterfactual Evaluation of Claims As indicated in  Figure 2 , the learned representation and pixel input share a relationship with saliency maps generated over a sequence of frames. Given that the representation learned is static, the relationship between the learned representation and saliency should be invariant under different manipulations of pixel input. We use this property to assess saliency under counterfactual conditions. We generate counterfactual conditions by intervening on the RL environment. Prior work has fo- cused on manipulating the pixel input. However, this does not modify the underlying latent game state. Instead, we intervene directly on game state. In the do-calculus formalism (Pearl, 2000), this shifts the intervention node in Figure 2a to game state, which leaves the generative process F of the pixel image intact. We employ TOYBOX, a set of fully parameterized implementation of Atari games (Foley et al., 2018), to generate interventional data under counterfactual conditions. The interventions are de- pendent on the mapping between semantic concepts and learned representations in the hypotheses. Given a mapping between concept set X and a learned representation R, any intervention would require meaningfully manipulating the state in which X resides to assess the saliency on X un- der the semantic treatment applied. Saliency on x ∈ X is defined as the average saliency over a bounding-box 1 around x. Since the learned policies should be semantically invariant under manipulations of the RL environ- ment, by intervening on state, we can verify whether the counterfactual states produce expected patterns of saliency on the associated concept set X. If the counterfactual saliency maps reflect sim- ilar saliency patterns, this provides stronger evidence that the observed saliency indicates the agent has learned representation R corresponding to semantic concept set X.

Section Title: EVALUATION OF HYPOTHESES ON AGENT BEHAVIOR
  EVALUATION OF HYPOTHESES ON AGENT BEHAVIOR We conduct three case studies to evaluate hypotheses about the relationship between semantic con- cepts and semantic processes formed from saliency maps. Each case study uses observed saliency maps to identify hypotheses in the format described in Section 4. The hypotheses were generated by watching multiple episodes and noting atypical, interesting or popular behaviors from saliency maps. In each case study, we produce Jacobian, perturbation and object saliency maps from the same set of counterfactual states. We include examples of each map in Appendix A. Using TOYBOX al- lows us to produce counterfactual states and to generate saliency maps in these altered states. The case studies are conducted on two Atari games, Breakout and Amidar. 2 The deterministic nature of both games allows some stability in the way we interpret the network's action selection. Each map is produced from an agent trained with A2C (Mnih et al., 2016) using a CNN-based (Mnih et al., 2015) OpenAI Baselines implementation (Dhariwal et al., 2017) with default hyperparameters (see Appendix B for more details). Our choice of model is arbitrary. The emphasis of this work is on methods of explanation, not the explanations themselves. Case Study 1: Breakout Brick Translation. Here we evaluate the behavior from Section 2: Hypothesis 1: {bricks} are salient =⇒ agent has learned to {identify a partially complete tunnel} resulting in {maneuvering the paddle to hit the ball toward that region}. To evaluate this hypothesis, we intervene on the state by translating the brick configurations horizon- tally. Because the semantic concepts relating to the tunnel are preserved under translation, we expect salience will be nearly invariant to the horizontal translation of the brick configuration. Figure 3a depicts saliency after intervention. Salience on the tunnel is less pronounced under left translation, and more pronounced under right translation. Since the paddle appears on the right, we additionally move the ball and paddle to the far left (Figure 3b).

Section Title: Conclusion
  Conclusion Temporal association (e.g. formation of a tunnel followed by higher saliency) does not generally imply causal dependence. In this case, tunnel formation and salience appear to be confounded by location or, at least, the dependence of these phenomena are highly dependent on location.

Section Title: Case Study 2: Amidar Score
  Case Study 2: Amidar Score Amidar is a Pac-Man-like game in which an agent attempts to completely traverse a series of passages while avoiding enemies. The yellow sprite that indicates the location of the agent is almost always salient in Amidar. Surprisingly, the displayed score is often as salient as the yellow sprite throughout the episode with varying levels of intensity. This can lead to multiple hypotheses about the agent's learned representation: (1) the agent has learned to associate increasing score with higher reward; (2) due to the deterministic nature of Amidar, the agent has created a lookup table that associates its score and its actions. We can summarize these as follows: Hypothesis 2: {score} is salient =⇒ agent has learned to {use score as a guide to traverse the board} resulting in {successfully following similar paths in games}. To evaluate hypothesis 2, we designed four interventions on score: • intermittent reset: modify the score to 0 every x ∈ [5, 20] timesteps. • random varying: modify the score to a random number between [1,200] every x ∈ [5, 20] timesteps. • fixed: select a score from [0,200] and fix it for the whole game. • decremented: modify score to be 3000 initially and decrement score by d ∈ [1, 20] at every timestep. Figures 4a and 4b show the result of intervening on displayed score on reward and saliency inten- sity, measured as the average saliency over a 25x15 bounding box, respectively for the first 1000 timesteps of an episode. The mean is calculated over 50 samples. If an agent died before 1000 timesteps, the last reward was extended for the remainder of the timesteps and saliency was set to zero. Using reward as a summary of agent behavior, different interventions on score produce different agent behavior. Total accumulated reward differs over time for all interventions, typically due to early agent death. However, salience intensity patterns of all interventions follow the original trajec- tory very closely. Different interventions on displayed score cause differing degrees of degraded per- formance (Figure 4a) despite producing similar saliency maps (Figure 4b), indicating that agent be- havior is underdetermined by salience. Specifically, the salience intensity patterns are similar for the control, fixed, and decremented scores, while the non-ordered score interventions result in degraded performance. Figure 4c indicates only very weak correlations between the difference-in-reward and difference-in-saliency-under-intervention as compared to the original trajectory. Correlation coef- ficients range from 0.041 to 0.274, yielding insignificant p-values for all but one intervention. See full results in Appendix E.1, Table 6. Similar trends are noted for Jacobian and perturbation saliency methods in Appendix E.1.

Section Title: Conclusion
  Conclusion The existence of a high correlation between two processes (e.g., incrementing score and persistence of saliency) does not imply causation. Interventions can be useful in identifying the common cause leading to the high correlation.

Section Title: Case Study 3: Amidar Enemy Distance
  Case Study 3: Amidar Enemy Distance Enemies are salient in Amidar at varying times. From visual inspection, we observe that enemies close to the player tend to have higher saliency. Accord- ingly, we generate the following hypothesis: Hypothesis 3: {enemy} is salient =⇒ agent has learned to {identify enemies close to it} resulting in {successful avoidance of enemy collision}. Without directly intervening on the game state, we can first identify whether the player-enemy dis- tance and enemy saliency is correlated using observational data. We collect 1000 frames of an episode of Amidar and record the Manhattan distance between the midpoints of the player and en- emies, represented by 7x7 bounding boxes, along with the object salience of each enemy. Figure 5a shows the distance of each enemy to the player over time with saliency intensity represented by the shaded region. Figure 5b shows the correlation between the distance to each enemy and the corresponding saliency. Correlation coefficients and significance values are reported in  Table 3 . It is clear that there is no correlation between saliency and distance of each enemy to the player. Given that statistical dependence is almost always a necessary pre-condition for causation, we expect that there will not be any causal dependence. To further examine this, we intervene on enemy positions of salient enemies at each timestep by moving the enemy closer and farther away from the player. Figure 5c contains these results. Given Hypothesis 3, we would expect to see an increasing trend in saliency for enemies closer to the player. However, the size of the effect is close to 0 (see  Table 3 ). In addition, we find no correlation in the enemy distance experiments for the Jacobian or perturbation saliency methods (included in Appendix E.2).

Section Title: Conclusion
  Conclusion Spurious correlations, or misinterpretations of existing correlation, can occur between two processes (e.g. correlation between player-enemy distance and saliency), and human observers are susceptible to identifying spurious correlations (Simon, 1954). Spurious correlations can some- times be identified from observational analysis without requiring interventional analysis.

Section Title: DISCUSSION AND RELATED WORK
  DISCUSSION AND RELATED WORK Thinking counterfactually about the explanations generated from saliency maps facilitates empirical evaluation of those explanations. The experiments above show some of the difficulties in drawing conclusions from saliency maps. These include the tendency of human observers to incorrectly infer association between observed processes, the potential for experimental evidence to contradict seem- ingly obvious observational conclusions, and the challenges of potential confounding in temporal processes. One of the main conclusions from this evaluation is that saliency maps are an exploratory tool rather than an explanatory tool for evaluating agent behavior in deep RL. Saliency maps alone cannot be reliably used to infer explanations and instead require other supporting tools. This can include combining evidence from saliency maps with other explanation methods or employing a more experimental approach to evaluation of saliency maps such as the approach demonstrated in the case studies above. The framework for generating falsifiable hypotheses suggested in Section 4 can assist with designing more specific and falsifiable explanations. The distinction between the components of an explana- tion, particularly the semantic concept set X, learned representation R and observed behavior B, can further assist in experimental evaluation. Note that the semantic space devised by an agent might be quite different from the semantic space given by the latent factors of the environment. It is cru- cial to note that this mismatch is one aspect of what plays out when researchers create hypotheses about agent behavior, and the methodology we provide in this work demonstrates how to evaluate hypotheses that reflect that mismatch.

Section Title: Generalization of Proposed Methodology
  Generalization of Proposed Methodology The methodology presented in this work can be easily extended to other vision-based domains in deep RL. Particularly, the framework of the graphical model introduced in Figure 2a applies to all domains where the input to the network is image data. An extended version of the model for Breakout can be found in Appendix 7. We propose intervention-based experimentation as a primary tool to evaluate the hypotheses gener- ated from saliency maps. Yet, alternative methods can identify a false hypothesis even earlier. For instance, evaluating statistical dependence alone can provide strong evidence against causal depen- dence (e.g., Case Study 3). In this work, we employ a particularly capable simulation environment (TOYBOX). However, limited forms of evaluation may be possible in non-intervenable environ- ments, though they may be more tedious to implement. For instance, each of the interventions conducted in Case Study 1 can be produced in an observation-only environment by manipulating the pixel input (Brunelli, 2009; Chalupka et al., 2015). Developing more experimental systems for evaluating explanations is an open area of research. This work analyzes explanations generated from feed-forward deep RL agents. However, the pro- posed methodology is not model dependent, and aspects of the approach will carry over to recurrent deep RL agents. The proposed methodology would not work well for repeated interventions on recurrent deep RL agents due to their capacity for memorization.

Section Title: Explanations in Deep RL
  Explanations in Deep RL Prior work has introduced alternatives to the use of saliency maps to support explanation of deep RL agents. Some of these methods also use counterfactual reasoning to develop explanations. TOYBOX was developed to support experimental evaluation and behavioral tests of deep RL models (Tosch et al., 2019). Olson et al. (2019) use a generative deep learning architecture to produce counterfactual states resulting in the agent taking a different action. Others have proposed alternative methods for developing semantically meaningful interpretations of agent behavior. Juozapaitis et al. (2019) use reward decomposition to attribute policy behaviors accord- ing to semantically meaningful components of reward. Verma et al. (2018) use domain-specific languages for policy representation, allowing for human-readable policy descriptions.

Section Title: Evaluation and Critiques of Saliency Maps
  Evaluation and Critiques of Saliency Maps Prior work in the deep network literature has eval- uated and critiqued saliency maps. Kindermans et al. (2019) and Adebayo et al. (2018) demonstrate the utility of saliency maps by adding random variance in input. Seo et al. (2018) provide a theoret- ical justification of saliency and hypothesize that there exists a correlation between gradients-based saliency methods and model interpretation. Samek et al. (2017) and Hooker et al. (2019) present evaluations of existing saliency methods for image classification.

Section Title: CONCLUSIONS
  CONCLUSIONS We conduct a survey of uses of saliency maps, propose a methodology to evaluate saliency maps, and examine the extent to which the agent's learned representations can be inferred from saliency maps. We investigate how well the pixel-level inferences of saliency maps correspond to the semantic concept-level inferences of human-level interventions. Our results show saliency maps cannot be trusted to reflect causal relationships between semantic concepts and agent behavior. We recommend saliency maps to be used as an exploratory tool, not explanatory tool.

```
