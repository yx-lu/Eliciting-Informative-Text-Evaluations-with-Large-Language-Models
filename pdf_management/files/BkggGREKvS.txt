Title:
```
Under review as a conference paper at ICLR 2020
```
Abstract:
```
A central challenge in multi-agent reinforcement learning is the induction of coordination between agents of a team. In this work, we investigate how to promote inter-agent coordination using policy regularization and discuss two possible avenues respectively based on inter-agent modelling and synchronized sub-policy selection. We test each approach in four challenging continuous control tasks with sparse rewards and compare them against three baselines including MADDPG, a state-of-the-art multi-agent reinforcement learning algorithm. To ensure a fair comparison, we rely on a thorough hyper-parameter selection and training methodology that allows a fixed hyper-parameter search budget for each algorithm and environment. We consequently assess both the hyper- parameter sensitivity, sample-efficiency and asymptotic performance of each learning method. Our experiments show that the proposed methods lead to significant improvements on cooperative problems. We further analyse the effects of the proposed regularizations on the behaviors learned by the agents.
```

Figures/Tables Captions:
```
Figure 1: Illustration of TeamReg with two agents. Each agent's policy is equipped with additional heads that are trained to predict other agents' actions and every agent is regular- ized to produce actions that its team- mates correctly predict. Note that the method is depicted for agent 1 only to avoid cluttering.
Figure 2: Illustration of CoachReg with two agents. A central model, the coach, takes all agents' observations as input and outputs the current mode (policy mask). Agents are regular- ized to predict the same mask from their local observations only and op- timize the corresponding sub-policy.
Figure 3: Multi-agent tasks used in this work. (a) SPREAD: Agents must spread out and cover a set of landmarks. (b) BOUNCE: Two agents are linked together by a spring and must position themselves so that the falling black ball bounces towards a target. (c) COMPROMISE: Two linked agents must compete or cooperate to reach their own assigned landmark. (d) CHASE: Two agents chase a (non-learning) prey (turquoise) that moves w.r.t repulsion forces from predators and walls.
Figure 4: Learning curves (mean return over agents) for all algorithms on all four environments. Solid lines are the mean and envelopes are the Standard Error (SE) across the 10 training seeds.
Figure 5: Learning curves (mean return over agents) for the ablated algorithms on all environments. Solid lines are the mean and envelopes are the Standard Error (SE) across the 10 training seeds.
Figure 6: Average performance difference (∆ perf ) between the two agents in COM- PROMISE for each 150 runs of the hyper- parameter searches (left). All occurrences of abnormally high performance difference are associated with high values of λ 2 (right).
Figure 7: Comparison between enabling and disabling the regularizing weights λ 1 and λ 2 for MADDPG+TeamReg on the SPREAD environment. Values are averaged over the 3 agents and over the 3 seeds used in the hyper-parameter exploration.
Figure 8: Visualization of two different BOUNCE evaluation episodes. Note that here, the agents' colors represent their chosen policy mask. Agents have learned to synchronously identify two distinct situations and act accordingly. The coach's masks (not used at evaluation time) are displayed with the timestep at the bottom of each frame.
Figure 9: (a) Entropy of the policy mask distributions for each task, averaged over agents and training seeds. H max,k is the entropy of a k- CUD. (b) Hamming Proxim- ity between the policy mask sequence of each agent aver- aged across agent pairs and seeds. rand k stands for agents independently sampling their masks from k-CUD. Error bars are SE across seeds.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Multi-Agent Reinforcement Learning (MARL) refers to the task of training an agent to maximize its expected return by interacting with an environment that contains other learning agents. It represents a challenging branch of Reinforcement Learning (RL) with interesting developments in recent years ( Hernandez-Leal et al., 2018 ). A popular framework for MARL is the use of a Centralized Training and a Decentralized Execution (CTDE) procedure ( Lowe et al., 2017 ;  Foerster et al., 2018 ;  Iqbal & Sha, 2019 ;  Foerster et al., 2019 ;  Rashid et al., 2018 ). It is typically implemented by training critics that approximate the value of the joint observations and actions, which are used to train actors restricted to the observation of a single agent. Such critics, if exposed to coordinated joint actions leading to high returns, can steer the agents' policies toward these highly rewarding behaviors. However, these approaches depend on the agents luckily stumbling on these actions in order to grasp their benefit. Thus, it might fail in scenarios where coordination is unlikely to occur by chance. We hypothesize that in such scenarios, coordination-promoting inductive biases on the policy search could help discover coordinated behaviors more efficiently and supersede task-specific reward shaping and curriculum learning. In this work, we explore two different priors for successful coordination and use these to regularize the learned policies. The first avenue, TeamReg, assumes that an agent must be able to predict the behavior of its teammates in order to coordinate with them. The second, CoachReg, supposes that coordinating agents individually recognize different situations and synchronously use different sub- policies to react to them. In the following sections we show how to derive practical regularization terms from these premises and meticulously evaluate them 1 . Our contributions are twofold. First, we propose two novel approaches that aim at promoting coordination in multi-agent systems. Our methods augment CTDE MARL algorithms with additional multi-agent objectives that act as regularizers and are optimized jointly with the main return-maximization objective. Second, we design two new sparse-reward cooperative tasks in the Under review as a conference paper at ICLR 2020 multi-agent particle environment ( Mordatch & Abbeel, 2018 ). We use them along with two standard multi-agent tasks to present a detailed evaluation of our approaches against three different baselines. Finally, we validate our methods' key components by performing an ablation study. Our experiments suggest that our TeamReg objective provides a dense learning signal that helps to guide the policy towards coordination in the absence of external reward, eventually leading it to the discovery of high performing team strategies in a number of cooperative tasks. Similarly, by enforcing synchronous sub-policy selections, CoachReg enables to fine-tune a sub-behavior for each recognized situation yielding significant improvements on the overall performance.

Section Title: BACKGROUND
  BACKGROUND

Section Title: MARKOV GAMES
  MARKOV GAMES In this work we consider the framework of Markov Games ( Littman, 1994 ), a multi-agent extension of Markov Decision Processes (MDPs) with N independent agents. A Markov Game is defined by the tuple S, T , P, {O i , A i , R i } N i=1 . S, T , and P respectively are the set of all possible states, the transition function and the initial state distribution. While these are global properties of the environment, O i , A i and R i are individually defined for each agent i. They are respectively the observation functions, the sets of all possible actions and the reward functions. At each time-step t, the global state of the environment is given by s t ∈ S and every agent's individual action vector is denoted by a i t ∈ A i . To select their action, each agent i has only access to its own observation vector o i t which is extracted by its observation function O i from the global state s t . The initial global state s 0 is sampled from the initial state distribution P : S → [0, 1] and the next states of the environment s t+1 are sampled from the probability distribution over the possible next states given by the transition function T : S × S × A 1 × ... × A N → [0, 1]. Finally, at each time-step, each agent receives an individual scalar reward r i t from its reward function R i : S × S × A 1 × ... × A N → R. Agents aim at maximizing their expected discounted return E T t=0 γ t r i t over the time horizon T , where γ ∈ [0, 1] is a discount factor.

Section Title: MULTI-AGENT DEEP DETERMINISTIC POLICY GRADIENT (MADDPG)
  MULTI-AGENT DEEP DETERMINISTIC POLICY GRADIENT (MADDPG) MADDPG ( Lowe et al., 2017 ) is an adaptation of the Deep Deterministic Policy Gradient algorithm (DDPG) ( Lillicrap et al., 2015 ) to the multi-agent setting. It allows the training of cooperating and competing decentralized policies through the use of a centralized training procedure. In this framework, each agent i possesses its own deterministic policy µ i for action selection and critic Q i for state-action value estimation, which are respectively parametrized by θ i and φ i . All parametric models are trained off-policy from previous transitions ζ t := (o t , a t , r t , o t+1 ) uniformly sampled from a replay buffer D. Note that o t := [o 1 t , ..., o N t ] is the joint observation vector and a t := [a 1 t , ..., a N t ] is the joint action vector, obtained by concatenating the individual observation vectors o i t and action vectors a i t of all N agents. Each centralized critic is trained to estimate the expected return for a particular agent i using the Deep Q-Network (DQN) ( Mnih et al., 2015 ) loss: For a given set of weights w, we define its target counterpartw, updated fromw ← τ w + (1 − τ )w where τ is a hyper-parameter. Each policy is updated to maximize the expected discounted return of the corresponding agent i : By taking into account all agents' observation-action pairs when guiding an agent's policy, the value- functions are trained in a centralized, stationary environment, despite taking place in a multi-agent setting. In addition, this mechanism can allow to implicitly learn coordinated strategies that can then be deployed in a decentralized way. However, this procedure does not encourage the discovery of coordinated strategies since high-reward behaviors have to be randomly experienced through unguided exploration. This work aims at alleviating this limitation.

Section Title: RELATED WORK
  RELATED WORK Many works in MARL consider explicit communication channels between the agents and distinguish between communicative actions (e.g. broadcasting a given message) and physical actions (e.g. moving in a given direction) ( Foerster et al., 2016 ;  Mordatch & Abbeel, 2018 ;  Lazaridou et al., 2016 ). Consequently, they often focus on the emergence of language, considering tasks where the agents must discover a common communication protocol in order to succeed. Deriving a successful communication protocol can already be seen as coordination in the communicative action space and can enable, to some extent, successful coordination in the physical action space ( Ahilan & Dayan, 2019 ). Yet, explicit communication is not a necessary condition for coordination as agents can rely on physical communication ( Mordatch & Abbeel, 2018 ;  Gupta et al., 2017 ). Approaches to shape RL agents' behaviors with respect to other agents have also been explored.  Strouse et al. (2018)  use the mutual information between the agent's policy and a goal-independent policy to shape the agent's behavior towards hiding or spelling out its current goal. However, this approach is only applicable for tasks with an explicit goal representation and is not specifically intended for coordination.  Jaques et al. (2019)  approximate the direct causal effect between agent's actions and use it as an intrinsic reward to encourage social empowerment. This approximation relies on each agent learning a model of other agents' policies to predict its effect on them. In general, this type of behavior prediction can be referred to as agent modelling (or opponent modelling) and has been used in previous work to enrich representations ( Hernandez-Leal et al., 2019 ), to stabilise the learning dynamics ( He et al., 2016 ) or to classify the opponent's play style ( Schadd et al., 2007 ). In our work, agent modelling is extended to derive a novel incentive toward team-predictable behaviors. Finally,  Barton et al. (2018)  propose convergent cross mapping (CCM) to measure the degree of effective coordination between two agents. Although this may represent an interesting avenue for behavior analysis, it fails to provide a tool for effectively enforcing coordination as CCM must be computed over long time series which makes it an impractical learning signal for single- step temporal difference methods. In this work, we design two coordination-driven multi-agent approaches that do not rely on the existence of explicit communication channels and allow to carry the learned coordinated behaviors at test time, when all agents act in a decentralized fashion. Intuitively, coordination can be defined as an agent's behav- ior being informed by the one of another agent, i.e. structure in the agents' interactions. Namely, a team where agents act independently of one another would not be coordinated. To promote such structure, our proposed methods rely on team-objectives as regularizers of the common policy gra- dient update. In this regard, our approach is closely related to General Value Functions and Auxiliary tasks ( Sutton & Barto, 2018 ) used in Deep RL to learn efficient representa- tions ( Jaderberg et al., 2019 ). However, this work's novelty lies in the explicit bias of agents' policy towards either pre- dictability for their teammates or synchronous sub-policy selection. Pseudocodes of our implementations are provided in Appendix C (see Algorithms 1 and 2).

Section Title: TEAM REGULARIZATION
  TEAM REGULARIZATION The structure of coordinated interactions can be leveraged to attain a certain degree of predictability of one agent's behavior with respect to its teammate(s). We hypothesize that the reciprocal also holds i.e. that promoting agents' predictability could foster such team structure and lead to more coordinated behaviors. This assumption is cast into the decentralized framework by training agents to predict their teammates' actions given only their own observation. For continuous control, the loss is defined as the Mean Squared Error (MSE) between the predicted Under review as a conference paper at ICLR 2020 and true actions of the teammates, yielding a teammate-modelling secondary objective. While the previous work of  Hernandez-Leal et al. (2019)  focus on stationary, non-learning teammates and exclusively use this approach to learn richer internal representations, we propose to extend this objective to drive the teammates' behaviors closer to the prediction by leveraging a differentiable action selection mechanism. We call team-spirit this novel objective J i,j T S between agents i and j: whereμ i,j is the policy head of agent i trying to predict the action of agent j. The total gradient for a given agent i becomes: where λ 1 and λ 2 are hyper-parameters that respectively weight how well an agent should predict its teammates' actions, and how predictable an agent should be for its teammates. We call TeamReg this dual regularization from team-spirit objectives.  Figure 1  summarizes these interactions.

Section Title: COACH REGULARIZATION
  COACH REGULARIZATION In order to foster structured agents interactions, this method aims at teaching the agents to recognize different situations and synchronously select corresponding sub-behaviors.

Section Title: SUB-POLICY SELECTION
  SUB-POLICY SELECTION Firstly, to enable explicit sub-behavior selection, we pro- pose policy masks that modulate the agents' policy. A pol- icy mask u j is a one-hot vector of size K with its j th compo- nent set to one. In practice, we use policy masks to perform dropout ( Srivastava et al., 2014 ) in a structured manner oñ h 1 ∈ R M , the pre-activations of the first hidden layer h 1 of the policy network π. To do so, we construct the vector u j , which is the concatenation of C copies of u j , in order to reach the dimensionality M = C * K. The element-wise product u j h 1 is then performed and only the units ofh 1 at indices m modulo K = j are kept for m = 0, . . . , M −1. In our contribution, each agent i generates e i t , its own policy mask, from its observation o i t . Here, a simple linear layer l i is used to produce a categorical probability distribution p i (e i t |o i t ) from which the one-hot vector is sampled: To our knowledge, while this method draws similarity to the options and hierarchical frameworks ( Sutton & Barto, 2018 ;  Ahilan & Dayan, 2019 ) and to policy dropout for exploration ( Xie et al., 2018 ), it is the first to introduce an agent induced modulation of the policy network by a structured dropout that is decentralized at evaluation and without an explicit communication channel. Although the policy masking mechanism enables the agent to swiftly switch between sub-policies it does not encourage the agents to synchronously modulate their behavior.

Section Title: SYNCHRONOUS SUB-POLICY SELECTION
  SYNCHRONOUS SUB-POLICY SELECTION To promote synchronization we introduce the coach entity, parametrized by ψ, which learns to produce policy-masks e c t from the joint observations, i.e. p c (e i c |o t ; ψ). The coach is used at training Under review as a conference paper at ICLR 2020 time only and drives the agents toward synchronously selecting the same behavior mask. In other words, the coach is trained to output masks that (1) yield high returns when used by the agents and (2) are predictable by the agents. Similarly, each agent is regularized so that (1) its private mask matches the coach's mask and (2) it derives efficient behavior when using the coach's mask. At evaluation time, the coach is removed and the agents only rely on their own policy masks. The policy gradient loss when agent i is provided with the coach's mask is given by: The difference between the mask of agent i and the coach's one is measured from the Kullback-Leibler divergence: The total gradient for agent i is: with λ 1 and λ 2 the regularization coefficients. Similarly, the coach is trained with the following dual objective, weighted by the λ 3 coefficient: In order to propagate gradients through the sampled policy mask we reparametrized the categorical distribution using the Gumbel-softmax trick ( Jang et al., 2017 ) with a temperature of 1. We call this coordinated sub-policy selection regularization CoachReg and illustrate it in  Figure 2 .

Section Title: TRAINING ENVIRONMENTS
  TRAINING ENVIRONMENTS All of our tasks are based on the OpenAI multi-agent particle environments ( Mordatch & Abbeel, 2018 ). SPREAD and CHASE were introduced by ( Lowe et al., 2017 ). We use SPREAD as is but with sparse rewards only. CHASE is modified with a prey controlled by repulsion forces and only the predators are learnable, as we wish to focus on coordination in cooperative tasks. Finally we introduce COMPROMISE and BOUNCE where agents are explicitly tied together. While non- zero return can be achieved in these tasks by selfish agents, they all benefit from coordinated strategies and optimal return can only be achieved by agents working closely together.  Figure 3  presents visualizations and a brief description of all four tasks. A detailed description is provided in Appendix A. In all tasks, agents receive as observation their own global position and velocity as well as the relative position of other entities. Note that work showcasing experiments on this environment often use discrete action spaces and (dense) reward shaping (e.g. the proximity with the objective) ( Iqbal & Sha, 2019 ;  Lowe et al., 2017 ;  Jiang & Lu, 2018 ). However, in our experiments, agents learn with continuous action spaces and from sparse rewards.

Section Title: RESULTS AND DISCUSSION
  RESULTS AND DISCUSSION The proposed methods offer a way to incorporate new inductive biases in CTDE multi-agent policy search algorithms. In this work, we evaluate them by extending MADDPG, a state of the art algorithm widely used in the MARL litterature. We compare against vanilla MADDPG as well as two of its variations in the four cooperative multi-agent tasks described in Section 5. The first variation (DDPG) is the single-agent counterpart of MADDPG (decentralized training). The second (MADDPG + sharing) shares the policy and value-function models across agents. To offer a fair comparison between all methods, the hyper-parameter search routine is the same for each algorithm and environment (see Appendix D.1). For each search-experiment (one per algorithm per environment), 50 randomly sampled hyper-parameter configurations each using 3 training seeds (total of 150 runs) are used to train the models for 15, 000 episodes. For each algorithm-environment pair, we then select the best hyper-parameter configuration for the final comparison and retrain them on 10 seeds for twice as long. We give more details about the training setup and model selection in Appendix B and D.2. The results of the hyperparameter searches are given in Appendix D.5.

Section Title: ASYMPTOTIC PERFORMANCE
  ASYMPTOTIC PERFORMANCE From the average learning curves reported in  Figure 4  we observe that CoachReg significantly improves performance on three environments (SPREAD, BOUNCE and COMPROMISE) and performs on par with the baselines on the last one (CHASE). The same can be said for TeamReg, except on COMPROMISE, the only task with an adversarial component, where it significantly underperforms compared to the other algorithms. We discuss this specific case in Section 6.3. Finally, parameter sharing is the best performing choice on CHASE, yet this superiority is restricted to this task where the optimal play is to move symmetrically and squeeze the prey into a corner.

Section Title: ABLATION STUDY
  ABLATION STUDY Additionally to our two proposed algorithms and the three baselines, we present results for two ablated versions of our methods. The first ablation (MADDPG + agent modelling) is similar to TeamReg but with λ 2 = 0, which results in only enforcing agent modelling (i.e. agent predictability is not encouraged). The second ablation (MADDPG + policy mask) is structurally equivalent to CoachReg, but with λ 1,2,3 = 0, which means that agents still predict and apply a mask to their own policy, but synchronicity is not encouraged. Figure 12 and 13 (Appendix D.6) present the results of the corresponding hyper-parameter search and  Figure 5  shows the learning curves for our full regularization approaches, their respective ablated versions and MADDPG. The use of unsynchronized policy masks might result in swift and unpredictable behavioral changes and make it difficult for agents to perform together and coordinate. Experimentally, "MADDPG + policy mask" performs similarly or worse than MADDPG on all but one environment, and never outperforms the full CoachReg approach. However, policy masks alone seem enough to succeed on SPREAD, which is about selecting a landmark from a set. Regarding "MADDPG + agent modelling", it does not drastically improve on MADDPG apart from on the SPREAD environment, and the full TeamReg approach shows improvement over its ablated version except on the COMPROMISE task, which we discuss in Section 6.3.

Section Title: EFFECTS OF ENFORCING PREDICTABLE BEHAVIOR
  EFFECTS OF ENFORCING PREDICTABLE BEHAVIOR First, we investigate the reason for TeamReg's poor performance on COMPROMISE. Then, we analyse how TeamReg might be helpful in other environments. COMPROMISE is the only task with a competitive component (and the only one in which agents do not share their rewards). The two agents being linked, a good policy has both agents reach their landmark successively (maybe by simply having both agents navigate towards the closest landmark). However, if one agent never reaches for its landmark, the optimal strategy for the other one becomes to drag it around and always go for its own, leading to a strong imbalance in the return cumulated by both agents. While this scenario very rarely occurs for the other algorithms, we found TeamReg to often lead to such domination cases (see Figure 14 in Appendix E).  Figure 6  depicts the agents' performance difference for every 150 runs of the hyperparameter search for TeamReg and the baselines, and shows that (1) TeamReg is the only algorithm that does lead to large imbalances in performance between the two agents and (2) that these cases where one agent becomes dominant are all associated with high values of λ 2 , which drives the agents to behave in a predictable fashion to one another. However, the dominated agent eventually gets exposed more and more to sparse reward gathered by being dragged (by chance) onto its own landmark, picks up the goal of the task and starts pulling in its own direction, which causes the average return over agents to drop as we see in  Figure 4 . This experiment demonstrates that using a predictability-based team-regularization Under review as a conference paper at ICLR 2020 in a competitive task can be harmful; quite understandably, you might not want to optimize an objective that aims at making your behavior predictable to your opponent. On SPREAD and BOUNCE, TeamReg significantly improves the performance over the baselines. We aim to analyze here the effects of λ 2 on cooperative tasks and investigate if it does make the agent modelling task more successful (by encouraging the agent to be predictable). To this end, we compare the best performing hyper-parameter configuration for TeamReg on the SPREAD environment with its ablated versions. The average return and team-spirit loss defined in Section 4.1 are presented in  Figure 7  for these three experiments. Initially, due to the weight initialization, the predicted and actual actions both have relatively small norms yielding small values of team-spirit loss. As training goes on (∼1000 episodes), the norms of the action-vector increase and the regularization loss becomes more important. As expected, λ 2 OF F |λ 1 OF F leads to the highest team-spirit loss as it is not trained to predict the actions of other agents correctly. When using only the agent-modelling objective (λ 1 ON ), the agents significantly decrease the team-spirit loss, but it never reaches values as low as when using the full TeamReg objective. Finally, when also pushing agents to be predictable (λ 2 ON ), the agents best predict each others' actions and performance is also improved. We also notice that the team-spirit loss increases when performance starts to improve i.e. when agents start to master the task (∼8000 episodes). Indeed, once the reward maximisation signals becomes stronger, the relative importance of the second task is reduced. We hypothesize that being predictable with respect to one-another may push agents to explore in a more structured and informed manner in the absence of reward signal, as similarly pursued by intrinsic motivation approaches ( Chentanez et al., 2005 ).

Section Title: ANALYSIS OF SYNCHRONOUS SUB-POLICY SELECTION
  ANALYSIS OF SYNCHRONOUS SUB-POLICY SELECTION (a) The ball is on the left side of the target, agents both select the purple policy mask (b) The ball is on the right side of the target, agents both select the green policy mask In this section we aim at experimentally verifying that CoachReg yields the desired behavior: agents synchronously alternating be- tween varied sub-policies. A special attention is given when the sub-policies are interpretable. To this end we record and analyze the agents' policy masks on 100 different episodes for each task. From the collected masks, we reconstructed the empirical mask distribution of each agent (see Figure 15 in Appendix F.1) whose entropy provides an indication of the mask diversity used by a given agent.  Figure 9 (a)  shows the mean entropy for each environment compared to the entropy of Categorical Uniform Distributions of size k (k-CUD). It shows that, on all the environments, agents use at least two distinct masks by having non-zero entropy. In addition, agents tend to alternate between masks with more variety (close to uniformly switching between 3 masks) on SPREAD (where there are 3 agents and 3 goals) than on the other environments (comprised of 2 agents). To test if agents are synchronously selecting the same policy mask at test time (without a coach), we compute the Hamming proximity between the agents' mask sequences with 1 − D h where D h is the Hamming distance, i.e. the number of timesteps where the two sequences are different divided by the total number of timesteps. From  Figure 9 (b)  we observe that agents are producing similar mask sequences. Notably, their mask sequences are significantly more similar that the ones of two agent randomly choosing between two masks at each timestep. Finally, we observe that some settings result in the agents coming up with interesting strategies, like the one depicted in  Figure 8  where the agents alternate between two sub- policies depending on the position of the target. More cases where the agents change sub-policies during an episode are presented in Appendix F.1. These results indicate that, in addition to improving the performance on coordination tasks, CoachReg indeed yields the expected behaviors. An interesting following work would be to use entropy regularization to increase the mask usage variety and mutual information to further disentangle sub-policies.

Section Title: ROBUSTNESS TO HYPER-PARAMETERS
  ROBUSTNESS TO HYPER-PARAMETERS Stability across hyper-parameter configurations is a recurring challenge in Deep RL. The average performance for each sampled configuration allow to empirically evaluate the robustness of an algorithm w.r.t. its hyper-parameters. We share the full results of the hyper-parameter searches in Figures 10, 11, 12 and 13 in Appendix D.5 and D.6. Figure 11 shows that while most algorithms can perform reasonably well with the correct configuration, our proposed coordination regularizers can improve robustness to hyper-parameter despite the fact that they have more hyper-parameters to search over. Such robustness can be of great value with limited computational budgets.

Section Title: ROBUSTNESS TO THE NUMBER OF AGENTS
  ROBUSTNESS TO THE NUMBER OF AGENTS To assess how the proposed methods perform when using an greater number of agents, we present additional experiments for which the number of agents in the SPREAD task is gradually increased from three to six agents. The results presented in Figure 18 (Appendix G) show that the performance benefits provided by our methods hold when the number of agents is increased. Strikingly, we also note how quickly the performance of all methods drop when the number of agents rises. Indeed, with each new agent, the coordination problem becomes more and more difficult, and that might explain why our methods that promote coordination maintain a higher degree of performance in the case of 4 agents. Nonetheless, estimating the value function also becomes increasingly challenging as the input space grows exponentially with the number of agents. In the sparse reward setting, the complexity of the task soon becomes too difficult and none of the algorithms is able to solve it with six agents.

Section Title: CONCLUSION
  CONCLUSION In this work we introduced two policy regularization methods to promote multi-agent coordination within the CTDE framework: TeamReg, which is based on inter-agent action predictability and CoachReg that relies on synchronized behavior selection. A thorough empirical evaluation of these methods showed that they significantly improve asymptotic performances on cooperative multi- agent tasks. Interesting avenues for future work would be to study the proposed regularizations on other policy search methods as well as to combine both incentives and investigate how the two coordinating objectives interact. Finally, a limitation of the current formulation is that it relies on single-step metrics, which simplifies off-policy learning but also limits the longer-term coordination opportunities. A promising direction is thus to explore model-based planning approaches to promote long-term multi-agent interactions.
  Source code for the algorithms and environments will be made public upon publication of this work.

```
