Title:
```
Under review as a conference paper at ICLR 2020 SLM LAB: A COMPREHENSIVE BENCHMARK AND MODULAR SOFTWARE FRAMEWORK FOR REPRO- DUCIBLE DEEP REINFORCEMENT LEARNING
```
Abstract:
```
We introduce SLM Lab, a software framework for reproducible reinforcement learning (RL) research. SLM Lab implements a number of popular RL algo- rithms, provides synchronous and asynchronous parallel experiment execution, hyperparameter search, and result analysis. RL algorithms in SLM Lab are imple- mented in a modular way such that differences in algorithm performance can be confidently ascribed to differences between algorithms, not between implemen- tations. In this work we present the design choices behind SLM Lab and use it to produce a comprehensive single-codebase RL algorithm benchmark. In addi- tion, as a consequence of SLM Lab's modular design, we introduce and evaluate a discrete-action variant of the Soft Actor-Critic algorithm (Haarnoja et al., 2018) and a hybrid synchronous/asynchronous training method for RL agents.
```

Figures/Tables Captions:
```
Figure 1: SLM Lab classes and their inheritance structure.
Figure 2: Reinforce, ActorCritic, and PPO class methods in SLM Lab. + indicates that a method is added or overridden in the class.
Figure 3: Experiment organization in SLM Lab. A Session is a single run of an algorithm on an environment. A Trial is a collection of Sessions. An Experiment is a collection of Trials with different algorithms and/or environments.
Figure 4: Average frames per second as number of vector environments and Hogwild! workers are varied. Each setting was trained using PPO with the same hyperparameters on the RoboschoolAnt environment.
Table 1: Episode score at the end of training attained by SLM Lab implementations on discrete- action control problems. Reported episode scores are the average over the last 100 checkpoints, and then averaged over 4 Sessions. A Random baseline with score averaged over 100 episodes is included. Results marked with '*' were trained using the hybrid synchronous/asynchronous version of SAC to parallelize and speed up training time. For SAC, Breakout, Pong and Seaquest were trained for 2M frames instead of 10M frames.
Table 2: Episode score at the end of training attained by SLM Lab implementations on continuous control problems. Reported episode scores are the average over the last 100 checkpoints, and then averaged over 4 Sessions. A Random baseline with score averaged over 100 episodes is included. Results marked with '*' require 50M-100M frames, so we use the hybrid synchronous/asynchronous version of SAC to parallelize and speed up training time.
Table 3: Comparison of RL software libraries. Algorithm acronyms are explained in Supplementary Section A.1. REINFORCE is excluded as are less well-known algorithms. "Benchmark" indicates whether the library reports the performance of their implementations. "Config" indicates whether hyperparameters are specified separately from the implementation and run scripts; "split" indicates that the configuration is divided across multiple files, "partial" indicates that some but not all hyper- parameters are included. "Parallel" denotes whether training for any algorithms can be parallelized. "HPO" denotes support for hyperparameter optimization. "Plot" denotes whether the library pro- vides any methods for visualizing results.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Progress in reinforcement learning (RL) research proceeds only as quickly as researchers can im- plement new algorithms and publish reproducible empirical results. But it is no secret that modern RL algorithms are hard to implement correctly ( Tucker et al., 2018 ), and many empirical results are challenging to reproduce ( Henderson et al., 2017 ;  Islam et al., 2017 ;  Machado et al., 2017 ). Ad- dressing these problems is aided by providing better software tools to the RL research community. In this work we introduce SLM Lab, 1 a software framework for reinforcement learning research designed for reproducibility and extensibility. SLM Lab is the first open source library that includes algorithm implementations, parallelization, hyperparameter search, and experiment analysis in one framework. After presenting the design and organization of SLM Lab, we demonstrate the correctness of its implementations by producing a comprehensive performance benchmark of RL algorithms across 77 environments. To our knowledge this is the largest single-codebase RL algorithm comparison in the literature. In addition, we leverage the modular design of SLM Lab to introduce a variant of the SAC algorithm for use in discrete-action-space environments and a hybrid synchronous/asynchronous training scheme for RL algorithms.

Section Title: SLM LAB
  SLM LAB

Section Title: LIBRARY ORGANIZATION
  LIBRARY ORGANIZATION Modularity is the central design choice in SLM Lab, as depicted in  Figure 1 . Reinforcement learning algorithms in SLM Lab are built around three base classes: • Algorithm: Handles interaction with the environment, implements an action policy, computes the algorithm-specific loss functions, and runs the training step. • Net: Implements the deep networks that serve as the function approximators for an Algorithm.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 • Memory: Provides the data storage and retrieval necessary for training. The deep learning components in SLM Lab are implemented using PyTorch  Paszke et al. (2017) . The Net and Memory classes abstract network training details, data storage, and data retrieval, sim- plifying algorithm implementations. Furthermore, many Algorithm classes are natural extensions of each other. Modular code is critical for deep RL research because many RL algorithms are extensions of other RL algorithms. If two RL algorithms differ in only a small way, but a researcher compares their performance by running a standalone implementation of each algorithm, they cannot know whether differences in algorithm performance are due to meaningful differences between the algorithms or merely due to quirks in the two implementations.  Henderson et al. (2017)  showcase this, demonstrat- ing significant performance differences between different implementations of the same algorithm. Modular code is also important for research progress. It makes it as simple as possible for a re- searcher to implement - and reliably evaluate - new RL algorithms. And for the student of RL, modular code is easier to read and learn from due to its brevity and organization into digestible components. Proximal Policy Optimization (PPO) ( Schulman et al., 2017 ) is a good example. When considered as a stand alone algorithm, PPO has a number of different components. However, it differs from the Actor-Critic algorithm only in how it computes the policy loss, runs the training loop, and by needing to maintain an additional actor network during training.  Figure 2  shows how this similarity is reflected in the SLM Lab implementation of PPO. The result is that the PPO class in SLM Lab has five overridden methods and contains only about 140 lines of code. Implementing it was straightforward once the ActorCritic class was implemented and thoroughly tested. More importantly, we can be sure that the performance difference between Actor-Critic and PPO observed in experiments using SLM Lab, shown below in Section 3, are due to something in the 140 lines of code that differentiate ActorCritic and PPO, and not to other implementation differences. Another example of modularity in SLM Lab is that, thanks to the consistent API shared between Algorithm, Net, and Memory subclasses, synchronous parallelization using vector environments ( Dhariwal et al., 2017 ) can be combined with asynchronous parallelization of an individual RL agent's learning algorithm. This multi-level parallelization is further discussed in Section 3.3, where we demonstrate its performance benefits.

Section Title: EXPERIMENT ORGANIZATION
  EXPERIMENT ORGANIZATION Reinforcement learning algorithms vary greatly in their performance across different environments, hyperparameter settings, and even within a single environment due to inherent randomness. SLM Lab is designed to easily allow users to study all these types of variability. SLM Lab organizes experiments in the following hierarchy, shown in  Figure 3 : 1. Session The lowest level of the SLM Lab experiment framework: a single training run of one agent on one environment with one set of hyperparameters, all with a fixed random seed. 2. Trial A trial consists of multiple Sessions, with the Sessions varying only in the random seed. 3. Experiment Generates different sets of hyperparameters (according to a spec file, see be- low) and runs a Trial for each one. It can be thought of as a study, e.g. "What values of n of A2C n-step returns provide the fastest, most stable solution, if the other variables are held constant?" SLM Lab automatically produces plots for Sessions, Trials, and Experiments for any combination of environments and algorithms. It also logs and tracks metrics during training such as rewards, loss, exploration and entropy variables, model weights and biases, action distributions, frames-per- second and wall-clock time. The metrics are also visualized using TensorBoard ( Abadi et al., 2015 ). Hyperparameter search is implemented using Ray Tune ( Liaw et al., 2018 ), and the results are automatically analyzed and presented hierarchically in increasingly granular detail.

Section Title: REPRODUCIBILITY
  REPRODUCIBILITY The complexity of RL algorithms makes reproducing RL results challenging ( Henderson et al., 2017 ;  Machado et al., 2017 ). Every RL researcher knows the difficulty of trying to reproduce an algorithm from its description in a paper alone. Even if code is published along with RL research results, the key algorithm design choices (or mistakes ( Tucker et al., 2018 )) are often buried in the algorithm implementation and not exposed naturally to the user. In SLM Lab, every configurable hyperparameter for an algorithm is specified in a spec file. The spec file is a JSON file containing a git SHA and all the information required to reproduce a Session, Trial, or Experiment as per Section 2.2. Reproducing the entirety of an RL experiment merely requires checking out the code at the git SHA and running the saved spec file. The main entries in a spec file are given below. Examples of spec files are given in full in Supple- mentary Section A.4. 1. agent - A list (to allow for multi-agent settings in the future), each element of which con- tains the spec for a single agent. Each agent spec contains the details for its components as described in Section 2.1: (a) algorithm. The main parameters specific to the algorithm, such as the policy type, exploration strategy (e.g. -greedy, Boltzmann), algorithm coefficients, rate decays, and training schedules. (b) memory. Specifies which memory to use as appropriate to the algorithm along with any specific memory hyperparameters such as the batch size and the memory size. (c) net. The type of neural network, its hidden layer architecture, activations, gradient clipping, loss function, optimizer, rate decays, update method, and CUDA usage. 2. env - A list (to allow for multi-environment settings in the future), each element of which specifies an environment. Each environment spec includes an optional maximum time step per episode, the total time steps (frames) in a Session, the state and reward preprocessing methods, and the number of environments in a vector environment. 3. body - Specifies how (multi-)agents connect to (multi-)environments. 4. meta - The high-level configuration of how the experiment is to be run. It gives the number of Trials and Sessions to run, the evaluation and logging frequency, and a toggle to activate asynchronous training. 5. search - The hyperparameters to search over and the methods used to sample them. Any variables in the spec file can be searched over, including environment variables.

Section Title: RESULTS
  RESULTS Reporting benchmark results is essential for validating algorithm implementations. SLM Lab main- tains a benchmark page and a public directory containing all of the experiment data, models, plots, and spec files associated with the reported results. 2 We welcome contributions to this benchmark page via a Pull Request. We tested the algorithms implemented in SLM Lab on 77 environments: 62 Atari games and 11 Roboschool environments available through OpenAI gym ( Brockman et al., 2016 ) and 4 Unity en- vironments ( Juliani et al., 2018 ). These environments span discrete ( Table 1 ) and continuous ( Table 2 ) control problems with high- and low-dimensional state spaces. The results we report in each of the tables are the score per episode at the end of training averaged over the previous 100 training checkpoints. Agents were checkpointed every 10k (Atari) or 1k (Roboschool, Unity) training frames. This measure is less sensitive to rapid increases or decreases in performance, instead reflecting average performance over a substantial number of training frames. To our knowledge, the results we present below are a more comprehensive performance comparison than has been previously published for a single codebase. A full set of learning curves as well as a full table of results for the Atari environments are provided in the supplementary materials.

Section Title: EXPERIMENT DETAILS
  EXPERIMENT DETAILS A complete set of spec files listing all hyperparameters for all algorithms and experiments are in- cluded in the slm_lab/spec/benchmark directory of SLM Lab as well in the experiment data released along with the results. Example spec files listing all of the hyperparameters for PPO and DDQN + PER on the Atari and Roboschool environments are included in Supplementary Section A.4. For the Atari environments, agents were trained for 10M frames (40M accounting for skipped frames). For the Roboschool environments, agents were trained for 2M frames except for RoboschoolHumanoid (50M frames), RoboschoolHumanoidFlagrun (100M) and RoboschoolHu- manoidFlagrunHarder (100M). For the Unity environments, agents were trained for 2M frames. Training was parallelized either synchronously or with a hybrid of synchronous and asynchronous methods. Our results for PPO and A2C on Atari games are comparable the results published by  Schulman et al. (2017) . The results on DQN and DDQN + PER on Atari games are mixed: at the same number of training frames 3 we sometimes match or exceed the reported results and sometimes perform worse. This is likely due to two hyperparameter differences. We used a replay memory of size 200,000 compared to 1M in  Mnih et al. (2015) ,  van Hasselt et al. (2015) , and  Schaul et al. (2015) . The final output layer of the network is smaller fully-connected layer with 256 instead of 512 units. Finally, our results for SAC confirm the strength of this algorithm compared to PPO for continuous control problems. However the absolute performance is typically worse than the published results from  Haarnoja et al. (2018b) . Due to computational constraints, SAC was trained with a replay buffer of 0.2M elements and combined experience replay ( Zhang & Sutton, 2017 ) compared with 1M elements in  Haarnoja et al. (2018b)  and this is potentially a significant difference.

Section Title: SOFT ACTOR-CRITIC FOR DISCRETE ENVIRONMENTS
  SOFT ACTOR-CRITIC FOR DISCRETE ENVIRONMENTS All published results for the Soft Actor-Critic (SAC) algorithm ( Haarnoja et al., 2018a ;b) are for continuous control environments. However, nothing in its algorithmic description makes it unsuit- able in principle for use in discrete action-space environments. As a consequence of the modular structure of SLM Lab, it was straightforward to design and implement a discrete variant of SAC. We did so by using policy Nets that produced parameters of a Gumbel-Softmax distribution ( Jang et al., 2016 ;  Maddison et al., 2016 ) from which discrete actions were sampled. The results of this discrete variant of SAC are in  Table 1 . On environments for which its training converged, we found SAC to be of comparable or better sample efficiency than all of the other algorithms that we tested. For example, SAC achieves an Under review as a conference paper at ICLR 2020 average score over the previous 100 checkpoints of around 20 after 1M frames on Atari Pong, whereas all other algorithms that we tested require at least 2M frames to achieve the same result (plot shown in Supplementary Section A.3). However, even though SAC trained successfully on Pong and Lunar Lander, we were not able to successfully train it on all other Atari environments. We also note that while SAC is sample efficient it is more computationally expensive than the other algorithms, which presents an obstacle for extensive performance tuning.

Section Title: HYBRID SYNCHRONOUS AND ASYNCHRONOUS TRAINING
  HYBRID SYNCHRONOUS AND ASYNCHRONOUS TRAINING Synchronous and asynchronous parallelization can be combined in SLM Lab to accelerate training, as shown in  Figure 4 . SLM Lab implements synchronous parallelization within Sessions (Section 2.2) using vector environments ( Dhariwal et al., 2017 ) and asynchronous parallelization within Tri- als (Section 2.2) using multiple workers, one per Session. There are two available methods for Trial level parallelization; Hogwild! ( Recht et al., 2011 ) or a server-worker model in which workers periodically push gradients to a central network and pull copies of the updated parameters. If training is constrained by data sampling from the environment, then increasing the number of vector environments (synchronous parallelization) speeds up training. But this speed-up saturates as the training step becomes a bottleneck and the environment waits for the agent to train. In the example shown in  Figure 4  the frames per seconds (fps) increases from around 200 for a single worker and 1 environment to around 360 for 1 worker and 16 environments, and saturates thereafter. Once fps becomes constrained by the training step, it is beneficial to add workers (asynchronous parallelization) to effectively parallelize the parameter updates. A hybrid of 16 workers each with 4 environments resulted in the maximum fps of around 3800.

Section Title: RELATED WORK
  RELATED WORK

Section Title: REPRODUCIBILITY IN REINFORCEMENT LEARNING
  REPRODUCIBILITY IN REINFORCEMENT LEARNING The instability of RL algorithms ( Haarnoja et al., 2018a ), randomness in agent policies and the environment ( Henderson et al., 2017 ;  Islam et al., 2017 ), as well as differences in hyperparameter tuning ( Islam et al., 2017 ) and implementations ( Henderson et al., 2017 ;  Tucker et al., 2018 ) all contribute to the challenge of reproducing RL results. Consequently, the importance of comprehensively documenting all hyperparameters along with pub- lished results and software is well recognized within the research community ( Machado et al., 2017 ;  Castro et al., 2018 ).

Section Title: SOFTWARE FOR REINFORCEMENT LEARNING
  SOFTWARE FOR REINFORCEMENT LEARNING To date more than twenty reinforcement-learning-themed open source software libraries have been released. They can be organized into two categories: those implementing RL algorithms and those implement RL environments. 4 SLM Lab is an algorithm-focused library with built-in integration with the OpenAI gym ( Brockman et al., 2016 ), OpenAI Roboschool, VizDoom ( Kempka et al., 2016 ), and Unity ML-Agents ( Juliani et al., 2018 ) environment libraries.  Table 3  summarizes the algorithm-focused reinforcement learning software libraries. Libraries such as Catalyst ( Kolesnikov, 2018 ), ChainerRL (chainer, 2017;  Tokui & Oono, 2015 ), coach ( Caspi et al., 2017 ), DeepRL ( Zhang, 2017 ), OpenAI baselines ( Dhariwal et al., 2017 ), RL- graph ( Schaarschmidt et al., 2019 ), RLkit ( Pong, 2018 ), rlpyt ( Stooke & Abbeel, 2019 ), RLLib ( Liang et al., 2017 ), Stable Baselines ( Hill et al., 2018 ), TensorForce ( Kuhnle et al., 2017 ), TF- Agents ( Guadarrama et al., 2018 ), and vel ( Tworek, 2018 ) implement a wide variety of algorithms and are intended to be applied to a variety of RL problems. Most of these libraries also provide some benchmark results for the implemented algorithms to validate their performance. These can be thought of as generalist RL libraries and are the most closely related to SLM Lab. Other libraries focus on specific algorithms (Dopamine ( Castro et al., 2018 ), Softlearning ( Haarnoja et al., 2018c ), a2c-ppo-acktr-gail ( Kostrikov, 2018 ), Keras-RL ( Plappert, 2016 )), problems (Open- Spiel ( Lanctot et al., 2019 ), ELF ( Tian et al., 2019 ), MAgent ( Zheng et al., 2017 ), reaver ( Ring, 2018 )), components such as loss functions (TRFL ( DeepMind, 2018 ) or scaling training (Horizon ( Gauci et al., 2018 )). The use of configuration files to specify hyperparameters varies. Catalyst, coach, DeepRL, Dopamine, Horizon, reaver, Softlearning, RLgraph, RLLib, and vel use configuration files and pro- vide a number of configured examples. In most cases the network architecture is excluded from the 4 A few libraries such as OpenSpiel ( Lanctot et al., 2019 ) and ELF ( Tian et al., 2019 ) implement both. Almost all libraries include some methods for parallelizing agent training, especially for on-policy methods. However most do not include hyperparameter optimization as a feature. The two excep- tions are Stable Baselines ( Hill et al., 2018 ) and Tensorforce ( Kuhnle et al., 2017 ). Finally, many libraries include some tools for visualizing and plotting results. Notably coach ( Caspi et al., 2017 ) provides an interactive dashboard for exploring a variety of metrics which are automat- ically tracked during training.

Section Title: Annex Figures
  Annex Figures   fig_4        

```
