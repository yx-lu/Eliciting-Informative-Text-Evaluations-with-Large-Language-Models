Title:
```
Under review as a conference paper at ICLR 2020 VILD: VARIATIONAL IMITATION LEARNING WITH DIVERSE-QUALITY DEMONSTRATIONS
```
Abstract:
```
The goal of imitation learning (IL) is to learn a good policy from high-quality demonstrations. However, the quality of demonstrations in reality can be diverse, since it is easier and cheaper to collect demonstrations from a mix of experts and amateurs. IL in such situations can be challenging, especially when the level of demonstrators' expertise is unknown. We propose a new IL paradigm called Variational Imitation Learning with Diverse-quality demonstrations (VILD), where we explicitly model the level of demonstrators' expertise with a probabilistic graphical model and estimate it along with a reward function. We show that a naive estimation approach is not suitable to large state and action spaces, and fix this issue by using a variational approach that can be easily implemented using existing reinforcement-learning methods. Experiments on continuous-control benchmarks and real-world crowdsourced demonstrations denote that VILD outperforms state- of-the-art methods. Our work enables scalable and data-efficient IL under more realistic settings than before.
```

Figures/Tables Captions:
```
Figure 1: Graphical models describe expert demonstrations and diverse-quality demonstrations. Shaded and unshaded nodes indicate observed and unobserved random variables, respectively. Plate notations indicate that the sampling process is repeated for N times. s t P S is a state with transition densities pps t'1 |s t , a t q, a t P A is an action with density π ‹ pa t |s t q, u t P A is a noisy action with density ppu t |s t , a t , kq, and k P t1, . . . , Ku is an identification number with distribution ppkq.
Figure 2: Performance averaged over 5 trials in terms of the mean and standard error. Demonstrations are generated by 10 demonstrators using (a) Gaussian and (b) TSD noisy policies. Horizontal dotted lines indicate performance of k " 1, 3, 5, 7, 10 demonstrators. IS denotes importance sampling.
Figure 3: Performance of VILD with IS and baseline methods for the robosuite reaching task.
Figure 4: Performance of InfoGAIL with difffer- ent contexts z for the robosuite reaching task.
Figure 5: An example of trajectory generated by VILD's policy in the robosuite reaching task. The goal of the agent is to control the robot's end-effector to reach the red object. The value of reward function (for performance evaluation) is inverse proportion to the distance between the end-effector and the red object.
Table 1: Performance in the last 100 iterations in terms of the mean and standard error of cumulative rewards over 5 trials (higher is better). Boldfaces indicate best and comparable methods according to t-test with significance level 0.01. (G) denotes Gaussian noisy policy and (TSD) denotes time-signal- dependent noisy policy. The performance of VAIL is similar to that of GAIL and is omitted. The performance of InfoGAIL (best context) is overall similar to that of InfoGAIL and is also omitted.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The goal of sequential decision making is to learn a policy that makes good decisions ( Puterman, 1994 ). As an important branch of sequential decision making, imitation learning (IL) ( Schaal, 1999 ) aims to learn such a policy from demonstrations (i.e., sequences of decisions) collected from experts. However, high-quality demonstrations can be difficult to obtain in reality, since such experts may not always be available and sometimes are too costly ( Osa et al., 2018 ). This is especially true when the quality of decisions depends on specific domain-knowledge not typically available to amateurs; e.g., in applications such as robot control ( Osa et al., 2018 ) and autonomous driving ( Silver et al., 2012 ). In practice, demonstrations are often diverse in quality, since it is cheaper to collect demonstrations from mixed demonstrators, containing both experts and amateurs (Audiffren et al., 2015). Unfortu- nately, IL in such settings tends to perform poorly, since low-quality demonstrations often negatively affect the performance of IL methods ( Shiarlis et al., 2016 ). For example, amateurs' demonstrations for robotics can be cheaply collected via a robot simulation ( Mandlekar et al., 2018 ), but such demonstrations may cause damages to the robot which is catastrophic in the real-world ( Shiarlis et al., 2016 ). Similarly, demonstrations for autonomous driving can be collected from drivers in public roads ( Fridman et al., 2017 ), which may contain traffic-accident demonstrations. Learning a self-driving car from these low-quality demonstrations may cause traffic accidents. When the level of demonstrators' expertise is known, multi-modal IL (MM-IL) can be used to learn a good policy with diverse-quality demonstrations ( Li et al., 2017 ;  Hausman et al., 2017 ;  Wang et al., 2017 ). Specifically, MM-IL aims to learn a multi-modal policy, where each mode of the policy represents the decision making of each demonstrator. When knowing the level of demonstrators' expertise, good policies can be obtained by selecting modes that correspond to the decision making of high-expertise demonstrators. However, in practice, it is difficult to truly determine the level of demonstrators' expertise beforehand. Without knowing the level of expertise, it is difficult to distinguish the decision making of experts and amateurs, and learning a good policy is challenging. To overcome the issue of MM-IL, pioneer works have proposed to estimate the quality of each demonstration using auxiliary information from experts (Audiffren et al., 2015;  Wu et al., 2019 ;  Brown et al., 2019 ). Specifically,  Audiffren et al. (2015)  inferred the demonstration quality using Under review as a conference paper at ICLR 2020 similarities between diverse-quality demonstrations and high-quality demonstrations, where the latter are collected in a small number from experts. In contrast,  Wu et al. (2019)  proposed to estimate the demonstration quality using a small number of demonstrations with confidence scores. Namely, the score value given by an expert is proportion to the demonstration quality. Similarly, the demonstration quality can be estimated by ranked demonstrations, where ranking from an expert is evaluated due to the relative quality ( Brown et al., 2019 ). To sum up, these methods rely on auxiliary information from experts, namely high-quality demonstrations, confidence scores, and ranking. In practice, these pieces of information can be scarce or noisy, which leads to a poor performance of these methods. In this paper, we consider a novel but realistic setting of IL where only diverse-quality demonstrations are available. Meanwhile, the level of demonstrators' expertise and auxiliary information from experts are fully absent. To tackle this challenging setting, we propose a new learning paradigm called variational imitation learning with diverse-quality demonstrations (VILD). The central idea of VILD is to model the level of demonstrators' expertise via a probabilistic graphical model, and learn it along with a reward function that represents an intention of expert's decision making. To scale up our model for large state and action spaces, we leverage the variational approach ( Jordan et al., 1999 ), which can be implemented using reinforcement learning (RL) for flexibility ( Sutton & Barto, 1998 ). To further improve data-efficiency of VILD when learning the reward function, we utilize importance sampling (IS) to re-weight a sampling distribution according to the estimated level of demonstrators' expertise. Experiments on continuous-control benchmarks and real-world crowdsourced demonstrations ( Mandlekar et al., 2018 ) denote that: 1) VILD is robust against diverse-quality demonstrations and outperforms existing methods significantly. 2) VILD with IS is data-efficient, since it learns the policy using a less number of transition samples.

Section Title: IL FROM DIVERSE-QUALITY DEMONSTRATIONS AND ITS CHALLENGE
  IL FROM DIVERSE-QUALITY DEMONSTRATIONS AND ITS CHALLENGE Before delving into our main contribution, we first give the minimum background about RL and IL. Then, we formulate a new setting in IL called diverse-quality demonstrations, discuss its challenge, and reveal the deficiency of existing methods.

Section Title: Reinforcement learning
  Reinforcement learning Reinforcement learning (RL) ( Sutton & Barto, 1998 ) aims to learn an optimal policy of a sequential decision making problem, which is often mathematically formulated as a Markov decision process (MDP) ( Puterman, 1994 ). We consider a finite-horizon MDP with continuous state and action spaces defined by a tuple M " pS, A, pps 1 |s, aq, p 1 ps 1 q, rps, aqq with a state s t P S Ď R ds , an action a t P A Ď R da , an initial state density p 1 ps 1 q, a transition probability density pps t'1 |s t , a t q, and a reward function r : SˆA Þ Ñ R, where the subscript t P t1, . . . , T u denotes the time step. A sequence of states and actions, ps 1:T , a 1:T q, is called a trajectory. A decision making of an agent is determined by a policy πpa t |s t q, which is a conditional probability density of action given state. RL seeks for an optimal policy π ‹ pa t |s t q which maximizes the expected cumulative reward: E pπps 1:T ,a 1:T q rΣ T t"1 rps t , a t qs, where p π ps 1:T , a 1:T q " p 1 ps 1 qΠ T t"1 pps t'1 |s t , a t qπpa t |s t q is a trajectory probability density induced by π. RL has shown great successes recently, especially when combined with deep neural networks ( Silver et al., 2017 ). However, a major limitation of RL is that it relies on the reward function which may be unavailable in practice ( Schaal, 1999 ).

Section Title: Imitation learning
  Imitation learning To address the above limitation of RL, imitation learning (IL) was pro- posed ( Schaal, 1999 ). Without using the reward function, IL aims to learn the optimal policy from demonstrations that encode information about the optimal policy. A common assumption in most IL methods is that, demonstrations are collected by K ě 1 demonstrators who execute actions a t drawn from π ‹ pa t |s t q for every states s t . A graphical model describing this data collection process is depicted in Figure 1(a), where a random variable k P t1, . . . , Ku denotes each demonstrator's identification number and ppkq denotes the probability of collecting a demonstration from the k-th demonstrator. Under this assumption, demonstrations tps 1:T , a 1:T , kq n u N n"1 (i.e., observed random variables in Figure 1(a)) are called expert demonstrations and are regarded to be drawn independently from a probability density p ‹ ps 1:T , a 1:T qppkq " ppkqp 1 ps 1 qΠ T t"1 pps t'1 |s t , a t qπ ‹ pa t |s t q. We note that k does not affect the trajectory density p ‹ ps 1:T , a 1:T q and can be omitted. We assume a common assumption that p 1 ps 1 q and pps t'1 |s t , a t q are unknown but we can sample states from them. IL has shown great successes in benchmark settings ( Ho & Ermon, 2016 ;  Fu et al., 2018 ;  Peng et al., 2019 ). However, practical applications of IL in the real-world is relatively few ( Schroecker et al., 2019 ). One of the main reasons is that most IL methods aim to learn with expert demonstrations. In practice, such demonstrations are often too costly to obtain due to a limited number of experts, and Under review as a conference paper at ICLR 2020 s 1 s 2¨¨¨sT . . . a 1 a 2 a T k N (a) Expert demonstrations. even when we obtain them, the number of demonstrations is often too few to accurately learn the optimal policy (Audiffren et al., 2015;  Wu et al., 2019 ;  Brown et al., 2019 ). New setting in IL: Diverse-quality demonstrations. To improve practicality of IL, we consider a new learning paradigm called IL with diverse-quality demonstrations, where demonstrations are collected from demonstrators with different level of expertise. Compared to expert demonstrations, diverse-quality demonstrations can be collected more cheaply, e.g., via crowdsourcing ( Mandlekar et al., 2018 ). The graphical model in Figure 1(b) depicts the process of collecting such demonstrations from K ą 1 demonstrators. Formally, we select the k-th demonstrator according to a distribution ppkq. After selecting k, for each time step t, the k-th demonstrator observes state s t and samples action a t using π ‹ pa t |s t q. However, the demonstrator may not execute a t in the MDP if this demonstrator is not expertised. Instead, he/she may sample an action u t P A with another probability density ppu t |s t , a t , kq and execute it. Then, the next state s t'1 is observed with a probability density pps t'1 |s t , u t q, and the demonstrator continues making decision until time step T . We repeat this process for N times to collect diverse-quality demonstrations D d " tps 1:T , u 1:T , kq n u N n"1 . These demonstrations are regarded to be drawn independently from a probability density We refer to ppu t |s t , a t , kq as a noisy policy of the k-th demonstrator, since it is used to execute a noisy action u t . Our goal is to learn the optimal policy π ‹ using diverse-quality demonstrations D d . Note that Eq. (1) can be described equivalently by using a marginal density πpu t |s t , kq " ş A π ‹ pa t |s t qppu t |s t , a t , kqda t and removing a t from the graphical model. However, we explic- itly write a t as above to emphasize the dependency between π ‹ pa t |s t q and ppu t |s t , a t , kq. This emphasis will be made more clear in Section 3.1 when we describe our choice of model. The deficiency of existing methods. We conjecture that existing IL methods are not suitable to learn with diverse-quality demonstrations according to p d . Specifically, these methods always treat observed demonstrations as if they were drawn from p ‹ . By comparing p ‹ and p d , we can see that ex- isting methods would learn πpu t |s t q such that πpu t |s t q « Σ K k"1 ppkq ş A π ‹ pa t |s t qppu t |s t , a t , kqda t . In other words, they learn a policy that averages over decisions of all demonstrators. This would be problematic when amateurs are present, as averaged decisions of all demonstrators would be highly different from those of all experts. Worse yet, state distributions of amateurs and experts tend to be highly different, which often leads to the unstable learning: The learned policy oscillated between well-performed policy and poorly-performed policy. For these reasons, we believe that existing methods tend to learn a policy that achieves average performances, and are not suitable for handling the setting of diverse-quality demonstrations.

Section Title: VILD: A ROBUST METHOD FOR DIVERSE-QUALITY DEMONSTRATIONS
  VILD: A ROBUST METHOD FOR DIVERSE-QUALITY DEMONSTRATIONS This section presents VILD, namely a robust method for tackling the challenge from diverse-quality demonstrations. Specifically, we build a probabilistic model that explicitly describes the level of demonstrators' expertise and a reward function (Section 3.1), and estimate its parameters by a variational approach (Section 3.2), which can be implemented easily by RL (Section 3.3). We also improve data-efficiency by using importance sampling (Section 3.4). Mathematical derivations are provided in Appendix A.

Section Title: MODELING DIVERSE-QUALITY DEMONSTRATIONS
  MODELING DIVERSE-QUALITY DEMONSTRATIONS This section describes a model which enables estimating the level of demonstrators' expertise. We first describe a naive model, whose parameters can be estimated trivially via supervised learning, but suffers from the issue of compounding error. Then, we describe our proposed model, which avoids the issue of the naive model by learning a reward function.

Section Title: Naive model
  Naive model where π θ and p ω are learned to estimate the optimal policy and the noisy policy, respectively. The parameters θ and ω can be learned by minimizing the Kullback-Leibler (KL) divergence from the data distribution to the model. This naive model can be regarded as an extension of a model proposed by  Raykar et al. (2010)  for handling diverse-quality data in supervised learning. The main advantage of this naive model is that its parameters can be estimated trivially via supervised learning. However, this native model suffers from the issue of compounding error ( Ross & Bagnell, 2010 ) and tends to perform poorly. Specifically, supervised-learning methods assume that data distributions during training and testing are identical. However, data distributions during training and testing are different in IL, since data distributions depend on policies ( Puterman, 1994 ). A discrepancy of data distributions causes compounding errors during testing, where prediction errors increase further in future predictions. Due to this issue, supervised-learning methods often perform poorly in IL ( Ross & Bagnell, 2010 ). The issue becomes even worse with diverse-quality demonstrations, since data distributions of different demonstrators tend to be highly different. For these reasons, this naive model is not suitable for our setting.

Section Title: Proposed model
  Proposed model To avoid the issue of compounding error, our method utilizes the inverse RL (IRL) approach ( Ng & Russell, 2000 ), where we aim to learn a reward function from diverse-quality demonstrations 1 . IL problems can be solved by a combination of IRL and RL, where we learn a reward function by IRL and then learn a policy from the reward function by RL. This combination avoids the issue of compounding error, since the policy is learned by RL which generalizes to states not presented in demonstrations ( Ho & Ermon, 2016 ). Specifically, our proposed model is based on a model of maximum entropy IRL (MaxEnt- IRL) ( Ziebart et al., 2010 ). Briefly speaking, MaxEnt-IRL learns a reward function from expert demonstrations by using a model p φ ps 1:T , a 1:T q 9 pps 1 qΠ T t"1 p 1 ps t'1 |s t , a t q exppr φ ps t , a t qq. Based on this model, we propose to learn the reward function and the level of expertise by a model where φ and ω are parameters. We denote a normalization term of this model by Z φ,ω . By comparing the proposed model p φ,ω to the data distribution p d , the reward parameter φ should be learned so that the cumulative reward is proportion to a joint probability density of actions given by the optimal policy, i.e., exppΣ T t"1 r φ ps t , a t qq 9 Π T t"1 π ‹ pa t |s t q. In other words, the cumulative reward is large for trajectories induced by the optimal policy. Therefore, the optimal policy can be learned by maximizing the cumulative reward. Meanwhile, the density p ω pu t |s t , a t , kq is learned to estimate the noisy policy ppu t |s t , a t , kq. In the remainder, we refer to ω as an expertise parameter. To learn parameters of this model, we propose to minimize the KL divergence from the data distribution to the model: min φ,ω KLpp d ps 1:T , u 1:T |kqppkq||p φ,ω ps 1:T , u 1:T , kqq. By Under review as a conference paper at ICLR 2020 rearranging terms and ignoring constant terms, minimizing this KL divergence is equiv- alent to solving an optimization problem max φ,ω f pφ, ωq´gpφ, ωq, where f pφ, ωq " E p d ps 1:T ,u 1:T |kqppkq rΣ T t"1 logp ş A exppr φ ps t , a t qqp ω pu t |s t , a t , kqda t qs and gpφ, ωq " log Z φ,ω . To solve this optimization, we need to compute the integrals over both state space S and action space A. Computing these integrals is feasible for small state and action spaces, but is infeasible for large state and action spaces. To scale up our model to MDPs with large state and action spaces, we leverage a variational approach in the followings.

Section Title: VARIATIONAL APPROACH FOR PARAMETER ESTIMATION
  VARIATIONAL APPROACH FOR PARAMETER ESTIMATION The central idea of the variational approach is to lower-bound an integral by the Jensen inequality and a variational distribution ( Jordan et al., 1999 ). The main benefit of the variational approach is that the integral can be indirectly computed via the lower-bound, given an optimal variational distribution. However, finding the optimal distribution often requires solving a sub-optimization problem. Before we proceed, notice that f pφ, ωq´gpφ, ωq is not a joint concave function of the integrals, and this prohibits using the Jensen inequality. However, we can separately lower-bound f and g by the Jensen inequality, since they are concave functions of their corresponding integrals. Specifi- cally, let l φ,ω ps t , a t , u t , kq " r φ ps t , a t q'log p ω pu t |s t , a t , kq. By using a variational distribution q ψ pa t |s t , u t , kq with parameter ψ, we obtain an inequality f pφ, ωq ě Fpφ, ω, ψq, where and H t pq ψ q "´E q ψ pat|st,ut,kq rlog q ψ pa t |s t , u t , kqs. It is trivial to verify that the equality f pφ, ωq " max ψ Fpφ, ω, ψq holds ( Jordan et al., 1999 ), where the maximizer ψ ‹ of the lower- bound yields q ψ ‹ pa t |s t , u t , kq 9 exppl φ,ω ps t , a t , u t , kqq. Therefore, the function f pφ, ωq can be substituted by max ψ Fpφ, ω, ψq. Meanwhile, by using a variational distribution q θ pa t , u t |s t , kq with parameter θ, we obtain an inequality gpφ, ωq ě Gpφ, ω, θq, where The lower-bound G resembles an objective function of maximum entropy RL (MaxEnt-RL) ( Ziebart et al., 2010 ). By using the optimality results of MaxEnt-RL ( Haarnoja et al., 2018 ), we have an equality gpφ, ωq " max θ Gpφ, ω, θq. Therefore, the function gpφ, ωq can be substituted by max θ Gpφ, ω, θq. By using these lower-bounds, we have that max φ,ω f pφ, ωq´gpφ, ωq " max φ,ω,ψ Fpφ, ω, ψqḿ ax θ Gpφ, ω, θq " max φ,ω,ψ min θ Fpφ, ω, ψq´Gpφ, ω, θq. Solving the max-min problem is often feasible even for large state and action spaces, since Fpφ, ω, ψq and Gpφ, ω, θq are defined as an expectation and can be optimized straightforwardly. Nevertheless, in practice, we represent the variational distributions by parameterized functions, and iteratively solve the sub-optimization (w.r.t. ψ and θ) by stochastic optimization methods. However, in this scenario, the equalities f pφ, ωq " max ψ Fpφ, ω, ψq and gpφ, ωq " max θ Gpφ, ω, θq may not hold for two reasons. First, the optimal variational distributions may not be in the space of our parameterized functions. Second, stochastic optimization methods may yield local solutions. Nonetheless, when the variational distributions are represented by deep neural networks, the obtained variational distributions are often reasonably accurate and the equalities approximately hold ( Ranganath et al., 2014 ).

Section Title: MODEL SPECIFICATION
  MODEL SPECIFICATION In practice, we are required to specify models for q θ and p ω . We propose to use q θ pa t , u t |s t , kq " q θ pa t |s t qN pu t |a t , Σq and p ω pu t |s t , a t , kq " N pu t |a t , C ω pkqq. As shown below, the choice for q θ pa t , u t |s t , kq enables us to solve the sub-optimization w.r.t. θ by using RL with reward function r φ . Meanwhile, the choice for p ω pu t |s t , a t , kq incorporates our prior assumption that the noisy policy tends to Gaussian, which is a reasonable assumption for actual human motor behavior ( van Beers et al., 2004 ). Under these model specifications, solving max φ,ω,ψ min θ Fpφ, ω, ψq´Gpφ, ω, θq is equivalent to solving max φ,ω,ψ min θ Hpφ, ω, ψ, θq, where Under review as a conference paper at ICLR 2020 Here, r q θ ps 1:T , a 1:T q " p 1 ps 1 qΠ T t"1 ş A pps t'1 |s t , u t qN pu t |a t , Σqdu t q θ pa t |s t q is a noisy trajectory density induced by a policy q θ pa t |s t q, where N pu t |a t , Σq can be regarded as an approximation of the noisy policy in Figure 1(b). Minimizing H w.r.t. θ resembles solving a MaxEnt-RL problem with a reward function r φ ps t , a t q, except that trajectories are collected according to the noisy trajectory density. In other words, this minimization problem can be solved using RL, and q θ pa t |s t q can be regarded as an approximation of the optimal policy. The hyper-parameter Σ determines the quality of this approximation: smaller value of Σ gives a better approximation. Therefore, by choosing a reasonably small value of Σ, solving the max-min problem in Eq. (5) yields a reward function r φ ps t , a t q and a policy q θ pa t |s t q. This policy imitates the optimal policy, which is the goal of IL. The model specification for p ω incorporates our prior assumption about the noisy policy. Namely, p ω pu t |s t , a t , kq " N pu t |a t , C ω pkqq assumes that the noisy policy tends to Gaussian, where C ω pkq gives an estimated expertise of the k-th demonstrator: High-expertise demonstrators have small C ω pkq and vice-versa for low-expertise demonstrators. Note that VILD is not restricted to this choice. Different choices of p ω incorporate different prior assumptions. For example, a Laplace distribution incorporates a prior assumption about demonstrators who tend to execute outlier actions ( Murphy, 2013 ). In such a case, the squared error in H is replaced by the absolute error (see Appendix A.3). It should be mentioned that q ψ pa t |s t , u t , kq maximizes the immediate reward and minimizes the weighted squared error between u t and a t . The trade-off between the reward and squared-error is determined by C ω pkq. Specifically, for demonstrators with a small C ω pkq (i.e., high-expertise demonstrators), the squared error has a large magnitude and q ψ tends to minimize the squared error. Meanwhile, for demonstrators with a large value of C ω pkq (i.e., low-expertise demonstrators), the squared error has a small magnitude and q ψ tends to maximize the immediate reward. We implement VILD with deep neural networks where we iteratively update φ, ω, and ψ by stochastic gradient methods, and update θ by policy gradient methods. A pseudo-code of VILD and implementation details are given in Appendix B. In our implementation, we include a regularization term Lpωq " T E ppkq rlog |C´1 ω pkq|s{2, to penalize large value of C ω pkq. Without this regularization, C ω pkq can be overly large which makes learning degenerate. We note that H already includes such a penalty via the trace term: E ppkq rTrpC´1 ω pkqΣqs. However, the strength of this penalty tends to be too small, since we choose Σ to be small. VILD requires variable k to be given along with demonstrations. However, There is no need for this variable to be provided by experts. When k is not given, a simple strategy is to set k " n and K " N . In other words, this strategy assumes that there is a one-to-one mapping between demonstration and demonstrator. We apply this strategy in our experiments with real-world demonstrations.

Section Title: IMPORTANCE SAMPLING FOR REWARD LEARNING
  IMPORTANCE SAMPLING FOR REWARD LEARNING To improve the convergence rate of VILD when updating φ, we use importance sampling (IS). Specifi- cally, by analyzing the gradient ∇ φ H " ∇ φ tE p d ps 1:T ,u 1:T |kqppkq rΣ T t"1 E q ψ pat|st,ut,kq rr φ ps t , a t qssÉ r q θ ps 1:T ,a 1:T q rΣ T t"1 r φ ps t , a t qsu, we can see that the reward function is updated to maximize the expected cumulative reward obtained by demonstrators and q ψ , while minimizing the expected cumu- lative reward obtained by q θ . However, low-quality demonstrations often yield low reward values. For this reason, stochastic gradients estimated by these demonstrations tend to be uninformative, which leads to slow convergence and poor data-efficiency. To avoid estimating such uninformative gradients, we use IS to estimate gradients using high-quality demonstrations which are sampled with high probability. Briefly, IS is a technique for estimat- ing an expectation over a distribution by using samples from a different distribution ( Robert & Casella, 2005 ). For VILD, we propose to sample k from a distributionppkq 9 }vecpC´1 ω pkqq} 1 . This distribution assigns high probabilities to demonstrators with high estimated level of ex- pertise (i.e., demonstrators with a small C ω pkq). With this distribution, the estimated gradi- ents tend to be more informative which leads to a faster convergence. To reduce a sampling bias, we use a truncated importance weight: wpkq " minpppkq{ppkq, 1q ( Ionides, 2008 ), which leads to an IS gradient: Computing wpkq requires ppkq, which can be estimated accurately since k is a discrete random variable. For simplicity, we assume that ppkq is a uniform distribution.

Section Title: RELATED WORK
  RELATED WORK In this section, we will discuss a related area of supervised learning with diverse-quality data. Besides, we will discuss existing IL methods that use the variational approach.

Section Title: Supervised learning with diverse-quality data
  Supervised learning with diverse-quality data In supervised learning, diverse-quality data has been extensively studied, e.g. learning with noisy labels ( Angluin & Laird, 1988 ). This task assumes that human labelers may assign incorrect labels to training inputs. With such labelers, the obtained dataset consists of high-quality data with correct labels and low-quality data with incorrect labels. To handle this setting, many methods were proposed ( Natarajan et al., 2013 ;  Han et al., 2018 ). The most related methods are probabilistic models, which aim to infer correct labels and the level of labelers' expertise ( Raykar et al., 2010 ;  Khetan et al., 2018 ). Specifically,  Raykar et al. (2010)  proposed a method based on a two-coin model which enables estimating the correct labels and level of expertise. Recently,  Khetan et al. (2018)  proposed a method based on weighted loss functions, where the weight is determined by the estimated labels and level of expertise. Methods for supervised learning with diverse-quality data can be leveraged to learn a policy in our setting. However, they tend to perform poorly due to the issue of compounding error, as discussed previously in Section 3.1.

Section Title: Variational approach in IL
  Variational approach in IL The variational approach has been previously utilized in IL to perform MM-IL and reduce over-fitting. Specifically, MM-IL aims to learn a multi-modal policy from diverse demonstrations collected by many experts ( Li et al., 2017 ), where each mode of the policy represents decision making of each expert 2 . A multi-modal policy is commonly represented by a context- dependent policy, where each context represents each mode of the policy. The variational approach has been used to learn such contexts, i.e., by learning a variational auto-encoder ( Wang et al., 2017 ) and maximizing a variational lower-bound of mutual information ( Li et al., 2017 ;  Hausman et al., 2017 ). Meanwhile, variational information bottleneck (VIB) ( Alemi et al., 2017 ) has been used to reduce over-fitting in IL ( Peng et al., 2019 ). Specifically, VIB aims to compress information flow by minimizing a variational bound of mutual information. This compression filters irrelevant signals, which leads to less over-fitting. Unlike these existing works, we utilize the variational approach to aid computing integrals in large state-action spaces, but not for learning a variational auto-encoder or optimizing a variational bound of mutual information.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we experimentally evaluate the performance of VILD (with and without IS) in continuous-control benchmarks and real-world crowdsourced demonstrations. For benchmarks, we use four continuous-control tasks from OpenAI gym ( Brockman et al., 2016 ) with demonstrations from a pre-trained RL agent. For real-world demonstrations, we use a robosuite reaching task ( Fan et al., 2018 ) with demonstrations from real-world crowdsourcing platform ( Mandlekar et al., 2018 ). Performance is evaluated using a cumulative ground-truth reward along trajectories (i.e., higher is better) ( Ho & Ermon, 2016 ), and this cumulative reward is computed using test trajectories generated by learned policies (i.e., q θ pa t |s t q). We use 10 test trajectories for the benchmark tasks, and use 100 test trajectories for the robosuite reaching task. Note that we use a larger number of test trajectories due to high variability of initial states in the robosuite reaching task. We repeat experiments for 5 trials with different random seeds and report the mean and standard error.

Section Title: Baseline
  Baseline We compare VILD against GAIL ( Ho & Ermon, 2016 ), AIRL ( Fu et al., 2018 ), VAIL ( Peng et al., 2019 ), MaxEnt-IRL ( Ziebart et al., 2010 ), and InfoGAIL ( Li et al., 2017 ). These are online IL methods which collect transition samples to learn policies. We use trust region policy optimization (TRPO) ( Schulman et al., 2015 ) to update policies, except for the Humanoid task where we use soft actor-critic (SAC) ( Haarnoja et al., 2018 ). For InfoGAIL, we report the performance averaged over uniformly sampled contexts, as well as the performance with the best context chosen during testing.

Section Title: CONTINUOUS-CONTROL BENCHMARK TASKS
  CONTINUOUS-CONTROL BENCHMARK TASKS

Section Title: Data generation
  Data generation To generate demonstrations from π ‹ (pre-trained by TRPO) according to Fig- ure 1(b), we use two types of noisy policy ppu t |a t , s t , kq: Gaussian noisy policy: N pu t |a t , σ 2 k Iq and time-signal-dependent (TSD) noisy policy: N pu t |a t , diagpb k ptqˆ}a t } 1 {d a qq, where b k ptq is sampled from a noise process. We use K " 10 demonstrators with different σ k and noise processes for b k ptq. Each demonstrator generates trajectories with approximately T " 1000 time steps. The number of state-action pairs in each dataset is approximately 10000. Notice that for TSD, the noise variance depends on time and magnitude of actions. This characteristic of TSD has been observed in human motor control ( van Beers et al., 2004 ). More details of data generation are given in Appendix C. Results against online IL methods.  Figure 2  shows learning curves of VILD and existing methods against the number of transition samples in HalfCheetah and Ant 3 , whereas  Table 1  reports the performance achieved in the last 100 iterations. Clearly, VILD with IS overall outperforms existing methods in terms of both data-efficiency and final performance, i.e., VILD with IS learns better policies using less numbers of transition samples. VILD without IS tends to outperform existing methods in terms of the final performance. However, it is less data-efficient when compared to VILD with IS, except on Humanoid with the Gaussian noisy policy, where VILD without IS tends to perform better than VILD with IS. We conjecture that this is because IS slightly biases gradient estimation, which may have a negative effect on the performance. Nonetheless, the overall good performance of VILD with IS suggests that it is an effective method to handle diverse-quality demonstrations. On the contrary, existing methods perform poorly as expected, except on the Humanoid task. For the Humanoid task, VILD tends to perform the best in terms of the mean performance. Nonetheless, all methods except GAIL achieve statistically comparable performance according to t-test. This is perhaps because amateurs in this task perform relatively well compared to amateurs in other tasks, as seen from demonstrators' performance given in Table 2 and 3 (Appendix C). Since amateurs perform relatively well, demonstrations from these amateurs do not severely affect the performance of IL methods in this task when compared to the other tasks. We found that InfoGAIL, which learns a context-dependent policy, may achieve good performance when the policy is conditioned on specific contexts. For instance, InfoGAIL (best context) performs quite well in the Walker2d task with the TSD noisy policy (the learning curves are provided in Figure 7(b)). However, as shown in Figure 10, its performance varies across contexts and is quite poor on average when using contexts from a uniform distribution. These results support our conjecture that MM-IL methods are not suitable for our setting where the level of demonstrators' expertise is absent. It can be seen that VILD without IS performs better for the Gaussian noisy policy when compared to the TSD noisy policy. This is because the model of VILD is correctly specified for the Gaussian noisy policy, but the model is incorrectly specified for the TSD noisy policy; misspecified model indeed leads to the reduction in performance. Nonetheless, VILD with IS still performs well for both types of noisy policy. This is perhaps because negative effects of a misspecified model are not too severe for learning expertise parameters, which are required to compute r ppkq. We also conduct the following evaluations. Due to space limitation, figures are given in Appendix D.

Section Title: Results against offline IL methods
  Results against offline IL methods We compare VILD against offline IL methods based on supervised learning, namely behavior cloning (BC) ( Pomerleau, 1988 ), Co-Teaching which is based on a method for learning with noisy labels ( Han et al., 2018 ), and BC from diverse-quality demonstrations (BC-D) which optimizes the naive model described in Section 3.1. Results in Figure 8 show that these methods perform worse than VILD overall; BC performs the worst since it severely suffers from both the compounding error and low-quality demonstrations. Compared to BC, BC-D and Co-teaching are quite robust against low-quality demonstrations, but they still perform worse than VILD with IS.

Section Title: Accuracy of estimated expertise parameter
  Accuracy of estimated expertise parameter To evaluate accuracy of estimated expertise parame- ter, we compare the ground-truth value of σ k under the Gaussian noisy policy against the learned covariance C ω pkq. Figure 9 shows that VILD learns an accurate ranking of demonstrators' exper- tise. The values of these parameters are also quite accurate compared to the ground-truth, except for demonstrators with low-level of expertise. A reason for this phenomena is that low-quality demonstrations are highly dissimilar, which makes learning the expertise more challenging.

Section Title: ROBOSUITE REACHING TASK
  ROBOSUITE REACHING TASK In this experiment, we evaluate the robustness of VILD against real-world demonstrations. Specifi- cally, we conduct an experiment using real-world demonstrations collected by a robotic crowdsourcing platform ( Mandlekar et al., 2018 ). The public datasets were collected in the robosuite environment for object-manipulation tasks such as assembly tasks ( Fan et al., 2018 ). In our experiment, we consider a reaching task, where demonstrations come from clipped assembly tasks when the robot's end-effector contacts the target object. We uses N " 10 demonstrations whose length are approximately T " 500 and set K " 10. The number of state-action pairs in a demonstration dataset is approximately 5000. For VILD, we apply the log-sigmoid function to the reward function, which improves the performance in this task. More details of the experimental setting are provided in Appendix C.2.  Figure 3  shows the performance of all methods, except VILD without IS and VAIL. We do not evaluate VILD without IS and VAIL since IS improves the performance and VAIL is comparable to GAIL. It can be seen that VILD with IS performs better than GAIL, AIRL, and MaxEnt-IRL. VILD also performs better than InfoGAIL in terms of the final performance; InfoGAIL learns faster in the early stage of learning, but its performance saturates and VILD eventually outperforms InfoGAIL. These experimental results show that VILD is more robust against real-world demonstrations with diverse- quality when compared to existing state-of-the-art methods. An example of trajectory generated by VILD's policy is shown in  Figure 5 .  Figure 4  shows the performance of InfoGAIL with different context variables z ( Li et al., 2017 ). We can see that InfoGAIL performs well when the policy is conditioned on specific contexts, e.g., z " 7. Indeed, the best context during testing can improve the performance of InfoGAIL. The effectiveness of such an approach is demonstrated in  Figure 3 , where InfoGAIL (best context) performs very well. However, InfoGAIL (best context) is less practical than VILD, since choosing the best context requires an expert to evaluate the performance of all contexts. In contrast, the performance of VILD does not depend on contexts, since VILD does not learn a context-dependent policy. Moreover, the Under review as a conference paper at ICLR performance of InfoGAIL (best context) is quite unstable, and it is still outperformed by VILD in terms of the final performance.

Section Title: CONCLUSION AND FUTURE WORK
  CONCLUSION AND FUTURE WORK In this paper, we explored a practical setting in IL where demonstrations have diverse-quality. We showed the deficiency of existing methods, and proposed a robust method called VILD, which learns both the reward function and the level of demonstrators' expertise by using the variational approach. Empirical results demonstrated that our work enables scalable and data-efficient IL under this practical setting. In future, we will explore other approaches to efficiently estimate parameters of the proposed model except the variational approach. We will also explore approaches to handle model misspecification, i.e., scenarios where the noisy policy differs from the model p ω . Specifically, we will explore more flexible models of p ω such as neural networks, as well as using the tempered posterior ap- proach ( Grünwald & van Ommen, 2017 ) to improve robustness of our model.
  We emphasize that diverse demonstrations are different from diverse-quality demonstrations. Diverse demonstrations are collected by experts who execute equally good policies, while diverse-quality demonstrations are collected by mixed demonstrators; The former consists of demonstrations that are equally high-quality but diverse in behavior, while the latter consists of demonstrations that are diverse in both quality and behavior.

```
