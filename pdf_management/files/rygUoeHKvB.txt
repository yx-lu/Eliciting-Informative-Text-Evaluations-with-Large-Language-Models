Title:
```
Under review as a conference paper at ICLR 2020 DEEP EXPLORATION BY NOVELTY-PURSUIT WITH MAXIMUM STATE ENTROPY
```
Abstract:
```
Efficient exploration is essential to reinforcement learning in huge state space. Re- cent approaches to address this issue include the intrinsically motivated goal ex- ploration process (IMGEP) and the maximum state entropy exploration (MSEE). In this paper, we disclose that goal-conditioned exploration behaviors in IMGEP can also maximize the state entropy, which bridges the IMGEP and the MSEE. From this connection, we propose a maximum entropy criterion for goal selec- tion in goal-conditioned exploration, which results in the new exploration method novelty-pursuit. Novelty-pursuit performs the exploration in two stages: first, it selects a goal for the goal-conditioned exploration policy to reach the boundary of the explored region; then, it takes random actions to explore the non-explored region. We demonstrate the effectiveness of the proposed method in environments from simple maze environments, Mujoco tasks, to the long-horizon video game of SuperMarioBros. Experiment results show that the proposed method outperforms the state-of-the-art approaches that use curiosity-driven exploration.
```

Figures/Tables Captions:
```
Figure 1: Illustration for the proposed method. A goal-conditioned policy firstly reaches the explo- ration boundary, then perform random actions to discover new states.
Figure 2: Histograms for normalized state visitation counts, where the x-axis represents the index of state. Top row: directly maximizing the entropy of empirical state distribution over visited states; Bottom row: firstly maximizing the counting measure of induced state distribution support, then maximizing the entropy of state distribution with full support.
Figure 3: Illustration of four environments considered in this paper.
Figure 4: Comparison of goal-selection.
Figure 5: Comparison of training techniques.
Figure 6: Average episode returns over 5 seeds on the Empty Room, Four Rooms and FetchReach environments. Shadows indicate the standard deviation.
Figure 7: Average episode returns over 3 seeds on SuperMarioBros. Shadows indicate the standard deviation.
Figure 8: Trajectory visualization on SuperMarioBros-1-3. Trajectories are plotted in green cycles with the same number samples (18M). The agent starts from the most left part and needs to fetch the flag on the most right part. Top row: vanilla; middle row: bonus; bottom row: novelty-pursuit.
Table 1: Average entropy of visited state distribution at timesteps 200k over 5 seeds on Empty Room.
Table 2: Final Performance over 3 seeds on SuperMarioBros.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Efficient exploration is important to learn a (near-) optimal policy for reinforcement learning (RL) in huge state space ( Sutton & Barto, 1998 ). Dithering strategies like epsilon-greedy, Gaussian action noise, and Boltzmann exploration are inefficient and require exponential interactions to explore the whole state space. In contrast, deep exploration ( Osband et al., 2016 ) overcomes this dilemma via temporally extended behaviors with a long-term vision. Recently, proposed methods include the in- trinsically motivated goal exploration process (IMGEP) ( Forestier et al., 2017 ), and maximum state entropy exploration (MSEE) ( Hazan et al., 2019 ). In particular, IMGEP selects interesting states from the experience buffer as goals for a goal-conditioned exploration policy. In this way, explo- ration behaviors are naturally temporally-extended via accomplishing self-generated goals. On the other hand, MSEE aims to search for a policy such that it maximizes the entropy of state distribution. In this way, the agent can escape from the local optimum caused by insufficient state exploration. In this paper, we show that the target of maximizing the support of state distribution (discovering new states) and maximizing the entropy of state distribution (unifying visited state distribution) can be both achieved by the goal-conditioned policy. From this connection, we propose an exploration method called novelty-pursuit. Abstractly, our method performs in two stages: first, it selects a visited state with the least visitation counts as the goal to reach the boundary of the explored region; then, it takes random actions to explore the non-explored region. An illustration can be seen in  Figure 1 . Intuitively, this process is efficient since the agent avoids exploring within the explored region. Besides, the exploration boundary will be expanded further as more and more new states are discovered. Finally, the agent will probably explore the whole state space to find the optimal policy. A naive implementation of the above strategies can lead to inefficient exploration and exploitation on complex environments. First, to tackle the problem of the curse of dimension and exhaustive stor- age when selecting the least visited states, we approximate the visitation counts via prediction errors given by Random Network Distillation ( Burda et al., 2019b ). Besides, we observe that previous methods used in IMGEP ( Forestier et al., 2017 ) are inefficient to train the goal-conditioned explo- ration policy. We employ training techniques based on reward shaping ( Ng et al., 1999 ) and HER ( Andrychowicz et al., 2017 ) to accelerate training the goal-conditioned policy. Finally, we addition- ally train an unconditioned exploitation policy to utilize samples collected by the goal-conditioned Under review as a conference paper at ICLR 2020 exploration policy with environment-specific rewards. Thus, the exploration and exploitation is de- coupled in our method. Our contributions are summarized as follows: (1) We disclose that goal-conditioned behaviors can also maximize the state entropy, which bridges the intrinsically motivated goal exploration process and the maximum state entropy explore. (2) We propose a method called novelty-pursuit from this connection and give practical implementations. (3) We demonstrate the exploration efficiency of the proposed method and achieve better performance on environments from the maze, Mujoco tasks, to long-horizon video games of SuperMarioBros.

Section Title: BACKGROUND
  BACKGROUND

Section Title: Reinforcement Learning
  Reinforcement Learning In the standard reinforcement learning framework ( Sutton & Barto, 1998 ) a learning agent interacts with a Markov Decision Process (MDP). The sequential decision process is characterized as follows: at each time t, the agent receives a state s t from the environment and selects an action a t from its policy π(s, a) = Pr{a = a t |s = s t }; that decision is sent back to the environment, and the environment gives a reward signal r(s t , a t ) and transits to the next state s t+1 based on the state transition probability p a ss ′ = Pr{s ′ = s t+1 |s = s t , a = a t }. This process repeats until the agent encounters a terminal state after which the process restarts. The main target of reinforcement learning is to maximize the expected discounted return E π [ ∑ ∞ t=0 γ t r t ] in an unknown environment, where γ ∈ (0, 1] is a factor that balances the importance of future reward. Without information about environment dynamics and task-specific rewards in advance, the agent needs ex- ploration to discover potential valuable states. Apparently, the learned policy may be sub-optimal if the exploration strategy cannot lead to explore the whole state space.

Section Title: Intrinsically Motivated Goal Exploration Process
  Intrinsically Motivated Goal Exploration Process Intrinsically motivated goal exploration pro- cess (IMGEP) ( Baranes & Oudeyer, 2009 ;  Forestier et al., 2017 ) relies on a goal-conditioned (or goal-parameterized) policy π g for unsupervised exploration. It involves the following steps: 1) se- lecting an intrinsic or interesting state from the experience buffer as the desired goal; 2) exploring with a goal-conditioned policy π g (s, a, g) = Pr{a t = a|s t = s, g t = g}; 3) reusing experience for an exploitaion policy π e (s, a) = Pr{a t = a|s t = s} to maximize the external reward. Note that the performance of exploitation policy π e relies on samples collected by the goal-exploration policy π g . Thus, the criterion of goal selection is crucial for IMGEP.

Section Title: Maximum State Entropy Exploration
  Maximum State Entropy Exploration Maximum state entropy exploration ( Hazan et al., 2019 ) aims to search an exploration policy π * such that it maximizes the entropy of induced state distri- bution (or minimizes the KL-divergence between the uniform distribution and induced state distri- bution) among the class of stationary policies (i.e., π * ∈ arg max π H[d π ], where d π is the state distribution induced by π). Without any information about tasks given by the environment, we think maximum state entropy exploration is safe for exploitation.

Section Title: IMGEP WITH MAXIMUM STATE ENTROPY EXPLORATION
  IMGEP WITH MAXIMUM STATE ENTROPY EXPLORATION In this section, we bridge the intrinsically motivated goal exploration process and maximum state entropy exploration. We begin with practical considerations when maximizing state entropy, then analyze the exploration characteristics of the proposed goal-selection method for IMGEP.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 In practice, an exact density estimator for high-dimension state space is intractable, and the state space is unknown, which leads to an empirical state distribution over visited states. The differences are important. For example, directly optimizing the entropy of empirical state distribution over visited states is not what we want, because it ignores the non-visited states outside of the empirical state distribution (see the top row in  Fig 2 ). Instead, we need to first maximize the support of induced state distribution (i.e., discovering new states), then we maximize the entropy of induced state distribution with full support (see the bottom row in  Fig 2 ). In the following, we demonstrate that selecting the states with the least visitation counts among visited states as goals can achieve the above functions under some assumptions. Let the set {1, 2, · · · , |S|} denotes the state space S, π 1:t denotes the set of policies {π 1 , π 2 , · · · , π t } over previous iterations, π t+1 denotes the policy of next iteration, x i t denotes the cumulative visi- tation counts of state i induced by history policies π 1:t , and N t = ∑ |S| i=1 x i t denotes the sum of all state visitation counts. Hence, the entropy of empirical state distribution induced by policies π 1:t is defined as H[d π1:t (s)] = ∑ |S| i=1 x i t Nt log x i t Nt (H t for short), and the counting measure of empirical state distribution support induced by policies π 1:t is defined as µ[d π1:t (s)] = ∑ |S| i=1 I(x i t ≥ 1) (µ t for short), where I is the indicator function. The theoretical analysis starts with the situation that each iteration the goal-conditioned exploration policy can only select a state to visit without consideration of trajectories towards the goal. Our question is which state to visit gives the most benefits in terms of maximum state entropy. This question is closely related to the goal generation in IMGEP. To facilitate the analysis, let the unit vector e = [0, · · · , 1, . . . ] ∈ R |S| denotes a choice (i.e., e(i) = 1 indicates that the policy selects i-th state to visit). Note that x t+1 = x t + e t with this assumption. This Proposition states visiting the non-visited states is to maximize the counting measure of induced state distribution support. The agent improves its policy by discovering new valuable states. In practical applications, we don't have access to non-visited states in advance. In other words, we can't select these non-visited states as goals since they are not contained in the experience buffer. To deal with this problem, we assume that the chance of discovering non-visited states is high when the agent perform random actions to explore around the exploration boundary. The exploration boundary can be understood as the set of visited states with the least visitation counts (See  Figure 1  for the illustration). Our assumption is based on the fact that the total visitations counts of the visited region are large and the total visitation counts of the non-visited region are small. In conclusion, the goal-conditioned exploration policy is asked to reach the exploration boundary, then it performs random actions to discover new states to maximize the counting measure. We provide the proof in the appendix A.1. Theorem 1 characterizes the behavior of visiting the states with the least visitations when the whole state space has been explored (i.e., the stage after maximizing the counting measure of induced state distribution support). Since Theorem 1 still suggests selecting states with the least visitation counts as goals, the above method can also be applied to maximize the entropy of induced state distribution. Actually, it is easy to unify the two stages via a smoothed entropy H σ (d π ) = −E dπ [log(d π ) + σ] ( Hazan et al., 2019 ). For our problem, the definition of entropy is proper by assigning non-visited states with a "dummy" visitation counts between 0 and 1. In that case, Theorem 1 still holds and suggests firstly selecting these non-visited states and subsequently selecting the states with least visitation counts to maximize the smoothed state entropy. The proposed exploration method is called novelty-pursuit. We notice that the above analysis neglects the influences of trajectories towards the exploration boundary. However, the fluctuation of state distribution entropy by the trajectories towards the exploration boundary is less significant from practical considerations. In fact, the goal-conditioned policy should be trained to reach the exploration boundary quickly and pays more efforts to discover new states around the exploration boundary, as our experiment results in Section 5.1 indicate.

Section Title: METHOD
  METHOD In this section, we present practical implementations for the proposed method. How to approximate visitation counts in high-dimension space and how to estimate the exploration boundary is given in Section 4.1. We describe the training technique of goal-conditioned policy in Section 4.2. Fi- nally, we introduce an exploitation policy to learn the experience collected by the goal-conditioned exploration policy in Section 4.3. We outline the proposed exploration method in Algorithm 1.

Section Title: APPROXIMATING EXPLORATION BOUNDARY IN HIGH-DIMENSION SPACE
  APPROXIMATING EXPLORATION BOUNDARY IN HIGH-DIMENSION SPACE Generally, computing the visitation counts in high-dimension space is intractable. However, it is possible to build some variables related to the visitation counts. For example,  Burda et al. (2019b)  show that prediction errors given by two randomly initialized network have a strong relationship to the number of training samples on the MNIST dataset. Thus, we can use the prediction errors to sort visited states. Other approaches like pseudo-counts ( Bellemare et al., 2016 ;  Ostrovski et al., 2017 ) can be also applied, but we find that RND is easy to scale up. RND is consist of two randomly initialized neural networks: a fixed network called target network f (x; ω t ), and a trainable network called predictor networkf (x; ω p ). Both two networks take a state s as input and output a vector with the same dimension. Each time a batch of data feeds into the predictor network to minimize the difference between the predictor network and the target network concerning the predictor network's parameters, shown in Equation 1. In practice, we employ an online learning setting to train RND and maintain a priority queue to store states with the highest prediction errors. In particular, after a goal-conditioned policy collects a mini-batch of transitions, this data feed to train the predictor network. Also, a state with high prediction error will be stored into the priority queue and the state with the least prediction error will be removed out of the priority queue if full. This process repeats and no historical data will be reused to train the predictor network. Besides, each iteration a state will be selected from the priority queue as a goal for the goal-conditioned policy. After achieving the goal, the exploration policy will perform random actions to discover new states. Consider the bias due to approximation, we sample goals from a distribution based on their prediction errors (e.g., softmax distribution). end if end for end for

Section Title: TRAINING GOAL-CONDITIONED POLICY EFFICIENTLY
  TRAINING GOAL-CONDITIONED POLICY EFFICIENTLY Before we describe the training techniques for the goal-conditioned policy, we emphasize that train- ing this policy doesn't require the external reward signal from the environment. But we additionally use the external reward for the goal-conditioned policy to reduce the mismatch behaviors between the goal-conditioned policy π g and the exploitation policy π e . Following multi-goal reinforcement learning ( Andrychowicz et al., 2017 ;  Plappert et al., 2018a ), we manually extract goal information from state space. Specifically, each state s is associated with an achieved goal of ag, and the desired goal is denoted as g. To avoid ambiguity, a goal-conditioned policy π g (s, a, g; θ) 1 is asked to accomplish a desired goal g. For our settings, the achieved goal is coordinate information. A proper reward function for the goal-conditioned policy is an indicator function with some tol- erance, shown in Equation 2. With a little abuse of notations, between the achieved goal ag and the desired goal g we use d(ag, g) to denote some "distance" (e.g., L 1 or L 2 norm) between the achieved goal ag and the desired goal g. If the distance is less than some threshold ϵ, the goal- conditioned policy receives a positive reward otherwise zero. Note that this function is also be used to judge whether agents reach the exploration boundary. However, the training of goal-conditioned policy is slow with this sparse reward function. Next, we introduce some techniques to deal with this problem. Reward shaping introduces additional training rewards to guide the agent. Reward shaping is in- variant to the optimal policy if shaping reward function is a potential function ( Ng et al., 1999 ). Under review as a conference paper at ICLR 2020 Specifically, we define the difference of two consecutive distances (between the achieved goal and the desired goal) as shaping reward function, shown in Equation 3. Since shaping reward function is dense, it can lead to substantial reductions in learning time. Verification of the optimal goal- conditioned policy is invariant between this function and the indicator reward function is given in Appendix A.2. Alternatively, one can use also Hindsight Experience Replay (HER) ( Andrychowicz et al., 2017 ) to train the goal-conditioned policy via replacing each episode with an achieved goal rather than one that the agent was trying to achieve. But one should be careful since HER changes the goal distribution for learning. Besides, one can also utilize past trajectories to accelerate training, which we discuss in Appendix A.3.

Section Title: EXPLOITING EXPERIENCE FROM EXPLORATION POLICY
  EXPLOITING EXPERIENCE FROM EXPLORATION POLICY Parallel to the goal-conditioned exploration, we additionally train an unconditioned exploitation policy π e , which only takes the state as input. This policy learns from experience collected by the exploration policy π g in an off-policy learning fashion. At the same time, the exploitation policy also interacts with the environment to mitigate the side effect of exploration error ( Fujimoto et al., 2019 ), a phenomenon that off-policy learning degenerates when data from the exploration policy is not correlated to the experience generated by the exploitation policy. Note that exploitation policy is trained with an RL objective to maximize expected discounted external return. Therefore, the exploration and exploitation are naturally decoupled, which turns out to help escape the local opti- mum on SuperMarioBros environments. From this perspective, our method is distinguished from  Go-Explore Ecoffet et al. (2019) , which employs exploration followed by exploitation.

Section Title: EXPERIMENT
  EXPERIMENT In this section, we aim to answer the following research questions: 1) Does novelty-pursuit effec- tively maximize the state entropy? 2) Do the proposed goal-selection criterion and training tech- niques improve performance for IMGEP? 3) How does the performance of novelty-pursuit compare with the state-of-the-art approaches in complex environments? We conduct experiments from the simple maze environments, Mujoco tasks, to long-horizon video games of SuperMarioBros to eval- uate the proposed method. Detailed policy network architecture and hyperparameters are given in Appendix A.6 and A.7, respectively. Here we briefly describe the environment settings (see  Figure 3  for illustrations). Detailed settings are given in the Appendix A.5.

Section Title: Empty Room & Four Rooms
  Empty Room & Four Rooms An agent navigates in the maze of 17×17 to find the exit ( Chevalier- Boisvert et al., 2018 ). The agent receives a time penalty until it finds the exit and receives a positive reward. The maximum return for both two environments is +1, and the minimum total reward is −1. Note that the observation is a partial image of shape (7, 7, 3). FetchReach. A 7-DOF Fetch Robotics arm (simulated in the Mujoco ( Todorov et al., 2012 )) is asked to grip spheres above a table. There are 4 spheres on the table, and the robot receives a positive reward of +1 when its gripper catches a sphere (the sphere will disappear after being caught) otherwise it receives a time penalty. The maximum total reward is +4, and the minimum total reward is −1.

Section Title: SuperMarioBros
  SuperMarioBros A Mario agent with raw image observation explores to discover the flag. The reward is based on the score given by the NES simulator ( Kauten, 2018 ) and is clipped into −1 and +1 except +50 when getting a flag. There are 24 stages in the game, but we only focus on the 1-1, 1-2, and 1-3.

Section Title: COMPARISON OF EXPLORATION EFFICIENCY
  COMPARISON OF EXPLORATION EFFICIENCY In this section, we study the exploration efficiency in terms of the state distribution entropy. We focus on the Empty Room environment because it is tractable to calculate the state distribution entropy. Note that we don't use any external reward the observation for RND is a local-view image. We consider the following baselines: 1) random: uniformly selecting actions; 2) bonus: a policy receiving exploration bonus based on the prediction errors of RND ( Burda et al., 2019b ); 3) novelty- Under review as a conference paper at ICLR 2020 (a) Empty Room (b) Four Rooms (c) FetchReach (d) SuperMarioBros pursuit: the proposed method. We also consider three variants of our method: 4) novelty-pursuit- planning oracle: the proposed method with a perfect goal-conditioned policy; 5) novelty-pursuit- counts-oracle: the proposed method with selecting goals based on true visitation counts; 6) novelty- pursuit-oracles: the proposed method with both two oracles. The results are summarized in  Table 1 . Note that the maximum state distribution entropy for this environment is 5.666. First, we can see that novelty-pursuit achieves a higher entropy than the random and bonus method. Though exploration bonus via prediction errors of RND may help makes an exploration-exploitation trade-off ( Burda et al., 2019b ), but is inefficient to a maximum state entropy exploration. We at- tribute this to delayed and indirect feedbacks of the exploration bonus. Second, when the planning oracle and visitation counts oracle are available, the entropy of our method roughly improves by 0.228 and 0.124, respectively. We notice that the planning-oracle avoids exploration towards the ex- ploration boundary and spends more meaningful steps to explore around the exploration boundary, thus greatly improves the entropy. Based on this observation, we think accelerating goal-conditioned policy training is more important for our method. Actually, we find the proposed method can satisfy our need to approximate the exploration boundary via prediction errors of RND (See Appendix A.4 for more results). Third, the combination of two oracles gives a near-perfect performance (the gap between the maximum state entropy is only 0.039). This result demonstrates that goal-condition exploration behaviors presented by novelty-pursuit can maximize the state entropy and validates the analysis in Section 3.

Section Title: ABLATION STUDY OF GOAL-SELECTION AND TRAINING TECHNIQUES
  ABLATION STUDY OF GOAL-SELECTION AND TRAINING TECHNIQUES In this section, we study the factors that contribute to our method by ablation experiments. Firstly, we focus on the criterion of goal-section in IMGEP. We compare novelty-pursuit with two other goal- selection methods: 1) random-selection: selecting states randomly from the experience buffer; 2) learning-progress: selecting a feasible state (goal success rate is between 0.3 and 0.7) with probabil- ity of 0.8 and an arbitrary visited state with the probability of 0.2, which is adopted from ( Forestier et al., 2017 ). Results on the Empty Room are shown in  Figure 4 . Secondly, we study how goal- conditioned policy learning affects performance. We compare HER and the reward-shaping with distance reward (i.e., reward based on L 1 norm in our problem) used in ( Forestier et al., 2017 ). Results on the Empty Room are shown in  Figure 5 . From  Figure 4 , we see that IMGEP doesn't work when randomly selecting goals, but novelty-pursuit gives a greater boost compared to the learning-progress. We think the reason is that this heuristic method is brittle to the estimation of goal success rate and lacks an explicit exploration objective. From  Figure 5 , we find that the IMGEP with HER or reward shaping outperforms than the IMGEP with distance reward. As discussed in  Ng et al. (1999) , reward based on distance may change the optimal behavior of goal-condition exploration policy, thus hurts the performance for IMGEP.

Section Title: EVALUATION ON COMPLEX ENVIRONMENTS
  EVALUATION ON COMPLEX ENVIRONMENTS In this section, we compare different methods in terms of external reward. We will see that without sufficient and efficient exploration, the policy may be stuck into the local optimum. Two baseline methods using reinforcement learning are considered: 1) vanilla: DDPG ( Lillicrap et al., 2016 ) with Gaussian action noise on Fetch Reach and ACER ( Wang et al., 2017 ) with policy entropy regular- ization on others; 2) bonus: an off-policy version of ( Burda et al., 2019b ) that combines the external reward and intrinsic reward based on the vanilla policy. Note reported results of novelty-pursuit are the performances of the exploitation policy π e rather than the goal-conditioned exploration policy π g . We keep the number of samples and training iterations same for all methods. First, we consider the previously used Empty Room and the Four Room environments. The results are shown in  Figure 6 . We see that the vanilla policy hardly finds the exit. Novelty-pursuit is comparative to bonus and outperforms bonus on the Four Rooms environment, where we observe that bonus is somewhat misled by the intrinsic reward though we have tried many weights to balance the external reward and intrinsic reward. Secondly, we consider the FetchReach environment and results are shown in  Figure 6 . We see that novelty-pursuit can consistently grip 4 spheres while other methods sometimes fail to efficiently explore the whole state space to grip 4 spheres. Finally, we consider the SuperMarioBros environments, in which it is very hard to discover the flag due to the huge space state and the long horizon. Learning curves are plotted in  Figure 7  and the final performance is listed in  Table 2 . We find the vanilla method gets stuck into the local optimum on SuperMarioBros-1-1 while the bonus method and ours can find a near-optimal policy. All methods perform well on SuperMarioBros-1-2 thanks to dense rewards. On SuperMarioBros-1-3, reward is sparse and the task is very challenging. We plot trajectories of SuperMarioBros-1-3 in  Figure 8 , and more results can be found in Appendix A.4. It turns out only our method can get positive rewards via a deep exploration presented by the goal-conditioned policy on SuperMarioBros-1-3.

Section Title: Exploration
  Exploration Traditionally, the exploration strategy is based on the exploitation policy that receives an external reward from the environment. Traditional exploration methods include injecting noise on action space ( Mnih et al., 2015 ;  Lillicrap et al., 2016 ) or parameter space ( Plappert et al., 2018b ;  Fortunato et al., 2018 ), and adding the policy's entropy regularization ( Schulman et al., 2017 ;  Mnih et al., 2016 ). For tabular Markov Decision Process, there are lots of work utilizing confidence based reward to balance exploration and exploitation ( Kearns & Singh, 2002 ;  Strehl & Littman, 2008 ;  Kolter & Ng, 2009 ;  Lattimore & Hutter, 2014 ). Several exploration strategies for deep RL based approximation visitation counts have been proposed in high-dimension space ( Bellemare et al., 2016 ;  Ostrovski et al., 2017 ). Another type of exploration is curiosity-driven exploration. These methods track the uncertainty of dynamic ( Stadie et al., 2015 ;  Pathak et al., 2017 ;  Burda et al., 2019a ;b) to explore intrinsic states. Deep (temporally extended) exploration via tracking the uncertainty of value func- tion is studied in ( Osband et al., 2016 ). Besides, maximum (policy) entropy reinforcement learning Under review as a conference paper at ICLR 2020 encourages exploration by maximizing the cumulative sum of external reward and policy entropy ( Ziebart et al., 2008 ;  Haarnoja et al., 2017 ;  O'Donoghue et al., 2016 ;  Haarnoja et al., 2018 ). Recently,  Hazan et al. (2019)  introduce a new exploration objective: maximum state entropy. They provide an efficient algorithm when restricted to a known tabular MDP (a density estimator oracle is required for an unknown tabular MDP) and gives the theoretical analysis. We derive the criterion of goal generation based on the principle of maximum state entropy. Our method is based on the framework of intrinsically motivated goal exploration processes (IMGEP) ( Baranes & Oudeyer, 2009 ;  Forestier et al., 2017 ;  Péré et al., 2018 ). Go-Explore ( Ecoffet et al., 2019 ) is reminiscent of IMGEP and achieves dramatic improvement on the hard exploration problem of Montezumas Revenge. But with the assumption that the environments are resettable or deterministic and many hand-engineering designs, Go-Explore is restricted to specific environments. Our method shares a similar exploration strategy like Go-Explore, but our method is implemented practically and can be applied to stochastic environments. Importantly, we aim to answer the core question: why such defined goal-conditioned exploration is efficient?

Section Title: Goal-conditioned Policy
  Goal-conditioned Policy By taking environment observation and desired goal as inputs, the goal- conditioned policy is expected to accomplish a series of tasks.  Schaul et al. (2015)  propose the uni- versal value function approximator (UVFA) and train it by bootstrapping from the Bellman equation. However, training goal-condtioned policy is also still a challenging problem due to goal-condition reward is sparse (e.g. 1 for success, 0 for failure).  Andrychowicz et al. (2017)  propose hindsight experience replay (HER) by replacing each episode with an achieved goal rather than one that the agent was trying to achieve. This operation introduces more reward signals and serves as an implicit curriculum.  Florensa et al. (2018)  use a generator network to adaptively produce artificial feasible goals. We also use a goal-conditioned policy, but goals are selected from the experience buffer rather than being specified in advance. What's more, we utilize the technique of reward shaping ( Ng et al., 1999 ) to accelerate training.

Section Title: Learning from experience
  Learning from experience Off-policy reinforcement learning algorithms such as DQN( Mnih et al., 2015 ), DDPG ( Lillicrap et al., 2016 ), and ACER ( Wang et al., 2017 ), reuse experience to improve data efficiency. Besides, how to additionally utilize (good) experience to overcome exploration dilemma is studied in ( Oh et al., 2018 ;  Goyal et al., 2019 ). These works are perpendicular to ours since we focus on how to discover these valuable states.

Section Title: CONCLUSION
  CONCLUSION This paper bridges the intrinsically motivated goal exploration process (IMGEP) and the maximum state entropy exploration (MSEE). We propose a method called novelty-pursuit from the connection. We demonstrate the proposed method is efficient towards exploring the whole state space. Therefore, the proposed method can escape from the local optimum and heads the (near-) optimal policy. We notice that current training techniques of the exploitation policy are based on an RL objective, which may not be efficient to utilize experience collected by the exploration policy. Theoretically, the influence of trajectories towards the exploration bound should be considered. We leave these for future works.

```
