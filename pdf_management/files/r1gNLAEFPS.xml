<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 NEURAL ODES FOR IMAGE SEGMENTATION WITH LEVEL SETS</article-title></title-group><abstract><p>We propose a novel approach for image segmentation that combines Neural Or- dinary Differential Equations (NODEs) and the Level Set method. Our approach parametrizes the evolution of an initial contour with a NODE that implicitly learns from data a forcing function describing the evolution. In cases where an initial contour is not available or to alleviate the need for careful choice or design of contour embedding functions, we propose using NODEs to directly evolve the embedding of an input image into a pixel-wise dense semantic label. We evalu- ate our methods on kidney segmentation (KiTS19) and on salient object detection (PASCAL-S, ECSSD and HKU-IS). In addition to improving initial contours pro- vided by deep learning models while using a fraction of their number of parame- ters, our approach achieves F &#946; scores that are higher than several state-of-the-art deep learning algorithms.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Image segmentation is the task of delineating pixels belonging to semantic labels. The ability to automatically segment objects is important because accurate labeling is expensive and hard (<xref ref-type="bibr" rid="b0">Vit- tayakorn &amp; Hays, 2011</xref>; <xref ref-type="bibr" rid="b12">Zhang et al., 2018</xref>). Automatic image segmentation can have large impact in many domains, e.g. obstacle avoidance in autonomous driving and treatment planning in medical imaging.</p><p>Accurate classification of pixels in close proximity to inter-class boundaries remains a challenging task in image segmentation. Object boundaries can have high curvature contours or weak pixel intensity that complicate separating the object from surrounding ones. In deep CNNs (<xref ref-type="bibr" rid="b32">Simonyan &amp; Zisserman, 2014</xref>; <xref ref-type="bibr" rid="b0">Zeiler &amp; Fergus, 2014</xref>; <xref ref-type="bibr" rid="b16">Szegedy et al., 2015</xref>; <xref ref-type="bibr" rid="b12">He et al., 2016</xref>; <xref ref-type="bibr" rid="b4">Chen et al., 2017</xref>), the object-of-interest and surrounding competing objects can provide equal context to a receptive field of a boundary pixel, which can make accurate classification difficult. Humans also find it difficult to accurately label pixels near object boundaries.</p><p>Level Set methods (<xref ref-type="bibr" rid="b21">Zhao et al., 1996</xref>; <xref ref-type="bibr" rid="b2">Brox et al., 2006</xref>) and Active Shapes (<xref ref-type="bibr" rid="b26">Paragios &amp; Deriche, 2000</xref>; <xref ref-type="bibr" rid="b3">Chan &amp; Vese, 2001</xref>) have been proposed to incorporate shape and image priors to mitigate boundary ambiguities (<xref ref-type="bibr" rid="b35">Tsai et al., 2003</xref>; <xref ref-type="bibr" rid="b30">Rousson &amp; Paragios, 2002</xref>). The Level Set method for image segmentation evolves an initial contour of an object-of-interest along the normal direction with a forcing function. A contour is represented by an embedding function, typically a signed distance function, and its evolution amounts to solving a differential equation (<xref ref-type="bibr" rid="b25">Osher &amp; Sethian, 1988</xref>).</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>In this work, we extend the formulation of the level set method. Inspired by the recent progress in Neural Ordinary Diferential Equations (NODEs) (Chen et al., 2018; <xref ref-type="bibr" rid="b8">Dupont et al., 2019</xref>; <xref ref-type="bibr" rid="b10">Gholami et al., 2019</xref>), we propose to use NODEs to solve the level set formulation of the contour evolution, thus learning the forcing function in an end-to-end data driven manner. Unlike earlier attempts in combining the level set method with CNNs, we benefit from NODEs parametrization of the deriva- tive of the contour because it allows us to incorporate external constraints that guide the contour evolution, e.g. by adding a regularization penalty to the curvature of the front or exploiting images at the evolving front by extracting appearance constraints in a non-supervised way.</p><p>Finally, similar to experiments in (Chen et al., 2018), to alleviate the need for careful choice or design of contour embedding functions, we propose a NODE-based method that evolves an image embedding into a dense per-pixel semantic label space.</p><p>To the best of our knowledge, this work is the first to apply Neural ODEs to real world problems. We validate our methods on two 2D segmentation tasks: kidney segmentation in transversal slices of CT scans and salient object segmentation. Given an initial estimate of kidney via existing algorithms, our method effectively evolves the initial estimates and achieves improved kidney segmentation, as we show in <xref ref-type="fig" rid="fig_0">Figure 1</xref>. On real life salient objects, in addition to contour evolution, we use our method to directly evolve the embedding of an input image into a pixel-wise dense semantic label. Following (<xref ref-type="bibr" rid="b7">Hu et al., 2017</xref>), we compare against the results in (<xref ref-type="bibr" rid="b15">Wang et al., 2017</xref>; <xref ref-type="bibr" rid="b14">Li et al., 2016</xref>; <xref ref-type="bibr" rid="b14">Li &amp; Yu, 2015</xref>; <xref ref-type="bibr" rid="b21">Zhao et al., 2015</xref>; <xref ref-type="bibr" rid="b19">Lee et al., 2016</xref>; <xref ref-type="bibr" rid="b15">Wang et al., 2015</xref>; <xref ref-type="bibr" rid="b7">Hu et al., 2017</xref>) and achieve &#969;-F &#946; scores, PASCAL-S 0.668 and ECSSD 0.768, that are higher than several state-of-the art algorithms. Our results suggest the potential of utilizing NODEs for solving the contour evolution of level set methods or the direct evolution of image embeddings into segmentation maps. We hope our findings will inspire future research in using NODEs for semantic segmentation tasks. We foresee that our method would allow for intervention on intermediate states of the solution of the ODE, allowing for injection of shape priors or other regularizing constraints.</p><p>In summary, our contributions are:</p><p>&#8226; We propose to use NODEs to solve the level set formulation of the contour evolution.</p><p>&#8226; We propose using NODEs to learn the forcing function in an end-to-end data driven way.</p><p>&#8226; We show NODEs can also evolve image embeddings directly into dense per-pixel semantic label spaces, which may alleviate the need for careful choice or design of contour embed- ding functions.</p></sec><sec><title>METHODS</title><p>Suppose I is a 2D image, S is the contour of an object we want to segment, and &#966; is a contour embedding function, defined as a distance map, such that S = {(x, y)|&#966;(x, y) = 0}. We assume an initial but rough contour of the object is given by a human operator or by an existing algorithm. A level set segmentation (<xref ref-type="bibr" rid="b25">Osher &amp; Sethian, 1988</xref>) solves a differential equation to evolve a contour along its normal direction with a speed function F as: d&#966; dt = |&#8711;&#966;|F for t &#8712; [0, 1], (1) where the initial value &#966; 0 (x, y) is defined as a signed Euclidean distance from (x, y) to the closest point on the initial contour S 0 . The speed function F is often modelled to be a function of the target image I, the shape statistics of the object contour (derived from training shapes), or a regularizing curvature term (&#8711; &#8711;&#966; &#8711;&#966; ).</p><p>In Neural ODEs, we parametrize the derivative of the hidden state h using a neural network f &#952; parametrized by &#952;:</p><p>The relationship between Eq. 1 and Eq. 2 is immediate. In the next section, we propose two approaches that adapt NODEs to the level set method for image segmentation.</p></sec><sec><title>CONTOUR EVOLUTION WITH NODES</title><p>We propose to solve a more general form of Eq. 1 to evolve an initial contour estimate&#966; for image segmentation. We define the state of the NODE to be&#966; augmented with the input image's embedding, h. We then advance the augmented state, &#947; = (&#966;, h), using NODEs, which can be interpreted as estimating the speed function F described in Eq. 1. Mathematically, &#947; = (&#966;, h), d&#947; dt = f &#952; (&#947;, t) for t &#8712; [0, 1], &#947; (0) = (&#966; (0) , h (0) ), &#966; =&#966; (1) + &#968;(&#947; (1) ), (3) where t is the time step in the evolution, &#947; is the augmented state of the NODE, f is a neural network parametrized by &#952;,&#966; (0) is the initial value of the distance map, h (0) is the initial value of the image embedding, &#968; is a learned function and&#966; is the dense per-pixel distance map prediction. Figure 2a schematically illustrates our initial contour evolution approach. Throughout this paper, we will refer to this method as Contour Evolution.</p></sec><sec><title>IMAGE EVOLUTION WITH NODES</title><p>In our first approach, we obtain a final optimal contour by evolving an initial estimate. In our second approach, inspired by Chen et al. (2018), we evolve an image embedding h and project it into a dense per-pixel distance map&#966;, whose zero level set defines the final segmentation map,</p><p>where t is the time step in the evolution, f is a neural network parametrized by &#952;, I is an image, &#955; is a learned image embedding function and &#968; is a learned function that maps an image embedding to a distance map. Figure 2b schematically illustrates our direct image evolution approach. Throughout this paper, we will refer to this method as Image Evolution.</p></sec><sec><title>IMPLEMENTATION</title><p>In the following subsections, we describe our design choices in loss function and their regularization terms, architectures, strategies for emphasizing the evolution of the contour on a region of interest. We also detail our model initialization strategies to prevent drifting from the sub-optimal initial value, and choices of error tolerances and activation normalization.</p></sec><sec><title>LOSS FUNCTION AND REGULARIZATION TERMS</title><p>We optimize the parameters of our NODE models, described in Figures 2a and 2b, to minimize the empirical risk computed as the Mean Squared Error (MSE) between the target (&#966;) and predicted (&#966;) distance maps. We remind the reader that although our techniques can access intermediate NODE states, which could allow injection of priors or other constraints, we do not explore this in our current experiments, and relay it to future work.</p></sec><sec><title>NARROW BAND AND RE-INITIALIZATION</title><p>In the level set formulation, all levels that describe the propagating contour are tracked. <xref ref-type="bibr" rid="b0">Adalsteins- son &amp; Sethian (1995)</xref> proposed limiting the evolution to the subset of levels within a narrow band of the zero level contour. In our approach, we obtain the equivalent of a narrow band by applying a hyperbolic tangent non- linearity on the evolved distance map. It effectively attenuates the contribution of levels in the optimization process. This transformation is especially valuable in refinement setups because it weights the gradients of the loss according to the proximity to the contour 1 .</p><p>Re-initialization of &#966; is another common practice in classical level set methods. It ensures the states in the trajectory of the numerical solution remain valid distance maps. (<xref ref-type="bibr" rid="b33">Sussman et al., 1994</xref>; <xref ref-type="bibr" rid="b11">Hartmann et al., 2010</xref>) propose to first extract a zero level set of an evolving state, and re-calculate a distance map of that contour. In our experiments, we found that our optimization is not sensitive to non valid distance maps, and we did not find it necessary to reinitialize &#966;.</p></sec><sec><title>PARAMETER INITIALIZATION AND LEARNING RATE RAMPUP</title><p>In tasks where the initial value is already close to the desired solution, not initializing the model parameters to represent the identity function and not using learning rate ramp up can slow down the optimization process as the model predictions can immediately drift away from the initial value. In addition to using learning rate rampup, we prevent this issue by setting the weights and biases on the last layer of the NODE and Postnet layers to zero. This approach has been successfully used in normalizing flow models (<xref ref-type="bibr" rid="b17">Kingma &amp; Dhariwal, 2018</xref>; <xref ref-type="bibr" rid="b28">Prenger et al., 2019</xref>).</p></sec><sec><title>ADAPTIVE SOLVERS AND ERROR TOLERANCES</title><p>In ordinary differential equations, adaptive step solvers vary the step size according to the error estimate of the current step and the error tolerance. If the error estimate is larger than the threshold, the step will be redone with a smaller size until the error is smaller than the error tolerance. The error tolerance e i tol given the current state i is the sum of the absolute error tolerance a tol and the infinity norm of the current state h weighted by the relative error tolerance r tol :</p><p>Under review as a conference paper at ICLR 2020 Given that we do not know in advance the infinity norm of h i , which in our case contains the image embedding as described in Equations 3 and 4, we set the contribution of the relative error tolerance term to zero and adjust the absolute error tolerance.</p></sec><sec><title>ACTIVATION NORMALIZATION</title><p>When the batch size is too small for using batch size dependent normalization schemes like Batch- Norm (<xref ref-type="bibr" rid="b16">Ioffe &amp; Szegedy, 2015</xref>), researchers rely on dataparallel multi-processor training setups with BatchNorm statistics reduced over all processes, for example SyncBatchNorm in the APEX library (<xref ref-type="bibr" rid="b31">Sarofeen et al., 2019</xref>).</p><p>When training in multi-processor environments with data parallelism and NODEs with adaptive step solvers, the number of NODE function evaluations on each processor can differ. Consequently, the number of BatchNorm calls inside each NODE layer will be dependent on the number of function evaluations, thus making reduction over processes complex.</p><p>In our setup, we circumvent this issue by using GroupNorm (<xref ref-type="bibr" rid="b21">Wu &amp; He, 2018</xref>) in layers where no convolution groups are used and LayerNorm (<xref ref-type="bibr" rid="b1">Ba et al., 2016</xref>) when convolution groups are used.</p></sec><sec><title>EXPERIMENTS</title></sec><sec><title>DATASETS AND TASKS</title><p>The KiTS19 Challenge Data (<xref ref-type="bibr" rid="b13">Heller et al., 2019</xref>) consists of CT scans from 210 patients with tumour and kidney annotations. The MSRA10K dataset (<xref ref-type="bibr" rid="b7">Cheng et al., 2014</xref>) consists of 10000 images with pixel-level saliency labeling from the MSRA dataset. The PASCAL-S dataset (<xref ref-type="bibr" rid="b14">Li et al., 2014</xref>) consists of free-viewing fixations on a subset of 850 images from PASCAL VOC, the ECSSD dataset (<xref ref-type="bibr" rid="b13">Yan et al., 2013</xref>) consists of 1000 semantically meaningful but structurally complex images and the HKU-IS (<xref ref-type="bibr" rid="b14">Li &amp; Yu, 2015</xref>) consists of 4447 including multiple disconnected salient objects.</p><p>For the kidney segmentation task, we prune images that do not contain a kidney and resize the data to 200 by 200, without any loss of generality given that the original images are interpolated with the same affine transformation for each patient. We divide the dataset between 7108 images from 168 patients for training and 1786 images from another 42 patients for validation.</p><p>For the salient object detection task, we train on the MSRA10K dataset and use 512 by 512 image crops, padding where necessary and masking the loss accordingly. We use data augmentation pro- cedures such as scale, horizontal flip and change in brightness. We use all images for training and compute validations scores on the other salient object detection datasets.</p></sec><sec><title>EVALUATION METRICS</title><p>In our experiments we use three metrics: Intersection Over Union (IOU), adaptive F &#946; (&#945;-F &#946; ) and weighted F &#946; (&#969;-F &#946; ) described in (<xref ref-type="bibr" rid="b23">Margolin et al., 2014</xref>).</p><p>For computing IOUs, we rely on the definition from the PASCAL-VOC challenge (<xref ref-type="bibr" rid="b9">Everingham et al., 2015</xref>) and compute it as T P /(T P + F P + F N ), where T P , F P , and F N represent true positive, false positive and false negative pixels determined over the whole validation set. The &#945;-F &#946; metric is computed as the weighted F 1 score, F 1 * (1 + &#946; 2 )/&#946; 2 , where we set &#946; 2 = 0.3 (<xref ref-type="bibr" rid="b7">Hu et al., 2017</xref>) and compute F 1 over the entire validation set . For computing the weighted F &#946; , we use the MatLab code provided by (<xref ref-type="bibr" rid="b23">Margolin et al., 2014</xref>), compute scores per image and average over all images. We understand these are the mechanism used to compute the scores reported in <xref ref-type="bibr" rid="b7">Hu et al. (2017)</xref>.</p></sec><sec><title>TRAINING SETUP</title><p>All our models are trained in PyTorch (<xref ref-type="bibr" rid="b27">Paszke et al., 2017</xref>). We use the Adam optimizer (Kingma &amp; Ba, 2014) with default params and learning rates between 1e-3 and 1e-4. We anneal the learning rate once the loss curves start to plateau.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>We use the Runge-Kutta (RK-45) adaptive solver and the adjoint sensitivity method provided in (<xref ref-type="bibr" rid="b4">Chen, 2019</xref>). We set the relative error tolerance to zero and explore absolute error tolerances be- tween 1e-3 and 1e-5.</p><p>The model architectures we evaluate include the UNet (<xref ref-type="bibr" rid="b29">Ronneberger et al., 2015</xref>), NODEs parametrized by UNets (NodeUnet), and an architecture inspired by DeepLabV3 (<xref ref-type="bibr" rid="b4">Chen et al., 2017</xref>), in which we stack NODEs (NodeStack) with Squeeze and Excitation modules (<xref ref-type="bibr" rid="b7">Hu et al., 2018</xref>) fol- lowed by an Atrous Spatial Pyramid Pooling Layer (<xref ref-type="bibr" rid="b4">Chen et al., 2017</xref>). The kidney segmentation experiments were conducted on a single NVIDIA DGX-1 with 8 GPUs. The salient object detection experiments on UNet and NodeUNet were conducted on a single NVIDIA DGX-1 with 8 GPUs, and the experiments on NodeStack were conducted on 4 NVIDIA DGX-1 nodes with 32 GPUs total. We used the largest possible batch size given memory constraints. The code for replicating our experiments and pre-trained weights will be made available on github.</p></sec><sec><title>RESULTS</title><p>In this section, we provide comparative results between our methods (contour evolution and image evolution) and other methods over multiple datasets. In our experiments, our contour evolution method focuses on using a NODE to refine suboptimal contours obtained from a regression model trained to predict distance maps from an image; our image evolution method focuses on using a NODE to learn to evolve an image embedding into a distance map. We first provide results on kidney segmentation and then provide results on salient object detection.</p></sec><sec><title>KIDNEY SEGMENTATION</title><p>In this task we compare three setups: the first trains a UNet regression model that maps an image to a distance map; the second uses our contour evolution method to refine the prediction of the aforementioned UNet model with a NODE parametrized by a UNet (NodeUNet); the third uses our image evolution method to train a NodeUnet that evolves an image into a distance map. We chose the UNet model checkpoint used in the contour evolution experiment by selecting the checkpoint with the lowest validation loss on the first 8 samples of the validation set right before the UNet starts overfitting the training data and the validation loss starts going up.</p><p>We use the same training and validation setup for all models and provide results over the validation set below on <xref ref-type="table" rid="tab_0">Table 1</xref>. The UNet models provide the worse IOU scores. Our image evolution method produces goods results, showing evidence that it is possible to use NODEs to evolve an image into a distance map.</p><p>Lastly, our contour evolution method is able to improve the suboptimal initial contour provided by the UNet model and represents our best results in this experiment. These promising results show evidence that our method could generally be used to improve suboptimal models that underfit or overfit the training data. This is specially valuable for domains with scarcity of data. We provide results of the NodeUNet model trained with the contour evolution method in <xref ref-type="fig" rid="fig_2">Figure 3</xref>.</p></sec><sec><title>SALIENT OBJECT DETECTION</title><p>In this task we replicate the training setup described in <xref ref-type="bibr" rid="b7">Hu et al. (2017)</xref>: we train our models on the MSRA10K dataset and compute their validation F &#946; scores on PASCAL-S, ECSSD and HKU-IS. We first evaluate the effect of different methods and model architectures on F &#946; scores. We compare results from a regression model trained using the UNet architecture, a contour evolution model using the NodeUNet architecture and an image evolution model using the NodeStack architecture. We choose the UNet model for contour evolution by repeating the procedure described in 4.4.1.</p><p>We provide results over the validation set below on <xref ref-type="table" rid="tab_1">Table 2</xref>, wherein the UNet model provides the baseline F &#946; scores. In all experiments, our refinement method provides 5% relative improvement over our UNet baseline, which has 3x more paremeters than our NodeUNet (5M vs 15M). We foresee that this trend will continue and models with more parameters will yield better refinement results. Finally, we compare our best model against a contrast based model DRFI (<xref ref-type="bibr" rid="b15">Wang et al., 2017</xref>) and recent deep learning based models such as MTDS (<xref ref-type="bibr" rid="b14">Li et al., 2016</xref>), MDF (<xref ref-type="bibr" rid="b14">Li &amp; Yu, 2015</xref>), MCDL (<xref ref-type="bibr" rid="b21">Zhao et al., 2015</xref>), ELD (<xref ref-type="bibr" rid="b19">Lee et al., 2016</xref>), LEGS (<xref ref-type="bibr" rid="b15">Wang et al., 2015</xref>). We also compare against DLS (<xref ref-type="bibr" rid="b7">Hu et al., 2017</xref>), a deep learning model based on level sets. Whenever possible, we use the original code provided by the authors for computing scores or collect the scores from their publications. We reproduce the setup in <xref ref-type="bibr" rid="b7">Hu et al. (2017)</xref> by computing our PASCAL-S scores on binary maps produced by thresholding the ground truth saliency maps at 0.5. <xref ref-type="fig" rid="fig_3">Figure 4</xref> below illustrate our model performance on PASCAL-S, ECCSD and HKU-IS. <xref ref-type="table" rid="tab_2">Table 3</xref> below shows that the NodeStack model (Ours) achieves the best results on all but one metric.</p></sec><sec><title>DISCUSSION</title><p>In this paper, we extend the level set segmentation method to use NODEs to solve the contour evolu- tion problem. We learn a forcing function in an end-to-end data driven manner. We demonstrate that our techniques can effectively evolve rough estimates of contours into final segmentation of objects. Our techniques can also evolve input image's embedding into a pixel-wise dense semantic label. Experimental results on several benchmark datasets suggest using NODEs for image segmenta- tion task is viable. Compared to state-of-the-art methods, our proposed techniques also produce favourable segmentation results.</p><p>Although we benefit from NODEs' parametrization of the derivative of the contour, in this paper we do not explore the incorporation of external constraints to guide the contour evolution and that is an area for future exploration. We also foresee that our method can generalize to 3D images. Finally, in some cases during our hyperparameter search we found that training the same model architecture with different learning rates and error tolerances yielded similar losses but largely dif- ferent number of NODE function evaluations, prohibitively increasing wall clock time. This hyper- parameter search can be replaced with automated approaches such as the Gated Info CNF described in <xref ref-type="bibr" rid="b24">Nguyen et al. (2019)</xref> , where the error tolerances are estimated by the model.</p><p>Under review as a conference paper at ICLR 2020</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Transversal slices of CT scans. Leftmost image: initial contour provided by a UNet model. Other images: intermediate steps of the evolution of the initial contour with our Neural ODE model.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Diagrams for the contour and image evolution methods described in sections 2.1 and 2.2. Superscripts 0 and 1 represent initial value and numerical solution of the NODE respectively.</p></caption><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Validation IOU scores from three methods using similar model architectures with simi- lar and different number of parameters. Image Evolution represents evolution from an image to a distance map and Contour Evolution represents refinement of an initial contour.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Kidney segmentation results. Black and orange contours are ground truth and model prediction, respectively. Blue, white and orange images are narrow band distance map prediction.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Scores for different methods and models with 15, 5 and 41 million parameters. All scores are computed on binary maps produced by thresholding the ground truth saliency maps at 0.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_2"><label>Table 3:</label><caption><title>Table 3:</title><p>F &#946; Scores. PASCAL-S scores are computed on binary maps produced by thresholding the ground truth saliency maps at 0.5.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Salient object results. Black and orange contours are ground truth and model prediction, respectively. Blue, white and orange images are narrow band distance map prediction.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig></sec></body><back><sec><p>For the hyperbolic tangent, the gradients decrease as it moves away from zero, which represents the contour at the zero level set.</p></sec><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>A fast level set method for propagating interfaces</article-title><source>Journal of computational physics</source><year>1995</year><volume>118</volume><issue>2</issue><fpage>269</fpage><lpage>277</lpage><person-group person-group-type="author"><name><surname>References David Adalsteinsson</surname><given-names>James A</given-names></name><name><surname>Sethian</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Layer normalization</article-title><source>arXiv preprint arXiv:1607.06450</source><year>2016</year><person-group person-group-type="author"><name><surname>Ba</surname><given-names>Jimmy Lei</given-names></name><name><surname>Kiros</surname><given-names>Jamie Ryan</given-names></name><name><surname>Hinton</surname><given-names>Geoffrey E</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><source>European Conference on Computer Vision</source><year>2006</year><fpage>471</fpage><lpage>483</lpage><person-group person-group-type="author"><name><surname>Brox</surname><given-names>Thomas</given-names></name><name><surname>Bruhn</surname><given-names>Andr&#233;s</given-names></name><name><surname>Weickert</surname><given-names>Joachim</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Active contours without edges</article-title><source>IEEE Transactions on image processing</source><year>2001</year><volume>10</volume><issue>2</issue><fpage>266</fpage><lpage>277</lpage><person-group person-group-type="author"><name><surname>Tony</surname><given-names>F</given-names></name><name><surname>Chan</surname><given-names /></name><name><surname>Luminita</surname><given-names>A</given-names></name><name><surname>Vese</surname><given-names /></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Rethinking atrous convolution for semantic image segmentation</article-title><source>arXiv preprint arXiv:1706.05587</source><year>2017</year><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Liang-Chieh</given-names></name><name><surname>Papandreou</surname><given-names>George</given-names></name><name><surname>Schroff</surname><given-names>Florian</given-names></name><name><surname>Adam</surname><given-names>Hartwig</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><source>torchdiffeq</source><year>2019</year><person-group person-group-type="author"><name><surname>Tian Qi Chen</surname><given-names /></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Neural ordinary differ- ential equations</article-title><source>Advances in Neural Information Processing Systems</source><year>2018</year><fpage>6571</fpage><lpage>6583</lpage><person-group person-group-type="author"><name><surname>Tian Qi Chen</surname><given-names>Yulia</given-names></name><name><surname>Rubanova</surname><given-names>Jesse</given-names></name><name><surname>Bettencourt</surname><given-names>David K</given-names></name><name><surname>Duvenaud</surname><given-names /></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Global contrast based salient region detection</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2014</year><volume>37</volume><issue>3</issue><fpage>569</fpage><lpage>582</lpage><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Ming-Ming</given-names></name><name><surname>Niloy</surname><given-names>J</given-names></name><name><surname>Mitra</surname><given-names>Xiaolei</given-names></name><name><surname>Huang</surname><given-names /></name><name><surname>Philip</surname><given-names>H S</given-names></name><name><surname>Torr</surname><given-names>Shi-Min</given-names></name><name><surname>Hu</surname><given-names /></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Augmented neural odes</article-title><year>1904</year><person-group person-group-type="author"><name><surname>Dupont</surname><given-names>Emilien</given-names></name><name><surname>Doucet</surname><given-names>Arnaud</given-names></name><name><surname>Teh</surname><given-names>Yee Whye</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>The pascal visual object classes challenge: A retrospective</article-title><source>International journal of computer vision</source><year>2015</year><volume>111</volume><issue>1</issue><fpage>98</fpage><lpage>136</lpage><person-group person-group-type="author"><name><surname>Everingham</surname><given-names>Mark</given-names></name><name><surname>Eslami</surname><given-names>Ali</given-names></name><name><surname>Van Gool</surname><given-names>Luc</given-names></name><name><surname>Christopher</surname><given-names>K I</given-names></name><name><surname>Williams</surname><given-names>John</given-names></name><name><surname>Winn</surname><given-names>Andrew</given-names></name><name><surname>Zisserman</surname><given-names /></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Anode: Unconditionally accurate memory-efficient gradients for neural odes</article-title><source>arXiv preprint arXiv:1902.10298</source><year>2019</year><person-group person-group-type="author"><name><surname>Gholami</surname><given-names>Amir</given-names></name><name><surname>Keutzer</surname><given-names>Kurt</given-names></name><name><surname>Biros</surname><given-names>George</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>The constrained reinitialization equa- tion for level set methods</article-title><source>Journal of Computational Physics</source><year>2010</year><volume>229</volume><issue>5</issue><fpage>1514</fpage><lpage>1535</lpage><person-group person-group-type="author"><name><surname>Hartmann</surname><given-names>Daniel</given-names></name><name><surname>Meinke</surname><given-names>Matthias</given-names></name><name><surname>Schr&#246;der</surname><given-names>Wolfgang</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Deep residual learning for image recog- nition</article-title><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><year>2016</year><fpage>770</fpage><lpage>778</lpage><person-group person-group-type="author"><name><surname>He</surname><given-names>Kaiming</given-names></name><name><surname>Zhang</surname><given-names>Xiangyu</given-names></name><name><surname>Ren</surname><given-names>Shaoqing</given-names></name><name><surname>Sun</surname><given-names>Jian</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>The kits19 challenge data: 300 kidney tumor cases with clinical context, ct semantic segmentations, and surgical outcomes</article-title><source>arXiv preprint arXiv:1904.00445</source><year>2019</year><person-group person-group-type="author"><name><surname>Heller</surname><given-names>Nicholas</given-names></name><name><surname>Sathianathen</surname><given-names>Niranjan</given-names></name><name><surname>Kalapara</surname><given-names>Arveen</given-names></name><name><surname>Walczak</surname><given-names>Edward</given-names></name><name><surname>Moore</surname><given-names>Keenan</given-names></name><name><surname>Kaluzniak</surname><given-names>Heather</given-names></name><name><surname>Rosenberg</surname><given-names>Joel</given-names></name><name><surname>Blake</surname><given-names>Paul</given-names></name><name><surname>Rengel</surname><given-names>Zachary</given-names></name><name><surname>Oestreich</surname><given-names>Makinna</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Squeeze-and-excitation networks</article-title><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><year>2018</year><fpage>7132</fpage><lpage>7141</lpage><person-group person-group-type="author"><name><surname>Hu</surname><given-names>Jie</given-names></name><name><surname>Shen</surname><given-names>Li</given-names></name><name><surname>Sun</surname><given-names>Gang</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Deep level sets for salient object detection</article-title><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><year>2017</year><fpage>2300</fpage><lpage>2309</lpage><person-group person-group-type="author"><name><surname>Hu</surname><given-names>Ping</given-names></name><name><surname>Shuai</surname><given-names>Bing</given-names></name><name><surname>Liu</surname><given-names>Jun</given-names></name><name><surname>Wang</surname><given-names>Gang</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift</article-title><source>arXiv preprint arXiv:1502.03167</source><year>2015</year></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv preprint arXiv:1412.6980</source><year>2014</year><person-group person-group-type="author"><name><surname>Diederik</surname><given-names>P</given-names></name><name><surname>Kingma</surname><given-names>Jimmy</given-names></name><name><surname>Ba</surname><given-names /></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Glow: Generative flow with invertible 1x1 convolutions</article-title><source>Advances in Neural Information Processing Systems</source><year>2018</year><fpage>10215</fpage><lpage>10224</lpage><person-group person-group-type="author"><name><surname>Durk</surname><given-names>P</given-names></name><name><surname>Kingma</surname><given-names>Prafulla</given-names></name><name><surname>Dhariwal</surname><given-names /></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Deep saliency with encoded low level distance map and high level features</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2016</year><fpage>660</fpage><lpage>668</lpage><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Gayoung</given-names></name><name><surname>Tai</surname><given-names>Yu-Wing</given-names></name><name><surname>Kim</surname><given-names>Junmo</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Visual saliency based on multiscale deep features</article-title><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><year>2015</year><fpage>5455</fpage><lpage>5463</lpage><person-group person-group-type="author"><name><surname>Li</surname><given-names>Guanbin</given-names></name><name><surname>Yu</surname><given-names>Yizhou</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>Deepsaliency: Multi-task deep neural network model for salient object detection</article-title><source>IEEE Transactions on Image Processing</source><year>2016</year><volume>25</volume><issue>8</issue><fpage>3919</fpage><lpage>3930</lpage><person-group person-group-type="author"><name><surname>Li</surname><given-names>Xi</given-names></name><name><surname>Zhao</surname><given-names>Liming</given-names></name><name><surname>Wei</surname><given-names>Lina</given-names></name><name><surname>Yang</surname><given-names>Ming-Hsuan</given-names></name><name><surname>Wu</surname><given-names>Fei</given-names></name><name><surname>Zhuang</surname><given-names>Yueting</given-names></name><name><surname>Ling</surname><given-names>Haibin</given-names></name><name><surname>Wang</surname><given-names>Jingdong</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>The secrets of salient object segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2014</year><fpage>280</fpage><lpage>287</lpage><person-group person-group-type="author"><name><surname>Li</surname><given-names>Yin</given-names></name><name><surname>Hou</surname><given-names>Xiaodi</given-names></name><name><surname>Koch</surname><given-names>Christof</given-names></name><name><surname>James</surname><given-names>M</given-names></name><name><surname>Rehg</surname><given-names>Alan L</given-names></name><name><surname>Yuille</surname><given-names /></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>How to evaluate foreground maps?</article-title><source>Proceed- ings of the IEEE conference on computer vision and pattern recognition</source><year>2014</year><fpage>248</fpage><lpage>255</lpage><person-group person-group-type="author"><name><surname>Margolin</surname><given-names>Ran</given-names></name><name><surname>Zelnik-Manor</surname><given-names>Lihi</given-names></name><name><surname>Tal</surname><given-names>Ayellet</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>Infocnf: An efficient conditional continuous normalizing flow with adaptive solvers</article-title><source>Workshop on Invertible Neural Nets and Normalizing Flows</source><year>2019</year><person-group person-group-type="author"><name><surname>Tan M Nguyen</surname><given-names>Animesh</given-names></name><name><surname>Garg</surname><given-names /></name><name><surname>Richard</surname><given-names>G</given-names></name><name><surname>Baraniuk</surname><given-names>Anima</given-names></name><name><surname>Anandkumar</surname><given-names /></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><article-title>Fronts propagating with curvature-dependent speed: algorithms based on hamilton-jacobi formulations</article-title><source>Journal of computational physics</source><year>1988</year><volume>79</volume><issue>1</issue><fpage>12</fpage><lpage>49</lpage><person-group person-group-type="author"><name><surname>Osher</surname><given-names>Stanley</given-names></name><name><surname>Sethian</surname><given-names>James A</given-names></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>Geodesic active contours and level sets for the detection and tracking of moving objects</article-title><source>IEEE Transactions on pattern analysis and machine intelligence</source><year>2000</year><volume>22</volume><issue>3</issue><fpage>266</fpage><lpage>280</lpage><person-group person-group-type="author"><name><surname>Paragios</surname><given-names>Nikos</given-names></name><name><surname>Deriche</surname><given-names>Rachid</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><source>Automatic differentiation in pytorch</source><year>2017</year><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>Adam</given-names></name><name><surname>Gross</surname><given-names>Sam</given-names></name><name><surname>Chintala</surname><given-names>Soumith</given-names></name><name><surname>Chanan</surname><given-names>Gregory</given-names></name><name><surname>Yang</surname><given-names>Edward</given-names></name><name><surname>Devito</surname><given-names>Zachary</given-names></name><name><surname>Lin</surname><given-names>Zeming</given-names></name><name><surname>Desmaison</surname><given-names>Alban</given-names></name><name><surname>Antiga</surname><given-names>Luca</given-names></name><name><surname>Lerer</surname><given-names>Adam</given-names></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>Waveglow: A flow-based generative network for speech synthesis</article-title><source>ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><year>2019</year><fpage>3617</fpage><lpage>3621</lpage><person-group person-group-type="author"><name><surname>Prenger</surname><given-names>Ryan</given-names></name><name><surname>Valle</surname><given-names>Rafael</given-names></name><name><surname>Catanzaro</surname><given-names>Bryan</given-names></name></person-group></element-citation></ref><ref id="b29"><element-citation publication-type="journal"><source>International Conference on Medical image computing and computer- assisted intervention</source><year>2015</year><fpage>234</fpage><lpage>241</lpage><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>Olaf</given-names></name><name><surname>Fischer</surname><given-names>Philipp</given-names></name><name><surname>Brox</surname><given-names>Thomas</given-names></name></person-group></element-citation></ref><ref id="b30"><element-citation publication-type="journal"><source>European Con- ference on Computer Vision</source><year>2002</year><fpage>78</fpage><lpage>92</lpage><person-group person-group-type="author"><name><surname>Rousson</surname><given-names>Mikael</given-names></name></person-group></element-citation></ref><ref id="b31"><element-citation publication-type="journal"><source>Apex</source><year>2019</year><person-group person-group-type="author"><name><surname>Sarofeen</surname><given-names>Christian</given-names></name><name><surname>Carilli</surname><given-names>Michael</given-names></name><name><surname>Kolodziej</surname><given-names>Marek</given-names></name></person-group></element-citation></ref><ref id="b32"><element-citation publication-type="journal"><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv preprint arXiv:1409.1556</source><year>2014</year><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>Karen</given-names></name><name><surname>Zisserman</surname><given-names>Andrew</given-names></name></person-group></element-citation></ref><ref id="b33"><element-citation publication-type="journal"><article-title>A level set approach for computing solutions to incompressible two-phase flow</article-title><source>Journal of Computational physics</source><year>1994</year><volume>114</volume><issue>1</issue><fpage>146</fpage><lpage>159</lpage><person-group person-group-type="author"><name><surname>Sussman</surname><given-names>Mark</given-names></name><name><surname>Smereka</surname><given-names>Peter</given-names></name><name><surname>Osher</surname><given-names>Stanley</given-names></name></person-group></element-citation></ref><ref id="b34"><element-citation publication-type="journal"><article-title>Going deeper with convolutions</article-title><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><year>2015</year><fpage>1</fpage><lpage>9</lpage><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>Christian</given-names></name><name><surname>Liu</surname><given-names>Wei</given-names></name><name><surname>Jia</surname><given-names>Yangqing</given-names></name><name><surname>Sermanet</surname><given-names>Pierre</given-names></name><name><surname>Reed</surname><given-names>Scott</given-names></name><name><surname>Anguelov</surname><given-names>Dragomir</given-names></name><name><surname>Du- Mitru Erhan</surname><given-names>Vincent</given-names></name><name><surname>Vanhoucke</surname><given-names>Andrew</given-names></name><name><surname>Rabinovich</surname><given-names /></name></person-group></element-citation></ref><ref id="b35"><element-citation publication-type="journal"><source>A shape-based approach to the segmentation of medical imagery using level sets</source><year>2003</year><person-group person-group-type="author"><name><surname>Tsai</surname><given-names>Andy</given-names></name><name><surname>Yezzi</surname><given-names>Anthony</given-names></name><name><surname>Wells</surname><given-names>William</given-names></name><name><surname>Tempany</surname><given-names>Clare</given-names></name><name><surname>Tucker</surname><given-names>Dewey</given-names></name><name><surname>Fan</surname><given-names>Ayres</given-names></name><name><surname>Grimson</surname><given-names>Eric</given-names></name><name><surname>Willsky</surname><given-names>Alan S</given-names></name></person-group></element-citation></ref><ref id="b36"><element-citation publication-type="journal"><article-title>Quality assessment for crowdsourced object annotations</article-title><source>BMVC</source><year>2011</year><fpage>1</fpage><lpage>11</lpage><person-group person-group-type="author"><name><surname>Vittayakorn</surname><given-names>Sirion</given-names></name><name><surname>Hays</surname><given-names>James</given-names></name></person-group></element-citation></ref></ref-list></back></article>