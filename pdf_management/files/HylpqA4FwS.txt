Title:
```
Published as a conference paper at ICLR 2020 RNNS INCREMENTALLY EVOLVING ON AN EQUILIBRIUM MANIFOLD: A PANACEA FOR VANISHING AND EXPLODING GRADIENTS?
```
Abstract:
```
Recurrent neural networks (RNNs) are particularly well-suited for modeling long- term dependencies in sequential data, but are notoriously hard to train because the error backpropagated in time either vanishes or explodes at an exponential rate. While a number of works attempt to mitigate this effect through gated recurrent units, skip-connections, parametric constraints and design choices, we propose a novel incremental RNN (iRNN), where hidden state vectors keep track of incre- mental changes, and as such approximate state-vector increments of Rosenblatt's (1962) continuous-time RNNs. iRNN exhibits identity gradients and is able to account for long-term dependencies (LTD). We show that our method is computa- tionally efficient overcoming overheads of many existing methods that attempt to improve RNN training, while suffering no performance degradation. We demon- strate the utility of our approach with extensive experiments and show competitive performance against standard LSTMs on LTD and other non-LTD tasks.
```

Figures/Tables Captions:
```
Figure 1: iRNN depicted by unfolding into K recursions for one transition from g 0 = h m−1 to h m = g K . Here, ϕ(x, g, h) = φ(U (g + h) + W x + b) − α(g + h). See Sec. A.2 for implementation and pseudo-code. This resembles Graves (2016), who propose to vary K with m as a way to attend to important input transitions. However, the transition functions used are gated units, unlike our conventional ungated functions. As such, while this is not their concern, equilibrium may not even exist and identity gradients are not guaranteed in their setup.
Figure 2: Phase-space trajectory with tanh activa- tion of RNN, FastRNN, iRNN. X-axis denotes 1st dimension, and Y-axis 2nd dimension of 2D hidden state subject to random walk input with variance 10 for 1000 time-steps. Parameters U, W, b are ran- domly initialized. RNN states are scaled to fit plot since FastRNN is not required to be in the cube.
Figure 3: Exploratory experiments for the Add task (a) Convergence with varying K; (b) Ratio ∂h T ∂h 1 / ∂h T ∂h T −1 illustrates Vanishing/Exploding gradient ( ∂h T ∂h T −1 and loss gradients are omitted but displayed in A.7.8. For iRNN (a) and (b) together show strong correlation of gradient with accuracy in contrast to other methods.
Fig. 3 ). While other models h t = αh t−1 + βφ((U − γI)h t−1 + W x t ), can realize identity gradients for suitable choices; linear (α = 1, β = 1, γ = 0, U = 0), FastRNN (α ≈ 1, β ≈ 0, γ = 0) and Antisymmetric (α = 1, β = 1, U = V − V T , U ≤ γ), this goal may not be correlated with improved test accuracy. FastRNN(η = 0.001), Antisymmetric (γ = 0.01, = 0.001) have good gradients but poorer test accuracy relative to FastRNN(η = 0.01), Antisymmetric(γ = 0.01, = 0.1), with poorer gradients. (b) Identity gradient implies faster convergence: Identity gradient, whenever effective, must be capable of assigning credit to the informative parts, which in turn results in larger loss gradients, and significantly faster convergence with number of iterations. This is borne out in figure 3(a). iRNN for larger K is closer to identity gradient with fewer (unstable) spikes (K = 1, 5, 10). With K = 10, iRNN converges within 300 iterations while competing methods take about twice this time (other baselines not included here exhibited poorer performance than the once plotted).
Figure 4: Following Arjovsky et al. (2016) we display average Cross Entropy for the Copy Task (Sequence Length (with baseline memoryless strategy)): (a) 200 (0.09) (b) 500 (0.039). Mean Squared Error for the Add Task, baseline performance is 0.167 (Sequence Length) : (c) 200 (d) 750. For both tasks, iRNN runs K = 5.
Table 1: Results for Pixel-by-Pixel MNIST and Permuted MNIST datasets. K denotes pre-defined recursions embedded in graph to reach equilibrium.
Table 2: Results for Noise Padded CIFAR-10 and MNIST datasets. Since the equilibrium surface is smooth and resilient to small perturbations, iRNN achieves better performance than the baselines with faster convergence.
Table 3: Results for Activity Recognition Datasets. iRNN outperforms the baselines on all metrics even with K = 1. Its worth noticing that although K = 5 increases test time, it's well within LSTM's numbers, the overall train time and resulting performance are better than K = 1.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Recurrent neural networks (RNNs) in each round store a hidden state vector, h m ∈ R D , and upon receiving the input vector, x m+1 ∈ R d , linearly transform the tuple (h m , x m+1 ) and pass it through a memoryless non-linearity to update the state over T rounds. Subsequently, RNNs output an affine function of the hidden states as its prediction. The model parameters (state/input/prediction parameters) are learnt by minimizing an empirical loss. This seemingly simple update rule has had significant success in learning complex patterns for sequential input data. Nevertheless, that training RNNs can be challenging, and that performance can be uneven on tasks that require long-term-dependency (LTD), was first noted by Hochreiter (1991), Bengio et al. (1994) and later by other researchers. Pascanu et al. (2013b) attributed this to the fact that the error gradient back-propagated in time (BPTT), for the time-step m, is dominated by product of partials of hidden- state vectors, T −1 j=m ∂hj+1 ∂hj , and these products typically exhibit exponentially vanishing decay or explosion, resulting in incorrect credit assignment during training and test-time. Rosenblatt (1962), on whose work we draw inspiration from, introduced continuous-time RNN (CTRNN) to mimic activation propagation in neural circuitry. CTRNN dynamics evolves as follows: Here, x(t) ∈ R d is the input signal, g(t) ∈ R D is the hidden state vector of D neurons,ġ i (t) is the rate of change of the i-th state component; τ, α ∈ R + , referred to as the post-synaptic time-constant, impacts the rate of a neuron's response to the instantaneous activation φ(U g(t) + W x(t) + b); and U ∈ R D×D , W ∈ R D×d , b ∈ R D are model parameters. In passing, note that recent RNN works that draw inspiration from ODE's (Chang et al., 2019) are special cases of CTRNN (τ = 1, α = 0).

Section Title: Vanishing Gradients
  Vanishing Gradients The qualitative aspects of the CTRNN dynamics is transparent in its integral form: This integral form reveals that the partials of hidden-state vector with respect to the initial condition, ∂g(t) ∂g(t0) , gets attenuated rapidly (first term in RHS), and so we face a vanishing gradient problem. We will address this issue later but we note that this is not an artifact of CTRNN but is exhibited by ODEs that have motivated other RNNs (see Sec. 2).

Section Title: Shannon-Nyquist Sampling
  Shannon-Nyquist Sampling A key property of CTRNN is that the time-constant τ together with the first term −g(t), is in effect a low-pass filter with bandwidth ατ −1 suppressing high frequency components of the activation signal, φ((U g(s)) + (W x(s)) + b). This is good, because, by virtue of the Shannon-Nyquist sampling theorem, we can now maintain fidelity of discrete samples with respect to continuous time dynamics, in contrast to conventional ODEs (α = 0). Additionally, since high-frequencies are already suppressed, in effect we may assume that the input signal x(t) is slowly varying relative to the post-synaptic time constant τ .

Section Title: Equilibrium
  Equilibrium The combination of low pass filtering and slowly time varying input has a significant bearing. The state vector as well as the discrete samples evolve close to the equilibrium state, i.e., g(t) ≈ φ(U g(t) + W x(t) + b) under general conditions (Sec. 3).

Section Title: Incremental Updates
  Incremental Updates Whether or not system is in equilibrium, the integral form in Eq. 2 points to gradient attenuation as a fundamental issue. To overcome this situation, we store and process increments rather than the cumulative values g(t) and propose dynamic evolution in terms of in- crements. Let us denote hidden state sequence as h m ∈ R D and input sequence x m ∈ R d . For m = 1, 2, . . . , T , and a suitable β > 0 Intuitively, say system is in equilibrium and −α(µ(x m , h m−1 ))+φ(U µ(x m , h m−1 )+W x m +b) = 0. We note state transitions are marginal changes from previous states, namely, h m = µ(x m , h m−1 ) − h m−1 . Now for a fixed input x m , as to which equilibrium is reached depends on h m−1 , but are nevertheless finitely many. So encoding marginal changes as states leads to "identity" gradient. Incremental RNN (iRNN) achieves Identity Gradient. We propose to discretize Eq. 3 to realize iRNN (see Sec. 3). At time m, it takes the previous state h m−1 ∈ R D and input x m ∈ R d and outputs h m ∈ R D after simulating the CTRNN evolution in discrete-time, for a suitable number of discrete steps. We show that the proposed RNN approximates the continuous dynamics and solves the vanishing/exploding gradient issue by ensuring identity gradientIn general, we consider two options, SiRNN, whose state is updated with a single CTRNN sample, similar to vanilla RNNs, and, iRNN, with many intermediate samples. SiRNN is well-suited for slowly varying inputs.

Section Title: Contributions
  Contributions To summarize, we list our main contributions: (A) iRNN converges to equilibrium for typical activation functions. The partial gradients of hidden- state vectors for iRNNs converge to identity, thus solving vanishing/exploding gradient problem! (B) iRNN converges rapidly, at an exponential rate in the number of discrete samplings of Eq. 1. SiRNN, the single-step iRNN, is efficient and can be leveraged for slowly varying input sequences. It exhibits fast training time, has fewer parameters and better accuracy relative to standard LSTMs. (C) Extensive experiments on LTD datasets show that we improve upon standard LSTM accuracy as well as other recent proposals that are based on designing transition matrices and/or skip connections. iRNNs/SiRNNs are robust to time-series distortions such as noise paddings (D) While our method extends directly (see Appendix A.1) to Deep RNNs, we deem these extensions complementary, and focus on single-layer to highlight our incremental perspective.

Section Title: RELATED WORK
  RELATED WORK Gated Architectures. Long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) is widely used in RNNs to model long-term dependency in sequential data. Gated recurrent unit (GRU) (Cho et al., 2014) is another gating mechanism that has been demonstrated to achieve similar Published as a conference paper at ICLR 2020 performance of LSTM with fewer parameters. Some recent gated RNNs include UGRNN (Collins et al., 2016), and FastGRNN (Kusupati et al., 2018). While mitigating vanishing/exploding gradients, they do not eliminate it. Often, these models incur increased inference, training costs, and model size. Unitary RNNs. Arjovsky et al. (2016); Jing et al. (2017); Zhang et al. (2018); Mhammedi et al. (2016) focus on designing well-conditioned state transition matrices, attempting to enforce unitary-property, during training. Unitary property does not generally circumvent vanishing gradient (Pennington et al. (2017)). Also, it limits expressive power and prediction accuracy while also increasing training time.

Section Title: Deep RNNs
  Deep RNNs These are nonlinear transition functions incorporated into RNNs for performance improvement. For instance, Pascanu et al. (2013a) empirically analyzed the problem of how to construct deep RNNs. Zilly et al. (2017) proposed extending the LSTM architecture to allow step-to- step transition depths larger than one. Mujika et al. (2017) proposed incorporating the strengths of both multiscale RNNs and deep transition RNNs to learn complex transition functions. While Deep RNNs offer richer representations relative to single-layers, it is complementary to iRNNs. Residual/Skip Connections. Jaeger et al. (2007); Bengio et al. (2013); Chang et al. (2017); Campos et al. (2017); Kusupati et al. (2018) feed-forward state vectors to induce skip or residual connections, to serve as a middle ground between feed-forward and recurrent models, and to mitigate gradient decay. Nevertheless, these connections, cannot entirely eliminate gradient explosion/decay. For instance, Kusupati et al. (2018) suggest h m = α m h m−1 + β m φ(U h m−1 + W x m + b), and learn parameters so that α m ≈ 1 and β m ≈ 0. Evidently, this setting can lead to identity gradient, observe that setting β m ≈ 0, implies little contribution from the inputs and can conflict with good accuracy, as also observed in our experiments. Linear RNNs. (Bradbury et al., 2016; Lei et al., 2018; Balduzzi & Ghifary, 2016) focus on speeding up RNNs by replacing recurrent connections, such as hidden-to-hidden interactions, with light weight linear components. This reduces training time, but results in significantly increased model size. For example, Lei et al. (2018) requires twice the number of cells for LSTM level performance.

Section Title: ODE/Dynamical Perspective
  ODE/Dynamical Perspective Few ODE inspired architectures attempt to address stability, but do not end up eliminating vanishing/exploding gradients. Talathi & Vartak (2015) proposed a modified weight initialization strategy based on a dynamical system perspective on weight initialization process to successfully train RNNs composed of ReLUs. Niu et al. (2019) analyzed RNN architectures using numerical methods of ODE and propose a family of ODE-RNNs. Chang et al. (2019), propose Antisymmetric-RNN. Their key idea is to express the transition matrix in Eq. 1, for the special case α = 0, τ = 1, as a difference: U = V − V T and note that the eigenspectrum is imaginary. Nevertheless, Euler discretization, in this context leads to instability, necessitating damping of the system. As such vanishing gradient cannot be completely eliminated. Its behavior is analogous to FastRNN Kusupati et al. (2018), in that, identity gradient conflicts with high accuracy. In summary, we are the first to propose evolution over the equilibrium manifold, and demonstrating identity gradients. Neural ODEs (Chen et al., 2018; Rubanova et al., 2019) have also been proposed for time-series prediction to deal with irregularly sampled inputs. They parameterize the derivative of the hidden-state in terms of an autonomous differential equation and let the ODE evolve in continuous time until the next input arrives. As such, this is not our goal, our ODE explicitly depends on the input, and evolves until equilibrium for that input is reached. We introduce incremental updates to bypass vanishing/exploding gradient issues, which is not of specific concern for these works.

Section Title: METHOD
  METHOD We use Euler's method to discretize Eq. 3 in steps δ = ητ . Denoting the kth step as g k = g(kδ) Rearranging terms we get a compact form for iRNN (see  Fig. 1 ). In addition we introduce a learnable parameter η k m and let it be a function of time m and the recursion-step k. We run the recursion for k ∈ [K] with some suitable initial condition. This could be g 0 = 0 or initialized to the previous state, i.e., g 0 = h m−1 at time m. In many of our examples, we find the input sequence is slowly varying, and K = 1 can also realize good empirical performance. We refer to this as single-step-incremental-RNN (SiRNN). For both iRNN and SiRNN we drop the superscript whenever it is clear from the context.

Section Title: Root Finding and Transitions
  Root Finding and Transitions The two indices k and m should not be confused. The index m ∈ [T ] refers to the time index, and indexes input, x m and hidden state h m over time horizon T . The index k ∈ [K] is a fixed-point recursion for converging to the equilibrium solution at each time m, given input x m and the hidden state h m−1 . We iterate over k so that at k = K, g K satisfies, The recursion (Eq. 5) at time m runs for K rounds, terminates, and recursion is reset for the new input, x m+1 . Indeed, Eq. 5 is a standard root-finding recursion, with g k−1 serving as the previous solution, plus a correction term, which is the error, φ(U (g k−1 + h m−1 ) + W x m + b) − α(g k−1 + h m−1 ). If the sequence converges, the resulting solution is the equilibrium point. Proposition 2 guarantees a geometric rate of convergence.

Section Title: Identity Gradient
  Identity Gradient We will informally (see Theorem 1) show here that partial gradients are identity. Say we have for sufficiently large K, h m = g K is the equilibrium solution. It follows that, Taking derivatives, we have, Thus if the matrix (∇φ(·)U − αI) is not singular, it follows that ( ∂hm ∂hm−1 + I) = 0. SiRNN vs. iRNN. SiRNN approximates iRNN. In particular, say x m is a constant in the segment, m ∈ [m 0 , m 0 + K], then SiRNN trajectory of hidden states, denoted as h 1 m0+K is equal to the iRNN hidden state h K m0 , when both SiRNN and iRNN are initialized with g 0 = h m−1 . Thus, for slowly time-varying inputs we can expect SiRNN to closely approximate iRNN.

Section Title: Residual Connections vs. iRNN/SiRNN
  Residual Connections vs. iRNN/SiRNN As such, our architecture is a special case of skip/residual connections. Nevertheless, unlike skip connections, our connections are structured, and the dynamics driven by the error term ensures that the hidden state is associated with equilibrium and leads to identity gradient. No such guarantees are possible with unstructured skip connections. Note that for slowly varying inputs, after a certain transition-time period, we should expect SiRNN to be close to equilibrium as well. Without this imposed structure, general residual architectures can learn patterns that can be dramatically different (see  Fig. 2 ).

Section Title: IDENTITY GRADIENT PROPERTY AND CONVERGENCE GUARANTEES.
  IDENTITY GRADIENT PROPERTY AND CONVERGENCE GUARANTEES. Let us now collect a few properties of Eq. 3 and Eq. 5. First, denote the equilibrium solutions for an arbirary input x ∈ R d , arbitrary state-vector ν ∈ R D , in an arbitrary round: Published as a conference paper at ICLR 2020 Whenever the equilibrium set is a singleton, we denote it as a function h eq (x, ν). For simplicity, we assume below that η i k is a positive constant independent of k and i. Proposition 1. Suppose, φ(·) is a 1-Lipshitz function in the norm induced by · , and U < α, then for any x m ∈ R d and h m−1 ∈ R D , it follows that M eq (x, ν) is a singleton and as K → ∞, the iRNN recursions converge to this solution, namely, h m = lim K→∞ g K = h eq (x m , h m−1 ) It follows that T (·) is a contraction: We now invoke the Banach fixed point theorem, which asserts that a contractive operator on a complete metric space converges to a unique fixed point, namely, T K (g) → g * . Upon substitution, we see that this point g * must be such that, φ(U (g * + h m−1 ) + W x m + b) − (g * + h m−1 ) = 0. Thus equilibrium point exists and is unique. Result follows by setting h m h eq (x m , h m−1 ). Handling U ≤ α. In experiments, we set α = 1, and do not enforce U ≤ α constraint. Instead, we initialize U as a Gaussian matrix with IID mean zero, small variance components. As such, the matrix norm is smaller than 1. Evidently, the resulting learnt U matrix does not violate this condition. Next we show for η > 0, iRNN converges at a linear rate, which follows directly from Proposition 1. Proposition 2. Under the setup in Proposition 1, it follows that, Remark. Proposition 1 accounts for typical activation functions ReLU, tanh, sigmoids as well as deep RNNs (appendix A.1). In passing we point out that, in our experiments, we learn parameters η k m , and a result that accounts for this case is desirable. We describe this case in Appendix A.3. A fundamental result we describe below is that partials of hidden-state vectors, on the equilibrium surface is unity. For technical simplicity, we assume a continuously differentiable activation, which appears to exclude ReLU activations. Nevertheless, we can overcome this issue, but requires more technical arguments. The main difficulty stems from ensuring that derivatives along the equilibrium surface exist, and this can be realized by invoking the implicit function theorem (IFT). IFT requires continuous differentiability, which ReLUs violate. Nevertheless, recent results 1 suggests that one can state implicit function theorem for everywhere differentiable functions, which includes ReLUs. Theorem 1. Suppose φ(·) is a continuously differentiable, 1-Lipshitz function, with U < α. Then as K → ∞, ∂hm ∂hm−1 → ∂heq(xm,hm−1) ∂hm−1 = −I. Furthermore, as K → ∞ the partial gradients over arbitrary number of rounds for iRNN is identity. Proof. Define, ψ(g, h m−1 ) = φ(U (g + h m−1 ) + W x m + b) − α(g + h m−1 ). We overload notation and view the equilibrium point as a function of h m−1 , i.e., g * (h m−1 ) = h eq (x m , h m−1 ). Invoking standard results 2 in ODE's, it follows that g * (h m−1 ) is a smooth function, so long as the Jacobian, ∇ g ψ(g * , h m−1 ) with respect to the first coordinate, g * , is non-singular. Upon computation, we see that, ∇ g ψ(g * , h m−1 ) = ∇φ(g * , h m−1 )U − αI, is non-singular, since ∇φ(g * , h m−1 )U ≤ U . It follows that we can take partials of the state-vectors. By taking the partial derivatives w.r.t. h m−1 in Eq. 5, at the equilibrium points we have [∇φ(g * , h m−1 )U − αI][ ∂g* ∂hm−1 + I] = 0 (see Eq. 7). The rest of the proof follows by observing that the first term is non-singular. Remark. We notice that replacing h m−1 with −h m−1 in Eq. 12 will lead to ∂heq ∂hm−1 = I, which also has no impact on magnitudes of gradients. As a result, both choices are suitable for circumventing vanishing or exploding gradients during training, but still may converge to different local minima and thus result in different test-time performance. Fur- thermore, notice that the norm preserving property is somewhat insensitive to choices of α, so long as the non-singular condition is satisfied.

Section Title: IRNN DESIGN IMPLICATIONS: LOW-RANK MODEL PARAMETRIZATION
  IRNN DESIGN IMPLICATIONS: LOW-RANK MODEL PARAMETRIZATION   Fig. 2  depicts phase portrait and illustrates salient differences between RNN, FastRNN (RNN with skip connection), and iRNN (K=5). RNN and Fas- tRNN exhibit complex trajectories, while iRNN trajectory is smooth, projecting initial point (black circle) onto the equilibrium surface (blue) and moving within it (green). This suggests that iRNN trajectory belongs to a low-dimensional manifold. Variation of Equilibrium w.r.t. Input. As before, h eq be an equilibrium solution for some tuple (h m−1 , x m ). It follows that, This suggests that, whenever the input undergoes a slow variation, we expect that the equilibrium point moves in such a way that U ∂h eq must lie in a transformed span of W . Now W ∈ R D×d with d D, which implies that (αI − ∇φ(U (h eq + h m−1 ) + W x m + b)U is rank-deficient.

Section Title: Low Rank Matrix Parameterization
  Low Rank Matrix Parameterization For typical activation functions, note that whenever the argument is in the unsaturated regime, ∇φ(·) ≈ I. We then approximately get span(αI − U ) ≈ span(W ). We can express these constraints as U = αI + V H with low-rank matrices V ∈ R D×d1 , H ∈ R d1×D , and further map both U h m and W x m onto a shared space. Since in our experiments the signal vectors we encounter are low-dimensional, and sequential inputs vary slowly over time, we enforce this restriction in all our experiments. In particular, we consider, The parameter matrix P ∈ R D×D maps the contributions from input and hidden states onto the same space. To decrease model-size we let P = U = (I + V H) learn these parameters.

Section Title: EXPERIMENTS
  EXPERIMENTS We organize this section as follows. First, the experimental setup, competing algorithms will be described. Then we present an ablative analysis to highlight salient aspects of iRNN and justify some of our experimental choices. We then plot and tabulate experimental results on benchmark datasets.

Section Title: EXPERIMENTAL SETUP AND BASELINES
  EXPERIMENTAL SETUP AND BASELINES Choice of Competing Methods: We choose competing methods based on the following criteria: (a) methods that are devoid of additional application or dataset-specific heuristics, (b) methods that leverage only single cell/block/layer, and (c) methods without the benefit of complementary add-ons (such as gating, advanced regularization, model compression, etc.). Requiring (a) is not controversial since our goal is methodological. Conditions (b),(c) are justifiable since we could also leverage these add-ons and are not germane to any particular method 3 . We benchmark iRNN against standard RNN, LSTM (Hochreiter & Schmidhuber, 1997), (ungated) AntisymmetricRNN (Chang et al., 2019), (ungated) FastRNN (Kusupati et al., 2018).

Section Title: Unitary RNN Variants
  Unitary RNN Variants Results for methods based on unitary transitions (such as Arjovsky et al. (2016); Wisdom et al. (2016); Vorontsov et al. (2017); Zhang et al. (2018)) are not reported in the main paper (when available reported in appendix) for the following reasons: (a) They are substantially more expensive, and requiring large model sizes; (b) Apart from the benchmark copy and add tasks, results tabulated by FastRNN and Antisymmetric authors (see Zhang et al. (2018); Chang et al. (2019)) show that they are well below SOTA; (c) iRNN dominates unitary-RNN variants on add-task (see Sec. 4.3.1); (d) On copy task, while unitary invariants are superior, Vorontsov et al. (2017) attributes it to modReLU or leaky ReLU activations. Leaky ReLUs allow for linear transitions, and copy task being a memory task benefits from it. With hard non-linear activation, unitary RNN variants can take up to 1000's of epochs for even 100-length sequences (Vorontsov et al. (2017)).

Section Title: Implementation
  Implementation For all our experiments, we used the parametrized update formulation in Eq. 9 for iRNN . We used tensorflow framework for our experiments. For most competing methods apart from AntisymmetricRNN, which we implemented, code is publicly available. All the experiments were run on an Nvidia GTX 1080 GPU with CUDA 9 and cuDNN 7.0 on a machine with Intel Xeon 2.60 GHz CPU with 20 cores.

Section Title: Datasets
  Datasets Pre-processing and feature extraction details for all publicly available datasets are in the appendix A.4. We replicate benchmark test/train split with 20% of training data for validation to tune hyperparameters. Reported results are based on the full training set, and performance achieved on the publicly available test set. Table 4 (Appendix) and A.4 describes details for all the data sets.

Section Title: Hyper Parameters
  Hyper Parameters We used grid search and fine-grained validation wherever possible to set the hyper-parameters of each algorithm, or according to the settings published in (Kusupati et al., 2018; Arjovsky et al., 2016) (e.g. number of hidden states). Both the learning rate and η's were initialized to 10 −2 . The batch size of 128 seems to work well across all the data sets. We used ReLU as the non-linearity and Adam (Kingma & Ba (2015)) as the optimizer for all the experiments.

Section Title: ABLATIVE ANALYSIS
  ABLATIVE ANALYSIS We perform ablative analysis on the benchmark add-task (Sec 4.3.1) for sequence length 200 for 1000 iterations and explore mean-squared error as a metric.  Fig. 3  depicts salient results. (c) SiRNN (iRNN with K = 1 delivers good performane in some cases. Fig. 3(a) illustrates that iRNN K = {5, 10} achieves faster convergence than SiRNN, but the computational overhead per iteration roughly doubles or triples in comparison. SiRNN is faster relative to competitors. For this reason, we sometimes tabulate only SiRNN, whenever it is SOTA in benchmark experiments, since accuracy improves with K but requires higher overhead.

Section Title: LONG-TERM DEPENDENCY AND OTHER TASKS
  LONG-TERM DEPENDENCY AND OTHER TASKS We list five types of datasets, all of which in some way require effective gradient propagation: (1) Conventional Benchmark LTD tasks (Add & Copy tasks) that illustrate that iRNN can rapidly learn long-term dependence; (2) Benchmark vision tasks (pixel MNIST, perm-MNIST) that may not require long-term, but nevertheless, demonstrates that iRNN achieves SOTA for short term dependencies but with less resources. (3) Noise Padded (LTD) Vision tasks (Noisy MNIST, Noisy CIFAR), where a large noise time segment separates information segments and the terminal state, and so the learner must extract information parts while rejecting the noisy parts; (4) short duration activity embedded in a larger time-window (HAR-2, Google-30 in Appendix Table 4 and many others A.7), that usually arise in the context of smart IoT applications and require a small model-size footprint. Chang et al. (2019) further justify (3) and (4) as LTD, because for these datasets where only a smaller unknown segment(s) of a longer sequence is informative. (5) Sequence-sequence prediction tasks (PTB language modeling) that are different from terminal prediction (reported in appendix A.7).

Section Title: STANDARD BENCHMARK LTD TASKS : ADDITION & COPY MEMORY
  STANDARD BENCHMARK LTD TASKS : ADDITION & COPY MEMORY Addition and Copy tasks (Hochreiter & Schmidhuber, 1997) have long been used as benchmarks in the literature to evaluate LTD (Hori et al., 2017; Zhang et al., 2018; Arjovsky et al., 2016; Martens & Sutskever, 2011). We follow the setup described in Arjovsky et al. (2016) to create the adding and copying tasks. See appendix A.4 for detailed description. For both tasks we run iRNN with K = 5.  Figure 4  show the average performance of various methods on these tasks. For the copying task we observe that iRNN converges rapidly to the naive baseline and is the only method to achieve zero average cross entropy. For the addition task, both FastRNN and iRNN solves the addition task but FastRNN takes twice the number of iterations to reach desired 0 MSE. 4 In both the tasks, Published as a conference paper at ICLR 2020 iRNN performance is much more stable across number of online training samples. In contrast, other methods either takes a lot of samples to match iRNN 's performance or depict high variance in the evaluation metric. This shows that iRNN converges faster than the baselines (to the desired error). These results demonstrate that iRNN easily and quickly learns the long term dependencies . We omitted reporting unitary RNN variants for Add and Copy task. See Sec. 4.1 for copy task. On Add-task we point out that our performance is superior. In particular, for the longer T = 750 length, Arjovsky et al. (2016), points out that MSE does not reach zero, and uRNN is noisy. Others either (Wisdom et al., 2016) do not report add-task or report only for shorter lengths (Zhang et al., 2018).

Section Title: NON LTD VISION TASKS: PIXEL MNIST, PERMUTE MNIST
  NON LTD VISION TASKS: PIXEL MNIST, PERMUTE MNIST Next, we perform experiments on the sequential vision tasks: (a) classification of MNIST images on a pixel-by-pixel sequence; (b) a fixed random permuted MNIST sequence (Lecun et al., 1998). These tasks typically do not fall in the LTD categories (Chang et al., 2019), but are useful to demonstrate faster training, which can be attributed to better gradients. For the pixel-MNIST task, Kusupati et al. (2018) reports that it takes significantly longer time for existing (LSTMs, Unitary, Gated, Spectral) RNNs to converge to reasonable performance. In contrast, FastRNN trains at least 2x faster than LSTMs. Our results ( table 1 ) for iRNN shows a 9x speedup relative LSTMs, and 2x speedup in comparison to Antisymmetric. In terms of test accuracy, iRNN matches the performance of Antisymmetric, but with at least 3x fewer parameters. We did not gain much with increased K values 5 . For the permuted version of this task, we seem to outperform the existing baselines 6 . In both tasks, iRNN trained at least 2x faster than the strongest baselines. These results demonstrate that iRNN converges much faster than the baselines with fewer parameters.

Section Title: NOISE PADDING TASKS: NOISY-MNIST, NOISY-CIFAR
  NOISE PADDING TASKS: NOISY-MNIST, NOISY-CIFAR Additionally, as in Chang et al. (2019), we induce LTD by padding CIFAR-10 with noise exactly replicating their setup, resulting in Noisy-CIFAR. We extend this setting to MNIST dataset resulting in Noisy-MNIST. Intuitively we expect our model to be resilient to such perturbations. We attribute iRNN's superior performance to the fact that it is capable of suppressing noise. For example, say noise is padded at t > τ and this results in W x t being zero on average. For iRNN the resulting states ceases to be updated. So iRNN recalls last informative state h τ (modulo const) unlike RNNs/variants! Thus information from signal component is possibly better preserved. Results for Noisy-MNIST and Noisy-CIFAR are shown in  Table 2 . Note that almost all timesteps contain noise in these datasets. LSTMs perform poorly on these tasks due to vanishing gradients. This We are interested in detecting activity embedded in a longer sequence with small footprint RNNs (Kusupati et al. (2018)): (a) Google-30 (Warden, 2018), i.e. detection of utterances of 30 commands plus background noise and silence, and (b) HAR-2 (Anguita et al., 2012), i.e. Human Activity Recognition from an accelerometer and gyroscope on a Samsung Galaxy S3 smartphone.  Table 3  shows accuracy, training time, number of parameters and prediction time. Even with K = 1, we compare well against competing methods, and iRNN accuracy improves with larger K. Interestingly, higher K yields faster training as well as moderate prediction time, despite the overhead of additional recursions. These results show that iRNN outperforms baselines on activity recognition tasks, and fits within IoT/edge-device budgets.

Section Title: CONCLUSION
  CONCLUSION Drawing inspiration from Rosenblatts Continuous RNNs, we developed discrete time incremental RNN (iRNN). Leveraging equilibrium properties of CTRNN, iRNN solves exploding/vanishing gradient problem. We show that iRNN improved gradients are directly correlated with improved test accuracy. A number of experiments demonstrate iRNNs responsiveness to long-term dependency tasks. In addition, due to its smooth low-dimensional trajectories, it has a lightweight footprint that can be leveraged for IoT applications.
  Note that there's no standard permutation in the literature. This may be the main reason we could not replicate Chang et al. (2019) performance on the permute MNIST task.

```
