Title:
```
Under review as a conference paper at ICLR 2020 CLOSED LOOP DEEP BAYESIAN INVERSION: UN- CERTAINTY DRIVEN ACQUISITION FOR ACCELERATED MRI
```
Abstract:
```
This work proposes a closed loop, uncertainty-driven adaptive sampling frame- work (CLUDAS) for accelerating magnetic resonance imaging (MRI) via deep Bayesian inversion. By closed loop, we mean that our samples adapt in real-time to the incoming data. To our knowledge, we demonstrate the first generative adver- sarial network (GAN) based framework for posterior estimation over a continuum sampling rates of an inverse problem. We use this estimator to drive the sampling for accelerated MRI. Our numerical evidence demonstrates that the variance es- timate strongly correlates with the expected mean squared error (MSE) improve- ment for different acceleration rates even with few posterior samples. Moreover, the resulting masks bring improvements to the state-of-the-art fixed and active mask designing approaches across MSE, posterior variance and structural similar- ity metric on real undersampled MRI scans.
```

Figures/Tables Captions:
```
Figure 1: Image (left) (resp. Fourier (right)) space MSE against image (resp. Fourier) space empirical posterior variance constructed with 2 and 50 posterior samples respectively. The coordinate of each point is given by the image (resp. Fourier) space MSE averaged over a reconstructed image and the pixel-wise image (resp. Fourier) space posterior variance averaged over this reconstructed image. The black lines represent the location obtained by averaging over a sampling rate over the whole testing set, with steps of 2.5% sampling rate. The light red and blue lines are the example of a trajectory for a given image of the testing set when increasing the sampling rate.
Figure 2: Comparison of reconstruction quality and variance estimation quality for CLUDAS (U-masks), as well as the CLOMDAS (MSE-masks), for different sampling rates. The zoomed-in data are taken at the location highlighted by the yellow square on the upper-right image. The MSE-/U-masks show the evolution of the masks with increasing sampling rate (x-axis). The data are averaged on 2 testing samples
Figure 3: Comparison of fixed masks obtained by the learning-based method of Gözcü et al. (2018). The horizontal axis shows the mask growing as ele- ments are iteratively added to it.
Table 1: MSE scaled by 10 3 , posterior variance scaled by 10 3 and SSIM on test data for different undersampling rates and mask design algorithms. The reconstruction is computed as the average of 2 samples. The lowest value (highetst for SSIM) for each undersampling rate is in blue. The value in parentheses corresponds to the number of samples used to compute the average.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Myriad of applications in control, data processing, and learning-from platform navigation to data mining and from channel estimation to compressive sensing (CS)-involve a linear projection of signals or data points into lower-dimensional space. In this dimensionality reduction, the measure- ment or the sensing matrix determines how much information we acquire per measurement, the ensuing computational ease of processing (since algorithms use the sensing matrix and its adjoint as subroutines), and the recovery guarantees. In a resource constrained setting, these utilities create a Pareto trade-off wherein improving one worsens another. To impact all these fronts simultaneously, adaptive sensing (or sequential exper- imental design, active learning, etc.) aims to close the loop between the data acquisition and the inference, for instance, by exploiting information collected in past samples to adjust the future sam- pling process. While adaptive procedures promise great improvements over non-adaptive methods, they are too computationally demanding for real-time online response. In the context of magnetic resonance imaging (MRI), the dimensionality reduction process (i.e., undersampling in the Fourier domain, often referred to as k-space) directly correlates with patient comfort, as it results in shorter scanning times. For the last decade, approaches motivated by com- pressed sensing have enabled successful reconstruction from highly accelerated (i.e., subsampled) data (see  Lustig et al. (2007) ;  Ravishankar & Bresler (2011b) ;  Lingala & Jacob (2013) ;  Otazo et al. (2015) ;  Jin et al. (2016)  and references therein). While compressed sensing prescribed fully random sampling ( Candès et al., 2006 ;  Donoho, 2006 ) for recovery, most CS-inspired approaches to MRI departed from this paradigm and relied on the heuristics of variable-density sampling (VDS) ( Lustig et al., 2007 ). There, the sampling pattern (or mask) is picked at random from a probability distribution that rea- sonably imitates the energy distribution in Fourier space, whereas fully random sampling of the Fourier space ignores this important structure in the signal, which leads to practically poor results ( Lustig et al., 2008 ). VDS appears as a heuristic middle ground for a sampling pattern to incorpo- rate the structure of energy distribution in Fourier space, while preserving the benefits of incoherent sampling. In VDS, the probability distribution considered has traditionally been parametric (Lustig Under review as a conference paper at  ICLR 2020 et al., 2007 ;  Chauffert et al., 2014 ;  Boyer et al., 2016 ) or constructed from data ( Knoll et al., 2011 ;  Vellagoundar & Machireddy, 2015 ;  Bahadir et al., 2019 ). The CS-inspired methods shift the burden from acquisition to reconstruction, as most of these meth- ods are iterative, preventing online reconstruction of accelerated data. This slow reconstruction rendered the problem of optimizing Fourier space sampling prohibitively expensive, so little work was devoted to general method of designing sampling patterns for generic sampling methods ( Gözcü et al., 2018 ). In recent years, deep learning applied to MRI enabled high quality reconstruction for unprecedented acceleration rates ( Wang et al., 2016 ;  Schlemper et al., 2017 ;  Hammernik et al., 2018 ), as well as near-online reconstruction times. This also led to a freshly renewed interest in the problem of optimizing the Fourier space sampling pattern ( Bahadir et al., 2019 ;  Weiss et al., 2019 ;  Sherry et al., 2019 ;  Jin et al., 2019 ;  Zhang et al., 2019 ), although most of the research energy is still focused on developing more efficient reconstruction methods. In its current stage, deep learning applied to MRI suffers from three main drawbacks: (i) as men- tioned, most of the research energy has been devoted to more efficient reconstruction methods, despite recent results showing that the sampling masks used have a significant effect on the quality of reconstruction ( Gözcü et al., 2018 ;  Gözcü et al., 2019 ), (ii) assessing the reliability of the predic- tion of a reconstructed image is difficult for a clinician, due to the black-box nature of deep learning methods and (iii) the commonly used metrics for assessing the quality of the reconstruction (e.g. MSE, PSNR, SSIM) do not align with what clinicians see as valuable ( Cheng et al., 2019 ). A recent work of  Adler &Öktem (2018)  successfully demonstrated that a conditional Wasserstein GAN (cWGAN) ( Arjovsky et al., 2017 ) can be used to learn the posterior distribution of images given undersampled measurements in a tractable fashion while only relying on samples from the joint distribution. This result provides a key opportunity to address all three drawbacks in the context of MRI by using the posterior variance of the reconstruction as an uncertainty estimate, which is a more natural criterion for image quality. To this end, we show that a conditional WGAN can be trained on a continuum of inverse problems on various sampling rates, yielding an estimator of the posterior variance in the Fourier domain that can be used to drive the whole sampling process in a closed loop fashion. Despite the model being trained only for reconstruction and not sampling, the resulting variance estimator can reliably be used to guide a closed loop sampling procedure, thus providing patient-adapted sampling masks. In particular, we demonstrate that the generated masks that minimize the uncertainty estimates in an online fashion reach similar reconstruction MSE as compared to the state-of-the-art fixed-mask approaches like  Gözcü et al. (2018) ;  Gözcü et al. (2019) . While these approaches explicitly focus on minimizing MSE, we show that CLUDAS naturally outperforms them in terms of key visual metrics such as SSIM ( Wang et al., 2004 ) without being trained on these metrics. In addition, we investigate the reliability of our estimator in a wide range of undersampling regimes and show that even when using a few samples from the cWGAN posterior, the variance estimate is reliable enough to be used to drive the design of the mask. This makes it feasible to use CLUDAS in a closed loop adaptive setting, where our approach is competitive with an approach using an MSE- oracle on the testing set (which is not feasible on real problem without the ground truth available), and also beats strong open loop adaptive baselines while being easier to train and easier to apply.

Section Title: Contributions
  Contributions • We show how to train a cWGAN to generate posterior samples across a continuum of sampling rates; We solve in a Bayesian fashion not only a single inverse problem, but a continuum of inverse problems. • We demonstrate the first posterior-based mask design method for MRI. • We propose to use the variance of the posterior distribution of images given measurements as quality metric for mask performance. • We show that despite the network being trained as a reconstruction method and not as a sampling method, our adaptive approach CLUDAS is competitive in both settings and a strong contender for being used in active settings as it matches computationally expensive state-of-the-art approaches.

Section Title: Implications
  Implications We contend that our uncertainty driven sampling framework can extend to many similar problems where one wants to reduce acquisition times, such as atomic force microscopy ( Abramovitch et al., 2007 ), transmission electron microscopy ( Kovarik et al., 2016 ), or trajectory optimization for ultrasound acquisitions ( Malinen et al., 2005 ). Our results open the door of lever- aging Bayesian experimental design methods for designing adaptive sampling patterns in clinical settings and beyond.

Section Title: Related works
  Related works We especially want to highlight the work of  Zhang et al. (2019) , which bears several similarities with our method. It is important to note that their approach is not generative, as they only have point estimates of the mean and some learned uncertainty metric. Moreover, they assume the reconstructed image to be normally distributed with a diagonal covariance, a practically unrealistic assumption, which is not required in  Adler &Öktem (2018) .

Section Title: NOTATION AND PROBLEM SETTING
  NOTATION AND PROBLEM SETTING

Section Title: Notation
  Notation Throughout this paper, we will refer to vectors as boldface lowercase letters e, x, y, and assume that they correspond to N × N images that are vectorized to a p dimensional space. In particular, we will assume that these vectors belong to some appropriately defined spaces X , Y ⊆ C p . We use vectors in C p as MR images are inherently complex. Boldface uppercase letters X, Y will in general refer to random variables (formally, they are random vectors), with the exception of F and P ω which will respectively refer to the discrete Fourier transform operator and the sampling operator, that will be defined below. Finally, we will use ω ∈ O = {0, 1} p to be a p dimensional binary vector, and we will refer to it interchangeably as a sampling, subsampling or undersampling pattern/mask. Finally, we will refer to the distribution of a random variable X as P X , and extend the nottation to conditioned random variables such as P X|y ≡ P X|Y=y .

Section Title: Problem setting
  Problem setting An inverse problem is the task of recovering the ground truth x ∈ X from measurements y ∈ Y. In the case of MRI, we consider the following acquisition model y ω = P ω Fx + e, (1) where y ω ∈ Y corresponds to the measurements obtained with a given sampling mask ω and where the sampling operator (P ω ) ii = 1 if i ∈ ω, 0 otherwise. ω ⊆ [p] := {1, . . . , p} is a set containing the sampled locations. Note that while the ground truth x is an image living in the image space X , the measurements y ω live in the Fourier space. The Fourier space is often referred to as k-space in the MRI literature. In this paper, we restrict ourselves to the setting where ω is composed of lines in the Fourier space, also known as Cartesian sampling in the MRI literature 1 , and usually constrained by a maximal number n of lines that can be acquired. F denotes the Fourier transform, x ∈ X is the ground truth image and e ∈ Y is a white additive noise. Without loss of generality, neglecting basic sampling effects, such as magnetic field inhomogeneity and spin relaxation, we assume e = 0 in the sequel. As we will be working in a Bayesian framework, we define a random variable X ∼ P X from which ground truth, complete measurements are generated, distributed according to the unknown prior P X . From this we also define the distribution P Yω as well as the joint distribution P X,Yω and the posterior P X|Yω , where Y ω = P ω FX.The posterior is especially of interest as for a fixed y ω ∈ Y , P X|yω (short for P X|Yω=yω ) represents the probability distribution of ground truths that are likely to have generated the observed data y ω with a given mask ω.

Section Title: BACKGROUND
  BACKGROUND

Section Title: MASK DESIGN IN MRI
  MASK DESIGN IN MRI

Section Title: Fixed masks
  Fixed masks The overwhelming majority of data-driven mask design approaches work in an open loop fashion: the sampling mask is built using training data and kept fixed at inference time. Even Under review as a conference paper at ICLR 2020 the VDS paradigm, that prescribes sampling a mask at random from a parametric distribution, uses a fixed mask that is tuned in an ad-hoc fashion when applied clinically ( Jaspan et al., 2015 ). Formally, we consider a training set of original data {x i } m i=1 that are assumed to originate from an unknown prior distribution P X . We are constrained to a maximal sampling budget n and want to find a mask that minimizes a given loss function (e.g. MSE, SSIM) on these training samples. The abstract problem of our interest then can be written as follows min ω∈A E X∼P X [ (X, Y = P ω FX)] . (2) where A denotes the constrained set of masks ω that are made of lines (cf. section ) and that respect the maximal sampling budget, i.e. |ω| ≤ n (here, | · | denotes the cardinality of the set ω). Formally, let us define S as a set of subsets of {1, . . . , p} that contains the N possible lines given a vectorized image of dimension p = N 2 . A is defined as A := {ω ⊆ [p] : ω = v∈S v, S ⊆ S, |ω| ≤ n}, which means that ω is constructed as a union of the elements v of a subset S of all possible lines S, with the additional constraint that the overall mask will respect the sampling budget n. However, the finite amount of samples requires to solve the empirical risk minimization version of Equation 2, namely min ω∈A 1 m m i=1 (x i , y = P ω Fx i ). Given that m is large enough and that the training and testing data do originate from P X , statistical learning theory guarantees that a mask performing well on the training set will adequately generalize. In the literature, two trends are noticeable. A first body of works focused on constructing a good distribution from which to sample  Ravishankar & Bresler (2011a) ;  Vellagoundar & Machireddy (2015) ;  Bahadir et al. (2019) ;  Sherry et al. (2019) . Other approaches tried to directly design a fixed sampling mask that performs well on training data ( Seeger et al., 2010 ;  Gözcü et al., 2018 ;  Gözcü et al., 2019 ;  Haldar & Kim, 2019 ). More recent approaches tried to jointly train the mask with a deep network  Weiss et al. (2019) ;  Bahadir et al. (2019) .

Section Title: Adaptive masks
  Adaptive masks Until the re-birth of deep learning, most reconstruction methods relying on CS suffered from long reconstruction times, due to their iterative nature. However, recent works leverag- ing the online reconstruction speed of deep learning achieved mask designs in a closed loop fashion, i.e., developing algorithms that could be used in an online fashion to adapt to patients. For a fixed, unknown data x, the adaptive approach aims at leveraging the information from the already measured frequencies to guide what should be acquired next. Instead of a fixed mask ω, we are building up partial masks ω t as union of individual Fourier space lines v t , with ω = ω T being the largest mask satisfying |ω T | ≤ n. The optimization now happens at runtime as a multistage problem which has to choose each new element v t such that min vt|v0,...,vt−1 (x, y ω T = P ω T Fx; ω T = T i=1 v i ), (3) that is, we want to take at each time step t the sample that will allow us to get the lowest final error, but being constrained by our previous acquisitions and the fact that we cannot look into the future to know where we should sample. The problem requires developing an online sampling method that uses the partial information y ωt at each t to decide on its next action. Two approaches have been proposed for this problem in the literature.  Jin et al. (2019)  took a self-supervised learning approach, where a sampling network learns to imitate a Monte-Carlo tree search method and predicts v t |v 0 , . . . , v t−1 for all t.  Zhang et al. (2019)  leveraged adversarial training to jointly train a reconstruction algorithm and an evaluator that gives scores to the quality of reconstructed lines in Fourier space. The sampling procedure simply iteratively added to the mask the lines with the lowest reconstruction score.

Section Title: Other types of sampling
  Other types of sampling While our work here focuses on Cartesian sampling, which is by far the most widely used trajectory in MRI ( Lustig et al., 2007 ), many other physically feasible trajectories have been investigated over the years for accelerated MRI. Radial trajectories have mainly been used in the context of dynamic MRI ( Zhang et al., 2010a ;  Feng et al., 2014 ), and non-structured trajectories have also been explored and validated on real acquisitions ( Lazarus et al., 2017 ). In particular, we note that some CS-based methods have shown online reconstruction times in the context of dynamic MRI ( Zhang et al., 2010b ) using radial trajectories, but such trajectories are rarely used in the context of static MRI.

Section Title: DEEP BAYESIAN INVERSION
  DEEP BAYESIAN INVERSION In ( Adler &Öktem, 2018 ), the authors propose a framework to estimate the posterior distribution P X|Y , i.e., the distribution of original images x that are likely to have generated the observed data y. They formulate the problem as finding a parametrized generator G θ * : Y → P X that allows to minimize the Wasserstein distance with the unknown posterior P X|Y over all the observations, namely minimizing Here, P X is the space of probability measures on X . As this approach in not tractable in practice, they show that Equation 4 can equivalently be formulated as After finding the optimal parameters (θ * , φ * ), the conditional generator G(z, y) : X × Y → X approximates the posterior distribution P X|y and different values of z yield different samples from P X|y . Note that  Adler &Öktem (2018)  applies this method to reconstruction data from ultra-low dose 3D helical computed tomography (CT). This differs from MRI in several regards, but it is significant in our case that we can generate different instances of Equation 4 by selecting different ω in Equation 1. Minimizing Equation 4 without giving ω explicitely would amount to generating a posterior distri- bution from data y acquired when using any possible mask ω sampled from a random vector Ω with a possibly unknown distribution. This is why the approach of ( Adler &Öktem, 2018 ) minimizes over the distance for observation in Equation 4 and mask designs approaches consider minimization over original data (cf., equation 2).

Section Title: Uncertainty estimation
  Uncertainty estimation Once the generator has been trained, we can sample from G θ * (Z, y ω ) which approximates P X|yω . Let {x i = G θ * (z i , y ω )} ns i=1 be samples of the posterior P X|yω , where z i are iid samples from Z and n s and the number of samples taken. Then, as in ( Adler &Öktem, 2018 ), the ground truth image x can be estimated by the empirical point-wise mean of these samples, namelyx = 1 ns ns i=1x i . The corresponding empirical point-wise variance can be definedσ 2 = 1 ns−1 ns i=1 |x i −x| 2 , where | · | denotes the modulus. Equally, one can estimate the ground truth Fourier spectrum Fx using the empirical average es- timator Fx. The empirical point-wise variance in the Fourier space can be obtained asσ 2 F = 1 ns−1 ns i=1 |Fx i − Fx| 2 . This feature is specific to generative models, as getting samples from P X|yω allows to transform these to a different domain, enabling to have simultaneous variance es- timates in both the image and Fourier spaces. This is not possible with methods that only provide point-wise estimates of the mean and the variance in image space, such as the one used by  Zhang et al. (2019)  or the direct estimation of  Adler &Öktem (2018) .

Section Title: UNCERTAINTY DRIVEN SAMPLING
  UNCERTAINTY DRIVEN SAMPLING Due to the ability of constructing estimates of both the spatial and Fourier space pixel-wise vari- ances of P X|yω , the approach of ( Adler &Öktem, 2018 ) can be leveraged to produce both fixed and adaptive sampling patterns by acquiring the frequencies with the highest empirical pixel-wise variance in the Fourier domain. As we constrained ourselves to acquiring full lines in the Fourier domain, we will consider the total estimated variance in the Fourier space along the i-th line v i ∈ S and define Note that u 1D i (y ω ) ∈ R N contains the variances of the N possible lines in the Fourier domain. We will refer to u 1D (y ω ) as the aggregated variance (along the x-dimension in Figure TODO). Under review as a conference paper at ICLR 2020 Fixed sampling. Using the aggregated variance as a loss function, we can reformulate Equation 2 as min ω∈A E X∼P X i u 1D i (Y ω = P ω FX) (7) where samples {x i } m s=1 are obtained from an unknown prior distribution P X , and Y ω contains partial information on X through the model 1. In practice, as P X is not available, one seeks to solve the empirical risk minimization (ERM) min ω∈A 1 m m s=1 i u 1D i (y ω,s = P ω Fx s ) . The aggregated point-wise variance of the posterior can be seen as a cost that one seeks to minimize on a training set, and consequently, can it can replace traditional cost functions such as the 2 -norm in most fixed sampling optimization method.

Section Title: Adaptive sampling
  Adaptive sampling Ideally, we aim at making a series of sampling decisions v 1 , . . . , v T to mini- mize the total final uncertainty for a given ground truth image x once our sampling budget n lines is spent, i.e., for each t, min vt|v0,...,vt−1 N i=1 u 1D i (y ω T ) , (8) where ω T = T t=1 v t . Due to causality, we do not have access to this final posterior, or even the posterior of the mask which will result from choosing the next innovation v t . We can only make use of the partial observations y ωt corresponding to the partial masks ω t = t−1 i=1 v t up to the time step t. We choose to adopt a greedy approach to approximately solve min vt∈S i u 1D i (y ωt∪vt ) at each time step t (9) by simply choosing as v t the line i with the largest aggregated variance u 1D i . The overall flow is then: at each time t, (i) observe y ωt , (ii) select the line v t = v i * , where i * = argmax i u 1D i (y ωt ), (iii) update ω t+1 = ω t ∪ v t and (iv) iterate until the cardinality constraint is met. As no assumptions are made on the underlying distribution of the posterior, this application is only made possible by leveraging a generative framework that can estimate the posterior at all sampling rates considered for widely different mask designs. It is rendered tractable by the fact that even two samples from the posterior allow to construct an empirical variance estimate that can efficiently drive the sampling procedure.

Section Title: IMPLEMENTATION
  IMPLEMENTATION

Section Title: Training data
  Training data The data set used in the first three experiments (subsections) below consists of a proprietary dataset of 2D T1-weighted brain scans. In our experiments, we use 100 slices of sizes 256×256 from five such subjects, 20 per subject. Three subjects (60 slices) were used for training the network, two subjects (30 slices) for testing. The data were then massively augmented with both rigid transformations and elastic deformations to counter overfitting as our dataset is very small, following the recommendations of ( Ronneberger et al., 2015 ;  Schlemper et al., 2018 ). Exact details on the dataset and augmentation methods used can be found in Appendix A.1.

Section Title: Architecture
  Architecture For posterior sampling, we used the same discriminator architecture as described in ( Adler &Öktem, 2018 ). For the conditional generator, we used the cascading network of ( Schlemper et al., 2018 ), where the data-consistency layer enforced perfect consistency. Perfect data consistency means that at the end of each block, one replaces the reconstructed value with the corresponding measured value in the Fourier space. This ensures that the reconstruction is consistent with the ob- servations where measurements were acquired. We used 3 CNN blocks, where each block contained 5 convolutional layers followed by ReLu. As our data are complex, we split the real and imaginary part as two channels and add two channels of Gaussian white noise to the conditional generator.

Section Title: Training
  Training We use the same loss as in ( Adler &Öktem, 2018 ). The loss is reproduced in Ap- pendix A.4 for completeness. We use Adam ( Kingma & Ba, 2014 ) with β 1 = 0.5, β 2 = 0.9, and learning rate 2 · 10 −4 as in  Adler &Öktem (2018)  although we do not use noisy linear cosine decay Under review as a conference paper at ICLR 2020 out of simplicity. The model is trained for 6 · 10 5 backpropagations, which was chosen adhoc to account for the fact there are combinatorial numbers of masks being observed for each image (our reference point  Adler &Öktem (2018)  uses 5 · 10 4 for a larger dataset).. For every 5 iterations, the generator was trained once and the discriminator was trained four times. In order to allow calculating the posterior throughout the sampling process, we generate observations of subsampled images at various rates by randomly generating horizontal Cartesian masks for sampling rates ∈ [0.025, 0.5], as described in detail in Appendix A.2.

Section Title: Metrics
  Metrics We will use mean squared-error (MSE), structural similarity (SSIM) ( Wang et al., 2004 ) as well as the posterior variance for comparisons. MSE and SSIM are computed between the re- constructed image and the corresponding original, ground truth image. The posterior variance is estimated through the a pixel-wise empirical variance estimate, and is averaged on the whole image to produce a single scalar. This metric does not require a reference.

Section Title: EXPERIMENTS
  EXPERIMENTS Throughout our experiments, we use the empirical mean obtained from two posterior samples, as well as the corresponding empirical standard deviation. We show exhaustively in appendix B that while using 10 samples from the posterior improves the quality of reconstruction, it is sufficient to use the variance estimate from 2 posterior samples in the CLUDAS method.

Section Title: CONSISTENCY OF THE UNCERTAINTY ESTIMATE
  CONSISTENCY OF THE UNCERTAINTY ESTIMATE As can be  Figure 1 , the average MSE correlates well with the average posterior variance, both in image space and in Fourier space, which suggests that the posterior variance could serve as an ap- proximate MSE oracle. While training was only performed in the image domain, the generated samples have consistency both in Fourier and in image space, showing that the uncertainty-based approach does provide meaningful information on the error in the reconstruction. The consistency in Fourier space is crucial for the sampling procedure, as our sampling method leverages the vari- ance estimates in Fourier space, while the consistency in image space gives valuable information to interpret the reconstructed data, which are important for clinicians.

Section Title: RECONSTRUCTION QUALITY
  RECONSTRUCTION QUALITY In order to assess the reconstruction obtained by the adaptive masks, we define a closed loop oracle MSE driven adaptive sampling method (CLOMDAS), which leverages MSE instead of uncertainty at inference time. While CLOMDAS is not feasible in practice, it remains an interesting baseline showing how the mask design could be improved by having access to the actual MSE at testing time. Under review as a conference paper at ICLR 2020  Figure 2  shows how the CLUDAS method compares against the CLOMDAS method on a sample from the testing set. CLUDAS is competitive with CLOMDAS at every sampling rate considered, even without having access to any oracle information. This behaviour is consistently observed on the whole testing set, as shown in  Tables 1  below.

Section Title: COMPARISON WITH OTHER METHODS
  COMPARISON WITH OTHER METHODS We compare out method to the following • Learning based compressed sensing (LBC) ( Gözcü et al., 2018 ;  Sanchez et al., 2019 ): This method incrementally builds up ω by computing an expected improvement of a loss at each step. This expected improvement is simply obtained by searching which line will add the largest improvement at the next step, and once it has been found, it is permanently added to the mask. Then, the algorithm proceeds until the cardinality constraint |ω| = n is met. When trained with MSE, we will refer to the method as LBC-M, and when trained to minimize variance, we will refer to it as LBC-V •  Vellagoundar & Machireddy (2015) : This method proposed the simple heuristic ap- proach of (i) constructing a PDF from a training data and (ii) sampling at random from the obtained PDF. Our implementation used the spectrum of the whole averaged training set for the PDF. We were not able to compare our method to the closed loop method of  Zhang et al. (2019) , since their code is not being publicly available at the time of writing. When comparing the performance of reconstruction of different mask designing methods on the modified generator of Adler andÖktem, we observe that the heuristic baseline of  Vellagoundar & Machireddy (2015)  performs significantly worse at any sampling rate and for any metric. This is not surprising, as this method simply samples art random from a constructed PDF. Comparing the variations of the LBC methods, we notice that increasing the number of averaged samples during the training phase of the mask is translated into a uniform improvment of the performance for any sampling rate. This is more exhaustively discussed in Appendix B . Focusing on the LBC methods trained with 10 posterior sample averaging, we see that the LBC-M method outperforms the LBC-V method. It is worth noting that the LBC-M uses the full ground truth to build its mask, while the LBC-V method only leverages the variance estimation in Fourier domain. This again highlight the reliability of the uncertainty as a mask designing technique. This conclusion is also supported by the CLUDAS approach remaining competitive with the CLOMDAS one, which is infeasible in practice, due to requiring oracle MSE calls at test time. Note also that our method does not require the heavy computational burden of generating the sampling mask ahead of time, and can immediately be used on-the-fly after training. The CLUDAS method is the most effective at reducing pos- terior variance, even if LBC-M(10) and CLOMDAS(2) are close runner-ups. More surprisingly, our CLUDAS method is found to yield the best SSIM performance, a metric designed to match the human perception of quality better than MSE.

Section Title: DISCUSSION AND FUTURE WORKS
  DISCUSSION AND FUTURE WORKS Posterior distribution for a continuum of sampling rates. Successfully modelling the continuum of sampling rates stems from the fact the these inverse problems depend on each other in a highly structured and regular fashion. This enabled us to successfully demonstrate for the first time that a principled Bayesian approach for a closed loop mask optimization with rigorous variance estimates is feasible.

Section Title: Generically trained generative reconstruction method
  Generically trained generative reconstruction method The current generative model was trained in a generic fashion and not specifically to optimize the quality of masks designed through it. The ability for designing masks stems purely from training it as a rigorous Bayesian modelling of the continuum of inverse problems. This allows the posterior to be conditioned on incrementally collected informa- tion in a closed loop fashion. However, our method could easily be incorporated in a reinforcement learning-based framework aimed at jointly training reconstruction and sampling such as  Jin et al. (2019) . This would give the best of both worlds, giving principled uncertainty estimates to the RL sampler, moving beyond greedy sampling and possibly speeding up the training of the reconstruc- tion method by focusing on regions with less reliable varaince estimates instead of using masks sampled from distributions as in this work.

Section Title: Limitations of the posterior estimation
  Limitations of the posterior estimation We leveraged the approach of  Adler &Öktem (2018) , which is the first of its kind to allow to construct a posterior estimator from samples of the joint distribution. While it works well empirically, the authors did not provide any analysis or guarantees on how well the generator captures the tails of the posterior distribution. Our observations suggest that unusual images, i.e. far away from the mean of the learned distribution might not be accurately Under review as a conference paper at ICLR 2020 captured, i.e. the estimated variance is lower than expected. This could be due to the limited training data available, but might also an artifact of the cWGAN approach which tends to struggle with capturing weaker modes of their distributions. Specifically, the problematic examples might be an indication that while the loss shown in eq. (11) avoids mode collapse, there might still be some "mode deflation" leading to the network underestimating the diversity of the data distribution.

Section Title: Adaptive vs fixed sampling
  Adaptive vs fixed sampling Adaptive and fixed sampling methods both have advantages and lim- itations from a practical perspective. The main advantages of a fixed mask approach lie in the ease of deployment of the obtained mask, as it simply needs to be programmed into a scanner. We also have a simple generalization bound of the obtained mask, relying on a simple application of Hoeffd- ing's inequality. In contrast, adaptive methods are currently difficult to deploy it on scanners, as it would require hardware capable of running a neural network guiding the sampling procedure. They are also harder to train and currently lack rigorous reliability guarantees. However, if successfully trained and deployed they avoid rigid assumptions about the problem and are able to incorporate partial data into the acquisition process, which in turn leads to an improved performance. In this work, we found that using the adaptive method also increased robustness to the noise in the quality criterion used to drive the mask Reliability guarantees beyond the variance estimate presented in this work (i.e. quantifying the uncertainty of the uncertainty estimate) are an important future direction of research.

Section Title: Extensions to the current model
  Extensions to the current model We showed that it is possible to use the uncertainty estimate to design fixed masks as in LBC-V, for settings where we are only interested in using the posterior variance as a natural criterion for reconstruction quality, e.g. masks for MRI systems where incor- porating a neural network at scanning time is not feasible. In this setting, there are low hanging fruits for improving the method by making use of the available ground truth information, i.e. jointly using posterior variance with other metrics which require a ground truth.
  This kind of structured acquisition originates from physical considerations and has the benefit of being easily implementable in practice.

```
