Title:
```
Under review as a conference paper at ICLR 2020 ENTROPY PENALTY: TOWARDS GENERALIZATION BE- YOND THE IID ASSUMPTION
```
Abstract:
```
It has been shown that instead of learning actual object features, deep networks tend to exploit non-robust (spurious) discriminative features that are shared be- tween training and test sets. Therefore, while they achieve state of the art per- formance on such test sets, they achieve poor generalization on out of distribu- tion (OOD) samples where the IID (independent, identical distribution) assump- tion breaks and the distribution of non-robust features shifts. Through theoretical and empirical analysis, we show that this happens because maximum likelihood training (without appropriate regularization) leads the model to depend on all the correlations (including spurious ones) present between inputs and targets in the dataset. We then show evidence that the information bottleneck (IB) principle can address this problem. To do so, we propose a regularization approach based on IB called Entropy Penalty, that reduces the model's dependence on spurious features- features corresponding to such spurious correlations. This allows deep networks trained with Entropy Penalty to generalize well even under distribution shift of spurious features. As a controlled test-bed for evaluating our claim, we train deep networks with Entropy Penalty on a colored MNIST (C-MNIST) dataset and show that it is able to generalize well on vanilla MNIST, MNIST-M and SVHN datasets in addition to an OOD version of C-MNIST itself. The baseline regularization methods we compare against fail to generalize on this test-bed.
```

Figures/Tables Captions:
```
Figure 1: Sensitivity s * i of output f θ * (x) with respect to input dimensions x i vs. the probability p i (controlling correlation between input dimension i and target) for synthetic dataset A (Eq. 9). Left plot shows θ * i (same as sensitivity) computed for a trained linear model. Right plot shows sensitivity computed for a trained MLP. IB regularization acts as a filter, suppressing the sensitivity of both these models to weak correlation features (p i close to 0.5).
Figure 2: Sensitivity s * i of output f θ * (x) with respect to input dimensions x i vs. the probability p i (deciding the choice between feature with variance σ 2 vs. 10σ 2 ) for synthetic dataset B (Eq. 10). Left plot shows θ * i (same as sensitivity) computed for a trained linear model. Right plot shows sensitivity computed for a trained MLP. IB regularization suppresses the sensitivity of both these models to large variance features (p i close to 0).
Figure 3: Performance on the distribution shifted test set of C- MNIST for various methods trained on C-MNIST training set. See figure 5 in appendix for samples from C-MNIST dataset.
Figure 4: Baseline methods severely overfit color features in the C-MNIST training set leading to near 100% accuracy on C-MNIST validation set but close to chance performance on the distribution shifted C-MNIST test set.
Table 1: Out of distribution performance on test sets using a model trained with Entropy Penalty on C-MNIST dataset.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION It is now known that deep networks trained on clean training data (without proper regularization) often learn spurious (non-robust) features which are features that can discriminate between classes but do not align with human perception ( Jo & Bengio, 2017 ;  Geirhos et al., 2018a ;  Tsipras et al., 2018 ;  Ilyas et al., 2019 ). An example of non-robust feature is the presence of desert in camel images, which may correlate well with this object class. More realistically, models can learn to exploit the abundance of input-target correlations present in datasets, not all of which may be invariant under different environments. Interestingly, such classifiers can achieve good performance on test sets which share the same non-robust features. However, due to this exploitation, these classifiers per- form poorly under distribution shift ( Geirhos et al., 2018a ;  Hendrycks & Dietterich, 2019 ) because it violates the IID assumption which is the foundation of existing generalization theory ( Bartlett & Mendelson, 2002 ;  McAllester, 1999b ;a). The research community has approached this problem from different directions. In part of domain adaptation literature ( Eg. Ganin & Lempitsky (2014) ), the goal is to adapt a model trained on a source domain (often using unlabeled data) so that its performance improves on a target domain that contains the same set of target classes but under a distribution shift. There has also been research on causal discovery ( Hoyer et al., 2009 ;  Janzing et al., 2009 ;  Lopez-Paz et al., 2017 ;  Kilbertus et al., 2018 ) where the problem is formulated as identifying the causal relation between random variables. This framework may potentially then be used to train a model that only depends on the relevant features. However, it is often hard to discover causal structure in realistic settings. Adversarial training ( Goodfellow et al., 2014 ;  Madry et al., 2017 ) on the other hand aims to learn models whose predictions are invariant under small perturbations that are humanly imperceptible. Thus adversarial training can be seen as the worst-case distribution shift in the local proximity of the original training distribution.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Our goal is different from the aforementioned approaches. We aim to directly learn a classification model using labeled data which is capable of generalizing well under input distribution shift (not constrained to being locally) without making any changes to the model during test time. Thus our goal is more aligned with the recently proposed Invariant Risk Minimization (Arjovsky et al., 2019), but imposes less constraints on the data collection process. For a detailed discussion on related work, see section 4. Our contributions are as follows: 1. Our theoretical and empirical analysis shows that models trained with maximum likelihood objec- tive without appropriate regularization can, in general, learn to exploit/depend on all the correlations present between inputs and targets in the training set, leading to non-robust representations. While this representation may allow them to yield state-of-the-art performance on test sets whose distribu- tion is identical to the training set, they would perform poorly when the distribution of non-robust features shift. This effect is not mitigated by a larger training set containing more variations between samples. Thus based on our analysis, it should not be surprising that deep networks trained on image datasets show poor performance under input perturbations and (in general) input distribution shifts as discussed in numerous recent papers ( Hendrycks & Dietterich, 2019 ;  Jo & Bengio, 2017 ;  Geirhos et al., 2018b ;a). 2. We provide evidence showing that the information bottleneck (IB) principle ( Tishby et al., 2000 ;  Tishby & Zaslavsky, 2015 ) is capable of addressing the out of distribution generalization prob- lem. Specifically, our proposal, that we call Entropy Penalty is based on the IB principle and aims at learning a representation that throws away maximum possible information about the input dis- tribution while achieving the goal of correctly predicting targets. Intuitively, doing so makes the representation agnostic to non-robust features in the input, thus allowing the model's predictions to be invariant under a shift of the distribution of such features during test time. 3. We show experimental results using Entropy Penalty in which a deep network trained on a colored version of MNIST dataset (see appendix A for samples) is able to generalize well on vanilla MNIST, MNIST-M, SVHN and a distribution-shifted version of the colored MNIST dataset itself. We note that most of the baseline methods failed to the extent of achieving performance close to random chance.

Section Title: HOW TO GENERALIZE UNDER DISTRIBUTION SHIFT?
  HOW TO GENERALIZE UNDER DISTRIBUTION SHIFT? In order to approach a solution to our problem, we observe that the out of distribution nature of samples during test time arises because certain aspects of the input change even though the input still corresponds to the same pool of targets as seen during training. An instance of this change would be seeing a camel in the city (during test time) which has dramatically different background features compared to the desert (seen during training). Thus, if a model is trained to depend only on camel features (which defines the class more universally) and ignore other aspects, a shift in the distribution of such aspects will no longer affect the model's decision during test time. We note that the above intuition is encapsulated by the information bottleneck learning principle ( Tishby et al., 2000 ;  Tishby & Zaslavsky, 2015 ) which minimizes the following objective, L IB (θ) = −I(f θ (X), Y) + βI(f θ (X), X) (1) where X and Y represent the input and target (often class label) random variables, f θ (.) denotes a deterministic model with learnable parameters θ, and β is a hyper-parameter. While the first term effectively maximizes the training data likelihood, it is the second term that regularizes the model representations to become invariant to non-robust features that are not dominantly present in all samples by minimizing mutual information between input and representation random variables. We now derive Entropy Penalty which is equivalent to the IB regularization for deterministic models. We note that I(f θ (X), Y) and I(f θ (X), X) can be written equivalently as follows, I(f θ (X), Y) = H(Y) − H(Y|f θ (X)) (2) I(f θ (X), X) = H(f θ (X)) − H(f θ (X)|X) (3) where H(.) denotes entropy and H(.|.) denotes conditional entropy. Note that H(Y) in Eq. 2 is fixed and H(Y|f θ (X)) is the same as the maximum likelihood loss. Secondly, since f θ (.) is a de- terministic function, the conditional entropy H(f θ (X)|X) is fixed. Thus we only need to minimize Under review as a conference paper at ICLR 2020 the entropy of the learned representation H(f θ (X)) in Eq. 3. Thus in the case of deterministic f θ (.), the minimization in IB objective can be equivalently framed as, Therefore, we call this general form of regularization Entropy Penalty. Note that while the first term (data likelihood) is easy to optimize, the second term is often intractable for continuous high dimen- sional distributions. The following proposition shows an equivalent form of the above objective. where C is a positive constant for discrete Y, independent of θ. The benefit of the above form forL IB (θ) instead of Eq. 4 is that it can often be easier to model the conditional Pr(f θ (X)|Y) compared to the marginal Pr(f θ (X)). For instance, if we assume Pr(f θ (X)|Y) is Gaussian for all class labels Y, the entropy of the conditional distribution Pr(f θ (X)|Y) has a closed-form solution given by 0.5 log(2πeσ 2 Y ), where σ 2 Y denotes the variance of the class conditional Gaussian distribution of f θ (X) for class Y. On a practical note, when applying entropy penalty to deep networks, we found that it was not effec- tive when applied to the last layer representations. However, applying this penalty to the first hidden layer representation improved performance under distribution shift. While we do not have a com- plete explanation for this behavior, we conjecture that this could be because of the data processing inequality for deep networks ( Tishby & Zaslavsky, 2015 ) which states, I(h 1 , X) ≥ I(h 2 , X) ≥ . . . ≥ I(h L , X) (5) where h i denotes the i th hidden layer representation and L is the depth of the network. Writing mutual information in terms of entropy and conditional entropy, and taking advantage of the fact that the conditional entropy term is fixed for a deterministic conditional, we have that, Thus entropy is larger for lower layers. Further, minimizing entropy for higher layers does not ensure entropy is minimized for lower layers due to the above inequalities. Thus, any excess infor- mation about the input captured by the first layer gets propagated to the higher layers, the effect of which may get amplified under distribution shift if entropy minimization at last layer is not done ap- propriately. For all experiments conducted using entropy penalty (EP), we use the aforementioned Gaussian assumption on the representation of the first hidden layer and compute its class condi- tional variance in order to minimize the conditional entropy. Specifically, let h(x) represent the first hidden layer representation of an input x (before non-linearity), then we implement EP as, R EP = K k=1 E x∼D(x|y=k) [(h(x) − µ k ) 2 ] (7) where µ k := E x∼D(x|y=k) [(h(x)] and D denotes the data distribution. In practice, we re- place expectation with average over mini-batch samples. For CNNs a mini-batch has dimensions (B, C, H, W ), where we denote B- batch size, C- channels, H- height, W- width. In this case, we reshape this tensor to take the shape (B × H × W, C) and treat each row as a hidden vector h.

Section Title: THEORETICAL ANALYSIS
  THEORETICAL ANALYSIS We now theoretically study the behavior of IB principle on two synthetic datasets designed to pro- vide insights into the invariant representation that IB helps in learning, and simultaneously reveals why it should not be surprising that models trained using maximum likelihood (without appropri- ate regularization) perform poorly under input perturbations and distribution shift during test time. Although these analyses are done for linear regression, in each case, we empirically verify these predictions on deep ReLU networks. For our analysis, we use the following objective, Here the IB regularization H(f θ (x)|y) is kept in the exponent for the ease of analytical simplicity. Also, setting β = 0 yields our baseline case without the IB regularization.

Section Title: SYNTHETIC DATASET A
  SYNTHETIC DATASET A Minimizing the class conditional entropy forces the distribution of neural network representation corresponding to each class to have the minimum amount of information about the input data. Therefore combining this regularization with the traditional classification losses (Eg. cross-entropy) should encourage the neural network to pick features that are dominantly present in class samples and able to discriminate between samples from different classes. To formalize the above intuition, we consider the following synthetic data generating process where the data samples x ∈ R d and labels y are sampled from the following distribution, y ∼ {−1, 1} x i ∼ N (y, σ 2 ) with probability p i N (−y, σ 2 ) with probability 1 − p i (9) where i ∈ {1, 2, · · · , d}, y is drawn with uniform probability, and x = [x 1 , x 2 , · · · , x d ] is a data sample. Also, all x i |y are independent of each other. Thus depending on the value of p i , a feature x i has a small or large amount of information about the label y. Specifically, values of p i close to 0.5 do not tell us anything about the value of y while values close to 0 and 1 can reliably predict its value. Here we make the assumption that features with p i closer to 0.5 are non-robust features whose distribution may shift during test time, while features with p i closer to 0 and 1 are robust ones. Thus we would ideally want a trained model to be insensitive to non-robust features. The theorem below shows how the model parameters depend on input dimensions for the optimal parameters when training a linear regression model f θ (x) := θ T x using the IB objective. Theorem 1 Let θ * be the minimizer of J(θ) in Eq. 8 where we have used synthetic dataset A. Then for a large enough d, θ * = M −1 |2p − 1|, where M := Σ + λI + β(σ 2 I + 4diag(p (1 − p))), such that Σ is a positive definite matrix if 1 p i ∈ {0, 0.5, 1} for all i. As an implication of the above statement, since M −1 is a full rank matrix, aside from the effects due to Σ (which is data dependent and beyond our control), θ * i can in general be non-zero for all input and output correlations. This is especially the case when β = 0 (no IB regularization). When using a sufficiently large β, we find that θ * i gets reduced for larger values of p i (1 − p i ), i.e., when p i is closer to 0.5. Thus the IB objective can help suppress dependence of the learned model on non- robust (low correlation) features. Although this analysis is for linear regression, it provides evidence that it should not be surprising that deep networks trained with maximum likelihood objective with- out an appropriate regularization could exhibit a similar behavior. Note that this is not a problem when training and test set are sampled IID from the same distribution, but only becomes one under distribution shift. Also note that since the analysis depends on expectation over data distribution, a larger training set cannot solve our problem of avoiding dependence on non-robust features. To verify that the behavior studied above also holds for deep networks, we conduct experiments with both linear and deep models on samples drawn randomly from synthetic dataset A. Details of the experimental setup can be found in appendix B. In  figure 1  (left), we plot the parameters θ * i vs. p i for the linear regression model. Since the same analysis cannot be done for deep networks, we use the perspective that the output-input sensitivity s * , where s * i := E x ∂f θ * (x) ∂xi , is equal to θ * i for linear regression. So for deep networks, we plot s * i vs. p i instead as shown in  figure 1  (right). In both models, we normalize the sensitivity values so that the maximum value is 1 for the ease of comparison across different β values. Both for linear and deep models, we find that the sensitivity profile goes to 0 away from p i = 0 and 1 when applying the IB regularization with larger values of coefficients β; this effect being more dramatic for deep networks. Thus the IB regularization helps suppress the dependence of model on non-robust (low correlation) features whose distribution may shift during test time, thus allowing its predictions to be invariant to such shifts. Here we additionally note that for a linear regression model, while sensitivity is same as the model parameter θ, it is not merely the first-order sensitivity that gets suppressed for certain input dimen- sions, the output becomes invariant to such dimensions altogether. Although this argument does not necessarily apply to deep networks, note that the IB regularization itself enforces a more gen- Under review as a conference paper at ICLR 2020 eral condition of finding low entropy representation rather than merely suppressing input sensitivity. Hence the implications of IB could be more general than what our sensitivity analysis shows.

Section Title: SYNTHETIC DATASET B
  SYNTHETIC DATASET B Using the same intuition that small class conditional entropy induces learned representations to have less uncertainty, given two features that can equally differentiate between classes in expectation, the IB objective should pick the one with smaller variance. To formalize this intuition, we consider the following binary classification problem where the data samples x ∈ R d and labels y are sampled from the following distribution, y ∼ {−1, 1} x i ∼ N (y, σ 2 ) with probability p i N (y, kσ 2 ) with probability 1 − p i (10) where i ∈ {1, 2, · · · , d}, y is drawn with uniform probability, and x = [x 1 , x 2 , · · · , x d ] is a data sample. Once again, all x i |y are independent of each other. Thus depending on the value of p i and k, a feature x i has a small or large variance. We would ideally like the model to avoid dependence on dimensions with high variance because they are non-robust and a minor shift in their distribution during test time can affect the model's decision by a large amount. The theorem below shows how the model parameters depend on input dimensions for the optimal parameters when training a linear regression model f θ (x) := θ T x using the IB objective. Theorem 2 Let θ * be the minimizer of J(θ) in Eq. 8 where we have used synthetic dataset B. Then for a large enough d, θ * = M −1 1, where, M := Σ + λI + βσ 2 diag(p + k(1 − p)), such that Σ is a positive definite matrix. Once again, we find that θ * i is non-zero for all dimensions of the input. Assume without loss of generality that k > 1. Then using a sufficiently large β would make the value of θ * i approach 0 if p i is close to 0. In other words, IB regularization forces the model to be less sensitive to features with high variance. Thus, such a model's prediction will not be affected significantly under a shift of the distribution of high variance features during test time. To study the extent of similarity of this behavior between linear regression and deep networks, we once again conduct experiments with both these models on a finite number of samples drawn randomly from synthetic dataset B with k = 10 and σ 2 = 0.001. The rest of the details regarding dataset generation and models and optimization are identical to what was used in section 2.1.1. The sensitivity s * i vs. p i plots are shown in  figure 2  (left) for linear regression and  figure 2  (right) for MLP. In the case of linear regression s * i = θ * i . For both linear regression and MLP, the model's sen- sitivity to all features are high irrespective of p i when trained without the IB regularization (β = 0) and this is especially more so for the MLP. On the other hand, when training with IB regularization, we find that a larger β forces the models to be less sensitive to input feature dimensions with higher variance (which correspond to p i = 0). The discussion around the generality of the IB regularization beyond sensitivity analysis is same as that discussed in section 2.1.1.

Section Title: EXPERIMENTS WITH DATA DISTRIBUTION SHIFT
  EXPERIMENTS WITH DATA DISTRIBUTION SHIFT The experiments below are aimed at investigating: 1. the ability of relevant existing methods to generalize under distribution shift; 2. how well can the proposed method generalize under this shift. Details not mentioned in the main text can be found in appendix B.

Section Title: Datasets
  Datasets We use a colored version of the MNIST dataset (see appendix A for dataset samples and details) for experiment 1, and MNIST-M ( Ganin et al., 2016 ), SVHN ( Netzer et al., 2011 ), MNIST ( LeCun & Cortes, 2010 ) in addition to C-MNIST for experiment 2. All image pixels lie in 0-1 range and are not normalized. The reason for this is that since we are interested in out of distribution (OOD) classification, the normalization constants of training distribution and OOD may be different, in which case data normalized with different statistics cannot be handled by the same network easily.

Section Title: Other Details
  Other Details We use ResNet-56 ( He et al., 2016b ) in all our experiments. We use Adam optimizer ( Kingma & Ba, 2014 ) with batch size 128 and weight decay 0.0001 for all experiments unless specified otherwise. We do not use batch normalization in any experiment except for the adaptive batch normalization baseline method. Discussion and experiments around batch normalization can be found in appendix D. We do not use any bias parameter in the network because we found it led to less overfitting overall. For all configurations specified for proposed method and baseline methods below, the hyper-parameter learning rate was chosen from {0.0001, 0.001} unless specified otherwise. For entropy penalty, the regularization coefficient is chosen from {0.1, 1, 10}.

Section Title: Baseline methods:
  Baseline methods: 1. Vanilla maximum likelihood (MLE) training: Since there are no regularization coefficients in this case, we search over batch sizes from {32, 64, 128} for each learning rate value. 2. Variational bottleneck method ( VIB, Alemi et al. (2016) ) is an existing approximation to the IB objective that uses a non-deterministic network. We therefore investigate its behavior under distribu- tion shift at test time. The regularization coefficient for VIB is chosen from the set {0.01, 0.1, 1, 5}. 3. Clean logit pairing (CLP): Proposed in  Kannan et al. (2018) , this method minimizes the 2 norm of the difference between the logits of different samples. As shown in proposition 3 (in appendix), minimizing this 2 norm is equivalent to minimizing the entropy of the distribution in logit space under the assumption that this distribution is Gaussian. In contrast entropy penalty minimizes the entropy of the class conditional distribution of the first hidden layer. Due to this similarity, we consider CLP a baseline. The regularization coefficient for CLP is chosen from {0.1, 0.5, 1, 10}. 4. Projected gradient descent (PGD) based adversarial training ( Madry et al., 2017 ) has been shown to yield human interpretable features. This makes it a good candidate for investigation. For PGD, inf perturbation is used with a maximum perturbation from the set {8, 12, 16, 20} and step size of 2, where all these numbers are divided by 255 since the input is normalized to lie in [0, 1] . The num- ber of PGD steps is chosen from the set {20, 50}. We randomly choose 12 different configurations out of these combinations. 5. Adversarial logit pairing (ALP,  Kannan et al. (2018) ) is another approach for adversarial ro- bustness and an alternative to PGD. Since it has the most number of hyper-parameters, we tried a larger number of configurations for this baseline. Specifically, we use inf norm with a maximum Under review as a conference paper at ICLR 2020 0 2 4 6 8 10 12 14 Hyper-parameter Configurations 0 20 40 60 80 100 Test Accuracy Entropy Penalty PGD VIB Input Noise CLP AdaBN MLE ALP perturbation from the set {8, 16, 20} and step size of 2, where all these numbers are divided by 255 since the input is normalized to lie in [0, 1] . The number of PGD steps is chosen from the set {20, 50}. The regularization coefficient is chosen from {0.1, 1, 10}. We randomly choose 15 different configurations out of these combinations. 6. Gaussian Input Noise has been shown to have a similar effect as that from adversarial training ( Ford et al., 2019 ) with even better performance in certain cases. We choose Gaussian input noise with standard deviation from the set {0.05, 0.1, 0.2, 0.3}. 7. Adaptive batch normalization ( AdaBN, Li et al. (2016) ) has been proposed as a simple way to achieve domain adaptation in which the running statistics of batch normalization are updated with the statistics of the target domain data. Although this does not fall within our goal of learning a model that does not need any adaption during test time, we investigate this method in experiment 1 due to its simplicity. Since there are no regularization coefficients in this case, we search over batch sizes from {32, 64, 128} for each learning rate value. Experiment 1: In this experiment, we train ResNet-56 on the colored MNIST dataset using the baseline methods and entropy regularization, and test the performance of the trained models on the distribution shifted test set of colored MNIST dataset in each case. For each method, we record the best test performance for each hyper-parameter configuration used, and after sorting these numbers across all configurations, we plot them in  figure 3 . We find that all the baseline methods tend to severely overfit the non-robust color features in C-MNIST leading to poor performance on the distribution shifted test set of C-MNIST.  Figure 4  further confirms this by plotting the validation and test accuracy vs. epoch for all methods for one of the hyper-parameter configurations (see appendix C for details). Clearly, baseline methods achieve near 100% accuracy on C-MNIST validation set but close to chance performance on the distribution shifted C-MNIST test set, showing that these methods have overfitted the color features. Entropy penalty is able to avoid this dependence. Surprisingly even VIB suffers from this issue. This could be because of improper minimization of the information bottleneck (IB) regularization, which could in turn be due to 1. the same reason due to which entropy penalty does not work when applied to higher layers; 2. VIB minimizes an upper bound of the original IB objective. Entropy penalty is able to overcome these difficulties. Experiments 2: In this experiment, we hand-pick the model trained with entropy penalty on C- MNIST in experiment 1 above, such that it simultaneously performs well on SVHN, MNIST-M and MNIST datasets (see section 5 for discussion on this). We used the C-MNIST test set for early stopping. These performances are shown in  table 1 . We note that it is non-trivial for a single model to perform well on all datasets with such distribution shifts without any domain adaptation, especially given it is trained on a dataset on which all baseline methods severely overfit to non-robust features.

Section Title: RELATED WORK
  RELATED WORK

Section Title: Invariant Risk Minimization
  Invariant Risk Minimization The goal of IRM (Arjovsky et al., 2019) is to achieve out of dis- tribution generalization by learning representations such that there exists a predictor (Eg. a linear classifier) that is simultaneously optimal across all environments. IRM achieves this goal by learning (stable) features whose correlation with target is invariant across different environments. In other words, if there are multiple features that correlate with label, then IRM aims to learn the feature which has the same degree of correlation with label irrespective of the environment, while ignoring other features. If the representation induced by such stable features, among others, simultaneously also contain the minimum amount of information about the input, then such representations can alternatively be learned using the information bottleneck principle. Thus it boils down to which strategy forms a better inductive bias for handling distribution shift. On a practical note, the main difference between IRM and our proposal is that IRM requires the explicit knowledge of the envi- ronment from which each training data is sampled from. Our approach does not have this restriction. Due to this, we cannot evaluate IRM in our experimental setting.

Section Title: Adversarial Training
  Adversarial Training There is an abundance of literature around robust optimization ( Wald, 1945 ;  Ben-Tal et al., 2009 ) and adversarial training ( Goodfellow et al., 2014 ;  Madry et al., 2017 ) which study robustness of models to small perturbations around input samples and are often studied us- ing first order methods. Such perturbations can be seen as the worst case distribution shift in the local proximity of the original training distribution. Further,  Tsipras et al. (2018)  discusses that the representations learned by adversarially trained deep network are more human interpretable. These factors make it a good candidate for investigating its behavior under distribution shift. Our theoretical analysis has similarities to this line of work, but our goal and conclusions are broader. Specifically, for linear regression, we derive the optimal parameter value analytically under the infor- mation bottleneck objective. Since, the value of parameters is same as the output-input sensitivity- the derivative of this model's output (not loss) with respect to its input, we plot sensitivity in the case of deep networks because parameters do not correspond to input dimensions for deep networks. Nonetheless, this is a limitation of our analysis and not of the information bottleneck principle be- cause its objective of minimizing representation entropy is more general than reducing first order sensitivity of the model.

Section Title: Domain Adaptation
  Domain Adaptation Domain adaptation ( Wang & Deng, 2018 ;  Patel et al., 2014 ) addresses the problem of distribution shift between source and target domain, and has attracted considerable at- tention in computer vision, NLP and speech communities ( Kulis et al., 2011 ;  Blitzer et al., 2007 ;  Hosseini-Asl et al., 2018 ). Some of these methods address this issue by aligning the two distribu- tions ( Jiang & Zhai, 2007 ;  Bruzzone & Marconcini, 2009 ), while others by making use of adversarial training ( Ganin & Lempitsky, 2014 ;  Ganin et al., 2016 ) and auxilliary losses ( Ghifary et al., 2015 ;  He et al., 2016a ). A common characteristic of all these methods is that they require labeled/unlabeled target domain data during the training process. This is not necessary in the information bottleneck approach, which makes it more flexible.

Section Title: DISCUSSION AND CONCLUSION
  DISCUSSION AND CONCLUSION Based on our analysis, it appears that deep networks are good at achieving state-of-the-art general- ization in common settings because a. they are able to exploit all the correlations present between inputs and targets; and b. the IID assumption holds between training and test sets. However, these attributes also make them perform poorly on distribution shifted test sets. Our analysis provides ev- idence that the information bottleneck (IB) principle can be a potential remedy to this problem. We reached this conclusion by introducing entropy penalty- an equivalent form of the IB regularization for deterministic networks, and showing it generalizes well on out of distribution test sets.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 However, note that while entropy penalty itself is a general form of regularization, our proposed implementation of entropy penalty has certain limitations and it lacks of complete theoretical under- standing. Specifically, 1. The Gaussian distribution assumption of hidden representation is a limitation and may not ap- ply to more general datasets other than MNIST, where class samples have multi-modal features. This requires alternate ways of minimizing the entropy of distributions which is currently a hard open problem. Additionally, since entropy penalty works best when there is a significant gap be- tween correlation/variance of robust and non-robust features (see section 2.1), it may not be easy to get good OOD performance when training set does not have this property. As evidence, we found this to be the case when training with entropy penalty on SVHN and testing on other datasets (not shown). Two possible solutions to this problem could be: a. design more generic algorithms for min- imizing entropy of hidden representation; b. data augmentation techniques that selectively amplify the difference in levels of correlation of robust feature with the target vs. the non-robust ones. 2. Despite our attempt to explain why entropy penalty improves performance under distribution shift when applied to the first hidden layer of deep networks, but not higher layers, a more detailed understanding of this phenomenon remains elusive and is left as future work. Disjoint from above discussion, the traditional practice of using a validation set for early stopping and selecting hyper-parameters is based on the assumption that training/validation/test sets are sam- pled IID from the same distribution ( Arlot et al., 2010 ). However, it is not clear how to early stop and select hyper-parameter values when the goal is to evaluate on out of distribution test sets. This is because a set of non-robust features can be shared between a training and validation set, and thus a high performance on such a validation set does not necessarily imply the learned model can generalize to out of distribution test sets. This topic needs further attention.
  This assumption is needed due to technicality.

```
