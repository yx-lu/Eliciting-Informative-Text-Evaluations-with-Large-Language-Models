<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 IMPROVED TRAINING OF CERTIFIABLY ROBUST MOD- ELS</article-title></title-group><abstract><p>Convex relaxations are effective for training and certifying neural networks against norm-bounded adversarial attacks, but they leave a large gap between certifiable and empirical robustness. In principle, convex relaxation can provide tight bounds if the solution to the relaxed problem is feasible for the original non-convex problem. Therefore, we propose two regularizers that can be used to train neural networks that yield tighter convex relaxation bounds for robustness. In all of our experiments, the proposed regularizers result in higher certified accuracy than non-regularized baselines.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Neural networks have achieved excellent performances on many computer vision tasks, but they could be vulnerable to small, adversarially chosen perturbations that are barely perceptible to humans while having a catastrophic impact on the network's performance (<xref ref-type="bibr" rid="b9">Szegedy et al., 2013</xref>; <xref ref-type="bibr" rid="b9">Goodfellow et al., 2014</xref>). Making classifiers robust to these adversarial perturbations is of great interest, especially when neural networks are applied to safety-critical applications. Several heuristic methods exist for obtaining robust classifiers, however powerful adversarial examples can be found against most of these defenses (<xref ref-type="bibr" rid="b0">Carlini &amp; Wagner, 2017</xref>; <xref ref-type="bibr" rid="b5">Uesato et al., 2018</xref>).</p><p>Recent studies focus on verifying or enforcing the certified accuracy of deep classifiers, especially for networks with ReLU activations. They provide guarantees of a network's robustness to any perturba- tion &#948; with norm bounded by &#948; p &#8804; (<xref ref-type="bibr" rid="b0">Wong &amp; Kolter, 2017</xref>; <xref ref-type="bibr" rid="b0">Wong et al., 2018</xref>; Raghunathan et al., 2018; <xref ref-type="bibr" rid="b5">Dvijotham et al., 2018b</xref>; <xref ref-type="bibr" rid="b21">Zhang et al., 2018</xref>; <xref ref-type="bibr" rid="b21">Salman et al., 2019</xref>). There are exact verifiers that find the exact minimum adversarial distortions &#948; or the robust error (<xref ref-type="bibr" rid="b7">Ehlers, 2017</xref>; <xref ref-type="bibr" rid="b1">Katz et al., 2017</xref>; <xref ref-type="bibr" rid="b28">Tjeng et al., 2017</xref>), but due to the non-convex nature of the problem, exact verification is NP-hard. To make verification efficient and scalable, convex relaxations are adopted to expand the non-convex feasible set given by non-linear activations into convex sets, resulting in a lower bound on the norm of adversarial perturbations (<xref ref-type="bibr" rid="b21">Zhang et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Weng et al., 2018</xref>), or an upper bound on the robust error (<xref ref-type="bibr" rid="b5">Dvijotham et al., 2018b</xref>; <xref ref-type="bibr" rid="b9">Gehr et al., 2018</xref>; <xref ref-type="bibr" rid="b26">Singh et al., 2018</xref>). With LP relaxations (<xref ref-type="bibr" rid="b0">Wong et al., 2018</xref>), such a verifier can be efficient enough to estimate the lower bound of the margin in each iteration for training certifiably robust networks. However, due to the relaxation of the underlying problem, the lower bound of the margin is potentially loose, and thus a barrier remains between the optimal values from the original and relaxed problems (<xref ref-type="bibr" rid="b21">Salman et al., 2019</xref>).</p><p>In this paper, we focus on improving the certified robustness of neural networks trained with convex relaxation bounds. To achieve this, we first give a more interpretable explanation for the bounds achieved in (<xref ref-type="bibr" rid="b0">Weng et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Wong et al., 2018</xref>). Namely, the constraints of the relaxed problem are defined by a simple linear network with adversaries injecting bounded perturbations to both the input of the network and the pre-activations of intermediate layers. The optimal solution of the relaxed problem can be written as a forward pass of the clean image through the linear network, plus the cumulative adversarial effects of all the perturbations added to the linear transforms, which makes it easier to identify the optimality conditions and serves as a bridge between the relaxed problem and the original non-convex problem. We further identify conditions for the bound to be tight, and we propose two indicators for the gap between the original non-convex problem and the relaxed problem. Adding the proposed indicators into the loss function results in classifiers with better certified accuracy.</p></sec><sec><title>BACKGROUND AND RELATED WORK</title><p>Adversarial defenses roughly fall into two categories: heuristic defenses and verifiable defenses. The heuristic defenses either try to identify adversarial examples and remove adversarial perturbations from images, or make the network invariant to small perturbations through training (<xref ref-type="bibr" rid="b19">Papernot &amp; McDaniel, 2018</xref>; <xref ref-type="bibr" rid="b25">Shan et al., 2019</xref>; <xref ref-type="bibr" rid="b22">Samangouei et al., 2018</xref>; <xref ref-type="bibr" rid="b13">Hwang et al., 2019</xref>). In addition, adversarial training uses adversarial examples as opposed to clean examples during training, so that the network can learn how to classify adversarial examples directly (<xref ref-type="bibr" rid="b17">Madry et al., 2017</xref>; <xref ref-type="bibr" rid="b24">Shafahi et al., 2019</xref>; <xref ref-type="bibr" rid="b21">Zhang et al., 2019a</xref>).</p><p>In response, a line of works have proposed to verify the robustness of neural nets. Exact methods obtain the perturbation &#948; with minimum &#948; p such that f (x) = f (x + &#948;), where f is a classifier and x is the data point. Nevertheless, the problem itself is NP-hard and the methods can hardly scale (<xref ref-type="bibr" rid="b2">Cheng et al., 2017</xref>; <xref ref-type="bibr" rid="b16">Lomuscio &amp; Maganti, 2017</xref>; <xref ref-type="bibr" rid="b4">Dutta et al., 2018</xref>; Fischetti &amp; Jo, 2017; <xref ref-type="bibr" rid="b28">Tjeng et al., 2017</xref>; <xref ref-type="bibr" rid="b23">Scheibler et al., 2015</xref>; <xref ref-type="bibr" rid="b1">Katz et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Carlini et al., 2017</xref>; <xref ref-type="bibr" rid="b7">Ehlers, 2017</xref>).</p><p>A body of work focuses on relaxing the non-linearities in the original problem into linear inequality constraints (<xref ref-type="bibr" rid="b26">Singh et al., 2018</xref>; <xref ref-type="bibr" rid="b9">Gehr et al., 2018</xref>; <xref ref-type="bibr" rid="b21">Zhang et al., 2018</xref>; <xref ref-type="bibr" rid="b9">Mirman et al., 2018</xref>), sometimes using the dual of the relaxed problem (<xref ref-type="bibr" rid="b0">Wong &amp; Kolter, 2017</xref>; <xref ref-type="bibr" rid="b0">Wong et al., 2018</xref>; <xref ref-type="bibr" rid="b5">Dvijotham et al., 2018b</xref>). Recently, <xref ref-type="bibr" rid="b21">Salman et al. (2019)</xref> unified the primal and dual views into a common convex relaxation framework, and suggested there is an inherent gap between the actual and the lower bound of robustness given by verifiers based on LP relaxations, which they called a convex relaxation barrier.</p><p>Some defense approaches integrate the verification methods into the training of a network to minimize robust loss directly. <xref ref-type="bibr" rid="b11">Hein &amp; Andriushchenko (2017)</xref> uses a local lipschitz regularization to improve certified robustness. In addition, a bound based on semi-definite programming (SDP) relaxation was developed and minimized as the objective (Raghunathan et al., 2018). <xref ref-type="bibr" rid="b0">Wong &amp; Kolter (2017)</xref> presents an upper bound on the robust loss caused by norm-bounded perturbation via LP relaxation, and minimizes this upper bound during training. <xref ref-type="bibr" rid="b0">Wong et al. (2018)</xref> further extend this method to much more general network structures with skip connections and general non-linearities, and provide a memory-friendly training strategy using random projections. Since LP relaxation is adopted, the aforementioned convex relaxation barrier exists for their methods.</p><p>While another line of work (IBP) have shown that an intuitively looser interval bound can be used to train much more robust networks than convex relaxation for large &#8734; perturbations (<xref ref-type="bibr" rid="b5">Gowal et al., 2018</xref>; <xref ref-type="bibr" rid="b21">Zhang et al., 2019b</xref>), it is still important to study convex relaxation bounds since it can provide better certificates against a broader class of adversaries that IBP struggles to certify in some cases, such as 2 adversaries for convolutional networks. We give a brief discussion for the reason in Appendix E.</p><p>We seek to enforce the tightness of the convex relaxation certificate during training. It reduces the optimality gap between the original and the relaxed problem by adding the proposed indicators into the loss function as regularizers. Compared with previous approaches, we have the following contributions: First, based upon the same relaxation in (<xref ref-type="bibr" rid="b0">Weng et al., 2018</xref>), we illustrate a more intuitive view for the bounds on intermediate ReLU activations achieved by (<xref ref-type="bibr" rid="b0">Wong et al., 2018</xref>) , which can be viewed as a linear network facing adversaries adding bounded perturbations to both the input and the intermediate layers. Second, starting from this view, we identify conditions where the bound from the relaxed problem is tight for the original non-convex problem. Third, based on the conditions, we propose regularizers that encourage the bound to be tight for the obtained network, which improves the certificate on both MNIST and CIFAR-10.</p></sec><sec><title>PROBLEM FORMULATION</title><p>In general, to train an adversarially robust network, we solve a constrained minimax problem where the adversary tries to maximize the loss given the norm constraint, and the parameters of the network are trained to minimize this maximal loss. Due to nonconvexity and the complexity of neural networks, it is expensive to solve the inner max problem exactly. To obtain certified robustness, like many related works (<xref ref-type="bibr" rid="b0">Wong et al., 2018</xref>; <xref ref-type="bibr" rid="b5">Gowal et al., 2018</xref>), we minimize an upper bound of the inner max problem, which is a cross entropy loss on the negation of the lower bounds of margins over each Under review as a conference paper at ICLR 2020 other class, as shown in Eq. 4. Without loss of generality, in this section we analyze the original and relaxed problems for minimizing the margin between the ground truth class y and some other class t under norm-bounded adversaries, which can be adapted directly to compute the loss in Eq. 4.</p><p>The original nonconvex constrained optimization problem for finding the norm-bounded adversary that minimizes the margin can be formulated as minimize z1&#8712;Bp, (x) c t x L , subject to z i+1 = &#963;(x i ), x i = f i (z i ), for i = 1, ..., L, (O) where c t = e y &#8722; e t , e y and e t are one-hot vectors corresponding to the label y and some other class t, &#963;(&#183;) is the ReLU activation, and f i is one functional block of the neural network. This can be a linear layer (f i (z i ) = W i z i + b i ), or even a residual block. We use h i (x) = f i (&#963;(f i&#8722;1 (&#183; &#183; &#183; f 1 (x)))) to denote the ReLU network up to the i-th layer, and p * O to denote the optimal solution to O.</p></sec><sec><title>EFFICIENT CONVEX RELAXATIONS</title></sec><sec><title>Grouping of ReLU Activations</title><p>The nonconvexity of O stems from the nonconvex feasible set given by the ReLU activations. Since the network is a continuous function, the pre-activations x i have lower and upper bounds x i andx i when the input z 1 &#8712; B p, (x). If a certain pre-activation x ij has x ij &lt; 0 &lt;x ij , its corresponding ReLU constraint z i+1,j = &#963;(x ij ) gives rise to a non-convex feasible set as shown in the left of <xref ref-type="fig" rid="fig_0">Figure 1</xref>, making Eq. O a non-convex optimization problem. On the other hand, ifx ij &#8804; 0 or x ij &#8805; 0, the constraints degenerate into linear constraints z i+1,j = 0 and z i+1,j = x ij respectively, which do not affect convexity. Based on x i andx i , we divide the ReLU activations into three disjoint subsets</p><p>If j &#8712; I i , we call the corresponding ReLU activation an unstable neuron.</p><p>Convex relaxation expands the non-convex feasible sets into convex ones and solves a convex optimization problem C. The feasible set of O is a subset of the feasible set of C, so the optimal value of C lower bounds the optimal value of Eq. O. Moreover, we want problem C to be solved efficiently, better with a closed form solution, so that it can be integrated into the training process.</p></sec><sec><title>Computational Challenge for the "optimal" Relaxation</title><p>As pointed out by (<xref ref-type="bibr" rid="b21">Salman et al., 2019</xref>), the optimal layer-wise convex relaxation, i.e., the optimal convex relaxation for the nonlinear constraint z i+1 = &#963;(x i ) of a single layer, can be obtained independently for each neuron. For each j &#8712; I i in a ReLU network, the optimal layer-wise convex relaxation is the closed convex hull conv ij of</p><p>, corresponding to the triangle region in the middle of <xref ref-type="fig" rid="fig_0">Figure 1</xref>. Despite being relatively tight, there is no closed-form solution to this relaxed problem. LP solvers are typically adopted to solve a linear programming problem for each neuron. Therefore, such a relaxation is hardly scalable to verify larger networks without any additional trick (like <xref ref-type="bibr" rid="b28">Xiao et al. (2018)</xref>). <xref ref-type="bibr" rid="b0">Weng et al. (2018)</xref> find it to be 34 to 1523 times slower than Fast-Lin, and it has difficulty verifying MLPs with more than 3 layers on MNIST. In (<xref ref-type="bibr" rid="b21">Salman et al., 2019</xref>), it takes 10,000 CPU cores to parallelize the LP solvers for bounding the activations of every neuron in a two-hidden-layer MLP with 100 neurons per layer. Since solving LP problems for all neurons are usually impractical, it is even more difficult to optimize the network to maximize the Under review as a conference paper at ICLR 2020 lower bounds of margin found by solving this relaxation problem, as differentiating through the LP optimization process is even more expensive.</p></sec><sec><title>Computationally Efficient Relaxations</title><p>In the layer-wise convex relaxation, instead of using a boundary nonlinear in x ij , (<xref ref-type="bibr" rid="b21">Zhang et al., 2018</xref>) has shown that for any nonlinearity, when both the lower and upper boundaries are linear in x ij , there exist closed-form solutions to the relaxed problem, which avoids using LP solvers and improves efficiency. Specifically, the following relaxation of O has closed-form solutions:</p><p>where &#183; denotes element-wise product, and for simplicity, we have only considered networks with no skip connections, and represent both Full Connected and Convolutional Layers as a linear transform</p><p>Before we can solve C to get the lower bound of margin, we need to know thee range [x i ,x i ] for the pre-activations x i . As in (<xref ref-type="bibr" rid="b0">Wong &amp; Kolter, 2017</xref>; <xref ref-type="bibr" rid="b0">Weng et al., 2018</xref>; <xref ref-type="bibr" rid="b21">Zhang et al., 2018</xref>), we can solve the same optimization problem for each neuron x ij starting from layer 1 to L, by replacing c t with e j or &#8722;e j for x ij orx ij respectively. 1</p><p>The most efficient approach in this category is Fast-Lin (<xref ref-type="bibr" rid="b0">Weng et al., 2018</xref>), which sets a ij =&#257; ij , as shown in the right of <xref ref-type="fig" rid="fig_0">Figure 1</xref>. A tighter choice is CROWN (<xref ref-type="bibr" rid="b21">Zhang et al., 2018</xref>), which chooses different a ij and&#257; ij such that the convex feasible set is minimized. However, CROWN has much higher complexity than Fast-Lin due to its varying slopes. We give detailed analysis of the closed- form solutions of both bounds and their complexities in Appendix F. Recently, CROWN-IBP (<xref ref-type="bibr" rid="b21">Zhang et al., 2019b</xref>) has been proposed to provide a better initialization to IBP, which uses IBP to estimate range [x i ,x i ] for CROWN. In this case, both CROWN and Fast-Lin have the same complexity and CROWN is a better choice.</p></sec><sec><title>TIGHTER BOUNDS VIA REGULARIZATION</title><p>Despite being relatively efficient to compute, Fast-Lin and CROWN are not even the tightest layer- wise convex relaxation. Using tighter bounds to train the networks could potentially lead to higher certified robustness by preventing such bounds from over-regularizing the networks. Nevertheless, there exist certain parameters and inputs such that the seemingly looser Fast-Lin is tight for O, i.e., the optimal value of Fast-Lin is the same as O. The most trivial case is where no unstable neuron exists. In practice, as shown in the illustrative example in Appendix A, Fast-Lin can be tight even when unstable neurons exist. It is therefore interesting to check the conditions for Fast-Lin or CROWN to be tight for O, and enforcing such conditions during training to improve the certified robustness.</p></sec><sec><title>CONDITIONS FOR TIGHTNESS</title><p>Here we look into conditions that make the optimal value p * C of the convex problem C to be equal to p * O . Let {z i , x i } L i=1 be some feasible solution of C, from which the objective value of C can be determined as p C = c t x L . Let {z i , x i } L i=1 be some feasible solution of O computed by passing z 1 through the ReLU sub-networks h i (z 1 ) defined in O, and denote the resulting feasible objective value as p O = c t x L .</p><p>Generally, for a given network with the set of weights {W i , b i } L i=1 , as long as the optimal solution</p><p>Therefore, for a given network and input x, to check the tightness of the convex relaxation, we can check whether its optimal solution {z * i , x * i } L i=1 is feasible for O. This can be achieved by passing z * 1 through the ReLU network, and either directly check the resultant objective value p O , or compare {z * i , x * i } L i=1 with the resultant feasible solution {z i , x i } L i=1 . Further, we can encourage such Under review as a conference paper at ICLR 2020 conditions to happen during the training process to improve the tightness of the bound. Based on such mechanisms, we propose two regularizers to enforce the tightness. Notice such regularizers are different from the RS Loss (<xref ref-type="bibr" rid="b28">Xiao et al., 2018</xref>) introduced to reduce the number of unstable neurons, since we have shown with Appendix A that C can be tight even when unstable neurons exist.</p></sec><sec><title>A INTUITIVE INDICATOR OF TIGHTNESS</title><p>The observation above motivates us to consider the non-negative value d(x, &#948; * 0 , W, b) = p O (x, &#948; * 0 ) &#8722; p * C (2) as an indicator of the difference between {z * i , x * i } L i=1 and {z i , x i } L i=1 , where p O (x, &#948; * 0 ) = c t h L (x + &#948; * 0 ) is the margin over class t computed by passing the optimal perturbation &#948; * 0 for C through the original network. &#948; * 0 can be computed efficiently from the optimality condition of Fast-Lin or CROWN, as demonstrated in Eq. 16. For example, when p = &#8734;, the optimal input perturbation</p><p>The larger d(x, &#948; * 0 , W, b) is, the more relaxed C is, and the higher p * O &#8722;p * C could be. Therefore, we can regularize the network to minimize d(x, &#948; * 0 , W, b) during training and maximize the lower-bound of the margin p * C , so that we can obtain a network where p * C is a better estimate of p * O and the robustness is better represented by p * C . Such an indicator avoids comparing the intermediate variables, which gives more flexibility for adjustment. It bears some similarities to knowledge distillation (Hinton et al., 2015), in that it encourages learning a network whose relaxed lower bound gives similar outputs of the corresponding ReLU network. It is worth noting that minimizing d(x, &#948; * 0 , W, b) does not necessarily lead to decreasing p O (x, &#948; * 0 ) or increasing p * C . In fact, both p O (x, &#948; * 0 ) and p * C can be increased or decreased at the same time with their difference decreasing.</p><p>The tightest indicator should give the minimum gap p * O &#8722; p * C , where we need to find the optimal perturbation for O. However, the minimum gap cannot be found in polynomial time, due to the non-convex nature of O. (<xref ref-type="bibr" rid="b0">Weng et al., 2018</xref>) also proved that there is no polynomial time algorithm to find the minimum 1 -norm adversarial distortion with 0.99 ln n approximation ratio unless NP=P, a problem equivalent to finding the minimum margin here.</p></sec><sec><title>A BETTER INDICATOR FOR REGULARIZATION: DIFFERENCE IN SOLUTIONS</title><p>Despite being intuitive and is able to achieve improvements, Eq. 2 which enforces similarity between objective values does not work as good as enforcing similarity between the solutions {z * i , x * i } L i=1 and {z i , x i } L i=1 in practice, an approach we will elaborate below. For both CROWN and Fast-Lin, unless d(x, &#948; * 0 , W, b) = 0, {z * i , x * i } L i=1 may deviate a lot from {z i , x i } L i=1 and does not correspond to any ReLU network, even if d(x, &#948; * 0 , W, b) may seem small. For example, it is possible that z * ij &lt; 0 for a given z * 1 , but a ReLU network will always have z ij &#8805; 0.</p><p>We find an alternative regularizer more effective at improving verifiable accuracy. The regularizer encourages the feasible solution {z i , x i } L i=1 of O to exactly match the feasible optimal solution {z * i , x * i } L i=1 of C. Since we are adopting the layer-wise convex relaxation, the optimal solutions of the unstable neurons can be considered independently.</p><p>Here we derive a sufficient condition for tightness for Fast-Lin, which also serves as a sufficient condition for CROWN. For linear programming, the optimal solution occurs on the boundaries of the feasible set. Since Fast-Lin is a layer-wise convex relaxation, the solution to each of its neurons in z i can be considered independently, and therefore for a specific layer i and j &#8712; I i , the pair of optimal solutions (x * ij , z * i+1,j ) should occur on the boundary in the right of <xref ref-type="fig" rid="fig_0">Figure 1</xref>. It follows that the only 3 optimal solutions (x * ij , z * i+1,j ) of C that are also feasible for O are (x ij , 0), (x ij , x ij ) and (0, 0). Notice they are also in the intersection between the boundary of CROWN and O.</p><p>In practice, out of efficiency concerns, both Fast-Lin and CROWN identify the boundaries that the optimal solution lies on and computes the optimal value by accumulating the contribution of each layer in a backward pass, without explicitly computing {z * i , x * i } L i=1 for each layer with a forward Under review as a conference paper at ICLR 2020 pass (see Appendix F for more details). It is therefore beneficial to link the feasible solutions of O to the parameters of the boundaries. Specifically, let &#948; * ij &#8712; {b ij ,b ij } be the intercept of the line that the optimal solution (x * ij , z * i+1,j ) lies on. We want to find a rule based on {&#948; * i } L i=1 to determine whether the bound is tight from the values of {x i } L i=1 . For both Fast-Lin and CROWN, b ij = 0,b ij = &#8722; xij x ij xij &#8722;x ij . For Fast-Lin, when &#948; * ij =b ij , only (x * ij , z * i+1,j ) = (x ij , 0) or (x ij , x ij ) are fesible for O; when &#948; * ij = b ij , only (x * ij , z * i+1,j ) = (0, 0) is feasible for O. Meanwhile, z i+1,j = max(x ij , 0) is deterministic if x ij is given. Therefore, when the bound is tight for Fast-Lin, if &#948; * ij = b ij , then x ij = 0. Otherwise, if &#948; * ij =b ij , and x ij = x ij or x ij . For CROWN, this condition is also feasible, though it could be either x ij &#8804; 0 or x ij &#8805; 0 when &#948; * ij = 0, depending on the optimal slope D (L) ij . Indeed, we achieve optimal tightness (p * C = p * O ) for both Fast-Lin and CROWN if x ij satisfy these conditions at all unstable neurons. Specifically, Proposition 1. Assume {z i , x i } L i=1 is obtained by the ReLU network h L with input z 1 , and {&#948; * i } L&#8722;1 i=0 is the optimal solution of Fast-Lin or CROWN.</p><p>We provide the proof of this simple proposition in the Appendix.</p><p>It remains to be discussed how to best enforce the similarity between the optimal solutions of O and Fast-Lin or CROWN. Like before, we choose to enforce the similarity between {x i } L i=1 and the closest optimal solution of Fast-Lin, where {x i } L i=1 is constructed by setting x 1 = x * 1 = W 1 (x + &#948; * 0 ) + b 1 and pass x 1 through the ReLU network to obtain x i = h i (x + &#948; * 0 ). By Proposition 1, the distance can be computed by considering the values of the intercepts {&#948; * i } L&#8722;1 i=1 as</p></sec><sec><title>CERTIFIED ROBUST TRAINING IN PRACTICE</title><p>In practice, for classification problems with more than two classes, we will compute the lower bound of the margins w.r.t. multiple classes. Denote p * C and p * O as the concatenated vector of lower bounds of the relaxed problem and original problem for multiple classes, and d t , r t as the regularizers for the margins w.r.t. class t. Together with the regularizers, we optimize the following objective minimize W,b L CE (&#8722;p * C , y) + &#955; t d t (x, &#948; * 0 , W, b) + &#947; t r t (x, &#948; * 0 , W, b), (4) where L CE (&#8722;p * C , y) is the cross entropy loss with label y, as adopted by many related works (<xref ref-type="bibr" rid="b0">Wong et al., 2018</xref>; <xref ref-type="bibr" rid="b5">Gowal et al., 2018</xref>), and we have implicitly abbreviated the inner maximization problem w.r.t. {&#948; i } L&#8722;1 i=0 into the optimal values p * C and solution &#948; * 0 . More details for computing the intermediate and output bounds can be found in Algorithm 1, where we have used &#183; 1,row to denote row-wise 1 norm, and (&#183;) :,j for taking the j-th column.</p><p>One major challenge of the convex relaxation approach is the high memory consumption. To compute the bounds x i , x i , we need to pass an identity matrix with the same number of diagonal entries as the total dimensions of the input images, which can make the batch size thousands of times larger than usual. To mitigate this, one can adopt the random projection from <xref ref-type="bibr" rid="b0">Wong et al. (2018)</xref>, which projects identity matrices into lower dimensions as W i:1 R to estimate the norm of W i:1 . Such projections add noise/variance to x i , x i , and the regularizers are affected as well.</p></sec><sec><title>EXPERIMENTS</title></sec><sec><title>Models, Datasets and Hyper-parameters</title><p>We evaluate the proposed regularizer on two datasets (MNIST and CIFAR10) with two different each. We consider only &#8734; adversaries. We experiment with a variety of different network structures, including a MLP (2x100) with two 100-neuron hidden layers as (<xref ref-type="bibr" rid="b21">Salman et al., 2019</xref>), two Conv Nets (Small and Large) that are the same as (<xref ref-type="bibr" rid="b0">Wong et al., 2018</xref>), a family of 10 small conv nets and a family of 8 larger conv nets, all the same as (<xref ref-type="bibr" rid="b21">Zhang et al., 2019b</xref>), and also the same 5-layer convolutional network (XLarge) as in the latest version of CROWN-IBP (<xref ref-type="bibr" rid="b21">Zhang et al., 2019b</xref>). For CROWN-IBP, we use the updated expensive training schedule as (<xref ref-type="bibr" rid="b21">Zhang et al., 2019b</xref>), which uses 200 epochs with batch size 256 for MNIST and 3200 epochs with batch size 1024 for CIFAR10. We use up to 4 GTX 1080Ti or 2080Ti for all our experiments.</p><p>Improved Training with Convex Relaxation <xref ref-type="table" rid="tab_0">Table 1</xref> shows comparisons with various approaches. All of our baseline implementations have improved compared with (<xref ref-type="bibr" rid="b0">Wong et al., 2018</xref>). When adding the proposed regularizers, the certified robust accuracy is further improved in all cases for both CP (<xref ref-type="bibr" rid="b0">Wong et al., 2018</xref>) and CROWN-IBP (<xref ref-type="bibr" rid="b21">Zhang et al., 2019b</xref>). We also provide results against Under review as a conference paper at ICLR 2020 a 100-step PGD adversary for our CP models. We will add PGD error rates for our CROWN-IBP models in our future version. Since both PGD errors and standard errors are reduced in most cases, the regularizer should have improved not only the certified upper bound, but also improved the actual robust error.</p><p>The relative improvement on 2x100 with our regularizer (10.3%/8.7%) are comparable to the im- provements (5.9%/10.0%) from (<xref ref-type="bibr" rid="b21">Salman et al., 2019</xref>), despite the fact that we start from a stronger baseline. This indicates that the improvement brought by using our regularizer is comparable with using the expensive and unstable optimal layer-wise convex relaxation for relaxation.</p><p>Our results with Small are better than the best results of (<xref ref-type="bibr" rid="b5">Dvijotham et al., 2018a</xref>; <xref ref-type="bibr" rid="b28">Xiao et al., 2018</xref>) on MNIST with = 0.1, though not as good as the best of (<xref ref-type="bibr" rid="b9">Mirman et al., 2018</xref>), which uses a larger model. When applying the same model on CIFAR10, we achieve better robust error than (<xref ref-type="bibr" rid="b9">Mirman et al., 2018</xref>).</p><p>The relative improvements in certified robust error for = 0.1 and 0.3 are 18%/3.4% for the small exact model on MNIST, compared with 0.03%/3.13% for the random projection counterparts. In the exact models, we have better estimates of x i , x i . These consistent improvements validate that our proposed regularizers improve the performance.</p><p>In comparison with IBP-based methods, our regularizers is able to further improve CP on CIFAR10 with = 2/255, and demonstrate the best result among all approaches compared in this setting. To our knowledge, this is the best result of models of the same size. By using our regularizers on CROWN- IBP to provide a better initialization for the later training stage of IBP, our method also achieves the best certified accuracy on CIFAR10 under = 8/255. To provide more comprehensive evaluations, Table 4 shows the mean and variance of the results with smaller models, demonstrating consistent Under review as a conference paper at ICLR 2020 improvements of our model, while <xref ref-type="table" rid="tab_1">Table 2</xref> gives the best, median and worst case results with the large models on the MNIST dataset. Note all the networks are significantly smaller than (<xref ref-type="bibr" rid="b5">Gowal et al., 2018</xref>), and our batch size and number of epochs (at most 256 and 140) are also much smaller than (<xref ref-type="bibr" rid="b5">Gowal et al., 2018</xref>) (1600 and 3200). Still, we are able to achieve better results.</p></sec><sec><title>CONCLUSIONS</title><p>We propose two regularizers that lead to tighter LP relaxation bounds for certifiable robustness. Extensive experiments validate that the regularizers improve robust accuracy over non-regularized baselines. This work is a step towards closing the gap between certified and empirical robustness. Future directions include methods to improve computational efficiency for LP relaxations (and certified methods in general), and better ways to leverage random projections for acceleration.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>The feasible sets (blue regions/lines) given by the bounded ReLU constraints (Eq. O), convex hull (conv ij ) and the relaxation (Fast-Lin) discussed in this paper (specific choice for Eq. C) for j &#8712; I i . The red lines and dots are the intersections between the boundaries of the convex feasible sets and the ReLU constraints.</p></caption><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Results on MNIST, and CIFAR10 with small networks, large networks, and different coefficients of d(x, &#948; * 0 , W, b), r(x, &#948; * 0 , W, b). All entries with positive &#955; or &#947; are using our regularizers. For all models not marked as "Exact", we have projected the input dimension of Wi:1 to 50, the same as Wong et al. (2018). For val- ues with * , larger is used for training. = 0.3, 2/255, 8/255 correspond to using = 0.4, 2.2/255, 8.8/255 for training respectively. For the methods: 1 : (Dvijotham et al., 2018a); 2 : (Xiao et al., 2018); 3 : (Mirman et al., 2018); 4 (Gowal et al., 2018).</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Our results on the MNIST dataset, with CROWN-IBP. CI Orig are results copied from the paper, CI ReImp are results of our implementation of CROWN-IBP, and CI Reg is with regularizer r.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap></sec></body><back><sec><p>1 Forxij, take an extra negation on the solution.</p></sec><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Adversarial examples are not easily detected: Bypassing ten detection methods</article-title><source>Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</source><year>2017</year><fpage>3</fpage><lpage>14</lpage><person-group person-group-type="author"><name><surname>References</surname><given-names>Nicholas</given-names></name><name><surname>Carlini</surname><given-names>David</given-names></name><name><surname>Wagner</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Provably minimally-distorted adversarial examples</article-title><source>arXiv preprint arXiv:1709.10207</source><year>2017</year><person-group person-group-type="author"><name><surname>Carlini</surname><given-names>Nicholas</given-names></name><name><surname>Katz</surname><given-names>Guy</given-names></name><name><surname>Barrett</surname><given-names>Clark</given-names></name><name><surname>Dill</surname><given-names>David L</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><source>International Symposium on Automated Technology for Verification and Analysis</source><year>2017</year><fpage>251</fpage><lpage>268</lpage><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Chih-Hong</given-names></name><name><surname>N&#252;hrenberg</surname><given-names>Georg</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Certified adversarial robustness via randomized smoothing</article-title><source>arXiv preprint arXiv:1902.02918</source><year>2019</year><person-group person-group-type="author"><name><surname>Jeremy M Cohen</surname><given-names>Elan</given-names></name><name><surname>Rosenfeld</surname><given-names>J Zico</given-names></name><name><surname>Kolter</surname><given-names /></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><source>NASA Formal Methods Symposium</source><year>2018</year><fpage>121</fpage><lpage>138</lpage><person-group person-group-type="author"><name><surname>Dutta</surname><given-names>Souradeep</given-names></name><name><surname>Jha</surname><given-names>Susmit</given-names></name><name><surname>Sankaranarayanan</surname><given-names>Sriram</given-names></name><name><surname>Tiwari</surname><given-names>Ashish</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Training verified learners with learned verifiers</article-title><source>arXiv preprint arXiv:1805.10265</source><year>2018</year><person-group person-group-type="author"><name><surname>Dvijotham</surname><given-names>Krishnamurthy</given-names></name><name><surname>Gowal</surname><given-names>Sven</given-names></name><name><surname>Stanforth</surname><given-names>Robert</given-names></name><name><surname>Arandjelovic</surname><given-names>Relja</given-names></name><name><surname>Brendan</surname><given-names>O '</given-names></name><name><surname>Donoghue</surname><given-names>Jonathan</given-names></name><name><surname>Uesato</surname><given-names>Pushmeet</given-names></name><name><surname>Kohli</surname><given-names /></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>A dual approach to scalable verification of deep networks</article-title><source>arXiv preprint arXiv:1803.06567</source><year>2018</year><fpage>104</fpage><lpage>104</lpage><person-group person-group-type="author"><name><surname>Dvijotham</surname><given-names>Krishnamurthy</given-names></name><name><surname>Stanforth</surname><given-names>Robert</given-names></name><name><surname>Gowal</surname><given-names>Sven</given-names></name><name><surname>Mann</surname><given-names>Timothy</given-names></name><name><surname>Kohli</surname><given-names>Pushmeet</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><source>Interna- tional Symposium on Automated Technology for Verification and Analysis</source><year>2017</year><fpage>269</fpage><lpage>286</lpage><person-group person-group-type="author"><name><surname>Ehlers</surname><given-names>Ruediger</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Deep neural networks as 0-1 mixed integer linear programs: A feasibility study</article-title><source>arXiv preprint arXiv:1712.06174</source><year>2017</year><person-group person-group-type="author"><name><surname>Fischetti</surname><given-names>Matteo</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Explaining and harnessing adversarial examples</article-title><year>2018</year><fpage>3</fpage><lpage>18</lpage><person-group person-group-type="author"><name><surname>Gehr</surname><given-names>Timon</given-names></name><name><surname>Mirman</surname><given-names>Matthew</given-names></name><name><surname>Drachsler-Cohen</surname><given-names>Dana</given-names></name><name><surname>Tsankov</surname><given-names>Petar</given-names></name><name><surname>Chaudhuri</surname><given-names>Swarat</given-names></name><name><surname>Vechev</surname><given-names>Martin</given-names></name><name><surname>Ian</surname><given-names>J</given-names></name><name><surname>Goodfellow</surname><given-names>Jonathon</given-names></name><name><surname>Shlens</surname><given-names>Christian</given-names></name><name><surname>Szegedy</surname><given-names /></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>On the effectiveness of interval bound propagation for training verifiably robust models</article-title><source>arXiv preprint arXiv:1810.12715</source><year>2018</year><person-group person-group-type="author"><name><surname>Gowal</surname><given-names>Sven</given-names></name><name><surname>Dvijotham</surname><given-names>Krishnamurthy</given-names></name><name><surname>Stanforth</surname><given-names>Robert</given-names></name><name><surname>Bunel</surname><given-names>Rudy</given-names></name><name><surname>Qin</surname><given-names>Chongli</given-names></name><name><surname>Uesato</surname><given-names>Jonathan</given-names></name><name><surname>Mann</surname><given-names>Timothy</given-names></name><name><surname>Kohli</surname><given-names>Pushmeet</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Formal guarantees on the robustness of a classifier against adversarial manipulation</article-title><source>Advances in Neural Information Processing Systems</source><year>2017</year><fpage>2266</fpage><lpage>2276</lpage><person-group person-group-type="author"><name><surname>Hein</surname><given-names>Matthias</given-names></name><name><surname>Andriushchenko</surname><given-names>Maksym</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Distilling the knowledge in a neural network</article-title><source>arXiv preprint arXiv:1503.02531</source><year>2015</year><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>Geoffrey</given-names></name><name><surname>Vinyals</surname><given-names>Oriol</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Puvae: A variational autoencoder to purify adversarial examples</article-title><source>arXiv preprint arXiv:1903.00585</source><year>2019</year><person-group person-group-type="author"><name><surname>Hwang</surname><given-names>Uiwon</given-names></name><name><surname>Park</surname><given-names>Jaewoo</given-names></name><name><surname>Jang</surname><given-names>Hyemi</given-names></name><name><surname>Yoon</surname><given-names>Sungroh</given-names></name><name><surname>Cho</surname><given-names>Nam Ik</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><source>International Conference on Computer Aided Verification</source><year>2017</year><fpage>97</fpage><lpage>117</lpage><person-group person-group-type="author"><name><surname>Katz</surname><given-names>Guy</given-names></name><name><surname>Barrett</surname><given-names>Clark</given-names></name><name><surname>David</surname><given-names>L</given-names></name><name><surname>Dill</surname><given-names>Kyle</given-names></name><name><surname>Julian</surname><given-names>Mykel</given-names></name><name><surname>Kochenderfer</surname><given-names /></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv preprint arXiv:1412.6980</source><year>2014</year><person-group person-group-type="author"><name><surname>Diederik</surname><given-names>P</given-names></name><name><surname>Kingma</surname><given-names>Jimmy</given-names></name><name><surname>Ba</surname><given-names /></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>An approach to reachability analysis for feed-forward relu neural networks</article-title><source>arXiv preprint arXiv:1706.07351</source><year>2017</year><person-group person-group-type="author"><name><surname>Lomuscio</surname><given-names>Alessio</given-names></name><name><surname>Maganti</surname><given-names>Lalit</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Towards deep learning models resistant to adversarial attacks</article-title><source>arXiv preprint arXiv:1706.06083</source><year>2017</year><person-group person-group-type="author"><name><surname>Madry</surname><given-names>Aleksander</given-names></name><name><surname>Makelov</surname><given-names>Aleksandar</given-names></name><name><surname>Schmidt</surname><given-names>Ludwig</given-names></name><name><surname>Tsipras</surname><given-names>Dimitris</given-names></name><name><surname>Vladu</surname><given-names>Adrian</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Differentiable abstract interpretation for provably robust neural networks</article-title><source>International Conference on Machine Learning</source><year>2018</year><fpage>3575</fpage><lpage>3583</lpage><person-group person-group-type="author"><name><surname>Mirman</surname><given-names>Matthew</given-names></name><name><surname>Gehr</surname><given-names>Timon</given-names></name><name><surname>Vechev</surname><given-names>Martin</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning</article-title><source>arXiv preprint arXiv:1803.04765</source><year>2018</year><person-group person-group-type="author"><name><surname>Papernot</surname><given-names>Nicolas</given-names></name><name><surname>Mcdaniel</surname><given-names>Patrick</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Certified defenses against adversarial examples</article-title><source>arXiv preprint arXiv:1801.09344</source><year>2018</year><person-group person-group-type="author"><name><surname>Raghunathan</surname><given-names>Aditi</given-names></name><name><surname>Steinhardt</surname><given-names>Jacob</given-names></name><name><surname>Liang</surname><given-names>Percy</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>A convex relaxation barrier to tight robust verification of neural networks</article-title><source>NeurIPS</source><year>2019</year><person-group person-group-type="author"><name><surname>Hadi Salman</surname><given-names>Greg</given-names></name><name><surname>Yang</surname><given-names>Huan</given-names></name><name><surname>Zhang</surname><given-names>Cho-Jui</given-names></name><name><surname>Hsieh</surname><given-names>Pengchuan</given-names></name><name><surname>Zhang</surname><given-names /></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Defense-gan: Protecting classifiers against adversarial attacks using generative models</article-title><source>arXiv preprint arXiv:1805.06605</source><year>2018</year><person-group person-group-type="author"><name><surname>Samangouei</surname><given-names>Pouya</given-names></name><name><surname>Kabkab</surname><given-names>Maya</given-names></name><name><surname>Chellappa</surname><given-names>Rama</given-names></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>Towards verification of artificial neural networks</article-title><source>MBMV</source><year>2015</year><fpage>30</fpage><lpage>40</lpage><person-group person-group-type="author"><name><surname>Scheibler</surname><given-names>Karsten</given-names></name><name><surname>Winterer</surname><given-names>Leonore</given-names></name><name><surname>Wimmer</surname><given-names>Ralf</given-names></name><name><surname>Becker</surname><given-names>Bernd</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>Adversarial training for free! arXiv preprint arXiv:1904</article-title><year>2019</year><person-group person-group-type="author"><name><surname>Shafahi</surname><given-names>Ali</given-names></name><name><surname>Najibi</surname><given-names>Mahyar</given-names></name><name><surname>Ghiasi</surname><given-names>Amin</given-names></name><name><surname>Xu</surname><given-names>Zheng</given-names></name><name><surname>Dickerson</surname><given-names>John</given-names></name><name><surname>Studer</surname><given-names>Christoph</given-names></name><name><surname>Larry</surname><given-names>S</given-names></name><name><surname>Davis</surname><given-names>Gavin</given-names></name><name><surname>Taylor</surname><given-names>Tom</given-names></name><name><surname>Goldstein</surname><given-names /></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><source>Gotta catch 'em all: Using concealed trapdoors to detect adversarial attacks on neural networks</source><year>2019</year><person-group person-group-type="author"><name><surname>Shan</surname><given-names>Shawn</given-names></name><name><surname>Willson</surname><given-names>Emily</given-names></name><name><surname>Wang</surname><given-names>Bolun</given-names></name><name><surname>Li</surname><given-names>Bo</given-names></name><name><surname>Zheng</surname><given-names>Haitao</given-names></name><name><surname>Zhao</surname><given-names>Ben Y</given-names></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>Fast and effective robustness certification</article-title><source>Advances in Neural Information Processing Systems</source><year>2018</year><fpage>10802</fpage><lpage>10813</lpage><person-group person-group-type="author"><name><surname>Singh</surname><given-names>Gagandeep</given-names></name><name><surname>Gehr</surname><given-names>Timon</given-names></name><name><surname>Mirman</surname><given-names>Matthew</given-names></name><name><surname>P&#252;schel</surname><given-names>Markus</given-names></name><name><surname>Vechev</surname><given-names>Martin</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><article-title>Intriguing properties of neural networks</article-title><source>arXiv preprint arXiv:1312.6199</source><year>2013</year><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>Christian</given-names></name><name><surname>Zaremba</surname><given-names>Wojciech</given-names></name><name><surname>Sutskever</surname><given-names>Ilya</given-names></name><name><surname>Bruna</surname><given-names>Joan</given-names></name><name><surname>Erhan</surname><given-names>Dumitru</given-names></name><name><surname>Goodfellow</surname><given-names>Ian</given-names></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>Evaluating robustness of neural networks with mixed integer programming</article-title><source>arXiv preprint arXiv:1711.07356</source><year>2017</year><person-group person-group-type="author"><name><surname>Tjeng</surname><given-names>Vincent</given-names></name><name><surname>Xiao</surname><given-names>Kai</given-names></name><name><surname>Tedrake</surname><given-names>Russ</given-names></name></person-group></element-citation></ref></ref-list></back></article>