Title:
```
Under review as a conference paper at ICLR 2020 A RANDOM MATRIX PERSPECTIVE ON MIXTURES OF NONLINEARITIES IN HIGH DIMENSIONS
```
Abstract:
```
One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features F = f (W X + B) for a random weight matrix W and random bias vector B, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis directly generalizes to such distributions, even those not expressible with a traditional additive bias. Intriguingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.
```

Figures/Tables Captions:
```
Figure 1: We get excellent agreement of theory and simulation for spectral densities for any bias distribution. We set φ = 1.5, ψ = 0.8, σ X = σ W = 1, and f = ReLU. Simulations are performed on matrices of size m = 2 14 . Left Gaussian distribution over the biases with distribution N (0, 1). Right Bernoulli distribution over biases with distribution p = 0.5.
Figure 2: Empirical spectral densities agree with our predictions for varied data distributions and shape parameters. Top left: One class from CIFAR (airplane), mean subtracted. Top right: Classes {0, 8} from MNIST, mean subtracted. Bottom left: Gaussian input data, varying the NN parameter settings and activation function. Bottom right: Input data with a bimodal spectrum. All plots used f = ReLU, φ = 1.5, ψ = 0.8 and σ W = σ B = 1, except for the indicated modified parameter. Empirical densities were smoothed using a Gaussian KDE.
Figure 3: Comparisons of simulated ridge regression error and our theoretical prediction. We use ReLU with σ X = σ W = σ B = 1 for all plots and vary the shape parameters φ and ψ. For simulations we use m = 2 13 throughout. Note we also normalize the activation function so that E b [η(b)] = 1. (a): Our predictions for ridge regression with random labels are solid lines. Simulated losses are the red stars. (b) Autoencoder error. (c) Noisy autoencoder error with σ ε = 0.2.
Figure 4: Performance on ridge-regularized noisy autoencoder with σ = 1, φ = 1/2, and ψ = 1/2. (a) Theoretical predictions for training error (solid lines) and 1σ error bars for empirical simulations of finite networks (n 0 = 192, n 1 = 384, m = 384) for various values of ridge regularization constant γ as the activation function varies. In the left panel, a single activation function f is used. In the right panel, the non-linearity is f p , a Bernoulli(p)-mixture of a purely linear (ζ = 1) and purely non-linear (ζ = 0) function. Each simulation uses a randomly-chosen non-linearity having the specified values of ζ, demonstrating that E train depends on the non-linearity solely through this constant. Red and blue stars denote minima. (b) Training error as a function of γ for the optimal f and f p , as determined in (a). The bottom panel shows the difference in training error, demonstrating that the optimal Bernoulli(p)-mixture of non-linearities has smaller training error than the best single non-linearity.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION It is undeniable that in recent years deep learning systems have found widespread success in their applications to a diverse and ever-expanding set of domains. The foundational results on many tasks such as image recognition ( Krizhevsky et al., 2012 ), speech recognition ( Hinton et al., 2012 ), and machine translation ( Wu et al., 2016 ), have begun to make their way into higher-level products that people interact with and rely upon in their daily lives. Whether these products generate a medical diagnosis, a navigation decision, or some other important output, it is crucial to understand the inner-workings of the deep learning algorithms that generate them. Unfortunately, our theoretical understanding of these deep learning algorithms continues to lag behind their impressive practical successes. One main challenge in building a fuller understanding stems from the fact that deep neural networks are complex nonlinear functions that employ millions or even billions ( Shazeer et al., 2017 ) of parameters. Traditional wisdom would suggest that to this parameter complexity corresponds to an optimization difficulty. Recent work, however, suggests that as the width of a network's hidden layers becomes large, the loss function simplifies and a theoretical analysis becomes tractable ( Jacot et al., 2018 ;  Mei et al., 2018 ;  Chizat & Bach, 2018a ;  Mei et al., 2019 ;  Rotskoff et al., 2019 ;  Rotskoff & Vanden-Eijnden, 2018 ). In some scenarios, the simplification is such that throughout training the parameters of the model stay within an infinitesimal radius of their initial (random) values, implying that much about neural network training can be understood by studying the random initialization ( Jacot et al., 2018 ;  Chizat & Bach, 2018b ;  Lee et al., 2019 ). Another main challenge in building a rich understanding of deep learning systems stems from the fact that they are often trained on very large, complex datasets: even if the models themselves are very large, they may not be large in comparison to the number of constraints they are designed to satisfy. Indeed, many important phenomena may become apparent only by examining the high-dimensional regime in which the dataset size and width are both large and of the same order. In this work, we focus on the high-dimensional regime and analyze the performance of a regression model trained on the random features F = f (W X + B) for a random weight matrix W and random Under review as a conference paper at ICLR 2020 bias vector B. We obtain an exact formula for the training error on a noisy autoencoding task in the limit that the width and dataset size both go to infinity. The result is determined by the resolvent of the kernel matrix F t F , whose properties we analyze via the resolvent method from random matrix theory. Our analysis also provides an exact formula for the eigenvalue density of the kernel matrix, which may be of independent interest since it provides a characterization for how spectral properties of the data covariance matrix propagate through neural network layers at initialization.

Section Title: OUR CONTRIBUTIONS
  OUR CONTRIBUTIONS The main contribution of our work is an exact characterization of the training error of a ridge- regularized random feature regression model on a noisy autoencoder task in the high-dimensional regime. This is one of the first non-trivial models to be solved exactly in the joint limit of large data and large width and provides an interesting testing ground in which to analyze this regime. Some of our additional contributions include, • An exact characterization of the spectral density of the randm feature matrix F = f (W X + B), extending prior results of  Pennington & Worah (2017)  to non-Gaussian data distributions and to non-zero bias distributions. • One interpretation of the random additive bias is that it induces a distribution of activation functions parameterized by B, i.e. f (Z; B) := f (Z + B). Our analysis trivially extends to any distribution of activation functions f (· ; B) parameterized by B. • We show that there exists a non-trivial distribution over activation functions that outperforms the best possible single activation function on a noisy autoencoding task. • Our method of proof introduces a surrogate "linearization" of F , F lin , that possesses the same spectral information as F . F lin and its properties are likely to be of further interest and utility in analyzing neural networks in high dimensions.

Section Title: RELATED WORK
  RELATED WORK Neural networks have been studied from the perspective of high-dimensional statistics in a number of recent works. Most prior work has focused on the bias-free case.  Pennington & Worah (2017)  studied the spectrum of the activation matrix f (W X) for iid Gaussian data and derived an analytic expression for the training error of a ridge-regularized random feature model trained on pure noise. It is natural to consider incorporating biases by appending a constant feature 1 to X. Unfortunately, this leads to biases that are the same order as the weights, and so the effect disappears in the large dataset limit. Moreover, this modification on the data violates the assumptions of  Pennington & Worah (2017) .  Hastie et al. (2019)  study ridgeless interpolation in high-dimensional interpolation for linear features as well as nonlinear random features of iid Gaussian data.  Louart et al. (2018)  derived a deterministic equivalent for the resolvent of the kernel matrix F t F which allowed for a characterization of the asymptotic training and test performance of linear ridge regression of random feature models. Other work has investigated learning dynamics and generalization in the high-dimensional regime ( Liao & Couillet, 2018a ;  Lampinen & Ganguli, 2018 ;  Advani & Ganguli, 2016 ;  Advani & Saxe, 2017 ) as well as the spectra of more complicated objects such as the Hessian ( Pennington & Bahri, 2017 ) and Fisher information matrix ( Pennington & Worah, 2018 ). From the mathematical perspective, random matrix theory provides natural tools ( Silverstein & Bai, 1995a ) for analyzing the behavior of neural networks in the high-dimensional regime.  Liao & Couillet (2018b)  examined spectra for data drawn from Gaussian mixture models; see also ( El Karoui, 2010 ) on the spectra of random kernel matrices.

Section Title: PRELIMINARIES
  PRELIMINARIES Consider a dataset X ∈ R n0×m and the random feature matrix, F = f (W X; B), generated by a single hidden-layer network with iid Gaussian weights W ∈ R n1×n0 (W ak ∼ N (0, σ 2 W /n 0 )), activation function f , and biases B = b1 m T ∈ R n1×m (for b ∈ R n1 ). We regard Under review as a conference paper at ICLR 2020 the second argument of f as parametrizing (continuously or discretely) an ensemble of activation functions. We refer to B (or b) as the bias, in reference to the important special case f (W X + B) (additive bias). In general, however, we only assume the parameters b a are such that the measure 1 n1 a δ ba converges in distribution to some limiting distribution µ B , effecting an arbitrary distribution over activation functions. We assume that E |f (N ; b)| k for N ∼ N (0, σ) is finite for all 1 ≤ k ≤ 3, σ > 0, and b ∈ support(µ B ). When µ B is a single Dirac mass at location b 0 , the activation function can be written as f (W X; B) = f (W X; b 0 ) = g(W X) for some single-argument function g (for an additive bias, g(W X) = f (W X + b 0 )). When this is the case, we say the model has a single activation function, as opposed to a mixture or distribution of activation functions. The quantity of interest for the investigations below is the kernel matrix 1 n1 F t F , and in particular its resolvent, As we review in Sec. 4, the optimal regression coefficients of a linear model on the random features F is a simple function of this resolvent. The high-dimensional regime that we study is the one in which the dataset size m, feature dimension- ality n 0 , and hidden layer width n 1 all go to infinity at the same rate. In particular, as is standard in the random matrix literature, we assume that we can parameterize the limit in terms of the dataset size m in such a way that there exist two positive constants, Note that the resolvent is a random matrix, but as m grows large, its normalized trace becomes a deterministic quantity. In the limit that m → ∞, this quantity is known as the Stieltjes transform 1 , Together with an auxiliary transformm(z), defined below, these deterministic quantities completely characterize the asymptotic training error of kernel ridge regression on a noisy autoencoder task in this high-dimensional regime. The Stieltjes transform frequently arises in random matrix methods as a way to encode the spectra of matrices. In particular, if λ i are the eigenvalues of 1 n1 F t F and the empirical distribution of eigenvalues converges in distribution to some deterministic limiting density as m → ∞, 1 m m i=1 δ λi → µ(λ)dλ , (4) then (with appropriate technical assumptions), the limiting spectral density itself can be recovered from the Stieltjes transform m(z) via the inversion formula, The Stieltjes transform then substitutes convergence in distribution for pointwise convergence for all z such that z > 0.

Section Title: METHODS FOR COMPUTING THE STIELTJES TRANSFORM
  METHODS FOR COMPUTING THE STIELTJES TRANSFORM We briefly review two standard methods for computing the Stieltjes transform m(z), the resolvent method and the moments method. The resolvent method is an approach for computing the Stieltes transform based on the application of the Schur complement formula to the resolvent itself (or to a closely-related block matrix). Intuitively, as the matrix size becomes large, the minors of the matrix are similar in distribution to the larger matrix, and, moreover, the Cauchy interlacing theorem guarantees that their Stieltjes transforms are Under review as a conference paper at ICLR 2020 close as well. This allows for the derivation of a self-consistent equation (SCE) in which the Stieltjes transform appears on the left-hand side as the trace of the resolvent, and on the right-hand side as the trace of one of its minors. The moments method is more combinatorial in nature and involves expanding the resolvent for large z and computing the traces of each term, The traces themselves are expended out as tr(F t F ) k = F a1α1 F a1α2 · · · F a k α1 , (7) where the sum runs over matrix indices a 1 , . . . , a k , α 1 , . . . α k . The essence of the moment method involves analyzing the asymptotic contribution of each term in the sum based on its combinatorial type and the details of F , and resumming the results to obtain m(z). We refer the reader to ( Erdos & Yau, 2017 ;  Tao, 2012 ) for more details about these methods and additional background on random matrix theory.

Section Title: RESULT FOR STIELTJES TRANSFORM
  RESULT FOR STIELTJES TRANSFORM

Section Title: MAIN THEOREM
  MAIN THEOREM We make the following assumptions on the data matrix X and bias vector b: 2. the empirical eigenvalue distribution of 1 n0 X t X converges in distribution to a measure µ X 3. 1 n1 n1 a=1 δ ba → µ B in distribution. Theorem 1. Define σ Z = σ W σ X and resolvent G(z) = 1 n1 F t F − zI −1 . Then under the above assumptions and for all z such that z > 0, the transforms 1 m tr G(z) and 1 m tr 1 n0 X t Xtr G(z) , (8) converge in probability to the unique solution, m(z) andm(z), of the Eqn. (9) that map C + to C + : and where η(B) and ζ(B) are the Gaussian expectations The proof is quite involved and is presented in the supplementary material. The basic idea is to derive a multivariate Gaussian random matrix model with the same correlation structure as F , then derive a self-consistent equation (SCE) using the resolvent method for this linearized version of F , F lin . Remark 1. The self-consistent equations consist of two coupled equations involving the Stieltjes transform m(z) and an auxiliary objectm(z), (cf. ( Paul & Silverstein, 2009 , Eq. (2))), which we will see in Cor. 1 essentially measures the autoencoding capacity of the network. Remark 2. Note that the self-consistent equations contain an expectation over the limiting spectral density of the input data. While the assumptions on the data matrix X in Thm. 1 are quite general, they may not be optimal. See Sec. 3.3, where we show strong agreement with empirical data from MNIST and CIFAR-10 and for a range of synthetic distributions. This suggests that the theorem may hold for even more general data distributions.

Section Title: ALTERNATE REPRESENTATION AND LIMITING RESULTS
  ALTERNATE REPRESENTATION AND LIMITING RESULTS When the data distribution is iid Gaussian, the expectations in Eqn. (9) can be expressed in closed form, though one must be careful to choose the correct branch of the resulting function. For simplicity and future reference, we focus on the setting where 0 < φ ≤ ψ ≤ 1, in which case we have the Under review as a conference paper at ICLR 2020 (a) (b) (c) When µ B in Eqn. (10) is trivial, i.e. a single Dirac mass, then the result should reduce to the single activation function case with F = f (W X), which was studied in ( Pennington & Worah, 2017 ). Indeed, writing η = E B [η(B)] and ζ = E B [ζ(b)] for such a distribution, and eliminatingm(z) from Eqns. (13) and (14), we find that m(z) satisfies the following quartic polynomial: which agrees with the result in ( Pennington & Worah, 2017 ) upon identifying m(z) = −(1 − φ/ψ)/z − φ/ψG(z).

Section Title: SPECTRAL DENSITY ESTIMATES
  SPECTRAL DENSITY ESTIMATES The self-consistent equations in Thm. 1 can be solved numerically by iterating Eqn. (9) until con- vergence, using numerical integration. By utilizing the Stieltjes inversion formula, Eqn. (5), we can extract predictions for the limiting eigenvalue density of 1 n1 F t F . The results show close agree- ment with empirical spectral simulations from several interesting practical datasets and synthetic distributions, see  Figs. 1  and 2.

Section Title: RIDGE-REGULARIZED NOISY AUTOENCODER
  RIDGE-REGULARIZED NOISY AUTOENCODER We consider the problem of kernel ridge regression with random features given by F = f (W X; B) and noisy regression targets given by Y = AX + for some A ∈ R n2×n0 and independent Gaussian noise ∈ R n2×m such that ij ∼ N (0, σ 2 ). As is common in the literature of high-dimensional statistics, we assume an isotropic prior on A such that A t A → n2 n0 σ 2 A I as m, n 0 , n 2 → ∞. Note that when σ A = 0, we have the pure memorization setting studied by  Pennington & Worah (2017) . with random features F = f (W X; B). Then the asymptotic training error converges in probability as In particular, we see that the derivative of the Stieltjes transform m (−γ) measures the capacity to learn noisy labels, whereasm (−γ) measures pure autoencoding capacity. See  Fig. 3  for a comparison between these theoretical predictions and simulation. Remark 3. As in ( Pennington & Worah, 2017 ), there is a scaling homogeneity in the E train : an increase in the regularization constant γ can be compensated by a decrease in scale of W 2 , which, in turn, can be compensated by increasing the scale of F , which is equivalent to increasing η(b) and ζ(b). Owing to this homogeneity, we are free to choose a normalization of the activation function for which E b∼µ B [η(b)] = 1.

Section Title: NONLINEAR MIXTURES CAN OUTPERFORM SINGLE NONLINEARITIES
  NONLINEAR MIXTURES CAN OUTPERFORM SINGLE NONLINEARITIES The bias term in our random feature model can be viewed as one way of defining a distribution over activation functions. The choice of distribution, in general, affects the performance of the model on a given task - as quantified by the expectations in Eqn. (10). A key benefit of our model and analytical approach is that it permits nontrivial distributions of nonlinear activation functions. In this section, we build on our results for the training loss on noisy autoencoder tasks to examine the benefits of utilizing nonlinear mixtures. The goal here is not to identify "good" mixtures, since this will clearly be a dataset- and architecture-dependent question, nor is it to identify a large performance gap. Instead, we merely seek to demonstrate a proof-of-principle, namely that there exist non-trivial distributions over nonlinearities that can provably outperform the best possible single nonlinearity. For this analysis, we consider the simplest possible nontrivial distribution over activation functions: a Bernoulli mixture of two different functions. To each of these functions we associate two constants, η and ζ, which derive from Eqn. (12) but have no B-dependence since each function is a single nonlinearity. Concretely, let For the two functions themselves, we utilize (i) a "pure linear" activation function with η = 1, i.e. the identity function and (ii) a "pure nonlinear" ( Hastie et al., 2019 ) activation function with η = 1 and ζ = 0. (The particular purely nonlinear function in (ii) is irrelevant, as our theory predicts and our experiments confirm; see Fig. 4(a)). To be precise, we define for p ∼ Bernoulli(p), f p (x) := x if p = 0 g ζ=0 (x) if p = 1 , (19) where g ζ=0 is any function with η = 1 and ζ = 0 (see, e.g., the functions in Fig. (3) of ( Pennington & Worah, 2017 )). The task of computing E train for f p is cumbersome but purely algebraic. To see how to proceed, notice that the expectations in Eqn. (10) are simple for f p : Plugging these equations into Eqns. (13) and (14), collecting terms and simplifying yields a set of coupled polynomial equations for m(z) andm(z). Taking the total derivative of these equations with respect to z yields two additional equations which can be solved to express m (z) andm (z) in terms of m(z) andm(z). Combining these results produces a polynomial system whose solution 2 encodes E train through Eqn. (17). Fig. 4(a) shows the result of this calculation in solid lines for various values of γ, while the 1σ error bars show empirical simulations with finite networks. The red stars in the figure show that for many values of γ, the optimal mixture percentage is intermediate, i.e. 0 < p < 1. The question is, does a non-trivial mixture actually outperform a single nonlinearity? First, we must understand the performance of the optimal single nonlinearity. We note that owing to the homogeneity of the the training loss in η, ζ, and γ, we can assume without loss of generality that η = 1. Therefore the entire effect of the nonlinearity should be encoded in the single constant ζ. In Fig. 4(a), we plot our theoretical prediction for E train in solid lines and empirical simulations for finite networks as 1σ error bars. The activation function used for each simulation is chosen randomly, conditional on the value of ζ. So the good agreement in the left panel of Fig. 4(a) demonstrates not just the correctness of our theoretical result but also the fact that E train depends on the activation function solely through the constant ζ. The blue stars in this figure indicate that the optimal single nonlinearity is neither purely linear (ζ = 1), nor purely nonlinear (ζ = 0), but rather something in between. For this particular problem setup, the performance of the optimal single nonlinearity and the optimal Bernoulli mixture are rather close, as indicated by the top panel of Fig. 4(b). However, owing to our precise analytical formulation, we can evaluate the training loss to high precision and observe that there is indeed a difference in performance between the two models, as shown in the bottom panel of Fig. 4(b). This result establishes that there are some problems for which even the best single nonlinearity is outperformed by a mixture of nonlinearities.

Section Title: CONCLUSIONS
  CONCLUSIONS In this work we studied the feature matrix F = f (W X; B) where W is a random matrix with iid Gaussian entries. Under mild assumptions on X and B, we obtained an exact analytic formula, Eqn. (9), that characterizes the Stieltjes transform of the spectral density of F . The result allowed us to describe the exact training loss of a ridge-regularized noisy autoencoder in the high-dimensional, large-dataset limit, providing one of the first closed-form solutions to a non-trivial model in this limit. We found excellent agreement between the asymptotic predictions of Eqn. (9) and a variety of finite-dimensional empirical simulations. We also advanced the interpretation of the bias B as one particular way of parameterizing a distri- bution of activation functions. Indeed, our derivations proceed completely unchanged whether this distribution is of the traditional additive form f (· + B) or the more general f (·; B). By examining the latter, we showed that there are configurations in which a non-trivial distribution over activation functions provably outperforms the best possible single activation function. This opens the door to future investigations regarding optimal methods for parameterizing distributions over activation functions for approximate kernel methods, and suggests the possibility that mixtures of nonlinearities could be a useful design consideration when constructing neural network architectures.

```
