Title:
```
Published as a conference paper at ICLR 2020 DYNAMICS-AWARE UNSUPERVISED DISCOVERY OF SKILLS
```
Abstract:
```
Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can poten- tially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynami- cal systems is difficult, and even then, the model might not generalize well out- side the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model- based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsuper- vised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demon- strate that zero-shot planning in the learned latent space significantly outper- forms standard MBRL and model-free goal-conditioned RL, can handle sparse- reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery. We have open-sourced our implementation at: https://github.com/google-research/dads Figure 1: A humanoid agent discovers diverse locomotion primitives without any reward using DADS. We show zero-shot generalization to downstream tasks by composing the learned primitives using model predic- tive control, enabling the agent to follow an online sequence of goals (green markers) without any additional training.
```

Figures/Tables Captions:
```
Figure 2: The agent π interacts with the environment to produce a transition s → s . Intrinsic reward is computed by computing the transition probability under q for the current skill z, compared to random samples from the prior p(z). The agent maximizes the intrinsic reward computed for a batch of episodes, while q maximizes the log-probability of the actual transitions of (s, z) → s .
Figure 3: At test time, the planner executes simulates the transitions in environment using skill-dynamics q, and updates the distribution of plans according to the computed reward on the simulated trajectories. After a few updates to the plan, the first primitive is executed in the environment using the learned agent π.
Figure 4: Skills learned on different MuJoCo environments in the OpenAI gym. DADS can discover diverse skills without any extrinsic rewards, even for problems with high-dimensional state and action spaces.
Figure 5: (Left, Centre) X-Y traces of Ant skills and (Right) Heatmap to visualize the learned continuous skill space. Traces demonstrate that the continuous space offers far greater diversity of skills, while the heatmap demonstrates that the learned space is smooth, as the orientation of the X-Y trace varies smoothly as a function of the skill.
Figure 6: (Top-Left) Standard deviation of Ant's position as a function of steps in the environment, averaged over multiple skills and normalized by the norm of the position. (Top-Right to Bottom-Left Clockwise) X-Y traces of skills learned using DIAYN with x-y prior, DADS with x-y prior and DADS without x-y prior, where the same color represents trajectories resulting from the execution of the same skill z in the environment. High variance skills from DIAYN offer limited utility for hierarchical control.
Figure 7: (Left) The results of the MPPI controller on skills learned using DADS-c (continuous primitives) and DADS-d (discrete primitives) significantly outperforms state-of-the-art model-based RL. (Right) Planning for a new task does not require any additional training and outperforms model-based RL being trained for the specific task.
Figure 8: (Left) A RL-trained meta-controller is unable to compose primitive learned by DIAYN to navigate Ant to a goal, while it succeeds to do so using the primitives learned by DADS. (Right) Goal-Conditioned RL (GCRL-dense/sparse) does not generalize outside its training distribution, while MPPI controller on learned skills (DADS-dense/sparse) experiences significantly smaller degrade in performance.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep reinforcement learning (RL) enables autonomous learning of diverse and complex tasks with rich sensory inputs, temporally extended goals, and challenging dynamics, such as discrete game- playing domains (Mnih et al., 2013; Silver et al., 2016), and continuous control domains including locomotion (Schulman et al., 2015; Heess et al., 2017) and manipulation (Rajeswaran et al., 2017; Kalashnikov et al., 2018; Gu et al., 2017). Most of the deep RL approaches learn a Q-function or a policy that are directly optimized for the training task, which limits their generalization to new scenarios. In contrast, MBRL methods (Li & Todorov, 2004; Deisenroth & Rasmussen, 2011; Watter et al., 2015) can acquire dynamics models that may be utilized to perform unseen tasks at test time. While this capability has been demonstrated in some of the recent works (Levine et al., 2016; Nagabandi et al., 2018; Chua et al., 2018b; Kurutach et al., 2018; Ha & Schmidhuber, Published as a conference paper at ICLR 2020 2018), learning an accurate global model that works for all state-action pairs can be exceedingly challenging, especially for high-dimensional system with complex and discontinuous dynamics. The problem is further exacerbated as the learned global model has limited generalization outside of the state distribution it was trained on and exploring the whole state space is generally infeasible. Can we retain the flexibility of model-based RL, while using model-free RL to acquire proficient low-level behaviors under complex dynamics? While learning a global dynamics model that captures all the different behaviors for the entire state- space can be extremely challenging, learning a model for a specific behavior that acts only in a small part of the state-space can be much easier. For example, consider learning a model for dynamics of all gaits of a quadruped versus a model which only works for a specific gait. If we can learn many such behaviors and their corresponding dynamics, we can leverage model-predictive control to plan in the behavior space, as opposed to planning in the action space. The question then becomes: how do we acquire such behaviors, considering that behaviors could be random and unpredictable? To this end, we propose Dynamics-Aware Discovery of Skills (DADS), an unsupervised RL framework for learning low-level skills using model-free RL with the explicit aim of making model-based con- trol easy. Skills obtained using DADS are directly optimized for predictability, providing a better representation on top of which predictive models can be learned. Crucially, the skills do not require any supervision to learn, and are acquired entirely through autonomous exploration. This means that the repertoire of skills and their predictive model are learned before the agent has been tasked with any goal or reward function. When a task is provided at test-time, the agent utilizes the previously learned skills and model to immediately perform the task without any further training. The key contribution of our work is an unsupervised reinforcement learning algorithm, DADS, grounded in mutual-information-based exploration. We demonstrate that our objective can em- bed learned primitives in continuous spaces, which allows us to learn a large, diverse set of skills. Crucially, our algorithm also learns to model the dynamics of the skills, which enables the use of model-based planning algorithms for downstream tasks. We adapt the conventional model predic- tive control algorithms to plan in the space of primitives, and demonstrate that we can compose the learned primitives to solve downstream tasks without any additional training.

Section Title: PRELIMINARIES
  PRELIMINARIES Mutual information can been used as an objective to encourage exploration in reinforcement learning (Houthooft et al., 2016; Mohamed & Rezende, 2015). According to its definition, I(X; Y ) = H(X) − H(X | Y ), maximizing mutual information I with respect to Y amounts to maximizing the entropy H of X while minimizing the conditional entropy H(X | Y ). In the context of RL, X is usually a function of the state and Y a function of actions. Maximizing this objective encourages the state entropy to be high, making the underlying policy to be exploratory. Recently, multiple works (Eysenbach et al., 2018; Gregor et al., 2016; Achiam et al., 2018) apply this idea to learn diverse skills which maximally cover the state space. To leverage planning-based control, MBRL estimates the true dynamics of the environment by learn- ing a modelp(s | s, a). This allows it to predict a trajectory of statesτ H = (s t ,ŝ t+1 , . . .ŝ t+H ) resulting from a sequence of actions without any additional interaction with the environment. While model-based RL methods have been demonstrated to be sample efficient compared to their model- free counterparts, learning an effective model for the whole state-space is challenging. An open- problem in model-based RL is to incorporate temporal abstraction in model-based control, to enable high-level planning and move-away from planning at the granular level of actions. These seemingly unrelated ideas can be combined into a single optimization scheme, where we first discover skills (and their models) without any extrinsic reward and then compose these skills to optimize for the task defined at test time using model-based planning. At train time, we assume a Markov Decision Process (MDP) M 1 ≡ (S, A, p). The state space S and action space A are assumed to be continuous, and the A bounded. We assume the transition dynamics p to be stochastic, such that p : S × A × S → [0, ∞). We learn a skill-conditioned policy π(a | s, z), where the skills z belongs to the space Z, detailed in Section 3. We assume that the skills are sampled from a prior p(z) over Z. We simultaneously learn a skill-conditioned transition function q(s | s, z), coined as skill-dynamics, which predicts the transition to the next state s from the current state s for the skill z under the given dynamics p. At test time, we assume an MDP M 2 ≡ (S, A, p, r), where S, A, p Published as a conference paper at ICLR 2020 match those defined in M 1 , and the reward function r : S × A → (−∞, ∞). We plan in Z using q(s | s, z) to compose the learned skills z for optimizing r in M 2 , which we detail in Section 4. We use the information theoretic paradigm of mutual information to obtain our unsupervised skill discovery algorithm. In particular, we propose to maximize the mutual information between the next state s and current skill z conditioned on the current state s. Mutual information in Equation 1 quantifies how much can be known about s given z and s, or symmetrically, z given the transition from s → s . From Equation 2, maximizing this objective corresponds to maximizing the diversity of transitions produced in the environment, that is denoted by the entropy H(s | s), while making z informative about the next state s by minimizing the entropy H(s | s, z). Intuitively, skills z can be interpreted as abstracted action sequences which are identifiable by the transitions generated in the environment (and not just by the current state). Thus, optimizing this mutual information can be understood as encoding a diverse set of skills in the latent space Z, while making the transitions for a given z ∈ Z predictable. We use the entropy- decomposition in Equation 2 to connect this objective with model-based control. We want to optimize the our skill-conditioned controller π(a | s, z) such that the latent space z ∼ p(z) is maximally informative about the transitions s → s . Using the definition of conditional mutual information, we can rewrite Equation 2 as: We assume the following generative model: p(z, s, s ) = p(z)p(s | z)p(s | s, z), where p(z) is user specified prior over Z, p(s|z) denotes the stationary state-distribution induced by π(a | s, z) for a skill z and p(s | s, z) denotes the transition distribution under skill z. Note, p(s | s, z) = p(s | s, a)π(a | s, z)da is intractable to compute because the underlying dynamics are unknown. However, we can variationally lower bound the objective as follows: Published as a conference paper at ICLR 2020 where we have used the non-negativity of KL-divergence, that is D KL ≥ 0. Note, skill-dynamics q φ represents the variational approximation for the transition function p(s | s, z), which enables model-based control as described in Section 4. Equation 4 suggests an alternating optimization between q φ and π, summarized in Algorithm 1. In every iteration: (Tighten variational lower bound) We minimize D KL (p(s | s, z) || q φ (s | s, z)) with respect to the parameters φ on z, s ∼ p to tighten the lower bound. For general function approximators like neural networks, we can write the gradient for φ as follows: ∇ φ E s,z [D KL (p(s | s, z) || q φ (s | s, z))] = ∇ φ E z,s,s log p(s | s, z) q φ (s | s, z) = −E z,s,s ∇ φ log q φ (s | s, z) (5) which corresponds to maximizing the likelihood of the samples from p under q φ . (Maximize approximate lower bound) After fitting q φ , we can optimize π to maximize E z,s,s [log q φ (s | s, z) − log p(s | s)]. Note, this is a reinforcement-learning style optimiza- tion with a reward function log q φ (s | s, z) − log p(s | s). However, log p(s | s) is intractable to compute, so we approximate the reward function for π: The approximation is motivated as follows: p(s | s) = p(s | s, z)p(z|s)dz ≈ q φ (s | s, z)p(z)dz ≈ 1 L L i=1 q φ (s | s, z i ) for z i ∼ p(z), where L denotes the number of samples from the prior. We are using the marginal of variational approximation q φ over the prior p(z) to approx- imate the marginal distribution of transitions. We discuss this approximation in Appendix C. Note, the final reward function r z encourages the policy π to produce transitions that are (a) predictable under q φ (predictability) and (b) different from the transitions produced under z i ∼ p(z) (diversity). To generate samples from p(z, s, s ), we use the rollouts from the current policy π for multiple samples z ∼ p(z) in an episodic setting for a fixed horizon T . We also introduce entropy regu- larization for π(a | s, z), which encourages the policy to discover action-sequences with similar state-transitions and to be clustered under the same skill z, making the policy robust besides en- couraging exploration (Haarnoja et al., 2018a). The use of entropy regularization can be justified from an information bottleneck perspective as discussed for Information Maximization algorithm in (Mohamed & Rezende, 2015). This is even more extensively discussed from the graphical model perspective in Appendix B, which connects unsupervised skill discovery and information bottleneck literature, while also revealing the temporal nature of skills z. Details corresponding to implemen- tation and hyperparameters are discussed in Appendix A.

Section Title: PLANNING USING SKILL DYNAMICS
  PLANNING USING SKILL DYNAMICS Given the learned skills π(a | s, z) and their respective skill-transition dynamics q φ (s | s, z), we can perform model-based planning in the latent space Z to optimize for a reward r that is given to the agent at test time. Note, that this essentially allows us to perform zero-shot planning given the unsupervised pre-training procedure described in Section 3. In order to perform planning, we employ the model-predictive-control (MPC) paradigm Garcia et al. (1989), which in a standard setting generates a set of action plans P k = (a k,1 , . . . a k,H ) ∼ P for a planning horizon H. The MPC plans can be generated due to the fact that the planner is able to simulate the trajectoryτ k = (s k,1 , a k,1 . . . s k,H+1 ) assuming access to the transition dynamicŝ p(s | s, a). In addition, each plan computes the reward r(τ k ) for its trajectory according to the reward function r that is provided for the test-time task. Following the MPC principle, the planner selects the best plan according to the reward function r and executes its first action a 1 . The planning algorithm repeats this procedure for the next state iteratively until it achieves its goal. We use a similar strategy to design an MPC planner to exploit previously-learned skill-transition dynamics q φ (s | s, z). Note that unlike conventional model-based RL, we generate a plan P k = (z k,1 , . . . z k,H P ) in the latent space Z as opposed to the action space A that would be used by a standard planner. Since the primitives are temporally meaningful, it is beneficial to hold a primitive Published as a conference paper at ICLR 2020 for a horizon H Z > 1, unlike actions which are usually held for a single step. Thus, effectively, the planning horizon for our latent space planner is H = H P × H Z , enabling longer-horizon planning using fewer primitives. Similar to the standard MPC setting, the latent space planner simulates the trajectoryτ k = (s k,1 , z k,1 , a k,1 , s k,2 , z k,2 , a k,2 , . . . s k,H+1 ) and computes the reward r(τ k ). After a small number of trajectory samples, the planner selects the first latent action z 1 of the best plan, executes it for H Z steps in the environment, and the repeats the process until goal completion. The latent planner P maintains a distribution of latent plans, each of length H P . Each element in the sequence represents the distribution of the primitive to be executed at that time step. For continuous spaces, each element of the sequence can be modelled using a normal distribution, N (µ 1 , Σ), . . . N (µ H P , Σ). We refine the planning distributions for R steps, using K samples of latent plans P k , and compute the r k for the simulated trajectoryτ k . The update for the parameters follows that in Model Predictive Path Integral (MPPI) controller Williams et al. (2016): While we keep the covariance matrix of the distributions fixed, it is possible to update that as well as shown in Williams et al. (2016). We show an overview of the planning algorithm in  Figure 3 , and provide more implementation details in Appendix A.

Section Title: RELATED WORK
  RELATED WORK Central to our method is the concept of skill discovery via mutual information maximization. This principle, proposed in prior work that utilized purely model-free unsupervised RL methods (Daniel et al., 2012; Florensa et al., 2017; Eysenbach et al., 2018; Gregor et al., 2016; Warde-Farley et al., 2018; Thomas et al., 2018), aims to learn diverse skills via a discriminability objective: a good set of skills is one where it is easy to distinguish the skills from each other, which means they perform distinct tasks and cover the space of possible behaviors. Building on this prior work, we distinguish our skills based on how they modify the original uncontrolled dynamics of the system. This simultaneously encourages the skills to be both diverse and predictable. We also demonstrate that constraining the skills to be predictable makes them more amenable for hierarchical composition and thus, more useful on downstream tasks. Another line of work that is conceptually close to our method copes with intrinsic motiva- tion (Oudeyer & Kaplan, 2009; Oudeyer et al., 2007; Schmidhuber, 2010) which is used to drive the agent's exploration. Examples of such works include empowerment Klyubin et al. (2005); Mo- hamed & Rezende (2015), count-based exploration Bellemare et al. (2016); Oh et al. (2015); Tang et al. (2017); Fu et al. (2017), information gain about agent's dynamics Stadie et al. (2015) and Published as a conference paper at ICLR 2020 forward-inverse dynamics models Pathak et al. (2017). While our method uses an information- theoretic objective that is similar to these approaches, it is used to learn a variety of skills that can be directly used for model-based planning, which is in contrast to learning a better exploration policy for a single skill. The skills discovered using our approach can also provide extended actions and temporal abstrac- tion, which enable more efficient exploration for the agent to solve various tasks, reminiscent of hierarchical RL (HRL) approaches. This ranges from the classic option-critic architecture (Sutton et al., 1999; Stolle & Precup, 2002; Perkins et al., 1999) to some of the more recent work (Bacon et al., 2017; Vezhnevets et al., 2017; Nachum et al., 2018; Hausman et al., 2018). However, in contrast to end-to-end HRL approaches (Heess et al., 2016; Peng et al., 2017), we can leverage a stable, two-phase learning setup. The primitives learned through our method provide action and temporal abstraction, while planning with skill-dynamics enables hierarchical composition of these primitives, bypassing many problems of end-to-end HRL. In the second phase of our approach, we use the learned skill-transition dynamics models to perform model-based planning - an idea that has been explored numerous times in the literature. Model-based reinforcement learning has been traditionally approached with methods that are well-suited for low- data regimes such as Gaussian Processes (Rasmussen, 2003) showing significant data-efficiency gains over model-free approaches (Deisenroth et al., 2013; Kamthe & Deisenroth, 2017; Kocijan et al., 2004; Ko et al., 2007). More recently, due to the challenges of applying these methods to high- dimensional state spaces, MBRL approaches employs Bayesian deep neural networks (Nagabandi et al., 2018; Chua et al., 2018b; Gal et al., 2016; Fu et al., 2016; Lenz et al., 2015) to learn dynamics models. In our approach, we take advantage of the deep dynamics models that are conditioned on the skill being executed, simplifying the modelling problem. In addition, the skills themselves are being learned with the objective of being predictable, further assists with the learning of the dynamics model. There also have been multiple approaches addressing the planning component of MBRL including linear controllers for local models (Levine et al., 2016; Kumar et al., 2016; Chebotar et al., 2017), uncertainty-aware (Chua et al., 2018b; Gal et al., 2016) or deterministic planners (Nagabandi et al., 2018) and stochastic optimization methods (Williams et al., 2016). The main contribution of our work lies in discovering model-based skill primitives that can be further combined by a standard model-based planner, therefore we take advantage of an existing planning approach - Model Predictive Path Integral (Williams et al., 2016) that can leverage our pre-trained setting.

Section Title: EXPERIMENTS
  EXPERIMENTS Through our experiments, we aim to demonstrate that: (a) DADS as a general purpose skill dis- covery algorithm can scale to high-dimensional problems; (b) discovered skills are amenable to hierarchical composition and; (c) not only is planning in the learned latent space feasible, but it is competitive to strong baselines. In Section 6.1, we provide visualizations and qualitative analysis of the skills learned using DADS. We demonstrate in Section 6.2 and Section 6.4 that optimizing the primitives for predictability renders skills more amenable to temporal composition that can be used for Hierarchical RL.We benchmark against state-of-the-art model-based RL baseline in Section 6.3, and against goal-conditioned RL in Section 6.5.

Section Title: QUALITATIVE ANALYSIS
  QUALITATIVE ANALYSIS

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 In this section, we provide a qualitative discussion of the unsupervised skills learned using DADS. We use the MuJoCo environments (Todorov et al., 2012) from the OpenAI gym as our test-bed (Brockman et al., 2016). We find that our proposed algorithm can learn diverse skills without any reward, even in problems with high-dimensional state and actuation, as illustrated in  Figure 4 . DADS can discover primitives for Half-Cheetah to run forward and backward with multiple different gaits, for Ant to navigate the environment using diverse locomotion primitives and for Humanoid to walk using stable locomotion primitives with diverse gaits and direction. The videos of the discovered primitives are available at: https://sites.google.com/view/dads-skill Qualitatively, we find the skills discovered by DADS to be predictable and stable, in line with im- plicit constraints of the proposed objective. While the Half-Cheetah will learn to run in both back- ward and forward directions, DADS will disincentivize skills which make Half-Cheetah flip owing to the reduced predictability on landing. Similarly, skills discovered for Ant rarely flip over, and tend to provide stable navigation primitives in the environment. This also incentivizes the Humanoid, which is characteristically prone to collapsing and extremely unstable by design, to discover gaits which are stable for sustainable locomotion. One of the significant advantages of the proposed objective is that it is compatible with continuous skill spaces, which has not been shown in prior work on skill discovery (Eysenbach et al., 2018). Not only does this allow us to embed a large and diverse set of skills into a compact latent space, but also the smoothness of the learned space allows us to interpolate between behaviors generated in the envi- ronment. We demonstrate this on the Ant environment ( Figure 5 ), where we learn two-dimensional continuous skill space with a uniform prior over (−1, 1) in each dimension, and compare it to a dis- crete skill space with a uniform prior over 20 skills. Similar to Eysenbach et al. (2018), we restrict the observation space of the skill-dynamics q to the cartesian coordinates (x, y). We hereby call this the x-y prior, and discuss its role in Section 6.2. In  Figure 5 , we project the trajectories of the learned Ant skills from both discrete and continuous spaces onto the Cartesian plane. From the traces of the skills, it is clear that the continuous latent space can generate more diverse trajectories. We demonstrate in Section 6.3, that continuous prim- itives are more amenable to hierarchical composition and generally perform better on downstream tasks. More importantly, we observe that the learned skill space is semantically meaningful. The heatmap in  Figure 5  shows the orientation of the trajectory (with respect to the x-axis) as a func- tion of the skill z ∈ Z, which varies smoothly as z is varied, with explicit interpolations shown in Appendix D.

Section Title: SKILL VARIANCE ANALYSIS
  SKILL VARIANCE ANALYSIS In an unsupervised skill learning setup, it is important to optimize the primitives to be diverse. How- ever, we argue that diversity is not sufficient for the learned primitives to be useful for downstream tasks. Primitives must exhibit low-variance behavior, which enables long-horizon composition of the learned skills in a hierarchical setup. We analyze the variance of the x-y trajectories in the en- vironment, where we also benchmark the variance of the primitives learned by DIAYN (Eysenbach et al., 2018). For DIAYN, we use the x-y prior for the skill-discriminator, which biases the dis- covered skills to diversify in the x-y space. This step was necessary for that baseline to obtain a Published as a conference paper at ICLR 2020 competitive set of navigation skills.  Figure 6  (Top-Left) demonstrates that DADS, which optimizes the primitives for predictability and diversity, yields significantly lower-variance primitives when compared to DIAYN, which only optimizes for diversity. This is starkly demonstrated in the plots of X-Y traces of skills learned in different setups. Skills learned by DADS show significant control over the trajectories generated in the environment, while skills from DIAYN exhibit high variance in the environment, which limits their utility for hierarchical control. This is further demonstrated quantitatively in Section 6.4. While optimizing for predictability already significantly reduces the variance of the trajectories gen- erated by a primitive, we find that using the x-y prior with DADS brings down the skill variance even further. For quantitative benchmarks in the next sections, we assume that the Ant skills are learned using an x-y prior on the observation space, for both DADS and DIAYN.

Section Title: MODEL-BASED REINFORCEMENT LEARNING
  MODEL-BASED REINFORCEMENT LEARNING The key utility of learning a parametric model q φ (s |s, z) is to take advantage of planning algorithms for downstream tasks, which can be extremely sample-efficient. In our setup, we can solve test- time tasks in zero-shot, that is without any learning on the downstream task. We compare with the state-of-the-art model-based RL method (Chua et al., 2018a), which learns a dynamics model parameterized as p(s |s, a), on the task of the Ant navigating to a specified goal with a dense reward. Given a goal g, reward at any position u is given by r(u) = − g − u 2 . We benchmark our method against the following variants: • Random-MBRL (rMBRL): We train the model p(s |s, a) on randomly collected trajecto- ries, and test the zero-shot generalization of the model on a distribution of goals. • Weak-oracle MBRL (WO-MBRL): We train the model p(s |s, a) on trajectories generated by the planner to navigate to a goal, randomly sampled in every episode. The distribution of goals during training matches the distribution at test time. • Strong-oracle MBRL (SO-MBRL): We train the model p(s |s, a) on a trajectories generated by the planner to navigate to a specific goal, which is fixed for both training and test time. Amongst the variants, only the rMBRL matches our assumptions of having an unsupervised task- agnostic training. Both WO-MBRL and SO-MBRL benefit from goal-directed exploration dur- ing training, a significant advantage over DADS, which only uses mutual-information-based explo- ration. We use ∆ = H t=1 −r(u) H g 2 as the metric, which represents the distance to the goal g averaged over the episode (with the same fixed horizon H for all models and experiments), normalized by the initial distance to the goal g. Therefore, lower ∆ indicates better performance and 0 < ∆ ≤ 1 (assuming the agent goes closer to the goal). The test set of goals is fixed for all the methods, sampled from [−15, 15] 2 .  Figure 7  demonstrates that the zero-shot planning significantly outperforms all model-based RL baselines, despite the advantage of the baselines being trained on the test goal(s). For the experi- ment depicted in  Figure 7  (Right), DADS has an unsupervised pre-training phase, unlike SO-MBRL which is training directly for the task. A comparison with Random-MBRL shows the significance of mutual-information-based exploration, especially with the right parameterization and priors. This experiment also demonstrates the advantage of learning a continuous space of primitives, which outperforms planning on discrete primitives.

Section Title: HIERARCHICAL CONTROL WITH UNSUPERVISED PRIMITIVES
  HIERARCHICAL CONTROL WITH UNSUPERVISED PRIMITIVES We benchmark hierarchical control for primitives learned without supervision, against our proposed scheme using an MPPI based planner on top of DADS-learned skills. We persist with the task of Ant-navigation as described in 6.3. We benchmark against Hierarchical DIAYN (Eysenbach et al., 2018), which learns the skills using the DIAYN objective, freezes the low-level policy and learns a meta-controller that outputs the skill to be executed for the next H Z steps. We provide the x-y prior to the DIAYN's disciminator while learning the skills for the Ant agent. The performance of the meta-controller is constrained by the low-level policy, however, this hierarchical scheme is agnostic to the algorithm used to learn the low-level policy. To contrast the quality of primitives learned by the DADS and DIAYN, we also benchmark against Hierarchical DADS, which learns a meta-controller the same way as Hierarchical DIAYN, but learns the skills using DADS. From  Figure 8  (Left) We find that the meta-controller is unable to compose the skills learned by DIAYN, while the same meta-controller can learn to compose skills by DADS to navigate the Ant to different goals. This result seems to confirm our intuition described in Section 6.2, that the high variance of the DIAYN skills limits their temporal compositionality. Interestingly, learning a RL meta-controller reaches similar performance to the MPPI controller, taking an additional 200, 000 samples per goal.

Section Title: GOAL-CONDITIONED RL
  GOAL-CONDITIONED RL To demonstrate the benefits of our approach over model-free RL, we benchmark against goal- conditioned RL on two versions of Ant-navigation: (a) with a dense reward r(u) and (b) with a sparse reward r(u) = 1 if u − g 2 ≤ , else 0. We train the goal-conditioned RL agent using soft actor-critic, where the state variable of the agent is augmented with u − g, the position delta to the goal. The agent gets a randomly sampled goal from [−10, 10] 2 at the beginning of the episode. In  Figure 8  (Right), we measure the average performance of the all the methods as a function of the initial distance of the goal, ranging from 5 to 30 metres. For dense reward navigation, we ob- serve that while model-based planning on DADS-learned skills degrades smoothly as the initial distance to goal to increases, goal-conditioned RL experiences a sudden deterioration outside the goal distribution it was trained on. Even within the goal distribution observed during training of goal-conditioned RL model, skill-space planning performs competitively to it. With sparse reward navigation, goal-conditioned RL is unable to navigate, while MPPI demonstrates comparable perfor- mance to the dense reward up to about 20 metres. This highlights the utility of learning task-agnostic skills, which makes them more general while showing that latent space planning can be leveraged for tasks requiring long-horizon planning.

Section Title: CONCLUSION
  CONCLUSION We have proposed a novel unsupervised skill learning algorithm that is amenable to model-based planning for hierarchical control on downstream tasks. We show that our skill learning method can scale to high-dimensional state-spaces, while discovering a diverse set of low-variance skills. In ad- dition, we demonstrated that, without any training on the specified task, we can compose the learned skills to outperform competitive model-based baselines that were trained with the knowledge of the test tasks. We plan to extend the algorithm to work with off-policy data, potentially using relabelling tricks (Andrychowicz et al., 2017; Nachum et al., 2018) and explore more nuanced planning algo- rithms. We plan to apply the hereby-introduced method to different domains, such as manipulation and enable skill/model discovery directly from images.

Section Title: ACKNOWLEDGEMENTS
  ACKNOWLEDGEMENTS

```
