Title:
```
Published as a conference paper at ICLR 2020 KEEP DOING WHAT WORKED: BEHAVIOR MODELLING PRIORS FOR OFFLINE REIN- FORCEMENT LEARNING
```
Abstract:
```
Off-policy reinforcement learning algorithms promise to be applicable in settings where only a fixed data-set (batch) of environment interactions is available and no new experience can be acquired. This property makes these algorithms appealing for real world problems such as robot control. In practice, however, standard off-policy algorithms fail in the batch setting for continuous control. In this paper, we propose a simple solution to this problem. It admits the use of data generated by arbitrary behavior policies and uses a learned prior - the advantage-weighted behavior model (ABM) - to bias the RL policy towards actions that have previously been executed and are likely to be successful on the new task. Our method can be seen as an extension of recent work on batch-RL that enables stable learning from conflicting data-sources. We find improvements on competitive baselines in a variety of RL tasks - including standard continuous control benchmarks and multi- task learning for simulated and real-world robots. Videos are available at https: //sites.google.com/view/behavior-modelling-priors.
```

Figures/Tables Captions:
```
Figure 1: Learning control suite tasks from fixed data. Standard off-policy RL fails on Cheetah, Hopper, Quadruped. Methods that regularize to the data distribution succeed; for hopper the behaviour distribution is multimodal and a standard behavior modlling prior is broad (see BM[prior]) here only ABM succeeds. Best trajectory in data marked with a star.
Figure 2: Control suite when using only the first 2k episodes of low-quality data from each run; return for best behavior episode in the data was 718/159/435/728 (left to right). While plain RL is able to learn on walker, learned priors improve performance and stabilize learning for cheetah and quadruped (with overall lower performance than when learning from good data).
Figure 3: Learning curves for MPO with and without behavior extraction priors. We show 4 of the 7 tasks here. Five training runs are shown for each parameter setting, evaluation is average reward over 50 episodes. Behavioral modelling priors significantly improve performance across all tasks, and ABM further improves learning speed and final performance.
Figure 4: (Left) Learning curves for the tasks "bring to corner" and "bring to center". These tasks were learned using only data from the seven intial stacking tasks. The stacking dataset was rich enough to learn these new tasks fully offline with ABM. (Right) Simulated Sawyer environment.
Figure 5: (Left) Original and offline learning on the real robot. The original training run takes over 200 hours to learn to stack and leave (blue bar, right y-axis) - bottle-necked by slow real-robot episode generation. Using the complete dataset of interactions in a batch-RL setting with the ABM prior, we are able to re-learn the final task in 12 hours purely from logged data (orange bar, right y-axis), while obtaining a policy that achieves similar performance on all tasks (box plots, distribution of returns over 50 test episodes, orange vs blue, left y-axis). (Right) Real Sawyer environment.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Batch reinforcement learning (RL) ( Ernst et al., 2005 ;  Lange et al., 2011 ) is the problem of learning a policy from a fixed, previously recorded, dataset without the opportunity to collect new data through interaction with the environment. This is in contrast to the typical RL setting which alternates between policy improvement and environment interaction (to acquire data for policy evaluation). In many real world domains collecting new data is laborious and costly, both in terms of experimentation time and hardware availability but also in terms of the human labour involved in supervising experiments. This is especially evident in robotics applications (see e.g.  Riedmiller et al. 2018 ;  Haarnoja et al. 2018b ;  Kalashnikov et al. 2018  for recent examples learning on robots). In these settings where gathering new data is expensive compared to the cost of learning, batch RL promises to be a powerful solution. There exist a wide class of off-policy algorithms for reinforcement learning designed to handle data generated by a behavior policy µ which might differ from π, the policy that we are interested in learning (see e.g.  Sutton & Barto (2018)  for an introduction). One might thus expect solving batch RL to be a straightforward application of these algorithms. Surprisingly, for batch RL in continuous control domains, however,  Fujimoto et al. (2018)  found that policies obtained via the naïve application of off-policy methods perform dramatically worse than the policy that was used to generate the data. This result highlights the key challenge in batch RL: we need to exhaustively exploit the information that is in the data but avoid drawing conclusions for which there is no evidence (i.e. we need to avoid over-valuing state-action sequences not present in the training data). As we will show in this paper, the problems with existing methods in the batch learning setting are further exacerbated when the provided data contains behavioral trajectories from different policies µ 1 , . . . , µ N which solve different tasks, or the same task in different ways (and thus potentially execute conflicting actions) that are not necessarily aligned with the target task that π should accomplish. We empirically show that previously suggested adaptations for off-policy learning Published as a conference paper at ICLR 2020 ( Fujimoto et al., 2018 ;  Kumar et al., 2019 ) can be led astray by behavioral patterns in the data that are consistent (i.e. policies that try to accomplish a different task or a subset of the goals for the target task) but not relevant for the task at hand. This situation is more damaging than learning from noisy or random data where the behavior policy is sub-optimal but is not predictable, i.e. the randomness is not a correlated signal that will be picked up by the learning algorithm. We propose to solve this problem by restricting our solutions to 'stay close to the relevant data'. This is done by: 1) learning a prior that gives information about which candidate policies are potentially supported by the data (while ensuring that the prior focuses on relevant trajectories), 2) enforcing the policy improvement step to stay close to the learned prior policy. We propose a policy iteration algorithm in which the prior is learned to form an advantage-weighted model of the behavior data. This prior biases the RL policy towards previously experienced actions that also have a high chance of being successful in the current task. Our method enables stable learning from conflicting data sources and we show improvements on competitive baselines in a variety of RL tasks - including standard continuous control benchmarks and multi-task learning for simulated and real-world robots. We also find that utilizing an appropriate prior is sufficient to stabilize learning; demonstrating that the policy evaluation step is implicitly stabilized when a policy iteration algorithm is used - as long as care is taken to faithfully evaluate the value function within temporal difference calculations. This results in a simpler algorithm than in previous work ( Fujimoto et al., 2018 ;  Kumar et al., 2019 ).

Section Title: BACKGROUND AND NOTATION
  BACKGROUND AND NOTATION In the following we consider the problem of reinforcement learning, modeling the environment as a markov decision process (MDP) consisting of the continuous states s ∈ S, actions a ∈ A, and transition probability distribution p(s t+1 |s t , a t ) - describing the evolution of the system dynamics over time (e.g. probability of reaching s t+1 from state s t when executing action a t ) - together with the state-visitation distribution p(s). The goal of reinforcement learning is to find a policy π(a|s) that maximizes the cumulative discounted return J(π) = E π [ ∞ t=1 γ t r(s t , a t )|a t ∼ π(·|s t ), s t+1 ∼ p(·|s t , a t ), s 1 ∼ p(·)], for the reward function r(s, a) ∈ R. We also define the state-action value function for taking action a t in state s t , and thereafter following π: Q π (s t , a t ) = E π [ ∞ i=t γ i r(s i , a i )|a i ∼ π, s i+1 ∼ p(·|s i , a i )], which we can relate to the objective J via J(π) = E s∼p(·) [E a∼π(·|s) [Q π * (s, a)]], where π * is the optimal policy. We parameterize the policy π θ (a|s) by θ but we will omit this dependency where unambiguous. In some of the experiments we will also consider a setting where we learn about multiple tasks k ∈ {1, . . . K}, each with their own reward function r k (s, a). We condition the policy and Q-function on the task index k (i.e. Q π k and π(a|s, k)), changing the objective to maximize the sum of returns across all tasks. For the batch RL setting we assume that we are given a dataset D µ containing trajectory snippets (i.e. sub-trajectories of length N ) τ = {(s 0 , a 0 ), · · · , (s T , a T )}, with τ ∈ D µ . We assume access to the reward function r for the task of interest and can evaluate it for all transitions in D µ (for example, r may be some function of s). We further assume D µ was filled, prior to training, by following a set of arbitrary N behavior policies µ 1 , · · · , µ N . Note that these behavior policies may try to accomplish the task we are interested in; or might indeed generate trajectories unrelated to the task at hand.

Section Title: A LEARNED PRIOR FOR OFFLINE OFF-POLICY RL FROM IMPERFECT DATA
  A LEARNED PRIOR FOR OFFLINE OFF-POLICY RL FROM IMPERFECT DATA To stabilize off-policy RL from batch data, we want to restrict the learned policy to those parts of the state-action space supported by the batch. In practice this means that we need to approximately restrict the policy to the support of the empirical state-conditional action distribution. This prevents the policy from taking actions for which the Q-function cannot be trained and for which it might thus give erroneous, overly optimistic values ( Fujimoto et al., 2018 ;  Kumar et al., 2019 ). In this paper we achieve this by adopting a policy iteration procedure - in which the policy is constrained in the improvement step. As in standard policy iteration ( Sutton & Barto, 2018 ), the procedure consists of two alternating steps. First, starting with a given policy π i = π θi in iteration i (with π θ0 corresponding to a randomly initialized policy distribution), we find an approximate action-value function Q πi (s, a) ≈Q(s, a; φ i ), with parameters φ (Section 3.1) (as with the policy we will drop the dependence on φ i and writeQ πi (s, a) where unambiguous). Second, we optimize for π i+1 with respect toQ πi subject to a constraint that ensures closeness to the empirical state-conditional action Published as a conference paper at ICLR 2020 distribution of the batch (Section 3.2). Iterating these steps, overall, optimizes J(π). We realize both policy evaluation and improvement via a fixed number of gradient descent steps - holding π andQ πi fixed via the use of target networks ( Mnih et al., 2015 ). We refer to Algorithm 1 for details.

Section Title: POLICY EVALUATION
  POLICY EVALUATION To learn the task action-value function in each iteration we minimize the squared temporal difference error for a given reward - note that when performing offline RL from a batch of data the reward r might be computed post-hoc and does not necessarily correspond to the reward optimized by the behavior policies µ 1 , · · · , µ N . The result after iteration i is given as We approximate the expectation required to calculateV πi (s) with M samples from π i , i.e.V πi (s) ≈ 1 M [ M j=1Q (s, a j ; φ i−1 ) | a j ∼ π i (·|s)]. As further discussed in the related work Section 4, the use of policy evaluation is different from the Q-learning approach pursued in  Fujimoto et al. (2018) ;  Kumar et al. (2019) , which requires a maximum over actions and may be more susceptible to over- estimation of Q-values. We find that when enough samples are taken (we use M = 20) and the policy is appropriately regularized (see Section 3.2) learning is stable without additional modifications.

Section Title: PRIOR LEARNING AND POLICY IMPROVEMENT
  PRIOR LEARNING AND POLICY IMPROVEMENT In the policy improvement step we solve the following constrained optimization problem π i+1 = arg max π E τ ∼Dµ E a∼π(·|s) Q πi (s, a) |s ∼ τ s.t. E τ ∼Dµ KL[π(·|s) π prior (·|s)]|s ∼ τ ≤ , (2) where D µ is the behavior data, π the policy being learned, and π prior is the prior policy. This is similar to the policy improvement step in  Abdolmaleki et al. (2018)  but instead of enforcing closeness to the previous policy here the constraint is with respect to a separately learned "prior" policy, the behavior model. The role of π prior in Equation 2 is to keep the policy close to the regime of the actions found in D µ . We consider two different ways to express this idea by learning a prior alongside the policy optimization. For learning the prior, we first consider simply modeling the raw behavior data. This is similar to the approach of BCQ and BEAR-QL ( Fujimoto et al., 2018 ;  Kumar et al., 2019 ), but we use a parametric behavior model and measure distance by KL; we refer to the related work for a discussion. The behavior model can be learned by maximizing the log likelihood of the observed data θ bm = arg max θbm E τ ∼Dµ   |τ | t=1 log π θbm (a t |s t )   , (3) where θ bm are the parameters of the behavior model prior. Regularizing towards the behavior model can help to prevent the use of unobserved actions, but it may also prevent the policy from improving over the behavior in D µ . In effect, the simple behavior prior in Equation 3 regularizes the new policy towards the empirical state-conditional action distribution in D µ . This may be acceptable for datasets dominated by successful trajectories for the task of interest or when the unsuccessful trajectories are not predictable (i.e. they correspond to random behaviour). However, we here are interested in the case where D µ is collected from imperfect data and from multiple tasks. In this case, D µ will contain a diverse set of trajectories - both (partially) successful and actively harmful for the target task. With this in mind, we consider a second learned prior, the advantage-weighted behavior model, π abm , with which we can bias the RL policy to choose actions that are both supported by D µ and also good for the current task (i.e. keep doing actions that work).

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 We can formulate this as maximizing the following objective: where f is an increasing, non-negative function, and the difference R (τ t:N ) −V πi is akin to an n-step advantage function, but here calculated off-policy representing the "advantage" of the behavior snippet over the policy π i . This objective still tries to maximize the log likelihood of observed actions, and avoids taking actions not supported by data. However, by "advantage weighting" we focus the model on "good" actions while ignoring poor actions. We let f = 1 + (the unit step function with f (x) = 1 for x ≥ 0 and 0 otherwise) both for simplicty - to keep the number of hyperparameters to a minimum while keeping the prior broad - and because it has an intuitive interpretation: such a prior will start by covering the full data and, over time, filter out trajectories that would lead to worse performance than the current policy, until it eventually converges to the best trajectory snippets contained in the data. We note that Equation 4 is similar to a policy gradient, though samples here stem from the buffer and it will thus not necessarily converge to the optimal policy in itself; π θabm will instead only cover the best trajectories in the data due to no importance weighting being performed for off-policy data. This bias is in fact desirable in a batch-RL setting; we want a broad prior that only considers actions present in the data. We also note that we tried several different functions for f including exponentiation, e.g. f (x) = exp(x), but found that choice of function did not make a significant difference in our experiments. Using either π θbm or π θabm as π prior , Equation 2 can be solved with a variety of optimization schemes. We experimented with an EM-style optimization following the derivations for the MPO algorithm ( Abdolmaleki et al., 2018 ), as well as directly using the stochastic value gradient ofQ πi wrt. policy parameters ( Heess et al., 2015 ). It should be noted that, if the prior itself is already good enough to solve the task we can learn Q πprior - e.g. if the data stems from an expert or has sufficiently high quality. In this case learning both Q πprior and π prior becomes independent of the RL policy improvement step; if we then set = 0, skipping the policy improvement step, we obtain a further simplified algorithm consisting only of learning π θabm and Q π θ abm (see Figure 9 for an ablation).

Section Title: EM-style optimization
  EM-style optimization We can optimize the objective from Equation 2 using a two-step procedure. Following ( Abdolmaleki et al., 2018 ), we first notice that the optimal π for Equation 2 can be expressed asπ(a|s) ∝ π prior (a|s) exp(Q π i (s,a) /η), where η is a temperature that depends on the used for the KL constraint and can be found automatically by a convex optimization (Appendix B.1). Conveniently, we can sample from this distribution by queryingQ πi using samples from π prior . These samples can then be used to learn the parametric policy by minimizing the divergence KL(π π θi+1 ), which is equivalent to maximizing the weighted log likelihood θ i+1 = arg max θ E τ ∼Dµ E a∼πprior(·|s) exp(Q π i (s,a) /η) log π θ (a|s)|s ∼ τ , (5) which we optimize via gradient descent subject to an additional trust-region constraint on π θ given as KL(π θi π θ ) < trust to ensure conservative updates (Appendix B.1).

Section Title: Stochastic value gradient optimization
  Stochastic value gradient optimization Alternatively, we can use Langrangian relaxation to turn Equation 2 into an objective amenable to gradient descent. Inserting π θ for π and relaxing results in (θ i+1 , η) = arg max θ min η E τ ∼Dµ E a∼π θ (·|s) Q πi (s, a) + η( − KL[π θ (·|s) π prior (·|s)]) , (6) for η > 0 and which we can optimize by alternating gradient descent steps on θ and η respectively, taking the stochastic gradient of the Q-value ( Heess et al., 2015 ) through the sampling of a ∼ π θ (·, s) via re-parameterization. See Appendix B.2 for a derivation of this gradient. Full pseudocode for our approach can be found in Appendix A, Algorithm 1.

Section Title: RELATED WORK
  RELATED WORK There exist a number of off-policy RL algorithms that have been developed since the inception of the RL paradigm (see e.g.  Sutton & Barto (2018)  for an overview). Most relevant for our work, some of these have been studied in combination with function approximators (for estimating value functions and policies) with an eye on convergence properties in the batch RL setting. In particular, several papers have theoretically analyzed the accumulation of bootstrapping errors in approximate dynamic programming ( Bertsekas & Tsitsiklis, 1996 ;  Munos, 2005 ) and approximate policy iteration ( Farahmand et al., 2010 ;  Scherrer et al., 2015 ); for the latter of which there exist well known algorithms that are stable at least with linear function approximation (see e.g.  Lagoudakis & Parr (2003) ). Work on RL with non-linear function approximators has mainly considered the "online" or "growing batch" settings, where additional exploration data is collected ( Ernst et al., 2005 ;  Riedmiller, 2005 ;  Ormoneit & Sen, 2002 ); though some success for batch RL in discrete domains has been reported (Agarwal et al., 2019). For continuous action domains, however, off-policy algorithms that are commonly used with powerful function approximators fail in the fixed batch setting. Prior work has identified the cause of these failures as extrapolation or bootstrapping errors ( Fujimoto et al., 2018 ;  Kumar et al., 2019 ) which occur due to a failure to accurately estimate Q-values, especially for state-action pairs not present in the fixed data set. Greedy exploitation of such misleading Q-values (e.g. due to a max operation) can then cause further propagation of such errors in the Bellman backup, and to inappropriate action choices during policy execution (leading to suboptimal behavior). In non-batch settings, new data gathered during exploration allows for the Q-function to be corrected. In the batch setting, however, this feedback loop is broken, and correction never occurs. To mitigate these problems, previous algorithms based on Q-learning identified two potential solutions: 1) correcting for overly optimistic Q-values in the Bellman update, and 2) restricting the policy from taking actions unlikely to occur in the data. To address 1) prior work uses a Bellman backup operator in which the max operation is replaced by a generative model of actions ( Fujimoto et al., 2018 ) which a learned policy is only allowed to minimally perturb; or via a maximum over actions sampled from a policy which is constrained to stay close to the data ( Kumar et al., 2019 ) (implemented through a constraint on the distance to a model of the empirical data, measured either in terms of maximum mean discrepancy or relative entropy). To further penalize uncertainty in the Q-values this can be combined with Clipped Double-Q learning ( Fujimoto et al., 2018 ) or an ensemble of Q-networks ( Kumar et al., 2019 ). To address 2) prior work uses a similarly constrained max also during execution, by considering only actions sampled from the perturbed generative model ( Fujimoto et al., 2018 ) or the constrained policy ( Kumar et al., 2019 ), and choosing the best among them. Our work is based on a policy iteration scheme instead of Q-learning - exchanging the max for an expectation. Thus we directly learn a parametric policy that we also use for execution. We estimate the Q-function as part of the policy evaluation step with standard TD-0 backups. We find that for an appropriately constrained policy no special treatment of the backup operator is necessary, and that it is sufficient to simply use an adequate number of samples to approximate the expectation when estimating V (see Equation 1). The only modification required is in the policy improvement step where we constrain the policy to remain close to the adaptive prior in Equation 2. As we demonstrate in the empirical evaluation it is the particular nature of the adaptive prior - which can adapt to the task at hand (see Equation 4) - that makes this constraint work well. Additional measures to account for uncertainty in the Q values could also be integrated into our policy evaluation step but we did not find it to be necessary for this work; we thus forego this in favor of our simpler procedure.  Kakade & Langford (2002) . Here we take a slightly different perspective: we enforce a trust region constraint not on the last policy in the policy optimization loop (conservative updates) but wrt. the advantage weighted behavior distribution.

Section Title: EXPERIMENTS
  EXPERIMENTS We experiment with continuous control tasks in two different settings. In a first set of experiments we compare our algorithm to strong off-policy baselines on tasks from the DeepMind control suite ( Tassa et al., 2018 ) - to give a reference point as to how our algorithm performs on common benchmarks. We then turn to the more challenging setting of learning multiple tasks involving manipulation of blocks using a robot arm in simulation. These span tasks from reaching toward a block to stacking one block on top of another. Finally, we experiment with analogous tasks on a real robot. We use the same networks for all algorithms that we compare, optimize parameters using Adam ( Kingma & Ba, 2015 ), and utilize proprioceptive features (e.g. joint positions / velocities) together with task relevant information (mujoco state for the control suite, and position/velocity estimates of the blocks for the manipulation tasks). All algorithms were implemented in the same framework, including our reproduction of BCQ and BEAR, and differ only in their update rules. Note that for BEAR we use a KL instead of the MMD as we found this to work well, see appendix. In the multi-task setting (Section 5.1) we learn a task conditional policy π θ (a|s, k) and Q-functionQ π φ (s, a, k) where k is a one-hot encoding of the task identifier, that is provided as an additional network input. We refer to the appendix for additional details.

Section Title: CONTROL SUITE EXPERIMENTS
  CONTROL SUITE EXPERIMENTS We start by performing experiments on four tasks from the DeepMind control suite: Cheetah, Hopper, Quadruped. To obtain data for the offline learning experiments we first generate a fixed dataset via a standard learning run using MPO, storing all transitions generated; we repeat this with 5 seeds for each environment. We then separate this collected data into two sets: for experiments in the high data regime, we use the first 10,000 episodes generated from each seed. For experiments with low-quality data we use the first 2,000 episodes from each seed. The high data regime therefore has both more data and data from policies which are of higher quality on average. A plot showing the performance of the initial training seeds over episodes is given in the appendix, Figure 6. We experiment with a Sawyer robot arm simulated in Mujoco ( Todorov et al., 2012 ) in a multi-task setting - as described above. The seven tasks are to manipulate blocks that are placed in the workspace of the robot. They include: reaching for the green block (Reach), grasping any block (Grasp), lifting the green block (Lift), hovering the green block over the yellow block (Place Wide), hovering the green block over the center of the yellow block (Place Narrow), stacking the green block on top of 1 Due to memory constraints, it can be prohibitive to store the full dataset in replay. To circumvent this problem we run a set of "restorers", which read data from disk and add it back into the replay in a loop. yellow (Stack and Stack/Leave i.e. without gripper contact). To generate the data for this experiment we again run MPO - here simultaneously learning all task-conditional policies for the full seven tasks. Data-was collected by randomly switching tasks after each episode (of 200 control steps) with random resets of the robot position every 20 episodes. As before, data from all executed tasks is collected in one big data-set annotating each trajectory snippet with all rewards (i.e. this is similar to the SAC-R setting from ( Riedmiller et al., 2018 ). During offline learning we then compare the performance of MPO and RL with a behavior modelling prior (BM+MPO and ABM+MPO). As shown in  Figure 3 , behavioral modelling priors improve performance across all tasks over standard MPO - which struggles in these more challenging tasks. This is likely due to the sequential nature of the tasks: later tasks implicitly include earlier tasks but only a smaller fraction of trajectories achieve success on stack and leave (and the actions needed for stack conflict, e.g., with lifting), this causes the BM prior to be overly broad (see plots in appendix). The ABM+MPO, on the other hand, achieves high performance across all tasks. Interestingly, even with ABM in place, the RL policy learned using this prior still outperforms the prior, demonstrating that RL is still useful in this setting. As an additional experiment we test whether we can learn new tasks entirely from previously recorded data. Since our rewards are specified as functions of observations, we compute rewards for two new tasks (bringing the green block to the center and bringing it to the corner) for the entire dataset - we then test the resulting policy in the simulator. As depicted in  Figure 4 , this is successful with ABM+MPO, demonstrating that we can learn tasks which were not originally executed in the dataset (as long as trajectory snippets that lead to successful task execution are contained in the data).

Section Title: REAL ROBOT EXPERIMENTS
  REAL ROBOT EXPERIMENTS Finally, to validate that our approach is a feasible solution to performing fast learning for real-robot experiments, we perform an experiment using a real Sawyer arm and the same set of seven tasks (implemented on the real robot) from Section 5.2. As before, data from all executed tasks is collected in one big data-set annotating each trajectory snippet with all rewards. The full buffer after about two weeks of real robot training is used as the data for offline learning; which we here only performed with ABM+MPO due to the costly evaluation. The goal is to re-learn all seven original tasks.  Figure 5  shows the results of this experiment - we ran an evaluation script on the robot, continuously testing the offline learned policy, and stopped when there was no improvement in average reward (as measured over a window of 50 episodes). As can be seen, ABM with MPO as the optimizer manages to reliably re-learn all seven tasks purely from the logged data in less than 12 hours. All tasks can jointly be learned with only small differences in convergence time - while during the initial training run the harder tasks, of course, took the most time to learn. This suggests that gathering large data-sets of experience from previous robot learning experiments, and then quickly extracting the skills of interest, might be a viable strategy for making progress in robotics.

Section Title: CONCLUSION
  CONCLUSION In this work, we considered the problem of stable learning from logged experience with off-policy RL algorithms. Our approach consists of using a learned prior that models the behavior distribution contained in the data (the advantage weighted behavior model) towards which the policy of an RL algorithm is regularized. This allows us to avoid drawing conclusions for which there is no evidence in the data. Our approach is robust to large amounts of sub-optimal data, and compares favourably to strong baselines on standard continuous control benchmarks. We further demonstrate that our approach can work in challenging robot manipulation domains - learning some tasks without ever seeing a single trajectory for them.

```
