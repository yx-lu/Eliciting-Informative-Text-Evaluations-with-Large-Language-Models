<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 ERGODIC INFERENCE: ACCELERATE CONVERGENCE BY OPTIMISATION</article-title></title-group><abstract><p>Statistical inference methods are fundamentally important in machine learning. Most state-of-the-art inference algorithms are variants of Markov chain Monte Carlo (MCMC) or variational inference (VI). However, both methods struggle with limitations in practice: MCMC methods can be computationally demanding; VI methods may have large bias. In this work, we aim to improve upon MCMC and VI by a novel hybrid method based on the idea of reducing simulation bias of finite-length MCMC chains using gradient-based optimisation. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. We show that our method produces promising results on popular benchmarks when compared to recent hybrid methods of MCMC and VI.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Statistical inference methods in machine learning are dominated by two approaches: simulation and optimisation. Markov chain Monte Carlo (MCMC) is a well-known simulation-based method, which promises asymptotically unbiased samples from arbitrary distributions at the cost of expensive Markov simulations. Variational inference (VI) is a well-known method using optimisation, which fits a parametric approximation to the target distribution. VI is biased but offers a computationally efficient generation of approximate samples.</p><p>There is a recent trend of hybrid methods of MCMC and VI to achieve a better balance between computational efficiency and bias. Hybrid methods often use MCMC or VI as an algorithmic component of the other. In particular, <xref ref-type="bibr" rid="b0">Salimans et al. (2015)</xref> proposed a promising modified VI method that reduces approximation bias by using MCMC transition kernels. Another technique reduces the computational complexity of MCMC by initialising the Markov simulation from a pre- trained variational approximation (<xref ref-type="bibr" rid="b7">Hoffman, 2017</xref>; <xref ref-type="bibr" rid="b6">Han et al., 2017</xref>). <xref ref-type="bibr" rid="b0">Levy et al. (2018)</xref> proposed to improve MCMC using flexible non-linear transformations given by neural networks and gradient- based auto-tuning strategies.</p><p>In this work, we propose a novel hybrid method, called ergodic inference (EI). EI improves over both MCMC and VI by tuning the hyper-parameters of a flexible finite-step MCMC chain so that its last state sampling distribution converges fast to a target distribution. EI optimises a tractable objective function which only requires to evaluate the logarithm of the unnormalized target density. Furthermore, unlike in traditional MCMC methods, the samples generated by EI from the last state of the MCMC chain are independent and have no correlations. EI offers an appealing option to balance computational complexity vs. bias on popular benchmarks in machine learning. Compared with previous hybrid methods, EI has following advantages:</p><p>&#8226; EI's hyperparameter tuning produces sampling distributions with lower approximation bias.</p><p>&#8226; The bias is guaranteed to decrease as the length of the MCMC chain increases.</p><p>&#8226; By stopping gradient computations, EI has less computational cost than related baselines.</p><p>We also state some disadvantages of our method:</p><p>&#8226; The initial state distribution in EI's MCMC chain has to have higher entropy than the target. Under review as a conference paper at ICLR 2020</p><p>&#8226; The computational complexity per simulated sample of EI is in general higher than in VI.</p></sec><sec><title>BACKGROUND</title></sec><sec><title>MONTE CARLO STATISTICAL INFERENCE</title><p>Monte Carlo (MC) statistical inference approximates expectations under a given distribution using simulated samples. Given a target distribution &#960;, MC estimations of an expectation E &#960; [f (x)] are defined as empirical average of the evaluation of f on samples from &#960;. To generate samples from &#960;, we assume that the unnormalized density function &#960; * (x) can be easily computed. In a Bayesian setting we typically work with &#960; * (x|y) given by the product of the prior p(x) and the likelihood p(y|x), where y denotes observed variables and x denotes the model parameters specifying p(y|x).</p></sec><sec><title>MARKOV CHAIN MONTE CARLO</title><p>Markov chain Monte Carlo (MCMC) casts inference as simulation of ergodic Markov chains that converge to the target &#960;. The MCMC kernel M (x |x) is characterised by the detailed balance (DB) property: &#960;(x)M (x |x) = &#960;(x )M (x|x ). Given an unnormalised target density &#960; * , an MCMC kernel can be constructed in three steps: first, sample an auxiliary random variable r from an auxiliary distribution q &#966;1 with parameters &#966; 1 ; second, create a new candidate sample as (x , r ) = f &#966;2 (x t&#8722;1 , r), where f &#966;2 is a deterministic function with parameters &#966; 2 ; finally, accept the proposal as x t = x with probability p MH = min {0, &#960; * (x )q &#966;1 (r )/[&#960; * (x t&#8722;1 )q &#966;1 (r)]}, otherwise duplicate the previous sample as x t = x t&#8722;1 . The last step is well known in the literature as the Metropolis-Hastings (M-H) correction step (<xref ref-type="bibr" rid="b0">Robert &amp; Casella, 2005</xref>) and it results in MCMC kernels that satisfy the DB condition. In the following, we denote the joint MCMC parameters (&#966; 1 , &#966; 2 ) by &#966;. If f &#966;2 does not preserve volume, then it requires a Jacobian correction factor in the ratio in p M H .</p><p>Hamiltonian Monte Carlo (HMC) is a successful MCMC method which has drawn great attention. A few recent works based on this method are <xref ref-type="bibr" rid="b0">Salimans et al. (2015)</xref>; <xref ref-type="bibr" rid="b7">Hoffman (2017)</xref>; <xref ref-type="bibr" rid="b0">Levy et al. (2018)</xref>. In HMC, the auxiliary distribution q &#966;1 is often chosen to be Gaussian with zero-mean and a constant diagonal covariance matrix specified by &#966; 1 . The most common f &#966;2 in HMC is a numeric integrator called the leapfrog algorithm, which simulates Hamiltonian dynamics defined by log &#960; * (<xref ref-type="bibr" rid="b0">Neal, 2010</xref>). The leapfrog integrator requires the gradient of log &#960; * and a step size parameter given by &#966; 2 . Given any initial state x 0 , MCMC can generate asymptotically unbiased samples x 1:n . For this, MCMC iteratively simulates the next sample x t through the application of the MCMC transition kernel to the previous sample x t&#8722;1 . It is well known in the literature that MCMC is computationally demanding (<xref ref-type="bibr" rid="b0">MacKay, 2002</xref>; <xref ref-type="bibr" rid="b1">Bishop, 2006</xref>). In particular, it is often necessary to run sufficiently long burn-in MCMC simulations to reduce simulation bias. Another drawback of MCMC is sample correlation, which increases the variance of the MC estimator (<xref ref-type="bibr" rid="b0">Neal, 2010</xref>). To avoid strong sample correlation, the common practice in MCMC to tune hyper-parameters manually using sample quality metrics like effective sample size, (<xref ref-type="bibr" rid="b7">Hoffman, 2017</xref>; <xref ref-type="bibr" rid="b0">Robert &amp; Casella, 2005</xref>), which has been developed into automated gradient-based tuning strategies in recent work (<xref ref-type="bibr" rid="b0">Levy et al., 2018</xref>).</p></sec><sec><title>MC ESTIMATION USING VARIATIONAL INFERENCE</title><p>Variational inference (VI) is a popular alternative to MCMC for generating approximate samples from &#960;. Unlike MCMC reducing sample bias by long burn-in simulation, VI casts the sample bias reduction as an optimisation problem, where a parametric approximate sampling distribution P is fit to the target &#960;. In particular, VI optimises the evidence lower bound (ELBO) given by L ELBO (P &#960; * ) = E p [log &#960; * (x)] + H(P ) , (1) where H(P ) = &#8722;E p [log p(x)], also known as the entropy, must be tractable to compute. L ELBO (P T &#960; * ) is a lower bound on the log normalising constant log Z = log &#960; * (x) dx. This bound is tight when P = &#960;. therefore, the approximation bias in VI can be defined as the gap between L ELBO (P &#960; * ) and log Z, that is, &#8710; bias (P ) = log Z &#8722; L ELBO (P &#960;) = D KL (P &#960;) &#8805; 0 , (2) where D KL (P &#960;) denotes the Kullback-Leibler (KL) divergence.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>Variational approximations often belong to simple parametric families like the multivariate Gaussian distribution with diagonal covariance matrix. This results in computationally efficient algorithms for bias reduction and sample generation, but may also produce highly biased samples in cases of over-simplified approximation that ignores correlation. Designing variational approximation to achieve low bias under the constraint of tractable entropy and efficient sampling procedures is possible using flexible distributions parameterised by neural networks (NNs) (<xref ref-type="bibr" rid="b0">Rezende &amp; Mohamed, 2015</xref>; <xref ref-type="bibr" rid="b4">Kingma et al., 2016</xref>). However, how to design such NNs for VI is still a research challenge.</p></sec><sec><title>HYBRID METHODS AND VARIANTS OF MCMC AND VI</title><p>The balance between computational efficiency and bias is a challenge at the heart of all inference methods. MCMC represents a family of simulation-based methods that guarantee low-bias samples at cost of expensive simulations; VI represents a family of optimisation-based methods that generate high-bias samples at a low computational cost.</p><p>Many recent works seek a better balance between efficiency and bias by combining MCMC and VI. <xref ref-type="bibr" rid="b0">Salimans et al. (2015)</xref> proposed to reduce variational bias by optimising an ELBO specified in terms of the tractable joint density of short MCMC chains. The idea seems initially promising, but the proposed ELBO becomes looser and looser as the chain grows longer. <xref ref-type="bibr" rid="b3">Caterini et al. (2018)</xref> construct an alternative ELBO for HMC that still has problems since the auxiliary momentum variables are sampled only once at the beginning of the chain, which reduces the empirical performance of HMC. Inspired by contrastive divergence, <xref ref-type="bibr" rid="b0">Ruiz &amp; Titsias (2019)</xref> proposed a novel variational objective function to optimise variational parameters by adding additional term that minimise the KL between a MCMC distribution and variational approximation to reduce variational bias. <xref ref-type="bibr" rid="b7">Hoffman (2017)</xref> and <xref ref-type="bibr" rid="b6">Han et al. (2017)</xref> proposed to replace expensive burn-in simulations in MCMC with samples from pre-trained variational approximations. This approach is effective at finding good initial proposal distributions. However, it does not offer a solution for tuning HMC parameters (<xref ref-type="bibr" rid="b7">Hoffman, 2017</xref>), which are critical for good empirical performance.</p><p>Another line of research has focused on improving inference using flexible distributions, which are transformed from simple parametric distributions by non-linear non-volume preserving (NVP) functions. <xref ref-type="bibr" rid="b0">Levy et al. (2018)</xref> proposed to tune NVP parameterised MCMC w.r.t. a variant of the expected squared jumped distance (ESJD) loss proposed by <xref ref-type="bibr" rid="b0">Pasarica &amp; Gelman (2010)</xref>. <xref ref-type="bibr" rid="b6">Song et al. (2017)</xref> proposed a similar auto-tuning for NVP parameterised MCMC using an adversarial loss.</p></sec><sec><title>ERGODIC INFERENCE</title><p>Ergodic inference (EI) is motivated by the well-known convergence of MCMC chains (<xref ref-type="bibr" rid="b0">Robert &amp; Casella, 2005</xref>): MCMC chains converge in terms of the total variation (TV) distance between the marginal distribution of the MCMC chain and the target &#960;. Inspired by the convergence property of MCMC chains, we define an ergodic approximation P T to &#960; with T MCMC steps as following. Given a parametric distribution P 0 with tractable density p 0 (x 0 ; &#966; 0 ) parameterlized by &#966; 0 and an MCMC kernel M (x |x; &#966;) constructed using the unnormalised target density &#960; * and with MCMC hyperparameter &#966;, an ergodic approximation of &#960; is the marginal distribution of the final state of an T -step MCMC chain initialized from P 0 :</p><p>We call &#966; 0 and &#966; the ergodic parameters of P T . Well known in MCMC literature like (<xref ref-type="bibr" rid="b0">Robert &amp; Casella, 2005</xref>; <xref ref-type="bibr" rid="b0">Murray &amp; Salakhutdinov, 2008</xref>), the ergodic approximation p T converges to &#960; after every MCMC transition and with sufficiently long chain p T is guaranteed to be arbitrarily close to &#960; with arbitrary &#966; and &#966; 0 .</p><p>It is important to clarify that ergodic approximation is different from the modified variational methods like (<xref ref-type="bibr" rid="b0">Ruiz &amp; Titsias, 2019</xref>) which only optimise the variational parameters &#966; 0 , but the optimisation objective functions involve MCMC similation. In the following section, we show how EI can tune the ergodic parameters to minimise the bias of P T as an approximation to the target &#960; with finite T . Under review as a conference paper at ICLR 2020</p></sec><sec><title>ERGODIC APPROXIMATION OBJECTIVE</title><p>To reduce the KL divergence D KL (P T &#960;), one could tune the burn-in parameter &#966; 0 and the MCMC parameter &#966; by minimizing equation 2. However, this is infeasible because we cannot analytically evaluate p T in equation 3. Instead, we exploit the convergence of ergodic Markov chains and propose to optimise an alternative objective as the following constrained optimisation problem: max &#966;0,&#966; E p T [log &#960; * (x)] + L ELBO (P 0 &#960; * ) &#8801; L(&#966; 0 , &#966;, &#960; * ) (4) subject to H(P 0 ) &gt; h , (5) where h is a hyperparameter that should be close to the entropy of the target, that is, h &#8776; H(&#960;). We call the objective in equation 4 the ergodic modified lower bound (EMLBO), denoted by L(&#966; 0 , &#966;, &#960; * ). Note that the EMLBO is similar to L ELBO (P T &#960; * ), with the intractable entropy H(P T ) replaced by the tractable L ELBO (P 0 &#960; * ). We now give some motivation for this constrained objective. First, we explain the inclusion of the term L ELBO (P 0 &#960; * ) in equation 4 and its connection to H(P T ). If we maximised only the first term E p [log &#960; * (x)] with respect to a fully flexible distribution P , the result would be a point probability mass at the mode of the target &#960;. This degenerate solution is avoided in VI by optimising the sum of E p [log &#960; * (x)] and the entropy term H(P ), which enforces P to move away from a point probability mass. However, H(P T ) is intractable in ergodic approximation. Fotunately, we notice that maximising the term L ELBO (P 0 &#960; * ) = E p0 [log &#960; * (x)] + H(P 0 ) has similar effect of maximising H(P T ) for preventing P 0 from collapsing to the mode of &#960;. It is easy to show that P T cannot be a delta unless H(P T ) = &#8722;&#8734;, which also implies L ELBO (P T &#960; * ) does not exist. Since the KL divergence D KL (P t &#960;) never increases after each MCMC transition step (<xref ref-type="bibr" rid="b0">Murray &amp; Salakhutdinov, 2008</xref>), L ELBO (P t &#960; * ) &#8805; L ELBO (P t&#8722;1 &#960; * ) &#8805; L ELBO (P 0 &#960; * ), by maximising L ELBO (P 0 &#960; * ), we also maximise L ELBO (P T &#960; * ), which implies L ELBO (P T &#960; * ) must exist.</p><p>The constraint in equation 5 is necessary to eliminate the following pathology. If P 0 does not satisfy H(P 0 ) &gt; H(&#960;), then E p0 [log &#960; * (x)] could be higher than E &#960; [log &#960; * (x)]. When this happens, if we maximise E p T [log &#960; * (x)], we will favor P T to stay close to P 0 instead of making it converge to &#960; faster. This is illustrated by the plot in the right part of <xref ref-type="fig" rid="fig_1">Figure 2</xref>. To avoid this pathological case, note that</p><p>equation 4 is expected to accelerate convergence.</p><p>It is interesting to compare the EMLBO with the objective function optimised by <xref ref-type="bibr" rid="b0">Salimans et al. (2015)</xref>, that is, the ELBO given by</p><p>where p(x 0:T &#8722;1 |x T ) denotes the conditional density of the first T states of the MCMC chain given the last one x T and r(x 0:T &#8722;1 |x T ) is an auxiliary variational distribution that approximates p(x 0:T &#8722;1 |x T ). Note that the negative KL term in equation 6 will increase as T increases. This makes the ELBO in equation 6 become looser and looser as the chain length increases. In this case, the optimisation of equation 6 results in an MCMC sampler that fits well the biased inverse model r(x 0:T &#8722;1 |x T ) but whose marginal distribution for x T does not approximate &#960; well. This limits the effectiveness of this method in chains with multiple MCMC transitions. By contrast, the EMLBO does not have this problem and its optimisation will produce a more and more accurate P T as T increases.</p><p>EI combines the benefits of MCMC and VI and avoids their drawbacks, as shown in <xref ref-type="table" rid="tab_0">Table 1</xref>. In particular, the bias in EI is reduced by using longer chains, as in MCMC, and EI generates independent samples, as in VI. Futhermore, EI optimises an objective that directly quantifies the bias of the generated samples, as in VI. Methods for tuning MCMC do not satisfy the latter and optimise instead indirect proxies for mixing speed, e.g. expected squared jumped distance (<xref ref-type="bibr" rid="b0">Levy et al., 2018</xref>). Importantly, EI can use gradients to tune different MCMC parameters at each step of the chain, as suggested by <xref ref-type="bibr" rid="b0">Salimans et al. (2015)</xref>. This gives EI an extra flexibility which existing MCMC methods do not have. Finally, EI is different from parallel-chain MCMC: while EI generates independent samples, parallel-chain MCMC draws correlated samples from several chains running in parallel.</p></sec><sec><title>STOCHASTIC GRADIENT OPTIMISATION FOR THE ERGODIC OBJECTIVE</title><p>We now show how to maximise the ergodic objective using gradient-based optimisation. The gradient &#8706; &#966;0,&#966; L(&#966; 0 , &#966;, &#960; * ) is equal to the sum of two gradient terms. The first one &#8706; &#966;0 L ELBO (P 0 &#960; * ) is Under review as a conference paper at ICLR 2020 affected by the constraint H(P 0 ) &gt; h, while the second term &#8706; &#966; E p T [log &#960; * (x T )] is not. If we ignore the constraint, the first gradient term can be estimated by Monte Carlo using the reparameterization trick proposed in (<xref ref-type="bibr" rid="b4">D.P. Kingma, 2014</xref>; <xref ref-type="bibr" rid="b0">Rezende &amp; Mohamed, 2015</xref>): &#8706; &#966;0 L ELBO (P 0 &#960; * ) &#8776; 1 N N i=1 &#8706; &#966;0 log &#960; * (f &#966;0 ( i )) + &#8706; &#966;0 H(P 0 ) , (7) where f &#966;0 (&#183;) is a deterministic function that maps the random variable i sampled from a simple distribution, e.g. a factorized standard Gaussian, into the random variable x i 0 sampled from p 0 (&#183;; &#966; 0 ). To guarantee that our gradient-based optimiser yields a solution satisfying the constraint, we first initialize &#966; 0 so that H(P 0 ) &gt; h and, afterwards, we force the gradient descent optimiser to leave &#966; 0 unchanged if H(P 0 ) is to get lower than h during the optimisation process.</p><p>The Monte Carlo estimation of &#8706; &#966; E p T [log &#960; * (x)] can also be computed using the reparameterization trick. For this, the Metropolis-Hastings (M-H) correction step in the MCMC transitions, as described in Section 2.2, can be reformulated as applying the following transformation to x t&#8722;1 :</p><p>where (x , r ) = f &#966; (x t&#8722;1 , r t ) as described in Section 2.2, r t &#8764; q(r), u t &#8764; Unif(0, 1), p MH = min {1, &#960; * (x )q(r )/[&#960; * (x t&#8722;1 )q(r t )]} and 1(p MH ; u) is an indicator function that takes value one if p MH &gt; u and zero otherwise. In Hamiltonian Monte Carlo (HMC), f &#966; is the leapfrog integrator of Hamiltonian dynamics with the leapfrog step size &#966;. We define the T -times composition of g &#966; , given in equation 8, as the transformation x T = g T &#966; (x 0 , r 1:T ; u 1:T ). Then, the second gradient term can be estimated by Monte Carlo as follows:</p><p>1:T are sampled independently from p 0 (x 0 ) T t=1 q(r t )Unif(u t ; 0, 1). Note that the gradient term equation 9 is correct under the assumption f &#966; is volume-preserving in the joint space of (x t&#8722;1 , r t ), otherwise additional gradient term of the Jacobian of f &#966; w.r.t. &#966; is required. However, it is not a concern for many popular MCMC kernels. For example, the leapfrog integrator in HMC f &#966; guarantees the preservation of volume as shown in (<xref ref-type="bibr" rid="b0">Neal, 2010</xref>). It is worth to mention that the indicator function in equation 8 is not continuous but differentiable almost everywhere. Therefore, the gradient in equation 9 can be computed conveniently using standard autodifferentiation tools.</p><p>The gradient in equation 9 requires computing &#8706; &#966; g T &#966; (x 0 , r 1:T ; u 1:T ), which can be done easily by using auto-differentiation and gradient backpropagation through the transfromations g &#966; (&#183;, r t ; u t ) with t = T, . . . , 1. However, backpropagation in deep compositions can be computationally demanding. We discovered a trick to accelerate the gradient computation by stopping the backpropagation of the gradient at the input x t&#8722;1 of g &#966; (x t&#8722;1 , r t ; u t ), for t = 1, . . . , T . Empirically this trick has almost no impact on the convergence speed of the ergodic approximation, as shown in <xref ref-type="fig" rid="fig_1">Figure 2</xref>.</p></sec><sec><title>THE ENTROPY CONSTRAINT AND HYPERPARAMETER TUNING</title><p>As mentioned previously, ignoring the constraint H(P 0 ) &gt; H(&#960;) may lead to pathological results when optimising the ergodic objective. To illustrate this, we consider fitting an ergodic approximation given by a Hamilton Monte Carlo (HMC) transition kernel with T = 9. P 9 denotes the initial ergodic approximation before traing and P * 9 denotes the same approximation after training. The target distribution is a correlated bivariate Gaussian given by &#960; = N (0, (2.0, 1.5; 1.5; 1.6)). Samples Under review as a conference paper at ICLR 2020 from this distribution are shown in plot (a) in <xref ref-type="fig" rid="fig_0">Figure 1</xref>. We optimise different a separate HMC parameter &#966; t , as described in Section 2.2, for each HMC step t. We consider two initial distributions. The first one is P 0 = N (0, 3I) which satisfies the assumption H(P 0 ) &gt; H(&#960;). The second one is P 0 = N (0, I) with the entropy H(P 0 ) &lt; H(&#960;), which violates the assumption. In this latter case, we perform the unconstrained optimisation of equation 4. Plots (b) and (c) in <xref ref-type="fig" rid="fig_0">Figure 1</xref> show samples from P 9 and P * 9 for the valid P 0 . In this first example, maximising the ergodic objective under equation 5 significantly accelerates the chain convergence as further shown by the left plot in <xref ref-type="fig" rid="fig_1">Figure 2</xref>. Plots (d) and (e) in <xref ref-type="fig" rid="fig_0">Figure 1</xref> show samples from P 9 and P * 9 for the invalid initial distribution P 0 . In the second example, E p0 [log &#960; * (x)] is higher than E &#960; [log &#960; * (x)] and, consequently, maximising the unconstrained ergodic objective actually deteriorates the quality of the resulting approximation. This is further illustrated by the right plot in <xref ref-type="fig" rid="fig_1">Figure 2</xref> which shows how the convergence of E pt [log &#960; * (x)] to E &#960; [log &#960; * (x)] is significantly slowed down by the optimisation under the invalid P 0 . Fortunately, it is straightforward to prevent this type of failure cases by appropriately tuning the scalar hyperparameter h in equation 5. A value of h that is too low may result in higher bias of P T after optimisation as illustrated by the convergence of E pt [log &#960; * (x)] in the blue and orange curves in Plot (b) in <xref ref-type="fig" rid="fig_1">Figure 2</xref>. Furthermore, in many cases, estimating an upper bound on H(&#960;) is feasible. For example, in Bayesian inference, the entropy of the prior distribution p(x) is often higher than the entropy of the posterior p(x|y). Therefore, the prior entropy can be used as a reference for tuning h.</p></sec><sec><title>EXPERIMENTS</title><p>We first describe the general configuration of the ergodic inference method used in our experiments. Our ergodic approximation is constructed using HMC, one of the most successful MCMC methods in machine learning literature. We use T HMC transitions, each one involving 5 steps of the vanilla leapfrog integrator which was implemented following <xref ref-type="bibr" rid="b0">Neal (2010)</xref>. The leapfrog pseudocode can be found in the appendix. In each HMC transition, the auxiliary variables are sampled from a zero-mean Gaussian distribution with diagonal covariance matrix. We tune the following HMC parameters: the variance of the auxiliary variables and the leapfrog step size, as mentioned in Section 2.2. We use and optimise a different value of the HMC parameters for each of the T HMC transitions considered. We call our ergodic inference method Hamiltonian ergodic inference (HEI). The burn-in model P 0 is Under review as a conference paper at ICLR 2020 factorized Gaussian. The initial entropy of P 0 is chosen to be the same as the entropy of the prior. The stocastic optimisation algorithm is Adam (<xref ref-type="bibr" rid="b4">Kingma &amp; Ba, 2015</xref>) with TensorFlow implemtation <xref ref-type="bibr" rid="b0">Abadi et al. (2015)</xref> and the optimiser hyperparameter setting is (&#946; 1 = 0.9, &#946; 2 = 0.999, = 10 &#8722;8 ). The initial HMC leapfrog step sizes are sampled uniformly between 0.01 and 0.025. Additional experiment on Bayesian neural networks is included in Appendix 6.3.</p></sec><sec><title>DEMONSTRATIONS</title><p>We first compare Hamiltonian ergodic inference (HEI) with previous related methods on 6 synthetic bivariate benchmark distributions. Histograms of ground truth samples from each target distribution using rejection sampling are shown in <xref ref-type="fig" rid="fig_2">Figure 3</xref>. The baselines considered include: 1) Hamiltonian variational inference (HVI) (<xref ref-type="bibr" rid="b0">Salimans et al., 2015</xref>); 2) generalized Hamiltonian Monte Carlo (GHMC) using an NVP parameterized HMC kernel and gradient-based auto-tuning of MCMC parameters w.r.t. sample correlation loss (<xref ref-type="bibr" rid="b0">Levy et al., 2018</xref>); 3) Hamiltonian annealed importance sampling (HAIS) (<xref ref-type="bibr" rid="b0">Sohl-Dickstein &amp; Culpepper, 2012</xref>). It is worth to mention that we do not consider other hybrid inference methods like (<xref ref-type="bibr" rid="b0">Ruiz &amp; Titsias, 2019</xref>; <xref ref-type="bibr" rid="b7">Hoffman, 2017</xref>) in our experiment, because these methods only combines MCMC simulation with VI but not optimise the parameters of MCMC kernel using the gradient-based approach like EI.</p><p>HVI is the most similar method to HEI among all three baselines, because both HEI and HVI methods generate samples from the last state of MCMC chains and use gradient-based MCMC hyperparameter tuning to reduce bias. For a fair comparison between HVI and HEI, we consider the HMC chains with exactly the same setting in both methods: the initial state follows a standard Gaussian distribution and the length of HMC chain is T = 10. The key difference between HVI and HEI is the hyperparameter tuning objective, as mentioned in Section 3.1. We trained HVI for 1000 iterations and verified the ELBO converges to a (local) minimum (plots of the training ELBO values are included in Appendix 6.2). We trained HEI for 50 iterations. Following the setting of HAIS by <xref ref-type="bibr" rid="b6">Wu et al. (2017)</xref>, we used 1,000 intermediate distributions with 5 leapfrog steps per HMC transition and manually tuned the HMC parameters to have acceptance rate around 70%. GHMC 1 was run using 100 parallel chains with 5 leapfrog steps per GHMC transition, 100 burn-in steps and 1000 auto-tuned training iterations <xref ref-type="bibr" rid="b0">Levy et al. (2018)</xref>. The verification of the convergence of E p T [log &#960; * (x)] to E &#960; [log &#960; * (x)] for HEI is shown in plot (a) of <xref ref-type="fig" rid="fig_4">Figure 5</xref>.</p><p>We generate 100,000 samples with each method and evaluate sample quality using two metrics: 1) the histogram of simulated samples for visual inspection; 2) the MC estimation of E &#960; [log &#960; * (x)]. Effective sample size (ESS) is a popular sample correlation based evaluation metric in recent MCMC literature (<xref ref-type="bibr" rid="b0">Levy et al., 2018</xref>). However, we do not consider ESS in this experiment, because GHMC is the only method among all methods generating correlated samples. Therefore, the ESS of GHMC is guaranteed to be lower than HVI and HEI. To generate ground truth samples from benchmark distributions, we use . The resulting sample histograms of the ground truth using rejection sampling are shown in <xref ref-type="fig" rid="fig_2">figures 3</xref> and considered approximated sampling methods are shown in 4. <xref ref-type="table" rid="tab_2">Table 2</xref> shows the resulting estimates of &#8722;E &#960; [log &#960; * (x)] together with the wall-clock simulation time for generating 100,000 samples. The left part of <xref ref-type="table" rid="tab_3">Table 3</xref> shows the training time of the MCMC parameter optimisation for all methods except HAIS, which does not support gradient-based HMC hyperparameter tuning. HEI is faster than HVI and GHMC. Note, however, that the acceleration of HEI over HSVI is due to the stopping gradient trick described in Section 3.2. The histograms and the estimates of &#8722;E &#960; [log &#960; * (x)] generated by HEI are consistent with the results of the more expensive unbiased samplers GHMC and HAIS, which are close to the ground truth. By contrast, HVI exhibits a clear bias in all benchmarks. Regarding the sampling time, HVI and HEI simulate HMC chains with the same length and, consequently, perform similarly in this case while sample simulation from HAIS and GHMC is much more expensive.</p></sec><sec><title>TRAINING DEEP GENERATIVE MODELS</title><p>We now evaluate HEI in the task of training deep generative models. MNIST is a standard benchmark problem in this case with 60,000 grey level 28 &#215; 28 images of handwritten digits. For fair comparison with previous works, we use the 10,000 prebinarised MNIST test images 2 used by <xref ref-type="bibr" rid="b2">Burda et al. (2015)</xref>. The architecture of the generative model considered follows the deconvolutional network from <xref ref-type="bibr" rid="b0">Salimans et al. (2015)</xref>. In particular, the unnormalised target p &#952; (x, y) consists of 32 dimensional latent variables x with Gaussian prior p(x) = N (0, I) and a deconvolutional network p &#952; (y|x) from top to bottom including a single fully-connected layer with 500 RELU hidden units, then three deconvolutional layers with 5 &#215; 5 filters, (16, 32, 32) feature maps, RELU activations and a logistic output layer. We consider a baseline given by a standard VAE with a factorised Gaussian approximate Under review as a conference paper at ICLR 2020 posterior generated by an encoder network q(x|y) which mirrors the architecture of the decoder (<xref ref-type="bibr" rid="b0">Salimans et al., 2015</xref>).</p><p>The code for HVI <xref ref-type="bibr" rid="b0">Salimans et al. (2015)</xref> is not publicly available. Nevertheless, we reimplemented their convolutional VAE and were able to reproduce the marginal likelihood reported by <xref ref-type="bibr" rid="b0">Salimans et al. (2015)</xref>, as shown in <xref ref-type="table" rid="tab_1">Table 4</xref>. This verifies that our implementation of the generation network is correct. We implemented HVI in (<xref ref-type="bibr" rid="b0">Salimans et al., 2015</xref>) using an auxiliary reverse model in the ELBO parameterized by a single hidden layer network with 640 hidden units and RELU activations. We also implemented the Hamiltonian variational encoder (HVAE) method (<xref ref-type="bibr" rid="b3">Caterini et al., 2018</xref>), which is similar to HVI but without the reverse model. Unlike in the original HVAE, our implementation does not use tempering but still produces results similar to those from <xref ref-type="bibr" rid="b3">Caterini et al. (2018)</xref>.</p><p>For the HEI encoder, we use T = 30 HMC steps, each with 5 leapfrog steps. The initial approximation P 0 is kept fixed to be the prior p(x). We optimise the decoder and the HEI encoder jointly using Adam. <xref ref-type="table" rid="tab_1">Table 4</xref> shows the marginal test log-likelihood for HEI and the other methods, as estimated with 1,000 HAIS samples (<xref ref-type="bibr" rid="b0">Sohl-Dickstein &amp; Culpepper, 2012</xref>). Following <xref ref-type="bibr" rid="b0">Li et al. (2017)</xref>, we also include the effective sample size (ESS) of HAIS samples for the purpose of verifying the reliability of the reported test log-likelihoods. Overall, HEI outperforms HVI, HVAE and the standard VAE in test log-likelihood when the training time of all methods is fixed to be 6 hours. HEI still produces significant gains when the training time is extended to 12 hours and, with only 1.6 hours of training, HEI can already outperform the convolutional VAE of <xref ref-type="bibr" rid="b0">Salimans et al. (2015)</xref> with 6 hours of training.</p><p>To verify the convergence of HEI, we show in plot (b) of <xref ref-type="fig" rid="fig_4">Figure 5</xref> estimates of E p T [log &#960; * (x)] &#8722; E &#960; [log &#960; * (x)] for T = 1, . . . , 10 on five randomly chosen test images, where the ground truth E &#960; [log &#960; * (x)] is estimated by HAIS, after HMC hyper-parameter tuning in HEI (blue) and without hyper-parameter tuning in HEI (green), i.e. just using the initial hyper-parameter values. Plot (c) in <xref ref-type="fig" rid="fig_4">Figure 5</xref> shows similar results, but using the maximum mean discrepancy (MMD) score (<xref ref-type="bibr" rid="b5">Gretton et al., 2012</xref>) to quantify the similarity of samples from p T to samples from &#960;, where the latter ground truth samples are generated by HAIS. These plots suggests that shortening the HEI chain to T = 10 HMC steps will have a negligible effect on final simulation accuracy. Finally, the right part of <xref ref-type="table" rid="tab_3">Table 3</xref> shows the training time of HEI with and without the stopping gradient trick. These resuls show that the former method is up to 5 times faster.</p></sec><sec><title>SUMMARY</title><p>We have proposed Ergodic Inference (EI), a novel hybrid inference method that bridges MCMC and VI. EI a) reduces the approximation bias by increasing the number of MCMC steps, b) generates independent samples and c) tunes MCMC hyperparameters by optimising an objective function that directly quantifies the bias of the resulting samples. The effectiveness of EI was verified on synthetic examples and on popular benchmarks for deep generative models. We have shown that we can generate samples much closer to a gold standard sampling method than similar hybrid inference methods and at a low computational cost. However, one disadvantage of EI is that it requires the entropy of the first MCMC step to be larger than the entropy of the target distribution.</p></sec><sec id="figures"><title>Figures</title><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Ergodic inference combines with pros of MCMC and VI and avoids their cons.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Histograms of samples from ergodic inference using HMC transition kernels. P 9 denotes the ergodic approximation before traing; P * 9 denotes the ergodic approximation after training.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>The plot of E p T [log &#960; * (x)] as a function of the length of the chain T using 10000 samples: Left: with the valid P 0 as H(P 0 ) &gt; H(&#960;); Right: with invalid P 0 as H(P 0 ) &lt; H(&#960;). SG training means the stop gradient is applied to the x from previous HMC step in equation 9.</p></caption><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Histograms of samples generated by rejection sampling on each benchmark problem.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Histograms of 100,000 samples generated by each method after parameter optimisation.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><table-wrap id="tab_1"><label>Table 4:</label><caption><title>Table 4:</title><p>Comparisons in terms of compuational efficiency and test log-likelihood in the training of deep generative models on the MNIST dataset. We implemented the deconvolutional decoder network in Salimans et al. (2015) to test HVI. In Salimans et al. (2015), the test likelihood is estimated using importence-weighted samples from the encoder network. In our experiment, we use Hamiltonian annealled importance sampling and report the effective sample size (ESS).</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_2"><label>Table 2:</label><caption><title>Table 2:</title><p>Estimation of &#8722;E &#960; [log &#960; * (x)] and the sampling time on CPU: Each score (a/b) above refers to: a) &#8722;E &#960; [log &#960; * (x)] estimated by 100k samples; b) time in seconds to generate 100,000 samples.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_3"><label>Table 3:</label><caption><title>Table 3:</title><p>Left. The training time of MCMC parameter optimisation in seconds for 100 iterations for all candidate methods to produce the results in Figure 4. The training time of HEI is lower than HVI because of the stop gradient trick mentioned in Section 3.2. We do not report the training time for HAIS, because HAIS requires manual tuning of MCMC hyperparameters which is not directly comparable to the gradient-based autotuning used by the other methods. Right. The training time in seconds per epoch for the experiments with deep generative models (DGM).</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>The verification of the convergence of E p T [log &#960; * (x)] to E &#960; [log &#960; * (x)]: a: the targets are 2D benchmarks with the ground truth of E &#960; [log &#960; * (x)]; b: the target &#960; is the VAE posterior p(x|y) each curve represents one random chosen test MNIST image y with the ground truth of E &#960; [log &#960; * (x)] estimated by HAIS using 100 samples; c: MMD score between HEI samples and HAIS samples.</p></caption><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>TensorFlow: Large-scale machine learning on heterogeneous systems</article-title><year>2015</year><person-group person-group-type="author"><name><surname>References Mart&#237;n Abadi</surname><given-names>Ashish</given-names></name><name><surname>Agarwal</surname><given-names>Paul</given-names></name><name><surname>Barham</surname><given-names>Eugene</given-names></name><name><surname>Brevdo</surname><given-names>Zhifeng</given-names></name><name><surname>Chen</surname><given-names>Craig</given-names></name><name><surname>Citro</surname><given-names>Greg S</given-names></name><name><surname>Corrado</surname><given-names>Andy</given-names></name><name><surname>Davis</surname><given-names>Jeffrey</given-names></name><name><surname>Dean</surname><given-names>Matthieu</given-names></name><name><surname>Devin</surname><given-names>Sanjay</given-names></name><name><surname>Ghemawat</surname><given-names>Ian</given-names></name><name><surname>Goodfellow</surname><given-names>Andrew</given-names></name><name><surname>Harp</surname><given-names>Geoffrey</given-names></name><name><surname>Irving</surname><given-names>Michael</given-names></name><name><surname>Isard</surname><given-names>Yangqing</given-names></name><name><surname>Jia</surname><given-names>Rafal</given-names></name><name><surname>Jozefowicz</surname><given-names>Lukasz</given-names></name><name><surname>Kaiser</surname><given-names>Manjunath</given-names></name><name><surname>Kudlur</surname><given-names>Josh</given-names></name><name><surname>Levenberg</surname><given-names>Dan</given-names></name><name><surname>Man&#233;</surname><given-names>Rajat</given-names></name><name><surname>Monga</surname><given-names>Sherry</given-names></name><name><surname>Moore</surname><given-names>Derek</given-names></name><name><surname>Murray</surname><given-names>Chris</given-names></name><name><surname>Olah</surname><given-names>Mike</given-names></name><name><surname>Schuster</surname><given-names>Jonathon</given-names></name><name><surname>Shlens</surname><given-names>Benoit</given-names></name><name><surname>Steiner</surname><given-names>Ilya</given-names></name><name><surname>Sutskever</surname><given-names>Kunal</given-names></name><name><surname>Talwar</surname><given-names>Paul</given-names></name><name><surname>Tucker</surname><given-names>Vincent</given-names></name><name><surname>Vanhoucke</surname><given-names>Vijay</given-names></name><name><surname>Vasudevan</surname><given-names>Fernanda</given-names></name><name><surname>Vi&#233;gas</surname><given-names>Oriol</given-names></name><name><surname>Vinyals</surname><given-names>Pete</given-names></name><name><surname>Warden</surname><given-names>Martin</given-names></name><name><surname>Wattenberg</surname><given-names>Martin</given-names></name><name><surname>Wicke</surname><given-names>Yuan</given-names></name><name><surname>Yu</surname><given-names>Xiaoqiang</given-names></name><name><surname>Zheng</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><source>Pattern Recognition and Machine Learning</source><year>2006</year><person-group person-group-type="author"><name><surname>Christopher</surname><given-names>M</given-names></name><name><surname>Bishop</surname><given-names /></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Importance weighted autoencoders</article-title><source>arXiv preprint arXiv:1509.00519</source><year>2015</year><person-group person-group-type="author"><name><surname>Burda</surname><given-names>Yuri</given-names></name><name><surname>Grosse</surname><given-names>Roger</given-names></name><name><surname>Salakhutdinov</surname><given-names>Ruslan</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Hamiltonian variational auto-encoder</article-title><source>Advances in Neural Information Processing Systems</source><year>2018</year><fpage>8167</fpage><lpage>8177</lpage><person-group person-group-type="author"><name><surname>Anthony L Caterini</surname><given-names>Arnaud</given-names></name><name><surname>Doucet</surname><given-names>Dino</given-names></name><name><surname>Sejdinovic</surname><given-names /></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Auto-encoding variational bayes</article-title><source>The International Conference on Learning Representations (ICLR)</source><year>2014</year><person-group person-group-type="author"><name><surname>Welling</surname><given-names>M</given-names></name><name><surname>Kingma</surname><given-names>D P</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>A kernel two-sample test</article-title><source>Journal of Machine Learning Research</source><year>2012</year><volume>13</volume><fpage>723</fpage><lpage>773</lpage><person-group person-group-type="author"><name><surname>Gretton</surname><given-names>Arthur</given-names></name><name><surname>Borgwardt</surname><given-names>Karsten M</given-names></name><name><surname>Rasch</surname><given-names>Malte J</given-names></name><name><surname>Sch&#246;lkopf</surname><given-names>Bernhard</given-names></name><name><surname>Smola</surname><given-names>Alexander</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Alternating back-propagation for generator network</article-title><source>AAAI</source><year>2017</year><volume>3</volume><fpage>13</fpage><lpage>13</lpage><person-group person-group-type="author"><name><surname>Han</surname><given-names>Tian</given-names></name><name><surname>Lu</surname><given-names>Yang</given-names></name><name><surname>Zhu</surname><given-names>Song-Chun</given-names></name><name><surname>Wu</surname><given-names>Ying Nian</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><source>Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research</source><fpage>1510</fpage><lpage>1519</lpage><person-group person-group-type="author"><name><surname>Matthew</surname><given-names>D</given-names></name><name><surname>Hoffman</surname><given-names /></name></person-group></element-citation></ref></ref-list></back></article>