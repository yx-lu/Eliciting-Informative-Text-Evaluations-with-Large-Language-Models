Title:
```
Published as a conference paper at ICLR 2020 FUNCTIONAL REGULARISATION FOR CONTINUAL LEARNING WITH GAUSSIAN PROCESSES
```
Abstract:
```
We introduce a framework for Continual Learning (CL) based on Bayesian infer- ence over the function space rather than the parameters of a deep neural network. This method, referred to as functional regularisation for Continual Learning, avoids forgetting a previous task by constructing and memorising an approximate posterior belief over the underlying task-specific function. To achieve this we rely on a Gaus- sian process obtained by treating the weights of the last layer of a neural network as random and Gaussian distributed. Then, the training algorithm sequentially encounters tasks and constructs posterior beliefs over the task-specific functions by using inducing point sparse Gaussian process methods. At each step a new task is first learnt and then a summary is constructed consisting of (i) inducing inputs - a fixed-size subset of the task inputs selected such that it optimally represents the task - and (ii) a posterior distribution over the function values at these inputs. This summary then regularises learning of future tasks, through Kullback-Leibler regu- larisation terms. Our method thus unites approaches focused on (pseudo-)rehearsal with those derived from a sequential Bayesian inference perspective in a principled way, leading to strong results on accepted benchmarks.
```

Figures/Tables Captions:
```
Figure 1: Depiction of the proposed approach. See also the provided pseudocode. When learning task 1, first, parameters of the network θ and output layer w are fitted (Panel A). Afterwards, the learned GP is sparsified and inducing points u1, .. are found (Panel B). When moving to the next task the same steps are repeated. The only difference is that now the previously found summaries (in this case points u1, .., u8) are used to regularise the function (via KL-divergence term), such that the first task is not forgotten.
Figure 2: Detecting task boundaries using the predictive uncertainty of a Gaussian Process. As GP predictions revert to the prior (shaded blue) when queried far from observed data (shown as black dots), we can test for a distribution shift by comparing the GP posterior over functions (in green) to the prior. Small distance between predictive distributions at test points (red dots) suggest a task switch.
Figure 3: Comparing optimisation criteria for varying number of inducing points.
Figure 4: Inducing point optimisation for the first task on the Permuted-MNIST benchmark. The number of inducing points was limited to 10. Left: A example optimisation shown in the feature space of a trained network. Points are coloured by class label. Data shown corresponds to the first row in the images on the right. Right: Optimised inducing points consistently cover examples of all classes. Each row corresponds to a different run with random initialisation. Best viewed in colour.
Figure 5: Visualising KL terms and test statistics on multiple Omniglot tasks.
Table 1: Results on Permuted- and Split-MNIST. Baseline results are taken from Nguyen et al. (2017). For the experiments conducted in this work we show the mean and standard deviation over 10 random repetitions. Where applicable, we also report the number of inducing points/replay buffer size per task in parentheses.
Table 2: Results on sequential Omniglot. Baseline results are taken from Schwarz et al. (2018). Shown are mean and standard deviation over 5 random task permutations. Note that methods 'Single model per Task' and 'Progressive Nets' are not directly comparable due to unrealistic assumptions, but serve as an upper bound on the performance for the remaining continual learning methods.
Table 3: Task boundary detection evaluated as a binary classification task (Positive labels corresponds to task switches).
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Recent years have seen a resurgence of interest in continual learning, which refers to systems that learn in an online fashion from data associated with possibly an ever-increasing number of tasks (Ring, 1994; Robins, 1995; Schmidhuber, 2013; Goodfellow et al., 2013). A continual learning system must adapt to perform well on all earlier tasks without requiring extensive re-training on previous data. There are two main challenges for continual learning (i) avoiding catastrophic forgetting, i.e. remembering how to solve earlier tasks, and (ii) scalability over the number of tasks. Other possible desiderata may include forward and backward transfer, i.e. learning new tasks faster and retrospectively improving on previously tasks. Similarly to many recent works on continual learning (Kirkpatrick et al., 2017; Nguyen et al., 2017; Rusu et al., 2016; Li & Hoiem, 2017; Farquhar & Gal, 2018), we focus on the scenario where a sequence of supervised learning tasks are presented to a continual learning system based on a deep neural network. While most methods assume known task boundaries, our approach will be also extended to deal with unknown task boundaries. Among the different techniques proposed to address this problem, we have methods which constrain or regularise the parameters of the network to not deviate significantly from those learnt on previous tasks. This includes methods that frame continual learning as sequential approximate Bayesian inference, including EWC (Kirkpatrick et al., 2017) and VCL (Nguyen et al., 2017). Such approaches suffer from brittleness due to representation drift. That is, as parameters adapt to new tasks the values that other parameters are constrained/regularised towards become obsolete (see Section 2.5 for further discussion on this). On the other hand, we have rehearsal/replay buffer methods, which use a memory store of past observations to remember previous Published as a conference paper at ICLR 2020 Backpropagated signal TASK 1 TASK 2 A. B. A. Backpropagated signal tasks (Robins, 1995; Robins & McCallum, 1998; Lopez-Paz et al., 2017; Rebuffi et al., 2017). While these methods tend to not suffer from brittleness, uncertainty about the unknown functions is not expressed. Furthermore, they rely on various heuristics to decide which data to store (Rolnick et al., 2018), often requiring large quantities of stored observations to achieving good performance. In this paper we will address the open problem of deriving an optimisation objective to select the best observations for storage. In this paper, we develop a new approach to continual learning which addresses the shortcomings of both categories. It is based on approximate Bayesian inference, but on the space of functions instead of neural network parameters, so does not suffer from the aforementioned brittleness. Intuitively, while previous approaches constrain the parameters of a neural network to limit deviations from previously learnt parameters, our approach instead constrains the neural network predictions from deviating too far from those that solve previous tasks. Effectively, our approach avoids forgetting an earlier task by memorising an approximate posterior belief over the underlying task-specific function. To implement this, we consider Gaussian processes (GPs) (Rasmussen & Williams, 2005), and make use of inducing point sparse GP methods (Csato & Opper, 2002; Titsias, 2009; Hensman et al., 2013; Bui et al., 2017b), which summarise posterior distributions over functions using small numbers of so-called inducing points. These inducing points are selected from the training set by optimising a variational objective, providing a principled mechanism to compress the dataset to a meaningful subset of fixed size. They are kept around when moving to the next task and, together with their posterior distributions, are used to regularise the continual learning of future tasks, through Kullback-Leibler regularisation terms within a variational inference framework, thus avoiding catastrophic forgetting of earlier tasks. Our approach bears similarities to replay-based approaches, with inducing points playing the role of the rehersal/replay buffer, but has two important advantages. First the approximate posterior distributions at the inducing points captures the uncertainty of the unknown function as well, rather than providing merely target values. Second, inducing points can be optimised using specialised criteria from the GP literature, achieving better performance than a random selection of observations. An intuitive depiction of our approach is given in  Figure 1 . To enable our functional regularisation approach to deal with high-dimensional and complex datasets, we use a linear kernel with features parameterised by neural networks (Wilson et al., 2016). Such GPs can be understood as Bayesian neural networks, where only the weights of the last layer are treated in a Bayesian fashion, while those in earlier layers are optimised. This view allows for a more computationally efficient and accurate training procedure to be carried out in weight space, before the approximation is translated into function space where the inducing points are constructed and then used for regularising learning of future tasks. Finally, note that inducing points are also used to regularize the deep network, even though they were selected to best represent functions given by the GP. See Section 2.3 for further details.

Section Title: FUNCTIONAL REGULARISATION FOR CONTINUAL LEARNING
  FUNCTIONAL REGULARISATION FOR CONTINUAL LEARNING We consider supervised learning of multiple tasks, with known task boundaries, that are processed sequentially one at a time. At each step we receive a set of examples (X i , y i ) where X i = {x i,j } Ni j=1 Published as a conference paper at ICLR 2020 are input vectors and y i = {y i,j } Ni j=1 are output targets so that each y i,j is assigned to the input x i,j ∈ R D . We assume the most extreme case (and challenging in terms of avoiding forgetting) where each dataset (X i , y i ) introduces a new task, while less extreme cases can be treated similarly. We wish to sequentially train a shared model or representation from all tasks so that catastrophic forgetting is avoided, i.e. when the model is trained on the i-th task it should still provide accurate predictions for all tasks j < i seen in the past. As a model we consider a deep neural network with its final hidden layer providing the feature vector φ(x; θ) ∈ R K where x is the input vector and θ are the model parameters. This representation is shared across tasks and θ is a task-shared parameter. To solve a specific task i we additionally construct an output layer f i (x; w i ) ≡ f i (x; w i , θ) = w i φ(x; θ), (1) where for simplicity we assume that f i (x; w i ) is a scalar function and w i is the vector of task-specific weights. Dealing with vector-valued functions is straightforward and is discussed in the Appendix. By placing a Gaussian prior on the output weights, w i ∼ N (w i |0, σ 2 w I), we obtain a distribution over functions. While each task has its own independent/private weight vector w i the whole distribution refers to the full infinite set of tasks that can be tackled by the same feature vector φ(x; θ). We can marginalise out w i and obtain the equivalent function space view of the model, where each task-specific function is an independent draw from a GP (Rasmussen & Williams, 2005), i.e. f i (x) ∼ GP(0, k(x, x )), k(x, x ) = σ 2 w φ(x; θ) φ(x ; θ), where the kernel function is defined by the dot product of the neural network feature vector. By assum- ing for now that all possible tasks are simultaneously present similarly to multi-task GPs (Bonilla et al., 2008; Álvarez et al., 2012), the joint distribution over function values and output data for all tasks is written as i p(y i |f i )p(f i ) = i p(y i |f i )N (f i |0, K Xi ), where the vector f i stores all function values for the input dataset X i , i.e. f i,j = f (x i,j ), j = 1, . . . , N i . Also the kernel matrix K Xi is obtained by evaluating the kernel function on X i , i.e. each element [K Xi ] j,k = σ 2 w φ(x i,j ; θ) φ(x i,k ; θ) where x i,j , x i,k ∈ X i . The form of each likelihood function p(y i |f i ) depends on the task, for example if the i-th task involves binary classification then p(y i |f i ) = Ni j=1 p(y i,j |f i,j ) = Ni j=1 1 1+e −y i,j f i,j where y i,j ∈ {−1, 1} indicates the binary class label. Inference in this model requires estimating each posterior distribution p(f i |y i , X i ), which can be approximated by a multivariate Gaussian N (f i |µ i , Σ i ). Given this Gaussian we can express our posterior belief over any function value f i,* at some test input x i,* using the posterior GP (Rasmussen & Williams, 2005), Given that the tasks arrive one at a time, the above suggests that one way to avoid forgetting the i-th task is to memorise the corresponding posterior belief N (f i |µ i , Σ i ). While this can regularise continual learning of subsequent tasks (similarly to the more general variational framework in the next section), it can be prohibitively expensive since the non-parametric nature of the model means that for each N (f i |µ i , Σ i ) we need to store O(N 2 i ) parameters and additionally we need to keep in memory the full set of input vectors X i . Therefore, in order to reduce the time and memory requirements we would like to apply data distillation and approximate each full posterior by applying sparse GP methods. As shown next, by applying variational sparse GP inference (Titsias, 2009) in a sequential fashion we obtain a new algorithm for function space regularisation in continual learning.

Section Title: LEARNING THE FIRST TASK
  LEARNING THE FIRST TASK Suppose we encounter the first task with data (X 1 , y 1 ). We introduce a small set Z 1 = {z 1,j } M1 j=1 of inducing inputs where each z 1,j lives in the same space as each training input x 1,j . The inducing set Z 1 can be a subset of X 1 or it can contain pseudo inputs (Snelson & Ghahramani, 2006), i.e. points lying between the training inputs. For simplicity next we consider Z 1 as pseudo points, although in practice for continual learning it can be more suitable to select them from the training inputs (see Section 2.4). By evaluating the function output at each z 1,j we obtain a vector of auxiliary function values u 1 = {u 1,j } M1 j=1 , where each u 1,j = f (z 1,j ). Hence, we obtain the joint distribution The exact posterior distribution p θ (f 1 |u 1 , y 1 )p θ (u 1 |y 1 ) is approximated by a distribution of the form, q(f 1 , u 1 ) = p θ (f 1 |u 1 )q(u 1 ), where q(u i ) is a variational distribution and p θ (f 1 |u 1 ) is the GP prior conditional, p θ (f 1 |u 1 ) = N (f 1 |K X1Z1 K −1 Z1 u 1 , K X1 − K X1Z1 K −1 Z1 K Z1X1 ). Here, K X1Z1 is the cross kernel matrix between the sets X 1 and Z 1 , K Z1X1 = K X1Z1 and K Z1 is the kernel matrix on Z 1 . The method learns (q(u 1 ), Z 1 ) by minimising the KL divergence KL(p θ (f 1 |u 1 )q(u 1 )||p θ (f 1 |u 1 , y 1 )p θ (u 1 |y 1 )). The ELBO is also maximised over the neural net- work feature vector parameters θ that determine the kernel matrices. This ELBO is generally written in the form (Hensman et al., 2013; Lloyd et al., 2015; Dezfouli & Bonilla, 2015; Hensman et al., 2015; Sheth et al., 2015), F(θ, q(u 1 )) = N1 j=1 E q(f1,j ) [log p(y 1,j |f 1,j )] − KL(q(u 1 )||p θ (u 1 )), (3) where q(f 1,j ) = p(f 1,j |u 1 )q(u 1 )du 1 is an univariate Gaussian distribution with analytic mean and variance that depend on (θ, Z 1 , q(u 1 ), x 1,j ). Each expectation E q(f1,j ) [log p(y 1,j |f 1,j )] is a one-dimensional integral and can be estimated by Gaussian quadrature. The variational distribution q(u 1 ) is chosen to be a Gaussian, parameterised as q(u 1 ) = N (u 1 |µ u1 , L u1 L u1 ), where L u1 is a square root matrix such as a lower triangular Cholesky factor. Then, based on the above we can jointly apply stochastic variational inference (Hensman et al., 2013) to maximise the ELBO over (θ, µ u1 , L u1 ) and optionally over the inducing inputs Z 1 .

Section Title: LEARNING THE SECOND AND SUBSEQUENT TASKS
  LEARNING THE SECOND AND SUBSEQUENT TASKS The functional regularisation framework for continual learning arises from the variational sparse GP inference method as we encounter the second and subsequent tasks. Once we have learned the first task we throw away the dataset (X 1 , y 1 ) and we keep in memory only a task summary consisting of the inducing inputs Z 1 and the variational Gaussian distribution q(u 1 ) (i.e. its parameters µ u1 and L u1 ). Note also that θ (that determines the neural network feature vector φ(x; θ)) has a current value obtained by learning the first task. When the dataset (X 2 , y 2 ) for the second task arrives, a suitable ELBO to continue learning θ and also estimate the second task summary (Z 2 , q(u 2 )) is which is just the sum of the corresponding ELBOs for the two tasks. We need to approximate this ideal objective by making use of the fixed summary (Z 1 , q(u 1 )) that we have kept in memory for the first task. By considering Z 1 ⊂ X 1 as our replay buffer with outputs y 1 ⊂ y and u 1 ⊂ f 1 the above can be approximated by N 1 M 1 M1 j=1 E q(u1,j ) [log p( y 1,j |u 1,j )] + N2 j=1 E q(f2,j ) [log p(y 2,j |f 2,j ) − i=1,2 KL(q(u i )|p θ (u i )), where each q(u 1,j ) is a univariate marginal of q(u 1 ). However, since q(u 1 ) is kept fixed the whole expected log-likelihood term N1 M1 M1 j=1 E q(u1,j ) [log p( y 1,j |u 1,j )] is just a constant that does not depend on the parameters θ any more. Thus, the objective function when learning the second task reduces to maximising, The only term associated with the first task is KL(q(u 1 )||p θ (u 1 )). While q(u 1 ) is fixed (i.e. its parameters are constant), the GP prior p θ (u 1 ) = N (u 1 |0, K Z1 ) is still a function of the feature vector parameters θ, since K Z1 depends on θ. Thus, this KL term regularises the parameters θ so that, while learning the second task, the feature vector still needs to explain the posterior distribution over the function values u 1 at input locations Z 1 . Notice that −KL(q(u i )||p θ (u i ) is further simplified as q(u 1 ) log p θ (u 1 )du 1 + const, which shows that the regularisation is such that p θ (u 1 ) needs to be consistent with all infinite draws from q(u 1 ) in a moment-matching or maximum likelihood sense.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 Similarly for the subsequent tasks we can conclude that for any new task k the objective will be objective for the current task − k−1 i=1 KL(q(u i )||p θ (u i )) regularisation from previous tasks . (4) Thus, functional regularisation when learning a new task is achieved through the sum of the KL divergences k−1 i=1 KL(q(u i )||p θ (u i )) of all previous tasks, where each q(u i ) is the fixed posterior distribution which encodes our previously obtained knowledge about task i < k. Furthermore, in order to keep the optimisation scalable over tasks, we can form unbiased approximations of this latter sum by sub-sampling the KL terms, i.e. by performing minibatch-based stochastic approximation over the regularisation terms associated with these previous tasks.

Section Title: ACCURATE WEIGHT SPACE INFERENCE FOR THE CURRENT TASK
  ACCURATE WEIGHT SPACE INFERENCE FOR THE CURRENT TASK While the above framework arises by applying sparse GP inference, it can still be limited. When the budget of inducing variables is small, the sparse GP approximation may lead to inaccurate estimates of the posterior belief q(u k ), which will degrade the quality of regularisation when learning new tasks. This is worrisome as in continual learning it is desirable to keep the size of the inducing set as small as possible. One way to deal with this issue is to use a much larger set of inducing points for the current task or even maximise the full GP ELBO N k j=1 E q(f k,j ) log p(y k,j |f k,j ) − KL(q(f k )||p θ (f k )) (i.e. by using as many inducing points as training examples), and once training is completed to distill the small subset Z k , u k ⊂ X k , f k , and the corresponding marginal distribution q(u k ) from q(f k ), for subsequently regularising continual learning. However, carrying out this maximisation in the function space can be extremely slow since it scales as O(N 3 k ) per optimisation step. To our rescue, there is an alternative computationally efficient way to achieve this, by relying on the linear form of the kernel, that performs inference over the current task in the weight space. While this inference does not immediately provides us with the summary (induced points) for building the functional regularisation term, we can distill this term afterwards as discussed next. This allows us to address the continual learning aspect of the problem. Given that the current k-th task is represented in the weight space as f k (x; w k ) = w k φ(x; θ), w k ∼ N (0, σ 2 w I), we introduce a full Gaussian variational approximation q(w k ) = N (w k |µ w k , Σ w k ), where µ k is a K dimensional mean vector and Σ w k is the corresponding K × K full covariance matrix parameterised as Σ w k = L w k L w k . Learning the k-th task is carried out by maximising the objective in equation 4, with the only difference that the ELBO for the current task is now in the weight space. The objective becomes can be re-written as one-dimensional integral and estimated using Gaussian quadrature. Once the variational distribution q(w k ) has been optimised, together with the constantly updated feature parameters θ, we can rely on this solution to select inducing points Z k . See Section 2.4 for more detail. We also compute the posterior distribution over their function values u k according to q(u k ) = N (u k |µ u k , L u k L u k ), where µ u k = Φ Z k µ w k , L u k = Φ Z k L w k (5) and the matrix Φ Z k stores as rows the feature vectors evaluated at Z k . Subsequently, we store the k-th task summary (Z k , µ u k , L u k ) and use it for regularising continual learning of subsequent tasks, by always maximising the objective F(θ, q(w k )). Pseudo-code of the procedure is given in Algorithm 1.

Section Title: SELECTION OF THE INDUCING POINTS
  SELECTION OF THE INDUCING POINTS After having seen the k-th task, and given that it is straightforward to compute the posterior distribution q(u k ) for any set of function values, the only issue remaining is to select the inducing inputs Z k . A simple choice that works well in practice is to select Z k as a random subset of the training inputs X k . The question is whether we can do better with some more structured criterion. In our experiments we will investigate several criteria where the most effective one will be an unsupervised criterion that only depends on the training inputs, while the other supervised criteria are described in the Appendix. This unsupervised criterion quantifies how well we reconstruct the full kernel matrix K X k from the inducing set Z k and it can be expressed as the trace of the covariance matrix of the prior GP conditional p(f k |u k ), i.e. where each k(x k,j , x k,j ) − k Z K ,x k,j K −1 Z k k Z k ,x k,j ≥ 0 is a reconstruction error for an individual training point. The above quantity appears in the ELBO in (Titsias, 2009), is also used in (Csato & Opper, 2002) and it has deep connections with the Nystróm approximation (Williams & Seeger, 2001) and principal component analysis. The criterion in equation 6 promotes finding inducing points Z k that are repulsive with one another and are spread evenly in the input space under a similarity/distance implied by the dot product of the feature vector φ(x; θ k ) (with θ k being the current parameter values after having trained with the k-th task). An illustration of this repulsive property is given in Section 4. To select Z k , we minimise T (Z k ) by applying discrete optimisation where we select points from the training inputs X k . The specific optimisation strategy we use in the experiments is to start with an initial random set Z k ⊂ X k and then further refine it by doing local moves where random points in Z k are proposed to be changed with random points of X k .

Section Title: PREDICTION AND DIFFERENCES WITH WEIGHT SPACE METHODS
  PREDICTION AND DIFFERENCES WITH WEIGHT SPACE METHODS Prediction at any i-th task that has been encountered in the past follows the standard sparse GP predictive equations. Given a test data point x i,* the predictive density of its output value y i,* takes the form where in µ i,* we have explicitly written the cross kernel vector k Z k xi,* = Φ Zi φ(x i,* ; θ) (assuming σ 2 w = 1 for simplicity) to reveal a crucial property of this prediction. Specifically, given that f i,* = w i φ(x i,* ; θ) the vector µ ui K −1 Zi Φ Zi acts as a mean prediction for the task-specific parameter row vector w i . As we learn future tasks and the parameter θ changes, this mean parameter prediction automatically adapts (recall that K Zi = Φ Zi Φ Zi and Φ Zi vary with θ and only µ ui is constant) in order to counteract changes in the feature vector φ(x i,* ; θ), so that the overall prediction for the function value, i.e. µ i,* = E[f i,* ], does not become obsolete. For instance, the prediction of the function values at the inducing inputs Z i always remains constant to our fixed/stored mean belief µ ui Published as a conference paper at ICLR 2020 since by setting x i,* = Z i the formula gives µ ui K −1 Zi Φ Zi Φ Zi = µ ui . Similar observations can be made for the predictive variances. The above analysis reveals an important difference between continual learning in function space and in weight space, where in the latter framework task-specific parameters such as w i might not automatically adapt to counteract changes in the feature vector φ(x; θ), as we learn new tasks and θ changes. For instance, if as a summary of the task, instead of the function space posterior distribution q(u i ), we had kept in memory the weight space posterior q(w i ) (see Section 2.3), then the corresponding mean prediction on the function value, E[f i,* ] = µ wi φ(x i,* ; θ), can become obsolete as φ(x i,* ; θ) changes and µ wi remains constant.

Section Title: DETECTING TASK BOUNDARIES USING BAYESIAN UNCERTAINTIES
  DETECTING TASK BOUNDARIES USING BAYESIAN UNCERTAINTIES So far we have made the assumption that task switches are known, which may not always be a realistic setting. Instead, we now introduce a novel approach for detecting task boundaries in continual learning arising naturally from our method by a simple observation: The GP predictive uncertainty grows as the model is queried far away from observed data, eventually falling back to the prior. When a minibatch of data {x i , y i } b i=1 from a new task arrives, we thus expect the distance between prior and posterior to be small (see  Figure 2 ). Thus, a simple way to detect a change in the input distribution is to compare the GP univariate posterior density q(f (x i )) = N (f (x i )|µ i , σ 2 i ) ≈ p(f (x i )|f )p(f |y, X)df (x i ), where (µ i , σ 2 i ) are predictive mean and variance, with the prior GP density p(f (x i )) = N (f (x i )|0, k(x i , x i )). This can be achieved by using a divergence measure between distributions such as the symmetrised KL divergence, computed separately for any x i in the minibatch. Given that all distributions are univariate Gaussians the above can be obtained analytically. When each score i is close to zero this indicate that the input distribution has changed so that a task switch can be detected. Each i ≥ 0 can be thought of as expressing a degree of surprise about x i , i.e. the smaller is i the more surprising is x i . Thus our idea has close links to Bayesian surprise (Itti & Baldi, 2006). In order to use this intuition to detect task switches, we can perform a statistical test between the values { i } b i=1 for the current batch and those from the previous batch { old i } b i=1 before making any updates to the parameters using the current batch. A suitable choice is Welch's t-test (due to unequal variances), demanding that with high statistical significance the mean of { i } b i=1 is smaller than the mean of { old i } b i=1 . The ability to detect changes based on the above procedure arises from our framework as we construct posterior distributions over function values f (x i ) that depend on inputs x i (while in contrast a posterior over weights alone does not depend on any input). Subsequently, these predictive densities contain information about the distribution of these inputs in the sense that when an x i is close to the Published as a conference paper at ICLR 2020 training inputs from the same task we expect reduced uncertainty, while for inputs of a different task we expect high uncertainty that falls back to the prior uncertainty.

Section Title: EXPERIMENTS
  EXPERIMENTS We now test the scalability and competitiveness of our method on various continual learning problems, referring to the proposed approach as Functional Regularised Continual Learning (FRCL). Throughout this section, we will aim to answer three key questions: (i) How does FRCL compare to state-of-the-art algorithms for Continual Learning? (ii) How important is a principled criterion for inducing point selection? How do varying numbers of inducing points/task affect overall performance? (iii) If ground truth task boundaries are not given, does the detection method outlined in Section 3 succeed in detecting task changes? In order to answer these questions, we consider experiments on three established Continual Learning classification problems: Split-MNIST, Permuted-MNIST and sequential Omniglot (Goodfellow et al., 2013; Zenke et al., 2017; Schwarz et al., 2018), described in the Appendix. FRCL methods have been implemented using GPflow (Matthews et al., 2017). In addition to comparing our method with other approaches in the literature by quoting published results, we also show results for an additional baseline (BASELINE) corresponding to a simple replay-buffer method for Continual Learning (explained in the Appendix).

Section Title: IS FRCL A COMPETITIVE MODEL FOR CONTINUAL LEARNING?
  IS FRCL A COMPETITIVE MODEL FOR CONTINUAL LEARNING? Addressing first question (i), we show results on the MNIST-variations in  Table 1  and on the more challenging Omniglot benchmark in  Table 2 . Note that we also specify the inducing point optimisation criterion in brackets, i.e. FRCL (TRACE-TERM) corresponds to the loss in equa- tion 6. We observe strong results on all bench- marks, setting a new state-of-the-art results on Permuted-MNIST & Omniglot while coming close to existing results on Split-MNIST. The improvement on the BASELINE shows that ap- proximate posterior distributions over functions values can lead to more effective regularisation for CL compared to just having a replay buffer of input-output pairs. Furthermore, despite its simplicity, the simple BASELINE strategy per- forms competitively. In line with other results, we conclude that rehearsal-based ideas continue to provide a strong baseline. This also gives justification to a main motivation of this work: To unite the two previously separate lines of CL work on rehearsal-based and Bayesian methods. Nevertheless, other methods may be more suitable when storing data is not feasible.

Section Title: INDUCING POINT OPTIMISATION AND TASK SWITCH DETECTION
  INDUCING POINT OPTIMISATION AND TASK SWITCH DETECTION An appealing theoretical property of our method is the principled selection of inducing points through optimisation. Answering question (ii), we now proceed to investigate the importance of the criterion used as well as the dependence on the number of inducing points. These results are shown in  Figure 3 . Note that the definition of objectives Class. Error, ELBO & Log pred. density are given in the Appendix. In accordance with our intuition, we observe that optimisation becomes increasingly important as the number of inducing points is reduced. The results also give strong statistical motivation to use the trace-term motivated before. Further, as can be seen looking at the results for Log pred. density, a poorly chosen criterion may behave worse than random.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 To provide an insight into the solutions obtained by the trace-term criterion, we provide a visualisation of inducing points in  Figure 4 . Remarkably, even though the objective is unsupervised, it results in a consistent allocation of one example per class. Furthermore, the optimised inducing points are spread across the input space as shown by the TNSE (Maaten & Hinton, 2008) visualisation, which is in line with the intuition that the objective encourages repulsive inducing points. Finally, we answer question (iii) by first showing both the mean of the terms { i } b i=1 (top) as well as the result of Welch's t-test (bottom) between terms { i } b i=1 , { old i } b i=1 in  Figure 5 , using only a small number Omniglot alphabets and 1000 training iterations per task for illustrative purposes. We note that the intuition built up in Section 3 holds, with clear spikes being shown whenever the t-test returns a positive result. Furthermore, we provide a quantitative comparison in  Table 3 . On the positive side, we note very strong results for Split- & Perm-MNIST and further observe that we find a similar t-test threshold value to apply to all dataset, making this an easy hyper-parameter to set. While the task boundary detection results for Omniglot are less strong, which may due to the smaller batch size (32 for Omniglot, ≥ 100 for the MNIST-versions), resulting in a noisier test result. Note that this could be easily mitigated by using a larger set { old i }, e.g. the last 10 minibatches, which would make this test more robust.

Section Title: DISCUSSION
  DISCUSSION We introduced a functional regularisation approach for supervised continual learning that combines inducing point GP inference with deep neural networks. Our method constructs task-specific posterior beliefs or summaries on inducing inputs. Subsequently, the task-specific summaries allow us to regularise continual learning and avoid catastrophic forgetting. Our approach unifies the two extant approaches to continual learning, of parameter regularisation and replay/rehersal. Viewed from the regularisation perspective, our approach regularises the functional outputs of the neural network, thus avoid the brittleness due to representation drift. Viewed from a rehearsal method perspective, we provide a principled way of compressing data from previous task, by means of optimizing the selection of inducing points. By investigating the behaviour of the posterior beliefs, we also proposed a method for detecting task boundaries. All these improvements lead to strong empirical gains compared to state-of-the-art continual learning methods. Regarding related work on online learning using GPs, notice that previous algorithms (Bui et al., 2017a; Csato & Opper, 2002; Csato, 2002) learn in an online fashion a single task where data from this task arrive sequentially. In contrast in this paper we developed a continual learning method for dealing with a sequence of different tasks. A direction for future research is to enforce a fixed memory buffer (or a buffer that grows sub-linearly w.r.t. the number of tasks), in which case one would need to compress the summaries of all previous seen tasks into a single summary. Finally, while in this paper we applied the method to supervised classification tasks, it will be interesting to consider also applications in other domains such as reinforcement learning.

```
