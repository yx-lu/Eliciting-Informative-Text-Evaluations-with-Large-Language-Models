Title:
```
Under review as a conference paper at ICLR 2020 RIGGING THE LOTTERY: MAKING ALL TICKETS WINNERS
```
Abstract:
```
Abstract
```

Figures/Tables Captions:
```
Figure 1: Dynamic sparse training aims to change connectivity during training to help out optimza- tion.
Figure 2: (left) Performance of various dynamic sparse training methods on ImageNet-2012 classi- fication task. We use 80% sparse ResNet-50 architecture with uniform sparsity distribution. Points at each curve correspond to the individual training runs with training multipliers from 1 to 5 (except pruning which is scaled between 0.5 and 2). We repeat training 3 times at every multiplier and report the mean accuracies. The number of FLOPs required to train a standard dense Resnet-50 along with its performance is indicated with a dashed red line. (right) Performance of RigL at different sparsity levels with extended training. Results are averaged over 3 runs.
Figure 2- left summarizes the performance of various methods on training an 80% sparse ResNet-50. We also train small dense networks with equivalent parameter count. All sparse networks use the constant sparsity distribution and a cosine update schedule (α = 0.3, ∆T = 100). Overall, we observe that the performance of all methods improves with training time; thus, for each method we run extended training with up to 5× the training steps of the original.
Figure 3: (left) RigL significantly improves the performance of Sparse MobileNets on ImageNet- 2012 dataset and exceeds the pruning results reported by Zhu & Gupta (2018). Performance of the dense MobileNets are indicated with red lines. (right) Performance of sparse MobileNet-v1 architectures presented with their inference FLOPs. Networks with ERK distribution get better performance with the same number of parameters but take more FLOPs to run. Training wider sparse models with RigL (Big-Sparse) yields a significant performance improvement over the dense model.
Figure 4: (left) Final validation loss of various sparse training methods on character level language modelling task. Cross entropy loss is converted to bits (from nats). Performance and the training cost of a dense model is indicated with dashed red lines. (right) Test accuracies of sparse WideResNet- 22-2's on CIFAR-10 task.
Figure 5: (left) Performance of RigL at different sparsities using different sparsity masks (right) Ablation study on cosine schedule. Other methods are in the appendix.
Figure 6: (left) Training loss evaluated at various points on interpolation curves between a magnitude pruning model (0.0) and a model trained with static sparsity (1.0). (right) Training loss of RigL and Static methods starting from the static sparse solution, and their final accuracies.
Table 1: Comparison of different sparse training techniques. Drop and Grow columns correspond to the strategies used during the mask update. Selectable FLOPs is possible if the cost of training the model is fixed at the beginning of training.
Table 2: Performance and cost of sparse training methods on training 80% and 90% sparse ResNet- 50s. FLOPs needed for training and test are normalized with the FLOPs of a dense model (see Appendix G for details on how FLOPs are calculated). Methods with a subscript indicate a rescaled training time, whereas '*' indicates reported results. (ERK) corresponds to the sparse networks with Erdős-Renyi-Kernel sparsity distribution. RigL 5× (ERK) achieves 77.1% Top-1 Accuracy using only 20% of the parameters of a dense model and 42% of its FLOPs.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The parameter and floating point operation (FLOP) efficiency of sparse neural networks is now well demonstrated on a variety of problems ( Han et al., 2015 ;  Srinivas et al., 2017 ). Some work has even shown inference time speedups are possible on Recurrent Neural Networks (RNNs) ( Kalchbrenner et al., 2018 ) and Convolutional Neural Networks (ConvNets) ( Park et al., 2016 ). Currently, the most accurate sparse models are obtained with techniques that require, at a minimum, the cost of training a dense model in terms of memory and FLOPs ( Zhu & Gupta, 2018 ;  Guo et al., 2016 ), and sometimes significantly more ( Molchanov et al., 2017 ). This paradigm has two main limitations: 1. The maximum size of sparse models is limited to the largest dense model that can be trained. Even if sparse models are more parameter efficient, we can't use pruning to train models that are larger and more accurate than the largest possible dense models. 2. It is inefficient. Large amounts of computation must be performed for parameters that are zero valued or that will be zero during inference. Additionally, it remains unknown if the performance of the current best pruning algorithms are an upper bound on the quality of sparse models.  Gale et al. (2019)  found that three different dense-to- sparse training algorithms all achieve about the same sparsity / accuracy trade-off. However, this is far from conclusive proof that no better performance is possible. In this work we show the surprising result that dynamic sparse training, which includes the method we introduce below, can find more accurate models than the current best approaches to pruning initially dense networks. Importantly, our method does not change the FLOPs required to execute the model during training, allowing one to decide on a specific inference cost prior to training. The Lottery Ticket Hypothesis ( Frankle & Carbin, 2019 ) hypothesized that if we can find a sparse neural network with iterative pruning, then we can train that sparse network from scratch, to the Under review as a conference paper at ICLR 2020 same level of accuracy, by starting from the original initial conditions. In this paper we introduce a new method for training sparse models without the need of a "lucky" initialization; for this reason, we call our method "The Rigged Lottery" or RigL * . We show that this method is: • Memory efficient: It requires memory only proportional to the size of the sparse model. It never requires storing quantities that are the size of the dense model. This is in contrast to  Dettmers & Zettlemoyer (2019)  which requires storing the momentum for all parameters, even those that are zero valued. • Computationally efficient: The amount of computation required to train the model is pro- portional to the number of nonzero parameters in the model. • Accurate: The performance achieved by the method matches and sometimes exceeds the performance of pruning based approaches. Our method works by infrequently using instantaneous gradient information to inform a re-wiring of the network. We show that this allows the optimization to escape local minima where it would otherwise become trapped if the sparsity pattern were to remain static. Crucially, as long as the full gradient information is needed less than every 1 1−sparsity iterations, then the overall work remains proportional to the model sparsity.

Section Title: RELATED WORK
  RELATED WORK Research on finding sparse neural networks dates back decades; for example, at least to  Thimm & Fiesler (1995)  who concluded that pruning weights based on magnitude was a simple and powerful technique.  Ström (1997)  later introduced the idea of retraining the previously pruned network to increase accuracy.  Han et al. (2016b)  went further and introduced multiple rounds of magnitude pruning and retraining. This is, however, relatively inefficient, requiring ten rounds of retraining when removing 20% of the connections to reach a final sparsity of 90%. To overcome this problem,  Narang et al. (2017)  introduced gradual pruning, where connections are slowly removed over the course of a single round of training.  Zhu & Gupta (2018)  refined the technique to minimize the amount of hyper-parameter selection required. A diversity of approaches not based on magnitude based pruning have also been proposed.  LeCun et al. (1990)  and  Hassibi & Stork (1993)  are some early examples, but impractical for modern neural networks as they use information from the Hessian to prune a trained network. More recent work includes L 0 Regularization ( Christos Louizos, 2018 ), Variational Dropout ( Molchanov et al., 2017 ), Dynamic Network Surgery ( Guo et al., 2016 ) and Sensitivity Driven Regularization ( Tartaglione et al., 2018 ).  Gale et al. (2019)  examined magnitude pruning, L 0 Regularization and Variational Dropout and concluded that they all achieve about the same accuracy versus sparsity trade-off on ResNet-50 and Transformer architectures. Training techniques that allow for sparsity throughout the entire training process were, to our knowl- edge, first introduced in Deep Rewiring (DeepR) ( Bellec et al., 2017 ). In DeepR, the standard Stochastic Gradient Descent (SGD) optimizer is augmented with a random walk in parameter space. Additionally, connections have a pre-defined sign assigned at random; when the optimizer would normally flip the sign, the weight is set to 0 instead and new weights are activated at random. Sparse Evolutionary Training (SET) ( Mocanu et al., 2018 ) proposed a simpler scheme where weights are pruned according to the standard magnitude criterion used in pruning and are added back at random. The method is simple and achieves reasonable performance in practice. Dynamic Sparse Reparameterization (DSR) ( Mostafa & Wang, 2019 ) introduced the idea of allowing the parameter budget to shift between different layers of the model, allowing for non-uniform sparsity. This allows the model to distribute parameters where they are most effective. Unfortunately, the models under consideration are mostly convolutional networks, so the result of this parameter reallocation (which is to decrease the sparsity of early layers and increase the sparsity of later layers) has the overall effect of increasing the FLOP count because the spatial size is largest at the beginning. Sparse Networks from Scratch (SNFS) ( Dettmers & Zettlemoyer, 2019 ) introduces the idea of using the momentum of each parameter as the criterion to be used for growing weights and demonstrates it Under review as a conference paper at ICLR 2020 leads to an improvement in test accuracy. Like DSR, they allow the sparsity of each layer to change and focus on a constant parameter, not FLOP, budget. Importantly, the method requires computing gradients and updating the momentum for every parameter in the model, even those that are zero, at every iteration. This can result in a significant amount of overall computation. Additionally, depending on the model and training setup, the required storage for the full momentum tensor could be prohibitive. Single-Shot Network Pruning (SNIP) ( Lee et al., 2019 ) attempts to find an initial mask with one-shot pruning and uses the saliency score of parameters to decide which parameters to keep. After pruning training proceeds with this static sparse network. Properties of the different sparse training techniques are summarized in  Table 1 . There has also been a line of work investigating the Lottery Ticket Hypothesis ( Frankle & Carbin, 2019 ).  Frankle et al. (2019)  showed that the formulation must be weakened to apply to larger networks such as ResNet-50 ( He et al., 2015 ). In large networks, instead of the original initial- ization, the values after thousands of optimization steps must be used for initialization.  Zhou et al. (2019)  showed that lottery tickets obtain non-random accuracies even before the training has started. Though the possibility of training sparse neural networks with a fixed sparsity mask using lottery tickets is intriguing, it remains unclear whether it is possible to generate such initializations - for both masks and parameters - de novo.

Section Title: RIGGING THE LOTTERY
  RIGGING THE LOTTERY Our method, RigL, is illustrated in  Figure 1 . At regularly spaced intervals our method removes a fraction of connections based on weight magnitudes and activates new ones using instantaneous gradient information. After updating the connectivity, training continues with the updated network until the next update. The main parts of our algorithm, Sparsity Distribution, Update Schedule, Drop Criterion, Grow Criterion, and the various options we considered for each, are explained below. The improved performance of RigL is due to two reasons: the use of a new method for activating connections that is efficient and more effective than choosing at random, and the use of a natural extension to an existing method for distributing parameters statically among convolutional layers. (0) Notation. Given a dataset D with individual inputs x i and targets y i , one can train a neural network to minimize the loss function i L(f θ (x i ), y i ), where f θ (x) is the neural network with (1) Sparsity Distribution (3) Drop (4) Grow Sparse Training Step Is Update Iteration? no yes (2) Update Schedule Initialization Under review as a conference paper at ICLR 2020 parameters θ of length N . The vector θ can be decomposed into parameters θ l , of length N l , for each layer l. A sparse network keeps only a fraction D ∈ (0, 1) of all connections, resulting in a sparsity of S = 1 − D. More precisely, denoting the sparsity of individual layers with s l , the total parameter count of the sparse neural network satisfies l (1 − s l )N l = (1 − S) * N . (1) Sparsity Distribution. There are many ways of distributing the non-zero weights across the layers while satisfying the equality above. We avoid re-allocating parameters between layers during the training process as it makes it difficult to target a specific final FLOP budget, which is important for many inference applications. We consider the following three strategies: 1. Uniform: The sparsity s l of each individual layer is the same as the total sparsity S. We keep the first layer dense (s 0 = 0), since it has negligible number of parameters. 2. Erdős-Rényi: As introduced in  Mocanu et al. (2018) , s l scales with 1 − n l−1 +n l n l−1 *n l , where n l denotes number of neurons at layer l. This enables the number of connections in a sparse layer to scale with the sum of the number of output and input channels. 3. Erdős-Rényi-Kernel (ERK): This method modifies the original Erdős-Rényi formulation by including the kernel dimensions in the scaling factors. In other words, the number of parameters of the sparse convolutional layers are scaled proportional to 1− n l−1 +n l +w l +h l n l−1 *n l *w l *h l , where w l and h l are the width and the height of the l'th convolutional kernel. Sparsity of the fully connected layers scale as in the original Erdős-Rényi formulation. Similar to Erdős- Rényi, ERK allocates higher sparsities to the layers with more parameters while allocating lower sparsities to the smaller ones. In all methods, the bias and batch-norm parameters are kept dense. (2) Update Schedule. The update schedule is defined by the following parameters: (1) the number of iterations between sparse connectivity updates (∆T ), (2) the iteration at which to stop updating the sparse connectivity (T end ), (3) the initial fraction of connections updated (α) and (4) a function f decay , invoked every ∆T iterations until T end , possibly decaying the fraction of updated connec- tions over time. For the latter we choose to use cosine annealing, as we find it slightly outperforms the other methods considered. Alternatives to cosine annealing like a constant schedule and inverse power annealing are studied in the Appendix F. (3) Drop criterion. Over the course of training, we drop the lowest magnitude weights according to the update schedule since they are expected to effect the training loss least. Specifically, we drop the connections given by (4) Grow criterion. The novelty of our method lies in how we grow new connections. We grow the connections with highest magnitude gradients, T opK w / ∈θ l active (|grad(θ l )|, f decay (t)(1−s l )N l ), where θ l active is the set of active connections after the drop step. Newly activated connections are initialized to zero and therefore don't effect the output of the network. However they are expected to receive gradients with high magnitudes in the next iteration and therefore reduce the loss fastest. This procedure can be applied to each layer in sequence and the dense gradients can be discarded immediately after selecting the top connections. If a layer is too large to materialize the full gradient with respect to the weights, then we can further reduce the memory requirements by performing an iterative calculation: 1. Initialize the set T K = {}. 2. Materialize a subset of size M of the full gradient, which we denote G i:i+M . 3. Update T K to contain the Top-K elements of G i:i+M concatenated with T K. 4. Repeat steps 1 through 3 until all of the gradients have been materialized. The final set T K contains the connections we wish to grow. As long as ∆T > 1 1−s the total work in calculating dense gradients is amortized and still propor- tional to 1 − S. This is in contrast to the method of  Dettmers & Zettlemoyer (2019) , which requires calculating and storing the full gradients at each optimization step.

Section Title: EMPIRICAL EVALUATION
  EMPIRICAL EVALUATION Our experiments include image classification using CNNs on the ImageNet-2012 ( Russakovsky et al., 2015 ) and CIFAR-10 (Krizhevsky et al.) datasets and character based language modelling using RNNs with the WikiText-103 dataset ( Merity et al., 2016 ). We use the TensorFlow Model Pruning library ( Zhu & Gupta, 2018 ) for our pruning baselines. A Tensorflow ( Abadi et al., 2015 ) implementation of our method along with three other baselines (SET, SNFS, SNIP) will be open sourced. When we increase the training steps by a factor M , the anchor epochs of the learning rate schedule and the end iteration of the mask update schedule are also scaled by the same factor; we indicate this scaling with a subscript (e.g. RigL M × ).

Section Title: IMAGENET-2012 DATASET
  IMAGENET-2012 DATASET In all experiments in this section, we use SGD with momentum as our optimizer. We set the momen- tum coefficient of the optimizer to 0.9, L 2 regularization coefficient to 0.0001, and label smoothing ( Szegedy et al., 2016 ) to 0.1. The learning rate schedule starts with a linear warm up reaching its maximum value of 1.6 at epoch 5 which is then dropped by a factor of 10 at epochs 30, 70 and 90. We train our networks with a batch size of 4096 for 32000 steps which roughly corresponds to 100 epochs of training. Our training pipeline uses standard data augmentation, which includes random flips and crops. As noted by  Gale et al. (2019) ,  Evci et al. (2019) ,  Frankle et al. (2019) , and  Mostafa & Wang (2019) , training a network with fixed sparsity from scratch (Static) leads to inferior performance. Training a small dense network with the same number of parameters gets better results than Static, but fails to match the performance of dynamic sparse models. Similarly SET improves the performance over Small-Dense, however saturates around 75% accuracy indicating the limits of growing new connec- Under review as a conference paper at ICLR 2020 tions randomly. Methods that use gradient information to grow new connections (RigL and SNFS) obtain higher accuracies, but RigL achieves the highest accuracy and does so while consistently requiring fewer FLOPs than the other methods. Given that different applications or scenarios might require a limit on the number of FLOPs for inference, we investigate the performance of our method at various sparsity levels. As mentioned previously, one strength of our method is that its resource requirements are constant throughout training and we can choose the level of sparsity that fits our training and/or inference constraints. In  Figure 2 -right we show the performance of our method at different sparsities and compare them with the pruning results of  Gale et al. (2019) , which uses 1.5x training steps, relative to the original 32k iterations. To make a fair comparison with regards to FLOPs, we scale the learning schedule of all other methods by 5x. Note that even after extending the training, it takes less FLOPs to train sparse networks using rigL (except for the 80% sparse RigL-ERK) compared to the pruning method. RigL, our method with constant sparsity distribution, exceeds the performance of magnitude based iterative pruning in all sparsity levels while requiring less FLOPs to train. Sparse networks that use Erdős-Renyi-Kernel (ERK) sparsity distribution obtains even greater performance. For exam- ple ResNet-50 with 96.5% sparsity achieves a remarkable 72.75% Top-1 Accuracy, around 3.5% higher than the extended magnitude pruning results reported by  Gale et al. (2019) . As observed earlier, smaller dense models (with the same number of parameters) or sparse models with a static connectivity can not perform at a comparable level. A more fine grained comparison of sparse training methods is presented in  Table 2 . Methods us- ing uniform sparsity distribution and whose FLOP/memory footprint scales directly with (1-S) are placed in the first sub-group of the table. The second sub-group includes DSR and networks with ERK sparsity distribution which require a higher number of FLOPs for inference with same param- eter count. The final sub-group includes methods that require the space and the work proportional to training a dense model.

Section Title: MOBILENET
  MOBILENET MobileNet is a compact architecture that performs remarkably well in resource constrained settings. Due to its compact nature with separable convolutions it is known to be difficult to sparsify ( Zhu & Gupta, 2018 ). In this section we apply our method to MobileNet-v1 ( Howard et al., 2017 ) and MobileNet-v2 ( Sandler et al., 2018 ). Due to its low parameter count we keep the first layer dense, and use ERK and Uniform sparsity distributions to sparsify the remaining layers. The performance of sparse MobileNets trained with RigL as well as the baselines are shown in  Figure 3 . We do extended training (5x of the original number of steps) for all runs in this section. Although MobileNets are more sensitive to sparsity compared to the ResNet-50 architecture, RigL successfully trains sparse MobileNets at high sparsities and exceeds the performance of previously reported pruning results. To demonstrate the advantages of sparse models, next, we train wider MobileNets while keeping the FLOPs and total number of parameters the same as the dense baseline using sparsity. A sparse MobileNet-v1 with width multiplier 1.98 and constant 75% sparsity has the same FLOPs and pa- rameter count as the dense baseline. Training this network with RigL yields an impressive 4.3% absolute improvement in Top-1 Accuracy.

Section Title: CHARACTER LEVEL LANGUAGE MODELLING
  CHARACTER LEVEL LANGUAGE MODELLING Most prior work has only examined sparse training on vision networks [the exception is the earliest work - Deep Rewiring ( Bellec et al., 2017 ) which trained an LSTM ( Hochreiter & Schmidhuber, 1997 ) on the TIMIT ( Garofolo et al., 1993 ) dataset]. To fully understand these techniques it is im- portant to examine different architectures on different datasets.  Kalchbrenner et al. (2018)  found sparse GRUs ( Cho et al., 2014 ) to be very effective at modeling speech, however the dataset they used is not available. We choose a proxy task with similar characteristics (dataset size and vocabu- lary size are approximately the same) - character level language modeling on the publicly available WikiText-103 ( Merity et al., 2016 ) dataset. Our network consists of a shared embedding with dimensionality 128, a vocabulary size of 256, a GRU with a state size of 512, a readout from the GRU state consisting of two linear layers with 256 units and 128 units respectively. We train the next step prediction task with the standard cross entropy loss, the Adam optimizer, a learning rate of 7e−4, an L2 regularization coefficient of 5e−4, a sequence length of 512, a batch size of 32 and gradient absolute value clipping of values larger (in magnitude) than 10. Baseline training length is 200,000 iterations. When inducing sparsity with magnitude pruning ( Zhu & Gupta, 2018 ), we perform pruning between iterations 50,000 and Under review as a conference paper at ICLR 2020 150,000 with a pruning frequency of 1,000. We initialize sparse networks with a uniform sparsity distribution and use a cosine update schedule with α = 0.1 and ∆T = 100. Unlike the previous experiments we keep updating the mask until the end of the training; we observed this performed slightly better than stopping at iteration 150,000. In  Figure 4 -left we report the validation loss of various solutions at the end of the training. For each method we perform extended runs to see how they scale with increasing training time. As observed before, SET performs worst than the other dynamic training methods and its performance improves only slightly with increased training time. On the other hand the performance of RigL and SNFS improves constantly with more training steps. Both of these methods falls short of matching the pruning performance.

Section Title: WIDERESNET-22-2 ON CIFAR-10
  WIDERESNET-22-2 ON CIFAR-10 In this section, we evaluate the performance of RigL on CIFAR-10 image classification benchmark. We train Wide Residual Network's ( Zagoruyko & Komodakis, 2016 ) with 22 layers using a width multiplier of 2 for 250 epochs (97656 steps). Learning rate starts at 0.1 and scaled down by a factor of 5 every 30,000 iterations. We use an L2 regularization coefficient of 5e-4, a batch size of 128 and a momentum coefficient of 0.9. We keep the hyper-parameters specific to RigL same as the ImageNet experiments, except the final iteration for mask updates; which is adjusted to 75000. Results with different mask update intervals can be found in Appendix H. Performance of RigL across different sparsity levels is presented in  Figure 4 -right. Corresponding final training losses of the trained networks can be found in Appendix H. The dense baseline ob- tains 94.1% test accuracy. Networks with half of the connections removed (50% sparsity) achieves roughly the same accuracy as the dense baseline. Surprisingly, some of the networks at this spar- sity level generalize better than the dense baseline demonstrating the regularization aspect of using sparsity. With increased sparsity, we start to see a performance gap between the Static and Pruning solutions. Training static RigL networks longer seems to have limited effect on the final perfor- mance. On the other hand, RigL, matches the performance of pruning using only a fraction of resources needed for training a dense network.

Section Title: ANALYZING THE PERFORMANCE OF RigL
  ANALYZING THE PERFORMANCE OF RigL In this section we study the effect of sparsity distributions, update schedules, and dynamic connec- tions on the performance of our method. The results for SET and SNFS are similar and are discussed in Appendices B and E.

Section Title: Effect of Mask Initialization
  Effect of Mask Initialization layers with few parameters by decreasing their sparsities † . This reallocation seems to be crucial for preserving the capacity of the network at high sparsity levels where ERK outperforms other distri- butions by a greater margin. Though it performs better, the ERK distribution requires approximately twice as many FLOPs compared to a uniform distribution. This highlights an interesting trade-off between accuracy and computational efficiency even though both models have the same number of parameters.

Section Title: Effect of Update Schedule and Frequency
  Effect of Update Schedule and Frequency In  Figure 5 -right, we evaluate the performance of our method on update intervals ∆T ∈ [50, 100, 500, 1000] and initial drop fractions α ∈ [0.1, 0.3, 0.5]. The best accuracies are obtained when the mask is updated every 100 iterations with an initial drop fraction of 0.3 or 0.5. Notably, even with frequent update intervals (e.g. every 1000 iterations), RigL performs above 73.5%. Effect of Dynamic connections:  Frankle et al. (2019)  and  Mostafa & Wang (2019)  observed that static sparse training converges to a solution with a higher loss than dynamic sparse training. In  Figure 6 -left we examine the loss landscape lying between a solution found via static sparse training and a solution found via pruning to understand whether former lies in a basin isolated from the latter. Performing a linear interpolation between the two reveals the expected result - high-loss barrier - demonstrating that the loss landscape is not trivially connected. However, this is only one of infinitely many paths between the two points optimization can be used to find parametric curves that connects solutions ( Garipov et al., 2018 ;  Draxler et al., 2018 ) subject to constraints. For example  Garipov et al. (2018)  showed different dense solutions lie in the same basin by finding 2nd order Bézier curves with low energy between the two solutions. Following their method, we attempt † see Appendix I for exact layer-wise sparsities given by ERK for 80% and 90% sparse ResNet-50's. Under review as a conference paper at ICLR 2020 to find quadratic and cubic Bézier curves between the two sparse solutions. Surprisingly, even with a cubic curve, we fail to find a path without a high-loss barrier. These results suggest that static sparse training can get stuck at local minima that are isolated from improved solutions. On the other hand, when we optimize the quadratic Bézier curve across the full dense space we find a near-monotonic path to the improved solution, suggesting that allowing new connections to grow lends dynamic sparse training greater flexibility in navigating the loss landscape. In  Figure 6 -right we train RigL starting from the sub-optimal solution found by static sparse train- ing, demonstrating that it is able to escape the local minimum, whereas re-training with static sparse training cannot. RigL first removes connections with the smallest magnitudes since removing these connections have been shown to have a minimal effect on the loss ( Han et al., 2015 ;  Evci, 2018 ). Next, it activates connections with the high gradients, since these connections are expected to de- crease the loss fastest. We hypothesize in Appendix A that RigL escapes bad critical points by replacing saddle directions with high gradient dimensions.

Section Title: DISCUSSION & CONCLUSION
  DISCUSSION & CONCLUSION In this work we introduced 'Rigged Lottery' or RigL, an algorithm for training sparse neural net- works efficiently. For a given computational budget RigL achieves higher accuracies than existing dense-to-sparse and sparse-to-sparse training algorithms. RigL is useful in three different scenarios: (1) To improve the accuracy of sparse models intended for deployment; (2) To improve the accuracy of large sparse models which can only be trained for a limited number of iterations; and (3) Com- bined with sparse primitives to enable training of extremely large sparse models which otherwise would not be possible. The third scenario is unexplored due to the lack of hardware and software support for sparsity. Nonetheless, work continues to improve the performance of sparse networks on current hard- ware ( Hong et al., 2019 ;  Merrill & Garland, 2016 ), and new types of hardware accelerators will have better support for parameter sparsity ( Wang et al., 2018 ;  Mike Ashby, 2019 ;  Liu et al., 2018 ;  Han et al., 2016a ;  Chen et al., 2019 ). RigL provides the tools to take advantage of, and motivation for, such advances.

```
