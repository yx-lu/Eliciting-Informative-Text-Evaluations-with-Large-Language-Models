Title:
```
None
```
Abstract:
```
We re-think the Two-Player Reinforcement Learning (RL) as an instance of a dis- tribution sampling problem in infinite dimensions. Using the powerful Stochastic Gradient Langevin Dynamics, we propose a new two-player RL algorithm, which is a sampling variant of the two-player policy gradient method. Our new algorithm consistently outperforms existing baselines, in terms of generalization across dif- fering training and testing conditions, on several MuJoCo environments.
```

Figures/Tables Captions:
```
Figure 1: Average performance (over 5 seeds) of Algorithm 3 (-), and Algorithm 4 (-), under the NR-MDP setting with δ = 0.1. The evaluation is performed without adversarial perturbations, on a range of mass values not encountered during training.
Figure 2: Average performance (over 5 seeds) of Algorithm 3 (-), and Algorithm 4 (-), under the NR-MDP setting with δ = 0.1. The evaluation is performed on a range of noise probability and mass values not encountered during training.
Figure 3: Average performance (over 5 seeds) of Algorithm 3 (-), and Algorithm 4 (-), under the NR-MDP setting with δ = 0. The evaluation is performed without adversarial perturbations, on a range of mass values not encountered during training.
Figure 4: Average performance (over 5 seeds) of Algorithm 3 (-), and Algorithm 4 (-), under the NR-MDP setting with δ = 0. The evaluation is performed on a range of noise probability and mass values not encountered during training.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) promise automated solutions to many real-world tasks with beyond- human performance. Indeed, recent advances in policy gradient methods ( Sutton et al., 2000 ;  Silver et al., 2014 ;  Schulman et al., 2015 ; 2017) and deep reinforcement learning have demonstrated im- pressive performance in games ( Mnih et al., 2015 ;  Silver et al., 2017 ), continuous control ( Lillicrap et al., 2015 ), and robotics ( Levine et al., 2016 ) towards this grand challenge. Despite the success of deep RL, the progress is still upset by the fragility in real-life deployments. In particular, majority of these methods fail to perform well when there is some difference between training and testing scenarios, thereby posting serious safety and security concerns. To this end, learning policies that are robust to environmental shifts, mismatched configurations, and even mis- matched control actions is becoming increasingly more important. A powerful framework to learning robust policies is to interpret the changing of the environment as an adversarial perturbation. This notion naturally lends itself to a two-player minimax problem involving a pair of agents, a protagonist and an adversary, where the protagonist learns to fulfill the original task goals while being robust to the disruptions generated by its adversary. Two prominent examples along this research vein, differing in how they model the adversary, are the Robust Adversarial Reinforcement Learning (RARL) ( Pinto et al., 2017 ) and Noisy Robust Markov Decision Process (NR-MDP) ( Tessler et al., 2019 ). Despite achieving impressive performance in practice, these existing frameworks heavily rely on heuristic algorithms, and hence, suffer from lack of theoretical guarantees, even in idealistic cases with infinite data as well as computational power. One critical challenge in robust RL setting is that while maximizing rewards is a well-studied sub- ject in classical/deep RL, the needed two-player minimax version is significantly more complicated to solve both in theory and practice. For instance,  Tessler et al. (2019)  prove that it is in fact strictly suboptimal to directly apply (deterministic) policy gradient steps to their NR-MDP max-min objec- tives. Owing to the lack of a better algorithm, the policy gradient is nonetheless still employed in their experiments; similar comments also apply to ( Pinto et al., 2017 ). Our paper precisely bridges this gap between theory and practice in previous works, by proposing the first theoretically convergent algorithm for robust RL. Our key idea is to switch from optimizing the max-min reward to sampling from the optimal randomized policies, which corresponds to finding a mixed Nash Equilibrium (NE) ( Nash et al., 1950 ) in the max-min objective. It is a classical fact in game theory that, while deterministic minimax objective is often ill-posed, the mixed NE is well-behaved under very mild assumptions. Furthermore, algorithmic approaches to finding mixed NE with finite strategies have been studied extensively ( Freund & Schapire, 1999 ;  Nemirovski, 2004 ) and is recently extended to the case of infinite strategies ( Hsieh et al., 2019 ). In particular, ( Hsieh et al., 2019 ) show that, by using the Stochastic Gradient Langevin Dynamics Under review as a conference paper at ICLR 2020 (SGLD) ( Welling & Teh, 2011 ) to take samples from randomized strategies, one can find a mixed NE of Generative Adversarial Networks ( Goodfellow et al., 2014 ). Our work introduces the same mixed NE perspective to the max-min objectives in robust RL, and substantiate the ensuing theoretical framework with extensive experimental evidence. We apply the new sampling framework to the well-known Deep Deterministic Policy Gradient (DDPG) method in the scope of NR-MDP. We demonstrate that the new algorithm achieves clearly superior performance in its generalization capabilities. Intriguingly, we also observe that the idea of mixed strategy in single-player RL (i.e., the non-robust or one-player formulation) can lead to substantially more robust policies over the standard DDPG algorithm. More precisely, we represent the agent's policy as a distribution over deterministic poli- cies, and aim to sample from the distribution µ(π) ∝ exp(− 1 σ J(π)) where π denotes a deterministic policy, J the associated expected reward, and σ a temperature parameter going to 0 during training. Our numerical evidence demonstrates that DDPG combined with this sampling approach for the actor update leads to learned policies that generalize better to unseen MDPs, when compared to the state-of-the-art DDPG variant of  Tessler et al. (2019)  while using similar computational resources.

Section Title: BACKGROUND
  BACKGROUND

Section Title: STOCHASTIC GRADIENT LANGEVIN DYNAMICS (SGLD)
  STOCHASTIC GRADIENT LANGEVIN DYNAMICS (SGLD) For any probability distribution p (z) ∝ exp (−g (z)), the Stochastic Gradient Langevin Dynamics (SGLD)  Welling & Teh (2011)  iterates as z k+1 ← z k − γ ∇ z g (z) z=z k + 2γ ξ k , (1) where γ is the step-size, ∇ z g (z) is an unbiased estimator of ∇ z g (z), > 0 is a temperature parameter, and ξ k ∼ N (0, I) is a standard normal vector, independently drawn across different iterations. In some cases, the convergence rate of SGLD can be improved by scaling the noise using a positive-definite symmetric matrix C. We thus define a preconditioned variant of the above update equation 1 as follows: In the experiments, we use a RMSProp-preconditioned version of the SGLD ( Li et al., 2016 ).

Section Title: INFINITE-DIMENSIONAL BI-LINEAR GAMES
  INFINITE-DIMENSIONAL BI-LINEAR GAMES In this section, we review some of the key results from ( Hsieh et al., 2019 ). We denote the set of all probability measures on Z by P (Z), and the set of all functions on Z by F (Z). Given a (sufficiently regular) function h : Θ × Ω → R, consider the following objective (a two-player game with infinitely many strategies): A pair (p * , q * ) achieving the max-min value in equation 3 is called a mixed Nash Equilibrium (NE). Define the operator G : P (Ω) → F (Θ), and its adjoint operator G † : P (Θ) → F (Ω) as follows: Denoting p, g := E z∼p [g (z)] for any probability measure p and function g on Z, we can write f as f (p, q) = p, Gq = G † p, q . Furthermore, the derivative (the analogue of gradient in infinite dimension) of f (p, q) with respect to p is simply Gq, and the derivative of f (p, q) with respect to q is G † p; i.e., ∇ p f (p, q) = Gq, and ∇ q f (p, q) = G † p. Conceptually, problem (3) can be solved via the so-called infinite-dimensional entropic mirror de- scent; see Algorithm 1.  Hsieh et al. (2019)  have proved the convergence (to the mixed NE) rate of Algorithm 1. But this algorithm is infinite-dimensional and requires infinite computational power to implement. For practical interest, by leveraging the SGLD sampling techniques and using some practical relaxations,  Hsieh et al. (2019)  have proposed a simplified variant of Algorithm 1. The pseudocode for their resulting algorithm can be found in Algorithm 2.

Section Title: TWO-PLAYER MARKOV GAMES
  TWO-PLAYER MARKOV GAMES

Section Title: Markov Decision Process
  Markov Decision Process We consider a Markov Decision Process (MDP) represented by M 1 := (S, A, T 1 , γ, P 0 , R 1 ), where the state and action spaces are denoted by S and A respec- tively. We focus on continuous control tasks, where the actions are real-valued, i.e., A = R d . T 1 : S × S × A → [0, 1] captures the state transition dynamics, i.e., T 1 (s | s, a) denotes the probability of landing in state s by taking action a from state s. Here γ is the discounting factor, P 0 : S → [0, 1] is the initial distribution over states S, and R 1 : S × A → R is the reward.

Section Title: Two-Player Zero-Sum Markov Games
  Two-Player Zero-Sum Markov Games Consider a two-player zero-sum Markov game  Littman (1994) ;  Perolat et al. (2015) , where at each step of the game, both players simultaneously choose an action. The reward each player gets after one step depends on the state and the joint action of both players. Furthermore, the transition kernel of the game is controlled jointly by both the players. In this work, we only consider simultaneous games, not the turn-based games. This game can be described by an MDP M 2 = (S, A, A , T 2 , γ, R 2 , P 0 ), where A and A are the continuous set of actions the players can take, T 2 : S × A × A × S → R is the state transition probability, and R 2 : S × A × A → R is the reward for both players. Consider an agent executing a policy µ : S → A, and an adversary executing a policy ν : S → A in the environment M. At each timestep t, both players observe the state s t and take actions a t = µ (s t ) and a t = ν (s t ). In the zero-sum game, the agent gets a reward r t = R 2 (s t , a t , a t ) while the adversary gets a negative reward −r t .

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 This two-player zero-sum Markov game formulation has been used to model the following robust RL settings: • Robust Adversarial Reinforcement Learning (RARL) ( Pinto et al., 2017 ), where the power of the adversary is limited by the action space A of the adversary. • Noisy Robust Markov Decision Process (NR-MDP) ( Tessler et al., 2019 ), where A = A, In our adversarial game, we consider the following performance objective: J (µ, ν) = E ∞ t=1 γ t−1 r t µ, ν, M , where ∞ t=1 γ t−1 r t be the random cumulative return. In particular, we consider the parameterized policies {µ θ : θ ∈ Θ}, and {ν ω : ω ∈ Ω}. By an abuse of notation, we denote J (θ, ω) = J (µ θ , ν ω ). We consider the following objective: Note that J is non-convex/concave in both θ and ω. Instead of solving equation 4 directly, we focus on the mixed strategy formulation of equation 4. In other words, we consider the set of all probability distributions over Θ and Ω, and we search for the optimal distribution that solves the following program: Then, we can use the techniques from Section 2.2 to solve the above problem.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we demonstrate the effectiveness of using infinite-dimensional sampling techniques to solve the robust RL problem.

Section Title: Two-Player DDPG
  Two-Player DDPG As a case study, we consider NR-MDP setting with δ = 0.1 (as recom- mended in Section 6.3 of ( Tessler et al., 2019 )). We design a two-player variant of DDPG ( Lillicrap et al., 2015 ) algorithm by adapting the Algorithm 2. As opposed to standard DDPG, in two-player DDPG two actor networks output two deterministic policies, the protagonist and adversary policies, denoted by µ θ and ν ω . The critic is trained to estimate the Q-function of the joint-policy. The gradi- ents of the protagonist and adversary parameters are given in Proposition 5 of ( Tessler et al., 2019 ). The resulting algorithm is given in Algorithm 3. We compare the performance of our algorithm against the baseline algorithm proposed in ( Tessler et al., 2019 ) (see Algorithm 4). ( Tessler et al., 2019 ) have suggested a training ratio of 1 : 1 for actors and critic updates. Note that the action noise is injected while collecting transitions for the replay buffer. In  Fujimoto et al. (2018) , authors noted that the action noise drawn from the  Ornstein- Uhlenbeck Uhlenbeck & Ornstein (1930)  process offered no performance benefits. Thus we also consider uncorrelated Gaussian noise.

Section Title: Setup
  Setup We evaluate the performance of Algorithm 3 and Algorithm 4 on standard continuous control benchmarks available on OpenAI Gym  Brockman et al. (2016)  utilizing the MuJoCo envi- ronment  Todorov et al. (2012) . Specifically, we benchmark on eight tasks: Walker, Hopper, Half- Cheetah, Ant, Swimmer, Reacher, Humanoid, and InvertedPendulum. Details of these environments can be found in  Brockman et al. (2016)  and on the GitHub website. The Algorithm 3 implementation is based on the codebase from ( Tessler et al., 2019 ). For all the algorithms, we use a two-layer feedforward neural network structure of (64, 64, tanh) for both actors (agent and adversary) and critic. The optimizer we use to update the critic is  Adam Kingma & Ba (2015)  with a learning rate of 10 −3 . The target networks are soft-updated with τ = 0.999. For the baseline, the actors are trained with RMSProp optimizer. For our algorithm, the actors are updated according to Algorithm 2 with warmup steps K t = min 15, (1 + 10 −5 ) t , and thermal noise σ t = σ 0 × (1 − 5 × 10 −5 ) t . The hyperparameters that are not related to exploration (see Table 1) are identical to both algorithms that are compared. And we tuned only the exploration-related hyper-parameters (for both algorithms) by grid search: (a) Algorithm 3 with (σ 0 , σ) ∈ 10 −2 , 10 −3 , 10 −4 , 10 −5 × {0, 0.01, 0.1, 0.2, 0.3, 0.4} ; (b) Al- gorithm 4 with σ ∈ {0, 0.01, 0.1, 0.2, 0.3, 0.4}. For each algorithm-environment pair, we identified the best performing exploration hyperparameter configuration (see Tables 2 and 3). Each algorithm is trained on 0.5M samples (i.e., 0.5M time steps in the environment). We run our experiments, for each environment, with 5 different seeds. The exploration noise is turned off for evaluation.

Section Title: Evaluation
  Evaluation We evaluate the robustness of both algorithms under different testing conditions, and in the presence of adversarial disturbances in the testing environment. We train both algorithms with the standard mass variables in OpenAI Gym. At test time, we evaluate the learned policies by changing the mass values (without adversarial perturbations) and estimating the cumulative rewards. As shown in  Figure 1 , our Algorithm 3 outperforms the baseline Algorithm 4 in terms of robust- ness. We also evaluate the robustness of the learned policies under both test condition changes, and adversarial disturbances (see  Figure 2 ).

Section Title: One-Player DDPG
  One-Player DDPG We evaluate the robustness of one-player variants of Algorithm 3, and Al- gorithm 4, i.e., we consider the NR-MDP setting with δ = 0. In this case, we set K t = 1 for Under review as a conference paper at ICLR 2020 Algorithm 3 (this choice of K t makes the computational complexity of both algorithms equal). The results are presented in  Figures 3  and 4. Here, we remark that Algorithm 3 with δ = 0, and K t = 1 is simply the standard DDPG with actor being updated by preconditioned version of SGLD. Thus we achieve robustness under dif- Under review as a conference paper at ICLR 2020 ferent testing conditions with just a simple change in the DDPG algorithm and without additional computational cost.

Section Title: CONCLUSION
  CONCLUSION In this work, we studied the robust reinforcement learning problem. By adapting the approximate infinite-dimensional entropic mirror descent from ( Hsieh et al., 2019 ), we design a robust variant of DDPG algorithm, under the NR-MDP setting. In our experiments, we evaluated the robustness of our algorithm on several continuous control tasks, and found that our algorithm clearly outperformed the baseline algorithm from ( Tessler et al., 2019 ).

```
