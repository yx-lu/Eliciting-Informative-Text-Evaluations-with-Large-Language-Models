Title:
```
None
```
Abstract:
```
Quantifying, enforcing and implementing fairness emerged as a major topic in machine learning. We investigate these questions in the context of deep learning. Our main algorithmic and theoretical tool is the computational estimation of similarities between probability, "à la Wasserstein", using adversarial networks. This idea is flexible enough to investigate different fairness constrained learning tasks, which we model by specifying properties of the underlying data generative process. The first setting considers bias in the generative model which should be filtered out. The second model is related to the presence of nuisance variables in the observations producing an unwanted bias for the learning task. For both models, we devise a learning algorithm based on approximation of Wasserstein distances using adversarial networks. We provide formal arguments describing the fairness enforcing properties of these algorithm in relation with the underlying fairness generative processes. Finally we perform experiments, both on synthetic and real world data, to demonstrate empirically the superiority of our approach compared to state of the art fairness algorithms as well as concurrent GAN type adversarial architectures based on Jensen divergence.
```

Figures/Tables Captions:
```
Figure 1: Fairness type 1.
Figure 2: Fairness type 2.
Figure 3: Networks for fairness type 1
Figure 4: Fair networks for fairness type 2
Figure 5: Accuracy / fairness tradeoffs between our Wasserstein approach and more traditional GAN approaches similar to Beutel et al. (2017); Madras et al. (2018) for demographic parity.
Figure 6: Fair auto encoder.a) T-shirt, b) fair representation of T-shirt, c) shirt, d)fair representation of shirt Learning based on fair representations: To illustrate Proposition 1 and the fact that our fairness constrinat can be applied to other type of problem than classification, we consider classification of T-shirts versus shirts (Y = {T shirt, shirt}) in the fashion-MNIST dataset Xiao et al. (2017) (12000 training examples, 2000 test examples). These two classes are known to be the most challenging to distinguish in this dataset (accuracy around 0.9). We bias the dataset by adding a color (S = {turquoise, yellow}) correlated to the class variable Y : P (S = yellow|Y = T − shirt) = 0.9, P (S = T urquoise|Y = shirt) = 0.9. We apply the following experimental process: train on the biased dataset, and compare validation performances both on the biased test set and the same biased test set, with switched colors: P (S = yellow|Y = T − shirt) = 0.1 and P (S = T urquoise|Y = shirt) = 0.1.
Table 1: Accuracy, Disparate impact and EMD for adult, bank and CelebA under demographic parity and equality of odds constraint
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Along the last few years, much emphasis has been laid on fairness issues in machine learning. Actually, when the learning sample presents biases, these are learnt by algorithms based on loss functions promoting closeness to observed data. Using such models for decision making generalizes biases to the whole population. This drawback of machine learning, also known as unfairness, has become a major challenge in the domain. For a recent survey on this topic we refer to  Dwork et al. (2012) ;  Zemel et al. (2013)  or Friedler et al. and references therein. Fairness usually deals with situations where an algorithm exhibits a different behavior for two different subgroups of the population, while these subgroups should not play any role in its outcome. This situation is often modeled as follows : the algorithm should aims at forecasting a variable Y based on observations X. Fairness is then defined with respect to a protected variable, called protected attribute, S which represents membership to each population subgroup. The algorithm is called fair if its predictions does not depend too much on S. Defining and quantifying this notion of dependency is a complicated task and has received much attention. One of the main tools is the so-called disparate impact which measures if the decision taken by an algorithm differs from one group to another. Absence of disparate impact is called demographic parity. Another measure of fairness is given by the dependency of prediction error with respect to S. The independent case is a form of fairness called equality of odds. We refer for instance to  Chouldechova (2017) ,  Friedler et al. (2016)  or Besse et al. (2018) and references therein. Both situations amounts to considering that either the distribution of the prediction, or its conditional distribution given the target variable Y , does not depend on S. Hence fairness quantification can be naturally implemented using distance between conditional distributions. This point of view has been extensively studied when trying to "repair" data sets as described for instance in  Feldman et al. (2015) ,  Johndrow & Lum (2017) ,  Hacker & Wiedemann (2017)  or  Friedler et al. (2019) . This solution consists in changing the input data so that predictability of the protected Under review as a conference paper at ICLR 2020 attribute is impossible. The data will be blurred in order to obtain a fair treatment of the protected class. The natural distance to measure the difference between the conditional distributions is the so-called Wasserstein distance, which provides an alternative framework to measure the dependency of the decision rule with respect to the protected attribute as shown in  Barrio et al. (2019a)  or  Barrio et al. (2019b) . Yet previous methods face the difficulty of computing the Wasserstein distance which is a challenging task as shown in  Peyré & Cuturi (2019) . In this work, we aim at building fair classifiers by considering a Wasserstein type constraint. Adding constraints to the classifiers to get fair behavior has been studied in several papers. We refer to  Friedler et al. (2019) ,  Zafar et al. (2017a)  and references therein. Our approach, yet sharing some similarities with  Edwards & Storkey (2016) , based on  Ganin et al. (2016) , is more flexible and enables to solve wider classes of fairness problems based on different adversarial architecture resulting in more suited loss functions. Wasserstein constraint for fairness has also been considered in  Jiang et al. (2019)  for binary logistic regression. In the following, we provide algorithms which, for both demographic parity and equality of odds, can incorporate fairness constraints based on the 1-Wasserstein distance. Here we will consider two different mathematical models, describing the relationships between the variables X the target variable Y and the protected variable S. Computing Wasserstein type constraints is difficult, we use neural networks as they have been proved useful to estimate Wasserstein type distances as discussed in  Arjovsky et al. (2017) . The proposed approach can be combined with any kind of neural network predictor. Hence we are able to manage a large variety of input data structure (e.g. images) as well as output labels (multiclass, regression, images . . . ). We demonstrate on fairness benchmark datasets that the proposed Wasserstein approximation framework outperforms both classical fair algorithms (e.g fair SVM) as well as similar adversarial architectures based on Jensen / GAN losses very close to the approaches described in  Beutel et al. (2017) ; Madras et al. (2018). The paper falls into the following parts. Section 2 is devoted to the presentation of Wasserstein distance, approximation schemes and applications to fair modeling. Section 3 describes a first model of fairness related to demographic parity. Section 4 considers a second option connected to equality of odds. For both models, we propose an adversarial network methodology to obtain a fair classifier for each type of fairness. Section 5 studies these algorithms on real benchmark data sets as well as synthetic simulations.

Section Title: FAIRNESS : DEFINITIONS AND METRICS
  FAIRNESS : DEFINITIONS AND METRICS

Section Title: FRAMEWORK
  FRAMEWORK The statistical model we consider is the following. The problem consists in forecasting a binary variable Y ∈ {0, 1}, using observed covariates X ∈ R d , d ≥ 1. We assume moreover that the population can be divided into two categories that represent a bias, modeled by a variable S ∈ {0, 1}. This variable is called the protected, or sensitive, attribute which takes the values S = 0 for the " minority" class and S = 1 for the " majority " class. 1 We observe n joint realizations of these variables D = {(X i , S i , Y i ), i ∈ {1, . . . , n}}. We use the following notations The fair classification problem aims at predicting Y from the variables X, using a family of binary classifiers g ∈ G : R d → {0, 1} without using the information conveyed by S. For every g ∈ G, the outcome of the classification will be the predictionŶ = g(X). We consider in the following that the classifier g comes from a score given by a predictor F : R d → R such thatŶ = 1 F (X)>η for a chosen threshold η > 0. Different criteria have been proposed for measuring the fairness off depending of the context. The disparate impact (DI) measures the sensitivity of the predicted valuesŶ with respect to S.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 The most favorable situation in terms of fairness with respect to the protected attribute S, is achieved when DI(Ŷ , S) = 0 (i.e. P (Ŷ = 1|S = 0) = P (Ŷ = 1|S = 1)), which corresponds to the situation known as demographic parity. In this case, the predicted class is independent from S. While its interpretation is clear, the mathematical properties of the disparate impact measure are not favorable, in particular it lacks robustness and smoothness features which would be necessary to blend algorithmic practice and mathematical theory. In the following, we propose an alternative measure of equality of opportunity which features smoothness properties and comes with a strong mathematical background. Given a score function F , we set L 1 (F (X)) = L(F (X)|S = 1) and L 0 (F (X)) = L(F (X)|S = 0) the laws of conditional distribution of the score for each class and denote the corresponding quantile functions by Q 0,F and Q 1,F . Independence of the decision with the variable S would entail that the repartition of the scores is similar for the two subgroups. So the distance between the quantiles of these two distributions acts as a measure of fairness measuring that the repartition of the score is spread in a similar ways whatever the values of the protected attribute, hence acting as a sensitivity index of the predicted values F (X) with respect to S. Namely define which corresponds to the so-called earth-mover or W 1 Wasserstein distance between the conditional distributions. Clearly W(L 0 (F (X)), L 1 (F (X))) = 0 implies that DI(Ŷ , S) = 0. So Wasserstein distance appears in this framework as a smooth criterion to assess the sensitivity w.r.t to the protected variable. This criterion corresponds to the quantity that is used to measure fairness in  Barrio et al. (2019b)  and  Barrio et al. (2019a) . Note that Wasserstein distance for fairness has been also considered in the seminal paper by  Feldman et al. (2015) . Another important criterion is the equality of odds, which measures the influence of the S on the accuracy of the algorithm. For this the prediction errors across the different class groups are compared and this notion of fairness is achieved when P (Ŷ = 1|Y = 1, S = 0) = P (Ŷ = 1|Y = 1, S = 1) and P (Ŷ = 0|Y = 0, S = 0) = P (Ŷ = 0|Y = 0, S = 1). Here again, this condition can be interpreted as a notion of independence of the conditional distributions defined for (i, s) ∈ {0, 1} 2 as L i s (f (X)) the distribution of the random variable (f (X)|Y = i, S = s). Hence as we exposed for the notion of equality of opportunity, fairness will be assessed through the computations of the Wasserstein distances Note that in some cases, we are only interested in equality of opportunities. This corresponds to the case where we only require that P (Ŷ = 1|Y = 1, S = 0) = P (Ŷ = 1|Y = 1, S = 1) as pointed out in  Hardt et al. (2016) . Hence in this case it amounts to control only W(L 1 0 (F (X)), L 1 1 (F (X))).

Section Title: WASSERSTEIN DIVERGENCES USING NEURAL NETWORKS AND PROPERTIES
  WASSERSTEIN DIVERGENCES USING NEURAL NETWORKS AND PROPERTIES The earth-mover, or Wasserstein-1 distances between probability distribution is defined as follows : W(L 1 , L 2 ) = inf γ∈Π(L1,L2) E X,Y ∼γ X − Y (2) where Π(L 1 , L 2 ) is the set of all probability measures on X, Y with marginals L 1 and L 2 . Disparate Impact is closely related to the notion of unpredictability of the variable S. Hence the aim in this case is to The distance associated to these notions is the total variation distance d TV (L 1 , L 2 but due to intractability of this distance, it has been replaced in the machine learning literature by W. Clearly the independent case is obtained when the distance is null and the decrease of W leads to smaller DI as shown in the experiments in  Barrio et al. (2019a) . Hence a constraint on the Wasserstein distance promotes fairness, also in terms of Disparate Impact. Although the infimum in Equation (2) is not tractable in general, it can be approximated by a neural network. The first step is to reformulate (2) using the  Kantorovich-Rubinstein duality Villani (2008) : W(L 1 , L 2 ) = sup f ∈F1 E X∼L1 f (X) − E X∼L2 f (X) (3) where F 1 denotes the space of 1 Lipschitz function. As a second step, the approach proposed in  Arjovsky et al. (2017)  is based on estimation of the supremum in (3) by replacing F 1 by the set of functions described by a fixed neural network architecture with spectral normalization Miyato et al. Under review as a conference paper at  ICLR 2020 (2018) . This provides a general methodology to estimate and optimize divergences à la Wasserstein and leads to interesting empirical results  Arjovsky et al. (2017) . Furthermore, we demonstrate empirically that this approach allows to control to some extent the empirical EMD divergence introduced above. The two examples of fairness measures which we have introduced are based on distributional divergence measured using Wasserstein distance and we propose to handle these divergence terms computationally using the dual formulation presented in this paragraph. One specificity of the fairness problem which we consider is that, empirically, we only have access to a finite number of samples for each values of the protected attribute (S ∈ {0, 1}). For example, we only have access to L 0 and L 1 through a fixed finite sample and the Wasserstein terms which we manipulate are only computed on finitely many samples. This raises the following comments. Other divergences, such as  Jensen-Shannon Goodfellow et al. (2014) , KL divergence or total variation  Arjovsky et al. (2017)  can be approximated using neural networks. These divergences reflect similarities between mutually absolutely continuous probability measures. However they degenerate when considering singular measures. For the problems which we intend to attack in this work, we aim at enforcing equality of distributions using only a fixed number of samples. Entropy or total variation based divergences fail to capture dissimilarity between singular measures, and in particular they degenerate when considering disjoint finite sample sets. On the other hand, Wasserstein metric is well defined and does not degenerate on empirical distributions given by finite samples. A second favorable property of this metric is its continuity features. When considering parametric distributions, this translate into continuity of the metric in the parameter space as remarked by  Arjovsky et al. (2017) , resulting in numerically more favorable situations compared to discontinuous problems. Another important consequence of the continuity properties of Wasserstein distance is that it translates into stable approximation of distribution divergence in the limit of large i.i.d samples (see Appendix). This last property is very desirable since all we can do from an empirical perspective is limited to finite samples.

Section Title: TYPE 1 FAIR LEARNING : DEMOGRAPHIC PARITY
  TYPE 1 FAIR LEARNING : DEMOGRAPHIC PARITY The first case where fairness is desirable corresponds to the situation where the target variable is biased (....). For instance, it is well-known that the income of a people is biased by the gender. The situation doesn't arise from a biased gathering of data but from bias that exist in the real data and that we don't want to reproduce in our model. Thus, a fair model in this case will change the prediction in order to make them independent from the protected variable. A suitable objective for this problem is to obtain of disparate impact (or the SDI) as close as possible to 1. This situation is formally represented in  Figure 1 . In this situation we require that : X ⊥ ⊥ S|Y and Y * ⊥ ⊥ S|Y where Y * is not observed. Note that, as it intuitively expected, Y is not independent from S (even conditionally to X). In this example, Y * could represent an ideal case where the income level reflect the proficiency and not the gender.

Section Title: TYPE 1 FAIR NEURAL NETWORKS
  TYPE 1 FAIR NEURAL NETWORKS The following can be applied either in multivariate regression, Y ∈ R d , or classification Y = {0, 1}, we consider the type 1 configuration described in  Figure 1 . We propose a neural network model with adversarial Wasserstein constraints on the output as described in  Figure 3  In this networks, the function F is a classifier or regressor. In order to have a prediction independent from S, we add Under review as a conference paper at ICLR 2020 update F by gradient descent : 9: end for Wasserstein penalization reflecting the dependency between F (X) and S, we obtain the following optimization: where l is the loss function for the problem. Note that the Wasserstein penalty term in (4) is exactly the EM D fairness measure which we introduced in (1). Applying the approximation scheme described in Section 2.2, we obtain the following saddle point problem : λ > 0 are hyper-parameters and F s represents the set of functions encoded by a fixed architecture neural network with spectral normalization. Approximating expectations using empirical sample, the learning process for this model is described in Alg. 1. As explained in Section 2.2, in the limit of large samples, the maximization in A provide a proxy for the Wasserstein distance between the two conditional distributions. Note that we can use a similar architecture for equality of opportunities.

Section Title: TYPE 2 FAIR LEARNING : EQUALITY OF ODDS
  TYPE 2 FAIR LEARNING : EQUALITY OF ODDS The second case where fairness is desirable corresponds to the situation where the data are subject to a bias nuisance variable which is in principle of no help for the learning task at hand and which influence should be removed. On famous example is the dog vs wolf problem exposed in  Ribeiro et al. (2016) . In this example, that data was heavily biased by the presence, for the wolfs, and the absence, for the dogs, of snow in the picture. Although the presence of snow is not independent from the presence of wolfs, we prefer a model that focuses on animal features rather than background. More generally, this kind of situation appears when the descriptors or target variables show dependency with the protected variable due to a biased data collection process or when we plan to use the model on data that have a different distribution with respect to the protected variable (this could be the case Under review as a conference paper at ICLR 2020 if we want to detect wolfs and dogs in a snow free area). The equality of odds is a suitable objective for this type of fairness. We represent the underlying data generation process formally in  Figure 3 , we require the following conditional independence: X * ⊥ ⊥ S|Y, Y ⊥ ⊥ S|X * where X * is not observed. Note that, as it is intuitively expected, Y and X are not independent from S (even conditionally on any other variable in the model). In this context, we suppose that there is a representation of the data X * from which we can build a model to predict Y which will be independent of S given Y . Back to our example for wolf and dog, X could be the pictures, X * could be physical features of the animal. A model learnt from this X * could predict Y = {dog, wolf } from a picture independently of the presence of snow (even if the probability of observing snow is greater when the animal on a picture is a wolf).

Section Title: TYPE 2 FAIR NEURAL NETWORKS FOR BINARY CLASSIFICATION
  TYPE 2 FAIR NEURAL NETWORKS FOR BINARY CLASSIFICATION In the following, we consider the case where Y ∈ {0, 1} and type 2 configuration. We propose a neural network model with adversarial Wasserstein constraints as described in  Figure 3 .1 In this networks, the function F • T (X) (or F (Z) with Z = T (X)) is a classifier constructed in two steps : a transformation T : X → Z and a classifier F : Z → F (Z) ∈ 0, 1. We expect to build T such that Z has the same properties as X * (i.e. X * ⊥ ⊥ S|Y ). In order to achieve this goal, we constraint the distribution Z conditionally to Y to be independent of S. Based on this idea, we obtain the following optimization problem : inf F,T E X [l(F (T (X)), Y )] + λ W(L 0 0 (T (X)), L 0 1 (T (X))) + W(L 1 0 (T (X)), L 1 1 (T (X))) (5) where l is a given loss function (binary cross entropy in our setting). We then apply the approximation procedure of Wasserstein distance described in Section 2.2 and obtain the following saddle point problem : where λ > 0 are hyper-parameters of the method and F s describes all functions generated by a given fixed architecture neural network with spectral normalization  Miyato et al. (2018) . Based on finite sample approximation of the various expectations in this formulation, the learning process is similar to Alg. 1 and is fully described in the appendix. The supremum over A 0 and A 1 for finite sample approximation of the expectations is a proxy to the Wasserstein distance between the two conditional distributions of interest in 2.1. Moreover, Property 1 states that in the limit of Wasserstein distance between conditional distribution set to 0 the latent space Z = T (X) satisfies the same properties of conditional Independence as X * . Furthermore any classifier build a posteriori on Z will satisfy equal opportunities with respect to Y .

Section Title: Proposition 1 Assume that the deterministic map T satisfies
  Proposition 1 Assume that the deterministic map T satisfies then, we have T (X) ⊥ ⊥ S|Y , for any measurable map G, G(T (X)) ⊥ ⊥ S|Y . Proof sketch: Both are expressions of equality in distribution. Nullity of Wasserstein distance entails T (X) ⊥ ⊥ S|Y . This implies that for any deterministic map G, G(T (X)) ⊥ ⊥ S|Y . Note that, contrary to other repair procedures for which the transformation must be recomputed for any new observations (in  Barrio et al. (2019a)  the transformation relies on the optimal transport map which depends on the observations), here the optimal transformation T can be used directly for all new observations.

Section Title: EXPERIMENTATION
  EXPERIMENTATION We show in this section, empirical results supporting our theoretical expectations. For simplicity and reproducibility purposes, we keep neural networks as simple as possible and try to use similar architectures as much as we can. For all the experiments, we set the learning rate of Adam to 1e −4 , and n w to 10 (see algorithm description in Sections 3 and 4). All experiments have been implemented with keras/tensorflow.

Section Title: Fairness benchmarks
  Fairness benchmarks We consider three state of the art fairness benchmarks : (i) impact of gender in adult database (predict income>50K, 48842 examples, 16 attributes) (ii) impact of age (boolean 25 < age > 60) in the bank database (predict credit acceptance, 45211 examples, 17 attributes) (iii) impact of the gender on the attractivity in a subset of the celebA dataset (64x64 rgb images, 19670 examples).

Section Title: Concurent methods
  Concurent methods We compare our results with the C-SVM implementation proposed by  Zafar et al. (2017a ;b) and a classifier based on our neural network architecture without fairness constraint (UnfairClf). We also compare our approach with GAN type Jensen adversarial as in  Beutel et al. (2017) ; Madras et al. (2018). Note that the architectures that we use are slightly different from the original papers, this was on purpose to ensure an objective comparison with our approach. Indeed we keep our adversarial architectures and only replace Wasserstein loss and network by a classifier with binary cross entropy and GAN trick for training. Results:  Table 1  reports accuracy (ACC), DI and EMD in a 70%train-30%test scheme with 10 repetitions to assess variability 2 in both demographic parity and equality of odds scenarios. For equality of odds, we aggregate fairness measures DI and EM D, conditioning on Y and summing over Y = 0, 1, these aggregated measures are denoted by DI Y and EM D Y . For the adversarial approaches we report one result with high fairness constraint and one result with a lesser constraint (obtained by considering different values of λ). For the demographic parity constraint, in all examples, our algorithm reduces the DI close to 0 with an acceptable accuracy decrease.  Figure 5  illustrates that our last hyper-parameter λ (see Equation (4)) controls the trade-off between accuracy and fairness both in wasserstein and GAN configurations. Our approach clearly dominates concurrent methods in all situations in terms of both accuracy and fairness level. This is further illustrated in the third part of  Figure 5  where the accuracy / fairness tradeoff is very favorable to our wasserstein approach compared to more traditional GAN methods. Finally  Table 1  illustrates difficulties for GAN models to enforce hard fairness constraints (DI close to 0). For equality of odds, our method and GAN approach perform similarly. We train a fair auto-encoder with two Wasserstein adversarial networks constraining equality of odds for the decoded images. We observe in  Figure 5  that equality of odds is achieved by assigning the same color to all transformed images. We train a first network (unfairClf) on the biased database. We train a similar network on the fair database, and construct a fair classifier (fairClf) by composition of the second trained classifier and the auto-encoder. As expected unfairClf generalizes better on the biased test set (accuracy 0.94 versus 0.87). However when switching the test color distribution fairClf is far more robust (accuracy 0.82 versus 0.60). This demonstrates that in addition to building a representation which looks fair, our auto-encoder approach is robust to fluctuations of the bias variable distribution.

Section Title: CONCLUSIONS
  CONCLUSIONS This work tackles the challenge of incorporating constraints to deal with bias issues in machine learn- ing. We show that Wasserstein is an appropriate choice of distance between conditional distributions to control fairness using adversarial neural networks. We also explicit mathematical models providing abstract frameworks to understand and apply two types of fair constraints (demographic parity and equality of odds). The predictor we obtain prove efficient on well known fairness benchmarks Under review as a conference paper at ICLR 2020 as well as synthetic problems. Our experiments designed with minimal hand tuning to overcome reproducibility issues. As expected adversarial wasserstein constraints are more efficient to enforce fairness than their traditional GAN counterparts.
  Note that in the case where S is not a binary variable but multidimensional or multi-class, we can consider one versus one fairness identifying in each case a "minority".

```
