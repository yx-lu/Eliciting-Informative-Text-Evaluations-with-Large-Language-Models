Title:
```
Published as a conference paper at ICLR 2020 CYCLICAL STOCHASTIC GRADIENT MCMC FOR BAYESIAN DEEP LEARNING
```
Abstract:
```
The posteriors over neural network weights are high dimensional and multimodal. Each mode typically characterizes a meaningfully different representation of the data. We develop Cyclical Stochastic Gradient MCMC (SG-MCMC) to auto- matically explore such distributions. In particular, we propose a cyclical step- size schedule, where larger steps discover new modes, and smaller steps charac- terize each mode. We prove non-asymptotic convergence of our proposed algo- rithm. Moreover, we provide extensive experimental results, including ImageNet, to demonstrate the effectiveness of cyclical SG-MCMC in learning complex mul- timodal distributions, especially for fully Bayesian inference with modern deep neural networks.
```

Figures/Tables Captions:
```
Figure 1: Illustration of the proposed cyclical stepsize schedule (red) and the traditional decreasing stepsize schedule (blue) for SG-MCMC algorithms.
Figure 3: Results of cSG-MCMC with DNNs on the CIFAR-100 dataset. (a) MDS visualization in weight space: cSG-MCMC show larger distance than traditional schedules. (b) Testing errors (%) on the path of two samples: cSG-MCMC shows more varied performance. (c) Testing errors (%) as a function of the number of cycles M : cSGLD yields consistently lower errors. Remark 1. i) The bound is decomposed into two parts: the first part measures convergence speed of exact solution to the stationary distribution, i.e., ν k α k to ν ∞ ; the second part measures the numerical error, i.e., between µ K and ν k α k . ii) The overall bound offers a same order of depen- dency on K as in standard SGLD (please see the bound for SGLD in Section E of the appendix. See also Raginsky et al. (2017)). iii) If one imposes stricter assumptions such as in the convex case, the bound can be further improved. Specific bounds are derived in the appendix. We did not consider this case due to the discrepancy from real applications.
Figure 2: Sampling from a mixture of 25 Gaus- sians shown in (a) for the parallel setting. With a budget of 50k × 4 = 200k samples, traditional SGLD in (b) has only discovered 4 of the 25 modes, while our cSGLD in (c) has fully explored the distribution.
Figure 4: Empirical CDF for the entropy of the predictive distribution on notMNIST dataset. cSGLD and cSGHMC show lower probability for the low entropy estimate than other algo- rithms.
Table 1: Comparison of test error (%) between cSG-MCMC with non-parallel algorithms. cS- GLD and cSGHMC yields lower errors than their optimization counterparts, respectively.
Table 2: Comparison of test error (%) between cSG-MCMC with parallel algorithm (M =4 chains) on CIFAR-10 and CIFAR-100. The method is reported in the format of "step-size schedule (cyclical or decreasing) + single/parallel chain". The cost is reported in the format of "#epoch per chain / #epoch used in all chains". Note that a parallel algorithm with a single chain reduces to a non- parallel algorithm. Integration of the cyclical schedule with parallel algorithms provides lower test- ing errors.
Table 3: Comparison on the testing set of Ima- geNet. cSGHMC yields lower testing NLL than Snapshot and SGHMC.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep neural networks are often trained with stochastic optimization methods such as stochastic gra- dient decent (SGD) and its variants. Bayesian methods provide a principled alternative, which ac- count for model uncertainty in weight space (MacKay, 1992;  Neal, 1996 ), and achieve an automatic balance between model complexity and data fitting. Indeed, Bayesian methods have been shown to improve the generalization performance of DNNs (Hernández-Lobato & Adams, 2015;  Blundell et al., 2015 ;  Li et al., 2016a ; Maddox et al., 2019), while providing a principled representation of uncertainty on predictions which is crucial for decision making. Approximate inference for Bayesian deep learning has typically focused on deterministic ap- proaches, such as variational methods (Hernández-Lobato & Adams, 2015;  Blundell et al., 2015 ). By contrast, MCMC methods are now essentially unused for inference with modern deep neural networks, despite previously providing the gold standard of performance with smaller neural net- works ( Neal, 1996 ). Stochastic gradient Markov Chain Monte Carlo (SG-MCMC) methods ( Welling & Teh, 2011 ;  Chen et al., 2014 ;  Ding et al., 2014 ;  Li et al., 2016a ) provide a promising direction for a sampling based approach to inference in Bayesian deep learning. Indeed, it has been shown that stochastic methods, which use mini-batches of data, are crucial for finding weight parameters that provide good generalization in modern deep neural networks (Keskar et al., 2016). However, SG-MCMC algorithms for inference with modern neural networks face several challenges: (i) In theory, SG-MCMC asymptotically converges to target distributions via a decreasing stepsize scheme, but suffers from a bounded estimation error in limited time (Teh et al., 2016;  Chen et al., 2015 ). (ii) In practice, empirical successes have been reported by training DNNs in relatively short time ( Li et al., 2016b ;  Chen et al., 2014 ;  Gan et al., 2016 ; Neelakantan et al., 2016; Saatchi & Wilson, 2017). For example, Saatchi & Wilson (2017) apply SG-MCMC to generative adversarial networks (GANs) to solve the mode collapse problem and capture diverse generation styles. However, the loss surface for DNNs is highly multimodal ( Auer et al., 1996 ;  Choromanska et al., 2015 ). In order for MCMC to be effective for posterior inference in modern neural networks, a crucial question remains: how do we make SG-MCMC efficiently explore a highly multimodal parameter space given a practical computational budget?

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 Several attempts have been made to improve the sampling efficiency of SG-MCMC. Stochastic Gra- dient Hamiltonian Monte Carlo (SGHMC) ( Chen et al., 2014 ) introduces momentum to Langevin dynamics. Preconditioned stochastic gradient Langevin dynamics (pSGLD) ( Li et al., 2016a ) adap- tively adjusts the sampler's step size according to the local geometry of parameter space. Though simple and promising, these methods are still inefficient at exploring multimodal distributions in practice. It is our contention that this limitation arises from difficulties escaping local modes when using the small stepsizes that SG-MCMC methods typically require. Note that the stepsize in SG- MCMC controls the sampler's behavior in two ways: the magnitude to deterministically drift to- wards high density regions wrt. the current stochastic gradient, and the level of injecting noise to randomly explore the parameter space. Therefore, a small stepsize reduces both abilities, resulting in a large numbers of iterations for the sampler to move across the modes. In this paper, we propose to replace the traditional decreasing stepsize schedule in SG-MCMC with a cyclical variant. To note the distinction from traditional SG-MCMC, we refer to this method as Cyclical SG-MCMC (cSG-MCMC). The comparison is illustrated in  Fig- ure 1 . The blue curve is the traditional decay, while the red curve shows the proposed cyclical schedule. Cyclical SG-MCMC operates in two stages: (i) Exploration: when the stepsize is large (dashed red curves), we consider this stage as an effective burn-in mechanism, encouraging the sampler to take large moves and leave the local mode using the stochastic gradient. (ii) Sampling: when the stepsize is small (solid red curves), the sampler explores one local mode. We collect samples for local distribution estimation during this stage. Further, we propose two practical techniques to improve estimation efficiency: (1) a system temperature for exploration and exploitation; (2) A weighted combination scheme for samples collected in different cycles to reflect their relative importance. This procedure can be viewed as SG-MCMC with warm restarts: the exploration stage provides the warm restarts for its following sampling stage. cSG-MCMC combines the advantages from (1) the traditional SG-MCMC to characterize the fine-scale local density of a distribution and (2) the cyclical schedule in optimization to efficiently explore multimodal posterior distributions of the pa- rameter space. In limited time, cSG-MCMC is a practical tool to provide significantly better mixing than the traditional SG-MCMC for complex distributions. cSG-MCMC can also be considered as an efficient approximation to parallel MCMC; cSG-MCMC can achieve similar performance to par- allel MCMC with only a fraction of cost (reciprocal to the number of chains) that parallel MCMC requires. To support our proposal, we also prove the non-asymptotic convergence for the cyclical schedule. We note that this is the first convergence analysis of a cyclical stepsize algorithm (including work in optimization). Moreover, we provide extensive experimental results to demonstrate the advantages of cSG-MCMC in sampling from multimodal distributions, including Bayesian neural networks and uncertainty estimation on several large and challenging datasets such as ImageNet. In short, cSG-MCMC provides a simple and automatic approach to inference in modern Bayesian deep learning, with promising results, and theoretical support. This work is a step towards enabling MCMC approaches in Bayesian deep learning. We release code at https://github.com/ruqizhang/csgmcmc.

Section Title: PRELIMINARIES: SG-MCMC WITH A DECREASING STEPSIZE SCHEDULE
  PRELIMINARIES: SG-MCMC WITH A DECREASING STEPSIZE SCHEDULE SG-MCMC is a family of scalable sampling methods that enables inference with mini-batches of data. For a dataset D = {d i } N i=1 and a θ-parameterized model, we have the likelihood p(D|θ) and prior p(θ). The posterior distribution is p(θ|D) ∝ exp(−U (θ)) , where U (θ) is the potential energy given by U (θ) = − log p(D|θ) − log p(θ) .

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 When D is too large, it is expensive to evaluate U (θ) for all the data points at each iteration. In- stead, SG-MCMC methods use a minibatch to approximate U (θ):Ũ (θ) = − N N N i=1 log p(x i |θ)− log p(θ) , where N N is the size of minibatch. We recommend Ma et al. (2015) for a general review of SG-MCMC algorithms. We describe two SG-MCMC algorithms considered in this paper. SGLD & SGHMC  Welling & Teh (2011)  proposed Stochastic Gradient Langevin Dynamics (SGLD), which uses stochastic gradients with Gaussian noise. Posterior samples are updated at the k-th step as: θ k = θ k−1 − α k ∇Ũ (θ k ) + √ 2α k k , where α k is the stepsize and k has a standard Gaussian distribution. To improve mixing over SGLD, Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) ( Chen et al., 2014 ) introduces an auxiliary momentum variable v. SGHMC is built upon HMC, with an additional friction term to counteract the noise introduced by a mini-batch. The update rule for posterior samples is: θ k = θ k−1 + v k−1 , and v k = v k−1 − α k ∇Ũ (θ k ) − ηv k−1 + 2(η −γ)α k k , where 1 − η is the momentum term andγ is the estimate of the noise. To guarantee asymptotic consistency with the true distribution, SG-MCMC requires that the step sizes satisfy the following assumption: Assumption 1. The step sizes {α k } are decreasing, i.e., 0 < α k+1 < α k , with 1) Without a decreasing step-size, the estimation error from numerical approximations is asymptoti- cally biased. One typical decaying step-size schedule is α k = a(b + k) −γ , with γ ∈ (0.5, 1] and (a, b) some positive constants ( Welling & Teh, 2011 ).

Section Title: CYCLICAL SG-MCMC
  CYCLICAL SG-MCMC We now introduce our cyclical SG-MCMC (cSG-MCMC) algorithm. cSG-MCMC consists of two stages: exploration and sampling. In the following, we first introduce the cyclical step-size schedule, and then describe the exploration stage in Section 3.1 and the sampling stage in Section 3.2. We propose an approach to combining samples for testing in Section F. Assumption 1 guarantees the consistency of our estimation with the true distribution in the asymp- totic time. The approximation error in limited time is characterized as the risk of an estimator R = B 2 + V , where B is the bias and V is the variance. In the case of infinite computation time, the traditional SG-MCMC setting can reduce the bias and variance to zero. However, the time budget is often limited in practice, and there is always a trade-off between bias and variance. We therefore decrease the overall approximation error R by reducing the variance through obtaining more effec- tive samples. The effective sample size can be increased if fewer correlated samples from different distribution modes are collected. For deep neural networks, the parameter space is highly multimodal. In practice, SG-MCMC with the traditional decreasing stepsize schedule becomes trapped in a local mode, though injecting noise may help the sampler to escape in the asymptotic regime ( Zhang et al., 2017 ). Inspired to improve the exploration of the multimodal posteriors for deep neural networks, with a simple and automatic approach, we propose the cyclical cosine stepsize schedule for SG-MCMC. The stepsize at iteration k is defined as: α k = α 0 2 cos π mod(k − 1, K/M ) K/M + 1 , (1) where α 0 is the initial stepsize, M is the number of cycles and K is the number of total iterations (Loshchilov & Hutter, 2016; Huang et al., 2017). The stepsize α k varies periodically with k. In each period, α k starts at α 0 , and gradually decreases to 0. Within one period, SG-MCMC starts with a large stepsize, resulting in aggressive exploration in the parameter space; as the stepsize is decreasing, SG-MCMC explores local regions. In the next period, the Markov chain restarts with a large stepsize, encouraging the sampler to escape from the current mode and explore a new area of the posterior.

Section Title: Related work in optimization
  Related work in optimization In optimization, the cyclical cosine annealing stepsize schedule has been demonstrated to be able to find diverse solutions in multimodal objectives, though not Published as a conference paper at ICLR 2020 specifically different modes, using stochastic gradient methods (Loshchilov & Hutter, 2016; Huang et al., 2017;  Garipov et al., 2018 ;  Fu et al., 2019 ). Alternatively, we adopt the technique to SG- MCMC as an effective scheme for sampling from multimodal distributions.

Section Title: EXPLORATION
  EXPLORATION The first stage of cyclical SG-MCMC, exploration, discovers parameters near local modes of an objective function. Unfortunately, it is undesirable to directly apply the cyclical schedule in op- timization to SG-MCMC for collecting samples at every step. SG-MCMC often requires a small stepsize in order to control the error induced by the noise from using a minibatch approximation. If the stepsize is too large, the stationary distribution of SG-MCMC might be far away from the true posterior distribution. To correct this error, it is possible to do stochastic Metropolis-Hastings (MH) ( Korattikara et al., 2014 ;  Bardenet et al., 2014 ;  Chen et al., 2016b ). However, stochastic MH correction is still computationally too expensive. Further, it is easy to get rejected with an aggressive large stepsize, and every rejection is a waste of gradient computations. To alleviate this problem, we propose to introduce a system temperature T to control the sampler's behaviour: p(θ|D) ∝ exp(−U (θ)/T ). Note that the setting T = 1 corresponds to sampling from the untempered posterior. When T → 0, the posterior distribution becomes a point mass. Sam- pling from lim T →0 exp(−U (θ)/T ) is equivalent to minimizing U (θ); in this context, SG-MCMC methods become stochastic gradient optimization methods. One may increase the temperature T from 0 to 1 when the step-size is decreasing. We simply consider T = 0 and perform optimization as the burn-in stage, when the completed proportion of a cycle r(k) = mod (k−1, K/M ) K/M is smaller than a given threshold: r(k) < β. Note that β ∈ (0, 1) balances the proportion of the exploration and sampling stages in cSG-MCMC.

Section Title: SAMPLING
  SAMPLING Algorithm 1 Cyclical SG-MCMC. Input: The initial stepsize α 0 , number of cycles M , number of training iterations K and the proportion of exploration stage β. for k = 1:K do α ← α k according to Eq equation 1.

Section Title: Collect samples using SG-MCMC methods
  Collect samples using SG-MCMC methods Output: Samples {θ k } The sampling stage corresponds to T = 1 of the exploration stage. When r(k) > β or step-sizes are sufficiently small, we ini- tiate SG-MCMC updates and collect sam- ples until this cycle ends. SG-MCMC with Warm Restarts. One may consider the exploration stage as au- tomatically providing warm restarts for the sampling stage. Exploration alleviates the inefficient mixing and inability to traverse the multimodal distributions of the tradi- tional SG-MCMC methods. SG-MCMC with warm restarts explores different parts of the posterior distribution and captures multiple modes in a single training procedure. In summary, the proposed cyclical SG-MCMC repeats the two stages, with three key advantages: (i) It restarts with a large stepsize at the beginning of a cycle which provides enough perturbation and encourages the model to escape from the current mode. (ii) The stepsize decreases more quickly inside one cycle than a traditional schedule, making the sampler better characterize the density of the local regions. (iii) This cyclical stepsize shares the advantage of the "super-convergence" property discussed in Smith & Topin (2017): cSG-MCMC can accelerate convergence for DNNs by up to an order of magnitude.

Section Title: Connection to the Santa algorithm
  Connection to the Santa algorithm It is interesting to note that our approach inverts steps of the Santa algorithm ( Chen et al., 2016a ) for optimization. Santa is a simulated-annealing-based optimization algorithm with an exploration stage when T = 1, then gradually anneals T → 0 in a refinement stage for global optimization. In contrast, our goal is to draw samples for multimodal distributions, thus we explore with T = 0 and sample with T = 1. Another fundamental difference is that Santa adopts the traditional stepsize decay, while we use the cyclical schedule. We visually compare the difference between cyclical and traditional step size schedules (described in Section 2) in  Figure 1 . The cyclical SG-MCMC algorithm is presented in Algorithm 1.

Section Title: Connection to Parallel MCMC
  Connection to Parallel MCMC Running parallel Markov chains is a natural and effective way to draw samples from multimodal distributions (VanDerwerken & Schmidler, 2013;  Ahn et al., 2014 ). However, the training cost increases linearly with the number of chains. Cyclical SG-MCMC can be seen as an efficient way to approximate parallel MCMC. Each cycle effectively estimates a different region of posterior. Note cyclical SG-MCMC runs along a single training pass. Therefore, its computational cost is the same as single chain SG-MCMC while significantly less than parallel MCMC.

Section Title: Combining Samples
  Combining Samples In cyclical SG-MCMC, we obtain samples from multiple modes of a poste- rior distribution by running the cyclical step size schedule for many periods. We provide a sampling combination scheme to effectively use the collected samples in Section F in the appendix.

Section Title: THEORETICAL ANALYSIS
  THEORETICAL ANALYSIS Our algorithm is based on the SDE characterizing the Langevin dynamics: dθ t = −∇U (θ t )dt + √ 2dW t , where W t ∈ R d is a d-dimensional Brownian motion. In this section, we prove non- asymptotic convergence rates for the proposed cSG-MCMC framework with a cyclical stepsize se- quence {α k } defined in equation 1. For simplicity, we do not consider the exploration stage in the analysis as that corresponds to stochastic optimization. Generally, there are two different ways to describe the convergence behaviours of SG-MCMC. One characterizes the sample average over a particular test function (e.g.,  Chen et al. (2015) ; Vollmer et al. (2016)); the other is in terms of the Wasserstein distance (e.g., Raginsky et al. (2017); Xu et al. (2017)). We study both in the following. Weak convergence Following  Chen et al. (2015)  and Vollmer et al. (2016), we define the posterior average of an ergodic SDE as:φ X φ(θ)ρ(θ)dθ for some test function φ(θ) of interest. For the corresponding algorithm with generated samples (θ k ) K k=1 , we use the sample averageφ defined aŝ φ = 1 K K k=1 φ(θ k ) to approximateφ. We prove weak convergence of cSGLD in terms of bias and MSE, as stated in Theorem 1. Theorem 1. Under Assumptions 2 in the appendix, for a smooth test function φ, the bias and MSE of cSGLD are bounded as: Convergence under the Wasserstein distance Next, we consider the more general case of SGLD and characterize convergence rates in terms of a stronger metric of 2-Wasserstein distance, defined as: where Γ(µ, ν) is the set of joint distributions over (θ, θ ) such that the two marginals equal µ and ν, respectively. Denote the distribution of θ t in the SDE as ν t . According to  Chiang & Hwang (1987) , the stationary distribution ν ∞ matches our target distribution. Let µ K be the distribution of the sample from our proposed cSGLD algorithm at the K-th iteration. Our goal is to derive a convergence bound on W 2 (µ K , ν ∞ ). We adopt standard assumptions as in most existing work, which are detailed in Assumption 3 in the appendix. Theorem 2 summarizes our main theoretical result. Theorem 2. Under Assumption 3 in the appendix, there exist constants (C 0 , C 1 , C 2 , C 3 ) indepen- dent of the stepsizes such that the convergence rate of our proposed cSGLD with cyclical stepsize sequence equation 1 is bounded for all K satisfying (K mod M =0), as

Section Title: EXPERIMENTS
  EXPERIMENTS We demonstrate cSG-MCMC on several tasks, including a synthetic multimodal distribution (Sec- tion 5.1), image classification on Bayesian neural networks (Section 5.2) and uncertainty estimation in Section 5.3. We also demonstrate cSG-MCMC can improve the estimate efficiency for uni-modal distributions using Bayesian logistic regression in Section A.2 in the appendix. We choose SLGD and SGHMC as the representative baseline algorithms. Their cyclical counterpart are called cSGLD and cSGHMC, respectively. We first demonstrate the ability of cSG-MCMC for sampling from a multi-modal distribution on a 2D mixture of 25 Gaussians. Specifi- cally, we compare cSGLD with SGLD in two setting: (1) parallel running with 4 chains and (2) running with a single chain, respectively. Each chain runs for 50k iterations. The step- size schedule of SGLD is α k ∝ 0.05k −0.55 . In cSGLD, we set M = 30 and the initial step- size α 0 = 0.09. The proportion of exploration stage β = 1 4 .  Fig 2  shows the estimated density using sampling results for SGLD and cSGLD in the parallel setting. We observed that SGLD gets trapped in the local modes, depending on the initial position. In any practical time period, SGLD could only characterize partial distribution. In contrast, cSGLD is able to find and character- ize all modes, regardless of the initial position. cSGLD leverages large step sizes to discover a new mode, and small step sizes to explore local modes. This result suggests cSGLD can be a significantly favourable choice in the non-asymptotic setting, for example only 50k iterations in this case. The single chain results and the quantitative results on mode coverage are reported in Section A.1 of the appendix.

Section Title: BAYESIAN NEURAL NETWORKS
  BAYESIAN NEURAL NETWORKS We demonstrate the effectiveness of cSG-MCMC on Bayesian neural networks for classification on CIFAR-10 and CIFAR-100. We compare with (i) traditional SG-MCMC; (ii) traditional stochastic optimization methods, including stochastic gradient descent (SGD) and stochastic gradient descent Published as a conference paper at ICLR 2020 with momentum (SGDM); and (iii) Snapshot: a stochastic optimization ensemble method method with a the cyclical stepsize schedule (Huang et al., 2017). We use a ResNet-18 (He et al., 2016) and run all algorithms for 200 epochs. We report the test errors averaged over 3 runs, and the standard error (±) from the mean predictor. We set M = 4 and α 0 = 0.5 for cSGLD, cSGHMC and Snapshot. The proportion hyper-parameter β =0.8 and 0.94 for CIFAR-10 and CIFAR-100, respectively. We collect 3 samples per cycle. In practice, we found that the collected samples share similarly high likelihood for DNNs, thus one may simply set the normalizing term w i in equation 33 to be the same for faster testing. We found that tempering helps improve performance for Bayesian inference with neural networks. Tempering for SG-MCMC was first used by  Li et al. (2016a)  as a practical technique for neural network training for fast convergence in limited time 1 . We simply use the prescribed temperature of  Li et al. (2016a)  without tuning, but better results of the sampling methods can be achieved by tuning the temperature. More details are in Appendix J. We hypothesize that tempering helps due to the overparametrization of neural networks. Tempering enables one to leverage the inductive biases of the network, while representing the belief that the model capacity can be misspecified. In work on Safe Bayes, also known as generalized and fractional Bayesian inference, tempered posteriors are well-known to help under misspecification (e.g.,  Barron & Cover, 1991 ;  de Heide et al., 2019 ;  Grünwald et al., 2017 ). For the traditional SG-MCMC methods, we found that noise injection early in training hurts con- vergence. To make these baselines as competitive as possible, we thus avoid noise injection for the first 150 epochs of training (corresponding to the zero temperature limit of SGLD and SGHMC), and resume SGMCMC as usual (with noise) for the last 50 epochs. This scheme is similar to the exploration and sampling stages within one cycle of cSG-MCMC. We collect 20 samples for the MCMC methods and average their predictions in testing.

Section Title: Testing Performance for Image Classification
  Testing Performance for Image Classification We report the testing errors in  Table 1  to com- pare with the non-parallel algorithms. Snapshot and traditional SG-MCMC reduce the testing er- rors on both datasets. Performance variance for these methods is also relatively small, due to the multiple networks in the Bayesian model av- erage. Further, cSG-MCMC significantly out- performs Snapshot ensembles and the traditional SG-MCMC, demonstrating the importance of (1) capturing diverse modes compared to traditional SG-MCMC, and (2) capturing fine-scale charac- teristics of the distribution compared with Snap- shot ensembles.

Section Title: Diversity in Weight Space
  Diversity in Weight Space To further demonstrate our hypothesis that with a limited budget cSG- MCMC can find diverse modes, while traditional SG-MCMC cannot, we visualize the 12 samples we collect from cSG-MCMC and SG-MCMC on CIFAR-100 respectively using Multidimensional Scaling (MDS) in  Figure 3 (a) . MDS uses a Euclidean distance metric between the weight of sam- ples. We see that the samples of cSG-MCMC form 4 clusters, which means they are from 4 different modes in weight space. However, all samples from SG-MCMC only form one cluster, which indi- cates traditional SG-MCMC gets trapped in one mode and only samples from that mode.

Section Title: Diversity in Prediction
  Diversity in Prediction To further demonstrate the samples from different cycles of cSG-MCMC provide diverse predictions we choose one sample from each cycle and linearly interpolate between two of them ( Goodfellow et al., 2014 ; Huang et al., 2017). Specifically, let J(θ) be the test error of a sample with parameter θ. We compute the test error of the convex combination of two samples J(λθ 1 + (1 − λ)θ 2 ), where λ ∈ [0, 1]. We linearly interpolate between two samples from neighboring chains of cSG-MCMC since they are the most likely to be similar. We randomly select 4 samples from SG-MCMC. If the samples are from the same mode, the test error of the linear interpolation of parameters will be relatively Published as a conference paper at ICLR 2020 smooth, while if the samples are from different modes, the test error of the parameter interpolation will have a spike when λ is between 0 and 1. We show the results of interpolation for cSG-MCMC and SG-MCMC on CIFAR-100 in  Figure 3 (b) . We see a spike in the test error in each linear interpolation of parameters between two samples from neighboring chains in cSG-MCMC while the linear interpolation for samples of SG-MCMC is smooth. This result suggests that samples of cSG-MCMC from different chains are from different modes while samples of SG-MCMC are from the same mode. Although the test error of a single sample of cSG-MCMC is worse than that of SG-MCMC shown in  Figure 3 (c) , the ensemble of these samples significantly improves the test error, indicating that samples from different modes provide different predictions and make mistakes on different data points. Thus these diverse samples can complement each other, resulting in a lower test error, and demonstrating the advantage of exploring diverse modes using cSG-MCMC.

Section Title: Comparison to Parallel MCMC
  Comparison to Parallel MCMC cSG-MCMC can be viewed as an economical alternative to parallel MCMC. We verify how closely cSG-MCMC can approximate the performance of parallel MCMC, but with more convenience and less computational expense. We also note that we can improve parallel MCMC with the proposed cyclical stepsize schedule. We report the testing errors in  Table 2  to compare multiple-chain results. (1) Four chains used, each runs 200 epochs (800 epochs in total), the results are shown in the first 4 columns (Cycli- cal+Parallel vs Decreasing+Parallel). We see that cSG-MCMC variants provide lower errors than plain SG-MCMC. (2) We reduce the number of epochs (#epoch) of parallel MCMC to 100 epoch each for decreasing stepsize schedule. The total cost is 400 epochs. We compare its performance with cyclical single chain (200 epochs in total) in the last 4 columns (Decreasing+Parallel vs Cycli- cal+Single). We see that the cyclical schedule running on a single chain performs best even with half the computational cost! All the results indicate the importance of warm re-starts using the proposed cyclical schedule. For a given total cost budget, the proposed cSGMCMC is preferable to parallel sampling.

Section Title: Comparison to Snapshot Optimization
  Comparison to Snapshot Optimization We carefully compared with Snapshot, as our cSG- MCMC can be viewed as the sampling counterpart of the Snapshot optimization method. We plot the test error wrt.various number of cycles M in  Fig. 3 . As M increases, cSG-MCMC and Snapshot both improve. However, given a fixed M , cSG-MCMC yields substantially lower test errors than Snapshot. This result is due to the ability of cSG-MCMC to better characterize the local distribution of modes: Snapshot provides a singe minimum per cycle, while cSG-MCMC fully exploits the mode with more samples, which could provide weight uncertainty estimate and avoid over-fitting.

Section Title: Results on ImageNet
  Results on ImageNet We further study dif- ferent learning algorithms on a large-scale dataset, ImageNet. ResNet-50 is used as the ar- chitecture, and 120 epochs for each run. The results on the testing set are summarized in  Ta- ble 3 , including NLL, Top1 and Top5 accuracy (%), respectively. 3 cycles are considered for both cSGHMC and Snapshot, and we collect 3 samples per cycle. We see that cSGHMC yields the lowest testing NLL, indicating that the cy- Published as a conference paper at ICLR 2020 cle schedule is an effective technique to explore the parameter space, and diversified samples can help prevent over-fitting.

Section Title: UNCERTAINTY EVALUATION
  UNCERTAINTY EVALUATION To demonstrate how predictive uncertainty ben- efits from exploring multiple modes in the pos- terior of neural network weights, we consider the task of uncertainty estimation for out-of- distribution samples (Lakshminarayanan et al., 2017). We train a three-layer MLP model on the standard MNIST train dataset until convergence using different algorithms, and estimate the en- tropy of the predictive distribution on the notM- NIST dataset ( Bulatov, 2011 ). Since the samples from the notMNIST dataset belong to the unseen classes, ideally the predictive distribution of the trained model should be uniform over the notM- NIST digits, which gives the maximum entropy. In  Figure 4 , we plot the empirical CDF for the entropy of the predictive distributions on notM- NIST. We see that the uncertainty estimates from cSGHMC and cSGLD are better than the other methods, since the probability of a low entropy prediction is overall lower. cSG-MCMC algorithms explore more modes in the weight space, each mode characterizes a meaningfully different represen- tation of MNIST data. When testing on the out-of-distribution dataset (notMNIST), each mode can provide different predictions over the label space, leading to more reasonable uncertainty estimates. Snapshot achieves less entropy than cSG-MCMC, since it represents each mode with a single point. The traditional SG-MCMC methods also provide better uncertainty estimation compared to their optimization counterparts, because they characterize a local region of the parameter space, rather than a single point. cSG-MCMC can be regarded as a combination of these two worlds: a wide coverage of many modes in Snapshot, and fine-scale characterization of local regions in SG-MCMC.

Section Title: DISCUSSION
  DISCUSSION We have proposed cyclical SG-MCMC methods to automatically explore complex multimodal dis- tributions. Our approach is particularly compelling for Bayesian deep learning, which involves rich multimodal parameter posteriors corresponding to meaningfully different representations. We have also shown that our cyclical methods explore unimodal distributions more efficiently. These results are in accordance with theory we developed to show that cyclical SG-MCMC will converge faster to samples from a stationary distribution in general settings. Moreover, we show cyclical SG-MCMC methods provide more accurate uncertainty estimation, by capturing more diversity in the hypothesis space corresponding to settings of model parameters. While MCMC was once the gold standard for inference with neural networks, it is now rarely used in modern deep learning. We hope that this paper will help renew interest in MCMC for posterior inference in deep learning. Indeed, MCMC is uniquely positioned to explore the rich multimodal posterior distributions of modern neural networks, which can lead to improved accuracy, reliability, and uncertainty representation.

```
