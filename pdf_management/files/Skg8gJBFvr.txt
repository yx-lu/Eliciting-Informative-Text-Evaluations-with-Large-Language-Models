Title:
```
Under review as a conference paper at ICLR 2020 FILLING THE SOAP BUBBLES: EFFICIENT BLACK-BOX ADVERSARIAL CERTIFICATION WITH NON-GAUSSIAN SMOOTHING
```
Abstract:
```
Randomized classifiers have been shown to provide a promising approach for achieving certified robustness against adversarial attacks in deep learning. How- ever, most existing methods only leverage Gaussian smoothing noise and only work for 2 perturbation. We propose a general framework of adversarial cer- tification with non-Gaussian noise and for more general types of attacks, from a unified variational optimization perspective. Our new framework allows us to identify a key trade-off between accuracy and robustness via designing smooth- ing distributions, helping to design two new families of non-Gaussian smoothing distributions that work more efficiently for 2 and ∞ attacks, respectively. Our proposed methods achieve better results than previous works and provide a new perspective on randomized smoothing certification.
```

Figures/Tables Captions:
```
Figure 1: Starting from radius distri- bution in Equation (8) with d = 100 σ = 1 and k = 0 (black start), increas- ing k (green curve) allows us to move the mean towards zero without signifi- cantly reducing the variance. Decreas- ing σ (red curve) can also decrease the mean, but with a cost of decreasing the variance quadratically.
Figure 3: For ∞ attacking, the mixed norm distribution (right) yields smaller TV distances (larger overlap areas), and hence higher robustness.
Figure 4: The Pareto frontier of accuracy and robustness (in the sense of Equation (7)) of the three smoothing fam- ilies in Equation (8), Equation (9), and Equation (10) for ∞ attacking, when we search for the best parameters (k, σ) for each of them. The mixed norm family Equation (9) yields the best trade-off than the other two. We assume f (x) = I( x 2 ≤ r) and dimension d = 5. The case when f (x) = I( x ∞ ≤ r) has similar result (not shown).
Figure 5: Results of ∞ verification on CIFAR10, on models trained with Gaussian noise data augmentation with different variances σ 0 . Our method obtains consistently better results.
Table 1: Certified top-1 accuracy of the best classifiers with various 2 radius on CIFAR-10.
Table 2: Certified top-1 accuracy of the best classifiers with various 2 radius on ImageNet.
Table 3: Certified top-1 accuracy of the best classifiers with various l ∞ radius on CIFAR-10.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep neural networks have achieved state-of-the-art performance on many tasks such as image clas- sification ( He et al., 2016 ;  Lu et al., 2018 ) and language modeling ( Devlin et al., 2019 ). Nonethe- less, modern deep learning models have been shown to be highly sensitive to small and adversarially crafted perturbations on the inputs ( Goodfellow et al., 2015 ), which means a human-imperceptible changes on inputs could cause the model to make dramatically different predictions. Although many robust training algorithms have been developed to overcome adversarial attacking, most heuristically developed methods can be shown to be broken by more powerful adversaries eventually, (e.g.,  Atha- lye et al., 2018 ;  Madry et al., 2018 ;  Zhang et al., 2019 ;  Wang et al., 2019 ). This casts an urgent demand for developing robust classifiers with provable worst case guarantees. One promising approach for certifiably robustness is the recent randomized smoothing method (e.g.,  Cohen et al., 2019 ;  Salman et al., 2019 ;  Lee et al., 2019 ;  Li et al., 2019 ;  Lecuyer et al., 2018 ), which constructs smoothed classifiers with certifiable robustness by introducing noise on the inputs. Compared with the other more traditional verification approaches (e.g.  Wong & Kolter, 2017 ;  Jordan et al., 2019 ;  Dvijotham et al., 2018 ) that exploits special structures of the neural networks (such as the properties of ReLU), the randomized smoothing methods work more flexibly on general black- box classifiers and is shown to be more scalable and provide tighter bounds on challenging datasets such as ImageNet ( Deng et al., 2009 ). However, the existing randomized smoothing methods can only work against 2 attack, in which the perturbations are allowed within an 2 ball of certain radius. A stronger type of attack, such as the ∞ attacks, is much more challenging to defense and verify due to the larger set of perturbations, but is more relevant in practice. In addition, all the existing randomized smoothing methods use Gaussian noise for smoothing. Al- though appearing to be a natural choice, one of our key observations is that Gaussian distributions is in fact a rather sub-optimal choice in high dimensional spaces, even for 2 attack. This is due to a counter-intuitive phenomenon in high dimensional spaces ( Vershynin, 2018 ) that almost all of the probability mass of standard Gaussian distribution concentrates around the sphere of radius one (and hence "soap bubble" in the title), instead of the center point (which corresponds to the original input). As a result, the variance of the Gaussian noise needs to be sufficiently small to yield good approximation to the original classifiers (by squeezing the "soap bubble" towards the center point), Under review as a conference paper at ICLR 2020 which, however, makes it difficult to verify due to the small noise. Further, for the more challenging ∞ attack, Gaussian smoothing provably degenerates in high dimensions.

Section Title: Our contribution
  Our contribution We propose a general framework of adversarial certification using non- Gaussian smoothing noises, based on a new perspective from variational optimization. Our frame- work re-derives the method of  Cohen et al. (2019)  as a special case, and is applicable to more general families of non-Gaussian smoothing distributions and more types of attacks beyond 2 norm. Im- portantly, our new framework reveals a fundamental trade-off between accuracy and robustness for guiding better choices of smoothing distributions. Leveraging our insight, we develop two new fam- ilies of distributions for better certification results on 2 and ∞ attacks, respectively. Efficient com- putational approaches are developed to enable our method in practice. Empirical results show that our new framework and smoothing distributions significantly outperform the existing approaches for both 2 and ∞ attacking, on challenging datasets such as CIFAR-10 and ImageNet.

Section Title: RELATED WORKS
  RELATED WORKS Empirical Defenses Since  Szegedy et al. (2013)  and  Goodfellow et al. (2015) , many previous works have focused on utilizing small perturbation δ under certain constraint, e.g. in a p norm ball, to attack a neural network. Adversarial training ( Madry et al., 2018 ) and its variants ( Kannan et al., 2018 ;  Zhang & Wang, 2019 ;  Zhai et al., 2019 ) are the most successful defense methods to date, in which the network is forced to solve a mini-max game between the defender and attacker with adversarial examples as data augmentation. However, these empirical defense methods are still easy to be broken and cannot provide provable defense.

Section Title: Certified Defenses
  Certified Defenses Unlike the empirical defense methods, once a classifier can guarantee a con- stant classification within a local region, it is called a robust certificate. Exact certification methods provide the minimal perturbation condition which leads to a different classification result. This line of work focus on deep neural networks with ReLU-like activation that makes the classifier a piece-wise linear function. This enables researchers to introduce satisfiability modulo theories ( Car- lini et al., 2017 ;  Ehlers, 2017 ) or mix integer linear programming ( Cheng et al., 2017 ;  Dutta et al., 2018 ). Sufficient certification methods take a conservative way and try to bound the Lipschitz con- stant or other information of the network ( Jordan et al., 2019 ;  Wong & Kolter, 2017 ;  Raghunathan et al., 2018 ;  Zhang et al., 2018 ). However, these certification strategies share a drawback that they are not feasible on large-scale scenarios, e.g. large enough practical networks, large enough datasets.

Section Title: Randomized Smoothing
  Randomized Smoothing To mitigate this limitation of previous certifiable defenses, improving network robustness via randomness has been recently discussed ( Xie et al., 2018 ;  Liu et al., 2018 ). In certification community,  Lecuyer et al. (2018)  first introduced randomization with technique in differential privacy.  Li et al. (2019)  improved their work with a bound given by Rényi divergence. In succession,  Cohen et al. (2019)  firstly provided a tight bound for arbitrary Gaussian smoothed classifiers based on previous theorems found by  Li & Kuelbs (1998) .  Salman et al. (2019)  combined the empirical and certification robustness, by applying adversarial training on randomized smoothed classifiers to achieve a higher certified accuracy.  Lee et al. (2019)  focused on 0 norm perturbation setting, and proposed a discrete smoothing distribution to beat the Gaussian distribution baseline. Similar to ( Lee et al., 2019 ), we also focus on finding a suitable distribution to trade-off accuracy and robustness for different types of adversarial attacks, such as 2 and ∞ .

Section Title: BLACK-BOX CERTIFICATION WITH VARIATIONAL OPTIMIZATION
  BLACK-BOX CERTIFICATION WITH VARIATIONAL OPTIMIZATION We start with introducing the background of adversarial certification problem and and randomized smoothing method. We then introduce in Section 3.1 our general framework of adversarial cer- tification using non-Gaussian smoothing noises, from a new variational optimization perspective. Our framework includes the method of  Cohen et al. (2019)  as a special case, and reveals a critical trade-off between accuracy and robustness that provides important guidance for better choices of smoothing distributions in Section 4.

Section Title: Adversarial Certification
  Adversarial Certification We consider binary classification of predicting binary labels y ∈ {0, 1} from feature vectors x ∈ R d for simplicity. The extension to multi-class cases is straightforward, and is discussed in Appendix D. Assume f : R d → [0, 1] is a pre-trained binary classifier ( means the classifier is given), which maps from the feature space R d to either the class probability in interval [0, 1] or the binary labels in {0, 1}. In the robustness certification problem, a testing data point x 0 ∈ R d is given, and one is asked to verify if the classifier outputs the same prediction when we perturb the input x 0 arbitrarily in B, a given neighborhood of x 0 . Specifically, let B be a set of possible perturbation vectors, e.g., B = {δ ∈ R d : δ p ≤ r} for p norm with a radius r. If the classifier predicts y = 1 on x 0 , i.e. f (x 0 ) > 1/2, we want to verify if f (x 0 + δ) > 1/2 holds for any δ ∈ B. In this paper, we consider two types of attacks, including the 2 attack B 2 ,r def = {δ : δ 2 ≤ r}, and the ∞ attack B ∞ ,r def = {δ : δ ∞ ≤ r}. More general p attack can also be handled by our framework but is left as future works.

Section Title: Black-box Certification with Randomness
  Black-box Certification with Randomness Directly verifying f heavily relies on the smooth- ness of f , which has been explored in a series of recent works ( Lecuyer et al., 2018 ;  Wong & Kolter, 2017 ). These methods typically depend on the special structure property (e.g., the use of ReLU units) of f , and thus can not serve as general purpose algorithms. We are instead interested in black-box verification methods that could work for arbitrary classifiers. One approach to en- able this, as explored in recent works ( Cohen et al., 2019 ;  Lee et al., 2019 ), is to replace f with a smoothed classifier by convovling with Gaussian noise, and verify the smoothed classifier. Specifically, assume π 0 is a smoothing distribution with zero mean and bounded variance, e.g., π 0 = N (0, σ 2 ). The randomized smoothed classifier is defined by f π0 (x 0 ) := E z∼π0 f (x 0 + z) , which returns the averaged probability of x 0 + z under the perturbation of z ∼ π 0 . Assume we replace the original classifier with f π0 , then the goal becomes verifying f π0 using its inherent smoothness. Specifically, if f π0 (x 0 ) > 1/2, we want to verify that f π0 (x 0 + δ) > 1/2 for every δ ∈ B, that is, In this case, it is sufficient to obtain a guaranteed lower bound of min δ∈B f π0 (x 0 + δ) and check if it is larger than 1/2. When π 0 is Gaussian N (0, σ 2 ) and for 2 attack, this problem was studied in  Cohen et al. (2019) , which shows that a lower bound of min z∈B E z∼π0 [f (x 0 + z)] ≥ Φ(Φ −1 (f π0 (x 0 )) − r/σ), (2) where Φ(·) is the cumulative density function (CDF) of standard Gaussian distribution, and Φ −1 (·) represents its inverse function. The proof of this result in  Cohen et al. (2019)  uses Neyman- Pearson lemma ( Li & Kuelbs, 1998 ), while in the following section another derivation using variational optimization is provided. Note that the bound in Equation (2) is tractable since it only requires to evaluate the smoothed clas- sifier f π0 (x 0 ) at the original image x 0 , instead of solving the difficult adversarial optimization over perturbation z in Equation (1). In practice, f π0 (x 0 ) is approximated by Monte Carlo approximation with a non-asymptotic confidence bound.

Section Title: VARIATIONAL ADVERSARIAL CERTIFICATION
  VARIATIONAL ADVERSARIAL CERTIFICATION We propose a variational adversarial certification (AVC) framework, which yields a guaranteed lower bound for Equation (1) based on variational optimization. The main idea is simple: assume F is a function class which is known to include f , then the following optimization immediately yields a guaranteed lower bound, min δ∈B f π0 (x 0 + δ) ≥ min f ∈F min δ∈B f π0 (x 0 + δ) s.t. f π0 (x 0 ) = f π0 (x 0 ) , (3) where we define f π0 (x 0 ) = E z∼π0 [f (x 0 + z)] for any f , and search for the minimum value of f π0 (x 0 + δ) for all classifiers in F and satisfies f π0 (x 0 ) = f π0 (x 0 ). This obviously yields a lower Under review as a conference paper at ICLR 2020 bound once f ∈ F. If F includes only f , then the bound is exact, but is computationally pro- hibitive due to the difficulty of optimizing δ. The idea is then to choose F properly to incorporate rich information of f , while allowing us to calculate the lower bound in Equation (3) computation- ally tractably. In this paper, we consider the set of all functions bounded in [0, 1], that is, F [0,1] = f : f (z) ∈ [0, 1], ∀z ∈ R d , (4) which guarantees to include f by definition. There are other F that also yields computationally tractable bounds, including the L p space F = {f : f Lp ≤ v}, which we leave for future work. Denote by V π0 (F, B) the lower bound in Equation (3). We can rewrite it into the following minimax form using the Lagrangian function, V π0 (F, B) = min f ∈F min δ∈B max λ∈R L(f, δ, λ) def = f π0 (x 0 + δ) − λ(f π0 (x 0 ) − f π0 (x 0 )) , where λ is the Lagrangian multiplier. Exchanging the min and max yields the following dual form. Theorem 1. I) (Dual Form) Denote by π δ the distribution of z + δ when z ∼ π 0 . Assume F and B are compact set. We have the following dual form of V π0 (F, B) via strong duality: which measures the difference of λπ 0 and π δ by seeking the maximum discrepancy of the expectation for f ∈ F. As we show later, the bound in (5) is computationally tractable with proper (F, B, π 0 ). II) When F = F [0,1] := {f : f (x) ∈ [0, 1], x ∈ R d }, we have in particular In addition, we have 0 ≤ D F [0,1] (λπ 0 π δ ) ≤ λ for any π 0 , π δ and λ > 0. Note that D F [0,1] (λπ 0 π δ ) coincides with the total variation distance between π 0 and π δ when λ = 1.

Section Title: Remark
  Remark We will show later that the proposed methods and the cases we study satisfy the condition in part III of the theorem and thus all the lower bounds of the proposed method are tight. Proof. First, observe that the constraint in Equation (3) can be equivalently replaced by an inequality constraint f π0 (x 0 ) ≥ f π0 (x 0 ). Therefore, the Lagrangian multiplier can be restricted to be λ ≥ 0. We have The proof of the strong duality is in Appendix A.1. II) follows a straightforward calculation.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Although the lower bound in Equation (5) still involves an optimization on δ and λ, both of them are much easier than the original adversarial optimization in Equation (1). With proper choices of F, B and π 0 , the optimization of δ can be shown to provide simple closed form solutions by exploiting the symmetry of B, and the optimization of λ is a very simple one-dimensional searching problem. As a corollary of Theorem 1, we can exactly recover the bound derived by  Cohen et al. (2019) . Corollary 1. With isotropic Gaussian noise π 0 = N (0, σ 2 I d×d ), 2 attack B = {δ : δ 2 ≤ r} and F = F [0,1] , the lower bound in Equation (5) equals the bound in Equation (2) by  Cohen et al. (2019) , that is, See Appendix A.2 for more details. A key step of the proof is to show that max δ 2≤r D F [0,1] (λπ 0 π δ ) is achieved when δ is on the bound- ary of the 2 ball B = { δ 2 ≤ r}, which can be, for example, δ = [r, 0, . . . , 0] , due to symmetry of B (see figure on the right).

Section Title: Trade-off between Accuracy and Robustness
  Trade-off between Accuracy and Robustness The lower bound in Equation (5) reflects an intuitive trade-off between the robustness and accuracy, max λ≥0 λf π0 (x 0 ) Accuracy − max δ∈B D F (λπ 0 π δ ) Robustness , (7) where the first term reflects the accuracy of the smoothed classifier (assuming the true label is y = 1), while the second term max δ∈B D F (λπ 0 π δ ) measures the robustness of the smoothed classifier, via the maximum difference between the original smoothing distribution π 0 and perturbed distribu- tion π δ for δ ∈ B. The scalar λ can be viewed as looking for a best balance between these two terms to achieve the largest lower bound. More critically, different choices of smoothing distributions also yields a fundamental trade-off be- tween accuracy and robustness in Equation (7). If π 0 has large variance or high tail probability, the distance D F (λπ 0 π δ ) will tend to be large and the model is robust. However, if the variance of π 0 is too large, we may obtain a low value of f π0 (x 0 ) and hence less accurate model. The opti- mal choice of the smoothing distribution should optimally balance the accuracy and robustness, by distribute its mass properly to yield small max δ∈B D F (λπ 0 π δ ) and large f π0 (x 0 ) simultaneously.

Section Title: FILLING THE SOAP BUBBLES: NEW FAMILIES OF NON-GAUSSIAN SMOOTHING DISTRIBUTIONS
  FILLING THE SOAP BUBBLES: NEW FAMILIES OF NON-GAUSSIAN SMOOTHING DISTRIBUTIONS In this section, we identify a key problem of using Gaussian smoothing noise in high dimensional space, due to the "thin shell" phenomenon that the probability mass of Gaussian distributions con- centrates on a sphere far away from the center points in high dimensional spaces. Motivated by this observation, we propose in Section 4.1 a new family of non-Gaussian smoothing distributions that alleviate this problem for 2 attack, and also in Section 4.2 another mixed norm smoothing distribution designed specifically for ∞ attack.

Section Title: 2 REGION CERTIFICATION
  2 REGION CERTIFICATION Although isotropic Gaussian distributions appears to be a natural choice of smoothing distributions, they are in fact sub-optimal for trading-off accuracy and robustness in Equation (7), especially in high dimensions. The key problem is that, in high dimensional spaces, the probability mass of Gaussian distributions concentrates on a thin shell away from the center, and hence looks like "soap bubbles", instead of "solid balls" as what it appears in low dimension spaces. Lemma 1 ( Vershynin (2018) , Section 3.1). Let z ∼ N (0, I d×d ) be a d-dimensional standard Gaussian random variable. Then there exists a constant c, such that for δ ∈ (0, 1),

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 This suggests that with high probability (e.g, 1−δ = 0.99), z takes values very close to the sphere of radius √ d, within a constant distance from that sphere! See  Vershynin (2018)  for more discussion. This phenomenon makes it problematic to use standard Gaussian distribution for adversarial certi- fication, because one would expect that the smoothing distribution should concentrate around the center (the original image) in order to make the smoothed classifier close to the original classifier (and hence accurate). To illustrate the problem, consider a simple example when the true classifier is f (x) = I( x − x 0 2 ≤ √ d) for a constant < 1, where I is the indicator function. Then when the dimension d is large, we would have f (x 0 ) = 1 while f π0 (x 0 ) ≈ 0 when π 0 = N (0, I d×d ). It is of course possible to decrease the variance of π 0 to improve the accuracy of the smoothed classi- fier f π0 . However, this would significantly improve the distance term in Equation (7) and does not yield an optimal trade-off on accuracy and robustness. In this work, we introduce a new family of non-Gaussian distributions to address this curse of dimen- sionality. To motivate our method, it is useful to examine the density function of the distributions of the radius of spherical distributions in general. Lemma 2. Assume z is a spherically symmetric random variable on R d with a probability density function (PDF) of form π 0 (z) ∝ φ( z 2 ), where φ : [0, ∞) → [0, ∞) is a univariate function, then the PDF of the norm of z is p z 2 (r) ∝ r d−1 φ(r). The term r d−1 arises due to the integration on the sphere of radius r in R d . In particular, when z ∼ π 0 = N (0, σ 2 I d×d ), we have φ(r) = exp(−r 2 /(2σ 2 )) and hence p z 2 (r) ∝ r d−1 exp(−r 2 /(2σ 2 )), which is a scaled Chi distribution, also known as Nakagami distribution. Examining this P.D.F., we can see that the concentration of the norm is caused by the r d−1 term, which makes the density to be highly peaked when d is large. To alleviate the concen- tration phenomenon, we need to have a way to cancel out the effect of r d−1 . This motivates the following family of smoothing distributions: where we introduce a z −k 2 term in π 0 , with k a positive parameter, to make the radius distribution less concentrated when k is large. The radius distribution in Equation (8) is controlled by two pa- rameters (σ, k), where σ controls the scale of the distribution (and is hence the scale parameter), while k controls the shape of the distribution (and hence the shape parameter). The key idea is that adjusting k allows us to trade-off the accuracy and robustness much more optimally. As shown in  Figure 1 , ad- justing σ enables us to move the mean close to zero (hence yielding higher accuracy), but at cost of decreasing the vari- ance quadratically (hence more less robust). In contrast, ad- justing k allows us to decrease the mean without significantly impacting the variance, and hence yield a much better trade-off on the accuracy and robustness.

Section Title: Computational Method
  Computational Method With the more general non- Gaussian smoothing distribution, we no longer have the closed form solution of the bound like Equation (6). However, efficient computational methods can be still developed for calculating the bound in Equation (5) with π 0 in Equa- tion (8). The key is that the maximum of the distance term D F [0,1] (λπ 0 || π δ ) over δ ∈ B is always achieved on the boundary of B as we show in the sequel, while the optimization on λ ≥ 0 is one-dimensional and can be solved numerically efficiently. Under review as a conference paper at ICLR 2020 With Theorem 2, we can compute Equation (5) with δ = δ * . We then calculate D F [0,1] (λπ 0 π δ * ) using Monte Carlo approximation. Note that which can be approximated with Monte Carlo method with Hoeffding concentration bound. Let {z i } n i=1 be an i.i.d. sample from π 0 , then we can approximate . Drawing sufficiently large number of samples allows us to achieve approximation with arbitrary accuracy.

Section Title: ∞ REGION CERTIFICATION
  ∞ REGION CERTIFICATION Going beyond the 2 attack, we consider the ∞ attack, whose attacking region is B ∞ ,r = {δ : δ ∞ ≤ r}. This is a far more difficult problem, because the ∞ ball is substantially larger than the 2 ball of the same radius in high dimensional space. This makes the Gaussian smooth- ing distribution, as well as our 2 -based smoothing distribution in Equation (8), unsuitable for ∞ attack. In fact, as shown in the following negative result, if we use Equation (8) as the smoothing distribution for ∞ attack, the bound we obtain is effectively the bound we would get for verifying a 2 ball with radius √ dr, which is too large to give meaningful results when the dimension is high. Theorem 3. With the smoothing distribution π 0 in Equation (8) for k ≥ 0, σ > 0, and F = F [0,1] shown in Equation (4), the bound we get for certifying the ∞ attack on B ∞,r = {δ : δ ∞ ≤ r} is equivalent to that for certifying the 2 attack on B Figure 2: ∞ and 2 balls in high dimension. The key reason of this negative result is that the furthest points to the origin (vertexes) in B ∞ ,r have an 2 radius of √ dr, illustrated as the "pointy" points in Figure 2. Thus, the maximum distance max δ∈B ∞ ,r D F (λπ 0 π δ ) is achieved at one of these pointy points, mak- ing it equivalent to optimizing in the 2 ball with radius √ dr. In order to address this problem, we propose the following new mixed norm family of smoothing distribution that uses a mix of 2 and ∞ norms: π 0 (z) ∝ z −k ∞ exp − z 2 2 2σ 2 , (9) in which we replace the z −k 2 term in Equation (8) with z −k ∞ . The motivation is that this allows us to allocate more probability mass along the "pointy" directions with larger ∞ norm, and hence decrease the maximum distance term max δ∈B ∞,r D F (λπ 0 π δ ). See Figure 2 for an illustration. In practice, we find that this mixed norm smoothing distribution in Equation (9) work much more efficiently than the 2 norm-based family in Equation (8). Given the difference of 2 and ∞ norms, it is also natural to consider the following pure ∞ norm distributions, which uses ∞ norm in both of the terms of the distribution, Unfortunately, this seemingly natural choice does not work efficiently for ∞ attacks (even worse than the 2 family Equation (8)). This is because the volume of the ∞ ball is in some sense "too large" (e.g., compared with the volume of 2 ball). As a result, in order to make the probability mass of Equation (10) in a reasonable scale, one has to choose a very small value of σ, which makes maximum distance term too large to be practically useful. Theorem 4. Consider the adversarial attacks on the ∞ ball B ∞ ,r = {δ : δ ∞ ≤ r}. Suppose we use the smoothing distribution π 0 in Equation (10) and choose the parameters (k, σ) such that 1) z ∞ is stochastic bounded when z ∼ π 0 , in that for any > 0, there exists a finite M > 0 such that P π0 (|z| > M ) ≤ ; 2) the mode of z ∞ under π 0 equals Cr, where C is some fixed positive constant, then for any ∈ (0, 1) and sufficiently large dimension d, there exists a constant t > 1, such that , we have This shows that, in very high dimensions, the maximum distance term is arbitrarily close to λ which is the maximum possible value of D F [0,1] (λπ 0 π δ ) (see Theorem 1). In particular, this implies that in high dimensional scenario, once f π0 (x 0 ) ≤ (1 − ) for some small , we have V π0 (F [0,1] , B ∞,r ) = O(t −d ) and thus fail to certify.

Section Title: Remark
  Remark The condition 1) and 2) in Theorem 4 are used to ensure that the magnitude of the random perturbations generated by π 0 is within a reasonable range such that the value of f π0 (x 0 ) is not too small, in order to have a high accuracy in the trade-off in Equation (7). Note that the natural images are often contained in cube [0, 1] d . If z ∞ is too large to exceed the region of natural images, the accuracy will be obviously rather poor. Note that if we use variants of Gaussian distribution, we only need ||z|| 2 / √ d to be not too large. Theorem 4 says that once z ∞ is in a reasonably small scale, the maximum distance term must be unreasonably large in high dimensions, yielding a vacuous lower bound.

Section Title: Empirical Justification
  Empirical Justification We construct a simple toy example to verify the advantages of the mixed norm family Equation (9) overall the 2 family in Equation (8) and the ∞ family in Equation (10). We assume that the true classifier is f (x) = I( x 2 ≤ r) in r = 0.65, d = 5 case and plot in  Figure 4  the Pareto frontier of the accuracy and robustness terms in Equation (7) for the three families of smoothing distributions, as we search for the best combinations of parameters (k, σ). We can see that the mixed norm smoothing distribution clearly obtain the best trade-off on accuracy and robustness, and hence guarantees a tighter lower bound for certification.

Section Title: Computational Method
  Computational Method In order to compute the lower bound when using the mixed norm fam- ily Equation (9), we need to establish the closed form solution of the maximum distance term max δ∈B D F [0,1] (λπ 0 || π δ ) similar to Theorem 2. The following result shows that the optimal δ is achieved at one vertex (the pointy points) of the ∞ ball. The proof of Theorem 2 and 5 is non-trivial, thus we defer the details to Appendix A.3. With the optimal δ * found above, we can calculate the bound with similar Monte Carlo approximation outlined in Section 4.1.

Section Title: EXPERIMENTS
  EXPERIMENTS We evaluate our new bound and smoothing distributions for both 2 and ∞ attacks. We compare with the randomized smoothing method of  Cohen et al. (2019)  with Gaussian smoothing distribu- tion. For fair comparisons, we use the same model architecture and pre-trained models provided by  Cohen et al. (2019)  and  Salman et al. (2019) , which are ResNet110 on CIFAR10 and ResNet50 on ImageNet.

Section Title: Settings and Hyperparameters
  Settings and Hyperparameters The details of our method are shown in Algorithm 2 in Appendix. Since our method requires Monte Carlo approximation, we draw 0.1M samples from π 0 and con- struct α = 99.9% confidence lower bounds of that in Equation (7). The optimization on λ is solved using grid search. For 2 attacks, we set k = 500 for CIFAR10 and k = 50000 for ImageNet in our non-Gaussian smoothing distribution Equation (8). If the used model was trained with a Gaus- sian perturbation noise of N (0, σ 2 0 ), then the σ parameter of our smoothing distribution is set to be (d − 1)/(d − 1 − k)σ 0 , such that the expectation of the norm z 2 under our non-Gaussian dis- tribution Equation (8) matches with the norm of N (0, σ 2 0 ). For ∞ situation, we set k = 250 and σ also equals to (d − 1)/(d − 1 − k)σ 0 for the mixed norm smoothing distribution Equation (9). In both cases, the baseline algorithm uses a Gaussian smoothing distribution N (0, σ 2 0 ). More ablation study about k is deferred to Appendix C.

Section Title: Evaluation Metrics
  Evaluation Metrics The methods are evaluated using the certified accuracy defined in  Cohen et al. (2019) . Given an input image x and a perturbation region B, the smoothed classifier is called certified correct if its prediction is correct and has a guaranteed lower bound larger than 1/2 for δ ∈ B. The certified accuracy is the percentage of images that are certified correct. Following  Salman et al. (2019) , we calculate the certified accuracy of all the classifiers in  Cohen et al. (2019)  or  Salman et al. (2019)  for various radius, and report the best results over all of classifiers.

Section Title: 2 CERTIFICATION
  2 CERTIFICATION We test our method on CIFAR10 and ImageNet for 2 certification. For fair comparison, we use the same pre-trained models as  Cohen et al. (2019) , which is trained with Gaussian noise on both CI- FAR10 and ImageNet dataset. The readers are referred to Appendix C for detailed ablation studies.  Table 1  and  Table 2  report the certified accuracy of our method with the non-Gaussian smoothing distribution in Equation (8) and the baseline on CIFAR10 and ImageNet, respectively. We find that our method consistently outperforms the baseline.

Section Title: ∞ CERTIFICATION
  ∞ CERTIFICATION We test our lower bound based on the mixed norm family in Equation (9) for verifying ∞ attacking on CIFAR10, using the models trained by  Salman et al. (2019) . The certified accuracy of our method and the baseline using Gaussian smoothing distribution are shown in  Table 3 . We can see that our method consistently outperforms the Gaussian distribution baseline by a large margin, which empirically shows our distribution is a more suitable distribution for ∞ perturbation. To further confirm the advantage of our method, we plot in  Figure 5  the certified accuracy of our method and Gaussian baseline using models trained with Gaussian perturbation of different vari- ances σ 0 , under different ∞ radius. We again find that our approach outperforms the baseline consistently, especially when the ∞ radius is large. We also experimented our method and baseline on ImageNet, but did not obtain non-trivial results. This is because ∞ verification is extremely hard with very large dimensions. Future work will investigate how to obtain non-trivial bounds for ∞ attacking at ImageNet scales with smoothing classifiers.

Section Title: CONCLUSIONS
  CONCLUSIONS We propose a general variational optimization based framework of adversarial certification with non-Gaussian smoothing distributions. Based on the insights from our new framework and high di- mensional geometry, we propose two new families of non-Gaussian smoothing distributions, which significantly outperform the Gaussian-based smoothing for 2 and ∞ attacking, respectively. Our work provides basis for a variety of future directions, including improved methods for p attacks, and tighter bounds based on adding additional constraints to our optimization framework.

```
