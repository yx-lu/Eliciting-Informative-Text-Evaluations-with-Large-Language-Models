Title:
```
Published as a conference paper at ICLR 2020 RTFM: GENERALISING TO NOVEL ENVIRONMENT DYNAMICS VIA READING
```
Abstract:
```
Obtaining policies that can generalise to new environments in reinforcement learn- ing is challenging. In this work, we demonstrate that language understanding via a reading policy learner is a promising vehicle for generalisation to new envi- ronments. We propose a grounded policy learning problem, Read to Fight Mon- sters (RTFM), in which the agent must jointly reason over a language goal, rele- vant dynamics described in a document, and environment observations. We pro- cedurally generate environment dynamics and corresponding language descrip- tions of the dynamics, such that agents must read to understand new environ- ment dynamics instead of memorising any particular information. In addition, we propose txt2π, a model that captures three-way interactions between the goal, document, and observations. On RTFM, txt2π generalises to new en- vironments with dynamics not seen during training via reading. Furthermore, our model outperforms baselines such as FiLM and language-conditioned CNNs on RTFM. Through curriculum learning, txt2π produces policies that excel on complex RTFM tasks requiring several reasoning and coreference steps.
```

Figures/Tables Captions:
```
Figure 1: RTFM requires jointly reasoning over the goal, a document describing environment dy- namics, and environment observations. This figure shows key snapshots from a trained policy on one randomly sampled environment. Frame 1 shows the initial world. In 4, the agent approaches "fanatical sword", which beats the target "fire goblin". In 5, the agent acquires the sword. In 10, the agent evades the distractor "poison bat" while chasing the target. In 11, the agent engages the target and defeats it, thereby winning the episode. Sprites are used for visualisation - the agent observes cell content in text (shown in white). More examples are in appendix A.
Figure 2: The FiLM 2 layer.
Figure 3: txt2π models interactions between the goal, document, and observations.
Figure 4: Ablation training curves on simplest variant of RTFM. Individual runs are in light colours. Average win rates are in bold, dark lines.
Figure 5: txt2π attention on the full RTFM. These include the document attention conditioned on the goal (top) as well as those conditioned on summaries produced by intermediate FiLM 2 layers. Weights are normalised across words (e.g. horizontally). Darker means higher attention weight.
Table 1: Final win rate on simplest variant of RTFM. The models are trained on one set of dynamics (e.g. training set) and evaluated on another set of dynamics (e.g. evaluation set). "Train" and "Eval" show final win rates on training and eval environments.
Table 2: Curriculum training results. We keep 5 randomly initialised models through the entire curriculum. A cell in row i and column j shows transfer from the best-performing setting in the previous stage (bold in row i − 1) to the new setting in column j. Each cell shows final mean and standard deviation of win rate on the training environments. Each experiment trains for 50 million frames, except for the initial stage (first row, 100 million instead). For the last stage (row 4), we also transfer to a 10 × 10 + dyna + group + nl variant and obtain 61 ± 18 win rate.
Table 3: Win rate when evaluating on new dynamics and world configurations for txt2π on the full RTFM problem.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) has been successful in a variety of areas such as continuous con- trol (Lillicrap et al., 2015), dialogue systems ( Li et al., 2016 ), and game-playing ( Mnih et al., 2013 ). However, RL adoption in real-world problems is limited due to poor sample efficiency and failure to generalise to environments even slightly different from those seen during training. We explore language-conditioned policy learning, where agents use machine reading to discover strategies re- quired to solve a task, thereby leveraging language as a means to generalise to new environments. Prior work on language grounding and language-based RL (see  Luketina et al. (2019)  for a recent survey) are limited to scenarios in which language specifies the goal for some fixed environment dynamics ( Branavan et al., 2011 ;  Hermann et al., 2017 ;  Bahdanau et al., 2019 ;  Fried et al., 2018 ;  Co-Reyes et al., 2019 ), or the dynamics of the environment vary and are presented in language for some fixed goal ( Branavan et al., 2012 ). In practice, changes to goals and to environment dynamics tend to occur simultaneously-given some goal, we need to find and interpret relevant information to understand how to achieve the goal. That is, the agent should account for variations in both by selectively reading, thereby generalising to environments with dynamics not seen during training. Our contributions are two-fold. First, we propose a grounded policy learning problem that we call Read to Fight Monsters (RTFM). In RTFM, the agent must jointly reason over a language goal, a document that specifies environment dynamics, and environment observations. In particular, it must identify relevant information in the document to shape its policy and accomplish the goal. To necessitate reading comprehension, we expose the agent to ever changing environment dynam- ics and corresponding language descriptions such that it cannot avoid reading by memorising any particular environment dynamics. We procedurally generate environment dynamics and natural lan- guage templated descriptions of dynamics and goals to produced a combinatorially large number of environment dynamics to train and evaluate RTFM.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 Second, we propose txt2π to model the joint reasoning problem in RTFM. We show that txt2π generalises to goals and environment dynamics not seen during training, and outper- forms previous language-conditioned models such as language-conditioned CNNs and FiLM ( Perez et al., 2018 ;  Bahdanau et al., 2019 ) both in terms of sample efficiency and final win-rate on RTFM. Through curriculum learning where we adapt txt2π trained on simpler tasks to more complex tasks, we obtain agents that generalise to tasks with natural language documents that require five hops of reasoning between the goal, document, and environment observations. Our qualitative anal- yses show that txt2π attends to parts of the document relevant to the goal and environment ob- servations, and that the resulting agents exhibit complex behaviour such as retrieving correct items, engaging correct enemies after acquiring correct items, and avoiding incorrect enemies. Finally, we highlight the complexity of RTFM in scaling to longer documents, richer dynamics, and natural language variations. We show that significant improvement in language-grounded policy learning is needed to solve these problems in the future.

Section Title: RELATED WORK
  RELATED WORK

Section Title: Language-conditioned policy learning
  Language-conditioned policy learning A growing body of research is learning policies that fol- low imperative instructions. The granularity of instructions vary from high-level instructions for application control ( Branavan, 2012 ) and games ( Hermann et al., 2017 ;  Bahdanau et al., 2019 ) to step-by-step navigation ( Fried et al., 2018 ). In contrast to learning policies for imperative instruc- tions,  Branavan et al. (2011 ; 2012);  Narasimhan et al. (2018)  infer a policy for a fixed goal using features extracted from high level strategy descriptions and general information about domain dy- namics. Unlike prior work, we study the combination of imperative instructions and descriptions of dynamics. Furthermore, we require that the agent learn to filter out irrelevant information to focus on dynamics relevant to accomplishing the goal.

Section Title: Language grounding
  Language grounding Language grounding refers to interpreting language in a non-linguistic con- text. Examples of such context include images ( Barnard & Forsyth, 2001 ), games ( Chen & Mooney, 2008 ;  Wang et al., 2016 ), robot control ( Kollar et al., 2010 ;  Tellex et al., 2011 ), and navigation ( An- derson et al., 2018 ). We study language grounding in interactive games similar to  Branavan (2012) ;  Hermann et al. (2017)  or  Co-Reyes et al. (2019) , where executable semantics are not provided and the agent must learn through experience. Unlike prior work, we require grounding between an un- derspecified goal, a document of environment dynamics, and world observations. In addition, we focus on generalisation to not only new goal descriptions but new environments dynamics.

Section Title: READ TO FIGHT MONSTERS
  READ TO FIGHT MONSTERS We consider a scenario where the agent must jointly reason over a language goal, relevant envi- ronment dynamics specified in a text document, and environment observations. In reading the document, the agent should identify relevant information key to solving the goal in the environment. A successful agent needs to perform this language grounding to generalise to new environments with dynamics not seen during training. To study generalisation via reading, the environment dynamics must differ every episode such that the agent cannot avoid reading by memorising a limited set of dynamics. Consequently, we proce- durally generate a large number of unique environment dynamics (e.g. effective(blessed items, poison monsters)), along with language descriptions of environment dynamics (e.g. blessed items are effective against poison monsters) and goals (e.g. Defeat the order of the forest). We couple a large, customisable ontology inspired by rogue-like games such as NetHack or Diablo, with natural language templates to create a combinatorially rich set of environment dynam- ics to learn from and evaluate on. In RTFM, the agent is given a document of environment dynamics, observations of the environment, and an underspecified goal instruction.  Figure 1  illustrates an instance of the game. Concretely, we design a set of dynamics that consists of monsters (e.g. wolf, goblin), teams (e.g. Order of the For- est), element types (e.g. fire, poison), item modifiers (e.g. fanatical, arcane), and items (e.g. sword, hammer). When the player is in the same cell with a monster or weapon, the player picks up the item or engages in combat with the monster. The player can possess one item at a time, and drops existing Published as a conference paper at ICLR 2020 weapons if they pick up a new weapon. A monster moves towards the player with 60% probability, and otherwise moves randomly. The dynamics, the agent's inventory, and the underspecified goal are rendered as text. The game world is rendered as a matrix of text in which each cell describes the entity occupying the cell. We use human-written templates for stating which monsters belong to which team, which modifiers are effective against which element, and which team the agent should defeat (see appendix H for details on collection and G for a list of entities in the game). In order to achieve the goal, the agent must cross-reference relevant information in the document and as well as in the observations. During every episode, we subsample a set of groups, monsters, modifiers, and elements to use. We randomly generate group assignments of which monsters belong to which team and which modifier is effective against which element. A document that consists of randomly ordered statements cor- responding to this group assignment is presented to the agent. We sample one element, one team, and a monster from that team (e.g. "fire goblin" from "Order of the forest") to be the target monster. Additionally, we sample one modifier that beats the element and an item to be the item that defeats the target monster (e.g. "fanatical sword"). Similarly, we sample an element, a team, and a monster from a different team to be the distractor monster (e.g. poison bat), as well as an item that defeats the distractor monster (e.g. arcane hammer). In order to win the game (e.g.  Figure 1 ), the agent must 1. identify the target team from the goal (e.g. Order of the Forest) 2. identify the monsters that belong to that team (e.g. goblin, jaguar, and ghost) 3. identify which monster is in the world (e.g. goblin), and its element (e.g. fire) 4. identify the modifiers that are effective against this element (e.g. fanatical, shimmering) 5. find which modifier is present (e.g. fanatical), and the item with the modifier (e.g. sword) Published as a conference paper at ICLR 2020 6. pick up the correct item (e.g. fanatical sword) 7. engage the correct monster in combat (e.g. fire goblin). If the agent deviates from this trajectory (e.g. does not have correct item before engaging in combat, engages with distractor monster), it cannot defeat the target monster and therefore will lose the game. The agent receives a reward of +1 if it wins the game and -1 otherwise. RTFM presents challenges not found in prior work in that it requires a large number of grounding steps in order to solve a task. In order to perform this grounding, the agent must jointly reason over a language goal and document of dynamics, as well as environment observations. In addition to the environment, the positions of the target and distractor within the document are randomised-the agent cannot memorise ordering patterns in order to solve the grounding problems, and must instead identify information relevant to the goal and environment at hand. We split environments into train and eval sets. No assignments of monster-team-modifier-element are shared between train and eval to test whether the agent is able to generalise to new environments with dynamics not seen during training via reading. There are more than 2 million train or eval environments without considering the natural language templates, and 200 million otherwise. With random ordering of templates, the number of unique documents exceeds 15 billion.

Section Title: MODEL
  MODEL We propose the txt2π model, which builds representations that capture three-way interactions between the goal, document describing environment dynamics, and environment observations. We begin with definition of the Bidirectional Feature-wise Linear Modulation (FiLM 2 ) layer, which forms the core of our model.

Section Title: BIDIRECTIONAL FEATURE-WISE LINEAR MODULATION (FILM 2 ) LAYER
  BIDIRECTIONAL FEATURE-WISE LINEAR MODULATION (FILM 2 ) LAYER Feature-wise linear modulation (FiLM), which modulates visual inputs using representations of textual instructions, is an effective method for image captioning ( Perez et al., 2018 ) and instruction fol- lowing ( Bahdanau et al., 2019 ). In RTFM, the agent must not only filter concepts in the visual domain using language but filter concepts in the text domain using visual observa- tions. To support this, FiLM 2 builds codependent representations of text and visual inputs by further incorporating conditional representations of the text given visual observations.  Figure 2  shows the FiLM 2 layer. We use upper-case bold letters to denote tensors, lower-case bold letters for vectors, and non-bold letters for scalars. Exact dimensions of these variables are shown in Table 4 in appendix B. Let x text denote a fixed-length d text -dimensional representation of the text and X vis the representation of visual inputs with height H, width W , and d vis channels. Let Conv denote a convolution layer. Let + and * symbols denote element-wise addition and multiplication operations that broadcast over spatial dimensions. We first modulate visual features using text features: Unlike FiLM, we additionally modulate text features using visual features: The output of the FiLM 2 layer consists of the sum of the modulated features V , as well as a max-pooled summary s over this sum across spatial dimensions.

Section Title: THE T X T 2π MODEL
  THE T X T 2π MODEL We model interactions between observations from the environment, goal, and document us- ing FiLM 2 layers. We first encode text inputs using bidirectional LSTMs, then compute summaries using self-attention and conditional summaries using attention. We concatenate text summaries into text features, which, along with visual features, are processed through consecutive FiLM 2 layers. In this case of a textual environment, we consider the grid of word embeddings as the visual features for FiLM 2 . The final FiLM 2 output is further processed by MLPs to compute a policy distribution over actions and a baseline for advantage estimation.  Figure 3  shows the txt2π model. Let E obs denote word embeddings corresponding to the observations from the environment, where E obs [:, :, i, j] represents the embeddings corresponding to the l obs -word string that describes the objects in location (i, j) in the grid-world. Let E doc , E inv , and E goal respectively denote the em- beddings corresponding to the l doc -word document, the l inv -word inventory, and the l goal -word goal. We first compute a fixed-length summary c goal of the the goal using a bidirectional LSTM ( Hochre- iter & Schmidhuber, 1997 ) followed by self-attention ( Lee et al., 2017 ;  Zhong et al., 2018 ). We abbreviate self-attention over the goal as c goal = selfattn(H goal ). We similarly compute a summary of the inventory as c inv = selfattn(BiLSTM inv (E inv )). Next, we represent the document encoding conditioned on the goal using dot-product attention ( Luong et al., 2015 ). We abbreviate attention over the document encoding conditioned on the goal summary as c doc = attend(H doc , c goal ). Next, we build the joint representation of the inputs using succes- sive FiLM 2 layers. At each layer, the visual input to the FiLM 2 layer is the concatenation of the output of the previous layer with positional features. For each cell, the positional feature X pos consists of the x and y distance from the cell to the agent's position respectively, normalized by the width and height of the grid-world. The text input is the concatenation of the goal summary, the inventory summary, the attention over the document given the goal, and the attention over the document given the previous visual summary. Let [a; b] denote the feature-wise concatenation of a BiLSTM vis-doc (E doc ) is another encoding of the document similar to H goal , produced using a sep- arate LSTM, such that the document is encoded differently for attention with the visual features and with the goal. For i = 0, we concatenate the bag-of-words embeddings of the grid with positional features as the initial visual features V (0) = [ j E obs,j ; X pos ]. We max pool a linear transform of the initial visual features to compute the initial visual summary s (0) = MaxPool(W ini V (0) + b ini ). Let s (last) denote visual summary of the last FiLM 2 layer. We compute the policy y policy and baseline y baseline as o = ReLU(W o s (last) + b o ) (20) y policy = MLP policy (o) (21) y baseline = MLP baseline (o) (22) where MLP policy and MLP baseline are 2-layer multi-layer perceptrons with ReLU activation. We train using TorchBeast ( Küttler et al., 2019 ), an implementation of IMPALA ( Espeholt et al., 2018 ). Please refer to appendix D for details.

Section Title: EXPERIMENTS
  EXPERIMENTS We consider variants of RTFM by varying the size of the grid-world (6 × 6 vs 10 × 10), allowing many-to-one group assignments to make disambiguation more difficult (group), allowing dynamic, moving monsters that hunt down the player (dyna), and using natural language templated docu- ments (nl). In the absence of many-to-one assignments, the agent does not need to perform steps 3 and 5 in section 3 as there is no need to disambiguate among many assignees, making it easier to identify relevant information. We compare txt2π to the FiLM model by  Bahdanau et al. (2019)  and a language-conditioned residual CNN model. We train on one set of dynamics (e.g. group assignments of monsters and modifiers) and evaluated on a held-out set of dynamics. We also study three variants of txt2π. In no task attn, the document attention conditioned on the goal utterance (equation 16) is re- moved and the goal instead represented through self-attention and concatenated with the rest of the text features. In no vis attn, we do not attend over the document given the visual output of the previous layer (equation 18), and the document is instead represented through self-attention. In no text mod, text modulation using visual features (equation 6) is removed. Please see ap- pendix C for model details on our model and baselines, and appendix D for training details.

Section Title: COMPARISON TO BASELINES AND ABLATIONS
  COMPARISON TO BASELINES AND ABLATIONS We compare txt2π to baselines and ablated variants on a simplified variant of RTFM in which there are one-to-one group assignments (no group), stationary monsters (no dyna), and no nat- ural language templated descriptions (no nl).  Figure 4  shows that compared to baselines and ab- lated variants, txt2π is more sample efficient and converges to higher performance. Moreover, no ablated variant is able to solve the tasks-it is the combination of ablated features that en- ables txt2π to win consistently. Qualitatively, the ablated variants converge to locally optimum policies in which the agent often picks up a random item and then attacks the correct monster, re- sulting in a ∼ 50% win rate.  Table 1  shows that all models, with the exception of the CNN baseline, generalise to new evaluation environments with dynamics and world configurations not seen during training, with txt2π outperforming FiLM and the CNN model. We find similar results for txt2π, its ablated variants, and baselines on a separate, language-based rock-paper-scissors task in which the agent needs to deduce cyclic dependencies (which type beats which other type) through reading in order to acquire the correct item and defeat a monster. We observe that the performance of reading models transfer from training environments to new envi- ronments with unseen types and unseen dependencies. Compared to ablated variants and baselines, txt2π is more sample efficient and achieves higher performance on both training and new envi- ronment dynamics. When transferring to new environments, txt2π remains more sample efficient than the other models. Details on these experiments are found in appendix E. Due to the long sequence of co-references the agent must perform in order to solve the full RTFM (10 × 10 with moving monsters, many-to-one group as- signments, and natural language templated docu- ments) we design a curriculum to facilitate policy learning by starting with simpler variants of RTFM. We start with the simplest variant (no group, no dyna, no nl) and then add in an additional di- mension of complexity. We repeatedly add more complexity until we obtain 10×10 worlds with mov- ing monsters, many-to-one group assignments and natural language templated descriptions. The per- formance across the curriculum is shown in  Table 2  Published as a conference paper at ICLR 2020 (see Figure 13 in appendix F for training curves of each stage). We see that curriculum learning is crucial to making progress on RTFM, and that initial policy training (first row of  Table 2 ) with addi- tional complexities in any of the dimensions result in significantly worse performance. We take each of the 5 runs after training through the whole curriculum and evaluate them on dynamics not seen during training.  Table 3  shows variants of the last stage of the curriculum in which the model was trained on 6 × 6 versions of the full RTFM and in which the model was trained on 10 × 10 versions of the full RTFM. We see that models trained on smaller worlds generalise to bigger worlds. Despite curriculum learning, however, performance of the final model trail that of human players, who can consistently solve RTFM. This highlights the difficulties of the RTFM problem and suggests that there is significant room for improvement in developing better language grounded policy learners. Attention maps.  Figure 5  shows attention conditioned on the goal and on observation summaries produced by intermediate FiLM 2 layers. Goal-conditioned attention consistently locates the clause that contains the team the agent is supposed to attack. Intermediate layer attentions focus on regions near modifiers and monsters, particularly those that are present in the observations. These results suggests that attention mechanisms in txt2π help identify relevant information in the document.

Section Title: Analysis of trajectories and failure modes
  Analysis of trajectories and failure modes We examine trajectories from well-performing poli- cies (80% win rate) as well as poorly-performing policies (50% win rate) on the full RTFM. We find that well-performing policies exhibit a number of consistent behaviours such as identifying the correct item to pick up to fight the target monster, avoiding distractors, and engaging target monsters after acquiring the correct item. In contrast, the poorly-performing policies occasionally pick up the wrong item, causing the agent to lose when engaging with a monster. In addition, it occasionally gets stuck in evading monsters indefinitely, causing the agent to lose when the time runs out. Replays of both policies can be found in GIFs in the supplementary materials 1 .

Section Title: CONCLUSION
  CONCLUSION We proposed RTFM, a grounded policy learning problem in which the agent must jointly reason over a language goal, relevant dynamics specified in a document, and environment observations. In order to study RTFM, we procedurally generated a combinatorially large number of environment dynam- ics such that the model cannot memorise a set of environment dynamics and must instead generalise via reading. We proposed txt2π, a model that captures three-way interactions between the goal, Published as a conference paper at ICLR 2020 document, and observations, and that generalises to new environments with dynamics not seen dur- ing training. txt2π outperforms baselines such as FiLM and language-conditioned CNNs. Through curriculum learning, txt2π performs well on complex RTFM tasks that require several reasoning and coreference steps with natural language templated goals and descriptions of the dynamics. Our work suggests that language understanding via reading is a promising way to learn policies that generalise to new environments. Despite curriculum learning, our best models trail performance of human players, suggesting that there is ample room for improvement in grounded policy learning on complex RTFM problems. In addition to jointly learning policies based on external documentation and language goals, we are interested in exploring how to use supporting evidence in external doc- umentation to reason about plans ( Andreas et al., 2018 ) and induce hierarchical policies ( Hu et al., 2019 ;  Jiang et al., 2019 ).

```
