Title:
```
Under review as a conference paper at ICLR 2020 UNSUPERVISED REPRESENTATION LEARNING BY PREDICTING RANDOM DISTANCES
```
Abstract:
```
Deep neural networks have gained tremendous success in a broad range of ma- chine learning tasks due to its remarkable capability to learn semantic-rich features from high-dimensional data. However, they often require large-scale labelled data to successfully learn such features, which significantly hinders their adaption into unsupervised learning tasks, such as anomaly detection and clustering, and limits their applications into critical domains where obtaining massive labelled data is prohibitively expensive. To enable downstream unsupervised learning on those domains, in this work we propose to learn features without using any labelled data by training neural networks to predict data distances in a randomly projected space. Random mapping is a theoretical proven approach to obtain approximately preserved distances. To well predict these random distances, the representation learner is optimised to learn genuine class structures that are implicitly embedded in the randomly projected space. Experimental results on 19 real-world datasets show our learned representations substantially outperform state-of-the-art com- peting methods in both anomaly detection and clustering tasks.
```

Figures/Tables Captions:
```
Figure 1: The proposed random distance prediction (RDP) framework. Specifically, a weight-shared two-branch neural network φ first projects x i and x j onto a new space, in which we aim to min- imise the random distance prediction loss L rdp , i.e., the difference between the learned distance φ(x i ; Θ), φ(x j ; Θ) and a predefined distance η(x i ), η(x j ) (η denotes an existing random map- ping). L aux is an auxiliary loss that is optionally applied to one network branch to learn comple- mentary information w.r.t. L rdp . The lower right figure presents a 2-D t-SNE (Hinton & Roweis, 2003) visualisation of the features learned by RDP on a small toy dataset optdigits with 10 classes.
Table 1: AUC-ROC (mean±std) performance of RDP and its five competing methods on 14 datasets.
Table 2: AUC-PR (mean±std) performance of RDP and its five competing methods on 14 datasets.
Table 3: AUC-ROC results of anomaly detection (see Appendix C for similar AUC-PR results).
Table 4: NMI and F-score performance of K-means on the original space and projected spaces.
Table 5: F-score performance of K-means clustering (see similar NMI results in Appendix D).
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Unsupervised representation learning aims at automatically extracting expressive feature represen- tations from data without any manually labelled data. Due to the remarkable capability to learn semantic-rich features, deep neural networks have been becoming one widely-used technique to em- power a broad range of machine learning tasks. One main issue with these deep learning techniques is that a massive amount of labelled data is typically required to successfully learn these expressive features. As a result, their transformation power is largely reduced for tasks that are unsupervised in nature, such as anomaly detection and clustering. This is also true to critical domains, such as healthcare and fintech, where collecting massive labelled data is prohibitively expensive and/or is impossible to scale. To bridge this gap, in this work we explore fully unsupervised representation learning techniques to enable downstream unsupervised learning methods on those critical domains. In recent years, many unsupervised representation learning methods ( Mikolov et al., 2013a ;  Le & Mikolov, 2014 ;  Misra et al., 2016 ;  Lee et al., 2017 ;  Gidaris et al., 2018 ) have been introduced, of which most are self-supervised approaches that formulate the problem as an annotation free pretext task. These methods explore easily accessible information, such as temporal or spatial neighbour- hood, to design a surrogate supervisory signal to empower the feature learning. These methods have achieved significantly improved feature representations of text/image/video data, but they are often inapplicable to tabular data since it does not contain the required temporal or spatial supervisory in- formation. We therefore focus on unsupervised representation learning of high-dimensional tabular data. Although many traditional approaches, such as random projection ( Li et al., 2006 ), principal component analysis (PCA) ( Rahmani & Atia, 2017 ), manifold learning ( Donoho & Grimes, 2003 ;  Hinton & Roweis, 2003 ) and autoencoder ( Vincent et al., 2010 ), are readily available for handling those data, many of them ( Donoho & Grimes, 2003 ;  Hinton & Roweis, 2003 ;  Rahmani & Atia, 2017 ) are often too computationally costly to scale up to large or high-dimensional data. Approaches like random projection and autoencoder are very efficient but they often fail to capture complex class structures due to its underlying data assumption or weak supervisory signal. In this paper, we introduce a Random Distance Prediction (RDP) model which trains neural networks to predict data distances in a randomly projected space. When the distance information captures in- Under review as a conference paper at ICLR 2020 trinsic class structure in the data, the representation learner is optimised to learn the class structure to minimise the prediction error. Since distances are concentrated and become meaningless in high dimensional spaces ( Beyer et al., 1999 ), we seek to obtain distances preserved in a projected space to be the supervisory signal. Random mapping is a highly efficient yet theoretical proven approach to obtain such approximately preserved distances. Therefore, we leverage the distances in the ran- domly projected space to learn the desired features. Intuitively, random mapping preserves rich local proximity information but may also keep misleading proximity when its underlying data distribu- tion assumption is inexact; by minimising the random distance prediction error, RDP essentially leverages the preserved data proximity and the power of neural networks to learn globally consis- tent proximity and rectify the inconsistent proximity information, resulting in a substantially better representation space than the original space. We show this simple random distance prediction en- ables us to achieve expressive representations with no manually labelled data. In addition, some task-dependent auxiliary losses can be optionally added as a complementary supervisory source to the random distance prediction, so as to learn the feature representations that are more tailored for a specific downstream task. In summary, this paper makes the following three main contributions. • We propose a random distance prediction formulation, which is very simple yet offers a highly effective supervisory signal for learning expressive feature representations that optimise the distance preserving in random projection. The learned features are sufficiently generic and work well in enabling different downstream learning tasks. • Our formulation is flexible to incorporate task-dependent auxiliary losses that are comple- mentary to random distance prediction to further enhance the learned features, i.e., features that are specifically optimised for a downstream task while at the same time preserving the generic proximity as much as possible. • As a result, we show that our instantiated model termed RDP enables substantially bet- ter performance than state-of-the-art competing methods in two key unsupervised tasks, anomaly detection and clustering, on 19 real-world high-dimensional tabular datasets.

Section Title: RANDOM DISTANCE PREDICTION MODEL
  RANDOM DISTANCE PREDICTION MODEL

Section Title: THE PROPOSED FORMULATION AND THE INSTANTIATED MODEL
  THE PROPOSED FORMULATION AND THE INSTANTIATED MODEL We propose to learn representations by training neural networks to predict distances in a randomly projected space without manually labelled data. The key intuition is that, given some distance information that faithfully encapsulates the underlying class structure in the data, the representation learner is forced to learn the class structure in order to yield distances that are as close as the given distances. Our proposed framework is illustrated in  Figure 1 . Specifically, given data points x i , x j ∈ R D , we first feed them into a weight-shared Siamese-style neural network φ(x; Θ). φ : R D → R M is a representation learner with the parameters Θ to map the data onto a M -dimensional new space. Then we formulate the subsequent step as a distance prediction task and define a loss function as: L rdp (x i , x j ) = l( φ(x i ; Θ), φ(x j ; Θ) , η(x i ), η(x j ) ), (1) where η is an existing projection method and l is a function of the difference between its two inputs. Here one key ingredient is how to obtain trustworthy distances via η. Also, to efficiently optimise the model, the distance derivation needs to be computationally efficient. In this work, we use the inner products in a randomly projected space as the source of distance/similarity since it is very efficient and there is strong theoretical support of its capacity in preserving the genuine distance information. Thus, our instantiated model RDP specifies L rdp (x i , x j ) as follows 1 : L rdp (x i , x j ) = (φ(x i ; Θ) · φ(x j ; Θ) − η(x i ) · η(x j )) 2 , (2) where φ is implemented by multilayer perceptron for dealing with tabular data and η : R D → R K is an off-the-shelf random data mapping function (see Sections 3.1 and 3.2 for detail). Despite its sim- plicity, this loss offers a powerful supervisory signal to learn semantic-rich feature representations that substantially optimise the underlying distance preserving in η (see Section 3.3 for detail).

Section Title: FLEXIBILITY TO INCORPORATE TASK-DEPENDENT COMPLEMENTARY AUXILIARY LOSS
  FLEXIBILITY TO INCORPORATE TASK-DEPENDENT COMPLEMENTARY AUXILIARY LOSS Minimising L rdp learns to preserve pairwise distances that are critical to different learning tasks. Moreover, our formulation is flexible to incorporate a task-dependent auxiliary loss L aux , such as reconstruction loss ( Hinton & Salakhutdinov, 2006 ) for clustering or novelty loss ( Burda et al., 2019 ) for anomaly detection, to complement the proximity information and enhance the feature learning. For clustering, an auxiliary reconstruction loss is defined as: L clu aux (x) = (x − φ (φ(x; Θ); Θ )) 2 , (3) where φ is an encoder and φ : R M → R D is a decoder. This loss may be optionally added into RDP to better capture global feature representations. Similarly, in anomaly detection a novelty loss may be optionally added, which is defined as: By using a fixed η, minimising L ad aux helps learn the frequency of underlying patterns in the data ( Burda et al., 2019 ), which is an important complementary supervisory source for the sake of anomaly detection. As a result, anomalies or novel points are expected to have substantially larger (φ(x; Θ ) − η(x)) 2 than normal points, so this value can be directly leveraged to detect anomalies. Note since L ad aux involves a mean squared error between two vectors, the dimension of the projected space resulted by φ and η is required to be equal in this case. Therefore, when this loss is added into RDP, the M in φ and K in η need to be the same. We do not have this constraint in other cases.

Section Title: THEORETICAL ANALYSIS OF RDP
  THEORETICAL ANALYSIS OF RDP This section shows the proximity information can be well approximated using inner products in two types of random projection spaces. This is a key theoretical foundation to RDP. Also, to accurately predict these distances, RDP is forced to learn the genuine class structure in the data.

Section Title: WHEN LINEAR PROJECTION IS USED
  WHEN LINEAR PROJECTION IS USED Random projection is a simple yet very effective linear feature mapping technique which has proven the capability of distance preservation. Let X ⊂ R N ×D be a set of N data points, random projection Under review as a conference paper at ICLR 2020 uses a random matrix A ⊂ R K×D to project the data onto a lower K-dimensional space by X = AX . The Johnson-Lindenstrauss lemma ( Johnson & Lindenstrauss, 1984 ) guarantees the data points can be mapped to a randomly selected space of suitably lower dimension with the distances between the points are approximately preserved. More specifically, let ∈ (0, 1 2 ) and K = 20 log n 2 . There exists a linear mapping f : R D → R K such that for all x i , x j ∈ X : Furthermore, assume the entries of the matrix A are sampled independently from a Gaussian distri- bution N (0, 1). Then, the norm of x ∈ R D can be preserved as: Under such random projections, the norm preservation helps well preserve the inner products: wherex is a normalised x such that ||x|| ≤ 1. The proofs of Eqns. (5), (6) and (7) can be found in ( Vempala, 1998 ). Eqn. (7) states that the inner products in the randomly projected space can largely preserve the inner products in the original space, particularly when the projected dimension K is large.

Section Title: WHEN NON-LINEAR PROJECTION IS USED
  WHEN NON-LINEAR PROJECTION IS USED Here we show that some non-linear random mapping methods are approximate to kernel functions which are a well-established approach to obtain reliable distance/similarity information. The key to this approach is the kernel function k : X ×X → R, which is defined as k(x i , x j ) = ψ(x i ), ψ(x j ) , where ψ is a feature mapping function but needs not to be explicitly defined and ·, · denotes a suitable inner product. A non-linear kernel function such as polynomial or radial basis function (RBF) kernel is typically used to project linear-inseparable data onto a linear-separable space. The relation between non-linear random mapping and kernel methods is justified in ( Rahimi & Recht, 2008 ), which shows that an explicit randomised mapping function g : R D → R K can be defined to project the data points onto a low-dimensional Euclidean inner product space such that the inner products in the projected space approximate the kernel evaluation: Let A be the mapping matrix. Then to achieve the above approximation, A is required to be drawn from Fourier transform and shift-invariant functions such as cosine function are finally applied to Ax to yield a real-valued output. By transforming the two data points x i and x j in this manner, their inner product g(x i ) · g(x j ) is an unbiased estimator of k(x i , x j ).

Section Title: LEARNING CLASS STRUCTURE BY RANDOM DISTANCE PREDICTION
  LEARNING CLASS STRUCTURE BY RANDOM DISTANCE PREDICTION Our model using only the random distances as the supervisory signal can be formulated as: where y ij = η(x i ) · η(x j ). Let Y η ∈ R N ×N be the distance/similarity matrix of the N data points resulted by η. Then to minimise the prediction error in Eqn. (9), φ is optimised to learn the underlying class structure embedded in Y. As shown in the properties in Eqns. (7) and (8), Y η can effectively preserve local proximity information when η is set to be either the random projection- based f function or the kernel method-based g function. However, those proven η is often built upon some underlying data distribution assumption, e.g., Gaussian distribution in random projection or Gaussian RBF kernel, so the η-projected features can preserve misleading proximity when the distribution assumption is inexact. In this case, Y η is equivalent to the imperfect ground truth with partial noise. Then optimisation with Eqn. (9) is to leverage the power of neural networks to learn consistent local proximity information and rectify inconsistent proximity, resulting in a significantly optimised distance preserving space. The resulting space conveys substantially richer semantics than the η projected space when Y η contains sufficient genuine supervision information.

Section Title: EXPERIMENTS
  EXPERIMENTS This section evaluates the learned representations through two typical unsupervised tasks: anomaly detection and clustering. Some preliminary results of classification can be found in Appendix H.

Section Title: PERFORMANCE EVALUATION IN ANOMALY DETECTION
  PERFORMANCE EVALUATION IN ANOMALY DETECTION

Section Title: EXPERIMENTAL SETTINGS
  EXPERIMENTAL SETTINGS Our RDP model is compared with five state-of-the-art methods, including iForest ( Liu et al., 2008 ), autoencoder (AE) ( Hinton & Salakhutdinov, 2006 ), REPEN ( Pang et al., 2018 ), DAGMM ( Zong et al., 2018 ) and RND ( Burda et al., 2019 ). iForest and AE are two of the most popular baselines. The other three methods learn representations specifically for anomaly detection. As shown in  Table 1 , the comparison is performed on 14 publicly available datasets of various domains, including network intrusion, credit card fraud detection, disease detection and bank cam- paigning. Many of the datasets contain real anomalies, including DDoS, Donors, Backdoor, Cred- itcard, Lung, Probe and U2R. Following ( Liu et al., 2008 ;  Pang et al., 2018 ;  Zong et al., 2018 ), the rare class(es) is treated as anomalies in the other datasets to create semantically real anomalies. The Area Under Receiver Operating Characteristic Curve (AUC-ROC) and the Area Under Precision- Recall Curve (AUC-PR) are used as our performance metrics. Larger AUC-ROC/AUC-PR indicates better performance. The reported performance is averaged over 10 independent runs. Our RDP model uses the optional novelty loss for anomaly detection task by default. Similar to RND, given a data point x, its anomaly score in RDP is defined as the mean squared error between the two projections resulted by φ(x; Θ ) and η(x). Also, a boosting process is used to filter out 5% likely anomalies per iteration to iteratively improve the modelling of RDP. This is because the modelling is otherwise largely biased when anomalies are presented. In the ablation study in Section 4.1.3, we will show the contribution of all these components.

Section Title: COMPARISON TO THE STATE-OF-THE-ART COMPETING METHODS
  COMPARISON TO THE STATE-OF-THE-ART COMPETING METHODS The AUC-ROC and AUC-PR results are respectively shown in  Tables 1  and 2. RDP outperforms all the five competing methods in both of AUC-ROC and AUC-PR in at least 12 out of 14 datasets. This improvement is statistically significant at the 95% confidence level according to the two-tailed sign test ( Demšar, 2006 ). Remarkably, RDP obtains more than 10% AUC-ROC/AUC-PR improvement over the best competing method on six datasets, including Donors, Ad, Bank, Celeba, Lung and U2R. RDP can be thought as a high-level synthesis of REPEN and RND, because REPEN leverages a pairwise distance-based ranking loss to learn representations for anomaly detection while RND is built using L ad aux . In nearly all the datasets, RDP well leverages both L rdp and L ad aux to achieve significant improvement over both REPEN and RND. In very limited cases, such as on datasets Backdoor and Census where RND performs very well while REPEN performs less effectively, RDP is slightly downgraded due to the use of L rdp . In the opposite case, such as Probe, on which REPEN performs much better than RND, the use of L ad aux may drag down the performance of RDP a bit.

Section Title: ABLATION STUDY
  ABLATION STUDY This section examines the contribution of L rdp , L ad aux and the boosting process to the performance of RDP. The experimental results in AUC-ROC are given in  Table 3 , where RDP\X means the RDP variant that removes the 'X' module from RDP. In the last two columns, Org SS indicates that we directly use the distance information calculated in the original space as the supervisory signal, while SRP SS indicates that we use SRP to obtain the distances as the supervisory signal. It is clear that the full RDP model is the best performer. Using the L rdp loss only, i.e., RDP\L ad aux , can achieve performance substantially better than, or comparably well to, the five competing methods in  Table 1 . This is mainly because the L rdp loss alone can effectively force our representation learner to learn the underlying class structure on most datasets so as to minimise its prediction error. The use of L ad aux and boosting process well complement the L rdp loss on the other datasets. In terms of supervisory source, RDP and SRP SS perform substantially better than Org SS on most datasets. This is because the distances in both the non-linear random projection in RDP and the linear projection in SRP SS well preserve the distance information, enabling RDP to effectively learn much more faithful class structure than that working on the original space.

Section Title: PERFORMANCE EVALUATION IN CLUSTERING
  PERFORMANCE EVALUATION IN CLUSTERING

Section Title: EXPERIMENTAL SETTINGS
  EXPERIMENTAL SETTINGS For clustering, RDP is compared with four state-of-the-art unsupervised representation learning methods in four different areas, including HLLE ( Donoho & Grimes, 2003 ) in manifold learning, Sparse Random Projection (SRP) ( Li et al., 2006 ) in random projection, autoencoder (AE) ( Hinton & Salakhutdinov, 2006 ) in data reconstruction-based neural network methods and Coherence Pursuit (COP) ( Rahmani & Atia, 2017 ) in robust PCA. These representation learning methods are first used to yield the new representations, and K-means ( Hartigan & Wong, 1979 ) is then applied to the Under review as a conference paper at ICLR 2020 representations to perform clustering. Two widely-used clustering performance metrics, Normalised Mutual Info (NMI) score and F-score, are used. Larger NMI or F-score indicates better performance. The clustering performance in the original feature space, denoted as Org, is used as a baseline. As shown in  Table 4 , five high-dimensional real-world datasets are used. Some of the datasets are image/text data. Since here we focus on the performance on tabular data, they are converted into tabular data using simple methods, i.e., by treating each pixel as a feature unit for image data or using bag-of-words representation for text data 2 . The reported NMI score and F-score are averaged over 30 times to address the randomisation issue in K-means clustering. In this section RDP adds the reconstruction loss L clu aux by default, but RDP also works very well without the use of L clu aux .

Section Title: COMPARISON TO THE-STATE-OF-THE-ART COMPETING METHODS
  COMPARISON TO THE-STATE-OF-THE-ART COMPETING METHODS   Table 4  shows the NMI and F-score performance of K-means clustering. Our method RDP en- ables K-means to achieve the best performance on three datasets and ranks second in the other two datasets. RDP-enabled clustering performs substantially and consistently better than that based on AE in terms of both NMI and F-score. This demonstrates that the random distance loss enables RDP to effectively capture some class structure in the data which cannot be captured by using the reconstruction loss. RDP also consistently outperforms the random projection method, SRP, and the robust PCA method, COP. It is interesting that K-means clustering performs best in the original space on Sector. This may be due to that this data contains many relevant features, resulting in no obvious curse of dimensionality issue. Olivetti may contain complex manifolds which require ex- tensive neighbourhood information to find them, so only HLLE can achieve this goal in such cases. Nevertheless, RDP performs much more stably than HLLE across the five datasets.

Section Title: ABLATION STUDY
  ABLATION STUDY Similar to anomaly detection, this section examines the contribution of the two loss functions L rdp and L clu aux to the performance of RDP, as well as the impact of different supervisory sources on the performance. The F-score results of this experiment are shown in  Table 5 , in which the notations have exactly the same meaning as in  Table 3 . The full RDP model that uses both L rdp and L clu aux performs more favourably than its two variants, RDP\L rdp and RDP\L clu aux , but it is clear that using L rdp only performs very comparably to the full RDP. However, using L clu aux only may result in large 2 RDP can also build upon advanced representation learning methods for the data transformation, for which some interesting preliminary results are presented in Appendix G. Under review as a conference paper at ICLR 2020 performance drops in some datasets, such as R8, 20news and Olivetti. This indicates L rdp is a more important loss function to the overall performance of the full RDP model. In terms of supervisory source, distances obtained by the non-linear random projection in RDP are much more effective than the two other sources on some datasets such as Olivetti and RCV1. Three different supervisory sources are very comparable on the other three datasets.

Section Title: RELATED WORK
  RELATED WORK

Section Title: Self-supervised Learning
  Self-supervised Learning Self-supervised learning has been recently emerging as one of the most popular and effective approaches for representation learning. Many of the self-supervised methods learn high-level representations by predicting some sort of 'context' information, such as spatial or temporal neighbourhood information. For example, the popular distributed representation learning techniques in NLP, such as CBOW/skip-gram ( Mikolov et al., 2013a ) and phrase/sentence embed- dings in ( Mikolov et al., 2013b ;  Le & Mikolov, 2014 ;  Hill et al., 2016 ), learn the representations by predicting the text pieces (e.g., words/phrases/sentences) using its surrounding pieces as the con- text. In image processing, the pretext task can be the prediction of a patch of missing pixels ( Pathak et al., 2016 ;  Zhang et al., 2017 ) or the relative position of two patches ( Doersch et al., 2015 ). Also, a number of studies ( Goroshin et al., 2015 ;  Misra et al., 2016 ;  Lee et al., 2017 ;  Oord et al., 2018 ) explore temporal contexts to learn representations from video data, e.g., by learning the temporal order of sequential frames. Some other methods ( Agrawal et al., 2015 ;  Zhou et al., 2017 ;  Gidaris et al., 2018 ) are built upon a discriminative framework which aims at discriminating the images be- fore and after some transformation, e.g., ego motion in video data ( Agrawal et al., 2015 ;  Zhou et al., 2017 ) and rotation of images ( Gidaris et al., 2018 ). There have also been popular to use generative adversarial networks (GANs) to learn features ( Radford et al., 2015 ;  Chen et al., 2016 ). The above methods have demonstrated powerful capability to learn semantic representations. However, most of them use the supervisory signals available in image/video data only, which limits their application into other types of data, such as traditional tabular data. Although our method may also work on image/video data, we focus on handling high-dimensional tabular data to bridge this gap.

Section Title: Other Approaches
  Other Approaches There have been several well-established unsupervised representation learn- ing approaches for handling tabular data, such as random projection ( Arriaga & Vempala, 1999 ;  Bingham & Mannila, 2001 ;  Li et al., 2006 ), PCA ( Wold et al., 1987 ;  Schölkopf et al., 1997 ;  Rah- mani & Atia, 2017 ), manifold learning ( Roweis & Saul, 2000 ;  Donoho & Grimes, 2003 ;  Hinton & Roweis, 2003 ;  McInnes et al., 2018 ) and autoencoder ( Hinton & Salakhutdinov, 2006 ;  Vincent et al., 2010 ). One notorious issue of PCA or manifold learning approaches is their prohibitive computa- tional cost in dealing with large-scale high-dimensional data due to the costly neighbourhood search and/or eigen decomposition. Random projection is a computationally efficient approach, supported by proven distance preservation theories such as the Johnson-Lindenstrauss lemma ( Johnson & Lin- denstrauss, 1984 ). We show that the preserved distances by random projection can be harvested to effectively supervise the representation learning. Autoencoder networks are another widely-used efficient feature learning approach which learns low-dimensional representations by minimising re- construction errors. One main issue with autoencoders is that they focus on preserving global infor- mation only, which may result in loss of local structure information. Some representation learning methods are specifically designed for anomaly detection ( Pang et al., 2018 ;  Zong et al., 2018 ;  Burda et al., 2019 ). By contrast, we aim at generic representations learning while being flexible to incor- porate optionally task-dependent losses to learn task-specific semantic-rich representations.

Section Title: CONCLUSION
  CONCLUSION We introduce a novel Random Distance Prediction (RDP) model which learns features in a fully unsupervised fashion by predicting data distances in a randomly projected space. The key insight is that random mapping is a theoretical proven approach to obtain approximately preserved distances, and to well predict these random distances, the representation learner is optimised to learn consistent preserved proximity information while at the same time rectifying inconsistent proximity, resulting in representations with optimised distance preserving. Our idea is justified by thorough experiments in two unsupervised tasks, anomaly detection and clustering, which show RDP-enabled anomaly detectors and clustering substantially outperform their counterparts on 19 real-world datasets. We plan to extend RDP to other types of data to broaden its application scenarios.
  Since we operate on real-valued vector space, the inner product is implemented by the dot product. The dot product is used hereafter to simplify the notation.

```
