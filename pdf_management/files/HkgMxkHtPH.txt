Title:
```
UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING
```
Abstract:
```
In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect) from in-air image and depth map pairs based on improved underwater imaging model. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and dehazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results obtained by the proposed model demonstrate well performance on open real-world underwater datasets, and the processing speed can reach up to 125FPS running on one NVIDIA 1060 GPU.
```

Figures/Tables Captions:
```
Figure 1. Synthetic underwater-style images through Eq. 2. (a) are in-air sample images, (b)-(d) are synthetic underwater-style sample images of different water types.
Figure 2: UnderwaterGAN architecture. UWGAN takes color image and its depth map as input, then it synthesizes underwater realistic images based on underwater optical imaging model by learning parameters through generative adversarial training.
Figure 3: Proposed U-net Architecture for underwater image restoration and enhancement.
Figure 4: Typical images of datasets. (a)-(b) are color images and depth maps of NYU-Depth datasets, (c) are sample images of RealA dataset, (d) are sample images of RealB dataset, (e) are sample images of RealC dataset.
Figure 5: Qualitative comparisons for samples from the real-world underwater image dataset RealC. (a)-(j) represent the samples selected from RealC.
Figure 6: Qualitative comparisons for samples from real-world underwater image dataset RealA and RealB. (a)-(j) represents the samples selected from RealA and RealB.
Figure 7: Underwater target detection results before and after enhancement. (A) Real-world underwater images and (B) output of our model for the real-world image. Red boxes represent scallops, blue boxes represent sea cucumbers, and green boxes represent sea urchins.
Table 1: Quantitative UIQM values of samples in Figure 5. The greater the UIQM values, the better the enhanced results, with blue representing the maximum and green representing the minimum.
Table 2: Quantitative UIQM values of samples in Figure 6. The greater the UIQM values, the better the enhanced results, with blue representing the maximum and green representing the minimum.
Table 3: Average quantitative UICM, UISM, UIConM and UIQM values on real-world underwater image datasets RealA, RealB and RealC. The greater the values, the better the enhanced results, with blue representing the maximum
Table 4: Quantitative results evaluation on synthetic dataset by full-reference metrics: MSE, PSNR, SSIM values. The smaller the MSE values, the greater the PSNR and SSIM values, the better the enhanced results, with blue representing the best results
Table 5: Testing time and parameters of generator of different enhancement methods
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION In recent years, underwater vision plays an important role in a lot of different applications. Therefore, underwater image processing has received extensive attention and research due to the poor underwater imaging environment and image quality. The main reason is the scattering and attenuation of light, the scattering results in haze effect, and the attenuation of light leads to color cast. So far many image enhancement algorithms have been proposed, such as white balance algorithm ( Liu Y C, 1995 ), gray world algorithm ( Rizzi A, 2002 ), histogram equalization ( Pizer S M, 1987 ) and fusion algorithm ( Ancuti C, 2012 ), however, these methods are not based on the underwater physical imaging model, so it is challenging and ineffective to apply these algorithms to different underwater scenes directly. Many underwater image enhancement algorithms based on imaging models have been proposed. For instance, He et al ( He K, 2010 ) proposed a dark channel prior (Dark channel prior, DCP) dehazing algorithm based on many experiments. Chiang et al ( Chiang J Y, 2011  ) apply DCP model on underwater image dehazing problem. These traditional methods are not intelligent, it is very time-consuming to calculate the characteristics of the image. In these years, the deep learning network developed rapidly, especially the convolutional neural network (CNN), which is used in image classification (Krizhevsky A, 2012), object detection ( Redmon J, 2016 ), and motion recognition (  Kuehne H, 2011 ), the performance is much better than traditional methods. However, the current research on underwater image enhancement using CNN is limited due to lack of underwater datasets. It is difficult to obtain images without water in real-world underwater scenes. Therefore, using synthetic underwater datasets is an important approach ( Anwar S, 2018 ;  Ancuti C, 2016 ;  Uplavikar P, 2019 ). Some model based on generative adversarial network (GAN ( Goodfellow I, 2014 )) are used to generating realistic underwater images. For instance, CycleGAN ( Zhu J Y, 2017 ) generates images through style transfer. WaterGAN ( Li J, 2017  ) takes in-air images, depth maps and noise vectors as input, followed by a camera model, then output synthetic images. Based on our experimental results, the image generated by WaterGAN suffers color noise and they differ a lot from real world underwater images. Therefore, to generate realistic underwater images with both color cast and haze effect, we improved the underwater imaging model, and proposed an unsupervised GAN based on this model to generate realistic underwater images from clear in-air images. Then, U-Net with different loss functions ( Ronneberger O, 2015 ) is trained to enhance underwater images through synthetic datasets. Finally, the performance of the proposed algorithm is validated on real underwater images as well as underwater target detection datasets for both low-level and high-level computer vision tasks. The experimental results show that the proposed method can recover the underwater image while maintaining structural similarities. Apart from this, the effects of different loss functions in U- net are compared, the most suitable loss function for underwater image restoration is suggested based on the comparison (This part can be found in APPENDIX), which provides a new idea for underwater image enhancement.

Section Title: OUR PROPOSED METHOD
  OUR PROPOSED METHOD To generate the realistic underwater images (color casts, low contrast and haze effect), we improved underwater imaging model, and proposed an underwater generative adversarial network (UWGAN), which takes in-air RGB-D images and a sample set of underwater images of a specific survey site as input to train a generative network adversarially. These synthetic underwater images, which were used to train a restoration network based on U-Net ( Ronneberger O, 2015 ) that can enhance underwater images in real-time.

Section Title: IMPROVED UNDERWATER IMAGING MODEL
  IMPROVED UNDERWATER IMAGING MODEL As is well known, a simplified underwater imaging model is shown in Eq.1. We can generate underwater-style images using the in-air image and its depth map by Eq. 1, which can well simulate color cast caused by light attenuation in water. However, it is difficult to simulate the haze effect caused by the scattering of water impurities. As shown in  Figure 4 , obvious haze effect can be observed on real underwater images. Inspired by related dehazing methods ( Ancuti C, 2016 ), we improved the second term in Eq. 1. The improved imaging model is shown in Eq. 2. ( ) is ambient light based on the light attenuation of different wavelength. is the scene scattering coefficient, which corresponds to the scattering coefficient in the atmospheric imaging model, and is set by default to 1, corresponding to a moderate and homogeneous haze effect. Three types of realistic underwater images were synthesized with color cast and haze effect are shown in  Figure 1 .

Section Title: UWGAN FOR GENERATING REALISTIC UNDERWATER IMAGES
  UWGAN FOR GENERATING REALISTIC UNDERWATER IMAGES Underwater-style images are generated based on Eq. 2, whose parameters are estimated through adversarial learning using GAN, as shown in  Figure 2 .

Section Title: UNDERWATER IMAGE RESTORATION BASED ON U-NET
  UNDERWATER IMAGE RESTORATION BASED ON U-NET U-Net is used for color restoration and haze removal of underwater images. A detailed description of U-Net architecture proposed in the paper is shown in  Figure 3 . Firstly, a degraded underwater RGB image is resized to 256x256 and then fed into the encoder part of U-net. In the encoder, the image is finally downsampled into a 32x32x256-dimensional latent vector through a series of convolution and max-pooling operations. In each downsampling stage, 3x3 convolution with a stride of 1 followed by a rectified linear unit (ReLU) activation function are conducted twice, then a 2x2 max pooling with a stride of 2 is used. The number of feature maps are doubled at each stage. In the decoder part, upsampling is done from the latent high dimensional vector back to the original input size sequentially. After each upsampling operation, output tensor is concatenated to the corresponding symmetric layer in the encoder side, then followed by two consecutive convolution layers and a rectified linear activation layer. The number of feature maps is gradually reduced to three channels.

Section Title: DATASET
  DATASET The in-air datasets we used are images of indoor scenes that has been labeled in the NYU Depth dataset V1 (Silberman N, 2011) and V2 ( Silberman N, 2012  ), which contain a total of 3733 RGB images and corresponding depth maps. The underwater dataset contains real-world underwater images collected from marine organisms' farms (including scallops, sea cucumbers, sea urchins, etc.), which can be roughly divided into two categories, one contains near-field green hued images (RealA), and the other contains blue-green hued images of far-field scenes (RealB). We also use underwater open datasets (  Li C, 2019 ) (RealC) as testing sets, where RealA contains 2069 underwater images, RealB contains 2173 underwater images, and RealC contains 890 underwater images. Several typical images of the datasets are shown in  Figure 4 .

Section Title: EXPERIMENTAL SETUP
  EXPERIMENTAL SETUP The training settings of our proposed method are presented in details in this section. Our models are trained in the computer with the following configurations: Intel i7 HQ 8700 processor, 16GB RAMï¼Œ NVIDIA TITAN X 12GB graphics card. Firstly, UWGAN is trained to synthesize underwater-style images using the NYU-Depth Dataset, RealA and RealB datasets. Our model was trained for 30 epochs, using Adam optimizer with a learning rate of 0.0001, and the momentum term was set to 0.5. The batch size was set to 64 with output images set to 256x256. Secondly, U-net is trained as an image enhancement network using synthetic pairs. The batch size was set to 32 and the output image size is 256x256. The learning rate is set to 0.0001 according to Adam optimizer, our model is trained for 200 epochs.

Section Title: RESULT AND DISCUSSION
  RESULT AND DISCUSSION In this section, we quantitatively and qualitatively compare our proposed method with several representative underwater image enhancement algorithms, including Unsupervised Color Correction Method (UCM) ( Iqbal K, 2010 ), Histogram equalization (HE) ( Hummel R, 1975 ), Multi-Scale Retinex with Color Restoration (MSRCR) ( Rahman Z, 1996 ), Fusion ( Ancuti C, 2012 ), Underwater Dark Channel Prior (UDCP) (  Drews P, 2013 ), Image Blurriness and Light Absorption (IBLA) ( Peng Y T, 2017 ), Underwater Color Correction using GAN (UGAN) ( Fabbri C, 2018 ), WaterGAN-color-correction (WaterGAN) ( Li J, 2017  ). We employ a non-reference metric, UIQM ( Panetta K, 2015 ), for the quantitative assessment of underwater image quality on RealA, RealB, and RealC datasets as no ground truth scenes are available as the reference for real-world underwater images. Besides, we employ three full-reference metrics, namely MSE, PSNR ( Hore A, 2010 ), SSIM, for assessment image quality on synthetic datasets. To reasonably assess the time spent on various algorithms, we resize all images to 256x256, which provides a stable output for enhancements in later experiments. Firstly, we compare the capabilities of different methods to improve the image visibility on the RealA, RealB, and RealC datasets. The qualitative comparison is shown in  Figure 5  and 6. Most methods can improve the quality of images of a slight haze effect. UCM, HE, and Fusion can enhance the brightness and contrast of the image, but are less uniform for color restoration and seem to be over-enhanced in some areas of the image. The results of MSRCR appear to have a suitable hue but lack sufficient saturation and contrast. UDCP and IBLA do not recover well for green-toned images, they make the image darker but enhance the contrast of the image. UGAN, WaterGAN can enhance the contrast of the image but they don't recover color well and generate some artifacts, which destroy the structural information of the image. The proposed method can recover the color of degraded underwater images while keeping a proper brightness and contrast.  Table 1  and  Table 2  quantitatively show the scores of sample images in  Figure 5  and  Figure 6  respectively. Our proposed method has achieved the highest scores in (a), (c) and (f). In addition, the average quantized scores evaluated on RealA, RealB, and RealC datasets are shown in  Table 3 . Our model achieves the best scores in terms of color restoration. UIQM is a non-reference assessment metric whose quantitative results depend largely on the value of scale factors. Structural information of images is not considered in these kinds of non-reference evaluation metrics. Although some enhanced images can get higher score, the visual quality is poor, the reason is that the metric is calculated from the pixels. Therefore, we also employ three full-reference assessment metrics MSE, PSNR, and SSIM to evaluate the performance of different methods on synthetic datasets without training. The comparison results in  Table 4  demonstrate that our proposed method achieves the best results in terms of MSE, PSNR, and SSIM. The average inference time of different algorithms are compared in one computer with following configuration: Intel i7-8750H CPU, 16GB RAM, and GTX1060 6G GPU. The results are shown in  Table 5 . Our model has the fastest processing speed compared to other methods. Moreover, the model we proposed has the fewest Params and FLOPs compared to other deep-learning-based methods. UGAN employs many convolution layers with 512 kernels, which causes that there are too many network parameters. WaterGAN employs multiple networks, resulting in slow processing speed. As indicated by some previous works ( Uplavikar P, 2019 ;  Anwar S, 2019 ; Ding X, 2019), the performance of high-level computer vision tasks (such as underwater target detection) on enhanced images is an indicator of image enhancement methods. We applied YOLO v3 ( Redmon J, 2018 ) target detector on degraded underwater images and their enhanced versions generated by our model. The performance of underwater target detection is better on enhanced versions on degraded images.  Figure 7  shows the results of YOLO v3 detector before and after processing the images with our model.

Section Title: CONCLUSION
  CONCLUSION Based on an improved underwater imaging model, a generative adversarial network (UWGAN) for generating realistic underwater images is proposed in this paper. Then, U-net with combined loss functions is used for degraded underwater images enhancement. Our model is validated on both low-level and high- level underwater computer vision tasks, which demonstrate its effectiveness and robustness.

```
