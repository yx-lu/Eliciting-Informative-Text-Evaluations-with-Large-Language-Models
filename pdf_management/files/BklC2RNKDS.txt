Title:
```
Under review as a conference paper at ICLR 2020 SCALABLE NEURAL LEARNING FOR VERIFIABLE CONSISTENCY WITH TEMPORAL SPECIFICATIONS
```
Abstract:
```
Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to misclassification under perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, par- ticularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.
```

Figures/Tables Captions:
```
Figure 1: MMNIST Image
Table 1: Comparison of methods developed for training for consistency with specifications.
Table 2: Comparison of GRU training methods on the MMNIST task. We evaluate against the termination specification on different metrics, and also report nominal accuracy. '-' indicates a trivial verified accuracy of 0% obtained with bound propagation. The entries with Verified Termination Accuracies corresponding to 0.0 are those where we were able to generate adversarial examples (counter-examples) to the specification for every point in the test-set. We found that adversarial training is difficult because of the presence of the sigmoid & tanh activation functions commonly used in GRUs. To have a meaningful baseline, we performed adversarial training on an RNN (feedforward cells with ReLU activation). For = 0.1, attacking the loss from Wang et al. (2019) to produce longer sequences performs better, while for the other values adversarial training with the STL quantitative loss performs better. Adversarial training performs well but is difficult to verify. At larger , verified training results in both better guarantees (specification conformance), and better nominal accuracies.
Table 3: We train the RNN with ReLU activations from (Wang et al., 2019) to be verifiable with = 0.3, and compare its verifiability with MILP based verification reported in Wang et al. (2019) at different perturbation radii. The nominal accuracy for the model trained to be verifiable is 93.9% and model trained in a standard manner is 96.4%. For larger perturbations, the MILP solver times out. '-' indicates that we were unable to certify robustness for any of the points in the test-set, for the given perturbation within the time-out window of 30 minutes.
Table 4: Mean/Variance performance (across 5 agents of each type) across different metrics. For each agent, reward is computed as mean across 100 episodes. is distance from the center of the grid cells, and for each we report the fraction of the cells for which we are able to certify that ϕ recharge holds.
Table 5: Language model perplexity, number of failures during an exhaustive enumerative search over the 25M perturbations, and computational cost of verification (number of forward passes).
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION While deep neural networks (DNNs) have shown immense progress on diverse tasks ( Sutskever et al., 2014 ;  Mnih et al., 2015 ;  Silver et al., 2016 ), they are often deployed without formal guarantees of their correctness and functionality. Their performance is typically evaluated using test data, or sometimes with adversarial evaluation ( Carlini & Wagner, 2017 ;  Uesato et al., 2018 ;  Ebrahimi et al., 2018 ;  Wang et al., 2019 ). However, such evaluation does not provide formal guarantees regarding the absence of rare but possibly catastrophic failures (Administration; Board;  Ross & Swetlitz, 2018 ). Researchers have therefore started investigating formal verification techniques for DNNs. Most of the focus in this direction has been restricted to feedforward networks and robustness to adversarial perturbations ( Tjeng et al., 2017 ;  Raghunathan et al., 2018b ;  Ko et al., 2019 ). However, many practically relevant systems involve DNNs that lead to sequential outputs (e.g., an RNN that generates captions for images, or the states of an RL agent). These sequential outputs can be interpreted as real-valued, discrete-time signals. For such signals, it is of interest to provide guarantees with respect to temporal specifications (e.g., absence of repetitions in a generated sequence, or that a generated sequence halts appropriately). Temporal logic provides a compact and intuitive formalism for capturing such properties that deal with temporal abstractions. Here, we focus on Signal Temporal Logic (STL) ( Donzé & Maler, 2010 ) as the specification language and exploit its quantitative semantics to integrate a verification procedure into training to provide guarantees with regard to temporal specifications. Our approach builds on recent work ( Mirman et al., 2018 ;  Gowal et al., 2018 ), which is based on propagating differentiable numerical bounds through DNNs, to include specifications that go beyond adversarial robustness. Additionally, we propose extensions to  Mirman et al. (2018) ;  Gowal et al. (2018)  that allow us to train auto-regressive GRUs/RNNs to certifiably satisfy temporal specifications. We focus on the problem of verified training for consistency rather than post-facto verification. To summarize, our contributions are as: • We present extensions to  Mirman et al. (2018) ;  Gowal et al. (2018)  that allow us to extend verified training to novel architectures and specifications, including complex temporal specifications. To handle the auto-regressive decoder often used in RNN-based systems, we leverage differentiable approximations of the non-differentiable operations. • We empirically demonstrate the applicability of our approach to ensure verifiable consistency with temporal specifications while maintaining the ability of neural networks to achieve high accuracy on the underlying tasks across domains. For supervised learning, verified training on the train-data enables us to provide similar verification guarantees for unseen test-data. • We show that verified training results in robust DNNs whose specification conformance is significantly easier to guarantee than those trained adversarially or with data augmentation.

Section Title: RELATED WORK
  RELATED WORK Here, we discuss the most closely related approaches covering AI safety and DNN verification.

Section Title: NEURAL NETWORK VERIFICATION
  NEURAL NETWORK VERIFICATION There has been considerable recent progress on developing techniques for neural network verification, starting with the pioneering work of  Katz et al. (2017) , where a satisfiability-modulo-theories (SMT) solver was developed to verify simple properties for piecewise linear deep neural networks. Subsequently, several mature solvers that rely on combinatorial search ( Tjeng et al., 2017 ;  Dutta et al., 2017 ;  Bunel et al., 2018 ) have helped scale these techniques to larger networks. More recently, verification of neural networks using incomplete over-approximations - using dual based approaches ( Dvijotham et al., 2018 ), and propagating bounds through the neural network ( Gehr et al., 2018 ;  Wang et al., 2018c ;  Weng et al., 2018a ;  Singh et al., 2019 ) - has emerged as a more scalable alternative for neural network verification.  Raghunathan et al. (2018a) ;  Wong et al. (2018)  showed that folding the verification procedure into the training loop (called verified training) enables us to obtain stronger guarantees. Building on this line of work,  Gowal et al. (2018)  showed that training with simple interval bounds, with carefully chosen heuristics, is an effective approach towards training for verifiability. However, the focus of the above mentioned works on verified training is limited to adversarial robustness properties, and feedforward networks with monotonic activation functions. In this work, we build on  Mirman et al. (2018) ;  Gowal et al. (2018)  to consider richer specifications that capture desired temporal behavior, and novel architectures with non-differentiable components.  Table 1  compares different methods developed for verified training. In independent and concurrent work,  Jia et al. (2019)  develop an approach for verifying LSTMs, CNNs and networks with attention-mechanism. They use a similar approach as developed in our paper to compute bounds through the softmax function, word-substitutions, and also extend bound- propagation to handle the gating mechanism. Their main focus is robustness to misclassification. In contrast, we consider complex temporal specifications, and auto-regressive architectures.

Section Title: SATISFACTION OF TEMPORAL PROPERTIES
  SATISFACTION OF TEMPORAL PROPERTIES

Section Title: Temporal Specifications
  Temporal Specifications While training networks to satisfy temporal logic specifications has been considered before, it has largely been from the perspective of encouraging RL agents to do so through a modified reward ( Icarte et al., 2018b ;  Aksaray et al., 2016 ;  Hasanbeig et al., 2018 ;  Icarte et al., 2018a ;  Sadigh et al., 2014 ;  Wen et al., 2017 ;  Li et al., 2017a ). The temporal logic specification is used to express the task to be performed by the agent, rather than as a verifiable property of the system.  Ghosh et al. (2018)  encourage specification conformance during training by regularizing with a loss arising from the desired specification. However, the specification is enforced on specific inputs and does not guarantee that the property holds across continuous regions of the input space (e.g., all Under review as a conference paper at ICLR 2020 inputs in the neighborhood of a given image). In contrast, we train DNNs to verifiably satisfy rich temporal specifications over large/continuous sets of inputs. Further, we note that there is work on falsifying STL specifications using stochastic optimization( Annpureddy et al., 2011 ;  Donzé, 2010 ), however our focus here is on verified training. Safe RL Safe exploration methods ( Garcıa & Fernández, 2015 ) consider explorations that do not visit unsafe states. Temporal logics, in general, permit richer specifications of desired temporal behaviors than avoiding unsafe states.  Junges et al. (2016)  synthesize a scheduler that limits the agent explorations to safe regions of the environment specified using probabilistic computation tree logic (PCTL). An alternative mechanism, called shields, monitors actions of an agent and restricts it to a safe subset ensuring conformance to specifications in linear temporal logic ( Alshiekh et al., 2018 ) or PCTL ( Jansen et al., 2018 ). Instead of using external mechanisms to restrict the agent choices, we incorporate temporal logic objectives into training and achieve verified training of our agents. Furthermore, our work is not restricted to training verifiable RL agents. We demonstrate the generality of our approach on image captioning and language generation tasks involving RNNs. Verification using Interpretable Policies PIRL ( Verma et al., 2018 ) and VIPER ( Bastani et al., 2018 ) extract interpretable policies that are also amenable to verification. These approaches first learn DNN agents and then use imitation learning to extract interpretable/verifiable policies.  Wang et al. (2018a)  analyze RNNs by distillation to a Deterministic Finite Automaton (DFA), and demonstrate successful distillation on recognizing Tommika Grammar. However, this distillation is often difficult, and remains an open challenge for vision/language processing tasks. Further, there are no guarantees about the original DNN. In contrast, we focus on guarantees for the DNN itself.

Section Title: FORMULATING TEMPORAL CONSTRAINTS ON ML MODELS WITH SIGNAL TEMPORAL LOGIC
  FORMULATING TEMPORAL CONSTRAINTS ON ML MODELS WITH SIGNAL TEMPORAL LOGIC We consider the problem of verified training of a model with respect to a desired property. In what follows, we describe how signal temporal logic provides a formalism to describe properties of interest.

Section Title: PRELIMINARIES
  PRELIMINARIES We use Signal Temporal Logic (STL) ( Donzé & Maler, 2010 ), an extension of Linear Temporal Logic (LTL) ( Pnueli, 1977 ) that can reason about real-valued signals. Syntax and Qualitative Semantics STL has the following syntax: where true is the Boolean constant for truth, and ¬ and ∧ are Boolean negation and conjunction operators. The symbol q is a quantifier-free non-linear real-valued arithmetic function over the vector-valued state denoted by s; the formula q(s) ≥ 0 is called an atom. The formula ϕ 1 U I ϕ 2 is the until temporal operator, meaning ϕ 1 holds until ϕ 2 holds in the interval I. We define ♦ I ϕ (meaning ϕ eventually holds in the interval I) as true U I ϕ and I ϕ (meaning ϕ always holds in I) as ¬♦ I ¬ϕ. In this work, we interpret STL formulae over a trace σ which is a discrete-time, vector-valued signal; σ t denotes the value of the signal at time t. We write (σ, t) |= ϕ to indicate that ϕ holds for σ at time t. An atom q(s) ≥ 0 holds at time t if q(σ t ) ≥ 0. Trivially, (σ, t) |= true always holds. An until formula ϕ 1 U I ϕ 2 , with I = [a, b], holds at a time instance t if (σ, t ) |= ϕ 2 for some time t ∈ [t + a, t + b] and (σ, t ) |= ϕ 1 for all t ∈ [t, t ]. In the rest of the paper, we use t + I to denote [t + a, t + b] for I = [a, b]. A formula ϕ is said to be satisfied over a trace σ if (σ, 0) |= ϕ. In this work, we restrict I to be bounded-intervals of time. We refer the reader to  Donzé & Maler (2010)  for a detailed introduction to the semantics of STL specifications. While bounded-time STL properties can be unrolled through time into logical properties using the Boolean conjunction and disjunction operators ( Raman et al., 2015 ), STL provides a succinct and intuitive notation for expressing desired temporal properties. In contrast with prior work on verified training that only considers adversarial robustness (a linear constraint on the logits), we consider general specifications that assert temporal properties over input-output behaviors of neural networks. Section 3.2 lists several examples of relevant properties that can be expressed in bounded-time STL.

Section Title: STL SPECIFICATIONS FOR LEARNING TASKS
  STL SPECIFICATIONS FOR LEARNING TASKS To illustrate our approach, we consider three temporal properties that we want our networks to satisfy. Multi-MNIST images consist of non-overlapping MNIST digits on a canvas of fixed size ( Figure 1 ). The number of digits in each image varies between 1 and 3. The task is to label the sequence of digits in the image, followed by an end of sequence token. Prior work on this task ( Wang et al., 2019 ) has shown image-to-sequence models to be vulnerable to generating sequences longer than the true number of digits in the image, under small adversarially chosen perturbations. Here, we consider the task of training a DNN that does not output sequences longer than the desired length, while achieving similar nominal task performance. Let y := f (x) be the sequence of logits output by the RNN model when given input image x. For an image x, the termination specification is formalized as follows: x is the true number of digits in the image x, e is the label corresponding to the end of sequence token, > 0 is the perturbation bound. Informally, this specification enforces that the end of sequence token is output no later than after the true number of digits have been output by the RNN, for all inputs within distance from a true-image.

Section Title: VERIFYING THAT A ROBOT NEVER RUNS OUT OF CHARGE
  VERIFYING THAT A ROBOT NEVER RUNS OUT OF CHARGE To demonstrate our approach in the RL setting, we consider a task with a vacuum cleaning robot. We summarize this task here (See Appendix F for more details). The agent (robot) operates in a continuous domain with its location in (x, y) ∈ [0, 25] 2 (Figure 2, Appendix). The room is divided into discrete cells, and the agent gets a reward for visiting any "dirty" cell which has not been visited in the previous T dirt time-steps. The agent must visit one of the recharge cells every T recharge time-steps, or the episode is terminated with no further reward. The policy maps observations (of the agent location and a map of the room) to continuous velocity controls. We use f θ to denote the result of applying the policy, parameterized by θ, followed by the environment update. For this agent, we want to verify the specification: ∀z ∈ S .(f θ (z), 0) |= [0,T ] ♦ [0,T recharge ] ϕ recharge , where ϕ recharge corresponds to the agent being in one of the recharge cells. This specification ensures that, for a set of feasible starting positions S , for every time-step t in [0, T ], the agent recharges itself at least once within T recharge time-steps. See Appendix F.1 for a detailed description of S .

Section Title: VERIFYING GENERATED OUTPUTS FROM A LANGUAGE MODEL
  VERIFYING GENERATED OUTPUTS FROM A LANGUAGE MODEL A common failure mode for language models is their tendency to fall into degenerate loops, often repeating a stop-word ( Wang et al., 2019 ). To illustrate the applicability of STL specifications in this setting, we show how to formalize the property that a GRU language model does not repeat words consecutively. We call this specification bigram non-repetition. More concretely, the desired specification is that the output sequence does not contain bigram repetition amongst the 100 most frequent tokens in the training corpus vocabulary. We want to verify this property over a large set of possible conditioning inputs for the generative model. Concretely, we define an input set S of roughly 25 million prefixes generated from a syntactic template (See Appendix G.1 for details). These prefixes are input to the LM, and then we evaluate the specification on the model output. Now, consider a prefix x and the sequence of logits y output by the recurrent GRU network f (i.e. y = f (x)), with y(t) k referring to the logit corresponding to the k th most-frequent token in the vocabulary at time t. A compact formal specification ϕ bigram ruling out bigram repetition is: where T sample denotes the length of the generated sample, in our case 10. The RNN f is required to satisfy the specification ∀x ∈ S.(f (x), 0) |= ϕ bigram .

Section Title: VERIFIABLE DNN TRAINING FOR STL SPECIFICATIONS
  VERIFIABLE DNN TRAINING FOR STL SPECIFICATIONS We consider the problem of learning a trace-valued function f θ to verifiably satisfy a specification of the form ∀x ∈ S. (f θ (x), 0) |= ϕ, where input x ranges over set S, and f θ (x) is the trace generated by f θ when evaluated on x, θ represents the trainable parameters, and ϕ is an STL specification. We drop θ for brevity, and simply denote f θ (x) as f (x). Formally, our problem statement is: Given a set of inputs S, train the parameters θ of f θ so that ∀x ∈ S. (f θ (x), 0) |= ϕ, where ϕ is a bounded-time STL specification.

Section Title: OPTIMIZATION FORMULATION OF STL VERIFICATION
  OPTIMIZATION FORMULATION OF STL VERIFICATION For an STL specification ϕ, its quantitative semantics can be used to construct a function ρ(ϕ, f (x), t) whose scalar valued output is such that ρ(ϕ, f (x), t) ≥ 0 ⇐⇒ (f (x), t) |= ϕ ( Donzé & Maler, 2010 ). In terms of the quantitative semantics, the verification problem is equivalent to showing that ∀x ∈ S. ρ(ϕ, f (x), 0) ≥ 0. This verification task can be written as the optimization problem of finding the sequence of inputs x such that the sequence of outputs f (x) result in the strongest violation of the specification with regard to the quantitative semantics: If the solution to equation 4 is negative, then there exists an input leading to the violation of ϕ.

Section Title: BOUND PROPAGATION
  BOUND PROPAGATION The optimization problem in equation 4 itself is often intractable; even in the case when the specifica- tion is limited to robustness against perturbations in a classification task, it is NP-hard ( Katz et al., 2017 ). There are tractable approaches to bounding the problem in equation 4 ( Raghunathan et al., 2018a ;  Dvijotham et al., 2018 ), but the bounds are often too loose to provide meaningful guarantees. To obtain a tighter bound tractably, interval bound propagation - which by itself provides loose bounds, but is efficient to compute (2x computational cost) - can be leveraged for verified training to give meaningful bounds on robustness under l ∞ perturbations ( Mirman et al., 2018 ;  Gowal et al., 2018 ). Our general approach for doing bound propagation on the function f is to use standard interval arithmetic. While this is straightforward when f is a feedforward DNN ( Gowal et al., 2018 ), here we extend bound propagation to a richer set of (temporal) specifications and architectures. First, we highlight the novel aspects of bound propagation required for (a) auto-regressive RNNs/GRUs, (b) STL specifications. Bound Propagation through GRUs Computing bounds across GRU cells involves propagating bounds through a multiplication operation (as a part of gating mechanisms), which can be handled by a straightforward application of interval arithmetic ( Hickey et al., 2001 ) (see Appendix H).

Section Title: Bound Propagation through auto-regressive RNNs
  Bound Propagation through auto-regressive RNNs For language modeling and image caption- ing, we use GRU decoders with greedy decoding. Greedy-decoding involves a composition of the one-hot and the argmax operations. Both of these operations are non-differentiable. To overcome this and compute differentiable bounds (during training), we approximate this composition with a softmax operator (with a low temperature T ). In the limit, as T → 0, the softmax operator converges to the composition one-hot(argmax(·)) For propagating bounds through the softmax operator, we leverage that the bounds are monotonic in each of the individual inputs. Formally, given a lower (p) and upper (p) bound on the input p to a softmax layer (i.e., p ≤ p ≤ p), the lower w and upper bound(w) on the output can be computed as: where p i is the i th coordinate of p and p ∈ R N . During evaluation, the one-hot(argmax(.)) function is used as is. Given bounds on each coordinate of p (i.e., p ≤ p ≤ p) and s = Under review as a conference paper at ICLR 2020 one-hot(argmax(p)), bounds on coordinate s i can be computed as: We discuss bound propagation for discrete inputs in Appendix C Bound Propagation through the specification First, we extend the quantiative semantics for STL specifications ( Donzé & Maler, 2010 ) to allow us to reason over sets of inputs. For a STL specification ϕ in negation normal form (NNF) (See Appendix A for details on the quantitative semantics and conversion to NNF), we first define a lower bound for the quantitative semantics of ϕ over the set S, which we denote by ω S,f (ϕ, 0). We define this bound assuming we have lower bounds on all the atoms occurring in ϕ. Specifically, let Ω S,f (q, t) be a lower bound on q(f (x) t ) over all inputs x ∈ S; in other words, at each time t we have ∀x ∈ S. Ω S,f (q, t) ≤ q(f (x) t ). Now, we define the lower bound on a specification ϕ inductively as: Lemma 1. For any time t, given lower bounds Ω S,f (q, t) on all the atoms q(s) ≥ 0 in ϕ, we have: See Appendix B for proof of Lemma 1. In order to compute the lower bounds Ω S,f (q, t) required for Lemma 1, given bounds on the input x, we can first compute bounds on the outputs f (x) t at each time t. For the atoms q(s) ≥ 0 appearing in ϕ, given bounds on the input s we can compute bounds on q(s). These bounds can then be propagated through the specification inductively.

Section Title: VERIFIED TRAINING FOR STL SPECIFICATIONS
  VERIFIED TRAINING FOR STL SPECIFICATIONS In this section, we describe how to train a network to satisfy an STL specification ϕ. The quantitative semantics ρ(ϕ, σ, 0) gives a degree to which σ satisfies ϕ. First, we compute lower bounds on the values of the atoms in ϕ at each instance of time. Then, by application of Lemma 1, we can compute the lower bound ω S,f (ϕ, 0) satisfying ∀x ∈ S. ω S,f (ϕ, 0) ≤ ρ (ϕ, f (x), 0). Subsequently we optimize the lower bound ω S,f (ϕ, 0) to be non-negative, thereby guaranteeing that the specification of interest holds: ∀x ∈ S. ρ (ϕ, f (x), 0) ≥ 0. Let L obj be the loss objective corresponding to the base task, for example, the cross-entropy loss for classification tasks. Training thus requires balancing two objectives: minimizing loss on the base task by optimizing L obj (f θ ), and ensuring the positivity of ω S,f θ . We can use gradient descent to directly optimize the joint loss: L obj (f θ ) − λ min{ω S,f θ (ϕ, 0), τ , where λ is a scalar hyper-parameter, τ is a positive scalar threshold (τ ∈ R + ). The clipping avoids having to carefully balance the two losses. The quantitative semantics of an STL specification ϕ is a non-smooth function of the weights of the neural network, and is difficult to optimize directly with gradient descent. We find in practice that curriculum training, similar to  Gowal et al. (2018) , works best for optimizing the specification loss, starting with enforcing the specification over a subset S ⊂ S, and gradually covering the entire S. Empirically, the curriculum approach means that the task performance (L obj ) does not degrade much.

Section Title: EXPERIMENTAL RESULTS
  EXPERIMENTAL RESULTS

Section Title: SEQUENTIAL CAPTIONING OF MULTI-MNIST IMAGES
  SEQUENTIAL CAPTIONING OF MULTI-MNIST IMAGES For this task, we perform verified training to enforce the termination specification ϕ x (equation 4) on the training data as discussed in Section 4.3. Post training, for unseen test-set images, we evaluate the quantitative specification loss ω Sx, ,f (ϕ x , 0). For an image x from the test-set, if ω Sx, ,f (ϕ x , 0) is positive, it is guaranteed that there is no input within an l ∞ radius of around the current image that can cause the RNN to generate a longer sequence than the number of true digits in the image. In  Tables 2  and 3, veri- fied termination accuracy refers to the fraction of un- seen data for which we can verify the absence of counter-examples to the termination property (equation 4). Nominal ac- curacy refers to the per- centage of correctly pre- dicted tokens - including the end of sequence token.  Table 2  compares verified training with nominal and adversarial training. Veri- fied training outperforms both adversarial and nominal training on both adversarial and verified termination accuracy metrics. The pixel values are scaled to be in the range [0, 1]. At perturbations of size = 0.5, the images can be turned gray; however, the DNN remains robust to such large perturbations by predicting that the image has no more than a single digit at large perturbations, while maintaining nominal accuracy on clean data. This in contrast with robustness against misclassification, where it is not possible to be robust at large perturbations because the specifications for images from different classes conflict. Adversarial accuracy is evaluated with the iterative attack from  Wang et al. (2019)  (10000 steps).

Section Title: Run-time Considerations
  Run-time Considerations As another baseline, we compare with verified termination accuracies from  Wang et al. (2019) ( Table 3 ). In  Wang et al. (2019) , the greedy-decoding and the specification are turned into a MILP-query solved with the SCIP solver ( Gleixner et al., 2018 ). Further, we use ReLU RNNs here because GRUs are not amenable to MILP solvers. Verified training allows us to certify specification conformance for much larger perturbations (≈ 2 orders of magnitude larger).

Section Title: AN RL MOBILE-ROBOT AGENT
  AN RL MOBILE-ROBOT AGENT We consider the recharging specification ϕ recharge over a time-horizon of T = 10, for an agent starting within a l ∞ distance of from the center of the any of the cells (See Appendix F.1 for Under review as a conference paper at ICLR 2020 details on feasible initial states). To regularize the DNN to be verifiable with regard to ϕ recharge , the specification loss is obtained by rolling out the current policy through time, and propagating bounds through the rolled out policy and the dynamics. This assumes a deterministic dynamics model. We compare our verifiably trained agent to both a vanilla RL agent, and an agent trained with reward shaping as in  Li et al. (2017b) . All agents achieve a similar reward, and we do not find specification violations for roll-outs from 10 6 random (feasible) initial states. To compare verifiability, we discretize a region within a distance of to each cell-center into 10 2 l ∞ balls, and verify with bound-propagation that the agent satisfies ϕ recharge for each sub-region. Agents trained with verified training are significantly more verifiable than agents trained otherwise, with little degradation in performance ( Table 4 ), which is consistent with prior work in classification ( Wong & Kolter, 2018 ) Our language model consists of a 2-layer GRU with 64 hidden nodes per layer, trained on the tiny Shakespeare corpus using a word embed- ding dimension of 32, and vocabulary truncated to the 2500 most frequent training words. We evaluate the model's ability to satisfy ϕ bigram . We compare both a nominal model trained using log-likelihood, a model that randomly samples prefixes from the input space and penalizes vio- lations to the specification, and verified training that covers the full input space (Details in Appendix G). We report test set perplexity and count of violations observed over the 25M prefixes ( Table 5 ). We find that while standard training achieves the best perplexity results, it also produces numerous specification failures. Sampling prefixes and regularizing them to avoid bigram repetition using ρ(ϕ bigram , f (x), 0) eliminates failures, but the overall evaluation cost of the exhaustive search is large. Verifiable training with bound propagation, by contrast, comes with a constant computational cost of ≈ 2 forward passes. This is because matrix multiplications form a significant majority of the computational cost during a forward pass, and propagating bounds through a layer of the form y = σ(W x + b), where σ is a monotonic activation function (e.g. ReLU, sigmoid, tanh), can be performed such that it only costs twice as much as a normal forward pass ( Gowal et al., 2018 ).

Section Title: Run-time Considerations
  Run-time Considerations Verification with propagating bounds can be performed in under 0.4 seconds (including propagating bounds through the spec), while exhaustive search over 25M prefixes for specification violations takes over 50 minutes. Further, as possible word substitutions increase, the cost for exhaustive search grows exponentially while that for bound propagation stays constant.

Section Title: CONCLUSION
  CONCLUSION Temporal properties are commonly desired from DNNs in settings where the outputs have a sequential nature. We extend verified training to tasks that require temporal properties to be satisfied, and to architectures such as auto-regressive RNNs whose outputs have a sequential nature. Our experiments suggest that verified training leads to DNNs that are more verifiable, and often with fewer failures. Future work includes extending verification/verified training to unbounded temporal properties. Another important direction is to develop better bound propagation techniques that can be leveraged for verified training. In the RL setting, an important direction is data-driven verification in the absence of a known model of the environment. Under review as a conference paper at ICLR 2020

```
