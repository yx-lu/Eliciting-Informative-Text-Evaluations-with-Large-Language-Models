Title:
```
TIONS VIA COMMUNICATION MINIMIZATION
```
Abstract:
```
Reinforcement learning encounters major challenges in multi-agent settings, such as scalability and non-stationarity. Recently, value function factorization learning emerges as a promising way to address these challenges in collaborative multi- agent systems. However, existing methods have been focusing on learning fully decentralized value functions, which are not efficient for tasks requiring com- munication. To address this limitation, this paper presents a novel framework for learning nearly decomposable Q-functions (NDQ) via communication mini- mization, with which agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function factorization learning and communication learning by introducing two information-theoretic regularizers. These regularizers are maxi- mizing mutual information between agents' action selection and communication messages while minimizing the entropy of messages between agents. We show how to optimize these regularizers in a way that is easily integrated with existing value function factorization methods such as QMIX. Finally, we demonstrate that, on the StarCraft unit micromanagement benchmark, our framework significantly outperforms baseline methods and allows us to cut off more than 80% of commu- nication without sacrificing the performance. The videos of our experiments are available at https://sites.google.com/view/ndq.
```

Figures/Tables Captions:
```
Figure 1: Schematics of our approach. The message encoder generates an embedding distribution that is sampled and concatenated with the current local history to serve as an input to the local action-value function. Local action values are fed into a mixing network to to get an estimation of the global action value.
Figure 2: (a) Task sensor; (b) Performance comparison on sensor; (c) Performance comparison when different percentages of messages are dropped. We measure the drop rate of our method in two ways: count by the number of messages (NDQ) or count by the number of bits (NDQ (bits)). QMIX (5M) is the performance of QMIX after training for 5 million time steps.
Figure 3: Message distributions learned by our method on sensor under different values of β. (Mes- sages are cut by bit, if µ < 2.0). A mean of 0 means that the corresponding bit is below the cutting threshold and is not sent. When β = 10 −3 , NDQ learns the minimized communication strategy that is effective.
Figure 4: Results on hallway. (a, b) Task hallway and performance comparison. (c) Similar to Fig. 2(c), we show performance comparison when different percentages of messages are dropped.
Figure 5: Message embedding representations learned by our method on hallway. A mean of 0 means that the corresponding bit is below the cutting threshold (µ=3) and is not sent.
Figure 6: Snapshots of the StarCraft II scenarios that we consider.
Figure 7: Learning curves of our method and baselines when no message is cut for NDQ and QMIX+TarMAC.
Figure 8: Performance of our method and QMIX+TarMAC when 80% of messages are cut off. We also plot the learning curves of QMIX for comparison.
Figure 9: Performance of our method and QMIX+TarMAC when 100% messages are cut off. We also plot the learning curves of QMIX for comparison.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Cooperative multi-agent reinforcement learning (MARL) are finding applications in many real- world domains, such as autonomous vehicle teams (Cao et al., 2012), intelligent warehouse sys- tems (Nowé et al., 2012), and sensor networks (Zhang & Lesser, 2011). To help address these problems, recent years have made a great progress in MARL methods (Lowe et al., 2017; Foer- ster et al., 2018; Rashid et al., 2018; Jaques et al., 2019). Among these successes, the paradigm of centralized training with decentralized execution has attracted much attention for its scalability and ability to deal with non-stationarity. Value function decomposition methods provide a promising way to exploit such paradigm. They learn a decentralized Q function for each agent and use a mixing network to combine these local Q values into a global action value. In previous works, VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), and QTRAN (Son et al., 2019) have progressively enlarged the family of functions that can be represented by the mixing network. Despite their increasing ability in terms of value factorization representation, existing methods have been focusing on learning full decomposition, where each agent acts upon its local observations. However, many multi-agent tasks in the real world are not fully decomposable - agents sometimes require information from other agents in order to effectively coordinate their behaviors. This is because partial observability and stochasticity in a multi-agent environment can exacerbate an agent's uncertainty of other agents' states and actions during decentralized execution, which may result in catastrophic miscoordination.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 To address this limitation, this paper presents a scalable multi-agent learning framework for learning nearly decomposable Q-functions (NDQ) via communication minimization, with which agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function factorization learning and communication learning by introducing an information-theoretic regularizer for maximizing mutual information between agents' action selection and communication messages. Messages are parameterized in a stochastic embedding space. To optimize communication, we introduce an additional information- theoretic regularizer to minimize the entropy of messages between agents. With these two regular- izers, our framework implicitly learn when, what, and with whom to communicate and also ensure communication to be both expressive (i.e., effectively reducing the uncertainty of agents' action- value functions) and succinct (i.e., only sending useful and necessary information). To optimize these regularizers, we derive a variational lower bound objective, which is easily integrated with existing value function factorization methods such as QMIX. We demonstrate the effectiveness of our learning framework on StarCraft II unit micromanage- ment benchmark used in Foerster et al. (2017; 2018); Rashid et al. (2018); Samvelyan et al. (2019). Empirical results show that NDQ significantly outperforms baseline methods and allows to cut off more than 80% communication without sacrificing the performance. We also observe that agents can effectively learn to coordinate their actions at the cost of sending one or two bits of messages even in complex StarCraft II tasks.

Section Title: BACKGROUND
  BACKGROUND In our work, we consider a fully cooperative multi-agent task that can be modelled by a Dec- POMDP (Oliehoek et al., 2016) G = I, S, A, P, R, Ω, O, n, γ , where I ≡ {1, 2, ..., n} is the finite set of agents. s ∈ S is the true state of the environment from which each agent i draws an individual partial observation o i ∈ Ω according to the observation function O(s, i). Each agent has an action-observation history τ i ∈ T ≡ (Ω × A) * . At each timestep, each agent i selects an action a i ∈ A, forming a joint action a ∈ A n , resulting in a shared reward r = R(s, a) for each agent and the next state s according to the transition function P (s |s, a). The joint policy π induces a joint action-value function: Q π tot (τ , a) = E s0:∞,a0:∞ [ ∞ t=0 γ t r t |s 0 =s, a 0 =a, π], where τ is the joint action-observation history and γ ∈ [0, 1) is the discount factor. Learning the optimal action-value function encounters challenges in multi-agent settings. On the one hand, to properly coordinate actions of agents, learning a centralized action-value function Q tot seems a good choice. However, such a function is difficult to learn when the number of agents is large. On the other hand, directly learning decentralized action-value function Q i for each agent alleviates the scalability problem (Tan, 1993; Tampuu et al., 2017). Nevertheless, such independent learning method largely neglects interactions among agents, which often results in miscoordination and inferior performance. In between, value function factorization method provides a promising way to attenuate such dilemma by representing Q tot as a mixing of decentralized Q i conditioned on local information. Such method has shown their effectiveness on complex task (Samvelyan et al., 2019). However, current value function factorization methods have been mainly focusing on full decompo- sition. Such decomposition reduces the complexity of learning Q tot by first learning independent Q i and putting the burden of coordinating actions on the mixing networks whose input is all Q i 's and output is Q tot . For many tasks with partial observability and stochastic dynamics, mixing networks are not sufficient to learn coordinated actions, regardless of how powerful its representation ability is. The reason is that full decomposition cuts off all dependencies among decentralized action-value functions and agents will be uncertain about states and actions of other agents. Such uncertainty will increase as time goes by and can result in severe miscoordination and arbitrarily worse performance during decentralized execution. In this section, we propose to learn nearly decomposable Q-functions (NDQ) via communication minimization, a new framework to overcome the miscoordination issue of full factorization methods. In our learning framework ( Fig. 1 ), individual action-value functions condition on local action- observation history and, at certain timesteps, messages received from a few other agents. Messages from agent i to agent j are drawn from a multivariate Gaussian distribution whose parameters are given by an encoder f m (τ i , j; θ c ), where τ i is the local observation-action history of agent i, and θ c are parameters of the encoder f m . Formally, message m ij ∼ N (f m (τ i , j; θ c ), I), where I is an identity matrix. Here we use an identity covariance matrix and the reasons will be discussed in the next section. m (-i)j is used to denote the messages sent to j from agents other than i. We learn a nearly decomposable structure via learning minimized communication. We thus expect the communication to have the following properties: i) Expressiveness: The message passed to one agent should effectively reduce the uncertainty in its action-value function. ii) Succinctness: Agents are expected to send messages as short as possible to the agents who need it and only when necessary. To learn such a communicating strategy, we draw inspiration from variational inference for its proven ability in learning structure from data and endow a stochastic latent message space, which we also refer to as "message embedding". We impose constraints, which will be discussed in detail in the next section, on the latent message embedding to enable an agent to decide locally which bits in a message should be sent according to their utility in terms of helping other agents make decisions. Agent j will receive an input message m in j that has been selectively cut, on which it conditions the local action-value function Q j (τ j , a j , m in j ). All the individual Q values are then fed into a mixing network such as that used by QMIX (Rashid et al., 2018). Apart from the constraints on the message embedding, all the components (the individual action- value functions, the message encoder, and the mixing network) are trained in an end-to-end manner by minimizing the TD loss. Thus, our overall objective is to minimize L(θ) = L T D (θ) + λL c (θ c ), (1) where L T D (θ) = [r + γ max a Q tot (τ , a ; θ − ) − Q tot (τ , a; θ)] 2 (θ − are the parameters of a periodically updated target network as in DQN) is the TD loss, θ are all parameters in the model, Published as a conference paper at ICLR 2020 and λ is a weighting term. We will discuss how to define and optimize L c (θ c ) to regularize the message embedding in the next section.

Section Title: MINIMIZED COMMUNICATION OBJECTIVE AND VARIATIONAL BOUND
  MINIMIZED COMMUNICATION OBJECTIVE AND VARIATIONAL BOUND Introducing latent variables facilitates the representation of the message, but it does not mean that the messages can reduce uncertainty in the action-value functions of other agents. To make mes- sage expressive, we maximize the mutual information between message and agent's action selection. Formally, we maximize I θc (A j ; M ij |T j , M (-i)j ) where A j is agent j's action selection, T j is the random variable of the local action-observation history of agent j, M ij and M (-i)j are random vari- ables of m ij and m (-i)j . However, if this is the only objective, the encoder can easily learn to cheat by giving messages under different histories representations in different regions in the latent space, rendering cutting off useless messages difficult. A natural constraint to avoid such representations is to minimize the entropy of the messages. Therefore, our objective for optimizing communication of agent i is to maximize: J c (θ c ) = n j=1 I θc (A j ; M ij |T j , M (-i)j ) − βH θc (M ij ) , (2) where β is a scaling factor trading expressiveness and succinctness. This objective is appealing because it agrees exactly with the desiderata that we impose on the message embedding. However, optimizing this objective needs extra efforts because computation involving mutual information is intractable. By introducing a variational approximator, a popular technique from variational toolkit (Alemi et al., 2017), we can derive a lower bound for the mutual information term in Eq. 2 (a detailed derivation can be found in Appendix A): I θc (A j ; M ij |T j , M (-i)j ) ≥ E T∼D,M in j ∼fm(T,j;θc) −CE p(A j |T) q ξ (A j |T j , M in j ) , (3) where T = T 1 , T 2 , . . . , T n is the joint local history sampled from the replay buffer D, q ξ (A j |T j , M in j ) is the variational posterior estimator with parameters ξ, and CE is the cross en- tropy operator. We share ξ among agents to accelerate learning. Next we discuss how to minimize the term H θc (M ij ). Directly minimizing this can cause the variances of the Gaussian distributions to collapse to 0. To deal with this numeric issue, we use the unit covariance matrix and try to minimize H(M ij ) − H(M ij |T i ) instead. This is equivalent to minimizing H(M ij ) because H(M ij |T i ) is the entropy of a multivariate Gaussian random variable and thus is a constant log(det(2πeΣ))/2, where Σ is a unit matrix in our formulation). Then we have: We use a similar technique as for the mutual information term by introducing an distribution r(m ij ) to get a upper bound of Eq. 4: This bound holds for any distribution r(M ij ). To facilitate cutting off messages, we use unit Gaus- sian distribution N (0, I). Combining Eq. 3 and 5, we get a tractable variational lower bound of our objective in Eq. 2: We optimize this bound to generate an expressive and succinct message embedding. Specifically, we minimize: Published as a conference paper at ICLR 2020 Intuitively, the first term, which we call the expressiveness loss, ensures that communication aims to reduce the uncertainty in action-value functions of other agents. The second term, called the succinctness loss, forces messages to get close to the unit Gaussian distribution. Since we set the covariances of the latent message variables to the unit matrix, this term actually pushes the means of the message distributions to the origin of the latent space. Using these two losses leads to an embedding space where useless messages distribute near the origin, while messages that contain important information for the decision-making processes of other agents occupy other spaces. Note that the loss shown in Eq. 7 is used to update the parameters in the message encoder. In the meantime, all components (the individual action-value functions, the message encoder, and the mixing network) are trained in an end-to-end manner. Thus, the message encoder is updated by two gradients: the gradient induced by L c (θ c ) and the gradient associated with the TD loss L T D (θ).

Section Title: CUTTING OFF MESSAGES
  CUTTING OFF MESSAGES Our objective pushes messages which can not reduce the uncertainties in action-value functions of other agents close to the origin of the latent message space. This naturally gives us a hint on how to drop meaningless messages - we can order the message distributions according to their means and drop accordingly. Note that since we use a unit covariance matrix for the latent message distribution, bits in a message are independent. Thus, we can make decisions in a bit-by-bit fashion and send messages with various lengths. In this way, our method learns not only when and who (agent i does not communicate with agent j when all bits of m ij are dropped) to communicate, but also what to communicate (how many bits are sent and their values). More details are discussed in Appendix B. Our framework adopts the centralized training with decentralized execution paradigm. During cen- tralized training, we assume the learning algorithm has access to all agents' individual observation- action histories and the global state s. During execution, agents communicate and act in a decentral- ized fashion based on the learned message encoder and action-value functions.

Section Title: RELATED WORKS
  RELATED WORKS Deep multi-agent reinforcement learning has witnessed vigorous progress in recent years. COMA (Foerster et al., 2018), MADDPG (Lowe et al., 2017), and PR2 (Wen et al., 2019) explores multi-agent policy gradients and respectively address the problem of credit assignment, learning in mixed environments and recursive reasoning. Another line of research focuses on value-based multi-agent RL, among which value-function factorization is the most popular method. Three rep- resentative examples: VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), and QTRAN (Son et al., 2019) gradually increase the representation ability of the mixing network. In particular, QMIX (Rashid et al., 2018) stands out as a scalable and robust algorithm and achieves state-of- the-art results on StarCraft unit micromanagement benchmark (Samvelyan et al., 2019). Communication is a hot topic in multi-agent reinforcement learning. End-to-end learning with dif- ferentiable communication channel is a popular approach now. Sukhbaatar et al. (2016); Hoshen (2017); Jiang & Lu (2018); Singh et al. (2019); Das et al. (2019) focus on learning decentralized communication protocol and address the problem of when and who to communicate. Foerster et al. (2016); Das et al. (2017); Lazaridou et al. (2017); Mordatch & Abbeel (2018) study the emergence of natural language in the context of multi-agent learning. IC3Net (Singh et al., 2019) learns gate to control the agents to only communicate with their teammates in mixed multi-agent environment. Zhang & Lesser (2013); Kim et al. (2019) study action coordination under limited communication channel and thus are related to our works. The difference lies in that they do not explicitly minimize communication. Social influence (Jaques et al., 2019) and InfoBot (Goyal et al., 2019) penalize message that has no influence on policies of other agents. Work that is most related to this paper is TarMAC (Das et al., 2019), where attention mechanism is used to differentiate the importance of incoming messages. In comparison, we use variation infer- ence to decide the content of messages and whether a message should be sent under the guidance of global reward signals. We compare our method with TarMAC and a baseline combining TarMAC and QMIX in our experiments. Related works on the task of StarCraft II unit micromanagement are discussed in Appendix C.2.

Section Title: EXPERIMENTAL RESULTS
  EXPERIMENTAL RESULTS In this section, we show our experiments to answer the following questions: (i) Is the miscoordina- tion problem of full value function factorization methods widespread? (ii) Can our method learn the minimized communication protocol required by a task? (iii) Can the learned message distributions reduce uncertainties in value functions of other agents? (iv) How does our method differ from com- munication with attention mechanism? (v) How does β influence the communication protocol? We will first show three simple examples to clarify our idea from different perspectives and then provide performance analysis on StarCraftII unit micromanagement benchmark. For evaluation, all exper- iments are carried out with 5 random seeds and results are shown with a 95% confidence interval. Details of the NDQ network architecture are given in Appendix B.2. Videos of our experiments on StarCraft II are available online 2 . We compare NDQ with the following baselines: (i) QMIX (Rashid et al., 2018); (ii) TarMAC (Das et al., 2019). QMIX and TarMAC are state-of-the-art full value function factorization and attentional communication methods, respectively. (iii) QMIX+TarMAC. We introduce the attentional commu- nication mechanism into the value function factorization paradigm by integrating the communication component of TarMAC into QMIX.

Section Title: DIDACTIC EXAMPLES
  DIDACTIC EXAMPLES We first demonstrate our idea on three didactic examples: sensor, hallway, and independent search. Sensor network is a frequently used testbed in multi-agent learning field (Kumar et al., 2011; Zhang & Lesser, 2011). We use a 3-chain sensor configuration in the task sensor (Fig. 2(a)). Each sensor is controlled by one agent, and they are rewarded for successfully locating targets, which requires two sensors to scan the same area simultaneously when the target appears. At each timestep, target 1 appears in area 1 with possibility 1, and locating it induces a team reward of 20; target 2 appears with probability 0.5 in area 2, and agents are rewarded 30 for locating it. Agents can observe whether a target is present in nearby areas and need to choose one of the five actions: scanning north, east, south, west, and noop. Every scan induces a cost of -5. In the optimal policy, when target 2 appears, sensor 1 should turn itself off while sensors 2 and 3 are expected to scan area 2 to get the reward. And when target 2 is absent, sensors 1 and 2 need to cooperatively scan area 1 while sensor 3 takes noop. Sensor is representative of a class of tasks where the uncertainties about the true states cause policies learned by full value function factorization method to be sub-optimal - sensor 1 has to know whether the target is present in area 2 to make a decision. However, the mixing network of QMIX cannot provide such information. As a result, QMIX converges to a sub-optimal policy, which gets a team reward of 12.5 a step on average (see Fig. 2(b)). We are particularly interested in whether our method can learn the minimized communication strat- egy.  Fig. 3  shows the latent message space learned by NDQ. When β = 10 −3 , agent 3 learns to send a bit to tell agent 1 whether target 2 appears. In the meantime, the latent message distribution between any other pair of agents is close to the standard Gaussian distribution and thus is dropped. This result indicates that NDQ has discovered the minimized conditional graph and can explain why our method can still perform optimally when 80% of the messages are cut off (Fig. 2(c)). When β becomes too large (1.0), all the message bits are pushed below the cutting threshold (Fig. 3(a) and 3(d)). When β is too small (10 −5 ), NDQ pays more attention on reducing uncertainties in Q-functions rather than compressing messages. Correspondingly, both agent 3 and agent 2 send a message to agent 1 (Fig. 3(c) and 3(f)), which is a redundant communication strategy. The second example, hallway (Fig. 4(a)), is a Dec-POMDP with two agents randomly starting at states a 1 to a m and b 1 to b n , respectively. Agents can observe their position and choose to move left, move right, or keep still at each timestep. Agents will win and get a reward of 10 if they arrive at state g simultaneously. Otherwise, if any agent arrives at g earlier than the other, the team will not be rewarded, and the next episode will begin. The horizon is set to max(m, n) + 10 to avoid an infinite loop. Hallway aims to show that the miscoordination problem of full factorization methods can be severe in multi-step scenarios. We set m and n to 4 and show comparison of performance in Fig. 4(b). The miscoordination problem causes QMIX to lose about half of the games. We are again particularly interested in the message embedding representations learned by NDQ. We show an episode in  Fig. 5 . Two agents begin at a 4 and b 3 , respectively. They first move left silently (t = 1 and t = 2) until agent B arrives at b 1 . On arriving b 1 , it sends a bit whose value is 5.24 to A. After sending this bit, B stays at b 1 and sends this message repeatedly until it receives a bit from A indicating that A has arrived at a 1 . They then move left together and win. This is the minimized communication strategy. Taking advantage of this strategy, NDQ can still win in 100% of episodes when 80% of the communicating bits are dropped (Fig. 4(c)). The third task, independent search, aims to demonstrate that NDQ can learn not to communicate in scenarios where agents are independent. Task description and results analysis are deferred to Appendix C.1.

Section Title: MAXIMUM VALUE FUNCTION FACTORIZATION IN STARCRAFT II
  MAXIMUM VALUE FUNCTION FACTORIZATION IN STARCRAFT II To demonstrate that the miscoordination problem of full decomposition methods is widespread in multi-agent learning, we apply our method and baselines to the StarCraft II micromanagement benchmark introduced by Samvelyan et al. (2019), which is described in detail in Appendix C.2. We further increase the difficulty of action coordination by i) reducing the sight range of agents from 9 to 2; ii) introducing challenging maps with complex terrain or highly random spawning po- sitions for units. We test our method on the six maps shown in  Fig. 6 . Detailed descriptions of these scenarios are provided in Appendix C.2. We use the same hyper-parameter setting for NDQ on all maps: β is set to 10 −5 , λ is set to 0.1, and the length of message m ij is set to 3. For evaluation, we pause training every 100k environment steps and run 48 testing episodes. Other hyper-parameters for NDQ are described in Appendix B.2.

Section Title: PERFORMANCE COMPARISON
  PERFORMANCE COMPARISON We show the performance of our method and baselines when no message is cut in  Fig. 7 . The superior performance of NDQ against QMIX demonstrates that the miscoordination problem of full factorization methods is widespread, especially in scenarios with high stochasticity, such as 1o2r vs 4r, 3b vs 1h1m, and 1o10b vs 1r. Notably, our method also outperforms the attentional communication mechanism (QMIX+TarMAC) by a large margin. Since agents communicate in both of these two methods and the same TD error is used, these results highlight the role of the constraints that we impose on our message embedding. TarMAC struggles in all the scenarios. We believe that this is because it does not deal with the issue of reward assignment.

Section Title: MESSAGE CUT OFF
  MESSAGE CUT OFF To demonstrate that our method can learn nearly decomposable Q-functions in complex tasks, we cut off 80% of messages according to the means of distributions when testing and show the results in  Fig. 8 . The results indicate that we can omit more than 80% of communication without significantly affecting performance. For comparison, we cut off messages in QMIX+TarMAC whose weights are 80% smallest and find that its performance drops significantly ( Fig. 8 ). These results indicate that our method is more robust in terms of message cutting off compared to the attentional communication methods. We further drop all the messages and show the developments of testing performance in  Fig. 9 . As expected, the win rates of NDQ decrease dramatically, proving that the superiority of our method when 80% of messages are dropped comes from expressive and succinct communication protocols.

Section Title: CLOSING REMARKS
  CLOSING REMARKS In this paper, we presented a novel multi-agent learning framework within the paradigm of cen- tralized training with decentralized execution. This framework fuses value function factorization learning and communication learning and efficiently learns nearly decomposable value functions for agents to act most of the time independently and communicate when it is necessary for coordi- nation. We introduce two information-theoretical regularizers to minimize overall communication while maximizing the message information for coordination. Empirical results in challenging Star- Craft II tasks show that our method significantly outperforms baseline methods and allows us to reduce communication by more than 80% without sacrificing the performance. We also observe that nearly minimal messages (e.g., with one or two bits) are learned to communicate between agents in order to ensure effective coordination.

```
