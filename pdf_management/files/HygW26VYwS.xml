<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 ATTENTION PRIVILEGED REINFORCEMENT LEARNING FOR DOMAIN TRANSFER</article-title></title-group><abstract><p>Applying reinforcement learning (RL) to physical systems presents notable chal- lenges, given requirements regarding sample efficiency, safety, and physical con- straints compared to simulated environments. To enable transfer of policies trained in simulation, randomising simulation parameters leads to more robust policies, but also significantly extends training time. In this paper, we exploit access to privileged information (such as environment states) often available in simulation, in order to improve and accelerate learning over randomised envi- ronments. We introduce Attention Privileged Reinforcement Learning (APRiL), which equips the agent with an attention mechanism and makes use of state infor- mation in simulation, learning to align attention between state- and image-based policies while additionally sharing generated data. During deployment we can apply the image-based policy to remove the requirement of access to additional information. We experimentally demonstrate accelerated and more robust learn- ing on a number of diverse domains, leading to improved final performance for environments both within and outside the training distribution. 1 .</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Deep Reinforcement Learning (RL) has recently provided significant successes in a range of areas, including video games (<xref ref-type="bibr" rid="b15">Mnih et al., 2015</xref>), board games (<xref ref-type="bibr" rid="b14">Silver et al., 2017</xref>), simulated continu- ous control tasks (<xref ref-type="bibr" rid="b14">Lillicrap et al., 2015</xref>), and robotic manipulation (<xref ref-type="bibr" rid="b8">Haarnoja et al., 2018</xref>; <xref ref-type="bibr" rid="b8">Haarnoja, 2018</xref>; <xref ref-type="bibr" rid="b15">Riedmiller et al., 2018</xref>; <xref ref-type="bibr" rid="b17">OpenAI et al., 2018</xref>; <xref ref-type="bibr" rid="b25">Schwab et al., 2019</xref>; <xref ref-type="bibr" rid="b1">Andrychowicz et al., 2017</xref>). However, application to physical systems has proven to be challenging in general, due to expen- sive and slow data generation as well as safety challenges when running untrained policies. A common approach to circumvent these issues is to transfer models trained in simulation to the real world (<xref ref-type="bibr" rid="b1">Tobin et al., 2017</xref>; <xref ref-type="bibr" rid="b15">Rusu et al., 2016</xref>; <xref ref-type="bibr" rid="b9">Held et al., 2017</xref>). However, simulators only repre- sent approximations of a physical system. Due to physical, visual, and behavioural discrepancies, naively transferring RL agents trained in simulation onto the real world can be challenging.</p><p>To bridge the gap between simulation and the real world, we can either aim to align both do- mains (<xref ref-type="bibr" rid="b6">Ganin et al., 2016</xref>; <xref ref-type="bibr" rid="b3">Bousmalis et al., 2016</xref>; Wulfmeier et al., 2017) or ensure that the real system is covered by the distribution of simulated training data (<xref ref-type="bibr" rid="b17">OpenAI et al., 2018</xref>; <xref ref-type="bibr" rid="b1">Tobin et al., 2017</xref>; <xref ref-type="bibr" rid="b18">Pinto et al., 2018</xref>; <xref ref-type="bibr" rid="b24">Sadeghi &amp; Levine, 2016</xref>; Viereck et al., 2017). However, training under a distribution of randomised visual attributes of the simulator, such as textures and lighting (<xref ref-type="bibr" rid="b24">Sadeghi &amp; Levine, 2016</xref>; Viereck et al., 2017), as well as physics (<xref ref-type="bibr" rid="b17">OpenAI et al., 2018</xref>), can be substantially more difficult and slower due to the increased variability of the learning domain (<xref ref-type="bibr" rid="b17">OpenAI et al., 2018</xref>; <xref ref-type="bibr" rid="b1">Tobin et al., 2017</xref>).</p><p>The more structured and informative the input representation is with respect to the task, the quicker the agent can be trained. A clear example of this effect can be found when an agent is trained with image inputs, versus training with access to the exact simulator states (<xref ref-type="bibr" rid="b14">Tassa et al., 2018</xref>; <xref ref-type="bibr" rid="b18">Pinto et al., 2018</xref>). However, visual perception is more general and access to more compressed representations can often be limited. When exact states are available during training but not deployment, we can make use of information asymmetric actor-critic methods (<xref ref-type="bibr" rid="b18">Pinto et al., 2018</xref>; <xref ref-type="bibr" rid="b25">Schwab et al., 2019</xref>) to train the critic faster via access to the state while providing only images for the actor.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>By introducing Attention Privileged Reinforcement Learning (APRiL), we aim to further leverage access to exact states. APRiL leverages states not only to train the critic, but indirectly also for an image-based actor. Extending asymmetric actor-critic methods, APRiL concurrently trains two actor-critic systems (one symmetric, state-based agent, and another asymmetric agent with image- dependent actor). Both actors utilise an attention mechanism to filter input data and by having access to the simulation rendering system, we can optimise image and state based attention masks to align. By additionally sharing the replay buffer between both agents, we can accelerate the learning process of the image-based actor by training on better performing states that are more quickly discovered by the state-based actor due to its lower dimensional input that is invariant to visual randomisation. The key benefits of APRiL lie in its application to domain transfer. When training with domain randomisation for transfer, bootstrapping via asymmetric information has displayed crucial bene- fits (<xref ref-type="bibr" rid="b18">Pinto et al., 2018</xref>). Visual randomisation substantially increases the complexity of the image- based actor's task. Under this setting, the attention network can support invariance with respect to the irrelevant, but highly varying, parts of the image. Furthermore, the convergence of the state- space actor remains unaffected by visual randomisation.</p><p>We experimentally demonstrate considerable improvements regarding learning convergence and more robust transfer on a set of continuous action domains including: 2D navigation, 2D locomotion and 3D robotic manipulation.</p></sec><sec><title>PROBLEM SETUP</title><p>Before introducing Attention Privileged Reinforcement Learning (APRiL), this section provides a background for the RL algorithms used. For a more in-depth introduction please refer to <xref ref-type="bibr" rid="b14">Lillicrap et al. (2015)</xref> and <xref ref-type="bibr" rid="b18">Pinto et al. (2018)</xref>.</p></sec><sec><title>REINFORCEMENT LEARNING</title><p>We describe an agent's environment as a Partially Observable Markov Decision Process which is represented as the tuple (S, O, A, P, r, &#947;, s 0 ), where S denotes a set of continuous states, A denotes a set of either discrete or continuous actions, P : S &#215; A &#215; S &#8594; {x &#8712; R|0 &#8804; x &#8804; 1} is the transition probability function, r : S &#215; A &#8594; R is the reward function, &#947; is the discount factor, and s 0 is the initial state distribution. O is a set of continuous observations corresponding to continuous states in S. At every time-step t, the agent takes action a t = &#960;(&#183;|s t ) according to its policy &#960; : S &#8594; A. The policy is optimised as to maximize the expected return R t = E s0 [ &#8734; i=t &#947; i&#8722;t r i |s 0 ]. The agent's Q-function is defined as Q &#960; (s t , a t ) = E[R t |s t , a t ].</p></sec><sec><title>ASYMMETRIC DEEP DETERMINISTIC POLICY GRADIENTS</title><p>Asymmetric Deep Deterministic Policy Gradients (asymmetric DDPG) (<xref ref-type="bibr" rid="b18">Pinto et al., 2018</xref>) repre- sents a type of actor-critic algorithm designed specifically for efficient learning of a deterministic, observation-based policy in simulation for sim-to-real transfer. This is achieved by leveraging ac- cess to more compressed, informative environment states, available in simulation, to speed up and stabilise training of the critic.</p><p>The algorithm maintains two neural networks: an observation-based actor or policy &#960; &#952; : O &#8594; A (with parameters &#952;) used during training and test time, and a state-based Q-function (also known as critic) Q &#960; &#966; : S &#215; A &#8594; R (with parameters &#966;) which is only used during training. To enable exploration, the method (like its symmetric version (<xref ref-type="bibr" rid="b14">Silver et al., 2014</xref>)) relies on a noisy version of the policy (called behavioural policy), e.g. &#960; b (o) = &#960;(o) + z where z &#8764; N (0, 1) (see Appendix C for our particular instantiation). The transition tuples (s t , o t , a t , r t , s t+1 , o t+1 ) encountered during training are stored in a replay buffer (<xref ref-type="bibr" rid="b15">Mnih et al., 2015</xref>). Training exam- ples sampled from the replay buffer are used to optimize the critic and actor. By minimizing the Bellman error loss L critic = (Q(s t , a t ) &#8722; y t ) 2 , where y t = r t + &#947;Q(s t+1 , &#960;(o t+1 )), the critic is optimized to approximate the true Q values. The actor is optimized by minimizing the loss L actor = &#8722;E s,o&#8764;&#960; b (o) [Q(s, &#960;(o))].</p></sec><sec><title>ATTENTION PRIVILEGED REINFORCEMENT LEARNING (APRIL)</title><p>APRiL proposes to improve the performance and sample efficiency of an observation-based agent by using a quicker learning actor that has access to exact environment states, sharing replay buffers, and aligning attention mechanisms between both actors. While we focus in the following sections on extending asymmetric DDPG (<xref ref-type="bibr" rid="b18">Pinto et al., 2018</xref>), these ideas are generally applicable to off-policy actor-critic methods (<xref ref-type="bibr" rid="b11">Konda &amp; Tsitsiklis, 2000</xref>).</p><p>APRiL is comprised of three modules as displayed in <xref ref-type="fig" rid="fig_0">Figure 1</xref>. The first two modules, A s and A o , are actor-critic algorithms with an attention network incorporated over the input to each actor. For the state-based module A s we use standard symmetric DDPG, while the observation-based module A o builds on asymmetric DDPG. Finally, the third part A T represents the alignment process between attention mechanisms of both actor-critic agents to more effectively transfer knowledge between the quicker and slower learners, A s and A o , respectively.</p><p>A s consists of three networks: Q &#960; s , &#960; s , h s (respectively critic, actor, and attention) with parameters {&#966; s , &#952; s , &#968; s }. Given input state s t , the attention network outputs a soft gating mask h t of same dimensionality as the input, with values ranging between [0, 1]. The input to the actor is an attention- filtered version of the state, s a t = h s (s t ) s t . To encourage a sparse masking function, we found that training this attention module on both the traditional DDPG loss as well as an entropy loss helped: L hs = &#8722;E s&#8764;&#960; b [Q s (s, &#960; s (s a )) &#8722; &#946;H(h s (s))], (1) where &#946; is a hyperparameter to weight the additional entropy objective, and &#960; b is the behaviour pol- icy used to obtain experience (in this case from a shared replay buffer). The actor and critic networks &#960; s and Q s are trained with the symmetric DDPG actor and Bellman error losses respectively.</p><p>Within A T , the state-attention obtained in A s is converted to corresponding observation-attention T to act as a self-supervised target for the observation-based agent in A o . This is achieved in a two- step process. First, state-attention h s (s) is converted into object-attention c, which specifies how task-relevant each object in the scene is. Second, object-attention is converted to observation-space attention by performing a weighted sum over object-specific segmentation maps:</p><p>Here, M &#8712; {0, 1} N &#215;ns (where n s is the dimensionality of s) is an environment-specific, predefined adjacency matrix that maps the dimensions of s to each corresponding object, and c &#8712; [0, 1] N is Under review as a conference paper at ICLR 2020 then an attention vector over the N objects in the environment. c i corresponds to the i th object attention value. z i &#8712; {0, 1} W &#215;H is the binary segmentation map 2 of the i th object segmenting the object with the rest of the scene, and has the same dimensions as the image observation. z i assigns values of 1 for pixels in the image occupied by the i th object, and 0 elsewhere. T &#8712; [0, 1] W &#215;H is the converted state-attention to observation-space attention to act as a target to train the observation- attention network h o on.</p><p>The observation-based module A o also consists of three networks: Q &#960; o , &#960; o , h o (respectively critic, actor, and attention) with parameters {&#966; o , &#952; o , &#968; o }. The structure of this module is the same as A s except the actor and critic now have asymmetric inputs. The input to the actor is the attention- filtered version of the observation, o a t = h o (o t ) o t 3 .The actor and critic networks &#960; o and Q o are trained with the standard asymmetric DDPG actor and Bellman error losses respectively defined in Section 2.2. The main difference between A o and A s is that the observation attention network h o is trained on both the actor loss and an object-weighted mean squared error loss:</p><p>where weights w ij correspond to the fraction of the partial observation o that the object present in o i,j,1:3 occupies, and &#957; represents the relative weighting of both loss components. The weight terms, w, ensure that the attention network becomes invariant to the size of objects during training and does not simply fit to the most predominant object in the scene. Combining the self-supervised attention loss and the RL loss leverages efficient state-space learning unaffected by visual randomisation. During training, experiences are collected evenly from both state and observation based agents and stored in a shared replay buffer (similar to <xref ref-type="bibr" rid="b25">Schwab et al. (2019)</xref>). This is to ensure that: 1. Both state- based critic Q s and observation-based critic Q o observe states that would be visited by either of their respective policies. 2. The attention modules h s and h o are trained on the same data distribution to better facilitate alignment. 3. Efficient discovery of highly performing states from &#960; s are used to speed up learning of &#960; o .</p><p>Algorithm 1 shows pseudocode for a single actor implementation of APRiL. In practice, in order to speed up data collection and gradient computation, we parallelise the agents and environments and ensure data collection from state- and image- based agents is even.</p></sec><sec><title>EXPERIMENTS</title><p>To demonstrate the performance and generality of our method, we apply APRiL to a range of envi- ronments, and compare with a competitive asymmetric DDPG baseline and various ablations. We evaluate APRiL over different metrics to investigate how attention helps with robustness and gener- alisation to unseen environments and transfer scenarios. Further experimental details can be found in Appendix C.</p></sec><sec><title>EVALUATION PROTOCOL</title><p>In order to investigate APRiL under varying conditions, we evaluate in scenarios of increasing com- plexity covering simple 2D navigation, 3D reaching and 2D dynamic locomotion.</p><p>We use the following continuous action-space environments (see Appendix A for further details):</p><p>1. NavWorld: In this 2D environment, the goal is for the circular agent to reach the triangular target in the presence of distractors. The agent is sparsely rewarded if the target is reached.</p><p>2. JacoReach: In this 3D environment the goal of the Kinova arm (<xref ref-type="bibr" rid="b4">Campeau-Lecours et al., 2017</xref>) agent is to reach the diamond ShapeStacks object (<xref ref-type="bibr" rid="b7">Groth et al., 2018</xref>) in the presence of distractors. The agent is rewarded for approaching and reaching its goal.</p><p>3. Walker2D: In this slightly modified 2D Deepmind Control Suite environment (<xref ref-type="bibr" rid="b14">Tassa et al., 2018</xref>) the goal of the agent is to walk forward as far as possible within a time-limit. The agent receives a reward for moving forward as well as a reward for keeping its torso upright. For these domains we randomise visuals during training as to enable generalisation to these variable aspects of the environment. We randomise a combination of: camera position and orientation, textures, materials, colours, object locations, background. Refer to Appendix B for more details.</p></sec><sec><title>KEY RESEARCH QUESTIONS</title><p>We investigate the following questions to evaluate how well APRiL accommodates for the transfer- ring of policies across visually distinct environments: Does APRiL 1. Increase sample-efficiency during training? 2. Affect interpolation performance on unseen environments from the training dis- tribution? 3. Affect extrapolation performance on environments outside the training distribution? We qualitatively analyse the learnt attention maps (both on interpolated and extrapolated domains). Finally, we perform an ablation study to investigate which parts of the APRiL contribute to perfor- mance gains. This ablation consists of the following models:</p><p>1. APRiL no self-supervision (APRiL no sup): APRiL except without the self-supervision provided by the state agent to train the observation-based attention. Both agents are still equipped with an attention module, but the observation attention must now learn without guidance from the state agent. Without bootstrapping from the state agent in this way we expect learning of informative observation-based attention to be hindered.</p><p>2. APRiL no shared buffer (APRiL no share): APRiL except each agent has its own replay buffer, instead of one shared replay buffer, and hence does not share experiences during training. Under this setting, the observation agent will not be able to benefit from earlier visitation of lucrative states by the state agent. Both agents have an attention module and attention alignment still occurs.</p><p>3. APRiL no background (APRiL no back): APRiL except the state agent's attention is no longer used to calculate object-space attention values c. Instead, all objects are given equal attention and we hence learn a background suppressor. This most competitive ablation in- vestigates how important object suppression is for learning, robustness, and generalisation. Both agents still maintain attention have a shared replay buffer.</p></sec><sec><title>PERFORMANCE ON THE TRAINING DISTRIBUTION</title><p>We evaluate the performance on all domains during training and observe APRiL 's benefits. As seen in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, APRiL provides performance gains across all continuous action domains. APRiL not only helps learn useful representations quicker (improving learning rate) but also improves final policy performance (within the allotted training time).</p><p>The ablations demonstrate that self-supervision and shared replay both independently provide per- formance gains for JacoReach and Walker2D 4 . For Walker2D, shared replay is crucial as stabilises learning (observe APRiL , APRiL no back, APRiL no sup), due to constant visitation for highly performing states. Suppression of task-irrelevant, yet highly varying, information also speed up learning as simplifies the observation space. For this reason, APRiL no back proves to be a com- petitive ablation, approaching the performance of APRiL for JacoReach and Walker2D. For these domains, the background occupies the majority of the observation space and ignoring it already suppresses most of the irrelevant information. Minimal improvement can be achieved by suppress- ing additional irrelevant objects. None of the ablations, however, are able to outperform the full APRiL framework, demonstrating that the combination of a shared replay buffer and state-space- informed image-attention module cooperate constructively toward more efficient feature learning and effective policy and critic updates.</p></sec><sec><title>INTERPOLATION: TRANSFER TO DOMAINS FROM THE TRAINING DISTRIBUTION</title><p>We evaluate the performance of all actor-critic algorithms on a hold out set of simulation parameters, unseen during training, from the training distribution. For a detailed description of the training dis- tribution for each domain please refer to Appendix B. For both NavWorld and JacoReach, the inter- polated environments have the same number of distractors, sampled from the same object catalogue, as the training distribution. <xref ref-type="table" rid="tab_0">Table 1</xref> displays final policy performance on these domains. For APRiL , we observe no degradation in policy performance between training and interpolated domains. We see a very similar trend for the asymmetric DDPG baseline. However, as APRiL performs better on the training distribution, its final performance on the interpolated domains is significantly better. We therefore demonstrate that on these domains APRiL's attention mechanism does not hurt with respect to overfitting.</p></sec><sec><title>EXTRAPOLATION: TRANSFER TO DOMAINS OUTSIDE THE TRAINING DISTRIBUTION</title><p>We investigate performances on simulation parameters outside the training distribution. In partic- ular, we investigate how well APRiL , its ablations, and asymmetric DDPG, generalise to environ- ments with more distractor objects than seen during training. For NavWorld and JacoReach, we run two sets of increasingly extrapolated experiments with an additional 4 or 8 distractors (refered to as ext-4 and ext-8 in <xref ref-type="table" rid="tab_0">Table 1</xref>). The textures and colours of these objects are sampled from a held-old out set of simulation parameters not seen during training. For NavWorld, the locations and orientations of the additional distractors are randomly sampled. For JacoReach, the locations are Under review as a conference paper at ICLR 2020 sampled from arcs of two concentric circles of different radii (extrapolated arcs and radii to those seen during training), in such a way that each object remains visible. The shapes of the additional distractor object are sampled from the training catalogue of distractor objects. Please refer to <xref ref-type="fig" rid="fig_2">Figure 3</xref> for examples of the extrapolated domains. <xref ref-type="table" rid="tab_0">Table 1</xref> compares performances on the extrapolated sets (except Walker2D) varying in difficulty (ext-4 and ext-8). APRiL yields performance gains over the asymmetric DDPG baseline on every extrapolated domain. For JacoReach, APRiL's generalisation is so effective that, for the hardest domain with additional 8 distractors, its performance degrades by only 9% 5 opposed to 41% (base- line).</p><p>APRiL generalises favorably due to the attention module. <xref ref-type="fig" rid="fig_2">Figure 3</xref> shows that attention generalises and suppresses the additional distractors, thereby effectively converting the hold-out observations to those seen during training, which the image-policy can handle. The ablations in <xref ref-type="table" rid="tab_0">Table 1</xref> confirm that in this setting, distractor suppression is crucial. This is seen when comparing the maximum degra- dation in policy performance of APRiL, APRiL no share, APRiL no back and APRiL no sup (9%, 16%, 27% and 47% respectively). APRiL and APRiL no share both align attention between image and state agents during training, and therefore effectively suppress distractors (yielding a favourable decrease in policy performance of only 9% and 16%). APRiL no back learns a background suppres- sor, but does not suppress the distractors (leading to a larger degradation of 27%). APRiL no sup has an attention module trained only on the asymmetric actor-critic loss and yields the worst extrapolated performance (47% policy degradation). For these extrapolated domains, the successful suppression of the background and additional distractors (achieved only by the full APRiL framework), creates policy invariance with respect to them and helps generalise.</p></sec><sec><title>ATTENTION MODULE ANALYSIS</title><p>To better comprehend the role of the attention, we visualise APRiL's attention maps (<xref ref-type="fig" rid="fig_2">Figure 3</xref>, 4, 5) on both interpolated and extrapolated domains. For NavWorld, attention is correctly paid to all relevant aspects (agent and target; circle and triangle respectively). Attention generalises reasonably well to the extrapolated environments. For JacoReach, attention looks at the target, diamond-shaped, object as well as every other link (alternating links) of the Kinova arm. Interestingly, APRiL learnt that as the arm is a constrained system, the state of every other link can be indirectly inferred without explicit attention. The state of the unobserved link can be inferred by observing the links either side of it. The entropy loss over the state-attention module encourages this form of attention over minimal set of objects. Attention here generalises very well to the extrapolated domains. For Walker2D, we observe attention that is dynamic in object space. The attention module attends different subsets of links depending on the state of the system (see Figure 5). When the walker is upright, walking, and collapsing, APRiL pays attention to the lower limbs, every other link, and foot and upper body, respectively. We suspect that in these scenarios, the magnitude of the optimal action depends on the state of and as is largest for the lower links (due to stability), every link (coordination), and foot and upper body (large torque required), respectively.</p></sec><sec><title>RELATED WORK</title><p>Domain Randomisation has been applied for reinforcement learning to facilitate transfer between domains (<xref ref-type="bibr" rid="b1">Tobin et al., 2017</xref>; <xref ref-type="bibr" rid="b18">Pinto et al., 2018</xref>; <xref ref-type="bibr" rid="b24">Sadeghi &amp; Levine, 2016</xref>; Viereck et al., 2017; <xref ref-type="bibr" rid="b17">OpenAI et al., 2018</xref>; <xref ref-type="bibr" rid="b9">Held et al., 2017</xref>) and increase robustness of the learned policies (<xref ref-type="bibr" rid="b20">Rajeswaran et al., 2016</xref>). However, while domain randomisation enables us to generate more robust and transferable policies, it leads to a significant increase in required training time (<xref ref-type="bibr" rid="b17">OpenAI et al., 2018</xref>). Existing comparisons in the literature demonstrate that, even without domain randomisation, the in- creased dimensionality and potential partial observability complicates learning for RL agents (<xref ref-type="bibr" rid="b14">Tassa et al., 2018</xref>; <xref ref-type="bibr" rid="b25">Schwab et al., 2019</xref>; <xref ref-type="bibr" rid="b0">Watter et al., 2015</xref>; <xref ref-type="bibr" rid="b13">Lesort et al., 2018</xref>). In this context, accelerated training has been achieved by using access to privileged information such as environment states to asymmetrically train the critic in actor-critic RL (<xref ref-type="bibr" rid="b25">Schwab et al., 2019</xref>; <xref ref-type="bibr" rid="b18">Pinto et al., 2018</xref>). In addition to using additional information to train the critic, <xref ref-type="bibr" rid="b25">Schwab et al. (2019)</xref> use a shared replay buffer for data generated by image- and state-based actors to further accelerate training for the image- based agent. Our method extends these approaches by sharing information about relevant objects by aligning agent-integrated attention mechanisms between an image- and state-based actors.</p><p>Recent experiments have demonstrated the strong dependency and bidirectional interaction between attention and learning in human subjects (<xref ref-type="bibr" rid="b12">Leong et al., 2017</xref>). In the context of machine learning, attention mechanisms have been integrated into RL agents to increase robustness and enable in- terpretability of an agent's behaviour (<xref ref-type="bibr" rid="b28">Sorokin et al., 2015</xref>; <xref ref-type="bibr" rid="b5">Choi et al., 2017</xref>; <xref ref-type="bibr" rid="b16">Mott et al., 2019</xref>). In comparison to these works, we focus on utilising the attention mechanism as an interface to transfer information between two agents to enable faster training.</p></sec><sec><title>CONCLUSION</title><p>We introduce Attention Privileged Reinforcement Learning (APRiL), an extension to asymmetric actor-critic algorithms that leverages access to privileged information like exact simulator states. The method benefits in two ways, via sharing a replay buffer as well as aligning attention masks between image- and state-space agents. By leveraging simulator ground-truth information about system states, we are able to learn efficiently in the image domain especially during domain ran- domisation where feature learning becomes increasingly difficult. Our evaluation on a diverse set of environments demonstrates significant improvements over the competitive asymmetric DDPG baseline and reveals that APRiL learns to generalise favourably to environments not seen during training (both within and outside of the training distribution) in comparison to the strong baseline; emphasising the importance of attention and shared experience for robustness of the learnt policies. Under review as a conference paper at ICLR 2020</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Attention Privileged Reinforcement Learning model structure. Dashed lines indicate at- tention alignment process. The &#8764; operator signifies that experiences are evenly sampled from both agents. The &#8855; operator represents element-wise multiplication.</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Learning curves during training of APRiL , its ablations, and the asymmetric DDPG base- line. Solid line: mean performance. Shaded region: covers minimum and maximum performances across 5 seeds.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Ablation comparing average return over training, interpolated and extrapolated environ- ments (100 each). Results reflect mean and standard deviation of average return over 5 seeds.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Example held-out domains (top) and APRiL attention maps (bottom). White and black signify high and low attention values. Attention correctly suppresses background and distractors.</p></caption><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Tensorflow: A system for large- scale machine learning</article-title><source>OSDI</source><year>2016</year><volume>16</volume><fpage>265</fpage><lpage>283</lpage><person-group person-group-type="author"><name><surname>References Mart&#237;n Abadi</surname><given-names>Paul</given-names></name><name><surname>Barham</surname><given-names>Jianmin</given-names></name><name><surname>Chen</surname><given-names>Zhifeng</given-names></name><name><surname>Chen</surname><given-names>Andy</given-names></name><name><surname>Davis</surname><given-names>Jeffrey</given-names></name><name><surname>Dean</surname><given-names>Matthieu</given-names></name><name><surname>Devin</surname><given-names>Sanjay</given-names></name><name><surname>Ghemawat</surname><given-names>Geoffrey</given-names></name><name><surname>Irving</surname><given-names>Michael</given-names></name><name><surname>Isard</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><source>Hindsight experience replay</source><year>2017</year><person-group person-group-type="author"><name><surname>Andrychowicz</surname><given-names>Marcin</given-names></name><name><surname>Wolski</surname><given-names>Filip</given-names></name><name><surname>Ray</surname><given-names>Alex</given-names></name><name><surname>Schneider</surname><given-names>Jonas</given-names></name><name><surname>Fong</surname><given-names>Rachel</given-names></name><name><surname>Welinder</surname><given-names>Peter</given-names></name><name><surname>Mcgrew</surname><given-names>Bob</given-names></name><name><surname>Tobin</surname><given-names>Josh</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name><name><surname>Zaremba</surname><given-names>Wojciech</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Layer normalization</article-title><source>arXiv preprint arXiv:1607.06450</source><year>2016</year><person-group person-group-type="author"><name><surname>Ba</surname><given-names>Jimmy Lei</given-names></name><name><surname>Kiros</surname><given-names>Jamie Ryan</given-names></name><name><surname>Hinton</surname><given-names>Geoffrey E</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Domain separation networks</article-title><source>Advances in Neural Information Processing Systems</source><year>2016</year><fpage>343</fpage><lpage>351</lpage><person-group person-group-type="author"><name><surname>Bousmalis</surname><given-names>Konstantinos</given-names></name><name><surname>Trigeorgis</surname><given-names>George</given-names></name><name><surname>Silberman</surname><given-names>Nathan</given-names></name><name><surname>Krishnan</surname><given-names>Dilip</given-names></name><name><surname>Erhan</surname><given-names>Dumitru</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Kinova modular robot arms for service robotics applications</article-title><source>Int. J. Robot. Appl. Technol.</source><year>2017</year><volume>5</volume><issue>2</issue><fpage>49</fpage><lpage>71</lpage><person-group person-group-type="author"><name><surname>Campeau-Lecours</surname><given-names>Alexandre</given-names></name><name><surname>Lamontagne</surname><given-names>Hugo</given-names></name><name><surname>Latour</surname><given-names>Simon</given-names></name><name><surname>Fauteux</surname><given-names>Philippe</given-names></name><name><surname>Ma- Heu</surname><given-names>V&#233;ronique</given-names></name><name><surname>Boucher</surname><given-names>Fran&#231;ois</given-names></name><name><surname>Deguire</surname><given-names>Charles</given-names></name><name><surname>Caron</surname><given-names>Louis-Joseph</given-names></name><name><surname>Ecuyer</surname><given-names>L</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Multi-focus attention network for efficient deep reinforcement learning</article-title><source>Workshops at the Thirty-First AAAI Conference on Artificial Intelligence</source><year>2017</year><person-group person-group-type="author"><name><surname>Choi</surname><given-names>Jinyoung</given-names></name><name><surname>Lee</surname><given-names>Beom-Jin</given-names></name><name><surname>Zhang</surname><given-names>Byoung-Tak</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Domain-adversarial training of neural net- works</article-title><source>The Journal of Machine Learning Research</source><year>2016</year><volume>17</volume><issue>1</issue><fpage>2096</fpage><lpage>2030</lpage><person-group person-group-type="author"><name><surname>Ganin</surname><given-names>Yaroslav</given-names></name><name><surname>Ustinova</surname><given-names>Evgeniya</given-names></name><name><surname>Ajakan</surname><given-names>Hana</given-names></name><name><surname>Germain</surname><given-names>Pascal</given-names></name><name><surname>Larochelle</surname><given-names>Hugo</given-names></name><name><surname>Laviolette</surname><given-names>Fran&#231;ois</given-names></name><name><surname>Marchand</surname><given-names>Mario</given-names></name><name><surname>Lempitsky</surname><given-names>Victor</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><source>ECCV (1), volume 11205 of Lecture Notes in Computer Science</source><year>2018</year><fpage>724</fpage><lpage>739</lpage><person-group person-group-type="author"><name><surname>Groth</surname><given-names>Oliver</given-names></name><name><surname>Fuchs</surname><given-names>Fabian B</given-names></name><name><surname>Posner</surname><given-names>Ingmar</given-names></name><name><surname>Vedaldi</surname><given-names>Andrea</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><source>Tuomas Haarnoja. Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning</source><year>2018</year><person-group person-group-type="author"><name><surname>Haarnoja</surname><given-names>T</given-names></name><name><surname>Ha</surname><given-names>S</given-names></name><name><surname>Zhou</surname><given-names>A</given-names></name><name><surname>Tan</surname><given-names>J</given-names></name><name><surname>Tucker</surname><given-names>G</given-names></name><name><surname>Levine</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Probabilistically safe policy transfer</article-title><source>Robotics and Automation (ICRA), 2017 IEEE International Conference on</source><fpage>5798</fpage><lpage>5805</lpage><person-group person-group-type="author"><name><surname>Held</surname><given-names>David</given-names></name><name><surname>Mccarthy</surname><given-names>Zoe</given-names></name><name><surname>Zhang</surname><given-names>Michael</given-names></name><name><surname>Shentu</surname><given-names>Fred</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv preprint arXiv:1412.6980</source><year>2014</year><person-group person-group-type="author"><name><surname>Diederik</surname><given-names>P</given-names></name><name><surname>Kingma</surname><given-names>Jimmy</given-names></name><name><surname>Ba</surname><given-names /></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Actor-critic algorithms</article-title><source>Advances in neural information processing systems</source><year>2000</year><fpage>1008</fpage><lpage>1014</lpage><person-group person-group-type="author"><name><surname>Vijay</surname><given-names>R</given-names></name><name><surname>Konda</surname><given-names>John N</given-names></name><name><surname>Tsitsiklis</surname><given-names /></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Dy- namic interaction between reinforcement learning and attention in multidimensional environ- ments</article-title><source>Neuron</source><year>2017</year><volume>93</volume><issue>2</issue><fpage>451</fpage><lpage>463</lpage><person-group person-group-type="author"><name><surname>Yuan Chang Leong</surname><given-names>Angela</given-names></name><name><surname>Radulescu</surname><given-names>Reka</given-names></name><name><surname>Daniel</surname><given-names>Vivian</given-names></name><name><surname>Dewoskin</surname><given-names>Yael</given-names></name><name><surname>Niv</surname><given-names /></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>State represen- tation learning for control: An overview</article-title><source>Neural Networks</source><year>2018</year><person-group person-group-type="author"><name><surname>Lesort</surname><given-names>Timoth&#233;e</given-names></name><name><surname>D&#237;az-Rodr&#237;guez</surname><given-names>Natalia</given-names></name><name><surname>Goudou</surname><given-names>Jean-Franois</given-names></name><name><surname>Filliat</surname><given-names>David</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><source>Continuous control with deep reinforcement learning</source><year>2015</year><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>Timothy P</given-names></name><name><surname>Hunt</surname><given-names>Jonathan J</given-names></name><name><surname>Pritzel</surname><given-names>Alexander</given-names></name><name><surname>Heess</surname><given-names>Nicolas</given-names></name><name><surname>Erez</surname><given-names>Tom</given-names></name><name><surname>Tassa</surname><given-names>Yuval</given-names></name><name><surname>Silver</surname><given-names>David</given-names></name><name><surname>Wierstra</surname><given-names>Daan</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Human-level control through deep reinforcement learning</article-title><source>Nature</source><year>2015</year><volume>518</volume><issue>7540</issue><fpage>529</fpage><lpage>529</lpage><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>Volodymyr</given-names></name><name><surname>Kavukcuoglu</surname><given-names>Koray</given-names></name><name><surname>Silver</surname><given-names>David</given-names></name><name><surname>Rusu</surname><given-names>Andrei A</given-names></name><name><surname>Veness</surname><given-names>Joel</given-names></name><name><surname>Marc</surname><given-names>G</given-names></name><name><surname>Belle- Mare</surname><given-names>Alex</given-names></name><name><surname>Graves</surname><given-names>Martin</given-names></name><name><surname>Riedmiller</surname><given-names>Andreas K</given-names></name><name><surname>Fidjeland</surname><given-names>Georg</given-names></name><name><surname>Ostrovski</surname><given-names /></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>Towards interpretable reinforcement learning using attention augmented agents</article-title><source>ArXiv</source><year>2019</year><person-group person-group-type="author"><name><surname>Mott</surname><given-names>Alex</given-names></name><name><surname>Zoran</surname><given-names>Daniel</given-names></name><name><surname>Chrzanowski</surname><given-names>Mike</given-names></name><name><surname>Wierstra</surname><given-names>Daan</given-names></name><name><surname>Rezende</surname><given-names>Danilo J</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><source>Learning dexterous in-hand manipulation</source><year>2018</year><person-group person-group-type="author"><name><surname>Openai</surname><given-names>:</given-names></name><name><surname>Andrychowicz</surname><given-names>Marcin</given-names></name><name><surname>Baker</surname><given-names>Bowen</given-names></name><name><surname>Chociej</surname><given-names>Maciek</given-names></name><name><surname>Jozefowicz</surname><given-names>Rafal</given-names></name><name><surname>Mcgrew</surname><given-names>Bob</given-names></name><name><surname>Pachocki</surname><given-names>Jakub</given-names></name><name><surname>Petron</surname><given-names>Arthur</given-names></name><name><surname>Plappert</surname><given-names>Matthias</given-names></name><name><surname>Powell</surname><given-names>Glenn</given-names></name><name><surname>Ray</surname><given-names>Alex</given-names></name><name><surname>Schneider</surname><given-names>Jonas</given-names></name><name><surname>Szy- Mon</surname><given-names /></name><name><surname>Sidor</surname><given-names>Josh</given-names></name><name><surname>Tobin</surname><given-names>Peter</given-names></name><name><surname>Welinder</surname><given-names>Lilian</given-names></name><name><surname>Weng</surname><given-names>Wojciech</given-names></name><name><surname>Zaremba</surname><given-names /></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Asym- metric actor critic for image-based robot learning</article-title><year>2018</year><person-group person-group-type="author"><name><surname>Pinto</surname><given-names>Lerrel</given-names></name><name><surname>Andrychowicz</surname><given-names>Marcin</given-names></name><name><surname>Welinder</surname><given-names>Peter</given-names></name><name><surname>Zaremba</surname><given-names>Wojciech</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Parameter space noise for exploration</article-title><source>arXiv preprint arXiv:1706.01905</source><year>2017</year><person-group person-group-type="author"><name><surname>Plappert</surname><given-names>Matthias</given-names></name><name><surname>Houthooft</surname><given-names>Rein</given-names></name><name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name><name><surname>Sidor</surname><given-names>Szymon</given-names></name><name><surname>Richard</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>Xi</given-names></name><name><surname>Chen</surname><given-names>Tamim</given-names></name><name><surname>Asfour</surname><given-names>Pieter</given-names></name><name><surname>Abbeel</surname><given-names>Marcin</given-names></name><name><surname>Andrychowicz</surname><given-names /></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Epopt: Learning robust neural network policies using model ensembles</article-title><source>arXiv preprint arXiv:1610.01283</source><year>2016</year><person-group person-group-type="author"><name><surname>Rajeswaran</surname><given-names>Aravind</given-names></name><name><surname>Ghotra</surname><given-names>Sarvjeet</given-names></name><name><surname>Ravindran</surname><given-names>Balaraman</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><source>Learning by playing - solving sparse reward tasks from scratch</source><year>2018</year><person-group person-group-type="author"><name><surname>Riedmiller</surname><given-names>Martin</given-names></name><name><surname>Hafner</surname><given-names>Roland</given-names></name><name><surname>Lampe</surname><given-names>Thomas</given-names></name><name><surname>Neunert</surname><given-names>Michael</given-names></name><name><surname>Degrave</surname><given-names>Jonas</given-names></name><name><surname>Van De Wiele</surname><given-names>Tom</given-names></name><name><surname>Mnih</surname><given-names>Volodymyr</given-names></name><name><surname>Heess</surname><given-names>Nicolas</given-names></name><name><surname>Springenberg</surname><given-names>Jost Tobias</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>International Conference on Learning Representations</source><year>2015</year><person-group person-group-type="author"><name><surname>Romero</surname><given-names>A</given-names></name><name><surname>Ballas</surname><given-names /></name><name><surname>Se Kahou</surname><given-names /></name><name><surname>Chassang</surname><given-names>C</given-names></name><name><surname>Gatta</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names /></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>Sim-to-real robot learning from pixels with progressive nets</article-title><source>arXiv preprint arXiv:1610.04286</source><year>2016</year><person-group person-group-type="author"><name><surname>Andrei</surname><given-names>A</given-names></name><name><surname>Rusu</surname><given-names>Mel</given-names></name><name><surname>Vecerik</surname><given-names>Thomas</given-names></name><name><surname>Roth&#246;rl</surname><given-names>Nicolas</given-names></name><name><surname>Heess</surname><given-names>Razvan</given-names></name><name><surname>Pascanu</surname><given-names>Raia</given-names></name><name><surname>Hadsell</surname><given-names /></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>Cad2rl: Real single-image flight without a single real image</article-title><source>arXiv preprint arXiv:1611.04201</source><year>2016</year><person-group person-group-type="author"><name><surname>Sadeghi</surname><given-names>Fereshteh</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><article-title>Simul- taneously learning vision and feature-based control policies for real-world ball-in-a-cup</article-title><source>arXiv preprint arXiv:1902.04706</source><year>2019</year><person-group person-group-type="author"><name><surname>Schwab</surname><given-names>Devin</given-names></name><name><surname>Springenberg</surname><given-names>Tobias</given-names></name><name><surname>Murilo</surname><given-names>F</given-names></name><name><surname>Martins</surname><given-names>Thomas</given-names></name><name><surname>Lampe</surname><given-names>Michael</given-names></name><name><surname>Neunert</surname><given-names>Abbas</given-names></name><name><surname>Abdolmaleki</surname><given-names>Tim</given-names></name><name><surname>Herkweck</surname><given-names>Roland</given-names></name><name><surname>Hafner</surname><given-names>Francesco</given-names></name><name><surname>Nori</surname><given-names>Martin</given-names></name><name><surname>Riedmiller</surname><given-names /></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>Deterministic policy gradient algorithms</article-title><source>ICML</source><year>2014</year><person-group person-group-type="author"><name><surname>Silver</surname><given-names>David</given-names></name><name><surname>Lever</surname><given-names>Guy</given-names></name><name><surname>Heess</surname><given-names>Nicolas</given-names></name><name><surname>Degris</surname><given-names>Thomas</given-names></name><name><surname>Wierstra</surname><given-names>Daan</given-names></name><name><surname>Riedmiller</surname><given-names>Martin</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><article-title>Mastering the game of go without human knowledge</article-title><source>Nature</source><year>2017</year><volume>550</volume><issue>7676</issue><fpage>354</fpage><lpage>354</lpage><person-group person-group-type="author"><name><surname>Silver</surname><given-names>David</given-names></name><name><surname>Schrittwieser</surname><given-names>Julian</given-names></name><name><surname>Simonyan</surname><given-names>Karen</given-names></name><name><surname>Antonoglou</surname><given-names>Ioannis</given-names></name><name><surname>Huang</surname><given-names>Aja</given-names></name><name><surname>Guez</surname><given-names>Arthur</given-names></name><name><surname>Hubert</surname><given-names>Thomas</given-names></name><name><surname>Baker</surname><given-names>Lucas</given-names></name><name><surname>Lai</surname><given-names>Matthew</given-names></name><name><surname>Bolton</surname><given-names>Adrian</given-names></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>Deep attention recurrent q-network</article-title><source>arXiv preprint arXiv:1512.01693</source><year>2015</year><person-group person-group-type="author"><name><surname>Sorokin</surname><given-names>Ivan</given-names></name><name><surname>Seleznev</surname><given-names>Alexey</given-names></name><name><surname>Pavlov</surname><given-names>Mikhail</given-names></name><name><surname>Fedorov</surname><given-names>Aleksandr</given-names></name><name><surname>Ignateva</surname><given-names>Anastasiia</given-names></name></person-group></element-citation></ref><ref id="b29"><element-citation publication-type="journal"><article-title>Deepmind control suite</article-title><source>arXiv preprint arXiv:1801.00690</source><year>2018</year><person-group person-group-type="author"><name><surname>Tassa</surname><given-names>Yuval</given-names></name><name><surname>Doron</surname><given-names>Yotam</given-names></name><name><surname>Muldal</surname><given-names>Alistair</given-names></name><name><surname>Erez</surname><given-names>Tom</given-names></name><name><surname>Li</surname><given-names>Yazhe</given-names></name><name><surname>De Las</surname><given-names>Diego</given-names></name><name><surname>Casas</surname><given-names>David</given-names></name><name><surname>Bud- Den</surname><given-names>Abbas</given-names></name><name><surname>Abdolmaleki</surname><given-names>Josh</given-names></name><name><surname>Merel</surname><given-names>Andrew</given-names></name><name><surname>Lefrancq</surname><given-names /></name></person-group></element-citation></ref><ref id="b30"><element-citation publication-type="journal"><article-title>Do- main randomization for transferring deep neural networks from simulation to the real world</article-title><source>Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on</source><fpage>23</fpage><lpage>30</lpage><person-group person-group-type="author"><name><surname>Tobin</surname><given-names>Josh</given-names></name><name><surname>Fong</surname><given-names>Rachel</given-names></name><name><surname>Ray</surname><given-names>Alex</given-names></name><name><surname>Schneider</surname><given-names>Jonas</given-names></name><name><surname>Zaremba</surname><given-names>Wojciech</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name></person-group></element-citation></ref><ref id="b31"><element-citation publication-type="journal"><article-title>Mujoco: A physics engine for model-based control</article-title><source>Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on</source><fpage>5026</fpage><lpage>5033</lpage><person-group person-group-type="author"><name><surname>Todorov</surname><given-names>Emanuel</given-names></name><name><surname>Erez</surname><given-names>Tom</given-names></name><name><surname>Tassa</surname><given-names>Yuval</given-names></name></person-group></element-citation></ref></ref-list></back></article>