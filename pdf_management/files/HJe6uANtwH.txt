Title:
```
Published as a conference paper at ICLR 2020 CAPSULES WITH INVERTED DOT-PRODUCT ATTENTION ROUTING
```
Abstract:
```
We introduce a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the par- ent's state and the child's vote. The new mechanism 1) designs routing via inverted dot-product attention; 2) imposes Layer Normalization as normal- ization; and 3) replaces sequential iterative routing with concurrent iterative routing. When compared to previously proposed routing algorithms, our method improves performance on benchmark datasets such as CIFAR-10 and CIFAR-100, and it performs at-par with a powerful CNN (ResNet-18) with 4× fewer parameters. On a different task of recognizing digits from over- layed digit images, the proposed capsule model performs favorably against CNNs given the same number of layers and neurons per layer. We believe that our work raises the possibility of applying capsule networks to complex real-world tasks. Our code is publicly available at: https://github. com/apple/ml-capsules-inverted-attention-routing. An alternative implementation is available at: https://github.com/ yaohungt/Capsules-Inverted-Attention-Routing/blob/ master/README.md.
```

Figures/Tables Captions:
```
Figure 1: Illustration of a Capsule network with a backbone block, 3 convolutional capsule layers, 2 fully- connected capsule layers, and a classifier. The first convolutional capsule layer is called the primary capsule layer. The last fully-connected capsule layer is called the class capsule layer.
Figure 2: Illustration of the Inverted Dot-Product Attention Routing with the pose admitting matrix structure.
Figure 3: Illustration of the proposed concurrent routing from iteration 2 to t with the example in Figure 1. The concurrent routing is a parallel-in-time routing procedure for all capsules layers.
Figure 4: Convergence analysis for CapsNets on CIFAR-10 with simple backbone model. Top: convergence plots for different routing mechanisms. Bottom left: classification results with respect to different routing iterations. Inverted Dot-Product Attention-A denotes our routing approach without Layer Normalization. In- verted Dot-Product Attention-B denotes our routing approach with sequential routing. Inverted Dot-Product Attention-C denotes our routing approach with activations in capsules. * indicates a uniform prediction. Bot- tom right: memory usage and inference time for the proposed Inverted Dot-Product Attention Routing. For fairness, the numbers are benchmarked using the same 8-GPU machine with batch size 128. Note that for fu- ture comparisons, we refer the readers to an alternative implementation for our model: https://github. com/yaohungt/Capsules-Inverted-Attention-Routing/blob/master/README.md. It uses much less memory, has a significantly faster inference speed, and retains the same performance.
Figure 5: Table and convergence plot for baseline CNN and CapsNets with different pose structures.
Table 1: Classification results on CIFAR-10/CIFAR-100 without ensembling models. We report the best performance for CapsNets when considering 1 to 5 routing iterations. We report the performance from the best test model for baseline routing methods, our routing method, and ResNet (He et al., 2016).
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Capsule Networks (CapsNets) represent visual features using groups of neurons. Each group (called a "capsule") encodes a feature and represents one visual entity. Grouping all the information about one entity into one computational unit makes it easy to incorporate priors such as "a part can belong to only one whole" by routing the entire part capsule to its parent whole capsule. Routing is mutually exclusive among parents, which ensures that one part cannot belong to multiple parents. Therefore, capsule routing has the potential to produce an interpretable hierarchical parsing of a visual scene. Such a structure is hard to impose in a typical convolutional neural network (CNN). This hierarchical relationship modeling has spurred a lot of interest in designing capsules and their routing algorithms ( Sabour et al., 2017 ;  Hinton et al., 2018 ;  Wang & Liu, 2018 ;  Zhang et al., 2018 ;  Li et al., 2018 ;  Rajasegaran et al., 2019 ;  Kosiorek et al., 2019 ). In order to do routing, each lower-level capsule votes for the state of each higher-level capsule. The higher-level (parent) capsule aggregates the votes, updates its state, and uses the updated state to explain each lower-level capsule. The ones that are well-explained end up routing more towards that parent. This process is repeated, with the vote aggregation step taking into account the extent to which a part is routed to that parent. Therefore, the states of the hidden units and the routing probabilities are inferred in an iterative way, analogous to the M-step and E-step, respectively, of an Expectation-Maximization (EM) algorithm. Dynamic Routing ( Sabour et al., 2017 ) and EM- routing ( Hinton et al., 2018 ) can both be seen as variants of this scheme that share the basic iterative structure but differ in terms of details, such as their capsule design, how the votes are aggregated, and whether a non-linearity is used. We introduce a novel routing algorithm, which we called Inverted Dot-Product Attention Routing. In our method, the routing procedure resembles an inverted attention mechanism, where dot products are used to measure agreement. Specifically, the higher-level (parent) units compete for the attention of the lower-level (child) units, instead of the other way around, which is commonly used in attention Published as a conference paper at ICLR 2020 models. Hence, the routing probability directly depends on the agreement between the parent's pose (from the previous iteration step) and the child's vote for the parent's pose (in the current iteration step). We also propose two modifications for our routing procedure - (1) using Layer Normalization ( Ba et al., 2016 ) as normalization, and (2) doing inference of the latent capsule states and routing probabilities jointly across multiple capsule layers (instead of doing it layer-wise). These modifications help scale up the model to more challenging datasets. Our model achieves comparable performance as the state-of-the-art convolutional neural networks (CNNs), but with much fewer parameters, on CIFAR-10 (95.14% test accuracy) and CIFAR-100 (78.02% test accuracy). We also introduce a challenging task to recognize single and multiple over- lapping objects simultaneously. To be more precise, we construct the DiverseMultiMNIST dataset that contains both single-digit and overlapping-digits images. With the same number of layers and the same number of neurons per layer, the proposed CapsNet has better convergence than a baseline CNN. Overall, we argue that with the proposed routing mechanism, it is no longer impractical to apply CapsNets on real-world tasks. We will release the source code to reproduce the experiments.

Section Title: CAPSULE NETWORK ARCHITECTURE
  CAPSULE NETWORK ARCHITECTURE An example of our proposed architecture is shown in  Figure 1 . The backbone is a standard feed- forward convolutional neural network. The features extracted from this network are fed through another convolutional layer. At each spatial location, groups of 16 channels are made to create capsules (we assume a 16-dimensional pose in a capsule). LayerNorm is then applied across the 16 channels to obtain the primary capsules. This is followed by two convolutional capsule layers, and then by two fully-connected capsule layers. In the last capsule layer, each capsule corresponds to a class. These capsules are then used to compute logits that feed into a softmax to computed the classification probabilities. Inference in this network requires a feed-forward pass up to the primary capsules. After this, our proposed routing mechanism (discussed in the next section) takes over. In prior work, each capsule has a pose and some way of representing an activation probability. In Dynamic Routing CapsNets ( Sabour et al., 2017 ), the pose is represented by a vector and the activa- tion probability is implicitly represented by the norm of the pose. In EM Routing CapsNets ( Hinton et al., 2018 ), the pose is represented by a matrix and the activation probability is determined by the EM algorithm. In our work, we consider a matrix-structured pose in a capsule. We denote the capsules in layer L as P L and the i-th capsule in layer L as p L i . The pose p L i ∈ R d L in a vector form and will be reshaped to R √ d L × √ d L when representing it as a matrix, where d L is the number of hidden units grouped together to make capsules in layer L. The activation probability is not ex- plicitly represented. By doing this, we are essentially asking the network to represent the absence of a capsule by some special value of its pose.

Section Title: INVERTED DOT-PRODUCT ATTENTION ROUTING
  INVERTED DOT-PRODUCT ATTENTION ROUTING The proposed routing process consists of two steps. The first step computes the agreement between lower-level capsules and higher-level capsules. The second step updates the pose of the higher-level capsules.

Section Title: Step 1: Computing Agreement
  Step 1: Computing Agreement where the matrix W L ij ∈ R d L+1 ×d L if the pose has a vector structure and W L ij ∈ R √ d L+1 × √ d L (requires d L+1 = d L ) if the pose has a matrix structure. Next, the agreement (a L ij ) is computed by the dot-product similarity between a pose p L+1 j and a vote v L ij : The pose p L+1 j is obtained from the previous iteration of this procedure, and will be set to 0 initially. Step 2: Computing Poses: The agreement scores a L ij are passed through a softmax function to determine the routing probabilities r L ij : r L ij = exp(a L ij ) j exp(a L ij ) , (3) where r L ij is an inverted attention score representing how higher-level capsules compete for attention of lower-level capsules. Using the routing probabilities, we update the pose p L+1 j for capsule j in layer L + 1 from all capsules in layer L: We adopt Layer Normalization ( Ba et al., 2016 ) as the normalization, which we empirically find it to be able to improve the convergence for routing. The routing algorithm is summarized in Proce- dure 1 and  Figure 2 .

Section Title: INFERENCE AND LEARNING
  INFERENCE AND LEARNING To explain how inference and learning are performed, we use  Figure 1  as an example. Note that the choice of the backbone, the number of capsules layers, the number of capsules per layer, the design of the classifier may vary for different sets of experiments. We leave the discussions of configurations in Sections 5 and 6, and in the Appendix.

Section Title: INFERENCE
  INFERENCE For ease of exposition, we decompose a CapsNet into pre-capsule, capsule and post-capsule layers.

Section Title: Pre-Capsule Layers
  Pre-Capsule Layers The goal is to obtain a backbone feature F from the input image I. The backbone model can be either a single convolutional layer or ResNet computational blocks ( He et al., 2016 ). Capsule Layers: The primary capsules P 1 are computed by applying a convolution layer and Layer Normalization to the backbone feature F. The non-primary capsules layers P 2:N are ini- tialized to be zeros 1 . For the first iteration, we perform one step of routing sequentially in each capsule layer. In other words, the primary capsules are used to update their parent convolutional capsules, which are then used to update the next higher-level capsule layer, and so on. After doing this first pass, the rest of the routing iterations are performed concurrently. Specifically, all capsule layers look at their preceding lower-level capsule layer and perform one step of routing simultane- ously. This procedure is an example of a parallel-in-time inference method. We call it "concurrent routing" as it concurrently performs routing between capsules layers per iteration, leading to bet- ter parallelism.  Figure 3  illustrates this procedure from routing iteration 2 to t. It is worth noting that, our proposed variant of CapsNet is a weight-tied concurrent routing architecture with Layer Normalization, which  Bai et al. (2019)  empirically showed could converge to fixed points. Previous CapsNets ( Sabour et al., 2017 ;  Hinton et al., 2018 ) used sequential layer-wise iterative routing between the capsules layers. For example, the model first performs routing between layer L − 1 and layer L for a few iterations. Next, the model performs routing between layer L and L + 1 for a few iterations. When unrolled, this sequential iterative routing defines a very deep computational graph with a single path going from the inputs to the outputs. This deep graph could lead to a vanishing gradients problem and limit the depth of a CapsNet that can be trained well, especially if any squashing non-linearities are present. With concurrent routing, the training can be made more stable, since each iteration has a more cumulative effect.

Section Title: Post-Capsule Layers
  Post-Capsule Layers

Section Title: LEARNING
  LEARNING We update the parameters θ, W 1:N −1 by stochastic gradient descent. For multiclass classification, we use multiclass cross-entropy loss. For multilabel classification, we use binary cross-entropy loss. We also tried Margin loss and Spread loss which are introduced by prior work ( Sabour et al., 2017 ;  Hinton et al., 2018 ). However, these losses do not give us better performance against cross-entropy and binary cross-entropy losses.

Section Title: COMPARISONS WITH EXISTING CAPSNET MODELS
  COMPARISONS WITH EXISTING CAPSNET MODELS Having described our model in detail, we can now place the model in the context of previous work. In the following table, we list the major differences among different variants of CapsNets.

Section Title: Comparisons with other CapsNets and CNNs
  Comparisons with other CapsNets and CNNs In  Table 1 , we report the test accuracy obtained by our model, along with other CapsNets and CNNs. Two prior CapsNets are chosen: Dynamic Routing CapsNets ( Sabour et al., 2017 ) and EM Routing CapsNets ( Hinton et al., 2018 ). For each CapsNet, we apply two backbone feature models: simple convolution followed by ReLU nonlinear activation and a ResNet ( He et al., 2016 ) backbone. For CNNs, we consider a baseline CNN with 3 convolutional layers followed by 1 fully-connected classifier layer. ResNet-18 is selected as a representative of SOTA CNNs. See Appendix A.1 for detailed configurations. First, we compare previous routing approaches against ours. In a general trend, the proposed Cap- sNets perform better than the Dynamic Routing CapsNets, and the Dynamic Routing CapsNets perform better than EM Routing CapsNets. The performance differs more on CIFAR-100 than on CIFAR-10. For example, with simple convolutional backbone, EM Routing CapsNet can only achieve 37.73% test accuracy while ours can achieve 57.32%. Additionally, for all CapsNets, we see improved performance when replacing a single convolutional backbone with ResNet backbone. This result is not surprising since ResNet structure has better generalizability than a single convo- lutional layer. For the number of parameters, ours and EM Routing CapsNets have much fewer as compared to Dynamic Routing CapsNets. The reason is due to different structures of capsule's pose. Ours and EM Routing CapsNets have matrix-structure poses, and Dynamic Routing CapsNets have vector-structure poses. With matrix structure, weights between capsules are only O(d) with d being pose's dimension; with vector structure, weights are O(d 2 ). To conclude, combining the proposed Inverted Dot-Product Attention Routing with ResNet backbone gives us both the advantages of a low number of parameters and high performance. Second, we discuss the performance difference between CNNs and CapsNets. We see that, with a simple backbone (a single convolutional layer), it is hard for CapsNets to reach the same per- formance as CNNs. For instance, our routing approach can only achieve 57.32% test accuracy on CIFAR-100 while the baseline CNN achieves 62.30%. However, with a SOTA backbone structure (ResNet backbone), the proposed routing approach can reach competitive performance (95.14% on CIFAR-10) as compared to the SOTA CNN model (ResNet-18 with 95.11% on CIFAR-10).

Section Title: Convergence Analysis
  Convergence Analysis In  Figure 4 , top row, we analyze the convergence for CapsNets with respect to the number of routing iterations. The optimization hyperparameters are chosen optimally for each routing mechanism. For Dynamic Routing CapsNets ( Sabour et al., 2017 ), we observe a mild performance drop when the number of iterations increases. For EM Routing CapsNets ( Hinton et al., 2018 ), the best-performed number of iterations is 2. Increasing or decreasing this number severely hurts the performance. For our proposed routing mechanism, we find a positive correlation between performance and number of routing iterations. The performance variance is also the smallest among Published as a conference paper at ICLR 2020 We consider the same optimizer, the same number of layers, and the same number of neurons per layer for the models. For fairness, memory usage/ inference time are benchmarked using the same 8-GPU machine with batch size 128. CapsNet * denotes the Dynamic routing method and CapsNet denotes our proposed Inverted Dot-Product Attention Routing method. Note that for future comparisons, we re- fer the readers to an alternative implementation for our model: https://github.com/yaohungt/ Capsules-Inverted-Attention-Routing/blob/master/README.md. It uses much less mem- ory, has a significantly faster inference speed, and retains the same performance. The memory usage and inference time in this implementation now are only marginally higher than the baseline CNN. the three routing mechanisms. This result suggests our approach has better optimization and stable inference. However, selecting a larger iteration number may not be ideal since memory usage and inference time will also increase (shown in the bottom right in  Figure 4 ). Note that, we observe sharp performance jitters during training when the model has not converged (especially when the number of iterations is high). This phenomenon is due to applying LayerNorm on a low-dimensional vector. The jittering is reduced when we increase the pose dimension in capsules.

Section Title: Ablation Study
  Ablation Study Furthermore, we inspect our routing approach with the following ablations: 1) In- verted Dot-Product Attention-A: without Layer Normalization; 2) Inverted Dot-Product Attention- B: replacing concurrent to sequential iterative routing; and 3) Inverted Dot-Product Attention-C: adding activations in capsules 2 . The results are presented in  Figure 4  bottom row. When remov- ing Layer Normalization, performance dramatically drops from our routing mechanism. Notably, the prediction becomes uniform when the iteration number increases to 5. This result implies that the normalization step is crucial to the stability of our method. When replacing concurrent with sequential iterative routing, the positive correlation between performance and iteration number no longer exists. This fact happens in the Dynamic Routing CapsNet as well, which also uses sequential iterative routing. When adding activations to our capsule design, we obtain a performance deterio- ration. Typically, squashing activations such as sigmoids make it harder for gradients to flow, which might explain this. Discovering the best strategy to incorporate activations in capsule networks is an interesting direction for future work.

Section Title: EXPERIMENTS ON DIVERSEMULTIMNIST
  EXPERIMENTS ON DIVERSEMULTIMNIST The goal in this section is to compare CapsNets and CNNs when they have the same number of layers and the same number of neurons per layer. Specifically, we would like to examine the dif- ference of the representation power between the routing mechanism (in CapsNets) and the pooling operation (in CNNs). A challenging setting is considered in which objects may be overlapping with each other, and there may be a diverse number of objects in the image. To this end, we construct the DiverseMultiMNIST dataset which is extended from MNIST ( LeCun et al., 1998 ), and it contains both single-digit and two overlapping digit images. The task will be multilabel classification, where the prediction is said to be correct if and only if the recognized digits match all the digits in the image. We plot the convergence curve when the model is trained on 21M images from DiverseMul- tiMNIST. Please see Appendix B.2 for more details on the dataset and Appendix B.1 for detailed model configurations. The results are reported in  Figure 5 . First, we compare our routing method against the Dynamic routing one. We observe an improved performance from the CapsNet * to the CapsNet (83.39% to 85.74% with vector-structured poses). The result suggests a better viewpoint generalization for our routing mechanism. Second, we compare baseline CNN against our CapsNet. From the table, we see that CapsNet has better test accuracy compared to CNN. For example, the CapsNet with vector-structured poses reaches 85.74% test accuracy, and the baseline CNN reaches 79.81% test accuracy. In our CNN implementation, we use average pooling from the last convolutional layer to its next fully-connected layer. We can see that having a routing mechanism works better than pooling. However, one may argue that the pooling operations requires no extra parameter but routing mechanism does, and hence it may not be fair to compare their performance. To address this issue, in the baseline CNN, we replace the pooling operation with a fully-connected operation. To be more precise, instead of using average pooling, we learn the entire transformation matrix from the last convolutional layer to its next fully-connected layer. This procedure can be regarded as considering pooling with learnable parameters. After doing this, the number of parameters in CNN increases to 42.49M , and the corresponding test accuracy is 84.84%, which is still lower than 85.74% from the CapsNet. We conclude that, when recognizing overlapping and diverse number of objects, the routing mechanism has better representation power against the pooling operation. Last, we compare CapsNet with different pose structures. The CapsNet with vector-structured poses works better than the CapsNet with matrix-structured poses (80.59% vs 85.74%). However, the former requires more parameters, more memory usage, and more inference time. If we increase the number of parameters in the matrix-pose CapsNet to 42M , its test accuracy rises to 91.17%. Nevertheless, the model now requires more memory usage and inference time as compared to using vector-structured poses. We conclude that more performance can be extracted from vector-structured poses but at the cost of high memory usage and inference time.

Section Title: RELATED WORK
  RELATED WORK The idea of grouping a set of neurons into a capsule was first proposed in Transforming Auto- Encoders ( Hinton et al., 2011 ). The capsule represented the multi-scale recognized fragments of the input images. Given the transformation matrix, Transforming Auto-Encoders learned to dis- cover capsules' instantiation parameters from an affine-transformed image pair.  Sabour et al. (2017)  extended this idea to learn part-whole relationships in images systematically.  Hinton et al. (2018)  cast the routing mechanism as fitting a mixture of Gaussians. The model demonstrated an im- pressive ability for recognizing objects from novel viewpoints. Recently, Stacked Capsule Auto- Encoders ( Kosiorek et al., 2019 ) proposed to segment and compose the image fragments without any supervision. The work achieved SOTA results on unsupervised classification. However, despite showing promising applications by leveraging inherent structures in images, the current literature on capsule networks has only been applied on datasets of limited complexity. Our proposed new routing mechanism instead attempts to apply capsule networks to more complex data. Our model also relates to Transformers ( Vaswani et al., 2017 ) and Set Transformers ( Lee et al., 2019 ), where dot-product attention is also used. In the language of capsules, a Set Transformer can be seen as a model in which a higher-level unit can choose to pay attention to K lower-level units (using K attention heads). Our model inverts the attention direction (lower-level units "attend" to parents), enforces exclusivity among routing to parents and does not impose any limits on how many lower-level units can be routed to any parent. Therefore, it combines the ease and parallelism of dot-product routing derived from a Transformer, with the interpretability of building a hierarchical parsing of a scene derived from capsule networks. There are other works presenting different routing mechanisms for capsules.  Wang & Liu (2018)  formulated the Dynamic routing ( Sabour et al., 2017 ) as an optimization problem consisting of a clustering loss and a KL regularization term.  Zhang et al. (2018)  generalized the routing method within the framework of weighted kernel density estimation.  Li et al. (2018)  approximated the routing process with two branches and minimized the distributions between capsules layers by an optimal transport divergence constraint.  Phaye et al. (2018)  replaced standard convolutional struc- tures before capsules layers by densely connected convolutions. It is worth noting that this work was the first to combine SOTA CNN backbones with capsules layers.  Rajasegaran et al. (2019)  pro- posed DeepCaps by stacking 10+ capsules layers. It achieved 92.74% test accuracy on CIFAR-10, which was the previous best for capsule networks. Instead of looking for agreement between cap- sules layers,  Choi et al. (2019)  proposed to learn deterministic attention scores only from lower-level capsules. Nevertheless, without agreement, their best-performed model achieved only 88.61% test accuracy on CIFAR-10. In contrast to these prior work, we present a combination of inverted dot- product attention routing, layer normalization, and concurrent routing. To the best of our knowledge, we are the first to show that capsule networks can achieve comparable performance against SOTA CNNs. In particular, we achieve 95.14% test accuracy for CIFAR-10 and 78.02% for CIFAR-100.

Section Title: CONCLUSION AND FUTURE WORK
  CONCLUSION AND FUTURE WORK In this work, we propose a novel Inverted Dot-Product Attention Routing algorithm for Capsule networks. Our method directly determines the routing probability by the agreements between parent and child capsules. Routing algorithms from prior work require child capsules to be explained by parent capsules. By removing this constraint, we are able to achieve competitive performance against SOTA CNN architectures on CIFAR-10 and CIFAR-100 with the use of a low number of parameters. We believe that it is no longer impractical to apply capsule networks to datasets with complex data distribution. Two future directions can be extended from this paper: • In the experiments, we show how capsules layers can be combined with SOTA CNN back- bones. The optimal combinations between SOTA CNN structures and capsules layers may be the key to scale up to a much larger dataset such as ImageNet. • The proposed concurrent routing is as a parallel-in-time and weight-tied inference process. The strong connection with Deep Equilibrium Models ( Bai et al., 2019 ) can potentially lead us to infinite-iteration routing.
  The configuration choices of Dynamic Routing CapsNets and EM Routing CapsNets are followed by prior work ( Sabour et al., 2017 ;  Hinton et al., 2018 ). We empirically find their configurations perform the best for their routing mechanisms (instead of applying our network configurations to their routing mechanisms). The optimizers are chosen to reach the best performance for all models. We list the model specifications in Table 2, 3, 4, 5, 6, 7, 8, and 9. We only show the specifications for CapsNets with a simple convolutional backbone. When con- sidering a ResNet backbone, two modifications are performed. First, we replace the simple feature backbone with ResNet feature backbone. Then, the input dimension of the weights after the back- bone is set as 128. A ResNet backbone contains a 3 × 3 convolutional layer (output 64-dim.), three 64-dim. residual building block ( He et al., 2016 ) with stride 1, and four 128-dim. residual building block with stride 2. The ResNet backbone returns a 16 × 16 × 128 tensor. For the optimizers, we use stochastic gradient descent with learning rate 0.1 for our proposed method, baseline CNN, and ResNet-18 ( He et al., 2016 ). We use Adam (Kingma & Ba, 2014) with learning rate 0.001 for Dynamic Routing CapsNets and Adam with learning rate 0.01 for EM Routing CapsNets. We decrease the learning rate by 10 times when the model trained on 150 epochs and 250 epochs, and there are 350 epochs in total.

Section Title: A.2 DATA AUGMENTATIONS
  A.2 DATA AUGMENTATIONS We consider the same data augmentation for all networks. During training, we first pad four zero- value pixels to each image and randomly crop the image to the size 32 × 32. Then, we horizontally flip the image with probability 0.5. During evaluation, we do not perform data augmentation. All the model is trained on a 8-GPU machine with batch size 128.

Section Title: B MODEL CONFIGURATIONS FOR DIVERSE MULTIMNIST
  B MODEL CONFIGURATIONS FOR DIVERSE MULTIMNIST

Section Title: B.1 MODEL SPECIFICATIONS
  B.1 MODEL SPECIFICATIONS To fairly compare CNNs and CapsNets, we fix the number of layers and the number of neurons per layer in the models. These models consider the design: 36x36 image → 18x18x1024 neurons → 8x8x1024 neurons → 6x6x1024 neurons → 640 neurons → 10 class logits. The configurations are presented in Table 10, 11, and 12. We also fix the optimizers across all the models. We use stochastic gradient descent with learning rate 0.1 and decay the learning rate by 10 times when the models trained on 150 steps and 250 steps. One step corresponds to 60, 000 training samples, and we train the models with a total of 350 steps.

```
