Title:
```
Published as a conference paper at ICLR 2020 GRAPH CONSTRAINED REINFORCEMENT LEARNING FOR NATURAL LANGUAGE ACTION SPACES
```
Abstract:
```
Interactive Fiction games are text-based simulations in which an agent interacts with the world purely through natural language. They are ideal environments for studying how to extend reinforcement learning agents to meet the challenges of natural language understanding, partial observability, and action generation in combinatorially-large text-based action spaces. We present KG-A2C 1 , an agent that builds a dynamic knowledge graph while exploring and generates actions us- ing a template-based action space. We contend that the dual uses of the knowledge graph to reason about game state and to constrain natural language generation are the keys to scalable exploration of combinatorially large natural language actions. Results across a wide variety of IF games show that KG-A2C outperforms current IF agents despite the exponential increase in action space size.
```

Figures/Tables Captions:
```
Figure 1: The full KG-A2C architecture. Solid lines represent computation flow along which the gradient can be back-propagated.
Figure 2: An overall example of the knowledge graph building and subsequent action decoding process for a given state in Zork1, illustrating the use of interactive objects and the graph mask.
Figure 3: Ablation results on Zork1, averaged across 5 independent runs.
Table 1: Raw scores comparing KG-A2C to TDQN across a wide set of games supported by Jericho. † Advent starts at a score of 36.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Natural language communication has long been considered a defining characteristic of human in- telligence. We are motivated by the question of how learning agents can understand and generate contextually relevant natural language in service of achieving a goal. In pursuit of this objective we study Interactive Fiction (IF) games, or text-adventures: simulations in which an agent interacts with the world purely through natural language-"seeing" and "talking" to the world using textual descriptions and commands. To progress in these games, an agent must generate natural language actions that are coherent, contextually relevant, and able to effect the desired change in the world. Complicating the problem of generating contextually relevant language in these games is the issue of partial observability: the fact that the agent never has access to the true underlying world state. IF games are structured as puzzles and often consist of an complex, interconnected web of distinct locations, objects, and characters. The agent needs to thus reason about the complexities of such a world solely through the textual descriptions that it receives, descriptions that are often incomplete. Further, an agent must be able to perform commonsense reasoning-IF games assume that human players possess prior commonsense and thematic knowledge-e.g. knowing that swords can kill trolls or that trolls live in dark places. Knowledge graphs provide us with an intuitive way of rep- resenting these partially observable worlds. Prior works have shown how using knowledge graphs aid in the twin issues of partial observability ( Ammanabrolu & Riedl, 2019a ) and commonsense reasoning ( Ammanabrolu & Riedl, 2019b ), but do not use them in the context of generating natural language. To gain a sense for the challenges surrounding natural language generation, we need to first un- derstand how large this space really is. In order to solve solve a popular IF game such as Zork1 it's necessary to generate actions consisting of up to five-words from a relatively modest vocab- ulary of 697 words recognized by Zork's parser. Even this modestly sized vocabulary leads to O(697 5 ) = 1.64 × 10 14 possible actions at every step-a dauntingly-large combinatorially-sized action space for a learning agent to explore. In order to reduce the size of this space while maintain- ing expressiveness,  Hausknecht et al. (2019a)  propose the use of template-actions in which the agent first selects a template (e.g. [put] [in] ) then fills in the blanks using vocabulary words. There are 237 templates in Zork1, each with up to two blanks, yielding a template-action space of size Published as a conference paper at ICLR 2020 O(237 × 697 2 ) = 1.15 × 10 8 . This space is six orders of magnitude smaller than the word-based space, but still six orders of magnitude larger than the action spaces used by previous text-based agents ( Narasimhan et al., 2015 ;  Zahavy et al., 2018 ). We demonstrate how these templates provide the structure required to further constrain our action space via our knowledge graph-and make the argument that the combination of these approaches allows us to generate meaningful natural language commands. Our contributions are as follows: We introduce an novel agent that utilizes both a knowledge graph based state space and template based action space and show how to train such an agent. We then conduct an empirical study evaluating our agent across a diverse set of IF games followed by an ablation analysis studying the effectiveness of various components of our algorithm as well as its overall generalizability. Remarkably we show that our agent achieves state-of-the-art performance on a large proportion of the games despite the exponential increase in action space size.

Section Title: RELATED WORK
  RELATED WORK We examine prior work in three broad categories: text-based game playing agents and frameworks as well as knowledge graphs used for natural language generation and game playing agents. LSTM-DQN ( Narasimhan et al., 2015 ), considers verb-noun actions up to two-words in length. Separate Q-Value estimates are produced for each possible verb and object, and the action consists of pairing the maximally valued verb combined with the maximally valued object. The DRRN algorithm for choice-based games ( He et al., 2016 ;  Zelinka, 2018 ) estimates Q-Values for a particular action from a particular state.  Fulda et al. (2017)  use Word2Vec ( Mikolov et al., 2013 ) to aid in extracting affordances for items in these games and use this information to produce relevant action verbs.  Zahavy et al. (2018)  reduce the combinatorially-sized action space into a discrete form using a walkthrough of the game and introduce the Action Elimination DQN, which learns to eliminate actions unlikely to cause a world change.  Côté et al. (2018)  introduce TextWorld, a framework for procedurally generating parser-based games, allowing a user to control the difficulty of a generated game. Yuan et al. (2019)  intro- duce the concept of interactive question-answering in the form of QAit-modeling QA tasks in TextWorld.  Urbanek et al. (2019)  introduce Light, a dataset of crowdsourced text-adventure game dialogs focusing on giving collaborative agents the ability to generate contextually relevant dialog and emotes.  Hausknecht et al. (2019a)  have open-sourced Jericho 2 , an optimized interface for play- ing human-made IF games-formalizing this task. They further provide a comparative study of various types of agents on their set of games, testing the performance of heuristic based agents such as NAIL ( Hausknecht et al., 2019b ) and various reinforcement learning agents are benchmarked. We use Jericho and the tools that it provides to develop our agents. Knowledge graphs have been shown to be useful representations for a variety of tasks surround- ing natural language generation and interactive fiction.  Ghazvininejad et al. (2017)  and  Guan et al. (2018)  effectively use knowledge graph representations to improve neural conversational and story ending prediction models respectively.  Ammanabrolu et al. (2019)  explore procedural content gen- eration in text-adventure games-looking at constructing a quest for a given game world, and use knowledge graphs to ground generative systems trained to produce quest content. From the perspec- tive of text-game playing agent and most in line with the spirit of our work,  Ammanabrolu & Riedl (2019a)  present the Knowledge Graph DQN or KG-DQN, an approach where a knowledge graph built during exploration is used as a state representation for a deep reinforcement learning based agent.  Ammanabrolu & Riedl (2019b)  further expand on this work, exploring methods of transfer- ring control policies in text-games, using knowledge graphs to seed an agent with useful common- sense knowledge and to transfer knowledge between different games within a domain. Both of these works, however, identify a discrete set of actions required to play the game beforehand and so do not fully tackle the issue of the combinatorial action space.

Section Title: STATE AND ACTION SPACES
  STATE AND ACTION SPACES Formally, IF games are partially observable Markov decision processes (POMDP), represented as a 7-tuple of S, T, A, Ω, O, R, γ representing the set of environment states, mostly deterministic conditional transition probabilities between states, the vocabulary or words used to compose text commands, observations returned by the game, observation conditional probabilities, reward func- tion, and the discount factor respectively ( Côté et al., 2018 ;  Hausknecht et al., 2019a ). To deal with the resulting twin challenges of partial observability and combinatorial actions, we use a knowledge graph based state space and a template-based action space-each described in detail below. Knowledge Graph State Space. Building on  Ammanabrolu & Riedl (2019a) , we use a knowledge graph as a state representation that is learnt during exploration. The knowledge graph is stored as a set of 3-tuples of subject, relation, object . These triples are extracted from the observations using Stanford's Open Information Extraction (OpenIE) ( Angeli et al., 2015 ). Human-made IF games often contain relatively complex semi-structured information that OpenIE is not designed to parse and so we add additional rules to ensure that we are parsing the relevant information. Updated after every action, the knowledge graph helps the agent form a map of the world that it is exploring, in addition to retaining information that it has learned such as the affordances associated with an object, the properties of a character, current inventory, etc. Nodes relating to such informa- tion are shown on the basis of their relation to the agent which is presented on the graph using a "you" node (see example in Fig. 2a).  Ammanabrolu & Riedl (2019a)  build a knowledge graph in a similar manner but restrict themselves to a single domain. In contrast, we test our methods on a much more diverse set of games defined in the Jericho framework ( Hausknecht et al., 2019a ). These games are each structured differently- covering a wider variety of genres-and so to be able to extract the same information from all of them in a general manner, we relax many of the rules found in  Ammanabrolu & Riedl (2019a) . To aid in the generalizability of graph building, we introduce the concept of interactive objects-items that an agent is able to directly interact with in the surrounding environment. These items are directly linked to the "you" node, indicating that the agent can interact with them, and the node for the current room, showing their relative position. All other triples built from the graph are extracted by OpenIE. Further details regarding knowledge graph updates are found in Appendix B.1 An example of a graph built using these rules is seen in Fig. 2a.

Section Title: Template Action Space
  Template Action Space Templates are subroutines used by the game's parser to interpret the player's action. They consist of interchangeable verbs phrases (V P ) optionally followed by prepo- sitional phrases (V P P P ), e.g. ([carry/hold/take] ) and ([drop/throw/discard/put] [at/against/on/onto] ), where the verbs and prepositions within [.] are aliases. As shown in Figure 2b, actions may be constructed from templates by filling in the template's blanks using words in the game's vocabulary. Templates and vocabulary words are programmatically accessible through the Jericho framework and are thus available for every IF game. Further details about how we prioritize interchangeable verbs and prepositions are available in Appendix B.2.

Section Title: KNOWLEDGE GRAPH ADVANTAGE ACTOR CRITIC
  KNOWLEDGE GRAPH ADVANTAGE ACTOR CRITIC Combining the knowledge-graph state space with the template action space, Knowledge Graph Ad- vantage Actor Critic or KG-A2C, is an on-policy reinforcement learning agent that collects experi- ence from many parallel environments. We first discuss the architecture of KG-A2C, then detail the training algorithm. As seen in  Fig. 1 , KG-A2C's architecture can broadly be described in terms of encoding a state representation and then using this encoded representation to decode an action. We describe each of these processes below.

Section Title: Input Representation
  Input Representation sists of narrative and flavor text. The inventory o tinv and previous action a t−1 components inform the agent about the contents of its inventory and the last action taken respectively. The observation encoder processes each component of o t using a separate GRU encoder. As we are not given the vocabulary that o t is comprised of, we use subword tokenization-specifically using the unigram subword tokenization method described in  Kudo & Richardson (2018) . This method predicts the most likely sequence of subword tokens for a given input using a unigram language model which, in our case, is trained on a dataset of human playthroughs of IF games 3 and contains a total vocabulary of size 8000. For each of the GRUs, we pass in the final hidden state of the GRU at step t − 1 to initialize the hidden state at step t. We concatenate each of the encoded components and use a linear layer to combine them into the final encoded observation o t . At each step, we update our knowledge graph G t using o t as described in Sec. 3 and it is then embedded into a single vector g t . Following  Ammanabrolu & Riedl (2019a)  we use Graph Attention networks or GATs ( Veličković et al., 2018 ) with an attention mechanism similar to that described in  Bahdanau et al. (2014) . Node features are computed as H = {h 1 , h 2 , . . . , h N }, h i ∈ IR F , where N is the number of nodes and F the number of features in each node, consist of the average subword embeddings of the entity and of the relations for all incoming edges using our unigram language model. Self-attention is then used after a learnable linear transformation W ∈ IR 2F×F applied to all the node features. Attention coefficients α ij are then computed by softmaxing k ∈ N with N being the neighborhood in which we compute the attention coefficients and consists of all edges in G t . where p ∈ IR 2F is a learnable parameter. The final knowledge graph embedding vector g t is computed as: g t = f (W g ( K k=1 σ( j∈N α (k) ij W (k) h j )) + b g ) (3) where k refers to the parameters of the k th independent attention mechanism, W g and b g the weights and biases of the output linear layer, and represents concatenation. The final component of state embedding vector is a binary encoding c t of the total score obtained so far in the game-giving the agent a sense for how far it has progressed in the game even when it is not collecting reward. The state embedding vector is then calculated as s t = g t ⊕ o t ⊕ c t .

Section Title: Action Decoder
  Action Decoder The state embedding vector s t is then used to sequentially construct an action by first predicting a template and then picking the objects to fill into the template using a series of Decoder GRUs. This gives rise to a template policy π T and a policy for each object π Oi . Architecture wise, at every decoding step all previously predicted parts of the action are encoded and passed along with s t through an attention layer which learns to attend over these representations-conditioning every predicted object on all the previously predicted objects and template. All the object decoder GRUs share parameters while the template decoder GRU T remains separate. To effectively constrain the space of template-actions, we introduce the concept of a graph mask, leveraging our knowledge graph at that timestep G t to streamline the object decoding process. For- mally, the graph mask m t = {o : o ∈ G t ∧ o ∈ V }, consists of all the entities found within the knowledge graph G t and vocabulary V and is applied to the outputs of the object decoder GRUs- restricting them to predict objects in the mask. Generally, in an IF game, it is impossible to interact with an object that you never seen or that are not in your inventory and so the mask lets us ex- plore the action space more efficiently. To account for cases where this assumption does not hold, i.e. when an object that the agent has never interacted with before must be referenced in order to progress in the game, we randomly add objects o ∈ V to m t with a probability p m . An example of the graph-constrained action decoding process is illustrated in Fig. 2b.

Section Title: TRAINING
  TRAINING We adapt the Advantage Actor Critic (A2C) method ( Mnih et al., 2016 ) to train our network, using multiple workers to gather experiences from the simulator, making several significant changes along the way-as described below.

Section Title: Valid Actions
  Valid Actions Using a template-action space there are millions of possible actions at each step. Most of these actions do not make sense, are ungrammatical, etc. and an even fewer number of them actually cause the agent effect change in the world. Without any sense for which actions present valid interactions with the world, the combinatorial action space becomes prohibitively large for effective exploration. We thus use the concept of valid actions, actions that can change the world in a particular state. These actions can usually be recognized through the game feedback, with responses like "Nothing happens" or "That phrase is not recognized." In practice, we follow  Hausknecht et al. (2019a)  and use the valid action detection algorithm provided by Jericho. Formally, V alid(s t ) = a 0 , a 1 ...a N and from this we can construct the corresponding set of valid templates T valid (s t ) = τ 0 , τ 1 ...τ N . We further define a set of valid objects O valid (s t ) = o 0 , o 1 ...o M which consists of all objects in the graph mask as defined in Sec. 4. This lets us introduce two cross-entropy loss terms to aid the action decoding process. The template loss given a particular state and current network parameters is applied to the decoder GRU T . Similarly, the object loss is applied across the decoder GRU O and is calculated by summing cross-entropy loss from all the object decoding steps. Updates. A2C training starts with calculating the advantage of taking an action in a state A(s t , a t ), defined as the value of taking an action Q(s t , a t ) compared to the average value of taking all possible valid actions in that state V (s t ): V (s t ) is predicted by the critic as shown in  Fig. 1  and r t is the reward received at step t. The action decoder or actor is then updated according to the gradient: updating the template policy π T and object policies π Oi based on the fact that each step in the action decoding process is conditioned on all the previously decoded portions. The critic is updated with respect to the gradient: 1 2 ∇ θ (Q(s t , a t ; θ t ) − V (s t ; θ t )) 2 (9) bringing the critic's prediction of the value of being in a state closer to its true underlying value. We further add an entropy loss over the valid actions, designed to prevent the agent from prematurely converging on a trajectory.

Section Title: EXPERIMENTAL RESULTS
  EXPERIMENTAL RESULTS The KG-A2C is tested on a suite of Jericho supported games and is compared to strong, established baselines. Additionally, as encouraged by  Hausknecht et al. (2019a) , we present the set of handicaps used by our agents: (1) Jericho's ability to identify valid actions and (2) the Load, Save handicap in order to acquire o t desc and o tinv using the look and inventory commands without changing the game state. Hyperparameters are provided in Appendix C.

Section Title: Template DQN Baseline
  Template DQN Baseline We compare KG-A2C against Template-DQN, a strong baseline also utilizing the template based action space. TDQN ( Hausknecht et al., 2019a ) is an extension of LSTM-DQN ( Narasimhan et al., 2015 ) to template-based action spaces. This is accomplished using three output heads: one for estimating the Q-Values over templates Q(s t , u)∀u ∈ T and two for estimating Q-Values Q(s t , o 1 ), Q(s t , o 2 )∀o i ∈ O over vocabulary to fill in the blanks of the tem- plate. The final executed action is constructed by greedily sampling from the predicted Q-values. Importantly, TDQN uses the same set of handicaps as KG-A2C allowing a fair comparison between these two algorithms.  Table 1  shows how KG-A2C fares across a diverse set of games supported by Jericho-testing the agent's ability to generalize to different genres, game structures, reward functions, and state-action spaces. KG-A2C matches or outperforms TDQN on 23 out of the 28 games that we test on. Our agent is thus shown to be capable of extracting a knowledge graph that can sufficiently constrain the template based action space to enable effective exploration in a broad range of games. In order to understand the contributions of different components of KG-A2C's architec- ture, we ablate KG-A2C's knowledge graph, template-action space, and valid-action loss. These ablations are performed on Zork1 4 and result in the following agents: A2C removes all components of KG-A2C's knowledge graph. In particular, the state em- bedding vector is now computed as s t = o t ⊕c t and the graph mask is not used to constrain ac- tion decoding. KG-A2C-no-gat remove's the Graph Attention network, but retains the graph masking compo- nents. The knowledge graph is still constructed as usual but the agent uses the same state em- bedding vector as A2C. KG-A2C-no-mask ablates the graph mask for purposes of action decoding. The knowledge graph is constructed as usual and the agent re- tains graph attention. On Zork1 as shown in  Figure 3 , we observe similar asymptotic performance between the all of the ablations - all reach approximately 34 points. This level of performance corresponds to a local optima where the agent collects the majority of available rewards without fighting the troll. Several other authors also report scores at this threshold ( Jain et al., 2019 ;  Zahavy et al., 2018 ). In terms of learning speed, the methods which have access to either the graph attention or the graph mask converge slightly faster than pure A2C which has neither. To further understand these differences we performed a larger study across the full set of games comparing KG-A2C-full with KG-A2C-no-mask. The results in Table 2 show KG-A2C-full outper- forms KG-A2C-no-mask on 10 games and is outperformed by KG-A2C-no-mask on 6. From this larger study we thus conclude the graph mask and knowledge graph are broadly useful components. We perform two final ablations to study the importance of the supervised valid-action loss and the template action space:

Section Title: KG-A2C-unsupervised
  KG-A2C-unsupervised In order to understand the importance of training with valid-actions, KG- A2C-unsupervised is not allowed to access the list of valid actions-the valid-action-losses L T and L O are disabled and L E now based on the full action set. Thus, the agent must explore the template action space manually. KG-A2C-unsupervised, when trained for the same number of steps as all the other agents, fails to achieve any score. We can infer that the valid action auxiliary loss remains an important part of the overall algorithm, and access to the knowledge graph alone is not yet sufficient for removing this auxiliary loss. KG-A2C-seq discards the template action space and instead decodes actions word by word up to a maximum of four words. A supervised cross-entropy-based valid action loss L Valid is now calculated by selecting a random valid action a tvalid ∈ Valid(s t ) and using each token in it as a target label. As this action space is orders of magnitude larger than template actions, we use teacher-forcing to enable more effective exploration while training the agent-executing a tvalid with a probability p valid = 0.5 and the decoded action otherwise. All other components remain the same as in the full KG-A2C. KG-A2C-seq reaches a relatively low asymptotic performance of 8 points. This agent, using a action space consisting of the full vocabulary, performs significantly worse than the rest of the agents even when given the handicaps of teacher forcing and being allowed to train for significantly longer- indicating that the template based action space is also necessary for effective exploration.

Section Title: CONCLUSION
  CONCLUSION Tabula rasa reinforcement learning offers an intuitive paradigm for exploring goal driven, contextu- ally aware natural language generation. The sheer size of the natural language action space, however, has proven to be out of the reach of existing algorithms. In this paper we introduced KG-A2C, a novel learning agent that demonstrates the feasibility of scaling reinforcement learning towards nat- ural language actions spaces with hundreds of millions of actions. The key insight to being able to efficiently explore such large spaces is the combination of a knowledge-graph-based state space and a template-based action space. The knowledge graph serves as a means for the agent to understand its surroundings, accumulate information about the game, and disambiguate similar textual obser- vations while the template-based action space lends a measure of structure that enables us to exploit that same knowledge graph for language generation. Together they constrain the vast space of possi- ble actions into the compact space of sensible ones. A suite of experiments across a diverse set of 28 human-made IF games shows wide improvement over TDQN, the current state-of-the-art template- based agent. Finally, an ablation study replicates state-of-the-art performance on Zork1 even though KG-A2C is using an action space six orders of magnitude larger than previous agents-indicating the overall efficacy of our combined state-action space.

```
